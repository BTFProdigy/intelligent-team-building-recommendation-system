Ranking suspected answers to natural language questions using 
predictive annotation 
Dragomir R. Radev" 
School of Information 
University of Michigan 
Ann Arbor, MI 48103 
radev@umich, edu 
John Prager 
TJ  Watson Research Center 
IBM Research Division 
Hawthorne, NY 10532 
jprager@us. ibm.com 
Valerie Samn* 
Teachers College 
Columbia University 
New York, NY 10027 
vs l l5@columbia.edu 
Abstract 
In this paper, we describe a system to rank sus- 
pected answers to natural language questions. 
We process both corpus and query using a new 
technique, predictive annotation, which aug- 
ments phrases in texts with labels anticipating 
their being targets of certain kinds of questions. 
Given a natural anguage question, our IR sys- 
tem returns a set of matching passages, which 
we then rank using a linear function of seven 
predictor variables. We provide an evaluation of 
the techniques based on results from the TREC 
Q&A evaluation in which our system partici- 
pated. 
1 Introduction 
Question Answering is a task that calls for a 
combination of techniques from Information Re- 
trieval and Natural Language Processing. The 
former has the advantage of years of develop- 
ment of efficient techniques for indexing and 
searching large collections of data, but lacks of 
any meaningful treatment of the semantics of 
the query or the texts indexed. NLP tackles 
the semantics, but tends to be computationally 
expensive. 
We have attempted to carve out a middle 
ground, whereby we use a modified IR system 
augmented by shallow NL parsing. Our ap- 
proach was motivated by the following problem 
with traditional IR systems. Suppose the user 
asks "Where did <some event> happen?". If 
the system does no pre-processing of the query, 
then "where" will be included in the bag of 
words submitted to the search engine, but this 
will not be helpful since the target text will 
be unlikely to contain the word "where". If 
the word is stripped out as a stop-word, then 
* The work presented in this paper was performed while 
the first and third authors were at 1BM Research. 
the search engine will have no idea that a lo- 
cation is sought. Our approach, called predic- 
tive annotation, is to augment he query with 
semantic ategory markers (which we call QA- 
Tokens), in this case with the PLACES to- 
ken, and also to label with QA-Tokens all oc- 
currences in text that are recognized entities, 
(for example, places). Then traditional bag-of- 
words matching proceeds uccessfully, and will 
return matching passages. The answer-selection 
process then looks for and ranks in these pas- 
sages occurrences ofphrases containing the par- 
ticular QA-Token(s) from the augmented query. 
This classification of questions is conceptually 
similar to the query expansion in (Voorhees, 
1994) but is expected to achieve much better 
performance since potentially matching phrases 
in text are classified in a similar and synergistic 
way. 
Our system participated in the official TREC 
Q&A evaluation. For 200 questions in the eval- 
uation set, we were asked to provide a list of 
50-byte and 250-byte xtracts from a 2-GB cor- 
pus. The results are shown in Section 7. 
Some techniques used by other participants in
the TREC evaluation are paragraph indexing, 
followed by abductive inference (Harabagiu and 
Maiorano, 1999) and knowledge-representation 
combined with information retrieval (Breck et 
al., 1999). Some earlier systems related to our 
work are FaqFinder (Kulyukin et al, 1998), 
MURAX (Kupiec, 1993), which uses an encyclo- 
pedia as a knowledge base from which to extract 
answers, and PROFILE (Radev and McKeown, 
1997) which identifies named entities and noun 
phrases that describe them in text. 
2 System description 
Our system (Figure 1) consists of two pieces: 
an IR component (GuruQA) that which returns 
matching texts, and an answer selection compo- 
150 
neat (AnSel/Werlect) that extracts and ranks 
potential answers from these texts. 
This paper focuses on the process of rank- 
ing potential answers selected by the IR engine, 
which is itself described in (Prager et al, 1999). 
~ lndexer 
~ Searc~x'~ GuruQA 
\ 
\ 
Rankcd ~ AnSel/ ~ Hit List 
H tL st \[ \[ Werlect I 
Answer selection 
Figure 1: System Architecture. 
2.1 The Information Retrieval 
component 
In the context of fact-seeking questions, we 
made the following observations: 
? In documents that contain the answers, the 
query terms tend to occur in close proxim- 
ity to each other. 
? The answers to fact-seeking questions are 
usually phrases: "President Clinton", "in 
the Rocky Mountains", and "today"). 
? These phrases can be categorized by a set of 
a dozen or so labels (Figure 2) correspond- 
ing to question types. 
? The phrases can be identified in text by 
pattern matching techniques (without full 
NLP). 
As a result, we defined a set of about 20 cat- 
egories, each labeled with its own QA-Token, 
and built an IR system which deviates from the 
traditional model in three important aspects. 
? We process the query against a set of ap- 
proximately 200 question templates which, 
may replace some of the query words 
with a set of QA-Tokens, called a SYN-  
class. Thus "Where" gets mapped 
to "PLACES", but "How long " goes 
to "@SYN(LENGTH$, DURATIONS)". 
Some templates do not cause complete re- 
placement of the matched string. For ex- 
ample, the pattern "What is the popula- 
tion" gets replaced by "NUMBERS popu- 
lation'. 
? Before indexing the text, we process it 
with Textract (Byrd and Ravin, 1998; 
Wacholder et al, 1997), which performs 
lemmatization, and discovers proper names 
and technical terms. We added a new 
module (Resporator) which annotates text 
segments with QA-Tokens using pattern 
matching. Thus the text "for 5 centuries" 
matches the DURATIONS pattern "for 
:CARDINAL _timeperiod", where :CAR- 
DINAL is the label for cardinal numbers, 
and _timeperiod marks a time expression. 
? GuruQA scores text passages instead of 
documents. We use a simple document- 
and collection-independent weighting 
scheme: QA-Tokens get a weight of 400, 
proper nouns get 200 and any other word 
- 100 (stop words are removed in query 
processing after the pattern template 
matching operation). The density of 
matching query tokens within a passage is 
contributes a score of 1 to 99 (the highest 
scores occur when all matched terms are 
consecutive). 
Predictive Annotation works best for Where, 
When, What, Which and How+adjective ques- 
tions than for How+verb and Why questions, 
since the latter are typically not answered by 
phrases. However, we observed that "by" + 
the present participle would usually indicate 
the description of a procedure, so we instan- 
tiate a METHODS QA-Token for such occur- 
rences. We have no such QA-Token for Why 
questions, but we do replace the word "why" 
with "@SYN(result, cause, because)", since the 
occurrence of any of these words usually beto- 
kens an explanation. 
3 Answer  select ion 
So far, we have described how we retrieve rel- 
evant passages that may contain the answer to 
a query. The output of GuruQA is a list of 
10 short passages containing altogether a large 
151 
QA-Token Question type Example 
PLACES 
COUNTRY$ 
STATES 
PERSONS 
ROLES 
NAMES 
ORG$ 
DURATIONS 
AGES 
YEARS 
TIMES 
DATES 
VOLUMES 
AREAS 
LENGTHS 
WEIGHTS 
NUMBERS 
METHODS 
RATES 
MONEYS 
Where 
Where/What country 
Where/What state 
Who 
Who 
Who/What/Which 
Who/What 
How long 
How old 
When/What year 
When 
When/What date 
How big 
How big 
How big/long/high 
How big/heavy 
How many 
How 
How much 
How much 
In the Rocky Mountains 
United Kingdom 
Massachusetts 
Albert Einstein 
Doctor 
The Shakespeare F stival 
The US Post Office 
For 5 centuries 
30 years old 
1999 
In the afternoon 
July 4th, 1776 
3 gallons 
4 square inches 
3 miles 
25 tons 
1,234.5 
By rubbing 
50 per cent 
4 million dollars 
Figure 2: Sample QA-Tokens. 
number (often more than 30 or 40) of potential 
answers in the form of phrases annotated with 
QA-Tokens. 
3.1 Answer  rank ing  
We now describe two algorithms, AnSel and 
Werlect, which rank the spans returned by Gu- 
ruQA. AnSel and Werlect 1 use different ap- 
proaches, which we describe, evaluate and com- 
pare and contrast. The output of either system 
consists of five text extracts per question that 
contain the likeliest answers to the questions. 
3.2 Sample  Input to AnSel /Wer lect  
The role of answer selection is to decide which 
among the spans extracted by GuruQA are 
most likely to contain the precise answer to the 
questions. Figure 3 contains an example of the 
data structure passed from GuruQA to our an- 
swer selection module. 
The input consists of four items: 
? a query (marked with <QUERY> tokens 
in the example), 
? a list of 10 passages (one of which is shown 
above), 
? a list of annotated text spans within the 
passages, annotated with QA-Tokens, and 
1 from ANswer SELect and ansWER seLECT, respec- 
tively 
? the SYN-class corresponding to the type of 
question (e.g., "PERSONS NAMES"). 
The text in Figure 3 contains five spans (po- 
tential answers), of which three ("Biography of 
Margaret Thatcher", "Hugo Young", and "Mar- 
garet Thatcher") are of types included in the 
SYN-class for the question (PERSON NAME). 
The full output of GuruQA for this question in- 
cludes a total of 14 potential spans (5 PERSONs 
and 9 NAMEs). 
3.3 Sample  Output  o f  AnSel /Wer lect  
The answer selection module has two outputs: 
internal (phrase) and external (text passage). 
Internal output:  The internal output is a 
ranked list of spans as shown in Table 1. It 
represents a ranked list of the spans (potential 
answers) sent by GuruQA. 
Externa l  output :  The external output is 
a ranked list of 50-byte and 250-byte xtracts. 
These extracts are selected in a way to cover 
the highest-ranked spans in the list of potential 
answers. Examples are given later in the paper. 
The external output was required for the 
TREC evaluation while system's internal out- 
put can be used in a variety of applications, e.g., 
to highlight the actual span that we believe is 
the answer to the question within the context 
of the passage in which it appears. 
152 
<p> <NUMBER> 1 </NUMBER> </p> 
<p><QUERY>Who is the author of the book, "The Iron Lady: A Biography of Margaret Thatcher"? 
</QUERY></p> 
<p> <PROCESSED_QUERY> @excwin(*dynamic* @weight (200 * Iron_Lady) @weight (200 
Biography_of_Margaret_Thatcher) @weight(200 Margaret) @weight(100 author) 
@weight(100 book) @weight(100 iron) @weight(100 lady) @weight(100 :) @weight(100 biography) 
@weight(100 thatcher) @weight(400 @syn(PERSON$ NAME$)))</PROCESSED_QUERY></p> 
<p> <DOC>LA090290-0118</DOC> </p> <p> <SCORE> 1020.8114</SCORE> d/p> 
<TEXT><p>THE IRON LADY; A <span class="NAME">Biography of Margaret Thatcher </span> 
by <span class--"PERSON">Hugo Y ung</span> (<span class='ORG">Farrar , Straus 
& Giroux</span>) The central riddle revealed here is why, as a woman <span class--'PLACEDEF'>in a 
man</span>'s world, <span class--'PERSON'>Margaret Thatcher</span> evinces uch an exclusionary 
attitude toward women.</p></TEXT> 
Figure 3: Input sent from GuruQA to AnSel/Werlect. 
Score Span 
5.06 
-8.14 
-13.60 
-18.00 
-19.38 
-26.06 
-31.75 
-32.38 
-36.78 
-42.68 
-198.34 
-217.80 
-234.55 
Hugo Young 
Biography of Margaret Thatcher 
David Williams 
Williams 
Sir Ronald Millar 
Santiago 
Oxford 
Maggie 
Seriously Rich 
FT 
Margaret Thatcher 
Thatcher 
Iron Lady 
Questlon/Answer (TR38) 
Q: Who was Johnny Mathis' high school 
track coach? 
A: Lou Vasquez 
Q: What year was the Magna Carta signed? 
A: 1215 
Q: What two companies produce bovine 
somatotropin? 
A: Monsanto and Eli Lilly 
Figure 4: Sample questions from TR38. 
Table 1: Ranked potential answers to Quest. 1. 
4 Ana lys i s  o f  corpus  and quest ion 
sets 
In this section we describe the corpora used for 
training and evaluation as well as the questions 
contained in the training and evaluation ques- 
tion sets. 
4.1 Corpus  analysis 
For both training and evaluation, we used the 
TREC corpus, consisting of approximately 2
GB of articles from four news agencies. 
4.2 Training set TR38 
To train our system, we used 38 questions (see 
Figure 4) for which the answers were provided 
by NIST. 
4.3 Test set T200 
The majority of the 200 questions (see Figure 5) 
in the evaluation set (T200) were not substan- 
tially different from these in TR38, although the 
introduction of "why" and "how" questions as 
well as the wording of questions in the format 
"Name X" made the task slightly harder. 
Questlon/Answer (T200) 
Q: Why did David Koresh ask the FBI for a 
word processor? 
A: to record his revelations. 
Q: How tall is the Matterhorn? 
A: 14,776 feet 9 inches 
Q: How tall is the replica of the Matterhorn 
at Disneyland? 
A: 147-foot 
Figure 5: Sample questions from T200. 
Some examples of problematic questions are 
shown in Figure 6. 
153 
Q: Why did David Koresh ask the FBI for 
a word processor? 
Q: Name the first private citizen to fly in 
space. 
Q: What is considered the costliest disaster 
the insurance industry has ever faced? 
Q: What did John Hinckley do to impress 
Jodie Foster? 
Q: How did Socrates die? 
Figure 6: Sample harder questions from T200. 
5 AnSe l  
AnSel uses an optimization algorithm with 7 
predictive variables to describe how likely a 
given span is to be the correct answer to a 
question. The variables are illustrated with ex- 
amples related to the sample question number 
10001 from TR38 "Who was Johnny Mathis' 
high school track coach?". The potential an- 
swers (extracted by GuruQA) are shown in Ta- 
ble 2. 
5.1 Feature selection 
The seven span features described below were 
found to correlate with the correct answers. 
Number :  position of the span among M1 spans 
returned from the hit-list. 
Rspanno:  position of the span among all spans 
returned within the current passage. 
Count:  number of spans of any span class re- 
trieved within the current passage. 
Notinq: the number of words in the span that 
do not appear in the query. 
Type: the position of the span type in the list 
of potential span types. Example: Type 
("Lou Vasquez") = 1, because the span 
type of "Lou Vasquez", namely "PER- 
SON" appears first in the SYN-class "PER- 
SON ORG NAME ROLE". 
Avgdst:  the average distance in words between 
the beginning of the span and query words 
that also appear in the passage. Example: 
given the passage "Tim O'Donohue, Wood- 
bridge High School's varsity baseball coach, 
resigned Monday and will be replaced by 
assistant Johnny Ceballos, Athletic Direc- 
tor Dave Cowen said." and the span "Tim 
O'Donohue", the value of avgdst is equal 
to 8. 
Sscore: passage relevance as computed by Gu- 
ruQA. 
Number :  the position of the span among all 
retrieved spans. 
5.2 AnSel  algorithm 
The TOTAL score for a given potential answer 
is computed as a linear combination of the fea- 
tures described in the previous ubsection: 
TOTAL  = ~ w~ , fi 
i 
The Mgorithm that the training component 
of AnSel uses to learn the weights used in the 
formula is shown in Figure 7. 
For each <question,span> tuple in training 
set : 
i. Compute features for each span 
2. Compute TOTAL score for each span 
using current set of weights 
Kepeat 
3. Compute performance on training 
set 
4. Adjust weights wi through 
logistic regression 
Until performance > threshold 
Figure 7: Training algorithm used by AnSel. 
Training discovered the following weights: 
Wnurnbe r -~ --0.3; Wrspann o -~ --0.5; Wcount : 
3 .0 ;  Wnot inq  = 2 .0 ;  Wtypes  = 15 .0 ;  Wavgdst  ---- 
-1.0; W~score = 1.5 
At runtime, the weights are used to rank po- 
tential answers. Each span is assigned a TO- 
TAL score and the top 5 distinct extracts of 
50 (or 250) bytes centered around the span are 
output. The 50-byte xtracts for question 10001 
are shown in Figure 8. For lack of space, we are 
omitting the 250-byte xtracts. 
6 Wer lec t  
The Werlect algorithm used many of the same 
features of phrases used by AnSel, but employed 
a different ranking scheme. 
6.1 Approach 
Unlike AnSel, Werlect is based on a two-step, 
rule-based process approximating a function 
with interaction between variables. In the first 
stage of this algorithm, we assign a rank to 
154 
Spat ,  
Ollie Matson 
Lou Vasquez 
Tim O'Donohue 
Athlet ic Director Dave Cowen 
Johnny Ceballos 
Civic Center Director Mart in Durham 
Johnny Hodges 
Derric Evans 
NEWSWIRE Johnny Majors 
Woodbr idge High School 
Evan 
Gary Edwards 
O. J .  Simpson 
South Lake Tahoe 
Washington High 
Morgan 
Tennessee football  
El l ington 
assistant 
the Volunteers 
J ohnny  Mathis 
Mathis 
coach 
Type  Nun lber  
PERSON 3 
PERSON 1 
PERSON 17 
PERSON 23 
PERSON 22 
PERSON 13 
PERSON 25 
PERSON 33 
PERSON 3O 
ORG 18 
PERSON 37 
PERSON 38 
NAME 2 
NAME 7 
NAME 10 
NAME 26 
NAME 31 
NAME 24 
ROLE 21 
ROLE 34 
PERSON 4 
NAME 14 
ROLE 19 
Rspanno Count  Not lnq  Type  Avgdst  Sscore  
3 6 2 I 12 0 .02507 
1 6 2 1 16 0 .02507 
1 4 2 I 8 0 .02257 
6 4 4 1 I I  0 .02257 
5 4 I I 9 0 .02257 
I 2 5 1 16 0 .02505 
2 4 I I 15 0 .02256 
4 4 2 l 14 0 .02256 
1 4 2 1 17 0 .02256 
2 4 1 2 6 0 .02257 
6 4 1 1 14 0 .02256 
7 4 2 1 17 0 .02256 
2 6 2 3 12 0 .02507 
5 6 3 3 14 0 .02507 
6 6 1 3 18 0 .02507 
3 4 1 3 12 0 .02256 
2 4 1 3 15 0 .02256 
1 4 1 3 20 0 .02256 
4 4 1 4 8 0 .02257 
5 4 2 4 14 0 .02256 
4 6 - I00  I I I  0 .02507 
2 2 -100 3 I0  0 .02505 
3 4 -100 4 4 0 .02257 
Table 2: Feature set and span rankings for training question 10001. 
Document  ID Score Extract 
LA053189-0069 892.5 
LA053189-0069 890.1 
LA060889-0181 887.4 
LA060889-0181 884.1 
LA060889-0181 880.9 
of O.J. Simpson , Ollie Matson and Johnny Mathis 
Lou Vasquez , track coach of O.J. Simpson , Ollie 
Tim O'Donohue, Woodbridge High School's varsity 
nny Ceballos , Athletic Director Dave Cowen said. 
aced by assistant Johnny Ceballos , Athletic Direc 
Figure 8: Fifty-byte extracts. 
TOTAL  
-7.53 
-9.93 
-12.57 
-15.87 
-19.07 
-19.36 
-25.22 
-25.37 
-25.47 
-28.37 
-29.57 
-30,87 
-37.40 
-40.06 
-49.80 
-52 .52  
-56.27 
-59.42 
-62.77 
-71.17 
-211.33 
-254.16 
-259.67 
every relevant phrase within each sentence ac- 
cording to how likely it is to be the target an- 
swer. Next, we generate and rank each N-byte 
fragment based on the sentence score given by 
GuruQA, measures of the fragment's relevance, 
and the ranks of its component phrases. Unlike 
AnSel, Werlect was optimized through manual 
trial-and-error using the TR38 questions. 
6.2 Step  One: Feature  Se lect ion  
The features considered in Werlect that were 
also used by AnSel, were Type, Avgdst and Ss- 
core. Two additional features were also taken 
into account: 
Not inqW:  a modified version of Notinq. As 
in AnSel, spans that are contained in the 
query are given a rank of 0. However, par- 
tial matches are weighted favorably in some 
cases. For example, if the question asks, 
"Who was Lincoln's Secretary of State?" 
a noun phrase that contains "Secretary of 
State" is more likely to be the answer than 
one that does not. In this example, the 
phrase, "Secretary of State William Se- 
ward" is the most likely candidate. This 
criterion also seems to play a role in the 
event that Resporator fails to identify rel- 
evant phrase types. For example, in the 
training question, "What shape is a por- 
poise's tooth?" the phrase "spade-shaped" 
is correctly selected from among all nouns 
and adjectives of the sentences returned by 
Guru-QA. 
F requency :  how often the span occurs across 
different passages. For example, the test 
question, "How many lives were lost in the 
Pan Am crash in Lockerbie, Scotland?" re- 
sulted in four potential answers in the first 
two sentences returned by Guru-QA. Ta- 
ble 3 shows the frequencies of each term, 
and their eventual influence on the span 
rank. The repeated occurrence of "270", 
helps promote it to first place. 
6.3 Step  two:  rank ing the  sentence  
spans  
After each relevant span is assigned a rank, we 
rank all possible text segments of 50 (or 250) 
bytes from the hit list based on the sum of the 
phrase ranks plus additional points for other 
words in the segment that match the query. 
The algorithm used by Werlect is shown in 
Figure 9. 
155 
I n i t ia l  Sentence  Rank  Phrase  F requency  Span Rank  
1 Two 5 2 
1 365 million 1 3 
1 11 1 4 
2 270 7 1 (ranked highest) 
Table 3: Influence of frequency on span rank. 
i. Let cand idate_set  = all potential 
answers, ranked and sorted. 
2. For each hit-l ist passage, extract 
ali spans of 50 (or 250) bytes, on 
word boundaries. 
3. Rank and sort all segments based 
on phrase ranks, matching terms, 
and sentence ranks. 
4. For each candidate in sorted 
candidate_set 
- Let highest_ranked_span 
= highest-ranked span 
containing candidate 
- Let answer_set\[i++\] = 
h ighest_ rankedspan 
- Remove every candidate from 
candidate_set that is found in 
h ighest_ rankedspan 
- Exit if i > 5 
5. Output answer_set 
noted that on the 14 questions we were unable 
to classify with a QA-Token, Werlect (runs W50 
and W250) achieved an MRAR of 3.5 to Ansel's 
2.0. 
The cumulative RAR of A50 on T200 (Ta- 
ble 4) is 63.22 (i.e., we got 49 questions among 
the 198 right from our first try and 39 others 
within the first five answers). 
The performance of A250 on T200 is shown 
in Table 5. We were able to answer 71 questions 
with our first answer and 38 others within our 
first five answers (cumulative RAR = 85.17). 
To better characterize the performance of our 
system, we split the 198 questions into 20 groups 
of 10 questions. Our performance on groups 
of questions ranged from 0.87 to 5.50 MRAR 
for A50 and from 1.98 to 7.5 MRAR for A250 
(Table 6). 
Figure 9: Algorithm used by Werlect. 
7 Eva luat ion  
In this section, we describe the performance of
our system using results from our four official 
runs. 
7.1 Evaluat ion scheme 
For each question, the performance is computed 
as the reciprocal value of the rank (RAR) of 
the highest-ranked correct answer given by the 
system. For example, if the system has given 
the correct answer in three positions: second, 
third, and fifth, RAR for that question is ! 2" 
The Mean Reciprocal Answer Rank (MRAR) 
is used to compute the overall performance of
systems participating in the TREC evaluation: 
RAR - rank i  MRAR = . rank i  ) 
$ 
7.2 Per fo rmance  on the official 
evaluat ion data 
Overall, Ansel (runs A50 and A25) performed 
marginally better than Werlect. However, we 
50 bytes 250 bytes  
n 
Avg 
Min 
Max 
Std Dev 
20 
3.19 
0.87 
5.50 
1.17 
20 
4.30 
1.98 
7.50 
1.27 
Table 6: Performance on groups of ten questions 
Finally, Table 7 shows how our official runs 
compare to the rest of the 25 official submis- 
sions. Our performance using AnSel and 50- 
byte output was 0.430. The performance of 
Werlect was 0.395. On 250 bytes, AnSel scored 
0.319 and Werlect - 0.280. 
8 Conc lus ion  
We presented a new technique, predict ive an- 
notat ion,  for finding answers to natural an- 
guage questions in text corpora. We showed 
that a system based on predictive annotation 
can deliver very good results compared to other 
competing systems. 
We described a set of features that correlate 
with the plausibility of a given text span be- 
ing a good answer to a question. We experi- 
156 
nb of cases 
Points 
nb of cases 
Points 
First I Second Third Fourth Fifth TOTAL 
49 I 15 ll 9 4 88 
49.00 7.50 3.67 2.25 0.80 63.22 
Table 4: Performance of A50 on T200 
First Second Third Fourth Fifth TOTAL 
71 16 11 6 5 109 
71.00 8.00 3.67 1.50 1.00 85.17 
Run 
Table 
Median Average 
W50 0.12 
A50 0.12 
W250 0.29 
A250 0.29 
5: Performance of A250 on 
Our Average 
0.280 
0.319 
0.395 
0.430 
T200 
Nb Times Nb Times 
> Median -=- Median 
56 126 
72 112 
60 106 
66 110 
Nb Times 
< Median 
16 
14 
32 
22 
Table 7: Comparison of our system with the other participants 
mented with two algorithms for ranking poten- 
tial answers based on these features. We discov- 
ered that a linear combination of these features 
performs better overall, while a non-linear algo- 
rithm performs better on unclassified questions. 
9 Acknowledgments  
We would like to thank Eric Brown, Anni Co- 
den, and Wlodek Zadrozny from IBM Research 
for useful comments and collaboration. We 
would also like to thank the organizers of the 
TREC Q~zA evaluation for initiating such a 
wonderful research initiative. 
References  
Eric Breck, John Burger, David House, Marc 
Light, and Inderjeet Mani. 1999. Ques- 
tion answering from large document collec- 
tions. In Proceedings of AAAI Fall Sympo- 
sium on Question Answering Systems, North 
Falmouth, Massachusetts. 
Roy Byrd and Yael Ravin. 1998. Identifying 
and extracting relations in text. In Proceed- 
ings of NLDB, Klagenfurt, Austria. 
Sanda Harabagiu and Steven J. Maiorano. 
1999. Finding answers in large collections of 
texts : Paragraph indexing + abductive in- 
ference. In Proceedings ofAAAI Fall Sympo- 
sium on Question Answering Systems, North 
Falmouth, Massachusetts. 
Vladimir Kulyukin, Kristian Hammond, and 
Robin Burke. 1998. Answering questions 
for an organization online. In Proceedings of
AAAI, Madison, Wisconsin. 
Julian M. Kupiec. 1993. MURAX: A robust 
linguistic approach for question answering us- 
ing an ondine encyclopedia. In Proceedings, 
16th Annual International ACM SIGIR Con- 
ference on Research and Development in In- 
formation Retrieval. 
John Prager, Dragomir R. Radev, Eric Brown, 
Anni Coden, and Valerie Samn. 1999. The 
use of predictive annotation for question an- 
swering in TREC8. In Proceedings o/TREC- 
8, Gaithersburg, Maryland. 
Dragomir R. Radev and Kathleen R. McKe- 
own. 1997. Building a generation knowledge 
source using internet-accessible newswire. In 
Proceedings ofthe 5th Conference on Applied 
Natural Language Processing, pages 221-228, 
Washington, DC, April. 
Ellen Voorhees. 1994. Query expansion using 
lexical-semantic relations. In Proceedings of
A CM SIGIR, Dublin, Ireland. 
Nina Wacholder, Yael Ravin, and Misook Choi. 
1997. Disambiguation of proper names in 
text. In Proceedings ofthe Fifth Applied Nat- 
ural Language Processing Conference, Wash- 
ington, D.C. Association for Computational 
Linguistics. 
157 
  
	 
	Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 313?320
Manchester, August 2008
Tracking the Dynamic Evolution of Participant Salience in a Discussion
Ahmed Hassan
University of Michigan
hassanam@umich.edu
Anthony Fader
University of Michigan
afader@umich.edu
Michael H. Crespin
University of Georgia
crespin@uga.edu
Kevin M. Quinn
Harvard University
kquinn@fsa.harvard.edu
Burt L. Monroe
Pennsylvania State University
burtmonroe@psu.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a technique for analyzing the
temporal evolution of the salience of par-
ticipants in a discussion. Our method can
dynamically track how the relative impor-
tance of speakers evolve over time using
graph based techniques. Speaker salience
is computed based on the eigenvector cen-
trality in a graph representation of partici-
pants in a discussion. Two participants in a
discussion are linked with an edge if they
use similar rhetoric. The method is dy-
namic in the sense that the graph evolves
over time to capture the evolution inher-
ent to the participants salience. We used
our method to track the salience of mem-
bers of the US Senate using data from the
US Congressional Record. Our analysis
investigated how the salience of speakers
changes over time. Our results show that
the scores can capture speaker centrality
in topics as well as events that result in
change of salience or influence among dif-
ferent participants.
1 Introduction
There are several sources of data that record
speeches or participations in debates or discus-
sions among a group of speakers or participants.
Those include parliamentary records, blogs, and
news groups. This data represents a very important
and unexploited source of information that con-
tains several trends and ideas. In any debate or
discussion, there are certain types of persons who
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
influence other people and pass information or ad-
vice to them. Those persons are often regarded
as experts in the field or simply influential peo-
ple and they tend to affect the ideas and rhetoric
of other participants. This effect can be tracked
down by tracking the similarity between different
speeches. We can then imagine a debate with many
people arguing about many different things as a
network of speeches or participations interacting
with each other. We can then try to identify the
most salient or important participants by identify-
ing the most central speeches in this network and
associating them with their speakers. When we
have a large dataset of debates and conversations
that expand over a long period of time, the salience
of participants becomes a dynamic property that
changes over time. To capture this dynamic nature
of the process, the graph of speeches must evolve
over time such that we have a different graph at
each instance of time that reflects the interaction
of speeches at this instant.
We apply our method to the US Congressional
Record. The US Congressional Record documents
everything said and done in the US Congress
House and Senate. The speeches in this data set
are made by a large number of people over a long
period of time. Using political speeches as test
data for the proposed method adds an extra layer
of meaning onto the measure of speakers salience.
Speaker salience of the Congress members can re-
flect the importance or influence in the US leg-
islative process. The way salience scores evolve
over time can answer several interesting issues like
how the influence of the speakers vary with major-
ity status and change of party control. It can also
study the dynamics of the relative distribution of
attention to each topic area in different time peri-
ods.
313
The rest of this paper will proceed as follows.
Section 2 reviews some related work. In Section 3,
we describe how the data can be clustered into dif-
ferent topic clusters. In Section 4, we describe
our method for computing the salience of different
participant in a discussion, we also describe how
to the network of speakers varies over time. Sec-
tion 5 describes the experimental setup. Finally,
we present the conclusions in Section 6.
2 Related Work
Several methods have been proposed for identify-
ing the most central nodes in a network. Degree
centrality, closeness, and betweenness (Newman,
2003) are among the most known methods for
measuring centrality of nodes in a network. Eigen-
vector centrality is another powerful method that
that has been applied to several types of networks.
For example it has been used to measure cen-
trality in hyperlinked web pages networks (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et
al., 2004).
The interest of applying natural language pro-
cessing techniques in the area of political science
has been recently increasing.
(Quinn et al, 2006) introduce a multinomial
mixture model to cluster political speeches into
topics or related categories. In (Porter et al, 2005),
a network analysis of the members and committees
of the US House of Representatives is performed.
The authors prove that there are connections link-
ing some political positions to certain committees.
This suggests that there are factors affecting com-
mittee membership and that they are not deter-
mined at random. In (Thomas et al, 2006), the au-
thors try to automatically classify speeches, from
the US Congress debates, as supporting or oppos-
ing a given topic by taking advantage of the voting
records of the speakers. (Fader et al, 2007) in-
troduce MavenRank , which is a method based on
lexical centrality that identifies the most influen-
tial members of the US Senate. It computes a sin-
gle salience score for each speaker that is constant
over time.
In this paper, we introduce a new method for
tracking the evolution of the salience of partici-
pants in a discussion over time. Our method is
based on the ones described in (Erkan and Radev,
2004; Mihalcea and Tarau, 2004; Fader et al,
2007), The objective of this paper is to dynami-
cally rank speakers or participants in a discussion.
The proposed method is dynamic in the sense that
the computed importance varies over time.
3 Topic Clusters
Before applying the proposed method to a data
set with speeches in multiple topics, we first need
to divide the speech documents into topic clus-
ters. We used the model described in (Quinn et al,
2006) for this purpose. The model presented in this
paper assumes that the probabilities of a document
belonging to a certain topic varies smoothly over
time and the words within a given document have
exactly the same probability of being drawn from
a particular topic (Quinn et al, 2006). These two
properties make the model different than standard
mixture models (McLachlan and Peel, 2000) and
the latent Dirichlet alocation model of (Blei et al,
2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006).
The output from the topic model is a D ? K
matrix Z where D is the number of speeches , K
is the number of topics and the element z
dk
repre-
sents the probability of the dth speech being gen-
erated by topic k. We then assign each speech d
to the kth cluster where k = argmax
j
z
dj
. If the
maximum value is not unique, one of the clusters
having the maximum value is arbitrary selected.
4 Speaker Centrality
In this section we describe how to build a network
of speeches and use it to identify speaker centrality.
We also describe how to generate different projec-
tions of the network at different times, and how
to use those projection to get dynamic salience
scores.
4.1 Computing Speaker Salience
The method we used is similar to the methods de-
scribed in (Erkan and Radev, 2004; Mihalcea and
Tarau, 2004; Kurland and Lee, 2005), which were
originally used for ranking sentences and docu-
ments in extractive summarization and information
retrieval systems.
A collection of speeches can be represented as
a network where similar speeches are linked to
each other. The proposed method is based on
314
the premise that important speeches tend to be
lexically similar to other important speeches, and
important speeches tend to belong to important
speakers. Hence given a collection of speeches and
a similarity measure, we can build a network and
define the centrality score of a speech recursively
in terms of the scores of other similar speeches.
Later, we can compute the salience of a speaker
as the sum of the centrality measure of all his
speeches.
To measure the similarity between two
speeches, we use the bag-of-words model to repre-
sent each sentence as an N-dimensional vector of
tf-idf scores, where N is the number of all possible
words in the target language. The similarity
between two speeches is then computed using the
cosine similarity between the two vectors.
A vector of term frequencies is used to represent
each speech. Those term frequencies are weighted
according to the relative importance of the given
term in the cluster.
The vectors representing speeches contain term
frequencies (or tf), which are weighted according
to their inverse document frequencies to account
for the relative importance of the given term in the
cluster. The inverse document frequency of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
n
w
)
(1)
where n
w
is the number of speeches in the clus-
ter containing the term w, and N is the number of
documents in the cluster. We calculated idf values
specific to each topic, rather than to all speeches.
We preferred to use topic-specific idf values be-
cause the relative importance of words may vary
from one topic to the other.
The tf-idf cosine similarity measure is computed
as the cosine of the angle between the tf-idf vec-
tors. It is defined as follows:
P
w?u,v
tf
u
(w) tf
v
(w) idf(w)
2
?
P
w?u
(tf
u
(w) idf(w))
2
?
P
w?v
(tf
v
(w) idf(w))
2
, (2)
The choice of tf-idf scores to measure speech
similarity is an arbitrary choice. Some other possi-
ble similarity measures are edit distance, language
models (Kurland and Lee, 2005), or generation
probabilities (Erkan, 2006).
The recursive definition of the score of any
speech s in the speeches network is given by
p(s) =
?
t?adj[s]
p(t)
deg(t)
(3)
where deg(t) is the degree of node t, and adj[s] is
the set of all speeches adjacent to s in the network.
This can be rewritten in matrix notation as:
p = pB (4)
where p = (p(s
1
), p(s
2
), . . . , p(s
N
)) and the ma-
trix B is the row normalized similarity matrix of
the graph
B(i, j) =
S(i, j)
?
k
S(i, k)
(5)
where S(i, j) = sim(s
i
, s
j
). Equation (4) shows
that the vector of salience scores p is the left eigen-
vector of B with eigenvalue 1.
The matrix B can be thought of as a stochastic
matrix that acts as transition matrix of a Markov
chain. An element X(i, j) of a stochastic matrix
specifies the transition probability from state i to
state j in the corresponding Markov chain. And
the whole process can be seen as a Markovian ran-
dom walk on the speeches graph. To help the ran-
dom walker escape from periodic or disconnected
components, (Brin and Page, 1998) suggests re-
serving a small escape probability at each node
that represents a chance of jumping to any node
in the graph, making the Markov chain irreducible
and aperiodic, which guarantees the existence of
the eigenvector.
Equation (4) can then be rewritten, assuming a
uniform escape probability, as:
p = p[dU+ (1 ? d)B] (6)
where N is the total number of nodes, U is a
square matrix with U(i, j) = 1/N for all i, j, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998).
4.2 Dynamic Salience Scores
We use the time stamps associated with the data to
compute dynamic salience scores p
T
(u) that iden-
tify central speakers at some time T . To do this,
we create a speech graph that evolves over time.
Let T be the current date and let u and v be two
speech documents that occur on days t
u
and t
v
.
Our goal is to discount the lexical similarity of u
and v based on how far apart they are. One way
to do this is by defining a new similarity measure
s(u, v;T ) as:
s(u, v;T ) = tf-idf-cosine(u, v) ? f(u, v;T ) (7)
315
where f(u, v;T ) is a function taking values in
[0, 1].
If f(u, v;T ) = 1 for all u, v, and T , then time is
ignored when calculating similarity and p
T
(u) =
p(u). On the other hand, suppose we let
f(u, v;T ) =
{
1 if t
u
= t
v
= T ,
0 else.
(8)
This removes all edges that link a speech, occur-
ring at some time T , to all other speeches occur-
ring at some time other than T and the ranking al-
gorithm will be run on what is essentially the sub-
graph of documents restricted to time T (although
the isolated speech documents will receive small
non-zero scores because of the escape probability
from Section 4.1). These two cases act as the ex-
treme boundaries of possible functions f : in the
first case time difference has no effect on document
similarity, while in the second case two documents
must occur on the same day to be similar.
We use the following time weight functions in
our experiments. In each case, we assume that the
speeches represented by speech documents u and v
have already occurred, that is, t
u
, t
v
? T . We will
use the convention that f(u, v;T ) = 0 if t
u
> T
or t
v
> T for all time weight functions, which
captures the idea that speeches that have not yet
occurred have no influence on the graph at time T .
Also define
age(u, v;T ) = T ? min{t
u
, t
v
} (9)
which gives the age of the oldest speech document
from the pair u, v at time T .
? Exponential: Given a parameter a > 0, define
f
exp,a
(u, v;T ) = e
?a age(u,v;T )
. (10)
This function will decrease the impact of sim-
ilarity as time increases in an exponential
fashion. a is a parameter that controls how
fast this happens, where a larger value of a
makes earlier speeches have a small impact
on current scores and a smaller value of a
means that earlier speeches will have a larger
impact on current scores.
? Linear: Given b > 0, define
f
lin,d
(u, v;T ) =
?
?
?
?
?
1 ?
1
b
age(u, v;T )
if age(u, v;T ) ? b
0 if age(u, v;T ) > b
(11)
Figure 1: The Dynamic boundary cases for Sena-
tor Santorum.
This function gives speech documents that
occur at time T full weight and then decreases
their weight linearly towards time T + b,
where it becomes 0.
? Boundary: Given d ? 0, define
f
bnd,d
(u, v;T ) =
{
1 if age(u, v;T ) ? d
0 if age(u, v;T ) > d
(12)
This function gives speech documents occur-
ring within d days of T the regular tf-idf sim-
ilarity score, but sets the similarity of speech
documents occurring outside of d days to 0.
The case when d = 0 is one of the boundary
cases explained above.
Figure 1 gives an example of different time
weighting functions for Senator Rick Santorum
(R - Pennsylvania) on topic 22 (Abortion) during
1997, the first session of the 105th Congress. The
dashed line shows the case when time has no ef-
fect on similarity (his score is constant over time),
while the solid line shows the case where only
speeches on the current day are considered simi-
lar (his score spikes only on days where he speaks
and is near zero otherwise). The dotted line shows
the case when the influence of older speeches de-
creases exponentially, which is more dynamic than
the first case but smoother than the second case.
5 Experiments and Results
5.1 Data
We used the United States Congressional Speech
corpus (Monroe et al, 2006) in our experiment.
316
This corpus is in XML formatted version of the
electronic United States Congressional Record
from the Library of Congress
1
. The Congressional
Record is a verbatim transcript of the speeches
made in the US House of Representatives and Sen-
ate and includes tens of thousands of speeches per
year (Monroe et al, 2006). The data we used cover
the period from January 2001 to January 2003.
5.2 Experimental Setup
We used results from (Quinn et al, 2006) to get
topic clusters from the data, as described in Sec-
tion 3. The total number of topics was 42. The
average sized topic cluster had several hundred
speech documents (Quinn et al, 2006).
We set up a pipeline using a Perl implementa-
tion of the proposed method We ran it on the topic
clusters and ranked the speakers based on the cen-
trality scores of their speeches. The graph nodes
were speech documents. A speaker?s score was
determined by the average of the scores of the
speeches given by that speaker. After comparing
the different time weighting function as shown in
Figure 1, we decided to use the exponential time
weight function for all the experiments discussed
below. Exponential time weighting function de-
creases the impact of similarity as time increases
in an exponential fashion. It also allows us to con-
trol the rate of decay using the parameter a.
5.3 Baseline
We compare the performance of our system to
a simple baseline that calculates the salience of
a speaker as a weighted count of the number of
times he has spoken. The baseline gives high
weight to recent speeches . The weight decreases
as the speeches gets older. The salience score of a
speaker is calculate as follows:
BS(i) =
?
d
?
d
0
?d
? S
i
d
(13)
Where BS(i) is the baseline score of speaker i,
? is the discounting factor, d
0
is the current date,
and S
i
d
is the number of speeches made by speaker
i at date d. We used ? = 0.9 for all our experi-
ments.
5.4 Results
One way to evaluate the dynamic salience scores,
is to look at changes when party control of the
1
http://thomas.loc.gov
chamber switches. Similar to (Hartog and Mon-
roe, 2004), we exploit the party switch made by
Senator Jim Jeffords of Vermont and the result-
ing change in majority control of the Senate dur-
ing the 107th Congress as a quasi-experimental
design. In short, Jeffords announced his switch
on May 24, 2001 from Republican to Independent
status, effective June 6, 2001. Jeffords stated that
he would vote with the Democrats to organize the
Senate, giving the Democrats a one-seat advantage
and change control of the Senate from the Repub-
licans back to the Democrats. This change of ma-
jority status during the 107th Congress allows us
to ignore many of the factors that could potentially
influence dynamic salience scores at the start of a
new congress.
On average, we expect committee chairs or a
member of the majority party to be the most im-
portant speaker on each topic followed by ranking
members or a member of the minority party. If
our measure is capturing dynamics in the central-
ity of Senators, we expect Republicans to be more
central before the Jeffords switch and Democrats
becoming central soon afterwards, assuming the
topic is being discussed on the Senate floor. We
show that the proposed technique captures several
interesting events in the data and also show that the
baseline explained above fails to capture the same
set of events.
Figure 2(a) shows the dynamic salience scores
over time for Senator John McCain (R - Arizona)
and Senator Carl Levin (D - Michigan) on topic
5 (Armed Forces 2) for the 107th Senate. Mc-
Cain was the most salient speaker for this topic
until June 2001. Soon after the change in major-
ity status a switch happened and Levin, the new
chair of Senate Armed Services, replaced McCain
as the most salient speaker. On the other hand,
Figure 2(b) shows the baseline scores for the same
topic and same speakers. We notice here that the
baseline failed to capture the switch of salience
near June 2001.
We can also observe similar behavior in Fig-
ure 3(a). This figure shows how Senate Majority
Leader Trent Lott (R - Mississippi) was the most
salient speaker on topic 35 (Procedural Legisla-
tion) until July 2001. Topic 35 does not map to
a specific committee but rather is related to ma-
neuvering bills through the legislative process on
the floor, a job generally delegated to members in
the Senate leadership. Just after his party gained
317
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2 (Infra
structure
)??, Expo
nential, a
=0.02, th
=
MCCA
IN
LEVIN
CARL
(a) Dynamic Lexrank
 
0
 
2
 
4
 
6
 
8
 
10 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
MCCA
IN
LEVIN
CARL
(b) Baseline
Figure 2: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2).
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Proce
dural 4
 (Legisla
ton 2)??, E
xponenti
al, a=0.0
2, th= REID LOTT
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Proced
ural 4 (L
egislaton
 2)??
REID LOTT
(b) Baseline
Figure 3: The Switch of Speakers Salience near Jun 2001 for Topic 35(Procedural Legislation).
majority status, Senator Harry Reid (D - Nevada)
became the most salient speaker for this topic. This
is consistent with Reid?s switch from Assistant mi-
nority Leader to Assistant majority Leader. Again
the baseline scores for the same topic and speakers
in Figure 3(b) fails to capture the switch.
An even more interesting test would be to check
whether the Democrats in general become more
central than Republicans after the Jeffords switch.
Figure 4(a) shows the normalized sum of the
scores of all the Democrats and all the Republicans
on topic 5 (Armed Forces 2) for the 107th Senate.
The figure shows how the Republicans were most
salient until soon after the Jeffords switch when the
Democrats regained the majority and became more
salient. We even discovered similar behavior when
we studied how the average salience of Democrats
and Republicans change across all topics. This is
shown in Figure 5(a) where we can see that the
Republicans were more salient on average for all
topics until June 2001. Soon after the change in
majority status, Democrats became more central.
Figures 4(b) and 5(b) show the same results using
the baseline system. We notice that the number of
speeches made by the Democrats and the Repub-
licans is very similar in most of the times. Even
when one of the parties has more speeches than
the other, it does not quite reflect the salience of
the speakers or the parties in general.
An alternative approach to evaluate the dynamic
scores is to exploit the cyclical nature of the leg-
islative process as some bills are re-authorized on
a fairly regular time schedule. For example, the
farm bill comes due about every five years. As a
new topic is coming up for debate, we expect the
saliency scores for relevant legislators to increase.
Figure 6 shows the dynamic scores of Senator
Thomas Harkin (D - Iowa), and Senator Richard
318
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2??
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20
 
25
 
30 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
Democ
rates
Repub
licans
(b) Baseline
Figure 4: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2), Republicans vs
Democrats.
 
10
 
12
 
14
 
16
 
18
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107
Democ
rates
Repub
licans
(b) Baseline
Figure 5: The Switch of Speakers Salience near Jun 2001 for All Topics, Republicans vs Democrats.
Lugar (R - Indiana) during the 107th senate on
topic 24 (Agriculture). The two senators were
identified, by the proposed method, as the most
salient speakers for this topic, as expected, since
they both served as chairmen of the Senate Com-
mittee on Agriculture, Nutrition, and Forestry
when their party was in the majority during the
107th Senate. This committee was in charge of
shepherding the Farm Bill through the Senate. The
scores of both senators on the agriculture topic sig-
nificantly increased starting late 2001 until June
2002. The debate began on the bill starting in
September of 2001 and it was not passed until May
2002.
6 Conclusion
We presented a graph based method for analyz-
ing the temporal evolution of the salience of par-
ticipants in a discussion. We used this method to
track the evolution of salience of speakers in the
US Congressional Record. We showed that the
way salience scores evolve over time can answer
several interesting issues. We tracked how the in-
fluence of the speakers vary with majority status
and change of party control. We also show how
a baseline system that depends on the number of
speeches fails to capture the interesting events cap-
tured by the proposed system. We also studied the
dynamics of the relative distribution of attention to
each topic area in different time periods and cap-
tured the cyclical nature of the legislative process
as some bills are re-authorized on a fairly regular
time schedule.
319
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Agric
ulture??
, Expon
ential, 
a=0.02
, th= LUG
AR HARKI
N
Figure 6: The Farm Bill Discussions on the Rela-
tive Distribution of Attention to Topic 24 (Agricul-
ture).
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Blei, David and John Lafferty. 2006. Dynamic topic
models. In ICML 2006.
Blei, David, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
CNIS, 30(1?7):107?117.
Erkan, G?unes? and Dragomir Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, Gunes. 2006. Language model-based document
clustering using random walks. In HLT/NAACL
2006, pages 479?486. Association for Computa-
tional Linguistics.
Fader, Anthony, Dragomir Radev, Michael Crespin,
Burt Monroe, Kevin Quinn, and Michael Colaresi.
2007. Mavenrank: Identifying influential members
of the us senate using lexical centrality. In EMNLP
2007.
Hartog, Chris Den and Nathan Monroe. 2004. The
value of majority status: The effect of jeffords?s
switch on asset prices of republican and democratic
firms. Legislative Studies Quarterly, 33:63?84.
Kleinberg, Jon. 1998. Authoritative sources in a hyper-
linked environment. In the ACM-SIAM Symposium
on Discrete Algorithms, pages 668?677.
Kurland, Oren and Lillian Lee. 2005. PageRank with-
out hyperlinks: Structural re-ranking using links in-
duced by language models. In SIGIR 2005, pages
306?313.
Kurland, Oren and Lillian Lee. 2006. Respect my au-
thority! HITS without hyperlinks, utilizing cluster-
based language models. In SIGIR 2006, pages 83?
90.
McLachlan, Geoffrey and David Peel. 2000. Finite
Mixture Models. New York: Wiley.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In EMNLP 2004.
Mihalcea, Rada, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application
to word sense disambiguation. In COLING 2004,
pages 1126?1132.
Monroe, Burt, Cheryl Monroe, Kevin Quinn, Dragomir
Radev, Michael Crespin, Michael Colaresi, Anthony
Fader, Jacob Balazer, and Steven Abney. 2006.
United states congressional speech corpus. Depart-
ment of Political Science, The Pennsylvania State
University.
Newman, Mark. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Porter, Mason, Peter Mucha, Miark Newman, and
Casey Warmbrand. 2005. A network analysis of
committees in the U.S. House of Representatives.
PNAS, 102(20).
Quinn, Kevin, Burt Monroe, Michael Colaresi, Michael
Crespin, and Dragomir Radev. 2006. An automated
method of topic-coding legislative speech over time
with application to the 105th-108th U.S. senate. In
Midwest Political Science Association Meeting.
Sparck-Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(1):11?20.
Thomas, Matt, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
320
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 609?616
Manchester, August 2008
Detecting multiple facets of an event using graph-based unsupervised
methods
Pradeep Muthukrishnan
Dept of EECS
University of Michigan
mpradeep@umich.edu
Joshua Gerrish
School of Information
University of Michigan
jgerrish@umich.edu
Dragomir R. Radev
Dept of EECS &
School of Information,
University of Michigan
radev@umich.edu
Abstract
We propose a new unsupervised method
for topic detection that automatically iden-
tifies the different facets of an event. We
use pointwise Kullback-Leibler divergence
along with the Jaccard coefficient to build
a topic graph which represents the com-
munity structure of the different facets.
The problem is formulated as a weighted
set cover problem with dynamically vary-
ing weights. The algorithm is domain-
independent and generates a representa-
tive set of informative and discriminative
phrases that cover the entire event. We
evaluate this algorithm on a large collec-
tion of blog postings about different news
events and report promising results.
1 Introduction
Finding a list of topics that a collection of docu-
ments cover is an important problem in informa-
tion retrieval. Topics can be used to describe or
summarize the collection, or they can be used to
cluster the collection. Topics provide a short and
informative description of the documents that can
be used for quickly browsing and finding related
documents.
Inside a given corpus, there may be multiple top-
ics. Individual documents can also contain multi-
ple topics.
Traditionally, information retrieval systems re-
turn a ranked list of query results based on the
similarity between the user?s query and the docu-
ments. Unfortunately, the results returned will of-
ten be redundant. Users may need to reformulate
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
their search to find the specific topic they are in-
terested in. This active searching process leads to
inefficiencies, especially in cases where queries or
information needs are ambiguous.For example, a
user wants to get an overview of the Virginia tech
shootings, then the first query he/she might try is
?Virginia tech shooting?. Most of the results re-
turned would be posts just mentioning the shoot-
ings and the death toll. But the user might want
a more detailed overview of the shootings. Thus
this leads to continuously reformulating the search
query to discover all the facets of the event.
2 Related Work
Topic detection and tracking was studied exten-
sively on newswire and broadcast collections by
the NIST TDT research program (Allan et al, ).
The large number of people blogging on the web
provides a new source of information for topic de-
tection and tracking.
The TDT task defines topics as ?an event or ac-
tivity, along with all directly related events and ac-
tivities.? In this paper we will stay with this defini-
tion of topic.
Zhai et al proposed several methods for dealing
with a related task, which they called subtopic re-
trieval (Zhai et al, 2003). This is an information
retrieval task where the goal is to retrieve and re-
turn documents that cover the different subtopics
of a given query. As they point out, the utility
of each document is dependent on the other doc-
uments in the ranking, which violates the indepen-
dent relevance assumption traditionally used in IR.
Blei et al (Blei et al, 2003) proposed Latent
Dirichlet Allocation (LDA), a generative model
that allows sets of documents to be explained by
unobserved groups of documents, each based on
a single topic. The LDA model assumes the bag-
609
of-words model and posits that each document is
composed of different topics. Specifically, each
word?s existence is attributed to one of the docu-
ment?s topic. This algorithm outputs a set of n-
grams for each topic whereas our algorithm mod-
els each subtopic using a single n-gram. Due to
limitations of time we were not able to compare
this approach with ours. We plan to have this com-
parison in our future work.
To reduce the complexity of this task, a candi-
date set of subtopics needs to be generated that
cover the document collection. We choose to
use a keyphrase detection algorithm to generate
topic labels. Several keyphrase extraction algo-
rithms have been discussed in the literature, in-
cluding ones based on machine learning methods
(Turney, 2000), (Hulth, 2003) and tf-idf ((Frank
et al, 1999)). Our method uses language models
and pointwise mutual information expressed as the
Kullback-Leibler divergence.
Kullback-Leibler divergence has been found to
be an effective method of finding keyphrases in
text collections. But identification of keyphrases
is not enough to find topics in document. The
keyphrases identified may describe the entire col-
lection, or aspects of the collection. We wish to
summarize subtopics within these collections.
The problem of subtopic detection is also related
to novelty detection in (Allan et al, ). In this prob-
lem, given a set of previously seen documents, the
task is to determine whether a new document con-
tains new or novel content. The TREC 2002 nov-
elty track, the task was to discard sentences that
did not contain new material. This is similar to our
goal of reducing redundancy in the list of returned
subtopics.
In most cases, novelty detection is implemented
as an online algorithm. The system has a set of ex-
isting documents they have seen up until a certain
point. The task is to determine whether a new doc-
ument is novel based on the previous documents.
Once a decision has been made, the status of that
document is fixed. The subtopic detection task dif-
fers from this because it is an offline task. The al-
gorithm typically has access to the entire document
set. Our method differs from this novelty detection
task in that it has access to the entire document col-
lection.
2.1 Existing redundancy measures
Zhang et al examine five different redundancy
measures for adaptive information filtering (Zhang
et al, ). Information filtering systems return rel-
evant documents in a document stream to a user.
Examples of information filtering systems include
traditional information retrieval systems that return
relevant documents depending on the user?s query.
The redundancy measures Zhang et al examine
are based on online analysis of documents. They
identify two methods of measuring redundancy:
? Given n documents, they are considered one
by one, and suppose we have processed i doc-
uments and we have k clusters. Now we need
to process the i+1th document. We compute
the distance of the i + 1th document with the
k clusters and add the document to the clos-
est cluster if the distance is above a certain
threshold, else we create a new cluster with
only the i+ 1th document.
? Measure distance between the new document
and each previously seen document.
They evaluate several measures like set difference,
geometric distance, Distributional similarity and
mixture models. Evaluating the five systems, they
found that cosine similarity was the most effective
measure, followed by the new mixture model mea-
sure.
3 Data
We choose several news events that occurred in
2007 and 2008 based on the popularity in the bl-
ogosphere. We were looking for events that were
widely discussed and commented on. The events
in our collection are the top-level events that we
have gathered. Table 1 lists the events that were
chosen for analysis:
To help illustrate our subtopic detection method,
we will use the Virginia Tech tragedy as an ex-
ample throughout the rest of this paper. People
throughout the blogosphere posted responses ex-
pressing support and condolences for the people
involved, along with their own opinions on what
caused it.
Figures 1 and 2 show two different responses to
the event. The quote in figure 1 shows an example
post from LiveJournal, a popular blogging com-
munity. In this post, the user is discussing his view
on gun control, a hotly debated topic in the after-
math of the shooting. Figure 2 expresses another
person?s emotional response to this event. Both
posts show different aspects of the same story. Our
subtopic detection system seeks to automatically
610
Event Description Posts Dates
iPhone iPhone release hype 48810 June 20 , 2007 - July 7, 2007
petfoodrecall Melamine tainted petfood recall 4285 March 10, 2007 - May 10, 2007
spitzer Eliot Spitzer prostitution scandal 10379 March 6, 2008 - March 23, 2008
vtech Virginia Tech shooting 12256 April 16, 2007 - April 30, 2007
Table 1: Major events summarized
identify these and other distinct discussions that
occur around an event.
After the Virginia Tech murders, there?s
the usual outcry for something to be
done, and in particular, for more gun
control. As usual, I am not persuaded.
The Virginia Tech campus had gun con-
trol, which meant that Cho Seung-Hui
was in violation of the law even before
he started shooting, and also that no law-
abiding citizens were able to draw.
Figure 1: Example blog post from LiveJournal dis-
cussing gun control (Rosen, 2007)
... Predictably, there have been rum-
blings in the media that video games
contributed to Cho Seung-Hui?s mas-
sacre at Virginia Tech. Jack Thomp-
son has come out screaming, referring
to gamers as ?knuckleheads? and calling
video games ?mental masturbation? all
the while referring to himself as an ?ed-
ucator? and ?pioneer? out to ?right? so-
ciety. ...
Figure 2: Example blog post discussing video
games (hoopdog, 2007)
Figure 3 shows a generalized Venn diagram
(Kestler et al, 2005) of the cluster overlap between
different keyphrases from the Virginia Tech event.
3.1 Preprocessing
Data was collected from the Blogocenter blog-
lines database. The Blogocenter group at UCLA
has been retrieving RSS feeds from the Bloglines,
Blogspot, Microsoft Live Spaces, and syndic8 ag-
gregators for the past several years. They currently
have over 192 million blog posts collected.
For each news item, relevant posts were re-
trieved, based on keyword searching and date of
blog post. Posts from the date of occurrence of
the item to two weeks after the event occurred
Figure 3: Generalized Venn diagram of topic over-
lap in the Virginia Tech collection
were gathered, regardless of the actual length of
the event.
Since many RSS feeds indexed by Bloglines are
from commercial news organizations or commer-
cial sites, we had to clean up the retrieved data.
Table 1 lists the event we analyzed along with ba-
sic statistics.
4 Method
Our algorithm should find discriminative labels for
the different topics that exist in a collection of doc-
uments. Taken together, these labels should satisfy
the following conditions:
? Describe a large portion of the collection
? The overlap between the topics should be
minimal
This task is similar to Minimum Set Cover,
which is NP-complete (Garey and Johnson, 1990).
Therefore, trying to find the optimal solution by
enumerating all possible phrases in the corpus
would be impossible, instead we propose a two-
step method for subtopic detection.
The first step is to generate a list of candidate
phrases. These phrases should be informative and
representative of all of the different subtopics. The
second step should select from these phrases con-
sistent with the two conditions stated above.
611
4.1 Generating Candidate Phrases
We want to generate a list of phrases that have a
high probability of covering the document space.
There are many methods that could be used to find
informative keyphrases. One such method is using
the standard information retrieval TF-IDF model
(Salton and McGill, 1986).
Witten et al(Witten et al, 1999) proposed
KEA, an algorithm which generates a list of can-
didate keyphrases using lexical features. They
keyphrases are then selected from these candidates
using a supervised machine learning algorithm.
This approach is not plausible for our purposes be-
cause of the following two reasons.
1. The algorithm is domain-dependent and
needs a training set of documents with anno-
tated keyphrases. But our data sets come from
various domains and it is not a very viable op-
tion to create a training set for each domain.
2. The algorithm generates keyphrases for a sin-
gle document, but for our purposes we need
keyphrases for a corpus.
Another method is using Kullback-Leibler di-
vergence to find informative keyphrases. We found
that KL divergence generated good candidate top-
ics.
Tomokiyo and Hurst (2003) developed a method
of extracting keyphrases using statistical language
models. They considered keyphrases as consisting
of two features, phraseness and informativeness.
Phraseness is described by them as the ?degree to
which a given word sequence is considered to be a
phrase.? For example, collocations could be con-
sidered sequences with a high phraseness. Infor-
mativeness is the extent to which a phrase captures
the key idea or main topic in a set of documents.
To find keyphrases, they compared two lan-
guage models, the target document set and a back-
ground corpus. Pointwise KL divergence was cho-
sen as the method of finding the difference be-
tween two language models.
The KL divergence D(p||q) between two prob-
ability mass functions p(x) and q(x) with alphabet
? is given in equation 1.
D(p||q) =
?
x??
p(x)log
p(x)
q(x)
(1)
KL divergence is an asymmetric function.
D(p||q) may not equal D(q||p).
Pointwise KL divergence is the individual con-
tribution of x to the loss of the entire distribution.
The pointwise KL divergence of a single phrase w
is ?
w
(p||q):
?
w
(p||q) = p(w)log
p(w)
q(w)
(2)
The phraseness of a phrase can be found by
comparing the foreground n-gram language model
against the background unigram model. For ex-
ample, if we were judging the phraseness of ?gun
control?, we would find the pointwise KL diver-
gence of ?gun control? between the foreground bi-
gram language model and the foreground unigram
language model.
?
p
= ?
w
(LM
f
g
N
||LM
f
g
1
) (3)
The informativeness of a phrase can be found by
finding the pointwise KL divergence of the fore-
ground model against the background model.
?
i
= ?
w
(LM
f
g
N
||LM
b
g
N
) (4)
A unified score can be formed by adding the
phraseness and informative score: ? = ?
p
+ ?
i
4.2 Selecting Topic Labels
Once keyphrases have been extracted from the
document set, they are sorted based on their
combined score. We select the top n-ranked
keyphrases as candidate phrases. This step will
hereafter be referred to as ?KL divergence mod-
ule?.
Based on our chosen task conditions regarding
coverage of the documents and minimized overlap
between topics, we need an undirected mapping
between phrases and documents. A natural repre-
sentation for this is a bipartite graph where the two
sets of nodes are phrases and documents. Let the
graph be: G = (W,D,E) where W is the set of
candidate phrases generated by the first step and D
is the entire set of documents. E is the set of edges
between W and D where there is an edge between
a phrase and a document if the document contains
the phrase.
We formulate the task as a variation of Weighted
Set Cover problem in theoretical computer science.
In normal Set Cover we are given a collection of
sets S over a universe U , and the goal is to select a
minimal subset of S such that the whole universe,
U is covered. Unfortunately this problem is NP-
complete (Garey and Johnson, 1990), so we must
612
d1
w3
w1
d4
d2
w2
d5
d3
Figure 4: Bipartite graph representation of topic
document coverage, where the d
i
?s are the docu-
ments and the w
i
?s are the n-grams
settle for an approximate solution. But fortunately
there exist very good ?-approximation algorithms
for this problem (Cui, 2007).
The difference in Weighted Set Cover is that
each set has an associated real-valued weight or
cost and the goal is to find the minimal or maximal
cost subset which covers the universe U .
In our problem, each phrase can be thought of
as a set of the documents which contain it. The
universe is the set of all documents.
4.3 Greedy Algorithm
To solve the above problem, we propose a greedy
algorithm. This algorithm computes a cost for each
node iteratively and selects the node with the low-
est cost at every iteration. The cost of a keyphrase
should be such that we do not choose a phrase with
very high coverage, like ?Virginia? and at the same
time not choose words with very low document
frequency since a very small collection of docu-
ments can not be judged a topic.
Based on these two conditions we have come up
with a linear combination of two cost components,
similar to Maximal Marginal Relevance (MMR)
(Carbonell and Goldstein, 1998).
1. Relative Document Size:
f
1
(w
i
) =
|adj(w
i
)|
N
(5)
where |adj(w
i
)| is the document frequency of
the word.
This factor takes into account that we do not
want to choose words which cover the whole
document collection. For example, phrases
such as ?Virginia? or ?Virginia tech? are bad
subtopics, because they cover most of the
document set.
2. Redundancy Penalty:
We want to choose elements that do not have a
lot of overlap with other elements. One mea-
sure of set overlap is the Jaccard similarity co-
efficient:
J(A,B) =
|A ?B|
|A ?B|
(6)
f
2
(w
i
) = 1?
?
w
j
?W?w
i
J(w
i
, w
j
)
|W | ? 1
(7)
This component is essentially 1? average
Jaccard similarity.
We calculate the pairwise Jaccard coefficient
between the target keyphrase and every other
keyphrase. The pairwise coefficient vector
provides information on how much overlap
there is between a keyphrase and every other
keyphrase. Phrases with a high average Jac-
card coefficient are general facets that cover
the entire collection. Phrases with a low Jac-
card coefficient are facets that cover specific
topics with little overlap.
3. Subtopic Redundancy Memory Effect
Once a keyphrase has been chosen we also
want to penalize other keyphrases that cover
the same content or documents. Equation 8
represents a redundancy ?memory? for each
keyphrase or subtopic. This memory is up-
dated for every step in the greedy algorithm.
R(w
i
) = R(w
i
) + J(w
i
, w
j
) (8)
where w
j
is the newly selected phrase.
A general cost function can be formed from a
linear combination of the three cost components.
We provide two parameters, ? and ? to represent
the trade-off between coverage, cohesiveness and
intersection. For our experiments, we found that
an ? value of 0.7 and a ? value of 0.2 performed
well.
cost(w
i
) = ?? f
1
(w
i
)
+? ? f
2
(w
i
)
+(1? (? + ?))?R(w
i
)
(9)
613
The pseudocode for the greedy algorithm is
given in Figure 5. It should be noted that the al-
gorithm requires the costs to be recomputed af-
ter every iteration. This is because the cost of a
keyphrase may change due to a change in any of
the three components. This is because after select-
ing a keyphrase, it might make another keyphrase
redundant, that is, covering the same content. This
makes the whole problem a dynamic weighted set
cover problem. Hence, the performance guaran-
tees associated with the greedy algorithm for the
Weighted Set Cover problem do not hold true for
the dynamic version.
Algorithm Greedy algorithm for weighted set-cover
Input: Graph G = (W,D,E)
1. N: number of documents to cover
2.
Output: Set of discriminative phrases for the different topics
3. W = {w
1
, w
2
, . . . , w
n
}
4. W
chosen
= ?
5. num docs covered = 0
6. while num docs covered < N
7. do for w
i
? W
8. do cost(w
i
) = ?? f
1
(w
i
)
9. +? ? f
2
(w
i
)
10. +(1? (?+ ?))?R(w
i
)
11. w
selected
= argmax
w
cost(w
i
)
12. for w
i
? W
13. do R(w
i
) = R(w
i
) + J(w
selected
, w
i
)
14. num docs covered = num docs covered +
|adj(w
selected
)|
15. W
chosen
= W
chosen
? {w
selected
}
16. W = W ? {w
selected
}
17. D = D ? adj(selected)
18. return W
chosen
Figure 5: A greedy set-cover algorithm for detect-
ing sub-topics
5 Experiments
As a baseline measure, we extracted the top k
phrases from the word distribution as the topic la-
bels. As a gold standard, we manually annotated
the four different collections of blog posts. Each
annotator generated a list of subtopics.
6 Evaluation
In evaluating topic detection, there exist two cate-
gories of methods, intrinsic and extrinsic (Liddy,
2001). Extrinsic methods evaluate the labels
against a particular task whereas intrinsic methods
measure the quality of the labels directly. We pro-
vide intrinsic and extrinsic evaluations of our algo-
rithm.
To evaluate our facet detection algorithm, we
created a gold standard list of facets for each data
set. A list of the top 300 keyphrases generated by
the KL divergence module was given to two eval-
uators. The evaluators were the first and second
author of this paper. The evaluators labeled each
keyphrase as a positive example of a subtopic or
a negative example of a subtopic. The positive
examples taken together form the gold standard.
For this evaluation process we defined a positive
subtopic as a cohesive collection of documents dis-
cussing the same topic.
Cohen?s Kappa coefficient (Cohen, 1960) was
calculated for the gold standard. Table 6 lists the ?
value for the four data sets.
iPhone petfoodrecall spitzer vtech
0.62 0.86 0.77 0.88
Table 2: Kappa scores for the gold standard
The kappa scores for the petfoodrecall and vtech
datasets showed good agreement among the raters,
while the spitzer data set had only fair agreement.
For the iPhone data set, both evaluators had a large
amount of disagreement on what they considered
subtopics.
A separate group of two evaluators was given
the output from our graph-based algorithm, a list
of the top KL divergence keyphrases of the same
length, and the gold standard for all four data sets.
Evaluators were asked to rate the keyphrases on a
scale from one to five, with one indicating a poor
subtopic, and five indicating a good subtopic. The
number k of subtopics for the algorithm was cutoff
where the f-score is maximized. The same number
of phrases was chosen for KL divergence as well.
Table 3 lists the cutoffs for the four data sets.
iPhone Petfood recall Spitzer Vtech
25 30 24 18
Table 3: Number of generated subtopics for each
collection.
In addition, the precision, F-score, coverage and
average pairwise Jaccard coefficient were calcu-
lated for the four data sets. Precision, recall and
the F-score are given in table 4. The precision,
recall and F-score for the gold standards is one.
The others are shown in table 5. Average pairwise
Jaccard coefficient is calculated by finding the Jac-
card coefficient for every pair of subtopics in the
output and averaging this value. This value is a
measure of the redundancy. The average relevance
is a normalized version of the combined ?phrase-
614
ness? and ?informativeness? score calculated by
the keyphrase detection module. This value is nor-
malized by dividing by the KL divergence for the
entire 300 phrase list. This provides a relevancy
score for the output.
Data set Precision Recall F-score
iphone
KL-Divergence 0.08 0.10 0.09
Graph-based method 0.52 0.60 0.56
petfoodrecall
KL-Divergence 0.37 0.39 0.38
Graph-based method 0.61 0.57 0.59
spitzer
KL-Divergence 0.10 0.08 0.09
Graph-based method 0.79 0.59 0.68
vtech
KL-Divergence 0.05 0.06 0.05
Graph-based method 0.72 0.76 0.74
Table 4: Precision, recall and F-score for the base-
line and graph-based algorithm.
Data set
Coverage Average Normalized Human
pairwise KL rating
JC divergence
iphone
KL-Divergence 40168 0.08 18.19 1.92
Gold standard 12977 0.02 2.81 3.13
Graph-based 9850 0.01 1.98 2.82
petfoodrecall
KL-Divergence 4280 0.18 19.53 1.82
Gold standard 2659 0.05 4.30 3.43
Graph-based 2055 0.01 1.75 2.81
spitzer
KL-Divergence 9291 0.19 22.90 1.33
Gold standard 4036 0.03 2.29 3.31
Graph-based 2468 0.01 1.60 2.88
vtech
KL-Divergence 12215 0.29 24.61 1.61
Gold standard 5058 0.03 2.79 3.76
Graph-based 4342 0.01 1.66 3.28
Table 5: Coverage, overlap and relevance and eval-
uation scores for the gold standard, baseline and
graph-based method.
7 Results
Table 6 shows some of the different subtopics cho-
sen by our algorithm for the different data sets.
There is no manual involvement required in the al-
gorithm except for the intial preprocessing to re-
move commercial news feeds and spam posts. Our
graph-based method performs very well and al-
most achieves the gold standard?s rating. The F-
score for the iPhone data set was only 0.56, but we
believe part of this may be because this data set did
not have clearly defined subtopics, as shown by the
low agreement (0.62) among human evaluators.
Spitzer Petfood recall
Ashley Alexandra Dupre Under Wal-Mart
Oberweis Xuzhou Anying
Emperor?s club People who buy
Governor of New Cuts and Gravy
Spitzer?s resignation Cat and Dog
Dr Laura Cats and Dogs
Mayflower hotel Food and Drug
Sex workers Cyanuric acid
former New york recent pet
High priced prostitution industrial chemical
McGreevey massive pet food
Geraldine Ferraro Royal canin
High priced call Iams and Eukanuba
legally blind Dry food
money laundering
Virginia Tech shooting iPhone
Korean American Photo sent from
Gun Ownership Waiting in line
Holocaust survivor About the iPhone
Mentally ill Unlimited data
Shooting spree From my iPhone
Don Imus cell phones
Video Games Multi-touch
Gun free zone Guided tour
West Ambler Johnston iPhone launch
Columbine High school Walt Mossberg
Self defense Apple Inc
Two hours later Windows Mobile
Gun violence June 29th
Seung Hui Cho Web browser
Second Amendment Activation
South Korean
Table 6: Different topics chosen by the graph-
based algorithm for the different data sets
Figure 6 shows the trade off between coverage
and redundancy. This graph clearly shows that
the overlap between the subtopics increases very
slowly as compared to the number of documents
covered. The slope of the curves increases slowly
when the number of documents to be covered is
small and later increases rapidly. This means that
initially there are a lot of small focused subtopics
and once we have selected all the focused ones the
algorithm is forced to pick the bigger topics and
hence the average pairwise intersection increases.
0 0.5 1 1.5 2 2.5 3 3.5 4
x 104
0
10
20
30
40
50
60
70
80
Number of documents to be covered
Av
er
ag
e 
pa
irw
ise
 in
te
rs
ec
tio
n 
of
 to
pi
cs
 
 
iPhone
Spitzer
petfoodrecall
vtech
Figure 6: Subtopic redundancy vs. coverage
615
8 Conclusion
We have presented a new algorithm based on
weighted set cover for finding subtopics in a
corpus of selected blog postings. The algo-
rithm performs very well in practice compared
to the baseline standard, which outputs the top
keyphrases according to the Kullback-Leibler di-
vergence method. While the baseline standard out-
puts keyphrases which are redundant, in the sense,
they cover the same documents, the graph-based
method outputs keyphrases which have very little
intersection. We provide a new method of ranking
keyphrases that can help users find different facets
of an event.
The identification of facets has many applica-
tions to natural language processing. Once facets
have been identified in a collection, documents can
be clustered based on these facets. These clusters
can be used to generate document summaries or
for visualization of the event space.
The keyphrases themselves provide a succinct
summary of the different subtopics. In future
work, we intend to investigate summarization of
documents based on subtopic clustering using this
method.
9 Acknowledgments
This work was supported by NSF grants IIS
0534323 ?Collaborative Research: BlogoCenter -
Infrastructure for Collecting, Mining and Access-
ing Blogs? awarded to The University of Michigan
and ?iOPENER: A Flexible Framework to Support
Rapid Learning in Unfamiliar Research Domains?,
jointly awarded to U. of Michigan and U. of Mary-
land as IIS 0705832.? Also we would like to thank
Vahed Qazvinian and Arzucan ?Ozgu?r for helping
with the evaluation and their valuable suggestions.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
Allan, James, Courtney Wade, and Alvaro Bolivar. Re-
trieval and novelty detection at the sentence level.
SIGIR 2003, pages 314?321.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022, January.
Carbonell, Jaime G. and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. SIGIR 1998,
pages 335?336.
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measure-
ment, 20:37.
Cui, Peng. 2007. A tighter analysis of set cover greedy
algorithm for test set. In ESCAPE, pages 24?35.
Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. Sixteenth In-
ternational Joint Conference on Artificial Intelli-
gence, pages 668?673.
Garey, Michael R. and David S. Johnson. 1990. Com-
puters and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman.
hoopdog. 2007. Follow-up: Blame game.
http://hoopdogg.livejournal.com/39060.html.
Hulth, Anette. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. EMNLP
2003, pages 216?223.
Kestler, Hans A., Andre Muller, Thomas M. Gress, and
Malte Buchholz. 2005. Generalized venn diagrams:
a new method of visualizing complex genetic set re-
lations. Bioinformatics, 21:1592?1595, April.
Liddy, Elizabeth. 2001. Advances in automatic text
summarization. Information Retrieval, 4:82?83.
Rosen, Nicholas D. 2007. Gun control and mental
health. http://ndrosen.livejournal.com/128715.html.
Salton, G. and M. J. McGill. 1986. Introduction to
Modern Information Retrieval. McGraw-Hill, Inc.
New York, NY, USA.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. ACL
2003 workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, pages 33?40.
Turney, Peter D. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303?
336.
Witten, Ian H., Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In 1st
ACM/IEEE-CS joint conference on Digital libraries,
pages 254?255.
Zhai, Chengxiang, William W. Cohen, and John Laf-
ferty. 2003. Beyond independent relevance: meth-
ods and evaluation metrics for subtopic retrieval. SI-
GIR 2003, pages 10?17.
Zhang, Yi, James P. Callan, and Thomas P. Minka.
Novelty and redundancy detection in adaptive filter-
ing. In SIGIR 2002, pages 81?88.
616
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689?696
Manchester, August 2008
Scientific Paper Summarization Using Citation Summary Networks
Vahed Qazvinian
School of Information
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
Quickly moving to a new area of research
is painful for researchers due to the vast
amount of scientific literature in each field
of study. One possible way to overcome
this problem is to summarize a scientific
topic. In this paper, we propose a model of
summarizing a single article, which can be
further used to summarize an entire topic.
Our model is based on analyzing others?
viewpoint of the target article?s contribu-
tions and the study of its citation summary
network using a clustering approach.
1 Introduction
It is quite common for researchers to have to
quickly move into a new area of research. For
instance, someone trained in text generation may
want to learn about parsing and someone who
knows summarization well, may need to learn
about question answering. In our work, we try to
make this transition as painless as possible by au-
tomatically generating summaries of an entire re-
search topic. This enables a researcher to find the
chronological order and the progress in that par-
ticular field of study. An ideal such system will re-
ceive a topic of research, as the user query, and will
return a summary of related work on that topic. In
this paper, we take the first step toward building
such a system.
Studies have shown that different citations to the
same article often focus on different aspects of that
article, while none alone may cover a full descrip-
tion of its entire contributions. Hence, the set of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
citation summaries, can be a good resource to un-
derstand the main contributions of a paper and how
that paper affects others. The citation summary of
an article (A), as defined in (Elkiss et al, 2008),
is a the set of citing sentences pointing to that ar-
ticle. Thus, this source contains information about
A from others? point of view. Part of a sample ci-
tation summary is as follows:
In the context of DPs, this edge based factorization method
was proposed by (Eisner, 1996).
Eisner (1996) gave a generative model with a cubic parsing
algorithm based on an edge factorization of trees.
Eisner (Eisner, 1996) proposed an
O(n
3
) parsing algorithm for PDG.
If the parse has to be projective, Eisner?s
bottom-up-span algorithm (Eisner, 1996) can be
used for the search.
The problem of summarizing a whole scientific
topic, in its simpler form, may reduce to summa-
rizing one particular article. A citation summary
can be a good resource to make a summary of a
target paper. Then using each paper?s summary
and some knowledge of the citation network, we?ll
be able to generate a summary of an entire topic.
Analyzing citation networks is an important com-
ponent of this goal, and has been widely studied
before (Newman, 2001).
Our main contribution in this paper is to use ci-
tation summaries and network analysis techniques
to produce a summary of a single scientific article
as a framework for future research on topic sum-
marization. Given that the citation summary of any
article usually has more than a few sentences, the
main challenge of this task is to find a subset of
these sentences that will lead to a better and shorter
summary.
689
Cluster Nodes Edges
DP 167 323
PBMT 186 516
Summ 839 1425
QA 238 202
TE 56 44
Table 1: Clusters and their citation network size
1.1 Related Work
Although there has been work on analyzing ci-
tation and collaboration networks (Teufel et al,
2006; Newman, 2001) and scientific article sum-
marization (Teufel and Moens, 2002), to the
knowledge of the author there is no previous work
that study the text of the citation summaries to
produce a summary. (Bradshaw, 2003; Bradshaw,
2002) get benefit from citations to determine the
content of articles and introduce ?Reference Di-
rected Indexing? to improve the results of a search
engine.
In other work, (Nanba et al, 2004b; Nanba et
al., 2004a) analyze citation sentences and automat-
ically categorize citations into three groups using
160 pre-defined phrase-based rules. This catego-
rization is then used to build a tool for survey gen-
eration. (Nanba and Okumura, 1999) also discuss
the same citation categorization to support a sys-
tem for writing a survey. (Nanba and Okumura,
1999; Nanba et al, 2004b) report that co-citation
implies similarity by showing that the textual simi-
larity of co-cited papers is proportional to the prox-
imity of their citations in the citing article.
Previous work has shown the importance of
the citation summaries in understanding what a
paper says. The citation summary of an article
A is the set of sentences in other articles which
cite A. (Elkiss et al, 2008) performed a large-
scale study on citation summaries and their impor-
tance. They conducted several experiments on a
set of 2, 497 articles from the free PubMed Cen-
tral (PMC) repository
1
. Results from this exper-
iment confirmed that the ?Self Cohesion? (Elkiss
et al, 2008) of a citation summary of an arti-
cle is consistently higher than the that of its ab-
stract. (Elkiss et al, 2008) also conclude that ci-
tation summaries are more focused than abstracts,
and that they contain additional information that
does not appear in abstracts. (Kupiec et al, 1995)
use the abstracts of scientific articles as a target
summary, where they use 188 Engineering Infor-
mation summaries that are mostly indicative in na-
1
http://www.pubmedcentral.gov
ture. Abstracts tend to summarize the documents
topics well, however, they don?t include much use
of metadata. (Kan et al, 2002) use annotated bib-
liographies to cover certain aspects of summariza-
tion and suggest guidelines that summaries should
also include metadata and critical document fea-
tures as well as the prominent content-based fea-
tures.
Siddharthan and Teufel describe a new task to
decide the scientific attribution of an article (Sid-
dharthan and Teufel, 2007) and show high human
agreement as well as an improvement in the per-
formance of Argumentative Zoning (Teufel, 2005).
Argumentative Zoning is a rhetorical classification
task, in which sentences are labeled as one of Own,
Other, Background, Textual, Aim, Basis, Contrast
according to their role in the author?s argument.
These all show the importance of citation sum-
maries and the vast area for new work to analyze
them to produce a summary for a given topic.
2 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and pro-
ceedings from ACL conferences and workshops
and includes almost 11, 000 papers. To produce
the ACL Anthology Network (AAN), (Joseph and
Radev, 2007) manually performed some prepro-
cessing tasks including parsing references and
building the network metadata, the citation, and
the author collaboration networks.
The full AAN includes all citation and collabo-
ration data within the ACL papers, with the citation
network consisting of 8, 898 nodes and 38, 765 di-
rected edges.
2.1 Clusters
We built our corpus by extracting small clusters
from the AAN data. Each cluster includes pa-
pers with a specific phrase in the title or con-
tent. We used a very simple approach to col-
lect papers of a cluster, which are likely to be
talking about the same topic. Each cluster con-
sists of a set of articles, in which the topic
phrase is matched within the title or the content
of papers in AAN. In particular, the five clus-
ters that we collected this way, are: Dependency
Parsing (DP), Phrased Based Machine Translation
(PBMT), Text Summarization (Summ), Question
Answering (QA), and Textual Entailment (TE).
Table 1 shows the number of articles and citations
in each cluster. For the evaluation purpose we
690
ACL-ID Title Year CS Size
D
P
C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 1996 66
P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 1997 55
P99-1065 A Statistical Parser For Czech 1999 54
P05-1013 Pseudo-Projective Dependency Parsing 2005 40
P05-1012 On-line Large-Margin Training Of Dependency Parsers 2005 71
P
B
M
T
N03-1017 Statistical Phrase-Based Translation 2003 180
W03-0301 An Evaluation Exercise For Word Alignment 2003 14
J04-4002 The Alignment Template Approach To Statistical Machine Translation 2004 50
N04-1033 Improvements In Phrase-Based Statistical Machine Translation 2004 24
P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 2005 65
S
u
m
m
A00-1043 Sentence Reduction For Automatic Text Summarization 2000 19
A00-2024 Cut And Paste Based Text Summarization 2000 20
C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 2000 19
W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 2000 31
W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 2003 15
Q
A
A00-1023 A Question Answering System Supported By Information Extraction 2000 13
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 2002 19
P02-1006 Learning Surface Text Patterns For A Question Answering System 2002 74
D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 2003 42
P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 2003 27
T
E
D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 2004 12
H05-1047 A Semantic Approach To Recognizing Textual Entailment 2005 8
H05-1079 Recognising Textual Entailment With Logical Inference 2005 9
W05-1203 Measuring The Semantic Similarity Of Texts 2005 17
P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 2005 10
Table 2: Papers chosen from clusters for evaluation, with their publication year, and citation summary
size
chose five articles from each cluster. Table 2 shows
the title, year, and citation summary size for the 5
papers chosen from each cluster. The citation sum-
mary size of a paper is the size of the set of citation
sentences that cite that paper.
3 Analysis
3.1 Fact Distribution
We started with an annotation task on 25 papers,
listed in Table 2, and asked a number of annota-
tors to read the citation summary of each paper
and extract a list of the main contributions of that
paper. Each item on the list is a non-overlapping
contribution (fact) perceived by reading the cita-
tion summary. We asked the annotators to focus
on the citation summary to do the task and not on
their background on this topic.
As our next step we manually created the union
of the shared and similar facts by different anno-
tators to make a list of facts for each paper. This
fact list made it possible to review all sentences in
the citation summary to see which facts each sen-
tence contained. There were also some unshared
facts, facts that only appear in one annotator?s re-
sult, which we ignored for this paper.
Table 3 shows the shared and unshared facts ex-
tracted by four annotators for P99-1065.
The manual annotation of P99-1065 shows that
the fact ?Czech DP? appears in 10 sentences out
of all 54 sentences in the citation summary. This
shows the importance of this fact, and that ?Depen-
Fact Occurrences
S
h
a
r
e
d
f
4
: ?Czech DP? 10
f
1
: ?lexical rules? 6
f
3
: ?POS/ tag classification? 6
f
2
: ?constituency parsing? 5
f
5
: ?Punctuation? 2
f
6
: ?Reordering Technique? 2
f
7
: ?Flat Rules? 2
U
n
s
h
a
r
e
d
?Dependency conversion?
?80% UAS?
?97.0% F-measure?
?Generative model?
?Relabel coordinated phrases?
?Projective trees?
?Markovization?
Table 3: Facts of P99-1065
dency Parsing of Czech? is one of the main contri-
butions of this paper. Table 3 also shows the num-
ber of times each shared fact appears in P99-1065?s
citation summary sorted by occurrence.
After scanning through all sentences in the ci-
tation summary, we can come up with a fact dis-
tribution matrix for an article. The rows of this
matrix represent sentences in the citation summary
and the columns show facts. A 1 value in this ma-
trix means that the sentence covers the fact. The
matrix D shows the fact distribution of P99-1065.
IDs in each row show the citing article?s ACL ID,
and the sentence number in the citation summary.
These matrices, created using annotations, are par-
ticularly useful in the evaluation process.
691
D =
0
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
B
@
f
1
f
2
f
3
f
4
f
5
f
6
f
7
W06-2935:1 1 0 0 0 0 0 0
W06-2935:2 0 0 0 0 0 0 0
W06-2935:3 0 0 1 1 0 0 0
W06-2935:4 0 0 0 0 0 0 1
W06-2935:5 0 0 0 0 0 0 0
W06-2935:6 0 0 0 0 1 0 0
W05-1505:7 0 1 0 1 0 0 0
W05-1505:8 0 0 0 0 0 1 0
.
.
.
.
.
.
.
.
.
.
.
.
W05-1518:54 0 0 0 0 0 0 0
1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
A
3.2 Similarity Measures
We want to build a network with citing sentences
as nodes and similarities of two sentences as edge
weights. We?d like this network to have a nice
community structure, whereby each cluster corre-
sponds to a fact. So, a similarity measure in which
we are interested is the one which results in high
values for pairs of sentences that cover the same
facts. On the other hand, it should return a low
value for pairs that do not share a common contri-
bution of the target article.
The following shows two sample sentences from
P99-1065 that cover the same fact and we want the
chosen similarity measure to return a high value
for them:
So, Collins et al(1999) proposed a tag classification for
parsing the Czech treebank.
The Czech parser of Collins et al(1999) was run on a dif-
ferent data set... .
Conversely, we?d like the similarity of the two fol-
lowing sentences that cover no shared facts, to be
quite low:
Collins (1999) explicitly added features to his parser to im-
prove punctuation dependency parsing accuracy.
The trees are then transformed into Penn Treebank
style constituencies- using the technique described in
(Collins et al 1999).
We used P99-1065 as the training sample, on
which similarity metrics were trained, and left the
others for the test. To evaluate a similarity mea-
sure for our purpose we use a simple approach. For
each measure, we sorted the similarity values of all
pairs of sentences in P99-1065?s citation summary
in a descending order. Then we simply counted the
number of pairs that cover the same fact (out of 172
such fact sharing pairs) in the top 100, 200 and 300
highly similar ones out of total 2, 862 pairs. Table
4 shows the number of fact sharing pairs among
the top highest similar pairs. Table 4 shows how
cosine similarity that uses a tf-idf measure outper-
forms the others. We tried three different poli-
cies for computing IDF values to compute cosine
Measure Top 100 Top 200 Top 300
tf-idf (General) 34 66 74
tf-idf (AAN) 34 56 74
LCSS 26 37 54
tf 24 34 46
tf2gen 13 26 35
tf-idf (DP) 16 26 28
Levenshtein 2 9 16
Table 4: Different similarity measures and their
performances in favoring fact-sharing sentences.
Each column shows the number of fact-sharing
pairs among top highly similar pairs.
similarity: a general IDF, an AAN-specific IDF
where IDF values are calculated only using the
documents of AAN, and finally DP-specific IDF
in which we only used all-DP data set. Table 4
also shows the results for an asymmetric similarity
measure, generation probability (Erkan, 2006) as
well as two string edit distances: the longest com-
mon substring and the Levenshtein distance (Lev-
enshtein, 1966).
4 Methodology
In this section we discuss our graph clustering
method for article summarization, as well as other
baseline methods used for comparisons.
4.1 Network-Based Clustering
The Citation Summary Network of an article A is
a network in which each sentence in the citation
summary of A is a node. This network is a com-
plete undirected weighted graph where the weight
of an edge between two nodes shows the similarity
of the two corresponding sentences of those nodes.
The similarity that we use, as described in sec-
tion 3.2, is such that sentences with the same facts
have high similarity values. In other words, strong
edges in the citation summary network are likely
to indicate a shared fact between two sentences.
A graph clustering method tries to cluster the
nodes of a graph in a way that the average intra-
cluster similarity is maximum and the average
inter-cluster similarity is minimum. To find the
communities in the citation summary network we
use (Clauset et al, 2004), a hierarchical agglom-
eration algorithm which works by greedily opti-
mizing the modularity in a linear running time for
sparse graphs.
To evaluate how well the clustering method works,
we calculated the purity for the clusters found of
each paper. Purity (Manning et al, 2008) is a
method in which each cluster is assigned to the
class with the majority vote in the cluster, and then
692
ACL-ID #Facts |C| #Clusters |?| Purity(?,C)
D
P
C96-1058 4 4 0.636
P97-1003 5 5 0.750
P99-1065 7 7 0.724
P05-1013 5 3 0.689
P05-1012 7 5 0.500
P
B
M
T
N03-1017 8 4 0.464
W03-0301 3 3 0.777
J04-4002 5 5 0.807
N04-1033 5 4 0.615
P05-1033 6 5 0.650
S
u
m
m
A00-1043 5 4 0.812
A00-2024 5 2 0.333
C00-1072 3 4 0.857
W00-0403 6 4 0.682
W03-0510 4 3 0.727
Q
A
A00-1023 3 2 0.833
W00-0603 7 4 0.692
P02-1006 7 5 0.590
D03-1017 7 4 0.500
P03-1001 6 4 0.500
T
E
D04-9907 7 3 0.545
H05-1047 4 3 0.833
H05-1079 5 3 0.625
W05-1203 3 3 0.583
P05-1014 4 2 0.667
Table 5: Number of real facts, clusters and purity
for each evaluated article
the accuracy of this assignment is measured by di-
viding the number of correctly assigned documents
by N . More formally:
purity(?,C) =
1
N
?
k
max
j
|?
k
? c
j
|
where ? = {?
1
, ?
2
, . . . , ?
K
} is the set of clus-
ters and C = {c
1
, c
2
, . . . , c
J
} is the set of classes.
?
k
is interpreted as the set of documents in ?
k
and
c
j
as the set of documents in c
j
. For each evalu-
ated article, Table 5 shows the number of real facts
(|C| = J), the number of clusters (|?| = K) and
purity(?,C) for each evaluated article. Figure 1
shows the clustering result for J04-4002, in which
each color (number) shows a real fact, while the
boundaries and capital labels show the clustering
result.
4.1.1 Sentence Extraction
Once the graph is clustered and communities are
formed, to build a summary we extract sentences
from the clusters. We tried these two different sim-
ple methods:
? Cluster Round-Robin (C-RR)
We start with the largest cluster, and extract
sentences in the order they appear in each
cluster. So we extract first, the first sentences
from each cluster, then the second ones, and
so on, until we reach the summary length
limit |S|. Previously, we mentioned that facts
with higher weights appear in greater num-
ber of sentences, and clustering aims to clus-
ter such fact-sharing sentences in the same
"
#
$
%
&
Figure 1: Each node is a sentence in the citation
summary for paper J04-4002. Colors (numbers)
represent facts and boundaries show the clustering
result
communities. Thus, starting with the largest
community is important to ensure that the
system summary first covers the facts that
have higher frequencies and therefore higher
weights.
? Cluster Lexrank (C-lexrank)
The second method we used was Lexrank
(Erkan and Radev, 2004) inside each cluster.
In other words, for each cluster ?
i
we made a
lexical network of the sentences in that clus-
ter (N
i
) . Using Lexrank we can find the
most central sentences in N
i
as salient sen-
tences of ?
i
to include in the main summary.
We simply choose, for each cluster ?
i
, the
most salient sentence of ?
i
, and if we have
not reached the summary length limit, we do
that for the second most salient sentences of
each cluster, and so on. The way of ordering
clusters is again by decreasing size.
Table 6 shows the two system summaries gen-
erated with C-RR and C-lexrank methods for P99-
1065. The sentences in the table appear as they
were extracted automatically from the text files of
papers, containing sentence fragments and malfor-
mations occurring while doing the automatic seg-
mentation.
4.2 Baseline Methods
We also conducted experiments with two baseline
approaches. To produce the citation summary we
used Mead?s (Radev et al, 2004) Random Sum-
mary and Lexrank (Erkan and Radev, 2004) on
the entire citation summary network as baseline
techniques. Lexrank is proved to work well in
multi-document summarization (Erkan and Radev,
2004). It first builds a lexical network, in which
693
ID Sentence
C-RR
W05-1505:9 3 Constituency Parsing for Dependency Trees A pragmatic justification for using constituency- based parser in order
to predict dependency struc- tures is that currently the best Czech dependency- tree parser is a constituency-based parser (Collins et al 1999; Zeman, 2004).
W04-2407:27 However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al 1999), we will give this measure as well.
J03-4003:33 3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).
H05-1066:51 Furthermore, we can also see that the MST parsers perform favorably compared to the more powerful
lexicalized phrase-structure parsers, such as those presented by Collins et al(1999) and Zeman (2004) that use expensive O(n5) parsing al- gorithms.
E06-1011:21 5.2 Czech Results For the Czech data, we used the predefined train- ing, development and testing split
of the Prague Dependency Treebank (Hajic et al 2001), and the automatically generated POS tags supplied with the data,
which we reduce to the POS tag set from Collins et al(1999).
C-Lexrank
P05-1012:16 The Czech parser of Collins et al(1999) was run on a different data set and most other dependency parsers are evaluated using English.
W04-2407:26 More precisely, parsing accuracy is measured by the attachment score, which is
a standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al 1999).
W05-1505:14 In an attempt to extend a constituency-based pars- ing model to train on dependency trees,
Collins transforms the PDT dependency trees into con- stituency trees (Collins et al 1999).
P06-1033:31 More specifi- cally for PDT, Collins et al(1999) relabel coordi- nated phrases after converting dependency struc- tures to phrase
structures, and Zeman (2004) uses a kind of pattern matching, based on frequencies of the parts-of-speech of conjuncts and conjunc- tions.
P05-1012:17 In par- ticular, we used the method of Collins et al(1999) to simplify part-of-speech tags since
the rich tags used by Czech would have led to a large but rarely seen set of POS features.
Table 6: System Summaries for P99-1065. (a) Using C-RR, (b) using C-Lexrank with length of 5
sentences
nodes are sentences and a weighted edge between
two nodes shows the lexical similarity. Once this
network is built, Lexrank performs a random walk
to find the most central nodes in the graph and re-
ports them as summary sentences.
5 Experimental Setup
5.1 Evaluation Method
Fact-based evaluation systems have been used in
several past projects (Lin and Demner-Fushman,
2006; Marton and Radul, 2006), especially in
the TREC question answering track. (Lin and
Demner-Fushman, 2006) use stemmed unigram
similarity of responses with nugget descriptions to
produce the evaluation results, whereas (Marton
and Radul, 2006) uses both human judgments and
human descriptions to evaluate a response.
An ideal summary in our model is one that cov-
ers more facts and more important facts. Our def-
inition for the properties of a ?good? summary of
a paper is one that is relatively short and consists
of the main contributions of that paper. From this
viewpoint, there are two criteria for our evaluation
metric. First, summaries that contain more impor-
tant facts are preferred over summaries that cover
fewer relevant facts. Second, facts should not be
equally weighted in this model, as some of them
may show more important contributions of a pa-
per, while others may not.
To evaluate our system, we use the pyra-
mid evaluation method (Nenkova and Passonneau,
2004) at sentence level. Each fact in the citation
summary of a paper is a summarization content
unit (SCU) (Nenkova and Passonneau, 2004), and
the fact distribution matrix, created by annotation,
provides the information about the importance of
each fact in the citation summary.
The score given by the pyramid method for a
summary is a ratio of the sum of the weights of
its facts to the sum of the weights of an optimal
summary. This score ranges from 0 to 1, and high
scores show the summary content contain more
heavily weighted facts. We believe that if a fact
appears in more sentences of the citation summary
than another fact, it is more important, and thus
should be assigned a higher weight. To weight the
facts we build a pyramid, and each fact falls in a
tier. Each tier shows the number of sentences a fact
appears in. Thus, the number of tiers in the pyra-
mid is equal to the citation summary size. If a fact
appears in more sentences, it falls in a higher tier.
So, if the fact f
i
appears |f
i
| times in the citation
summary it is assigned to the tier T
|f
i
|
.
The pyramid score formula that we use is com-
puted as follows. Suppose the pyramid has n tiers,
T
i
, where tier T
n
on top and T
1
on the bottom. The
weight of the facts in tier T
i
will be i (i.e. they ap-
peared in i sentences). If |T
i
| denotes the number
of facts in tier T
i
, and D
i
is the number of facts in
the summary that appear in T
i
, then the total fact
weight for the summary is D =
?
n
i=1
i?D
i
. Ad-
ditionally, the optimal pyramid score for a sum-
mary with X facts, is
Max =
?
n
i=j+1
i?|T
i
|+j?(X?
?
n
i=j+1
|T
i
|)
where j = max
i
(
?
n
t=i
|T
t
| ? X). Subsequently,
the pyramid score for a summary is calculated as
P =
D
Max
.
694
5.2 Results and Discussion
Based on the described evaluation method we con-
ducted a number of experiments to evaluate dif-
ferent summaries of a given length. In particular,
we use a gold standard and a random summary to
determine how good a system summary is. The
gold standard is a summary of a given length that
covers as many highly weighted facts as possible.
To make a gold summary we start picking sen-
tences that cover new and highly weighted facts,
until the summary length limit is reached. On the
other hand, in the random summary sentences are
extracted from the citation summary in a random
manner. We expect a good system summary to be
closer to the gold than it is to the random one.
Table 7 shows the value of pyramid score P , for
the experiments on the set of 25 papers. A P score
of less than 1 for a gold shows that there are more
facts than can be covered with a set of |S| sen-
tences.
This table suggests that C-lexrank has a higher
average score, P , for the set of evaluated articles
comparing C-RR and Lexrank.
As mentioned earlier in section 4.1.1, once the
citation summary network is clustered in the C-RR
method, the sentences from each cluster are chosen
in a round robin fashion, which will not guarantee
that a fact-bearing sentence is chosen.
This is because all sentences, whether they
cover any facts or not, are assigned to some clus-
ter anyway and such sentences might appear as the
first sentence in a cluster. This will sometimes re-
sult in a low P score, for which P05-1012 is a good
example.
6 Conclusion and Future Work
In this work we use the citation summaries to un-
derstand the main contributions of articles. The
citation summary size, in our experiments, ranges
from a few sentences to a few hundred, of which
we pick merely a few (5 in our experiments) most
important ones.
As a method of summarizing a scientific paper,
we propose a clustering approach where commu-
nities in the citation summary?s lexical network
are formed and sentences are extracted from sep-
arate clusters. Our experiments show how our
clustering method outperforms one of the cur-
rent state-of-art multi-document summarizing al-
gorithms, Lexrank, on this particular problem.
A future improvement will be to use a reorder-
ing approach like Maximal Marginal Relevance
A
r
t
i
c
l
e
G
o
l
d
M
e
a
d
?
s
R
a
n
d
o
m
L
e
x
r
a
n
k
C
-
R
R
C
-
l
e
x
r
a
n
k
D
P
C96-1058 1.00 0.27 0.73 0.73 0.73
P97-1003 1.00 0.08 0.40 0.60 0.40
P99-1065 0.94 0.30 0.54 0.82 0.67
P05-1013 1.00 0.15 0.69 0.97 0.67
P05-1012 0.95 0.14 0.57 0.26 0.62
P
B
M
T
N03-1017 0.96 0.26 0.36 0.61 0.64
W03-0301 1.00 0.60 1.00 1.00 1.00
J04-4002 1.00 0.33 0.70 0.48 0.48
N04-1033 1.00 0.38 0.38 0.31 0.85
P05-1033 1.00 0.37 0.77 0.77 0.85
S
u
m
m
A00-1043 1.00 0.66 0.95 0.71 0.95
A00-2024 1.00 0.26 0.86 0.73 0.60
C00-1072 1.00 0.85 0.85 0.93 0.93
W00-0403 1.00 0.55 0.81 0.41 0.70
W03-0510 1.00 0.58 1.00 0.83 0.83
Q
A
A00-1023 1.00 0.57 0.86 0.86 0.86
W00-0603 1.00 0.33 0.53 0.53 0.60
P02-1006 1.00 0.49 0.92 0.49 0.87
D03-1017 1.00 0.00 0.53 0.26 0.85
P03-1001 1.00 0.12 0.29 0.59 0.59
T
E
D04-9907 1.00 0.53 0.88 0.65 0.94
H05-1047 1.00 0.83 0.66 0.83 1.00
H05-1079 1.00 0.67 0.78 0.89 0.56
W05-1203 1.00 0.50 0.71 1.00 0.71
P05-1014 1.00 0.44 1.00 0.89 0.78
Mean 0.99 0.41 0.71 0.69 0.75
Table 7: Evaluation Results (|S| = 5)
(MMR) (Carbonell and Goldstein, 1998) to re-rank
clustered documents within each cluster in order
to reduce the redundancy in a final summary. An-
other possible approach is to assume the set of sen-
tences in the citation summary as sentences talk-
ing about the same event, yet generated in differ-
ent sources. Then one can apply the method in-
spired by (Barzilay et al, 1999) to identify com-
mon phrases across sentences and use language
generation to form a more coherent summary. The
ultimate goal, however, is to produce a topic sum-
marizer system in which the query is a scientific
topic and the output is a summary of all previous
works in that topic, preferably sorted to preserve
chronology and topicality.
7 Acknowledgments
The authors would like to thank Bonnie Dorr,
Jimmy Lin, Saif Mohammad, Judith L. Klavans,
Ben Shneiderman, and Aleks Aris from UMD,
Bryan Gibson, Joshua Gerrish, Pradeep Muthukr-
ishnan, Arzucan
?
Ozg?ur, Ahmed Hassan, and Thuy
Vu from University of Michigan for annotations.
This paper is based upon work supported by the
National Science Foundation grant ?iOPENER: A
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains?, jointly awarded
to U. of Michigan and U. of Maryland as IIS
0705832. Any opinions, findings, and conclusions
or recommendations expressed in this paper are
695
those of the authors and do not necessarily reflect
the views of the National Science Foundation.
References
Barzilay, Regina, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context of
multi-document summarization. In ACL?99, pages
550?557.
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
Conference on Research and Advanced Technology
for Digital Libraries.
Carbonell, Jaime G. and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In SIGIR?98,
pages 335?336.
Clauset, Aaron, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6):066111, Dec.
Elkiss, Aaron, Siwei Shen, Anthony Fader, G?unes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of the
American Society for Information Science and Tech-
nology, 59(1):51?62.
Erkan, G?unes? and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, G?unes?. 2006. Language model-based docu-
ment clustering using random walks. In Proceed-
ings of the HLT-NAACL conference, pages 479?486,
New York City, USA, June. Association for Compu-
tational Linguistics.
Joseph, Mark T. and Dragomir R. Radev. 2007. Ci-
tation analysis, centrality, and the ACL Anthol-
ogy. Technical Report CSE-TR-535-07, University
of Michigan. Department of Electrical Engineering
and Computer Science.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.
Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
?95, pages 68?73, New York, NY, USA. ACM.
Levenshtein, Vladimir I. 1966. Binary Codes Capa-
ble of Correcting Deletions, Insertions and Rever-
sals. Soviet Physics Doklady, 10:707.
Lin, Jimmy J. and Dina Demner-Fushman. 2006.
Methods for automatically evaluating answers
to complex questions. Information Retrieval,
9(5):565?587.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press.
Marton, Gregory and Alexey Radul. 2006. Nugge-
teer: Automatic nugget-based evaluation using de-
scriptions and judgements. In Proceedings of
NAACL/HLT.
Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI1999, pages 926?931.
Nanba, Hidetsugu, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004a. Bilingual presri:
Integration of multiple research paper databases. In
Proceedings of RIAO 2004, pages 195?211, Avi-
gnon, France.
Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004b. Classification of research papers using
citation links and citation types: Towards automatic
review article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.
Newman, Mark E. J. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko Dim-
itrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu
Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion,
Simone Teufel, Michael Topper, Adam Winkel, and
Zhu Zhang. 2004. MEAD - a platform for multi-
document multilingual text summarization. In LREC
2004, Lisbon, Portugal, May.
Siddharthan, Advaith and Simone Teufel. 2007.
Whose idea was this, and why does it matter? at-
tributing scientific work to citations. In Proceedings
of NAACL/HLT-07.
Teufel, Simone and Marc Moens. 2002. Summarizing
scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Teufel, Simone, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the EMNLP, Sydney, Australia,
July.
Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
696
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 228?237, Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Supervised Classification for Extracting Protein Interaction Sentences
using Dependency Parsing
Gu?nes? Erkan
University of Michigan
gerkan@umich.edu
Arzucan ?Ozgu?r
University of Michigan
ozgur@umich.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a relation extraction method to
identify the sentences in biomedical text that
indicate an interaction among the protein
names mentioned. Our approach is based on
the analysis of the paths between two protein
names in the dependency parse trees of the
sentences. Given two dependency trees, we
define two separate similarity functions (ker-
nels) based on cosine similarity and edit dis-
tance among the paths between the protein
names. Using these similarity functions, we
investigate the performances of two classes
of learning algorithms, Support Vector Ma-
chines and k-nearest-neighbor, and the semi-
supervised counterparts of these algorithms,
transductive SVMs and harmonic functions,
respectively. Significant improvement over
the previous results in the literature is re-
ported as well as a new benchmark dataset
is introduced. Semi-supervised algorithms
perform better than their supervised ver-
sion by a wide margin especially when the
amount of labeled data is limited.
1 Introduction
Protein-protein interactions play an important role
in vital biological processes such as metabolic and
signaling pathways, cell cycle control, and DNA
replication and transcription (Phizicky and Fields,
1995). A number of (mostly manually curated)
databases such as MINT (Zanzoni et al, 2002),
BIND (Bader et al, 2003), and SwissProt (Bairoch
and Apweiler, 2000) have been created to store pro-
tein interaction information in structured and stan-
dard formats. However, the amount of biomedical
literature regarding protein interactions is increas-
ing rapidly and it is difficult for interaction database
curators to detect and curate protein interaction in-
formation manually. Thus, most of the protein in-
teraction information remains hidden in the text of
the papers in the biomedical literature. Therefore,
the development of information extraction and text
mining techniques for automatic extraction of pro-
tein interaction information from free texts has be-
come an important research area.
In this paper, we introduce an information extrac-
tion approach to identify sentences in text that in-
dicate an interaction relation between two proteins.
Our method is different than most of the previous
studies (see Section 2) on this problem in two as-
pects: First, we generate the dependency parses of
the sentences that we analyze, making use of the
dependency relationships among the words. This
enables us to make more syntax-aware inferences
about the roles of the proteins in a sentence com-
pared to the classical pattern-matching information
extraction methods. Second, we investigate semi-
supervised machine learning methods on top of the
dependency features we generate. Although there
have been a number of learning-based studies in this
domain, our methods are the first semi-supervised
efforts to our knowledge. The high cost of label-
ing free text for this problem makes semi-supervised
methods particularly valuable.
We focus on two semi-supervised learning meth-
ods: transductive SVMs (TSVM) (Joachims, 1999),
228
and harmonic functions (Zhu et al, 2003). We also
compare these two methods with their supervised
counterparts, namely SVMs and k-nearest neigh-
bor algorithm. Because of the nature of these al-
gorithms, we propose two similarity functions (ker-
nels in SVM terminology) among the instances of
the learning problem. The instances in this problem
are natural language sentences with protein names in
them, and the similarity functions are defined on the
positions of the protein names in the corresponding
parse trees. Our motivating assumption is that the
path between two protein names in a dependency
tree is a good description of the semantic relation
between them in the corresponding sentence. We
consider two similarity functions; one based on the
cosine similarity and the other based on the edit dis-
tance among such paths.
2 Related Work
There have been many approaches to extract pro-
tein interactions from free text. One of them is
based on matching pre-specified patterns and rules
(Blaschke et al, 1999; Ono et al, 2001). How-
ever, complex cases that are not covered by the
pre-defined patterns and rules cannot be extracted
by these methods. Huang et al (2004) proposed a
method where patterns are discovered automatically
from a set of sentences by dynamic programming.
Bunescu et al (2005) have studied the performance
of rule learning algorithms. They propose two meth-
ods for protein interaction extraction. One is based
on the rule learning method Rapier and the other
on longest common subsequences. They show that
these methods outperform hand-written rules.
Another class of approaches is using more syntax-
aware natural language processing (NLP) tech-
niques. Both full and partial (shallow) parsing
strategies have been applied in the literature. In
partial parsing the sentence structure is decomposed
partially and local dependencies between certain
phrasal components are extracted. An example of
the application of this method is relational parsing
for the inhibition relation (Pustejovsky et al, 2002).
In full parsing, however, the full sentence structure
is taken into account. Temkin and Gilder (2003)
used a full parser with a lexical analyzer and a con-
text free grammar (CFG) to extract protein-protein
interaction from text. Another study that uses full-
sentence parsing to extract human protein interac-
tions is (Daraselia et al, 2004). Alternatively,
Yakushiji et al (2005) propose a system based on
head-driven phrase structure grammar (HPSG). In
their system protein interaction expressions are pre-
sented as predicate argument structure patterns from
the HPSG parser. These parsing approaches con-
sider only syntactic properties of the sentences and
do not take into account semantic properties. Thus,
although they are complicated and require many re-
sources, their performance is not satisfactory.
Machine learning techniques for extracting pro-
tein interaction information have gained interest in
the recent years. The PreBIND system uses SVM to
identify the existence of protein interactions in ab-
stracts and uses this type of information to enhance
manual expert reviewing for the BIND database
(Donaldson et al, 2003). Words and word bigrams
are used as binary features. This system is also
tested with the Naive Bayes classifier, but SVM is
reported to perform better. Mitsumori et al (2006)
also use SVM to extract protein-protein interac-
tions. They use bag-of-words features, specifically
the words around the protein names. These sys-
tems do not use any syntactic or semantic informa-
tion. Sugiyama et al (2003) extract features from
the sentences based on the verbs and nouns in the
sentences such as the verbal forms, and the part of
speech tags of the 20 words surrounding the verb
(10 before and 10 after it). Further features are used
to indicate whether a noun is found, as well as the
part of speech tags for the 20 words surrounding
the noun, and whether the noun contains numeri-
cal characters, non-alpha characters, or uppercase
letters. They construct k-nearest neighbor, decision
tree, neural network, and SVM classifiers by using
these features. They report that the SVM classifier
performs the best. They use part-of-speech informa-
tion, but do not consider any dependency or seman-
tic information.
The paper is organized as follows. In Section 3 we
describe our method of extracting features from the
dependency parse trees of the sentences and defin-
ing the similarity between two sentences. In Section
4 we discuss our supervised and semi-supervised
methods. In Section 5 we describe the data sets and
evaluation metrics that we used, and present our re-
229
sults. We conclude in Section 6.
3 Sentence Similarity Based on
Dependency Parsing
In order to apply the semi-supervised harmonic
functions and its supervised counterpart kNN, and
the kernel based TSVM and SVM methods, we need
to define a similarity measure between two sen-
tences. For this purpose, we use the dependency
parse trees of the sentences. Unlike a syntactic parse
(which describes the syntactic constituent structure
of a sentence), the dependency parse of a sentence
captures the semantic predicate-argument relation-
ships among its words. The idea of using depen-
dency parse trees for relation extraction in general
was studied by Bunescu and Mooney (2005a). To
extract the relationship between two entities, they
design a kernel function that uses the shortest path in
the dependency tree between them. The motivation
is based on the observation that the shortest path be-
tween the entities usually captures the necessary in-
formation to identify their relationship. They show
that their approach outperforms the dependency tree
kernel of Culotta and Sorensen (2004), which is
based on the subtree that contains the two entities.
We adapt the idea of Bunescu and Mooney (2005a)
to the task of identifying protein-protein interaction
sentences. We define the similarity between two
sentences based on the paths between two proteins
in the dependency parse trees of the sentences.
In this study we assume that the protein names
have already been annotated and focus instead on
the task of extracting protein-protein interaction sen-
tences for a given protein pair. We parse the sen-
tences with the Stanford Parser1 (de Marneffe et al,
2006). From the dependency parse trees of each sen-
tence we extract the shortest path between a protein
pair.
For example, Figure 1 shows the dependency tree
we got for the sentence ?The results demonstrated
that KaiC interacts rhythmically with KaiA, KaiB,
and SasA.? This example sentence illustrates that
the dependency path between a protein pair captures
the relevant information regarding the relationship
between the proteins better compared to using the
words in the unparsed sentence. Consider the pro-
1http://nlp.stanford.edu/software/lex-parser.shtml
tein pair KaiC and SasA. The words in the sentence
between these proteins are interacts, rhythmically,
with, KaiA, KaiB, and and. Among these words
rhythmically, KaiA, and and KaiB are not directly
related to the interaction relationship between KaiC
and SasA. On the other hand, the words in the depen-
dency path between this protein pair give sufficient
information to identify their relationship.
In this sentence we have four proteins (KaiC,
KaiA, KaiB, and SasA). So there are six pairs of
proteins for which a sentence may or may not be de-
scribing an interaction. The following are the paths
between the six protein pairs. In this example there
is a single path between each protein pair. However,
there may be more than one paths between a pro-
tein pair, if one or both appear multiple times in the
sentence. In such cases, we select the shortest paths
between the protein pairs.
ccomp
prep_with
results interacts
The
KaiA KaiB
rhytmically SasAthat KaiC
demonstrated
nsubj
complm nsubj
advmod
conj_and conj_and
det
Figure 1: The dependency tree of the sentence ?The
results demonstrated that KaiC interacts rhythmi-
cally with KaiA, KaiB, and SasA.?
1. KaiC - nsubj - interacts - prep with - SasA
2. KaiC - nsubj - interacts - prep with - SasA - conj and -
KaiA
3. KaiC - nsubj - interacts - prep with ? SasA - conj and -
KaiB
4. SasA - conj and - KaiA
5. SasA - conj and - KaiB
6. KaiA ? conj and ? SasA - conj and - KaiB
If a sentence contains n different proteins, there
are
(n
2
)
different pairs of proteins. We use machine
learning approaches to classify each sentence as an
interaction sentence or not for a protein pair. A sen-
tence may be an interaction sentence for one protein
230
pair, while not for another protein pair. For instance,
our example sentence is a positive interaction sen-
tence for the KaiC and SasA protein pair. However,
it is a negative interaction sentence for the KaiA and
SasA protein pair, i.e., it does not describe an inter-
action between this pair of proteins. Thus, before
parsing a sentence, we make multiple copies of it,
one for each protein pair. To reduce data sparseness,
we rename the proteins in the pair as PROTX1 and
PROTX2, and all the other proteins in the sentence
as PROTX0. So, for our example sentence we have
the following instances in the training set:
1. PROTX1 - nsubj - interacts - prep with - PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
3. PROTX1 - nsubj - interacts - prep with ? PROTX0 -
conj and - PROTX2
4. PROTX1 - conj and - PROTX2
5. PROTX1 - conj and - PROTX2
6. PROTX1 ? conj and ? PROTX0 - conj and - PROTX2
The first three instances are positive as they describe
an interaction between PROTX1 and PROTX2. The
last three are negative, as they do not describe an
interaction between PROTX1 and PROTX2.
We define the similarity between two instances
based on cosine similarity and edit distance based
similarity between the paths in the instances.
3.1 Cosine Similarity
Suppose pi and pj are the paths between PROTX1
and PROTX2 in instance xi and instance xj , respec-
tively. We represent pi and pj as vectors of term
frequencies in the vector-space model. The cosine
similarity measure is the cosine of the angle between
these two vectors and is calculated as follows:
cos sim(pi, pj) = cos(pi,pj) =
pi ? pj
?pi??pj?
(1)
that is, it is the dot product of pi and pj divided by
the lengths of pi and pj. The cosine similarity mea-
sure takes values in the range [0, 1]. If all the terms
in pi and pj are common, then it takes the maximum
value of 1. If none of the terms are common, then it
takes the minimum value of 0.
3.2 Similarity Based on Edit Distance
A shortcoming of cosine similarity is that it only
takes into account the common terms, but does not
consider their order in the path. For this reason, we
also use a similarity measure based on edit distance
(also called Levenshtein distance). Edit distance be-
tween two strings is the minimum number of op-
erations that have to be performed to transform the
first string to the second. In the original character-
based edit distance there are three types of opera-
tions. These are insertion, deletion, or substitution
of a single character. We modify the character-based
edit distance into a word-based one, where the oper-
ations are defined as insertion, deletion, or substitu-
tion of a single word.
The edit distance between path 1 and path 2 of
our example sentence is 2. We insert PROTX0 and
conj and to path 1 to convert it to path 2.
1. PROTX1 - nsubj - interacts - prep with - insert (PROTX0)
- insert (conj and) ? PROTX2
2. PROTX1 - nsubj - interacts - prep with - PROTX0 -
conj and - PROTX2
We normalize edit distance by dividing it by the
length (number of words) of the longer path, so that
it takes values in the range [0, 1]. We convert the dis-
tance measure into a similarity measure as follows.
edit sim(pi, pj) = e??(edit distance(pi,pj)) (2)
Bunescu and Mooney (2005a) propose a similar
method for relation extraction in general. However,
their similarity measure is based on the number of
the overlapping words between two paths. When
two paths have different lengths, they assume the
similarity between them is zero. On the other hand,
our edit distance based measure can also account for
deletions and insertions of words.
4 Semi-Supervised Machine Learning
Approaches
4.1 kNN and Harmonic Functions
When a similarity measure is defined among the in-
stances of a learning problem, a simple and natural
choice is to use a nearest neighbor based approach
that classifies each instance by looking at the labels
of the instances that are most similar to it. Per-
haps the simplest and most popular similarity-based
231
learning algorithm is the k-nearest neighbor classifi-
cation method (kNN). Let U be the set of unlabeled
instances, and L be the set of labeled instances in
a learning problem. Given an instance x ? U , let
NLk (x) be the set of top k instances in L that are
most similar to x with respect to some similarity
measure. The kNN equation for a binary classifi-
cation problem can be written as:
y(x) =
?
z?NLk (x)
sim(x, z)y(z)
?
z??NLk (x)
sim(x, z?) (3)
where y(z) ? {0, 1} is the label of the instance z.2
Note that y(x) can take any real value in the [0, 1]
interval. The final classification decision is made by
setting a threshold in this interval (e.g. 0.5) and clas-
sifying the instances above the threshold as positive
and others as negative. For our problem, each in-
stance is a dependency path between the proteins in
the pair and the similarity function can be one of the
functions we have defined in Section 3.
Equation 3 can be seen as averaging the labels (0
or 1) of the nearest neighbors of each unlabeled in-
stance. This suggests a generalized semi-supervised
version of the same algorithm by incorporating un-
labeled instances as neighbors as well:
y(x) =
?
z?NL?Uk (x)
sim(x, z)y(z)
?
z??NL?Uk (x)
sim(x, z?) (4)
Unlike Equation 3, the unlabeled instances are also
considered in Equation 4 when finding the nearest
neighbors. We can visualize this as an undirected
graph, where each data instance (labeled or unla-
beled) is a node that is connected to its k nearest
neighbor nodes. The value of y(?) is set to 0 or 1
for labeled nodes depending on their class. For each
unlabeled node x, y(x) is equal to the average of the
y(?) values of its neighbors. Such a function that
satisfies the average property on all unlabeled nodes
is called a harmonic function and is known to exist
and have a unique solution (Doyle and Snell, 1984).
Harmonic functions were first introduced as a semi-
supervised learning method by Zhu et al (2003).
There are interesting alternative interpretations of
2Equation 3 is the weighted (or soft) version of the kNN
algorithm. In the classical voting scheme, x is classified in the
category that the majority of its neighbors belong to.
a harmonic function on a graph. One of them can
be explained in terms of random walks on a graph.
Consider a random walk on a graph where at each
time point we move from the current node to one of
its neighbors. The next node is chosen among the
neighbors of the current node with probability pro-
portional to the weight (similarity) of the edge that
connects the two nodes. Assuming we start the ran-
dom walk from the node x, y(x) in Equation 4 is
then equal to the probability that this random walk
will hit a node labeled 1 before it hits a node labeled
0.
4.2 Transductive SVM
Support vector machines (SVM) is a supervised ma-
chine learning approach designed for solving two-
class pattern recognition problems. The aim is to
find the decision surface that separates the positive
and negative labeled training examples of a class
with maximum margin (Burges, 1998).
Transductive support vector machines (TSVM)
are an extension of SVM, where unlabeled data is
used in addition to labeled data. The aim now is
to assign labels to the unlabeled data and find a de-
cision surface that separates the positive and nega-
tive instances of the original labeled data and the
(now labeled) unlabeled data with maximum mar-
gin. Intuitively, the unlabeled data pushes the deci-
sion boundary away from the dense regions. How-
ever, unlike SVM, the optimization problem now
is NP-hard (Zhu, 2005). Pointers to studies for
approximation algorithms can be found in (Zhu,
2005).
In Section 3 we defined the similarity between
two instances based on the cosine similarity and
the edit distance based similarity between the paths
in the instances. Here, we use these path similar-
ity measures as kernels for SVM and TSVM and
modify the SV M light package (Joachims, 1999) by
plugging in our two kernel functions.
A well-defined kernel function should be sym-
metric positive definite. While cosine kernel is well-
defined, Cortes et al (2004) proved that edit kernel
is not always positive definite. However, it is pos-
sible to make the kernel matrix positive definite by
adjusting the ? parameter, which is a positive real
number. Li and Jiang (2005) applied the edit kernel
to predict initiation sites in eucaryotic mRNAs and
232
obtained improved results compared to polynomial
kernel.
5 Experimental Results
5.1 Data Sets
One of the problems in the field of protein-protein
interaction extraction is that different studies gen-
erally use different data sets and evaluation met-
rics. Thus, it is difficult to compare their re-
sults. Bunescu et al (2005) manually developed the
AIMED corpus3 for protein-protein interaction and
protein name recognition. They tagged 199 Medline
abstracts, obtained from the Database of Interacting
Proteins (DIP) (Xenarios et al, 2001) and known to
contain protein interactions. This corpus is becom-
ing a standard, as it has been used in the recent stud-
ies by (Bunescu et al, 2005; Bunescu and Mooney,
2005b; Bunescu and Mooney, 2006; Mitsumori et
al., 2006; Yakushiji et al, 2005).
In our study we used the AIMED corpus and the
CB (Christine Brun) corpus that is provided as a re-
source by BioCreAtIvE II (Critical Assessment for
Information Extraction in Biology) challenge eval-
uation4. We pre-processed the CB corpus by first
annotating the protein names in the corpus automat-
ically and then, refining the annotation manually. As
discussed in Section 3, we pre-processed both of the
data sets as follows. We replicated each sentence
for each different protein pair. For n different pro-
teins in a sentence,
(n
2
)
new sentences are created,
as there are that many different pairs of proteins.
In each newly created sentence we marked the pro-
tein pair considered for interaction as PROTX1 and
PROTX2, and all the remaining proteins in the sen-
tence as PROTX0. If a sentence describes an inter-
action between PROTX1 and PROTX2, it is labeled
as positive, otherwise it is labeled as negative. The
summary of the data sets after pre-processing is dis-
played in Table 15.
Since previous studies that use AIMED corpus
perform 10-fold cross-validation. We also per-
formed 10-fold cross-validation in both data sets and
report the average results over the runs.
3ftp://ftp.cs.utexas.edu/pub/mooney/bio-data/
4http://biocreative.sourceforge.net/biocreative 2.html
5The pre-processed data sets are available at
http://belobog.si.umich.edu/clair/biocreative
Data Set Sentences + Sentences - Sentences
AIMED 4026 951 3075
CB 4056 2202 1854
Table 1: Data Sets
5.2 Evaluation Metrics
We use precision, recall, and F-score as our metrics
to evaluate the performances of the methods. Preci-
sion (pi) and recall (?) are defined as follows:
pi = TPTP + FP ; ? =
TP
TP + FN (5)
Here, TP (True Positives) is the number of sen-
tences classified correctly as positive; FP (False
Positives) is the number of negative sentences that
are classified as positive incorrectly by the classifier;
and FN (False Negatives) is the number of positive
sentences that are classified as negative incorrectly
by the classifier.
F-score is the harmonic mean of recall and precision.
F -score = 2pi?pi + ? (6)
5.3 Results and Discussion
We evaluate and compare the performances of
the semi-supervised machine learning approaches
(TSVM and harmonic functions) with their super-
vised counterparts (SVM and kNN) for the task of
protein-protein interaction extraction. As discussed
in Section 3, we use cosine similarity and edit dis-
tance based similarity as similarity functions in har-
monic functions and kNN, and as kernel functions
in TSVM and SVM. Our instances consist of the
shortest paths between the protein pairs in the de-
pendency parse trees of the sentences. In our ex-
periments, we tuned the ? parameter of the edit
distance based path similarity function to 4.5 with
cross-validation. The results in Table 2 and Table 3
are obtained with 10-fold cross-validation. We re-
port the average results over the runs.
Table 2 shows the results obtained for the AIMED
data set. Edit distance based path similarity function
performs considerably better than the cosine sim-
ilarity function with harmonic functions and kNN
and usually slightly better with SVM and TSVM.
We achieve our best F-score performance of 59.96%
with TSVM with edit kernel. While SVM with edit
233
kernel achieves the highest precision of 77.52%, it
performs slightly worse than SVM with cosine ker-
nel in terms of F-score measure. TSVM performs
slightly better than SVM, both of which perform bet-
ter than harmonic functions. kNN is the worst per-
forming algorithm for this data set.
In Table 2, we also show the results obtained pre-
viously in the literature by using the same data set.
Yakushiji et al (2005) use an HPSG parser to pro-
duce predicate argument structures. They utilize
these structures to automatically construct protein
interaction extraction rules. Mitsumori et al (2006)
use SVM with the unparsed text around the pro-
tein names as features to extract protein interac-
tion sentences. Here, we show their best result ob-
tained by using the three words to the left and to the
right of the proteins. The most closely related study
to ours is that by Bunescu and Mooney (2005a).
They define a kernel function based on the short-
est path between two entities of a relationship in
the dependency parse tree of a sentence (the SPK
method). They apply this method to the domain
of protein-protein interaction extraction in (Bunescu
and Mooney, 2006). Here, they also test the meth-
ods ELCS (Extraction Using Longest Common Sub-
sequences) (Bunescu et al, 2005) and SSK (Sub-
sequence Kernel) (Bunescu and Mooney, 2005b).
We cannot compare our results to theirs directly,
because they report their results as a precision-
recall graph. However, the best F-score in their
graph seems to be around 0.50 and definitely lower
than the best F-scores we have achieved (? 0.59).
Bunescu and Mooney (2006) also use SVM as their
learning method in their SPK approach. They define
their similarity based on the number of overlapping
words between two paths and assign a similarity of
zero if the two paths have different lengths. Our
improved performance with SVM and the shortest
path dependency features may be due to the edit-
distance based kernel, which takes into account not
only the overlapping words, but also word order and
accounts for deletions and insertions of words. Our
results show that, SVM, TSVM, and harmonic func-
tions achieve better F-score and recall performances
than the previous studies by Yakushiji et al (2005),
Mitsumori et al (2006), and the SSK and ELCS ap-
proaches of Bunescu and Mooney (2006). SVM and
TSVM also achieve higher precision scores. Since,
Mitsumori et al (2006) also use SVM in their study,
our improved results with SVM confirms our moti-
vation of using dependency paths as features.
Table 3 shows the results we got with the CB
data set. The F-score performance with the edit
distance based similarity function is always better
than that of cosine similarity function for this data
set. The difference in performances is considerable
for harmonic functions and kNN. Our best F-score
is achieved with TSVM with edit kernel (85.22%).
TSVM performs slightly better than SVM. When
cosine similarity function is used, kNN performs
better than harmonic functions. However, when edit
distance based similarity is used, harmonic functions
achieve better performance. SVM and TSVM per-
form better than harmonic functions. But, the gap in
performance is low when edit distance based simi-
larity is used with harmonic functions.
Method Precision Recall F-Score
SVM-edit 77.52 43.51 55.61
SVM-cos 61.99 54.99 58.09
TSVM-edit 59.59 60.68 59.96
TSVM-cos 58.37 61.19 59.62
Harmonic-edit 44.17 74.20 55.29
Harmonic-cos 36.02 67.65 46.97
kNN-edit 68.77 42.17 52.20
kNN-cos 40.37 49.49 44.36
(Yakushiji et al, 2005) 33.70 33.10 33.40
(Mitsumori et al, 2006) 54.20 42.60 47.70
Table 2: Experimental Results ? AIMED Data Set
Method Precision Recall F-Score
SVM-edit 85.15 84.79 84.96
SVM-cos 87.83 81.45 84.49
TSVM-edit 85.62 84.89 85.22
TSVM-cos 85.67 84.31 84.96
Harmonic-edit 86.69 80.15 83.26
Harmonic-cos 72.28 70.91 71.56
kNN-edit 72.89 86.95 79.28
kNN-cos 65.42 89.49 75.54
Table 3: Experimental Results ? CB Data Set
Semi-supervised approaches are usually more ef-
fective when there is less labeled data than unlabeled
data, which is usually the case in real applications.
To see the effect of semi-supervised approaches we
perform experiments by varying the amount of la-
234
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 2: The F-score on the AIMED dataset with
varying sizes of training data
beled training sentences in the range [10, 3000]. For
each labeled training set size, sentences are selected
randomly among all the sentences, and the remain-
ing sentences are used as the unlabeled test set. The
results that we report are the averages over 10 such
random runs for each labeled training set size. We
report the results for the algorithms when edit dis-
tance based similarity is used, as it mostly performs
better than cosine similarity. Figure 2 shows the
results obtained over the AIMED data set. Semi-
supervised approaches TSVM and harmonic func-
tions perform considerably better than their super-
vised counterparts SVM and kNN when we have
small number of labeled training data. It is inter-
esting to note that, although SVM is one of the best
performing algorithms with more training data, it is
the worst performing algorithm with small amount
of labeled training sentences. Its performance starts
to increase when number of training data is larger
than 200. Eventually, its performance gets close to
that of the other algorithms. Harmonic functions is
the best performing algorithm when we have less
than 200 labeled training data. TSVM achieves bet-
ter performance when there are more than 500 la-
beled training sentences.
Figure 3 shows the results obtained over the CB
data set. When we have less than 500 labeled sen-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
300020001000500200100502010
F-
Sc
o
re
Number of Labeled Sentences
kNN
Harmonic
SVM
TSVM
Figure 3: The F-score on the CB dataset with vary-
ing sizes of training data
tences, harmonic functions and TSVM perform sig-
nificantly better than kNN, while SVM is the worst
performing algorithm. When we have more than
500 labeled training sentences, kNN is the worst per-
forming algorithm, while the performance of SVM
increases and gets similar to that of TSVM and
slightly better than that of harmonic functions.
6 Conclusion
We introduced a relation extraction approach based
on dependency parsing and machine learning to
identify protein interaction sentences in biomedical
text. Unlike syntactic parsing, dependency parsing
captures the semantic predicate argument relation-
ships between the entities in addition to the syntac-
tic relationships. We extracted the shortest paths be-
tween protein pairs in the dependency parse trees of
the sentences and defined similarity functions (ker-
nels in SVM terminology) for these paths based on
cosine similarity and edit distance. Supervised ma-
chine learning approaches have been applied to this
domain. However, they rely only on labeled training
data, which is difficult to gather. To our knowledge,
this is the first effort in this domain to apply semi-
supervised algorithms, which make use of both la-
beled and unlabeled data. We evaluated and com-
pared the performances of two semi-supervised ma-
235
chine learning approaches (harmonic functions and
TSVM), with their supervised counterparts (kNN
and SVM). We showed that, edit distance based sim-
ilarity function performs better than cosine simi-
larity function since it takes into account not only
common words, but also word order. Our 10-fold
cross validation results showed that, TSVM per-
forms slightly better than SVM, both of which per-
form better than harmonic functions. The worst per-
forming algorithm is kNN. We compared our results
with previous results published with the AIMED
data set. We achieved the best F-score performance
with TSVM with the edit distance kernel (59.96%)
which is significantly higher than the previously re-
ported results for the same data set.
In most real-world applications there are much
more unlabeled data than labeled data. Semi-
supervised approaches are usually more effective in
these cases, because they make use of both the la-
beled and unlabeled instances when making deci-
sions. To test this hypothesis for the application
of extracting protein interaction sentences from text,
we performed experiments by varying the number
of labeled training sentences. Our results show
that, semi-supervised algorithms perform consider-
ably better than their supervised counterparts, when
there are small number of labeled training sentences.
An interesting result is that, in such cases SVM per-
forms significantly worse than the other algorithms.
Harmonic functions achieve the best performance
when there are only a few labeled training sentences.
As number of labeled training sentences increases
the performance gap between supervised and semi-
supervised algorithms decreases.
Acknowledgments
This work was supported in part by grants R01-
LM008106 and U54-DA021519 from the US Na-
tional Institutes of Health.
References
G. Bader, D. Betel, and C. Hogue. 2003. Bind - the
biomolecular interaction network database. Nucleic
Acids Research, 31(1):248?250.
A. Bairoch and R. Apweiler. 2000. The swiss-prot pro-
tein sequence database and its supplement trembl in
2000. Nucleic Acids Research, 28(1):45?48.
C. Blaschke, M. A. Andrade, C. A. Ouzounis, and A. Va-
lencia. 1999. Automatic extraction of biological in-
formation from scientific text: Protein-protein interac-
tions. In Proceedings of the AAAI Conference on In-
telligent Systems for Molecular Biology (ISMB 1999),
pages 60?67.
R. C. Bunescu and R. J. Mooney. 2005a. A shortest
path dependency kernel for relation extraction. In Pro-
ceedings of the Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 724?731, Vancouver,
B.C, October.
R. C. Bunescu and R. J. Mooney. 2005b. Subsequence
kernels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems (NIPS), Vancouver, B.C, December.
R. C. Bunescu and R. J. Mooney, 2006. Text Mining and
Natural Language Processing, chapter Extracting Re-
lations from Text: From Word Sequences to Depen-
dency Paths. forthcoming book.
R. Bunescu, R. Ge, J. R. Kate, M. E. Marcotte, R. J.
Mooney, K. A. Ramani, and W. Y. Wong. 2005. Com-
parative experiments on learning information extrac-
tors for proteins and their interactions. Artificial Intel-
ligence in Medicine, 33(2):139?155, February.
C. J. C. Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data Mining and
Knowledge Discovery, 2(2):121?167.
C. Cortes, P. Haffner, and M. Mohri. 2004. Rational
kernels: Theory and algorithms. Journal of Machine
Learning Research, (5):1035?1062, August.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL-04), Barcelona, Spain, July.
N. Daraselia, A. Yuryev, S. Egorov, S. Novichkova,
A. Nikitin, and I. Mazo. 2004. Extracting human
protein interactions from medline using a full-sentence
parser. Bioinformatics, 20(5):604?611.
M-C. de Marneffe, B. MacCartney, and C. D. Manning.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proceedings of the IEEE /
ACL 2006 Workshop on Spoken Language Technology.
The Stanford Natural Language Processing Group.
I. Donaldson, J. Martin, B. de Bruijn, C. Wolting,
V. Lay, B. Tuekam, S. Zhang, B. Baskin, G. D. Bader,
K. Michalockova, T. Pawson, and C. W. V. Hogue.
2003. Prebind and textomy - mining the biomedical
literature for protein-protein interactions using a sup-
port vector machine. BMC Bioinformatics, 4:11.
236
P. G. Doyle and J. L. Snell. 1984. Random Walks
and Electric Networks. Mathematical Association of
America.
M. Huang, X. Zhu, Y. Hao, D. G. Payan, K. Qu, and
M. Li. 2004. Discovering patterns to extract protein-
protein interactions from full texts. Bioinformatics,
20(18):3604?3612.
T. Joachims. 1999. Transductive inference for text
classification using support vector machines. In Ivan
Bratko and Saso Dzeroski, editors, Proceedings of
ICML-99, 16th International Conference on Machine
Learning, pages 200?209. Morgan Kaufmann Publish-
ers, San Francisco, US.
H. Li and T. Jiang. 2005. A class of edit kernels for
svms to predict translation initiation sites in eukaryotic
mrnas. Journal of Computational Biology, 12(6):702?
718.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE Trans-
actions on Information and Systems, E89-D(8):2464?
2466.
T. Ono, H. Hishigaki, A. Tanigami, and T. Takagi.
2001. Automated extraction of information on
protein-protein interactions from the biological liter-
ature. Bioinformatics, 17(2):155?161.
E. M. Phizicky and S. Fields. 1995. Protein-protein in-
teractions: methods for detection and analysis. Micro-
biol. Rev., 59(1):94?123, March.
J. Pustejovsky, J. Castano, J. Zhang, M. Kotecki, and
B. Cochran. 2002. Robust relational parsing over
biomedical literature: Extracting inhibit relations. In
Proceedings of the seventh Pacific Symposium on Bio-
computing (PSB 2002), pages 362?373.
K. Sugiyama, K. Hatano, M. Yoshikawa, and S. Uemura.
2003. Extracting information on protein-protein in-
teractions from biological literature based on machine
learning approaches. Genome Informatics, 14:699?
700.
J. M. Temkin and M. R. Gilder. 2003. Extraction of pro-
tein interaction information from unstructured text us-
ing a context-free grammar. Bioinformatics, 19:2046?
2053.
I. Xenarios, E. Fernandez, L. Salwinski, X. J. Duan, M. J.
Thompson, E. M. Marcotte, and D. Eisenberg. 2001.
Dip: The database of interacting proteins: 2001 up-
date. Nucleic Acids Res., 29:239 ? 241, January.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In Proceedings of The
Eleventh Annual Meeting of The Association for Natu-
ral Language Processing, pages 93?96.
A. Zanzoni, L. Montecchi-Palazzi, M. Quondam,
G. Ausiello, M. Helmer-Citterich, and G. Cesareni.
2002. Mint: A molecular interaction database. FEBS
Letters, 513:135?140.
X. Zhu, Z. Ghahramani, and J. D. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In T. Fawcett and N. Mishra, editors,
ICML, pages 912?919. AAAI Press.
X. Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
237
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 658?666, Prague, June 2007. c?2007 Association for Computational Linguistics
MavenRank: Identifying Influential Members of the US Senate Using
Lexical Centrality
Anthony Fader
University of Michigan
afader@umich.edu
Dragomir Radev
University of Michigan
radev@umich.edu
Michael H. Crespin
The University of Georgia
crespin@uga.edu
Burt L. Monroe
The Pennsylvania State University
burtmonroe@psu.edu
Kevin M. Quinn
Harvard University
kevin quinn@harvard.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Abstract
We introduce a technique for identifying the
most salient participants in a discussion. Our
method, MavenRank is based on lexical cen-
trality: a random walk is performed on a
graph in which each node is a participant in
the discussion and an edge links two partici-
pants who use similar rhetoric. As a test, we
used MavenRank to identify the most influ-
ential members of the US Senate using data
from the US Congressional Record and used
committee ranking to evaluate the output.
Our results show that MavenRank scores are
largely driven by committee status in most
topics, but can capture speaker centrality in
topics where speeches are used to indicate
ideological position instead of influence leg-
islation.
1 Introduction
In a conversation or debate between a group of
people, we can think of two remarks as interact-
ing if they are both comments on the same topic.
For example, if one speaker says ?taxes should
be lowered to help business,? while another argues
?taxes should be raised to support our schools,? the
speeches are interacting with each other by describ-
ing the same issue. In a debate with many people
arguing about many different things, we could imag-
ine a large network of speeches interacting with each
other in the same way. If we associate each speech
in the network with its speaker, we can try to iden-
tify the most important people in the debate based
on how central their speeches are in the network.
To describe this type of centrality, we borrow a
term from The Tipping Point (Gladwell, 2002), in
which Gladwell describes a certain type of person-
ality in a social network called a maven. A maven
is a trusted expert in a specific field who influences
other people by passing information and advice. In
this paper, our goal is to identify authoritative speak-
ers who control the spread of ideas within a topic. To
do this, we introduce MavenRank, which measures
the centrality of speeches as nodes in the type of net-
work described in the previous paragraph.
Significant research has been done in the area
of identifying central nodes in a network. Vari-
ous methods exist for measuring centrality, includ-
ing degree centrality, closeness, betweenness (Free-
man, 1977; Newman, 2003), and eigenvector cen-
trality. Eigenvector centrality in particular has
been successfully applied to many different types
of networks, including hyperlinked web pages (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et al,
2004). The authors of (Lin and Kan, 2007) extended
these methods to include timestamped graphs where
nodes are added over time and applied it to multi-
document summarization. In (Tong and Faloutsos,
2006), the authors use random walks on a graph as
a method for finding a subgraph that best connects
some or all of a set of query nodes. In our paper,
we introduce a new application of eigenvector cen-
trality for identifying the central speakers in the type
of debate or conversation network described above.
Our method is based on the one described in (Erkan
658
and Radev, 2004) and (Mihalcea and Tarau, 2004),
but modified to rank speakers instead of documents
or sentences.
In our paper, we apply our method to analyze the
US Congressional Record, which is a verbatim tran-
script of speeches given in the United States House
of Representatives and Senate. The Record is a
dense corpus of speeches made by a large number
of people over a long period of time. Using the tran-
scripts of political speeches adds an extra layer of
meaning onto the measure of speaker centrality. The
centrality of speakers in Congress can be thought of
as a measure of relative importance or influence in
the US legislative process. We can also use speaker
centrality to analyze committee membership: are the
central speakers on a given issue ranking members
of a related committee? Is there a type of impor-
tance captured through speaker centrality that isn?t
obvious in the natural committee rankings?
There has been growing interest in using tech-
niques from natural language processing in the area
of political science. In (Porter et al, 2005) the
authors performed a network analysis of members
and committees of the US House of Representatives.
They found connections between certain commit-
tees and political positions that suggest that com-
mittee membership is not determined at random.
In (Thomas et al, 2006), the authors use the tran-
scripts of debates from the US Congress to auto-
matically classify speeches as supporting or oppos-
ing a given topic by taking advantage of the vot-
ing records of the speakers. In (Wang et al, 2005),
the authors use a generative model to simultane-
ously discover groups of voters and topics using
the voting records and the text from bills of the
US Senate and the United Nations. The authors
of (Quinn et al, 2006) introduce a multinomial mix-
ture model to perform unsupervised clustering of
Congressional speech documents into topically re-
lated categories. We rely on the output of this model
to cluster the speeches from the Record in order to
compare speaker rankings within a topic to related
committees.
We take advantage of the natural measures of
prestige in Senate committees and use them as a
standard for comparison with MavenRank. Our hy-
pothesis is that MavenRank centrality will capture
the importance of speakers based on the natural
committee rankings and seniority. We can test this
claim by clustering speeches into topics and then
mapping the topics to related committees. If the hy-
pothesis is correct, then the speaker centrality should
be correlated with the natural committee rankings.
There have been other attempts to link floor par-
ticipation with topics in political science. In (Hall,
1996), the author found that serving on a commit-
tee can positively predict participation in Congress,
but that seniority was not a good predictor. His
measure only looked at six bills in three commit-
tees, so his method is by far not as comprehensive
as the one that we present here. Our approach with
MavenRank differs from previous work by provid-
ing a large scale analysis of speaker centrality and
bringing natural language processing techniques to
the realm of political science.
2 Data
2.1 The US Congressional Speech Corpus
The text used in the experiments is from the United
States Congressional Speech corpus (Monroe et
al., 2006), which is an XML formatted version of
the electronic United States Congressional Record
from the Library of Congress1. The Congressional
Record is a verbatim transcript of the speeches made
in the US House of Representatives and Senate be-
ginning with the 101st Congress in 1998 and in-
cludes tens of thousands of speeches per year. In
our experiments we focused on the records from the
105th and 106th Senates. The basic unit of the US
Congressional Speech corpus is a record, which cor-
responds to a single subsection of the print version
of the Congressional Record and may contain zero
or more speakers. Each paragraph of text within
a record is tagged as either speech or non-speech
and each paragraph of speech text is tagged with the
unique id of the speaker. Figure 1 shows an example
record file for the sixth record on July 14th, 1997 in
the 105th Senate.
In our experiments we use a smaller unit of anal-
ysis called a speech document by taking all of the
text of a speaker within a single record. The cap-
italization and punctuation is then removed from
the text as in (Monroe et al, 2006) and then the
1http://thomas.loc.gov
659
text stemmed using Porter?s Snowball II stemmer2.
Figure 1 shows an example speech document for
speaker 15703 (Herb Kohl of Wisconsin) that has
been generated from the record in Figure 1.
In addition to speech documents, we also use
speaker documents. A speaker document is the
concatenation of all of a speaker?s speech docu-
ments within a single session and topic (so a sin-
gle speaker may have multiple speaker documents
across topics). For example within the 105th Senate
in topic 1 (?Judicial Nominations?), Senator Kohl
has four speech documents, so the speaker document
attributed to him within this session and topic would
be the text of these four documents treated as a sin-
gle unit. The order of the concatenation does not
matter since we will look at it as a vector of weighted
term frequencies (see Section 3.2).
2.2 Topic Clusters
We used the direct output of the 42-topic model of
the 105th-108th Senates from (Quinn et al, 2006)
to further divide the speech documents into topic
clusters. In their paper, they use a model where the
probabilities of a document belonging to a certain
topic varies smoothly over time and the words within
a given document have exactly the same probabil-
ity of being drawn from a particular topic. These
two properties make the model different than stan-
dard mixture models (McLachlan and Peel, 2000)
and the latent Dirichlet alocation model of (Blei et
al., 2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006). Table 1 lists the 42
topics and their related committees.
The output from the topic model is a D ? 42 ma-
trix Z where D is the number of speech documents
and the element zdk represents the probability of the
dth speech document being generated by topic k.
We clustered the speech documents by assigning a
speech document d to the kth cluster where
k = argmax
j
zdj .
If the maximum value is not unique, we arbitrarily
assign d to the lowest numbered cluster where zdj is
2http://snowball.tartarus.org/
algorithms/english/stemmer.html
a maximum. A typical topic cluster contains several
hundred speech documents, while some of the larger
topic clusters contain several thousand.
2.3 Committee Membership Information
The committee membership information that we
used in the experiments is from Stewart and
Woon?s committee assignment codebook (Stewart
and Woon, 2005). This provided us with a roster
for each committee and rank and seniority informa-
tion for each member. In our experiments we use
the rank within party and committee seniority mem-
ber attributes to test the output of our pipeline. The
rank within party attribute orders the members of a
committee based on the Resolution that appointed
the members with the highest ranking members hav-
ing the lowest number. The chair and ranking mem-
bers always receive a rank of 1 within their party. A
committee member?s committee seniority attribute
corresponds to the number of years that the member
has served on the given committee.
2.4 Mapping Topics to Committees
In order to test our hypothesis that lexical centrality
is correlated with the natural committee rankings,
we needed a map from topics to related commit-
tees. We based our mapping on Senate Rule XXV,3
which defines the committees, and the descriptions
on committee home pages. Table 1 shows the map,
where a topic?s related committees are listed in ital-
ics below the topic name. Because we are matching
short topic names to the complex descriptions given
by Rule XXV, the topic-committee map is not one
to one or even particularly well defined: some top-
ics are mapped to multiple committees, some top-
ics are not mapped to any committees, and two dif-
ferent topics may be mapped to the same commit-
tee. This is not a major problem because even if a
one to one map between topics and committees ex-
isted, speakers from outside a topic?s related com-
mittee are free to participate in the topic simply by
giving a speech. Therefore there is no way to rank
all speakers in a topic using committee information.
To test our hypotheses, we focused our attention on
topics that have at least one related committee. In
Section 4.3 we describe how the MavenRank scores
3http://rules.senate.gov/senaterules/
rule25.php
660
<?xml version="1.0" standalone="no"?>
<!DOCTYPE RECORD SYSTEM "record.dtd">
<RECORD>
<HEADER>
<CHAMBER>Senate</CHAMBER>
<TITLE>NOMINATION OF JOEL KLEIN TO BE ASSISTANT ATTORNEY
GENERAL IN CHARGE OF THE ANTITRUST DIVISION </TITLE>
<DATE>19970714</DATE>
</HEADER>
<BODY>
<GRAF>
<PAGEREF></PAGEREF>
<SPEAKER>NULL</SPEAKER>
<NONSPEECH>NOMINATION OF JOEL KLEIN TO BE ASSISTANT
ATTORNEY GENERAL IN CHARGE OF THE ANTITRUST DIVISION
(Senate - July 14, 1997)</NONSPEECH>
</GRAF>
<GRAF>
<PAGEREF>S7413</PAGEREF>
<SPEAKER>15703</SPEAKER>
<SPEECH> Mr. President, as the ranking Democrat on the
Antitrust Subcommittee, let me tell you why I support Mr.
Klein?s nomination, why he is a good choice for the job,
and why we ought to confirm him today.
</SPEECH>
</GRAF>
. . .
<GRAF>
<PAGEREF>S7414</PAGEREF>
<SPEAKER>UNK1</SPEAKER>
<SPEECH> Without objection, it is so ordered. </SPEECH>
</GRAF>
</BODY>
</RECORD>
mr presid a the rank democrat on the antitrust subcommitte
let me tell you why i support mr klein nomin why he i a
good choic for the job and why we ought to confirm him
todai
first joel klein i an accomplish lawyer with a distinguish
career he graduat from columbia univers and harvard law
school and clerk for the u court of appeal here in
washington then for justic powel just a importantli he i
the presid choic to head the antitrust divis and i believ
that ani presid democrat or republican i entitl to a strong
presumpt in favor of hi execut branch nomine second joel
klein i a pragmatist not an idealogu hi answer at hi confirm
hear suggest that he i not antibusi a some would claim the
antitrust divis wa in the late 1970 nor anticonsum a some
argu the divis wa dure the 1980 instead he will plot a middl
cours i believ that promot free market fair competit and
consum welfar
the third reason we should confirm joel klein i becaus no on
deserv to linger in thi type of legisl limbo here in congress
we need the input of a confirm head of the antitrust divis
to give u the administr view on a varieti of import polici
matter defens consolid electr deregul and telecommun merger
among other we need someon who can speak with author for the
divis without a cloud hang over hi head
more than that without a confirm leader moral at the
antitrust divis i suffer and given the pace at which the
presid ha nomin and the senat ha confirm appointe if we fail
to approv mr klein it will be at least a year befor we confirm
a replac mayb longer and mayb never so we need to act now we
can?t afford to let the antitrust divis continu to drift
final mr presid i have great respect for the senat from south
carolina a well a the senat from nebraska and north dakota
thei have been forc advoc for consum on telecommun matter and
. . .
Figure 1: A sample of the text from record 105.sen.19970714.006.xml and the speech document for Senator
Herb Kohl of Wisconsin (id 15703) generated from it. The ?. . . ? represents omitted text.
1 Judicial Nominations 15 Health 2 (Economics - Seniors) 27 Procedural 1 (Housekeeping 1)
Judiciary Health, Education, Labor, and Pensions 28 Procedural 2 (Housekeeping 2)
2 Law & Crime 1 (Violence / Drugs) Veterans? Affairs 29 Campaign Finance
Judiciary Agriculture, Nutrition, and Forestry Rules and Administration
3 Banking / Finance Aging (Special Committee) 30 Law & Crime 2 (Federal)
Banking, Housing, and Urban Affairs Finance Judiciary
4 Armed Forces 1 (Manpower) 16 Gordon Smith re Hate Crime 31 Child Protection
Armed Services 17 Debt / Deficit / Social Security Health, Education, Labor, and Pensions
5 Armed Forces 2 (Infrastructure) Appropriations Agriculture, Nutrition, and Forestry
Armed Services Budget 32 Labor 1 (Workers, esp. Retirement)
6 Symbolic (Tribute - Living) Finance Health, Education, Labor, and Pensions
7 Symbolic (Congratulations - Sports) Aging (Special Committee) Aging (Special Committee)
8 Energy 18 Supreme Court / Constitutional Small Business and Entrepreneurship
Energy and Natural Resources Judiciary 33 Environment 2 (Regulation)
9 Defense (Use of Force) 19 Commercial Infrastructure Environment and Public Works
Armed Services Commerce, Science, and Transportation Agriculture, Nutrition, and Forestry
Homeland Security and Governmental Affairs 20 Symbolic (Remembrance - Military) Energy and Natural Resources
Intelligence (Select Committee) 21 International Affairs (Diplomacy) 34 Procedural 3 (Legislation 1)
10 Jesse Helms re Debt Foreign Relations 35 Procedural 4 (Legislation 2)
11 Environment 1 (Public Lands) 22 Abortion 36 Procedural 5 (Housekeeping 3)
Energy and Natural Resources Judiciary 37 Procedural 6 (Housekeeping 4)
Agriculture, Nutrition, and Forestry Health, Education, Labor, and Pensions 38 Taxes
12 Health 1 (Medical) 23 Symbolic (Tribute - Constituent) Finance
Health, Education, Labor, and Pensions 24 Agriculture 39 Symbolic (Remembrance - Nonmilitary)
13 International Affairs (Arms Control) Agriculture, Nutrition, and Forestry 40 Labor 2 (Employment)
Foreign Relations 25 Intelligence Health, Education, Labor, and Pensions
14 Social Welfare Intelligence (Select Committee) Small Business and Entrepreneurship
Agriculture, Nutrition, and Forestry Homeland Security and Governmental Affairs 41 Foreign Trade
Banking, Housing, and Urban Affairs 26 Health 3 (Economics - General) Finance
Health, Education, Labor, and Pensions Health, Education, Labor, and Pensions Banking, Housing, and Urban Affairs
Finance Finance 42 Education
Health, Education, Labor, and Pensions
Table 1: The numbers and names of the 42 topics from (Quinn et al, 2006) with our mappings to related
committees (listed below the topic name, if available).
661
of speakers who are not members of related commit-
tees were taken into account when we measured the
rank correlations.
3 MavenRank and Lexical Similarity
The following sections describe MavenRank, a mea-
sure of speaker centrality, and tf-idf cosine similar-
ity, which is used to measure the lexical similarity of
speeches.
3.1 MavenRank
MavenRank is a graph-based method for finding
speaker centrality. It is similar to the methods
in (Erkan and Radev, 2004; Mihalcea and Tarau,
2004; Kurland and Lee, 2005), which can be used
for ranking sentences in extractive summaries and
documents in an information retrieval system. Given
a collection of speeches s1, . . . , sN and a measure
of lexical similarity between pairs sim(si, sj) ? 0,
a similarity graph can be constructed. The nodes
of the graph represent the speeches and a weighted
similarity edge is placed between pairs that exceed
a similarity threshold smin. MavenRank is based on
the premise that important speakers will have cen-
tral speeches in the graph, and that central speeches
should be similar to other central speeches. A recur-
sive explanation of this concept is that the score of
a speech should be proportional to the scores of its
similar neighbors.
Given a speech s in the graph, we can express the
recursive definition of its score p(s) as
p(s) =
?
t?adj[s]
p(t)
wdeg(t)
(1)
where adj[s] is the set of all speeches adjacent to
s and wdeg(t) =
?
u?adj[t] sim(t, u), the weighted
degree of t. Equation (1) captures the idea that the
MavenRank score of a speech is distributed to its
neighbors. We can rewrite this using matrix notation
as
p = pB (2)
where p = (p(s1), p(s2), . . . , p(sN )) and the ma-
trixB is the row normalized similarity matrix of the
graph
B(i, j) =
S(i, j)
?
k S(i, k)
(3)
where S(i, j) = sim(si, sj). Equation (2) shows
that the vector of MavenRank scores p is the left
eigenvector of B with eigenvalue 1.
We can prove that the eigenvector p exists by us-
ing a techinque from (Page et al, 1999). We can
treat the matrix B as a Markov chain describing
the transition probabilities of a random walk on the
speech similarity graph. The vector p then repre-
sents the stationary distribution of the random walk.
It is possible that some parts of the graph are dis-
connected or that the walk gets trapped in a com-
ponent. These problems are solved by reserving
a small escape probability at each node that repre-
sents a chance of jumping to any node in the graph,
making the Markov chain irreducible and aperiodic,
which guarantees the existence of the eigenvector.
Assuming a uniform escape probability for each
node on the graph, we can rewrite Equation (2) as
p = p[dU+ (1? d)B] (4)
where U is a square matrix with U(i, j) = 1/N
for all i and j, N is the number of nodes, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998). Equation (4) is
known as PageRank (Page et al, 1999) and is used
for determining prestige on the web in the Google
search engine.
3.2 Lexical Similarity
In our experiments, we used tf-idf cosine similarity
to measure lexical similarity between speech docu-
ments. We represent each speech document as a vec-
tor of term frequencies (or tf), which are weighted
according to the relative importance of the given
term in the cluster. The terms are weighted by their
inverse document frequency or idf. The idf of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
nw
)
(5)
where N is the number of documents in the corpus
and nw is the number of documents in the corpus
containing the term w. It follows that very common
words like ?of? or ?the? have a very low idf, while
the idf values of rare words are higher. In our experi-
ments, we calculated the idf values for each topic us-
ing all speech documents across sessions within the
662
 20
 30
 40
 50
 60
 70
 80
 90
 100
Abortion ChildProtection Education Workers,Retirement
SantorumBoxerKennedy
Figure 2: MavenRank percentiles for three speakers
over four topics.
given topic. We calculated topic-specific idf values
because some words may be relatively unimportant
in one topic, but important in another. For example,
in topic 22 (?Abortion?), the idf of the term ?abort?
is near 0.20, while in topic 38 (?Taxes?), its idf is
near 7.18.
The tf-idf cosine similarity measure
tf-idf-cosine(u, v) is defined as
P
w?u,v tfu(w) tfv(w) idf(w)
2
?P
w?u(tfu(w) idf(w))
2
?P
w?v(tfv(w) idf(w))
2
, (6)
which is the cosine of the angle between the tf-idf
vectors.
There are other alternatives to tf-idf cosine sim-
ilarity. Some other possible similarity measures
are document edit distance, the language models
from (Kurland and Lee, 2005), or generation proba-
bilities from (Erkan, 2006). For simplicity, we only
used tf-idf similarities in our experiments, but any of
these measures could be used in this case.
4 Experiments and Results
4.1 Data
We used the topic clusters from the 105th Senate
as training data to adjust the parameter smin and
observe trends in the data. We did not run experi-
ments to test the effect of different values of smin on
MavenRank scores, but our chosen value of 0.25 has
shown to give acceptable results in similar experi-
ments (Erkan and Radev, 2004). We used the topic
clusters from the 106th Senate as test data. For the
speech document networks, there was an average of
351 nodes (speech documents) and 2142 edges per
topic. For the speaker document networks, there was
an average of 63 nodes (speakers) and 545 edges per
topic.
4.2 Experimental Setup
We set up a pipeline using a Perl implementation
of tf-idf cosine similarity and MavenRank. We ran
MavenRank on the topic clusters and ranked the
speakers based on the output. We used two different
types granularities of the graphs as input: one where
the nodes are speech documents and another where
the nodes are speaker documents (see Section 2.1).
For the speech document graph, a speaker?s score is
determined by the sum of the MavenRank scores of
the speeches given by that speaker.
4.3 Evaluation Methods
To evaluate our output, we estimate independent
ordinary least squares linear regression models of
MavenRank centrality for topics with at least one re-
lated committee (there are 29 total):
MavenRankik = ?0k + ?skSeniorityik +
+?rkRankingMemberjk + ik (7)
where i indexes Senators, k indexes topics,
Seniorityik is the number of years Senator i has
served on the relevant committee for topic k (value
zero for those not on a relevant committee) and
RankingMemberjk has the value of one only for
the Chair and ranking minority member of a rele-
vant committee. We are interested primarily in the
overall significance of the estimated model (indicat-
ing committee effects) and, secondarily, in the spe-
cific source of any committee effect in seniority or
committee rank.
4.4 Results
Table 2 summarizes the results. ?Maven? status on
most topics does appear to be driven by committee
status, as expected. There are particularly strong ef-
fects of seniority and rank in topics tied to the Judi-
ciary, Foreign Relations, and Armed Services com-
mittees, as well as legislation-rich areas of domestic
policy. Perhaps of greater interest are the topics that
do not have committee effects. These are of three
distinct types. The first are highly politicized top-
ics for which speeches are intended not to influence
663
Topic p(F )a p(?s > 0)b p(?r > 0)c Topic p(F ) p(?s > 0) p(?r > 0)
Seniority and Ranking Status Both Significant Seniority and Ranking Status Jointly Significant
2 Law & Crime 1 [Violent] < .001 0.016 < .001 26 Health 3 [Economics] 0.001 0.106 0.064
18 Constitutional < .001 0.003 < .001 32 Labor 1 [Workers] 0.007 0.156 0.181
33 Environment 2 [Regulation] 0.007 0.063 0.056
Seniority Significant 3 Banking / Finance 0.042 0.141 0.579
12 Health 1 [Medical] < .001 < .001 0.567
42 Education < .001 < .001 0.337 No Significant Effects of Committee Status
41 Trade < .001 < .001 0.087 11 Environment 1 [Public Lands] 0.104 0.102 0.565
21 Int?l Affairs [Nonmilitary] < .001 0.007 0.338 22 Abortion 0.419 0.609 0.252
9 Defense [Use of Force] 0.002 0.001 0.926 5 Armed Forces 2 [Infrastructure] 0.479 0.267 0.919
19 Commercial Infrastructure 0.007 0.032 0.332 24 Agriculture 0.496 0.643 0.425
40 Labor 2 [Employment] 0.029 0.010 0.114 17 Debt / Social Security 0.502 0.905 0.295
38 Taxes 0.037 0.033 0.895 15 Health 2 [Seniors] 0.706 0.502 0.922
25 Intelligence 0.735 0.489 0.834
Ranking Status Significant 29 Campaign Finance 0.814 0.748 0.560
30 Crime 2 [Federal] < .001 0.334 < .001 31 Child Protection 0.856 0.580 0.718
8 Energy < .001 0.145 < .001
1 Judicial Nominations < .001 0.668 < .001
14 Social Welfare < .001 0.072 0.005
13 Int?l Affairs [Arms] < .001 0.759 0.001
4 Armed Forces 1 [Manpower] 0.007 0.180 0.049
aF-test for joint significance of committee variables.
bT-test for significance of committee seniority.
cT-test for significance of chair or ranking member status.
Table 2: Significance tests for ordinary least squares (OLS) linear regressions ofMavenRank scores (Speech-
documents graph) on committee seniority (in years) and ranking status (chair or ranking member), 106th
Senate, topic-by-topic. Results for the speaker-documents graph are similar.
legislation as much as indicate an ideological or par-
tisan position, so the mavens are not on particular
committees (abortion, children, seniors, the econ-
omy). The second are ?distributive politics? topics
where many Senators speak to defend state or re-
gional interests, so debate is broadly distributed and
there are no clear mavens (agriculture, military base
closures, public lands). Third are topics where there
are not enough speeches for clear results, because
most debate occurred after 1999-2000 (post-9/11
intelligence reform, McCain-Feingold campaign fi-
nance reform).
Alternative models, using measures of centrality
based on the centroid were also examined. Dis-
tance to centroid provides broadly similar results as
MavenRank, with several marginal significance re-
sults reversed in each direction. Cosine similarity
with centroid, on the other hand, appears to have no
relationship with committee structure.
Figure 2 shows the MavenRank percentiles (us-
ing the speech document network) for Senators Rick
Santorum, Barbara Boxer, and Edward Kennedy
across a few topics in the 106th Senate. These
sample scores conform to the expected rankings for
these speakers. In this session, Santorum was the
sponsor of a bill to ban partial birth abortions and
was a spokesman for Social Security reform, which
support his high ranking in abortion and work-
ers/retirement. Boxer acted as the lead opposition
to Santorum?s abortion bill and is known for her
support of child abuse laws. Kennedy was ranking
member of the Health, Education, Labor, and Pen-
sions committee and the Judiciary committee (which
was involved with the abortion bill).
4.5 MavenRank in Other Contexts
MavenRank is a general method for finding central
speakers in a discussion and can be applied to areas
outside of political science. One potential applica-
tion would be analyzing blog posts to find ?Maven?
bloggers by treating blogs as speakers and posts as
speeches. Similarly, MavenRank could be used to
find central participants in a newsgroup, a forum, or
a collection of email conversations.
5 Conclusion
We have presented a technique for identifying lexi-
cally central speakers using a graph based method
called MavenRank. To test our method for find-
ing central speakers, we analyzed the Congressional
664
Record by creating a map from the clusters of
speeches to Senate committees and comparing the
natural ranking committee members to the output of
MavenRank. We found evidence of a possible rela-
tionship between the lexical centrality and commit-
tee rank of a speaker by ranking the speeches us-
ing MavenRank and computing the rank correlation
with the natural ordering of speakers. Some spe-
cific committees disagreed with our hypothesis that
MavenRank and committee position are correlated,
which we propose is because of the non-legislative
aspects of those specific committees. The results
of our experiment suggest that MavenRank can in-
deed be used to find central speakers in a corpus of
speeches.
We are currently working on applying our meth-
ods to the US House of Representatives and other
records of parliamentary speech from the United
Kingdom and Australia. We have also developed a
dynamic version of MavenRank that takes time into
account when finding lexical centrality and plan on
using it with the various parliamentary records. We
are interested in dynamic MavenRank to go further
with the idea of tracking how ideas get propagated
through a network of debates, including congres-
sional records, blogs, and newsgroups.
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the views of the National Sci-
ence Foundation.
References
David Blei and John Lafferty. 2006. Dynamic topic
models. In Machine Learning: Proceedings of the
Twenty-Third International Conference (ICML).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Gunes Erkan. 2006. Language model-based document
clustering using random walks. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 479?486, New York
City, USA, June. Association for Computational Lin-
guistics.
L. C. Freeman. 1977. A set of measures of central-
ity based on betweenness. Sociometry, 40(1):35?41,
March.
Malcolm Gladwell. 2002. The Tipping Point: How Little
Things Can Make a Big Difference. Back Bay Books,
January.
Richard L. Hall. 1996. Participation in Congress. Yale
University Press.
Jon M. Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th An-
nual ACM-SIAM Symposium on Discrete Algorithms,
pages 668?677.
Oren Kurland and Lillian Lee. 2005. PageRank without
hyperlinks: Structural re-ranking using links induced
by language models. In Proceedings of SIGIR, pages
306?313.
Oren Kurland and Lillian Lee. 2006. Respect my author-
ity! HITS without hyperlinks, utilizing cluster-based
language models. In Proceedings of SIGIR, pages 83?
90.
Ziheng Lin and Min-Yen Kan. 2007. Timestamped
graphs: Evolutionary models of text for multi-
document summarization. In Proceedings of the Sec-
ond Workshop on TextGraphs: Graph-Based Algo-
rithms for Natural Language Processing, pages 25?32,
Rochester, NY, USA. Association for Computational
Linguistics.
Geoffrey McLachlan and David Peel. 2000. Finite Mix-
ture Models. New York: Wiley.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proceedings of the Ninth Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ?04).
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application to
word sense disambiguation. In Proceedings of the
Twentieth International Conference on Computational
Linguistics (COLING ?04), pages 1126?1132.
665
Burt L. Monroe, Cheryl L. Monroe, Kevin M. Quinn,
Dragomir Radev, Michael H. Crespin, Michael P. Co-
laresi, Anthony Fader, Jacob Balazer, and Steven P.
Abney. 2006. United states congressional speech cor-
pus. Department of Political Science, The Pennsylva-
nia State University.
Mark E. J. Newman. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the Web. Technical Report 1999-66,
Stanford Digital Library Technologies Project, Stan-
ford University, November 11,.
Mason A. Porter, Peter J. Mucha, M. E. J. Newman, and
Casey M. Warmbrand. 2005. A network analysis of
committees in the u.s. house of representatives. PNAS,
102(20), May.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th-108th
U.S. senate. In Midwest Political Science Association
Meeting.
K. Sparck-Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28(1):11?20.
Charles Stewart and Jonathan Woon. 2005. Con-
gressional committee assignments, 103rd to 105th
congresses, 1993?1998: Senate, july 12, 2005.
http://web.mit.edu/17.251/www/
data_page.html.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Hanghang Tong and Christos Faloutsos. 2006. Center-
piece subgraphs: problem definition and fast solutions.
In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, and
Dimitrios Gunopulos, editors, KDD, pages 404?413.
ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2005. Group and topic discovery from relations and
their attributes. In NIPS.
666
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1398?1407,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Detecting Speculations and their Scopes in Scientific Text
Arzucan
?
Ozg?ur
Department of EECS
University of Michigan
Ann Arbor, MI 48109, USA
ozgur@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI 48109, USA
radev@umich.edu
Abstract
Distinguishing speculative statements
from factual ones is important for most
biomedical text mining applications. We
introduce an approach which is based
on solving two sub-problems to identify
speculative sentence fragments. The first
sub-problem is identifying the speculation
keywords in the sentences and the second
one is resolving their linguistic scopes.
We formulate the first sub-problem as a
supervised classification task, where we
classify the potential keywords as real
speculation keywords or not by using
a diverse set of linguistic features that
represent the contexts of the keywords.
After detecting the actual speculation
keywords, we use the syntactic structures
of the sentences to determine their scopes.
1 Introduction
Speculation, also known as hedging, is a fre-
quently used language phenomenon in scientific
articles, especially in experimental studies, which
are common in the biomedical domain. When re-
searchers are not completely certain about the in-
ferred conclusions, they use speculative language
to convey this uncertainty. Consider the follow-
ing example sentences from abstracts of articles in
the biomedical domain. The abstracts are available
at the U.S. National Library of Medicine PubMed
web page
1
. The PubMed Identifier (PMID) of the
corresponding article is given in parenthesis.
1. We showed that the Roaz protein bound specifically to
O/E-1 by using the yeast two-hybrid system. (PMID:
9151733)
2. These data suggest that p56lck is physically associated
with Fc gamma RIIIA (CD16) and functions to mediate
1
http://www.ncbi.nlm.nih.gov/pubmed/
signaling events related to the control of NK cellular
cytotoxicity. (PMID: 8405050)
The first sentence is definite, whereas the sec-
ond one contains speculative information, which is
conveyed by the use of the word ?suggest?. While
speculative information might still be useful for
biomedical scientists, it is important that it is dis-
tinguished from the factual information.
Recognizing speculations in scientific text has
gained interest in the recent years. Previous
studies focus on identifying speculative sentences
(Light et al, 2004; Medlock and Briscoe, 2007;
Szarvas, 2008; Kilicoglu and Bergler, 2008).
However, in many cases, not the entire sentence,
but fragments of a sentence are speculative. Con-
sider the following example sentences.
1. The mature mitochondrial forms of the erythroid and
housekeeping ALAS isozymes are predicted to have
molecular weights of 59.5 kd and 64.6 kd, respectively.
(PMID: 2050125)
2. Like RAD9, RAD9B associates with HUS1, RAD1, and
RAD17, suggesting that it is a RAD9 paralog that
engages in similar biochemical reactions. (PMID:
14611806)
Both sentences are speculative, since they con-
tain speculative information, which is signaled by
the use of the word ?predicted? in the first sen-
tence and the word ?suggesting? in the second
sentence. The scope of the speculation keyword
?predicted? in the first sentence spans the entire
sentence. Therefore, classifying the sentence as
speculative does not cause information loss. How-
ever, the scope of the speculation keyword ?sug-
gesting? in the second sentence applies only to
the second clause of the sentence. In other words,
only the statement ?RAD9B is a RAD9 paralog
that engages in similar biochemical reactions? is
speculative. The statement ?Like RAD9, RAD9B
associates with HUS1, RAD1, and RAD17? con-
veys factual information. Therefore, classifying
1398
the entire sentence as speculative will result in in-
formation loss.
In this paper, we aim to go beyond recogniz-
ing speculative sentences and tackle the problem
of identifying speculative fragments of sentences.
We propose an approach which is based on solv-
ing two sub-problems: (1) detecting the real spec-
ulation keywords, (2) resolving their linguistic
scopes in the sentences. As the previous exam-
ples demonstrated speculations are signaled with
speculation keywords (e.g. might, suggest, likely,
hypothesize, could, predict, and etc.). However,
these keywords are not always used in a specula-
tive context. In other words, they are not always
real speculation keywords. Unlike previous ap-
proaches which classify sentences as speculative
or not, we formulate the problem as classifying the
keywords as real speculation keywords or not. We
extract a diverse set of features such as linguistic
features that represent the context of the keyword
and positional features of the sentence in which
the keyword occurs. We use these features with
Support Vector Machines (SVM) to learn models
to classify whether the occurrence of a keyword
is in a speculative context or not. After detecting
the real speculation keywords, we use the syntactic
structures of the sentences to identify their linguis-
tic scopes.
2 Related Work
Although hedging in scientific articles has been
studied from a linguistics perspective since the
1990s (e.g. (Hyland, 1998)), it has only gained in-
terest from a natural language processing perspec-
tive in the recent years.
The problem of identifying speculative sen-
tences in biomedical articles has been introduced
by Light et al (2004). The authors discussed the
possible application areas of recognizing specu-
lative language and investigated whether the no-
tion of speculative sentences can be characterized
to enable manual annotation. The authors devel-
oped two automated systems to classify sentences
as speculative or not. The first method is based
on substring matching. A sentence is classified as
speculative if it contains one of the 14 predefined
strings (suggest, potential, likely, may, at least, in
part, possibl, further investigation, unlikely, pu-
tative, insights, point toward, promise, propose).
The second method is based on using SVM with
bag-of-words features. The substring matching
method performed slightly better than the SVM
with bag-of-words features approach.
Medlock and Briscoe (2007) extended the work
of Light et al (2004) by refining their annota-
tion guidelines and creating a publicly available
data set (FlyBase data set) for speculative sen-
tence classification. They proposed a weakly su-
pervised machine learning approach to classify
sentences as speculative or not with the aim of
minimizing the need for manually labeled train-
ing data. Their approach achieved 76% preci-
sion/recall break-even point (BEP) performance
on the FlyBase data set, compared to the BEP
of 60% obtained by Light et al?s (2004) sub-
string matching approach on the same data set.
Szarvas (2008) extended the weakly supervised
machine learning methodology of Medlock and
Briscoe (2007) by applying feature selection to re-
duce the number of candidate keywords, by us-
ing limited manual supervision to filter the fea-
tures, and by extending the feature representation
with bigrams and trigrams. In addition, by fol-
lowing the annotation guidelines of Medlock and
Briscoe (2007), Szarvas (2008) made available the
BMC Bioinformatics data set, by annotating four
full text papers from the open access BMC Bioin-
formatics website. They achieved a BEP perfor-
mance of 85.29% and an F-measure of 85.08% on
the FlyBase data set. The F-measure performance
achieved on the BMC Bioinformatics data set was
74.93% when the FlyBase data set was used for
training. Kilicoglu and Bergler (2008) compiled
a list of speculation keywords from the examples
in (Hyland, 1998) and extended this list by us-
ing WordNet (Fellbaum, 1998) and UMLS SPE-
CIALIST Lexicon (McCray et al, 1994). They
used manually crafted syntactic patterns to iden-
tify speculative sentences and achieved a BEP and
an F-measure of 85% on the FlyBase data set and a
BEP and an F-measure of 82% on the BMC Bioin-
formatics data set.
Unlike pervious studies, which treat the prob-
lem of identifying speculative language as a sen-
tence classification task, we tackle the more chal-
lenging problem of identifying the portions of sen-
tences which are speculative. In other words, we
allow a sentence to include both speculative and
non-speculative parts. We introduce and eval-
uate a diverse set of features that represent the
context of a keyword and use these features in
a supervised machine learning setting to classify
1399
the keywords as real speculation keywords or not.
Then, we develop a rule-based method to deter-
mine their linguistic scopes by considering the
keyword-specific features and the syntactic struc-
tures of the sentences. To the best of our knowl-
edge, the BioScope corpus (Vincze et al, 2008) is
the only available data set that has been annotated
for speculative sentence fragments and we report
the first results on this corpus.
3 Corpus
The BioScope corpus
2
has been annotated at the
token level for speculation keywords and at the
sentence level for their linguistic scopes (Vincze
et al, 2008). The corpus consists of three sub-
corpora: medical free texts (radiology reports),
biomedical article abstracts, and biomedical full
text articles. In this paper we focus on identifying
speculations in scientific text. Therefore, we use
the biomedical article abstracts and the biomedi-
cal full text articles in our experiments. The statis-
tics (number of documents, number of sentences,
and number of occurrences of speculation key-
words) for these two sub-corpora are given in Ta-
ble 1. The scientific abstracts in the BioScope cor-
Data Set Documents Sentences Hedge Keywords
Abstracts 1273 11871 2694
Full Papers 9 2670 682
Table 1: Summary of the biomedical scientific articles sub-
corpora of the BioScope corpus
pus were included from the Genia corpus (Col-
lier et al, 1999). The full text papers consist of
five articles from the FlyBase data set and four
articles from the open access BMC Bioinformat-
ics website. The sentences in the FlyBase and
BMC Bioinformatics data sets were annotated as
speculative or not and made available by Med-
lock and Briscoe (2007) and Szarvas (2008), re-
spectively and have been used by previous stud-
ies in identifying speculative sentences (Medlock
and Briscoe, 2007; Kilicoglu and Bergler, 2008;
Szarvas, 2008). Vincze et al (2008) annotated
these full text papers and the Genia abstracts for
speculation keywords and their scopes and in-
cluded them to the BioScope corpus. The key-
words were annotated with a minimalist strategy.
In other words, the minimal unit that expresses
speculation was annotated as a keyword. A key-
word can be a single word (e.g. suggest, predict,
2
Available at: http://www.inf.u-szeged.hu/rgai/bioscope
might) or a phrase (complex keyword), if none of
the words constituting the phrase expresses a spec-
ulation by itself. For example the phrase ?no ev-
idence of?? in the sentence ?Direct sequencing of
the viral genomes and reinfection kinetics showed
no evidence of wild-type reversion even after pro-
longed infection with the Tat- virus.? is an example
of a complex keyword, since the words forming
the phrase can only express speculation together.
In contrast to the minimalist strategy followed
when annotating the keywords, the annotation of
scopes of the keywords was performed by assign-
ing the scope to the largest syntactic unit possible
by including all the elements between the keyword
and the target word to the scope (in order to avoid
scopes without a keyword) and by including the
modifiers of the target word to the scope (Vincze
et al, 2008). The reader can refer to (Vincze et al,
2008) for the details of the corpus and the annota-
tion guidelines.
The inter-annotator agreement rate was mea-
sured as the F-measure of the annotations of the
first annotator by considering the annotations of
the second one as the gold standard. The agree-
ment rate for speculation keyword annotation is
reported as 92.05% for the abstracts and 90.81%
for the full text articles and the agreement rate for
speculation scope resolution is reported as 94.04%
for the abstracts and 89.67% for the full text ar-
ticles (Vincze et al, 2008). These rates can be
considered as the upper bounds for the automated
methods proposed in this paper.
4 Identifying Speculation Keywords
Words and phrases such as ?might?, ?suggest?,
?likely?, ?no evidence of?, and ?remains to be
elucidated? that can render statements speculative
are called speculation keywords. Speculation key-
words are not always used in speculative context.
For instance, consider the following sentences:
1. Thus, it appears that the T-cell-specific activation of
the proenkephalin promoter is mediated by NF-kappa
B. (PMID: 91117203)
2. Differentiation assays using water soluble phorbol es-
ters reveal that differentiation becomes irreversible
soon after AP-1 appears. (PMID: 92088960)
The keyword ?appears? in the first sentence ren-
ders it speculative. However, in the second sen-
tence, ?appears? is not used in a speculative con-
text.
1400
The first sub-problem that we need to solve in
order to identify speculative sentence fragments is
identifying the real speculation keywords in a sen-
tence (i.e. the keywords which convey speculative
meaning in the sentence). We formulate the prob-
lem as a supervised classification task. We extract
the list of keywords from the training data which
has been labeled for speculation keywords. We
match this list of keywords in the unlabeled (test
data) and train a model to classify each occurrence
of a keyword in the unlabeled test set as a real
speculation keyword or not. The challenge of the
task can be demonstrated by the following statis-
tics from the Genia Abstracts of the BioScope cor-
pus. There are 1273 abstracts in the corpus. There
are 138 unique speculation keywords and the to-
tal number of their occurrence in the abstracts is
6125. In only 2694 (less than 50%) of their occur-
rences they are used in speculative context (i.e.,
are real speculation keywords).
In this study we focus on identifying the fea-
tures that represent the context of a speculation
keyword and use SVM with linear kernel (we
used the SVM
light
package (Joachims, 1999)) as
our classification algorithm. The following sub-
section describes the set of features that we pro-
pose.
4.1 Feature Extraction
We introduce a set of diverse types of features
including keyword specific features such as the
stem and the part-of-speech (POS) of the keyword,
and keyword context features such as the words
surrounding the keyword, the dependency rela-
tion types originating at the keyword, the other
keywords that occur in the same sentence as the
keyword, and positional features such as the sec-
tion of the paper in which the keyword occurs.
While designing the features, we were inspired by
studies on other natural language processing prob-
lems such as Word Sense Disambiguation (WSD)
and summarization. For example, machine learn-
ing methods with features based on part-of-speech
tags, word stems, surrounding and co-occurring
words, and dependency relationships have been
successfully used in WSD (Montoyo et al, 2005;
Ng and Lee, 1996; Dligach and Palmer, 2008) and
positional features such as the position of a sen-
tence in the document have been used in text sum-
marization (e.g. (Radev et al, 2004)).
4.1.1 Keyword Features
Statistics from the BioScope corpus suggest that
different keywords have different likelihoods of
being used in a speculative context (Vincze et al,
2008). For example, the keyword ?suggest? has
been used in a speculative context in all its oc-
currences in the abstracts and in the full papers.
On the other hand, ?appear? is a real specula-
tion keyword in 86% of its occurrences in the ab-
stracts and in 83% of its occurrences in the full
papers, whereas ?can? is a real speculation key-
word in 12% of its occurrences in the abstracts and
in 16% of its occurrences in the full papers. POS
of a keyword might also play a role in determining
whether it is a real speculation keyword or not. For
example, consider the keyword ?can?. It is more
likely to have been used in a speculative context
when it is a modal verb, than when it is a noun.
Based on these observations, we hypothesize that
features specific to a keyword such as the keyword
itself, the stem of the keyword, and the POS of
the keyword might be useful in discriminating the
speculative versus non-speculative use of it. We
use Porter?s Stemming Algorithm (Porter, 1980)
to obtain the stems of the keywords and Stanford
Parser (de Marneffe et al, 2006) to get the POS of
the keywords. If a keywords consists of multiple
words, we use the concatenation of the POS of the
words constituting the keyword as a feature. For
example, the extracted POS feature for the key-
words ?no evidence? and ?no proof? is ?DT.NN?
.
4.1.2 Dependency Relation Features
Besides the occurrence of a speculation keyword,
the syntactic structure of the sentence also plays
an important role in characterizing speculations.
Kilicoglu and Bergler (2008) showed that man-
ually identified syntactic patterns are effective in
classifying sentences as speculative or not. They
identified that, while some keywords do not in-
dicate hedging when used alone, they might act
as good indicators of hedging when used with a
clausal complement or with an infinitival clause.
For example, the ?appears? keyword in the ex-
ample sentences, which are given in the beginning
of Section 4, is not a real speculation keyword in
the second example ?...soon after AP-1 appears.?
, whereas it is a real speculation keyword in the
first example, where it is used with a that clausal
complement ?...it appears that...?. Similarly, ?ap-
pears? is used in a speculative context in the fol-
1401
lowing sentence, where it is used with an infini-
tival clause: ?Synergistic transactivation of the
BMRF1 promoter by the Z/c-myb combination ap-
pears to involve direct binding by the Z protein.?.
Another observation is that, some keywords
act as real speculation keywords only when used
with a negation. For example, words such as
?know?, ?evidence?, and ?proof? express cer-
tainty when used alone, but express a speculation
when used with a negation (e.g., ?not known?,
?no evidence?, ?no proof? ).
Auxiliaries in verbal elements might also give
clues for the speculative meaning of the main
verbs. Consider the example sentence: ?Our find-
ings may indicate the presence of a reactivated
virus hosted in these cells.?. The modal auxiliary
?may? acts as a clue for the speculative context of
the main verb ?indicate?.
We defined boolean features to represent the
syntactic structures of the contexts of the key-
words. We used the Stanford Dependency Parser
(de Marneffe et al, 2006) to parse the sentences
that contain a candidate speculation keyword and
extracted the following features from the depen-
dency parse trees.
Clausal Complement: A Boolean feature which is set to 1,
if the keyword has a child which is connected to it with
a clausal complement or infinitival clause dependency
type.
Negation: A Boolean feature which is set to 1, if the key-
word (1) has a child which is connected to it with a
negation dependency type (e.g. ?not known?: ?not? is
a child of ?known?, and the Stanford Dependency Type
connecting them is ?neg?) or (2) the determiner ?no? is
a child of the keyword (e.g., ?no evidence?: ?no? is a
child of ?evidence? and the Stanford Dependency Type
connecting them is ?det?).
Auxiliary: A Boolean feature which is set to 1, if the key-
word has a child which is connected to it with an auxil-
iary dependency type (e.g. ?may indicate?: ?may? is a
child of ?indicate?, and the Stanford Dependency Type
connecting them is ?aux?).
If a keyword consists of multiple-words, we ex-
amine the children of the word which is the an-
cestor of the other words constituting the key-
word. For example, ?no evidence? is a multi-word
keyword, where ?evidence? is the parent of ?no?.
Therefore, we extract the dependency parse tree
features for the word ?evidence?.
4.1.3 Surrounding Words
Recent studies showed that using machine learn-
ing with variants of the ?bag-of-words? feature
representation is effective in classifying sentences
as speculative vs. non-speculative (Light et al,
2004; Medlock and Briscoe, 2007; Szarvas, 2008).
Therefore, we also decided to include bag-of-
words features that represent the context of the
speculation keyword. We extracted the words sur-
rounding the keyword and performed experiments
both with and without stemming, and with win-
dow sizes of one, two, and three. Consider the
sentence: ?Our findings may indicate the presence
of a reactivated virus hosted in these cells.?. The
bag-of-words features for the keyword ?indicate?,
when a window size of three and no stemming is
used are: ?our?, ?findings?, ?may?, ?indicate?,
?the?, ?presence?, ?of?. In other words, the fea-
ture set consists of the keyword, the three words to
the left of the keyword, and the three words to the
right of the keyword.
4.1.4 Positional Features
Different parts of a scientific article might have
different characteristics in terms of the usage of
speculative language. For example, Hyland (1998)
analyzed a data set of molecular biology articles
and reported that the distribution of speculations
is similar between abstracts and full text articles,
whereas the Results and Discussion sections tend
to contain more speculative statements compared
to the other sections (e.g. Materials and Methods
or Introduction and Background sections). The
analysis of Light et al (2004) showed that the last
sentence of an abstract is more likely to be specu-
lative than non-speculative.
For the scientific abstracts data set, we defined
the following boolean features to represent the po-
sition of the sentence the keyword occurs in. Our
intuition is that titles and the first sentences in the
abstract tend to be non-speculative, whereas the
last sentence of the abstract tends to be specula-
tive.
Title: A Boolean feature which is set to 1, if the keyword
occurs in the title.
First Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the first sentence of the abstract.
Last Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the last sentence of the abstract.
For the scientific full text articles data set, we
defined the following features that represent the
position of the sentence in which the keyword oc-
curs. Our assumption is that the ?Results and Dis-
cussion? and the ?Conclusion? sections tend to
1402
contain more speculative statements than the ?Ma-
terials and Methods? and ?Introduction and Back-
ground? sections. We also assume that figure and
table legends are not likely to contain speculative
statements.
Title: A Boolean feature which is set to 1, if the keyword
occurs in the title of the article, or in the title of a sec-
tion or sub-section.
First Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the first sentence of the abstract.
Last Sentence: A Boolean feature which is set to 1, if the
keyword occurs in the last sentence of the abstract.
Background: A Boolean feature which is set to 1, if the
keyword occurs in the Background or Introduction sec-
tion.
Results: A Boolean feature which is set to 1, if the keyword
occurs in the Results or in the Discussion section.
Methods: A Boolean feature which is set to 1, if the key-
word occurs in the Materials and Methods section.
Conclusion: A Boolean feature which is set to 1, if the key-
word occurs in the Conclusion section.
Legend: A Boolean feature which is set to 1, if the keyword
occurs in a table or figure legend.
4.1.5 Co-occurring Keywords
Speculation keywords usually co-occur in the sen-
tences. Consider the sentence: ?We, therefore,
wished to determine whether T3SO4 could mimic
the action of thyroid hormone in vitro.?. Here,
?whether? and ?could? are speculation keywords
and their co-occurence might be a clue for their
speculative context. Therefore, we decided to in-
clude the co-occurring keywords to the feature set
of a keyword.
5 Resolving the Scope of a Speculation
After identifying the real speculation keywords,
the next step is determining their scopes in the sen-
tences, so that the speculative sentence fragments
can be detected. Manual analysis of sample sen-
tences from the BioScope corpus and their parse
trees suggests that the scope of a keyword can be
characterized by its part-of-speech and the syntac-
tic structure of the sentence in which it occurs.
Consider the example sentence whose parse tree
is shown in Figure 1. The sentence contains three
speculation keywords, ?or? and two occurrences
of ?might?. The scope of the conjunction ?or?, ex-
tends to the ?VP? whose children it coordinates.
In other words, the scope of ?or? is ?[might be
one of the earliest crucial steps in the lysis of nor-
mal and dex-resistant CEM cells, or might serve
as a marker for the process]?. Here, ?or? con-
veys a speculative meaning, since we are not cer-
tain which of the two sub-clauses (sub-clause 1:
[might be one of the earliest crucial steps in the
lysis of normal and dex-resistant CEM cells] or
sub-clause 2: [might serve as a marker for the pro-
cess]) is correct. The scope of both occurrences
of the modal verb ?might? is the parent ?VP?. In
other words, the scope of the first occurrence of
?might? is ?[might be one of the earliest crucial
steps in the lysis of normal and dex-resistant CEM
cells]? and the scope of the second occurrence of
?might? is ?[might serve as a marker for the pro-
cess]?. By examining the keywords, sample sen-
tences and their syntactic parse trees we devel-
oped the following rule-based approach to resolve
the scopes of speculation keywords. The exam-
ples given in this section are based on the syntactic
structure of the Penn Tree Bank. But, the rules are
generic (e.g. ?the scope of a verb followed by an
infinitival clause, extends to the whole sentence?).
The scope of a conjunction or a determiner (e.g.
or, and/or, vs) is the syntactic phrase to which it
is attached. For example, the scope of ?or? in
Figure 1 is the ?VP? immediately dominating the
?CC?.
The scope of a modal verb (e.g. may, might,
could) is the ?VP? to which it is attached. For
example, the scope of ?might? in Figure 1 is the
?VP? immediately dominating the ?MD?.
The scope of an adjective or an adverb starts
with the keyword and ends with the last token of
the highest level ?NP? which dominates the ad-
jective or the adverb. Consider the sentence ?The
endocrine events that are rapidly expressed (sec-
onds) are due to a [possible interaction with cellu-
lar membrane].? The scope of the speculation key-
word ?possible? is enclosed in rectangular brack-
ets. The sub-tree that this scope maps to is: ?(NP
(NP (DT a) (JJ possible) (NN interaction)) (PP
(IN with) (NP (JJ cellular) (NN membrane))))?.
If there does not exist a ?NP? dominating the ad-
verb or adjective keyword, the scope extends to
the whole sentence. For example the scope of
the speculation adverb ?probably? in the sentence
?[The remaining portion of the ZFB motif was
probably lost in TPases of insect Transib trans-
posons]? is the whole sentence.
The scope of a verb followed by an infinitival
1403
Figure 1: The syntactic parse tree of the sentence ?Positive induction of GR mRNA might be one of the earliest crucial steps
in the lysis of normal and dex-resistant CEM cells, or might serve as a marker for the process.?
clause extends to the whole sentence. For exam-
ple, the scope of the verb ?appears? followed by
the ?to? infinitival clause is the whole sentence in
?[The block of pupariation appears to involve sig-
naling through the adenosine receptor (AdoR)]?.
The scope of a verb in passive voice extends
to the whole sentence such as the scope of ?sug-
gested? in ?[The existence of such an indepen-
dent mechanism has also been suggested in mam-
mals]?.
If none of the above rules apply, the scope of a
keyword starts with the keyword and ends at the
end of the sentence (or clause). An example is
the scope of ?suggested? in ?This [suggested that
there is insufficient data currently available to de-
termine a reliable ratio for human]?.
6 Evaluation
We evaluated our approach on two different types
of scientific text from the biomedical domain,
namely the scientific abstracts sub-corpus and the
full text articles sub-corpus of the BioScope cor-
pus (see Section 3). We used stratified 10-fold
cross-validation to evaluate the performance on
the abstracts. In each fold, 90% of the abstracts are
used for training and 10% are used to test. To facil-
itate comparison with future studies the PubMed
Identifiers of the abstracts that we used as a test
set in each fold are provided
3
. The full text pa-
pers sub-corpus consists of nine articles. We used
leave-one-out cross-validation to evaluate the per-
3
http://belobog.si.umich.edu/clair/bioscope/
formance on the full text papers. In each iteration
eight articles are used for training and one article
is used to test. We report the average results over
the runs for each data set.
6.1 Evaluation of Identifying Speculation
Keywords
To classify whether the occurrence of a keyword is
in speculative context or not, we built linear SVM
models by using various combinations of the fea-
tures introduced in Section 4.1. Tables 2 and 3
summarize the results obtained for the abstracts
and the full text papers, respectively. BOW N is
the bag-of-words features obtained from the words
surrounding the keyword (see Section 4.1.3). N is
the window size. We experimented both with the
stemmed and non-stemmed versions of this fea-
ture type. The non-stemmed versions performed
slightly better than the stemmed versions. The rea-
son might be due to the different likelihoods of
being used in a speculative context of different in-
flected forms of words. For example, consider the
words ?appears? and ?appearance?. They have the
same stems, but ?appearance? is less likely to be a
real speculation keyword than ?appears?. Another
observation is that, decreasing the window size
led to improvement in performance. This suggests
that the words right before and right after the can-
didate speculation keyword are more effective in
distinguishing its speculative vs. non-speculative
context compared to a wider local context. Wider
local context might create sparse data and degrade
1404
performance. Consider the example, ?it appears
that TP53 interacts with AR?. The keyword ?ap-
pears?, and BOW1 (?it? and ?that?) are more rel-
evant for the speculative context of the keyword
than ?TP53?, ?interacts?, and ?with?. Therefore,
for the rest of the experiments we used the BOW
1 version, i.e., the non-stemmed surrounding bag-
of-words with window size of 1. KW stands for
the keyword specific features, i.e., the keyword, its
stem, and its part-of-speech (discussed in Section
4.1.1). DEP stands for the dependency relation
features (discussed in Section 4.1.2). POS stands
for the positional features (discussed in Section
4.1.4) and CO-KW stands for the co-occurring
keywords feature (discussed in Section 4.1.5).
Our results are not directly comparable with
the prior studies about identifying speculative sen-
tences (see Section 2), since we attempted to solve
a different problem, which is identifying specula-
tive parts of sentences. Only the substring match-
ing approach that was introduced in (Light et al,
2004) could be adapted as a keyword classification
task, since the substrings are keywords themselves
and we used this approach as a baseline in the
keyword classification sub-problem. We compare
the performances of our models with two baseline
methods, which are based on the substring match-
ing approach. Light et al (2004) have shown that
the substring matching method with a predefined
set of 14 strings performs slightly better than an
SVM model with bag-of-words features in classi-
fying sentences as speculative vs. non-speculative
(see Section 2). In baseline 1, we use the 14 strings
identified in (Light et al, 2004) and classify all the
keywords in the test set that match any of them as
real speculation keywords. Baseline 2 is similar
to baseline 1, with the difference that rather than
using the set of strings in (Light et al, 2004), we
extract the set of keywords from the training set
and classify all the words (or phrases) in the test
set that match any of the keywords in the list as
real speculation keywords.
Baseline 1 achieves high precision, but low re-
call. Whereas, baseline 2 achieves high recall in
the expense of low precision. All the SVM mod-
els in Tables 2 and 3 achieve more balanced preci-
sion and recall values, with F-measure values sig-
nificantly higher than the baseline methods. We
start with a model that uses only the keyword-
specific features (KW). This type of feature alone
achieved a significantly better performance than
the baseline methods (90.61% F-measure for the
abstracts and 80.57% F-measure for the full text
papers), suggesting that the keyword-specific fea-
tures are important in determining its specula-
tive context. We extended the feature set by in-
cluding the dependency relation (DEP), surround-
ing words (BOW 1), positional (POS), and co-
occurring keywords (CO-KW) features. Each new
type of included feature improved the performance
of the model for the abstracts. The best F-measure
(91.69%) is achieved by using all the proposed
types of features. This performance is close to the
upper bound, which is the human inter-annotator
agreement F-measure of 92.05%.
Including the co-occurring keywords to the fea-
ture set for full text articles slightly improved pre-
cision, but deceased recall, which led to lower F-
measure. The best F-measure (82.82%) for the
full text articles is achieved by using all the fea-
ture types except the co-occurring keywords. The
achieved performance is significantly higher than
the baseline methods, but lower than the human
inter-annotator agreement F-measure of 90.81%.
The lower performance for the full text papers
might be due to the small size of the data set (9
full text papers compared to 1273 abstracts).
Method Recall Precision F-Measure
Baseline 1 52.84 92.71 67.25
Baseline 2 97.54 43.66 60.30
BOW 3 - stemmed 81.47 92.36 86.51
BOW 2 - stemmed 81.56 93.29 86.97
BOW 1 - stemmed 83.08 93.83 88.05
BOW 3 82.58 92.04 86.98
BOW 2 82.77 92.74 87.41
BOW 1 83.27 93.67 88.10
KW: kw, kw-stem, kw-pos 88.62 92.77 90.61
KW, DEP 88.77 92.67 90.64
KW, DEP, BOW 1 88.46 94.71 91.43
KW, DEP, BOW 1, POS 88.16 95.21 91.50
KW, DEP, BOW 1, POS, CO-KW 88.22 95.56 91.69
Table 2: Results for the Scientific Abstracts
Method Recall Precision F-Measure
Baseline 1 33.77 86.75 47.13
Baseline 2 88.22 52.57 64.70
BOW 3 - stemmed 70.79 83.88 76.58
BOW 2 - stemmed 72.31 85.49 78.11
BOW 1 - stemmed 73.49 84.35 78.41
BOW 3 70.54 82.56 75.88
BOW 2 71.52 85.93 77.94
BOW 1 73.72 86.27 79.43
KW: kw, kw-stem, kw-pos 75.21 87.08 80.57
KW, DEP 75.02 89.49 81.53
KW, DEP, BOW 1 76.15 89.54 82.27
KW, DEP, BOW 1, POS 76.17 90.81 82.82
KW, DEP, BOW 1, POS, CO-KW 75.76 90.82 82.58
Table 3: Results for the Scientific Full Text Papers
1405
6.2 Evaluation of Resolving the Scope of a
Speculation
We compared the proposed rule-based approach
for scope resolution with two baseline methods.
Previous studies classify sentences as speculative
or not, therefore implicitly assigning the scope of
a speculation to the whole sentence (Light et al,
2004; Medlock and Briscoe, 2007; Szarvas, 2008;
Kilicoglu and Bergler, 2008). Baseline 1 follows
this approach and assigns the scope of a specu-
lation keyword to the whole sentence. Szarvas
(2008) suggest assigning the scope of a keyword
from its occurrence to the end of the sentence.
They state that this approach works accurately for
clinical free texts, but no any results are reported
(Szarvas, 2008). Baseline 2 follows the approach
proposed in (Szarvas, 2008) and assigns the scope
of a keyword to the fragment of the sentence that
starts with the keyword and ends at the end of the
sentence. Table 4 summarizes the accuracy results
obtained for the abstracts and the full text papers.
The poor performance of baseline 1, empha-
sizes the importance of detecting the portions of
sentences that are speculative, since less than 5%
of the sentences that contain speculation keywords
are entirely speculative. Classifying the entire sen-
tences as speculative or not leads to loss in infor-
mation for more than 95% of the sentences. The
rule-based method significantly outperformed the
two baseline methods, indicating that the part-of-
speech of the keywords and the syntactic parses
of the sentences are effective in characterizing the
speculation scopes.
Method Accuracy-Abstracts Accuracy-Full text
Baseline 1 4.82 4.29
Baseline 2 67.60 42.82
Rule-based method 79.89 61.13
Table 4: Scope resolution results
7 Conclusion
We presented an approach to identify speculative
sentence fragments in scientific articles. Our ap-
proach is based on solving two sub-problems. The
first one is identifying the keywords which are
used in speculative context and the second one is
determining the scopes of these keywords in the
sentences. We evaluated our approach for two
types of scientific texts, namely abstracts and full
text papers from the BioScope corpus.
We formulated the first sub-problem as a super-
vised classification task, where the aim is to learn
models to classify the candidate speculation key-
words as real speculation keywords or not. We fo-
cused on identifying different types of linguistic
features that capture the contexts of the keywords.
We achieved a performance which is significantly
better than the baseline methods and comparable
to the upper-bound, which is the human inter-
annotator agreement F-measure.
We hypothesized that the scope of a specula-
tion keyword can be characterized by its part-of-
speech and the syntactic structure of the sentence
and developed rules to map the scope of a key-
word to the nodes in the syntactic parse tree. We
achieved a significantly better performance com-
pared to the baseline methods. The considerably
lower performance of the baseline of assigning the
scope of a speculation keyword to the whole sen-
tence indicates the importance of detecting specu-
lative sentence portions rather than classifying the
entire sentences as speculative or not.
Acknowledgements
This work was supported in part by the NIH Grant
U54 DA021519 to the National Center for Integra-
tive Biomedical Informatics.
References
Nigel Collier, Hyun S. Park, Norihiro Ogata, Yuka
Tateishi, Chikashi Nobata, Tomoko Ohta, Tateshi
Sekimizu, Hisao Imai, Katsutoshi Ibushi, and Jun I.
Tsujii. 1999. The GENIA project: corpus-based
knowledge acquisition and information extraction
from genome research papers. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, pages 271?
272. Association for Computational Linguistics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of LREC-06.
Dmitriy Dligach and Martha Palmer. 2008. Novel Se-
mantic Features for Verb Sense Disambiguation. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
Ken Hyland. 1998. Hedging in Scientific Research
Articles. John Benjamins Publishing Co.
1406
T. Joachims, 1999. Advances in Kernel Methods-
Support Vector Learning, chapter Making Large-
Scale SVM Learning Practical. MIT-Press.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9(Suppl 11).
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17?24, Boston, Massachusetts, USA, May 6.
Association for Computational Linguistics.
A. T. McCray, S. Srinivasan, and A. C. Browne. 1994.
Lexical methods for managing variation in biomed-
ical terminologies. Proc Annu Symp Comput Appl
Med Care, pages 235?239.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992?999, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Andres Montoyo, Armando Suarez, German Rigau,
and Manuel Palomar. 2005. Combining knowledge-
and corpus-based word-sense-disambiguation meth-
ods. Journal of Artificial Intelligence Research,
23:299?330.
H. T. Ng and H. B Lee. 1996. Integrating multi-
ple knowledge sources to disambiguate word senses:
An exemplar-based approach. In Proceedings of the
34th Annual Meeting of the Association for Compu-
tational Linguistics.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137.
Dragomir R. Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Adam Winkel, and Zhang
Zhu. 2004. Mead - a platform for multidocument
multilingual text summarization. In Proceedings of
LREC 2004.
Gyorgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In ACL 2008.
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The BioScope
corpus: biomedical texts annotated for uncertainty,
negation and their scopes. BMC Bioinformatics,
9(Suppl 11).
1407
Answering What-Is Questions by Virtual Annotation 
 
John Prager  
IBM T.J. Watson Research Center 
Yorktown Heights, N.Y. 10598 
(914) 784-6809 
jprager@us.ibm.com 
Dragomir Radev 
University of Michigan 
Ann Arbor, MI 48109 
(734) 615-5225 
radev@umich.edu 
Krzysztof Czuba  
Carnegie-Mellon University 
Pittsburgh, PA 15213 
(412) 268 6521 
kczuba@cs.cmu.edu 
 
 
ABSTRACT 
We present the technique of Virtual Annotation as a specialization 
of Predictive Annotation for answering definitional What is 
questions.  These questions generally have the property that the 
type of the answer is not given away by the question, which poses 
problems for a system which has to select answer strings from 
suggested passages.  Virtual Annotation uses a combination of 
knowledge-based techniques using an ontology, and statistical 
techniques using a large corpus to achieve high precision. 
 
Keywords 
Question-Answering, Information Retrieval, Ontologies 
 
1. INTRODUCTION 
Question Answering is gaining increased attention in both the 
commercial and academic arenas.  While algorithms for general 
question answering have already been proposed, we find that such 
algorithms fail to capture certain subtleties of particular types of 
questions.  We propose an approach in which different types of 
questions are processed using different algorithms.  We introduce a 
technique named Virtual Annotation (VA) for answering one such 
type of question, namely the What is question. 
 
We have previously presented the technique of Predictive 
Annotation (PA) [Prager, 2000], which has proven to be an 
effective approach to the problem of Question Answering.  The 
essence of PA is to index the semantic types of all entities in the 
corpus, identify the desired answer type from the question, search 
for passages that contain entities with the desired answer type as 
well as the other query terms, and to extract the answer term or 
phrase.  One of the weaknesses of PA, though, has been in dealing 
with questions for which the system cannot determine the correct 
answer type required.  We introduce here an extension to PA 
which we call Virtual Annotation and show it to be effective for 
those ?What is/are (a/an) X? questions that are seeking hypernyms 
of X.  These are a type of definition question, which other QA 
systems attempt to answer by searching in the document collection 
for textual clues similar to those proposed by [Hearst, 1998], that 
are characteristic of definitions.  Such an approach does not use 
the strengths of PA and is not successful in the cases in which a 
deeper understanding of the text is needed in order to identify 
the defining term in question. 
 
We first give a brief description of PA.  We look at a certain 
class of What is questions and describe our basic algorithm.  
Using this algorithm we develop the Virtual Annotation 
technique, and evaluate its performance with respect to both the 
standard TREC and our own benchmark.  We demonstrate on 
two question sets that the precision improves from .15 and .33 to 
.78 and .83 with the addition of VA. 
2.  BACKGROUND 
For our purposes, a question-answering (QA) system is one 
which takes a well-formed user question and returns an 
appropriate answer phrase found in a body of text.  This 
generally excludes How and Why questions from consideration, 
except in the relatively rare cases when they can be answered by 
simple phrases, such as ?by fermenting grapes? or ?because of 
the scattering of light?.  In general, the response of a QA system 
will be a named entity such as a person, place, time, numerical 
measure or a noun phrase, optionally within the context of a 
sentence or short paragraph.  
 
The core of most QA systems participating in TREC [TREC8, 
2000 & TREC9, 2001] is the identification of the answer type 
desired by analyzing the question.  For example, Who questions 
seek people or organizations, Where questions seek places, 
When questions seek times, and so on.  The goal, then, is to find 
an entity of the right type in the text corpus in a context that 
justifies it as the answer to the question.  To achieve this goal, 
we have been using the technique of PA to annotate the text 
corpus with semantic categories (QA-Tokens) prior to indexing. 
 
Each QA-Token is identified by a set of terms, patterns, or 
finite-state machines defining matching text sequences.  Thus 
?Shakespeare? is annotated with ?PERSON$?, and the text 
string ?PERSON$? is indexed at the same text location as 
?Shakespeare?.  Similarly, ?$123.45? is annotated with 
?MONEY$?.  When a question is processed, the desired QA-
Token is identified and it replaces the Wh-words and their 
auxiliaries.  Thus, ?Who? is replaced by ?PERSON$?, and 
?How much? + ?cost? are replaced by ?MONEY$?.  The 
resulting query is then input to the search engine as a bag of 
words.  The expectation here is that if the initial question were 
?Who wrote Hamlet?, for example, then the modified query of 
?PERSON$ write Hamlet? (after lemmatization) would be a 
 
 
 
 perfect match to text that states ?Shakespeare wrote Hamlet? or 
?Hamlet was written by Shakespeare?. 
 
The modified query is matched by the search engine against 
passages of 1-2 sentences, rather than documents.  The top 10 
passages returned are processed by our Answer Selection module 
which re-annotates the text, identifies all potential answer phrases, 
ranks them using a learned evaluation function and selects the top 
5 answers (see [Radev et al, 2000]). 
 
The problem with ?What is/are (a/an) X? questions is that the 
question usually does not betray the desired answer type.  All the 
system can deduce is that it must find a noun phrase (the QA-
Token THING$).  The trouble with THING$ is that it is too 
general and labels a large percentage of the nouns in the corpus, 
and so does not help much in narrowing down the possibilities.  A 
second problem is that for many such questions the desired answer 
type is not one of the approximately 50 high-level classes (i.e. QA-
Tokens) that we can anticipate at indexing; this phenomenon is 
seen in TREC9, whose 24 definitional What is questions are listed 
in the Appendix.  These all appear to be calling out for a 
hypernym.  To handle such questions we developed the technique 
of Virtual Annotation which is like PA and shares much of the 
same machinery, but does not rely on the appropriate class being 
known at indexing time.  We will illustrate with examples from the 
animal kingdom, including a few from TREC9. 
 
3.  VIRTUAL ANNOTATION 
If we look up a word in a thesaurus such as WordNet [Miller et al, 
1993]), we can discover its hypernym tree, but there is no 
indication which hypernym is the most appropriate to answer a 
What is question.  For example, the hypernym hierarchy for 
?nematode? is shown in Table 1.  The level numbering counts 
levels up from the starting term.  The numbers in parentheses will 
be explained later. 
 
Table 1.  Parentage of ?nematode? according to WordNet. 
 
Level Synset 
0 {nematode, roundworm} 
1 {worm(13)} 
2 {invertebrate} 
3 {animal(2), animate being, beast, brute, creature, 
fauna} 
4 {life form(2), organism(3), being, living thing} 
5 {entity, something} 
 
 
At first sight, the desirability of the hypernyms seems to decrease 
with increasing level number.  However, if we examine ?meerkat? 
we find the hierarchy in Table 2. 
 
We are leaving much unsaid here about the context of the question 
and what is known of the questioner, but it is not unreasonable to 
assert that the ?best? answer to ?What is a meerkat? is either ?a 
mammal? (level 4) or ?an animal? (level 7).  How do we get an 
automatic system to pick the right candidate? 
 
 
 
Table 2.  Parentage of ?meerkat? according to WordNet 
 
Level Synset 
0 {meerkat, mierkat} 
1 {viverrine, viverrine mammal} 
2 {carnivore} 
3 {placental, placental mammal, eutherian, eutherian 
mammal} 
4 {mammal} 
5 {vertebrate, craniate} 
6 {chordate} 
7 {animal(2), animate being, beast, brute, creature, 
fauna} 
8 {life form, organism, being, living thing} 
9 {entity, something} 
 
 
It seems very much that what we would choose intuitively as the 
best answer corresponds to Rosch et al?s basic categories 
[Rosch et al, 1976].  According to psychological testing, these 
are categorization levels of intermediate specificity that people 
tend to use in unconstrained settings.  If that is indeed true, then 
we can use online text as a source of evidence for this tendency.  
For example, we might find sentences such as ??  meerkats and 
other Y ? ?, where Y is one of its hypernyms, indicating that Y 
is in some sense the preferred descriptor. 
 
We count the co-occurrences of the target search term (e.g. 
?meerkat? or ?nematode?) with each of its hypernyms (e.g. 
?animal?) in 2-sentence passages, in the TREC9 corpus.  These 
counts are the parenthetical numbers in Tables 1 and 2.  The 
absence of a numerical label there indicates zero co-occurrences.  
Intuitively, the larger the count, the better the corresponding 
term is as a descriptor. 
 
3.1  Hypernym Scoring and Selection  
Since our ultimate goal is to find passages describing the target 
term, discovering zero co-occurrences allows elimination of 
useless candidates.  Of those remaining, we are drawn to those 
with the highest counts, but we would like to bias our system 
away from the higher levels.  Calling a nematode a life-form is 
correct, but hardly helpful.   
 
The top levels of WordNet (or any ontology) are by definition 
very general, and therefore are unlikely to be of much use for 
purposes of definition.  However, if none of the immediate 
parents of a term we are looking up co-occur in our text corpus, 
we clearly will be forced to use a more general term that does.  
We want to go further, though, in those cases where the 
immediate parents do occur, but in small numbers, and the very 
general parents occur with such high frequencies that our 
algorithm would select them.  In those cases we introduce a 
tentative level ceiling to prevent higher-level terms from being 
chosen if there are suitable lower-level alternatives.   
 
We would like to use a weighting function that decreases 
monotonically with level distance.  Mihalcea and  Moldovan 
[1999], in an analogous context, use the logarithm of the number 
of terms in a given term?s subtree to calculate weights, and they 
claim to have shown that this function is optimal.  Since it is 
approximately true that the level population increases 
 exponentially in an ontology, this suggests that a linear function of 
level number will perform just as well. 
 
Our first step is to generate a level-adapted count (LAC) by 
dividing the co-occurrence counts by the level number (we are 
only interested in levels 1 and greater).  We then select the best 
hypernym(s) by using a fuzzy maximum calculation.  We locate 
the one or more hypernyms with greatest LAC, and then also select 
any others with a LAC within a predefined threshold of it; in our 
experimentation we have found that a threshold value of 20% 
works well.  Thus if, for example, a term has one hypernym at 
level 1 with a count of 30, and another at level 2 with a count of 
50, and all other entries have much smaller counts, then since the 
LAC 25 is within 20% of the LAC 30, both of these hypernyms 
will be proposed.   
 
To prevent the highest levels from being selected if there is any 
alternative, we tentatively exclude them from consideration 
according to the following scheme: 
 
If the top of the tree is at level N, where N <= 3, we set a tentative 
ceiling at N-1, otherwise if N<=5, we set the ceiling at N-2, 
otherwise we set the ceiling at N-3.  If no co-occurrences are found 
at or below this ceiling, then it is raised until a positive value is 
found, and the corresponding term is selected.  
 
If no hypernym at all co-occurs with the target term, then this 
approach is abandoned:  the ?What? in the question is replaced by 
?THING$? and normal procedures of Predictive Annotation are 
followed. 
 
When successful, the algorithm described above discovers one or 
more candidate hypernyms that are known to co-occur with the 
target term.  There is a question, though, of what to do when the 
question term has more than one sense, and hence more than one 
ancestral line in WordNet.  We face a choice of either selecting the 
hypernym(s) with the highest overall score as calculated by the 
algorithm described above, or collecting together the best 
hypernyms in each parental branch.  After some experimentation 
we made the latter choice.  One of the questions that benefitted 
from this was ?What is sake?.  WordNet has three senses for sake: 
good (in the sense of welfare), wine (the Japanese drink) and 
aim/end, with computed scores of 122, 29 and 87/99 respectively.  
It seems likely (from the phrasing of the question) that the ?wine? 
sense is the desired one, but this would be missed entirely if only 
the top-scoring hypernyms were chosen. 
 
We now describe how we arrange for our Predictive Annotation 
system to find these answers.  We do this by using these 
descriptors as virtual QA-Tokens; they are not part of the search 
engine index, but are tagged in the passages that the search engine 
returns at run time. 
 
3.2 Integration 
Let us use H to represent either the single hypernym or a 
disjunction of the several hypernyms found through the WordNet 
analysis.  The original question Q =  
?What is (a/an) X? 
is converted to Q? =  
?DEFINE$ X H? 
where DEFINE$ is a virtual QA-Token that was never seen at 
indexing time, does not annotate any text and does not occur in the 
index.  The processed query Q? then will find passages that 
contain occurrences of both X and H; the token DEFINE$ will 
be ignored by the search engine.  The top passages returned by 
the search engine are then passed to Answer Selection, which re-
annotates the text.  However, this time the virtual QA-Token 
DEFINE$ is introduced and the patterns it matches are defined 
to be the disjuncts in H.  In this way, all occurrences of the 
proposed hypernyms of X in the search engine passages are 
found, and are scored and ranked in the regular fashion.  The 
end result is that the top passages contain the target term and one 
of its most frequently co-occurring hypernyms in close 
proximity, and these hypernyms are selected as answers. 
 
When we use this technique of Virtual Annotation on the 
aforementioned questions, we get answer passages such as 
  
?Such genes have been found in nematode worms 
but not yet in higher animals.? 
and 
?South African golfer Butch Kruger had a good 
round going in the central Orange Free State trials, 
until a mongoose-like animal grabbed his ball with 
its mouth and dropped down its hole. Kruger wrote 
on his card: "Meerkat."? 
 
4 RESULTS 
4.1 Evaluation 
We evaluated Virtual Annotation on two sets of questions ? the 
definitional questions from TREC9 and similar kinds of 
questions from the Excite query log (see 
http://www.excite.com).  In both cases we were looking for 
definitional text in the TREC corpus.  The TREC questions had 
been previously verified (by NIST) to have answers there; the 
Excite questions had no such guarantee.   We started with 174 
Excite questions of the form ?What is X?, where X was a 1- or 
2-word phrase.  We removed those questions that we felt would 
not have been acceptable as TREC9 questions.  These were 
questions where: 
o The query terms did not appear in the TREC corpus, 
and some may not even have been real words (e.g. 
?What is a gigapop?).1  37 questions. 
o The query terms were in the corpus, but there was no 
definition present (e.g ?What is a computer 
monitor?).2  18 questions. 
o The question was not asking about the class of the 
term but how to distinguish it from other members of 
the class (e.g. ?What is a star fruit?).  17 questions. 
o The question was about computer technology that 
emerged after the articles in the TREC corpus were 
written (e.g. ?What is a pci slot?).  19 questions. 
o The question was very likely seeking an example, not 
a definition (e.g. ?What is a powerful adhesive?).  1 
question plus maybe some others ? see the Discussion 
                                               
1 That is, after automatic spelling correction was attempted.   
2 The TREC10 evaluation in August 2001 is expected to contain 
questions for which there is no answer in the corpus 
(deliberately).   While it is important for a system to be able to 
make this distinction, we kept within the TREC9 framework for 
this evaluation. 
 section later.  How to automatically distinguish these 
cases is a matter for further research. 
 
Of the remaining 82 Excite questions, 13 did not have entries in 
WordNet.  We did not disqualify those questions. 
 
For both the TREC and Excite question sets we report two 
evaluation measures.  In the TREC QA track, 5 answers are 
submitted per question, and the score for the question is the 
reciprocal of the rank of the first correct answer in these 5 
candidates, or 0 if the correct answer is not present at all.  A 
submission?s overall score is the mean reciprocal rank (MRR) over 
all questions.  We calculate MRR as well as mean binary score 
(MBS) over the top 5 candidates; the binary score for a question is 
1 if a correct answer was present in the top 5 candidates, 0 
otherwise.  The first sets of MBS and MRR figures are for our base 
system, the second set the system with VA. 
 
Table 3.  Comparison of base system and system with VA on 
both TREC9 and Excite definitional questions. 
 
Source No. of 
Questions 
MBS 
w/o 
VA 
MRR 
w/o 
VA 
MBS 
with 
VA 
MRR 
with 
VA 
TREC9  
(in WN) 
20 .3 .2 .9 .9 
TREC9  
(not in WN) 
4 .5 .375 .5 .5 
TREC9 
Overall 
24 .333 .229 .833 .833 
Excite 
(in WN) 
69 .101 .085 .855 
 
.824 
Excite  
(not in WN) 
13 .384 .295 .384 .295 
Excite 
Overall 
82 .146 .118 .780 .740 
 
 
We see that for the 24 TREC9 definitional questions, our MRR 
score with VA was the same as the MBS score.  This was because 
for each of the 20 questions where the system found a correct 
answer, it was in the top position. 
 
By comparison, our base system achieved an overall MRR score of 
.315 across the 693 questions of TREC9.  Thus we see that with 
VA, the average score of definitional questions improves from 
below our TREC average to considerably higher.  While the 
percentage of definitional questions in TREC9 was quite small, we 
shall explain in a later section how we plan to extend our 
techniques to other question types. 
 
4.2  Errors 
The VA process is not flawless, for a variety of reasons.  One is 
that the hierarchy in WordNet does not always exactly correspond 
to the way people classify the world.  For example, in WordNet a 
dog is not a pet, so ?pet? will never even be a candidate answer to 
?What is a dog?. 
 
When the question term is in WordNet, VA succeeds most of the 
time.  One of the error sources is due to the lack of uniformity of 
the semantic distance between levels.  For example, the parents 
of ?architect? are ?creator? and ?human?, the latter being what 
our system answers to ?What is an architect?.  This is 
technically correct, but not very useful.   
 
Another error source is polysemy.  This does not seem to cause 
problems with VA very often ? indeed the co-occurrence 
calculations that we perform are similar to those done by 
[Mihalcea and Moldovan, 1999] to perform word sense 
disambiguation ? but it can give rise to amusing results.  For 
example, when asked ?What is an ass? the system responded 
with ?Congress?.  Ass has four senses, the last of which in 
WordNet is a slang term for sex.  The parent synset contains the 
archaic synonym congress (uncapitalized!).  In the TREC corpus 
there are several passages containing the words ass and 
Congress, which lead to congress being the hypernym with the 
greatest score.  Clearly this particular problem can be avoided 
by using orthography to indicate word-sense, but the general 
problem remains.  
 
5  DISCUSSION AND FURTHER WORK 
5.1  Discussion 
While we chose not to use Hearst?s approach of key-phrase 
identification as the primary mechanism for answering What is 
questions, we don?t reject the utility of the approach.  Indeed, a 
combination of VA as described here with a key-phrase analysis 
to further filter candidate answer passages might well reduce the 
incidence of errors such as the one with ass mentioned in the 
previous section.  Such an investigation remains to be done. 
 
We have seen that VA gives very high performance scores at 
answering What is questions ? and we suggest it can be 
extended to other types ? but we have not fully addressed the 
issue of automatically selecting the questions to which to apply 
it.  We have used the heuristic of only looking at questions of 
the form ?What is (a/an) X? where X is a phrase of one or two 
words.  By inspection of the Excite questions, almost all of those 
that pass this test are looking for definitions, but some - such as 
?What is a powerful adhesive? - very probably do not.  There 
are also a few questions that are inherently ambiguous 
(understanding that the questioners are not all perfect 
grammarians):  is ?What is an antacid? asking for a definition or 
a brand name?  Even if it is known or assumed that a definition 
is required, there remains the ambiguity of the state of 
knowledge of the questioner.  If the person has no clue what the 
term means, then a parent class, which is what VA finds, is the 
right answer.  If the person knows the class but needs to know 
how to distinguish the object from others in the class, for 
example ?What is a star fruit?, then a very different approach is 
required.  If the question seems very specific, but uses common 
words entirely, such as the Excite question ?What is a yellow 
spotted lizard?, then the only reasonable interpretation seems to 
be a request for a subclass of the head noun that has the given 
property.  Finally, questions such as ?What is a nanometer? and 
?What is rubella? are looking for a value or more common 
synonym.   
 
 5.2 Other Question Types 
The preceding discussion has centered upon What is questions and 
the use of WordNet, but the same principles can be applied to other 
question types and other ontologies.  Consider the question ?Where 
is Chicago?, from the training set NIST supplied for TREC8.  Let 
us assume we can use statistical arguments to decide that, in a 
vanilla context, the question is about the city as opposed to the 
rock group, any of the city?s sports teams or the University.  There 
is still considerable ambiguity regarding the granularity of the 
desired answer.  Is it:  Cook County?  Illinois?  The Mid-West?  
The United States?  North America?  The Western Hemisphere? ?  
 
There are a number of geographical databases available, which 
either alone or with some data massaging can be viewed as 
ontologies with ?located within? as the primary relationship.  Then 
by applying Virtual Annotation to Where questions we can find 
the enclosing region that is most commonly referred to in the 
context of the question term.  By manually applying our algorithm 
to ?Chicago? and the list of geographic regions in the previous 
paragraph we find that ?Illinois? wins, as expected, just beating out 
?The United States?.  However, it should be mentioned that a more 
extensive investigation might find a different weighting scheme 
more appropriate for geographic hierarchies. 
 
The aforementioned answer of ?Illinois? to the question ?Where is 
Chicago?? might be the best answer for an American user, but for 
anyone else, an answer providing the country might be preferred.  
How can we expect Virtual Annotation to take this into account?  
The ?hidden variable? in the operation of VA is the corpus.  It is 
assumed that the user belongs to the intended readership of the 
articles in the corpus, and to the extent that this is true, the results 
of VA will be useful to the user.    
 
Virtual Annotation can also be used to answer questions that are 
seeking examples or instances of a class.  We can use WordNet 
again, but this time look to hyponyms.  These questions are more 
varied in syntax than the What is kind;  they include, for example 
from TREC9 again: 
?Name a flying mammal.? 
?What flower did Vincent Van Gogh paint?? 
and 
?What type of bridge is the Golden Gate Bridge?? 
 
6.  SUMMARY 
We presented Virtual Annotation, a technique to extend the 
capabilities of PA to a class of definition questions in which the 
answer type is not easily identifiable.  Moreover, VA can find text 
snippets that do not contain the regular textual clues for presence 
of definitions.  We have shown that VA can considerably improve 
the performance of answering What is questions, and we indicate 
how other kinds of questions can be tackled by similar techniques. 
 
7.  REFERENCES 
[1] Hearst, M.A. ?Automated Discovery of WordNet Relations? 
in WordNet: an Electronic Lexical Database, Christiane 
Fellbaum Ed, MIT Press, Cambridge MA, 1998. 
[2] Mihalcea, R. and Moldovan, D. ?A Method for Word Sense 
Disambiguation of Unrestricted Text?.  Proceedings of the 
37th Annual Meeting of the Association for Computational 
Linguistics (ACL-99), pp. 152-158, College Park, MD, 1999. 
[3] Miller, G. ?WordNet: A Lexical Database for English?, 
Communications of the ACM 38(11) pp. 39-41, 1995. 
[4] Moldovan, D.I. and Mihalcea, R. ?Using WordNet and 
Lexical Operators to Improve Internet Searches?, IEEE 
Internet Computing, pp. 34-43, Jan-Feb 2000.  
[5] Prager, J.M., Radev, D.R., Brown, E.W. and Coden, A.R. 
?The Use of Predictive Annotation for Question-Answering 
in TREC8?, Proceedings of TREC8, Gaithersburg, MD, 
2000. 
[6] Prager, J.M., Brown, E.W., Coden, A.R., and Radev, D.R. 
"Question-Answering by Predictive Annotation", 
Proceedings of SIGIR 2000, pp. 184-191, Athens, Greece, 
2000. 
[7] Radev, D.R., Prager, J.M. and Samn, V. ?Ranking 
Suspected Answers to Natural Language Questions using 
Predictive Annotation?, Proceedings of ANLP?00, Seattle, 
WA, 2000. 
[8] Rosch, E. et al ?Basic Objects in Natural Categories?, 
Cognitive Psychology 8, pp. 382-439, 1976. 
[9] TREC8 - ?The Eighth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
2000. 
[10] TREC9 - ?The Ninth Text Retrieval Conference?, E.M. 
Voorhees and D.K. Harman Eds., NIST, Gaithersburg, MD, 
to appear. 
 
APPENDIX 
What-is questions from TREC9 
617: What are chloroplasts?  (X) 
528: What are geckos? 
544: What are pomegranates?   
241: What is a caldera?  (X) 
358: What is a meerkat? 
434: What is a nanometer?  (X) 
354: What is a nematode? 
463: What is a stratocaster? 
447: What is anise? 
386: What is anorexia nervosa? 
635: What is cribbage? 
300: What is leukemia? 
305: What is molybdenum? 
644: What is ouzo? 
420: What is pandoro?  (X) 
228: What is platinum? 
374: What is porphyria? 
483: What is sake? 
395: What is saltpeter? 
421: What is thalassemia? 
438: What is titanium? 
600: What is typhoid fever? 
468: What is tyvek? 
539: What is witch hazel? 
 
Our system did not correctly answer the questions marked with 
an ?X?.  For all of the others the correct answer was the first of 
the 5 attempts returned. 
 
NewsInEssence: A System For Domain-Independent,
Real-Time News Clustering and Multi-Document
Summarization
Dragomir R. Radev?y, Sasha Blair-Goldensohn?, Zhu Zhang?, Revathi Sundara Raghavany
?School of Information
yDepartment of EECS
University of Michigan
Ann Arbor, MI 48109
fradev,sashabg,zhuzhang,rsundarag@umich.edu
1. INTRODUCTION
NEWSINESSENCE is a system for finding, visualizing and sum-
marizing a topic-based cluster of news stories. In the generic sce-
nario for NEWSINESSENCE, a user selects a single news story from
a news Web site. Our system then searches other live sources of
news for other stories related to the same event and produces sum-
maries of a subset of the stories that it finds, according to parame-
ters specified by the user.
2. THE NEWSINESSENCE SYSTEM
NewsInEssence?s search agent, NewsTroll, runs in two phases.
First, it looks for related articles by traversing links from the page
containing the seed article. Using the seed article and any related
articles it finds in this way, the agent then decides on a set of key-
words for further search. In the second phase, it attempts to add
to the cluster of related articles by going to the search engines of
various news websites and using the keywords which it found in
the first phase as search terms.
In both phases, NewsTroll selectively follows hyperlinks with
the aim of reaching pages which contain related stories and/or fur-
ther hyperlinks to related stories pages.
Both general and site-specific rules help NewsTroll determine
which URLs are likely to be useful. Only if NewsTroll determines
that a URL is ?interesting?, will it go to the Internet to fetch the new
page. A more stringent set of rules are applied to determine whether
the URL is likely to be a news story itself. If so, the similarity
of its text to that of the original seed page is computed using an
IDF-weighted vector measure. If the similarity is above a certain
threshold, the page is considered to contain a related article and
added to the cluster. The user may use our web interface (Figure 2)
to adjust the similarity threshold used in a given search.
Using several levels of filtering, NewsTroll is able to screen out
large numbers web pages quite efficiently. The expensive opera-
tion of testing lexical similarity is reserved for the small number of
.
pages which NewsTroll finds interesting. Consequently, the agent
can return useful results in real time.
3. ANNOTATED SAMPLE RUN
The example begins when we find a news article we would like
to read more about. In this case we pick a story is about a break-
ing story regarding one of President-Elect Bush?s cabinet nominees
(see Figure 1).
We input the URL using the web interface of the NEWSINESSENCE
system, then select our search options, click ?Proceed? and wait for
our results (see Figure 2).
In response to the user query, NewsTroll begins looking for re-
lated articles linked from the chosen start page. In a selection from
the agent?s output log in Figure 3, we can see that it extracts and
tests links from the page, and decides to test one which looks like a
news article. We then see that it tests this article and determines it
to be related. This article is added to the initial cluster, from which
the list of top keywords is drawn.
In its secondary phase, NewsTroll inputs its keywords to the
search engines of news sites and lets them do the work of find-
ing stories. Since we have selected good keywords, most of the
links seen by NewsTroll in this part of the search are indeed related
articles (see Figure 4). Upon exiting, NewsTroll reports the num-
ber of links it has considered, followed, tested, and retrieved (see
Figure 4).
The system?s web interface reports its progress to the user in real
time and provides a link to the visualization GUI once the cluster
is complete (Figure 5). Using the GUI, the user can select which of
the articles to summarize (see Figures 6 and 7). Figure 8 shows the
output of the cluster summarizer.
4. FUTURE WORK
We are currently working on the integration of Cross-Document
structure theory (CST) [1] with NEWSINESSENCE. CST is used to
describe relations between textual units in multi-document clusters.
It is used for example to identify which portions of a cluster contain
background information, which sections are redundant, and which
ones contain additional information about an event.
5. REFERENCES
[1] Dragomir Radev. A common theory of information fusion
from multiple text sources, step one: Cross-document
structure. In Proceedings, 1st ACL SIGDIAL Workshop on
Discourse and Dialogue, Hong Kong, October 2000.
Figure 1: Seed article.
Figure 2: User interface.
Figure 3: Run-time log (part I).
Figure 4: Run-time log (part II).
Figure 5: System progress.
Figure 6: Cluster visualization.
Figure 7: Selected articles.
Figure 8: Summarization interface.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 915?922, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Random Walks for Question-focused Sentence Retrieval
Jahna Otterbacher1, Gu?nes? Erkan2, Dragomir R. Radev1,2
1School of Information, 2Department of EECS
University of Michigan
{jahna,gerkan,radev}@umich.edu
Abstract
We consider the problem of question-
focused sentence retrieval from complex
news articles describing multi-event sto-
ries published over time. Annotators gen-
erated a list of questions central to under-
standing each story in our corpus. Be-
cause of the dynamic nature of the stories,
many questions are time-sensitive (e.g.
?How many victims have been found??)
Judges found sentences providing an an-
swer to each question. To address the
sentence retrieval problem, we apply a
stochastic, graph-based method for com-
paring the relative importance of the tex-
tual units, which was previously used suc-
cessfully for generic summarization. Cur-
rently, we present a topic-sensitive version
of our method and hypothesize that it can
outperform a competitive baseline, which
compares the similarity of each sentence
to the input question via IDF-weighted
word overlap. In our experiments, the
method achieves a TRDR score that is sig-
nificantly higher than that of the baseline.
1 Introduction
Recent work has motivated the need for systems
that support ?Information Synthesis? tasks, in which
a user seeks a global understanding of a topic or
story (Amigo et al, 2004). In contrast to the clas-
sical question answering setting (e.g. TREC-style
Q&A (Voorhees and Tice, 2000)), in which the user
presents a single question and the system returns a
corresponding answer (or a set of likely answers), in
this case the user has a more complex information
need.
Similarly, when reading about a complex news
story, such as an emergency situation, users might
seek answers to a set of questions in order to un-
derstand it better. For example, Figure 1 shows
the interface to our Web-based news summarization
system, which a user has queried for information
about Hurricane Isabel. Understanding such stories
is challenging for a number of reasons. In particular,
complex stories contain many sub-events (e.g. the
devastation of the hurricane, the relief effort, etc.) In
addition, while some facts surrounding the situation
do not change (such as ?Which area did the hurri-
cane first hit??), others may change with time (?How
many people have been left homeless??). There-
fore, we are working towards developing a system
for question answering from clusters of complex sto-
ries published over time. As can be seen at the bot-
tom of Figure 1, we plan to add a component to our
current system that allows users to ask questions as
they read a story. They may then choose to receive
either a precise answer or a question-focused sum-
mary.
Currently, we address the question-focused sen-
tence retrieval task. While passage retrieval (PR) is
clearly not a new problem (e.g. (Robertson et al,
1992; Salton et al, 1993)), it remains important and
yet often overlooked. As noted by (Gaizauskas et al,
2004), while PR is the crucial first step for question
answering, Q&A research has typically not empha-
915
Hurricane Isabel's outer bands moving onshore
produced on 09/18, 6:18 AM
2% Summary
The North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as far
away as Pennsylvania prepared for possibly ruinous flooding. (2:3) A hurricane warning was in effect from Cape
Fear in southern North Carolina to the Virginia-Maryland line, and tropical storm warnings extended from South Carolina
to New Jersey. (2:14)
While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still
400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning. (3:10) BBC NEWS World Americas
Hurricane Isabel prompts US shutdown (4:1)
Ask us:
What states have been affected by the hurricane so far?
Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trapped
by flooding from storm surges up to 11 feet. (5:8) The storm was expected to hit with its full fury today, slamming into
the North Carolina coast with 105-mph winds and 45-foot wave crests, before moving through Virginia and bashing the
capital with gusts of about 60 mph. (7:6)
Figure 1: Question tracking interface to a summa-
rization system.
sized it. The specific problem we consider differs
from the classic task of PR for a Q&A system in
interesting ways, due to the time-sensitive nature of
the stories in our corpus. For example, one challenge
is that the answer to a user?s question may be up-
dated and reworded over time by journalists in order
to keep a running story fresh, or because the facts
themselves change. Therefore, there is often more
than one correct answer to a question.
We aim to develop a method for sentence re-
trieval that goes beyond finding sentences that are
similar to a single query. To this end, we pro-
pose to use a stochastic, graph-based method. Re-
cently, graph-based methods have proved useful for
a number of NLP and IR tasks such as document
re-ranking in ad hoc IR (Kurland and Lee, 2005)
and analyzing sentiments in text (Pang and Lee,
2004). In (Erkan and Radev, 2004), we introduced
the LexRank method and successfully applied it to
generic, multi-document summarization. Presently,
we introduce topic-sensitive LexRank in creating a
sentence retrieval system. We evaluate its perfor-
mance against a competitive baseline, which con-
siders the similarity between each sentence and the
question (using IDF-weighed word overlap). We
demonstrate that LexRank significantly improves
question-focused sentence selection over the base-
line.
2 Formal description of the problem
Our goal is to build a question-focused sentence re-
trieval mechanism using a topic-sensitive version of
the LexRank method. In contrast to previous PR sys-
tems such as Okapi (Robertson et al, 1992), which
ranks documents for relevancy and then proceeds to
find paragraphs related to a question, we address the
finer-grained problem of finding sentences contain-
ing answers. In addition, the input to our system is
a set of documents relevant to the topic of the query
that the user has already identified (e.g. via a search
engine). Our system does not rank the input docu-
ments, nor is it restricted in terms of the number of
sentences that may be selected from the same docu-
ment.
The output of our system, a ranked list of sen-
tences relevant to the user?s question, can be sub-
sequently used as input to an answer selection sys-
tem in order to find specific answers from the ex-
tracted sentences. Alternatively, the sentences can
be returned to the user as a question-focused sum-
mary. This is similar to ?snippet retrieval? (Wu et
al., 2004). However, in our system answers are ex-
tracted from a set of multiple documents rather than
on a document-by-document basis.
3 Our approach: topic-sensitive LexRank
3.1 The LexRank method
In (Erkan and Radev, 2004), the concept of graph-
based centrality was used to rank a set of sentences,
in producing generic multi-document summaries.
To apply LexRank, a similarity graph is produced
for the sentences in an input document set. In the
graph, each node represents a sentence. There are
edges between nodes for which the cosine similar-
ity between the respective pair of sentences exceeds
a given threshold. The degree of a given node is
an indication of how much information the respec-
tive sentence has in common with other sentences.
Therefore, sentences that contain the most salient in-
formation in the document set should be very central
within the graph.
Figure 2 shows an example of a similarity graph
for a set of five input sentences, using a cosine simi-
larity threshold of 0.15. Once the similarity graph is
constructed, the sentences are then ranked according
to their eigenvector centrality. As previously men-
tioned, the original LexRank method performed well
in the context of generic summarization. Below,
we describe a topic-sensitive version of LexRank,
which is more appropriate for the question-focused
sentence retrieval problem. In the new approach, the
916
score of a sentence is determined by a mixture model
of the relevance of the sentence to the query and the
similarity of the sentence to other high-scoring sen-
tences.
3.2 Relevance to the question
In topic-sensitive LexRank, we first stem all of the
sentences in a set of articles and compute word IDFs
by the following formula:
idfw = log
(
N + 1
0.5 + sfw
)
(1)
where N is the total number of sentences in the clus-
ter, and sf
w
is the number of sentences that the word
w appears in.
We also stem the question and remove the stop
words from it. Then the relevance of a sentence s to
the question q is computed by:
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw (2)
where tf
w,s
and tf
w,q
are the number of times w
appears in s and q, respectively. This model has
proven to be successful in query-based sentence re-
trieval (Allan et al, 2003), and is used as our com-
petitive baseline in this study (e.g. Tables 4, 5 and
7).
3.3 The mixture model
The baseline system explained above does not make
use of any inter-sentence information in a cluster.
We hypothesize that a sentence that is similar to
the high scoring sentences in the cluster should also
have a high score. For instance, if a sentence that
gets a high score in our baseline model is likely to
contain an answer to the question, then a related sen-
tence, which may not be similar to the question it-
self, is also likely to contain an answer.
This idea is captured by the following mixture
model, where p(s|q), the score of a sentence s given
a question q, is determined as the sum of its rele-
vance to the question (using the same measure as
the baseline described above) and the similarity to
the other sentences in the document cluster:
p(s|q) = d
rel(s|q)
P
z?C rel(z|q)
+(1?d)
X
v?C
sim(s, v)
P
z?C sim(z, v)
p(v|q) (3)
where C is the set of all sentences in the cluster. The
value of d, which we will also refer to as the ?ques-
tion bias,? is a trade-off between two terms in the
Vertices:
Sentence Index Salience Sentence
4 0.1973852892722677 Milan fire brigade officials said that...
1 0.03614457831325301 At least two people are dead, inclu...
0 0.28454242157110576 Officials said the plane was carryin...
2 0.1973852892722677 Italian police said the plane was car..
3 0.28454242157110576 Rescue officials said that at least th...
Graph
Figure 2: LexRank example: sentence similarity
graph with a cosine threshold of 0.15.
equation and is determined empirically. For higher
values of d, we give more importance to the rele-
vance to the question compared to the similarity to
the other sentences in the cluster. The denominators
in both terms are for normalization, which are de-
scribed below. We use the cosine measure weighted
by word IDFs as the similarity between two sen-
tences in a cluster:
sim(x, y) =
P
w?x,y tfw,xtfw,y(idfw)
2
q
P
xi?x
(tfxi,xidfxi )2 ?
q
P
yi?y
(tfyi,y idfyi )2
(4)
Equation 3 can be written in matrix notation as
follows:
p = [dA+ (1 ? d)B]Tp (5)
A is the square matrix such that for a given index i,
all the elements in the ith column are proportional
to rel(i|q). B is also a square matrix such that each
entry B(i, j) is proportional to sim(i, j). Both ma-
trices are normalized so that row sums add up to 1.
Note that as a result of this normalization, all rows
of the resulting square matrixQ = [dA+(1?d)B]
also add up to 1. Such a matrix is called stochastic
and defines a Markov chain. If we view each sen-
tence as a state in a Markov chain, thenQ(i, j) spec-
ifies the transition probability from state i to state j
in the corresponding Markov chain. The vector p
we are looking for in Equation 5 is the stationary
distribution of the Markov chain. An intuitive inter-
pretation of the stationary distribution can be under-
917
stood by the concept of a random walk on the graph
representation of the Markov chain.
With probability d, a transition is made from the
current node (sentence) to the nodes that are simi-
lar to the query. With probability (1-d), a transition
is made to the nodes that are lexically similar to the
current node. Every transition is weighted according
to the similarity distributions. Each element of the
vector p gives the asymptotic probability of ending
up at the corresponding state in the long run regard-
less of the starting state. The stationary distribution
of a Markov chain can be computed by a simple it-
erative algorithm, called power method.1
A simpler version of Equation 5, where A is a
uniform matrix andB is a normalized binary matrix,
is known as PageRank (Brin and Page, 1998; Page
et al, 1998) and used to rank the web pages by the
Google search engine. It was also the model used to
rank sentences in (Erkan and Radev, 2004).
3.4 Experiments with topic-sensitive LexRank
We experimented with different values of d on our
training data. We also considered several threshold
values for inter-sentence cosine similarities, where
we ignored the similarities between the sentences
that are below the threshold. In the training phase
of the experiment, we evaluated all combinations
of LexRank with d in the range of [0, 1] (in incre-
ments of 0.10) and with a similarity threshold rang-
ing from [0, 0.9] (in increments of 0.05). We then
found all configurations that outperformed the base-
line. These configurations were then applied to our
development/test set. Finally, our best sentence re-
trieval system was applied to our test data set and
evaluated against the baseline. The remainder of the
paper will explain this process and the results in de-
tail.
4 Experimental setup
4.1 Corpus
We built a corpus of 20 multi-document clusters of
complex news stories, such as plane crashes, polit-
ical controversies and natural disasters. The data
1The stationary distribution is unique and the power method
is guaranteed to converge provided that the Markov chain is
ergodic (Seneta, 1981). A non-ergodic Markov chain can be
made ergodic by reserving a small probability for jumping to
any other state from the current state (Page et al, 1998).
clusters and their characteristics are shown in Ta-
ble 1. The news articles were collected from various
sources. ?Newstracker? clusters were collected au-
tomatically by our Web-based news summarization
system. The number of clusters randomly assigned
to the training, development/test and test data sets
were 11, 3 and 6, respectively.
Next, we assigned each cluster of articles to an
annotator, who was asked to read all articles in the
cluster. He or she then generated a list of factual
questions key to understanding the story. Once we
collected the questions for each cluster, two judges
independently annotated nine of the training clus-
ters. For each sentence and question pair in a given
cluster, the judges were asked to indicate whether
or not the sentence contained a complete answer
to the question. Once an acceptable rate of inter-
judge agreement was verified on the first nine clus-
ters (Kappa (Carletta, 1996) of 0.68), the remaining
11 clusters were annotated by one judge each.
In some cases, the judges did not find any sen-
tences containing the answer for a given question.
Such questions were removed from the corpus. The
final number of questions annotated for answers
over the entire corpus was 341, and the distributions
of questions per cluster can be found in Table 1.
4.2 Evaluation metrics and methods
To evaluate our sentence retrieval mechanism, we
produced extract files, which contain a list of sen-
tences deemed to be relevant to the question, for the
system and from human judgment. To compare dif-
ferent configurations of our system to the baseline
system, we produced extracts at a fixed length of 20
sentences. While evaluations of question answering
systems are often based on a shorter list of ranked
sentences, we chose to generate longer lists for sev-
eral reasons. One is that we are developing a PR
system, of which the output can then be input to an
answer extraction system for further processing. In
such a setting, we would most likely want to gener-
ate a relatively longer list of candidate sentences. As
previously mentioned, in our corpus the questions
often have more than one relevant answer, so ideally,
our PR system would find many of the relevant sen-
tences, sending them on to the answer component
to decide which answer(s) should be returned to the
user. Each system?s extract file lists the document
918
Cluster Sources Articles Questions Data set Sample question
Algerian terror AFP, UPI 2 12 train What is the condition under which
threat GIA will take its action?
Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in the
crash Fox, USAToday building at the time of the crash?
Turkish plane BBC, ABC, 10 12 train To where was the plane headed?
crash FoxNews, Yahoo
Moscow terror UPI, AFP, AP 7 7 train How many people were killed in
attack the most recent explosion?
Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame for
club fire Fox, BBC, Ananova the fire?
FBI most AFP, UPI 3 14 train How much is the State Department offering
wanted for information leading to bin Laden?s arrest?
Russia bombing AP, AFP 2 11 train What was the cause of the blast?
Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivations
attack BBC, Ananova of the attackers?
Washington DC FoxNews, Ha?aretz, BBC, 8 28 train What kinds of equipment or weapons
sniper BBC, Washington Times, CBS were used in the killings?
GSPC terror Newstracker 8 29 train What are the charges against
group the GSPC suspects?
China Novelty 43 25 18 train What was the magnitude of the
earthquake earthquake in Zhangjiakou?
Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people
FoxNews, Washington Post were on board?
David Beckham AFP 20 28 dev/test How long had Beckham been playing for
trade MU before he moved to RM?
Miami airport Newstracker 12 15 dev/test How many concourses does
evacuation the airport have?
US hurricane DUC d04a 14 14 test In which places had the hurricane landed?
EgyptAir crash Novelty 4 25 29 test How many people were killed?
Kursk submarine Novelty 33 25 30 test When did the Kursk sink?
Hebrew University bombing Newstracker 11 27 test How many people were injured?
Finland mall bombing Newstracker 9 15 test How many people were in the mall
at the time of the bombing?
Putin visits Newstracker 12 20 test What issue concerned British
England human rights groups?
Table 1: Corpus of complex news stories.
and sentence numbers of the top 20 sentences. The
?gold standard? extracts list the sentences judged as
containing answers to a given question by the anno-
tators (and therefore have variable sizes) in no par-
ticular order.2
We evaluated the performance of the systems us-
ing two metrics - Mean Reciprocal Rank (MRR)
(Voorhees and Tice, 2000) and Total Reciprocal
Document Rank (TRDR) (Radev et al, 2005).
MRR, used in the TREC Q&A evaluations, is the
reciprocal rank of the first correct answer (or sen-
tence, in our case) to a given question. This measure
gives us an idea of how far down we must look in the
ranked list in order to find a correct answer. To con-
trast, TRDR is the total of the reciprocal ranks of all
answers found by the system. In the context of an-
swering questions from complex stories, where there
is often more than one correct answer to a question,
and where answers are typically time-dependent, we
should focus on maximizing TRDR, which gives us
2For clusters annotated by two judges, all sentences chosen
by at least one judge were included.
a measure of how many of the relevant sentences
were identified by the system. However, we report
both the average MRR and TRDR over all questions
in a given data set.
5 LexRank versus the baseline system
In the training phase, we searched the parameter
space for the values of d (the question bias) and the
similarity threshold in order to optimize the resulting
TRDR scores. For our problem, we expected that a
relatively low similarity threshold pair with a high
question bias would achieve the best results. Table 2
shows the effect of varying the similarity threshold.3
The notation LR[a, d] is used, where a is the simi-
larity threshold and d is the question bias. The opti-
mal range for the parameter a was between 0.14 and
0.20. This is intuitive because if the threshold is too
high, such that only the most lexically similar sen-
tences are represented in the graph, the method does
not find sentences that are related but are more lex-
3A threshold of -1 means that no threshold was used such
that all sentences were included in the graph.
919
System Ave. MRR Ave. TRDR
LR[-1.0,0.65] 0.5270 0.8117
LR[0.02,0.65] 0.5261 0.7950
LR[0.16,0.65] 0.5131 0.8134
LR[0.18,0.65] 0.5062 0.8020
LR[0.20,0.65] 0.5091 0.7944
LR[-1.0,0.80] 0.5288 0.8152
LR[0.02,0.80] 0.5324 0.8043
LR[0.16,0.80] 0.5184 0.8160
LR[0.18,0.80] 0.5199 0.8154
LR[0.20,0.80] 0.5282 0.8152
Table 2: Training phase: effect of similarity thresh-
old (a) on Ave. MRR and TRDR.
System Ave. MRR Ave. TRDR
LR[0.02,0.65] 0.5261 0.7950
LR[0.02,0.70] 0.5290 0.7997
LR[0.02,0.75] 0.5299 0.8013
LR[0.02,0.80] 0.5324 0.8043
LR[0.02,0.85] 0.5322 0.8038
LR[0.02,0.90] 0.5323 0.8077
LR[0.20,0.65] 0.5091 0.7944
LR[0.20,0.70] 0.5244 0.8105
LR[0.20,0.75] 0.5285 0.8137
LR[0.20,0.80] 0.5282 0.8152
LR[0.20,0.85] 0.5317 0.8203
LR[0.20,0.90] 0.5368 0.8265
Table 3: Training phase: effect of question bias (d)
on Ave. MRR and TRDR.
ically diverse (e.g. paraphrases). Table 3 shows the
effect of varying the question bias at two different
similarity thresholds (0.02 and 0.20). It is clear that a
high question bias is needed. However, a small prob-
ability for jumping to a node that is lexically simi-
lar to the given sentence (rather than the question
itself) is needed. Table 4 shows the configurations
of LexRank that performed better than the baseline
system on the training data, based on mean TRDR
scores over the 184 training questions. We applied
all four of these configurations to our unseen devel-
opment/test data, in order to see if we could further
differentiate their performances.
5.1 Development/testing phase
The scores for the four LexRank systems and the
baseline on the development/test data are shown in
System Ave. MRR Ave. TRDR
Baseline 0.5518 0.8297
LR[0.14,0.95] 0.5267 0.8305
LR[0.18,0.90] 0.5376 0.8382
LR[0.18,0.95] 0.5421 0.8382
LR[0.20,0.95] 0.5404 0.8311
Table 4: Training phase: systems outperforming the
baseline in terms of TRDR score.
System Ave. MRR Ave. TRDR
Baseline 0.5709 1.0002
LR[0.14,0.95] 0.5882 1.0469
LR[0.18,0.90] 0.5820 1.0288
LR[0.18,0.95] 0.5956 1.0411
LR[0.20,0.95] 0.6068 1.0601
Table 5: Development testing evaluation.
Cluster B-MRR LR-MRR B-TRDR LR-TRDR
Gulfair 0.5446 0.5461 0.9116 0.9797
David Beckham trade 0.5074 0.5919 0.7088 0.7991
Miami airport 0.7401 0.7517 1.7157 1.7028
evacuation
Table 6: Average scores by cluster: baseline versus
LR[0.20,0.95].
Table 5. This time, all four LexRank systems outper-
formed the baseline, both in terms of average MRR
and TRDR scores. An analysis of the average scores
over the 72 questions within each of the three clus-
ters for the best system, LR[0.20,0.95], is shown
in Table 6. While LexRank outperforms the base-
line system on the first two clusters both in terms
of MRR and TRDR, their performances are not sub-
stantially different on the third cluster. Therefore,
we examined properties of the questions within each
cluster in order to see what effect they might have on
system performance.
We hypothesized that the baseline system, which
compares the similarity of each sentence to the ques-
tion using IDF-weighted word overlap, should per-
form well on questions that provide many content
words. To contrast, LexRank might perform bet-
ter when the question provides fewer content words,
since it considers both similarity to the query and
inter-sentence similarity. Out of the 72 questions in
the development/test set, the baseline system outper-
formed LexRank on 22 of the questions. In fact, the
average number of content words among these 22
questions was slightly, but not significantly, higher
than the average on the remaining questions (3.63
words per question versus 3.46). Given this obser-
vation, we experimented with two mixed strategies,
in which the number of content words in a question
determined whether LexRank or the baseline system
was used for sentence retrieval. We tried threshold
values of 4 and 6 content words, however, this did
not improve the performance over the pure strategy
of system LR[0.20,0.95]. Therefore, we applied this
920
Ave. MRR Ave. TRDR
Baseline 0.5780 0.8673
LR[0.20,0.95] 0.6189 0.9906
p-value na 0.0619
Table 7: Testing phase: baseline vs. LR[0.20,0.95].
system versus the baseline to our unseen test set of
134 questions.
5.2 Testing phase
As shown in Table 7, LR[0.20,0.95] outperformed
the baseline system on the test data both in terms
of average MRR and TRDR scores. The improve-
ment in average TRDR score was statistically sig-
nificant with a p-value of 0.0619. Since we are in-
terested in a passage retrieval mechanism that finds
sentences relevant to a given question, providing in-
put to the question answering component of our sys-
tem, the improvement in average TRDR score is
very promising. While we saw in Section 5.1 that
LR[0.20,0.95] may perform better on some question
or cluster types than others, we conclude that it beats
the competitive baseline when one is looking to op-
timize mean TRDR scores over a large set of ques-
tions. However, in future work, we will continue
to improve the performance, perhaps by develop-
ing mixed strategies using different configurations
of LexRank.
6 Discussion
The idea behind using LexRank for sentence re-
trieval is that a system that considers only the sim-
ilarity between candidate sentences and the input
query, and not the similarity between the candidate
sentences themselves, is likely to miss some impor-
tant sentences. When using any metric to compare
sentences and a query, there is always likely to be
a tie between multiple sentences (or, similarly, there
may be cases where fewer than the number of de-
sired sentences have similarity scores above zero).
LexRank effectively provides a means to break such
ties. An example of such a scenario is illustrated in
Tables 8 and 9, which show the top ranked sentences
by the baseline and LexRank, respectively for the
question ?What caused the Kursk to sink?? from the
Kursk submarine cluster. It can be seen that all top
five sentences chosen by the baseline system have
Rank Sentence Score Relevant?
1 The Russian governmental commission on the 4.2282 N
accident of the submarine Kursk sinking in
the Barents Sea on August 12 has rejected
11 original explanations for the disaster,
but still cannot conclude what caused the
tragedy indeed, Russian Deputy Premier Ilya
Klebanov said here Friday.
2 There has been no final word on what caused 4.2282 N
the submarine to sink while participating
in a major naval exercise, but Defense
Minister Igor Sergeyev said the theory
that Kursk may have collided with another
object is receiving increasingly
concrete confirmation.
3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
5 President Clinton?s national security adviser, 4.2282 N
Samuel Berger, has provided his Russian
counterpart with a written summary of what
U.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk to
sink last month in the Barents Sea, officials
said Wednesday.
Table 8: Top ranked sentences using baseline system
on the question ?What caused the Kursk to sink??.
the same sentence score (similarity to the query), yet
the top ranking two sentences are not actually rele-
vant according to the judges. To contrast, LexRank
achieved a better ranking of the sentences since it is
better able to differentiate between them. It should
be noted that both for the LexRank and baseline sys-
tems, chronological ordering of the documents and
sentences is preserved, such that in cases where two
sentences have the same score, the one published
earlier is ranked higher.
7 Conclusion
We presented topic-sensitive LexRank and applied
it to the problem of sentence retrieval. In a Web-
based news summarization setting, users of our sys-
tem could choose to see the retrieved sentences (as
in Table 9) as a question-focused summary. As in-
dicated in Table 9, each of the top three sentences
were judged by our annotators as providing a com-
plete answer to the respective question. While the
first two sentences provide the same answer (a col-
lision caused the Kursk to sink), the third sentence
provides a different answer (an explosion caused the
disaster). While the last two sentences do not pro-
vide answers according to our judges, they do pro-
vide context information about the situation. Alter-
natively, the user might prefer to see the extracted
921
Rank Sentence Score Relevant?
1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Y
said Thursday that collision with a big
object caused the Kursk nuclear submarine
to sink to the bottom of the Barents Sea.
3 The Russian navy refused to confirm this, 0.0125 Y
but officers have said an explosion in the
torpedo compartment at the front of the
submarine apparently caused the Kursk to sink.
4 President Clinton?s national security adviser, 0.0124 N
Samuel Berger, has provided his Russian
counterpart with a written summary of what
U.S. naval and intelligence officials believe
caused the nuclear-powered submarine Kursk to
sink last month in the Barents Sea, officials
said Wednesday.
5 There has been no final word on what caused 0.0123 N
the submarine to sink while participating
in a major naval exercise, but Defense
Minister Igor Sergeyev said the theory
that Kursk may have collided with another
object is receiving increasingly
concrete confirmation.
Table 9: Top ranked sentences using the
LR[0.20,0.95] system on the question ?What caused
the Kursk to sink??
answers from the retrieved sentences. In this case,
the sentences selected by our system would be sent
to an answer identification component for further
processing. As discussed in Section 2, our goal was
to develop a topic-sensitive version of LexRank and
to use it to improve a baseline system, which had
previously been used successfully for query-based
sentence retrieval (Allan et al, 2003). In terms of
this task, we have shown that over a large set of unal-
tered questions written by our annotators, LexRank
can, on average, outperform the baseline system,
particularly in terms of TRDR scores.
8 Acknowledgments
We would like to thank the members of the CLAIR
group at Michigan and in particular Siwei Shen and
Yang Ye for their assistance with this project.
References
James Allan, Courtney Wade, and Alvaro Bolivar. 2003.
Retrieval and novelty detection at the sentence level.
In SIGIR ?03: Proceedings of the 26th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in informaion retrieval, pages 314?321. ACM
Press.
Enrique Amigo, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An Empirical Study
of Information Synthesis Task. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 207?214,
Barcelona, Spain, July.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Gunes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text.
JAIR, 22:457?479.
Robert Gaizauskas, Mark Hepple, and Mark Greenwood.
2004. Information Retrieval for Question Answering:
a SIGIR 2004 Workshop. In SIGIR 2004 Workshop on
Information Retrieval for Question Answering.
Oren Kurland and Lillian Lee. 2005. PageRank without
hyperlinks: Structural re-ranking using links induced
by language models. In SIGIR 2005, Salvador, Brazil,
August.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford University, Stanford,
CA.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Association for
Computational Linguistics.
Dragomir Radev, Weiguo Fan, Hong Qi, Harris Wu, and
Amardeep Grewal. 2005. Probabilistic Question An-
swering on the Web. Journal of the American So-
ciety for Information Science and Technology, 56(3),
March.
Stephen E. Robertson, Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC. In Text REtrieval Conference,
pages 21?30.
G. Salton, J. Allan, and C. Buckley. 1993. Approaches
to Passage REtrieval in Full Text Information Systems.
In Proceedings of the 16th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 49?58.
E. Seneta. 1981. Non-negative matrices and markov
chains. Springer-Verlag, New York.
Ellen Voorhees and Dawn Tice. 2000. The TREC-8
Question Answering Track Evaluation. In Text Re-
trieval Conference TREC-8, Gaithersburg, MD.
Harris Wu, Dragomir R. Radev, and Weiguo Fan.
2004. Towards Answer-focused Summarization Using
Search Engines. New Directions in Question Answer-
ing.
922
c? 2002 Association for Computational Linguistics
Introduction to the Special Issue on
Summarization
Dragomir R. Radev? Eduard Hovy?
University of Michigan USC/ISI
Kathleen McKeown?
Columbia University
1. Introduction and Definitions
As the amount of on-line information increases, systems that can automatically sum-
marize one or more documents become increasingly desirable. Recent research has
investigated types of summaries, methods to create them, and methods to evaluate
them. Several evaluation competitions (in the style of the National Institute of Stan-
dards and Technology?s [NIST?s] Text Retrieval Conference [TREC]) have helped de-
termine baseline performance levels and provide a limited set of training material.
Frequent workshops and symposia reflect the ongoing interest of researchers around
the world. The volume of papers edited by Mani and Maybury (1999) and a book
(Mani 2001) provide good introductions to the state of the art in this rapidly evolving
subfield.
A summary can be loosely defined as a text that is produced from one or more
texts, that conveys important information in the original text(s), and that is no longer
than half of the original text(s) and usually significantly less than that. Text here is
used rather loosely and can refer to speech, multimedia documents, hypertext, etc.
The main goal of a summary is to present the main ideas in a document in less
space. If all sentences in a text document were of equal importance, producing a sum-
mary would not be very effective, as any reduction in the size of a document would
carry a proportional decrease in its informativeness. Luckily, information content in a
document appears in bursts, and one can therefore distinguish between more and less
informative segments. Identifying the informative segments at the expense of the rest
is the main challenge in summarization.
Of the many types of summary that have been identified (Borko and Bernier 1975;
Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide
an idea of what the text is about without conveying specific content, and informative
ones provide some shortened version of the content. Topic-oriented summaries con-
centrate on the reader?s desired topic(s) of interest, whereas generic summaries reflect
the author?s point of view. Extracts are summaries created by reusing portions (words,
sentences, etc.) of the input text verbatim, while abstracts are created by regenerating
? Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science
and Department of Linguistics, University of Michigan, Ann Arbor. E-mail: radev@umich.edu.
? ISI Fellow and Senior Project Leader, Information Sciences Institute of the University of Southern
California, Marina del Rey, CA. E-mail: hovy@isi.edu.
? Professor, Department of Computer Science, New York University, New York, NY. E-mail:
kathy@cs.columbia.edu.
400
Computational Linguistics Volume 28, Number 4
the extracted content. Extraction is the process of identifying important material in
the text, abstraction the process of reformulating it in novel terms, fusion the process
of combining extracted portions, and compression the process of squeezing out unim-
portant material. The need to maintain some degree of grammaticality and coherence
plays a role in all four processes.
The obvious overlap of text summarization with information extraction, and con-
nections from summarization to both automated question answering and natural lan-
guage generation, suggest that summarization is actually a part of a larger picture.
In fact, whereas early approaches drew more from information retrieval, more re-
cent approaches draw from the natural language field. Natural language generation
techniques have been adapted to work with typed textual phrases, in place of se-
mantics, as input, and this allows researchers to experiment with approaches to ab-
straction. Techniques that have been developed for topic-oriented summaries are now
being pushed further so that they can be applied to the production of long answers
for the question-answering task. However, as the articles in this special issue show,
domain-independent summarization has several specific, difficult aspects that make it
a research topic in its own right.
2. Major Approaches
We provide a sketch of the current state of the art of summarization by describing
the general areas of research, including single-document summarization through ex-
traction, the beginnings of abstractive approaches to single-document summarization,
and a variety of approaches to multidocument summarization.
2.1 Single-Document Summarization through Extraction
Despite the beginnings of research on alternatives to extraction, most work today
still relies on extraction of sentences from the original document to form a summary.
The majority of early extraction research focused on the development of relatively
simple surface-level techniques that tend to signal important passages in the source
text. Although most systems use sentences as units, some work with larger passages,
typically paragraphs. Typically, a set of features is computed for each passage, and
ultimately these features are normalized and summed. The passages with the highest
resulting scores are sorted and returned as the extract.
Early techniques for sentence extraction computed a score for each sentence based
on features such as position in the text (Baxendale 1958; Edmundson 1969), word
and phrase frequency (Luhn 1958), key phrases (e.g., ?it is important to note?) (Ed-
mundson 1969). Recent extraction approaches use more sophisticated techniques for
deciding which sentences to extract; these techniques often rely on machine learning
to identify important features, on natural language analysis to identify key passages,
or on relations between words rather than bags of words.
The application of machine learning to summarization was pioneered by Kupiec,
Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier
to combine features from a corpus of scientific articles and their abstracts. Aone et
al. (1999) and Lin (1999) experimented with other forms of machine learning and its
effectiveness. Machine learning has also been applied to learning individual features;
for example, Lin and Hovy (1997) applied machine learning to the problem of de-
termining how sentence position affects the selection of sentences, and Witbrock and
Mittal (1999) used statistical approaches to choose important words and phrases and
their syntactic context.
401
Radev, Hovy, and McKeown Summarization: Introduction
Approaches involving more sophisticated natural language analysis to identify key
passages rely on analysis either of word relatedness or of discourse structure. Some
research uses the degree of lexical connectedness between potential passages and the
remainder of the text; connectedness may be measured by the number of shared words,
synonyms, or anaphora (e.g., Salton et al 1997; Mani and Bloedorn 1997; Barzilay
and Elhadad 1999). Other research rewards passages that include topic words, that is,
words that have been determined to correlate well with the topic of interest to the user
(for topic-oriented summaries) or with the general theme of the source text (Buckley
and Cardie 1997; Strzalkowski et al 1999; Radev, Jing, and Budzikowska 2000).
Alternatively, a summarizer may reward passages that occupy important positions
in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This
method requires a system to compute discourse structure reliably, which is not possible
in all genres. This technique is the focus of one of the articles in this special issue (Teufel
and Moens 2002), which shows how particular types of rhetorical relations in the genre
of scientific journal articles can be reliably identified through the use of classification.
An open-source summarization environment, MEAD, was recently developed at the
Johns Hopkins summer workshop (Radev et al 2002). MEAD allows researchers to
experiment with different features and methods for combination.
Some recent work (Conroy and O?Leary 2001) has turned to the use of hidden
Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the
probability of inclusion of a sentence in an extract depends on whether the previous
sentence has been included as well.
2.2 Single-Document Summarization through Abstraction
At this early stage in research on summarization, we categorize any approach that
does not use extraction as an abstractive approach. Abstractive approaches have used
information extraction, ontological information, information fusion, and compression.
Information extraction approaches can be characterized as ?top-down,? since they
look for a set of predefined information types to include in the summary (in con-
trast, extractive approaches are more data-driven). For each topic, the user predefines
frames of expected information types, together with recognition criteria. For example,
an earthquake frame may contain slots for location, earthquake magnitude, number of
casualties, etc. The summarization engine must then locate the desired pieces of infor-
mation, fill them in, and generate a summary with the results (DeJong 1978; Rau and
Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in
restricted domains only.
Compressive summarization results from approaching the problem from the point
of view of language generation. Using the smallest units from the original document,
Witbrock and Mittal (1999) extract a set of words from the input document and then
order the words into sentences using a bigram language model. Jing and McKeown
(1999) point out that human summaries are often constructed from the source docu-
ment by a process of cutting and pasting document fragments that are then combined
and regenerated as summary sentences. Hence a summarizer can be developed to
extract sentences, reduce them by dropping unimportant fragments, and then use in-
formation fusion and generation to combine the remaining fragments. In this special
issue, Jing (2002) reports on automated techniques to build a corpus representing the
cut-and-paste process used by humans; such a corpus can then be used to train an
automated summarizer.
Other researchers focus on the reduction process. In an attempt to learn rules for
reduction, Knight and Marcu (2000) use expectation maximization to train a system
to compress the syntactic parse tree of a sentence in order to produce a shorter but
402
Computational Linguistics Volume 28, Number 4
still maximally grammatical version. Ultimately, this approach can likely be used for
shortening two sentences into one, three into two (or one), and so on.
Of course, true abstraction involves taking the process one step further. Abstraction
involves recognizing that a set of extracted passages together constitute something
new, something that is not explicitly mentioned in the source, and then replacing them
in the summary with the (ideally more concise) new concept(s). The requirement that
the new material not be in the text explicitly means that the system must have access
to external information of some kind, such as an ontology or a knowledge base, and be
able to perform combinatory inference (Hahn and Reimer 1997). Since no large-scale
resources of this kind yet exist, abstractive summarization has not progressed beyond
the proof-of-concept stage (although top-down information extraction can be seen as
one variant).
2.3 Multidocument Summarization
Multidocument summarization, the process of producing a single summary of a set
of related source documents, is relatively new. The three major problems introduced
by having to handle multiple input documents are (1) recognizing and coping with
redundancy, (2) identifying important differences among documents, and (3) ensuring
summary coherence, even when material stems from different source documents.
In an early approach to multidocument summarization, information extraction
was used to facilitate the identification of similarities and differences (McKeown and
Radev 1995). As for single-document summarization, this approach produces more of a
briefing than a summary, as it contains only preidentified information types. Identity of
slot values are used to determine when information is reliable enough to include in the
summary. Later work merged information extraction approaches with regeneration of
extracted text to improve summary generation (Radev and McKeown 1998). Important
differences (e.g., updates, trends, direct contradictions) are identified through a set of
discourse rules. Recent work also follows this approach, using enhanced information
extraction and additional forms of contrasts (White and Cardie 2002).
To identify redundancy in text documents, various similarity measures are used.
A common approach is to measure similarity between all pairs of sentences and then
use clustering to identify themes of common information (McKeown et al 1999; Radev,
Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure
the similarity of a candidate passage to that of already-selected passages and retain
it only if it contains enough new (dissimilar) information. A popular such measure is
maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell
and Goldstein 1998).
Once similar passages in the input documents have been identified, the infor-
mation they contain must be included in the summary. Rather than simply listing
all similar sentences (a lengthy solution), some approaches will select a representa-
tive passage to convey information in each cluster (Radev, Jing, and Budzikowska
2000), whereas other approaches use information fusion techniques to identify repet-
itive phrases from the clusters and combine the phrases into the summary (Barzilay,
McKeown, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of
human-generated compression and reformulation rules.
Ensuring coherence is difficult, because this in principle requires some understand-
ing of the content of each passage and knowledge about the structure of discourse.
In practice, most systems simply follow time order and text order (passages from
the oldest text appear first, sorted in the order in which they appear in the input).
To avoid misleading the reader when juxtaposed passages from different dates all
say ?yesterday,? some systems add explicit time stamps (Lin and Hovy 2002a). Other
403
Radev, Hovy, and McKeown Summarization: Introduction
systems use a combination of temporal and coherence constraints to order sentences
(Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002)
have focused on discourse-based revisions of multidocument clusters as a means for
improving summary coherence.
Although multidocument summarization is new and the approaches described
here are only the beginning, current research also branches out in other directions. Re-
search is beginning on the generation of updates on new information (Allan, Gupta,
and Khandelwal 2001). Researchers are currently studying the production of longer
answers (i.e., multidocument summaries) from retrieved documents, focusing on such
types as biographies of people, descriptions of multiple events of the same type
(e.g., multiple hurricanes), opinion pieces (e.g., editorials and letters discussing a con-
tentious topic), and causes of events. Another challenging ongoing topic is the gener-
ation of titles for either a single document or set of documents. This challenge will be
explored in an evaluation planned by NIST in 2003.
2.4 Evaluation
Evaluating the quality of a summary has proven to be a difficult problem, principally
because there is no obvious ?ideal? summary. Even for relatively straightforward news
articles, human summarizers tend to agree only approximately 60% of the time, mea-
suring sentence content overlap. The use of multiple models for system evaluation
could help alleviate this problem, but researchers also need to look at other methods
that can yield more acceptable models, perhaps using a task as motivation.
Two broad classes of metrics have been developed: form metrics and content met-
rics. Form metrics focus on grammaticality, overall text coherence, and organization
and are usually measured on a point scale (Brandow, Mitze, and Rau 1995). Content is
more difficult to measure. Typically, system output is compared sentence by sentence
or fragment by fragment to one or more human-made ideal abstracts, and as in in-
formation retrieval, the percentage of extraneous information present in the system?s
summary (precision) and the percentage of important information omitted from the
summary (recall) are recorded. Other commonly used measures include kappa (Car-
letta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take
into account the performance of a summarizer that randomly picks passages from the
original document to produce an extract. In the Document Understanding Conference
(DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn
and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface
(Lin 2001) to record values for precision and recall. These two competitions, run along
the lines of TREC, have served to establish overall baselines for single-document and
multidocument summarization and have provided several hundred human abstracts
as training material. (Another popular source of training material is the Ziff-Davis cor-
pus of computer product announcements.) Despite low interjudge agreement, DUC
has shown that humans are better summary producers than machines and that, for
the news article genre, certain algorithms do in fact do better than the simple baseline
of picking the lead material.
The largest task-oriented evaluation to date, the Summarization Evaluation Con-
ference (SUMMAC) (Mani et al 1998; Firmin and Chrzanowski 1999) included three
tests: the categorization task (how well can humans categorize a summary compared
to its full text?), the ad hoc task (how well can humans determine whether a full text is
relevant to a query just from reading the summary?) and the question task (how well
can humans answer questions about the main thrust of the source text from reading
just the summary?). But the interpretation of the results is not simple; studies (Jing et
al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000)
404
Computational Linguistics Volume 28, Number 4
show how the same summaries receive different scores under different measures or
when compared to different (but presumably equivalent) ideal summaries created by
humans. With regard to interhuman agreement, Jing et al find fairly high consistency
in the news genre only when the summary (extract) length is fixed relatively short.
Marcu (1997a) provides some evidence that other genres will deliver less consistency.
With regard to the lengths of the summaries produced by humans when not con-
strained by a particular compression rate, both Jing and Marcu find great variation.
Nonetheless, it is now generally accepted that for single news articles, systems produce
generic summaries indistinguishable from those of humans.
Automated summary evaluation is a gleam in everyone?s eye. Clearly, when an
ideal extract has been created by human(s), extractive summaries are easy to evalu-
ate. Marcu (1999) and Goldstein et al (1999) independently developed an automated
method to create extracts corresponding to abstracts. But when the number of available
extracts is not sufficient, it is not clear how to overcome the problems of low inter-
human agreement. Simply using a variant of the Bilingual Evaluation Understudy
(BLEU) scoring method (based on a linear combination of matching n-grams between
the system output and the ideal summary) developed for machine translation (Pap-
ineni et al 2001) is promising but not sufficient (Lin and Hovy 2002b).
3. The Articles in this Issue
The articles in this issue move beyond the current state of the art in various ways.
Whereas most research to date has focused on the use of sentence extraction for sum-
marization, we are beginning to see techniques that allow a system to extract, merge,
and edit phrases, as opposed to full sentences, to generate a summary. Whereas many
summarization systems are designed for summarization of news, new algorithms are
summarizing much longer and more complex documents, such as scientific journal
articles, medical journal articles, or patents. Whereas most research to date has fo-
cused on text summarization, we are beginning to see a move toward summarization
of speech, a medium that places additional demands on the summarization process.
Finally, in addition to providing full summarization systems, the articles in this issue
also focus on tools that can aid in the process of developing summarization systems,
on computational efficiency of algorithms, and on techniques needed for preprocessing
speech.
The four articles that focus on summarization of text share a common theme:
Each views the summarization process as consisting of two phases. In the first, mate-
rial within the original document that is important is identified and extracted. In the
second, this extracted material may be modified, merged, and edited using genera-
tion techniques. Two of the articles focus on the extraction stage (Teufel and Moens
2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically
constructing resources that can be used for the second stage.
Teufel and Moens propose significantly different techniques for sentence extraction
than have been used in the past. Noting the difference in both length and structure
between scientific articles and news, they claim that both the context of sentences and
a more focused search for sentences is needed in order to produce a good summary
that is only 2.5% of the original document. Their approach is to provide a summary
that focuses on the new contribution of the paper and its relation to previous work.
They rely on rhetorical relations to provide information about context and to identify
sentences relating to, for example, the aim of the paper, its basis in previous work,
or contrasts with other work. Their approach features the use of corpora annotated
both with rhetorical relations and with relevance; it uses text categorization to extract
405
Radev, Hovy, and McKeown Summarization: Introduction
sentences corresponding to any of seven rhetorical categories. The result is a set of
sentences that situate the article in respect to its original claims and in relation to other
research.
Silber and McCoy focus on computationally efficient algorithms for sentence ex-
traction. They present a linear time algorithm to extract lexical chains from a source
document (the lexical-chain approach was originally developed by Barzilay and El-
hadad [1997] but used an exponential time algorithm). This approach facilitates the
use of lexical chains as an intermediate representation for summarization. Barzilay and
Elhadad present an evaluation of the approach for summarization with both scientific
documents and university textbooks.
Jing advocates the use of a cut-and-paste approach to summarization in which
phrases, rather than sentences, are extracted from the original document. She shows
that such an approach is often used by human abstractors. She then presents an auto-
mated tool that is used to analyze a corpus of paired documents and abstracts written
by humans, in order to identify the phrases within the documents that are used in
the abstracts. She has developed an HMM solution to the matching problem. The
decomposition program is a tool that can produce training and testing corpora for
summarization, and its results have been used for her own summarization program.
Saggion and Lapalme (2002) describe a system, SumUM, that generates indicative-
informative summaries from technical documents. To build their system, Saggion and
Lapalme have studied a corpus of professionally written (short) abstracts. They have
manually aligned the abstracts and the original documents. Given the structured form
of technical papers, most of the information in the abstracts was also found in either the
author abstract (20%) or in the first section of the paper (40%) or the headlines or cap-
tions (23%). Based on their observations, the authors have developed an approach to
summarization, called selective analysis, which mimics the human abstractors? routine.
The four components of selective analysis are indicative selection, informative selection,
indicative generation, and informative generation.
The final article in the issue (Zechner 2002) is distinct from the other articles in
that it addresses problems in summarization of speech. As in text summarization,
Zechner also uses sentence extraction to determine the content of the summary. Given
the informal nature of speech, however, a number of significant steps must be taken
in order to identify useful segments for extraction. Zechner develops techniques for
removing disfluencies from speech, for identifying units for extraction that are in
some sense equivalent to sentences, and for identifying relations such as question-
answer across turns in order to determine when units from two separate turns should
be extracted as a whole. This preprocessing yields a transcript on which standard
techniques for extraction in text (here the use of MMR [Carbonell and Goldstein 1998]
to identify relevant units) can operate successfully.
Though true abstractive summarization remains a researcher?s dream, the success
of extractive summarizers and the rapid development of compressive and similar
techniques testifies to the effectiveness with which the research community can address
new problems and find workable solutions to them.
References
Allan, James, Rahul Gupta, and Vikas
Khandelwal. 2001. Temporal summaries
of news topics. In Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 10?18.
Aone, Chinatsu, Mary Ellen Okurowski,
James Gorlinsky, and Bjornar Larsen.
1999. A trainable summarizer with
knowledge acquired from robust NLP
techniques. In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text
Summarization. MIT Press, Cambridge,
pages 71?80.
Barzilay, Regina and Michael Elhadad. 1997.
406
Computational Linguistics Volume 28, Number 4
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17,
Madrid, July.
Barzilay, Regina and Michael Elhadad. 1999.
Using lexical chains for text
summarization. In I. Mani and M. T.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press,
Cambridge, pages 111?121.
Barzilay, Regina, Noe?mie Elhadad, and
Kathy McKeown. 2001. Sentence ordering
in multidocument summarization. In
Proceedings of the Human Language
Technology Conference.
Barzilay, Regina, Kathleen McKeown, and
Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, College Park,
MD, 20?26 June, pages 550?557.
Baxendale, P. B. 1958. Man-made index for
technical literature?An experiment. IBM
Journal of Research and Development,
2(4):354?361.
Borko, H. and C. Bernier. 1975. Abstracting
Concepts and Methods. Academic Press,
New York.
Brandow, Ron, Karl Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675?685.
Buckley, Chris and Claire Cardie. 1997.
Using empire and smart for
high-precision IR and summarization. In
Proceedings of the TIPSTER Text Phase III
12-Month Workshop, San Diego, CA,
October.
Carbonell, Jaime, Y. Geng, and Jade
Goldstein. 1997. Automated
query-relevant summarization and
diversity-based reranking. In Proceedings
of the IJCAI-97 Workshop on AI in Digital
Libraries, pages 12?19.
Carbonell, Jaime G. and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Alistair Moffat
and Justin Zobel, editors, Proceedings of the
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Melbourne,
Australia, pages 335?336.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Conroy, John and Dianne O?Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of the 24th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 406?407.
Cremmins, Edward T. 1996. The Art of
Abstracting. Information Resources Press,
Arlington, VA, second edition.
DeJong, Gerald Francis. 1978. Fast Skimming
of News Stories: The FRUMP System. Ph.D.
thesis, Yale University, New Haven, CT.
Donaway, R. L., K. W. Drummey, and
L. A. Mather. 2000. A comparison of
rankings produced by summarization
evaluation measures. In Proceedings of the
Workshop on Automatic Summarization,
ANLP-NAACL2000, Association for
Computational Linguistics, 30 April,
pages 69?78.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Firmin, T. and M. J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 325?336.
Goldstein, Jade, Mark Kantrowitz, Vibhu O.
Mittal, and Jaime G. Carbonell. 1999.
Summarizing text documents: Sentence
selection and evaluation metrics. In
Research and Development in Information
Retrieval, pages 121?128, Berkeley, CA.
Hahn, Udo and Donna Harman, editors.
2002. Proceedings of the Document
Understanding Conference (DUC-02).
Philadelphia, July.
Hahn, Udo and Ulrich Reimer. 1997.
Knowledge-based text summarization:
Salience and generalization operators for
knowledge base abstraction. In I. Mani
and M. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 215?232.
Harman, Donna and Daniel Marcu, editors.
2001. Proceedings of the Document
Understanding Conference (DUC-01). New
Orleans, September.
Hovy, E. and C.-Y. Lin. 1999. Automated
text summarization in SUMMARIST. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 81?94.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4), 527?543.
Jing, Hongyan and Kathleen McKeown.
1999. The decomposition of
human-written summary sentences. In
407
Radev, Hovy, and McKeown Summarization: Introduction
M. Hearst, F. Gey, and R. Tong, editors,
Proceedings of SIGIR?99: 22nd International
Conference on Research and Development in
Information Retrieval, University of
California, Berkeley, August,
pages 129?136.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium, Stanford, CA,
23?25 March. Technical Report SS-98-06.
AAAI Press, pages 60?68.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703?710.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Research and
Development in Information Retrieval,
pages 68?73.
Lin, C. and E. Hovy. 1997. Identifying topics
by position. In Fifth Conference on Applied
Natural Language Processing, Association
for Computational Linguistics, 31
March?3 April, pages 283?290.
Lin, Chin-Yew. 1999. Training a selection
function for extraction. In Proceedings of
the Eighteenth Annual International ACM
Conference on Information and Knowledge
Management (CIKM), Kansas City, 6
November. ACM, pages 55?62.
Lin, Chin-Yew. 2001. Summary evaluation
environment.
http://www.isi.edu/cyl/SEE.
Lin, Chin-Yew and Eduard Hovy. 2002a.
From single to multi-document
summarization: A prototype system and
its evaluation. In Proceedings of the 40th
Conference of the Association of
Computational Linguistics, Philadelphia,
July, pages 457?464.
Lin, Chin-Yew and Eduard Hovy. 2002b.
Manual and automatic evaluation of
summaries. In Proceedings of the Document
Understanding Conference (DUC-02)
Workshop on Multi-Document Summarization
Evaluation at the ACL Conference,
Philadelphia, July, pages 45?51.
Luhn, H. P. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
Development, 2(2):159?165.
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Eric Bloedorn. 1997.
Multi-document summarization by graph
search and matching. In Proceedings of the
Fourteenth National Conference on Artificial
Intelligence (AAAI-97), Providence, RI.
American Association for Artificial
Intelligence, pages 622?628.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL 99), College
Park, MD, June, pages 558?565.
Mani, Inderjeet, David House, G. Klein,
Lynette Hirshman, Leo Obrst, The?re`se
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report MTR 98W0000138, The Mitre
Corporation, McLean, VA.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge.
Marcu, Daniel. 1997a. From discourse
structures to text summaries. In
Proceedings of the ACL?97/EACL?97 Workshop
on Intelligent Scalable Text Summarization,
Madrid, July 11, pages 82?88.
Marcu, Daniel. 1997b. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto, Toronto.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In M. Hearst,
F. Gey, and R. Tong, editors, Proceedings of
SIGIR?99: 22nd International Conference on
Research and Development in Information
Retrieval, University of California,
Berkeley, August, pages 137?144.
Marcu, Daniel and Laurie Gerber. 2001. An
inquiry into the nature of multidocument
abstracts, extracts, and their evaluation. In
Proceedings of the NAACL-2001 Workshop on
Automatic Summarization, Pittsburgh, June.
NAACL, pages 1?8.
McKeown, Kathleen, Judith Klavans,
Vasileios Hatzivassiloglou, Regina
Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization
by reformulation: Progress and prospects.
In Proceedings of the 16th National
Conference of the American Association for
Artificial Intelligence (AAAI-1999), 18?22
July, pages 453?460.
McKeown, Kathleen R. and Dragomir R.
Radev. 1995. Generating summaries of
multiple news articles. In Proceedings of the
18th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Seattle, July,
pages 74?82.
Ono, K., K. Sumita, and S. Miike. 1994.
408
Computational Linguistics Volume 28, Number 4
Abstract generation based on rhetorical
structure extraction. In Proceedings of the
International Conference on Computational
Linguistics, Kyoto, Japan, pages 344?348.
Otterbacher, Jahna, Dragomir R. Radev, and
Airong Luo. 2002. Revisions that improve
cohesion in multi-document summaries:
A preliminary study. In ACL Workshop on
Text Summarization, Philadelphia.
Papineni, K., S. Roukos, T. Ward, and W-J.
Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation.
Research Report RC22176, IBM.
Radev, Dragomir, Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Arda
C?elebi, Hong Qi, Elliott Drabek, and
Danyu Liu. 2002. Evaluation of text
summarization in a cross-lingual
information retrieval framework.
Technical Report, Center for Language
and Speech Processing, Johns Hopkins
University, Baltimore, June.
Radev, Dragomir R., Hongyan Jing, and
Malgorzata Budzikowska. 2000.
Centroid-based summarization of
multiple documents: Sentence extraction,
utility-based evaluation, and user studies.
In ANLP/NAACL Workshop on
Summarization, Seattle, April.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rau, Lisa and Paul Jacobs. 1991. Creating
segmented databases from free text for
text retrieval. In Proceedings of the 14th
Annual International ACM-SIGIR Conference
on Research and Development in Information
Retrieval, New York, pages 337?346.
Saggion, Horacio and Guy Lapalme. 2002.
Generating indicative-informative
summaries with SumUM. Computational
Linguistics, 28(4), 497?526.
Salton, G., A. Singhal, M. Mitra, and
C. Buckley. 1997. Automatic text
structuring and summarization.
Information Processing & Management,
33(2):193?207.
Silber, H. Gregory and Kathleen McCoy.
2002. Efficiently computed lexical chains
as an intermediate representation for
automatic text summarization.
Computational Linguistics, 28(4), 487?496.
Sparck Jones, Karen. 1999. Automatic
summarizing: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1?13.
Strzalkowski, Tomek, Gees Stein, J. Wang,
and Bowden Wise. 1999. A robust
practical text summarizer. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 137?154.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4),
409?445.
White, Michael and Claire Cardie. 2002.
Selecting sentences for multidocument
summaries using randomized local
search. In Proceedings of the Workshop on
Automatic Summarization (including DUC
2002), Philadelphia, July. Association for
Computational Linguistics, New
Brunswick, NJ, pages 9?18.
Witbrock, Michael and Vibhu Mittal. 1999.
Ultra-summarization: A statistical
approach to generating highly condensed
non-extractive summaries. In Proceedings
of the 22nd Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Berkeley,
pages 315?316.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4), 447?485.
A Smorgasbord of Features for Statistical Machine Translation
Franz Josef Och
USC/ISI
Daniel Gildea
U. of Rochester
Sanjeev Khudanpur
Johns Hopkins U.
Anoop Sarkar
Simon Fraser U.
Kenji Yamada
Xerox/XRCE
Alex Fraser
USC/ISI
Shankar Kumar
Johns Hopkins U.
Libin Shen
U. of Pennsylvania
David Smith
Johns Hopkins U.
Katherine Eng
Stanford U.
Viren Jain
U. of Pennsylvania
Zhen Jin
Mt. Holyoke
Dragomir Radev
U. of Michigan
Abstract
We describe a methodology for rapid exper-
imentation in statistical machine translation
which we use to add a large number of features
to a baseline system exploiting features from a
wide range of levels of syntactic representation.
Feature values were combined in a log-linear
model to select the highest scoring candidate
translation from an n-best list. Feature weights
were optimized directly against the BLEU eval-
uation metric on held-out data. We present re-
sults for a small selection of features at each
level of syntactic representation.
1 Introduction
Despite the enormous progress in machine translation
(MT) due to the use of statistical techniques in recent
years, state-of-the-art statistical systems often produce
translations with obvious errors. Grammatical errors in-
clude lack of a main verb, wrong word order, and wrong
choice of function words. Frequent problems of a less
grammatical nature include missing content words and
incorrect punctuation.
In this paper, we attempt to address these problems by
exploring a variety of new features for scoring candidate
translations. A high-quality statistical translation system
is our baseline, and we add new features to the exist-
ing set, which are then combined in a log-linear model.
To allow an easy integration of new features, the base-
line system provides an n-best list of candidate transla-
tions which is then reranked using the new features. This
framework allows us to incorporate different types of fea-
tures, including features based on syntactic analyses of
the source and target sentences, which we hope will ad-
dress the grammaticality of the translations, as well as
lower-level features. As we work on n-best lists, we can
easily use global sentence-level features.
We begin by describing our baseline system and the
n-best rescoring framework within which we conducted
our experiments. We then present a selection of new fea-
tures, progressing from word-level features to those based
to part-of-speech tags and syntactic chunks, and then to
features based on Treebank-based syntactic parses of the
source and target sentences.
2 Log-linear Models for Statistical MT
The goal is the translation of a text given in some source
language into a target language. We are given a source
(?Chinese?) sentence f = fJ1 = f1, . . . , fj , . . . , fJ ,
which is to be translated into a target (?English?) sentence
e = eI1 = e1, . . . , ei, . . . , eI Among all possible target
sentences, we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|f
J
1 )} (1)
As an alternative to the often used source-channel ap-
proach (Brown et al, 1993), we directly model the pos-
terior probability Pr(eI1|fJ1 ) (Och and Ney, 2002) us-
ing a log-linear combination of feature functions. In
this framework, we have a set of M feature functions
hm(eI1, f
J
1 ),m = 1, . . . ,M . For each feature function,
there exists a model parameter ?m,m = 1, . . . ,M . The
direct translation probability is given by:
Pr(eI1|f
J
1 ) =
exp[
?M
m=1 ?mhm(e
I
1, f
J
1 )]
?
e?I1
exp[
?M
m=1 ?mhm(e
?I
1, f
J
1 )]
(2)
We obtain the following decision rule:
e?I1 = argmax
eI1
{ M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
The standard criterion for training such a log-linear
model is to maximize the probability of the parallel train-
ing corpus consisting of S sentence pairs {(fs, es) : s =
1, . . . , S}. However, this does not guarantee optimal per-
formance on the metric of translation quality by which
our system will ultimately be evaluated. For this reason,
we optimize the parameters directly against the BLEU
metric on held-out data. This is a more difficult optimiza-
tion problem, as the search space is no longer convex.
Figure 1: Example segmentation of Chinese sentence and
its English translation into alignment templates.
However, certain properties of the BLEU metric can be
exploited to speed up search, as described in detail by
Och (2003). We use this method of optimizing feature
weights throughout this paper.
2.1 Baseline MT System: Alignment Templates
Our baseline MT system is the alignment template system
described in detail by Och, Tillmann, and Ney (1999)
and Och and Ney (2004). In the following, we give a
short description of this baseline model.
The probability model of the alignment template sys-
tem for translating a sentence can be thought of in distinct
stages. First, the source sentence words fJ1 are grouped to
phrases f?K1 . For each phrase f? an alignment template z is
chosen and the sequence of chosen alignment templates
is reordered (according to piK1 ). Then, every phrase f?
produces its translation e? (using the corresponding align-
ment template z). Finally, the sequence of phrases e?K1
constitutes the sequence of words eI1.
Our baseline system incorporated the following feature
functions:
Alignment Template Selection Each alignment
template is chosen with probability p(z|f?), estimated by
relative frequency. The corresponding feature function in
our log-linear model is the log probability of the product
of p(z|f?) for all used alignment templates used.
Word Selection This feature is based on the lexical
translation probabilities p(e|f), estimated using relative
frequencies according to the highest-probability word-
level alignment for each training sentence. A translation
probability conditioned on the source and target position
within the alignment template p(e|f, i, j) is interpolated
with the position-independent probability p(e|f).
Phrase Alignment This feature favors monotonic
alignment at the phrase level. It measures the ?amount
of non-monotonicity? by summing over the distance (in
the source language) of alignment templates which are
consecutive in the target language.
Language Model Features As a language model
feature, we use a standard backing off word-based tri-
gram language model (Ney, Generet, and Wessel, 1995).
The baseline system actually includes four different lan-
guage model features trained on four different corpora:
the news part of the bilingual training data, a large Xin-
hua news corpus, a large AFP news corpus, and a set of
Chinese news texts downloaded from the web.
Word/Phrase Penalty This word penalty feature
counts the length in words of the target sentence. Without
this feature, the sentences produced tend to be too short.
The phrase penalty feature counts the number of phrases
produced, and can allow the model to prefer either short
or long phrases.
Phrases from Conventional Lexicon The baseline
alignment template system makes use of the Chinese-
English lexicon provided by LDC. Each lexicon entry is
a potential phrase translation pair in the alignment tem-
plate system. To score the use of these lexicon entries
(which have no normal translation probability), this fea-
ture function counts the number of times such a lexicon
entry is used.
Additional Features A major advantage of the log-
linear modeling approach is that it is easy to add new
features. In this paper, we explore a variety of features
based on successively deeper syntactic representations of
the source and target sentences, and their alignment. For
each of the new features discussed below, we added the
feature value to the set of baseline features, re-estimated
feature weights on development data, and obtained re-
sults on test data.
3 Experimental Framework
We worked with the Chinese-English data from the recent
evaluations, as both large amounts of sentence-aligned
training corpora and multiple gold standard reference
translations are available. This is a standard data set,
making it possible to compare results with other systems.
In addition, working on Chinese allows us to use the ex-
isting Chinese syntactic treebank and parsers based on it.
For the baseline MT system, we distinguish the fol-
lowing three different sentence- or chunk-aligned parallel
training corpora:
? training corpus (train): This is the basic training
corpus used to train the alignment template transla-
tion model (word lexicon and phrase lexicon). This
corpus consists of about 170M English words. Large
parts of this corpus are aligned on a sub-sentence
level to avoid the existence of very long sentences
which would be filtered out in the training process
to allow a manageable word alignment training.
? development corpus (dev): This is the training cor-
pus used in discriminative training of the model-
parameters of the log-linear translation model. In
most experiments described in this report this cor-
pus consists of 993 sentences (about 25K words) in
both languages.
? test corpus (test): This is the test corpus used to
assess the quality of the newly developed feature
functions. It consists of 878 sentences (about 25K
words).
For development and test data, we have four English (ref-
erence) translations for each Chinese sentence.
3.1 Reranking, n-best lists, and oracles
For each sentence in the development, test, and the blind
test corpus a set of 16,384 different alternative transla-
tions has been produced using the baseline system. For
extracting the n-best candidate translations, an A* search
is used. These n-best candidate translations are the basis
for discriminative training of the model parameters and
for re-ranking.
We used n-best reranking rather than implementing
new search algorithms. The development of efficient
search algorithms for long-range dependencies is very
complicated and a research topic in itself. The rerank-
ing strategy enabled us to quickly try out a lot of new
dependencies, which would not have been be possible if
the search algorithm had to be changed for each new de-
pendency.
On the other hand, the use of n-best list rescoring lim-
its the possibility of improvements to what is available
in the n-best list. Hence, it is important to analyze the
quality of the n-best lists by determining how much of an
improvement would be possible given a perfect reranking
algorithm. We computed the oracle translations, that is,
the set of translations from our n-best list that yields the
best BLEU score.1
We use the following two methods to compute the
BLEU score of an oracle translation:
1. optimal oracle (opt): We select the oracle sentences
which give the highest BLEU score compared to the
set of 4 reference translations. Then, we compute
BLEU score of oracle sentences using the same set
of reference translations.
2. round-robin oracle (rr): We select four differ-
ent sets of oracle sentences which give the highest
BLEU score compared to each of the 4 references
translations. Then, we compute for each set of or-
acle sentences a BLEU score using always those
three references to score that have not been cho-
sen to select the oracle. Then, these 4 3-reference
BLEU scores are averaged.
1Note that due to the corpus-level holistic nature of the
BLEU score it is not trivial to compute the optimal set of oracle
translations. We use a greedy search algorithm for the oracle
translations that might find only a local optimum. Empirically,
we do not observe a dependence on the starting point, hence we
believe that this does not pose a significant problem.
Table 1: Oracle BLEU scores for different sizes of the
n-best list. The avBLEUr3 scores are computed with
respect to three reference translations averaged over the
four different choices of holding out one reference.
avBLEUr3[%] BLEUr4
n rr opt opt
human 35.8 -
1 28.3 28.3 31.6
4 29.1 30.8 34.5
16 29.9 33.2 37.3
64 30.6 35.6 38.7
256 31.3 37.8 42.8
1024 31.7 40.0 45.3
4096 32.0 41.8 47.3
The first method provides the theoretical upper bound of
what BLEU score can be obtained by rescoring a given n-
best list. Using this method with a 1000-best list, we ob-
tain oracle translations that outperform the BLEU score
of the human translations. The oracle translations achieve
113% against the human BLEU score on the test data
(Table 1), while the first best translations obtain 79.2%
against the human BLEU score. The second method uses
a different references for selection and scoring. Here, us-
ing an 1000-best list, we obtain oracle translations with a
relative human BLEU score of 88.5%.
Based on the results of the oracle experiment, and
in order to make rescoring computationally feasible for
features requiring significant computation for each hy-
pothesis, we used the top 1000 translation candidates for
our experiments. The baseline system?s BLEU score is
31.6% on the test set (equivalent to the 1-best oracle in
Table 1). This is the benchmark against which the contri-
butions of the additional features described in the remain-
der of this paper are to be judged.
3.2 Preprocessing
As a precursor to developing the various syntactic fea-
tures described in this report, the syntactic represen-
tations on which they are based needed to be com-
puted. This involved part-of-speech tagging, chunking,
and parsing both the Chinese and English side of our
training, development, and test sets.
Applying the part-of-speech tagger to the often un-
grammatical MT output from our n-best lists sometimes
led to unexpected results. Often the tagger tries to ?fix
up? ungrammatical sentences, for example by looking for
a verb when none is present:
China NNP 14 CD open JJ border NN
cities NNS achievements VBZ remarkable JJ
Here, although achievements has never been seen as a
verb in the tagger?s training data, the prior for a verb
in this position is high enough to cause a present tense
verb tag to be produced. In addition to the inaccura-
cies of the MT system, the difference in genre from the
tagger?s training text can cause problems. For example,
while our MT data include news article headlines with no
verb, headlines are not included in the Wall Street Journal
text on which the tagger is trained. Similarly, the tagger
is trained on full sentences with normalized punctuation,
leading it to expect punctuation at the end of every sen-
tence, and produce a punctuation tag even when the evi-
dence does not support it:
China NNP ?s POS economic JJ
development NN and CC opening VBG
up RP 14 CD border NN cities NNS
remarkable JJ achievements .
The same issues affect the parser. For example the
parser can create verb phrases where none exist, as in the
following example in which the tagger correctly did not
identify a verb in the sentence:
These effects have serious implications for designing
syntactic feature functions. Features such ?is there a verb
phrase? may not do what you expect. One solution would
be features that involve the probability of a parse subtree
or tag sequence, allowing us to ask ?how good a verb
phrase is it?? Another solution is more detailed features
examining more of the structure, such as ?is there a verb
phrase with a verb??
4 Word-Level Feature Functions
These features, directly based on the source and target
strings of words, are intended to address such problems as
translation choice, missing content words, and incorrect
punctuation.
4.1 Model 1 Score
We used IBM Model 1 (Brown et al, 1993) as one of the
feature functions. Since Model 1 is a bag-of-word trans-
lation model and it gives the sum of all possible alignment
probabilities, a lexical co-occurrence effect, or triggering
effect, is expected. This captures a sort of topic or seman-
tic coherence in translations.
As defined by Brown et al (1993), Model 1 gives a
probability of any given translation pair, which is
p(f |e; M1) =

(l + 1)m
m?
j=1
l?
i=0
t(fj |ei).
We used GIZA++ to train the model. The training data is
a subset (30 million words on the English side) of the en-
tire corpus that was used to train the baseline MT system.
For a missing translation word pair or unknown words,
where t(fj |ei) = 0 according to the model, a constant
t(fj |ei) = 10?40 was used as a smoothing value.
The average %BLEU score (average of the best four
among different 20 search initial points) is 32.5. We also
tried p(e|f ; M1) as feature function, but did not obtain
improvements which might be due to an overlap with the
word selection feature in the baseline system.
The Model 1 score is one of the best performing fea-
tures. It seems to ?fix? the tendency of our baseline sys-
tem to delete content words and it improves word selec-
tion coherence by the triggering effect. It is also possible
that the triggering effect might work on selecting a proper
verb-noun combination, or a verb-preposition combina-
tion.
4.2 Lexical Re-ordering of Alignment Templates
As shown in Figure 1 the alignment templates (ATs)
used in the baseline system can appear in various con-
figurations which we will call left/right-monotone and
left/right-continuous. We built 2 out of these 4 models to
distinguish two types of lexicalized re-ordering of these
ATs:
The left-monotone model computes the total proba-
bility of all ATs being left monotone: where the lower
left corner of the AT touches the upper right corner of the
previous AT. Note that the first word in the current AT
may or may not immediately follow the last word in the
previous AT. The total probability is the product over all
alignment templates i, either P (ATi is left-monotone) or
1 ? P (ATi is left-monotone).
The right-continuous model computes the total prob-
ability of all ATs being right continuous: where the
lower left corner of the AT touches the upper right cor-
ner of the previous AT and the first word in the cur-
rent AT immediately follows the last word in the pre-
vious AT. The total probability is the product over all
alignment templates i, either P (ATi is right-continuous)
or 1 ? P (ATi is right-continuous).
In both models, the probabilities P have been esti-
mated from the full training data (train).
5 Shallow Syntactic Feature Functions
By shallow syntax, we mean the output of the part-of-
speech tagger and chunkers. We hope that such features
can combine the strengths of tag- and chunk-based trans-
lation systems (Schafer and Yarowsky, 2003) with our
baseline system.
5.1 Projected POS Language Model
This feature uses Chinese POS tag sequences as surro-
gates for Chinese words to model movement. Chinese
words are too sparse to model movement, but an attempt
to model movement using Chinese POS may be more
successful. We hope that this feature will compensate for
a weak model of word movement in the baseline system.
Chinese POS sequences are projected to English us-
ing the word alignment. Relative positions are indicated
for each Chinese tag. The feature function was also tried
without the relative positions:
CD +0 M +1 NN +3 NN -1 NN +2 NN +3
14 (measure) open border cities
The table shows an example tagging of an English hy-
pothesis showing how it was generated from the Chinese
sentence. The feature function is the log probability out-
put by a trigram language model over this sequence. This
is similar to the HMM Alignment model (Vogel, Ney, and
Tillmann, 1996) but in this case movement is calculated
on the basis of parts of speech.
The Projected POS feature function was one of the
strongest performing shallow syntactic feature functions,
with a %BLEU score of 31.8. This feature function can
be thought of as a trade-off between purely word-based
models, and full generative models based upon shallow
syntax.
6 Tree-Based Feature Functions
Syntax-based MT has shown promise in the
work of, among others, Wu and Wong (1998) and
Alshawi, Bangalore, and Douglas (2000). We hope that
adding features based on Treebank-based syntactic
analyses of the source and target sentences will address
grammatical errors in the output of the baseline system.
6.1 Parse Tree Probability
The most straightforward way to integrate a statistical
parser in the system would be the use of the (log of the)
parser probability as a feature function. Unfortunately,
this feature function did not help to obtain better results
(it actually seems to significantly hurt performance).
To analyze the reason for this, we performed an ex-
periment to test if the used statistical parser assigns a
higher probability to presumably grammatical sentences.
The following table shows the average log probability as-
signed by the Collins parser to the 1-best (produced), or-
acle and the reference translations:
Hypothesis 1-best Oracle Reference
log(parseProb) -147.2 -148.5 -154.9
We observe that the average parser log-probability of
the 1-best translation is higher than the average parse
log probability of the oracle or the reference translations.
Hence, it turns out that the parser is actually assigning
higher probabilities to the ungrammatical MT output than
to the presumably grammatical human translations. One
reason for that is that the MT output uses fewer unseen
words and typically more frequent words which lead to
a higher language model probability. We also performed
experiments to balance this effect by dividing the parser
probability by the word unigram probability and using
this ?normalized parser probability? as a feature function,
but also this did not yield improvements.
6.2 Tree-to-String Alignment
A tree-to-string model is one of several syntax-
based translation models used. The model is a
conditional probability p(f |T (e)). Here, we used
a model defined by Yamada and Knight (2001) and
Yamada and Knight (2002).
Internally, the model performs three types of opera-
tions on each node of a parse tree. First, it reorders the
child nodes, such as changing VP ? VB NP PP into
VP ? NP PP VB. Second, it inserts an optional word at
each node. Third, it translates the leaf English words into
Chinese words. These operations are stochastic and their
probabilities are assumed to depend only on the node, and
are independent of other operations on the node, or other
nodes. The probability of each operation is automatically
obtained by a training algorithm, using about 780,000 En-
glish parse tree-Chinese sentence pairs. The probability
of these operations ?(eki,j) is assumed to depend on the
edge of the tree being modified, eki,j , but independent of
everything else, giving the following equation,
p(f |T (e)) =
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j) (4)
where ? varies over the possible alignments between the
f and e and ?(eki,j) is the particular operations (in ?) for
the edge eki,j .
The model is further extended to incorporate phrasal
translations performed at each node of the input parse
tree (Yamada and Knight, 2002). An English phrase cov-
ered by a node can be directly translated into a Chinese
phrase without regular reorderings, insertions, and leaf-
word translations.
The model was trained using about 780,000 English
parse tree-Chinese sentence pairs. There are about 3 mil-
lion words on the English side, and they were parsed by
Collins? parser.
Since the model is computationally expensive, we
added some limitations on the model operations. As the
base MT system does not produce a translation with a
big word jump, we restrict the model not to reorder child
nodes when the node covers more than seven words. For
a node that has more than four children, the reordering
probability is set to be uniform. We also introduced prun-
ing, which discards partial (subtree-substring) alignments
if the probability is lower than a threshold.
The model gives a sum of all possible alignment prob-
abilities for a pair of a Chinese sentence and an English
parse tree. We also calculate the probability of the best
alignment according to the model. Thus, we have the fol-
lowing two feature functions:
hTreeToStringSum(e, f) = log(
?
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
hTreeToStringViterbi(e, f) = log(max
?
?
?(eki,j)
p(?(eki,j)|e
k
i,j))
As the model is computationally expensive, we sorted the
n-best list by the sentence length, and processed them
from the shorter ones to the longer ones. We used 10
CPUs for about five days, and 273/997 development sen-
tences and 237/878 test sentences were processed.
The average %BLEU score (average of the best four
among different 20 search initial points) was 31.7 for
both hTreeToStringSum and hTreeToStringViterbi. Among the pro-
cessed development sentences, the model preferred the
oracle sentences over the produced sentence in 61% of
the cases.
The biggest problem of this model is that it is compu-
tationally very expensive. It processed less than 30% of
the n-best lists in long CPU hours. In addition, we pro-
cessed short sentences only. For long sentences, it is not
practical to use this model as it is.
6.3 Tree-to-Tree Alignment
A tree-to-tree translation model makes use of syntac-
tic tree for both the source and target language. As in
the tree-to-string model, a set of operations apply, each
with some probability, to transform one tree into another.
However, when training the model, trees for both the
source and target languages are provided, in our case
from the Chinese and English parsers.
We began with the tree-to-tree alignment model pre-
sented by Gildea (2003). The model was extended to han-
dle dependency trees, and to make use of the word-level
alignments produced by the baseline MT system. The
probability assigned by the tree-to-tree alignment model,
given the word-level alignment with which the candidate
translation was generated, was used as a feature in our
rescoring system.
We trained the parameters of the tree transformation
operations on 42,000 sentence pairs of parallel Chinese-
English data from the Foreign Broadcast Information Ser-
vice (FBIS) corpus. The lexical translation probabili-
ties Pt were trained using IBM Model 1 on the 30 mil-
lion word training corpus. This was done to overcome
the sparseness of the lexical translation probabilities es-
timated while training the tree-to-tree model, which was
not able to make use of as much training data.
As a test of the tree-to-tree model?s discrimination, we
performed an oracle experiment, comparing the model
scores on the first sentence in the n-best list with candi-
date giving highest BLEU score. On the 1000-best list for
the 993-sentence development set, restricting ourselves
to sentences with no more than 60 words and a branching
factor of no more than five in either the Chinese or En-
glish tree, we achieved results for 480, or 48% of the 993
sentences. Of these 480, the model preferred the pro-
duced over the oracle 52% of the time, indicating that
it does not in fact seem likely to significantly improve
BLEU scores when used for reranking. Using the prob-
ability of the source Chinese dependency parse aligning
with the n-best hypothesis dependency parse as a feature
function, making use of the word-level alignments, yields
a 31.6 %BLEU score ? identical to our baseline.
6.4 Markov Assumption for Tree Alignments
The tree-based feature functions described so far have the
following limitations: full parse tree models are expen-
sive to compute for long sentences and for trees with flat
constituents and there is limited reordering observed in
the n-best lists that form the basis of our experiments. In
addition to this, higher levels of parse tree are rarely ob-
served to be reordered between source and target parse
trees.
In this section we attack these problems using a simple
Markov model for tree-based alignments. It guarantees
tractability: compared to a coverage of approximately
30% of the n-best list by the unconstrained tree-based
models, using the Markov model approach provides 98%
coverage of the n-best list. In addition, this approach is
robust to inaccurate parse trees.
The algorithm works as follows: we start with word
alignments and two parameters: n for maximum number
of words in tree fragment and k for maximum height of
tree fragment. We proceed from left to right in the Chi-
nese sentence and incrementally grow a pair of subtrees,
one subtree in Chinese and the other in English, such that
each word in the Chinese subtree is aligned to a word in
the English subtree. We grow this pair of subtrees un-
til we can no longer grow either subtree without violat-
ing the two parameter values n and k. Note that these
aligned subtree pairs have properties similar to alignment
templates. They can rearrange in complex ways between
source and target. Figure 2 shows how subtree-pairs for
parameters n = 3 and k = 3 can be drawn for this
sentence pair. In our experiments, we use substantially
bigger tree fragments with parameters set to n = 8 and
k = 9.
Once these subtree-pairs have been obtained, we can
easily assert a Markov assumption for the tree-to-tree and
tree-to-string translation models that exploits these pair-
ings. Let consider a sentence pair in which we have dis-
covered n subtree-pairs which we can call Frag0, . . .,
Fragn. We can then compute a feature function for the
sentence pair using the tree-to-string translation model as
follows:
hMarkovTreeToString =
logPtree-to-string(Frag0) + . . . + logPtree-to-string(Fragn)
Using this Markov assumption on tree alignments with
Figure 2: Markov assumption on tree alignments.
the Tree to String model described in Section 6.2 we ob-
tain a coverage improvement to 98% coverage from the
original 30%. The accuracy of the tree to string model
also improved with a %BLEU score of 32.0 which is the
best performing single syntactic feature.
6.5 Using TAG elementary trees for scoring word
alignments
In this section, we consider another method for carving
up the full parse tree. However, in this method, instead of
subtree-pairs we consider a decomposition of parse trees
that provides each word with a fragment of the original
parse tree as shown in Figure 3. The formalism of Tree-
Adjoining Grammar (TAG) provides the definition what
each tree fragment should be and in addition how to de-
compose the original parse trees to provide the fragments.
Each fragment is a TAG elementary tree and the compo-
sition of these TAG elementary trees in a TAG deriva-
tion tree provides the decomposition of the parse trees.
The decomposition into TAG elementary trees is done by
augmenting the parse tree for source and target sentence
with head-word and argument (or complement) informa-
tion using heuristics that are common to most contempo-
rary statistical parsers and easily available for both En-
glish and Chinese. Note that we do not use the word
alignment information for the decomposition into TAG
elementary trees.
Once we have a TAG elementary tree per word,
we can create several models that score word align-
ments by exploiting the alignments between TAG ele-
mentary trees between source and target. Let tfi and
tei be the TAG elementary trees associated with the
aligned words fi and ei respectively. We experimented
with two models over alignments: unigram model over
alignments:
?
i P (fi, tfi , ei, tei) and conditional model:?
i P (ei, tei | fi, tfi) ? P (fi+1, tfi+1 | fi, tfi)
We trained both of these models using the SRI Lan-
guage Modeling Toolkit using 60K aligned parse trees.
We extracted 1300 TAG elementary trees each for Chi-
Figure 3: Word alignments with TAG elementary trees.
nese and for English. The unigram model gets a %BLEU
score of 31.7 and the conditional model gets a %BLEU
score of 31.9.
%BLEU
Baseline 31.6
IBM Model 1 p(f |e) 32.5
Tree-to-String Markov fragments 32.0
Right-continuous alignment template 32.0
TAG conditional bigrams 31.9
Left-monotone alignment template 31.9
Projected POS LM 31.8
Tree-to-String 31.7
TAG unigram 31.7
Tree-to-Tree 31.6
combination 32.9
Table 2: Results for the baseline features, each new fea-
ture added to the baseline features on its own, and a com-
bination of new features.
7 Conclusions
The use of discriminative reranking of an n-best list pro-
duced with a state-of-the-art statistical MT system al-
lowed us to rapidly evaluate the benefits of off-the-shelf
parsers, chunkers, and POS taggers for improving syntac-
tic well-formedness of the MT output. Results are sum-
marized in Table 2; the best single new feature improved
the %BLEU score from 31.6 to 32.5. The 95% confi-
dence intervals computed with the bootstrap resampling
method are about 0.8%. In addition to experiments with
single features we also integrated multiple features using
a greedy approach where we integrated at each step the
feature that most improves the BLEU score. This feature
integration produced a statistically significant improve-
ment of absolute 1.3% to 32.9 %BLEU score.
Our single best feature, and in fact the only single fea-
ture to produce a truly significant improvement, was the
IBM Model 1 score. We attribute its success that it ad-
dresses the weakness of the baseline system to omit con-
tent words and that it improves word selection by em-
ploying a triggering effect. We hypothesize that this al-
lows for better use of context in, for example, choosing
among senses of the source language word.
A major goal of this work was to find out if we can ex-
ploit annotated data such as treebanks for Chinese and
English and make use of state-of-the-art deep or shal-
low parsers to improve MT quality. Unfortunately, none
of the implemented syntactic features achieved a statisti-
cally significant improvement in the BLEU score. Poten-
tial reasons for this might be:
? As described in Section 3.2, the use of off-the-shelf
taggers and parsers has various problems due to vari-
ous mismatches between the parser training data and
our application domain. This might explain that the
use of the parser probability as feature function was
not successful. A potential improvement might be to
adapt the parser by retraining it on the full training
data that has been used by the baseline system.
? The use of a 1000-best list limits the potential im-
provements. It is possible that more improvements
could be obtained using a larger n-best list or a word
graph representation of the candidates.
? The BLEU score is possibly not sufficiently sensi-
tive to the grammaticality of MT output. This could
not only make it difficult to see an improvement in
the system?s output, but also potentially mislead the
BLEU-based optimization of the feature weights. A
significantly larger corpus for discriminative train-
ing and for evaluation would yield much smaller
confidence intervals.
? Our discriminative training technique, which di-
rectly optimizes the BLEU score on a development
corpus, seems to have overfitting problems with
large number of features. One could use a larger de-
velopment corpus for discriminative training or in-
vestigate alternative discriminative training criteria.
? The amount of annotated data that has been used
to train the taggers and parsers is two orders of
magnitude smaller than the parallel training data
that has been used to train the baseline system (or
the word-based features). Possibly, a comparable
amount of annotated data (e.g. a treebank with 100
million words) is needed to obtain significant im-
provements.
This is the first large scale integration of syntactic analy-
sis operating on many different levels with a state-of-the-
art phrase-based MT system. The methodology of using
a log-linear feature combination approach, discriminative
reranking of n-best lists computed with a state-of-the-art
baseline system allowed members of a large team to si-
multaneously experiment with hundreds of syntactic fea-
ture functions on a common platform.
Acknowledgments
This material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0121285.
References
Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 2000.
Learning dependency translation models as collections of
finite state head transducers. Computational Linguistics,
26(1):45?60.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Gildea, Daniel. 2003. Loosely tree-based alignment for ma-
chine translation. In Proc. of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), Sapporo,
Japan.
Ney, Hermann, M. Generet, and Frank Wessel. 1995. Ex-
tensions of absolute discounting for language modeling. In
Proc. of the Fourth European Conf. on Speech Communica-
tion and Technology, Madrid, Spain.
Och, Franz Josef. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL), Sap-
poro, Japan.
Och, Franz Josef and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), Philadel-
phia, PA.
Och, Franz Josef and Hermann Ney. 2004. The alignment tem-
plate approach to statistical machine translation. Computa-
tional Linguistics. Accepted for Publication.
Och, Franz Josef, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical machine
translation. In Proc. of the Joint SIGDAT Conf. on Empiri-
cal Methods in Natural Language Processing and Very Large
Corpora, College Park, MD.
Schafer, Charles and David Yarowsky. 2003. Statistical ma-
chine translation using coercive two-level syntactic transduc-
tion. In Proc. of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), Philadel-
phia, PA.
Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996.
HMM-based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational Lin-
guistics, Copenhagen, Denmark.
Wu, Dekai and H. Wong. 1998. Machine translation with a
stochastic grammatical channel. In COLING-ACL ?98: 36th
Annual Meeting of the Association for Computational Lin-
guistics and 17th Int. Conf. on Computational Linguistics,
Montreal, Canada.
Yamada, Kenji and Kevin Knight. 2001. A syntax-based sta-
tistical translation model. In Proc. of the 39th Annual Meet-
ing of the Association for Computational Linguistics (ACL),
Toulouse, France.
Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-
based MT. In Proc. of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), Philadelphia,
PA.
A Scaleable Multi-document Centroid-based Summarizer
Dragomir Radev
  
, Timothy Allison

, Matthew Craig

, Stanko Dimitrov

,
Omer Kareem

, Michael Topper

, Adam Winkel

, and Jin Yi

 
School of Information

Department of Electrical Engineering and Computer Science

Department of Classical Studies
University of Michigan, Ann Arbor, MI 48109

radev,tballiso,mwcraig,sdimitro,okareem,mtopper,winkela,jyi  @umich.edu
1 Introduction
We are presenting the most recent version of MEAD (v.
3.08), a large-scale public-domain summarizer that has
been used in a number of applications, including the 2001
JHU summer workshop and the NewsInEssence project
(www.newsinessence.com). A version of MEAD
finished in first place on task 4 at DUC 2003 and finished
in the top three on two other tasks.
In this demo, we will be showing several interfaces
to MEAD, including a WAP-based cell phone interface,
a Web-based interface, a command-line interface, and a
Nutch-based interface.
1.1 Text summarization
Text summarization is the process of identifying salient
concepts in text, conceptualizing the relationships that
exist among them and generating concise representations
of the input text that preserve the gist of its content.
One distinguishes between single-document sum-
marization (SDS) and multi-document summarization
(MDS). MDS, which our approach will be focusing on,
is much more complicated than SDS in nature. Besides
the obvious difference in input size, several other factors
account for the complication, e.g.:
 Multiple articles might come from different sources,
written by different authors, and therefore have dif-
ferent styles, although they are topically related.
This means that a summarizer cannot make the same
coherence assumption that it can for a single article.
 Multiple articles might come out of different time
frames. Therefore an intelligent summarizer has to
take care of the temporal information and try to max-
imize the overall temporal cohesiveness of the sum-
mary.
 Descriptions of the same event may differ in per-
spective, or even conflict with one another. The
summarizer should provide a mechanism to deal
with issues of this kind.
We also make the distinction between information-
extraction- vs. sentence-extraction-based summarizers.
The former, such as (Radev and McKeown, 1998), rely on
an information extraction system to extract very specific
aspects of some events and generate abstracts thereof.
This approach can produce good summaries but is usu-
ally knowledge intensive and domain dependent. Sen-
tence extraction techniques (Luhn, 1958; Radev et al,
2000), on the other hand, compute a score for each sen-
tence based on certain features and output the most highly
ranked sentences. This approach is conceptually straight-
forward and usually domain independent, but the sum-
maries produced by it often need further revision to be
more smooth and coherent.
1.2 Centroid-based summarization and MEAD
Centroid-based summarization is a method of multi-
document summarization. It operates on a cluster of doc-
uments with a common subject (the cluster may be pro-
duced by a Topic Detection and Tracking, or TDT, sys-
tem). A cluster centroid, a collection of the most impor-
tant words from the whole cluster, is built. The centroid is
then used to determine which sentences from individual
documents are most representative of the entire cluster.
MEAD is a publicly available toolkit for multi-
document summarization (Radev et al, 2000; MEAD,
2003). The toolkit implements multiple summariza-
tion algorithms (at arbitrary compression rates) such as
position-based, TF*IDF, largest common subsequence,
and keywords. The methods for evaluating the quality of
the summaries are both intrinsic (such as percent agree-
ment, precision/recall, and relative utility) and extrinsic
(document rank).
MEAD has an expansive architecture which allows
end users to interface with its summarization capabilities
through a Perl and Java API.
2 Demonstration of MEAD
The Mead Demo is a web-based demonstration of
MEAD. Users are able to add multiple documents for the
MEAD toolkit to summarize and display (see Figures 1?
4).
The Demo allows users to add documents by: selecting
files from their computer, adding text in the text box or
by supplying a URL to a specified web document. Doc-
uments can be plain text, HTML files or Microsoft Word
files.
Figure 1: The user inputs a combination of files from
their hard drive, text into the text box and document
URLs.
Figure 2: After user has added all of the documents to be
summarized, they select the compression rate and then
submit for summarization.
3 Demonstration of WapMead
WapMead (Figure 5) is a WAP (Wireless Access Pro-
tocol) interface to MEAD to access IMAP-based email
Figure 3: The summary page displays the summary as
well as provides links to each document the user has sub-
mitted.
Figure 4: The summary page also allows users to select
specific documents from the summarization to be high-
lighted.
mailboxes from a cell phone or other WAP-enabled de-
vice. WapMead has two modes: a mailbox view, in which
a user can search for an email message and a summary
view, in which a summary of a message is displayed.
Summaries are displayed hierarchically, first showing the
most salient sentences in the entire message and then (on
a need basis) showing in greater detail particular areas of
the message.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
Figure 5: WapMead: (Left) mailbox view, (Middle) sum-
mary view, (Right) hierarchical view.
Figure 6: NewsInEssence (NIE) interface.
4 Other interfaces
We will be also showing the Java-MEAD interface (Fig-
ure 7) implemented in Nutch (a public-domain search
engine from www.nutch.org), an older interface, NewsI-
nEssence (Radev et al, 2001; Radev et al, 2002) (Fig-
ure 6), as well as the MEAD command-line interface.
Figure 7: Nutch interface.
5 Acknowledgements
This work is partially supported by grant ITR 0082884
from the National Science Foundation (NSF). The au-
thors would like to thank the MEAD team (Sasha Blair-
Goldensohn, Simone Teufel, Horacio Saggion, Wai Lam,
Arda C?elebi, John Blitzer, Hong Qi, Elliott Drabek, and
Danyu Liu) for their hard work on various versions of the
MEAD system.
References
H.P. Luhn. 1958. The Automatic Creation of Litera-
ture Abstracts. IBM Journal of Research Development,
2(2):159?165.
MEAD. 2003. Mead documentation. WWW site, URL:
http://www.summarization.com/mead.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):469?500, September.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based summarization
of multiple documents: sentence extraction, utility-
based evaluation, and user studies. In ANLP/NAACL
Workshop on Summarization, Seattle, WA, April.
Dragomir R. Radev, Sasha Blair-Goldensohn, Zhu
Zhang, and Revathi Sundara Raghavan. 2001. Newsi-
nessence: A system for domain-independent, real-
time news clustering and multi-document summariza-
tion. In Human Language Technology Conference,
San Diego, CA.
Dragomir R. Radev, Michael Topper, and Adam Winkel.
2002. Multi Document Centroid-based Text Summa-
rization. In ACL Demo Session, Philadelphia, PA.
Computational Linkuistics: word triggers across hyperlinks
Dragomir R. Radev
  
, Hong Qi   , Daniel Tam  , Adam Winkel 
 
School of Information and

Department of EECS
University of Michigan
Ann Arbor, MI 48109-1092

radev,hqi,dtam,winkela  @umich.edu
Abstract
It is known that context words tend to be self-
triggers, that is, the probability of a content
word to appear more than once in a document,
given that it already appears once, is signifi-
cantly higher than the probability of the first oc-
currence. We look at self-triggerability across
hyperlinks on the Web. We show that the prob-
ability of a word  to appear in a Web docu-
ment 	
 depends on the presence of  in doc-
uments pointing to 	
 . In Document Model-
ing, we will propose the use of a correction fac-
tor,  , which indicates how much more likely
a word is to appear in a document given that
another document containing the same word is
linked to it.
1 Introduction
Given the size of the Web, it is intuitively very hard to find
a given page of interest by just following links. Classic
results have shown however, that the link structure of the
Web is not random. Various models have been proposed
including power law distributions (the ?rich get richer?
model), and lexical models. In this paper, we will in-
vestigate how the presence of a given word in a given
Web document 	 
 affects the presence of the same word
in documents linked to 	 
 . We will use the term Compu-
tational Linkuistics to describe the study of hyperlinks for
Document Modeling and Information Retrieval purposes.
1.1 Link structure of the Web
Random graphs have been studied by Erdo?s and Re?nyi
(Erdo?s and Re?nyi, 1960). In a random graph, edges are
added sequentially with both vertices of a new edge cho-
sen randomly.
The diameter 	 of the Web (that is, the average number
of links from any given page to another) has been found to
be a constant (approximately ffProceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 303?304,
New York City, June 2006. c?2006 Association for Computational Linguistics
3. Graph-Based Algorithms For Natural Language Processing And Information Retrieval
Rada Mihalcea, University of North Texas, and Dragomir Radev, University of Michigan
Graph theory is a well studied discipline, and so are the fields of natural language processing and in-
formation retrieval. However, most of the times, they are perceived as different disciplines, with different
algorithms, different applications, and different potential end-users.
The goal of this tutorial is to provide an overview of methods and applications in natural language
processing and information retrieval that rely on graph-based algorithms. This will include techniques for
graph traversal, minimum path length, min-cut algorithms, minimum spanning trees, random walks, etc. and
their application to information retrieval and Web search, text understanding (word sense disambiguation
and semantic classes), parsing, text summarization, keyword extraction, text clustering, and others.
3.1 Tutorial Outline
1. Graph-based Algorithms Basics
* Vectors, matrices, graphs
* Graph representations and notations
? Traversal, min-cut/max-flow, matching
* Algorithms for graph traversal
* Minimum path length
* Minimum spanning trees
* Min-cut/max-flow algorithms
* Graph-matching algorithms
? Ranking, clustering, learning
* Eigenvector analysis
* Node-ranking algorithms
* Graph-based centrality
* Graph-based clustering
* Machine learning on graphs
2. Information Retrieval applications
* Web-page ranking
* Text classification and clustering
3. Natural language processing applications
? Semantics
* Word sense disambiguation
* Semantic classes
* Textual entailment
* Sentiment classification
? Syntax, Summarization
* Dependency parsing
* Prepositional attachment
* Keyword extraction
* Text summarization
3.2 Target Audience
This tutorial is intended for researchers and practitioners who seek a general understanding of the appli-
cation of graph-theoretical representations and algorithms to natural language processing and information
303
retrieval. It is introductory in nature, no special knowledge or background is required.
Rada Mihalcea is an Assistant Professor of Computer Science at the University of North Texas. Her research
interests are in lexical semantics, graph-based algorithms for natural language processing and information
retrieval, minimally supervised natural language learning, and multilingual natural language processing.
She has published more than 80 articles in books, journals, and proceedings, in these and related areas. She
is the president of the ACL Special Group on the Lexicon (SIGLEX), and a board member for the ACL
Special Group on Natural Language Learning (SIGNLL). She serves on the editorial board of the journal
of Computational Linguistics, the journal of Language Resources and Evaluations, and the recently estab-
lished journal of Interesting Negative Results in Natural Language Processing and Machine Learning. Her
research is supported by NSF, Google, and the state of Texas.
Dragomir Radev is an Associate Professor of Information, of Computer Science and Engineering, and of
Linguistics at the University of Michigan. He has a PhD in Computer Science from Columbia University.
He has held numerous posts within NAACL and ACL. He is on the editorial boards of Information Retrieval
and the Journal of Artificial Intelligence Research and was recently nominated to the board of the Journal
of Natural Language Engineering. He has co-chaired 5 ACL/NAACL workshops and given 6 tutorials at
venues like SIGIR, AAAI, and RANLP. Dragomir?s current interests are in text summarization, information
extraction, information retrieval, graph models, semi-supervised learning, and machine translation. He has
more than 50 peer-reviewed papers as well as more than 50 talks at various universities and other venues.
Dragomir?s work has been funded by NSF, NIH, and ONR.
304
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Citations to Generate Surveys of Scientific Paradigms
Saif Mohammad??, Bonnie Dorr???, Melissa Egan??, Ahmed Hassan?,
Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev??, David Zajic?
Institute for Advanced Computer Studies? and Computer Science?, University of Maryland.
Human Language Technology Center of Excellence?. Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu
Department of Electrical Engineering and Computer Science?
School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.edu
Abstract
The number of research publications in var-
ious disciplines is growing exponentially.
Researchers and scientists are increasingly
finding themselves in the position of having
to quickly understand large amounts of tech-
nical material. In this paper we present the
first steps in producing an automatically gen-
erated, readily consumable, technical survey.
Specifically we explore the combination of
citation information and summarization tech-
niques. Even though prior work (Teufel et
al., 2006) argues that citation text is unsuitable
for summarization, we show that in the frame-
work of multi-document survey creation, cita-
tion texts can play a crucial role.
1 Introduction
In today?s rapidly expanding disciplines, scientists
and scholars are constantly faced with the daunting
task of keeping up with knowledge in their field. In
addition, the increasingly interconnected nature of
real-world tasks often requires experts in one dis-
cipline to rapidly learn about other areas in a short
amount of time.
Cross-disciplinary research requires scientists in
areas such as linguistics, biology, and sociology
to learn about computational approaches and appli-
cations, e.g., computational linguistics, biological
modeling, social networks. Authors of journal ar-
ticles and books must write accurate surveys of pre-
vious work, ranging from short summaries of related
research to in-depth historical notes.
Interdisciplinary review panels are often called
upon to review proposals in a wide range of areas,
some of which may be unfamiliar to panelists. Thus,
they must learn about a new discipline ?on the fly?
in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by
combining two currently available technologies: (1)
bibliometric lexical link mining that exploits the
structure of citations and relations among citations;
and (2) summarization techniques that exploit the
content of the material in both the citing and cited
papers.
It is generally agreed upon that manually written
abstracts are good summaries of individual papers.
More recently, Qazvinian and Radev (2008) argue
that citation texts are useful in creating a summary
of the important contributions of a research paper.
The citation text of a target paper is the set of sen-
tences in other technical papers that explicitly refer
to it (Elkiss et al, 2008a). However, Teufel (2005)
argues that using citation text directly is not suitable
for document summarization.
In this paper, we compare and contrast the use-
fulness of abstracts and of citation text in automati-
cally generating a technical survey on a given topic
from multiple research papers. The next section pro-
vides the background for this work, including the
primary features of a technical survey and also the
types of input that are used in our study (full pa-
pers, abstracts, and citation texts). Following this,
we describe related work and point out the advances
of our work over previous work. We then describe
how citation texts are used as a new input for multi-
document summarization to produce surveys of a
given technical area. We apply four different sum-
marization techniques to data in the ACL Anthol-
584
ogy and evaluate our results using both automatic
(ROUGE) and human-mediated (nugget-based pyra-
mid) measures. We observe that, as expected, ab-
stracts are useful in survey creation, but, notably, we
also conclude that citation texts have crucial survey-
worthy information not present in (or at least, not
easily extractable from) abstracts. We further dis-
cover that abstracts are author-biased and thus com-
plementary to the broader perspective inherent in ci-
tation texts; these differences enable the use of a
range of different levels and types of information in
the survey?the extent of which is subject to survey
length restrictions (if any).
2 Background
Automatically creating technical surveys is sig-
nificantly distinct from that of traditional multi-
document summarization. Below we describe pri-
mary characteristics of a technical survey and we
present three types of input texts that we used for
the production of surveys.
2.1 Technical Survey
In the case of multi-document summarization, the
goal is to produce a readable presentation of mul-
tiple documents, whereas in the case of technical
survey creation, the goal is to convey the key fea-
tures of a particular field, basic underpinnings of the
field, early and late developments, important con-
tributions and findings, contradicting positions that
may reverse trends or start new sub-fields, and ba-
sic definitions and examples that enable rapid un-
derstanding of a field by non-experts.
A prototypical example of a technical survey is
that of ?chapter notes,? i.e., short (50?500 word)
descriptions of sub-areas found at the end of chap-
ters of textbook, such as Jurafsky and Martin (2008).
One might imagine producing such descriptions au-
tomatically, then hand-editing them and refining
them for use in an actual textbook.
We conducted a human analysis of these chapter
notes that revealed a set of conventions, an outline
of which is provided here (with example sentences
in italics):
1. Introductory/opening statement: The earliest
computational use of X was in Y, considered by
many to be the foundational work in this area.
2. Definitional follow up: X is def ined as Y.
3. Elaboration of definition (e.g., with an exam-
ple): Most early algorithms were based on Z.
4. Deeper elaboration, e.g., pointing out issues
with initial approaches: Unfortunately, this
model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances /
historical background with citations: Two clas-
sic approaches are described in Q.
7. References to other summaries: R provides a
comprehensive guide to the details behind X.
The notion of text level categories or zoning
of technical papers?related to the survey compo-
nents enumerated above?has been investigated pre-
viously in the work of Nanba and Kan (2004b) and
Teufel (2002). These earlier works focused on the
analysis of scientific papers based on their rhetori-
cal structure and on determining the portions of pa-
pers that contain new results, comparisons to ear-
lier work, etc. The work described in this paper fo-
cuses on the synthesis of technical surveys based on
knowledge gleaned from rhetorical structure not un-
like that of the work of these earlier researchers, but
perhaps guided by structural patterns along the lines
of the conventions listed above.
Although our current approach to survey creation
does not yet incorporate a fully pattern-based com-
ponent, our ultimate objective is to apply these pat-
terns to guide the creation and refinement of the final
output. As a first step toward this goal, we use cita-
tion texts (closest in structure to the patterns iden-
tified by convention 7 above) to pick out the most
important content for survey creation.
2.2 Full papers, abstracts, and citation texts
Published research on a particular topic can be sum-
marized from two different kinds of sources: (1)
where an author describes her own work and (2)
where others describe an author?s work (usually in
relation to their own work). The author?s descrip-
tion of her own work can be found in her paper. How
others perceive her work is spread across other pa-
pers that cite her work. We will refer to the set of
sentences that explicitly mention a target paper Y as
the citation text of Y.
585
Traditionally, technical survey generation has
been tackled by summarizing a set of research pa-
pers pertaining to the topic. However, individual re-
search papers usually come with manually-created
?summaries??their abstracts. The abstract of a pa-
per may have sentences that set the context, state the
problem statement, mention how the problem is ap-
proached, and the bottom-line results?all in 200 to
500 words. Thus, using only the abstracts (instead
of full papers) as input to a summarization system is
worth exploring.
Whereas the abstract of a paper presents what the
authors think to be the important contributions of a
paper, the citation text of a paper captures what oth-
ers in the field perceive as the contributions of the
paper. The two perspectives are expected to have
some overlap in their content, but the citation text
also contains additional information not found in ab-
stracts (Elkiss et al, 2008a). For example, how a
particular methodology (described in one paper) was
combined with another (described in a different pa-
per) to overcome some of the drawbacks of each.
A citation text is also an indicator of what contri-
butions described in a paper were more influential
over time. Another distinguishing feature of citation
texts in contrast to abstracts is that a citation text
tends to have a certain amount of redundant informa-
tion. This is because multiple papers may describe
the same contributions of a target paper. This redun-
dancy can be exploited to determine the important
contributions of the target paper.
Our goal is to test the hypothesis that an ef-
fective technical survey will reflect information on
research not only from the perspective of its au-
thors but also from the perspective of others who
use/commend/discredit/add to it. Before describ-
ing our experiments with technical papers, abstracts,
and citation texts, we first summarize relevant prior
work that used these sources of information as input.
3 Related work
Previous work has focused on the analysis of cita-
tion and collaboration networks (Teufel et al, 2006;
Newman, 2001) and scientific article summarization
(Teufel and Moens, 2002). Bradshaw (2003) used
citation texts to determine the content of articles and
improve the results of a search engine. Citation
texts have also been used to create summaries of sin-
gle scientific articles in Qazvinian and Radev (2008)
and Mei and Zhai (2008). However, there is no pre-
vious work that uses the text of the citations to pro-
duce a multi-document survey of scientific articles.
Furthermore, there is no study contrasting the qual-
ity of surveys generated from citation summaries?
both automatically and manually produced?to sur-
veys generated from other forms of input such as the
abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-
gorization to support a system for writing a survey.
Nanba et al (2004a) automatically categorize cita-
tion sentences into three groups using pre-defined
phrase-based rules. Based on this categorization a
survey generation tool is introduced in Nanba et al
(2004b). They report that co-citation (where both
papers are cited by many other papers) implies sim-
ilarity by showing that the textual similarity of co-
cited papers is proportional to the proximity of their
citations in the citing article.
Elkiss et al (2008b) conducted several exper-
iments on a set of 2,497 articles from the free
PubMed Central (PMC) repository.1 Results from
this experiment confirmed that the cohesion of a ci-
tation text of an article is consistently higher than
the that of its abstract. They also concluded that ci-
tation texts contain additional information are more
focused than abstracts.
Nakov et al (2004) use sentences surrounding ci-
tations to create training and testing data for seman-
tic analysis, synonym set creation, database cura-
tion, document summarization, and information re-
trieval. Kan et al (2002) use annotated bibliogra-
phies to cover certain aspects of summarization and
suggest using metadata and critical document fea-
tures as well as the prominent content-based features
to summarize documents. Kupiec et al (1995) use a
statistical method and show how extracts can be used
to create summaries but use no annotated metadata
in summarization.
Siddharthan and Teufel (2007) describe a new
reference task and show high human agreement as
well as an improvement in the performance of ar-
gumentative zoning (Teufel, 2005). In argumenta-
tive zoning?a rhetorical classification task?seven
1http://www.pubmedcentral.gov
586
classes (Own, Other, Background, Textual, Aim,
Basis, and Contrast) are used to label sentences ac-
cording to their role in the author?s argument.
Our aim is not only to determine the utility of cita-
tion texts for survey creation, but also to examine the
quality distinctions between this form of input and
others such as abstracts and full texts?comparing
the results to human-generated surveys using both
automatic and nugget-based pyramid evaluation
(Lin and Demner-Fushman, 2006; Nenkova and Pas-
sonneau, 2004; Lin, 2004).
4 Summarization systems
We used four summarization systems for our
survey-creation approach: Trimmer, LexRank, C-
LexRank, and C-RR. Trimmer is a syntactically-
motivated parse-and-trim approach. LexRank is a
graph-based similarity approach. C-LexRank and C-
RR use graph clustering (?C? stands for clustering).
We describe each of these, in turn, below.
4.1 Trimmer
Trimmer is a sentence-compression tool that extends
the scope of an extractive summarization system by
generating multiple alternative sentence compres-
sions of the most important sentences in target doc-
uments (Zajic et al, 2007). Trimmer compressions
are generated by applying linguistically-motivated
rules to mask syntactic components of a parse of a
source sentence. The rules can be applied iteratively
to compress sentences below a configurable length
threshold, or can be applied in all combinations to
generate the full space of compressions.
Trimmer can leverage the output of any con-
stituency parser that uses the Penn Treebank con-
ventions. At present, the Stanford Parser (Klein and
Manning, 2003) is used. The set of compressions
is ranked according to a set of features that may in-
clude metadata about the source sentences, details of
the compression process that generated the compres-
sion, and externally calculated features of the com-
pression.
Summaries are constructed from the highest scor-
ing compressions, using the metadata and maximal
marginal relevance (Carbonell and Goldstein, 1998)
to avoid redundancy and over-representation of a
single source.
4.2 LexRank
We also used LexRank (Erkan and Radev, 2004), a
state-of-the-art multidocument summarization sys-
tem, to generate summaries. LexRank first builds a
graph of all the candidate sentences. Two candidate
sentences are connected with an edge if the similar-
ity between them is above a threshold. We used co-
sine as the similarity metric with a threshold of 0.15.
Once the network is built, the system finds the most
central sentences by performing a random walk on
the graph.
The salience of a node is recursively defined on
the salience of adjacent nodes. This is similar to
the concept of prestige in social networks, where the
prestige of a person is dependent on the prestige of
the people he/she knows. However, since random
walk may get caught in cycles or in disconnected
components, we reserve a low probability to jump
to random nodes instead of neighbors (a technique
suggested by Langville and Meyer (2006)).
Note also that unlike the original PageRank
method, the graph of sentences is undirected. This
updated measure of sentence salience is called as
LexRank. The sentences with the highest LexRank
scores form the summary.
4.3 Cluster Summarizers: C-LexRank, C-RR
Two clustering methods proposed by Qazvinian and
Radev (2008)?C-RR and C-LexRank?were used
to create summaries. Both create a fully connected
network in which nodes are sentences and edges are
cosine similarities. A cutoff value of 0.1 is applied
to prune the graph and make a binary network. The
largest connected component of the network is then
extracted and clustered.
Both of the mentioned summarizers cluster the
network similarly but use different approaches to se-
lect sentences from different communities. In C-
RR sentences are picked from different clusters in
a round robin (RR) fashion. C-LexRank first calcu-
lates LexRank within each cluster to find the most
salient sentences of each community. Then it picks
the most salient sentence of each cluster, and then
the second most salient and so forth until the sum-
mary length limit is reached.
587
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer
locator Rinaldi et al 2003; Tomuro, 2003. In addition, number of researchers have built systems to take reading
comprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al
1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ? definition ? or ? other ?
questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples. To better facilitate user
information needs, recent trends in QA research have shifted towards complex, context-based, and interactive
question answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005. [And so on.]
Table 1: First few sentences of the QA citation texts survey generated by Trimmer.
5 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and proceed-
ings of ACL conferences and workshops. It has
almost 11,000 papers. To produce the ACL An-
thology Network (AAN), Joseph and Radev (2007)
manually parsed the references before automatically
compiling the network metadata, and generating ci-
tation and author collaboration networks. The AAN
includes all citation and collaboration data within
the ACL papers, with the citation network consist-
ing of 11,773 nodes and 38,765 directed edges.
Our evaluation experiments are on a set of papers
in the research area of Question Answering (QA)
and another set of papers on Dependency parsing
(DP). The two sets of papers were compiled by se-
lecting all the papers in AAN that had the words
Question Answering and Dependency Parsing, re-
spectively, in the title and the content. There were
10 papers in the QA set and 16 papers in the DP set.
We also compiled the citation texts for the 10 QA
papers and the citation texts for the 16 DP papers.
6 Experiments
We automatically generated surveys for both QA
and DP from three different types of documents: (1)
full papers from the QA and DP sets?QA and DP
full papers (PA), (2) only the abstracts of the QA
and DP papers?QA and DP abstracts (AB), and
(3) the citation texts corresponding to the QA and
DP papers?QA and DP citations texts (CT).
We generated twenty four (4x3x2) surveys,
each of length 250 words, by applying Trimmer,
LexRank, C-LexRank and C-RR on the three data
types (citation texts, abstracts, and full papers) for
both QA and DP. (Table 1 shows a fragment of one
of the surveys automatically generated from QA ci-
tation texts.) We created six (3x2) additional 250-
word surveys by randomly choosing sentences from
the citation texts, abstracts, and full papers of QA
and DP. We will refer to them as random surveys.
6.1 Evaluation
Our goal was to determine if citation texts do in-
deed have useful information that one will want to
put in a survey and if so, how much of this infor-
mation is not available in the original papers and
their abstracts. For this we evaluated each of the
automatically generated surveys using two separate
approaches: nugget-based pyramid evaluation and
ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-
ated from the QA and DP citation texts and abstracts,
respectively:2 (1) We asked two impartial judges to
identify important nuggets of information worth in-
cluding in a survey. (2) We asked four fluent speak-
ers of English to create 250-word surveys of the
datasets. Then we determined how well the differ-
ent automatically generated surveys perform against
these gold standards. If the citation texts have only
redundant information with respect to the abstracts
and original papers, then the surveys of citation texts
will not perform better than others.
6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based
evaluation methodology (Lin and Demner-Fushman,
2006; Nenkova and Passonneau, 2004; Hildebrandt
et al, 2004; Voorhees, 2003). We asked three impar-
tial annotators (knowledgeable in NLP but not affil-
iated with the project) to review the citation texts
and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists
2Creating gold standard data from complete papers is fairly
arduous, and was not pursued.
588
of 2?8 ?nuggets,? or main contributions, supplied
by each paper. Each nugget was assigned a weight
based on the frequency with which it was listed by
annotators as well as the priority it was assigned
in each case. Our automatically generated surveys
were then scored based on the number and weight
of the nuggets that they covered. This evaluation ap-
proach is similar to the one adopted by Qazvinian
and Radev (2008), but adapted here for use in the
multi-document case.
The annotators had two distinct tasks for the QA
set, and one for the DP set: (1) extract nuggets for
each of the 10 QA papers, based only on the citation
texts for those papers; (2) extract nuggets for each
of the 10 QA papers, based only on the abstracts of
those papers; and (3) extract nuggets for each of the
16 DP papers, based only on the citation texts for
those papers.3
We obtained a weight for each nugget by revers-
ing its priority out of 8 (e.g., a nugget listed with
priority 1 was assigned a weight of 8) and summing
the weights over each listing of that nugget.4
To evaluate a given survey, we counted the num-
ber and weight of nuggets that it covered. Nuggets
were detected via the combined use of annotator-
provided regular expressions and careful human re-
view. Recall was calculated by dividing the com-
bined weight of covered nuggets by the combined
weight of all nuggets in the nugget set. Precision
was calculated by dividing the number of distinct
nuggets covered in a survey by the number of sen-
tences constituting that survey, with a cap of 1. F-
measure, the weighted harmonic mean of precision
and recall, was calculated with a beta value of 3 in
order to assign the greatest weight to recall. Recall
is favored because it rewards surveys that include
highly weighted (important) facts, rather than just a
3We first experimented using only the QA set. Then to show
that the results apply to other datasets, we asked human anno-
tators for gold standard data on the DP citation texts. Addi-
tional experiments on DP abstracts were not pursued because
this would have required additional human annotation effort to
establish a point we had already made with the QA set, i.e., that
abstracts are useful for survey creation.
4Results obtained with other weighting schemes that ig-
nored priority ratings and multiple mentions of a nugget by a
single annotator showed the same trends as the ones shown by
the selected weighting scheme, but the latter was a stronger dis-
tinguisher among the four systems.
Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 Average
Input: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413
Table 2: Pyramid F-measure scores of human-created
surveys of QA and DP data. The surveys are evaluated
using nuggets drawn from QA citation texts (QA?CT),
QA abstracts (QA?AB), and DP citation texts (DP?CT).
great number of facts.
Table 2 gives the F-measure values of the 250-
word surveys manually generated by humans. The
surveys were evaluated using the nuggets drawn
from the QA citation texts, QA abstracts, and DP ci-
tation texts. The average of their scores (listed in the
rightmost column) may be considered a good score
to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys
generated by the four automatic summarizers, evalu-
ated using nuggets drawn from the QA citation texts,
QA abstracts, and DP citation texts. The table also
includes results for the baseline random summaries.
When we used the nuggets from the abstracts
set for evaluation, the surveys created from ab-
stracts scored higher than the corresponding surveys
created from citation texts and papers. Further, the
best surveys generated from citation texts outscored
the best surveys generated from papers. When we
used the nuggets from citation sets for evaluation,
the best automatic surveys generated from citation
texts outperform those generated from abstracts and
full papers. All these pyramid results demonstrate
that citation texts can contain useful information that
is not available in the abstracts or the original papers,
and that abstracts can contain useful information that
is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-
mer performed best at this task, in two cases ex-
ceeding the average human performance. Note also
that the random summarizer outscored the automatic
summarizers in cases where the nuggets were taken
from a source different from that used to generate
the survey. However, one or two summarizers still
tended to do well. This indicates a difficulty in ex-
589
System Performance: Pyramid F-measure
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT nuggets 0.321 0.434 0.268 0.295 0.616
QA?AB nuggets 0.305 0.388 0.349 0.320 0.543
Input: QA abstract surveys
QA?CT nuggets 0.452 0.383 0.480 0.441 0.404
QA?AB nuggets 0.623 0.484 0.574 0.606 0.622
Input: QA full paper surveys
QA?CT nuggets 0.239 0.446 0.299 0.190 0.199
QA?AB nuggets 0.294 0.520 0.387 0.301 0.290
Input: DP citation surveys
DP?CT nuggets 0.219 0.231 0.170 0.372 0.136
Input: DP abstract surveys
DP?CT nuggets 0.321 0.301 0.263 0.311 0.312
Input: DP full paper surveys
DP?CT nuggets 0.032 0.000 0.144 * 0.280
Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets
drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).
Human Performance: ROUGE-2human1 human2 human3 human4 average
Input: QA citation surveysQA?CT refs. 0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs. 0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs. 0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs. 0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs. 0.1550 0.1259 0.1200 0.1654 0.1416
Table 4: ROUGE-2 scores obtained for each of the manu-
ally created surveys by using the other three as reference.
ROUGE-1 and ROUGE-L followed similar patterns.
tracting the overlapping survey-worthy information
across the two sources.
6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of
each of human-generated 250-word surveys against
each other. The average (last column) is what the au-
tomatic surveys can aim for. We then evaluated each
of the random surveys and those generated by the
four summarization systems against the references.
Table 5 lists ROUGE scores of surveys when the
manually created 250-word survey of the QA cita-
tion texts, survey of the QA abstracts, and the survey
of the DP citation texts, were used as gold standard.
When we use manually created citation text
surveys as reference, then the surveys gener-
ated from citation texts obtained significantly bet-
ter ROUGE scores than the surveys generated from
abstracts and full papers (p < 0.05) [RESULT 1].
This shows that crucial survey-worthy information
present in citation texts is not available, or hard to
extract, from abstracts and papers alone. Further,
the surveys generated from abstracts performed sig-
nificantly better than those generated from the full
papers (p < 0.05) [RESULT 2]. This shows that ab-
stracts and citation texts are generally denser in sur-
vey worthy information than full papers.
When we use manually created abstract sur-
veys as reference, then the surveys generated
from abstracts obtained significantly better ROUGE
scores than the surveys generated from citation texts
and full papers (p < 0.05) [RESULT 3]. Further, and
more importantly, the surveys generated from cita-
tion texts performed significantly better than those
generated from the full papers (p < 0.05) [RESULT
4]. Again, this shows that abstracts and citation texts
are richer in survey-worthy information. These re-
sults also show that abstracts of papers and citation
texts have some overlapping information (RESULT
2 and RESULT 4), but they also have a signifi-
cant amount of unique survey-worthy information
(RESULT 1 and RESULT 3).
Among the automatic summarizers, C-LexRank
and LexRank perform best. This is unlike the results
found through the nugget-evaluation method, where
Trimmer performed best. This suggests that Trim-
590
System Performance: ROUGE-2
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT refs. 0.11561 0.17013 0.09522 0.13501 0.16984
QA?AB refs. 0.08264 0.11653 0.07600 0.07013 0.10336
Input: QA abstract surveys
QA?CT refs. 0.04516 0.05892 0.06149 0.05369 0.04114
QA?AB refs. 0.12085 0.13634 0.12190 0.20311 0.13357
Input: QA full paper surveys
QA?CT refs. 0.03042 0.03606 0.03599 0.28244 0.03986
QA?AB refs. 0.04621 0.05901 0.04976 0.10540 0.07505
Input: DP citation surveys
DP?CT refs. 0.10690 0.13164 0.08748 0.04901 0.10052
Input: DP abstract surveys
DP?CT refs. 0.07027 0.07321 0.05318 0.20311 0.07176
Input: DP full paper surveys
DP?CT refs. 0.03770 0.02511 0.03433 * 0.04554
Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human
references created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT). These
results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).
mer is better at identifying more useful nuggets of
information, but C-LexRank and LexRank are bet-
ter at producing unigrams and bigrams expected in
a survey. To some extent this may be due to the fact
that Trimmer uses smaller (trimmed) fragments of
source sentences in its summaries.
7 Conclusion
In this paper, we investigated the usefulness of di-
rectly summarizing citation texts (sentences that cite
other papers) in the automatic creation of technical
surveys. We generated surveys of a set of Ques-
tion Answering (QA) and Dependency Parsing (DP)
papers, their abstracts, and their citation texts us-
ing four state-of-the-art summarization systems (C-
LexRank, C-RR, LexRank, and Trimmer). We then
used two separate approaches, nugget-based pyra-
mid and ROUGE, to evaluate the surveys. The re-
sults from both approaches and all four summa-
rization systems show that both citation texts and
abstracts have unique survey-worthy information.
These results also demonstrate that, unlike single
document summarization (where citing sentences
have been suggested to be inappropriate (Teufel
et al, 2006)), multidocument summarization?
especially technical survey creation?benefits con-
siderably from citation texts.
We next plan to generate surveys using both cita-
tion texts and abstracts together as input. Given the
overlapping content of abstracts and citation texts,
discovered in the current study, it is clear that re-
dundancy detection will be an integral component of
this future work. Creating readily consumable sur-
veys is a hard task, especially when using only raw
text and simple summarization techniques. There-
fore we intend to combine these summarization and
bibliometric techniques with suitable visualization
methods towards the creation of iterative technical
survey tools?systems that present surveys and bib-
liometric links in a visually convenient manner and
which incorporate user feedback to produce even
better surveys.
Acknowledgments
This work was supported, in part, by the National
Science Foundation under Grant No. IIS-0705832
(iOPENER: Information Organization for PENning
Expositions on Research) and Grant No. 0534323
(Collaborative Research: BlogoCenter - Infrastruc-
ture for Collecting, Mining and Accessing Blogs),
in part, by the Human Language Technology Cen-
ter of Excellence, and in part, by the Center for Ad-
vanced Study of Language (CASL). Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsors.
591
References
Shannon Bradshaw. 2003. Reference directed indexing:
Redeeming relevance for subject search in citation in-
dexes. In Proceedings of the 7th European Conference
on Research and Advanced Technology for Digital Li-
braries.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 335?336, Melbourne, Australia.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008a. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008b. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Overview of the trec 2003 question-answering track.
In Proceedings of the 2004 Human Language Tech-
nology Conference and the North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT/NAACL 2004).
Mark Joseph and Dragomir Radev. 2007. Citation analy-
sis, centrality, and the ACL Anthology. Technical Re-
port CSE-TR-535-07, University of Michigan. Dept.
of Electrical Engineering and Computer Science.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics (2nd edition). Prentice-Hall.
Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 2002. Using the Annotated Bibliography as a
Resource for Indicative Summarization. In Proceed-
ings of LREC 2002, Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95,
pages 68?73, New York, NY, USA. ACM.
Amy Langville and Carl Meyer. 2006. Google?s PageR-
ank and Beyond: The Science of Search Engine Rank-
ings. Princeton University Press.
Jimmy J. Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to complex
questions. Information Retrieval, 9(5):565?587.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
workshop on Text Summarization Branches Out.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In Workshop on Search and
Discovery in Bioinformatics.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference informa-
tion. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,
and Suguru Saito. 2004a. Bilingual presri: Integration
of multiple research paper databases. In Proceedings
of RIAO 2004, pages 195?211, Avignon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.
2004b. Classification of research papers using cita-
tion links and citation types: Towards automatic re-
view article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
Proceedings of EMNLP, pages 103?110, Australia.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and Af-
fect in Text: Theory and Applications, pages 159?170.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003).
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing and Management (Spe-
cial Issue on Summarization).
592
Evaluation challenges in large-scale document summarization
Dragomir R. Radev
U. of Michigan
radev@umich.edu
Wai Lam
Chinese U. of Hong Kong
wlam@se.cuhk.edu.hk
Arda C?elebi
USC/ISI
ardax@isi.edu
Simone Teufel
U. of Cambridge
simone.teufel@cl.cam.ac.uk
John Blitzer
U. of Pennsylvania
blitzer@seas.upenn.edu
Danyu Liu
U. of Alabama
liudy@cis.uab.edu
Horacio Saggion
U. of Sheffield
h.saggion@dcs.shef.ac.uk
Hong Qi
U. of Michigan
hqi@umich.edu
Elliott Drabek
Johns Hopkins U.
edrabek@cs.jhu.edu
Abstract
We present a large-scale meta evaluation
of eight evaluation measures for both
single-document and multi-document
summarizers. To this end we built a
corpus consisting of (a) 100 Million auto-
matic summaries using six summarizers
and baselines at ten summary lengths in
both English and Chinese, (b) more than
10,000 manual abstracts and extracts, and
(c) 200 Million automatic document and
summary retrievals using 20 queries. We
present both qualitative and quantitative
results showing the strengths and draw-
backs of all evaluation methods and how
they rank the different summarizers.
1 Introduction
Automatic document summarization is a field that
has seen increasing attention from the NLP commu-
nity in recent years. In part, this is because sum-
marization incorporates many important aspects of
both natural language understanding and natural lan-
guage generation. In part it is because effective auto-
matic summarization would be useful in a variety of
areas. Unfortunately, evaluating automatic summa-
rization in a standard and inexpensive way is a diffi-
cult task (Mani et al, 2001). Traditional large-scale
evaluations are either too simplistic (using measures
like precision, recall, and percent agreement which
(1) don?t take chance agreement into account and (2)
don?t account for the fact that human judges don?t
agree which sentences should be in a summary) or
too expensive (an approach using manual judge-
ments can scale up to a few hundred summaries but
not to tens or hundreds of thousands).
In this paper, we present a comparison of six
summarizers as well as a meta-evaluation including
eight measures: Precision/Recall, Percent Agree-
ment, Kappa, Relative Utility, Relevance Correla-
tion, and three types of Content-Based measures
(cosine, longest common subsequence, and word
overlap). We found that while all measures tend
to rank summarizers in different orders, measures
like Kappa, Relative Utility, Relevance Correlation
and Content-Based each offer significant advantages
over the more simplistic methods.
2 Data, Annotation, and Experimental
Design
We performed our experiments on the Hong Kong
News corpus provided by the Hong Kong SAR of
the People?s Republic of China (LDC catalog num-
ber LDC2000T46). It contains 18,146 pairs of par-
allel documents in English and Chinese. The texts
are not typical news articles. The Hong Kong News-
paper mainly publishes announcements of the local
administration and descriptions of municipal events,
such as an anniversary of the fire department, or sea-
sonal festivals. We tokenized the corpus to iden-
tify headlines and sentence boundaries. For the En-
glish text, we used a lemmatizer for nouns and verbs.
We also segmented the Chinese documents using the
tool provided at http://www.mandarintools.com.
Several steps of the meta evaluation that we per-
formed involved human annotator support. First, we
Cluster 2 Meetings with foreign leaders
Cluster 46 Improving Employment Opportunities
Cluster 54 Illegal immigrants
Cluster 60 Customs staff doing good job.
Cluster 61 Permits for charitable fund raising
Cluster 62 Y2K readiness
Cluster 112 Autumn and sports carnivals
Cluster 125 Narcotics Rehabilitation
Cluster 199 Intellectual Property Rights
Cluster 241 Fire safety, building management concerns
Cluster 323 Battle against disc piracy
Cluster 398 Flu results in Health Controls
Cluster 447 Housing (Amendment) Bill Brings Assorted Improvements
Cluster 551 Natural disaster victims aided
Cluster 827 Health education for youngsters
Cluster 885 Customs combats contraband/dutiable cigarette operations
Cluster 883 Public health concerns cause food-business closings
Cluster 1014 Traffic Safety Enforcement
Cluster 1018 Flower shows
Cluster 1197 Museums: exhibits/hours
Figure 1: Twenty queries created by the LDC for
this experiment.
asked LDC to build a set of queries (Figure 1). Each
of these queries produced a cluster of relevant doc-
uments. Twenty of these clusters were used in the
experiments in this paper.
Additionally, we needed manual summaries or ex-
tracts for reference. The LDC annotators produced
summaries for each document in all clusters. In or-
der to produce human extracts, our judges also la-
beled sentences with ?relevance judgements?, which
indicate the relevance of sentence to the topic of the
document. The relevance judgements for sentences
range from 0 (irrelevant) to 10 (essential). As in
(Radev et al, 2000), in order to create an extract of
a certain length, we simply extract the top scoring
sentences that add up to that length.
For each target summary length, we produce an
extract using a summarizer or baseline. Then we
compare the output of the summarizer or baseline
with the extract produced from the human relevance
judgements. Both the summarizers and the evalua-
tion measures are described in greater detail in the
next two sections.
2.1 Summarizers and baselines
This section briefly describes the summarizers we
used in the evaluation. All summarizers take as input
a target length (n%) and a document (or cluster) split
into sentences. Their output is an n% extract of the
document (or cluster).
? MEAD (Radev et al, 2000): MEAD is
a centroid-based extractive summarizer that
scores sentences based on sentence-level and
inter-sentence features which indicate the qual-
ity of the sentence as a summary sentence. It
then chooses the top-ranked sentences for in-
clusion in the output summary. MEAD runs on
both English documents and on BIG5-encoded
Chinese. We tested the summarizer in both lan-
guages.
? WEBS (Websumm (Mani and Bloedorn,
2000)): can be used to produce generic and
query-based summaries. Websumm uses a
graph-connectivity model and operates under
the assumption that nodes which are connected
to many other nodes are likely to carry salient
information.
? SUMM (Summarist (Hovy and Lin, 1999)):
an extractive summarizer based on topic signa-
tures.
? ALGN (alignment-based): We ran a sentence
alignment algorithm (Gale and Church, 1993)
for each pair of English and Chinese stories.
We used it to automatically generate Chinese
?manual? extracts from the English manual ex-
tracts we received from LDC.
? LEAD (lead-based): n% sentences are chosen
from the beginning of the text.
? RAND (random): n% sentences are chosen at
random.
The six summarizers were run at ten different tar-
get lengths to produce more than 100 million sum-
maries (Figure 2). For the purpose of this paper, we
only focus on a small portion of the possible experi-
ments that our corpus can facilitate.
3 Summary Evaluation Techniques
We used three general types of evaluation measures:
co-selection, content-based similarity, and relevance
correlation. Co-selection measures include preci-
sion and recall of co-selected sentences, relative util-
ity (Radev et al, 2000), and Kappa (Siegel and
Castellan, 1988; Carletta, 1996). Co-selection meth-
ods have some restrictions: they only work for ex-
tractive summarizers. Two manual summaries of the
same input do not in general share many identical
sentences. We address this weakness of co-selection
Lengths #dj
05W 05S 10W 10S 20W 20S 30W 30S 40W 40S FD
E-FD - - - - - - - - - - x 40
E-LD X X X X x x X X X X - 440
E-RA X X X X x x X X X X - 440
E-MO x x X x x x X x X x - 540
E-M2 - - - - - X - - - - - 20
E-M3 - - - - - X - - - - - 8
E-S2 - - - - - X - - - - - 8
E-WS - X - X x x - X - X - 160
E-WQ - - - - - X - - - - - 10
E-LC - - - - - - x - - - - 40
E-CY - X - X - x - X - X - 120
E-AL X X X X X X X X X X - 200
E-AR X X X X X X X X X X - 200
E-AM X X X X X X X X X X - 200
C-FD - - - - - - - - - - x 40
C-LD X X X X x x X X X X - 240
C-RA X X X X x x X X X X - 240
C-MO X x X x x x X x X x - 320
C-M2 - - - - - X - - - - - 20
C-CY - X - X - x - X - X - 120
C-AL X X X X X X X X X X - 180
C-AR X X X X X X X X X X - 200
C-AM - X X X X X X X X - 120
X-FD - - - - - - - - - - x 40
X-LD X X X X x x X X X X - 240
X-RA X X X X x x X X X X - 240
X-MO X x X x x x X x X x - 320
X-M2 - - - - - X - - - - - 20
X-CY - X - X - x - X - X - 120
X-AL X X X X X X X X X X - 140
X-AR X X X X X X X X X X - 160
X-AM - X X X X X X X - X - 120
Figure 2: All runs performed (X = 20 clusters, x = 10 clusters). Language: E = English, C = Chinese,
X = cross-lingual; Summarizer: LD=LEAD, RA=RAND, WS=WEBS, WQ=WEBS-query based, etc.; S =
sentence-based, W = word-based; #dj = number of ?docjudges? (ranked lists of documents and summaries).
Target lengths above 50% are not shown in this table for lack of space. Each run is available using two
different retrieval schemes. We report results using the cross-lingual retrievals in a separate paper.
measures with several content-based similarity mea-
sures. The similarity measures we use are word
overlap, longest common subsequence, and cosine.
One advantage of similarity measures is that they
can compare manual and automatic extracts with
manual abstracts. To our knowledge, no system-
atic experiments about agreement on the task of
summary writing have been performed before. We
use similarity measures to measure interjudge agree-
ment among three judges per topic. We also ap-
ply the measures between human extracts and sum-
maries, which answers the question if human ex-
tracts are more similar to automatic extracts or to
human summaries.
The third group of evaluation measures includes
relevance correlation. It shows the relative perfor-
mance of a summary: how much the performance
of document retrieval decreases when indexing sum-
maries rather than full texts.
Task-based evaluations (e.g., SUMMAC (Mani
et al, 2001), DUC (Harman and Marcu, 2001), or
(Tombros et al, 1998) measure human performance
using the summaries for a certain task (after the
summaries are created). Although they can be a
very effective way of measuring summary quality,
task-based evaluations are prohibitively expensive at
large scales. In this project, we didn?t perform any
task-based evaluations as they would not be appro-
priate at the scale of millions of summaries.
3.1 Evaluation by sentence co-selection
For each document and target length we produce
three extracts from the three different judges, which
we label throughout as J1, J2, and J3.
We used the rates 5%, 10%, 20%, 30%, 40% for
most experiments. For some experiments, we also
consider summaries of 50%, 60%, 70%, 80% and
90% of the original length of the documents. Figure
3 shows some abbreviations for co-selection that we
will use throughout this section.
3.1.1 Precision and Recall
Precision and recall are defined as:
PJ2 (J1) =
A
A+ C
,RJ2 (J1) =
A
A+ B
J2
Sentence in
Extract
Sentence not
in Extract
Sentence in
Extract
A B A+ B
J1 Sentence not
in Extract
C D C +D
A+ C B +D N = A +
B+C+D
Figure 3: Contingency table comparing sentences
extracted by the system and the judges.
In our case, each set of documents which is com-
pared has the same number of sentences and also
the same number of sentences are extracted; thus
P = R.
The average precision Pavg(SY STEM) and re-
call Ravg(SY STEM) are calculated by summing
over individual judges and normalizing. The aver-
age interjudge precision and recall is computed by
averaging over all judge pairs.
However, precision and recall do not take chance
agreement into account. The amount of agreement
one would expect two judges to reach by chance de-
pends on the number and relative proportions of the
categories used by the coders. The next section on
Kappa shows that chance agreement is very high in
extractive summarization.
3.1.2 Kappa
Kappa (Siegel and Castellan, 1988) is an evalua-
tion measure which is increasingly used in NLP an-
notation work (Krippendorff, 1980; Carletta, 1996).
Kappa has the following advantages over P and R:
? It factors out random agreement. Random
agreement is defined as the level of agreement
which would be reached by random annotation
using the same distribution of categories as the
real annotators.
? It allows for comparisons between arbitrary
numbers of annotators and items.
? It treats less frequent categories as more im-
portant (in our case: selected sentences), simi-
larly to precision and recall but it also consid-
ers (with a smaller weight) more frequent cate-
gories as well.
The Kappa coefficient controls agreement P (A)
by taking into account agreement by chance P (E) :
K =
P (A)? P (E)
1? P (E)
No matter how many items or annotators, or how
the categories are distributed, K = 0 when there is
no agreement other than what would be expected by
chance, and K = 1 when agreement is perfect. If
two annotators agree less than expected by chance,
Kappa can also be negative.
We report Kappa between three annotators in the
case of human agreement, and between three hu-
mans and a system (i.e. four judges) in the next sec-
tion.
3.1.3 Relative Utility
Relative Utility (RU) (Radev et al, 2000) is tested
on a large corpus for the first time in this project.
RU takes into account chance agreement as a lower
bound and interjudge agreement as an upper bound
of performance. RU allows judges and summarizers
to pick different sentences with similar content in
their summaries without penalizing them for doing
so. Each judge is asked to indicate the importance
of each sentence in a cluster on a scale from 0 to
10. Judges also specify which sentences subsume or
paraphrase each other. In relative utility, the score
of an automatic summary increases with the impor-
tance of the sentences that it includes but goes down
with the inclusion of redundant sentences.
3.2 Content-based Similarity measures
Content-based similarity measures compute the sim-
ilarity between two summaries at a more fine-
grained level than just sentences. For each automatic
extract S and similarity measure M we compute the
following number:
sim(M,S, {J1, J2, J3}) =
M(S, J1) +M(S, J2) +M(S, J3)
3
We used several content-based similarity mea-
sures that take into account different properties of
the text:
Cosine similarity is computed using the follow-
ing formula (Salton, 1988):
cos(X,Y ) =
?
xi ? yi
??
(xi)2 ?
??
(yi)2
where X and Y are text representations based on
the vector space model.
Longest Common Subsequence is computed as
follows:
lcs(X,Y ) = (length(X) + length(Y )? d(X,Y ))/2
where X and Y are representations based on
sequences and where lcs(X,Y ) is the length of
the longest common subsequence between X and
Y , length(X) is the length of the string X , and
d(X,Y ) is the minimum number of deletion and in-
sertions needed to transform X into Y (Crochemore
and Rytter, 1994).
3.3 Relevance Correlation
Relevance correlation (RC) is a new measure for as-
sessing the relative decrease in retrieval performance
when indexing summaries instead of full documents.
The idea behind it is similar to (Sparck-Jones and
Sakai, 2001). In that experiment, Sparck-Jones and
Sakai determine that short summaries are good sub-
stitutes for full documents at the high precision end.
With RC we attempt to rank all documents given a
query.
Suppose that given a queryQ and a corpus of doc-
uments Di, a search engine ranks all documents in
Di according to their relevance to the query Q. If
instead of the corpus Di, the respective summaries
of all documents are substituted for the full docu-
ments and the resulting corpus of summaries Si is
ranked by the same retrieval engine for relevance to
the query, a different ranking will be obtained. If
the summaries are good surrogates for the full docu-
ments, then it can be expected that rankings will be
similar.
There exist several methods for measuring the
similarity of rankings. One such method is Kendall?s
tau and another is Spearman?s rank correlation. Both
methods are quite appropriate for the task that we
want to perform; however, since search engines pro-
duce relevance scores in addition to rankings, we
can use a stronger similarity test, linear correlation
between retrieval scores. When two identical rank-
ings are compared, their correlation is 1. Two com-
pletely independent rankings result in a score of 0
while two rankings that are reverse versions of one
another have a score of -1. Although rank correla-
tion seems to be another valid measure, given the
large number of irrelevant documents per query re-
sulting in a large number of tied ranks, we opted for
linear correlation. Interestingly enough, linear cor-
relation and rank correlation agreed with each other.
Relevance correlation r is defined as the linear
correlation of the relevance scores (x and y) as-
signed by two different IR algorithms on the same
set of documents or by the same IR algorithm on
different data sets:
r =
?
i
(xi ? x)(yi ? y)
??
i
(xi ? x)2
??
i
(yi ? y)2
Here x and y are the means of the relevance scores
for the document sequence.
We preprocess the documents and use Smart to
index and retrieve them. After the retrieval process,
each summary is associated with a score indicating
the relevance of the summary to the query. The
relevance score is actually calculated as the inner
product of the summary vector and the query vec-
tor. Based on the relevance score, we can produce a
full ranking of all the summaries in the corpus.
In contrast to (Brandow et al, 1995) who run 12
Boolean queries on a corpus of 21,000 documents
and compare three types of documents (full docu-
ments, lead extracts, and ANES extracts), we mea-
sure retrieval performance under more than 300 con-
ditions (by language, summary length, retrieval pol-
icy for 8 summarizers or baselines).
4 Results
This section reports results for the summarizers and
baselines described above. We relied directly on the
relevance judgements to create ?manual extracts? to
use as gold standards for evaluating the English sys-
tems. To evaluate Chinese, we made use of a ta-
ble of automatically produced alignments. While
the accuracy of the alignments is quite high, we
have not thoroughly measured the errors produced
when mapping target English summaries into Chi-
nese. This will be done in future work.
4.1 Co-selection results
Co-selection agreement (Section 3.1) is reported in
Figures 4, and 5). The tables assume human perfor-
mance is the upper bound, the next rows compare
the different summarizers.
Figure 4 shows results for precision and recall.
We observe the effect of a dependence of the nu-
merical results on the length of the summary, which
is a well-known fact from information retrieval eval-
uations.
Websumm has an advantage over MEAD for
longer summaries but not for 20% or less. Lead
summaries perform better than all the automatic
summarizers, and better than the human judges.
This result usually occurs when the judges choose
different, but early sentences. Human judgements
overtake the lead baseline for summaries of length
50% or more.
5% 10% 20% 30% 40%
Humans .187 .246 .379 .467 .579
MEAD .160 .231 .351 .420 .519
WEBS .310 .305 .358 .439 .543
LEAD .354 .387 .447 .483 .583
RAND .094 .113 .224 .357 .432
Figure 4: Results in precision=recall (averaged over
20 clusters).
Figure 5 shows results using Kappa. Random
agreement is 0 by definition between a random pro-
cess and a non-random process.
While the results are overall rather low, the num-
bers still show the following trends:
? MEAD outperforms Websumm for all but the
5% target length.
? Lead summaries perform best below 20%,
whereas human agreement is higher after that.
? There is a rather large difference between the
two summarizers and the humans (except for
the 5% case for Websumm). This numerical
difference is relatively higher than for any other
co-selection measure treated here.
? Random is overall the worst performer.
? Agreement improves with summary length.
Figures 6 and 7 summarize the results obtained
through Relative Utility. As the figures indicate,
random performance is quite high although all non-
random methods outperform it significantly. Fur-
ther, and in contrast with other co-selection evalua-
tion criteria, in both the single- and multi-document
5% 10% 20% 30% 40%
Humans .127 .157 .194 .225 .274
MEAD .109 .136 .168 .192 .230
WEBS .138 .128 .146 .159 .192
LEAD .180 .198 .213 .220 .261
RAND .064 .081 .097 .116 .137
Figure 5: Results in kappa, averaged over 20 clus-
ters.
case MEAD outperforms LEAD for shorter sum-
maries (5-30%). The lower bound (R) represents the
average performance of all extracts at the given sum-
mary length while the upper bound (J) is the inter-
judge agreement among the three judges.
5% 10% 20% 30% 40%
R 0.66 0.68 0.71 0.74 0.76
RAND 0.67 0.67 0.71 0.75 0.77
WEBS 0.72 0.73 0.76 0.79 0.82
LEAD 0.72 0.73 0.77 0.80 0.83
MEAD 0.78 0.79 0.79 0.81 0.83
J 0.80 0.81 0.83 0.85 0.87
Figure 6: RU per summarizer and summary length
(Single-document).
5% 10% 20% 30% 40%
R 0.64 0.66 0.69 0.72 0.74
RAND 0.63 0.65 0.71 0.72 0.74
LEAD 0.71 0.71 0.76 0.79 0.82
MEAD 0.73 0.75 0.78 0.79 0.81
J 0.76 0.78 0.81 0.83 0.85
Figure 7: RU per summarizer and summary length
(Multi-document).
4.2 Content-based results
The results obtained for a subset of target lengths
using content-based evaluation can be seen in Fig-
ures 8 and 9. In all our experiments with tf ? idf -
weighted cosine, the lead-based summarizer ob-
tained results close to the judges in most of the target
lengths while MEAD is ranked in second position.
In all our experiments using longest common sub-
sequence, no system obtained better results in the
majority of the cases.
10% 20% 30% 40%
LEAD 0.55 0.65 0.70 0.79
MEAD 0.46 0.61 0.70 0.78
RAND 0.31 0.47 0.60 0.69
WEBS 0.52 0.60 0.68 0.77
Figure 8: Cosine (tf?idf ). Average over 10 clusters.
10% 20% 30% 40%
LEAD 0.47 0.55 0.60 0.70
MEAD 0.37 0.52 0.61 0.70
RAND 0.25 0.38 0.50 0.58
WEBS 0.39 0.45 0.53 0.64
Figure 9: Longest Common Subsequence. Average
over 10 clusters.
The numbers obtained in the evaluation of Chi-
nese summaries for cosine and longest common sub-
sequence can be seen in Figures 10 and 11. Both
measures identify MEAD as the summarizer that
produced results closer to the ideal summaries (these
results also were observed across measures and text
representations).
10% 20% 30% 40%
SUMM 0.44 0.65 0.71 0.78
LEAD 0.54 0.63 0.68 0.77
MEAD 0.49 0.65 0.74 0.82
RAND 0.31 0.50 0.65 0.71
Figure 10: Chinese Summaries. Cosine (tf ? idf ).
Average over 10 clusters. Vector space of Words as
Text Representation.
10% 20% 30% 40%
SUMM 0.32 0.53 0.57 0.65
LEAD 0.42 0.49 0.54 0.64
MEAD 0.35 0.50 0.60 0.70
RAND 0.21 0.35 0.49 0.54
Figure 11: Chinese Summaries. Longest Common
Subsequence. Average over 10 clusters. Chinese
Words as Text Representation.
We have based this evaluation on target sum-
maries produced by LDC assessors, although other
alternatives exist. Content-based similarity mea-
sures do not require the target summary to be a sub-
set of sentences from the source document, thus,
content evaluation based on similarity measures
can be done using summaries published with the
source documents which are in many cases available
(Teufel and Moens, 1997; Saggion, 2000).
4.3 Relevance Correlation results
We present several results using Relevance Correla-
tion. Figures 12 and 13 show how RC changes de-
pending on the summarizer and the language used.
RC is as high as 1.0 when full documents (FD) are
compared to themselves. One can notice that even
random extracts get a relatively high RC score. It is
also worth observing that Chinese summaries score
lower than their corresponding English summaries.
Figure 14 shows the effects of summary length and
summarizers on RC. As one might expect, longer
summaries carry more of the content of the full doc-
ument than shorter ones. At the same time, the rel-
ative performance of the different summarizers re-
mains the same across compression rates.
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.91 0.92 0.93 0.92 0.90 0.903
WEBS 0.88 0.82 0.89 0.91 0.88 0.843
LEAD 0.80 0.80 0.84 0.85 0.81 0.802
RAND 0.80 0.78 0.87 0.85 0.79 0.800
SUMM 0.77 0.79 0.85 0.88 0.81 0.775
Figure 12: RC per summarizer (English 20%).
C112 C125 C241 C323 C551 AVG10
FD 1.00 1.00 1.00 1.00 1.00 1.000
MEAD 0.78 0.87 0.93 0.66 0.91 0.850
SUMM 0.76 0.75 0.85 0.84 0.75 0.755
RAND 0.71 0.75 0.85 0.60 0.74 0.744
ALGN 0.74 0.72 0.83 0.95 0.72 0.738
LEAD 0.72 0.71 0.83 0.58 0.75 0.733
Figure 13: RC per summarizer (Chinese, 20%).
5% 10% 20% 30% 40%
FD 1.000 1.000 1.000 1.000 1.000
MEAD 0.724 0.834 0.916 0.946 0.962
WEBS 0.730 0.804 0.876 0.912 0.936
LEAD 0.660 0.730 0.820 0.880 0.906
SUMM 0.622 0.710 0.820 0.848 0.862
RAND 0.554 0.708 0.818 0.884 0.922
Figure 14: RC per summary length and summarizer.
5 Conclusion
This paper describes several contributions to text
summarization:
First, we observed that different measures rank
summaries differently, although most of them
showed that ?intelligent? summarizers outperform
lead-based summaries which is encouraging given
that previous results had cast doubt on the ability of
summarizers to do better than simple baselines.
Second, we found that measures like Kappa, Rel-
ative Utility, Relevance Correlation and Content-
Based, each offer significant advantages over more
simplistic methods like Precision, Recall, and Per-
cent Agreement with respect to scalability, applica-
bility to multidocument summaries, and ability to
include human and chance agreement. Figure 15
Property Prec, recall Kappa Normalized RU Word overlap, cosine, LCS Relevance Correlation
Intrinsic (I)/extrinsic (E) I I I I E
Agreement between human extracts X X X X X
Agreement human extracts and automatic extracts X X X X X
Agreement human abstracts and human extracts X
Non-binary decisions X X
Takes random agreement into account by design X X
Full documents vs. extracts X X
Systems with different sentence segmentation X X
Multidocument extracts X X X X
Full corpus coverage X X
Figure 15: Properties of evaluation measures used in this project.
presents a short comparison of all these evaluation
measures.
Third, we performed extensive experiments using
a new evaluation measure, Relevance Correlation,
which measures how well a summary can be used
to replace a document for retrieval purposes.
Finally, we have packaged the code used for this
project into a summarization evaluation toolkit and
produced what we believe is the largest and most
complete annotated corpus for further research in
text summarization. The corpus and related software
is slated for release by the LDC in mid 2003.
References
Ron Brandow, Karl Mitze, and Lisa F. Rau. 1995. Auto-
matic Condensation of Electronic Publications by Sen-
tence Selection. Information Processing and Manage-
ment, 31(5):675?685.
Jean Carletta. 1996. Assessing Agreement on Classifica-
tion Tasks: The Kappa Statistic. CL, 22(2):249?254.
Maxime Crochemore and Wojciech Rytter. 1994. Text
Algorithms. Oxford University Press.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Donna Harman and Daniel Marcu, editors. 2001. Pro-
ceedings of the 1st Document Understanding Confer-
ence. New Orleans, LA, September.
Eduard Hovy and Chin Yew Lin. 1999. Automated Text
Summarization in SUMMARIST. In Inderjeet Mani
and Mark T. Maybury, editors, Advances in Automatic
Text Summarization, pages 81?94. The MIT Press.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications, Bev-
erly Hills, CA.
Inderjeet Mani and Eric Bloedorn. 2000. Summariz-
ing Similarities and Differences Among Related Doc-
uments. Information Retrieval, 1(1).
Inderjeet Mani, The?re`se Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2001.
The TIPSTER SUMMAC Text Summarization Evalu-
ation. In Natural Language Engineering.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-Based Summarization
of Multiple Documents: Sentence Extraction, Utility-
Based Evaluation, and User Studies. In Proceedings
of the Workshop on Automatic Summarization at the
6th Applied Natural Language Processing Conference
and the 1st Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Seattle, WA, April.
Horacio Saggion. 2000. Ge?ne?ration automatique
de re?sume?s par analyse se?lective. Ph.D. the-
sis, De?partement d?informatique et de recherche
ope?rationnelle. Faculte? des arts et des sciences. Uni-
versite? de Montre?al, August.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Sidney Siegel and N. John Jr. Castellan. 1988. Non-
parametric Statistics for the Behavioral Sciences.
McGraw-Hill, Berkeley, CA, 2nd edition.
Karen Sparck-Jones and Tetsuya Sakai. 2001. Generic
Summaries for Indexing in IR. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 190?198, New Orleans, LA, September.
Simone Teufel and Marc Moens. 1997. Sentence Ex-
traction as a Classification Task. In Proceedings of the
Workshop on Intelligent Scalable Text Summarization
at the 35th Meeting of the Association for Computa-
tional Linguistics, and the 8th Conference of the Eu-
ropean Chapter of the Assocation for Computational
Linguistics, Madrid, Spain.
Anastasios Tombros, Mark Sanderson, and Phil Gray.
1998. Advantages of Query Biased Summaries in In-
formation Retrieval. In Eduard Hovy and Dragomir R.
Radev, editors, Proceedings of the AAAI Symposium
on Intelligent Text Summarization, pages 34?43, Stan-
ford, California, USA, March 23?25,. The AAAI
Press.
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 747?754,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Adding Syntax to Dynamic Programming for Aligning Comparable Texts
for the Generation of Paraphrases
Siwei Shen1, Dragomir R. Radev1;2, Agam Patel1, Gu?nes? Erkan1
Department of Electrical Engineering and Computer Science
School of Information
University of Michigan
Ann Arbor, MI 48109
fshens, radev, agamrp, gerkang@umich.edu
Abstract
Multiple sequence alignment techniques
have recently gained popularity in the Nat-
ural Language community, especially for
tasks such as machine translation, text
generation, and paraphrase identification.
Prior work falls into two categories, de-
pending on the type of input used: (a)
parallel corpora (e.g., multiple translations
of the same text) or (b) comparable texts
(non-parallel but on the same topic). So
far, only techniques based on parallel texts
have successfully used syntactic informa-
tion to guide alignments. In this paper,
we describe an algorithm for incorporat-
ing syntactic features in the alignment pro-
cess for non-parallel texts with the goal of
generating novel paraphrases of existing
texts. Our method uses dynamic program-
ming with alignment decision based on
the local syntactic similarity between two
sentences. Our results show that syntac-
tic alignment outrivals syntax-free meth-
ods by 20% in both grammaticality and fi-
delity when computed over the novel sen-
tences generated by alignment-induced fi-
nite state automata.
1 Introduction
In real life, we often encounter comparable texts
such as news on the same events reported by dif-
ferent sources and papers on the same topic au-
thored by different people. It is useful to recog-
nize if one text cites another in cases like news
sharing among media agencies or citations in aca-
demic work. Applications of such recognition in-
clude machine translation, text generation, para-
phrase identification, and question answering, all
of which have recently drawn the attention of a
number of researchers in natural language pro-
cessing community.
Multiple sequence alignment (MSA) is the ba-
sis for accomplishing these tasks. Previous work
aligns a group of sentences into a compact word
lattice (Barzilay and Lee, 2003), a finite state au-
tomaton representation that can be used to iden-
tify commonality or variability among compara-
ble texts and generate paraphrases. Nevertheless,
this approach has a drawback of over-generating
ungrammatical sentences due to its ?almost-free?
alignment. Pang et al provide a remedy to this
problem by performing alignment on the Charniak
parse trees of the clustered sentences (Pang et al,
2003). Although it is so far the most similar work
to ours, Pang?s solution assumes the input sen-
tences to be semantically equivalent. Two other
important references for string-based alignments
algorithms, mostly with applications in Biology,
are (Gusfield, 1997) and (Durbin et al, 1998).
In our approach, we work on comparable texts
(not necessarily equivalent in their semantic mean-
ings) as Barzilay and Lee did. However, we use lo-
cal syntactic similarity (as opposed to lexical simi-
larity) in doing the alignment on the raw sentences
instead of on their parse trees. Because of the se-
mantic discrepancies among the inputs, applying
syntactic features in the alignment has a larger im-
pact on the grammaticality and fidelity of the gen-
erated unseen sentences. While previous work po-
sitions the primary focus on the quality of para-
phrases and/or translations, we are more interested
in the relation between the use of syntactic fea-
tures and the correctness of the sentences being
generated, including those that are not paraphrases
of the original input. Figure 1 illustrates the dif-
ference between alignment based solely on lexi-
cal similarity and alignment with consideration of
syntactic features.
Ignoring syntax, the word ?Milan? in both sen-
tences is aligned. But it would unfortunately gen-
erate an ungrammatical sentence ?I went to Mi-
lan is beautiful?. Aligning according to syntac-
747
Start
II
Milan
Milan
wentwent
isis
AcceptAccept
to
to
Milan
beautifulbeautiful Accept
Start
II
Milan
Milan
wentwent
is
is
toto Milan
Milan
Accept
Accept
beautifulbeautiful
Accept
Figure 1: Alignment on lexical similarity and alignment with syntactic features of the sentences ?Milan
is beautiful? and ?I went to Milan?.
tic features, on the other hand, would avoid this
improper alignment by detecting that the syntactic
feature values of the two ?Milan? differ too much.
We shall explain syntactic features and their us-
ages later. In this small example, our syntax-based
alignment will align nothing (the bottom FSA in
Figure 1) since ?Milan? is the only lexically com-
mon word in both sentences. For much larger
clusters in our experiments, we are able to pro-
duce a significant number of novel sentences from
our alignment with such tightened syntactic con-
ditions. Figure 2 shows one of the actual clusters
used in our work that has 18 unique sentences.
Two of the many automatically generated gram-
matical sentences are also shown.
Another piece of related work, (Quirk et al,
2004), starts off with parallel inputs and uses
monolingual Statistical Machine Translation tech-
niques to align them and generate novel sentences.
In our work, the input text does not need to be
nearly as parallel.
The main contribution of this paper is a syntax-
based alignment technique for generating novel
paraphrases of sentences that describe a par-
ticular fact. Such techniques can be poten-
tially useful in multi-document summarizers such
as Newsblaster (http://newsblaster.cs.
columbia.edu) and NewsInEssence (http:
//www.newsinessence.com). Such sys-
tems are notorious for mostly reusing text from
existing news stories. We believe that allowing
them to use novel formulations of known facts will
make these systems much more successful.
2 Related work
Our work is closest in spirit to the two papers that
inspired us (Barzilay and Lee, 2003) and (Pang
et al, 2003). Both of these papers describe how
multiple sequence alignment can be used for ex-
tracting paraphrases from clustered texts. Pang et
al. use as their input the multiple human English
translations of Chinese documents provided by the
LDC as part of the NIST machine translation eval-
uation. Their approach is to merge multiple parse
trees into a single finite state automaton in which
identical input subconstituents are merged while
alternatives are converted to parallel paths in the
output FSA. Barzilay and Lee, on the other hand,
make use of classic techniques in biological se-
quence analysis to identify paraphrases from com-
parable texts (news from different sources on the
same event).
In summary, Pang et al use syntactic align-
ment of parallel texts while Barzilay and Lee
use comparable (not parallel) input but ignore
syntax. Our work differs from the two in that
we apply syntactic information on aligning com-
parable texts and that the syntactic clues we
use are drawn from Chunklink ilk.uvt.nl/
?sabine/homepage/software.html out-
put, which is further analysis from the syntactic
parse trees.
Another related paper using multiple sequence
alignment for text generation was (Barzilay and
Lee, 2002). In that work, the authors were able
to automatically acquire different lexicalizations
of the same concept from ?multiple-parallel cor-
pora?. We also draw some ideas from the Fitch-
Margoliash method for building evolutionary trees
748
1. A police official said it was a Piper tourist plane and that the crash had set the top floors on fire.
2. According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles or
other problems that would lead a pilot to lose control.
3. April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper in Milan,
setting the top floors of the 32-story building on fire.
4. Authorities said the pilot of a small Piper plane called in a problem with the landing gear to the Milan?s
Linate airport at 5:54 p.m., the smaller airport that has a landing strip for private planes.
5. Initial reports described the plane as a Piper, but did not note the specific model.
6. Italian rescue officials reported that at least two people were killed after the Piper aircraft struck the
32-story Pirelli building, which is in the heart of the city s financial district.
7. MILAN, Italy AP A small piper plane with only the pilot on board crashed Thursday into a 30-story landmark
skyscraper, killing at least two people and injuring at least 30.
8. Police officer Celerissimo De Simone said the pilot of the Piper Air Commander plane had sent out a
distress call at 5:50 p.m. just before the crash near Milan?s main train station.
9. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress call at
5:50 p.m. 11:50 a.m.
10. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before the crash near Milan?s main train station.
11. Police officer Celerissimo De Simone said the pilot of the Piper aircraft sent out a distress call at
5:50 p.m. just before the crash near Milan?s main train station.
12. Police officer Celerissimo De Simone told The AP the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before crashing.
13. Police say the aircraft was a Piper tourism plane with only the pilot on board.
14. Police say the plane was an Air Commando 8212; a small plane similar to a Piper.
15. Rescue officials said that at least three people were killed, including the pilot, while dozens were
injured after the Piper aircraft struck the Pirelli high-rise in the heart of the city s financial
district.
16. The crash by the Piper tourist plane into the 26th floor occurred at 5:50 p.m. 1450 GMT on Thursday, said
journalist Desideria Cavina.
17. The pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. just
before the crash, said police officer Celerissimo De Simone.
18. There were conflicting reports as to whether it was a terrorist attack or an accident after the pilot of
the Piper tourist plane reported that he had lost control.
1. Police officer Celerissimo De Simone said the pilot of the Piper aircraft, en route from Switzerland, sent
out a distress call at 5:54 p.m. just before the crash near Milan?s main train station.
2. Italian rescue officials reported that at least three people were killed, including the pilot, while
dozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heart
of the city s financial district.
Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment.
described in (Fitch and Margoliash, 1967). That
method and related techniques in Bioinformatics
such as (Felsenstein, 1995) also make use of a sim-
ilarity matrix for aligning a number of sequences.
3 Alignment Algorithms
Our alignment algorithm can be described as mod-
ifying Levenshtein Edit Distance by assigning dif-
ferent scores to lexically matched words according
to their syntactic similarity. And the decision of
whether to align a pair of words is based on such
syntax scores.
3.1 Modified Levenshtein Edit Distance
The Levenshtein Edit Distance (LED) is a mea-
sure of similarity between two strings named after
the Russian scientist Vladimir Levenshtein, who
devised the algorithm in 1965. It is the num-
ber of substitutions, deletions or insertions (hence
?edits?) needed to transform one string into the
other. We extend LED to sentence level by count-
ing the substitutions, deletions and insertions of
words necessary to transform a sentence into the
other. We abbreviate this sentence-level edit dis-
tance as MLED. Similar to LED, MLED compu-
tation produces an M+1 by N+1 distance matrix,
D, given two input sentences of length M and N
respectively. This matrix is constructed through
dynamic programming as shown in Figure 3.
D[i][j] =
8
>
>
<
>
>
:
0 if j = 0
0 if i = 0
max
 
D[i   1][j   1] + match;
D[i   1][j] + gap;
D[i][j   1] + gap
!
otherwise
Figure 3: Dynamic programming in computing
MLED of two sentences of length M and N.
?match? is 2 if the ith word in Sentence 1 and
the jth word in Sentence 2 syntactically match,
and is -1 otherwise. ?gap? represents the score
for inserting a gap rather than aligning, and is set
to -1. The matching conditions of two words are
far more complicated than lexical equality. Rather,
we judge whether two lexically equal words match
based on a predefined set of syntactic features.
The output matrix is used to guide the align-
ment. Starting from the bottom right entry of the
matrix, we go to the matrix entry from which the
value of the current cell is derived in the recursion
of the dynamic programming. Call the current en-
try D[i][j]. If it gets its value from D[i 1][j 1],
the ith word in Sentence 1 and the jth word in Sen-
tence 2 are either aligned or both aligned to a gap
depending on whether they syntactically match; if
the value of D[i][j] is derived from D[i][j   1] +
749
?gap?, the ith word in Sentence 1 is aligned to a
gap inserted into Sentence 2 (the jth word in Sen-
tence 2 is not consumed); otherwise, the jth word
in Sentence 2 is aligned to a gap inserted into Sen-
tence 1.
Now that we know how to align two sentences,
aligning a cluster of sentences is done progres-
sively. We start with the overall most similar pair
and then respect the initial ordering of the cluster,
aligning remaining sentences sequentially. Each
sentence is aligned against its best match in the
pool of already-aligned ones. This approach is
a hybrid of the Feng-Doolittle?s Algorithm (Feng
and Doolittle, 1987) and a variant described in
(Fitch and Margoliash, 1967).
3.2 Syntax-based Alignment
As remarked earlier, our alignment scheme judges
whether two words match according to their
syntactic similarity on top of lexical equality.
The syntactic features are obtained from run-
ning Chunklink (Buchholz, 2000) on the Charniak
parses of the clustered sentences.
3.2.1 Syntactic Features
Among all the information Chunklink provides,
we use in particular the part-of-speech tags, the
Chunk tags, and the syntactic dependence traces.
The Chunk tag shows the constituent of a word
and its relative position in that constituent. It can
take one of the three values,
 ?O? meaning that the word is outside of any
chunk;
 ?I-XP? meaning that this word is inside an
XP chunk where X = N, V, P, ADV, ...;
 ?B-XP? meaning that the word is at the be-
ginning of an XP chunk.
From now on, we shall refer to the Chunk
tag of a word as its IOB value (IOB was named
by Tjong Kim Sang and Jorn Veeenstra (Tjong
Kim Sang and Veenstra, 1999) after Ratnaparkhi
(Ratnaparkhi, 1998)). For example, in the sen-
tence ?I visited Milan Theater?, the IOB value for
?I? is B-NP since it marks the beginning of a noun-
phrase (NP). On the other hand, ?Theater? has an
IOB value of I-NP because it is inside a noun-
phrase (Milan Theater) and is not at the beginning
of that constituent. Finally, the syntactic depen-
dence trace of a word is the path of IOB values
from the root of the tree to the word itself. The
last element in the trace is hence the IOB of the
word itself.
3.2.2 The Algorithm
Lexically matched words but with different
POS are considered not syntactically matched
(e.g., race VB vs. race NN). Hence, our focus
is really on pairs of lexically matched words with
the same POS. We first compare their IOB values.
Two IOB values are exactly matched only if they
are identical (same constituent and same position);
they are partially matched if they share a common
constituent but have different position (e.g., B-PP
vs. I-PP); and they are unmatched otherwise. For
a pair of words with exactly matched IOB values,
we assign 1 as their IOB-score; for those with par-
tially matched IOB values, 0; and -1 for those with
unmatched IOB values. The numeric values of the
score are from experimental experience.
The next step is to compare syntactic depen-
dence traces of the two words. We start with the
second last element in the traces and go backward
because the last one is already taken care of by the
previous step. We also discard the front element of
both traces since it is ?I-S? for all words. The cor-
responding elements in the two traces are checked
by the IOB-comparison described above and the
scores accumulated. The process terminates as
soon as one of the two traces is exhausted. Last,
we adjust down the cumulative score by the length
difference between the two traces. Such final score
is named the trace-score of the two words.
We declare ?unmatched? if the sum of the IOB-
score and the trace-score falls below 0. Otherwise,
we perform one last measurement ? the relative
position of the two words in their respective sen-
tences. The relative position is defined to be the
word?s absolute position divided by the length of
the sentence it appears in (e.g. the 4th word of a
20-word sentence has a relative position of 0.2).
If the difference between two relative positions
is larger than 0.4 (empirically chosen before run-
ning the experiments), we consider the two words
?unmatched?. Otherwise, they are syntactically
matched.
The pseudo-code of checking syntactic match is
shown in Figure 4.
750
Algorithm Check Syntactic Match of Two Words
For a pair of words W
1
, W
2
if W
1
6= W
2
or pos(W
1
) 6= pos(W
2
) then
return ?unmatched?
endif
score := 0
iob
1
:= iob(W
1
)
iob
2
:= iob(W
2
)
score += compare iobs(iob
1
; iob
2
)
trace
1
:= trace(W
1
)
trace
2
:= trace(W
2
)
score += compare traces(trace
1
; trace
2
)
if score < 0 then
return ?unmatched?
endif
relpos
1
:= pos(W
1
)/lengthOf(S
1
)
relpos
2
:= pos(W
2
)/lengthOf(S
2
)
if jrelpos
1
  relpos
2
j  0:4 then
return ?unmatched?
endif
return ?matched?
Function compare iobs(iob
1
; iob
2
)
if iob
1
= iob
2
then
return 1
endif
if substring(iob
1
; 1) = substring(iob
2
; 1) then
return 0
endif
return  1
Function compare traces(trace
1
; trace
2
)
Remove first and last elements from both traces
score := 0
i := lengthOf(trace
1
)   1
j := lengthOf(trace
2
)  1
while i  0 and j  0 do
next := compare iobs(trace
1
[i]; trace
2
[j])
score += next  0:5
i   
j   
endwhile
score   = jlengthOf(trace
1
)  
lengthOf(trace
2
)j  0:5
return score
Figure 4: Algorithm for checking the syntactic
match between two words.
4 Evaluation
4.1 Experimental Setup
4.1.1 Data
The data we use in our experiment come from
a number of sentence clusters on a variety of top-
ics, but all related to the Milan plane crash event.
This cluster was collected manually from the Web
of five different news agencies (ABC, CNN, Fox,
MSNBC, and USAToday). It concerns the April
2002 crash of a small plane into a building in Mi-
lan, Italy and contains a total of 56 documents
published over a period of 1.5 days. To divide this
corpus into representative smaller clusters, we had
a colleague thoroughly read all 56 documents in
the cluster and then create a list of important facts
surrounding the story. We then picked key terms
related to these facts, such as names (Fasulo - the
pilot) and locations (Locarno - the city from which
the plane had departed). Finally, we automatically
clustered sentences based on the presence of these
key terms, resulting in 21 clusters of topically re-
lated (comparable) sentences. The 21 clusters are
grouped into three categories: 7 in training set, 3
in dev-testing set, and the remaining 11 in testing
set. Table 1 shows the name and size of each clus-
ter.
Cluster Number of Sentences
Training clusters
ambulance 10
belie 14
built 6
malpensa 4
piper 18
president 17
route 11
Dev-test clusters
hospital 17
rescue 12
witness 6
Test clusters
accident 30
cause 18
fasulo 33
floor 79
government 22
injur 43
linate 21
rockwell 9
spokes 18
suicide 22
terror 62
Table 1: Experimental clusters.
751
4.1.2 Different Versions of Alignment
To test the usefulness of our work, we ran 5 dif-
ferent alignments on the clusters. The first three
represent different levels of baseline performance
(without syntax consideration) whereas the last
two fully employ the syntactic features but treat
stop words differently. Table 2 describes the 5 ver-
sions of alignment.
Run Description
V1 Lexical alignment on everything possible
V2 Lexical alignment on everything but commas
V3 Lexical alignment on everything but commas and stop words
V4 Syntactic alignment on everything but commas and stop words
V5 Syntactic alignment on everything but commas
Table 2: Alignment techniques used in the experi-
ments.
Alignment Grammaticality Fidelity
V1 2.89 2.98
V2 3.00 2.95
V3 3.15 3.22
V4 3.68 3.59
V5 3.47 3.30
Table 3: Evaluation results on training and dev-
testing clusters. For the results on the test clusters,
see Table 6
The motivation of trying such variations is as
follows. Stop words often cause invalid alignment
because of their high frequencies, and so do punc-
tuations. Aligning on commas, in particular, is
likely to produce long sentences that contain mul-
tiple sentence segments ungrammatically patched
together.
4.1.3 Training and Testing
In order to get the best possible performance
of the syntactic alignment versions, we use clus-
ters in the training and dev-test sets to tune up
the parameter values in our algorithm for check-
ing syntactic match. The parameters in our algo-
rithm are not independent. We pay special atten-
tion to the threshold of relative position difference,
the discount factor of the trace length difference
penalty, and the scores for exactly matched and
partially matched IOB values. We try different pa-
rameter settings on the training clusters, and apply
the top ranking combinations (according to human
judgments described later) on clusters in the dev-
testing set. The values presented in this paper are
the manually selected ones that yield the best per-
formance on the training and dev-testing sets.
Experimenting on the testing data, we have
two hypotheses to verify: 1) the 2 syntactic ver-
sions outperform the 3 baseline versions by both
grammaticality and fidelity (discussed later) of the
novel sentences produced by alignment; and 2)
disallowing alignment on stop words and commas
enhances the performance.
4.2 Experimental Results
For each cluster, we ran the 5 alignment versions
and produce 5 FSA?s. From each FSA (corre-
sponding to a cluster A and alignment version i),
100 sentences are randomly generated. We re-
moved those that appear in the original cluster.
The remaining ones are hence novel sentences,
among which we randomly chose 10 to test the
performance of alignment version i on cluster A.
In the human evaluation, each sentence received
two scores ? grammaticality and fidelity. These
two properties are independent since a sentence
could possibly score high on fidelity even if it is
not fully grammatical. Four different scores are
possible for both criteria: (4) perfect (fully gram-
matical or faithful); (3) good (occasional errors or
quite faithful); (2) bad (many grammar errors or
unfaithful pieces); and (1) nonsense.
4.2.1 Results from the Training Phase
Four judges help our evaluation in the training
phase. They are provided with the original clusters
during the evaluation process, yet they are given
the sentences in shuffled order so that they have
no knowledge about from which alignment ver-
sion each sentence is generated. Table 3 shows
the averages of their evaluation on the 10 clusters
in training and dev-testing set. Each cell corre-
sponds to 400 data points as we presented 10 sen-
tences per cluster per alignment version to each of
the 4 judges (10 x 10 x 4 = 400).
4.2.2 Results from the Testing Phase
After we have optimized the parameter config-
uration for our syntactic alignment in the training
phase, we ask another 6 human judges to evaluate
our work on the testing data. These 6 judges come
from diverse background including Information,
Computer Science, Linguistics, and Bioinformat-
ics. We distribute the 11 testing clusters among
them so that each cluster gets evaluated by at least
3 judges. The workload for each judge is 6 clus-
ters x 5 versions/cluster x 10 sentences/cluster-
version = 300 sentences. Similar to the training
phase, they receive the sentences in shuffled or-
der without knowing the correspondence between
752
sentences and alignment versions. Detailed aver-
age statistics are shown in Table 4 and Table 5 for
grammaticality and fidelity, respectively. Each cell
is the average over 30 - 40 data points, and notice
the last row is not the mean of the other rows since
the number of sentences evaluated for each cluster
varies.
Cluster V1 V2 V3 V4 V5
rockwell 2.27 2.93 3.00 3.60 3.03
cause 2.77 2.83 3.07 3.10 2.93
spokes 2.87 3.07 3.57 3.83 3.50
linate 2.93 3.14 3.26 3.64 3.77
government 2.75 2.83 3.27 3.80 3.20
suicide 2.19 2.51 3.29 3.57 3.11
accident 2.92 3.27 3.54 3.72 3.56
fasulo 2.52 2.52 3.15 3.54 3.32
injur 2.29 2.92 3.03 3.62 3.29
terror 3.04 3.11 3.61 3.23 3.63
floor 2.47 2.77 3.40 3.47 3.27
Overall 2.74 2.75 3.12 3.74 3.29
Table 4: Average grammaticality scores on testing
clusters.
Cluster V1 V2 V3 V4 V5
rockwell 2.25 2.75 3.20 3.80 2.70
cause 2.42 3.04 2.92 3.48 3.17
spokes 2.65 2.50 3.20 3.00 3.05
linate 3.15 3.27 3.15 3.36 3.42
government 2.85 3.24 3.14 3.81 3.20
suicide 2.38 2.69 2.93 3.68 3.23
accident 3.14 3.42 3.56 3.91 3.57
fasulo 2.30 2.48 3.14 3.50 3.48
injur 2.56 2.28 2.29 3.18 3.22
terror 2.65 2.48 3.68 3.47 3.20
floor 2.80 2.90 3.10 3.70 3.30
Overall 2.67 2.69 3.07 3.77 3.23
Table 5: Average fidelity scores on testing clusters.
2.00
2.20
2.40
2.60
2.80
3.00
3.20
3.40
3.60
3.80
4.00
ro
ck
w
ell
ca
us
e
sp
ok
es
lin
ate
go
ve
rn
m
en
t
su
icid
e
ac
cid
en
t
fas
ulo inju
r
ter
ro
r
flo
or
V 1
V 2
V 3
V 4
V 5
Figure 5: Performance of 5 alignment versions by
grammaticality.
2.00
2.20
2.40
2.60
2.80
3.00
3.20
3.40
3.60
3.80
4.00
ro
ck
w
ell
ca
us
e
sp
ok
es
lin
ate
go
ve
rn
m
en
t
su
ici
de
ac
cid
en
t
fas
ulo inju
r
ter
ro
r
flo
or
V 1
V 2
V 3
V 4
V 5
Figure 6: Performance of 5 alignment versions by
fidelity.
4.3 Result Analysis
The results support both our hypotheses. For Hy-
pothesis I, we see that the performance of the
two syntactic alignments was higher than the non-
syntactic versions. In particular, Version 4 outper-
forms the the best baseline version by 19.9% on
grammaticality and by 22.8% on fidelity. Our sec-
ond hypothesis is also verified ? disallowing align-
ment on stop words and commas yields better re-
sults. This is reflected by the fact that Version 4
beats Version 5, and Version 3 wins over the other
two baseline versions by both criteria.
At the level of individual clusters, the syntactic
versions are also found to outrival the syntax-blind
baselines. Applying a t-test on the score sets for
the 5 versions, we can reject the null hypothesis
with 99.5% confidence to ensure that the syntactic
alignment performs better. Similarly, for hypoth-
esis II, the same is true for the versions with and
without stop word alignment. Figures 5 and 6 pro-
vide a graphical view of how each alignment ver-
sion performs on the testing clusters. The clusters
along the x-axis are listed in the order of increas-
ing size.
We have also done an analysis on interjudge
agreement in the evaluation. The judges are in-
structed about the evaluation scheme individually,
and do their work independently. We do not en-
force them to be mutually consistent, as long as
they are self-consistent. However, Table 6 shows
the mean and standard deviation of human judg-
ments (grammaticality and fidelity) on each ver-
sion. The small deviation values indicate a fairly
high agreement.
Finally, because human evaluation is expensive,
we additionally tried to use a language-model ap-
753
Alignment Gr. Mean Gr. StdDev Fi. Mean Fi. StdDev
V1 2.74 0.11 2.67 0.43
V2 2.75 0.08 2.69 0.30
V3 3.12 0.07 3.07 0.27
V4 3.74 0.08 3.77 0.16
V5 3.29 0.16 3.23 0.33
Table 6: Mean and standard deviation of human
judgments.
proach in the training phase for automatic eval-
uation of grammaticality. We have used BLEU
scores(Papineni et al, 2001), but have observed
that they are not consistent with those of human
judges. In particular, BLEU assigns too high
scores to segmented sentences that are otherwise
grammatical. It has been noted in the literature
that metrics like BLEU that are solely based on
N-grams might not be suitable for checking gram-
maticality.
5 Conclusion
In this paper, we presented a paraphrase genera-
tion method based on multiple sequence alignment
which combines traditional dynamic program-
ming techniques with linguistically motivated syn-
tactic information. We apply our work on compa-
rable texts for which syntax has not been success-
fully explored in alignment by previous work. We
showed that using syntactic features improves the
quality of the alignment-induced finite state au-
tomaton when it is used for generating novel sen-
tences. The strongest syntax guided alignment sig-
nificantly outperformed all other versions in both
grammaticality and fidelity of the novel sentences.
In this paper we showed the effectiveness of us-
ing syntax in the alignment of structurally diverse
comparable texts as needed for text generation.
References
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
Lexical Choice via Multiple-Sequence Alignment.
In Proceedings of EMNLP 2002, Philadelphia.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
NAACL-HLT03, Edmonton.
Sabine Buchholz. 2000. Readme
for perl script chunklink.pl.
http://ilk.uvt.nl/ sabine/chunklink/README.html.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis. Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Joseph Felsenstein. 1995. PHYLIP:
Phylogeny Inference Package.
http://evolution.genetics.washington.edu/phylip.html.
DF. Feng and Russell F. Doolittle. 1987. Progres-
sive sequence alignment as a prerequisite to correct
phylogenetic trees. Journal of Molecular Evolution,
25(4).
Walter M. Fitch and Emanuel Margoliash. 1967.
Construction of Phylogenetic Trees. Science,
155(3760):279?284, January.
Dan Gusfield, 1997. Algorithms On Strings: A Dual
View from Computer Science and Computational
Molecular Biology. Cambridge University Press.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT/NAACL 2003, Ed-
monton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A Method for Automatic
Evaluation of Machine Translation. Research Re-
port RC22176, IBM.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142?
149, Barcelona, Spain, July. Association for Com-
putational Linguistics.
A Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Phd. The-
sis, University of Pennsylvania.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In EACL, pages 173?179.
754
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 45?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LexNet: A Graphical Environment for Graph-Based NLP
Dragomir R. Radev
 
, Gu?nes? Erkan
 
, Anthony Fader

,
Patrick Jordan
 
, Siwei Shen
 
, and James P. Sweeney

Department of Electrical Engineering and Computer Science
School of Information
Department of Mathematics
University of Michigan
Ann Arbor, MI 48109

radev, gerkan, afader, prjordan, shens, jpsweeney@umich.edu
Abstract
This interactive presentation describes
LexNet, a graphical environment for
graph-based NLP developed at the Uni-
versity of Michigan. LexNet includes
LexRank (for text summarization), bi-
ased LexRank (for passage retrieval), and
TUMBL (for binary classification). All
tools in the collection are based on random
walks on lexical graphs, that is graphs
where different NLP objects (e.g., sen-
tences or phrases) are represented as nodes
linked by edges proportional to the lexi-
cal similarity between the two nodes. We
will demonstrate these tools on a variety of
NLP tasks including summarization, ques-
tion answering, and prepositional phrase
attachment.
1 Introduction
We will present a series of graph-based tools for a
variety of NLP tasks such as text summarization,
passage retrieval, prepositional phrase attachment,
and binary classification in general.
Recently proposed graph-based methods
(Szummer and Jaakkola, 2001; Zhu and Ghahra-
mani, 2002b; Zhu and Ghahramani, 2002a;
Toutanova et al, 2004) are particularly well
suited for transductive learning (Vapnik, 1998;
Joachims, 1999). Transductive learning is based
on the idea (Vapnik, 1998) that instead of splitting
a learning problem into two possibly harder
problems, namely induction and deduction, one
can build a model that covers both labeled and
unlabeled data. Unlabeled data are abundant as
well as significantly cheaper than labeled data in
a variety of natural language applications. Parsing
and machine translation both offer examples of
this relationship, with unparsed text from the Web
and untranslated texts being computationally less
costly. These can then be used to supplement
manually translated and aligned corpora. Hence
transductive methods are of great potential for
NLP problems and, as a result, LexNet includes a
number of transductive methods.
2 LexRank: text summarization
LexRank (Erkan and Radev, 2004) embodies the
idea of representing a text (e.g., a document or a
collection of related documents) as a graph. Each
node corresponds to a sentence in the input and the
edge between two nodes is related to the lexical
similarity (either cosine similarity or n-gram gen-
eration probability) between the two sentences.
LexRank computes the steady-state distribution of
the random walk probabilities on this similarity
graph. The LexRank score of each node gives
the probability of a random walk ending up in
that node in the long run. An extractive summary
is generated by retrieving the sentences with the
highest score in the graph. Such sentences typ-
ically correspond to the nodes that have strong
connections to other nodes with high scores in the
graph. Figure 1 demonstrates LexRank.
3 Biased LexRank: passage retrieval
The basic idea behind Biased LexRank is to label
a small number of sentences (or passages) that are
relevant to a particular query and then propagate
relevance from these sentences to other (unanno-
tated) sentences. Relevance propagation is per-
formed on a bipartite graph. In that graph, one
of the modes corresponds to the sentences and
the other ? to certain words from these sentences.
Each sentence is connected to the words that ap-
pear in it. Thus indirectly, each sentence is two
hops away from any other sentence that shares
words in it. Intuitively, the sentences that are
close to the labeled sentences tend to get higher
scores. However, the relevance propagation en-
45
Figure 1: A sample snapshot of LexRank. A 3-
sentence summary is produced from a set of 11
related input sentences. The summary sentences
are shown as larger squares.
ables us to mark certain sentences that are not im-
mediate neighbors of the labeled sentences via in-
direct connections. The effect of the propagation
is discounted by a parameter at each step so that
the relationships between closer nodes are favored
more. Biased LexRank also allows for negative
relevance to be propagated through the network as
the example shows. See Figures 2? 3 for a demon-
stration of Biased LexRank.
Figure 2: Display of Biased LexRank. One sen-
tence at the top is annotated as positive while an-
other at the bottom is marked negative. Sentences
are displayed as circles and the word features are
shown as squares.
Figure 3: After convergence of Biased LexRank.
4 TUMBL: prepositional phrase
attachment
A number of NLP problems such as word sense
disambiguation, text categorization, and extractive
summarization can be cast as classification prob-
lems. This fact is used to great effect in the de-
sign and application of many machine learning
methods used in modern NLP, including TUMBL,
through the utilization of vector representations.
Each object is represented as a vector   of fea-
tures. The main assumption made is that a pair of
objects   and  will be classified the same way
if the distance between them in some space  is
small (Zhu and Ghahramani, 2002a).
This algorithm propagates polarity information
first from the labeled data to the features, capturing
whether each feature is more indicative of posi-
tive class or more negative learned. Such informa-
tion is further transferred to the unlabeled set. The
backward steps update feature polarity with infor-
mation learned from the structure of the unlabeled
data. This process is repeated with a damping fac-
tor to discount later rounds. This process is illus-
tracted in Figure 4. TUMBL was first described
in (Radev, 2004). A series of snapshots showing
TUMBL in Figures 5? 7.
5 Technical information
5.1 Code implementation
The LexRank and TUMBL demonstrations are
provided as both an applet and an application.
The user is presented with a graphical visualiza-
tion of the algorithm that was conveniently de-
veloped using the JUNG API (http://jung.
sourceforge.net/faq.html).
46
(a) Initial graph (b) Forward pass
(c) Backward
pass
(d) Convergence
Figure 4: TUMBL snapshots: the circular vertices
are objects while the square vertices are features.
(a) The initial graph with features showing no bias.
(b) The forward pass where objects propagate la-
bels forward. (c) The backward pass where fea-
tures propagate labels backward. (d) Convergence
of the TUMBL algorithm after successive itera-
tions.
Figure 5: A 10-pp prepositional phrase attachment
problem is displayed. The head of each preposi-
tional phrase is ine middle column. Four types of
features are represented in four columns. The first
column is Noun1 in the 4-tuple. The second col-
umn is Noun2. The first column from the right is
verb of the 4-tuple while the second column from
the right is the actual head of the prepositional
phrase. At this time one positive and one negative
example (high and low attachment) are annotated.
The rest of the circles correspond to the unlabeled
examples.
Figure 6: The final configuration.
47
Figure 7: XML file corresponding to the PP at-
tachment problem. The XML DTD allows layout
information to be encoded along with algorithmic
information such as label and polarity.
In TUMBL, each object is represented by a cir-
cular vertex in the graph and each feature as a
square. Vertices are assigned a color according to
their label. The colors are assignable by the user
and designate the probability of membership of a
class.
To allow for a range of uses, data can be
entered either though the GUI or read in from
an XML file. The schema for TUMBL files is
shown at http://tangra.si.umich.edu/
clair/tumbl.
In the LexRank demo, each sentence becomes a
node. Selected nodes for the summary are shown
in larger size and in blue while the rest are smaller
and drawn in red. The link between two nodes has
a weight proportional to the lexical similarity be-
tween the two corresponding sentences. The demo
also reports the metrics precision, recall, and F-
measure.
5.2 Availability
The demos are available both as locally based and
remotely accessible from http://tangra.
si.umich.edu/clair/lexrank and
http://tangra.si.umich.edu/clair/
tumbl.
6 Acknowledgments
This work was partially supported by the U.S.
National Science Foundation under the follow-
ing two grants: 0329043 ?Probabilistic and link-
based Methods for Exploiting Very Large Textual
Repositories? administered through the IDM pro-
gram and 0308024 ?Collaborative Research: Se-
mantic Entity and Relation Extraction from Web-
Scale Text Document Collections? run by the HLC
program. All opinions, findings, conclusions, and
recommendations in this paper are made by the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML ?99.
Dragomir Radev. 2004. Weakly supervised graph-
based methods for classification. Technical Report
CSE-TR-500-04, University of Michigan.
Martin Szummer and Tommi Jaakkola. 2001. Partially
labeled classification with Markov random walks. In
NIPS ?01, volume 14. MIT Pres.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk mod-
els for inducing word dependency distributions. In
ICML ?04, New York, New York, USA. ACM Press.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. Wiley-Interscience.
Xiaojin Zhu and Zoubin Ghahramani. 2002a. Learn-
ing from labeled and unlabeled data with label prop-
agation. Technical report, CMU-CALD-02-107.
Xiaojin Zhu and Zoubin Ghahramani. 2002b. Towards
semi-supervised classification with Markov random
fields. Technical report, CMU-CALD-02-106.
48
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
Centroid-based summarization of multiple documents: sentence 
extraction, utility-based evaluation, and user studies 
Dragomir R. Radev 
School of Information 
University of Michigan 
Ann Arbor, MI 48103 
radev@umich.edu 
Hongyan Jing 
Department o f  Computer Sc ience 
Columbia University 
New York, NY 10027 
hjing@cs.columbia.edu 
Malgorzata Budzikowska 
IBM TJ Watson Research Center 
30 Saw Mill River Road 
Hawthorne, NY 10532 
sm I@us.ibm.com 
Abstract 
We present a multi-document summarizer, called 
MEAD, which generates summaries using 
cluster centroids produced by a topic detection 
and tracking system. We also describe two new 
techniques, based on sentence utility and 
subsumption, which we have applied to the 
evaluation of both single and multiple document 
summaries. Finally, we describe two user studies 
that test our models of multi-document 
summarization. 
1 Introduction 
On October 12, 1999, a relatively small number of 
news sources mentioned in passing that Pakistani 
Defense Minister Gen. Pervaiz Musharraf was away 
visiting Sri Lanka. However, all world agencies 
would be actively reporting on the major events that 
were to happen in Pakistan in the following days: 
Prime Minister Nawaz Sharif announced that in Gen. 
Musharrafs absence, the Defense Minister had been 
-sacked and replaced by General Zia Addin. Large 
numbers of  messages from various ources tarted to 
inundate the newswire: about the army's occupation 
of the capital, the Prime Minister's ouster and his 
subsequent placement under house arrest, Gen. 
Musharrafs return to his country, his ascendancy to 
power, and the imposition of military control over 
Pakistan. 
The paragraph above summarizes a large amount of 
news from different sources. While it was not 
automatically generated, one can imagine the use of 
such automatically generated summaries. In this 
paper we will describe how multi-document 
summaries are built and evaluated. 
1.1 Topic detection and multi-document 
summarization 
The process of identifying all articles on an emerging 
event is called Topic Detection and Tracking (TDT). 
A large body of research in TDT has been created 
over the past two years \[Allan et al, 98\]. We will 
present an extension of our own research on TDT 
\[Radev et al, 1999\] to cover summarization f multi- 
document clusters. 
Our entry in the official TDT evaluation, called 
CIDR ~adev et al, 1999\], uses modified TF*IDF to 
produce clusters of  news articles on the same event. 
We developed a new technique for multi-document 
summarization (or MDS), called centroid-based 
summarization (CBS) which uses as input the 
centroids of the clusters produced by CIDR to 
identify which sentences are central to the topic of  
the cluster, rather than the individual articles. We 
have implemented CBS in a system, named MEAD. 
The main contributions of this paper are: the 
development of a centroid-based multi-document 
summarizer, the use of cluster-based sentence utility 
(CBSU) and cross-sentence informational 
subsumption (CSIS) for evaluation of single and 
multi-document summaries, two user studies that 
support our findings, and an evaluation of MEAD. 
An event cluster, produced by a TDT system, 
consists of chronologically ordered news articles 
from multiple sources, which describe an event as it 
develops over time. Event clusters range from2 to 10 
documents from which MEAD produces ummaries 
in the form of sentence xtracts. 
A key feature of MEAD is its use of  cluster centroids, 
which consist of words which are central not only to 
one article in a cluster, but to all the articles. 
MEAD is significantly different from previous work 
on multi-document summarization \[Radev & 
McKeown, 1998; Carbonell and Goldstein, 1998; 
Mani and Bloedorn, 1999; MeKeown et aI., 1999\], 
21 
which use techniques such as graph matching, 
maximal marginal relevance, or language generation. 
Finally, evaluation of  multi-document summaries i a 
difficult problem. There is not yet a widely accepted 
evaluation scheme. We propose a utility-based 
evaluation scheme, which can be used to evaluate 
both single-document and multi-document 
summaries. 
2 Informational content of sentences 
2.1 Cluster-based sentence utility (CBSU) 
Cluster-based sentence utility (CBSU, or utility) 
refers to the degree of relevance (from 0 to 10) of a " 
particular sentence to the general topic of the entire 
cluster (for a dis cussion of  what is a topic, see \[Allan 
et al 1998\]). A utility of 0 means that the sentence is 
not relevant to the cluster and a 10 inarks an essential 
sentence. 
2.2 Cross -sentence  in fo rmat iona l  
subsumption (CS IS)  
A related notion to CBSU is cross-sentence 
informational subsumption (CSIS, or subsumption), 
which reflects that certain sentences repeat some of  
the information present in other sentences and may, 
therefore, be omitted during summarization. If the 
information content of  sentence a (denoted as i(a)) is 
contained within sentence b, then a becomes 
informationally redundant and the content of b is said 
to subsume that of a: 
i(a) c: i(b) 
In the example below, (2) subsumes (1) because the 
crucial information in (1) is also included in (2) 
which presents additional content: "the court", "last. 
August", and "sentenced him to life". 
(1) John Doe was found guilty of the murder. 
(2) The court found John Doe guilty of the murder 
of Jane Doe last August and sentenced him to life. 
The cluster shown in Figure I shows subsumption 
links across two articles ~ about recent terrorist 
activities in Algeria (ALG 18853 and ALG 18854). 
An arrow from sentence A to sentence B indicates 
that the information content of A is subsumed by the 
information content of B. Sentences 2, 4, and 5 from 
the first article repeat he information from sentence 
I The full text of these articles is shown in the 
Appendix. 
2 in the second article, while sentence' 9 from the 
former article is later repeated in sentences 3 and 4 of 
the latter article. 
Figure 1: Subsumption links across two articles: 
ALG 18853 and ALG 18854. 
2.3 Equivalence classes of sentences 
Sentences subsuming each other are said to belong to 
the same equivalence class. An equivalence class 
may contain more than two sentences within the 
same or different articles. In the following example, 
although sentences (3) and (4) are not exact 
paraphrases of each other, they can be substituted for 
each other without crucial loss of information and 
therefore belong to the same equivalence class, i.e. 
i(3) c i(4) and i(4) c i(3). In the user study section 
we will take a look at the way humans perceive CSIS 
and equivalence class. 
(3) Eighteen decapitated bodies have been found 
in a mass grave in northern Algeria, press reports 
said Thursday. 
(4) Algerian newspapers have reported on 
Thursday that 18 decapitated bodies have been 
found by the authorities. 
2.4 Comparison with MMR 
Maximal marginal relevance (or MMR) is a 
technique similar to CSIS and was introduced in 
\[Carbonell and Goldstein, 1998\]. In that paper, MMR 
is used to produce summaries of single documents 
that avoid redundancy. The authors mention that their 
preliminary results indicate that multiple documents 
on the same topic also contain redundancy but they 
fall short of using MMR for multi-document 
summarization. Their metric is used as an 
enhancement to a query-based summary whereas 
CSIS is designed for query-independent (a.k.a., 
generic) summaries. 
22 
I 
I 
II 
3 MEAD: a centroid-based multi- 
document summarizer 
We now describe the corpus used for the evaluation 
of MEAD, and later in this section we present 
MEAD's algorithm. 
Cluster # does # sent source news sources topic 
clari.world.africa.northwestem AFP, UPI Algerian terrorists threaten Belgium A 
B 
'C 
D 
2 25 
3 45 clari.world.terrorism 
2 65 clari.wodd.europe.russia 
7 189 clari.world.europe.russia 
I 0 151 TDT-3 corpus topic 78 
3 83 TDT-3 corpus topic 67 
AFP, UP! 
AP, AFP 
AP, AFP, UPI 
AP, PRI, VOA 
AP, NYT 
The FB1 puts Osama bin Laden on 
the most wanted list 
Explosion in a Moscow apartment 
building (September 9, 1999) 
Explosion in a Moscow apartment 
building (September 13, 1999) 
General strike in Denmark 
Toxic spill in Spain 
Table 1: Corpm comi~osition 
3.1 Descr ip t ion  o f  the  corpus 
For our experiments, we prepared, a small corpus 
consisting of a total of 558 sentences in 27 
documents, organized in 6 clusters (Table 1), all 
extracted by CIDR. Four of the clusters are from 
Usenet newsgroups. The remaining two clusters are 
from the official TDT corpus 2. Among the factors for 
our selection of clusters are: coverage of as many 
news sources as possible, coverage of both TDT and 
non-TDT data, coverage of different ypes of news 
(e.g., terrorism, internal affairs, and environment), 
and diversity in cluster sizes (in our case, from 2 to 
10 articles). The test corpus is used in the evaluation 
in such a way that each cluster is summarized at 9 
different compression rates, thus giving nine times as 
many sample points as one would expect from the 
size of the corpus. 
3.2 Cluster centroids 
Table 2 shows a sample centroid, produced by CIDR 
\[Radev et al, 1999\] from cluster A. The "count" 
column indicates the average number of occurrences 
of a word'across the entire cluster. The IDF values 
were computed from the TDT corpus. A centroid, in 
this context, is a pseudo-document which consists of 
words which have Count*IDF scores above a pre- 
defined threshold in the documents that constitute the 
cluster. CIDR computes Count*IDF in an iterative 
fashion, updating its values as more articles are 
inserted in a given cluster. We hypothesize that 
sentences that contain the words from the centroid 
are more indicative of the topic of the cluster. 
2 The selection of Cluster E is due to an idea by the 
participants in the Novelty Detection Workshop, led 
by James Allan. 
Word Count IDF  Count  * IDF  
belgium 
gia 
algerian 
hayat 
algeria 
islamic 
melouk 
arabic 
battalion 
15.50 4.96 76.86 
7.50 8.39 62.90 
6.00 6.36 38.15 
3.00 8.90 26.69 
4.50 5.63 25.32 
6.00 4.13 24.76 
2.00 10.00 19.99 
3.00 5.99 17.97 
2.50 7.16 17.91 
Table 2: Sample c*ntroid produced by CIDR 
3.3 Cent ro id -based  a lgor i thm 
MEAD decides which sentences to include in the 
extract by ranking them according to a set of  
parameters. The input to MEAD is a cluster of 
articles (e.g., extracted by CIDR) and a value for the 
compression rate r. For example, if  the cluster 
contains a total of 50 sentences (n = 50) and the 
value of r is 20%, the output of MEAD will contain 
10 sentences. Sentences are laid in the same order as 
they appear in the original documents with 
documents ordered chronologically. We benefit here 
from the time stamps associated with each document. 
SCORE (s) = Z i  (wcC, + + wpJ  
where i (1 ~ i ~_ n) is the sentence number within 
the cluster. 
INPUT: Cluster of d documents 3 with n sentences 
(compression rate = r) 
3 Note that currently, MEAD requires that sentence 
boundaries be marked. 
23 
4.2.3 System performance (S) 
The system performance S is one of the numbers 6 
described in the previous subsection. For { 13}, the 
value of S is 0.627 (which is lower than random). For 
{14}, S is 0.833, which is between R and J. In the 
example, only two of the six possible sentence 
selections, {14} and {24} are between R and J. Three 
others, {13}, {23}, and {34} are below R. while {12} 
is better than J. 
4.2.4. Normalized system performance (1)) 
To restrict system performance (mostly) between 0 
and 1, we use a mapping between R and J in such a 
way that when S ffi R, the normalized system 
performance, D, is equal to 0 and when S = J, D 
becomes 1. The corresponding linear function 7is: 
D = (S-R) / (J-R) 
Figure 2 shows the mapping .between system 
performance S on the left (a) and normalized system 
performance D on the fight Co). A small part of the 0- 
i segment is mapped to the entire 0-1 segment; 
therefore the difference between two systems, 
performing at e.g., 0.785 and 0.812 can be 
significant! 
I J9~ 
. i :0.$414 
S-0.8331 
R - f lT J2  ~ 
05 --  
09 
(a) 
:.-...-.:. --. : -. . . . .  ...:......:...-.'.'. 
"% 
% 
- r - l .0  
S" .  0.9"Ze/- D 
O5 
I t ' -  0.0 
Ib) 
Figure 2: Performance mapping 
Example: the normalized system performance for the 
{14} system then becomes (0.833 - 0.732)/(0.841 - 
0.732) or 0.927. Since the score is close to I, the 
{14} system is almost as good as the interjudge 
agreement. The normalized system performance for 
the {24} system is similarly (0.837 - 0.732) / (0.841 
7 The formula is valid when J > R (that is, the judges 
agree among each other better than randomly). 
- 0.732) or 0.963. Of the two systems, {24} 
outperforms { 14}. 
4.3 Using CSIS to evaluate multi-document 
summaries 
To use CSIS in the evaluation, we introduce a new 
parameter, E, which tells us how much to penalize a
system that includes redundant information. In the 
example from Table 7 (arrows indicate subsumption), 
a summarizer with r = 20% needs to pick 2 out of 12 
sentences. Suppose that it picks 1/I and 2/1 (in bold). 
l fE  = 1, it should get full credit of 20 utility points. If  
E = 0, it should get no credit for the second sentence 
as it is subsumed by the first sentence. By varying E 
between 0 and 1, the evaluation may favor or ignore 
subsumption. 
Senti 
Sent2 
Sent3 
Sent4 
Article l Article2 Article3 
10 ~10 5 
8 9 8 
5 6 
Table 7: Sample subsumption table (12 sentences, 
3 articles) 
5 User studies and system evaluation 
We ran two user experiments. First, six judges were 
each given six clusters and asked to ascribe an 
importance score from 0 to 10 to each sentence 
within a particular cluster. Next, five judges had to 
indicate for each sentence which other sentence(s), if  
any, it subsumes s.
5.1 CBSU: interjudge agreement 
Using the techniques described in Section 0, we 
computed the cross-judge agreement (J) for the 6 
clusters for various r (Figure 3). Overall, interjudge 
agreement was quite high. An interesting drop in 
interjudge agreement occurs for 20-30% summaries. 
The drop most likely results from the fact that 10% 
summaries are typically easier to produce because the 
few most imporiant sentences in a cluster are easier 
to identify. 
s We should note that both annotation tasks were 
quite time consuming and frustrating for the users 
who took anywhere from 6 to 10 hours each to 
complete their part. 
26 
I 
I 
I 
I 
I 
I 
I 
l 
I 
I 
iota. 
J S  
Figure 3: Cross-judge agreement (J) on the CBSU 
annotation task. 
5.2 CSIS:  in ter judge  agreement  
In the second experiment, we asked users to indicate 
all cases when within a cluster, a sentence is 
subsumed by another. The judges' data on the first 
seven sentences of cluster A are shown in Table 8. 
The "+ score" indicates the number of judges who 
agree on the most frequent subsumption. The '-* 
score" indicates that the consensus was no 
subsumption. We found relatively low interjudge 
agreement on the cases in which at least one judge 
indicated evidence of subsumption. Overall, out of 
558 sentences, there was full agreement (5judges) on 
292 sentences (Table 9). Unfortunately, h 291 of 
these 292 sentences the agreement was that there is 
no subsumption. When the bar of agreement was 
lowered to four judges, 23 out of 406 agreements are 
on sentences with subsumption. Overall, out of 80 
sentences with subsumption, only 24 had an 
agreement of four or more judges. However, in 54 
eases at least hree judges agreed on the presence of a 
particular instance of subsumption. 
Sentence 
AI-I 
A1-2 
AI-3 
AI-4 
AI-5 
A1-6 
A1-7 
Judge1 
A2-5 
A2-10 
Judge2 Judge3 Judge4 Judge5 + score - score 
A2-1 A2-1 A2-1 3 
A2-5 A2-5 3 
- A2-10 
A2-10 A2-10 A2-10 4 
A2-1 - A2-2 A2-4 
- - A2-7 
- A2-8 
Table 8: Judges' indication for subsumption for the first seven sentences in cluster A 
# iudszes a~reeinf 
Cluster .4 
-6 
0 7 
1 6 
3 6 
I I 
ClusterB Cluster C ClusterD ClusterE 
+ + + + 
0 24 0 45 0 88 I 73 
3 6 1 10 9 37 8 35 
4 5 4 4 28 20 5 23 
2 1 1 0 7 0 7 0 i 
Table 9: lnterjudge CSIS agreement 
In conclusion, we found very high interjudge 
agreement in the first experiment and moderately 
low agreement in the second experiment. We 
concede that the time necessary to do a proper job 
at the second task is partly to blame. 
5.3 Eva luat ion  o f  MEAD 
Since the baseline of random sentence selection is 
already included in the evaluation formulae, we 
used the Lead-based method (selecting the 
Cluster F 
+ 
0 61 
0 I1 
3 7 
1 '0  
positionally first (n'r/e) sentences from each cluster 
where c -- number of clusters) as the baseline to 
evaluate our syt;tem. 
In Table 10 we show the normalized performance 
(D) of MEAD, for the six clusters at nine 
compression rates. MEAD performed better than 
Lead in 29 (in bold) out of 54 cases. Note that for 
the largest cluster, Cluster D, MEAD outperformed 
Lead at all compression rates. 
27 
Cluster A 
Cluster B 
Cluster C 
Cluster D 
Cluster E 
Cluster F 
10% 20% 30% 40% 50% 60% 70% 80% 90% 
0.855 0.572 0.427 0,759 0.862 0.910 0.554 1.001 0.584 
0.365 0A02 0.690 0.714 0.867 0.640 0.845 0.713 1.317 
0.753 0,938 0.841 1.029 0.751 0.819 0.595 0.611 0.683 
0.739 0.764 0.683 ?0.723 0.614 0.568 0.668 0.719 1.100 
1.083 0.937 0.581 0.373 0.438 0.369 0A29 0A87 0.261 
1.064 0.893 0.928 1.000 0.732 0,805 0,910 0.689 0.199 
Table 10: Normalized performance (D) of MEAD 
I 
I 
I 
I 
I 
We then modified the MEAD algorithm to include 
lead information as well as centroids (see Section 0). 
In this case, MEAD+Lead performed better than the 
Lead baseline in 41 cases. We are in the process of 
running experiments with other SCORE formulas. 
5.4 D iscuss ion  
It may seem that utility-based evaluation requires too 
much effort and is prone to low interjudge agreement. 
We believe that our results show that interjudge 
agreement is quite high. As far as the amount of  
effort required, we believe that the larger effort on 
the part of the judges is more or less compensated 
with the ability to evaluate summaries off-line and at 
variable compression rates. Alternative valuations 
don't make such evaluations possible. We should 
concede that a utility-based approach is probably not 
feasible for query-based summaries as these are 
typically done only on-line. 
We discussed the possibility of  a sentence 
contributing negatively to the utility of another 
sentence due to redundancy. We should also point out 
that sentences can also reinforce one another 
positively. For example, if  a sentence mentioning a
new entity is included in a summary, one might also 
want to include a sentence that puts the entity in the 
context of the re?t of the article or cluster. 
6 Contributions and future work 
We presented a new multi-document summarizer, 
MEAD. It summarizes clusters of news articles 
automatically grouped by a topic detection system. 
MEAD uses information from the centroids of the 
clusters to select sentences that are most likely to be 
relevant to the cluster topic. 
We used a new utility-based technique, CBSU, for 
the evaluation of  MEAD and of summarizers in 
general. We found that MEAD produces ummaries 
that are similar in quality to the ones produced by 
humans. We also compared MEAD's performance to 
an alternative method, multi-document lead, and 
28 
showed how MEAD's sentence scoring weights can 
be modified to produce summaries significantly 
better than the alternatives. 
We also looked at a property of multi-document 
chisters, namely cross-sentence information 
subsumption (which is related to the MMR metric 
proposed in \[Carbonell and Goldstein, 1998\]) and 
showed how it can be used in evaluating multi- 
document summaries. 
All our findings are backed by the analysis of two 
experiments hat we performed with human subjects. 
We found that the interjudge agreement on sentence 
utility is very high while the agreement on cross- 
sentence subsumption is moderately low, although 
promising. 
In the future, we would like to test our 
multidocument summarizer on a larger corpus and 
improve the summarization algorithm. We would 
also like to explore how the techniques we proposed 
here can be used for multiligual multidocument 
summarization. 
7 Acknowledgments 
We would like to thank Inderjeet Mani, Wlodek 
Zadrozny, Rie Kubota Ando, Joyce Chai, and Nanda 
Kambhatla for their valuable feedback. We would 
also like to thank Carl Sable, Min-Yen Kan, Dave 
Evans, Adam Budzikowski, and Veronika Horvath 
for their help with the evaluation. 
References 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang, Topic 
detection and tracking pilot study: final report, In 
Proceedings of the Broadcast News Understanding 
and Transcription Workshop, 1998. 
Jaime Carbonell and Jade Goldstein. The use of 
MMR, diversity-based reranking for reordering 
documents and producing summaries. In 
Proceedings of ACM-SIGIR'98, Melbourne, 
Australia, August 1998. 
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and 
Jaime Carbonell, Summarizing Text Documents: 
Sentence Selection and Evaluation Metrics, In 
Proceedings of ACM-SIGIR'99, Berkeley, CA, 
August 1999. 
Th6r6se Hand. A Proposal for Task-Based Evaluation 
of Text Summarization Systems, in Mani, I., and 
Maybury, M., eds., Proceedings of the 
ACL/EACL'97 Workshop on Intelligent Scalable 
Text Summarization, Madrid, Spain, July 1997. 
Hongyan Jing, Regina Barzilay, Kathleen McKeown, 
and Michael Elhadad, Summarization Evaluation 
Methods: Experiments and Analysis, In Working 
Notes, AAAI Spring Symposium on Intelligent 
Text Summarization, Stanford, CA, April 1998. 
Inderjeet Mani and Eric Bloedorn, Summarizing 
Similarities and Differences Among Related 
Documents, Information Retrieval 1 (l-2), pages 
35-67, June 1999. 
29 
Inderjeet Mani, David House, Gary Klein, Lynette 
Hirschman, Leo Orbst, Th6r6se Firmin, Michael 
Chrzanowski, and Beth Sundheim The TIPSTER 
SUMMAC text summarization evaluation. 
Technical Report MTR98W0000138, MITRE, 
McLean, Virginia, October 1998. 
Inderjeet Mani and Mark Maybury. Advances in 
Automatic Text Summarization. MIT Press, 1999. 
Kathleen McKeown, Judith Klavans, Vasileios 
Hatzivassiloglou, Regina Barzilay, and Eleazar 
Eskin, Towards Multidocument Summarization by 
Reformulation: Progress and Prospects, In 
Proceedings of AAAI'99, Orlando, FL, July 1999. 
Dragomir R. Radev and Kathleen McKeown. 
Generating natural language summaries from 
multiple on-line sources. Computational 
Linguistics, 24 (3), pages 469-500, September 
1998. 
Dragomir R. Radev, Vasileios Hatzivassiloglou, and 
Kathleen R. McKeown. A description of the CIDR 
system as used for TDT-2. In DARPA Broadcast 
News Workshop, Herndon, VA, February 1999. 
Appendix 
ARTICLE 18853: ALGIERS, May 20 (AFP) 
I. Eighteen decapitated bodies have been found !n a "'1 
mass grave in northern Algeria, press reports aid I ~, 
Thursday, adding that two shepherds were murdered \['~ 
earlier this week. 1 
2. Security forces found the mass grave on Wednesday 
at Chbika, near Djelfa, 275 kilometers (170 miles) 
south of the capital. 
3. It contained the bodies of people killed last year 
during a wedding ceremony, according to Le Quotidien 
Liberte. 
4. The victims included women, children and old men. 
5. Most of them had been decapitated and their heads 
thrown on a road, reported the Es Sahara. 
6. Another mass grave containing the bodies of around 
10 people was discovered recently near Algiers, in the 
Eucalyptus district. 
7. The two shepherds were killed Monday evening by a 
group of nine armed lslamists near the Moulay Slissen 
forest. 
8. After being injured in a hail of automatic weapons 
fire, the pair were finished offwith machete blows 
before being decapitated, Le Quotidien d'Oran reported. 
9. Seven people, six of them children, were killed and 
two injured Wednesday by armed lslamists near 
Medea, 120 kilometers (75 miles) south of Algiers, 
security forces said. 
10. The same day a parcel bomb explosion injured 17 
people in Algiers itself. 
11. Since early March, violence linked to armed 
Islamists has claimed more than 500 lives, according to 
press tallies. 
ARTICLE 18854: ALGIERS, May 20 (UPI) 
J !. Algerian ewspapers have reported that 18 
F\] decapitated bodies have been found by authorities 
in the south of the country. 
2. Police found the "decapitated bodies of women, 
rchildren and old men,with their heads thrown on a 
f road" near the town of Jelfa, 275 kilometers (170 
miles) south of the capital Algiers. 
3. In another incident on Wednesday, seven people 
-- including six children -- were killed by terrorists, 
Algerian security forces said. 
4. Extremist Muslim militants were responsible for 
the slaughter of the seven people in the province of 
Medea, 120 kilometers (74 miles) south of Algiers. 
5. The killers also kidnapped three girls during the 
same attack, authorities said, and one of the girls 
was found wounded on a nearby road. 
6. Meanwhile, the Algerian daily Le Matin today 
quoted Interior Minister Abdul Malik Silal as 
saying that "terrorism has not been eradicated, but 
the movement of the terrorists has significantly 
declined" 
7. Algerian violence has claimed the lives of more 
than 70,000 people since the army cancelled the 
1992 general elections that Islamic parties were 
likely to win. 
8. Mainstream Islamic groups, most of which are 
banned in the country, insist heir members are not 
responsible for the violence against civilians. 
9. Some Muslim groups have blamed the army, 
while others accuse "foreign elements conspiring 
against Algeria." 
30 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
l 
I 
I 
I 
I 
I 
I 
I 
I 
A Common Theory of Information Fusion 
from Multiple Text Sources 
Step One: Cross-Document Structure 
Dragomir R. Radev 
550 E. University St. 
University of Michigan 
Ann Arbor, MI 48109 
radev@umi ch. edu 
Abst ract  
We introduce CST (cross-document 
slructure theory), a paradigm for multi- 
document analysis. CST takes into aceount 
the rhetorical structure o f  clusters of related 
textual documents. We present a taxonomy 
of cross-document relationships. We argue 
that CST can be the basis for multi- 
document summarization guided by user 
preferences for summary length, information 
provenance, cross-source agreement, and 
chronological ordering of  facts. 
1 Int roduct ion  
The Topic Detection and Tracking model (TDT) 
\[Allan et al 98\] describes news events as they 
are reflected in news sources. First, many 
sources write on the same event and, second, the 
same source typically produces a number of 
accounts of  the event over a period of  time. 
Sixteen news stories related to the same event 
from six news sources over a two-hour time 
period are represented in Figure 1. 
- i  II I 
I I 
_ I ! I I, 
_1  I I 
_ I I 
I1! I i 
06:30 06:45 07:00 07:15 07:30 07:45 08:00 08:15 08:30 
Figure 1 : Time distribution of related documents 
from multiple sources 
A careful analysis of related news articles hows 
that they exhibit some interesting properties 
\[Radev & McKeown 98\]. In some cases, 
different sourees agree with each other, at other 
times, the information presented in them is 
contradictory. The same source sometimes adds 
new information as it becomes available and 
puts it in the context of  what has already been 
discussed earlier. In other eases, to get a full 
picture of  an event, one has to read stories from 
multiple sources as neither of  them presents all 
relevant information. All these examples point 
to the existence of  cross-document structure that 
is waiting to be exploited. 
Figure 2 illustrates how the same story can be 
told in several different ways. The six extracts 
are from news stories about the same event: the 
declaration by Bill Clinton at a press conference 
that millions of  dollars will be handed out to low 
income people affected by recent surges in oil 
pFices. 
In this paper we introduce CST, a theory of  
cross-document s ructure. CST assigns labels 
74 
such as subsumption, update, and elaboration to
cross-document conceptual links. We argue that 
CST is essential for the analysis of 
contradiction, redundancy, and complementarity 
in related documents and for multi-document 
summarization (MDS). 
FoxNews: President Clinton announced Wednesday he will release funds to help New Englanders through the 
heating oil "crisis" and spoke on topics ranging from gun violence to his personal life and foreign policy. 
The president said he ordered the release of $125 million from the Low Income Home Energy Assistance Program to 
help families who must rely on oil to heat heir homes. He said that amount was all that is left in the fund this year. 
ABCN~s:  President Clinton today ordered the release of  millions of dollars in assistance for Northeastern families 
struggling with soaring fuel costs, saying Americans have together to help their fellow citizens in times of need. 
The release of $120 million from the Low Income Home Energy Assistance Program is to help families who must 
rely on oil to heat heir homes, he said ... 
CNN: Citing rising energy costs, President Clinton said Wednesday he is releasing $120 million in funds to help 
families buy home heating oil in the mid-Atlantic and northeastern states. 
The president said lie decided the help was needed heeause steadily rising oil prices have slammed consumers 
dependent on home heating oil and have boosted the cost of,gasoline. 
MSNBC: AT A WASHINGTON, D.C. news conference, Clinton announced that he had ordered the release of the 
remainder of federal heating assistance money to help families in the Northeast hit hard by this year's cold weather 
and the soaring cost of  home heating oil. 
Clinton said lie ordered the release of $125 million from the Low Income Home Energy Assistance Program to help 
families who must rely on oil to heat heir homes. He said that amount was all that is left in the fund this year. 
USA Today: President Clinton, saying too many families are being hurt by the soaring cost of heating their homes, 
announced Wednesday he will ask Congress for $600 million in emergency assistance tohelp people meet heating 
oil costs. He also released an additional $125 million for more immediate help to low-income families. 
NY Times: WASHINGTON, Feb. 16 - President Clinton ordered the release today of another $125 million in 
government aid to help needy families pay the soaring costs of heating their homes this winter. 
The release, announced by Mr. Clinton at the start of a White House news conference this afternoon, comes only six 
days after the government made $130 million in home-heating aid available. 
Figure 2: Six different accounts of the same event 
2 Related Work  
2.1 Document structure 
Rhetorical Structure Theory (RST) \[Mann & 
Thompson 88, Mann 00\] is a comprehensive 
theory of text organization. It is based on "text 
coherence", orthe presence in "earefuUy written 
text" of unity that would not appear in random 
sequences ofsentences. RST posits the existence 
of relations among sentences. Most relations 
consist of one or more nuclei (the central 
components of a rhetorical relation) and zero or 
more satellites (the supporting components of 
the relation). An example of an RST relation is 
evidence which is decomposed into a nuelens (a 
claim) and a satellite (text that supports the 
claim). RST is intentionally limited to single 
documents. With CST, we attempt to describe 
the rhetorical structure of sets of related 
documents. Unlike RST, CST cannot rely on the 
deliberateness of writing style. We can however 
make use of some observations of structure 
across documents which, while clearly not 
deliberate in the RST sense, can be quite 
predictable and useful. In a sense, CST 
associates a certain behavior to a "collective 
document author" (that is, the collectivity of all 
authors of the related ocuments). 
A pioneering study in the typology of links 
among documents i described in \[Trigg 83, 
Trigg & Weiser 87\]. Trigg introduces a 
taxonomy of link types across cientific papers. 
The 80 suggested link types such as citation, 
refutation, revision, equivalence, and 
comparison are grouped in two categories: 
Normal (inter-document li ks) and Commentary 
(deliberate cross-document links). While the 
taxonomy is quite exhaustive, it is by no means 
appropriate or intended for general domain texts 
(that is, other than scientific articles). 
75 
A large deal of research in the automatic 
induction of document and hyperdocument 
structure is due to Salton's group at Cornell 
\[Salton et al 91\]. \[Allan 96\] presents a graph 
simplification technique for "hyperlink typing", 
that is, assigning link types from Trigg's list to 
links between sentences or paragraphs of a pair 
of documents. Allan tested his techniques on 
sets of very distinct articles (e.g. "John F. 
Kennedy" and "United States of America" from 
the Funk and Wagnalls encyclopedia). As the 
author himself admits, the evaluation in \[Allan 
96\] is very weak and doesn't indicate to any 
extent whether the techniques actually achieve 
anything useful. 
More recently, \[Salton et al 97\] introduced a
technique for document structuring based on 
semantic hyperlinks (among pairs of paragraphs 
which are related by a lexieal similarity 
significantly higher than random). The authors 
represent single documents from the Funk and 
Wagnalls encyclopedia on topics such as 
Abortion or Nuclear Weapons in the form of text 
relationship maps. These maps exploit the 
bushiness (or number of connecting edges) of a 
paragraph to decide whether to include it in a 
summary of the entire article. The assumption 
underlying their technique isthat bushypaths (or 
paths connecting highly connected paragraphs) 
are more likely to contain information central to 
the topic of the article. The summarization 
techniques described in Salton et al's research 
are limited to single documents. 
One of the goals of CST is to extend the 
techniques set forth in Trigg, Salton, and Allan's 
work to cover sets of related documents in 
arbitrary domains. 
2.2 Multi-document summarization 
SUMMONS \[Radev & McKeown 98\] is a 
knowledge-based multi-document 
summarization system, which produces 
summaries of a small number of news articles 
within the domain of terrorism. SUMMONS 
uses as input a set of semantic templates 
extracted by a message understanding system 
\[Fisher et al 96\] and identifies ome patterns in 
them such as chang e of perspective, 
contradiction, refinement, agreement, and 
elaboration. The techniques used in SUMMONS 
involved a large amount of knowledge 
engineering even for a relatively small domain 
of text (such as accounts of terrorist events) and 
is not directly suitable for domain-independent 
text analysis. The planning operators used in it 
present, however, the ideal first step towards 
CST. 
\[Mani & Bloedorn 99\] use similarities and 
differences among related news articles for 
MDS. They measure the effectiveness of their 
method in two scenarios: paragraph ahgnment 
across two articles and query-based information 
retrieval. None of these scenarios evaluates the 
generation of query-independent summaries of 
multiple articles in open domains. 
The Stimulate projects at Columbia University 
\[Barzflay & al. 99\], \[McKeown & al. 99\] have 
been using natural language generation to 
produce multi-document summaries. Their 
technique iscalled theme intersection: paragraph 
alignment across news stories with the help of a 
semantic network to identify phrases which 
convey the same meaning and then generate new 
sentences from each theme and order them 
chronologically toproduce a summary. 
We should note here that RST has been used to 
produce single-document summaries \[Marcu 
97\]. For multi-document summaries, CST can 
present areasonable equivalent to RST. 
2_3 Time-dependent documents 
Time-dependent documents are related to the 
observation that perception of an event changes 
over time and include (a) evolving summaries 
(summaries of new documents related to an 
ongoing event that are presented to the user 
assuming that he or she has read earlier 
summaries of related documents) \[Radev 99\] 
and (b) chronological briefings \[Radev & 
McKeown 98\]. \[Carbonell et al 98\] discuss the 
motivation behind the use of time-dependent 
documents and \[Berger & Miller 98\] describe a
language model for time-dependent corpora. 
76 
3 Representing cross-document 
structure 
We will introduce two complementary data 
structures to represent multi-document clusters: 
the multi-document cube (Section 0) and the 
multi-document graph (Section 0). 
3.1 Multi-document cubes 
Definition A multi-document cube C (see Figure 
3 (a)) is a three dimensional structure that 
represents related documents. The three 
dimensions are t (time), s (source) and p 
(position within the document). 
Def'mition A document unit U is a tuple (t,s,p) -
see Figure 3 (b). Document units can be defined 
at different levels of granularity, e.g., 
paragraphs, entences, or words. 
Definition A document D is a sequence of 
document units U1U2... Un which corresponds to 
a one-dimensional projection of a multi- 
document cube along the source and time 
dimensions. 
Some additional concepts can be defined based 
on the above definitions. 
Definition A snapshot is a slice of the multi- 
document cube over a period of time At - see 
Figure 3 (c). 
Definition An evolving document is a slice of 
the multi-document cube in which the source is 
fixed and time and position may vary. 
Definition An extractive summary S of a cube C 
is a set of  document units, S c C, see Figure 3 
(d). 
Definition A summarization operator 
transforms a cube C into a summary S. 
(a) Co) 
(c) (d) 
Figure 3: (a) A multi-document cube, (b) A document unit, (c) A cube slice, (d) An extracted 
summary 
3.2 Multi-document graphs 
While multi-document cubes are a useful 
abstraction, they cannot easily represent ext 
simultaneously at different levels of granularity 
(words, phrases, sentences, paragraphs, and 
documents). The second formalism that we 
introduce is the multi-document graph. Each 
graph consists of smaller subgraphs for each 
individual document (Figure 4). We use two 
types of links. The first type represents 
inheritance r lationships among elements within 
a single document. These links are drawn using 
thicker lines. The second type represents 
semantic relationships among textual units. The 
example illustrates sample links among 
documents, phrases, sentences, and phrases. 
77 
4 A taxonomy of cross-document 
relationships 
(W), phrases (P), sentences orparagraphs (S), or 
entire documents (D). The examples are from 
our MDS corpus (built from TDT and Web- 
based sources). 
Figure 5 presents a proposed, taxonomy of cross- 
document relationships. The Level column 
indicates whether the relation applies to words 
? f ? sssJ" " " " "  x " link cross-sent~ exit 
i ! , /  
i I / I  ! / ; word l ink _ - . , , v ,  ~ ,, ! \.- % !, j 
,, ...-" i 
t . . . .  I I  
\ 11 
- . . . .  " ~  . . . .  - \~ , / '  wo~i~d 
~..,..~ _../..~ i Paragraph/semence level 
# 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
Figure 4: Sample 
Relationship type Level 
Identity Any 
Equivalence (paraphrasing) S, D 
Translation P, S 
Subsumption S, D 
Contradiction S, D 
Historical background S 
Cross-reference P 
Citation S, D 
Modality S 
Attribution S 
Summary S, D 
Follow-up S 
multi-document graph 
Description 
The same text appears in more than one location 
Two text spans have the same information content 
Same information content in different languages 
One sentence contains more information than another 
Conflicting information 
Information that puts current information i context 
The same entity is mentioned 
One sentence cites another document 
Qualified version of a sentence 
One sentence r peats the information of another while 
adding an attribution 
Similar to Summary in RST: one textual unit summarizes 
another 
Additional information which reflects facts that have 
happened since the last account 
78 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
Elaboration 
Indirect speech 
Refinement 
Agreement S 
Judgment S 
Fulfilment S 
Description S 
Reader profde S 
Contrast S 
Parallel S 
Generalization S 
Change of perspective S, D 
Additional information that wasn't included in the last 
account 
ShiR from direct o indirect speech or vice-versa 
Additional information that is more specific that the one 
previously included 
One source expresses agreement with another, 
A qualified account of  a fact 
A prediction turned true 
Insertion of  a description 
Style and background-specific change 
Contrasting two accounts or facts 
Comparing two accounts of  facts 
Generalization 
The same source presents a fact in a different light 
Figure 5: Sample types of edges (relationships between textual spans) 
One example of a cross-document relationship is
the cross-sentence informational subsumption 
(CSIS, or subsumption), which reflects that 
certain sentences repeat some of  the information 
present in other sentences and may, under 
certain circumstances, be omitted during 
summarization. In the following example, 
sentence (2) subsumes (1) because the crucial 
information in (1) is also included in (2) which 
presents additional content: "the court", "last 
August", and "sentenced him to life". 
(1) John Doe was found guilty of the murder. 
(2) The court found John Doe guilty of the 
murder of Jane Doe last August and 
sentenced him to life. 
e.g., by referring to a person arrested at a crime 
scene as an "alleged" or "suspected" perpetrator. 
(5) Adams reportedly called for an emergency 
meeting with Trirnblc to lry to salvage the 
assembly. 
(6) Sirra Fein leader Gerry Adams appealed for 
an urgent meeting with Trimble. 
(7) The GIA is the most hardline of the Islamic 
militant groups which have fought he 
Algerian authorities since 1992. 
(8) The GIA is seen as most hardline of the 
Islamic militant groups which have fought he 
Algerian government during the past seven 
years. 
Paraphrase 
(3) Ford's program will be launched in the 
United States in April and globally within 
12 months. 
(4) Ford plans to introduce the program first for 
its employees inthe United States, then 
expand it for workers abroad. 
Modality 
New stories are often written in a way  that 
makes misattributions of information difficult, 
Attribution 
(9) In the strongest ign yet that Russia's era of 
space glo~ is coming to an end, space 
officials announced today that cosmonauts 
will leave the Mir space station in August and 
it will remain unmanned. 
(I O) The crew aboard the Mir space station will 
leave in August, and the craft will orbit the 
Earth unmanned until early next year. 
Indirect Speech 
79 
(I 1) An anonymous caller told the Interfax news 
agency that the Moscow explosion and a 
Saturday night bomb blast in southern Russia 
were in response to Russia's military 
campaign against Islamic, rebels in the 
southern territory of Dagestan. 
(12) An anonymous caller to Interfax said the 
blast and a car-bomb earlier this week at a 
military apartment building in Dagestan were 
"our response to the bombing of villages in 
Chechnya and Dagestan." 
Followup 
(13) Denmark's largest industrial unions have 
rejected awage proposal, setting the stage for 
a nationwide general strike, officials 
announced Friday. 
(14) A national strike entered its second week 
Monday, paralyzing Denmark's main airport 
and leaving most gasoline stations out of fuel 
and groceries short of frozen and canned 
foods. 
Judgment 
(15) Hardline militants of A\]geria's Armed 
Islamic Group (GIA) threatened Sunday to 
create a "bloodbath" in Belgium if the 
authorities there do not release several of its 
leaders jailed last month. 
(16) The GIA is demanding that Belgium 
release several of its leaders jailed in Belgium 
last month. 
Fulfillment 
(17) WASHINGTON, May 31 The Federal 
Bureau of Investigation plans to put 
suspected terrorist Osarna bin Laden, sought 
in connection with the bombings of the US 
embassy bombings in Afr/ea, on its "Ten 
Most Wanted" list, CNN reported Saturday. 
(18) WASHINGTON, June 7 The Federal 
Bureau of Investigation added Saudi fugitive 
Osama Bin Laden, sought for his part in the 
1998 bombings of US embassies inAfrica, to 
its "Ten Most Wanted List" Monday. 
Elaboration 
(19) Fugitive Saudi national bin Laden is 
believed to be the mastermind behind last 
year's bloody attacks against US embassies in
Kenya and Tanzania. 
(20) Bin Laden, 41, is believed to be the 
mastermind behind last year's bloody attacks 
against US embassies in Kenya and Tanzania. 
Update 
(21) The confirmed eath toll has already reached 
49, while over 50 people are still unaccounted 
for, many presumed dead and buried in the 
ruins. 
(22) The con.firmed eath toll has already reached 
60, and another 40 people are still unaccounted 
for, most presumed dead and buried in the 
ruins. 
Definition 
(23) Yeltsin said the security forces must unite to 
fight terrorists, adding that he had appointed 
Interior Minister Vladimir Rushailo to head 
a special tea m coordinating anti-terrorist 
activities. 
(24) Yeltsin said the security forces must unite to 
fight terrorists, adding that he had named 
Rushailo to head a special team coordinating 
anti-terrorist activities. 
Contrast 
(25) Agriculture Minister Loyola de Palacio 
estimated the loss at dlrs 10 million. 
(26) Agriculture Minister Loyola de Palacio has 
estimated losses from mined produce at 1.5 
billion pesetas (dlrs 10 million), although 
farmers groups earlier claimed total damages 
of nearly eight imes that amount. 
Historical background 
(27) Elian's mother and 10 others died when their 
boat sank as they tried to reach the United 
States from Cuba. 
5 Using CST for information fusion 
In this section we describe how CST can be used 
to generate personalized multi-document 
summaries from clusters of related articles in 
four steps: ehstering, document structure 
analysis, link analysis, and personalized graph- 
based summarization (Figure 6). 
The first stage, clustering, can be either query- 
independent (e.g., based on pure document 
80 
similarity [Allan et al 98]) or based on a user 
query (in which case clusters will be the sets of 
documents returned by a search engine). 
The second stage, document analysis, includes 
the generation of  document trees representing 
the sentential and phrasal structure of the 
document [Hearst 94, Kan et al 98]. 
: \ 
4. Stnnmar izat ion  
Figure 6: Processing stages 
G ~ 
/ 
/ 
/ 
\ 
\ 
DOC2 
_ J  
Figure 7: Summarization using graph cover operators 
The third stage is the automatic creation and 
typing of links among textual spans across 
documents. Four techniques for identifying 
related textual units across documents can be 
used: lexical distance, lexical chains, 
information extraction, and linguistic template 
matching. Lexical distance (see e.g., [Allan 
96]) uses cosine similarity across pairs of 
sentences. Lexieal chains [Barzilay & Elhadad 
97] are more robust than lexical matching as 
they take into account linguistic phenomena 
such as synonymy and hypernymy. The third 
technique, information extraction [Radev & 
McKeown 98] identifies alient semantic roles 
in text (e.g., the place, perpetrator, and effect 
of a terrorist event) and converts them to 
semantic templates. Two textual units are 
considered related whenever their semantic 
templates are related. Finally, a technique that 
will be used to identify some relationships 
such as citation, contradiction, and attribution 
is template matching which takes into account 
transformational grammar (e.g., relative clause 
insertion). For link type analysis, machine 
learning using lexieal metrics and cue words is 
most appropriate (see [Kupiec et al 95], 
[Cohen & Singer 96]). 
81 
The final step is summary extraction, based on 
the user-specified constraints on the 
summarizer. A graph-based operator defines a
transformation on a multi-document graph 
(MDG) G which preserves some of its 
properties while reducing the number of 
nodes. An example of such an operator is the 
link-preserving graph cover operator (Figure 
7). Its effect is to preserve only these nodes 
from the source MDG that are associated with 
the preferred cross-document links. In the 
example, the shaded area represents the 
summary subgraph G" of G that contains all 
four cross-document links and only these 
nodes and edges of G which are necessary to 
preserve the textual structure of G'. 
Sumzo~ 1 
The ~th~ of Elian Gotmd~ arrived Thmtday 
in the United State* saying he w~ated U.S. 
authorities to hand over his r.~ as soon as  
p~s~l?: ,~o be could hug ~ nnd take hkn 
back to Cuba. 
TMt* ctb~s whe w~ gnmted visat to mtved 
to the United Stats with the Gomml~ family - 
Elinn~ pediauieiaaa, l nded'Urn teadaer and a 
male cousin -- wen not oil the pfalae. 
Summary 2 
The father of ~ Gcn~alez m-bred Thursday 
in the United States saying he vamtod U~. 
audaotltles to hand owe his son a~ soon as  
p~ss~le so he could hug Elian and take him 
back to Cuba. 
Three eche*s whe were g~mted vlsas to travel to 
the United States wilh the Gonzalez f~uqy -
Eliia~ pediatrician, kindel.gartel~ ~ and a 
male cousin - were not on the plane. 
The U,S. govermmmt proved itself iatramigent 
On April $, on the issue of the vlsas l ~  by 
Cuba fev a delegation composed of childn~, 
~ton  saxl p~holo~..~ts that wou~ 
acc~ml~my EliZa's father to that eoutm3, to 
receive custody of the child, reports 
Pmasa Lamina from Washington. 
The child's motha" aud I0 othea's were Idlled 
whim the boat sank 8s tl~y tfiod to flee Cuba for 
the United States. Elima and two adohe sa~vlved. 
(a) (b) 
Figure 8: Two summaries from the 
5.1 Example 
The example in Figure 8 shows two 
summaries based on different user preferences. 
Summary (b) is based on "longer extract", 
"report background information", and "include 
all sources". Summary (a) is generated from 
two CNN articles, while (b) is generated from 
two CNN articles plus one ffirom the Gramna of 
Havana, and one from ABC News. 
6 Ongoing work and conclusion 
6.1 Ongoing work 
We are in the process of performing a user 
study to collect interagreement data among 
judges who are asked to label cross-document 
rhetorical relations. 
We are also currently building a system for 
automatic identification of relationships in 
document clusters as well as a library of 
summarization perators. User preferrenees are 
used to constrain the summarizers. For 
example, a user may prefer that in the event of 
same set of input documents 
contradiction, both sources of information 
should be represented in the summary. 
Another user may have preferences for a given 
source over all others and choose an operator 
which will only reflect his preferred source. 
We will facilitate the user's navigation in the 
space of all possible summarizers. By 
specifying their preferences, users will build 
their own summarizers and test them on a 
collection of documents and then refine them 
to fit their needs. 
6.2 Conclusion 
We introduced a theory of cross-document 
structure based on inter-document 
relationships uch as paraphrase, citation, 
attribution, modality, and development. We 
presented a taxonomy of cross-document 
links. We argued that a CST-based analysis of 
related documents can facilitate multi- 
document summarization. 
82 
References 
James Allan. "Automatic hypertext link typing". 
Hypertext'96, The Seventh ACM Conference on 
Hypertext, pages 42--52. 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yaruron, and Yiming Yang, "Topic 
detection and tracking pilot study: final report". 
Proceedings of the Broadcast News 
Understanding and Transcription Workshop, 
1998. 
Regina Barzilay and Michael Elhadad. "Using 
Lexical Chains for Text Summarization". 
Proceedings of the ACL/EACL 97 Workshop on 
Intelligent Scalable Text Summarization. Madrid, 
Spain, July 1997, Pages 10---17. 
Regina Barzilay, Kathleen McKeown, and Michael 
Elhadad. "Information Fusion in the Context of 
Multi-Document Summarization". A CL "99. 
College Park, Maryland, June 1999. 
Adam Berger and Robert Miller. "Just in Time 
Language Modelling". IEEE Conference on 
Acoustic, Speech and Signal Processing. Seattle, 
WA. 
Jaime Carbonell and Jade Goldstein. "The use of 
MMR, diversity-based reranking for reordering 
documents and producing summaries". 
Proceedings of ACM-SIGIR 98. Melbourne, 
Australia, August 1998. 
Jairne Carbonell, Mark Craven, Steve Fienberg, 
Tom Mitchell, and Yiming Yang. "Report on the 
CONALD Workshop on Learning from Text and 
the Web", Pittsburgh, PA, June 1998. 
William Cohen and Yoram Singer. "Context- 
sensitive learning methods for text 
categorization". Proceedings, 19th Annual 
International ACM SIGIR Conference on 
Research and Development in Information 
Retrieval, Zurich, Switzerland, August 1996. 
Pages 307--315. 
David Fisher, Stephen Soderland, Joseph 
McCarthy, Fangfang Feng, and Wendy Lehnert. 
"Description of the UMass System As Used for 
MUC-6". Proceedings of the Sixth Message 
Understanding Conference (MUC-6). 1995. 
Pages 221--236. 
Marti Hearst. "Multi-Paragraph Segmentation of 
Expository Text". Proceedings of the 32 ~d 
Annual Meeting of the Association for 
Computational Linguistics. Las Cruces, NM, 
June 1994. 
Min-Yen Kan, Judith L. Klavans, and Kathleen 
MeKeown. "Linear segmentation and segment 
relevance". Proceedings of 6 ~ International 
Workshop of Very Large Corpora (WVLC-6), 
pages 197--205, Montreal, Quebec, Canada, 
August 1998. 
Julian Kupiec, Jan Pedersen, and Francine Chen. 
"A Trainable Document Summarizer". 
Proceedings, 18th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval. Seattle, WA, July 1995. 
Indexjjeet Mani and Eric Bloedom, "Summarizing 
Similarities and Differences Among Related 
Documents", Information Retrieval 1 (1-2), 
pages 35--67, June 1999. 
William Mann and Sandra Thompson. "Rhetorical 
Structure Theory: Toward a functional theory of 
text organization". Text, 8(3). 243-281. 
William Mann. Rhetorical Structure Theory Web 
Site. http://www.sil.org/linguistics/RST/ 
Daniel Marcu. "From Discourse Structures to Text 
Summaries". Proceedings of the ACL/EACL 
Workshop on Intelligent Scalable Text 
Summarization, Madrid, Spain, July 1997. 
Kathleen McKeown, Judith Klavans, Vasileios 
Hatzivassiloglou, Regina Barzilay, and Eleazar 
Eskin, "Towards Multidocument Summarization 
by Reformulation: Progress and Prospects", 
Proceedings of AAA1"99, Orlando, FL, July 
1999. 
Dragomir R. Radev and Kathleen McKeown. 
"Generating natural language summaries from 
multiple on-line sources". Computational 
Linguistics, 24 (3), pages 469--500, September 
1998. 
Dragomir R. Radev. "Topic Shift Detection -
finding new information in threaded news". 
Technical Report CUCS-026-99, Columbia 
University Department of Computer Science. 
January 1999. 
Gerard Salton, Chris Buckley and James Allan. 
"Automatic structuring of text files". Technical 
Report TR 91-1241, Computer Science 
Department, Comell University, Ithaca, NY, 
1991. 
Cmard Salton, Amit Singhal, Mandar Mitra, Chris 
Buckley. "Automatic Text Structuring and 
Summarization". Information Processing and 
Management 33 (2), pages 193--207, 1997. 
Randall Trigg. "A Network-Based Approach to 
Text Handling for the Online Scientific 
Community". Ph.D. Thesis. Department of 
Computer Science, University of Maryland. 
November 1983. 
Randall Trigg and Mark Weiser. "TEXTNET: A 
network-based approach to text handling". ACM 
Transactions on Office Information Systems, 4
(1), pages l J23,  January 1987. 
83 
Automatic summarization of search engine hit lists 
Dragomir R. Radev 
School of Information, University of Michigan 
550 E. University St. 
Ann Arbor,/vii 48109 
radev@umich, edu 
Weiguo Fan 
University of Michigan Business School 
701 Tappan St. 
Ann Arbor, M148109 
wfan@umich, edu 
Abstract 
We present our work on open-domain 
multi-document summarization in the 
framework of Web search. Our system, 
SNS (pronounced "essence"), retrieves 
documents related to an unrestricted user 
query and summarizes a subset of them as 
selected by the user. We present a task- 
based extrinsic evaluation of the quality of 
the produced multi-document summaries. 
The evaluation results show that 
summarization quality is relatively high 
and does help improve the reading speed 
and judge the relevance of the retrieved 
URLs. 
1 Introduction 
Online information is increasingly available at 
an exponential rate. According to a recent 
study by NetSizer (2000), the number of web 
hosts has increased from 30 million in 
Jan.1998 to 44 million in Jan. 1999, and to 
more than 70 million in Jan. 2000. More than 
2 million new hosts were added to the Internet 
in Feb. 2000, according to this report. Similar 
Internet growth results were reported by 
Intemet Domain Service (IDS, 2000). The 
number of web pages on the Intemet was 320 
million pages in Dec. 1997 as reported by 
Lawrence t al. (1997), 800 million in Feb. 
1999 (Lawrence t al. 1999), and more than 
1,720 million in March, 2000 (Censorware, 
2000). The number of pages available on the 
Internet alost doubles every ear. 
To help alleviate the information overload 
problem and help users find the information 
they need, many search engines emerge. They 
build a huge centralized atabase to index a 
portion of the Intemet: ranging from 10 
million to more than 300 million of web 
pages. Search engines do help reduce the 
information overload problem by allowing a 
user to do a centralized search, but they also 
bring up another problem for the user: too 
many web pages are returned for a single 
query. To find out which documents are 
useful, the user often have to sift through 
hundreds of pages to find out that only a few 
of them are relevant. Moreover, browsing 
through the long list of retrieval results is so 
tedious that few users would be willing to go 
through. That's why research results have 
shown that search engine users often give up 
their search in the first try, examining no more 
than 10 documents (Jansen et al 2000). It 
would be very helpful if an effective search 
engine could be designed to help classify the 
retrieved web pages into clusters and provide 
more contextual nd summary information to 
help these users explore the retrieval set more 
efficiently. 
Recent advances in information retrieval, 
natural language processing, computational 
linguistics make it easier to build a helpful 
search engine based on summaries of hit lists. 
We describe in this paper a prototype system, 
SNS, which blends the traditional information 
retrieval technology with the advanced 
document clustering and multi-document 
summarization technology in an integrated 
framework. The following steps are performed 
for a given query: 
99 
Figure 1: Architecture diagram 
The general architecture of our system is 
shown in Figure 1. User interaction with SNS 
can be done in three different modes: 
? Web search mode. The user enters a 
general-domain query in the search engine 
(MySearch). The result is a set of related 
documents (the hit-list). The user then 
selects which of  the hits should be 
summarized. MEAD, the summarization 
component produces a cross-document 
summary of the documents selected by the 
user from the hit list. 
? Intranet mode. The user indicates what 
collection of documents needs to be 
summarized. These documents are not 
necessarily extracted from the Web. 
? Clustering mode. The user indicates that 
either the hit list of the search engine or a 
stand-alone document collection needs to 
be clustered. CIDR, the clustering 
component, creates clusters of documents. 
For each cluster, MEAD produces a cross- 
document summary. 
Our paper is organized as follows. Sections 2 
4 describe the system. More specifically: 
Section 2 explains how the search engine 
operates, Section 3 deals with the clustering 
module while Section 4 presents the multi- 
document summarizer. Section 5 describes the 
user interface of  the system. In Section 6, we 
present some experimental results. After we 
compare our work to related research in 
Section 7, we conclude the paper in Section 8. 
2 Search 
The search component of SNS is a 
personalized search engine called MySearch. 
MySearch utilizes a centralized relational 
database to store all the URL indexes and 
other related URL information. Spiders are 
used to fetch URLs from the Internet. After a 
URL is downloaded, the following steps are 
applied to index the URL: 
? Parse the HTML file, remove all those 
tags 
? Apply Porter's stemming algorithms to 
each keyword. 
? Remove stop words 
? Index each keyword into the database 
along with its frequency and position 
information. 
The contents of URLs are indexed based on 
the locations of the keywords: Anchor, Title, 
and Body. This allows weighted retrieval 
based on different word positions. For 
example, a user can specify that he'd like to 
give a weight 5 for the keyword appearing in 
the title, 4 for anchor, and 2 for body. This 
information can be saved in his personal 
profile and used for later weighted ranking. 
Besides the weighted search, MySearch also 
supports Boolean search and Vector Space 
search (Salton, 1989). For the vector space 
model, the famous TF-IDF is used for ranking 
purpose. We used a modified version of TF- 
IDF: log(or+O.5)*log(N/df), where if means the 
number of times a term appeared in the 
content of an URL, N is the total number of 
documents in the text collection, and dfstands 
for the number of  unique URLs in which a 
term appears in the entire collection. 
A user can choose which search method he 
wants to use. He/she can also combine 
Boolean search with Vector Space search. 
These options are provided to give users more 
flexibility to control the retrieval results as 
100 
past research indicated that different ranking 
functions give different performances (Salton, 
1989). 
A sample search for "Clinton" using the TF- 
IDF Vector Space search is shown in Figure 3. 
The keyword "Clinton" is highlighted using a 
different color to help users get more 
contextual information. The retrieval status 
value is shown in a bold black font after the 
URL title. 
3 Clustering 
Our system uses two types of clustered input- 
either the set of hits that the user has selected 
or the output of our own clustering engine -
CIDR (Columbia Intelligent Document 
Relater). CIDR is described in (Radev et al, 
1999). It uses an iterative algorithm that 
creates as a side product so-called "document 
centroids". The centroids contain the most 
highly relevant words to the entire cluster (not 
to the user query). We use these words to find 
the most salient "themes" in the cluster of 
documents. 
3.1 Finding themes within clusters 
One of the underlying assumptions behind 
SNS is that when a user selects a set of hits 
after reading the single-document summaries 
from the hit list retrieved by the system, he or 
she performs a cognitive activity whereby he 
or she selects documents which appear to be 
related to one or more common themes. The 
multi-document summarization algorithm 
attempts to identify these themes and to 
identify the most salient passages from the 
selected ocuments using a pseudo-document 
called the cluster centroid which is computed 
automatically from the entire list of hits 
selected by the user. 
3.2 Computing centroids 
Figure 2 describes a sample of a cluster 
centroid. The TF column indicates the average 
term frequency of a given term within the 
cluster. E.g., a TF value of 13.33 for three 
documents indicates that the term "'deny" 
appears 40 times in the three documents. The 
IDF values are computed from a mixture of 
200 MB of news and web-based ocuments. 
Term TF IDF Score 
app 20.67 8.90 'I 83.88 
lewinsky 34.67 5.25 182.03 
currie 15.33 7.60 116.50 
ms 32.00 3.06 '97.97 
january 25.33 3.30 83.60 
jordan 18.67 4.06 75.81 
referrai 9.00 7.43 66.88 
magaziner 6.67 10.00 66.64 
Deny 13.33 4.92 65.61 
Admit 13.00 4.92 63.97 
monica 14.67 4.29 62.85 
oic 5.67 I 0.00 56.64 
betty 8.00 6.01 48.06 
vernon 8.67 5.49 47.54 
'do .... 32.67 1.40 45.80 
Telephoned 6.67 6.86 45.74 
.you 36.33 1.19 43.30 
i 42.67 0.96 40.84 
clinton 16.33 2.23 36.39 
jones 11.33 3.17 35.88 
or 32.33 ~ 1.09 35.20 
gif 3.33 9.30 31.01 
white 12.00 2.50 30.01 
tripp 4.67 6.23 29.10 
ctv 3.00 ~ 9.30 27.91 
december 7.33 3.71 27.19 
Figure 2: A sample cluster centroifl 
4 Centroid-based summarization 
The main technique that we use for 
summarization is sentence extraction. We 
score individually each sentence within a 
cluster and output hese that score the highest. 
A more detailed escription of the summarizer 
can be found in (Radev et al, 2000). 
The input to the summarization component is
a cluster of documents. These documents can 
be either the result of a user query or the 
output of CIDR. 
The summarizer takes as input a cluster o ld  
documents with a total of n sentences as well 
as a compression ratio parameter r which 
indicates how much of the original cluster to 
preserve. 
101 
The output consists of a sequence of In * r\] 
sentences from the original documents in the 
same order as the input documents. The 
highest-ranking sentences are included 
according to the scoring formula below: 
S~ = wcC~ + wpPi + wfFi 
In the formula, we, wp, wf are weights. Ci is the 
centroid score of the sentence, P~ is the 
positional score of the sentence, and F~ is the 
score of the sentence according to the overlap 
with the first sentence of the document. 
4.1 Centroid value 
The centroid value C~ for sentence Si is 
computed as the sum of the centroid values Cw 
of all words in the sentence. For example, the 
sentence "President Clinton met with Vernon 
Jordon in January" gets a score of 243.34 
which is the sum of the individual eentroid 
values of the words (clinton = 36.39; vernon = 
47.54; jordan = 75.81; january = 83.60). 
Ci = E cw 
w 
4.2 Positional value 
The positional value is computed as follows: 
the first sentence in a document gets the same 
score Cm,~, as the highest-ranking sentence in 
the document according to the centroid value. 
The score for all sentences within a document 
is computed according to the following 
formula: 
Pi = (n - i + 1) . mFx(Ci ) 
n t 
For example, if the sentence described above 
appears as the third sentence out of 30 in a 
document and the largest centroid value of any 
sentence in the given document is 917.31, the 
positional value P3  will be = 28/30 * 917.31 
4.3 First-sentence overlap 
The overlap value is computed as the inner 
product of the sentence vectors for the current 
sentence i and the first sentence of the 
document. The sentence vectors are the n- 
dimensional representations of the words in 
each sentence whereby the value at position i
of  a sentence vector indicates the number of  
occurrences of that word in the sentence. 
Fi = Sl Si 
4.4 Combining the three parameters 
As indicated in (Radev & al., 2000) we have 
experimented with several weighting schemes 
for the three parameters (centroid, position, 
and first-sentence overlap). Until this moment, 
we have not come to the point in which the 
three weights we, wp, and wf are either 
automatically learned or derived from a user 
profile. Instead, we have experimented with 
various sets of empirically determined values 
for the weights. In this paper the results are 
based on equal weights for the three 
parameters wc = wp = wf= 1. 
5 User Interface 
We describe in this section the user interface 
for web search mode as described earlier in 
Section 1. 
One component of  our system is the search 
engine (MySearch). The detailed esign of the 
search component is discussed in Section 2. 
The result of  a sample query "'Clinton" to our 
search engine is shown starting in Figure 4. 
102 
SSEARCH 
Tcmponn'y 
Web In~fa~ 
? ~bgm 
._~J~ 
Displaying t lom 1-10 oftolal ~1~ N 
Sea'c.h: I .  court v online \[LgS\] r 
Court 'IV Onrme Tcxt ofPrtsident Cllhaton's respomcs to Judkkty 
Dragomir R. Radcv * httv:/fwww.?ourttv.?om/? ase ffiles/dlntonerisis/1127' '~ amwerptcxt.htrd 
43 KB 
0 Cohmabla U. 
1990-2000 
? ~. of Michipn 
2000 index JLJU.I~. STAR.R SCANDAl., A site chronicling the ?n'ongdoings ofth,c .., 
Shadow Oovcmmcat of Kcrmct5 W . . . . .  ~: 
? http.J/www, gt ocitles. ? om/c apitoll'lill/$~at e/9634/ 27KB '..'i 
Maintain?d by radev(~fumich.edu 
Figure 3: Sample user query 
A user has the option to choose a specific 
ranking function as well as the number of  
retrieval results to be shown in a single screen. 
The keyword contained in the query string will 
be automatically highlighted in the search 
results to provide contextual information for 
the user. 
The overall interface for SNS is shown in 
Figure 4. On the top right of the frame is the 
MySearch search engine. When a user 
submits a query, the screen in Figure 5 
appears. As can be seen from Figure 5, there is 
a check box along with each retrieved record. 
This allows the user to tell the summarization 
engine which documents he/she wants to 
summarize. After the user clicks the 
summarization button, the summarization 
option screen is displayed as shown in bottom 
of Figure 6. The summarization ption screen 
allows a user to specify the summarization 
compression ratio. Figure 7 shows the 
summarization result for four URLs with the 
compression ratio set as 30%. 
103 
_------:tl . . . . . . .  , : - - - -  ,,,~,,, I I I =1~i ,~ 
. . . . . .  >~,<... . . . .  _ .... ~,, ~7~L ,.~ ? ~.~ 
"raltllw.=y 
? Abou~ 
Wlllli0 Fmt 
Stmmii~=~mc 
o Columbh U. 
I~8.1000 
O U. of ~,clillm 
2OOO 
,==,~, , ,=  I I~  -~.~,!i l iq i 
Please click on "submit" in the frame above to continue. 
Mtilidm=d by rad~n,(~a, s i eh,~a~ 
F igure  4: SNS in ter face  (framed) 
SSEARCH s~ iop s~ ~_~a~ Io . . . .  
" - -  5 ~,,.B 
T ~ ' y  . ; 
Web \ ]n~l??  i! 9. ooficv ncws events ~ ~?f in l  rc~r t  dot=Is chinese ?s~ie~rm~? c f f~ \[1.76\] I~ 
? Ab~t  1~o~?y corn Niwi Evmts Dili~F Biiii~i i Cox Ripmt D, ctliRi Chi~st 
? ~ F..il#muqpl FJTolrtl TodI~fl NIwl  D.-. :: 
W "ti~t~ Fm Close Up Foundation U S PoFlcy Towlrd Cuba ~ Up Foundafiou Spctill :;:l 
Summ~mtfiom Tol~c Psq~ U S Poficy TowL.. 
Drlt!,omh" R. Rt~l~ o htlod/www.dgleuo.om/euba.hi m 291-~ :::! 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  : ................... 
o C~lumbil U. 
199g-2000 ENext 1 > >1 : ' : ~ I  
0 'I3. o f~ J~ i~ ........................................................................................................................................ 
2000 Pmme~ by MySQL. ~,~pacl~e m =~m L~m SP.~RC mv~. l~ySmscl ~s ~ sazm a~e sncegvv~at ~ 
e~cmmmezce Ib  l i  ~ U~,~.~'~ of ~,~c~s Bm~e~ 
Ally ~ ' , ,~et l  in~ llulllellion.l Io ttle syl l l=l  iloul~l be lt.ected Io lbe w ~b~ter .  
lease c !ck on su mi in he frame a ov e con :uue.  .... ~l 
ld .a ,~o4 by rade~,t~ch.fdu .I 
I 
~ . .  , - : s ,= .=o. . ; . ,  o : , . :  . , -  , .  , . :~ , . : . .+  , "<-  . . . . .  - ? . ~ - -  , - -  ~ ~  ~1 
~igure 5: Sea:ch output along with user selection of documents to be summarized 
104 
SSEARCH 
r~ 
Web Int~'fa~e 
? ,About 
10. dose foundatlon pol;c V cub:~ \[1.74;\] I~ 
Close Up Foundation U S Policy Toward Cuba Close Up F o ~  Special 
Topic Page U S Policy Towa... 
o http~',~.ww.ClOSeUp.o:~cu~htm 29 K~ 
S?areb: Stmm, m~2~ag 4 
welguo Fan 
:t0:i hnp.~/w~v ?!o,;c,~.o~cubxhtm 
o U. of Mid ian 
i ~ e d  by radev(~umfch.edu l 
t 
Figure 6: Selected documents for summarization 
SSEARCH 
Tempm'm'y 
Web Im:c:ffacc 
? .Abou+. 
Se~cl~ 
Wci~uo Fan 
Smmmeiantio~ 
Dragomir R. P, adev 
C Columbia U. 
1998-20130 
O U. of Miehi~n 
2OOO 
10. clos9 fotmda6on pol;c.y cuba !1.76\] I~ 
Summary @ 30% of the URLs that you 
htqrJ/www.?Om'ttv, eem/e~gf~s/c~tonczLt~/l12798_answerst~ht~ 
hl~'J/~,,w.scatllcl~c~ om/czlz~/l~owsc&tm~71~_O52297.hmd 
selected: 
ii I 
Snn'mq~,~ng 1360 .~cm, anccs ~ 30% = 408 scntc~nccs 
S '~, t ion  stmrt?d 
iTightcnmg tbc Embm'g?F?r akn?st fmW Yeses the Unlt?d States has n?t imported any Cubm ~53.71 
~ .nor allowed ~ ~ food.m~dk~ supplies.re', c~ to. ?~ ~ . . . . . .  i i ! 
~o o~c~ ?ounUy has joined lhc Umlcd States in the lmdc cmbmrgo agminst Cuba in fact the 
M=finta~ncd by r~evC,wnich.e~ 
Figure 7: Output of the summarizer 
105 
The following information is shown in the 
summarization result screen in Figure 7: 
? The number of sentences in the text of the 
set of  URLs that the user selected 
? The number of sentences in the summary 
The sentences representing the themes of those 
selected URLs and their relative scores. The 
sentences are ordered the same way they appear 
in the original set of documents. 
6 Experimental results 
Our system was evaluated using the task-based 
extrinsic measure as suggested in (Mani et al 
1999). The experiment was set up as follows: 
Three sets of  documents on different topics were 
selected prior to the experiment. The topics and 
their corresponding document information are 
shown in Table 1. 
~opie No. Lengtl~ 
S1 200k 
$2 Introduction to Data Mining 100k 
$3 Intelligent Agents and their application in 
Information retrieval 5 160k 
Table 1: Evaluation Topics and their corresponding document set information 
!i 
I 
The term data mining is then this high-level application techniques / tools used to 5^9..  
present and analyze data for decision makers ~ 
~! !  ~! '  ==I! i i1 \ ]  I i l I i=I i i~ " 
? ~,  = . . . . .  . . . . .  , ....... . ,= = ~.~ ,116111 =. ,  = ...... - .=  ...,... ......... ,, .......... = iI~.',=~ .==.,~ 
!\],!~:;= =!, I i  
Figure 8: A sample of the summarization result for $2 at 10% compression rate 
As Table 1 shows, the articles in topic set $1 are 
longer than both these in $2 and $3. The articles 
in $3 are the shortest, with each 32k in average. 
The number of documents in each topic set is 
106 
also different. The variations of document length 
and different number of documents in each topic 
set will help test the robustness of our 
summarization algorithms. 
We used SNS to generate both 10% and 20% 
summaries for each topic. A sample of the 10% 
summary for topic $2 is shown in Figure 8. Four 
users were selected for evaluation of these 
summarization results. Each user was asked to 
read through the set of full articles for each topic 
f'wst, followed by its corresponding 10% and 
20% summaries. After these 4 users finished 
each set, they were asked to assign a readability 
score (1-10) for each summary. The higher the 
readability score is, the more readable and 
meaningful for comprehension is the summary. 
The time of reading both full articles and 
summaries was tracked and recorded. 
Table 2: Summarization evaluation: detailed results 
7.92 
Table 3: Summary of the evaluation results 
The detailed evaluation results are shown in topics. The summaries generated by SNS are 
Table 2. Table 3 gives the summary of the Table also very readable. For example, The average 
2. It's shown in Table 2 that these four users readability score (which is obtained by 
have different reading speeds. However, their averaging the readability scores assigned by the 
reading speed is pretty consistent across the 3 four users) for 10% and 20% summaries for 
107 
topic S1, is 8, 8 respectively. For topic $3, the 
average readability score for 10% and 20% 
summaries is 7.75, and 8.75, respectively. 
Similarly, for $2 the average readability score 
for 10% and 20% summaries is 8 and 8.5, 
respectively. The differences in the average 
readability score also suggest that (a) our 
summarizer favors longer documents over 
shorter documents; (tO 20% summaries are 
generally favorable over 10% summaries. The 
difference in the readability score between 10% 
and 20% summaries i bigger in $3 (diff = 1.0) 
than in S1 (diff = 0). These interesting findings 
raise interesting questions for future research. 
As can be seen from Table 3, the 20% summary 
achieves better eadability score in overall than 
the 10% summary. The speedup of the 10% 
summary over full articles is 6.87. That is, with 
reading material reduced by 900%, the speedup 
in reading is only 687%. This suggests that there 
may be a little bit difficulty in reading the 10% 
summary result. This may be due to the simple 
sentence boundary detection algorithm we used. 
The feedback from users in the evaluation seems 
to corffirm the above reason. As more sentences 
were included in the 20% summaries, the 
speedup in reading (4.22) almost approached the 
optimal speedup ratio (5.0)L 
7 Related Work 
Neto et al (2000) describes a text mining tool 
that performs document clustering and text 
summarization. They used the Autoclass 
algorithm to perform document clustering and 
used TF-ISF (an adaptation of TF-IDF) to 
perform sentence ranking and generate the 
summarization output. Our work is different 
from theirs in that we perform personalized 
summarization based on the retrieval result from 
a generic personalized web-based search engine. 
A more complicated sentence ranking functions 
is employed to boost the ranking performance. 
The compression ratio for the summary is 
customizable by a user. Both single-document 
for a single URL and multiple-document 
i Since the length of the summary is only 20% of the 
original documents, he maximum speedup in terms of 
reading time is 1/0.2=5. 
summarization for a cluster of URLs are 
supported inour system. 
More related work can be found in Extractor 
web site http'J/extractor.iit.nrc.ca/. They use 
MetaCrawler to perform web-based search and 
automatically generate summaries for each 
URLs retrieved. They only support single 
document summarization i  their engine and the 
compression rate of the summarizer is also non- 
customizable. We not only support both single 
and multiple document summarization, but also 
allow the user to specify the summarization 
compression ratio as well as to get per-cluster 
summaries of automatically generated clusters, 
which, we believe, are more valuable to online 
users and give them more flexibility and control 
of the surnrnarization results. 
8 Conclusion and Future Work  
We described in this paper a prototype system 
SNS, which integrates natural language 
processing and information retrieval techniques 
to perform automatic ustomized summarization 
of search engine results. The user interface and 
detailed design of SNS's components are also 
discussed. Task-based extrinsic evaluation 
showed that the system is of reasonably high 
quality. 
The following issues will be addressed in the 
future. 
8.1 Interaction between sentence inclusion 
in a summary 
There are two types of interaction (or 
reinforcement) between sentences in a summary: 
negative and positive. 
Negative interaction occurs when the inclusion 
of one sentence in the summary indicates that 
another sentence should not appear in the 
summary. This is particularly relevant o multi- 
document summarization as in this case: 
negative interaction models the non-inclusion of 
redundant information. 
The case of positive interaction i volves positive 
reinforcement between sentences. For example, 
if a sentence with a referring expression is to be 
108 
included in a stma~lary, typically the sentence 
containing the antecedent should also be added. 
We will investigate specific setups in which 
positive and/or negative reinforcement between 
sentences i  practical and useful. 
8.2 Personalization 
We will investigate additional techniques for 
producing personalized summaries. Some of the 
approaches that we are considering are: 
Query words: favoring sentences that 
include words from the user query in the 
Web-based scenario 
Personal preferences and interaction history: 
we would favor sentences that match the 
user profile (e.g., overlapping with his or her 
long-term interests and/or recent queries 
logged by the system). 
8.3 Technical limitations 
The current version of our system uses a fairly 
basic sentence delimiting component. We will 
investigate the user of robust sentence boundary 
identification modules in the future. 
We will also investigate the possibility of some 
limited-form anaphora resolution component. 
8.4 Availability 
A demonstration version of SNS is available at 
the following UP.L: 
http://www.si.umich.edu/-radev/ssearch/ 
Lawrence, S., and Giles, C. L. (1997). Searching the 
World Wide Web, Science, 280(3), 98-100. 
Lawrence, S., and Giles, C. L. (1999). Accessibility of 
information on the web, Nature, 400, 107-109. 
Mani, I. and BIoedorn, E. (1999). Summarizing 
similarities and di~rences among related 
documents. Information Retrieval 1(1): 35--67. 
Mani, I., House, D., Klein, G., Hirschman, L., Obrst, 
L., Firmin, T., Chrzanowski, M., and Sundheim, B. 
(1998). The TIPSTER SUMMA C Text 
Summarization Evaluation. The MITRE 
Corporation Technical Report MTR 98W0000138, 
McLean, Virginia. 
McKeown, K. and D. R. Radev. Generating Summaries 
of Multiple News Articles. Proceedings, ACM 
Conference on Research and Development in 
Information Retrieval S1GIR'95 (Seattle, WA, July 
1995). 
NetSizer (2000). http~//www.netsizer.com/. 
Neto, J. L., Santos, A. D., Kaestner, C. A. A., and 
Freitas, A. A. (2000). Document clustering and text 
summarization. In Proceedings, 4th Int. Conference 
on Practical Applications of Knowledge Discovery 
and Data Mining (PADD-2000), 41-55. London: 
The Practical Application Company. 
Radev, D. R., Hatzivassiloglou, V., and McKeown, 
K. A Description of the CIDR System as Used for 
TDT-2. Proceedings, DARPA Broadcast News 
Workshop, (Herndon, VA, February 1999). 
Radev, D. R, Jing, H., and Stys-Budzikowska, M.
Summarization of multiple documents: clustering, 
sentence xtraction, and evaluation. Proceedings, 
ANLP-NAACL Workshop on Automatic 
Summarization, (Seattle, WA, April 2000) 
Salton, G. (1989). Automatic Text Processing. 
Addison-Wesley Publishing Co., Reading, MA, 
1989. 
References 
Carbonell, J. and Goldstein, J. (1998). The use of 
MMR, Diversity-Based Reranking for Reordering 
Documents and Producing Summaries. Poster 
Session, SIGIR'98, Melbourne, Australia. 
Censorware (2000). 
http://www.censorware.org/web size/. 
Extractor (2000). http://extractor.iit.nrc.ca/. 
IDS . (2000). lnternet Domain Survey. 
http://www:isc.org/dsl. 
Jansen, B. J., Spink, A., and Saracevic, T. (2000). 
Real life. real users, and real needs: a study and 
analysis of user queries on the web. Information 
Processing and Management. 36(2), 207-227. 
109 
Revisions that Improve Cohesion in Multi-document Summaries:  
A Preliminary Study 
Jahna C. Otterbacher, Dragomir R. Radev and Airong Luo 
School of Information 
University of Michigan 
Ann Arbor, MI 48109-1092 
{clear, radev, airongl}@umich.edu 
Abstract 
Extractive summaries produced from multiple 
source documents suffer from an array of prob-
lems with respect to text cohesion.  In this pre-
liminary study, we seek to understand what 
problems occur in such summaries and how of-
ten.  We present an analysis of a small corpus 
of manually revised summaries and discuss the 
feasibility of making such repairs automati-
cally.  Additionally, we present a taxonomy of 
the problems that occur in the corpus, as well 
as the operators which, when applied to the 
summaries, can address these concerns.  This 
study represents a first step toward identifying 
and automating revision operators that could 
work with current summarization systems in 
order to repair cohesion problems in multi-
document summaries. 
1 Introduction 
With the increasing availability of online news 
sources, interest in automatic summarization has 
continued to grow in recent years.  Many systems 
have been developed for this purpose, including 
those that can produce summaries based on several 
documents (Multi-document summarization, or 
MDS).  Generally speaking, most of these systems 
work by extracting sentences from the original 
texts.  Although significant improvements continue 
to be made to such summarizers, they still cannot 
produce summaries that resemble those written 
manually by humans.  One area in particular in 
which the automatically produced summaries dif-
fer markedly is text cohesion. 
Whether a summary is produced from one or 
more documents, important context may be ex-
cluded from the summary that disrupts its readabil-
ity.  A text is not a random collection of sentences, 
but rather, each sentence plays a role in conveying 
the ideas the author wants to express.  Selecting 
sentences from multiple texts one at a time disre-
gards this interdependence between sentences.  As 
a result, summaries often suffer from problems of 
cohesion. 
1.1 Text cohesion and coherence 
[Halliday & Hasan, 1976] offer a clear definition 
for text cohesion: 
[The concept of cohesion] refers to relations of 
meaning that exist within the text, and that define it 
as a text.  Cohesion occurs where the interpretation 
of some element in the discourse is dependent on 
that of another (p.2). 
It is this property of cohesion that allows the 
reader to comprehend the overall meaning of a 
text, and to understand the author?s intentions.  
Therefore, in automatically produced summaries, 
cohesion problems should be resolved.  Otherwise, 
the resulting text may be unintelligible, or worse, 
misleading. 
1.2 Problems of cohesion in automatically pro-
duced summaries 
The following is an example of a summary pro-
duced automatically from one source document. 
 [1] More than 130 bodies are reported to have 
been recovered after a Gulf Air jet carrying 143 
people crashed into the Gulf off Bahrain on 
Wednesday.  [2] Distraught relatives also gathered 
at Cairo airport, demanding information.  [3] He 
also declared three days of national mourning. [4] 
He said the jet fell ?sharply, like an arrow.? 
The most obvious problem with this summary is 
that in the last two sentences, the pronouns have no 
       Philadelphia, July 2002, pp. 27-36.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
antecedents; as a result, the reader does not know 
who the subjects of the sentences are.  In addition, 
the adverb ?also,? used in both the second and third 
sentences, makes reference to previous events not 
described in the summary.  Another concern is that 
there seems to be no transition between the sen-
tences.  The context from the source article neces-
sary to make the text cohesive is missing from the 
summary.  As a result, the summary is unintelligi-
ble. 
1.3 Text cohesion in MDS 
Using multiple documents to generate a summary 
further complicates the situation.  As contended by 
[Goldstein et al 2000] a multi-document summary 
may contain redundant messages, since a cluster of 
news articles tends to cover the same main point 
and shared background.  In addition, articles from 
various sources could contradict one another, as to 
how or when an event developed.  Finally, since 
the source articles are not all written simultane-
ously, they may describe different stages of the 
development of an event.  Not only do news stories 
come to different conclusions at various stages in 
an event, but also the attitudes of writers may 
change. 
Multi-document summaries may suffer further 
from problems of cohesion since their source arti-
cles may be written by different authors.  Not only 
do writers have their own styles, they have the 
overarching structure of the article in mind when 
producing it.  As a result, in MDS we are more 
likely to encounter text that is not cohesive. 
Previous research has addressed revision in 
single-document summaries [Jing & McKeown, 
2000] [Mani et al 1999] and has suggested that 
revising summaries can make them more informa-
tive and correct errors.  We believe that a generate-
and-revise strategy might also be used in creating 
better multiple-document summaries, within the 
framework of current extractive summarization 
systems.  However, as mentioned previously, there 
is reason to believe that multi-document summa-
ries suffer from many different coherence prob-
lems and that such problems occur more often than 
in single-document summaries.  Therefore, an im-
portant preliminary step in determining how we 
might revise such summaries is to closely examine 
the cohesion problems that occur in multi-
document summaries. 
In the current paper we analyze a small corpus 
of manually revised multi-document summaries.  
We present a taxonomy of pragmatic concerns 
with respect to cohesion in the summaries, as well 
as the operators that can address them.  Finally, we 
will discuss the feasibility of implementing such 
revisions automatically, which we hope to address 
in our future work. 
2 Background and previous work 
2.1 Theories on discourse structure 
Rhetorical Structure Theory (RST) [Mann & 
Thompson, 1988] has contributed a great deal to 
the understanding of the discourse of written 
documents.  RST describes the coherence nature of 
a text and is based on the assumption that the ele-
mentary textual units are non-overlapping text 
spans.  The central concept of RST is the rhetorical 
relation, which indicates the relationship between 
two spans. 
RST can be used in sentence selection for sin-
gle document summarization [Marcu, 1997].  
However, it cannot be applied to MDS.  In RST, 
text coherence is achieved because the writer in-
tentionally establishes relationships between the 
phrases in the text.  This is not the case in MDS, 
where sentences are extracted from different 
source articles, written by various authors. 
Inspired by RST, [Radev, 2000] endeavored to 
establish a Cross-document Structure Theory 
(CST) that is more appropriate for MDS.  CST fo-
cuses on the relationships between sentences that 
come from multiple documents, which vary sub-
stantially from those between sentences in the 
same text.  Such relationships include identity, 
paraphrase and subsumption (one sentence con-
tains more information than the other). 
2.2 Computational models of text coherence 
Based on RST, [Marcu, 2000] established a 
Rhetorical Parser.  The parser exploits cue phrases 
in an algorithm that discovers discourse 
relationships between phrases in a text.  This 
parser can be used to extract sentences in single-
document summarization.  To contrast, 
[Harabagiu, 1999] concentrated on the derivation 
of a model that can establish coherence relations in 
a text without relying on cue phrases.  She made 
use of large lexical databases, such as Wordnet, 
and of path finding algorithms that generate the 
algorithms that generate the cohesion structure of 
texts represented by a lexical path. 
[Hovy, 1993] summarized previous work that 
focused on the automated planning and generation 
of multi-sentence texts using discourse relation-
ships.  Text generation is relevant to MDS, as we 
can view MDS as an attempt to generate a new text 
by reusing sentences from different sources.  The 
systems discussed in [Hovy, 1993] relied on a 
knowledge base and a representation of discourse 
structure.  The dependency of text generation on 
knowledge of discourse structure was emphasized. 
2.3 Revision of single-document summaries 
[Mani et al 1999] focused on the revision of sin-
gle-document summaries in order to improve their 
informativeness.  They noted that such revision 
might also fix ?coherence errors.?  Three types of 
revision operators were identified: sentence com-
paction, sentence aggregation and sentence 
smoothing.  To contrast, [Jing & McKeown, 2000] 
concentrated on analyzing human-written summa-
ries in order to determine how professionals con-
struct summaries.  They found that most sentences 
could be traced back to specific cut-and-paste op-
erations applied to the source document.  They 
identified six operations and used them to imple-
ment an automatic revision module. 
2.4 Temporal ordering of events 
[Filatova & Hovy, 2001] addressed the issue of 
resolving temporal references in news stories.  Al-
though events in articles are not always presented 
in chronological order, readers must be able to re-
construct the timeline of events in order to com-
prehend the story.  They endeavored to develop a 
module that could automatically assign a time 
stamp to each clause in a document.  Using a syn-
tactic parser, patterns were discovered as to which 
syntactic phrases tend to indicate the occurrence of 
a new event.  In MDS, the correct temporal rela-
tionships between events described in the extracted 
sentences often needs to be reestablished, since 
they may be incorrect or unclear. 
[Barzilay et al 2001] evaluated three algo-
rithms for sentence ordering in multi-document 
summaries.  One algorithm implemented was the 
Chronological Ordering algorithm.  However, the 
resulting summaries often suffered from abrupt 
changes in topic.  After conducting an experiment 
in which they studied how humans manually or-
dered sentences in a summary, they concluded that 
topically related sentences should be grouped to-
gether.  The Chronological Ordering algorithm was 
augmented by introducing a cohesion constraint.  
The evaluation of the output summaries demon-
strated a significant improvement in quality. 
3 Revision-based system architecture 
The proposed architecture of our system, which 
would implement the generate-and-revise approach 
to summarization, is depicted in Figure 1.  Input to 
this system is a cluster of source documents related 
to the same topic.  Next, sentence extraction takes 
place, in which important sentences in the articles 
are identified.  The output of this module is an ex-
tract, which lists the sentences to be included in the 
summary. 
In the next stage, Cross-document Structure 
Theory (CST) relationships are established.  Spe-
cific relationships between sentences are identified.  
Here, a CST-enhancement procedure [Zhang et al 
2002] may take place, ensuring that interdependent 
sentences appear together in a summary.  Sen-
tences may also be reordered in the summary with 
respect to their temporal relations, topic, or other 
criteria. 
The next stage in the process is the revision 
module.  First, high level revision operators are 
chosen, with respect to the cohesion problems that 
need repair.  Afterwards, the specific lexical items 
to be added, deleted or modified are chosen.  The 
output of this module is the revised, enhanced 
summary. 
3.1 The MEAD summarizer  
The MEAD summarizer [Radev et al 2000] 
[Radev et al2002] is based on sentence extraction 
and uses a linear combination of three features to 
rank the sentences in the source documents.  The 
first of the three features is the centroid score, 
which quantifies the centrality of a sentence to the 
overall cluster of documents.  The second is the 
position score, which assigns higher scores to sen-
tences that are closer to the beginning of the docu-
ment.  The third feature, length, gives a higher 
score to longer sentences.  Using a linear combina-
tion of the three features, sentences are ranked by 
score and added to the summary until the desired 
length is attained. 
Summarization
CST Identification
C:1 - B:2
B:13
D:5 - A:1
Sentence Reordering
Temporal, Topical
Revision
High-level Operators
Lexical ChoiceA:1 - 1. <Delete> time exp2. Thursday
C:1
B:2
D:5 -1. <Add> adverb
2. Meanwhile
Multi-Document
Extract
CST Enhanced
Summary
CST Enhanced
Revised Summary
Source
Documents
A B C D
C:1
B:13
D:5 
 
Figure 1: Revision-based MDS architecture: 
Letters denote documents; numbers denote sen-
tence numbers (within documents) 
4 Data and procedure 
We generated a corpus of summaries using the 
MEAD summarizer.  The original documents come 
from three sources ? DUC 2001, the Hong Kong 
News corpus, and the GA3-11 data set.  One clus-
ter of related news articles was chosen from each 
source.  The DUC 2001 articles describe the 1991 
eruption of Mount Pinatubo in the Philippines.  
This cluster, which is not typical of the DUC data, 
focuses on this single event and its subevents over 
a 2-week time period.  Those taken from the HK 
corpus are about government initiatives surround-
ing the problem of drug rehabilitation.  Due to the 
expense and labor involved in the generation and 
revision of multi-document summaries, we have 
used a subset of 15 summaries from our corpus in 
order to develop our revision taxonomy and to pre-
sent some initial findings.  Our future revision 
studies will employ a much larger set of data. 
The summaries were revised manually by the 
first author.  This was a three-step process that in-
volved identifying each problem, choosing an 
operator that could address the problem and then 
selecting the lexical items to which the operator 
should be applied.  It is important to note that mul-
tiple lexical choices are possible in some cases. 
Since we were interested in identifying all 
types of cohesion problems as well as considering 
all possibilities for addressing these problems, the 
reviser was permitted to make any revision neces-
sary in order to correct problems in the summaries.  
Obviously, a module that makes revisions auto-
matically would be much more restricted in its set 
of revision operators.  However, since a major goal 
for this paper was to establish a taxonomy of prob-
lems specific to multi-document summarization 
and to consider the complexities involved in mak-
ing repairs in MDS, we did not place such restric-
tions on the reviser.  Rather, she applied 
corrections to the summaries as to make them as 
intelligible as possible, given the sentences chosen 
by the summarizer. 
 
Source Length  
(sentences) 
#Source 
documents 
DUC 2001 3 3 
DUC 2001 3 3 
DUC 2001 5 3 
DUC 2001 6 5 
DUC 2001 9 5 
GA3-11 3 3 
GA3-11 3 5 
GA3-11 6 5 
GA3-11 8 3 
GA3-11 7 3 
HK-125 3 3 
HK-125 5 3 
HK-125 6 5 
HK-125 5 5 
HK-125 8 3 
Table 1: Summaries from training data 
4.1 Revision example 
<DELETE-place stamp> Cairo, Egypt ? </DELETE> The 
crash of a Gulf Air flight that killed 143 people in Bahrain 
<ADD-time exp-day>Wednesday </ADD> is a disturbing 
d?j? vu for Egyptians: It is the second plane crash within a 
year to devastate this Arab country.  Egypt, which lacks the 
oil wealth of the Gulf and has an economy struggling to re-
vive from decades of socialist stagnation, has a long tradi-
tion of sending workers to the Gulf to fill everything from 
skilled to menial jobs.  <DELETE-place stamp> Manama, 
Bahrain (AP) ? </DELETE> <ADD-time exp-day> On Fri-
day, </ADD> three bodies wrapped in cloth, one the size of 
a small child, were lain before the faithful in the Grand 
Mosque during a special prayer for the dead in honor of the 
<DELETE-redundancy> 143 </DELETE> victims of the 
<DELETE-overspecified entity> Gulf Air </DELETE> 
crash.  Bahrain?s Prime Minister Sheik Khalifa bin Salman 
Al Khalifa and other top officials stood side-by-side with 
2,000 Muslins reciting funeral prayers before the bodies, 
<DELETE-redundancy> which were among the 107 adults 
and 36 children killed in Wednesday?s air disaster, 
</DELETE> said Information Ministry spokesman Syed el-
Bably. 
Figure 2: Revised multi-document summary 
The above figure shows an example of a revised 
summary that was produced from three source arti-
cles from the GA3-11 corpus.  The news stories 
were collected live from the web, and come from 
two different sources www.foxnews.com and 
www.abcnews.com.  The revision operator used 
and the corresponding pragmatic concern precede 
the modified text in pointed brackets.  This type of 
markup scheme was used because it enables us to 
use simple Perl scripts to move between the origi-
nal and revised versions of the summaries. 
5 Taxonomy of revision strategies 
Based on our corpus of revised summaries, we 
have identified five major categories of pragmatic 
concerns related to text cohesion in multi-
document summaries: 
1) Discourse ? Concerns the relationships be-
tween the sentences in a summary, as well as 
those between individual sentences and the 
overall summary. 
2) Identification of entities ? Involves the reso-
lution of referential expressions such that 
each entity mentioned in a summary can eas-
ily be identified by the reader. 
3) Temporal ? Concerns the establishment of 
the correct temporal relationships between 
events. 
4) Grammar ? Concerns the correction of 
grammatical problems, which may be the re-
sult of juxtaposing sentences from different 
sources, or due to the previous revisions that 
were made. 
5) Location/setting ? Involves establishing 
where each event in a summary takes place. 
Explanations of the specific pragmatic concerns in 
each category, as well as their corresponding op-
erator(s), are detailed in the appendix.  Overall, 
160 revisions were made across the 15 summaries. 
 
Pragmatic 
category # of revisions 
% of total 
revisions 
Discourse 54 34% 
Entities 41 26% 
Temporal 35 22% 
Grammar 20 12% 
Place/setting 10 6% 
Table 2: Revisions by pragmatic category 
The majority (82%) of the revisions fall into 
the first three categories.  This is not surprising, as 
in MDS, we expect to find many problems relating 
to discourse ? such as abrupt topic shifts or redun-
dant messages.  Additionally, concerns relating to 
the identification of entities in the text are likely to 
occur when the sentence from the original docu-
ment that introduced an entity is not included in 
the resulting summary, but sentences that make 
reference to the entity are included.  Finally, it may 
not be clear when events described in a summary 
occurred.  This could be because sentences which 
stated when the event occurred were left out of the 
summary or because the sentences include relative 
time expressions such as ?today? even though the 
stories were written at different times or on differ-
ent days. 
Revisions relating to grammar or to establish-
ing where an event occurred were less frequently 
used, accounting for only 12% and 6% of the total 
repairs, respectively.  Sentences extracted from the 
original news stories are usually grammatical.  
However, problems related to grammar may arise 
from previous revisions.  In our corpus, the place 
or setting of an event was typically obvious in the 
summary and rarely required repair. 
Next, we present the analysis of revisions 
within each of the five categories.  We are inter-
ested in revising our summaries to be as coherent 
as possible, without having to implement compli-
cated and knowledge-intensive discourse models.  
Therefore, we will discuss the feasibility of im-
plementing the revisions in our taxonomy auto-
matically. 
5.1 Discourse-related concerns in MDS 
It is intuitive that problems relating to discourse 
are abundant in our summaries and, at the same 
time, that such repairs would be the most difficult 
to make.  The first obstacle is the detection of each 
of these concerns, which requires knowledge of the 
rhetorical relations of the sentences in the sum-
mary. 
 
Problem Number (%) 
1) Topic shift 24  (45%) 
2) Purpose 18  (33%) 
3) Contrast   6  (11%) 
4) Redundancy   6  (11%) 
5) Conditional   0 
Total 54 
Table 3: Discourse-related revisions 
In all the instances of topic shift and lack of pur-
pose in our corpus, a phrase or an entire sentence 
was added to provide a transition or motivation for 
the troublesome sentence.  Therefore, our module 
would require the ability to generate text, in order 
to repair these problems, which occur often in our 
summaries. 
5.2 Identification of entities in MDS 
Nine specific problems were found that concern 
the reader?s ability to identify each entity men-
tioned in a summary.  Most of these revisions 
could be made using rewrite rules.  For example, if 
it can be determined that a definite article is used 
when a (non-proper noun) entity is mentioned for 
the first time, the misused definite article could be 
replaced with the corresponding indefinite article. 
The most frequent problem, underspecified en-
tity, is the most difficult one to correct.  This dis-
fluency typically occurs where an entity is referred 
to by a proper noun or other noun phrase, such as 
the name of a person or organization, but has no 
title or further description.  In such cases, the miss-
ing information may be found in the source docu-
ment only.   
 
Problem Number (%) 
1) Underspecified entity 15  (38%) 
2) Misused quantifier   6  (15%) 
3) Overspecified entity   5  (12%) 
4) Repeated entity   5  (12%) 
5) Bare anaphor   4  (10%) 
6) Misused definite article   3  (  7%) 
7) Misused indef. Article   1  (  2%) 
8) Missing article   1  (  2%) 
9) Missing entity   1  (  2%) 
Total 41 
Table 4: Revisions concerning entity identification 
Therefore, to correct the underspecified entity 
problem, a revision module might require a knowl-
edge source for the profiles of entities mentioned 
in a summary.  When an entity is introduced for 
the first time in a summary, it should be associated 
with its description (such as a title and full name 
for a person). 
Discourse information would be useful for 
solving problems such as a bare anaphor or miss-
ing subject.  In revising single-document summa-
ries, [Mani et al 1999] employed rules such as the 
referencing of pronouns with the most recently 
mentioned noun phrase.  However, this might be 
inappropriate in MDS, where the use of multiple 
documents increases the number of possible enti-
ties with which an anaphor could be referenced. 
5.3 Temporal relationships in MDS 
An important aspect of revision in MDS is the es-
tablishment of the correct temporal relationships 
between the events described in a summary.  We 
have identified five types of problems that fall into 
this category. 
 
Problem Number (%) 
1) Temporal ordering 31 (89%) 
2) Time of event 2    (6%) 
3) Event repetition 1   (2.5%) 
4) Synchrony 1   (2.5%) 
5) Anachronism 0 
Total 35 
Table 5: Temporal relationships revisions 
The most frequent revision in this category for 
our multi-document summaries was temporal or-
dering.  This is an important consideration for the 
summarization of news articles, which typically 
describe several events or a series of events in a 
given news story. 
A revision module might use metadata, includ-
ing the time stamps of source documents, in addi-
tion to surface properties of sentences in 
addressing this problem.  Temporal relations were 
typically established by adding a time expression 
to one or more sentences in a summary.  Therefore, 
our module will require a dictionary of such ex-
pressions as well as a set of rules for assigning an 
appropriate expression to a given sentence.  For 
example, if the time stamps of two source docu-
ments from which two adjacent summary sen-
tences come indicate that they were written one 
day apart, an appropriate way to order them might 
be: add a time expression indicating the day to the 
first sentence, and a relative time expression such 
as ?the following day? to the second sentence.  Our 
dictionary will require both relative and absolute 
time expressions at different levels of granularity 
(hour, day, etc.). 
Most of the temporal revisions in our corpus 
were made at points where sentences from differ-
ent sources followed one another or when sen-
tences from the same source were far apart in the 
original document.  By using such clues, it is 
hoped that temporal relations problems in summa-
ries can be corrected without knowledge of the 
discourse. 
5.4 Grammatical concerns in MDS 
The majority of grammatical problems in our cor-
pus resulted from previous revisions performed on 
the text.  For example, the addition of information 
to a sentence can result in it becoming too long.  
Such concerns can also occur because the grammar 
of one sentence, such as verb tense, does not match 
that of the next sentence. 
 
Problem Number  (%) 
1) Run-on sentence 7  (35%) 
2) Mismatched verb 3  (15%) 
3) Missing punctuation 3  (15%) 
4) Awkward syntax 3  (15%) 
5) Parenthetical 2  (10%) 
6) Subheading/titles 1  (  5%) 
7) Misused adverb 1  (  5%) 
Total 20 
Table 6: Grammatical revisions 
A revision module should be able to correct 
the above concerns using rules applied after other 
revisions are made and without any discourse 
knowledge. 
5.5 Location/setting concerns 
The least frequent type of revision made in our 
corpus related to establishing the correct locations 
of events in a summary.  Occasionally, a sentence 
in a summary retains the place/source stamp that 
appears at the beginning of a news article.  This 
appears ungrammatical unless the sentence is the 
first in the summary. 
Problem Number (%) 
1) Place/source stamp 6  (60%) 
2) Place of event 4  (40%) 
3) Collocation 0 
4) Change of location 0 
Total 10 
Table 7: Location/setting concerns 
In addition, such stamps might be inappropriate for 
a summary, since not all the sentences may share 
the same location.  In order to promote cohesion in 
the summary, our module could move the stamp 
information into the body of the summary. 
Sentences could be missing location informa-
tion altogether.  In such cases, the revision module 
might require information from the source docu-
ments in order to repair this problem.  Overall, the 
revisions related to establishing the location of 
events should not require knowledge of discourse 
in the summary.  Adding location information can 
usually be performed with the addition of a prepo-
sitional phrase, usually at the beginning of the sen-
tence. 
6 Conclusions and future work 
This paper represents preliminary work in our ef-
forts to address problems of text cohesion and co-
herence in multi-document summaries via revision.  
As a first step, we need to identify the specific 
problems that occur in MDS and consider how we 
might address such concerns.  To this end, we have 
investigated the optimal revisions that were per-
formed on a small set of summaries.  From this 
analysis, we have formulated a taxonomy of prag-
matic concerns and their operators for repairing 
multi-document summaries. 
Knowledge of 
discourse
and text generation 
<ADD>
transitional phrase or sentence;
<ADD>
motivational phrase or sentence;
Knowledge 
of discourse
Knowledge source
(entity 
descriptions)
Meta data
and dictionary 
of expressions
Sentence 
surface
structure
<DELETE>
Redundant information
<ADD/MODIFY>
discourse markers
<ADD/MODIFY> 
description of entity 
mentioned for first time
<ADD/MODIFY> 
Time expression
Grammar corrections;
<MODIFY/ADD/DELETE>
definite or indefinite articles;
<MODIFY/ADD/DELETE>
location of event
Op
er
ati
on
   C
om
ple
xit
y
 
Figure 3: Continuum of revision operations 
There is a scale of revision operations that can be 
performed (as shown in Figure 3), ranging from 
concrete repairs that require only knowledge of the 
surface structures of sentences, to knowledge-
intensive repairs that cannot be implemented with-
out a discourse model.  In the future, we plan to 
formalize our framework so that we might be able 
to implement such revision strategies automati-
cally.  Of course, such an automatic process will be 
much more constrained in the revisions it can ap-
ply, unlike the human reviser in our current study.  
For example, in automating the repair process we 
will be restricted to using only material from the 
source documents.  In addition, we may expand 
our taxonomy as necessary in exploring additional 
data.  We will need to relate revision in MDS to 
CST since revision required in a given summary 
depends on the relationships between sentences.  
Finally, we would like use the corpus of data we 
have collected to learn revision automatically. 
Acknowledgments 
The authors would like to thank Naomi Daniel, 
Hong Qi, Adam Winkel, Zhu Zhang and three 
anonymous reviewers for their helpful comments 
and feedback on this paper.  This work was par-
tially supported by the National Science Founda-
tion?s Information Technology Research program 
(ITR) under grant IIS-0082884. 
The version of MEAD that we used was de-
veloped at the Johns Hopkins summer workshop in 
2001 under the direction of Dragomir Radev.  We 
want to thank the following individuals for their 
work on MEAD: Sasha Blair-Goldensohn, John 
Blitzer, Arda Celebi, Elliott Drabek, Wai Lam, 
Danyu Liu, Hong Qi, Horacio Saggion and Simone 
Teufel.  
References 
[Barzilay et al 2001]  Regina Barzilay, Noemie 
Elhadad, and Kathleen R. McKeown. Sentence 
ordering in multi-document summarization.  In 
Proceedings of HLT, San Diego, CA, 2001.  
[Filatova & Hovy, 2001]  Elena Filatova and Edu-
ard Hovy.  Assigning time-stamps to event-clauses.  
In Proceedings, ACL Workshop on Temporal and 
Spatial Information Processing, Toulouse, France, 
July 2001.  
[Goldstein et al 2000]  Jade Goldstein, Mark Kan-
trowitz, Vibhu Mittal, and Jamie Carbonell.  Sum-
marizing text documents: sentence selection and 
evaluation metrics.  In Proceedings of the 22nd 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, Berkeley, CA, 
1999. 
[Halliday & Hasan, 1976]  M. Halliday and R. 
Hasan. Cohesion in English.  London: Longman, 
1976. 
[Harabagiu, 1999]  Sanda M. Harabagiu.  From 
lexical cohesion to textual coherence: a data driven 
perspective.  Journal of Pattern Recognition and 
Artificial Intelligence, 13(2): 247-265, 1999. 
[Hovy, 1993]  Eduard Hovy.  Automated discourse 
generation using discourse structure relations.  Ar-
tificial Intelligence 63, Special Issue on Natural 
Language Processing, 1993. 
[Jing & McKeown, 2000]  Hongyan Jing and 
Kathleen R. McKeown.  Cut and paste based text 
summarization. In Proceedings of the 1st Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics 
(NAACL'00), Seattle, WA, May 2000. 
[Mani et al 1999]  Inderjeet Mani, Barbara Gates, 
and Eric Bloedorn.  Improving summaries by re-
vising them.  In Proceedings of the 37th Annual 
Meeting of the ACL ?99, pages 558-565, Mary-
land, 1999. 
[Mann & Thompson, 1988]  William C. Mann and 
Sandra A. Thompson.  Rhetorical structure theory: 
toward a functional theory of text organization.  
Text, 8(3), 1988.   
[Marcu, 1997]  Daniel Marcu.  From discourse 
structure to text summaries.  In Proceedings of the 
ACL ?97 EACL ?97 Workshop on Intelligent Scal-
able Text Summarization, pages 82-88, Madrid, 
Spain, July 1997. 
[Marcu, 2000]  Daniel Marcu.  The theory and 
practice of discourse parsing and summarization, 
The MIT Press, November 2000. 
[Radev, 2000]  Dragomir Radev. A common the-
ory of information fusion from multiple text 
sources, step one: cross-document structure. In 
Proceedings, 1st ACL SIGDIAL Workshop on 
Discourse and Dialogue, Hong Kong, October 
2000. 
[Radev et al 2000]  Dragomir R. Radev, Hongyan 
Jing and Malgorzata Budzikowska. Centroid-based 
summarization of multiple documents: sentence, 
extraction, utility-based evaluation, and user stud-
ies.  In ANLP/NAACL Workshop on Summariza-
tion, Seattle, WA, April 2000. 
[Radev et al 2002]  Dragomir Radev, Simone Teu-
fel, Horacio Saggion, Wai Lam, John Blitzer, Arda 
Celebi, Hong Qi, Daniu Liu and Elliot Drabek.  
Evaluation challenges in large-scale multi-
document summarization: the MEAD project.  
Submitted to SIGIR 2002, Tampere, Finland, Au-
gust 2002. 
[Zhang et al, 2002]  Zhu Zhang, Sasha Blair-
Goldensohn, and Dragomir Radev. Towards CST-
enhanced summarization.  To appear in AAAI 
2002, August 2002. 
Appendix - Taxonomy of revisions in MDS 
 
 Description Operator(s) Example 
I. Discourse 
1) Topic shift In moving from one 
sentence to another, the 
topic shifts suddenly 
ADD transitional sen-
tence or phrase 
In a related story, the government of 
Hong Kong announced a proposal to 
require all drug rehabilitation centers.... 
2) Purpose Sentence lacks purpose 
in the context of the 
summary 
ADD a sentence or 
phrase that motivates 
the problematic seg-
ment 
In order to assist the ongoing investiga-
tion as to the cause of the crash, the 
U.S. team from the National Transporta-
tion Safety Board will join experts? 
3) Contrast Information in a given 
sentence contrasts with 
that in one or more 
previous sentences 
ADD a discourse 
marker such as ?how-
ever? or ?to contrast? 
MODIFY existing dis-
course marker 
However, according to reports on CNN, 
the control tower was concerned with 
the velocity and altitude of the plane, 
and had discussed these concerns with 
the pilot. 
4) Redundancy Sentence contains in-
formation that was pre-
viously reported 
DELETE the redundant 
constituent (non-head 
element of NP, PP or an 
entire relative clause or 
phrase) 
The crash of flight 072 that killed 143 
people?The plane, which was carrying 
the 143 victims, was headed to Bahrain 
from Egypt. 
5) Conditional Events in a given sen-
tence are conditioned 
on events in another 
sentence 
MODIFY the two sen-
tences: IF (sentence 
one), (sentence two).  
Change verb tenses to 
conditional. 
If the proposed measure were imple-
mented, it would ensure broadly the 
same registration standard to be applied 
to all drug treatment centers. 
II. Entities 
1) Underspecified 
entity 
A newly mentioned 
entity has no descrip-
tion or title; acronym is 
used with no name 
ADD full name, de-
scription or title for new 
entity; MODIFY acro-
nym by expanding 
Mrs. Clarie Lo, the Commissioner of 
Narcotics, said the proposal would be 
introduced for non-medical drug treat-
ment centers. 
2) Overspecified 
entity 
A noun phrase referring 
to an entity contains 
redundant information 
(full name and title, 
etc.)  
DELETE the redundant 
non-head elements of 
the NP; MODIFY alias 
a name 
Scientists around the world have been 
monitoring Mount Pinatubo?David 
Harlow, a ?guerrilla seismologist,? made 
accurate predictions of the eruptions of 
the volcano. 
3) Repeated entity A noun phrase describ-
ing an entity occurs too 
often in a given context. 
MODIFY replace NP 
with a pronoun; 
MODIFY use acronym 
In April 2000, Mrs. Lo announced that 
the number of young people abusing 
drugs fell in 1999.  She said, ?The num-
ber of drug abusers aged below 21?? 
4) Missing entity Sentence is missing 
subject/agent (perhaps 
as result of previous 
revision) 
ADD noun phrase or 
pronoun 
?the 28,000 Americans, who work at 
nearby naval bases.  They crowded into 
Subic Bay Naval Base as a bizarre tropi-
cal blizzard?  
5) Misused indefi-
nite article 
An indefinite article is 
used with a previously 
introduced entity 
MODIFY change in-
definite article to defi-
nite. 
The government of announced a pro-
posal?One year later, it announced that 
it intends to implement the proposed 
scheme. 
6) Misused definite 
article 
A definite article is used 
with a new entity 
MODIFY change defi-
nite article to indefinite 
article if entity is new. 
On Thursday, a second eruption ap-
peared to be smaller than anticipate. 
7) Missing article Entity is missing an 
article 
ADD definite article if 
entity has already been 
mentioned; ADD in-
definite article if entity 
is new 
The newspapers of Bahrain include: Al-
Ayam; Akhbar al-Khaleej (daily in Ara-
bic); Bahrain Tribune? 
8) Bare anaphor An anaphor has no an-
tecedent 
MODIFY change ana-
phor to its referential 
noun phrase 
If Pinatubo does have a massive erup-
tion, its primary means of causing 
death? 
 Description Operator(s) Example 
9) Misused quanti-
fier 
Quantifier used with an 
entity is inappropriate 
MODIFY quantifier to 
match with its antece-
dent; ?these? and ?those? 
must have plural ante-
cedent; ?such? can have 
a singular antecedent  
Mount Pinatubo erupted Satur-
day?Such volcanoes arise where one of 
the earth?s crust plates is slowly diving 
beneath another?  
III. Temporal relations concerns 
1) Temporal order-
ing 
Establish correct tempo-
ral relationships be-
tween events (or 
relative to a previous 
event) 
ADD time expression; 
ADD ordinal number; 
DELETE inappropriate 
time expression; 
MODIFY existing time 
expression 
Two days later, a second eruption ap-
peared to be smaller than scientists had 
anticipated. 
2) Absolute time of 
an event 
Indicate when an indi-
vidual event occurs 
ADD time expression 
(time, day, date, month, 
year) 
Lt. Col. Ron Rand announced at 5 a.m. 
Monday that the base should be evacu-
ated. 
3) Event repetition Indicate the repetition 
of an event 
ADD an adverb such as 
?again? 
Mount Pinatubo is likely to explode 
again in the next few days or weeks. 
4) Synchrony Two (or more) events 
occur at the same time 
ADD an adverb such as 
?meanwhile? or ?as?; 
MODIFY an existing 
adverb 
?all non-essential personnel should 
begin evacuating the base.  Meanwhile, 
dawn skies over central Luzon were 
filled with gray ash and steam? 
5) Anachronism Indicate that an event 
happened in the past 
(?flashback?) 
ADD a time expression  Pinatubo?s last eruption, over six hun-
dred years ago, yielded as much molten 
rock as the eruption of Mt. St. Helens... 
IV. Grammar concerns 
1) Run-on sentence Sentence is too long MODIFY split long 
sentence into two sepa-
rate sentences; 
DELETE conjunction 
Lt. Col. Ron Rand announced at 5 a.m. 
Monday that all personnel should begin 
evacuating the base.  Meanwhile, dawn 
skies over central Luzon were filled? 
2) Mismatched verb Verb tenses in the sen-
tences do not match 
MODIFY change verb 
tense; ADD aux verb 
The scheme would also impose uniform 
control on drug treatment centers. 
3) Missing punctua-
tion 
Punctuation is missing ADD appropriate punc-
tuation mark 
The ?guerrilla seismologist? from Menlo 
Park, who helped save thousands of 
lives in the Philippines, is right where? 
4) Awkward syntax Sentence is unclear due 
to its awkward syntax 
MODIFY syntactic 
transformation 
Since 1999, the ruling Emir has been 
Sheikh Hamad Bin-Isa Al-Khalifah , 
who was born on 28 January 1950. 
5) Parenthetical A parenthetical is inap-
propriate 
DELETE entire paren-
thetical; DELETE pa-
rentheses 
[ ( ]Volcanoes such as Pinatubo arise 
where one of the earth?s crust plates is 
slowly diving beneath another. [ ) ] 
6) Misused adverb An adverb is inappro-
priate 
DELETE adverb The scheme will [also] impose uniform 
control on drug treatment? 
7) Subhead-
ings/subtitles 
Subheadings or subtitles 
appear in summary and 
are not sentences 
DELETE subhead-
ings/subtitles; MODIFY 
to be grammatical 
[Smaller than anticipated;] On Thurs-
day a second eruption appeared to be 
smaller than anticipated by scientists? 
V. Location/setting concerns 
1) Location of event Establish where an 
event takes place 
ADD ? prepositional 
phrase indicating place 
(city, state, country) 
Three bodies were lain before the faith-
ful in the Grand Mosque in Manama, 
Bahrain during a special prayer? 
2) Collocation Two (or more) events 
occur in the same place 
ADD ? prepositional 
phrase or adverb that 
indicates collocation 
Meanwhile, in the same area, search 
teams sifted through the wreckage. 
3) Change of loca-
tion 
Summary moves from 
one event to another in 
a different location 
ADD ? prepositional 
phrase indicating place 
for both events  
Three bodies were lain before the faith-
ful in the Grand Mosque in Manama, 
Bahrain during a prayer?Meanwhile in 
Cairo, relatives of passengers waited...  
4) Place/source 
stamp 
Place/source stamp 
from original article 
ends up in summary 
DELETE ? stamp (but 
cache information for 
later use)  
[Cairo, Egypt (AP)] The crash of a Gulf 
Air flight that killed 143 people in Bah-
rain is a disturbing d?j? vu? 
Sub-event based multi-document summarization 
 
Naomi Daniel,1    Dragomir Radev,1,2    Timothy Allison3 
1School of Information 
2Department of Electrical Engineering and Computer Science 
3Department of Classical Studies 
University of Michigan 
{ndaniel, radev, tballiso}@umich.edu 
 
 
 
Abstract 
The production of accurate and complete 
multiple-document summaries is challenged by 
the complexity of judging the usefulness of 
information to the user.  Our aim is to determine 
whether identifying sub-events in a news topic 
could help us capture essential information to 
produce better summaries. In our first experiment, 
we asked human judges to determine the relative 
utility of sentences as they related to the sub-
events of a larger topic. We used this data to 
create summaries by three different methods, and 
we then compared these summaries with three 
automatically created summaries.  In our second 
experiment, we show how the results of our first 
experiment can be applied to a cluster-based 
automatic summarization system. Through both 
experiments, we examine the use of inter-judge 
agreement and a relative utility metric that 
accounts for the complexity of determining 
sentence quality in relation to a topic. 
 
1. Introduction 
Multiple articles on a particular topic tend to contain 
redundant information as well as information that is unique 
to each article.  For instance, different news sources 
covering the same topic may take different angles, or new 
information may become available in a later report.  So, 
while all the articles are related to the larger topic, each 
article may be associated with any of several sub-events.   
We wanted to find a way to capture the unique sub-event 
information that is characteristic in multiple-document 
coverage of a single topic.  We predicted that breaking 
documents down to their sub-events and capturing those 
sentences in each sub-event with the highest utility would 
produce an accurate, thorough, and diverse multi-
document summary. 
In our first experiment, we compared six 
methods of summarization to see which produces the best 
summaries.  The methods included three automatic and 
three manual methods of producing summaries.  We used 
relative utility to capture and measure subtleties in 
determining sentence relevance.  We created multiple 
document summaries using both a sub-event based 
approach and a topic-based approach.  Generally, we 
expected to find that the manual summaries performed 
better than the automatic summaries. In our second 
experiment, we designed a multi-document summarizer 
which relied on a clustering method, and we tested the 
three policies we devised for creating summaries from the 
manual summarization technique developed in our first 
experiment.  
2. Related Work 
Much work has preceded and informed this paper.  Allan 
et al?s (1998) work on summarizing novelty recognizes 
that news topics consist of a series of events ? what we call 
?sub-events,? to distinguish the difference between a news 
topic and its sub-events.  However, their method differs in 
its approach, which uses an algorithm to identify ?novel? 
sentences, rather than the use of human judges.  In other 
related work, sentences are either judged ?on-topic? or 
?off-topic? (Allan et al, 2001a) (Allan et al, 2001b).  
Carbonell and Goldstein use Maximal Marginal Relevance 
(MMR) to identify ?novel? information to improve query 
answering results, and they also apply this method to 
multiple-document summarization (Carbonell and 
Goldstein, 1997 and Goldstein, 1999).  Success in the use 
of inter-judge agreement has led us to pursue the use of the 
current evaluation methods.  However, this experiment 
differs from prior work in that we use judges to determine 
the relevance of sentences to sub-events rather than to 
evaluate summaries (Radev et al, 2000).  Finally, 
McKeown et al (1999), Hatzivassiloglou et al (2001) and 
Boros et al (2001) have shown the challenges and 
potential payoffs of using sentence clustering in extractive 
summarization.   
3. Article Corpus 
Our study involves two experiments carried out on one 
corpus of news articles.  The article corpus was selected 
from a cluster of eleven articles describing the 2000 crash 
of Gulf Air flight 072.  From these articles we chose a 
corpus of five articles, containing a total of 159 sentences.  
All the articles cover a single news event, the plane crash 
and its aftermath.  The articles were gathered on the web 
from sources reporting on the event as it unfolded, and 
come from various news agencies, such as ABC News, 
Fox News, and the BBC.  All of the articles give some 
discussion of the events leading up to and following the 
crash, with particular articles focusing on areas of special 
interest, such as the toll on Egypt, from where many of the 
passengers had come.  The article titles in Table 1, below, 
illustrate the range of sub-events that are covered under the 
crash topic. 
 
Article ID Source Date Headline 
30 BBC Aug. 25 Bodies recovered from 
Gulf Air crash 
41 Fox News Aug. 25 Egyptians Suffer Second 
Air Tragedy in a Year 
81 USA Today Aug. 25 One American among 143 
dead in crash 
87 ABC News Aug. 26 Prayers for victims of 
Bahrain crash 
97 Fox News Aug. 26 Did Pilot Error Cause Air 
Crash 
Table 1. Corpus article characteristics. 
4. Experiment 1: Sub-Event Analysis 
Our first experiment involved having human judges 
analyze the sentences in our corpus for degree of saliency 
to a series of sub-events comprising the topic.  
 
4.1 Description of Sub-Event User Study 
The goal of this experiment was to study the effectiveness 
of breaking a news topic down into sub-events, in order to 
capture not simply salience, but also diversity (Goldstein, 
1998). 
The sub-events were chosen to cover all of the 
material in the reports and to represent the most significant 
aspects of the news topic.  For the Gulf Air crash, we 
determined that the sub-events were: 
1. The plane takes off  
2. Something goes wrong  
3. The plane crashes  
4. Rescue and recovery effort  
5. Gulf Air releases information 
6. Government agencies react 
7. Friends, relatives and nations mourn 
8. Black box(es) are searched for 
9. Black box(es) are recovered 
10. Black box(es) are sent for analysis 
 
We instructed judges to rank the degree of 
sentence relevance to each sub-event.  Judges were 
instructed to use a scale, such that a score of ten indicated 
that the sentence was critical to the sub-event, and a score 
of 0 indicated that the sentence was irrelevant.  Thus, the 
judges processed the 159 sentences from 5 documents ten 
times, once pertaining to each sub-event.  This experiment 
produced for each judge 1590 data points which were 
analyzed according to the methods described in the next 
section.   
We used the data on the relevance of the 
sentences to the sub-events to calculate inter-judge 
agreement.  In this manner, we determined which 
sentences had the overall highest relevance to each sub-
event.  We used this ranking to produce summaries at 
different levels of compression. 
5. Methods for Producing Summaries 
To gather data about the effectiveness of dividing 
news topics into their sub-events for creating summaries, 
we utilized data from human judges, upon which we 
manually performed three algorithms. These algorithms 
and their application are described in detail below. We 
were interested to determine if the Round Robin method 
(described below,) which has been used by McKeown et 
al. (1999), Boros et al (2001) and by Hatzivassiloglou et 
al. (2001), was the most effective.  
5.1 Sub-Event-Based Algorithms 
After collecting judges? scores of relevance for each 
sentence for each subtopic, we then ranked the sentences 
according to three different algorithms to create multiple-
document summaries.  From this data, we created 
summary extracts using three algorithms, as follows:  
  Algorithm 1) Highest Score Anywhere - pick the 
sentence which is most relevant to any subevent, no matter 
the subevent; pick the next sentence which is most relevant 
to any subevent, etc. 
 Algorithm 2) Sum of All Scores - for each 
sentence, sum its relevance score for each cluster, pick 
the sentence with the highest sum; then pick the 
sentence with the second highest sum, etc. 
   Algorithm 3) Round Robin - pick the sentence 
which has the most relevance for subevent 1, pick the 
sentence with the most relevance for subevent 2, etc.  After 
picking 1 sentence from each subevent, pick the sentence 
with the 2nd best relevance to subevent 1, etc.
Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3 Judge 1 Judge 2 Judge 3
Article 30, 
Sentence 1 1 0 0 5 0 5 8 8 10
2 1 0 0 7 4 7 10 10 10
3 4 0 0 10 10 10 10 5 7
4 1 0 3 5 0 2 8 0 2
5 0 0 0 3 0 0 5 0 2
6 0 0 0 3 0 0 6 0 2
7 0 0 0 3 0 0 6 0 2
8 0 0 0 3 4 2 10 10 10
9 0 0 2 0 0 0 8 0 0
10 0 0 0 3 0 0 6 0 2
Sub-Event 1 Sub-Event 2 Sub-Event 3
 
Table 2. First ten sentences of article 30, shown with scores given by three judges for three sub-events. Judges often disagree 
on the degree of sentence relevancy.  Some sentences are used in more than one sub-event.
 
Algorithm 1 - Highest Score 
Anywhere (HSA): This algorithm was produced 
by summing the data across all judges to produce a total 
inter-judge score and keeping sub-events distinct, to see 
the inter-judge utility scores given to sub-events.  We 
ordered the sentences by ranking these scores in 
descending order and omitting duplicates, to produce the 
ten and twenty percent extracts.  For example, with data 
from seven judges on ten sub-events, the highest possible 
score for each sentence was seventy.  Thus seventy was 
the highest score.   
In the case that there was a tie between 
sentences, we ordered them by sub-event number (first 
sub-event first and tenth sub-event last). 
Algorithm 2 - Sum of All Scores 
(SAS): This algorithm was produced by summing the 
data across all judges to produce a total inter-judge score, 
and combining events so that we could see the utility 
scores given across sub-events.  We ordered the 
sentences by ranking these cross-event inter-judge utility 
scores in descending order and omitting duplicates, to 
produce the ten and twenty percent extracts. 
Algorithm 3 - Round Robin (RR): This 
algorithm was produced by summing the data across all 
judges to produce a total inter-judge score and keeping 
sub-events distinct, to see the inter-judge utility scores 
given to sub-events.  We ordered the sentences by 
ranking the inter-judge utility scores in descending order 
within each sub-event.  We then chose the top sentence 
from each sub-event (one through ten), the second 
highest sentence from each sub-event, and so on, 
omitting duplicates, until we had produced the ten and 
twenty percent extracts. 
In this manner, we created thirty-six sub-event-
based summary extracts ? six clusters, three algorithms, 
two compression rates ? which we then analyzed. 
The Sum of All Scores algorithm most closely 
replicates a centroid-based summary by combining the 
ten sub-event scores into one pan-topic score for each 
sentence.  Further, the Sum of All Scores algorithm is the 
sub-event algorithm most likely to pick sentences with a 
high ?general relevance,? which is what the baseline 
relative utility scores are meant to capture.  In contrast, 
the Highest Score Anywhere algorithm maintains the 
structure of the sub-event breakdown, preferring the 
highest score in any sub-event. Likewise, the Round 
Robin algorithm maintains the sub-event breakdown, but 
rather than preferring the highest score in any event, it 
selects the highest score from each sub-event, serially; 
this algorithm most closely resembles the Lead-based 
automatic summarizer, and is at the heart of 
Hatzivassiloglou et al?s (2001) SimFinder. 
5.2 Automatic Multi-Document 
Summaries 
The three automatic summarization methods that we 
used in our comparison have already been established. 
We compared our manual summaries to these established 
automatic multiple-document summarization methods: 
Centroid-based (MEAD), Lead-based and Random. 
MEAD:  First, we produced summaries 
using the MEAD system.  MEAD produces a centroid 
(vector) for all of the sentences and then selects those 
sentences which are closest to the centroid. MEAD 
measures similarity with the cosine measurement and 
TF*IDF weighting.  Mead also adjusts a sentence?s score 
based on its length, its position in the original document 
and its similarity to sentences already selected for the 
extract. (Radev et al 2000). 
Lead-Based: We also produced summaries 
by the Lead-based method.  This method involves 
assigning the highest score to the first sentence in each 
article, then the second sentence in each article, and so 
on. 
Random: We created summaries with every 
possible combination of sentences for each summary 
length.  This allowed us to compute the average random 
relative utility score. 
6. Relative Utility 
Following (Radev et al, 2000), we used relative utility as 
our metric.  Relative utility was chosen for advantages in 
a couple of areas.   
Relative utility is a metric which measures 
sentence relevance.  It allows us to distinguish the degree 
of importance between sentences, providing a more 
flexible model for evaluating sentence utility (Radev et 
al., 2000).  Studies involving sentence extraction have 
often been predicated upon determining the usefulness of 
sentences as either useful or non-useful (Allan et al 
2001b).  However, determining the usefulness of 
sentences is more complex than a simple a binary choice 
can account for.  We employ a relative utility metric to 
account for subtleties in determining the saliency of 
sentences. 
Another advantage of the relative utility metric 
is that, although human judges have often agree very 
little on which sentences belong in a summary, they tend 
to agree on how important sentences are to a topic or 
event; thus, relative utility makes it possible to leverage 
this agreement. 
To calculate relative utility, we had human 
subjects assign a score to each sentence in a corpus of 
articles.  The score reflects the subject?s perception of a 
sentence?s relevance to the overall topic of the corpus.  
The scale our judges were instructed to use ranged from 
zero to ten.  A score of zero indicated that the sentence 
was irrelevant; whereas a score of ten indicated that the 
sentence was crucial to the understanding of the topic.  
So that judges? scores can be fairly compared, each 
judge?s scores are normalized by the highest score and 
lowest score which that judge gives any sentence. 
Relative utility is determined by first adding 
together the utility scores given to each sentence by each 
judge.  Each sentence in a summary is then awarded the 
total of the judges? scores for that sentence.  Finally, the 
summary?s total score is divided by the best possible 
score, given the size of the summary.   
For example, let us assume that a cluster has 
three sentences (A, B and C) which have been judged by 
two judges in the following way: A 10, 9, B 8, 6 and C 6, 
5.  That is, judge 1 gives sentence A a 10, while judge 2 
gives sentence A a 9, and so on.  In the first step, we sum 
the judges? scores for each sentence, yielding (A 19, B 
14, C 11).  If a summarizer has to pick a 2 sentence 
summary, and it picks A and C, its utility score is 30.  
We then divide this score by the best possible 2 sentence 
summary, in this case A and B, whose utility is 33, 
yielding a final relative utility of .91.  
7. Extract Creation 
Summaries can be created by abstracting or extracting 
[Mani, 2001].  For purposes of comparison with MEAD, 
an extractive summarizer, we used an extractive method 
to create all six summary types: sum of all scores, highest 
score anywhere, round robin, MEAD, lead-based, and 
random.  
7.1 Clusters 
Each of the summarization methods was 
employed at both ten and twenty percent compression 
rates.  We used the summaries thus produced to consider 
how compression rates could influence the effectiveness 
of the six summarization methods.  In our first 
experiment, we additionally looked at varying 
combinations of the five articles, such that we examined 
the corpus in six clusters, as shown in the figure below.  
We selected these article combinations to maximize the 
diversity of sources in each cluster, and to achieve a 
variable number of articles in a cluster. 
Combination 1) articles 30 + 41 + 81 + 87 + 97 
Combination 2) articles 30 + 41 + 81 
Combination 3) articles 41 + 81 + 87 
Combination 4) articles 81 + 87 + 97 
Combination 5) articles 87 + 97 + 30 
Combination 6) articles 97+ 30 + 41 
Figure 1.  Article clusters. 
 
  
 
  
  10%      20%    
 HSA SAS RR MEAD Lead Rand HAS SAS RR MEAD Lead Rand 
Cluster 1 0.641 0.686 0.717 0.617 0.795 0.480 0.542 0.745 0.683 0.621 0.722 0.521 
Cluster 2 0.629 0.739 0.716 0.629 0.800 0.459 0.637 0.786 0.659 0.623 0.741 0.490 
Cluster 3 0.568 0.698 0.544 0.672 0.701 0.435 0.572 0.735 0.631 0.647 0.629 0.470 
Cluster 4 0.406 0.669 0.651 0.662 0.714 0.489 0.539 0.722 0.596 0.653 0.738 0.521 
Cluster 5 0.646 0.675 0.698 0.604 0.797 0.549 0.598 0.739 0.733 0.631 0.749 0.575 
Cluster 6 0.622 0.698 0.693 0.595 0.880 0.508 0.623 0.762 0.717 0.552 0.817 0.536 
Average = 0.585 0.694 0.670 0.630 0.781 0.487 0.585 0.748 0.670 0.621 0.733 0.519 
Table 3. Results: Best performing algorithm at each cluster/compression rate shown in bold. 
 
8. Results from the first experiment 
Some of our results met our expectations, while others 
surprised us (see Table 3).  The Sum of All Scores manual 
algorithm produces the best summaries at the twenty 
percent compression rate.  At the ten percent compression 
rate, data shows Lead-based summaries performing best, 
with the Sum of All Scores algorithm coming in right 
behind.  Mead scores in the mid-range as expected, for 
both compression rates, just behind the Round Robin 
Algorithm.  In contrast, the random method leads in low 
scores, with the Highest Score Anywhere algorithm 
coming in only slightly higher.  Random sets the lower 
bound.  Here, we discuss the details of our findings and 
their significance in more detail. 
8.1 Manual Algorithms   
Both the Sum of All Scores, and Round Robin algorithms 
performed better than MEAD, with the highest score 
anywhere algorithm performing less well.  This result is 
reasonable, based upon the characteristics of the 
algorithms.  Algorithm 2 (SAS), the best performer among 
the manual summaries, used the sum of all scores across 
events and judges; thus, it tapped into which sentences 
were most popular overall.  Algorithm 3 (RR), also better 
than MEAD, used a round robin technique, which, 
similarly to the Lead-based results, tapped into the 
pyramid quality of news journalism.  Algorithm 1 (HSA), 
poorest performer second to Random, used the highest 
score in any event by inter-judge score; its weakness was 
in negating both the benefits of the pyramid structure of 
the judges? sentence rankings, as well as the popularity of 
sentences across events.  
8.2 Compression Rate  
For extracts at the ten percent compression rate, Lead-
based sets the upper, and random the lower, bound.  
However, the Sum of All Scores algorithm performed 
better at the twenty percent compression rate, beating 
Lead-based for best summaries.  Each method produced 
better summaries overall at ten percent compression rate, 
except for Algorithm 2, which performed better at the 
twenty percent compression rate.   
We believe that SAS performed better at the 
twenty percent compression rate as a result of two 
characteristics: as the sum of scores across sub-events, this 
algorithm preferred both sentences that received higher 
scores, as well as sentences which were highly ranked 
most frequently.  Therefore, it is weighted toward those 
sentences that carry information essential to several sub-
events.  Because of these sentences? relevancy to more 
than one sub-event, they are most likely to be important to 
the majority of readers, regardless of the user?s particular 
information task.  This can also be seen as popularity 
weighting, with those sentences getting the most and best 
scores from judges producing the most useful summaries.  
The patterns uncovered by this result should be leveraged 
for future improvements to automatic summarizers. 
8.3 Lead-Based Summaries 
We were not extremely surprised to find that Lead-based 
summaries produced better summaries at the 10% 
summary rate.  This result may be explained by the 
pyramid structure of news journalism, which, in a sense, 
pre-ranks document sentences in order of importance, in 
order to convey the most critical information first.  As our 
corpus was comprised entirely of news articles, this effect 
could be exaggerated in our results.  As expected, though, 
the Random summarizer set the lower bound. 
8.4 Manual Summaries and MEAD 
Most significantly, among the mid-range performers, the 
data demonstrates what we expected to find:  Two of the 
three new sub-event-based algorithms perform better than 
MEAD.  Identifying sub-events in news topic coverage is 
one method that we have shown can be utilized to help 
create better summaries.   
 9. Automatic Clustering and Extraction 
In our second experiment, we were interested to see how 
the different strategies would work with a simple 
clustering-based multi-document summarizer.   We did not 
expect our clustering algorithm to neatly partition the data 
according to the subevents we identified in our first 
experiment, but we did want to see if our findings about 
SAS would hold true for automatically partitioned data.  
And so we turned to sentence clustering.  While Boros et 
al. (2001) report poor performance but some promise to 
this method, Hatzivassiloglou et al (2001) have exploited 
clustering with very good results in SimFinder.  Both rely 
on the RR method, although SimFinder considers several 
other important factors in sentence selection. 
9.1 Automatic Clustering 
Because of the vast number of variables associated with 
designing a cluster-based summarization algorithm, we 
chose to limit our system so that we could focus on RR, 
HSA and SAS.  To give a sense of our performance, we 
also ran a purely centroid-based summarization algorithm. 
We used K-means clustering, and obtained 
results for K = 2-20, at both the 10% and 20% summary 
levels.  By this process, we created K clusters, seeded them 
as discussed below, and then for each sentence, we found 
that cluster to which the sentence was closest.  After filling 
the clusters, we checked again to see if each sentence was 
in its best cluster.  We kept doing this until equilibrium 
was reached (usually no more than 6 cycles).   
For our similarity metric we used the cosine 
measure with inverse document frequency (IDF), inverse 
sentence frequency (ISF) (following Neto et al (2000) and 
no term-weighting.  We ran all of these permutations 
twice, once ignoring sentences with 9 words or fewer (as is 
MEAD?s default) and once ignoring sentences with 2 
words or just 1.  We did not use stop words, stemming, or 
syntactic parsing.  Further, we did not factor in the location 
of the sentences in their original documents, although both 
MEAD and SimFinder do this.   
Initially, we used a method of randomly seeding 
the clusters, but we found this method extremely unstable.  
We then devised the following method: 1) for the first 
cluster, find the sentence which is closest to the centroid of 
the document cluster, 2) for each sentence after that, find 
the sentence which is maximally different from those 
sentences already picked as seeds.  
9.2 Automatic Extraction 
After creating the clusters by this method, we extracted 
sentences with the same three methods of interest, HSA, 
SAS, and RR.  For this experiment, we also added a 
simple Centroid policy.  Under this policy, a centroid 
vector was created for all of the sentences, and then for 
each sentence the cosine measure was computed against 
the centroid.  The sentences were then sorted by their 
cosine scores with the centroid.  The top 10% or 20% were 
selected for the summary. 
 For all policies, the extraction algorithm would 
not select a sentence which had a cosine of 0.99 or higher 
with any sentence already in the summary.   For 
comparison, MEAD?s default is 0.7.  In the future, we 
would like to study the effect of this parameter on 
information diversity. 
10. Results for Automatic Clustering 
In Table 4, we report our findings from the second 
experiment.  This table presents the average of the 
performances across all of the clustering options (2 
clusters to 20 clusters) for the specified parameters.  In 
general for a 10% summary, the SAS method outperforms 
the other methods, leading Centroid by only a small 
amount.  At the 20% level, the Centroid policy beats all 
other algorithms, although SAS with ISF and a 2-word 
sentence minimum comes close. 
 Some other interesting findings emerge from this 
table as well, namely term-weighting seems beneficial for 
all methods except for HSA, and ISF seems generally 
more beneficial for SAS and Centroid than for RR or 
HSA. 
 
 
 
SAS RR HSA Centroid SAS RR HSA Centroid
min. 2 word IDF 0.602 0.560 0.481 0.546 0.639 0.570 0.533 0.617
min. 2 word ISF 0.672 0.485 0.453 0.669 0.650 0.520 0.522 0.656
min. 2 word none 0.531 0.550 0.528 0.515 0.581 0.557 0.576 0.588
min. 9 word IDF 0.608 0.488 0.472 0.546 0.634 0.535 0.523 0.616
min. 9 word ISF 0.609 0.501 0.460 0.670 0.630 0.529 0.525 0.656
min. 9 word none 0.528 0.511 0.498 0.517 0.588 0.558 0.562 0.582
10% 20%
 
Table 4: Results from our automatic, cluster-based summarizer 
 
 Table 4 is unable to capture, however, the 
marked variation in results depending on how many 
clusters were initially selected.  In Table 5, we present our 
findings for the overall best parameters.  As can be seen, 
SAS is the most common policy.  In fact, SAS appears in 
the top 22 out 25 combinations at the 10% level and 20 out 
of 25 at the 20% compression level.   
 
 
Top 10 performers, 10% summary Top 10 performers, 20% summary 
# clusters ISF/IDF min. sent. length policy rel. util. # clusters ISF/IDF min. sent. length policy rel. util. 
15 ISF 2 SAS 0.718 4 ISF 2 SAS 0.686 
16 ISF 2 SAS 0.711 3 ISF 2 SAS 0.682 
14 ISF 2 SAS 0.710 2 ISF 2 SAS 0.681 
20 ISF 2 SAS 0.705 2 ISF 9 RR 0.669 
13 ISF 2 SAS 0.704 3 ISF 9 HSA 0.665 
17 ISF 2 SAS 0.704 5 ISF 2 SAS 0.665 
11 IDF 9 SAS 0.684 2 ISF 9 HSA 0.664 
8 IDF 9 SAS 0.681 7 ISF 2 SAS 0.661 
7 ISF 2 SAS 0.679 9 IDF 9 SAS 0.660 
19 ISF 2 SAS 0.678 Na ISF 9 CENTROID 0.656 
Table 5: Top 10 parameters for the both rates of summarization 
 
Tables 4 and Tables 5, taken together, suggest 
that SAS should be leveraged to improve performance 
over the pure centroid method.  More work needs to be 
done to determine the appropriate number of clusters to 
begin with, but it is interesting that there appears to be an 
inverse relationship, namely, the smaller summary seems 
to benefit from small, tightly packed clusters, while the 
larger summary benefits from a few noisy clusters. 
11. Conclusions 
While the Lead-based policy from our first experiment still 
outperforms all of our automatic cluster-based summaries 
at the 10% and 20% levels, our findings about SAS are 
important for future efforts to summarize by partitioning.  
As discussed, the pyramid structure of news articles may 
have boosted the scores of the lead-based policy.  In 
applications of summarizers, where the information is not 
presorted, we believe that clustering and then extraction 
with SAS could offer the best results. 
We conclude that multi-document summarization 
is improved by two specific elements.  Firstly, taking into 
account varying degrees of relevancy, as opposed to a 
polarized relevant/non-relevant metric.  Secondly, 
recognizing the sub-events that comprise a single news 
event is essential. 
12. Future Work 
In future work, we see four areas for improvement.  We 
would like to improve our simple cluster-based algorithm.  
Hatzivassiloglou et al (2001) have shown several ways of 
doing this.  Second, we would like to have human judges 
evaluate the final summaries and give scores based on how 
well the summary captures the most relevant parts of the 
document cluster and how well the summary avoids 
repetition.  This would allow us to see how effective the 
RU method is as well as how well our summarizer is 
functioning.  Third, we would like to run a machine 
learning algorithm on a number of different and varied 
clusters to find which parameter settings work best for 
each type of cluster.  We suspect that the optimal number 
of original clusters, and the choice of ISF or IDF, could be 
determined by the amount of redundancy in the cluster and 
the desired size of the extract, but more work remains to be 
done on this.  Finally, we need to test the best clustering 
method against other methods -- centroid-based, MMR, 
lexical-chain, key-word to name a few. 
12. Acknowledgements 
This work was partially supported by the National Science 
Foundation's Information Technology Research program 
(ITR) under grant IIS-0082884. Our thanks go to the 
anonymous reviewers for their very helpful comments. 
The version of MEAD that we used was 
developed at the Johns Hopkins summer workshop in 
2001 under the direction of Dragomir Radev and later 
upgraded at the University of Michigan. We want to thank 
the following individuals for their work on MEAD: Sasha 
Blair-Goldensohn, Simone Teufel, Arda Celebi, Wai Lam, 
Hong Qi, John Blitzer, Horacio Saggion, Elliott Drabek, 
Danyu Liu, Michael Topper, and Adam Winkel. 
13. References 
 
[1] Allan, J. et al, 1998.  ?On-line New Event Detection 
and Tracking.? In Proceedings of the 21st annual 
international ACM SIGIR conference on Research and 
development in information retrieval. Melbourne, 
Australia. 
[2] Allan, J. et al, 2001a. ?Temporal summaries of news 
topics.? In Proceedings of the 24th annual international 
ACM SIGIR conference on Research and development in 
information retrieval. 
[3] Allan, J. et al, 2001b. ?Topic models for summarizing 
novelty.? ARDA Workshop on Language Modeling and 
Information Retrieval. Pittsburgh, Pennsylvania. 
[4] Boros, E. et al  2001.  ?A Clustering Based Approach 
to Creating Multi-Document Summaries.? In Proceedings of 
the 24th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, New 
Orleans, LA, 2001. 
[5] Carbonell, J. and J.G. Goldstein, 1998.  ?The use of 
MMR, diversity-based reranking for reordering documents 
and producing summaries.?  In Proceedings of the 21st 
annual international ACM SIGIR conference on Research 
and development in information retrieval. Melbourne, 
Australia. 
[6] Goldstein, J.G., 1999.  ?Automatic text summarization 
of multiple documents.?  Carnegie Mellon University.  
[7] Hatzivassiloglou et al, 2001.  ?SimFinder: A Flexible 
Clustering Tool for Summarization.?  NAACL, Workshop 
on Automatic Summarization.  Pittsburgh, PA. 
[8] Mani, I., 2001.  ?Automatic summarization.?  Natural 
Language Processing, ed. Ruslan Mitkov. Philadelphia, 
PA: John Benjamins Publishing. 
[9] Marcu, D., 2000. The theory and practice of discourse 
parsing and summarization.  Cambridge, MA: MIT Press. 
[10] McKeown, K. and J. Klavans, V. Hatzivassiloglou, R. 
Barzilay, E. Eskin, 1999.  ?Towards multidocument 
summarization by reformulation: Progress and prospects.? 
In Proceedings of AAAI-99, Orlando, Fl., pp. 453-60. 
[11] Neto, Joel et al, 2000. ?Document Clustering and 
Text Summarization.? In N. Mackin, editor, Proc. 4th 
International Conference Practical Applications of 
Knowledge Discovery and Data Mining (PADD-2000), 
pages 41-55, London, January. The Practical Application 
Company. 
[12] Radev D., H. Jing and M. Budzikowska, 2000. 
?Centroid-based summarization of multiple documents: 
sentence extraction, utility-based evaluation, and user 
studies.? ANLP/NAACL Workshop on Summarization. 
Seattle, WA. 
[13] Radev, D., S. Blair-Goldensohn and Z. Zhang, 2001. 
?Experiments in single and multi-document summarization 
using MEAD.? First Document Understanding 
Conference. New Orleans, LA. 
 
 
 
Multi-document summarization using off the shelf compression software
Amardeep Grewal
 
, Timothy Allison

, Stanko Dimitrov
 
, Dragomir Radev
  
 
Department of Electrical Engineering and Computer Science

Department of Classical Studies

School of Information
University of Michigan

asgrewal,tballiso,sdimitro,radev  @umich.edu
Abstract
This study examines the usefulness of common
off the shelf compression software such as gzip
in enhancing already existing summaries and
producing summaries from scratch. Since the
gzip algorithm works by removing repetitive
data from a file in order to compress it, we
should be able to determine which sentences in
a summary contain the least repetitive data by
judging the gzipped size of the summary with
the sentence compared to the gzipped size of
the summary without the sentence. By picking
the sentence that increased the size of the sum-
mary the most, we hypothesized that the sum-
mary will gain the sentence with the most new
information. This hypothesis was found to be
true in many cases and to varying degrees in
this study.
1 Introduction
1.1 The connection between text compression and
multidocument summarization
A standard way for producing summaries of text docu-
ments is sentence extraction. In sentence extraction, the
summary of a document (or a cluster of related docu-
ments) is a subset of the sentences in the original text
(Mani, 2001). A number of techniques for choosing the
right sentences to extract have been proposed in the litera-
ture, ranging from word counts (Luhn, 1958), key phrases
(Edmundson, 1969), naive Bayesian classification (Ku-
piec et al, 1995), lexical chains (Barzilay and Elhadad,
1997), topic signatures (Hovy and Lin, 1999) and cluster
centroids (Radev et al, 2000).
Most techniques for sentence extraction compute a
score for each individual sentence, although some recent
work has started to pay attention to interactions between
sentences. On the other hand, and particularly in mul-
tidocument summarization, some sentences may be re-
dundant in the presence of others and such redundancy
should lead to a lower score for each sentence propor-
tional to the degree of overlap with other sentences in
the summary. The Maximal Marginal Relevance (MMR)
method (Carbonell and Goldstein, 1998) does just that.
In this paper, we are taking the idea of penalizing re-
dundancy for multi-document summaries further. We
want to explore existing techniques for identifying re-
dundant information and using them for producing better
summaries.
As in many areas in NLP, one of the biggest challenges
in multi-document summarization is deciding on a way
of calculating the similarity between two sentences or
two groups of sentences. In extractive multi-document
summarization, the goal is, on the one hand, to select the
sentences which best represent the main point of the doc-
uments and, on the other, to pick sentences which do not
overlap much with those sentences which have already
been selected. To accomplish the task of sentence com-
parison, researchers have relied on stemming and count-
ing n-gram similarity between two sentences. So, for ex-
ample, if we have the following two sentences: ?The dogs
go to the parks? and ?The dog is going to the park,? they
would be nearly identical after stemming: ?the dog [be]
go to the park,? and any word overlap measure would be
quite high (unigram cosine of .943).
In some ways, gzip can be thought of as a radical stem-
mer which also takes into account n-gram similarity. If
the two sentences were in a file that was gzipped, the size
of the file would be much smaller than if the second sen-
tence were ?A cat wanders at night.? (unigram cosine of
0). By comparing the size of the compressed files, we can
pick that sentence which is most similar to what has al-
ready been selected for the summary (high compression
ratio) or the most different (low compression ratio), de-
pending on what type of summary we would prefer.
On a more information theoretic basis, as Benedetto et
al. observe (Benedetto et al, 2002a), comparing the size
of gzipped files allows us to roughly measure the distance
(increase in entropy) between a new sentence and the al-
ready selected sentences. Benedetto et al (Benedetto et
al., 2002a) find that on their task of language classifica-
tion, gzip?s measure of information distance can effec-
tively be used as a proxy for semantic distance. And so,
we set out to see if we could usefully apply gzip to the
task of multi-document summarization.
Gzip is a compression utility which is publicly avail-
able and widely used (www.gzip.org). Benedetto et al
(Benedetto et al, 2002a) summarize the algorithm behind
gzip and discuss its relationship to entropy and optimal
coding. Gzip relies on the algorithm developed by Ziv
and Lempel (Ziv and Lempel, 1977). Following this al-
gorithm, gzip reads along a string and looks for repeated
substrings, if it finds a substring which it has already read,
it replaces the second occurrence with two numbers, the
length of the substring and the distance from that loca-
tion back to the original string. If the substring length
is greater than the distance, then the unzipper will know
that the sequence repeats.
In our framework, we use an off-the-shelf extractive
summarizer to produce a base summary. We then create a
number of summaries containing precisely one more sen-
tence than the base summary. If  	 is the total number
of sentences in the input cluster, and 
 is the number of
sentences already included in the base, there are  

possible summaries of length 
 sentences. One of
them has to be chosen over the others. In this work, we
compress each of the  	
 candidate summaries and
observe the relative increase in the size of the compressed
file compared to the compressed base summary. The ba-
sic idea is that sentences containing the most new infor-
mation will result in relatively longer compressed sum-
maries (after normalizing for the uncompressed length of
the newly added sentence). We will discuss some varia-
tions of this algorithm in the next section.
There are two issues which must be kept in mind in ap-
plying gzip to problems beyond data compression. First,
because of the sequential nature of the algorithm, com-
pression towards the beginning of the file will not be as
great as that later in the file. Second, there is a 32k limit
on the length of the window that gzip considers. So, if
?abc? appears at the beginning of a string, and then also
appears 33k later (but nowhere in between), gzip will not
be able to compress the second appearance. This means
that our process is ?blind? to sentences in the summary
which happen 32k earlier. This could potentially be a
drawback to our approach, but in practice, given realistic
text lengths, we have not found a negative effect.
The impetus for our approach is (Benedetto et al,
2002a; Benedetto et al, 2002b) who report on their use
of gzip for language classification, authorship attribution,
and topic classification. In their approach, they begin
with a set of known documents. For each document,
they measure the ratio of the uncompressed document
to the compressed document. Then they append an un-
known document to each known document cluster, and
compress these new documents. Their algorithm then
chooses whichever document had the greatest compres-
sion in relation to its original. As (Goodman, 2002) ob-
serves, using compression techniques for these tasks is
not an entirely new approach, nor is it very fast. Never-
theless, we wanted to determine the efficacy of applying
Benedetto et al?s methods to the task of multi-document
summarization.
2 Description of the method
The aim of this study was to determine if gzip is effective
as a summarization tool when used in conjunction with an
existing summarizer. We chose MEAD1, a public-domain
summarization system, which can be downloaded on the
Internet (Radev et al, 2002). The version of MEAD used
in this experiment was 3.07.
To produce a summary of a target length 
 sen-
tences, we perform the following steps:
1. First, get MEAD to create a summary of size 
 sen-
tences, where 
 is specified in advance. This sum-
mary will be called the base summary.
2. Compress the base summary using gzip. Let  be
the length of the base summary before compression
and ffLexPageRank: Prestige in Multi-Document Text Summarization
Gu?nes? Erkan
 
, Dragomir R. Radev
  
 
Department of EECS,

School of Information
University of Michigan

gerkan,radev  @umich.edu
Abstract
Multidocument extractive summarization relies on
the concept of sentence centrality to identify the
most important sentences in a document. Central-
ity is typically defined in terms of the presence of
particular important words or in terms of similarity
to a centroid pseudo-sentence. We are now consid-
ering an approach for computing sentence impor-
tance based on the concept of eigenvector centrality
(prestige) that we call LexPageRank. In this model,
a sentence connectivity matrix is constructed based
on cosine similarity. If the cosine similarity be-
tween two sentences exceeds a particular predefined
threshold, a corresponding edge is added to the con-
nectivity matrix. We provide an evaluation of our
method on DUC 2004 data. The results show that
our approach outperforms centroid-based summa-
rization and is quite successful compared to other
summarization systems.
1 Introduction
Text summarization is the process of automatically
creating a compressed version of a given text that
provides useful information for the user. In this pa-
per, we focus on multi-document generic text sum-
marization, where the goal is to produce a summary
of multiple documents about the same, but unspeci-
fied topic.
Our summarization approach is to assess the cen-
trality of each sentence in a cluster and include the
most important ones in the summary. In Section 2,
we present centroid-based summarization, a well-
known method for judging sentence centrality. Then
we introduce two new measures for centrality, De-
gree and LexPageRank, inspired from the ?prestige?
concept in social networks and based on our new ap-
proach. We compare our new methods and centroid-
based summarization using a feature-based generic
summarization toolkit, MEAD, and show that new
features outperform Centroid in most of the cases.
Test data for our experiments is taken from Docu-
ment Understanding Conferences (DUC) 2004 sum-
marization evaluation to compare our system also
with other state-of-the-art summarization systems.
2 Sentence centrality and centroid-based
summarization
Extractive summarization produces summaries by
choosing a subset of the sentences in the original
documents. This process can be viewed as choosing
the most central sentences in a (multi-document)
cluster that give the necessary and enough amount
of information related to the main theme of the clus-
ter. Centrality of a sentence is often defined in terms
of the centrality of the words that it contains. A
common way of assessing word centrality is to look
at the centroid. The centroid of a cluster is a pseudo-
document which consists of words that have fre-
quency*IDF scores above a predefined threshold. In
centroid-based summarization (Radev et al, 2000),
the sentences that contain more words from the cen-
troid of the cluster are considered as central. For-
mally, the centroid score of a sentence is the co-
sine of the angle between the centroid vector of the
whole cluster and the individual centroid of the sen-
tence. This is a measure of how close the sentence is
to the centroid of the cluster. Centroid-based sum-
marization has given promising results in the past
(Radev et al, 2001).
3 Prestige-based sentence centrality
In this section, we propose a new method to mea-
sure sentence centrality based on prestige in social
networks, which has also inspired many ideas in the
computer networks and information retrieval.
A cluster of documents can be viewed as a net-
work of sentences that are related to each other.
Some sentences are more similar to each other while
some others may share only a little information with
the rest of the sentences. We hypothesize that the
sentences that are similar to many of the other sen-
tences in a cluster are more central (or prestigious)
to the topic. There are two points to clarify in this
definition of centrality. First is how to define sim-
ilarity between two sentences. Second is how to
compute the overall prestige of a sentence given its
similarity to other sentences. For the similarity met-
ric, we use cosine. A cluster may be represented by
a cosine similarity matrix where each entry in the
matrix is the similarity between the corresponding
sentence pair. Figure 1 shows a subset of a cluster
used in DUC 2004, and the corresponding cosine
similarity matrix. Sentence ID d   s  indicates the
 th sentence in the   th document. In the follow-
ing sections, we discuss two methods to compute
sentence prestige using this matrix.
3.1 Degree centrality
In a cluster of related documents, many of the sen-
tences are expected to be somewhat similar to each
other since they are all about the same topic. This
can be seen in Figure 1 where the majority of the
values in the similarity matrix are nonzero. Since
we are interested in significant similarities, we can
eliminate some low values in this matrix by defining
a threshold so that the cluster can be viewed as an
(undirected) graph, where each sentence of the clus-
ter is a node, and significantly similar sentences are
connected to each other. Figure 2 shows the graphs
that correspond to the adjacency matrix derived by
assuming the pair of sentences that have a similarity
above 	
 and  , respectively, in Figure 1 are
similar to each other. We define degree centrality as
the degree of each node in the similarity graph. As
seen in Table 1, the choice of cosine threshold dra-
matically influences the interpretation of centrality.
Too low thresholds may mistakenly take weak simi-
larities into consideration while too high thresholds
may lose much of the similarity relations in a clus-
ter.
ID Degree (0.1) Degree (0.2) Degree (0.3)
d1s1 4 3 1
d2s1 6 2 1
d2s2 1 0 0
d2s3 5 2 0
d3s1 4 1 0
d3s2 6 3 0
d3s3 1 1 0
d4s1 8 4 0
d5s1 4 3 1
d5s2 5 3 0
d5s3 4 1 1
Table 1: Degree centrality scores for the graphs in
Figure 2. Sentence d4s1 is the most central sentence
for thresholds 0.1 and 0.2.
3.2 Eigenvector centrality and LexPageRank
When computing degree centrality, we have treated
each edge as a vote to determine the overall pres-
tige value of each node. This is a totally democratic
method where each vote counts the same. How-
ever, this may have a negative effect in the qual-
ity of the summaries in some cases where several
unwanted sentences vote for each and raise their
prestiges. As an extreme example, consider a noisy
cluster where all the documents are related to each
other, but only one of them is about a somewhat dif-
ferent topic. Obviously, we wouldn?t want any of
the sentences in the unrelated document to be in-
cluded in a generic summary of the cluster. How-
ever, assume that the unrelated document contains
some sentences that are very prestigious consider-
ing only the votes in that document. These sen-
tences will get artificially high centrality scores by
the local votes from a specific set of sentences. This
situation can be avoided by considering where the
votes come from and taking the prestige of the vot-
ing node into account in weighting each vote. Our
approach is inspired by a similar idea used in com-
puting web page prestiges.
One of the most successful applications of pres-
tige is PageRank (Page et al, 1998), the underly-
ing technology behind the Google search engine.
PageRank is a method proposed for assigning a
prestige score to each page in the Web independent
of a specific query. In PageRank, the score of a page
is determined depending on the number of pages
that link to that page as well as the individual scores
of the linking pages. More formally, the PageRank
of a page  is given as follows:
PR Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 87?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The North American Computational Linguistics Olympiad (NACLO) 
 
 
Dragomir R. Radev Lori S. Levin Thomas E. Payne 
SI, EECS, and Linguistics Language Technologies Institute Department of Linguistics 
University of Michigan Carnegie-Mellon University University of Oregon 
radev@umich.edu lsl@cs.cmu.edu tpayne@uoregon.edu 
 
 
 
 
 
 
Abstract 
1 Introduction 
NACLO (North American Computational Linguis-
tics Olympiad) is an annual Olympiad-style contest 
for high school students, focusing on linguistics, 
computational linguistics, and language technolo-
gies. 
The goal of NACLO is to increase participation 
in these fields by introducing them before students 
reach college. Since these subjects are not nor-
mally taught in high school, we do not expect stu-
dents to have any background of these areas before 
the contest.  The contest consists of self-contained 
problems that can be solved with analytical think-
ing, but in the course of solving each problem, the 
students learn something about a language, culture, 
linguistic phenomenon, or computational tool.  
The winners of NACLO are eligible to partici-
pate in the International Linguistics Olympiad as 
part of the US team. 
1.1 History of the LO and ILO 
The International Olympiad in Linguistics is one 
of twelve international Science Olympiads (the 
others include Mathematics, Physics, Chemistry, 
Biology, Informatics, Philosophy, Astronomy, Ge-
ography, and Earth Science). It has existed since 
2003 and has, so far, been held exclusively in 
Europe (Russia, Estonia, Bulgaria, and the Nether-
lands). ILO 2007 took place in Zelenogorsk near 
St. Petersburg, Russia whereas ILO 2008 will be in 
Slantchev Bryag near Burgas, Bulgaria. ILO 2009 
will be held in Poland.  
Individual national linguistics Olympiads have 
been held in Russia since 1965 (based on an initia-
tive by Andrey Zaliznyak) and in other countries 
more recently1. Recently, a collection of problems 
from different decades appeared in Russian (Be-
likov et al, 2007). 
1.2 Linguistics Contests in the US 
Thomas Payne pioneered LO-style competitions 
in the USA by organizing three consecutive con-
tests for middle and high school students in the 
Eugene, Oregon area in 1998-2000. In the course 
of publicizing NACLO, we have discovered that 
other local linguistics contests have taken place in 
Tennessee, San Jose, and New York City. 
1.3 Origin of NACLO 
NACLO began with a planning workshop 
funded by NSF in September 2006. The attendees 
included faculty and graduate students from about 
ten universities as well as representatives from 
NSF and ACL.  Two high school teachers were 
present.  The workshop opened with presentations 
from organizers of other Olympiads and contests in 
linguistics and computer programming. In particu-
lar we received excellent advice from Ivan Derz-
hanski, representing the International Linguistics 
Olympiad, and Boris Iomdin, representing the 
Moscow Olympiad. The remainder of the work-
shop dealt with scheduling the first contest, elect-
                                                          
1
 The first author of this paper participated in the Bulgarian 
national LO in the early 1980s. 
87
ing committee chairs, and making organizational 
decisions. 
1.4 Pedagogical goals 
We have two goals in organizing NACLO.  We 
want to increase broad participation and diversity 
in all language-related careers.  We want every 
student to have a fun and educational experience 
and have a positive attitude toward taking linguis-
tics and language technologies courses in college.  
However, we also want to conduct a talent search 
for the most promising future researchers in our 
field.     NACLO uses two mechanisms to be sure 
that we reach all levels of participation.  The first 
mechanism is to separate an open round with easier 
problems from an invitation-only round with 
harder problems.  The second mechanism is related 
to grading the problems.  Forty percent of the score 
is for a correct answer and sixty percent is for ex-
plaining the answer.   The students who write the 
most insightful explanations are the focus of our 
talent search.    
 
When publicizing NACLO in high schools we 
have been focusing on certain aspects of linguistics 
and computer science.  With respect to linguistics, 
we emphasize that languages have rules and pat-
terns that native speakers are not aware of; that 
there are procedures by which these rules and pat-
terns can be discovered in your own language; and 
that the same procedures can be used to discover 
rules and patterns in languages other than your 
own.  With respect to computer science the term 
computational thinking has been coined (Wing 
2006) to refer to those parts of the field that are not 
about computers or programming:  thinking algo-
rithmically, using abstraction to model a problem, 
structuring and reducing a search space, etc.   
1.5 Organization at the national level 
NACLO has two co-chairs, currently Lori 
Levin, Carnegie Mellon University, and Thomas 
Payne, University of Oregon.  Dragomir Radev is 
the program chair and team coach.  Amy Troyani, 
a high school teacher with board certification, is 
the high school liaison and advisor on making the 
contest appropriate and beneficial to high school 
students. 
NACLO has several committees.  James Puste-
jovsky currently chairs the sponsorship committee.  
The other committees are currently unchaired, al-
though we would like to thank William Lewis (out-
reach and publicity) and Barbara Di Eugenio 
(followup) for chairing them in the first year.  
NACLO is not yet registered as a non-profit or-
ganization and does not yet have a constitution.  
We would welcome assistance in these areas. 
The national level organization provides materi-
als that are used at many local sites.  The materials 
include a comprehensive web site 
(http://www.naclo.cs.cmu.edu), practice 
problems, examples of flyers and press releases, 
PowerPoint presentations for use in high schools, 
as well as contest booklets from previous competi-
tions. 
The contest is held on the same day in all loca-
tions (universities and "online" sites as described 
below).  In 2007 there was a single round with 195 
participants.  In 2008 there was an open round with 
763 participants and an invitation-only round with 
115 participants.  Grading is done centrally.  Each 
problem is graded at one location to ensure consis-
tency. 
Three national prizes are awarded for first, sec-
ond, and third place. National prizes are also given 
for the best solution to each problem. Local hosts 
can also award prizes for first, second, and third 
place at their sites based on the national scores. 
1.6 Funding 
The main national expenses are prizes, planning 
meetings, and the trip to the International Linguis-
tics Olympiad (ILO).  The trip to the ILO is the 
largest expense, including airfare for eight team 
members (two teams of four), a coach, and two 
chaperones.  The national level sponsors are the 
National Science Foundation (2007, 2008), Google 
(2007, 2008), Cambridge University Press (2007, 
2008), and the North American Chapter of the As-
sociation for Computational Linguistics (2007). 
The organizers constantly seek additional sponsors. 
1.7 Publicity before the contest 
At the national level, NACLO is publicized 
through its own web site as well as on LinguistList 
and Language Log.  From there, word spreads 
through personal email and news groups. No press 
releases have been picked up by national papers 
that we know of.  Local level publicity depends on 
the organization of local schools and the hosting 
88
university's high school outreach programs. In 
Pittsburgh, publicity is facilitated by a central mail-
ing list for gifted program coordinators in the city 
and county. Some of the other local organizers (in-
cluding James Pustejosvky at Brandeis, Alina 
Johnson at the University of Michigan and Barry 
Schiffman at Columbia University as well as sev-
eral others) sent mail to literally hundreds of high 
schools in their areas. Word of mouth from the 
2007 contest also helped reach out to more places.  
1.8 Registration 
NSF REU-funded Justin Brown at CMU created 
an online registration site for the 2008 contest 
which proved very helpful. Without such a site, the 
overhead of dealing with close to 1,000 students, 
teachers, and other organizers would have been 
impossible. 
1.9 Participation of graduate and under-
graduate students 
Graduate and undergraduate students participate 
in many activities including:  web site design, vis-
iting high schools, formulating problems, testing 
problems, advising on policy decisions, and facili-
tating local competitions. 
2 Problem selection  
We made a difficult decision early on not to re-
quire knowledge of linguistics, programming or 
mathematics.  Requiring these subjects would have 
reduced diversity in our pool of contestants as well 
as its overall size.   Enrollment in high school pro-
gramming classes has dropped, perhaps because of 
a perception that programming jobs are not inter-
esting.  NACLO does not require students to know 
programming, but by introducing a career option, it 
gives them a reason to take programming classes 
later. 
2.1 Problem types 
The NACLO problem sets include two main 
categories of problems: ?traditional? and ?compu-
tational/formal?. The ILO includes mostly tradi-
tional problems which include translations from 
unknown languages, glyph decoding, calendar sys-
tems, kinship systems, mathematical expressions 
and counting systems, among others. The other 
category deals with linguistic phenomena (often in 
English) as well as algorithms and formal analyses 
of text. 
2.2 Problem committee 
A problem committee was formed each year to 
work on the creation, pre-testing, and grading of 
problems. The members in 2007 included Emily 
Bender, John Blatz, Ivan Derzhanski, Jason Eisner, 
Eugene Fink, Boris Iomdin, Mahesh Joshi, Anagha 
Kulkarni, Will Lewis, Patrick Littell, Ruslan Mit-
kov, Thomas Payne, James Pustejovsky, Roy 
Tromble, and Dragomir Radev (chair). In 2008, the 
following people were members: Emily Bender, 
Eric Breck, Lauren Collister, Eugene Fink, Adam 
Hesterberg, Joshua Katz, Stacy Kurnikova, Lori 
Levin, Will Lewis, Patrick Littell, David 
Mortensen, Barbara Partee, Thomas Payne, James 
Pustejovsky, Richard Sproat, Todor Tchervenkov, 
and Dragomir Radev (chair). 
2.3 Problem pool 
At all times, the problem committee maintains a 
pool of problems which are constantly being 
evaluated and improved. Professional linguists and 
language technologists contribute problems or 
problem ideas that reflect cutting-edges issues in 
their disciplines. These are edited and tested for 
age appropriateness,  and the data are thoroughly 
checked with independent experts. 
2.4 Booklets 
The three booklets (one for 2007 and two for 
2008) were prepared using MS Publisher. Addi-
tionally, booklets with solutions were prepared in 
MS Word. All of these are available from the 
NACLO web site. 
2.5 List of problems 
This is the list of problems for NACLO 2007 (8 
problems) and 2008 (12 problems). They can be 
divided into two categories: traditional (2007: C, 
D, G and 2008: A, C, D, E, G, J, K) and for-
mal/computational (2007: A, B, E, F, H and 2008: 
B, F, H, I, L). The traditional problems addressed 
topics such as phonology, writing systems, calen-
dar systems, and cognates, among others. The 
other category included problems on stemming, 
89
finite state automata, clustering, sentence similarity 
identification, and spectrograms. 
 
 
2007 
 
A. English (Molistic) 
B. English (Encyclopedia) 
C. Ancient Greek 
D. Hmong 
E. English (Verb forms) 
F. English (Spelling correction) 
G. Huishu (Phonology) 
H. English (Sentence processing) 
 
2008 (A-E Open; F-L Invitational) 
 
A. Apinaye (Brazil) 
B. Hindi 
C. Ilocano (Philippines) 
D. Swedish and Norwegian 
E. Aymara (South America) 
F. Japanese 
G. Manam Pile (Papua New Guinea) 
H. English (Stemming) 
I. Rotokas (Automata; Bougainville Island) 
J. Irish 
K. Mayan (Calendar) 
L. English (Spectrograms) 
 
Figure 1: List of languages used in NACLO 2007 and 
2008. 
3 Contest administration  
NACLO is run in a highly distributed fashion and 
involves a large number of sites across the USA in 
Canada. 
3.1 Local administration 
NACLO is held at hosting universities and also 
"online".  The online category includes students 
who cannot get to one of the hosting universities, 
but instead are monitored by a teacher at a conven-
ient location, usually the student's high school.  
There were three hosting universities (Carnegie-
Mellon, Brandeis, and Cornell) in 2007 and thir-
teen hosting universities (the three above + U. 
Michigan, U. Illinois, U. Oregon, Columbia, Mid-
dle Tennessee State, San Jose State, U. Wisconsin, 
U. Pennsylvania, U. Ottawa, and U. Toronto) in 
2008.  Any university in the US or Canada may 
host NACLO.  Local organizers are responsible for 
providing a room for the contest, contacting high 
local high schools, and facilitating the contest on 
the specified contest date.  Local organizers may 
decide on the number of participants.  The number 
of participants at the 2008 sites ranged from a 
handful to almost 200 (CMU-Pitt). 
Local organizers may choose their level of in-
vestment of time and money.  They may spend 
only a few hours recruiting participants from one 
or two local high schools and may spend a small 
amount of money on school visits and copying.  
But they may also run large scale operations in-
cluding extensive fundraising and publicity.  The 
site with the largest local participation, Carnegie 
Mellon/University of Pittsburgh, donated adminis-
trative staff time, invested hundreds of volunteer 
hours, and raised money for snacks and souvenirs 
from local sponsors2. The CMU-Pitt site also hosts 
a problem club for faculty and students where 
problems are proposed, fleshed out, and tested.  At 
the University of Oregon, a seminar course was 
taught on Language Task Creation (formulation of 
problems) for which university students received 
academic credit.    
3.2 Remote (?online?) sites 
We had about 65 such sites in 2008. All local 
teachers and other facilitators did an amazing job 
following the instructions for administering the 
competition and for promptly returning the sub-
missions by email or regular mail. 
3.3 Clarifications 
During each of the three competitions, the jury 
was online (in some cases for 8 hours in a row) to 
provide live clarifications. Each local facilitator 
was asked to be online during the contest and relay 
to the jury any questions from the students. The 
jury then, typically within 10 minutes, either re-
plied ?no clarification needed? (the most frequent 
reply) or provided an answer which was than 
posted online for all facilitators to see. We re-
ceived dozens of clarifications requests at each of 
the rounds. 
3.4 Grading 
Grading was done by the PC with assistance 
from local colleagues. To ensure grade consis-
tency, each problem was assigned to a single 
                                                          
2
 We are grateful to the Pittsburgh sponsors: M*Modal, 
Viv?simo, JustSystems Evans Research, and Carnegie Mel-
lon's Leonard Gelfand Center for Service Learning and Out-
reach. 
90
grader or team of graders. Graders were asked to 
provide grading rubrics which assigned individual 
points for both ?practice? (that is, getting the right 
answers) and ?theory? (justifying the answers). 
3.5 Results from 2007 
195 students participated in 2007. The winners 
are shown here. One of the students was a high 
school sophomore (15 years old) while three were 
seniors at the time of the 2007 NACLO. 
 
 
1. Rachel Zax, Ithaca, NY 
2. Ryan Musa, Ithaca, NY 
3. Adam Hesterberg, Seattle, WA 
4. Jeffrey Lim, Arlington, MA 
5. (tie) Rebecca Jacobs, Encino, CA 
5. (tie) Michael Gottlieb, Tarrytown, NY 
7. (tie) Mitha Nandagopalan, San Jose, CA 
7. (tie) Josh Falk, Pittsburgh, PA 
Alternate. Anna Tchetchetkine, San Jose, CA 
 
Figure 2: List of team members from 2007. Mitha was 
unable to travel and was replaced by Anna Tchetchet-
kine. 
3.6 2008 Winners 
The 2008 contest included 763 participants in 
the Open Round and 115 participants in the Invita-
tional Round. The winners of the Invitational 
Round are listed below. These are the eight stu-
dents who are eligible to represent the USA at the 
2008 ILO. As of the writing of this paper, all eight 
were available for the trip. One of the eight is a 
high school freshman (9th grade). 
 
 
1. Guy Tabachnick, New York, NY 
2. Jeffrey Lim, Arlington, MA 
3. Josh Falk, Pittsburgh, PA 
4. Anand Natarajan, San Jose, CA 
5. Jae-Kyu Lee, Andover, MA 
6. Rebecca Jacobs, Encino, CA 
7. Hanzhi Zhu, Shrewsbury, MA 
8. Morris Alper, San Jose, CA 
 
Figure 3: List of team members from 2008. 
3.7 Canadian Participation 
Canada participated for the first time in 2008 
(about 20 students from Toronto, a handful from 
Ottawa and one from Vancouver). Two students 
did really well at the 2008 Open (one ranked sec-
ond and two tied for 13th) but were not in the top 
20 at the Invitational. 
3.8 Diversity 
About half of the participants in NACLO were 
girls in 2007 and 2008. In 2007, 25 out of the top 
50 students were female. 
The two US teams that went to the ILO in 2007 
included three girls, out of eight total team mem-
bers (two teams of four).  The 2008 teams include 
only one girl. 
3.9 Other statistics 
Some random statistics: (a) of the top 20 stu-
dents in 2008, 14 are from public schools, (b) 26 
states, 3 Canadian provinces, and the District of 
Columbia were represented in 2008. 
4 Preparation for the ILO  
Preparation for the ILO was a long and painful 
process. We had to obtain visas for Russia, fund 
and arrange for the trip, and do a lot of practices. 
4.1 Teams 
 One of the students who was eligible to be on 
the second USA team was unable to travel. We 
went down the list of alternates and picked a dif-
ferent student to replace her. 
4.2 Funding 
The ILO covered room and board for the first 
team and the team coach. The second team was 
largely self-funded (including airfare and room and 
board). Everyone else was funded as part of the 
overall NACLO budget. The University of Michi-
gan covered the coach?s airfare. 
4.3 Training 
We ran multiple training sessions. The activities 
included individual problem solving, team problem 
solving (using Skype?s chat facility), readings, as 
well as live lectures (both at the summer school in 
Estonia and on the day before the main ILO in 
Russia).  
4.4 Travel logistics 
Four students, two chaperones, and one parent 
left early to attend a summer school organized by 
the Russian team in Narva, Estonia. The third 
91
chaperone and three students traveled directly to 
the ILO. The eighth student traveled with her par-
ents and did some sightseeing in Russia prior to the 
ILO. 
5 Participation in the ILO  
The ILO was organized by a local committee from 
St. Petersburg chaired by Stanislav Gurevych. The 
organization was extraordinary. Everything (prob-
lem selection, grading, hotel, activities, food) was 
excellent. 
5.1 Organization of the ILO 
The ILO was held at a decent hotel in Ze-
lenogorsk, a suburb of St. Petersburg on the Baltic 
Sea. The first day included an orientation, the sec-
ond day was the individual contest and team build-
ing activities, the third day ? an excursion to St. 
Petersburg, the fourth day ? the team contest and 
awards ceremony. 
5.2 Problems 
The problems given at the ILO were quite di-
verse and difficult. The hardest problems were the 
one in the Ndom language which involved a non-
standard number system and the Hawaiian problem 
given at the team contests which involved a very 
sophisticated kinship system. 
 
 
Turkish/Tatar 
Braille 
Ndom (Papua New Guinea) 
Movima (Bolivia) 
Georgian (Caucasus) 
Hawaiian 
 
Figure 4: List of languages used in ILO 2007. 
5.3 Results 
Adam Hesterberg scored the highest score in the 
individual contest. One of the two US teams (Re-
becca Jacobs, Joshua Falk, Michael Gottlieb, and 
Anna Tchetchetkine) tied for first place in the team 
event. 
6 Future directions  
The unexpected interest in the NACLO poses a 
number of challenges for the organizers. Further 
challenges arise from our desire to cover more 
computational problems. 
6.1 Grading and decentralization? 
Grading close to 5,000 submissions from 763 
students in 2008 took a toll on our problem com-
mittee. The process took more than two weeks. We 
are considering different options for future years, 
e.g., reducing the number of problems in the first 
round or involving some sort of self-selection (e.g., 
asking each potential participant to do a practice 
test and obtain a minimal score on it). These op-
tions are suboptimal as they detract from some of 
the stated goals of the NACLO and we will not 
consider them seriously unless all other options 
(e.g., recruiting more graders). have been ex-
hausted.  
6.2 Problem diversity 
We would like to include more problem types, 
especially on the computational end of the contest. 
This is somewhat of a conflict with the ILO which 
includes mostly ?traditional? LO problems. One 
possibility is to have the first round be more com-
putational whereas the invitational round would be 
more aimed at picking the team members for the 
ILO by focusing more on traditional problems. 
6.3 Practice problems 
We will be looking to recruit a larger pool of 
problem writers who can contribute problems of 
various levels of difficulty (including very easy 
problems and problems based on the state of the art 
in research in NLP). We are also looking for vol-
unteers to translate problems from Russian, includ-
ing the recently published collection ?Zadachi 
Lingvisticheskyh Olimpiad?. 
6.4 Other challenges 
The biggest challenges for the NACLO in both 
years were funding and time management.  
In 2007, four of the students had to pay for their 
own airfare and room and board. At the time of 
writing, the budget for 2008 is still not fully cov-
ered. The current approach with regard to sponsor-
ship is not sustainable since NSF cannot fund 
recurring events and the companies that we ap-
proached either gave nothing or gave a relatively 
92
small amount compared to the overall annual 
budget. 
The main organizers of the NACLO each spent 
several hundred hours (one of them claims ?the 
equivalent to 20 ACL program committee chair-
manships?), mostly above and beyond their regular 
appointments. For NACLO to scale up and be suc-
cessful in the future, a much wider pool of organ-
izers will be needed. 
6.5 Other countries 
Dominique Estival told us recently that an LO 
will take place in Australia in Winter 2008 (that is, 
Summer 2008 in the Northern Hemisphere). OzLO 
(as it is called) will be collaborating with NACLO 
on problem sets. Other countries such as the 
United Kingdom and the Republic of Ireland are 
considering contests as well. One advantage that 
these countries all have is that they can share (Eng-
lish-language) problem sets with NACLO. 
6.6 Participant self-selection 
Some Olympiads provide self-selection prob-
lems. Students who score poorly on these problem 
sets are effectively discouraged from participation 
in the official contest. If the number of participants 
keeps growing, we may need to consider this op-
tion for NACLO. 
6.7 More volunteers 
NACLO exerted a tremendous toll on the organ-
izers. Thousands of hours of volunteer work went 
into the event each year. NACLO desperately 
needs more volunteers to help at all levels (prob-
lem writing, local organization, web site mainte-
nance, outreach, grading, etc). 
7 Overall assessment  
While it will take a long time to properly assess the 
impact of NACLO 2007 and 2008, we have some 
preliminary observations to share. 
7.1 Openness 
We made a very clear effort to reach out to all 
high school students in the USA and Canada. 
Holding the contest online helped make it truly 
within everyone?s reach. Students and teachers 
overwhelmingly appreciated the opportunity to 
participate at no cost (other than postage to send 
the submissions back to the jury) and at their own 
schools. Students who participated at the university 
sites similarly expressed great satisfaction at the 
opportunity to meet with peers who share their in-
terests. 
7.2 Diversity and outreach 
We were pleased to see that the number of male 
and female participants was nearly equal. A num-
ber of high schools indicated that clubs in Linguis-
tics were being created or were in the works. 
7.3 Success at the ILO 
Even though the US participated for the first 
time at the ILO, the performance shown there (in-
cluding first place individually and a tie for first 
place in the team contest) was outstanding. 
Acknowledgments 
We want to thank everyone who helped turn 
NACLO into a successful event. Specifically, Amy 
Troyani from Taylor Allderdice High School in 
Pittsburgh, Mary Jo Bensasi of CMU, all problem 
writers and graders (which include the PC listed 
above as well as Rahel Ringger and Julia Work-
man) and all local contest organizers (James Puste-
jovsky, Lillian Lee, Claire Cardie, Mitch Marcus, 
Kathy McKeown, Barry Schiffman, Lori Levin, 
Catherine Arnott Smith, Richard Sproat, Roxana 
Girju, Steve Abney, Sally Thomason, Aleka 
Blackwell, Roula Svorou, Thomas Payne, Stan 
Szpakowicz, Diana Inkpen, Elaine Gold). James 
Pustejovsky was also the sponsorship chair, with 
help from Paula Chesley. Ankit Srivastava, Ronnie 
Sim and Willie Costello co-wrote some of the 
problems with members of the PC. Eugene Fink 
helped with the solutions booklets, Justin Brown 
worked on the web site, and Adam Hesterberg was 
an invaluable member of the team throughout. 
Other people who deserve our gratitude include 
Cheryl Hickey, Alina Johnson, Patti Kardia, Josh 
Cannon, Christina Hunt, Jennifer Wofford, and 
Cindy Robinson. Finally, NACLO couldn?t have 
happened without the leadership and funding pro-
vided by NSF and Tanya Korelsky in particular as 
well as the generous sponsorship from Google, 
Cambridge University Press, and the North Ameri-
can Chapter of the ACL (NAACL). 
93
The authors of this paper are also thankful to 
Martha Palmer for giving us feedback on an earlier 
draft.  
NACLO was partially funded by the National 
Science Foundation under grant IIS 0633871 Plan-
ning Workshop for a Computational Linguistics 
Olympiad. 
References  
Vasileios Hatzivassiloglou and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of Adjec-
tives, ACL 1997.  
Jeannette Wing, Computational Thinking, CACM vol. 
49, no. 3, March 2006, pp. 33-35. 
V. I. Belikov, E. V. Muravenko and M. E. Alexeev, 
editors. Zadachi Lingvisticheskikh Olimpiad. 
MTsNMO. Moscow, 2007. 
Appendix A. Summary of freeform com-
ments 
?I think it's a great outreach tool to high schools.  I was es-
pecially impressed by the teachers who came and talked to 
[the linguistics professors] about starting a linguistics club? 
?The problems are great.  One of our undergraduates ex-
pressed interest in a linguistics puzzle contest (on the model of 
Google's and MS's puzzle contests) at the undergrad level.? 
?We got a small but very high-quality group of students.  
To get a larger group, we'd need to start earlier.? 
?Things could be more streamlined.  I think actually *less* 
communication, but at key points in the process, would be 
more effective.? 
?It also would have been nice if there were a camp, like 
with the other US olympiads, so that more students would get 
the chance to learn about linguistics? 
?Just get the word out to as many schools as possible. You 
could also advertise on forums like AOPS, Cogito, and even 
CollegeConfidential ? where students are looking for intel-
lectual challenges?. 
?The problems helped develop the basic code breaking.? 
?Having a camp would be a huge benefit, but otherwise I 
think the contest was done very well.  Thank you for bringing 
it to the US.? 
?Maybe send a press release to school newspapers and ask 
them to print something about it.? 
?My 9 students enjoyed participating even though none of 
them made it to the second round.  Several have indicated that 
they want to do it again next year now that they know what it 
is like.? 
?I used every opportunity to utter the phrase "computa-
tional linguistics" to other administrators, at meetings, with 
parents, students, other teachers. People inevitably want to 
know more!? 
?As I mentioned previously, we are all set to start up a new 
math/WL club next year. YAY!? 
?Advertise with world language professional organizations 
(i.e., ACTFL) and on our ListServs (i.e., FLTeach)? 
?It was wonderful. KUDOS!? 
?There were several practice sessions, about half run by a 
math teacher (who organizes many of the competitions of this 
nature) and half by the Spanish teacher. Also, several of the 
English teachers got really excited about it (especially the 
teacher who teaches AP English Language, who teaches often 
about logical reasoning) and offered extra credit to the stu-
dents who took it.? 
?The preparation for the naclo was done entirely by the 
math club.? 
?It was a very useful competition. First, it raised awareness 
about linguistics among our students. They knew nothing 
about this area before, and now they are looking for opportuni-
ties to study linguistics and some started visiting linguistic 
research seminars at the University of Washington.? 
?The Olympiad was interesting to most students because it 
was very different from all the other math Olympiads we par-
ticipate in. Students saw possibilities for other application of 
their general math skills. In addition, the students who won 
(reasonably succeeded in) this Olympiad were not the same 
students that usually win math contests at  our school. This 
was very useful for their confidence, and showed everybody 
that broadening skills is important.? 
?I was the only one to take the contest from my school, so 
it didn't really increase awareness that much. I, however, 
learned a lot about linguistics, and the people who I told about 
the contest seemed to find it interesting also.? 
?As a result of this competition, an Independent-Study 
Linguistics Course was offered this spring for a few interested 
students.? 
?Three students who participated in NACLO are now do-
ing an Independent Study course with my colleague from th 
e World Languages dept (who had a linguistics course in 
college)? 
?I'd like to see more linguistic indoctrination, so that math 
nerds are converted over to the good side.? 
?next year I will  teach a Computational Linguistics semi-
nar? 
Appendix B. Related URLs 
 
http://www.naclo.cs.cmu.edu/ 
http://www.cogito.org/ContentRedirect.aspx? 
    ContentID=16832 
http://www.cogito.org/Interviews/ 
    InterviewsDetail.aspx?ContentID=16901 
http://www.ilolympiad.spb.ru/ 
http://cty.jhu.edu/imagine/PDFs/Linguistics.pdf 
http://www.nsf.gov/news/news_summ.jsp? 
    cntn_id=109891 
http://photofile.name/users/anna_stargazer/2949079/ 
 
Figure 5: List of additional references URLs. 
Appendix C. Sample problems 
We include here some sample problems as well as 
one solution. The rest of the solutions are available 
on the NACLO Web site. 
94
C.1. Molistic 
This is a problem from 2007 written by 
Dragomir Radev and based on [Hatzivassiloglou 
and McKeown 1997]. 
 
 
Imagine that you heard these sentences: 
 
      Jane is molistic and slatty. 
      Jennifer is cluvious and brastic. 
      Molly and Kyle are slatty but danty. 
      The teacher is danty and cloovy. 
      Mary is blitty but cloovy. 
      Jeremiah is not only sloshful but also weasy. 
      Even though frumsy, Jim is sloshful. 
      Strungy and struffy, Diane was a pleasure to watch. 
      Even though weasy, John is strungy. 
      Carla is blitty but struffy. 
      The salespeople were cluvious and not slatty. 
 
1. Then which of the following would you be likely to 
hear? 
 
      a. Meredith is blitty and brastic. 
      b. The singer was not only molistic but also cluvious. 
      c. May found a dog that was danty but sloshful. 
 
2. What quality or qualities would you be looking for in a 
person? 
 
      a. blitty 
      b. weasy 
      c. sloshful 
      d. frumsy 
 
3. Explain all your answers. (Hint: The sounds of the words 
are not relevant to their meanings.) 
 
Figure 6: ?Molistic? problem from 2007. 
C.2. Garden Path 
This is another problem from 2007. 
 
 
True story: a major wireless company recently started an advertising 
campaign focusing on its claim that callers who use its phones 
experience fewer dropped calls. 
 
The billboards for this company feature sentences that are split into 
two parts. The first one is what the recipient of the call hears, and the 
second one - what the caller actually said before realizing that the call 
got dropped. The punch line is that dropped calls can lead to serious 
misunderstandings. We will use the symbol // to separate the two parts 
of such sentences. 
 
(1) Don't bother coming // early. 
(2) Take the turkey out at five // to four. 
(3) I got canned // peaches. 
 
These sentences are representative of a common phenomenon in 
language, called "garden path sentences". Psychologically, people 
interpret sentences incrementally, before waiting to hear the full text. 
When they hear the ambiguous start of a garden path sentence, they 
assume the most likely interpretation that is consistent with what they 
have heard so far. They then later backtrack in search of a new parse, 
should the first one fail. 
 
In the specific examples above, on hearing the first part, one 
incorrectly assumes that the sentence is over. However, when more 
words arrive, the original interpretation will need to be abandoned. 
 
(4) All Americans need to buy a house // is a large amount of money. 
(5) Melanie is pretty // busy. 
(6) Fat people eat // accumulates in their bodies. 
 
1. Come up with two examples of garden path sentences that are not 
just modifications of the ones above and of each other. Split each of 
these two sentences into two parts and indicate how hearing the 
second part causes the hearer to revise his or her current parse.  
 
For full credit, your sentences need to be such that the interpretation 
of the first part should change as much as possible on hearing the 
second part. For example, in sentence (6) above, the interpretation of 
the word "fat" changes from an adjective ("fat people") to a noun ("fat 
[that] people eat...").  Note: sentences like "You did a great job..., // 
NOT!" don't count.  
 
2.  Rank sentences (4), (5), (6) as well as the two sentences from your 
solution to H1 above, based on how surprised the hearer is after 
hearing the second part.  What, in your opinion, makes a garden path 
sentence harder to process by the hearer?  
 
Figure 7: ?Garden Path? problem from 2007. 
 
95
C.3. Ilocano 
This 2008 problem was written by Patrick Littell 
of the University of Pittsburgh. 
 
 
The Ilocano language is one of the major languages of the Philippines, 
spoken by more than 8 million people.  Today is it written in the 
Roman alphabet, which was introduced by the Spanish, but before that 
Ilocano was written in the Baybayin script.  Baybayin (which literally 
means ?spelling?) was used to write many Philippine languages and 
was in use from the 14th to the 19th centuries. 
 
1. Below are twelve Ilocano words written in Baybayin. Match them 
to their English translations, listed in scrambled order below.  
 
____________ 
  ____________ 
  ____________ 
   ____________ 
	 
   ____________ 
	  
 	 
   ____________ 
	 	 
 ____________ 
	 	 	 	 
 ____________ 
	  	 	 	 
 ____________ 

   ____________ 
  ____________ 
   ____________ 
 
{ to look, is skipping for joy, is becoming a skeleton, to buy, various 
skeletons, various appearances, to reach the top, is looking, 
appearance, summit, happiness, skeleton } 
 
2. Fill in the missing forms. 
 
	  	 
 ____________ 
    ____________ 
     ____________ 
____________          (the/a) purchase 
____________          is buying 
 
3. Explain your answers to 1 and 2. 
 
Figure 8: Ilocano problem from 2008. 
 
 
Practical: 11 points 
 
1. Translations (1/2 point each) 

appearance 
  various appearances 
  to look 
   is looking 
	 
   happiness 
	  
 	 
   is skipping for joy 
	 	 
 skeleton 
	 	 	 	 
 various skeletons 
	  	 	 	 
 is becoming a skeleton 

   to buy 
  summit 
   to reach the top 
 
2. Missing forms (1 point each) 
 
	  	 
 to become a skeleton 
    various summits 
     is reaching the top 

            (the/a) purchase 

  
             is buying 
 
Assign ? point each if the basic symbols (the consonants) are correct, 
and the other ? point if the diacritics (the vowels) are correct. 
 
Theoretical: 9 points 
* The first step in this problem must be to divide the English items 
into semantically similar groups (1 pt) and divide the Baybayin items 
into groups based on shared symbols (1 pt). 
* From this they can deduce that the group including  must 
correspond to the ?look/appearances? group (4 members each), that 
including 	 	 
 to the ?skeleton? group (3 members each), and 

   must be ?to buy? (1 each).  For getting this far they should 
get another 2 points. 
* Figuring out the nature of the Baybayin alternations is the tricky 
part.  A maximally good explanation will discover that there are two 
basic processes: 
? From the basic form, copy the initial two symbols and add 
them to the beginning.  The first should retain whatever 
diacritic it might have, but the second should have its dia-
critic (if any) replaced by a cross below. 
? Insert  as the second symbol, and move the initial sym-
bol?s diacritic (if any) to this one.  Add an underdot to the 
first symbol. 
* Discovering these two processes, and determining that the third 
process is the result of doing both, is worth 3 points.  Discovering 
these two processes, and describing the third as an unrelated process ? 
that is, not figuring out that it?s just a combination of the first two ? is 
worth 2 points.  Figuring out these processes without reference to the 
diacritics is worth 1 point, whether or not they correctly determine the 
nature of the third process. 
* All that remains is to match up which processes indicate which 
categories, which shouldn?t be hard if they?ve gotten this far.  Their 
description of how to determine this is worth another 1 point. 
* The remaining 1 point is reserved to distinguish particularly elegant 
solutions described with unusual clarity. 
 
 
96
Proceedings of the Workshop on BioNLP: Shared Task, pages 111?114,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Supervised Classification for Extracting Biomedical Events
Arzucan O?zgu?r
Department of EECS
University of Michigan
Ann Arbor, MI 48109, USA
ozgur@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI 48109, USA
radev@umich.edu
Abstract
We introduce a supervised approach for ex-
tracting bio-molecular events by using linguis-
tic features that represent the contexts of the
candidate event triggers and participants. We
use Support Vector Machines as our learning
algorithm and train separate models for event
types that are described with a single theme
participant, multiple theme participants, or a
theme and a cause participant. We perform ex-
periments with linear kernel and edit-distance
based kernel and report our results on the
BioNLP?09 Shared Task test data set.
1 Introduction
Most previous work on biomedical information ex-
traction focuses on identifying relationships among
biomedical entities (e.g. protein-protein interac-
tions). Unlike relationships, which are in general
characterized with a pair of entities, events can be
characterized with event types and multiple entities
in varying roles. The BioNLP?09 Shared Task ad-
dresses the extraction of bio-molecular events from
the biomedical literature (Kim et al, 2009). We par-
ticipated in the ?Event Detection and Characteriza-
tion? task (Task 1). The goal was to recognize the
events concerning the given proteins by detecting
the event triggers, determining the event types, and
identifying the event participants.
In this study, we approach the problem as a su-
pervised classification task. We group the event
types into three general classes based on the num-
ber and types of participants that they involve. The
first class includes the event types that are described
with a single theme participant. The second class in-
cludes the event types that are described with one or
more theme participants. The third class includes
the events that are described with a theme and/or
a cause participant. We learn support vector ma-
chine (SVM) models for each class of events to clas-
sify each candidate event trigger/participant pair as
a real trigger/participant pair or not. We use vari-
ous types of linguistic features such as lexical, posi-
tional, and dependency relation features that repre-
sent the contexts of the candidate trigger/participant
pairs. The results that we submitted to the shared
task were based on using a linear kernel function. In
this paper, we also report our results based on using
an edit-distance based kernel defined on the shortest
dependency relation type paths between a candidate
trigger/participant pair.
2 System Description
2.1 Event Type Classes
We grouped the nine event types targeted at the
BioNLP?09 Shared Task into three general event
classes based on the number and types of partici-
pants that they involve.
Class 1 Events: Events that involve a single theme participant
(Gene expression, Transcription, Protein catabolism, Lo-
calization, and Phosphorylation event types).
Class 2 Events: Events that can involve one or more theme
participants (Binding event type).
Class 3 Events: Events that can be described with a theme
and/or a cause participant (Regulation, Positive regula-
tion, and Negative regulation event types). Unlike Class 1
111
and Class 2 events, where the participants are proteins, the
participants of Class 3 events can be proteins or events.
Since the event types in each class are similar to
each other based on the number and roles of par-
ticipants that they involve and different from the
event types in the other classes, we learned sepa-
rate classification models for each class. We for-
mulated the classification task as the classification
of trigger/participant pairs. We extracted positive
and negative training instances (trigger/participant
pairs) from the training data for each class of events.
We considered only the pairs that appear in the
same sentence. We used the tokenized and sentence
split abstracts provided by the shared task organiz-
ers1. Consider the sentence ?The phosphorylation of
TRAF2 inhibits binding to the CD40 cytoplasmic do-
main?. This sentence describes the following three
events:
1. Event1: Type: Phosphorylation Trigger: phosphorylation
Theme: TRAF2
2. Event2: Type: Binding Trigger: binding Theme1:
TRAF2 Theme2: CD40
3. Event3: Type: Negative regulation Trigger: inhibits
Theme: Event2 Cause: Event1
Event1 belongs to Class 1. The trigger/participant
pair (phosphorylation, TRAF2) is a positive instance
for Class 1. Event2 belongs to Class 2. It has
two theme participants. The instances for Class 2
events are created by decomposing the events into
trigger/theme pairs. The two positive instances ex-
tracted from the decomposition of Event2 are (bind-
ing, TRAF2) and (binding, CD40). Event3 belongs
to Class 3. It consists of two semantically differ-
ent participants, namely a theme and a cause. We
trained two separate models for Class 3 events, i.e.,
one model to classify the themes and another model
to classify the causes. Another distinguishing char-
acteristic of Class 3 events is that a participant of
an event can be a protein or an event. We repre-
sent the participants that are events with their cor-
responding event triggers. We decompose Event3
into its theme and cause and represent its cause
Event1 with its trigger word ?phosphorylation? and
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA
/SharedTask/tools.html
its theme Event2 with its trigger word ?binding?. As
a result, (inhibits, binding) and (inhibits, phosphory-
lation) are included as positive instances to the Class
3 theme and Class 3 cause training sets, respectively.
Negative instances for Class 1 and Class 2 are cre-
ated by including all the trigger/protein pairs which
are not among the positive instances of that class.
Negative instances for Class 3 theme and Class 3
cause are created by including all the trigger/protein
and trigger1/trigger2 pairs which are not among the
positive instances of that class. For example, (phos-
phorylation, CD40) is a negative instance for Class
1 and (inhibits, TRAF2) is a negative instance for
Class 3 theme and Class 3 cause.
2.2 Feature Extraction
2.2.1 Lexical and Part-of-Speech Features
We used the candidate trigger and its part-of-
speech, which was obtained by using the Stanford
Parser, as features, based on our observation that dif-
ferent candidate triggers might have different likeli-
hoods of being a real trigger for a certain event. For
example, ?transcription? is a trigger for the Tran-
scription event 277 times in the training set and has
not been used as a trigger for other types of events.
On the other hand, ?concentration? is used only
once as a trigger for a Transcription event and three
times as a trigger for Regulation events.
2.2.2 Positional Features
We used two features to represent the relative po-
sition of the participant with regard to the trigger
in the sentence. The first feature has two values,
namely ?before? (the participant appears before the
trigger) or ?after? (the participant appears after the
trigger). The second feature encodes the distance
between the trigger and the participant. Distance is
measured as the number of tokens between the trig-
ger and the participant. Our intuition is that, if a
candidate trigger and participant are far away from
each other, it is less likely that they characterize an
event.
2.2.3 Dependency Relation Features
A dependency parse tree captures the semantic
predicate-argument dependencies among the words
of a sentence. Dependency tree paths between pro-
tein pairs have successfully been used to identify
112
protein interactions (Bunescu and Mooney, 2007;
Erkan et al, 2007). In this paper, we use the
dependency paths to extract events. For a given
trigger/participant pair, we extract the shortest path
from the trigger to the participant, from the depen-
dency parse of the sentence. We use the McClosky-
Charniak parses which are converted to the Stan-
ford Typed Dependencies format and provided to the
participants by the shared task organizers. Previous
approaches use both the words and the dependency
relation types to represent the paths (Bunescu and
Mooney, 2007; Erkan et al, 2007). Consider the de-
pendency tree in Figure 1. The path from ?phospho-
rylation? to ?CD40? is ?nsubj inhibits acomp bind-
ing prep to domain num?. Due to the large num-
ber of possible words, using the words on the paths
might lead to data sparsity problems and to poor
generalization. Suppose we have a sentence with
similar semantics, where the synonym word ?pre-
vents? is used instead of ?inhibits?. If we use the
words on the path to represent the path feature, we
end up with two different paths for the two sen-
tences that have similar semantics. Therefore, in
this study we use only the dependency relation types
among the words to represent the paths. For ex-
ample, the path feature extracted for the (phospho-
rylation, CD40) negative trigger/participant pair is
?nsubj acomp prep to num? and the path feature ex-
tracted for the (phosphorylation, TRAF2) positive
trigger/participant pair is ?prep of?.
inhibits
phosphorylation binding
TRAF2 domain
cytoplasmic CD40 the
acomp
prep_of prep_to
amod detnum
nsubj
Figure 1: The dependency tree of the sentence ?The phos-
phorylation of TRAF2 inhibits binding to the CD40 cyto-
plasmic domain.?
2.3 Classification
We used the SVM light library (Joachims, 1999)
with two different kernel functions and feature sets
for learning the classification models. Our first ap-
proach is based on using linear SVM with the fea-
tures described in Section 2.2. In this approach
the path feature is used as a nominal feature. Our
second approach is based on integrating to SVM a
kernel function based on the word-based edit dis-
tance between the dependency relation paths, where
each dependency relation type on the path is treated
as a word. For example, the word-based edit dis-
tance between the paths ?prep of? and ?prep of
prep with? is 1, since 1 insertion operation (i.e., in-
serting ?prep with? to the first path) is sufficient to
transform the first path to the second one. The edit-
distance based similarity between two paths pi and
pj and the corresponding kernel function are defined
as follows (Erkan et al, 2007).
edit sim(pi, pj) = e??(edit distance(pi,pj)) (1)
3 Experimental Results
The data provided for the shared task is prepared
from the GENIA corpus (Kim et al, 2008). We used
the training and the development sets for training.
The candidate triggers are detected by using a dic-
tionary based approach, where the dictionary is ex-
tracted from the training set. We filtered out the
noisy trigger candidates such as ?with?, ?+?, ?:?, and
?-?, which are rarely used as real triggers and com-
monly used in other contexts. The candidate trig-
ger/participant pairs are classified by using the clas-
sifiers learned for Class 1, Class 2, and/or Class 3
depending on whether the candidate trigger matched
one of the triggers in these classes. The SVM score
is used to disambiguate the event types, if a candi-
date trigger matches a trigger in more than one of the
event classes. A trigger which is ambiguous among
the event types in the same class is assigned to the
event type for which it is most frequently used as a
trigger.
The results that we submitted to the shared task
were obtained by using the linear SVM approach
with the set of features described in Section 2.2.
After submitting the results, we noticed that we
made an error in pre-processing the data set. While
aligning the provided dependency parses with the
113
sentence, we incorrectly assumed that all the sen-
tences had dependency parses and ended up using
the wrong dependency parses for most of the sen-
tences. The overall performance scores for our of-
ficial submission are 30.42% recall, 14.11% preci-
sion, and 19.28% F-measure. The results obtained
after correcting the error are reported in Table 1.
Correcting the error significantly improved the per-
formance of the system. Table 2 shows the re-
sults obtained by using SVM with dependency path
edit kernel. The two SVM models achieve similar
performances. The performance for the regulation
events is considerably lower, since errors in identi-
fying the events are carried to identifying the event
participants of a regulation event. The performances
for the events which have multiple participants, i.e.,
binding and regulation events, are lower compared
to the events with a single participant. The perfor-
mance is higher when computed by decomposing
the events (49.00 and 31.82 F-measure for binding
and regulation events, respectively). This suggests
that even when participants of events are identified
correctly, there is significant amount of error in com-
posing the events.
Event Type Recall Precision F-measure
Localization 41.95 60.83 49.66
Binding 31.41 34.94 33.08
Gene expression 61.36 69.00 64.96
Transcription 37.23 30.72 33.66
Protein catabolism 64.29 64.29 64.29
Phosphorylation 68.15 80.70 73.90
Event Total 50.82 56.80 53.64
Regulation 15.12 19.82 17.15
Positive regulation 24.21 33.33 28.05
Negative regulation 21.64 32.93 26.11
Regulation Total 22.02 30.72 25.65
All Total 35.86 44.69 39.79
Table 1: Approximate span & recursive matching results
using linear SVM with the set of features described in
Section 2.2 (after correcting the error in pre-processing
the data set).
4 Conclusion
We described a supervised approach to extract bio-
molecular events. We grouped the event types into
three general classes based on the number and types
of participants that they can involve and learned sep-
arate SVM models for each class. We used various
Event Type Recall Precision F-measure
Localization 49.43 64.18 55.84
Binding 31.70 35.03 33.28
Gene expression 66.34 69.72 67.99
Transcription 39.42 25.59 31.03
Protein catabolism 78.57 73.33 75.86
Phosphorylation 76.30 80.47 78.33
Event Total 55.13 56.62 55.86
Regulation 17.87 16.46 17.13
Positive regulation 26.45 26.03 26.24
Negative regulation 25.33 32.54 28.49
Regulation Total 24.68 25.34 25.01
All Total 39.31 40.37 39.83
Table 2: Approximate span & recursive matching results
using SVM with dependency relation path edit kernel.
types of linguistic features that represent the context
of the candidate event trigger/participant pairs. We
achieved an F-measure of 39.83% on the shared task
test data. Error analysis suggests that improving the
approach of event composition for types of events
with multiple participants and improving the strat-
egy for detecting and disambiguating triggers can
enhance the performance of the system.
Acknowledgments
This work was supported in part by the NIH Grant
U54 DA021519.
References
R. C. Bunescu and R. J. Mooney, 2007. Text Mining and
Natural Language Processing, Chapter Extracting Re-
lations from Text: From Word Sequences to Depen-
dency Paths, pages 29?44, Springer.
Gu?nes? Erkan, Arzucan O?zgu?r, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting
protein interaction sentences using dependency pars-
ing. In Proceedings of EMNLP, pages 228?237.
T. Joachims, 1999. Advances in Kernel Methods-Support
Vector Learning, Chapter Making Large-Scale SVM
Learning Practical. MIT-Press.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
114
Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, ACL-IJCNLP 2009, pages 54?61,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
The ACL Anthology Network Corpus 
 
Dragomir R. Radev1,2, Pradeep Muthukrishnan1, Vahed Qazvinian1 
1Department of Electrical Engineering and Computer Science 
2School of Information 
University of Michigan 
{radev,mpradeep,vahed}@umich.edu 
 
Abstract 
We introduce the ACL Anthology Net-
work (AAN), a manually curated net-
worked database of citations, 
collaborations, and summaries in the field 
of Computational Linguistics. We also 
present a number of statistics about the 
network including the most cited authors, 
the most central collaborators, as well as 
network statistics about the paper citation, 
author citation, and author collaboration 
networks. 
1 Introduction 
The ACL Anthology is one of the most 
successful initiatives of the ACL. It was in-
itiated by Steven Bird and is now maintained 
by Min Yen Kan. It includes all papers pub-
lished by ACL and related organizations as 
well as the Computational Linguistics journal 
over a period of four decades. It is available 
at http://www.aclweb.org/anthology-new/ .  
One fundamental problem with the ACL 
Anthology, however, is the fact that it is just 
a collection of papers. It doesn?t include any 
citation information or any statistics about the 
productivity of the various researchers who 
contributed papers to it. We embarked on an 
ambitious initiative to manually annotate the 
entire Anthology in order to make it possible 
to compute such statistics.  
In addition, we were able to use the anno-
tated data for extracting citation summaries of 
all papers in the collection and we also anno-
tated each paper by the gender of the authors 
(and are currently in the process of doing si-
milarly for their institutions) in the goal of 
creating multiple gold standard data sets for 
training automated systems for performing 
such tasks.  
2 Curation 
The ACL Anthology includes 13,739 pa-
pers (excluding book reviews and posters). 
Each of the papers was converted from pdf to 
text using an OCR tool (www.pdfbox.org). 
After this conversion, we extracted the refer-
ences semi-automatically using string match-
ing. The above process outputs all the 
references as a single block so we then ma-
nually inserted line breaks between refer-
ences. These references were then manually 
matched to other papers in the ACL Antholo-
gy using a ?k-best? (with k = 5) string match-
ing algorithm built into a CGI interface. A 
snapshot of this interface is shown in Figure 
1. The matched references were stored to-
gether to produce the citation network. Refer-
ences to publications outside of the AAN 
were recorded but not included in the net-
work. 
 In order to fix the issue of wrong author 
names and multiple author identities we had 
to perform a lot of manual post-processing. 
The first names and the last names were 
swapped for a lot of authors. For example, the 
author name "Caroline Brun" was present as 
"Brun Caroline" in some of her papers. 
Another big source of error was the exclusion 
of middle names or initials in a number of 
papers. For example, Julia Hirschberg had 
two identities as "Julia Hirschberg" and "Julia 
B. Hirschberg". There were a few spelling 
mistakes, like "Madeleine Bates" was miss-
pelled as "Medeleine Bates". 
Finally, many papers included incorrect 
titles in their citation sections. Some used the 
wrong years and/or venues as well. 
 
54
 Figure 1: CGI interface used for matching new references to existing papers 
 
 
Figure 2: Snapshot of the different statistics computed for an author 
55
                  Figure 3: Snapshot of the different statistics for a paper 
 
3 Statistics 
 
Using the metadata and the citations ex-
tracted after curation, we have built three dif-
ferent networks.   
The paper citation network is a directed 
network with each node representing a paper 
labeled with an ACL ID number and the 
edges representing a citation within that paper 
to another paper represented by an ACL ID. 
The paper citation network consists of 13,739 
papers and 54,538 citations.   
The author citation network and the author 
collaboration network are additional networks 
derived from the paper citation network. In 
both of these networks a node is created for 
each unique author. In the author citation 
network an edge is an occurrence of an author 
citing another author. For example, if a paper 
written by Franz Josef Och cites a paper writ-
ten by Joshua Goodman, then an edge is 
created between Franz Josef Och and Joshua 
Goodman. Self citations cause self loops in 
the author citation network. The author cita-
tion network consists of 11,180 unique au-
thors and 332,815 edges (196,905 edges if 
duplicates are removed). 
In the author collaboration network, an 
edge is created for each collaboration. For 
example, if a paper is written by Franz Josef 
Och and Hermann Ney, then an edge is 
created between the two authors.  
Table 1 shows some brief statistics about 
the first two releases of the data set (2006 and 
2007). Table 2 describes the most current re-
lease of the data set (from 2008).  
 
2006 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 8898 7849 7849 
m 8765 137,007 41,362 
2007 
 Paper 
citation 
network 
Author 
citation 
network 
Author 
collaboration 
network 
n 9767 9421 9421 
m 44,142 158,479 45,878 
       Table 1: Growth of citation volume 
 
 
 
 
Paper 
Citation 
Network  
Author 
Citation 
Network  
Author 
Collaboration 
Network 
Nodes  13,739  10,409 10,409 
Edges  54,538  195,505 57,614 
Diameter  22  10  20 
Average 9.34  43.11  11.07 
56
Degree  
Largest 
Connected 
Component  
11,409  9061  7910 
Watts Stro-
gatz cluster-
ing 
coefficient 
0.18 0.46 0.65 
Newman 
clustering 
coefficient 
0.07 0.14 0.36 
clairlib avg. 
directed 
shortest 
path  
5.91 3.32 5.87 
Ferrer avg. 
directed 
shortest 
path  
5.35 3.29 4.66 
harmonic 
mean geo-
desic dis-
tance  
63.93 5.47 9.40 
harmonic 
mean geo-
desic dis-
tance with 
self-loops 
counted 
63.94 5.47 9.40 
     Table 2: Network Statistics of the cita-
tion and collaboration network. The re-
maining authors (11,180-10,409) are not 
cited and are therefore removed from the 
network analysis
 
 Paper 
Citation 
Network 
Author 
Citation 
Network 
Author 
Collaboratio
n Network 
In-degree Stats 
Power Law 
Exponent 
2.50 2.20 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.00 1.55 2.18 
Out-degree stats 
Power Law 
Exponent 
3.70 2.56 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
2.12 1.54 2.18 
Total Degree Stats 
Power Law 
Exponent 
2.72 2.27 3.17 
Power Law 
Relationship? 
No No No 
Newman 
Power Law 
exponent 
1.81 1.46 2.18 
Table 3: Degree Statistics of the citation 
and collaboration networks 
 
A lot of different statistics have been 
computed based on the data set release in 
2007 by Radev et al The statistics include 
PageRank scores which eliminate PageRank's 
inherent bias towards older papers, Impact 
factor, correlations between different meas-
ures of impact like H-Index, total number of 
incoming citations, PageRank. They also re-
port results from a regression analysis using 
H-Index scores from different sources (AAN, 
Google Scholar) in an attempt to identify 
multi-disciplinary authors.  
4 Sample rankings 
This section shows some of the rankings 
that were computed using AAN. 
57
 Rank Icit Title 
1 590 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
2 444 The Mathematics Of Statistical 
Machine Translation: Parameter 
Estimation 
3 324 Attention Intentions And The 
Structure Of Discourse 
4 271 A Maximum Entropy Approach 
To Natural Language Processing 
5 270  Bleu: A Method For 
Automatic Evaluation Of 
6 246  A Maximum-Entropy-Inspired 
Parser 
7 230 A Stochastic Parts Program And 
Noun Phrase Parser For 
Unrestricted Text 
8 221 A Systematic Comparison Of 
Various  Statistical Alignment 
9 211 A Maximum Entropy Model For 
Part-Of-Speech Tagging 
10 211 Three Generative Lexicalized 
Models For Statistical Parsing 
Table 4: Papers with the most incoming 
citations (icit) 
Rank PR Title 
1 1099.1 A Stochastic Parts Program 
And Noun Phrase Parser For 
Unrestricted Text 
2 943.8 Finding Clauses In Unrestricted 
Text By Finitary And 
Stochastic Methods 
3 568.8 A Stochastic Approach To 
Parsing 4 543.1 A Statistical Approach To 
Machine Translation 
5 414.1 Building A Large Annotated 
Corpus Of English: The Penn 
Treebank 
6 364.9 The Mathematics Of Statistical 
Machine Translation: Parameter  
Estimation 
7 362.2 The Contribution Of Parsing To 
Prosodic Phrasing In An 
Experimental  
Text-To-Speech System 
8 301.6 Attention Intentions And The 
Structure Of Discourse 
9 250.5 Bleu: A Method For Automatic 
Evaluation Of Machine 
Translation 
10 242.5 A Maximum Entropy Approach 
To Natural Language 
Processing 
 Table 5: Papers with highest PageRank 
(PR) scores 
It must be noted that the PageRank scores 
are not accurate because of the lack of cita-
tions outside AAN. Specifically, out of the 
155,858 total number of citations, only 
54,538 are within AAN. 
 
  Rank Icit Author Name 
1 (1) 3886 (3815) Och, Franz Josef 
2 (2) 3297 (3119) Ney, Hermann 
3 (3) 3067 (3049) Della Pietra, Vincent J. 
4 (5) 2746 (2720) Mercer, Robert L. 
5 (4) 2741 (2724) Della Pietra, Stephen 
A. 6 (6) 2605 (2589) Marcus, Mitchell P. 
7 (8) 2454 (2407) Collins, Michael John 
8 (7) 2451 (2433) Brown, Peter F. 
9 (9) 2428 (2390)  Church, Kenneth Ward 
10 (10) 2047 (1991) Marcu, Daniel 
      Table 6: Authors with most incoming 
citations (the values in parentheses are us-
ing non-self- citations) 
 
Rank h Author Name 
1 18 Knight, Kevin 
2 16 Church, Kenneth Ward 
3 15 Manning, Christopher D. 
3 15 Grishman, Ralph 
3 15 Pereira, Fernando C. N. 
6 14 Marcu, Daniel 
6 14 Och, Franz Josef 
6 14 Ney, Hermann 
6 14 Joshi, Aravind K. 
6 14 Collins, Michael John 
      Table 7: Authors with the highest h-
index 
 
Rank ASP Author Name 
1  2.977 Hovy, Eduard H. 
2  2.989 Palmer, Martha Stone 
3  3.011 Rambow, Owen 
4  3.033 Marcus, Mitchell P. 
5  3.041 Levin, Lori S. 
6  3.052 Isahara, Hitoshi 
7  3.055 Flickinger, Daniel P. 
8  3.071 Klavans, Judith L. 
9  3.073 Radev, Dragomir R. 
10 3.077 Grishman, Ralph 
Table 8: Authors with the least average  
shortest path (ASP) length in the author 
collaboration network 
 
 
58
5 Related phrases 
We have also computed the related phras-
es for every author using the text from the 
papers they have authored, using the simple 
TF-IDF scoring scheme (see Figure 4).  
 
 
Figure 4: Snapshot of the related phrases 
for Franz Josef Och 
6 Citation summaries 
The citation summary of an article, P, is 
the set of sentences that appear in the litera-
ture and cite P. These sentences usually men-
tion at least one of the cited paper?s contribu-
tions. We use AAN to extract the citation 
summaries of all articles, and thus the citation 
summary of P is a self-contained set and only 
includes the citing sentences that appear in 
AAN papers. Extraction is performed auto-
matically using string-based heuristics by 
matching the citation pattern, author names 
and publication year, within the sentences. 
The following example shows the citation 
summary extracted for ?Koo, Terry, Carreras, 
Xavier, Collins, Michael John, Simple Semi-
supervised Dependency Parsing". The cita-
tion summary of (Koo et al, 2008) mentions 
KCC08, dependency parsing, and the use of 
word clustering in semi-supervised NLP. 
 
 
 
 
 
 
 
 
 
 
 
 
C08-1051 1 7:191 Furthermore, recent studies revealed that word clustering is useful for semi-supervised learn-
ing in NLP (Miller et al, 2004; Li and McCallum, 2005; Kazama and Torisawa, 2008; Koo et al, 2008). 
 
D08-1042 2 78:214 There has been a lot of progress in learning dependency tree parsers (McDonald et al, 2005; 
Koo et al, 2008; Wang et al, 2008). 
 
W08-2102 3 194:209 The method shows improvements over the method described in (Koo et al, 2008), which 
is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting 
that the incorporation of constituent structure can improve dependency accuracy. 
 
W08-2102 4 32:209 The model also recovers dependencies with significantly higher accuracy than state-of-the-
art dependency parsers such as (Koo et al, 2008; McDonald and Pereira, 2006). 
 
W08-2102 5 163:209 KCC08 unlabeled is from (Koo et al, 2008), a model that has previously been shown to 
have higher accuracy than (McDonald and Pereira, 2006). 
 
W08-2102 6 164:209 KCC08 labeled is the labeled dependency parser from (Koo et al, 2008); here we only 
evaluate the unlabeled accuracy. 
 
Figure 5: Sample citation summary 
 
59
 Figure 6: Snapshot of the citation summary for a paper 
 
 
The citation text that we have extracted for 
each paper is a good resource to generate 
summaries of the contributions of that paper. 
We have previously developed systems using 
clustering the similarity networks to generate 
short, and yet informative, summaries of in-
dividual papers (Qazvinian and Radev 2008), 
and more general scientific topics, such as 
Dependency Parsing, and Machine Transla-
tion (Radev et al 2009) . 
 
 
7 Gender annotation 
We have manually annotated the gender of 
most authors in AAN using the name of the 
author. If the gender cannot be identified 
without any ambiguity using the name of the 
author, we resorted to finding the homepage 
of the author. We have been able to annotate 
8,578 authors this way: 6,396 male and 2,182 
female.  
 
8 Downloads 
The following files can be downloaded: 
Text files of the paper: The raw text files 
of the papers after converting them from pdf 
to text is available for all papers. The files are 
named by the corresponding ACL ID. 
Metadata: This file contains all the meta-
data associated with each paper. The metada-
ta associated with every paper consists of the 
paper id, title, year, venue. 
Citations: The paper citation network indi-
cating which paper cites which other paper. 
Figure 7 includes some examples.  
 
 
id = {C98-1096} 
author = {Jing, Hongyan; McKeown, Kathleen R.} 
title = {Combining Multiple, Large-Scale Resources in a Reusable Lexicon for Natural Language Genera-
tion} 
venue = {International Conference On Computational Linguistics} 
year = {1998} 
 
id = {J82-3004} 
author = {Church, Kenneth Ward; Patil, Ramesh} 
title = {Coping With Syntactic Ambiguity Or How To Put The Block In  The Box On The Table} 
venue = {American Journal Of Computational Linguistics} 
year = {1982} 
60
 A00-1001 ==> J82-3002 
A00-1002 ==> C90-3057 
C08-1001 ==> N06-1007 
     C08-1001 ==> N06-1008 
 
Figure 7: Sample contents of the downloadable corpus 
 
We also include a large set of scripts 
which use the paper citation network and the 
metadata file to output the auxiliary networks 
and the different statistics. 
The scripts are documented here:  
http://clair.si.umich.edu/ .The data set has 
already been downloaded from 2,775 unique 
IPs since June 2007. Also, the website has 
been very popular based on access statistics. 
There have been more than 2M accesses in 
2009.  
References  
Vahed Qazvinian and Dragomir R. Radev. Scien-
tific paper summarization using citation sum-
mary networks. In COLING 2008, Manchester, 
UK, 2008. 
Dragomir R. Radev, Mark Joseph, Bryan Gibson, 
and Pradeep Muthukrishnan. A Bibliometric 
and Network Analysis of the Field of Computa-
tional Linguistics. JASIST, 2009 to appear. 
61
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895?903,
Beijing, August 2010
Citation Summarization Through Keyphrase Extraction
Vahed Qazvinian
Department of EECS
University of Michigan
vahed@umich.edu
Dragomir R. Radev
School of Information and
Department of EECS
University of Michigan
radev@umich.edu
Arzucan ?Ozgu?r
Department of EECS
University of Michigan
ozgur@umich.edu
Abstract
This paper presents an approach to sum-
marize single scientific papers, by extract-
ing its contributions from the set of cita-
tion sentences written in other papers. Our
methodology is based on extracting sig-
nificant keyphrases from the set of cita-
tion sentences and using these keyphrases
to build the summary. Comparisons show
how this methodology excels at the task
of single paper summarization, and how it
out-performs other multi-document sum-
marization methods.
1 Introduction
In recent years statistical physicists and computer
scientists have shown great interest in analyzing
complex adaptive systems. The study of such sys-
tems can provide valuable insight on the behav-
ioral aspects of the involved agents with potential
applications in economics and science. One such
aspect is to understand what motivates people to
provide the n+1st review of an artifact given that
they are unlikely to add something significant that
has not already been said or emphasized. Cita-
tions are part of such complex systems where ar-
ticles use citations as a way to mention different
contributions of other papers, resulting in a col-
lective system.
The focus of this work is on the corpora cre-
ated based on citation sentences. A citation sen-
tence is a sentence in an article containing a ci-
tation and can contain zero or more nuggets (i.e.,
non-overlapping contributions) about the cited ar-
ticle. For example the following sentences are a
few citation sentences that appeared in the NLP
literature in past that talk about Resnik?s work.
The STRAND system (Resnik, 1999), for example, uses
structural markup information from the pages, without
looking at their content, to attempt to align them.
Resnik (1999) addressed the issue of
language identification for finding Web pages in
the languages of interest.
Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufficient quantities of high quality
data..
The set of citations is important to analyze be-
cause human summarizers have put their effort
collectively but independently to read the target
article and cite its important contributions. This
has been shown in other work too (Elkiss et al,
2008; Nanba et al, 2004; Qazvinian and Radev,
2008; Mei and Zhai, 2008; Mohammad et al,
2009). In this work, we introduce a technique
to summarize the set of citation sentences and
cover the major contributions of the target paper.
Our methodology first finds the set of keyphrases
that represent important information units (i.e.,
nuggets), and then finds the best set of k sentences
to cover more, and more important nuggets.
Our results confirm the effectiveness of the
method and show that it outperforms other state
of the art summarization techniques. Moreover,
as shown in the paper, this method does not need
to calculate the full cosine similarity matrix for a
document cluster, which is the most time consum-
ing part of the mentioned baseline methods.
1.1 Related Work
Previous work has used citations to produce sum-
maries of scientific work (Qazvinian and Radev,
895
2008; Mei and Zhai, 2008; Elkiss et al, 2008).
Other work (Bradshaw, 2003; Bradshaw, 2002)
benefits from citations to determine the content of
articles and introduce ?Reference Directed Index-
ing? to improve the results of a search engine.
In other work, (Nanba and Okumura, 1999) an-
alyze citation sentences and automatically cate-
gorize citations into three groups using 160 pre-
defined phrase-based rules to support a system for
writing a survey. Previous research has shown
the importance of the citation summaries in un-
derstanding what a paper contributes. In partic-
ular, (Elkiss et al, 2008) performed a large-scale
study on citation summaries and their importance.
Results from this experiment confirmed that the
?Self Cohesion? (Elkiss et al, 2008) of a citation
summary of an article is consistently higher than
the that of its abstract and that citations contain
additional information that does not appear in ab-
stracts.
Kan et al (2002) use annotated bibliographies
to cover certain aspects of summarization and sug-
gest using metadata and critical document features
as well as the prominent content-based features to
summarize documents. Kupiec et al (1995) use
a statistical method and show how extracts can
be used to create summaries but use no annotated
metadata in summarization.
Siddharthan and Teufel describe a new task to
decide the scientific attribution of an article (Sid-
dharthan and Teufel, 2007) and show high hu-
man agreement as well as an improvement in the
performance of Argumentative Zoning (Teufel,
2005). Argumentative Zoning is a rhetorical clas-
sification task, in which sentences are labeled as
one of Own, Other, Background, Textual, Aim,
Basis, Contrast according to their role in the au-
thor?s argument. These all show the importance
of citation summaries and the vast area for new
work to analyze them to produce a summary for a
given topic.
The Maximal Marginal Relevance (MMR)
summarization method, which is based on a
greedy algorithm, is described in (Carbonell and
Goldstein, 1998). MMR uses the full similarity
matrix to choose the sentences that are the least
similar to the sentences already selected for the
summary. We selected this method as one of our
Fact Occurrences
f1: ? Supervised Learning? 5
f2: ? instance/concept relations? 3
f3: ?Part-of-Speech tagging? 3
f4: ?filtering QA results? 2
f5: ?lexico-semantic information? 2
f6: ?hyponym relations? 2
Table 2: Nuggets of P03-1001 extracted by anno-
tators.
baseline methods, which we have explained in
more details in Section 4.
2 Data
In order to evaluate our method, we use the ACL
Anthology Network (AAN), which is a collec-
tion of papers from the Computational Linguistics
journal and proceedings from ACL conferences
and workshops and includes more than 13, 000 pa-
pers (Radev et al, 2009). We use 25 manually an-
notated papers from (Qazvinian and Radev, 2008),
which are highly cited articles in AAN. Table 1
shows the ACL ID, title, and the number of cita-
tion sentences for these papers.
The annotation guidelines asked a number of
annotators to read the citation summary of each
paper and extract a list of the main contribu-
tions of that paper. Each item on the list is a
non-overlapping contribution (nugget) perceived
by reading the citation summary. The annota-
tion strictly instructed the annotators to focus on
the citing sentences to do the task and not their
own background on the topic. Then, extracted
nuggets are reviewed and those nuggets that have
only been mentioned by 1 annotator are removed.
Finally, the union of the rest is used as a set of
nuggets representing each paper.
Table 2 lists the nuggets extracted by annotators
for P03-1001.
3 Methodology
Our methodology assumes that each citation sen-
tence covers 0 or more nuggets about the cited
papers, and tries to pick sentences that maximize
nugget coverage with respect to summary length.
These nuggets are essentially represented using
keyphrases. Therefore, we try to extract signifi-
cant keyphrases in order to represent nuggets each
sentence contains. Here, the keyphrases are ex-
896
ACL-ID Title # citations
N03-1017 Statistical Phrase-Based Translation 180
P02-1006 Learning Surface Text Patterns For A Question Answering System 74
P05-1012 On-line Large-Margin Training Of Dependency Parsers 71
C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 66
P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 65
P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 55
P99-1065 A Statistical Parser For Czech 54
J04-4002 The Alignment Template Approach To Statistical Machine Translation 50
D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 42
P05-1013 Pseudo-Projective Dependency Parsing 40
W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 31
P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 27
N04-1033 Improvements In Phrase-Based Statistical Machine Translation 24
A00-2024 Cut And Paste Based Text Summarization 20
W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 19
A00-1043 Sentence Reduction For Automatic Text Summarization 19
C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 19
W05-1203 Measuring The Semantic Similarity Of Texts 17
W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 15
W03-0301 An Evaluation Exercise For Word Alignment 14
A00-1023 A Question Answering System Supported By Information Extraction 13
D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 12
P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 10
H05-1047 A Semantic Approach To Recognizing Textual Entailment 8
H05-1079 Recognising Textual Entailment With Logical Inference 9
Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citing
each.
unique all max freq
unigrams 229,631 7,746,792 437,308
bigrams 2,256,385 7,746,791 73,957
3-grams 5,125,249 7,746,790 3,600
4-grams 6,713,568 7,746,789 2,408
Table 3: Statistics on the abstract corpus in AAN
used as the background data
pressed using N -grams, and thus these building
units are the key elements to our summarization.
For each citation sentence di, our method first ex-
tracts a set of important keyphrases, Di, and then
tries to find sentences that have a larger number of
important and non-redundant keyphrases. In order
to take the first step, we extract statistically sig-
nificantly frequent N -grams (up to N = 4) from
each citing sentence and use them as the set of
representative keyphrases for that citing sentence.
3.1 Automatic Keyphrase Extraction
A list of keyphrases for each citation sentence can
be generated by extracting N -grams that occur
significantly frequently in that sentence compared
to a large corpus of such N -grams. Our method
for such an extraction is inspired by the previ-
ous work by Tomokiyo and Hurst (Tomokiyo and
Hurst, 2003).
A language model, M, is a statistical model
that assigns probabilities to a sequence of N -
grams. Every language model is a probability dis-
tribution over all N -grams and thus the probabili-
ties of all N -grams of the same length sum up to
1. In order to extract keyphrases from a text us-
ing statistical significance we need two language
models. The first model is referred to as the Back-
ground Model (BM) and is built using a large
text corpus. Here we build the BM using the text
of all the paper abstracts provided in AAN1. The
second language model is called the Foreground
Model (FM) and is the model built on the text
from which keyphrases are being extracted. In
this work, the set of all citation sentences that cite
a particular target paper are used to build a fore-
ground language model.
Let gi be an N -gram of size i and CM(gi) de-
note the count of gi in the modelM. First, we ex-
tract the counts of each N -grams in both the back-
ground (BM) and the foreground corpora (FM).
1http://chernobog.si.umich.edu/clair/anthology/index.cgi
897
MBM =
X
gi?{BM?FM}
1
NBM =
X
gi?{BM?FM}
CBM(gi)
NFM =
X
gi?FM
CFM(gi)
p?FM(gi) = CFM(gi)/NFM
p?BM(gi) = (CBM(gi) + 1)/(MBM +NBM)
The last equation is also known as Laplace
smoothing (Manning and Schutze, 2002) and han-
dles the N -grams in the foreground corpus that
have a 0 occurrence frequency in the background
corpus. Next, we extract N -grams from the fore-
ground corpus that have significant frequencies
compared to the frequency of the same N -grams
in the background model and its individual terms
in the foreground model.
To measure how randomly a set of consecu-
tive terms are forming an N -gram, Tomokiyo and
Hurst (Tomokiyo and Hurst, 2003) use pointwise
divergence. In particular, for an N -gram of size i,
gi = (w1w2 ? ? ?wi),
?gi(FMi?FM1) = p?FM(gi) log(
p?FM(gi)
Qi
j=1 p?FM(wj)
)
This equation shows the extent to which the
terms forming gi have occurred together ran-
domly. In other words, it indicates the extent of in-
formation that we lose by assuming independence
of each word by applying the unigram model, in-
stead of the N -gram model.
In addition, to measure how randomly a se-
quence of words appear in the foreground model
with respect to the background model, we use
pointwise divergence as well. Here, pointwise di-
vergence defines how much information we lose
by assuming that gi is drawn from the background
model instead of the foreground model:
?gi(FMi?BMi) = p?FM(gi) log(
p?FM(gi)
p?BM(gi)
)
(Corley and Mihalcea, 2005) applied or uti-
lized lexical based word overlap measures.
{overlap measures, word overlap, lexical
based, utilized lexical}
Table 4: Example: citation sentence for W05-
1203 written by D06-1621, and its extracted bi-
grams.
We set the criteria of choosing a sequence of
words as significant to be whether it has posi-
tive pointwise divergence with respect to both the
background model, and individual terms of the
foreground model. In other words we extract all gi
from FM for which the both properties are posi-
tive:
?gi(FMi?BMi) > 0
?gi(FMi?FM1) ? 0
The equality condition in the second equation
is specifically set to handle unigrams, in which
p?FM(gi) =
?i
j=1 p?FM(wj).
In order to handle the text corpora and build-
ing the language models, we have used the CMU-
Cambridge Language Model toolkit (Clarkson
and Rosenfeld, 1997). We use the set of cita-
tion sentences for each paper to build foreground
language models. Furthermore, we employ this
tool and make the background model using nearly
11,000 abstracts from AAN. Table 3 summarizes
some of the statistics about the background data.
Once keyphrases (significant N -grams) of each
sentence are extracted, we remove all N -grams in
which more than half of the terms are stopwords.
For instance, we remove all stopword unigrams,
if any, and all bigrams with at least one stop-
word in them. For 3-grams and 4-grams we use
a threshold of 2 and 3 stopwords respectively. Af-
ter that, the set of remaining N -grams is used to
represent each sentence and to build summaries.
Table 4 shows an example of a citation sentence
from D06-1621 citing W05-1203 (Corley and Mi-
halcea, 2005), and its extracted bigrams.
3.2 Sentence Selection
After extracting the set of keyphrases for each sen-
tence, di, the sentence is represented using its set
898
of N -grams, denoted by Di. Then, the goal is
to pick sentences (sets) for each paper that cover
more important and non-redundant keyphrases.
Essentially, keyphrases that have been repeated in
more sentences are more important and could rep-
resent more important nuggets. Therefore, sen-
tences that contain more frequent keyphrases are
more important. Based on this intuition we define
the reward of building a summary comprising a
set of keyphrases S as
f(S) = |S ?A|
where A is the set of all keyphrases from sen-
tences not in the summary.
The set function f has three main properties.
First, it is non-negative. Second, it is mono-
tone (i.e., For every set v we have f(S + v) ?
f(S)). Third, f is sub-modular. The submodular-
ity means that for a set v and two sets S ? T we
have
f(S + v)? f(S) ? f(T + v)? f(T )
Intuitively, this property implies that adding a set
v to S will increase the reward at least as much
as it would to a larger set T . In the summariza-
tion setting, this means that adding a sentence to
a smaller summary will increase the reward of the
summary at least as much as adding it to a larger
summary that subsumes it. The following theorem
formalizes this and is followed by a proof.
Theorem 1 The reward function f is submodular.
Proof
We start by defining a gain function G of adding
sentence (set) Di to Sk?1 where Sk?1 is the set
of keyphrases in a summary built using k? 1 sen-
tences, and Di is a candidate sentence to be added:
G(Di,Sk?1) = f(Sk?1 ?Di)? f(Sk?1)
Simple investigation through a Venn diagram
proof shows that G can be re-written as
G(Di,Sk?1) = |Di ? (?j 6=iDj)? Sk?1|
Let?s denote Di? (?j 6=iDj) by ?i. The follow-
ing equations prove the theorem.
Sk?1 ? Sk
S ?k?1 ? S ?k
?i ? S ?k?1 ? ?i ? S ?k
?i ? Sk?1 ? ?i ? Sk
| ?i ?Sk?1| ? | ?i ?Sk|
G(Di,Sk?1) ? G(Di,Sk)
f(Sk?1 ?Di)? f(Sk?1) ? f(Sk ?Di)? f(Sk)
Here, S ?k is the set of all N -grams in the vo-
cabulary that are not present in Sk. The gain of
adding a sentence, Di, to an empty summary is a
non-negative value.
G(Di,S0) = C ? 0
By induction, we will get
G(Di,S0) ? G(Di,S1) ? ? ? ? ? G(Di,Sk) ? 0
2
Theorem 1 implies the general case of submodu-
larity:
?m,n, 0 ? m ? n ? |D| ? G(Di,Sm) ? G(Di,Sn)
Maximizing this submodular function is an NP-
hard problem (Khuller et al, 1999). A common
way to solve this maximization problem is to start
with an empty set, and in each iteration pick a set
that maximizes the gain. It has been shown be-
fore in (Kulik et al, 2009) that if f is a submod-
ular, nondecreasing set function and f(?) = 0,
then such a greedy algorithm finds a set S , whose
gain is at least as high as (1 ? 1/e) of the best
possible solution. Therefore, we can optimize the
keyphrase coverage as described in Algorithm 1.
4 Experimental Setup
We use the annotated data described in Section 2.
In summary, the annotation consisted of two parts:
nugget extraction and nugget distribution analy-
sis. Five annotators were employed to annotate
the sentences in each of the 25 citation summaries
and write down the nuggets (non-overlapping con-
tributions) of the target paper. Then using these
899
Summary generated using bigram-based keyphrases
ID Sentence
P06-1048:1 Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al2003; Nguyen et al2004a; Turner and Charniak 2005;
McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.
J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore
1996; Mahesh 1997; Carroll et al1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu
2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.
A00-2024:9 The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.
N03-1026:17 To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000)
and Jing (2000).
P06-2019:5 Jing (2000) was perhaps the first to tackle the sentence compression problem.
Table 5: Bigram-based summary generated for A00-1043.
Algorithm 1 The greedy algorithm for summary
generation
k ? the number of sentences in the summary
Di ? keyphrases in di
S ? ?
for l = 1 to k do
sl ? argmaxDi?D |Di ? (?j 6=iDj)|
S ? S ? sl
for j = 1 to |D| do
Dj ? Dj ? sl
end for
end for
return S
nugget sets, each sentence was annotated with the
nuggets it contains. This results in a sentence-
fact matrix that helps with the evaluation of the
summary. The summarization goal and the intu-
ition behind the summarizing system is to select a
few (5 in our experiments) sentences and cover as
many nuggets as possible. Each sentence in a cita-
tion summary may contain 0 or more nuggets and
not all nuggets are mentioned an equal number of
times. Covering some nuggets (contributions) is
therefore more important than others and should
be weighted highly.
To capture this property, the pyramid score
seems the best evaluation metric to use. We use
the pyramid evaluation method (Nenkova and Pas-
sonneau, 2004) at the sentence level to evaluate
the summary created for each set. We benefit
from the list of annotated nuggets provided by the
annotators as the ground truth of the summariza-
tion evaluation. These annotations give the list of
nuggets covered by each sentence in each citation
summary, which are equivalent to the summariza-
tion content unit (SCU) as described in (Nenkova
and Passonneau, 2004).
The pyramid score for a summary is calculated
as follows. Assume a pyramid that has n tiers, Ti,
where tier Ti > Tj if i > j (i.e., Ti is not below
Tj , and that if a nugget appears in more sentences,
it falls in a higher tier.). Tier Ti contains nuggets
that appeared in i sentences, and thus has weight
i. Suppose |Ti| shows the number of nuggets in
tier Ti, and Qi is the size of a subset of Ti whose
members appear in the summary. Further suppose
Q shows the sum of the weights of the facts that
are covered by the summary. Q =
?n
i=1 i?Qi.
In addition, the optimal pyramid score for a sum-
mary with X facts, is
Max =
n
X
i=j+1
i? |Ti|+ j ? (X ?
n
X
i=j+1
|Ti|)
where j = maxi(
?n
t=i |Tt| ? X). The pyra-
mid score for a summary is then calculated as fol-
lows.
P = QMax
This score ranges from 0 to 1, and a high
score shows the summary contains more heavily
weighted facts.
4.1 Baselines and Gold Standards
To evaluate the quality of the summaries gen-
erated by the greedy algorithm, we compare its
pyramid score in each of the 25 citation sum-
maries with those of a gold standard, a random
summary, and four other methods. The gold stan-
dards are summaries created manually using 5
sentences. The 5 sentences are manually selected
in a way to cover as many nuggets as possible with
higher priority for the nuggets with higher fre-
quencies. We also created random summaries us-
ing Mead (Radev et al, 2004). These summaries
900
are basically a random selection of 5 sentences
from the pool of sentences in the citation sum-
mary. Generally we expect the summaries cre-
ated by the greedy method to be significantly bet-
ter than random ones.
In addition to the gold and random summaries,
we also used 4 baseline state of the art sum-
marizers: LexRank, the clustering C-RR and
C-LexRank, and Maximal Marginal Relevance
(MMR). LexRank (Erkan and Radev, 2004) works
based on a random walk on the cosine similar-
ity of sentences and prints out the most frequently
visited sentences. Said differently, LexRank first
builds a network in which nodes are sentences and
edges are cosine similarity values. It then uses the
eigenvalue centralities to find the most central sen-
tences. For each set, the top 5 sentences on the list
are chosen for the summary.
The clustering methods, C-RR and C-LexRank,
work by clustering the cosine similarity network
of sentences. In such a network, nodes are sen-
tences and edges are cosine similarity of node
pairs. Clustering would intuitively put nodes with
similar nuggets in the same clusters as they are
more similar to each other. The C-RR method as
described in (Qazvinian and Radev, 2008) uses a
round-robin fashion to pick sentences from each
cluster, assuming that the clustering will put the
sentences with similar facts into the same clus-
ters. Unlike C-RR, C-LexRank uses LexRank to
find the most salient sentences in each cluster, and
prints out the most central nodes of each cluster as
summary sentences.
Finally, MMR uses the full cosine similarity
matrix and greedily chooses sentences that are the
least similar to those already selected for the sum-
mary (Carbonell and Goldstein, 1998). In partic-
ular,
MMR = arg min
di?D?A
[
max
dj?A
Sim(di, dj)
]
where A is the set of sentences in the summary,
initially set to A = ?. This method is different
from ours in that it chooses the least similar sen-
tence to the summary in each iteration.
4.2 Results and Discussion
As mentioned before, we use the text of the ab-
stracts of all the papers in AAN as the back-
ground, and each citation set as a separate fore-
ground corpus. For each citation set, we use the
method described in Section 3.1 to extract signif-
icant N -grams of each sentence. We then use the
keyphrase set representation of each sentence to
build the summaries using Algorithm 1. For each
of the 25 citation summaries, we build 4 differ-
ent summaries using unigrams, bigrams, 3-grams,
and 4-grams respectively. Table 5 shows a 5-
sentence summary created using algorithm 1 for
the paper A00-1043 (Jing, 2000).
The pyramid scores for different methods are
reported in Figure 1 together with the scores
of gold standards, manually created to cover as
many nuggets as possible in 5 sentences, as
well as summary evaluations of the 4 baseline
methods described above. This Figure shows
how the keyphrase based summarization method
when employing N -grams of size 3 or smaller,
outperforms other baseline systems significantly.
More importantly, Figure 1 also indicates that this
method shows more stable results and low varia-
tion in summary quality when keyphrases of size 3
or smaller are employed. In contrast, MMR shows
high variation in summary qualities making sum-
maries that obtain pyramid scores as low as 0.15.
Another important advantage of this method is
that we do not need to calculate the cosine simi-
larity of the pairs of sentences, which would add a
running time of O(|D|2|V |) in the number of doc-
uments, |D|, and the size of the vocabulary |V | to
the algorithm.
5 Conclusion and Future Work
This paper presents a summarization methodol-
ogy that employs keyphrase extraction to find im-
portant contributions of scientific articles. The
summarization is based on citation sentences and
picks sentences to cover nuggets (represented by
keyphrases) or contributions of the target papers.
In this setting the best summary would have as few
sentences and at the same time as many nuggets
as possible. In this work, we use pointwise KL-
divergence to extract statistically significant N -
grams and use them to represent nuggets. We
then apply a new set function for the task of sum-
marizing scientific articles. We have proved that
this function is submodular and concluded that a
901
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Gold Mead LexRank C?RR C?LexRank MMR 1?gram 2?gram 3?gram 4?gram
Py
ra
m
id
 S
co
re
Figure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasets
using different methods.
greedy algorithm will result in a near-optimum set
of covered nuggets using only 5 sentences. Our
experiments in this paper confirm that the sum-
maries created based on the presented algorithm
are better than randomly generated summary, and
also outperform other state of the art summariza-
tion methods in most cases. Moreover, we show
how this method generates more stable summaries
with lower variation in summary quality when N -
grams of size 3 or smaller are employed.
A future direction for this work is to perform
post-processing on the summaries and re-generate
sentences that cover the extracted nuggets. How-
ever, the ultimate goal is to eventually develop
systems that can produce summaries of entire
research areas, summaries that will enable re-
searchers to easily and quickly switch between
fields of research.
One future study that will help us generate
better summaries is to understand how nuggets
are generated by authors. In fact, modeling the
nugget coverage behavior of paper authors will
help us identify more important nuggets and dis-
cover some aspects of the paper that would oth-
erwise be too difficult by just reading the paper
itself.
6 Acknowledgements
This work is in part supported by the National
Science Foundation grant ?iOPENER: A Flexi-
ble Framework to Support Rapid Learning in Un-
familiar Research Domains?, jointly awarded to
University of Michigan and University of Mary-
land as IIS 0705832, and in part by the NIH Grant
U54 DA021519 to the National Center for Inte-
grative Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those
of the authors and do not necessarily reflect the
views of the supporters.
References
Bradshaw, Shannon. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, Shannon. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
902
Conference on Research and Advanced Technology
for Digital Libraries.
Carbonell, Jaime G. and Jade Goldstein. 1998. The
use of MMR, diversity-based reranking for reorder-
ing documents and producing summaries. In SI-
GIR?98, pages 335?336.
Clarkson, PR and R Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Elkiss, Aaron, Siwei Shen, Anthony Fader, Gu?nes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51?62.
Erkan, Gu?nes? and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research.
Jing, Hongyan. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Kan, Min-Yen, Judith L. Klavans, and Kathleen R.
McKeown. 2002. Using the Annotated Bibliogra-
phy as a Resource for Indicative Summarization. In
Proceedings of LREC 2002, Las Palmas, Spain.
Khuller, Samir, Anna Moss, and Joseph (Seffi) Naor.
1999. The budgeted maximum coverage problem.
Inf. Process. Lett., 70(1):39?45.
Kulik, Ariel, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In SODA ?09, pages
545?554.
Kupiec, Julian, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In SIGIR
?95, pages 68?73, New York, NY, USA. ACM.
Manning, Christopher D. and Hirich Schutze. 2002.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Mas-
sachusetts, London, England.
Mei, Qiaozhu and ChengXiang Zhai. 2008. Generat-
ing impact-based summaries for scientific literature.
In Proceedings of ACL ?08, pages 816?824.
Mohammad, Saif, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using
citations to generate surveys of scientific paradigms.
In NAACL 2009, pages 584?592, June.
Nanba, Hidetsugu and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926?931.
Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-
mura. 2004. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117?134, Chicago, USA.
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. Proceedings of the HLT-NAACL con-
ference.
Qazvinian, Vahed and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Radev, Dragomir, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Radev, Dragomir R., Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.
Siddharthan, Advaith and Simone Teufel. 2007.
Whose idea was this, and why does it matter? at-
tributing scientific work to citations. In Proceedings
of NAACL/HLT-07.
Teufel, Simone. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 workshop on Multi-
word expressions, pages 33?40.
903
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1245?1255,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
What?s with the Attitude? Identifying Sentences with Attitude in Online
Discussions
Ahmed Hassan Vahed Qazvinian
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
hassanam,vahed,radev@umich.edu
Dragomir Radev
Abstract
Mining sentiment from user generated content
is a very important task in Natural Language
Processing. An example of such content is
threaded discussions which act as a very im-
portant tool for communication and collabo-
ration in the Web. Threaded discussions in-
clude e-mails, e-mail lists, bulletin boards,
newsgroups, and Internet forums. Most of the
work on sentiment analysis has been centered
around finding the sentiment toward products
or topics. In this work, we present a method
to identify the attitude of participants in an
online discussion toward one another. This
would enable us to build a signed network
representation of participant interaction where
every edge has a sign that indicates whether
the interaction is positive or negative. This
is different from most of the research on so-
cial networks that has focused almost exclu-
sively on positive links. The method is exper-
imentally tested using a manually labeled set
of discussion posts. The results show that the
proposed method is capable of identifying at-
titudinal sentences, and their signs, with high
accuracy and that it outperforms several other
baselines.
1 Introduction
Mining sentiment from text has a wide range of
applications from mining product reviews on the
Web (Morinaga et al, 2002; Turney and Littman,
2003) to analyzing political speeches (Thomas et al,
2006). Automatic methods for sentiment mining are
very important because manual extraction of them is
very costly, and inefficient. A new application of
sentiment mining is to automatically identify atti-
tudes between participants in an online discussion.
An automatic tool to identify attitudes will enable
us to build a signed network representation of par-
ticipant interaction in which the interaction between
two participants is represented using a positive or
a negative edge. Even though using signed edges
in social network studies is clearly important, most
of the social networks research has focused only on
positive links between entities. Some work has re-
cently investigated signed networks (Leskovec et al,
2010; Kunegis et al, 2009), however this work was
limited to a few number of datasets in which users
were allowed to explicitly add negative, as well as
positive, relations. This work will pave the way for
research efforts to examine signed social networks
in more detail. It will also allow us to study the re-
lation between explicit relations and the text under-
lying those relation.
Although similar, identifying sentences that dis-
play an attitude in discussions is different from iden-
tifying opinionated sentences. A sentence in a dis-
cussion may bear opinions about a definite target
(e.g., price of a camera) and yet have no attitude to-
ward the other participants in the discussion. For in-
stance, in the following discussion Alice?s sentence
has her opinion against something, yet no attitude
toward the recipient of the sentence, Bob.
Alice: ?You know what, he turned out to
be a great disappointment?
Bob: ?You are completely unqualified to
judge this great person?
However, Bob shows strong attitude toward Alice.
In this work, we look at ways to predict whether a
sentence displays an attitude toward the text recip-
ient. An attitude is the mental position of one par-
ticipant with regard to another participant. it could
be either positive or negative. We consider features
which takes into account the entire structure of sen-
tences at different levels or generalization. Those
1245
features include lexical items, part-of-speech tags,
and dependency relations. We use all those patterns
to build several pairs of models that represent sen-
tences with and without attitude.
The rest of the paper is organized as follows. In
Section 2 we review some of the related prior work
on identifying polarized words and subjectivity anal-
ysis. We explain the problem definition and discuss
our approach in Sections 3 & 4. Finally, in Sec-
tions 5 & 6 we introduce our dataset and discuss the
experimental setup. Finally, we conclude in Section
7.
2 Related Work
Identifying the polarity of individual words is a well
studied problem. In previous work, Hatzivassiloglou
and McKeown (1997) propose a method to iden-
tify the polarity of adjectives. They use a manu-
ally labeled corpus to classify each conjunction of
an adjective as ?the same orientation? as the adjec-
tive or ?different orientation?. Their method can
label simple in ?simple and well-received? as the
same orientation and simplistic in ?simplistic but
well-received? as the opposite orientation of well-
received. Although the results look promising, the
method would only be applicable to adjectives since
noun conjunctions may collocate regardless of their
semantic orientations (e.g., ?rise and fall?).
In other work, Turney and Littman (2003) use sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
In order to get word co-occurrence statistics they use
the ?near? operator from a commercial search en-
gine on a given word and a seed word.
In more recent work, Takamura et al (2005) used
the spin model to extract word semantic orientation.
First, they construct a network of words using def-
initions, thesaurus, and co-occurrence statistics. In
this network, each word is regarded as an electron,
which has a spin and each spin has a direction tak-
ing one of two values: up or down. Then, they use
the energy point of view to propose that neighboring
electrons tend to have the same spin direction, and
therefore neighboring words tend to have the same
polarity orientations. Finally, they use the mean field
method to find the optimal solution for electron spin
directions.
Previous work has also used WordNet, a lexi-
cal database of English, to identify word polarity.
Specifically, Hu and Liu (2004) use WordNet syn-
onyms and antonyms to predict the polarity of any
given word with unknown polarity. They label each
word with the polarity of its synonyms and the op-
posite polarity of its antonyms. They continue in
a bootstrapping manner to label all unlabeled in-
stances. This work is very similar to (Kamps et al,
2004) in which a network of WordNet synonyms
is used to find the shortest path between any given
word, and the words ?good? and ?bad?. Kim and
Hovy (Kim and Hovy, 2004) used WordNet syn-
onyms and antonyms to expand two lists of positive
and negative seed words. Similarly, Andreevskaia
and Bergler (2006) used WordNet to expand seed
lists with fuzzy sentiment categories, in which words
could be more central to one category than the other.
Finally, Kanayama and Nasukawa (2006) used syn-
tactic features and context coherency, defined as the
tendency for same polarities to appear successively,
to acquire polar atoms.
All the work mentioned above focus on the task
of identifying the polarity of individual words. Our
proposed work is identifying attitudes in sentences
that appear in online discussions. Perhaps the most
similar work to ours is the prior work on subjectivity
analysis, which is to identify text that present opin-
ions as opposed to objective text that present fac-
tual information (Wiebe, 2000). Prior work on sub-
jectivity analysis mainly consists of two main cate-
gories: The first category is concerned with identify-
ing the subjectivity of individual phrases and words
regardless of the sentence and context they appear
in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008). In the second category, sub-
jectivity of a phrase or word is analyzed within its
context (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, ). A good study of the applications
of subjectivity analysis from review mining to email
classification is given in (Wiebe, 2000). Somasun-
daran et al (2007) develop genre-speci.c lexicons
using interesting function word combinations for de-
tecting opinions in meetings. Despite similarities,
our work is different from subjectivity analysis be-
cause the later only discriminates between opinions
and facts. A discussion sentence may display an
1246
opinion about some topic yet no attitude. The lan-
guage constituents considered in opinion detection
may be different from those used to detect attitude.
Moreover, extracting attitudes from online discus-
sions is different from targeting subjective expres-
sions (Josef Ruppenhofer and Wiebe, 2008; Kim
and Hovy, 2004). The later usually has a limited
set of targets that compete for the subjective expres-
sions (for example in movie review, targets could be:
director, actors, plot, and so forth). We cannot use
similar methods because we are working on an open
domain where anything could be a target. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
There is also some related work on mining on-
line discussions. Lin et al(2009) proposes a sparse
coding-based model simultaneously model seman-
tics and structure of threaded discussions. Shen
et al(2006) proposes three clustering methods for
exploiting the temporal information in the streams,
as well as an algorithm based on linguistic fea-
tures to analyze the discourse structure information.
Huang et al(2007) used an SVM classifier to extract
(thread-title, reply) pairs as chat knowledge from on-
line discussion forums to support the construction
of a chatbot for a certain domain. Other work has
focused on the structure of questions and question-
answer pairs in online forums and discussions (Ding
et al, 2008; Cong et al, 2008).
3 Problem Definition
Assume we have a set of sentences exchanged be-
tween participants in an online discussion. Our ob-
jective is to identify sentences that display an atti-
tude from the text writer to the text recepient from
those that do not. An attitude is the mental posi-
tion of one particpant with regard to another partic-
ipant. An attitude may not be directly observable,
but rather inferred from what particpants say to one
another. The attitude could be either positive or neg-
ative. Strategies for showing a positive attitude may
include agreement, and praise, while strategies for
showing a negative attitude may include disagree-
ment, insults, and negative slang. After identifying
sentences that display an attitude, we also predict the
sign (positive or negative) of that attitude.
4 Approach
In this section, we describe a model which, given a
sentence, predicts whether it carries an attitude from
the text writer toward the text recipient or not. Any
given piece of text exchanged between two partici-
pants in a discussion could carry an attitude toward
the text recipient, an attitude towards the topic, or
no attitude at all. As we are only interested in at-
titudes between participants, we limit our study to
sentences that use second person pronouns. Second
person pronouns are usually used in conversational
genre to indicate that the text writer is addressing the
text recipient. After identifying those sentences, we
do some pre-processing to extract the most relevant
fragments. We examine these fragments to to iden-
tify the polarity of every word in the sentence. Every
word could be assigned a semantic orientation. The
semantic orientation could be either positive, nega-
tive, or neutral. The existence of polarized words in
any sentence is an important indicator of whether it
carries an attitude or not.
The next step is to extract several patterns at
different levels of generalization representing any
given sentence. We use those patterns to build two
Markov models for every kind of patterns. The first
model characterizes the relation between different
tokens for all patterns that correspond to sentences
that have an attitude. The second model is similar to
the first one, but it uses all patterns that correspond
to sentences that do not have an attitude. Given a
new sentence, we extract the corresponding patterns
and estimate the likelihood of every pattern being
generated from the two corresponding models. We
then compare the likelihood of the sentence under
the two models and use this as a feature to predict
the existence of an attitude. A pair of models will
be built for every kind of patterns. If we have n dif-
ferent patterns, we will have n different likelihood
ratios that come from n pairs of models.
4.1 Word Polarity Identification
Identifying the polarity of words is an important step
for our method. Our word identification module is
similar to the work in (Annon, 2010). We construct
a graph where each node represent a word/part-of-
speech pair. Two nodes are linked if the words are
related. We use WordNet (Miller, 1995) to link re-
1247
lated words based on synonyms, hypernyms, and
similar to relations. For words that do not appear
in Wordnet, we used Wiktionary, a collaboratively
constructed dictionary. We also add some links
based on co-occurrence statistics between words as
from a large corpus. The resulting graph is a graph
G(W,E) where W is a set of word/part-of-speech
pairs, and E is the set of edges connecting related
words.
We define a random walk model on the graph,
where the set of nodes correspond to the state space
of the random walk. Transition probabilities are cal-
culated by normalizing the weights of the edges out
of every node. Let S+ and S? be two sets of ver-
tices representing seed words that are already la-
beled as either positive or negative respectively. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). For
any given word w, we calculate the mean hitting
time betweenw, and the two seed sets h(w|S+), and
h(w|S?). The mean hitting time h(i|k) is defined as
the average number of steps a random walker, start-
ing in state i 6= k, will take to enter state k for the
first time (Norris, 1997). If h(w|S+) is greater than
h(w|S?), the word is classified as negative, oth-
erwise it is classified as positive. We also use the
method described in (Wilson et al, 2005) to deter-
mine the contextual polarity of the identified words.
The set of features used to predict contextual polar-
ity include word, sentence, polarity, structure, and
other features.
4.2 Identifying Relevant Parts of Sentences
The writing style in online discussion forums is very
informal. Some of the sentence are very long, and
punctuation marks are not always properly used. To
solve this problem, we decided to use the grammat-
ical structure of sentences to identify the most rele-
vant part of sentences that would be the subject of
further analysis. Figure 1 shows a parse tree repre-
senting the grammatical structure of a particular sen-
tence. If we closely examine the sentence, we will
notice that we are only interested in a part of the
sentence that includes the second person pronoun
?you?. We extract this part, by starting at the word
of interest , in this case ?you?, and go up in the hi-
erarchy till we hit the first sentence clause. Once,
we reach a sentence clause, we extract the corre-
sponding text if it is grammatical, otherwise we go
up one more level to the closest sentence clause. We
used the Stanford parser to generate the grammatical
structure of sentences (Klein and Manning, 2003).
Figure 1: An example showing how to identify the rele-
vant part of a sentence.
4.3 Sentences as Patterns
The fragments we extracted earlier are more rele-
vant to our task and are more suitable for further
analysis. However, these fragments are completely
lexicalized and consequently the performance of any
analysis based on them will be limited by data spar-
sity. We can alleviate this by using more general
representations of words. Those general representa-
tions can be used a long with words to generate a set
of patterns that represent each fragment. Each pat-
tern consists of a sequence of tokens. Examples of
such patterns could use lexical items, part-of-speech
(POS) tags, word polarity tags, and dependency re-
lations.
We use three different patterns to represent each
fragments:
? Lexical patterns: All polarized words are re-
places with the corresponding polarity tag, and
all other words are left as is.
? Part-of-speech patterns: All words are replaced
with their POS tags. Second person pronouns
are left as is. Polarized words are replaced with
their polarity tags and their POS tags.
? Dependency grammar patterns: the shortest
path connecting every second person pronoun
1248
to the closed polarized word is extracted. The
second person pronoun, the polarized word tag,
and the types of the dependency relations along
the path connecting them are used as a pat-
tern. It has been shown in previous work on
relation extraction that the shortest path be-
tween any two entities captures the the in-
formation required to assert a relationship be-
tween them (Bunescu and Mooney, 2005). Ev-
ery polarized word is assigned to the closest
second person pronoun in the dependency tree.
This is only useful for sentences that have po-
larized words.
Table 1 shows the different kinds of representa-
tions for a particular sentence. We use text, part-
of-speech tags, polarity tags, and dependency rela-
tions. The corresponding patterns for this sentence
are shown in Table 2.
4.4 Building the Models
Given a set of patterns representing a set of sen-
tences, we can build a graph G = V,E,w where
V is the set of all possible token that may appear
in the patterns. E = V ? V is the set of possible
transitions between any two tokens. w : E ? [0..1]
is a weighting function that assigns to every pair of
states (i, j) a weight w(i, j) representing the proba-
bility that we have a transition from state i to state
j.
This graph corresponds to a Markovian model.
The set of states are the vocabulary, and the the tran-
sition probabilities between states are estimated us-
ing Maximum Likelihood estimation as follows:
Pij =
Nij
Ni
whereNij is the number of times we saw a transition
from i to state j, and Ni is the total number of times
we saw state i in the training data. This is similar to
building a language model over the language of the
patterns.
We build two such models for every kind of pat-
terns. The first model is built using all sentences that
appeared in the training dataset and was labeled as
having an attitude, and the second model is built us-
ing all sentences in the training dataset that do not
have an attitude. If we have n kinds of patterns, we
will build one such pair for every kind of patterns.
Hence, we will end up with 2n models.
4.5 Identifying Sentences with Attitude
We split our training data into two splits; the first
containing all sentences that have an attitude and the
second containing all sentences that do not have an
attitude. Given the methodology described in the
previous section, we build n pairs of Markov mod-
els. Given any sentence, we extract the correspond-
ing patterns and estimate the log likelihood that this
sequence of tokens was generated from every model.
Given a model M , and sequence of tokens T =
(T1, T2, . . . TSn), the probability of this token se-
quence being generated from M is:
PM (T ) =
n?
i=2
P (Ti|T1, . . . , Ti?1) =
n?
i=2
W (Ti?1, Ti)
where n is the number of tokens in the pattern, and
W is the probability transition function.
The log likelihood is then defined as:
LLM (T ) =
n?
i=2
logW (Ti?1, Ti)
For every pair of models, we may use the ratio be-
tween the two likelihoods as a feature:
f =
LLMatt(T )
LLMnoatt(T )
where T is the token sequence, LLMatt(T ) is the log
likelihood of the sequence given the attitude model,
and LLMnoatt(T ) is the log likelihood of the pattern
given the no-attitude model.
Given the n kinds of patterns, we can calculate
three different features. A standard machine learn-
ing classifier is then trained using those features to
predict whether a given sentence has an attitude or
not.
4.6 Identifying the Sign of an Attitude
To determine the orientation of an attitude sentence,
we tried two different methods. The first method as-
sumes that the orientation of an attitude sentence is
directly related to the polarity of the words it con-
tains. If the sentence has only positive and neutral
1249
Table 1: Tags used for building patterns
Text That makes your claims so ignorant
POS That/DT makes/VBZ your/PRP$ claims/NNS so/RB ignorant/JJ
Polarity That/O makes/O your/O claims/O so/O ignorant/NEG
Dependency your
poss
? claims
nsubj
? ignorant
Table 2: Sample patterns
Lexical pattern That makes your claims so NEG
POS pattern DT VBZ your PRP$ NNS RB NEG JJ
Dependency pattern your poss nsubj NEG
words, it is classified as positive. If the sentence
has only negative and neutral words, it is classified
as negative. If the sentence has both positive and
negative words, we calculate the summation of the
polarity scores of all positive words and that of all
negative words. The polarity score of a word is an
indicator of how strong of a polarized word it is. If
the former is greater, we classify the sentence as pos-
itive,otherwise we classify the sentence as negative.
The problem with this method is that it assumes
that all polarized words in a sentence with an atti-
tude target the text recipient. Unfortunately, that is
not always correct. For example, the sentence ?You
are completely unqualified to judge this great per-
son? has a positive word ?great? and a negative word
?unqualified?. The first method will not be able to
predict whether the sentence is positive or negative.
To solve this problem, we use another method that
is based on the paths that connect polarized words to
second person pronouns in a dependency parse tree.
For every positive word w , we identify the shortest
path connecting it to every second person pronoun
in the sentence then we compute the average length
of the shortest path connecting every positive word
to the closest second person pronoun. We repeat for
negative words and compare the two values. The
sentence is classified as positive if the average length
of the shortest path connecting positive words to the
closest second person pronoun is smaller than the
corresponding value for negative words. Otherwise,
we classify the sentence as negative.
5 Data
Our data was randomly collected from a set of dis-
cussion groups. We collected a large number of
threads from the first quarter of 2009 from a set of
Usenet discussion groups. All threads were in En-
glish, and had 5 posts or more. We parsed the down-
loaded threads to identify the posts and senders. We
kept posts that have quoted text and discarded all
other posts. The reason behind that is that partici-
pants usually quote other participants text when they
reply to them. This restriction allows us to iden-
tify the target of every post, and raises the proba-
bility that the post will display an attitude from its
writer to its target. We plan to use more sophsticated
methods for reconstructing the reply structure like
the one in (Lin et al, 2009). From those posts, we
randomly selected approximately 10,000 sentences
that use second person pronouns. We explained ear-
lier how second person pronouns are used in discus-
sions genres to indicate the writer is targeting the
text recipient. Given a random sentence selected
from some random discussion thread, the probabil-
ity that the sentence does not have an attitude is sig-
nificantly larger than the probability that it will have
an attitude. Hence, restricting our dataset to posts
with quoted text and sentences with second person
pronouns is very important to make sure that we
will have a considerable amount of attitudinal sen-
tences. The data was tokenized, sentence-split, part-
of-speech tagged with the OpenNLP toolkit. It was
parsed with the Stanford dependency parser (Klein
and Manning, 2003).
5.1 Annotation Scheme
The goals of the annotation scheme are to distin-
guish sentences that display an attitude from those
that do not. Sentences could display either a neg-
ative or a positive attitude. Disagreement, insults,
and negative slang are indicators of negative attitude.
1250
A B C D
A - 82.7 80.6 82.1
B 81.0 - 81.9 82.9
C 77.8 78.2 - 83.8
D 78.3 77.7 78.6 -
Table 3: Inter-annotator agreement
Agreement, and praise are indicators of positive at-
titude. Our annotators were instructed to read every
sentence and assign two labels to it. The first speci-
fies whether the sentence displays an attitude or not.
The existence of an attitude was judged on a three
point scale: attitude, unsure, and no-attitude. The
second is the sign of the attitude. If an attitude ex-
ists, annotators were asked to specify whether the
attitude is positive or negative. To evaluate inter-
annotator agreement, we use the agr operator pre-
sented in (Wiebe et al, 2005). This metric measures
the precision and recall of one annotator using the
annotations of another annotator as a gold standard.
The process is repeated for all pairs of annotators,
and then the harmonic mean of all values is reported.
Formally:
agr(A|B) =
|A ?B|
|A|
(1)
where A, and B are the annotation sets produced by
the two reviewers. Table 3 shows the value of the
agr operator for all pairs of annotators. The har-
monic mean of the agr operator is 80%. The agr
operator was used over the Kappa Statistic because
the distribution of the data was fairly skewed.
6 Experiments
6.1 Experimental Setup
We performed experiments on the data described in
the previous section. The number of sentences with
an attitude was around 20% of the entire dataset.
The class imbalance caused by the small number of
attitude sentences may hurt the performance of the
learning algorithm (Provost, 2000). A common way
of addressing this problem is to artificially rebal-
ance the training data. To do this we down-sample
the majority class by randomly selecting, without
replacement, a number of sentences without an at-
titude that equals the number of sentences with an
attitude. That resulted in a balanced subset, approx-
imately 4000 sentences, that we used in our experi-
ments.
We used Support Vector Machines (SVM) as a
classifier. We optimized SVM separately for every
experiment. We used 10-fold cross validation for all
tests. We evaluate our results in terms of precision,
recall, accuracy, and F1. Statistical significance was
tested using a 2-tailed paired t-test. All reported re-
sults are statistically significant at the 0.05 level. We
compare the proposed method to several other base-
lines that will be described in the next subsection.
We also perform experiments to measure the perfor-
mance if we mix features from the baselines and the
proposed method.
6.2 Baselines
The first baseline is based on the hypothesis that the
existence of polarized words is a strong indicator
that the sentence has an attitude. As a result, we
use the number of polarized word in the sentence,
the percentage of polarized words to all other words,
and whether the sentences has polarized words with
mixed or same sign as features to train an SVM clas-
sifier to detect attitude.
The second baseline is based on the proximity be-
tween the polarized words and the second person
pronouns. We assume that every polarized word is
associated with the closest second person pronoun.
Let w be a polarized word and p(w) be the closes
second person pronoun, and surf dist(w, p(w)) be
the surface distance betweenw and p(w). This base-
line uses the minimum, maximum, and average of
surf dist(w, p(w)) for all polarized words as fea-
tures to train an SVM classifier to identify sentences
with attitude.
The next baseline uses the dependency tree dis-
tance instead of the surface distance. We assume that
every polarized word is associated to the second per-
son pronoun that is connected to it using the smallest
shortest path. The dep dist(w, p(w)) is calculated
similar to the previous baselines but using the de-
pendency tree distance. The minimum, maximum,
and average of this distance for all polarized words
are used as features to train an SVM classifier.
1251
Figure 2: Accuracy, Precision, and Recall for the Pro-
posed Approach and the Baselines.
0 10 20 30 40 50 60 70 80 90 1000
10
20
30
40
50
60
70
80
90
100
Recall
Prec
ision
 
 MMSurfDistDepDistPol
Figure 3: Precision Recall Graph.
6.3 Results and Discussion
Figure 2 compares the accuracy, precision, and re-
call of the proposed method (ML), the polarity based
classifier (POL), the surface distance based classi-
fier (Surf Dist), and the dependency distance based
classifier (Dep Dist). The values are selected to opti-
mize F1. The figure shows that the surface distance
based classifier behaves poorly with low accuracy,
precision, and recall. The two other baselines be-
have poorly as well in terms of precision and accu-
racy, but they do very well in terms of recall. We
looked at some of the examples to understand why
those two baselines achieve very high recall. It turns
out that they tend to predict most sentences that have
polarized words as sentences with attitude. This re-
sults in many false positives and low true negative
rate. Achieving high recall at the expense of losing
precision is trivial. On the other hand, we notice that
0 10 20 30 40 50 60 70 80 90 10075
76
77
78
79
80
81
82
Training Set Size (%)
Accu
racy
Figure 4: Accuracy Learning Curve for the Proposed
Method.
the proposed method results in very close values of
precision and recall at the optimum F1 point.
To better compare the performance of the pro-
posed method and the baseline, we study the the
precision-recall curves for all methods in Figure 3.
We notice that the proposed method outperforms all
baselines at all operating points. We also notice that
the proposed method provides a nice trade-off be-
tween precision and recall. This allows us some flex-
ibility in choosing the operating point. For example,
in some applications we might be interested in very
high precision even if we lose recall, while in other
applications we might sacrifice precision in order to
get high recall. On the other hand, we notice that
the baselines always have low precision regardless
of recall.
Table 4 shows the accuracy, precision, recall, and
F1 for the proposed method and all baselines. It also
shows the performance when we add features from
the baselines to the proposed method, or merge some
of the baselines. We see that we did not get any im-
provement when we added the baseline features to
the proposed method. We believe that the proposed
method captures all the information captured by the
baselines and more.
Our proposed method uses three different features
that correspond to the three types of patterns we use
to represent every sentence. To understand the con-
tributions of every feature, we measure the perfor-
mance of every feature by itself and also all possible
combinations of pairs of features. We compare that
1252
to the performance we get when using all features in
Table 5. We see that the part-of-speech patterns per-
forms better than the text patterns. This makes sense
because the former suffers from data sparsity. De-
pendency patterns performs best in terms of recall,
while part-of-speech patterns outperform all others
in terms of precision, and accuracy. All pairs of
features outperform any single feature that belong
to the corresponding pair in terms of F1. We also
notice that using the three features results in better
performance when compared to all other combina-
tions. This shows that every kind of pattern captures
slightly different information when compared to the
others. It also shows that merging the three features
improves performance.
One important question is how much data is re-
quired to the proposed model. We constructed a
learning curve, shown in Figure 4, by fixing the
test set size at one tenth of the data, and varying
the training set size. We carried out ten-fold cross
validation as with our previous experiments. We see
that adding more data continues to increase the accu-
racy, and that accuracy is quite sensitive to the train-
ing data. This suggests that adding more data to this
model could lead to even better results.
We also measured the accuracy of the two meth-
ods we proposed for predicting the sign of attitudes.
The accuracy of the first model that only uses the
count and scores of polarized words was 95%. The
accuracy of the second method that used depen-
dency distance was 97%.
6.4 Error Analysis
We had a closer look at the results to find out what
are the reasons behind incorrect predictions. We
found two main reasons. First, errors in predicting
word polarity usually propagates and results in er-
rors in attitude prediction. The reasons behind incor-
rect word polarity predictions is ambiguity in word
senses and infrequent words that have very few con-
nection in thesaurus. A possible solution to this type
of errors is to improve the word polarity identifica-
tion module by including word sense disambigua-
tion and adding more links to the words graph using
glosses or co-occurrence statistics. The second rea-
son is that some sentences are sarcastic in nature. It
is so difficult to identify such sentences. Identify-
ing sarcasm should be addressed as a separate prob-
Method Accuracy Precision Recall F1
ML 80.3 81.0 79.4 80.2
POL 73.1 66.4 93.9 77.7
ML+POL 79.9 77.9 83.4 80.5
SurfDist 70.2 67.1 79.2 72.7
DepDist 73.1 66.4 93.8 77.8
SurfDist+ 73.1 66.4 93.8 77.7
DepDist
ML+SurfDist 73.9 67.2 93.6 78.2
ML+DepDist 72.8 66.1 93.8 77.6
ML+SurfDist+ 74.0 67.2 93.4 78.2
DepDist
SurfDist+ 73.1 66.3 93.8 77.7
DepDist+POL
ML+SurfDist+ 73.0 66.2 93.8 77.6
DepDist+POL
Table 4: Precision, Recall, F1, and Accuracy for the pro-
posed method, the baselines, and different combinations
of proposed method and the baselines features
Method Accuracy Precision Recall F1
txt 75.5 74.1 78.6 76.2
pos 77.7 78.2 76.9 77.5
dep 74.7 70.4 85.1 77.0
txt+pos 77.8 77.0 79.4 78.1
txt+dep 79.4 79.6 79.2 79.4
pos+dep 80.4 79.1 82.5 80.7
txt+pos+dep 80.3 81.0 79.4 80.2
Table 5: Precision, Recall, F1, and Accuracy for different
combinations of the proposed method?s features.
lem. A method that utilizes holistic approaches that
takes context and previous interactions between dis-
cussion participants into consideration could be used
to address it.
7 Conclusions
We have shown that training a supervised Markov
model of text, part-of-speech, and dependecy pat-
terns allows us to identify sentences with attitudes
from sentences without attitude. This model is more
accurate than several other baselines that use fea-
tures based on the existence of polarized word, and
proximity between polarized words and second per-
son pronouns both in text and dependecy trees. This
method allows to extract signed social networks
from multi-party online discussions. This opens the
door to research efforts that go beyond standard so-
cial network analysis that is based on positve links
1253
only. It also allows us to study dynamics behind in-
teractions in online discussions, the relation between
text and social interactions, and how groups form
and break in online discussions.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT ?05, pages 724?731, Morristown, NJ,
USA. Association for Computational Linguistics.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
pairs from online forums. In SIGIR ?08, pages 467?
474.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
texts and answers of questions from online forums. In
ACL?08, pages 710?718.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jizhou Huang, Ming Zhou, and Dan Yang. 2007. Ex-
tracting chatbot knowledge from online discussion fo-
rums. In IJCAI?07, pages 423?428.
Swapna Somasundaran Josef Ruppenhofer and Janyce
Wiebe. 2008. Finding the sources and targets
of subjective expressions. In Proceedings of the
Sixth International Language Resources and Evalua-
tion (LREC?08).
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In WWW?09, pages
741?750, New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010. Predicting positive and negative links in online
social networks. In WWW ?10, pages 641?650, New
York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. Extracting prod-
uct features and opinions from reviews. In HLT-
EMNLP?05.
Foster Provost. 2000. Machine learning from imbal-
anced data sets 101. In Proceedings of the AAAI Work-
shop on Imbalanced Data Sets.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
1254
meetings. In Proceedings of the SIGdial Workshop on
Discourse and Dialogue.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions
in language. Language Resources and Evaluation,
1(2):0.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP?05, Vancouver,
Canada.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
1255
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589?1599,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Rumor has it: Identifying Misinformation in Microblogs
Vahed Qazvinian Emily Rosengren Dragomir R. Radev Qiaozhu Mei
University of Michigan
Ann Arbor, MI
{vahed,emirose,radev,qmei}@umich.edu
Abstract
A rumor is commonly defined as a state-
ment whose true value is unverifiable. Ru-
mors may spread misinformation (false infor-
mation) or disinformation (deliberately false
information) on a network of people. Identi-
fying rumors is crucial in online social media
where large amounts of information are easily
spread across a large network by sources with
unverified authority. In this paper, we address
the problem of rumor detection in microblogs
and explore the effectiveness of 3 categories of
features: content-based, network-based, and
microblog-specific memes for correctly iden-
tifying rumors. Moreover, we show how these
features are also effective in identifying disin-
formers, users who endorse a rumor and fur-
ther help it to spread. We perform our exper-
iments on more than 10,000 manually anno-
tated tweets collected from Twitter and show
how our retrieval model achieves more than
0.95 in Mean Average Precision (MAP). Fi-
nally, we believe that our dataset is the first
large-scale dataset on rumor detection. It can
open new dimensions in analyzing online mis-
information and other aspects of microblog
conversations.
1 Introduction
A rumor is an unverified and instrumentally relevant
statement of information spread among people (Di-
Fonzo and Bordia, 2007). Social psychologists ar-
gue that rumors arise in contexts of ambiguity, when
the meaning of a situation is not readily apparent,
or potential threat, when people feel an acute need
for security. For instance rumors about ?office ren-
ovation in a company? is an example of an ambigu-
ous context, and the rumor that ?underarm deodor-
ants cause breast cancer? is an example of a context
in which one?s well-being is at risk (DiFonzo et al,
1994).
The rapid growth of online social media has made
it possible for rumors to spread more quickly. On-
line social media enable unreliable sources to spread
large amounts of unverified information among peo-
ple (Herman and Chomsky, 2002). Therefore, it is
crucial to design systems that automatically detect
misinformation and disinformation (the former of-
ten seen as simply false and the latter as deliberately
false information).
Our definition of a rumor is established based on
social psychology, where a rumor is defined as a
statement whose truth-value is unverifiable or delib-
erately false. In-depth rumor analysis such as deter-
mining the intent and impact behind the spread of
a rumor is a very challenging task and is not possi-
ble without first retrieving the complete set of social
conversations (e.g., tweets) that are actually about
the rumor. In our work, we take this first step to
retrieve a complete set of tweets that discuss a spe-
cific rumor. In our approach, we address two basic
problems. The first problem concerns retrieving on-
line microblogs that are rumor-related. In the second
problem, we try to identify tweets in which the ru-
mor is endorsed (the posters show that they believe
the rumor).
2 Related Work
We review related work on 3 main areas: Analyzing
rumors, mining microblogs, and sentiment analysis
and subjectivity detection.
2.1 Rumor Identification and Analysis
Though understanding rumors has been the sub-
ject of research in psychology for some time (All-
port and Lepkin, 1945), (Allport and Postman,
1947), (DiFonzo and Bordia, 2007), research has
1589
only recently begun to investigate how rumors are
manifested and spread differently online. Mi-
croblogging services, like Twitter, allow small
pieces of information to spread quickly to large au-
diences, allowing rumors to be created and spread in
new ways (Ratkiewicz et al, 2010).
Related research has used different methods to
study the spread of memes and false information
on the web. Leskovec et al use the evolution
of quotes reproduced online to identify memes and
track their spread overtime (Leskovec et al, 2009).
Ratkiewicz et al (Ratkiewicz et al, 2010) created
the ?Truthy? system, identifying misleading politi-
cal memes on Twitter using tweet features, includ-
ing hashtags, links, and mentions. Other projects
focus on highlighting disputed claims on the Inter-
net using pattern matching techniques (Ennals et al,
2010). Though our project builds on previous work,
our work differs in its general focus on identifying
rumors from a corpus of relevant phrases and our at-
tempts to further discriminate between phrases that
confirm, refute, question, and simply talk about ru-
mors of interest.
Mendoza et al explore Twitter data to analyze the
behavior of Twitter users under the emergency situ-
ation of 2010 earthquake in Chile (Mendoza et al,
). They analyze the re-tweet network topology and
find that the patterns of propagation in rumors dif-
fer from news because rumors tend to be questioned
more than news by the Twitter community.
2.2 Sentiment Analysis
The automated detection of rumors is similar to tra-
ditional NLP sentiment analysis tasks. Previous
work has used machine learning techniques to iden-
tify positive and negative movie reviews (Pang et
al., 2002). Hassan et al use a supervised Markov
model, part of speech, and dependency patterns to
identify attitudinal polarities in threads posted to
Usenet discussion posts (Hassan et al, 2010). Oth-
ers have designated sentiment scores for news sto-
ries and blog posts based on algorithmically gener-
ated lexicons of positive and negative words (God-
bole et al, 2007). Pang and Lee provide a detailed
overview of current techniques and practices in sen-
timent analysis and opinion mining (Pang and Lee,
2008; Pang and Lee, 2004).
Though rumor classification is closely related to
opinion mining and sentiment analysis, it presents
a different class of problem because we are con-
cerned not just with the opinion of the person post-
ing a tweet, but with whether the statements they
post appear controversial. The automatic identifica-
tion of rumors from a corpus is most closely related
to the identification of memes done in (Leskovec et
al., 2009), but presents new challenges since we seek
to highlight a certain type of recurring phrases. Our
work presents one of the first attempts at automatic
rumor analysis.
2.3 Mining Twitter Data
With its nearly constant update of new posts and
public API, Twitter can be a useful source for
collecting data to be used in exploring a num-
ber of problems related to natural language pro-
cessing and information diffusion (Bifet and Frank,
2010). Pak and Paroubek demonstrated experimen-
tally that despite frequent occurrences of irregular
speech patterns in tweets, Twitter can provide a use-
ful corpus for sentiment analysis (Pak and Paroubek,
2010). The diversity of Twitter users make this
corpus especially valuable. Ratkiewicz et alalso
use Twitter to detect and track misleading political
memes (Ratkiewicz et al, 2010).
Along with many advantages, using Twitter as a
corpus for sentiment analysis does present unusual
challenges. Because posts are limited to 140 charac-
ters, tweets often contain information in an unusu-
ally compressed form and, as a result, grammar used
may be unconventional. Instances of sarcasm and
humor are also prevalent (Bifet and Frank, 2010).
The procedures we used for the collection and anal-
ysis of tweets are similar to those described in previ-
ous work. However, our goal of developing compu-
tational methods to identify rumors being transmit-
ted through tweets differentiates our project.
3 Problem Definition
Assume we have a set of tweets that are about the
same topic that has some controversial aspects. Our
objective in this work is two-fold: (1) Extract tweets
that are about the controversial aspects of the story
and spread misinformation (Rumor retrieval). (2)
Identify users who believe that misinformation ver-
sus users who refute or question the rumor (Belief
1590
Name Rumor Regular Expression Query Status #tweets
obama Is Barack Obama muslim? Obama & (muslim|islam) false 4975
airfrance Air France mid-air crash photos? (air.france|air france) & (photo|pic|pix) false 505
cellphone Cell phone numbers going public? (cell|cellphone|cell phone) mostly false 215
michelle Michelle Obama hired too many staff? staff & (michelle obama|first lady|1st lady) partly true 299
palin Sarah Palin getting divorced? palin & divorce false 4423
Table 1: List of rumor examples and their corresponding queries used to collect data from Twitter
classification).
The following two tweets are two instances of the
tweets written about president Obama and the Mus-
lim world. The first tweet below is about president
Obama and Muslim world, where the second tweet
spread misinformation that president Obama is Mus-
lim.
(non-rumor) ?As Obama bows to Muslim leaders
Americans are less safe not only at home but also
overseas. Note: The terror alert in Europe... ?
(rumor) ?RT @johnnyA99 Ann Coulter Tells Larry
King Why People Think Obama Is A Muslim
http://bit.ly/9rs6pa #Hussein via @NewsBusters
#tcot ..?
The goal of the retrieval task is to discriminate
between such tweets. In the second task, we use
the tweets that are flagged as rumorous, and identify
users that endorse (believe) the rumor versus users
who deny or question it. The following three tweets
are about the same story. The first user is a believer
and the second and third are not.
(confirm) ?RT @moronwatch: Obama?s a Muslim. Or
if he?s not, he sure looks like one #whyimvotingre-
publican.?
(deny) ?Barack Obama is a Christian man who had
a Christian wedding with 2 kids baptised in Jesus
name. Tea Party clowns call that muslim #p2 #gop?
(doubtful) ?President Barack Obama?s Religion:
Christian, Muslim, or Agnostic? - The News
of Today (Google): Share With Friend...
http://bit.ly/bk42ZQ?
The first task is substantially more challenging
than a standard IR task because of the requirement of
both high precision (every result should be actually
discussing the rumor) and high recall (the set should
be complete). To do this, we submit a handcrafted
regexp (extracted from about.com) to Twitter and re-
trieve a large primitive set of tweets that is supposed
to have a high recall. This set however, contains a lot
of false positives, tweets that match the regexp but
are not about the rumor (e.g., ?Obama meets muslim
leaders?). Moreover, a rumor is usually stated using
various instances (e.g., ?Barack HUSSEIN Obama?
versus ?Obama is muslim?). Our goal is then to de-
sign a learning framework that filters all such false
positives and retrieves various instances of the same
rumor
Although our second task, belief classification,
can be viewed as an opinion mining task, it is sub-
stantially different from opinion mining in nature.
The difference from a standard opinion mining task
is that here we are looking for attitudes about a sub-
tle statement (e.g., ?Palin is getting divorce?) instead
of the overall sentiment of the text or the opinion
towards an explicit object or person (e.g., ?Sarah
Palin?).
4 Data
As September 2010, Twitter reports that its users
publish nearly 95 million tweets per day1. This
makes Twitter an excellent case to analyze misin-
formation in social media.
Our goal in this work was to collect and annotate
a large dataset that includes all the tweets that are
written about a rumor in a certain period of time. To
collect such a complete and self-contained dataset
about a rumor, we used the Twitter search API, and
retrieved all the tweets that matched a given regular
expression. This API is the only API that returns re-
sults from the entire public Twitter stream and not
a small randomly selected sample. To overcome the
rate limit enforced by Twitter, we collected match-
ing tweets once per hour, and remove any duplicates.
To use the search API, we carefully designed reg-
ular expression queries to be broad enough to match
1http://twitter.com/about
1591
all the tweets that are about a rumor. Each query
represents a popular rumor that is listed as ?false?
or only ?partly true? on About.com?s Urban Leg-
ends reference site2 between 2009 and 2010. Table 1
lists the rumor examples that we used to collect our
dataset alng with their corresponding regular ex-
pression queries and the number of tweets collected.
4.1 Annotation
We asked two annotators to go over all the tweets
in the dataset and mark each tweet with a ?1? if it
is about any of the rumors from Table 1, and with
a ?0? otherwise. This annotation scheme will be
used in our first task to detect false positives, tweets
that match the broad regular expressions and are re-
trieved, but are not about the rumor. For instance,
both of the following tweets match the regular ex-
pression for the palin example, but only the sec-
ond one is rumorous.
(0) ?McCain Divorces Palin over her ?untruths and out
right lies? in the book written for her. McCain?s
team says Palin is a petty liar and phony?
(1) ?Sarah and Todd Palin to divorce, according to local
Alaska paper. http://ow.ly/iNxF?
We also asked the annotators to mark each pre-
viously annotated rumorous tweet with ?11? if the
tweet poster endorses the rumor and with ?12? if the
user refutes the rumor, questions its credibility, or is
neutral.
(12) ?Sarah Palin Divorce Rumor Debunked on Face-
book http://ff.im/62Evd?
(11) ?Todd and Sarah Palin to divorce
http://bit.ly/15StNc?
Our annotation of more than 10,400 tweets shows
that %35 of all the instances that matched the regu-
lar expressions are false positives, tweets that are not
rumor-related but match the initial queries. More-
over, among tweets that are about particular ru-
mors, nearly %43 show the poster believe the rumor,
demonstrating the importance of identifying misin-
formation and those who are misinformed. Table 2
shows the basic statistics extracted from the annota-
tions for each story.
2http://urbanlegends.about.com
Rumor non-rumor (0) believe (11) deny/ (12) total
doubtful/neutral
obama 3,036 926 1,013 4975
airfrance 306 71 128 505
cellphone 132 74 9 215
michelle 83 191 25 299
palin 86 1,709 2,628 4,423
total 3,643 2,971 3,803 10,417
Table 2: Number of instances in each class from the an-
notated data
task ?
rumor retrieval 0.954
belief classification 0.853
Table 3: Inter-judge agreement in two annotation tasks in
terms of ?-statistic
4.2 Inter-Judge Agreement
To calculate the annotation accuracy, we annotated
500 instances twice. These annotations were com-
pared with each other, and the Kappa coefficient (?)
was calculated. The ? statistic is formulated as
? = Pr(a)? Pr(e)1? Pr(e)
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that anno-
tators agree by chance if each annotator is randomly
assigning categories (Krippendorff, 1980; Carletta,
1996). Table 3 shows that annotators can reach
a high agreement in both extracting rumors (? =
0.95) and identifying believers (? = 0.85).
5 Approach
In this section, we describe a general framework,
which given a tweet, predicts (1) whether it is a
rumor-related statement, and if so (2) whether the
user believes the rumor or not. We describe 3 sets of
features, and explain why these are intuitive to use
for identification of rumors.
We process the tweets as they appear in the user
timeline, and do not perform any pre-processing.
Specially, we think that capitalization might be an
important property. So, we do not lower-case the
tweet texts either.
Our approach is based on building different Bayes
classifiers as high level features and then learning
a linear function of these classifiers for retrieval in
the first task and classification in the second. Each
1592
Bayes classifier, which corresponds to a feature fi,
calculates the likelihood ratio for a given tweet t, as
shown in Equation 1.
P (?+i |t)
P (??i |t)
= P (?
+
i )
P (??i )
P (t|?+i )
P (t|??i )
(1)
Here ?+i and ??i are two probabilistic models built
based on feature fi using a set of positive (+) and
negative (?) training data. The likelihood ratio ex-
presses how many times more likely the tweet t is
under the positive model than the negative model
with respect to fi.
For computational reasons and to avoid dealing
with very small numbers we use the log of the like-
lihood ratio to build each classifier.
LLi = log
P (?+i |t)
P (??i |t)
= log P (?
+
i )
P (??i )
+ log P (t|?
+
i )
P (t|??i )
(2)
The first term P (?+i )P (??i ) can be easily calculated us-ing the maximum likelihood estimates of the prob-
abilities (i.e., the estimate of each probability is the
corresponding relative frequency). The second term
is calculated using various features that we explain
below.
5.1 Content-based Features
The first set of features are extracted from the text of
the tweets. We propose 4 content based features. We
follow (Hassan et al, 2010) and present the tweet
with 2 different patterns:
? Lexical patterns: All the words and segments
in the tweet are represented as they appear and
are tokenized using the space character.
? Part-of-speech patterns: All words are replaced
with their part-of-speech tags. To find the part-
of-speech of a hashtag we treat it as a word
(since they could have semantic roles in the
sentence), by omitting the tag sign, and then
precede the tag with the label TAG/. We also
introduce a new tag, URL, for URLs that appear
in a tweet.
From each tweet we extract 4 (2 ? 2) features,
corresponding to unigrams and bigrams of each rep-
resentation. Each feature is the log-likelihood ra-
tio calculated using Equation 2. More formally,
we represent each tweet t, of length n, lexically
as (w1w2 ? ? ?wn) and with part-of-speech tags as
(p1p2 ? ? ? pn). After building the positive and nega-
tive models (?+, ??) for each feature using the train-
ing data, we calculate the likelihood ratio as defined
in Equation 2 where
P (t|?+)
P (t|??) =
n?
j=1
log P (wj |?
+)
P (wj |??)
(3)
for unigram-lexical features (TXT1) and
P (t|?+)
P (t|??) =
n?1?
j=1
log P (wjwj+1|?
+)
P (wjwj+1|??)
(4)
for bigram-based lexical features (TXT2). Simi-
larly, we define the unigram and bigram-based part-
of-speech features (POS1 and POS2) as the log-
likelihood ratio with respect to the positive and neg-
ative part-of-speech models.
5.2 Network-based Features
The features that we have proposed so far are all
based on the content of individual tweets. In the
second set of features we focus on user behavior on
Twitter. We observe 4 types of network-based prop-
erties, and build 2 features that capture them.
Twitter enables users to re-tweet messages from
other people. This interaction is usually easy to de-
tect because the re-tweeted messages generally start
with the specific pattern: ?RT @user?. We use this
property to infer about the re-tweeted message.
Let?s suppose a user ui re-tweets a message t from
the user uj (ui: ?RT @uj t?). Intuitively, t is more
likely to be a rumor if (1) uj has a history of posting
or re-tweeting rumors, or (2) ui has posted or re-
tweeted rumors in the past.
Given a set of training instances, we build a pos-
itive (?+) and a negative (??) user models. The
first model is a probability distribution over all users
that have posted a positive instance or have been re-
tweeted in a positive instance. Similarly, the sec-
ond model is a probability distribution over users
1593
that have posted (or been re-tweeted in) a negative
instance. After building the models, for a given
tweet we calculate two log-likelihood ratios as two
network-based features.
The first feature is the log-likelihood ratio that ui
is under a positive user model (USR1) and the sec-
ond feature is the log-likelihood ratio that the tweet
is re-tweeted from a user (uj) who is under a positive
user model than a negative user model (USR2).
The distinction between the posting user and the
re-tweeted user is important, since some times the
users modify the re-tweeted message in a way that
changes its meaning and intent. In the following ex-
ample, the original user is quoting president Obama.
The second user is re-tweeting the first user, but has
added more content to the tweet and made it sound
rumorous.
original message (non-rumor) ?Obama says he?s do-
ing ?Christ?s work?.?
re-tweeted (rumor) ?Obama says he?s doing ?Christ?s
work.? Oh my God, CHRIST IS A MUSLIM.?
5.3 Twitter Specific Memes
Our final set of features are extracted from memes
that are specific to Twitter: hashtags and URLs.
Previous work has shown the usefulness of these
memes (Ratkiewicz et al, 2010).
5.3.1 Hashtags
One emergent phenomenon in the Twitter ecosys-
tem is the use of hashtags: words or phrases prefixed
with a hash symbol (#). These hashtags are created
by users, and are widely used for a few days, then
disappear when the topic is outdated (Huang et al,
2010).
In our approach, we investigate whether hashtags
used in rumor-related tweets are different from other
tweets. Moreover, we examine whether people who
believe and spread rumors use hashtags that are dif-
ferent from those seen in tweets that deny or ques-
tion a rumor.
Given a set of training tweets of positive and neg-
ative examples, we build two statistical models (?+,
??), each showing the usage probability distribution
of various hashtags. For a given tweet, t, with a set
of m hashtags (#h1 ? ? ?#hm), we calculate the log-
likelihood ratio using Equation 2 where
Feature LL-ratio model
Content
TXT1 content unigram content unigram
TXT2 content bigram content unigram
POS1 content pos content pos unigram
POS2 content pos content pos bigram
Twitter
URL1 content unigram target URL unigram
URL2 content bigram target URL bigram
TAG hashtag hashtag
Network USR1 tweeting user all users in the dataUSR2 re-tweeted user all users in the data
Table 4: List of features used in our optimization frame-
work. Each feature is a log-likelihood ratio calculated
against a a positive (+) and negative (?) training models.
P (t|?+)
P (t|??) =
m?
j=1
log P (#hj |?
+)
P (#hj |??)
(5)
5.3.2 URLs
Previous work has discussed the role of URLs
in information diffusion on Twitter (Honeycutt and
Herring, 2009). Twitter users share URLs in their
tweets to refer to external sources or overcome the
length limit forced by Twitter. Intuitively, if a tweet
is a positive instance, then it is likely to be similar to
the content of URLs shared by other positive tweets.
Using the same reasoning, if a tweet is a negative
instance, then it should be more similar to the web
pages shared by other negative instances.
Given a set of training tweets, we fetch all the
URLs in these tweets and build ?+ and ?? once for
unigrams and once for bigrams. These models are
merely built on the content of the URLs and ignore
the tweet content. Similar to previous features, we
calculate the log-likelihood ratio of the content of
each tweet with respect to ?+ and ?? for unigrams
(URL1) and bigrams URL2).
Table 4 summarizes the set of features used in our
proposed framework, where each feature is a log-
likelihood ratio calculated against a positive (+) and
negative (?) training models. To build these lan-
guage models, we use the CMU Language Modeling
toolkit (Clarkson and Rosenfeld, 1997).
5.4 Optimization
We build an L1-regularized log-linear model (An-
drew and Gao, 2007) on various features discussed
before to predict each tweet. Suppose, a procedure
generates a set of candidates for an input x. Also,
1594
let?s suppose ? : X ? Y ? RD is a function that
maps each (x, y) to a vector of feature values. Here,
the feature vector is the vector of coefficients corre-
sponding to different network, content, and twitter-
based properties, and the parameter vector ? ? RD
(D ? 9 in our experiments) assigns a real-valued
weight to each feature. This estimator chooses ? to
minimize the sum of least squares and a regulariza-
tion term R.
?? = argmin
?
{12
?
i
||??, xi? ? yi||22 +R(?)} (6)
where the regularizer term R(?) is the weighted L1
norm of the parameters.
R(?) = ?
?
j
|?j | (7)
Here, ? is a parameter that controls the amount of
regularization (set to 0.1 in our experiments).
Gao et. al (Gao et al, 2007) argue that op-
timizing L1-regularized objective function is chal-
lenging since its gradient is discontinuous whenever
some parameters equal zero. In this work, we use
the orthant-wise limited-memory quasi-Newton al-
gorithm (OWL-QN), which is a modification of L-
BFGS that allows it to effectively handle the dis-
continuity of the gradient (Andrew and Gao, 2007).
OWL-QN is based on the fact that when restricted
to a single orthant, the L1 regularizer is differen-
tiable, and is in fact a linear function of ?. Thus,
as long as each coordinate of any two consecutive
search points does not pass through zero R(?) does
not contribute at all to the curvature of the function
on the segment joining them. Therefore, we can use
L-BFGS to approximate the Hessian of L(?) alone
and use it to build an approximation to the full reg-
ularized objective that is valid on a given orthant.
This algorithm works quite well in practice, and typ-
ically reaches convergence in even fewer iterations
than standard L-BFGS (Gao et al, 2007).
6 Experiments
We design 2 sets of experiments to evaluate our ap-
proach. In the first experiment we assess the effec-
tiveness of the proposed method when employed in
an Information Retrieval (IR) framework for rumor
retrieval and in the second experiment we employ
various features to detect users? beliefs in rumors.
6.1 Rumor Retrieval
In this experiment, we view different stories as
queries, and build a relevance set for each query.
Each relevance set is an annotation of the entire
10,417 tweets, where each tweet is marked as rel-
evant if it matches the regular expression query and
is marked as a rumor-related tweet by the annotators.
For instance, according to Table 2 the cellphone
dataset has only 83 relevant documents out of the
entire 10,417 documents.
For each query we use 5-fold cross-validation,
and predict the relevance of tweets as a function of
their features. We use these predictions and rank
all the tweets with respect to the query. To evalu-
ate the performance of our ranking model for a sin-
gle query (Q) with the set of relevant documents
{d1, ? ? ? , dm}, we calculate Average Precision as
AP (Q) = 1m
m?
k=1
Precision(Rk) (8)
where Rk is the set of ranked retrieval results from
the top result to the kth relevant document, dk (Man-
ning et al, 2008).
6.1.1 Baselines
We compare our proposed ranking model with a
number of other retrieval models. The first two sim-
ple baselines that indicate a difficulty lower-bound
for the problem are Random and Uniform meth-
ods. In the Random baseline, documents are ranked
based on a random number assignment to them. In
the Uniform model, we use a 5-fold cross validation,
and in each fold the label of the test documents is de-
termined by the majority vote from the training set.
The main baseline that we use in this work, is the
regular expression that was submitted to Twitter to
collect data (regexp). Using the same regular ex-
pression to mark the relevance of the documents will
cause a recall value of 1.00 (since it will retrieve all
the relevant documents), but will also retrieve false
positives, tweets that match the regular expression
but are not rumor-related. We would like to inves-
tigate whether using training data will help us de-
crease the rate of false positives in retrieval.
Finally, using the Lemur Toolkit software3, we
employ a KL divergence retrieval model with
3http://www.lemurproject.org/
1595
Dirichlet smoothing (KL). In this model, documents
are ranked according to the negation of the diver-
gence of query and document language models.
More formally, given the query language model ?Q,
and the document language model ?D, the docu-
ments are ranked by ?D(?Q||?D), where D is the
KL-divergence between the two models.
D(?Q||?D) =
?
w
p(w|?Q) log
p(w|?Q)
p(w|?D)
(9)
To estimate p(w|?D), we use Bayesian smoothing
with Dirichlet priors (Berger, 1985).
ps(w|?D) =
C(w,D) + ?.p(w|?S)
?+
?
w C(w,D)
(10)
where, ? is a parameter, C is the count function, and
thetaS is the collection language model. Higher val-
ues of ? put more emphasis on the collection model.
Here, we try two variants of the model, one using
the default parameter value in Lemur (? = 2000),
and one in which ? is tuned based on the the data
(? = 10). Using the test data to tune the parameter
value, ?, will help us find an upper-bound estimate
of the effectiveness of this method.
Table 5 shows the Mean Average Precision
(MAP) and F?=1 for each method in the rumor re-
trieval task. This table shows that a method that
employs training data to re-rank documents with
respect to rumors makes significant improvements
over the baselines and outperforms other strong re-
trieval systems.
6.1.2 Feature Analysis
To investigate the effectiveness of using indi-
vidual features in retrieving rumors, we perform
5-fold cross validations for each query, using
different feature sets each time. Figure 1 shows
the average precision and recall for our pro-
posed optimization system when content-based
(TXT1+TXT2+POS1+POS2), network-based
(USR1+USR2), and twitter specific memes
(TAG+URL1+URL2) are employed individually.
Figure 1 shows that features that are calculated us-
ing the content language models are very effective in
achieving high precision and recall. Twitter specific
features, especially hashtags, can result in high pre-
cisions but lead to a low recall value because many
Figure 1: Average precision and recall of the proposed
method employing each set of features: content-based,
network-based, and twitter specific.
tweets do not share hashtags or are not written based
on the contents of external URLs.
Finally, we find that user history can be a good
indicator of rumors. However, we believe that this
feature could be more helpful with a complete user
set and a more comprehensive history of their activ-
ities.
6.1.3 Domain Training Data
As our last experiment with rumor retrieval we in-
vestigate how much new labeled data from an emer-
gent rumor is required to effectively retrieve in-
stances of that particular rumor. This experiment
helps us understand how our proposed framework
could be generalized to other stories.
To do this experiment, we use the obama story,
which is a large dataset with a significant number of
false positive instances. We extract 400 randomly
selected tweets from this dataset and keep them for
testing. We also build an initial training dataset of
the other 4 rumors, and label them as not relevant.
We assess the performance of the retrieval model as
we gradually add the rest of the obama tweets. Fig-
ure 2 shows both Average Precision and labeling ac-
curacy versus the size of the labeled data used from
the obama dataset. This plot shows that both mea-
sures exhibit a fast growth and reach 80% when the
number of labeled data reaches 2000.
6.2 Belief Classification
In previous experiments we showed that maximiz-
ing a linear function of log-likelihood ratios is an
effective method in retrieving rumors. Here, we in-
1596
Method MAP 95% C.I. F?=1 95% C.I.
Random 0.129 [-0.065, 0.323] 0.164 [-0.051, 0.379]
Uniform 0.129 [-0.066, 0.324] 0.198 [-0.080, 0.476]
regexp 0.587 [0.305, 0.869] 0.702 [0.479, 0.925]
KL (? = 2000) 0.678 [0.458, 0.898] 0.538 [0.248, 0.828]
KL (? = 10) 0.803 [0.641, 0.965] 0.681 [0.614, 0.748]
LL (all 9 features) 0.965 [0.936, 0.994] 0.897 [0.828, 0.966]
Table 5: Mean Average Precision (MAP) and F?=1 of each method in the rumor retrieval task. (C.I.: Confidence
Interval)
Method Accuracy Precision Recall F?=1 Win/Loss Ratio
random 0.501 0.441 0.513 0.474 1.004
uniform 0.439 0.439 1.000 0.610 0.781
TXT 0.934 0.925 0.924 0.924 14.087
POS 0.742 0.706 0.706 0.706 2.873
content (TXT+POS) 0.941 0.934 0.930 0.932 15.892
network (USR) 0.848 0.873 0.765 0.815 5.583
TAG 0.589 0.734 0.099 0.175 1.434
URL 0.664 0.630 0.570 0.598 1.978
twitter (TAG+URL) 0.683 0.658 0.579 0.616 2.155
all 0.935 0.944 0.906 0.925 14.395
Table 6: Accuracy, precision, recall, F?=1, and win/loss ratio of belief classification using different features.
Figure 2: Average Precision and Accuracy learning curve
for the proposed method employing all 9 features.
vestigate whether this method, and in particular, the
proposed features are useful in detecting users? be-
liefs in a rumor that they post about. Unlike re-
trieval, detecting whether a user endorses a rumor or
refutes it may be possible using similar methods re-
gardless of the rumor. Intuitively, linguistic features
such as negation (e.g., ?obama is not a muslim?), or
capitalization (e.g., ?barack HUSSEIN obama ...?),
user history (e.g., liberal tweeter vs. conservative
tweeter), hashtags (e.g., #tcot vs. #tdot), and URLs
(e.g., links to fake airfrance crash photos) should
help to identify endorsements.
We perform this experiment by making a pool
of all the tweets that are marked as ?rumorous? in
the annotation task. Table 2 shows that there are
6,774 such tweets, from which 2,971 show belief
and 3,803 tweets show that the user is doubtful, de-
nies, or questions it.
Using various feature settings, we perform 5-fold
cross-validation on these 6,774 rumorous tweets.
Table 6 shows the results of this experiment in terms
of F-score, classification accuracy, and win/loss ra-
tio, the ratio of correct classification to an incorrect
1597
classification.
7 Conclusion
In this paper we tackle the fairly unaddressed prob-
lem of identifying misinformation and disinform-
ers in Microblogs. Our contributions in this pa-
per are two-fold: (1) We propose a general frame-
work that employs statistical models and maximizes
a linear function of log-likelihood ratios to retrieve
rumorous tweets that match a more general query.
(2) We show the effectiveness of the proposed fea-
ture in capturing tweets that show user endorsement.
This will help us identify disinformers or users that
spread false information in online social media.
Our work has resulted in a manually annotated
dataset of 10,000 tweets from 5 different controver-
sial topics. To the knowledge of authors this is the
first large-scale publicly available rumor dataset, and
can open many new dimensions in studying the ef-
fects of misinformation or other aspects of informa-
tion diffusion in online social media.
In this paper we effectively retrieve instances of
rumors that are already identified and evaluated by
an external source such as About.com?s Urban Leg-
ends reference. Identifying new emergent rumors
directly from the Twitter data is a more challenging
task. As our future work, we aim to build a sys-
tem that employs our findings in this paper and the
emergent patterns in the re-tweet network topology
to identify whether a new trending topic is a rumor
or not.
8 Acknowledgments
The authors would like to thank Paul Resnick,
Rahul Sami, and Brendan Nyhan for helpful discus-
sions. This work is supported by the National Sci-
ence Foundation grant ?SoCS: Assessing Informa-
tion Credibility Without Authoritative Sources? as
IIS-0968489. Any opinions, findings, and conclu-
sions or recommendations expressed in this paper
are those of the authors and do not necessarily re-
flect the views of the supporters.
References
Floyd H. Allport and Milton Lepkin. 1945. Wartime ru-
mors of waste and special privilege: why some people
believe them. Journal of Abnormal and Social Psy-
chology, 40(1):3 ? 36.
Gordon Allport and Leo Postman. 1947. The psychology
of rumor. Holt, Rinehart, and Winston, New York.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In ICML ?07,
pages 33?40.
James Berger. 1985. Statistical decision theory and
Bayesian Analysis (2nd ed.). New York: Springer-
Verlag.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Bernhard
Pfahringer, Geoff Holmes, and Achim Hoffmann, edi-
tors, Discovery Science, volume 6332 of Lecture Notes
in Computer Science, pages 1?15. Springer Berlin /
Heidelberg.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical
language modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip, and urban legend. Diogenes, 54:19?35, Febru-
ary.
Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics, 23(1):47?62.
Rob Ennals, Dan Byler, John Mark Agosta, and Barbara
Rosario. 2010. What is disputed on the web? In Pro-
ceedings of the 4th workshop on Information Credibil-
ity, WICOW ?10, pages 67?74.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In ACL ?07.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM),
Boulder, CO, USA.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Edward S Herman and Noam Chomsky. 2002. Manu-
facturing Consent: The Political Economy of the Mass
Media. Pantheon.
Courtenay Honeycutt and Susan C. Herring. 2009. Be-
yond microblogging: Conversation and collaboration
1598
via twitter. Hawaii International Conference on Sys-
tem Sciences, 0:1?10.
Jeff Huang, Katherine M. Thornton, and Efthimis N.
Efthimiadis. 2010. Conversational tagging in twitter.
In Proceedings of the 21st ACM conference on Hyper-
text and hypermedia, HT ?10, pages 173?178.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ?09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497?506.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.
Twitter under crisis: Can we trust what we rt?
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL?04, Morristown,
NJ, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of confer-
ence on Empirical methods in natural language pro-
cessing, EMNLP?02, pages 79?86.
Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno
Gonc?alves, Snehal Patil, Alessandro Flammini, and
Filippo Menczer. 2010. Detecting and tracking
the spread of astroturf memes in microblog streams.
CoRR, abs/1011.3768.
1599
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 59?70, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Detecting Subgroups in Online Discussions by Modeling Positive and
Negative Relations among Participants
Ahmed Hassan
Microsoft Research
Redmond, WA
hassanam@microsoft.com
Amjad Abu-Jbara
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Dragomir Radev
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
A mixture of positive (friendly) and nega-
tive (antagonistic) relations exist among users
in most social media applications. However,
many such applications do not allow users to
explicitly express the polarity of their interac-
tions. As a result most research has either ig-
nored negative links or was limited to the few
domains where such relations are explicitly
expressed (e.g. Epinions trust/distrust). We
study text exchanged between users in online
communities. We find that the polarity of the
links between users can be predicted with high
accuracy given the text they exchange. This
allows us to build a signed network represen-
tation of discussions; where every edge has
a sign: positive to denote a friendly relation,
or negative to denote an antagonistic relation.
We also connect our analysis to social psy-
chology theories of balance. We show that the
automatically predicted networks are consis-
tent with those theories. Inspired by that, we
present a technique for identifying subgroups
in discussions by partitioning singed networks
representing them.
1 Introduction
Most online communities involve a mixture of pos-
itive and negative relations between users. Positive
relations may indicate friendship, agreement, or ap-
proval. Negative relations usually indicate antago-
nism, opposition, or disagreement.
Most of the research on relations in social media
applications has almost exclusively focused on pos-
itive links between individuals (e.g. friends, fans,
followers, etc.). We think that one of the main rea-
sons, of why the interplay of positive and negative
links did not receive enough attention, is the lack of
a notion for explicitly expressing negative interac-
tions. Recently, this problem has received increas-
ing attention. However, all studies have been limited
to a handful of datasets from applications that allow
users to explicitly label relations as either positive or
negative (e.g. trust/distrust on Epinion (Leskovec et
al., 2010b) and friends/foes on Slashdot (Kunegis et
al., 2009)).
Predicting positive/negative relations between
discussants is related to another well studied prob-
lem, namely debate stance recognition. The ob-
jective of this problem is to identify which partic-
ipants are supporting and which are opposing the
topic being discussed. This line of work does not
pay enough attention to the relations between par-
ticipants, rather it focuses on participant?s stance to-
ward the topic. It also assumes that every partici-
pant either supports or opposes the topic being dis-
cussed. This is a simplistic view that ignore the
nature of complex topics that has many aspects in-
volved which may result in more than two subgroups
with different opinions.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
lying signed social structure in online communities.
We present a method for identifying user attitude
and for automatically constructing a signed social
network representation of discussions. We apply
the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset. We also conduct a large scale
evaluation by showing that predicted links are con-
sistent with the principals of social psychology the-
ories, namely the Structural Balance Theory (Hei-
der, 1946). The balance theory has been shown to
hold both theoretically (Heider, 1946) and empiri-
cally (Leskovec et al2010c) for a variety of social
community settings. Finally, we present a method
for identifying subgroups in online discussions by
identifying groups with high density of intra-group
positive relations and high density of inter-group
negative relations. This method is capable of identi-
fying subgroups even if the community splits into
more than two subgroups which is more general
than stance recognition which assumes that only two
groups exist.
59
B 
C 
D 
A E 
F G 
I H 
Positive Negative 
Source Target Sign Evidence from Text A E - I have to disagree with what you are saying.  G A - You are missing the entire point, he is putting lives at risk. D I - and you manufacture lies for what reason? E G + you have explained your position very well. C H + I am neutral on this, but I agree with your assessment! 
Figure 1: An example showing a signed social network
along with evidence from text that justifies edge signs.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating positive or negative affinity between two
individuals.
Figure 1 shows a signed network representation
for a subset of posts from a long discussion thread.
The thread discussed the November 2010 Wikileaks
cable release. We notice that participants split into
two groups, one supporting and one opposing the
leak. We also notice that most negative edges are
between groups, and most positive edges are within
groups. It is worth mentioning that networks gen-
erated from larger datasets (i.e. with thousands of
posts) have much more noise compared to this ex-
ample.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a method for
identifying subgroups in online discussions in Sec-
tion 3.3. We conclude in Section 6.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one indi-
vidual toward another makes our work related to a
huge body of work on sentiment analysis. One such
line of research is the well-studied problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003; Kim and Hovy, 2004; Takamura et al2005).
Subjectivity analysis is yet another research line that
is closely related to our general goal of mining at-
titude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea et
al., 2008; Riloff and Wiebe, 2003). Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Stance Classification
Perhaps the closest work to this paper is the work on
stance classification. We notice that most of these
methods focus on the polarity of the written text as-
suming that anyone using positive text belongs to
one group and anyone using negative text belongs
to another. This works well for single-aspect topics
or entities like the ones used in (Tan et al2011)
(e.g. Obama, Sara Palin, Lakers, etc.). In this sim-
ple notion of topics, it is safe to assume that text
polarity is a good enough discriminator. This unfor-
tunately is not the case in online discussions about
complex topics having many aspects (e.g. abortion,
health care, etc.). In such complex topics, people use
positive and negative text targeting different aspects
of the topic, for example in the health care bill topic,
discussants expressed their opinion regarding many
aspects including: the enlarged coverage, the insur-
ance premiums, Obama, socialism, etc. This shows
that simply looking at text polarity is not enough to
identify groups.
Tan et al2011) studied how twitter following re-
lations can be used to improve stance classification.
Their main hypothesis is that connected users are
more likely to hold similar opinions. This may be
correct for the twitter following relations, but it is
not necessarily correct for open discussions where
60
no such relations exist. The only criterion that can be
used to connect discussants is how often they reply
to each other?s posts. We will show later that while
many people reply to people with similar opinions,
many others reply to people with different opinions
as well.
Thomas et al2006) address the same problem
of determining support and opposition as applied to
congressional floor-debates. They assess the agree-
ment/disagreement between different speakers by
training a text classifier and applying it to a win-
dow surrounding the names of other speakers. They
construct their training data by assuming that if two
speaker have the same vote, then every reference
connecting them is an agreement and vice versa.
We believe this will result in a very noisy train-
ing/testing set and hence we decided to recruit hu-
man annotators to create a training set. We found
out that many instances with references to other
discussants were labeled as neither agreement nor
disagreement regardless of whether the discussants
have similar or opposing positions. We will use this
system as a baseline and will show that the exis-
tence of positive/negative words close to a person
name does not necessarily show agreement or dis-
agreement with that person.
Hassan et al2010) use a language model based
approach for identifying agreement and disagree-
ment sentences in discussions. This work is limited
to sentences. It does not consider the overall rela-
tion between participants. It also does not consider
subgroup detection. We will use this method as a
baseline for one of our components and will show
that the proposed method outperforms it.
Murakami and Raymond (2010) present another
method for stance recognition. They use a small
number of hand crafted rules to identify agreement
and disagreement interactions. Hand crafted rules
usually result in systems with very low recall caus-
ing them to miss many agreement/disagreement in-
stances (they report 0.26 recall at the 0.56 preci-
sion level). We present a machine learning system
to solve this problem and achieve much better per-
formance. Park et al2011) propose a method for
finding news articles with different views on con-
tentious issues. Mohit et al2008) present a set
of heuristics for including disagreement informa-
tion in a minimum cut stance classification frame-
work. Galley et al2004) show the value of us-
ing durational and structural features for identify-
ing agreement and disagreement in spoken conver-
sational speech. They use features like duration of
spurts, speech rate, speaker overlap, etc. which are
not applicable to written language.
Our approach is different from agree-
ment/disagreement identification because we
not only study sentiment at the local sentiment
level but also at the global level that takes into
consideration many posts exchanged between
participants to build a signed network representation
of the discussion. Research on debate stance
recognition attempts to perform classification under
the ?supporting vs. opposing? paradigm. However
such simple view might not always be accurate
for discussions on more complex topics with
many aspects. After building the signed network
representation of discussions, we present a method
that can detect how the large group could split into
many subgroups (not necessarily two) with coherent
opinions.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities. Extracting so-
cial power relations from natural language (i.e. who
influences whom) has been studied in (Bramsen et
al., 2011; Danescu-Niculescu-Mizil et al2011).
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks with both
positive and negative links from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al2009) analyze user relationships in
61
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al2010b) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al
2010a).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
The research presented in this paper extends this
previous work in a number of ways: (i) we present
a method based on linguistic analysis that finds in-
stances of showing positive or negative attitude be-
tween participants (ii) we propose a technique for
representing discussions as signed networks where a
sign is associated with every edge to denote whether
the relation is friendly or antagonistic (iii) we eval-
uate the proposed methods using human annotated
data and also conduct a large scale evaluation based
on social psychology theories; (iv) finally we present
a method for identifying subgroups that globally
splits the community involved in the discussion by
utilizing the dynamics of the local interactions be-
tween participants.
3 Approach
3.1 Identifying Attitude from Text
To build a signed network representation of discus-
sants, we start by trying to identify sentences that
show positive or negative attitude from the writer to
the addressee. The first step toward identifying at-
titude is to identify words with positive/negative se-
mantic orientation. The semantic orientation or po-
larity of a word indicates the direction the word devi-
ates from the norm (Lehrer, 1974). We use Opinion-
Finder (Wilson et al2005a) to identify words with
positive or negative semantic orientation. The polar-
ity of a word is also affected by the context where
the word appears. For example, a positive word that
appears in a negated context should have a negative
polarity. Other polarized words sometimes appear as
neutral words in some contexts. To identify contex-
tual polarity of words, a large set of features is used
including words, sentences, structure, and other fea-
tures similar to the method described in (Wilson et
al., 2005b).
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which do not.
Text polarity alone cannot be used to identify at-
titude between participants. Sentences that show
an attitude are different from subjective sentences.
Subjective sentences are sentences used to express
opinions, evaluations, and speculations (Riloff and
Wiebe, 2003). While every sentence that shows an
attitude is a subjective sentence, not every subjective
sentence shows an attitude toward the recipient.
In this method, we address the problem of iden-
tifying sentences with attitude as a relation detec-
tion problem in a supervised learning setting. We
study sentences that has mentions to the addressee
and polarized expressions (negative/positive words
or phrases). Mentions could either be names of other
participants or second person pronouns (you, your,
yours) used in text posted as a reply to another par-
ticipant. Reply structure (i.e. who replies to whom)
is readily available in many discussion forums. In
cases where reply structure is not available, we can
use a method like the one in (Lin et al2009) to re-
cover it.
We predict whether the mention is related to the
polarized expression or not. We regard the mention
and the polarized expression as two entities and try
to learn a classifier that predicts whether the two en-
tities are related or not.
The text connecting the two entities offers a very
condensed representation of the information needed
to assess whether they are related or not. For ex-
ample the two sentences ?you are completely un-
qualified? and ?you know what, he is unqualified ...?
show two different ways the words ?you?, and ?un-
qualified? could appear in a sentence. In the first
case the polarized word ?unqualified? refers to the
word ?you?. In the second case, the two words are
not related. The information in the shortest path
between two entities in a dependency tree can be
used to assert whether a relationship exists between
them (Bunescu and Mooney, 2005).
The sequence of words connecting the two enti-
ties is a very good predictor of whether they are re-
lated or not. However, these paths are completely
62
lexicalized and consequently their performance will
be limited by data sparseness. To alleviate this prob-
lem, we use higher levels of generalization to rep-
resent the path connecting the two tokens. These
representations are the part-of-speech tags, and the
shortest path in a dependency graph connecting the
two tokens. We represent every sentence with sev-
eral representations at different levels of generaliza-
tion. For example, the sentence ?your ideas are very
inspiring? will be represented using lexical, polar-
ity, part-of-speech, and dependency information as
follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.2 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nent we described in the previous subsection. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edgeA? B, ifA replies toB?s posts at least
n times in m different threads. We set m, and n to
2 in all of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure. Alternatively, as
mentioned earlier, we can use a method similar to
the one presented in (Lin et al2009) to recover the
reply structure if it is not readily available.
Once we build the network, we move to the more
challenging task in which we associate a sign with
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Domain (e.g. politics, science, etc.)
Table 1: Features used by the Interaction Sign Classifier.
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts betweenA andB and clas-
sify the interaction according to the plurality value.
We will show later, in our experiments section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem as a
classical supervised learning problem. We came up
with a set of features that we think are good predic-
tors of the interaction sign, and we trained a classi-
fier using those features on a labeled dataset. Our
features include numbers and percentages of pos-
itive/negative sentences per post, posts per thread,
and so on. A sentence is labeled as positive/negative
if a relation has been detected in this sentence be-
tween a mention referring to the addressee and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
is related to the interactions between A and B. The
features are summarized in Table 1.
3.3 Sub-Group Detection
In any discussion, different subgroups may emerge.
Members of every subgroup usually have a common
focus (positive or negative) toward the topic being
discussed. Each member of a group is more likely
to show positive attitude to members of the same
group, and negative attitude to members of opposing
groups. The signed network representation could
prove to be very useful for identifying those sub-
groups. To detect subgroups in a discussion thread,
63
we would like to partition the corresponding signed
network such that positive intra-group links and neg-
ative inter-group links are dense.
This problem is related to the constrained cluster-
ing (Wagstaff et al2001) and the correlation clus-
tering problem (Bansal et al2004). In constrained
clustering, a pairwise similarity metric (which is
not available in our domain), and a set of must-
link/cannot-link constraints are used with a standard
data clustering algorithm. Correlation clustering op-
erates in a scenario where given a signed graph
G = (V,E) where the edge label indicates whether
two nodes are similar (+) or different (-), the task
is to cluster the vertices so that similar objects are
grouped together. Bansal et. al (2004) proved NP-
hardness and gave constant-factor approximation al-
gorithms for the special case in which the graph
is complete (full information) and every edge has
weight +1 or -1 which is not the case in our network.
Alternatively, we can use a greedy optimization al-
gorithm to find partitions. A criterion function for
a local optimization partitioning procedure is con-
structed such that positive links are dense within
groups and negative links are dense between groups.
For any potential partition C, we seek to optimize
the following function: P (C) = ?
?
n +(1??)
?
p
where
?
n is the number of negative links between
nodes in the same subgroup,
?
p is the number of
positive links between nodes in different subgroups,
and ? is a trade factor that represents the importance
of the two terms. We set ? to 0.5 in all our experi-
ments.
Clusters are selected such that: C? =
argminP (C). A greedy optimization framework
is used to minimize P (C). Initially, nodes are ran-
domly partitioned into t different clusters and the
criterion function P is evaluated for that cluster. Ev-
ery cluster has a set of neighbors in the cluster space.
A neighbor cluster is obtained by moving one node
from one cluster to another, or by exchanging two
nodes in two different clusters. Neighbor partitions
are evaluated, and if one with a lower value for the
criterion function is found, it is set as the current
partition. This greedy procedure is repeated with
random restarts until a minimal solution is found.
To determine the number of subgroups t, we select
t that minimizes the optimization function P (C). In
all experiments we used an upper limit of t = 5.
This technique was able to identify the correct num-
ber of subgroups in 77% of the times. In the rest of
the cases, the number was different from the correct
number by at most 1 except for a single case where
it was 2.
4 Data
4.1 Signed Network Extraction
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 topics (threads) and 1.2M
posts from the period between the end of 2008 and
the end of 2010. All threads were in English and had
5 posts or more. They covered 11 different domains
including: politics, religion, science, etc. The aver-
age number of participants per domain is 1320 and
per topic is 52. The data was tokenized, sentence-
split, and part-of-speech tagged with the OpenNLP
toolkit. It was parsed with the Stanford parser (Klein
and Manning, 2003).
We randomly selected around 5300 posts (1000
interactions), and asked human annotators to label
them. Our annotators were instructed to read all the
posts exchanged between two participants and de-
cide whether the interaction between them is posi-
tive or negative. We used Amazon Mechanical Turk
for annotations. Following previous work (Callison-
Burch, 2009; Akkaya et al2010), we took sev-
eral precautions to maintain data integrity. We re-
stricted annotators to those based in the US to main-
tain an acceptable level of English fluency. We also
restricted annotators to those who have more than
95% approval rate for all previous work. Moreover,
we asked three different annotators to label every in-
teraction. The label was computed by taking the ma-
jority vote among the three annotators. We refer to
this data as the Interactions Dataset.
We ran a different annotation task where we se-
lected sentences including mentions referring to dis-
cussants (names or pronouns) and polarized expres-
sions. Annotators were asked to select sentences
where the polarized attribute is referring to the men-
tion and hence show a positive or negative attitude
toward other discussion participants. This resulted
in a set of 5000 manually annotated sentences. We
refer to this data as the Sentences Dataset.
We asked three different annotators to label ev-
ery instance. The kappa measure between the three
groups of annotations was 0.62 for the Interactions
Dataset and 0.64 for the Sentences Dataset. To bet-
ter assess the quality of the annotations, we asked a
trained annotator to label 10% of the data. We mea-
sured the agreement between the expert annotator
64
Logistic Reg.
Class Pos. Neg. Weigh. Avg.
Precision 0.848 0.724 0.809
Recall 0.884 0.657 0.812
F-Measure 0.866 0.689 0.81
Accuracy - - 0.812
SVM
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier performance.
Classifier Random Thresh-Num Thresh-Perc. SVM
Accuracy 65% 69% 71% 83.5%
Table 3: A comparison of different sign interaction clas-
sifiers.
and the majority label from Mechanical Turk. The
kappa measure was 0.69 for the Interactions Dataset
and 0.67 for the Sentences Dataset.
4.2 Sub-group Detection
We used a dataset of more than 42 topics and ap-
proximately 9000 posts collected from two political
forums (Createdebate1 and Politicalforum2). The fo-
rum administrators ran a poll asking participants to
select their stance from a set of possible answers
and hence the dataset was self-labeled with respect
to groups. We also used a set of discussions from
the Wikipedia discussion section. When a topic on
Wikipedia is disputed, the editors of that topic start a
discussion about it. We collected 117 Wikipedia dis-
cussion threads. The threads contain a total of 1,867
posts. The discussions were annotated by an expert
annotator (a professor in sociolinguistics, not an au-
thor of the paper) who was instructed to read each
of the Wikipedia discussion threads in its entirety
and determine whether the discussants split into sub-
groups, in which case he was asked to identify the
subgroup membership for each discussant. In to-
tal, we had 159 topics with an average of approxi-
mately 500 posts, 60 participants and 2.7 subgroups
per topic. Examples of the topics include: Arizona
immigration law, airport security, oil spill, evolution,
Ireland partitions, abortion and many others.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with the attitude detection classifiers de-
scribed in Section 3.1 using the Sentences Dataset.
1www.createdebate.com
2www.politicalforum.com
We also trained and tested the interaction sign clas-
sifier described in Section 3.2 using the Interactions
Dataset. We built one signed social network for ev-
ery domain (e.g. politics, economics, etc.). We de-
cided to build a network for every domain as op-
posed to one single network because the relation be-
tween any two individuals may vary across domains
(e.g. politics vs. science). In the rest of this section,
we will describe the experiments we did to assess the
performance of the sentences with attitude detection
and interaction sign prediction steps.
In addition to classical evaluation, we evaluate
our results using the structural balance theory which
has been shown to hold both theoretically (Heider,
1946) and empirically (Leskovec et al2010c). We
validate our results by showing that the automati-
cally extracted networks mostly agree with the the-
ory. We evaluated the approach using the structural
balance theory because it presents a global (pertain-
ing to relations between multiple edges) and large-
scale (used millions of posts and thousands of users)
evaluation of the results as opposed to traditional
evaluation which is local in nature (only considers
one edge at a time) and smaller in scale (used thou-
sands of posts).
5.1 Identifying Sentences with Attitude
We compare the proposed methods to two baselines.
The first baseline is based on the work of (Thomas
et al2006). We used the speaker agreement com-
ponent presented in (Thomas et al2006) as a base-
line. The speaker agreement component is one step
in their approach. In this component, they used
an SVM classifier trained using a window of text
surrounding references to other speakers to predict
agreement/disagreement between speakers.
We build an SVM text classifier trained on the
sentence at which the mention referring to the other
participant occurred. We refer to this baseline as
the Text Classification approach. The second base-
lines adopts the language model approach presented
in (Hassan et al2010). Two language models
are trained using a stream of words, part-of-speech
tags, and dependency relations, one for sentences
that show an attitude and one for sentences that do
not. New sentences are classified based on gener-
ation likelihoods. We refer to this baseline as the
Language Models approach.
We tested this component using the Sentences
Dataset described in Section 4. We compared the
performance of the proposed method and the two
65
Extracted Networks Random Networks
Domain (+++) (++?) (+??) (???) (+++) (++?) (+??) (???)
abortion 51.67 26.31 18.92 0.48 35.39 43.92 18.16 2.52
current-events 67.36 22.26 8.76 0.23 54.08 36.90 8.39 0.64
off-topic-chat 65.28 23.54 9.45 0.25 58.07 34.59 6.88 0.46
economics 72.68 18.30 7.77 0.00 66.50 29.09 4.22 0.20
political opinions 60.60 24.24 12.81 0.43 45.97 40.79 12.06 1.19
environment 47.46 32.54 17.26 0.30 37.38 43.61 16.89 2.12
latest world news 58.29 22.41 16.33 0.62 42.26 42.20 13.98 1.56
religion 47.17 25.89 22.56 1.42 39.68 42.94 15.51 1.87
science-technology 57.53 26.03 14.33 0.00 50.14 38.93 10.05 0.87
terrorism 64.96 23.36 9.46 0.73 41.54 42.42 14.36 1.68
Table 4: Percentage of different types of triangles in the extracted networks vs. the random networks.
Method Accuracy Precision Recall F1
Text Classification 60.4 61.1 60.2 60.6
Language Models 80.3 81.0 79.4 80.2
Relation Extraction 82.3 82.3 82.3 82.3
Table 5: Comparison of attitude identification methods.
baselines. Table 5 compares the precision, recall,
F1, and accuracy for the three methods. The text
classification based approach does much worse than
others. The reasons is that it ignores the structure
and uses much less information (part-of-speech tags
and dependency trees are not used) compared to the
other methods. Additionally, the short length of the
sentences compared to what is typical in text clas-
sification may have had a bad effect on the perfor-
mance. Both other models try to learn the char-
acteristics of the path connecting the mention and
the polarized expression. We notice that optimizing
the weights for unigram and bigrams features using
SVM results in a better performance compared to
language models because it does not have the con-
straints imposed by the former model on the learned
weights.
We evaluated the importance of the feature types
(i.e. dependency vs. pos tags vs words) by measur-
ing the chi-squared statistic for every feature with
respect to the class. Dependency features were most
helpful, but other types of features helped improve
the performance as well.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.1 to find sentences with positive and nega-
tive attitude. The output of this classifier was used to
compute the features described in Section 3.2, which
were used to train a classifier that predicts the sign
of an interaction between any two individuals.
We used both Support Vector Machines (SVM)
and logistic regression to train the sign interaction
classifier. We report several performance metrics for
them in Table 2. We notice that the SVM classifier
performs better with an accuracy of 83.5% and an
F-measure of 81%. All results were computed using
10 fold cross validation on the labeled data. To bet-
ter assess the performance of the proposed classifier,
we compare it to a baseline that labels the relation as
negative if the percentage of negative sentences ex-
ceeds a particular threshold, otherwise it is labeled
as positive. The thresholds were empirically esti-
mated using a separate development set. The accu-
racy of this baseline is only 71%.
To better assess the performance of the proposed
classifier, we compare it to three baselines. The first
is a random baseline that predicts an interaction as
positive with probability p that equals the proportion
of positive instances to all instances in the training
set. The second classifier (Thresh-Num) labels the
edge as negative if the number of negative instances
exceeds a threshold Tn. The third classifier (Thresh-
Perc) labels the edge as negative if the percentage of
negative instances to all instances exceeds a thresh-
old Tp. The cutoff thresholds were estimated using
a separate development set.
The 3 baselines were tested using the entire la-
beled dataset. The SVM classifier was tested using
10 fold cross validation. The accuracy of the ran-
dom classifier, the two based on a cut off number
and percentage , and the SVM classifier are shown
in Table 3. We notice that the random classifier per-
forms worst, and the classifier based on percentage
cutoff outperforms the one based on number cut-
off. The SVM classifier significantly outperforms all
other classifiers. We tried to train a classifier using
both the number and percentage of negative and pos-
itive posts. The improvement over using the baseline
using the percentage of negative posts was not sta-
tistically significant.
We evaluated the importance of the features listed
66
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on counts. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion domain.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al2010c). In this section, we study the agree-
ment between the theory and our automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in
a graph theoretic form by (Cartwright and Harary,
1956). The theory is based on the principles that ?the
friend of my friend is my friend?, ?the enemy of my
friend is my enemy?, ?the friend of my enemy is
my enemy?, and variations on these. The structural
balance theory states that triangles that have an odd
number of positive signs (+ + + and + - -) are bal-
anced, while triangles that have an even number of
positive signs (- - - and + + -) are not.
In this section, we compare the predictions of
edge signs made by our system to the structural bal-
ance theory by counting the frequencies of differ-
ent types of triangles in the predicted network. Ta-
ble 4 shows the frequency of every type of trian-
gle for 10 different domains. To better understand
these numbers, we compare them to the frequencies
of triangles in a set of random networks. We shuf-
fle the signs for all edges on every network keeping
the fractions of positive and negative edges constant.
We repeat shuffling for 1000 times and report the av-
erage.
We find that the all-positive triangle (+ + +) is
overrepresented in the generated network compared
to chance across all domains. We also see that the
triangle with two positive edges (+ + ?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all domains. The tri-
angle with a single positive edge is slightly over-
represented in most but not all of the topics com-
pared to chance. This shows that the predicted net-
works mostly agree with the structural balance the-
ory. The slightly non standard behavior of the tri-
angle with one positive edge could be explained in
light of the weak balance theory. In this theory,
Davis (1967) states that this triangle, which corre-
sponds to the ?enemy of enemy is my friend? propo-
sition, holds only if the network can be partitioned
into exactly two subsets, but not when there are more
than two. In general, the percentage of balanced tri-
angles in the predicted networks is higher than in
the shuffled networks, and hence the balanced trian-
gles are significantly overrepresented compared to
chance showing that our automatically constructed
network is similar to explicit signed networks in that
they both mostly agree with the balance theory.
5.4 Sub-Group Detection
We compare the performance of the sub-group de-
tection method to three baselines. The first base-
line uses graph clustering (GC) to partition a net-
work based on the frequency of interaction between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of posts exchanged. The second base-
line (TC) is based on the premise that participants
with similar text are more likely to belong to the
same subgroup. We measure text similarity by com-
puting the cosine similarity between the tf-idf rep-
resentations of the text in a high dimensional vec-
tor space. We tried two methods for partitioning
those graphs: spectral partitioning (Luxburg, 2007)
and a hierarchical agglomeration algorithm which
works by greedily optimizing the modularity for
graphs (Clauset et al2004). The third baseline is
based on stance classification approaches (e.g. (Tan
et al2011)). In this baseline we put all the partic-
ipants who use more positive text in one subgroup
and the participants who use more negative text in
another subgroup. Text polarity is identified using
the method described in Section 3.1.
Table 6 shows the average purity (Purity), entropy
(Entropy), Normalizes Mutual Information (NMI),
and Rand Index (RandIndex) values of the method
based on signed networks and the baselines using
different partitioning algorithms. The differences in
the results shown in the table are statistically sig-
nificant at the 0.05 level (as indicated by a 2-tailed
67
Figure 2: A signed network representing participants in a discussion about the ?Health Care Reform Bill?. Blue (dark)
nodes represent participants with the bill, Yellow (light) nodes represent participants against the bill, red (solid) edges
represent negative attitude, while green (dashed) edges represent positive attitude.
Createdebate Politicalforum Wikipedia
Method Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex Purity Entropy NMI RandIndex
GC - Spectral 0.50 0.85 0.28 0.40 0.50 0.88 0.27 0.39 0.49 0.89 0.33 0.35
GC - Hierarchical 0.48 0.86 0.30 0.41 0.47 0.89 0.31 0.40 0.49 0.87 0.38 0.39
TC - Spectral 0.50 0.85 0.31 0.43 0.48 0.90 0.30 0.45 0.51 0.87 0.40 0.46
TC - Hierarchical 0.49 0.90 0.35 0.46 0.48 0.91 0.33 0.49 0.53 0.80 0.40 0.49
Text Polarity 0.55 0.80 0.38 0.49 0.54 0.91 0.31 0.38 0.34 0.95 0.30 0.40
Signed Networks 0.64 0.74 0.46 0.59 0.58 0.80 0.43 0.55 0.65 0.54 0.51 0.60
Table 6: Comparison of the sub-group detection method to baseline systems
paired t-test).
We notice that partitioning the signed network
that was automatically extracted from text results in
significantly better partitions on the three datasets as
indicated by the higher Purity, NMI, and RandIndex
and the lower Entropy values it achieves. We believe
that the first two baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well. The baseline that
classifies the stance of discussants based on the po-
larity of their text performed bad too because it over-
looks the fact that most of the discussed topics in our
datasets have multiple aspects and a discussant may
use both positive and negative text targeting differ-
ent aspects of the topic. An example of a signed net-
work and the corresponding subgtoups as extracted
from real data is showm in Figure 2.
6 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
Acknowledgments
This research was funded in part by the Office of the
Director of National Intelligence, Intelligence Ad-
vanced Research Projects Activity. All statements
of fact, opinion or conclusions contained herein are
those of the authors and should not be construed as
representing the official views or policies of IARPA,
the ODNI or the U.S. Government
68
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, CSLDAMT ?10, pages 195?203.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. 2004.
Correlation Clustering. Machine Learning, 56(1):89?
113.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label dis-
agreement in the min-cut classification framework. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 773?782.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In Proceeding of the twenty-sixth annual SIGCHI
conference on Human factors in computing systems,
pages 817?820, New York, NY, USA.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, HLT ?05, pages 724?731,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 1 - Volume 1, EMNLP ?09, pages 286?295.
Dorwin Cartwright and Frank Harary. 1956. Structure
balance: A generalization of heiders theory. Psych.
Rev., 63.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, Bo Pang,
and Jon M. Kleinberg. 2011. Echoes of power: Lan-
guage effects and power differences in social interac-
tion. CoRR.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations, 20:181?187.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden, July.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech: use
of bayesian networks to model pragmatic dependen-
cies. In Proceedings of the 42nd Annual Meeting on
Association for Computational Linguistics, ACL ?04,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In Proceedings of the In-
ternational Network of Social Network Analysis (IN-
SNA), St. Pete Beach, Florida.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology, 21:107?112.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03, pages 423?430.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a so-
cial network with negative edges. In Proceedings of
the 18th international conference on World wide web,
pages 741?750, New York, NY, USA.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In Proceedings of the 19th interna-
tional conference on World wide web, pages 641?650,
New York, NY, USA.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
69
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In CHI 2010,
pages 1361?1370, New York, NY, USA. ACM.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
Akiko Murakami and Rudy Raymond. 2010. Support or
oppose?: classifying positions in online debates from
reply activities and opinion expressions. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics: Posters, pages 869?875.
Souneil Park, KyungSoon Lee, and Junehwa Song. 2011.
Contrasting opposing views of news articles on con-
tentious issues. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, pages
340?349.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming
Zhou, and Ping Li. 2011. User-level sentiment anal-
ysis incorporating social networks. In Proceedings
of the 17th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?11,
pages 1397?1405.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Kiri Wagstaff, Claire Cardie, Seth Rogers, and Stefan
Schro?dl. 2001. Constrained k-means clustering with
background knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learning,
pages 577?584.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10):1333?1348.
70
A Random Walk?Based Model for
Identifying Semantic Orientation
Ahmed Hassan?
Microsoft Research
Amjad Abu-Jbara??
University of Michigan
Wanchen Lu?
University of Michigan
Dragomir Radev?
University of Michigan
Automatically identifying the sentiment polarity of words is a very important task that has
been used as the essential building block of many natural language processing systems such as
text classification, text filtering, product review analysis, survey response analysis, and on-line
discussion mining. We propose a method for identifying the sentiment polarity of words that
applies a Markov random walk model to a large word relatedness graph, and produces a polarity
estimate for any given word. The model can accurately and quickly assign a polarity sign and
magnitude to any word. It can be used both in a semi-supervised setting where a training set of
labeled words is used, and in a weakly supervised setting where only a handful of seed words is
used to define the two polarity classes. The method is experimentally tested using a gold standard
set of positive and negative words from the General Inquirer lexicon. We also show how our
method can be used for three-way classification which identifies neutral words in addition to
positive and negative words. Our experiments show that the proposed method outperforms the
state-of-the-art methods in the semi-supervised setting and is comparable to the best reported
values in the weakly supervised setting. In addition, the proposed method is faster and does not
need a large corpus. We also present extensions of our methods for identifying the polarity of
foreign words and out-of-vocabulary words.
? Microsoft Research, Redmond, WA, USA. E-mail: hassanam@microsoft.com. This research was
performed while at the University of Michigan.
?? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: amjbara@umich.edu.
? Department of Electrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, USA.
E-mail: wanchlu@umich.edu.
? Department of Electrical Engineering & Computer Science and School of Information, University of
Michigan, Ann Arbor, MI, USA. E-mail: radev@umich.edu.
Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication:
14 July 2013.
doi:10.1162/COLI a 00192
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Identifying emotions and attitudes from unstructured text has a variety of possible
applications. For example, there has been a large body of work for mining product
reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002)
have shown how product reputation mining helps with marketing and customer re-
lation management. The Google products catalog and many on-line shopping sites
like Amazon.com provide customers not only with comprehensive information and
reviews about a product, but also with faceted sentiment summaries. Such systems are
all supported by a sentiment lexicon, some even in multiple languages.
Another interesting application is mining on-line discussions. An enormous num-
ber of discussion groups exist on the Web. Millions of users post content to these groups
covering pretty much every possible topic. Tracking a participant attitude toward differ-
ent topics and toward other participants is a very important task that makes use of sen-
timent lexicons. For example, Tong (2001) presented the concept of sentiment timelines.
His system classifies discussion posts about movies as either positive or negative. This
is used to produce a plot of the number of positive and negative sentiment messages
over time. All these applications would benefit from an automatic way of identifying
semantic orientation of words.
In this article, we study the task of automatically identifying the semantic orienta-
tion of any word by analyzing its relations to other words, Automatically classifying
words as positive, negative, or neutral enables us to automatically identify the polarity
of larger pieces of text. This could be a very useful building block for systems that
mine surveys, product reviews, and on-line discussions. We apply a Markov random
walk model to a large semantic relatedness graph, producing a polarity estimate for
any given word. Previous work on identifying the semantic orientation of words has
addressed the problem as both a semi-supervised (Takamura, Inui, and Okumura 2005)
and a weakly supervised (Turney and Littman 2003) learning problem. In the semi-
supervised setting, a training set of labeled words is used to train the model. In the
weakly supervised setting, only a handful of seeds are used to define the two polarity
classes.
Our proposed method can be used both in a semi-supervised and in a weakly
supervised setting. Empirical experiments on a labeled set of positive and negative
words show that the proposed method outperforms the state-of-the-art methods in the
semi-supervised setting. The results in the weakly supervised setting are comparable to
the best reported values. The proposed method has the advantages that it is faster and
does not need a large training corpus.
The rest of the article is structured as follows. In Section 2, we review related work
on word polarity and subjectivity classification and note applications of the random
walk and hitting times framework. Section 3 presents our method for identifying word
polarity. We describe how the proposed method can be extended to cover foreign
languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes
our experimental set-up. We present our conclusions in Section 7.
2. Related Work
2.1 Identifying Word Polarity
Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word
polarity of adjectives. They extract all conjunctions of adjectives from a given corpus
540
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
and then they classify each conjunctive expression as either the same orientation such
as ?simple and well-received? or different orientation such as ?simplistic but well-
received.? The result is a graph that they cluster into two subsets of adjectives. They
classify the cluster with the higher average frequency as positive. They created and
labeled their own data set for experiments. Their approach works only with adjectives
because there is nothing wrong with conjunctions of nouns or verbs with opposite
polarities (?war and peace?, ?rise and fall?, etc.).
Turney and Littman (2003) identify word polarity by looking at its statistical asso-
ciation with a set of positive/negative seed words. They use two statistical measures
for estimating association: Pointwise Mutual Information (PMI) and Latent Semantic
Analysis (LSA). To get co-occurrence statistics, they submit several queries to a search
engine. Each query consists of the given word and one of the seed words. They use the
search engine NEAR operator to look for instances where the given word is physically
close to the seed word in the returned document. They present their method as an un-
supervised method where a very small number of seed words are used to define
semantic orientation rather than train the model. One of the limitations of their method
is that it requires a large corpus of text to achieve good performance. They use sev-
eral corpora; the size of the best performing data set is roughly one hundred billion
words (Turney and Littman 2003).
Takamura et al. (2005) propose using spin models for extracting semantic orienta-
tion of words. They construct a network of words using gloss definitions, thesaurus, and
co-occurrence statistics. They regard each word as an electron. Each electron has a spin
and each spin has a direction taking one of two values: up or down. Two neighboring
spins tend to have the same orientation from an energy point of view. Their hypothesis
is that as neighboring electrons tend to have the same spin direction, neighboring words
tend to have similar polarity. They pose the problem as an optimization problem and
use the mean field method to find the best solution. The analogy with electrons leads
them to assume that each word should be either positive or negative. This assumption
is not accurate because most of the words in the language do not have any semantic ori-
entation. They report that their method could get misled by noise in the gloss definition
and their computations sometimes get trapped in a local optimum because of its greedy
optimization flavor.
Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms
and then use the shortest paths between any given word and the words ?good? and
?bad? to determine word polarity. They report that using shortest paths could be very
noisy. For example, ?good? and ?bad? themselves are closely related in WordNet with
a 5-long sequence ?good, sound, heavy, big, bad.? A given word w may be more
connected to one set of words (e.g., positive words); yet have a shorter path connecting
it to one word in the other set. Restricting seed words to only two words affects their
accuracy. Adding more seed words could help but it will make their method extremely
costly from the computation point of view. They evaluate their method using only
adjectives.
Hu and Liu (2004) propose another method that uses WordNet. They use WordNet
synonyms and antonyms to predict the polarity of words. For any word whose polarity
is unknown, they search WordNet and a list of seed labeled words to predict its polarity.
They check if any of the synonyms of the given word has known polarity. If so, they
label it with the label of its synonym. Otherwise, they check if any of the antonyms
of the given word has known polarity. If so, they label it with the opposite label of
the antonym. They continue in a bootstrapping manner until they label all possible
words.
541
Computational Linguistics Volume 40, Number 3
2.2 Building Sentiment Lexicons
A number of other methods try to build lexicons of polarized words. Esuli and
Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses
of the word as found in some dictionary. Then, a binary text classifier is trained using
the textual representation and applied to new words.
Kim and Hovy (2004) start with two lists of positive and negative seed words. Word-
Net is used to expand these lists. Synonyms of positive words and antonyms of negative
words are considered positive, and synonyms of negative words and antonyms of posi-
tive words are considered negative. A similar method is presented in Andreevskaia and
Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively
expand a list of seeds. The sentiment classes are treated as fuzzy categories where some
words are very central to one category, whereas others may be interpreted differently.
Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that
overtly marked words such as dishonest, unhappy, and impure tend to have negative
semantic orientations whereas their unmarked counterparts (honest, happy, and pure)
tend to have positive semantic orientation. They use a set of 11 antonym-generating affix
patterns to generate overtly marked words and their counterparts from the Macquarie
Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the
sentiment lexicon using a Roget-like thesaurus. Their method does not require seed
sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of
the marking theory is language-dependent and cannot be applied from one language to
another.
Contrasting the dictionary based approaches that rely on resources such as Word-
Net, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons
semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic fea-
tures and context coherency (i.e., the tendency for same polarities to appear succes-
sively) to detect polar clauses.
2.3 Random Walk?Based Methods
Closest to our work in its methodology is probably the line of research on semi-
supervised graphical methods for sentiment classification. Rao and Ravichandran
(2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled
and labeled nodes, each node representing a word that can be either positive or neg-
ative, and each edge representing some semantic relatedness that can be constructed
using resources like WordNet or other thesaurus. They evaluate two semi-supervised
methods: Mincut (including its variant, Randomized Mincut) and label propagation.
The general idea of label propagation is defining a probability distribution over the
positive and negative classes for each node in the graph. A Markov random walk is
performed on the graph to recover this distribution for the unlabeled nodes.
Additionally, Rao and Ravichandra (2009) and Blair-Goldensohn et al. (2008) use a
similar label propagation method on a lexical graph built from WordNet, where a small
set of words with known polarities are used as seeds. Brody and Elhadad (2010) use
label propagation over a graph constructed of adjectives only.
Velikovich et al. (2010) compare label propagation with a Web-based method and
conclude that label propagation is not suitable when the whole Web is used as a
background corpus, because the constructed graph is very noisy and contains many
dense subgraphs, unlike the lexical graph constructed from WordNet.
542
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Random walk?based methods have been studied in the context of many other NLP
tasks. For example, Kok and Brockett (2010) construct a graph from bilingual parallel
corpora, where each node represents a phrase and two nodes are connected by an edge
if they are aligned in a phrase table. Then they compute hitting time of random walks
to learn paraphrases.
Our work is different from previous random walk methods in that it uses the mean
hitting time as the criterion for assigning polarity labels. Our experiments showed that
this achieves better results than methods that use label propagation.
2.4 Subjectivity Analysis
Subjectivity analysis is another research line that is closely related to our work. The
main task in subjectivity analysis is to identify text that presents opinion as opposed to
objective text that present factual information (Wiebe 2000). Text could be either words,
phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of
subjectivity analysis such as classifying e-mails and mining reviews. For example, to
analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from
individual sentences as nodes to determine whether a sentence is subjective or objective.
Each node (sentence) has an individual subjectivity score obtained from a first-pass
classifier using sentence features and linguistic knowledge. Edges are weighted by a
similarity metric of how likely it is that the two sentences will be in the same subjectivity
class. All sentences to be classified are represented as unlabeled nodes and the only two
labeled nodes represent the subjective and objective classes. A Mincut algorithm is then
performed on the constructed graph to obtain the subjectivity classes for individual
sentences. The authors also integrate the subjectivity classification of isolated sentences
to document level sentiment analysis.
There are two main categories of work on subjectivity analysis. In the first cate-
gory, subjective words and phrases are identified without considering their context
(Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In
the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff
and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and
Mihalcea (2006a) studied the association of word subjectivity and word sense. They
showed that different subjectivity labels can be assigned to different senses of the same
word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles
from a wide variety of news sources manually annotated for opinions and other private
states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions
and emotions in language.
In addition, there has been a large body of work on labeling subjectivity of WordNet
words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or ob-
jective, utilizing the MPQA corpus. They show that subjectivity information for Word-
Net senses can improve word sense disambiguation tasks for subjectivity ambiguous
words.
Su and Markert (2009) propose a semi-supervised minimum cut framework to label
word sense entries in WordNet with subjectivity information. Their method requires
less training data other than the sense definitions and relational structure of WordNet.
2.5 Word Polarity Classification for Foreign Languages
Word sentiment and subjectivity has also been studied for languages other than English.
Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity
543
Computational Linguistics Volume 40, Number 3
lexicon based on an English lexicon, an on-line translation service, and Wordnet.
Mihalcea and Banea (2007) use bilingual resources such as a bilingual dictionary or a
parallel corpus to generate subjectivity analysis resources for foreign languages. Rao
and Ravichandran (2009) adapt their label propagation model to Hindi using Hindi
WordNet and French using a French thesaurus.
3. Approach
We use a Markov random walk model to identify the polarity of words. Assume that
we have a network of words, some of which are labeled as either positive or nega-
tive. In this network, two words are connected if they are related. Different sources
of information are used to decide whether two words are related. For example, the
synonyms of a word are all semantically related to it. The intuition behind connect-
ing semantically related words is that those words tend to have similar polarities.
Now imagine a random surfer walking along the network starting from an unlabeled
word w.
The random walk continues until the surfer hits a labeled word. If the word w is
positive then the probability that the random walk hits a positive word is higher, and if
w is negative then the probability that the random walk hits a negative word is higher.
Thus, if the word w is positive then the average time it takes a random walk starting at
w to hit a positive node should be much less than the average time it takes a random
walk starting at w to hit a negative node. If w doesn?t have a clear polarity and we would
like to say that it is neutral, we expect that the positive hitting time and negative hitting
time to not have a significant difference.
We describe how we construct a word relatedness graph in Section 3.1. The random
walk model is described in Section 3.2. Hitting time is defined in Section 3.3. Finally,
an algorithm for computing a sign and magnitude for the polarity of any given word
is described in Section 3.4.
3.1 Network Construction
We construct a network where two nodes are linked if they are semantically related.
Several sources of information are used as indicators of the relatedness of words. One
such source is WordNet (Miller 1995). WordNet is a large lexical database of English.
Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms
(synsets), each expressing a distinct concept (Miller 1995). Synsets are interlinked by
means of conceptual-semantic and lexical relations.
The simplest approach is to connect words that occur in the same WordNet synset.
We can collect all words in WordNet, and add links between any two words that
occur in the same synset. The resulting graph is a graph G(W, E) where W is a set
of word/part-of-speech (POS) pairs for all the words in WordNet. E is the set of
edges connecting each pair of synonymous words. Nodes represent word/POS pairs
rather than words because the part of speech tags are helpful in disambiguating
the different senses for a given word. For example, the word ?fine? has two dif-
ferent meanings, with two opposite polarities when used as an adjective and as a
noun.
Several other methods can be used to link words. For example, we can use other
WordNet relations: hypernyms, similar to, and so forth. Another source of links be-
tween words is co-occurrence statistics from a corpus. Following the method presented
544
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
in Hatzivassiloglou and McKeown (1997), we can connect words if they appear together
in a conjunction in the corpus. This method is only applicable to adjectives. If two
adjectives are connected by ?and,? it is highly likely that they have the same semantic
orientation. In all our experiments, we restricted the network to only WordNet relations.
We study the effect of using co-occurrence statistics to connect words later at the end of
our experiments. If more than one relation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
3.2 Random Walk Model
Imagine a random surfer walking along the word relatedness graph G. Starting from a
word with unknown polarity i, it moves to a node j with probability Pij after the first
step. The walk continues until the surfer hits a word with known polarity. Seed words
with known polarity act as an absorbing boundary for the random walk. If we repeat
the number of random walks N times, the percentage of times in which the walk ends at
a positive/negative word could be used as an indicator of its positive/negative polarity.
The average time a random walk starting at w takes to hit the set of positive/negative
nodes is also an indicator of its polarity. This view is closely related to the partially la-
beled classification with random walks approach in Szummer and Jaakkola (2002) and
the semi-supervised learning using harmonic functions approach in Zhu, Ghahramani,
and Lafferty (2003).
Let W be the set of words in our lexicon. We construct a graph whose nodes V are
all words in W. Edges E correspond to the relatedness between words. We define the
transition probability Pt+1|t( j|i) from i to j by normalizing the weights of the edges out
of node i, so:
Pt+1|t( j|i) = Wij/
?
k
Wik (1)
where k represents all nodes in the neighborhood of i. Pt+1|t( j|i) denotes the transition
probability from node i at step t to node j at time step t + 1. We note that the matrix of
weights Wij is symmetric whereas the matrix of transition probabilities Pt+1|t( j|i) is not
necessarily symmetric because of the node outdegree normalization.
3.3 First-Passage Time
The mean first-passage (hitting) time h(i|k) is defined as the average number of steps a
random walker, starting in state i 6= k, will take to enter state k for the first time (Norris
1997). Let G = (V, E) be a graph with a set of vertices V and a set of edges E. Consider
a subset of vertices S ? V. Consider a random walk on G starting at node i 6? S. Let Nt
denote the position of the random surfer at time t. Let h(i|S) be the average number of
steps a random walker, starting in state i 6? S, will take to enter a state k ? S for the first
time. Let TS be the first-passage for any vertex in S.
P(TS = t|N0 = i) =
?
j?V
pij ? P(TS = t? 1|N0 = j) (2)
545
Computational Linguistics Volume 40, Number 3
h(i|S) is the expectation of TS. Hence:
h(i|S) = E(TS|N0 = i)
=
??
t=1
t? P(TS = t|N0 = i)
=
??
t=1
t
?
j?V
pijP(TS = t? 1|N0 = j)
=
?
j?V
??
t=1
(t? 1)pijP(TS = t? 1|N0 = j)
+
?
j?V
??
t=1
pijP(TS = t? 1|N0 = j)
=
?
j?V
pij
??
t=1
tP(TS = t|N0 = j) + 1
=
?
j?V
pij ? h( j|S) + 1 (3)
Hence the first-passage (hitting) time can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h( j|S) + 1 otherwise
(4)
3.4 Word Polarity Calculation
Based on the description of the random walk model and the first-passage (hitting)
time above, we now propose our word polarity identification algorithm. We begin by
constructing a word relatedness graph and defining a random walk on that graph as
described above. Let S+ and S? be two sets of vertices representing seed words that are
already labeled as either positive or negative, respectively.
For any given word w, we compute the hitting time h(w|S+) and h(w|S?) for the
two sets iteratively as described earlier. The ratio between the two hitting times is then
used as an indication of how positive/negative the given word is. This is useful in
case we need to provide a confidence measure for the prediction. This could be used
to allow the model to abstain from classifying words when the confidence level is low.
It also means that our method can be easily extended from two-way classification (i.e.,
positive or negative) to three-way classification (positive, negative, or neutral). This can
be done by setting a threshold ? on the ratio of positive and negative hitting time,
and classifying a word to positive or negative only when the two hitting times have
a significant difference; otherwise we classify it to neutral.
When the relatedness graph is very large, computing hitting time as described
earlier may be very time consuming. The graph constructed from the English WordNet
546
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Algorithm 1 3-class word polarity using random walks (parameter ? : 0 < ? < 1)
Require: A word relatedness graph G
1: Given a word w in V
2: Define a random walk on the graph. The transition probability between any two
nodes i, and j is defined as: Pt+1|t( j|i) = Wij/
?
k Wik
3: Start k independent random walks from w with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h?(w|S+) be the estimated value for h(w|S+)
6: Repeat for negative words computing h?(w|S?)
7: if h?(w|S+) ? ?h?(w|S?) then
8: Classify w as positive
9: else if h?(w|S?) ? ?h?(w|S+) then
10: Classify w as negative
11: else
12: Classify w as neutral
13: end if
and synsets contains 155,000 nodes and 117,000 edges. To overcome this problem, we
propose a Monte Carlo?based algorithm (Algorithm 1) for estimating it.
In the case of binary classification, where each word must be either positive or
negative, if h(w|S+) is greater than h(w|S?), the word is classified as negative and
positive otherwise. This can be achieved by setting parameter ? = 1 in Algorithm 1.
4. Foreign Word Polarity
As we mentioned earlier, a large body of research has focused on identifying the
semantic orientation of words. This work has almost exclusively dealt with English and
uses several language-dependent resources. When we try to apply these methods to
other languages, we run into the problem of the lack of resources in other languages
when compared with English. For example, the General Inquirer lexicon (Stone et al.
1966) has thousands of English words labeled with semantic orientation. Most of the
literature has used it as a source of labeled seeds or for evaluation. Such lexicons are
not readily available in other languages.
As we showed earlier, WordNet (Miller 1995) has been used for this task. How-
ever, even though W have been built for other languages, their coverage is relatively
limited when compared to the English WordNet. The current release of English Word-
Net (WordNet 3.0) includes over 155K words and over 117K synsets. Looking at the
resources for other languages, the Arabic WordNet (Black et al. 2006; Elkateb et al.
2006a, 2006b) contains only 11K synsets; the Hindi WordNet (Jha et al. 2001; Narayan
et al. 2002) contains 32K synsets; Euro WordNet (Vossen 1997) contains 23K synsets in
Spanish, 15K in German, and 22K in French, among other European languages. In some
cases, accuracy was traded for coverage. For example, the current release of the Japanese
WordNet has 57K synsets but contains errors in as many as 5% of the entries.1
In this section, we show how we can extend the methods presented earlier to predict
the semantic orientation of foreign words. The proposed method is based on creating
1 http://nlpwww.nict.go.jp/wn-ja/index.en.html.
547
Computational Linguistics Volume 40, Number 3
a multilingual network of words that represents both English and foreign words. The
network has English?English connections, as well as Foreign?Foreign connections and
English?Foreign connections. This allows us to benefit from the richness of the resources
built for the English language and at the same time utilize resources specific to foreign
languages. We define a random walk model over the multilingual network and pre-
dict the semantic orientation of any given word by comparing the mean hitting time
of a random walk starting from it to a positive and a negative set of seed English
words.
We use Arabic and Hindi in our experiments. We compare the performance of sev-
eral methods using the foreign language resources only, and the multilingual network
that has both English and foreign words. We show that bootstrapping from languages
with dense resources such as English is useful for improving the performance on other
languages with limited resources.
4.1 Multilingual Word Network
We build a network G(V, E) where V = Ven ? Vfr is the union of the sets of English and
Foreign words. E is a set of edges connecting nodes in V. There are three types of connec-
tions: English?English connections, Foreign?Foreign connections, and English?Foreign
connections. For the English?English connections, we use the same methodology as in
Section 3.
Foreign?Foreign connections are created in a similar way to the English con-
nections. Some foreign languages have lexical resources based on the design of the
Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic Word-
Net (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al.
2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of
Hatzivassiloglou and McKeown (1997).
Finally, to connect foreign words to English words, we use a Foreign to English dic-
tionary. For every word in a list of foreign words, we look up its meaning in a dictionary
and add an edge between the foreign word and every other English word that appeared
as a possible meaning for it. If there is no comprehensive enough dictionary available,
constructing a multilingual word network like a translation graph (Etzioni et al. 2007)
may be a resolution.
4.2 Foreign Word Semantic Orientation Prediction
We use the multilingual network described previously to predict the semantic orien-
tation of words based on the mean hitting time to two sets of positive and negative
seeds. Given two lists of seed English words with known polarity, we define two sets
of nodes S+ and S? representing those seeds. For any given word w, we calculate the
mean hitting time between w and the two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as negative; otherwise it is classified as
positive. We used the list of labeled seeds from Hatzivassiloglou and McKeown (1997)
and Stone et al. (1966).
5. Out-of-Vocabulary Words
We observed that a significant portion of the text used on-line in discussions, comments,
product reviews, and so on, contains words that are not defined in WordNet or in
548
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
standard dictionaries. We call these words Out-of-Vocabulary (OOV) words. Table 6
later in this article shows some OOV word examples. To show the importance of OOV
word polarity identification, we calculated the proportion of OOV words in three
corpora used for sentiment studies: a set of movie reviews, a set of on-line discussions
from a political forum, and a set of randomly sampled tweets. For each word in the
data, we look it up in two standard English dictionaries, together containing 160K
unique words. Table 1 shows the statistics.
OOV words have a high chance of being polarized because people tend to use
informal language or special acronyms to emphasize their attitudes or impress the
audience. Therefore, being able to automatically identify the polarity of OOV words
will essentially benefit real-world applications.
Consider the graph G(W, E) described in Section 3.1. So far, the only resource we use
to construct the graph is WordNet synsets. The first step in our approach to OOV word
polarity identification is to find the words in WordNet that are related to an OOV
word. Next, we add the OOV words to our graph by creating a new node for each OOV
word and adding an edge between each OOV word and each of its related words. Once
we have constructed the extended network, we use the random walk model described
in Section 3.2 to predict the polarity of each OOV word.
5.1 Mining OOV Word Relatedness from the Web
There are several alternative methods of linking words in the graph. Agirre et al. (2009)
studied the strengths and weaknesses of different approaches to term similarity and
relatedness. They noticed that lexicographical methods such as the WordNet suffer from
the limitation of lexicon coverage, which is the case here with OOV words. To overcome
this limitation, we use a Web-based distributional approach to find the set of related
words to each OOV word. We perform a Web search using the OOV word as a search
query and retrieve the top S search results. We extract the textual content of the retrieved
results and tokenize it. After removing all the stop words, we compute the number of
times each word co-occurs with the OOV word in the same document. We rank the
words based on their co-occurrence frequency and return the top R words as the set of
related words to the given OOV word.
We experimented with three different variants of this approach. In the first variant,
the frequency values of the co-occurring words are normalized by the lengths of the
Table 1
Proportion of OOV words in some corpora used for real world applications. (Numbers in
parentheses exclude words whose first letters are capitalized because they are likely to refer to
named entities.)
corpus source # of words Percentage
of OOV
Movie reviews 3, 411 customer reviews from IMDb for the
movie The Dark Knight (2008)
10.7 M (9.5 M) 5.3 (2.7)
Political forum 23K sentences from www.politicalforum.com
on various topics
381 K (348 K) 8 (6)
tweets 0.6M random English tweets from twitter.com.
(We count a tweet as in English if at least half
of the words are English dictionary words.
Tags and symbols were removed.)
7.1 M (5.9 M) 30 (27)
549
Computational Linguistics Volume 40, Number 3
documents that contributed to the count of each word. The intuition here is that longer
documents contain more words and hence the probability that a word in the that
document is related to the search query (i.e., the OOV word) is lower than when the
document is shorter.
In the second variant, we only consider the words that appear in the proximity of
the OOV word (i.e., within d words around the OOV word) when we compute the co-
occurrence frequency. The intuition here is that words that appear near the OOV word
are more likely to be semantically related than the words that appear far away.
In the third variant, instead of searching the entire Web, we limit the search to
social text. In the experiments described subsequently, we search for the OOV words
in tweets posted on Twitter.2 The intuition here is that searching the entire Web is likely
to return results that do not necessarily contain opinionated text?particularly because
many words have different senses. In contrast, the text written in a social context is more
likely to carry sentiment and express emotions. This helps us find better related words
that suit our task.
5.2 Word Network Extension with OOV Words
To extend the graph to include OOV words, we start with the graph G(W, E) constructed
from WordNet synsets. For each OOV word that does not exist in G, we create a new
node w. We set the part of speech of w to unspecified. Then we use the Web-based method
described in the previous section to find a set of words that are most related to w. Finally,
we create a link between each OOV word and each of its related words. To predict the
polarity of an OOV word, we use the same random walk model described earlier.
6. Experiments
We performed experiments on the gold-standard data set for positive/negative words
from the General Inquirer lexicon (Stone et al. 1966). The data set contains 4, 206 words,
1, 915 of which are positive and 2, 291 of which are negative. Some of the ambiguous
words were removed, as in Turney (2002) and Takamura, Inui, and Okumura (2005).
Some examples of positive/negative words are listed in Table 2.
We use WordNet (Miller 1995) as a source of synonyms and hypernyms for the
word relatedness graph. We used the Reuters Corpus, Volume 1 (Lewis et al. 2004) to
generate co-occurrence statistics in the experiments that used them. We used 10-fold
cross-validation for all tests. We evaluate our results in terms of accuracy. Statistical
significance was tested using a two-tailed paired t-test. All reported results are statisti-
cally significant at the 0.05 level. We perform experiments varying the parameters and
the network. We also look at the performance of the proposed method for different
parts of speech, and for different confidence levels. We compare our method to the
Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin
model described in Takamura, Inui, and Okumura (2005), the shortest path method
described in Kamps et al. (2004), a re-implementation of the label propagation and
Mincut methods described in Rao and Ravichandran (2009), and the bootstrapping
method described in Hu and Liu (2004).
2 http://www.twitter.com.
550
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Table 2
Examples of positive and negative words.
Positive Negative
able adjective abandon verb
acceptable adjective abuse verb
admire verb burglar noun
amazing adjective chaos noun
careful adjective contagious adjective
ease noun corruption noun
guide verb lie verb
inspire verb reluctant adjective
truthful adjective wrong adjective
6.1 Comparison with Other Methods
This method could be used in a semi-supervised setting where a set of labeled words are
used and the system learns from these labeled nodes and from other unlabeled nodes.
Under this setting, we compare our method to the spin model described in Takamura,
Inui, and Okumura (2005). Table 3 compares the performance using 10-fold cross val-
idation. The table shows that the proposed method outperforms the spin model. The
spin model approach uses word glosses, WordNet synonym, hypernym, and antonym
relations, in addition to co-occurrence statistics extracted from corpus. The proposed
method achieves better performance by only using WordNet synonym, hypernym, and
similar to relations. Adding co-occurrence statistics slightly improved performance, and
using glosses did not help at all.
We also compare our method to a re-implementation of the label propagation (LP)
method. Our method outperforms the LP method in both the 10-fold cross-validation
set-up and when only 14 seeds are used.
We also compare our method to the SO-PMI method. Turney and Littman (2002)
propose two methods for predicting the semantic orientation of words. They use
Latent Semantic Analysis (SO-LSA) and Pointwise Mutual Information (SO-PMI) for
measuring the statistical association between any given word and a set of 14 seed
words. They describe this method as unsupervised because they only use 14 seeds
as paradigm words that define the semantic orientation rather than train the model
(Turney 2002).
Table 3
Accuracy for SO-PMI with different data set sizes, the spin model, the label propagation model,
and the random walks model for 10-fold cross-validation and 14 seeds.
? CV 14 seeds
SO-PMI (1? 107) ? 61.3
SO-PMI (2? 109) ? 76.1
SO-PMI (1? 1011) ? 82.8
Spin Model 91.5 81.9
Label Propagation 88.40 74.83
Random Walks 93.1 82.1
551
Computational Linguistics Volume 40, Number 3
The SO-PMI value can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(5)
where w is a word with unknown polarity, hitsw,pos is the number of hits returned by a
commercial search engine when the search query is the given word and the disjunction
of all positive seed words. hitspos is the number of hits when we search for the disjunction
of all positive seed words. hitsw,neg, and hitsneg are defined similarly.
After Turney (2002), we use our method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al. 1966) using only 14 seed words. The network
we used contains only WordNet relations. No glosses or co-occurrence statistics are
used. The results comparing the SO-PMI method with different data set sizes, the spin
model, and the proposed method using only 14 seeds is shown in Table 3. We observe
that the random walk method outperforms SO-PMI when SO-PMI uses data sets of
sizes 1? 107 and 2? 109 words. The performance of SO-PMI and the random walk
methods are comparable when SO-PMI uses a very large data set (1? 1011 words). The
performance of the spin model approach is also comparable to the other two methods.
The advantages of the random walk method over SO-PMI is that it is faster and it does
not need a very large corpus. Another advantage is that the random walk method can
be used along with the labeled data from the General Inquirer lexicon (Stone et al. 1966)
to get much better performance. This is costly for the SO-PMI method because that will
require the submission of almost 4,000 queries to a commercial search engine.
We also compare our method with the bootstrapping method described in Hu and
Liu (2004), and the shortest path method described in Kamps et al. (2004). We build a
network using only WordNet synonyms and hypernyms. We restrict the test set to the
set of adjectives in the General Inquirer lexicon because our method is mainly interested
in classifying adjectives.
The performance of the spin model, the bootstrapping method, the shortest path
method, the LP method, the Mincut method, and the random walk method for only
adjectives is shown in Table 4. We notice from the table that the random walk method
outperforms the spin model, the bootstrapping method, the shortest path method,
the LP method, and the Mincut method for adjectives. The reported accuracy for the
shortest path method only considers the words it could assign a non-zero orientation
value. If we consider all words, its accuracy will drop to around 61%.
6.1.1 Varying Parameters. As we mentioned in Section 3.4, we use a parameter m to put
an upper bound on the length of random walks. In this section, we explore the impact
of this parameter on our method?s performance.
Figure 1 shows the accuracy of the random walk method as a function of the
maximum number of steps m as it varies from 5 to 50. We use a network built from
WordNet synonyms and hypernyms only. The number of samples k was set to 1, 000.
Table 4
Accuracy for adjectives only for the spin model, the bootstrap method, and the random walk
model.
Method Spin Model Bootstrap Shortest Path LP Mincut Random Walks
Accuracy 83.6 72.8 68.8 84.8 73.8 88.8
552
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 1
The effect of varying the maximum number of steps (m) on accuracy (k = 1,000).
We perform 10-fold cross-validation using the General Inquirer lexicon. We observe
that the maximum number of steps m has very little impact on performance until it
rises above 30. At that point, the performance drops by no more than 1%, and then it no
longer changes as m increases. An interesting observation is that the proposed method
performs quite well with a very small number of steps (around 10). We looked at the
data set to understand why increasing the number of steps beyond 30 negatively affects
performance. We found out that when the number of steps is very large compared with
the diameter of the graph, the random walk that starts at ambiguous words (which are
hard to classify) have the chance of moving until it hits a node in the opposite class.
That does not happen when the limit on the number of steps is smaller because those
walks are then terminated without hitting any labeled nodes and are hence ignored.
Next, we study the effect of the number of samples k on our method?s performance.
As explained in Section 3.4, k is the number of samples used by the Monte Carlo
algorithm to find an estimate for the hitting time. Figure 2 shows the accuracy of the
random walks method as a function of the number of samples k. We use the same
Figure 2
The effect of varying the number of samples (k) on accuracy.
553
Computational Linguistics Volume 40, Number 3
settings as in the previous experiment. The only difference is that we fix m at 15 and
vary k from 10 to 20, 000 (note the logarithmic scale). We notice that the performance
is badly affected when the value of k is very small (less than 100). We also notice that
after 1, 000, varying k has very little, if any, effect on performance. This shows that the
Monte Carlo algorithm for computing the random walks hitting time performs quite
well with values of the number of samples as small as 1, 000.
The preceding experiments suggest that the parameter m has very little impact
on the performance. This suggests that the approach is fairly robust (i.e., it is quite
insensitive to different parameter settings).
6.1.2 Other Experiments. We now measure the performance of the random walk method
when the system is allowed to abstain from classifying the words for which it has low
confidence. We regard the ratio between the hitting time to positive words and hitting
time to negative words as a confidence measure and evaluate the top words with the
highest confidence level at different values of threshold. Figure 3 shows the accuracy for
10-fold cross validation and for using only 14 seeds at different thresholds. We notice
that the accuracy improves by abstaining from classifying the difficult words. The figure
shows that the top 60% words are classified with accuracy greater than 99% for 10-fold
cross validation and 92% with 14 seed words. This may be compared with the work
described in Takamura, Inui, and Okumura (2005), where they achieve the 92% level
when they only consider the top 1,000 words (28%).
Figure 4 shows a learning curve displaying how the performance of both the pro-
posed method and the LP method is affected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy exceeds 90% when the training set size
rises above 20%. The accuracy steadily increases as the size of labeled data increases.
We also looked at the classification accuracy for different parts of speech in Figure 5.
We notice that, in the case of 10-fold cross-validation, the performance is consistent
across parts of speech. However, when we only use 14 seeds?all of which are ad-
jectives, similar to Turney and Littman (2003)?we notice that the performance on
adjectives is much better than other parts of speech. When we use 14 seeds but replace
some of the adjectives with verbs and nouns such as love, harm, friend, enemy, the per-
formance for nouns and verbs improves considerably at the cost of a small drop in the
Figure 3
Accuracy for words with high confidence measure.
554
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
Figure 4
The effect of varying the number of seeds on accuracy.
50
55
60
65
70
75
80
85
90
95
100
Adj Adv Noun Verb
CV 14 Adj Seeds 14 Seeds
Figure 5
Accuracy for different parts of speech.
performance on adjectives. Finally, we tried adding edges to the network from glosses
and co-occurrence statistics but we did not get any statistically significant improvement.
Some of the words that were very weakly linked benefited from adding new types
of links and they were correctly predicted. Others were misled by the noise and were
incorrectly classified. We had a closer look at the results to find out what are the reasons
behind incorrect predictions. We found two main reasons. First, some words have more
than one sense, possibly with different semantic orientations. Disambiguating the sense
of words given their context before trying to predict their polarity should solve this
problem. The second reason is that some words have very few connections in the
thesaurus. A possible solution to this might be to identify those words and add more
links to them from glosses of co-occurrence statistics in the corpus.
6.1.3 General Purpose Three-Way Classification. The experiments described so far all use
the General Inquirer lexicon, which contains a well-established gold standard data set
of positive and negative words. However, in realistic applications, a general purpose
555
Computational Linguistics Volume 40, Number 3
Table 5
Accuracy for three classes on a general purpose list of 2,000 words.
Class Positive Negative Neutral Overall
Accuracy 68.0 82.1 80.6 77.9
list of words will frequently have neutral words that don?t express sentiment polarity.
To evaluate the effectiveness of the random walk method in distinguishing polarized
words from neutral words, we constructed a data set of 2, 000 words randomly picked
from a standard English dictionary3 and hand labeled them with three classes: posi-
tive, negative, and neutral. Among the 2, 000 words, 494 were labeled positive,
491 negative, and 1, 015 neutral. The distribution among different parts of speech is
532 adjectives, 335 verbs, 1, 051 nouns, and 82 others.
We used the semi-supervised setting with the General Inquirer lexicon polarized
word list as the training set. Because the 2, 000 test set has some portion of polarized
words overlapping with the training set, we excluded the words that appear in the test
set from the training set. We performed Algorithm 2 in Section 3.4 with parameters
? = 0.8, m = 15, k = 1, 000. The overall accuracy as well as the precision for each class is
shown in Table 5. We can see that the accuracy of the positive class is much lower than
the negative class, due to the many positive words classified as neutral. This means
that the average confidence of negative words is higher than positive words. One factor
that could have caused this is the bias originating from the training set. Because there
are more negative seeds than positive ones, the constructed graph has an overall bias
towards the negative class.
6.2 Foreign Words
In addition to the English data we described earlier, we constructed a labeled set of 300
Arabic and 300 Hindi words for evaluation. For every language, we asked two native
speakers to examine a large amount of text and identify a set of positive and negative
words. We also used an Arabic?English and a Hindi?English dictionary to generate
Foreign?English links.
We compare our results with two baselines. The first is the SO-PMI method de-
scribed in Turney and Littman (2003). We used the same seven positive and seven
negative seeds as Turney and Littman (2003).
The second baseline constructs a network of only foreign words as described earlier.
It uses mean hitting time to find the semantic association of any given word. We used
10-fold cross-validation for this experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use the hitting time as before to predict
semantic orientation. We used the English words from Stone et al. (1966) as seeds and
the labeled foreign words for evaluation. We will refer to this system as HT-FR-EN.
Figure 6 compares the accuracy of the three methods for Arabic and Hindi. We
notice that the SO-PMI and the hitting time?based methods perform poorly on both
Arabic and Hindi. This is clearly evident when we consider that the accuracy of the two
systems on English was 83%, and 93%, respectively (Turney and Littman 2003; Hassan
3 Very infrequent words were filtered out by setting a threshold on the inverse document frequency of the
words in a corpus.
556
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT - FR HT - FR+ EN
Figure 6
Accuracy of foreign word polarity identification.
and Radev 2010). This supports our hypothesis that state-of-the-art methods, designed
for English, perform poorly on foreign languages due to the limited amount of resources
in them. The figure also shows that the proposed method, which combines resources
from both English and foreign languages, performs significantly better. Finally, we
studied how much improvement is achieved by including links between foreign words
from global WordNets. We found out that it improves the performance by 2.5% and 4%
for Arabic and Hindi, respectively.
6.3 OOV Words
We created a labeled set of 300 positive and negative OOV words. We asked a native
English speaker to examine a large number of threads posted on several on-line forums
and identify OOV words and label them with their polarities. Some examples of posi-
tive/negative OOV words are listed in Table 6.
The baseline we use for OOV words is the SO-PMI method with the same 14 seeds
as in Turney and Littman (2003). The calculation of SO-PMI is given in Equation (5).
We used the approach described in Section 5 to automatically label the words. We
used the words of the General Inquirer lexicon as labeled seeds. We set the maximum
number of steps m to 15 and the number of samples k to 1, 000. We experimented with
Table 6
Examples of positive and negative OOV words.
Positive Negative
Word Meaning Word Meaning
beautimous beautiful and fabulous disastrophy a catastrophy and a disaster
gr8 great banjaxed ruined
buffting attractive ijit idiot
557
Computational Linguistics Volume 40, Number 3
Figure 7
Accuracy of different methods in predicting OOV words polarity.
the three variants we proposed for extracting the related words as described in Section 5.
We give the experimental set-up for each variant here:
1. Search the entire Web (WS): We used Yahoo search4 to execute the search
queries. For each OOV word, we retrieve the top 500 results and use them
to extract the related words.
2. Search the entire Web and limit the extraction of related words to the
proximity of the OOV word (WSP): We fix the proximity of a given
OOV word to 15 words before and 15 words after the OOV word (we
experimented with different ranges but no significant changes were
observed).
3. Limit the search to social content (SOC): We limit the search for OOV
words to tweets posted on Twitter. We use the Twitter search API
to submit the search queries. For each OOV word, we retrieve
10,000 tweets. Each tweet is maximum of 140 characters long.
Figure 7 shows the results of the three methods compared with the baseline SO-PMI.
The results show that extracting related words from tweets gives the best accuracy. This
corroborated our intuition that using social content is more likely to provide sentiment-
related words. The baseline SO-PMI and WS obtain very similar accuracy. This agrees
with the comparable performance of the two methods in the earlier experiment on the
General Inquirer lexicon.
The three variant methods for obtaining related words have a tunable parameter
R, the number of related words extracted for each OOV word. We observe that R
has a non-negligible effect on the prediction accuracy. The results shown in Figure 8
correspond to R = 90. To better understand the impact of varying this parameter, we ran
the experiment that uses Twitter to extract related words several times using different
values for R. Figure 8 shows how the accuracy of polarity prediction changes as R
changes.
4 http://www.yahoo.com.
558
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
40%
45%
50%
55%
60%
65%
70%
0 20 40 60 80 100 120 140 160 180
Accu
racy
 
Number of related words  
Figure 8
The effect of varying the number of extracted related words on accuracy.
7. Conclusions
Predicting the semantic orientation of words is a very interesting task in natural lan-
guage processing and it has a wide variety of applications. We proposed a method for
automatically predicting the semantic orientation of words using random walks and
hitting time. The proposed method is based on the observation that a random walk
starting at a given word is more likely to hit another word with the same semantic
orientation before hitting a word with a different semantic orientation. The proposed
method can be used in a semi-supervised setting, where a training set of labeled words
is used, and in a weakly supervised setting, where only a handful of seeds is used to
define the two polarity classes. We predict semantic orientation with high accuracy.
The proposed method is fast, simple to implement, and does not need any corpus. We
also extended the proposed method to cover the problem of predicting the semantic
orientation of foreign words. All previous work on this task has almost exclusively
focused on English. Applying off-the-shelf methods developed for English to other
languages does not work well because of the limited amount of resources available
in foreign languages compared with English. We show that the proposed method can
predict the semantic orientation of foreign words with high accuracy and outperforms
state-of-the-art methods limited to using language specific resources. Finally, we further
extended the method to cover out-of-vocabulary words. These words do not exist in
WordNet and are not defined in the standard dictionaries of the language. We proposed
using a Web-based approach to add the OOV words to our words network based on
co-occurrence statistics, then use the same random walk model to predict the polar-
ity. We showed that this method can predict the polarity of OOV words with good
accuracy.
Acknowledgments
This research was funded by the Office of the
Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects
Activity (IARPA), through the U.S. Army
Research Lab. All statements of fact, opinion,
or conclusions contained herein are those of
the authors and should not be construed as
representing the official views or policies of
IARPA, the ODNI, or the U.S. Government.
559
Computational Linguistics Volume 40, Number 3
References
Agirre, Eneko, Enrique Alfonseca, Keith
Hall, Jana Kravalova, Marius Pas?ca, and
Aitor Soroa. 2009. A study on similarity
and relatedness using distributional and
wordnet-based approaches. In Proceedings
of Human Language Technologies: The 2009
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics, NAACL ?09, pages 19?27,
Stroudsburg, PA.
Andreevskaia, Alina and Sabine Bergler.
2006. Mining WordNet for fuzzy
sentiment: Sentiment tag extraction
from WordNet glosses. In EACL?06,
pages 209?216.
Banea, Carmen, Rada Mihalcea, and
Janyce Wiebe. 2008. A bootstrapping
method for building subjectivity lexicons
for languages with scarce resources.
In LREC?08, pages 2,764?2,767.
Black, W., S. Elkateb, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006. Introducing the
Arabic WordNet project. In Third
International WordNet Conference,
pages 295?299.
Blair-Goldensohn, Sasha, Tyler Neylon,
Kerry Hannan, George A. Reis, Ryan
McDonald, and Jeff Reynar. 2008. Building
a sentiment summarizer for local service
reviews. In NLP in the Information
Explosion Era.
Brody, Samuel and Noemie Elhadad. 2010.
An unsupervised aspect-sentiment model
for online reviews. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 804?812, Los Angeles, CA.
Elkateb, S., W. Black, H. Rodriguez,
M. Alkhalifa, P. Vossen, A. Pease, and
C. Fellbaum. 2006a. Building a WordNet
for Arabic. In Fifth International Conference
on Language Resources and Evaluation,
pages 29?34.
Elkateb, S., W. Black, P. Vossen, D. Farwell,
H. Rodriguez, A. Pease, and M. Alkhalifa.
2006b. Arabic WordNet and the challenges
of Arabic. In Arabic NLP/MT Conference,
pages 15?24.
Esuli, Andrea and Fabrizio Sebastiani. 2005.
Determining the semantic orientation
of terms through gloss classification.
In CIKM?05, pages 617?624.
Esuli, Andrea and Fabrizio Sebastiani. 2006.
Sentiwordnet: A publicly available lexical
resource for opinion mining. In LREC?06,
pages 417?422.
Etzioni, Oren, Kobi Reiter, Stephen Soderl,
and Marcus Sammer. 2007. Lexical
translation with application to image
search on the Web. In Proceedings of
Machine Translation Summit XI.
Hassan, Ahmed and Dragomir R. Radev.
2010. Identifying text polarity using
random walks. In Proceedings of the 48th
Annual Meeting of the Association for
Computational Linguistics, pages 395?403,
Uppsala.
Hatzivassiloglou, Vasileios and Kathleen R.
McKeown. 1997. Predicting the semantic
orientation of adjectives. In EACL?97,
pages 174?181.
Hatzivassiloglou, Vasileios and Janyce
Wiebe. 2000. Effects of adjective
orientation and gradability on sentence
subjectivity. In COLING, pages 299?305.
Hu, Minqing and Bing Liu. 2004. Mining
and summarizing customer reviews.
In KDD?04, pages 168?177.
Jha, S., D. Narayan, P. Pande, and
P. Bhattacharyya. 2001. A WordNet for
Hindi. In International Workshop on Lexical
Resources in Natural Language Processing.
Jijkoun, Valentin and Katja Hofmann. 2009.
Generating a non-English subjectivity
lexicon: Relations that matter. In
Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009),
pages 398?405, Athens.
Kamps, Jaap, Maarten Marx, Robert J.
Mokken, and Maarten De Rijke. 2004.
Using WordNet to measure semantic
orientations of adjectives. In Proceedings
of the 4th International Conference on
Language Resources and Evaluation
(LREC 2004), pages 1115?1118.
Kanayama, Hiroshi and Tetsuya Nasukawa.
2006. Fully automatic lexicon expansion
for domain-oriented sentiment analysis.
In EMNLP?06, pages 355?363.
Kim, Soo-Min and Eduard Hovy. 2004.
Determining the sentiment of opinions.
In COLING, pages 1,367?1,373.
Kok, Stanley and Chris Brockett. 2010.
Hitting the right paraphrases in good
time. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 145?153,
Los Angeles, CA.
Lewis, D. D., Y. Yang, T. Rose, and F. Li.
2004. Rcv1: A new benchmark collection
for text categorization research. Journal of
Machine Learning Research, 5:361?397.
Mihalcea, Rada and Carmen Banea. 2007.
Learning multilingual subjective language
560
Hassan et al. A Random Walk?Based Model for Identifying Semantic Orientation
via cross-lingual projections. In Proceedings
of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 976?983.
Miller, George A. 1995. Wordnet: A lexical
database for English. Communications of
ACM, 38(11):39?41.
Mohammad, Saif, Cody Dunne, and Bonnie
Dorr. 2009. Generating high-coverage
semantic orientation lexicons from overtly
marked words and a thesaurus. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing: Volume 2, EMNLP ?09,
pages 599?608, Stroudsburg, PA.
Morinaga, Satoshi, Kenji Yamanishi, Kenji
Tateishi, and Toshikazu Fukushima. 2002.
Mining product reputations on the Web.
In KDD?02, pages 341?349.
Narayan, Dipak, Debasri Chakrabarti,
Prabhakar Pande, and P. Bhattacharyya.
2002. An experience in building the Indo
WordNet?a WordNet for Hindi. In First
International Conference on Global WordNet.
Nasukawa, Tetsuya and Jeonghee Yi. 2003.
Sentiment analysis: Capturing favorability
using natural language processing.
In K-CAP ?03: Proceedings of the 2nd
International Conference on Knowledge
Capture, pages 70?77.
Norris, J. 1997. Markov Chains. Cambridge
University Press.
Pang, Bo and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using
subjectivity summarization based on
minimum cuts. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics, ACL ?04,
Stroudsburg, PA.
Popescu, Ana-Maria and Oren Etzioni. 2005.
Extracting product features and opinions
from reviews. In HLT-EMNLP?05,
pages 339?346.
Rao, Delip and Deepak Ravichandran.
2009. Semi-supervised polarity lexicon
induction. In Proceedings of the 12th
Conference of the European Chapter of the
ACL (EACL 2009), pages 675?682, Athens.
Riloff, Ellen and Janyce Wiebe. 2003.
Learning extraction patterns for
subjective expressions. In EMNLP?03,
pages 105?112.
Stone, Philip, Dexter Dunphy, Marchall
Smith, and Daniel Ogilvie. 1966. The
General Inquirer: A Computer Approach
to Content Analysis. The MIT Press.
Su, Fangzhong and Katja Markert. 2009.
Subjectivity recognition on word
senses via semi-supervised mincuts.
In Proceedings of Human Language
Technologies: The 2009 Annual Conference of
the North American Chapter of the Association
for Computational Linguistics, NAACL ?09,
pages 1?9, Stroudsburg, PA.
Szummer, Martin and Tommi Jaakkola.
2002. Partially labeled classification with
Markov random walks. In NIPS?02,
pages 945?952.
Takamura, Hiroya, Takashi Inui, and
Manabu Okumura. 2005. Extracting
semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Tong, Richard M. 2001. An operational
system for detecting and tracking opinions
in on-line discussion. Workshop note,
SIGIR 2001 Workshop on Operational Text
Classification.
Turney, Peter and Michael Littman. 2003.
Measuring praise and criticism: Inference
of semantic orientation from association.
ACM Transactions on Information Systems,
21:315?346.
Turney, Peter D. 2002. Thumbs up or thumbs
down?: Semantic orientation applied to
unsupervised classification of reviews.
In ACL?02, pages 417?424.
Velikovich, Leonid, Sasha Blair-Goldensohn,
Kerry Hannan, and Ryan McDonald. 2010.
The viability of Web-derived polarity
lexicons. In Human Language Technologies:
The 2010 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 777?785,
Los Angeles, CA.
Vossen, P. 1997. Eurowordnet: A multilingual
database for information retrieval. In
DELOS Workshop on Cross-Language
Information Retrieval, pages 5?7.
Wiebe, Janyce. 2000. Learning subjective
adjectives from corpora. In Proceedings of
the Seventeenth National Conference on
Artificial Intelligence and the Twelfth
Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Wiebe, Janyce, Rebecca Bruce, Matthew Bell,
Melanie Martin, and Theresa Wilson.
2001. A corpus study of evaluative and
speculative language. In Proceedings of the
Second SIGdial Workshop on Discourse and
Dialogue, pages 1?10.
Wiebe, Janyce and Rada Mihalcea.
2006a. Word sense and subjectivity.
In Proceedings of the 21st International
Conference on Computational Linguistics
and the 44th Annual Meeting of the
Association for Computational Linguistics,
pages 1,065?1,072, Sydney.
Wiebe, Janyce and Rada Mihalcea. 2006b.
Word sense and subjectivity. In Proceedings
561
Computational Linguistics Volume 40, Number 3
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, ACL-44,
pages 1,065?1,072, Stroudsburg, PA.
Wiebe, Janyce, Theresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2-3):165?210.
Yu, Hong and Vasileios Hatzivassiloglou.
2003. Towards answering opinion
questions: Separating facts from opinions
and identifying the polarity of opinion
sentences. In EMNLP?03, pages 129?136.
Zhu, Xiaojin, Zoubin Ghahramani, and
John Lafferty. 2003. Semi-supervised
learning using Gaussian fields and
harmonic functions. In ICML?03,
pages 912?919.
562
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 80?90,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Reference Scope Identification in Citing Sentences
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
A citing sentence is one that appears in a sci-
entific article and cites previous work. Cit-
ing sentences have been studied and used in
many applications. For example, they have
been used in scientific paper summarization,
automatic survey generation, paraphrase iden-
tification, and citation function classification.
Citing sentences that cite multiple papers are
common in scientific writing. This observa-
tion should be taken into consideration when
using citing sentences in applications. For in-
stance, when a citing sentence is used in a
summary of a scientific paper, only the frag-
ments of the sentence that are relevant to the
summarized paper should be included in the
summary. In this paper, we present and com-
pare three different approaches for identifying
the fragments of a citing sentence that are re-
lated to a given target reference. Our methods
are: word classification, sequence labeling,
and segment classification. Our experiments
show that segment classification achieves the
best results.
1 Introduction
Citation plays an important role in science. It makes
the accumulation of knowledge possible. When a
reference appears in a scientific article, it is usually
accompanied by a span of text that highlights the
important contributions of the cited article. We
call a sentence that contains an explicit reference
to previous work a citation sentence. For example,
sentence (1) below is a citing sentence that cites a
paper by Philip Resnik and describes the problem
Resnik addressed in his paper.
(1) Resnik (1999) addressed the issue of language identification
for finding Web pages in the languages of interest.
Previous work has studied and used citation sen-
tences in various applications such as: scientific pa-
per summarization (Elkiss et al, 2008; Qazvinian
and Radev, 2008; Mei and Zhai, 2008; Qazvinian
et al, 2010; Qazvinian and Radev, 2010; Abu-
Jbara and Radev, 2011), automatic survey genera-
tion (Nanba et al, 2000; Mohammad et al, 2009),
citation function classification (Nanba et al, 2000;
Teufel et al, 2006; Siddharthan and Teufel, 2007;
Teufel, 2007), and paraphrase recognition (Nakov et
al., 2004; Schwartz et al, 2007).
Sentence (1) above contains one reference, and
the whole sentence is talking about that reference.
This is not always the case in scientific writing.
Sentences that contain references to multiple papers
are very common. For example, sentence (2) below
contains three references.
(2) Grefenstette and Nioche (2000) and Jones and Ghani (2000)
use the web to generate corpora for languages where electronic
resources are scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
80
The first fragment describes the contribution of
Grefenstette and Nioche (2000) and Jones and Ghani
(2000). The second fragment describes the contribu-
tion of Resnik (1999).
This observation should be taken into considera-
tion when using citing sentences in the aforemen-
tioned applications. For example, in citation-based
summarization of scientific papers, a subset of cit-
ing sentences that cite a given target paper is se-
lected and used to form a summary of that paper.
It is very likely that one or more of the selected sen-
tences cite multiple papers besides the target. This
means that some of the text included in the sum-
mary might be irrelevant to the summarized paper.
Including irrelevant text in the summary introduces
several problems. First, the summarization task aims
at summarizing the contributions of the target paper
using minimal text. Extraneous text takes space in
the summary while being irrelevant and less impor-
tant. Second, including irrelevant text in the sum-
mary breaks the context and confuses the reader.
Therefore, if sentence (2) above is to be added to
a citation-based summary of Resniks? (1999) paper,
only the underlined fragment should be added to the
summary and the rest of the sentence should be ex-
cluded.
For another example, consider the task of citation
function classification. The goal of this task is to
determine the reason for citing paper B by paper A
based on linguistic and structural features extracted
from citing sentences that appear in A and cite B. If
a citing sentence in A cites multiple papers besides
B, classification features should be extracted only
from the fragments of the sentence that are relevant
to B. Sentence (3) below shows an examples of this
case.
(3) Cohn and Lapata (2008) used the GHKM extraction method (Galley
et al, 2004), which is limited to constituent phrases and thus produces
a reasonably small set of syntactic rules.
If the target reference is Cohn and Lapata (2008),
only the underlined segment should be used for fea-
ture extraction. The limitation stated in the sec-
ond segment of sentence is referring to Galley et al,
(2004).
In this paper, we address the problem of identi-
fying the fragments of a citing sentence that are re-
lated to a given target reference. Henceforth, we use
the term Reference Scope to refer to those fragments.
We present and compare three different approaches
to this problem.
In the first approach, we define the problem as a
word classification task. We classify each word in
the sentence as inside or outside the scope of the tar-
get reference.
In the second approach, we define the problem as
a sequence labeling problem. This is different from
the first approach in that the label assigned to each
word is dependent on the labels of nearby words. In
the third approach, instead of classifying individual
words, we split the sentence into segments and clas-
sify each segment as inside or outside the scope of
the target reference.
Applying any of the three approaches is pre-
ceded by a preprocessing stage. In this stage, cit-
ing sentences are analyzed to tag references, iden-
tify groups of references, and distinguish between
syntactic and non-syntactic references.
The rest of this paper is organized as follows. Sec-
tion 2 examines the related work. We define the
problem in Section3. Section 4 presents our ap-
proaches. Experiments, results and analysis are pre-
sented in Section 5. We conclude and provide direc-
tions to future work in Section 6
2 Related Work
Our work is related to a large body of research on
citations (Hodges, 1972; Garfield et al, 1984). The
interest in studying citations stems from the fact that
bibliometric measures are commonly used to esti-
mate the impact of a researcher?s work (Borgman
and Furner, 2002; Luukkonen, 1992). White (2004)
provides a good recent survey of the different re-
search lines that use citations. In this section we re-
view the research lines that are relevant to our work
81
and show how our work is different.
One line of research that is related to our work
has to do with identifying what Nanba and Oku-
mura (1999) call the citing area They define the cit-
ing area as the succession of sentences that appear
around the location of a given reference in a sci-
entific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al, 2000) they use their cit-
ing area identification algorithm to improve citation
type classification and automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
Although this work shares the same general goal
with ours (i.e identifying the pieces of text that are
relevant to a given target reference), our work is dif-
ferent in two ways. First, previous work mostly ig-
nored the fact that the citing sentence itself might
be citing multiple references. Second, it defined the
citing area (Nanba and Okumura, 1999) or the ci-
tation context (Qazvinian and Radev, 2010) as a set
of whole contiguous sentences. In our work, we ad-
dress the case where one citing sentence cites mul-
tiple papers, and define what we call the reference
scope to be the fragments (not necessarily contigu-
ous) of the citing sentence that are related to the tar-
get reference.
In a recent work on citation-based summarization
by Abu-Jbara and Radev (2011), the authors noticed
the issue of having multiple references in one sen-
tence. They raised this issue when they discussed
the factors that impede the coherence and the read-
ability of citation-based summaries. They suggested
that removing the fragments of a citing sentence that
are not relevant to the summarized paper will sig-
nificantly improve the quality of the produced sum-
maries. In their work, they defined the scope of a
given reference as the shortest fragment of the citing
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed. They identify the scope by generating
the syntactic parse tree of the sentence and then find-
ing the text that corresponds to the smallest subtree
rooted at an S node and contains the target reference
node as one of its leaf nodes. They admitted that
their method was very basic and works only when
the scope forms one grammatical fragment, which
is not true in many cases.
Athar (2011) noticed the same issue with cit-
ing sentences that cite multiple references, but this
time in the context of sentiment analysis in ci-
tations. He showed experimentally that identify-
ing what he termed the scope of citation influ-
ence improves sentiment classification accuracy. He
adapted the same basic method proposed by Abu-
Jbara and Radev (2011). We use this method as a
baseline in our evaluation below.
In addition to this related work, there is a large
body of research that used citing sentences in differ-
ent applications. For example, citing sentences have
been used to summarize the contributions of a scien-
tific paper (Qazvinian and Radev, 2008; Qazvinian
et al, 2010; Qazvinian and Radev, 2010; Abu-Jbara
and Radev, 2011). They have been also used to
generate surveys of scientific paradigms (Nanba and
Okumura, 1999; Mohammad et al, 2009). Several
other papers analyzed citing sentences to recognize
the citation function; i.e., the author?s reason for cit-
ing a given paper (Nanba et al, 2000; Teufel et al,
2006; Teufel, 2007). Schwartz et al (2007) pro-
posed a method for aligning the words within citing
sentences that cite the same paper. The goal of his
work was to aid named entity recognition and para-
phrase identification in scientific papers.
82
We believe that all the these applications will ben-
efit from the output of our work.
3 Problem Definition
The problem that we are trying to solve is to iden-
tify which fragments of a given citing sentence that
cites multiple references are semantically related
to a given target reference. As stated above, we
call these fragments the reference scope. Formally,
given a citing sentence S = {w1, w2, ..., wn} where
w1, w2, ..., wn are the tokens of the sentence and
given that S contains a set of two or more references
R, we want to assign the label 1 to the word wi if it
falls in the scope of a given target reference r ? R,
and 0 otherwise.
For example, sentences (4) and (5) below are
labeled for the target references Tetreault and
Chodorow (2008), and Cutting et al(1992) respec-
tively. The underlined words are labeled 1 (i.e.,
inside the target reference scope), while all others
are labeled 0.
(4) For example, Tetreault and Chodorow (2008) use a maximum
entropy classifier to build a model of correct preposition usage, with 7
million instances in their training set, and Lee and Knutsson (2008)
use memory-based learning, with 10 million sentences in their training
set.
(5) There are many POS taggers developed using different techniques
for many major languages such as transformation-based error-driven
learning (Brill, 1995), decision trees (Black et al, 1992), Markov
model (Cutting et al, 1992), maximum entropy methods (Ratnaparkhi,
1996) etc for English.
4 Approach
In this section, we present our approach for address-
ing the problem defined in the previous section. Our
approach involves two stages: 1) preprocessing and
2) reference scope identification. We present three
alternative methods for the second stage. The fol-
lowing two subsections describe the two stages.
4.1 Stage 1: Preprocessing
The goal of the preprocessing stage is to clean and
prepare the citing sentence for the next processing
steps. The second stage involves higher level text
processing such as part-of-speech tagging, syntac-
tic parsing, and dependency parsing. The available
tools for these tasks are not trained on citing sen-
tences which contain references written in a special
format. For example, it is very common in scien-
tific writing to have references (usually written be-
tween parentheses) that are not a syntactic part of the
sentence. It is also common to cite a group of ref-
erences who share the same contribution by listing
them between parentheses separated by a comma or
a semi-colon. We address these issues to improve
the accuracy of the processing done in the second
stage. The preprocessing stage involves three tasks:
4.1.1 Reference Tagging
The first preprocessing task is to find and tag all
the references that appear in the citing sentence.
Authors of scientific articles use standard patterns
to include references in text. We apply a regular
expression to find all the references that appear
in a sentence. We replace each reference with a
placeholder. The target reference is replaced by
TREF. Each other reference is replaced by REF.
We keep track of the original text of each replaced
reference. Sentence (6) below shows an example of
a citing sentence with the references replaced.
(6) These constraints can be lexicalized (REF.1; REF.2), un-
lexicalized (REF.3; TREF.4) or automatically learned (REF.5;
REF.6).
4.1.2 Reference Grouping
It is common in scientific writing to attribute one
contribution to a group of references. Sentence (6)
above contains three groups of references. Each
group constitutes one entity. Therefore, we replace
each group with a placeholder. We use GTREF
to replace a group of references that contains the
target reference, and GREF to replace a group of
references that does not contain the target reference.
83
Sentence (7) below is the same as sentence (6) but
with the three groups of references replaced.
(7) These constraints can be lexicalized (GREF.1), unlexicalized
(GTREF.2) or automatically learned (GREF.3).
4.1.3 Non-syntactic Reference Removal
A reference (REF or TREF) or a group of refer-
ences (GREF or GTREF) could either be a syntactic
constituent and has a semantic role in the sentence
(e.g. GTREF.1 in sentence (8) below) or not (e.g.
REF.2 in sentence (8)).
(8) (GTREF.1) apply fuzzy techniques for integrating source
syntax into hierarchical phrase-based systems (REF.2).
The task in this step is to determine whether a ref-
erence is a syntactic component in the sentence or
not. If yes, we keep it as is. If not, we remove it
from the sentence and keep track of its position. Ac-
cordingly, after this step, REF.2 in sentence (8) will
be removed. We use a rule-based algorithm to deter-
mine whether a reference should be removed from
the sentence or kept. Our algorithm (Algorithm 1)
uses stylistic and linguistic features such as the style
of the reference, the position of the reference, and
the surrounding words to make the decision.
When a reference is removed, we pick a word
from the sentence to represent it. This is needed for
feature extraction in the next stage. We use as a rep-
resentative the head of the closest noun phrase (NP)
that comes before the position of the removed refer-
ence. For example, in sentence (8) above, the closest
NP before REF.2 is hierarchical phrase-based sys-
tems and the head is the noun systems.
4.2 Stage 2: Reference Scope Identification
In this section we present three different methods
for identifying the scope of a given reference within
a citing sentence. We compare the performance of
these methods in Section 5. The following three sub-
sections describe the methods.
Algorithm 1 Remove Non-syntactic References
Require: A citing sentence S
1: for all Reference R (REF, TREF, GREF, or GTREF)
in S do
2: if R style matches ?Authors (year)? then
3: Keep R // syntactic
4: else if R is the first word in the sentence or in a
clause then
5: Keep R // syntactic
6: else if R is preceded by a preposition (in, of, by,
etc.) then
7: Keep R // syntactic
8: else
9: Remove R // non-syntactic
10: end if
11: end for
4.2.1 Word Classification
In this method we define reference scope identifi-
cation as a classification task of the individual words
of the citing sentence. Each word is classified as
inside or outside the scope of a given target refer-
ence. We use a number of linguistic and structural
features to train a classification model on a set of
labeled sentences. The trained model is then used
to label new sentences. The features that we use to
train the model are listed in Table 1. We use the
Stanford parser (Klein and Manning, 2003) for syn-
tactic and dependency parsing. We experiment with
two classification algorithms: Support Vector Ma-
chines (SVM) and logistic regression.
4.2.2 Sequence Labeling
In the method described in Section 4.2.1 above,
we classify each word independently from the la-
bels of the nearby words. The nature of our task,
however, suggests that the accuracy of word classifi-
cation can be improved by considering the labels of
the words surrounding the word being classified. It
is very likely that the word takes the same label as
the word before and after it if they all belong to the
same clause in the sentence. In this method we de-
fine the problem as a sequence labeling task. Now,
instead of looking for the best label for each word
individually, we look for the globally best sequence
84
Feature Description
Distance The distance (in words) between the word and the target reference.
Position This feature takes the value 1 if the word comes before the target reference, and 0 otherwise.
Segment After splitting the sentence into segments by punctuation and coordination conjunctions, this feature takes
the value 1 if the word occurs in the same segment with the target reference, and 0 otherwise.
Part of speech tag The part of speech tag of the word, the word before, and the word after.
Dependency Distance Length of the shortest dependency path (in the dependency parse tree) that connects the word to the tar-
get reference or its representative. It has been shown in previous work on relation extraction that the
shortest path between any two entities captures the information required to assert a relationship between
them (Bunescu and Mooney, 2005)
Dependency Relations This item includes a set of features. Each features corresponds to a dependency relation type. If the relation
appears in the dependency path that connects the word to the target reference or its representative, its
corresponding feature takes the value 1, and 0 otherwise.
Common Ancestor Node The type of the node in the syntactic parse tree that is the least common ancestor of the word and the target
reference.
Syntactic Distance The number of edges in the shortest path that connects the word and the target reference in the syntactic
parse tree.
Table 1: The features used for word classification and sequence labeling
of labels for all the words in the sentence at once.
We use Conditional Random Fields (CRF) as our
sequence labeling algorithm. In particular, we use
first-order chain-structured CRF. The chain consists
of two sets of nodes: a set of hidden nodes Y which
represent the scope labels (0 or 1) in our case, and
a set of observed nodes X which represent the ob-
served features. The task is to estimate the probabil-
ity of a sequence of labels Y given the sequence of
observed features X: P (Y|X)
Lafferty et al (2001) define this probability to be
a normalized product of potential functions ?:
P (y|x) =
?
t
?k(yt, yt?1, x) (1)
Where ?k(yt, yt?1, x) is defined as
?k(yt, yt?1, x) = exp(
?
k
?kf(yt, yt?1, x)) (2)
where f(yt, yt?1, x) is a transition feature func-
tion of the label at positions i ? 1 and i and the
observation sequence x; and ?j is parameter to be
estimated from training data. We use, as the obser-
vations at each position, the same features that we
used in Section 4.2.1 above (Table 1).
4.2.3 Segment Classification
We noticed that the scope of a given reference
often consists of units of higher granularity than
words. Therefore, in this method, we split the
sentence into segments of contiguous words and,
instead of labeling individual words, we label
the whole segment as inside or outside the scope
of the target reference. We experimented with
two different segmentation methods. In the first
method (method-1), we segment the sentence at
punctuation marks, coordination conjunctions, and
a set of special expressions such as ?for example?,
?for instance?, ?including?, ?includes?, ?such as?,
?like?, etc. Sentence (8) below shows an example of
this segmentation method (Segments are enclosed
in square brackets).
(8) [Rerankers have been successfully applied to numerous NLP
tasks such as] [parse selection (GTREF)], [parse reranking (GREF)],
[question-answering (REF)].
In the second segmentation method (method-2),
we split the sentence into segments of finer gran-
ularity. We use a chunking tool to identify noun
groups, verb groups, preposition groups, adjective
85
groups, and adverb groups. Each such group (or
chunk) forms a segment. If a word does not belong
to any chunk, it forms a singleton segment by
itself. Sentence (9) below shows an example of this
segmentation method (Segments are enclosed in
square brackets).
(9) [To] [score] [the output] [of] [the coreference models],
[we] [employ] [the commonly-used MUC scoring program (REF)]
[and] [the recently-developed CEAF scoring program (TREF)].
We assign a label to each segment in two steps. In
the first step, we use the sequence labeling method
described in Section 4.2.2 to assign labels to all the
individual words in the sentence. In the second step,
we aggregate the labels of all the words contained in
a segment to assign a label to the whole segment. We
experimented with three different label aggregation
rules: 1) rule-1: assign to the segment the majority
label of the words it contains, and 2) rule-2: assign
to the segment the label 1 (i.e., inside) if at least one
of the words contained in the segment is labeled 1,
and assign the label 0 to the segment otherwise, and
3) rule-3: assign the label 0 to the segment if at least
of the words it contains is labeled 0, and assign 1
otherwise.
5 Evaluation
5.1 Data
We use the ACL Anthology Network corpus
(AAN) (Radev et al, 2009) in our evaluation. AAN
is a publicly available collection of more than 19,000
NLP papers. AAN provides a manually curated cita-
tion network of its papers and the citing sentence(s)
associated with each edge. The current release of
AAN contains about 76,000 unique citing sentences
56% of which contain 2 or more references and 44%
contain 1 reference only. From this set, we ran-
domly selected 3500 citing sentences, each contain-
ing at least two references (3.75 references on aver-
age with a standard deviation of 2.5). The total num-
ber of references in this set of sentences is 19,591.
We split the data set into two random subsets:
a development set (200 sentences) and a train-
ing/testing set (3300 sentences). We used the devel-
opment set to study the data and develop our strate-
gies of addressing the problem. The second set was
used to train and test the system in a cross-validation
mode.
5.2 Annotation
We asked graduate students with good background
in NLP (the area of the annotated sentences) to pro-
vide three annotations for each sentence in the data
set described above. First, we asked them to de-
termine whether each of the references in the sen-
tence was correctly tagged or not. Second, we asked
them to determine for each reference whether it is a
syntactic constituent in the sentence or not. Third,
we asked them to determine and label the scope of
one reference in each sentence which was marked
as a target reference (TREF). We designed a user-
friendly tool to collect the annotations from the stu-
dents.
To estimate the inter-annotator agreement, we
picked 500 random sentences from our data set and
assigned them to two different annotators. The inter-
annotator agreement was perfect on both the refer-
ence tagging annotation and the reference syntacti-
cality annotation. This is expected since both are ob-
jective, clear, and easy tasks. To measure the inter-
annotator agreement on the scope annotation task,
we deal with it as a word classification task. This
allows us to use the popular classification agreement
measure, the Kappa coefficient (Cohen, 1968). The
Kappa coefficient is defined as follows:
K =
P (A)? P (E)
1? P (E)
(3)
where P(A) is the relative observed agreement
among raters and P(E) is the hypothetical probabil-
ity of chance agreement. The agreement between
the two annotators on the scope identification task
was K = 0.61. On Landis and Kochs (Landis and
Koch, 1977) scale, this value indicates substantial
agreement.
86
5.3 Experimental Setup
We use the Edinburgh Language Technology Text
Tokenization Toolkit (LT-TTT) (Grover et al, 2000)
for text tokenization, part-of-speech tagging, chunk-
ing, and noun phrase head identification. We use
the Stanford parser (Klein and Manning, 2003) for
syntactic and dependency parsing. We use Lib-
SVM (Chang and Lin, 2011) for Support Vector Ma-
chines (SVM) classification. Our SVM model uses a
linear kernel. We use Weka (Hall et al, 2009) for lo-
gistic regression classification. We use the Machine
Learning for Language Toolkit (MALLET) (McCal-
lum, 2002) for CRF-based sequence labeling. In
all the scope identification experiments and results
below, we use 10-fold cross validation for train-
ing/testing.
5.4 Preprocessing Component Evaluation
We ran our three rule-based preprocessing modules
on the testing data set and compared the output to
the human annotations. The test set was not used
in the tuning of the system but was done using the
development data set as described above. We report
the results for each of the preprocessing modules.
Our reference tagging module achieved 98.3% pre-
cision and 93.1% recall. Most of the errors were
due to issues with text extraction from PDF or due
to bad references practices by some authors (i.e., not
following scientific referencing standards). Our ref-
erence grouping module achieved perfect accuracy
for all the correctly tagged references. This was
expected since this is a straightforward task. The
non-syntactic reference removal module achieved
90.08% precision and 90.1% recall. Again, most of
the errors were the result of bad referencing prac-
tices by the authors.
5.5 Reference Scope Identification
Experiments
We conducted several experiments to compare the
methods proposed in Section 4 and their variants.
We ran all the experiments on the training/testing
set (the 3300 sentences) described in Section 5.1.
Method Accuracy Precision Recall F-measure
AR-2011 54.0% 63.3% 33.1% 41.5%
WC-SVM 74.9% 74.5% 93.4% 82.9%
WC-LR 74.3% 76.8% 88.0% 82.0%
SL-CRF 78.2% 80.1% 94.2% 86.6%
SC-S1-R1 73.7% 72.1% 97.8% 83.0%
SC-S1-R2 69.3% 68.4% 98.9% 80.8%
SC-S1-R3 60.0% 61.8% 73.3% 60.9%
SC-S2-R1 81.8% 81.2% 93.8% 87.0%
SC-S2-R2 78.2% 77.3% 94.9% 85.2%
SC-S2-R3 66.1% 67.1% 71.2% 69.1%
Table 3: Results of scope identification using the different
algorithms described in the paper
The experiments that we ran are as follows: 1) word
classification using a SVM classifier (WC-SVM);
2) word classification using a logistic regression
classifier(WC-LR); 3) CRF-based sequence labeling
(SL-CRF); 4) segment classification using segmen-
tation method-1 and label aggregation rule-1 (SC-
S1-R1); 5,6,7,8,9) same as (4) but using different
combinations of segmentation methods 1 and 2, and
label aggregation rules 1,2 and 3: SC-S1-R2, SC-
S1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sx
refers to segmentation method x and Ry refers to
label aggregation rule y all as explained in Sec-
tion 4.2.3). Finally, 10) we compare our meth-
ods to the baseline method proposed by Abu-Jbara
and Radev (2011) which was described in Section 4
(AR-2011).
To better understand which of the features listed
in Table 1 are more important for the task, we use
Guyon et al?s (2002) method for feature selection
using SVM to rank the features based on their im-
portance. The results of the experiments and the
feature analysis are presented and discussed in the
following subsection.
5.6 Results and Discussion
5.6.1 Experimental Results
We ran the experiments described in the previ-
ous subsection on the testing data described in Sec-
87
Method Output
E
xa
m
pl
e
1 Word Classification
(WC-SVM)
A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Sequence Labeling (SL-
CRF)
A wide range of contextual information, such as surrounding words (GREF), dependency or case structure
(GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Segment Classification
(SC-S2-R1)
A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved
considerable success.
E
xa
m
pl
e
2 Word Classification
(WC-SVM)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Sequence Labeling (SL-
CRF)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Segment Classification
(SC-S2-R1)
Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(REF).
Table 2: Two example outputs produced by the three methods
tion 5.1. Table 3 compares the precision, recall, F1,
and accuracy for the three methods described in Sec-
tion 4 and their variations. All the metrics were com-
puted at the word level. The results show that all our
methods outperform the baseline method AR-2011
that was proposed by Abu-Jbara and Radev (2011).
In the word classification method, we notice no sig-
nificant difference between the performance of the
SVM vs Logistic Regression classifier. We also no-
tice that the CRF-based sequence labeling method
performs significantly better than the word classi-
fication method. This result corroborates our intu-
ition that the labels of neighboring words are de-
pendent. The results also show that segment la-
beling generally performs better than word label-
ing. More specifically, the results indicate that seg-
mentation based on chunking and the label aggre-
gation based on plurality when used together (i.e.,
SC-S2-R1) achieve higher precision, accuracy, and
F-measure than the punctuation-based segmentation
and the other label aggregation rules.
Table 2 shows the output of the three methods on
two example sentences. The underlined words are
labeled by the system as scope words.
5.6.2 Feature Analysis
We performed an analysis of our classification
features using Guyon et al (2002) method. The
analysis revealed that both structural and syntactic
features are important. Among the syntactic fea-
tures, the dependency path is the most important.
Among the structural features, the segment feature
(as described in Table 1) is the most important.
6 Conclusions
We presented and compared three different meth-
ods for reference scope identification: word classi-
fication, sequence labeling, and segment classifica-
tion. Our results indicate that segment classification
achieves the best performance. The next direction in
this research is to extract the scope of a given refer-
ence as a standalone grammatical sentence. In many
cases, the scope identified by our method can form
a grammatical sentence with no or minimal postpro-
cessing. In other cases, more advanced text regener-
ation techniques are needed for scope extraction.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
88
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Christine L. Borgman and Jonathan Furner. 2002. Schol-
arly communication and bibliometrics. ANNUAL RE-
VIEW OF INFORMATION SCIENCE AND TECH-
NOLOGY, 36(1):2?72.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213?220.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147?1154.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Mach.
Learn., 46:389?422, March.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Terttu Luukkonen. 1992. Is scientists? publishing be-
haviour rewardseeking? Scientometrics, 24:297?319.
10.1007/BF02017913.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In In Proceedings of the SI-
GIR04 workshop on Search and Discovery in Bioin-
formatics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
89
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ariel Schwartz, Anna Divoli, and Marti Hearst. 2007.
Multiple alignment of citation sentences with con-
ditional random fields and posterior decoding. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 847?857.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
90
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 33?36,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
AttitudeMiner: Mining Attitude from Online Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
This demonstration presents AttitudeMiner, a
system for mining attitude from online dis-
cussions. AttitudeMiner uses linguistic tech-
niques to analyze the text exchanged between
participants of online discussion threads at dif-
ferent levels of granularity: the word level, the
sentence level, the post level, and the thread
level. The goal of this analysis is to iden-
tify the polarity of the attitude the discussants
carry towards one another. Attitude predic-
tions are used to construct a signed network
representation of the discussion thread. In this
network, each discussant is represented by a
node. An edge connects two discussants if
they exchanged posts. The sign (positive or
negative) of the edge is set based on the po-
larity of the attitude identified in the text asso-
ciated with the edge. The system can be used
in different applications such as: word polar-
ity identification, identifying attitudinal sen-
tences and their signs, signed social network
extraction from text, subgroup detect in dis-
cussion. The system is publicly available for
download and has an online demonstration at
http://clair.eecs.umich.edu/AttitudeMiner/.
1 Introduction
The rapid growth of social media has encouraged
people to interact with each other and get involved
in discussions more than anytime before. The most
common form of interaction on the web uses text
as the main communication medium. When people
discuss a topic, especially when it is a controversial
one, it is normal to see situations of both agreement
and disagreement among the discussants. It is even
not uncommon that the big group of discussants split
into two or more smaller subgroups. The members
of each subgroup mostly agree and show positive
attitude toward each other, while they mostly dis-
agree with the members of opposing subgroups and
possibly show negative attitude toward them. These
forms of sentiment are expressed in text by using
certain language constructs (e.g. use insult or nega-
tive slang to express negative attitude).
In this demonstration, we present a system that
applies linguistic analysis techniques to the text of
online discussions to predict the polarity of relations
that develop between discussants. This analysis is
done on words to identify their polarities, then on
sentences to identify attitudinal sentences and the
sign of attitude, then on the post level to identify the
sign of an interaction, and finally on the entire thread
level to identify the overall polarity of the relation.
Once the polarity of the pairwise relations that de-
velop between interacting discussants is identified,
this information is then used to construct a signed
network representation of the discussion thread.
The system also implements two signed network
partitioning techniques that can be used to detect
how the discussants split into subgroups regarding
the discussion topic.
The functionality of the system is based on
our previous research on word polarity identifica-
tion (Hassan and Radev, 2010) and attitude identifi-
cation (Hassan et al, 2010). The system is publicly
available for download and has a web interface to try
online1.
This work is related to previous work in the areas
of sentiment analysis and online discussion mining.
Many previous systems studied the problem of iden-
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003). Opinionfinder (Wilson et al, 2005a) is a sys-
tem for mining opinions from text. Another research
line focused on analyzing online discussions. For
example, Lin et al (2009) proposed a sparse coding-
based model that simultaneously models the seman-
tics and the structure of threaded discussions and
Shen et al (2006) proposed a method for exploit-
ing the temporal information in discussion streams
to identify the reply structure of the dialog. Many
systems addressed the problem of extracting social
networks from data (Elson et al, 2010; McCallum
et al, 2007), but none of them considered both pos-
itive and negative relations.
In the rest of the paper, we describe the system
architecture, implementation, usage, and its perfor-
1http://clair.eecs.umich.edu/AttitudeMiner/
33
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Text Polarity 
Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Attitude Identification 
? Identify Attitudinal 
Sentences 
? Predict the sign on 
attitude 
 
 
Post sign identification 
? Aggregate the signs of 
attitudinal sentences to 
assign a sign to the 
post. 
 
Relation Sign 
? Aggregate the signs of all 
the posts exchanged by 
interacting participants 
to assign a sign for their 
relation. 
Signed Network 
 
 
 
 
 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text 
? Split posts into sentences 
 
+ _ 
Figure 1: Overview of the system processing pipeline
mance evaluation.
2 System Overview
Figure 1 shows a block diagram of the system com-
ponents and the processing pipeline. The first com-
ponent in the system is the thread parsing com-
ponent which takes as input a discussion thread
and parses it to identify the posts, the participants,
and the reply structure of the thread. This compo-
nent uses a module from CLAIRLib (Abu-Jbara and
Radev, 2011) to tokenize the posts and split them
into sentences.
The second component in the pipeline processes
the text of the posts to identify polarized words and
tag them with their polarity. This component uses
the publicly available tool, opinionfinder (Wilson et
al., 2005a), as a framework for polarity identifica-
tion. This component uses an extended polarity lex-
icon created by applying a random walk model to
WordNet (Miller, 1995) and a set of seed polarized
words. This approach is described in detail in our
previous work (Hassan and Radev, 2010). The con-
text of words is taken into consideration by running
a contextual word classifier that determines whether
the word is used in a polarized sense given the con-
text (Wilson et al, 2005b). For example, a positive
word appearing in a negated scope is used in a neg-
ative, rather than a positive sense.
The next component is the attitude identification
component. Given a sentence, our model predicts
whether it carries an attitude from the text writer to-
ward the text recipient or not. As we are only in-
terested in attitudes between participants, we limit
our analysis to sentences that use mentions of a dis-
cussion participants (i.e. names or second person
pronouns). We also discard all sentences that do
not contain polarized expressions as detected by the
previous component. We extract several patterns at
different levels of generalization representing any
given sentence. We use words, part-of-speech tags,
and dependency relations. We use those patterns to
build two Markov models for every kind of patterns.
The first model characterizes the relation between
different tokens for all patterns that correspond to
sentences that have an attitude. The second model
is similar to the first one, but it uses all patterns that
correspond to sentences that do not have an attitude.
Given a new sentence, we extract the corresponding
patterns and estimate the likelihood of every pattern
being generated from the two corresponding mod-
els. We then compute the likelihood ratio of the sen-
tence under every pair of models. Notice that we
have a pair of models corresponding to every type of
patterns. The likelihood ratios are combined using a
linear model, the parameters of which are estimated
using a development dataset. Please refer to (Hassan
et al, 2010) for more details about this component.
The next component works on the post level. It
assigns a sign to each post based on the signs of the
sentences it contains. A post is classified as negative
if it has at leastNs negative sentences, otherwise it is
classified as positive. The value ofNs can be chosen
by the user or set to default which was estimated
using a small labeled development set. The default
value forNs is 1 (i.e. if the post contains at least one
negative sentence, the whole post is considered to be
negative).
The next component in the pipeline uses the atti-
tude predictions from posts to construct a signed net-
work representation of the discussion thread. Each
participant is represented by a node. An edge is
created between two participants if they interacted
with each other. A sign (positive or negative) is as-
signed to an edge based on the signs of the posts
the two participants connected by the edge have ex-
changed. This is done by comparing the number of
positive and negative posts. A negative sign is given
if the two participants exchanged at least Np nega-
tive posts. The value of Np can be set using a devel-
opment set. The default value is 1.
The last component is the subgroup identifica-
34
Figure 2: The web interface for detecting subgroups in discussions
tion component. This component provides imple-
mentations for two signed network partitioning algo-
rithms. The first one is a greedy optimization algo-
rithm that is based on the principals of the structural
balance theory. The algorithm uses a criterion func-
tion for a local optimization partitioning such that
positive links are dense within groups and negative
links are dense between groups. The algorithm is de-
scribed in detail in (Doreian and Mrvar, 1996). The
second algorithm is FEC (Yang et al, 2007). FEC
is based on an agent-based random walk model. It
starts by finding a sink community, and then extract-
ing it from the entire network based on a graph cut
criteria that Yang et al (2007) proposed. The same
process is then applied recursively to the extracted
community and the rest of the network.
3 Implementation Details
The system is implemented in Perl. Some of the
components in the processing pipeline use external
tools that are implemented in either Perl, Java, or
Python. All the external tools come bundled with the
system. The system is compatible with all the ma-
jor platforms including windows, Mac OS, and all
Linux distributions. The installation process is very
straightforward. There is a single installation script
that will install the system, install all the dependen-
cies, and do all the required configurations. The in-
stallation requires that Java JRE, Perl, and Python be
installed on the machine.
The system has a command-line interface that
provides full access to the system functionality. The
command-line interface can be used to run the whole
pipeline or any portion of it. It can also be used to ac-
cess any component directly. Each component has a
corresponding script that can be run separately. The
input and output specifications of each component
are described in the accompanying documentation.
All the parameters that control the performance of
the system can also be passed through the command-
line interface.
The system can process any discussion thread that
is input to it in a specific XML format. The fi-
nal output of the system is also in XML format.
The XML schema of the input/output is described
in the documentation. It is the user responsibil-
ity to write a parser that converts an online discus-
sion thread to the expected XML format. The sys-
tem package comes with three such parsers for three
different discussion sites: www.politicalforum.com,
groups.google.com, and www.createdebate.com.
The distribution also comes with three datasets
(from three different sources) comprising a total of
300 discussion threads. The datasets are annotated
with the subgroup labels of discussants. Included in
the distribution as well, a script for generating a vi-
sualization of the extracted signed network and the
identified subgroups.
AttitudeMiner also has a web interface that
demonstrates most of its functionality. The web in-
35
Figure 3: The web interface for identifying attitudinal
sentences and their polarity
terface is intended for demonstration purposes only.
No webservice is provided. Figure 2 and Figrue 3
show two screenshots for the web interface.
4 System Performance
In this section, we give a brief summary of the sys-
tem performance. The method that generates the
extended polarity lexicon that is used for word po-
larity identification achieves 88.8% accuracy as re-
ported in (Hassan and Radev, 2010). The attitude
identification component distinguishes between at-
titudinal and non-attitudinal sentences with 80.3%
accuracy, and predicts the signs of attitudinal sen-
tences with 97% accuracy as reported in (Hassan et
al., 2010). Our evaluation for the signed network
extraction component on a large annotated dataset
showed that it achieves 83.5% accuracy. Finally, our
experiments on an annotated discussion showed that
the system can detect subgroups with 77.8% purity.
The system was evaluated using a dataset with thou-
sands of posts labeled by human annotators.
5 Conclusion
We presented of a demonstration of a social me-
dia mining system that used linguistic analysis tech-
niques to understand the relations that develop be-
tween users in online communities. The system is
capable of analyzing the text exchanged during dis-
cussions and identifying positive and negative atti-
tudes. Positive attitude reflects a friendly relation
while negative attitude is a sign of an antagonistic
relation. The system can also use the attitude infor-
mation to identify subgroups with a homogeneous
and common focus among the discussants. The sys-
tem predicts attitudes and identifies subgroups with
high accuracy.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In ACL-HLT 2011-
Demo, June.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, pages 138?147, Uppsala, Sweden, July.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In ACL 2010.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP - Demo.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng.
36
Proceedings of NAACL-HLT 2013, pages 596?606,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Purpose and Polarity of Citation: Towards NLP-based Bibliometrics
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Jefferson Ezra
Department of EECS
University of Michigan
Ann Arbor, MI, USA
jezra@umich.edu
Dragomir Radev
Department of EECS
and School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
Bibliometric measures are commonly used to
estimate the popularity and the impact of pub-
lished research. Existing bibliometric mea-
sures provide ?quantitative? indicators of how
good a published paper is. This does not nec-
essarily reflect the ?quality? of the work pre-
sented in the paper. For example, when h-
index is computed for a researcher, all incom-
ing citations are treated equally, ignoring the
fact that some of these citations might be neg-
ative. In this paper, we propose using NLP
to add a ?qualitative? aspect to biblometrics.
We analyze the text that accompanies citations
in scientific articles (which we term citation
context). We propose supervised methods for
identifying citation text and analyzing it to de-
termine the purpose (i.e. author intention) and
the polarity (i.e. author sentiment) of citation.
1 Introduction
An objective and fair evaluation of the impact
of published research requires both quantitative
and qualitative assessment. Existing bibliometric
measures such as H-Index (Hirsch, 2005; Hirsch,
2010), G-index (Egghe, 2006), and Impact Fac-
tor (Garfield, 1994) focus on the quantitative aspect
of this evaluation which dose not always correlate
with the qualitative aspect.
For example, the number of papers published by
a researcher only tells how productive she or he is.
It does not say anything about the quality or the im-
pact of the work. Similarly, the number of citations
that a paper receives should not be used to gauge
the quality of the work as it really only measures
the popularity of the work and the interest of other
researchers in it (Garfield, 1979). Controversial pa-
pers or those based on fabricated data or experiments
may receive a large number of citations. A popular
example of fraudulent research that deceived many
researchers and caught media attention was the case
of a South Korean research scientist, Hwang Woo-
suk, who was found to have faked his research re-
sults in the area of human stem cell cloning. His re-
search was published in Science and received close
to 200 citations after the fraud was discovered. The
vast majority of those citations were negative.
This suggests that the purpose of citation should
be taken into consideration when biblometric mea-
sures are computed. Negative citations should be
weighted less than positive or neutral citations. This
motivates the need to automatically distinguish be-
tween positive, negative, and neutral citations and to
identify the purpose of a citation; i.e. the author?s in-
tention behind choosing a published article and cit-
ing it.
This analysis of citation purpose and polarity can
be useful for many applications. For example, it can
be used to build systems that help funding agencies
and hiring committees at universities and research
institutions evaluate researchers? work more accu-
rately. It can also be used as a preprocessing step in
systems that process scholarly data. For example,
citation-based summarization systems (Qazvinian
and Radev, 2008; Qazvinian et al, 2010; Abu-
Jbara and Radev, 2011) and survey generation sys-
tems (Mohammad et al, 2009; Qazvinian et al,
2013) can benefit from citation purpose and polar-
ity analysis to improve paper and content selection.
In this paper, we investigate the use of linguis-
tic analysis techniques to automatically identify the
purpose of citing a paper and the polarity of this cita-
tion. We first present a sequence labeling method for
extracting the text that cites a given target reference;
i.e. the text that appears in a scientific article and
refers to another article and comments on it. We use
the term citation context to refer to this text. Next,
596
we use supervised classification techniques to ana-
lyze this text and identify the purpose and polarity
of citation.
The rest of this paper is organized as follows. Sec-
tion 2 reviews the related work. We present our ap-
proach in Section 3. We then describe the data and
experiments in Section 4. Finally, Section 5 con-
cludes the paper and suggests directions for future
work.
2 Related Work
Our work is related to a large body of research
on citations. Studying citation patterns and ref-
erencing practices has interested researchers for
many years (Hodges, 1972; Garfield et al, 1984).
White (2004) provides a good survey of the differ-
ent research directions that study or use citations. In
the following subsections, we review three lines of
research that are closely related to our work.
2.1 Citation Context Identification
The first line of related research addresses the prob-
lem of identifying citation context. The context of a
citation that cites a given target paper can be a set of
sentences, one sentence, or a fragment of a sentence.
Nanba and Okumura (1999) use the term citing
area to refer to the same concept. They define the
citing area as the succession of sentences that ap-
pear around the location of a given reference in a
scientific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al, 2000), they use their
algorithm to improve citation type classification and
automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
In previous work, we addressed the issue of iden-
tifying the scope of a given target reference in citing
sentences that contain multiple references (2012).
Our definition of reference scope was limited to
fragments of the explicit citing sentence (i.e. the
sentence in which actual citation appears). That
method does not identify related text in surrounding
sentences.
In this work, we propose a supervised sequence
labeling method for identifying the citation context
of given reference which includes the explicit citing
sentence and the related surrounding sentences.
2.2 Citation Purpose Classification
Several research efforts have focused on studying
the different purposes for citing a paper (Garfield,
1964; Weinstock, 1971; Moravcsik and Muruge-
san, 1975; and Moitra, 1975; Bonzi, 1982).
Bonzi (1982) studied the characteristics of citing
and cited works that may aid in determining the re-
latedness between them. Garfield (1964) enumer-
ated several reasons why authors cite other publi-
cations, including ?alerting researchers to forthcom-
ing work?, paying homage to the leading scholars
in the area, and citations which provide pointers to
background readings. Weinstock (1971) adopted the
same scheme that Garfield proposed in her study of
citations.
Spiegel-Rosing (1977) proposed 13 categories for
citation purpose based on her analysis of the first
four volumes of Science Studies. Some of them are:
Cited source is the specific point of departure for
the research question investigated, Cited source con-
tains the concepts, definitions, interpretations used,
Cited source contains the data used by the citing pa-
per. Nanba and Okumura (1999) came up with a
simple schema composed of only three categories:
Basis, Comparison, and other Other. They pro-
posed a rule-based method that uses a set of statis-
tically selected cue words to determine the category
of a citation. They used this classification as a first
step for scientific paper summarization. Teufel et
al. (2006), in their work on citation function classifi-
cation, adopted 12 categories from Spiegel-Rosing?s
taxonomy. They trained an SVM classifier and used
it to label each citing sentence with exactly one cat-
egory. Further, they mapped the twelve categories to
four top level categories namely: weakness, contrast
597
(4 categories), positive (6 categories) and neutral.
The taxonomy that we use in this work is based
on previous work. We adopt a scheme that contains
six categories. We selected the six categories after
studying all the previously used citation taxonomies.
We included the ones we believed are important for
improving bibliometric measures and for the appli-
cations that we are planning to pursue in the future
(Section 5).
2.3 Citation Polarity Classification
The polarity (or sentiment) of a citation has also
been studied previously. Previous work showed
that positive and negative citations are common, al-
though negative citations might be expressed indi-
rectly or in an implicit way (Ziman, 1968; Mac-
Roberts and MacRoberts, 1984; THOMPSON and
YIYUN, 1991). Athar (2011) addressed the prob-
lem of identifying sentiment in citing sentences. He
used a set of structure-based features to train a ma-
chine learning classifier using annotated data. This
work uses the citing sentence only to predict senti-
ment. Context sentences were ignored. Athar and
Teufel (2012a) observed that taking the context into
consideration when judging sentiment in citations
increases the number of negative citations by a fac-
tor of 3. They proposed two methods for utilizing
the context. In the first method, they treat the citing
sentence and a fixed context (a window of four sen-
tences around the citing sentence) as if they were
a single sentence. They extract features from the
merged text and train a classifier similar to what they
did in their 2011 paper. In the second method, they
use a four-class annotation scheme. Each sentence
in a window of four sentences around the citation
is labeled as positive, negative, neutral, or excluded
(unrelated to the cited work). There experiments
surprisingly gave negative results and showed that
classifying sentiment without considering the con-
text achieves better results. They attributed this to
the small size of their training data and to the noise
that including the context text introduces to the data.
In (Athar and Teufel, 2012b), the authors present a
method for automatically identifying all the men-
tions of the cited paper in the citing paper. They
show that considering all the mentions improves the
performance of detecting sentiment in citations.
In our work, we propose a sequence labeling
method for identifying the citation context first, and
then use a supervised approach to determine the po-
larity of a given citation.
3 Approach
In this section, we describe our approach to three
tasks: citation context identification, citation pur-
pose classification, and citation polarity identifica-
tion. We also describe a preprocessing stage that is
applied to the citation text before performing any of
the three tasks.
3.1 Preprocessing
The goal of the preprocessing stage is to clean and
prepare the citation text for part-of-speech tagging
and parsing. The available POS taggers and parsers
are not trained on citation text. Citation text is dif-
ferent from normal text in that it contains references
written in a special format (e.g., author names and
publication year written in parentheses; or reference
indices written in square brackets). Many citing sen-
tences contain multiple references, some of which
might be grouped together in a pair of parentheses
and separated by a comma or a semi-colons. These
references are usually not syntactic nor semantic
constituents of the sentences they appear in. This
results in many POS tagging and parsing errors. We
address this issue in the pre-processing stage to im-
prove the performance of the feature extraction com-
ponent. We perform three pre-processing steps:
a. Reference Tagging: In the first step, we find
and tag all the references that appear in the text. We
use a regular expression to find references and re-
place each reference with a placeholder. The ref-
erence to the target paper is replaced by the place-
holder TREF. Each other reference is replaced by
REF.
b. Reference Grouping: In this step, we identify
grouped references (i.e. multiple references listed
between one pair of parentheses separated by semi-
colons). Each such group is replaced by a place-
holder, GREF. If the target reference is a member of
the group, we use a different placeholder: GTREF.
c. Non-syntactic Reference Removal: A refer-
ence or a group of references could either be a syn-
tactic constituent and has a semantic role in the sen-
tence or not (Whidby, 2012; Abu Jbara and Radev,
2012). If the reference is not a syntactic compo-
598
Feature Description
Demonstrative determiners Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these,
etc.), and 0 otherwise.
Conjunctive adverbs Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore,
Accordingly, etc.), and 0 otherwise.
Position Position of the current sentence with respect to the citing sentence. This feature takes one of four
values: -1, 0, 1, and 2.
Contains Closest Noun Phrase Takes a value of 1 if the current sentence contains closest noun phrase (if any) immediately before
the reference position in the citing sentence, and 0 otherwise. This noun phrase often is the name of
a method, a tool, or corpus originating from the cited reference.
2-3 grams The first bigram and trigram in the sentence (This approach, One problem with, etc.).
Contains Other references Takes a value of 1 if the current sentence contains references other than the target, and 0 otherwise.
Contains a Mention of target reference Takes a value of 1 if the current sentence contains a mention (explicit or anaphoric) of the target
reference, and 0 otherwise.
Multiple references Takes a value of 1 if the citing sentence contains multiple references, and 0 otherwise. If the cit-
ing sentence contains multiple references, it becomes less likely that the surrounding sentences are
related.
Table 1: Features used for citation context identification
nent in the sentence, we remove it to reduce pars-
ing errors. Following our previous work (Abu Jbara
and Radev, 2012), we use a rule-based algorithm to
determine whether a reference should be removed
from the sentence or kept. The algorithm uses stylis-
tic and linguistic features such as the style of the
reference, the position of the reference, and the sur-
rounding words to make the decision. When a ref-
erence is removed, the head of the closest noun
phrase (NP) immediately before the position of the
removed reference is used as a representative of the
reference. This is needed for feature extraction as
shown later in the paper.
3.2 Citation Context Identification
The task of identifying the citation context of a given
target reference can be formally defined as follows.
Given a scientific article A that cites another article
B, find a set of sentences in A that talk about the
work done in B such that at least one of these sen-
tences contains an explicit reference to B.
We treat this problem as a sequence labeling prob-
lem. The goal is to find the globally best sequence
of labels for all the sentences that appear within a
window around the citing sentence. The citing sen-
tence is the one that contains an explicit reference
to the cited paper. Each sentence within the window
is labeled as INCLUDED or EXCLUDED from the
citation context of the given target paper. To deter-
mine the size of the window, we examined a devel-
opment set of 300 sentences. We noticed that the re-
lated context almost always falls within a window of
four sentences. The window includes the citing sen-
tence, one sentence before the citing sentence, and
two sentences after the citing sentence.
We use Conditional Random Fields (CRFs) for
sequence labeling. In particular, we use a first-order
chain-structured CRF. The chain consists of two sets
of nodes: 1) a set of hidden nodes Y which represent
the context labels of sentences (INCLUDED or EX-
CLUDED), and 2) a set of observed nodes X which
represent the features extracted from the sentences.
The task is to estimate the probability of a sequence
of labels Y given the sequence of observed features
X: P (Y|X)
Lafferty et al (2001) define this probability to be
a normalized product of potential functions ?:
P (y|x) =
?
t
?k(yt, yt?1, x) (1)
Where ?k(yt, yt?1, x) is defined as
?k(yt, yt?1, x) = exp(
?
k
?kf(yt, yt?1, x)) (2)
where f(yt, yt?1, x) is a transition feature func-
tion of the label at positions i ? 1 and i and the ob-
servation sequence x; and ?j is a parameter that the
algorithm estimates from training data.
The features we use to train the CRF model in-
clude structural and lexical features that attempt to
capture indicators of relatedness to the given target
reference. The features that we used and their de-
scriptions are listed in table 1.
599
Category Description Example
Criticizing Criticism can be positive or negative. A citing sentence is classi-
fied as ?criticizing? when it mentions the weakness/strengths of
the cited approach, negatively/positively criticizes the cited ap-
proach, negatively/positively evaluates the cited source.
Chiang (2005) introduced a constituent feature to reward
phrases that match a syntactic tree but did not yield signif-
icant improvement.
Comparison A citing sentence is classified as ?comparison? when it compares
or contrasts the work in the cited paper to the author?s work. It
overlaps with the first category when the citing sentence says one
approach is not as good as the other approach. In this case we use
the first category.
Our approach permits an alternative to minimum error-rate
training (MERT; Och, 2003);
Use A citing sentence is classified as ?use? when the citing paper uses
the method, idea or tool of the cited paper.
We perform the MERT training (Och, 2003) to tune the
optimal feature weights on the development set.
Substantiating A citing sentence is classified as ?substantiating? when the re-
sults, claims of the citing work substantiate, verify the cited paper
and support each other.
It was found to produce automated scores, which strongly
correlate with human judgements about translation flu-
ency (Papineni et al , 2002).
Basis A citing sentence is classified as ?basis? when the author uses the
cited work as starting point or motivation and extends on the cited
work.
Our model is derived from the hidden-markov model for
word alignment (Vogel et al, 1996; Och and Ney, 2000).
Neutral (Other) A citing sentence is classified as ?neutral? when it is a neutral
description of the cited work or if it doesn?t come under any of
the above categories.
The solutions of these problems depend heavily on the
quality of the word alignment (Och and Ney, 2000).
Table 2: Annotation scheme for citation purpose. Motivated by the work of (Spiegel-Ro?sing, 1977) and (Teufel et al,
2006)
3.3 Citation Purpose Classification
In this section, we describe the citation purpose clas-
sification task. Given a target paper B and its cita-
tion context (extracted using the method described
above) in a given article A, we want to determine
the purpose of citing B by A. The purpose is de-
fined as intention behind selecting B and citing it by
the author of A (Garfield, 1964).
We use a taxonomy that consists of six categories.
We designed this taxonomy based on our study of
similar taxonomies proposed in previous work. We
selected the categories that we believe are more im-
portant and useful from a bibliometric point of view,
and the ones that can be detected through citation
text analysis. We also tried to limit the number of
categories by grouping similar categories proposed
in previous work under one category. The six cate-
gories, their descriptions, and an example for each
category are listed in Table 2.
We use a supervised approach whereby a classifi-
cation model is trained on a number of lexical and
structural features extracted from a set of labeled ci-
tation contexts. Some of the features that we use to
train the classifier are listed in table 3.
3.4 Citation Polarity Identification
In this section, we describe the citation polarity iden-
tification task. Given a target paper B and its citation
context in a given article A, we want to determine
the polarity of the citation text with respect to B.
The polarity can be: positive, negative, or neutral
(objective). Positive, negative, and neutral in this
context are defined in a slightly different way than
their usual sense. A citation is marked positive if it
either explicitly states a strength of the target paper
or indicates that the work done in the target paper
has been used either by the author or a third-party. It
is also marked as positive if it is compared to another
paper (possibly by the same authors) and deemed
better in some way. A citation is marked negative
if it explicitly points to a weakness of the target pa-
per. It is also marked as negative if it is compared
to another paper and deemed worse in some way. A
citation is marked as neutral if it is only descriptive.
Similar to citation purpose classification, we use
a supervised approach for this problem. We train a
classification model using the same features listed in
Table 3. Due to the high skewness in the data (more
than half of the citations are neutral), we use two
setups for binary classification. In the first setup,
the citation is classified as Polarized (Subjective) or
(Neutral) Objective. In the second one, Subjective
citations are classified as Positive or Negative. We
find that this method gives more intuitive results than
using a 3-way classifier.
600
Feature Description
Reference count The number of references that appear in the citation context.
Is Separate Whether the target reference appears within a group of references or separate (i.e. single reference).
Closest Verb / Adjective / Adverb The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mention
of it. Distance is measure based on the shortest path in the dependency tree.
Self Citation Whether the citation from the source paper to the target reference is a self citation.
Contains 1st/3rd PP Whether the citation context contains a first/third person pronoun.
Negation Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of
the *SEM 2012 negation detection shared task (Morante and Blanco, 2012).
Speculation Whether the citation context contains a speculation cue. The list is taken from Quirk et al (1985)
Closest Subjectivity Cue The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of
cues is taken from OpinionFinder (Wilson et al, 2005)
Contrary Expressions Whether the citation context contains a contrary expression. The list is taken from Biber (1988)
Section The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction,
Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc.
4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular
expressions.
Dependency Relations All the dependency relations that appear in the citation context. For example, nsubj(outperform, algorithm)
is one of the relations extracted from ?This algorithm outperforms the one proposed by...?. The arguments of the
dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good
results in similar tasks (Athar and Teufel, 2012a).
Table 3: The features used for citation purpose and polarity classification
4 Evaluation
In this section, we describe the data that we used for
evaluation and the experiments that we conducted.
4.1 Data
We use the ACL Anthology Network corpus
(AAN) (Radev et al, 2009; Radev et al, 2013) in
our evaluation. AAN is a publicly available collec-
tion of more than 19,000 NLP papers. It includes
a manually curated citation network of its papers
as well as the full text of the papers and the cit-
ing sentences associated with each edge in the ci-
tation network. From this set, we selected 30 pa-
pers that have different numbers of incoming cita-
tions and that were consistently cited since they were
published. These 30 papers received a total of about
3,500 citations from within AAN (average = 115 ci-
tation/paper, Min = 30, and Max = 338). These ci-
tations come from 1,493 unique papers. For each
of these citations, we extracted a window of 4 sen-
tences around the reference position. This brings
the number of sentences in our dataset to a total of
roughly 14,000 sentences. We refer to this dataset as
training/testing dataset.
In addition to this dataset, we created another
dataset that contains 300 citations that cite 5 papers
from AAN. We refer to this dataset as the develop-
ment dataset. This dataset was used to determine the
size of the citation context window, and to develop
the feature sets used in the three tasks described in
Section 3 above.
4.2 Annotation
In this section, we describe the annotation process.
We asked graduate students with good background
in NLP (the topic of the annotated sentences) to pro-
vide three annotations for each citation example (a
window of 4 sentences around the reference anchor)
in the training/testing dataset. We asked them to
mark the sentences that are related to a given tar-
get reference. In addition, we asked them to deter-
mine the purpose of citing the target reference by
choosing from the six purpose categories that we
described earlier. We also asked them to determine
whether the citation is negative, positive, or neutral.
To estimate the inter-annotator agreement, we
picked 400 sentences from the training/testing
dataset and assigned them to two different annota-
tors. We use the Kappa coefficient (Cohen, 1968)
to measure the agreement. The Kappa coefficient is
defined as follows:
K =
P (A)? P (E)
1? P (E)
(3)
where P(A) is the relative observed agreement
among annotators and P(E) is the hypothetical prob-
601
ability of chance agreement. The agreement be-
tween the two annotators on the context identifica-
tion task wasK = 0.89. On Landis and Kochs (Lan-
dis and Koch, 1977) scale, this value indicates al-
most perfect agreement. The agreement on the pur-
pose and the polarity classification task were K =
0.61 and K = 0.66, respectively; which indicates
substantial agreement on the same scale.
The annotation shows that in 22% of the citation
examples, the citation context consists of 2 or more
sentences. The distribution of the purpose categories
in the data was: 14.7% criticism, 8.5% comparison,
17.7% use, 7% substantiation, 5% basis, and 47%
other. The distribution of the polarity categories
was: 30% positive, 12% negative, and 58% neutral.
4.3 Experimental Setup
We use the CRF++1 toolkit for CRF training and
testing. We use the Stanford parser to parse the ci-
tation text and generate the dependency parse trees
of sentences. We use Weka for classification experi-
ments. We experimented with several classifiers in-
cluding: SVM, Logistic Regression (LR), and Naive
Bayes. All the experiments that we conducted used
the training/testing dataset in a 10-fold cross vali-
dation mode. All the results have been tested for
statistical significance using a 2-tailed paired t-test.
4.4 Evaluation of Citation Context
Identification
We compare the CRF approach to three baselines.
The first baseline (ALL) labels all the sentences in
the citation window of size 4 as INCLUDED in the
citation context. The second baseline (CS-ONLY)
labels the citing sentence only as INCLUDED in the
citation context. In the third baseline, we use a su-
pervised classification method instead of sequence
labeling. We use Support Vector Machines (SVM)
to train a model using the same set of features as in
the CRF approach.
Table 4 shows the precision, recall, and F1 score
of the CRF approach and the baselines. The re-
sults show that our CRF approach outperforms all
the baselines. It also asserts our expectation that ad-
dressing this problem as a sequence labeling prob-
lem leads to better performance than individual sen-
1http://crfpp.googlecode.com/svn/trunk/doc/index.html
Precision Recall F1
CRFs 98.5% 82.0% 89.5%
ALL 30.7% 100.0% 46.9%
CS-ONLY 88.0% 74.0% 80.4%
SVM 92.0% 76.4% 83.5%
Table 4: Results of citation context identification
tence classification, which is also clear from the na-
ture of the task.
Feature Analysis: We evaluated the importance
of the features listed in Table 1 by computing the
chi-squared statistic for every feature with respect to
the class. We found that the lexical features (such as
determiners and conjunction adverbs) are generally
more important than the structural features (such as
position and reference count). The features shown
in Table 1 are listed in the order of their importance
based on this analysis.
4.5 Evaluation of Citation Purpose
Classification
Our experiments with several classification algo-
rithms showed that the SVM classifier outperforms
Logistic Regression and Naive Bayes classifiers.
Due to space limitations, we only show the results
for SVM. Table 5 shows the precision, recall, and
F1 for each of the six categories. It also shows the
overall accuracy and the Macro-F measure.
Feature Analysis: The chi-squared evaluation of
the features listed in Table 3 shows that both lexical
and structural features are important. It also shows
that among lexical features, the ones that are limited
to the existence of a direct relation to the target ref-
erence (such as closest verb, adjective, adverb, sub-
jective cue, etc.) are most useful. This can be ex-
plained by the fact that the restricting the features to
having direct dependency relation introduces much
less noise than other features (such as Dependency
Triplets). Among the structural features, the num-
ber of references in the citation context showed to
be more useful.
4.6 Evaluation of Citation Polarity
Identification
Similar to the case of citation purpose classification,
our experiments showed that the SVM classifier out-
performs the other classifiers that we experimented
with. Table 6 shows the precision, recall, and F1 for
602
Criticism Comparison Use Substantiating Basis Other
Precision 53.0% 55.2% 60.0% 50.1% 47.3% 64.0%
Recall 77.4% 43.1% 73.0% 57.3% 39.1% 85.1%
F1 63.0% 48.4% 66.0% 53.5% 42.1% 73.1%
Accuracy: 70.5%
Macro-F: 58.0%
Table 5: Summary of Citation Purpose Classification Results (10-fold cross validation, SVM: Linear Kernel, c = 1.0)
each of the three categories. It also shows the over-
all accuracy and the Macro-F measure. The analysis
of the features used to train this classifier using chi-
squared analysis leads to the same conclusions about
the relative importance of the features as described
in the previous subsection. However, we noticed that
features that are related to subjectivity (Subjectiv-
ity Cues, Negation, Speculation) are ranked higher
which makes sense in the case of polarity classifica-
tion.
4.7 Impact of Context on Classification
Accuracy
To study the impact of using citation context in ad-
dition to the citing sentence on classification per-
formance, we ran two polarity classification exper-
iments. In the first experiment, we used the citing
sentence only to extract the features that are used
to train the classifiers. In the second experiment,
we used the gold context sentences (the ones la-
beled INCLUDED by human annotators). Table 6
shows the results of the first experiment between
rounded parentheses and the results of the second
experiments in square brackets. The results show
that adding citation context improves the classifica-
tion accuracy especially in the subjective categories,
specially in the negative category if we want to be
more specific. This supports our intuition about po-
larized citations that authors start their review of the
cited work with an objective (neutral) sentence and
then follow it with their criticism if they have any.
We also reached to similar conclusions with purpose
classification, but we are not showing the numbers
due to space limitations.
4.8 Other Experiments
4.8.1 Can We Do Better?
In this section, we investigate whether it is possi-
ble to improve the performance in the two classifica-
tion tasks. One factor that we believe could have an
Negative % Positive % Neutral %
Precision 68.7 (66.4) [69.8] 54.9 (52.1) [55.4] 83.6 (82.8) [84.2]
Recall 79.2 (71.1) [81.1] 48.1 (45.6) [46.3] 95.5 (95.1) [95.3]
F1 73.6 (68.7) [75.0] 51.3 (48.6) [50.4] 89.1 (88.5) [89.4]
Accuracy: 81.4 (74.2) [84.2] %
Macro-F: 71.3 (62.1) [74.2] %
Table 6: Summary of Citation Polarity Classification Re-
sults (10-fold cross validation, SVM: Linear Kernel, c =
1.0). Numbers between rounded parentheses are when
only the explicit citing sentence is used (i.e. no context).
Numbers in square brackets are when the gold standard
context is used.
impact on the result is the size of the training data.
To examine this hypothesis, we ran the experiment
on different sizes of data. Figure 1 shows the learn-
ing curve of the two classifiers for different sizes of
training data. The accuracy increases as more train-
ing data is available so we can expect that with even
more data, we can do even better.
4.8.2 Relation Between Citation
Purpose/Polarity and Citation Count
The main motivation of this work is our hypothet-
ical assumption that using NLP for analyzing cita-
tions gives a clearer picture of the impact of the cited
work. As a way to check the validity of this assump-
tion, we study the correlation between the counts of
the different purpose and polarity categories. We
also study the correlation between these categories
and the total number of citations that a paper re-
ceived since it was published. We use the train-
ing/testing dataset and the gold annotations for this
study.
We compute the Pearson correlation coefficient
between the counts of citations from the different
categories that a paper received per year since its
publication. We found that, on average, the correla-
tion between positive and negative citations is neg-
ative (AVG P = -0.194) and that the correlation be-
603
25
35
45
55
65
75
85
0 500 1000 1500 2000 2500 3000
Accu
racy
 
Dataset Size 
Purpose Accuracy
Polarity Accuracy
Figure 1: The effect of size of the data set size on the
classifiers accuracy.
tween the count of positive citations and the total
number of citations is higher than the correlation be-
tween negative citations and total citations (AVG P =
0.531 for positive vs. AVG P = 0.054 for negative).
Similarly, we noticed that there is a higher posi-
tive correlation between Use citations and total ci-
tations than in the case of both Substantiation and
Basis. This can be explained by the intuition that
publications that present new algorithms, tools, or
corpora that are used by the research community be-
come more and more popular with time and thus re-
ceive more and more citations.
Figure 2 shows the result of running our pur-
pose classifier on all the citations to Papineni et
al.?s (2002) paper about Bleu, an automatic metric
for evaluating Machine Translation (MT) systems.
The figure shows that this paper receives a high
number of Use citations. This makes sense for a pa-
per that describes an evaluation metric that has been
widely used in the MT area. The figure also shows
that in the recent years, this metric started to receive
some Criticizing citations that resulted in a slight de-
crease in the number of Use citations. Such a tempo-
ral analysis of citation purpose and polarity is useful
for studying the dynamics of research. It can also
be used to detect the emergence or de-emergence of
research techniques.
0
10
20
30
40
50
60
70
80
2001 2003 2005 2007 2009 2011
CriticizingComparisonUseSubstantiatingBasisOther
Figure 2: Change in the purpose of the citations to Pap-
ineni et al (2002)
5 Conclusion
In this paper, we presented methods for three tasks:
citation context identification, citation purpose clas-
sification, and citation polarity classification. This
work is motivated by the need for more accurate
bibliometric measures that evaluates the impact of
research both qualitatively and quantitatively. Our
experiments showed that we can classify the pur-
pose and polarity of citation with a good accuracy. It
also showed that using the citation context improves
the classification accuracy and increases the num-
ber of polarized citations detected. For future work,
we plan to use the output of this research in several
applications such as predicting future prominence of
publications, studying the dynamics of research, and
designing more accurate bibliometric measures.
Acknowledgement
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Center
(DoI/NBC) contract number D11PC20153. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Government.
604
References
Daryl E. and Soumyo D. Moitra. 1975. Content analysis
of references: Adjunct or alternative to citation count-
ing? Social Studies of Science, 5(4):pp. 423?441.
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amjad Abu Jbara and Dragomir Radev. 2012. Refer-
ence scope identification in citing sentences. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 80?
90, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012a. Context-
enhanced citation sentiment detection. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 597?
601, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Awais Athar and Simone Teufel. 2012b. Detection of
implicit citations for sentiment detection. In Proceed-
ings of the Workshop on Detecting Structure in Schol-
arly Discourse, pages 18?26, Jeju Island, Korea, July.
Association for Computational Linguistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Douglas Biber. 1988. Variation across speech and writ-
ing. Cambridge University Press, Cambridge.
Susan Bonzi. 1982. Characteristics of a literature as pre-
dictors of relatedness between cited and citing works.
Journal of the American Society for Information Sci-
ence, 33(4):208?216.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213?220.
Leo Egghe. 2006. Theory and practise of the g-index.
Scientometrics, 69:131?152.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
E. Garfield. 1979. Is citation analysis a legitimate evalu-
ation tool? Scientometrics, 1(4):359?375.
Eugene Garfield. 1994. The thomson reuters impact fac-
tor.
J. E. Hirsch. 2005. An index to quantify an individual?s
scientific research output. Proceedings of the National
Academy of Sciences, 102(46):16569?16572, Novem-
ber.
J. E. Hirsch. 2010. An index to quantify an individ-
ual?s scientific research output that takes into account
the effect of multiple coauthorship. Scientometrics,
85(3):741?754, December.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Michael H. MacRoberts and Barbara R. MacRoberts.
1984. The negational reference: Or the art of dissem-
bling. Social Studies of Science, 14(1):pp. 91?94.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Roser Morante and Eduardo Blanco. 2012. *sem 2012
shared task: resolving the scope and focus of nega-
tion. In Proceedings of the First Joint Conference
on Lexical and Computational Semantics - Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation, SemEval
?12, pages 265?274, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
M. J. Moravcsik and P. Murugesan. 1975. Some results
on the function and quality of citations. Social Studies
of Science, 5:86?92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
605
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Vahed Qazvinian, Dragomir R. Radev, Saif Mohammad,
Bonnie Dorr, David Zajic, Michael Whidby, and Tae-
sun Moon. 2013. Generating extractive summaries of
scientific paradigms. Journal of Artificial Intelligence
Research.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, London.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources and
Evaluation, pages 1?26.
Ina Spiegel-Ro?sing. 1977. Science Studies: Bibliomet-
ric and Content Analysis. Social Studies of Science,
7(1):97?113, February.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
GEOFF THOMPSON and YE YIYUN. 1991. Evalu-
ation in the reporting verbs used in academic papers.
Applied Linguistics, 12(4):365?382.
Melvin Weinstock. 1971. Citation Indexes. Encyclope-
dia of Library and Information Science.
Michael Alan Whidby. 2012. Citation handling: Pro-
cessing citation text in scientific documents. In Master
Thesis.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89?116.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, HLT-Demo ?05, pages 34?35, Stroudsburg,
PA, USA. Association for Computational Linguistics.
J. M. Ziman. 1968. Public knowledge: An essay con-
cerning the social dimension of science. Cambridge
U.P., London.
606
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395?403,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Identifying Text Polarity Using Random Walks
Ahmed Hassan
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
hassanam@umich.edu
Dragomir Radev
University of Michigan Ann Arbor
Ann Arbor, Michigan, USA
radev@umich.edu
Abstract
Automatically identifying the polarity of
words is a very important task in Natural
Language Processing. It has applications
in text classification, text filtering, analysis
of product review, analysis of responses
to surveys, and mining online discussions.
We propose a method for identifying the
polarity of words. We apply a Markov ran-
dom walk model to a large word related-
ness graph, producing a polarity estimate
for any given word. A key advantage of
the model is its ability to accurately and
quickly assign a polarity sign and mag-
nitude to any word. The method could
be used both in a semi-supervised setting
where a training set of labeled words is
used, and in an unsupervised setting where
a handful of seeds is used to define the
two polarity classes. The method is exper-
imentally tested using a manually labeled
set of positive and negative words. It out-
performs the state of the art methods in the
semi-supervised setting. The results in the
unsupervised setting is comparable to the
best reported values. However, the pro-
posed method is faster and does not need a
large corpus.
1 Introduction
Identifying emotions and attitudes from unstruc-
tured text is a very important task in Natural Lan-
guage Processing. This problem has a variety of
possible applications. For example, there has been
a great body of work for mining product reputation
on the Web (Morinaga et al, 2002; Turney, 2002).
Knowing the reputation of a product is very impor-
tant for marketing and customer relation manage-
ment (Morinaga et al, 2002). Manually handling
reviews to identify reputation is a very costly, and
time consuming process given the overwhelming
amount of reviews on the Web. A list of words
with positive/negative polarity is a very valuable
resource for such an application.
Another interesting application is mining online
discussions. A threaded discussion is an electronic
discussion in which software tools are used to help
individuals post messages and respond to other
messages. Threaded discussions include e-mails,
e-mail lists, bulletin boards, newsgroups, or Inter-
net forums. Threaded discussions act as a very im-
portant tool for communication and collaboration
in the Web. An enormous number of discussion
groups exists on the Web. Millions of users post
content to these groups covering pretty much ev-
ery possible topic. Tracking participant attitude
towards different topics and towards other partici-
pants is a very interesting task. For example,Tong
(2001) presented the concept of sentiment time-
lines. His system classifies discussion posts about
movies as either positive or negative. This is used
to produce a plot of the number of positive and
negative sentiment messages over time. All those
applications could benefit much from an automatic
way of identifying semantic orientation of words.
In this paper, we study the problem of automati-
cally identifying semantic orientation of any word
by analyzing its relations to other words. Auto-
matically classifying words as either positive or
negative enables us to automatically identify the
polarity of larger pieces of text. This could be
a very useful building block for mining surveys,
product reviews and online discussions. We ap-
ply a Markov random walk model to a large se-
mantic word graph, producing a polarity estimate
for any given word. Previous work on identifying
the semantic orientation of words has addressed
the problem as both a semi-supervised (Takamura
et al, 2005) and an unsupervised (Turney and
Littman, 2003) learning problem. In the semi-
supervised setting, a training set of labeled words
395
is used to train the model. In the unsupervised
setting, only a handful of seeds is used to define
the two polarity classes. The proposed method
could be used both in a semi-supervised and in
an unsupervised setting. Empirical experiments
on a labeled set of words show that the proposed
method outperforms the state of the art methods in
the semi-supervised setting. The results in the un-
supervised setting are comparable to the best re-
ported values. The proposed method has the ad-
vantages that it is faster and it does not need a large
training corpus.
The rest of the paper is structured as follows.
In Section 2, we discuss related work. Section 3
presents our method for identifying word polarity.
Section 4 describes our experimental setup. We
conclude in Section 5.
2 Related Work
Hatzivassiloglou and McKeown (1997) proposed
a method for identifying word polarity of adjec-
tives. They extract all conjunctions of adjectives
from a given corpus and then they classify each
conjunctive expression as either the same orien-
tation such as ?simple and well-received? or dif-
ferent orientation such as ?simplistic but well-
received?. The result is a graph that they cluster
into two subsets of adjectives. They classify the
cluster with the higher average frequency as posi-
tive. They created and labeled their own dataset
for experiments. Their approach will probably
works only with adjectives because there is noth-
ing wrong with conjunctions of nouns or verbs
with opposite polarities (e.g., ?war and peace?,
?rise and fall?, ..etc).
Turney and Littman (2003) identify word po-
larity by looking at its statistical association with
a set of positive/negative seed words. They use
two statistical measures for estimating association:
Pointwise Mutual Information (PMI) and Latent
Semantic Analysis (LSA). To get co-occurrence
statistics, they submit several queries to a search
engine. Each query consists of the given word and
one of the seed words. They use the search engine
near operator to look for instances where the given
word is physically close to the seed word in the re-
turned document. They present their method as an
unsupervised method where a very small amount
of seed words are used to define semantic orienta-
tion rather than train the model. One of the lim-
itations of their method is that it requires a large
corpus of text to achieve good performance. They
use several corpora, the size of the best performing
dataset is roughly one hundred billion words (Tur-
ney and Littman, 2003).
Takamura et al (2005) proposed using spin
models for extracting semantic orientation of
words. They construct a network of words us-
ing gloss definitions, thesaurus, and co-occurrence
statistics. They regard each word as an electron.
Each electron has a spin and each spin has a direc-
tion taking one of two values: up or down. Two
neighboring spins tend to have the same orienta-
tion from an energetic point of view. Their hy-
pothesis is that as neighboring electrons tend to
have the same spin direction, neighboring words
tend to have similar polarity. They pose the prob-
lem as an optimization problem and use the mean
field method to find the best solution. The anal-
ogy with electrons leads them to assume that each
word should be either positive or negative. This
assumption is not accurate because most of the
words in the language do not have any semantic
orientation. They report that their method could
get misled by noise in the gloss definition and their
computations sometimes get trapped in a local op-
timum because of its greedy optimization flavor.
Kamps et al (2004) construct a network
based on WordNet synonyms and then use the
shortest paths between any given word and the
words ?good? and ?bad? to determine word polar-
ity. They report that using shortest paths could be
very noisy. For example. ?good? and ?bad? them-
selves are closely related in WordNet with a 5-
long sequence ?good, sound, heavy, big, bad?. A
given word w may be more connected to one set
of words (e.g., positive words), yet have a shorter
path connecting it to one word in the other set. Re-
stricting seed words to only two words affects their
accuracy. Adding more seed words could help but
it will make their method extremely costly from
the computation point of view. They evaluate their
method only using adjectives.
Hu and Liu (2004) use WordNet synonyms and
antonyms to predict the polarity of words. For
any word, whose polarity is unknown, they search
WordNet and a list of seed labeled words to pre-
dict its polarity. They check if any of the syn-
onyms of the given word has known polarity. If
so, they label it with the label of its synonym. Oth-
erwise, they check if any of the antonyms of the
given word has known polarity. If so, they label it
396
with the opposite label of the antonym. They con-
tinue in a bootstrapping manner till they label all
possible word. This method is quite similar to the
shortest-path method proposed in (Kamps et al,
2004).
There are some other methods that try to build
lexicons of polarized words. Esuli and Sebas-
tiani (2005; 2006) use a textual representation of
words by collating all the glosses of the word as
found in some dictionary. Then, a binary text clas-
sifier is trained using the textual representation and
applied to new words. Kim and Hovy (2004) start
with two lists of positive and negative seed words.
WordNet is used to expand these lists. Synonyms
of positive words and antonyms of negative words
are considered positive, while synonyms of neg-
ative words and antonyms of positive words are
considered negative. A similar method is pre-
sented in (Andreevskaia and Bergler, 2006) where
WordNet synonyms, antonyms, and glosses are
used to iteratively expand a list of seeds. The senti-
ment classes are treated as fuzzy categories where
some words are very central to one category, while
others may be interpreted differently. Kanayama
and Nasukawa (2006) use syntactic features and
context coherency, the tendency for same polari-
ties to appear successively , to acquire polar atoms.
Other related work is concerned with subjec-
tivity analysis. Subjectivity analysis is the task
of identifying text that present opinions as op-
posed to objective text that present factual in-
formation (Wiebe, 2000). Text could be either
words, phrases, sentences, or any other chunks.
There are two main categories of work on sub-
jectivity analysis. In the first category, subjective
words and phrases are identified without consider-
ing their context (Wiebe, 2000; Hatzivassiloglou
and Wiebe, 2000; Banea et al, 2008). In the sec-
ond category, the context of subjective text is used
(Riloff and Wiebe, 2003; Yu and Hatzivassiloglou,
2003; Nasukawa and Yi, 2003; Popescu and Et-
zioni, 2005) Wiebe et al (2001) lists a lot of appli-
cations of subjectivity analysis such as classifying
emails and mining reviews. Subjectivity analysis
is related to the proposed method because identi-
fying the polarity of text is the natural next step
that should follow identifying subjective text.
3 Word Polarity
We use a Markov random walk model to identify
polarity of words. Assume that we have a network
of words, some of which are labeled as either pos-
itive or negative. In this network, two words are
connecting if they are related. Different sources of
information could be used to decide whether two
words are related or not. For example, the syn-
onyms of any word are semantically related to it.
The intuition behind that connecting semantically
related words is that those words tend to have simi-
lar polarity. Now imagine a random surfer walking
along the network starting from an unlabeled word
w. The random walk continues until the surfer
hits a labeled word. If the word w is positive then
the probability that the random walk hits a positive
word is higher and if w is negative then the prob-
ability that the random walk hits a negative word
is higher. Similarly, if the word w is positive then
the average time it takes a random walk starting
at w to hit a positive node is less than the average
time it takes a random walk starting at w to hit a
negative node.
In the rest of this section, we will describe how
we can construct a word relatedness graph in Sec-
tion 3.1. The random walk model is described in
Section 3.2. Hitting time is defined in Section?3.3.
Finally, an algorithm for computing a sign and
magnitude for the polarity of any given word is
described in Section 3.4.
3.1 Network Construction
We construct a network where two nodes are
linked if they are semantically related. Several
sources of information could be used as indicators
of the relatedness of words. One such important
source is WordNet (Miller, 1995). WordNet is a
large lexical database of English. Nouns, verbs,
adjectives and adverbs are grouped into sets of
cognitive synonyms (synsets), each expressing a
distinct concept (Miller, 1995). Synsets are inter-
linked by means of conceptual-semantic and lexi-
cal relations.
The simplest approach is to connect words that
occur in the same WordNet synset. We can col-
lect all words in WordNet, and add links between
any two words that occurr in the same synset. The
resulting graph is a graph G(W,E) where W is a
set of word / part-of-speech pairs for all the words
in WordNet. E is the set of edges connecting
each pair of synonymous words. Nodes represent
word/pos pairs rather than words because the part
of speech tags are helpful in disambiguating the
different senses for a given word. For example,
397
the word ?fine? has two different meanings when
used as an adjective and as a noun.
Several other methods could be used to link
words. For example, we can use other WordNet
relations: hypernyms, similar to,...etc. Another
source of links between words is co-occurrence
statistics from corpus. Following the method pre-
sented in (Hatzivassiloglou and McKeown, 1997),
we can connect words if they appear in a conjunc-
tive form in the corpus. This method is only appli-
cable to adjectives. If two adjectives are connected
by ?and? in conjunctive form, it is highly likely
that they have the same semantic orientation. In
all our experiments, we restricted the network to
only WordNet relations. We study the effect of us-
ing co-occurrence statistics to connect words later
at the end of our experiments. If more than one re-
lation exists between any two words, the strength
of the corresponding edge is adjusted accordingly.
3.2 Random Walk Model
Imagine a random surfer walking along the word
relatedness graph G. Starting from a word with
unknown polarity i , it moves to a node j with
probability Pij after the first step. The walk con-
tinues until the surfer hits a word with a known
polarity. Seed words with known polarity act as
an absorbing boundary for the random walk. If
we repeat the number of random walks N times,
the percentage of time at which the walk ends at
a positive/negative word could be used as an in-
dicator of its positive/negative polarity. The aver-
age time a random walk starting at w takes to hit
the set of positive/negative nodes is also an indi-
cator of its polarity. This view is closely related
to the partially labeled classification with random
walks approach in (Szummer and Jaakkola, 2002)
and the semi-supervised learning using harmonic
functions approach in (Zhu et al, 2003).
Let W be the set of words in our lexicon. We
construct a graph whose nodes V are all words
in W The edges E correspond to relatedness be-
tween words We define transition probabilities
Pt+1|t(j|i) from i to j by normalizing the weights
of the edges out of node i, so:
Pt+1|t(j|i) = Wij/
?
k
Wik (1)
where k represents all nodes in the neighborhood
of i. Pt2|t1(j|i) denotes the transition probability
from node i at step t1 to node j at time step t2.
We note that the weights Wij are symmetric and
the transition probabilities Pt+1|t(j|i) are not nec-
essarily symmetric because of the node out degree
normalization.
3.3 First-Passage Time
The mean first-passage (hitting) time h(i|k) is de-
fined as the average number of steps a random
walker, starting in state i 6= k, will take to en-
ter state k for the first time (Norris, 1997). Let
G = (V,E) be a graph with a set of vertices V ,
and a set of edges E. Consider a subset of vertices
S ? V , Consider a random walk on G starting at
node i 6? S. Let Nt denote the position of the ran-
dom surfer at time t. Let h(i|S) be the the average
number of steps a random walker, starting in state
i 6? S, will take to enter a state k ? S for the first
time. Let TS be the first-passage for any vertex in
S.
P (TS = t|N0 = i) =
?
j?V
pij ? P (TS = t? 1|N0 = j) (2)
h(i|S) is the expectation of TS . Hence:
h(i|S) = E(TS |N0 = i)
=
??
t=1
t? P (TS = t|N0 = i)
=
??
t=1
t
?
j?V
pijP (TS = t? 1|N0 = j)
=
?
j?V
??
t=1
(t? 1)pijP (TS = t? 1|N0 = j)
+
?
j?V
??
t=1
pijP (TS = t? 1|N0 = j)
=
?
j?V
pij
??
t=1
tP (TS = t|N0 = j) + 1
=
?
j?V
pij ? h(j|S) + 1 (3)
Hence the first-passage (hitting) time can be for-
mally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(4)
3.4 Word Polarity Calculation
Based on the description of the random walk
model and the first-passage (hitting) time above,
398
we now propose our word polarity identification
algorithm. We begin by constructing a word relat-
edness graph and defining a random walk on that
graph as described above. Let S+ and S? be two
sets of vertices representing seed words that are
already labeled as either positive or negative re-
spectively. For any given word w, we compute the
hitting time h(w|S+), and h(w|S?) for the two
sets iteratively as described earlier. if h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. The
ratio between the two hitting times could be used
as an indication of how positive/negative the given
word is. This is useful in case we need to pro-
vide a confidence measure for the prediction. This
could be used to allow the model to abstain from
classifying words with when the confidence level
is low.
Computing hitting time as described earlier may
be time consuming especially if the graph is large.
To overcome this problem, we propose a Monte
Carlo based algorithm for estimating it. The algo-
rithm is shown in Algorithm 1.
Algorithm 1 Word Polarity using Random Walks
Require: A word relatedness graph G
1: Given a word w in V
2: Define a randomwalk on the graph. the transi-
tion probability between any two nodes i, and
j is defined as: Pt+1|t(j|i) = Wij/
?
k Wik
3: Start k independent random walks from w
with a maximum number of steps m
4: Stop when a positive word is reached
5: Let h?(w|S+) be the estimated value for
h(w|S+)
6: Repeat for negative words computing
h?(w|S?)
7: if h?(w|S+) ? h?(w|S?) then
8: Classify w as positive
9: else
10: Classify w as negative
11: end if
4 Experiments
We performed experiments on the General In-
quirer lexicon (Stone et al, 1966). We used it
as a gold standard data set for positive/negative
words. The dataset contains 4206 words, 1915 of
which are positive and 2291 are negative. Some of
the ambiguous words were removed like (Turney,
2002; Takamura et al, 2005).
We use WordNet (Miller, 1995) as a source
of synonyms and hypernyms for the word relat-
edness graph. We used 10-fold cross validation
for all tests. We evaluate our results in terms of
accuracy. Statistical significance was tested us-
ing a 2-tailed paired t-test. All reported results
are statistically significant at the 0.05 level. We
perform experiments varying the parameters and
the network. We also look at the performance of
the proposed method for different parts of speech,
and for different confidence levels We compare
our method to the Semantic Orientation from PMI
(SO-PMI) method described in (Turney, 2002),
the Spin model (Spin) described in (Takamura et
al., 2005), the shortest path (short-path) described
in (Kamps et al, 2004), and the bootstrapping
(bootstrap) method described in (Hu and Liu,
2004).
4.1 Comparisons with other methods
This method could be used in a semi-supervised
setting where a set of labeled words are used and
the system learns from these labeled nodes and
from other unlabeled nodes. Under this setting, we
compare our method to the spin model described
in (Takamura et al, 2005). Table 2 compares the
performance using 10-fold cross validation. The
table shows that the proposed method outperforms
the spin model. The spin model approach uses
word glosses, WordNet synonym, hypernym, and
antonym relations, in addition to co-occurrence
statistics extracted from corpus. The proposed
method achieves better performance by only using
WordNet synonym, hypernym and similar to rela-
tions. Adding co-occurrence statistics slightly im-
proved performance, while using glosses did not
help at all.
We also compare our method to the SO-PMI
method presented in (Turney, 2002). They de-
scribe this setting as unsupervised (Turney, 2002)
because they only use 14 seeds as paradigm words
that define the semantic orientation rather than
train the model. After (Turney, 2002), we use our
method to predict semantic orientation of words in
the General Inquirer lexicon (Stone et al, 1966)
using only 14 seed words. The network we used
contains only WordNet relations. No glosses or
co-occurrence statistics are used. The results com-
paring the SO-PMI method with different dataset
sizes, the spin model, and the proposed method
using only 14 seeds is shown in Table 2. We no-
399
Table 1: Accuracy for adjectives only for the spin
model, the bootstrap method, and the random
walk model.
spin-model bootstrap short-path rand-walks
83.6 72.8 68.8 88.8
tice that the randomwalk method outperforms SO-
PMI when SO-PMI uses datasets of sizes 1? 107
and 2 ? 109 words. The performance of SO-PMI
and the random walk methods are comparable
when SO-PMI uses a very large dataset (1 ? 1011
words). The performance of the spin model ap-
proach is also comparable to the other 2 meth-
ods. The advantages of the random walk method
over SO-PMI is that it is faster and it does not
need a very large corpus like the one used by SO-
PMI. Another advantage is that the random walk
method can be used along with the labeled data
from the General Inquirer lexicon (Stone et al,
1966) to get much better performance. This is
costly for the SO-PMI method because that will
require the submission of almost 4000 queries to a
commercial search engine.
We also compare our method to the bootstrap-
ping method described in (Hu and Liu, 2004), and
the shortest path method described in (Kamps et
al., 2004). We build a network using only Word-
Net synonyms and hypernyms. We restrict the test
set to the set of adjectives in the General Inquirer
lexicon (Stone et al, 1966) because this method
is mainly interested in classifying adjectives. The
performance of the spin model method, the boot-
strapping method, the shortest path method, and
the random walk method for only adjectives is
shown in Table 1. We notice from the table that the
random walk method outperforms both the spin
model, the bootstrapping method, and the short-
est path method for adjectives. The reported ac-
curacy for the shortest path method only considers
the words it could assign a non-zero orientation
value. If we consider all words, the accuracy will
drop to around 61%.
4.1.1 Varying Parameters
As we mentioned in Section 3.4, we use a param-
eter m to put an upper bound on the length of ran-
dom walks. In this section, we explore the impact
Table 2: Accuracy for SO-PMI with different
dataset sizes, the spin model, and the random
walks model for 10-fold cross validation and 14
seeds.
- CV 14 seeds
SO-PMI (1? 107) - 61.3
SO-PMI (2? 109) - 76.1
SO-PMI (1? 1011) - 82.8
Spin Model 91.5 81.9
Random Walks 93.1 82.1
of this parameter on our method?s performance.
Figure 1 shows the accuracy of the randomwalk
method as a function of the maximum number of
steps m. m varies from 5 to 50. We use a net-
work built from WordNet synonyms and hyper-
nyms only. The number of samples k was set to
1000. We perform 10-fold cross validation using
the General Inquirer lexicon. We notice that the
maximum number of steps m has very little im-
pact on performance until it rises above 30. When
it does, the performance drops by no more than
1%, and then it does not change anymore as m
increases. An interesting observation is that the
proposed method performs quite well with a very
small number of steps (around 10). We looked at
the dataset to understand why increasing the num-
ber of steps beyond 30 negatively affects perfor-
mance. We found out that when the number of
steps is very large, compared to the diameter of the
graph, the random walk that starts at ambiguous
words, that are hard to classify, have the chance
of moving till it hits a node in the opposite class.
That does not happen when the limit on the num-
ber of steps is smaller because those walks are then
terminated without hitting any labeled nodes and
hence ignored.
Next, we study the effect of the random of sam-
ples k on our method?s performance. As explained
in Section 3.4, k is the number of samples used
by the Monte Carlo algorithm to find an estimate
for the hitting time. Figure 2 shows the accuracy
of the random walks method as a function of the
number of samples k. We use the same settings as
in the previous experiment. the only difference is
that we fix m at 15 and vary k from 10 to 20000
(note the logarithmic scale). We notice that the
performance is badly affected, when the value of
k is very small (less than 100). We also notice that
400
after 1000, varying k has very little, if any, effect
on performance. This shows that the Monte Carlo
algorithm for computing the random walks hitting
time performs quite well with values of the num-
ber of samples as small as 1000.
The preceding experiments suggest that the pa-
rameter have very little impact on performance.
This suggests that the approach is fairly robust
(i.e., it is quite insensitive to different parameter
settings).
Figure 1: The effect of varying the maximum
number of steps (m) on accuracy.
Figure 2: The effect of varying the number of sam-
ples (k) on accuracy.
4.1.2 Other Experiments
We now measure the performance of the proposed
method when the system is allowed to abstain
from classifying the words for which it have low
confidence. We regard the ratio between the hit-
ting time to positive words and hitting time to neg-
ative words as a confidence measure and evaluate
the top words with the highest confidence level at
different values of threshold. Figure 4 shows the
accuracy for 10-fold cross validation and for us-
ing only 14 seeds at different thresholds. We no-
tice that the accuracy improves by abstaining from
classifying the difficult words. The figure shows
that the top 60% words are classified with an ac-
curacy greater than 99% for 10-fold cross valida-
tion and 92% with 14 seed words. This may be
compared to the work descibed in (Takamura et
al., 2005) where they achieve the 92% level when
they only consider the top 1000 words (28%).
Figure 3 shows a learning curve displaying how
the performance of the proposed method is af-
fected with varying the labeled set size (i.e., the
number of seeds). We notice that the accuracy ex-
ceeds 90% when the training set size rises above
20%. The accuracy steadily increases as the la-
beled data increases.
We also looked at the classification accuracy for
different parts of speech in Figure 5. we notice
that, in the case of 10-fold cross validation, the
performance is consistent across parts of speech.
However, when we only use 14 seeds all of which
are adjectives, similar to (Turney and Littman,
2003), we notice that the performance on adjec-
tives is much better than other parts of speech.
When we use 14 seeds but replace some of the
adjectives with verbs and nouns like (love, harm,
friend, enemy), the performance for nouns and
verbs improves considerably at the cost of losing a
little bit of the performance on adjectives. We had
a closer look at the results to find out what are the
reasons behind incorrect predictions. We found
two main reasons. First, some words are ambigu-
ous and has more than one sense, possible with
different orientations. Disambiguating the sense
of words given their context before trying to pre-
dict their polarity should solve this problem. The
second reason is that some words have very few
connection in thesaurus. A possible solution to
this might be identifying those words and adding
more links to them from glosses of co-occurrence
statistics in corpus.
Figure 3: The effect of varying the number of
seeds on accuracy.
401
Figure 4: Accuracy for words with high confi-
dence measure.
Figure 5: Accuracy for different parts of speech.
5 Conclusions
Predicting the semantic orientation of words is
a very interesting task in Natural Language Pro-
cessing and it has a wide variety of applications.
We proposed a method for automatically predict-
ing the semantic orientation of words using ran-
dom walks and hitting time. The proposed method
is based on the observation that a random walk
starting at a given word is more likely to hit an-
other word with the same semantic orientation be-
fore hitting a word with a different semantic ori-
entation. The proposed method can be used in a
semi-supervised setting where a training set of la-
beled words is used, and in an unsupervised setting
where only a handful of seeds is used to define the
two polarity classes. We predict semantic orienta-
tion with high accuracy. The proposed method is
fast, simple to implement, and does not need any
corpus.
Acknowledgments
This research was funded by the Office of the
Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab.
All statements of fact, opinion or conclusions con-
tained herein are those of the authors and should
not be construed as representing the official views
or policies of IARPA, the ODNI or the U.S. Gov-
ernment.
References
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing wordnet for fuzzy sentiment: Sentiment tag ex-
traction from wordnet glosses. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics (EACL
2006).
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08).
Andrea Esuli and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms through
gloss classification. In Proceedings of the 14th Con-
ference on Information and Knowledge Manage-
ment (CIKM 2005), pages 617?624.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation (LREC
2006), pages 417?422.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to mea-
sure semantic orientations of adjectives. In National
Institute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In Proceedings of the
2006 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2006), pages 355?
363.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics (COLING 2004), pages 1367?1373.
402
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 341?349.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Senti-
ment analysis: capturing favorability using natural
language processing. In K-CAP ?03: Proceedings
of the 2nd international conference on Knowledge
capture, pages 70?77.
J. Norris. 1997. Markov chains. Cambridge Univer-
sity Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 105?112.
Philip Stone, Dexter Dunphy, Marchall Smith, and
Daniel Ogilvie. 1966. The general inquirer: A com-
puter approach to content analysis. The MIT Press.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks.
In Advances in Neural Information Processing Sys-
tems, pages 945?952.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 133?140.
Richard M. Tong. 2001. An operational system for de-
tecting and tracking opinions in on-line discussion.
Workshop note, SIGIR 2001 Workshop on Opera-
tional Text Classification.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 417?424.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study
of evaluative and speculative language. In Proceed-
ings of the Second SIGdial Workshop on Discourse
and Dialogue, pages 1?10.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the 2003 conference on
Empirical methods in natural language processing,
pages 129?136.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian
fields and harmonic functions. In In ICML, pages
912?919.
403
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555?564,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Identifying Non-explicit Citing Sentences for Citation-based
Summarization
Vahed Qazvinian
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
Identifying background (context) informa-
tion in scientific articles can help schol-
ars understand major contributions in their
research area more easily. In this paper,
we propose a general framework based
on probabilistic inference to extract such
context information from scientific papers.
We model the sentences in an article and
their lexical similarities as a Markov Ran-
dom Field tuned to detect the patterns that
context data create, and employ a Belief
Propagation mechanism to detect likely
context sentences. We also address the
problem of generating surveys of scien-
tific papers. Our experiments show greater
pyramid scores for surveys generated us-
ing such context information rather than
citation sentences alone.
1 Introduction
In scientific literature, scholars use citations to re-
fer to external sources. These secondary sources
are essential in comprehending the new research.
Previous work has shown the importance of cita-
tions in scientific domains and indicated that ci-
tations include survey-worthy information (Sid-
dharthan and Teufel, 2007; Elkiss et al, 2008;
Qazvinian and Radev, 2008; Mohammad et al,
2009; Mei and Zhai, 2008).
A citation to a paper in a scientific article may
contain explicit information about the cited re-
search. The following example is an excerpt from
a CoNLL paper1 that contains information about
Eisner?s work on bottom-up parsers and the notion
of span in parsing:
?Another use of bottom-up is due to Eisner
(1996), who introduced the notion of a span.?
1Buchholz and Marsi ?CoNLL-X Shared Task On Multi-
lingual Dependency Parsing?, CoNLL 2006
However, the citation to a paper may not always
include explicit information about the cited paper:
?This approach is one of those described in Eis-
ner (1996)?
Although this sentence alone does not provide any
information about the cited paper, it suggests that
its surrounding sentences describe the proposed
approach in Eisner?s paper:
?... In an all pairs approach, every possible
pair of two tokens in a sentence is considered
and some score is assigned to the possibility of
this pair having a (directed) dependency rela-
tion. Using that information as building blocks,
the parser then searches for the best parse for
the sentence. This approach is one of those de-
scribed in Eisner (1996).?
We refer to such implicit citations that contain
information about a specific secondary source but
do not explicitly cite it, as sentences with con-
text information or context sentences for short.
We look at the patterns that such sentences cre-
ate and observe that context sentences occur with-
ing a small neighborhood of explicit citations. We
also discuss the problem of extracting context sen-
tences for a source-reference article pair. We pro-
pose a general framework that looks at each sen-
tence as a random variable whose value deter-
mines its state about the target paper. In summary,
our proposed model is based on the probabilistic
inference of these random variables using graphi-
cal models. Finally we give evidence on how such
sentences can help us produce better surveys of re-
search areas. The rest of this paper is organized as
follows. Preceded by a review of prior work in
Section 2, we explain the data collection and our
annotation process in Section 3. Section 4 explains
our methodology and is followed by experimental
setup in Section 5.
555
#Refs
ACL-ID Author Title Year all AAN # Sents
P08-2026 McClosky & Charniak Self-Training for Biomedical Parsing 2008 12 8 102
N07-1025? Mihalcea Using Wikipedia for Automatic ... 2007 21 12 153
N07-3002 Wang Learning Structured Classifiers ... 2007 22 14 74
P06-1101 Snow et, al. Semantic Taxonomy Induction ... 2006 19 9 138
P06-1116 Abdalla & Teufel A Bootstrapping Approach To ... 2006 24 10 231
W06-2933 Nivre et, al. Labeled Pseudo-Projective Dependency ... 2006 27 5 84
P05-1044 Smith & Eisner Contrastive Estimation: Training Log-Linear ... 2005 30 13 262
P05-1073 Toutanova et, al. Joint Learning Improves Semantic Role Labeling 2005 14 10 185
N03-1003 Barzilay & Lee Learning To Paraphrase: An Unsupervised ... 2003 26 13 203
N03-2016? Kondrak et, al. Cognates Can Improve Statistical Translation ... 2003 8 5 92
Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publi-
cation year, number of references (in AAN) and number of sentences. Papers marked with ? are used to
calculate inter-judge agreement.
2 Prior Work
Analyzing the structure of scientific articles and
their relations has received a lot of attention re-
cently. The structure of citation and collaboration
networks has been studied in (Teufel et al, 2006;
Newman, 2001), and summarization of scientific
documents is discussed in (Teufel and Moens,
2002). In addition, there is some previous work
on the importance of citation sentences. Elkiss et
al, (Elkiss et al, 2008) perform a large-scale study
on citations in the free PubMed Central (PMC)
and show that they contain information that may
not be present in abstracts. In other work, Nanba
et al (Nanba and Okumura, 1999; Nanba et al,
2004b; Nanba et al, 2004a) analyze citation sen-
tences and automatically categorize them in order
to build a tool for survey generation.
The text of scientific citations has been used in
previous research. Bradshaw (Bradshaw, 2002;
Bradshaw, 2003) uses citations to determine the
content of articles. Similarly, the text of cita-
tion sentences has been directly used to produce
summaries of scientific papers in (Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Mohammad
et al, 2009). Determining the scientific attribu-
tion of an article has also been studied before.
Siddharthan and Teufel (Siddharthan and Teufel,
2007; Teufel, 2005) categorize sentences accord-
ing to their role in the author?s argument into pre-
defined classes: Own, Other, Background, Tex-
tual, Aim, Basis, Contrast.
Little work has been done on automatic cita-
tion extraction from research papers. Kaplan et
al, (Kaplan et al, 2009) introduces ?citation-site?
as a block of text in which the cited text is dis-
cussed. The mentioned work uses a machine
learning method for extracting citations from re-
search papers and evaluates the result using 4 an-
notated articles.
In our work we use graphical models to ex-
tract context sentences. Graphical models have
a number of properties and corresponding tech-
niques and have been used before on Information
Retrieval tasks. Romanello et al (Romanello et
al., 2009) use Conditional Random Fields (CRF)
to extract references from unstructured text in dig-
ital libraries of classic texts. Similar work include
term dependency extraction (Metzler and Croft,
2005), query expansion (Metzler and Croft, 2007),
and automatic feature selection (Metzler, 2007).
3 Data
The ACL Anthology Network (AAN)2 is a col-
lection of papers from the ACL Anthology3 pub-
lished in the Computational Linguistics journal
and proceedings from ACL conferences and work-
shops and includes more than 14, 000 papers over
a period of four decades (Radev et al, 2009).
AAN includes the citation network of the papers
in the ACL Anthology. The papers in AAN are
publicly available in text format retrieved by an
OCR process from the original pdf files, and are
segmented into sentences.
To build a corpus for our experiments we picked
10 recently published papers from various areas
in NLP4, each of which had references for a to-
tal of 203 candidate paper-reference pairs. Table 1
lists these papers together with their authors, titles,
publication year, number of references, number of
references within AAN, and the number of sen-
2http://clair.si.umich.edu/clair/anthology/
3http://www.aclweb.org/anthology-new/
4Regardless of data selection, the methodology in this
work is applicable to any of the papers in AAN.
556
L&PS&al Sentence
? ? ?
C C Jacquemin (1999) and Barzilay and McKeown (2001) identify
phrase level paraphrases, while Lin and Pantel (2001) and
Shinyama et al (2002) acquire structural paraphrases encoded
as templates.
1 1 These latter are the most closely related to the sentence-level para-
phrases we desire, and so we focus in this section on template-
induction approaches.
C 0 Lin and Pantel (2001) extract inference rules, which are related
to paraphrases (for example, X wrote Y implies X is the author of
Y), to improve question answering.
1 0 They assume that paths in dependency trees that take similar argu-
ments (leaves) are close in meaning.
1 0 However, only two-argument templates are considered.
0 C Shinyama et al (2002) also use dependency-tree information to
extract templates of a limited form (in their case, determined by
the underlying information extraction application).
1 1 Like us (and unlike Lin and Pantel, who employ a single large
corpus), they use articles written about the same event in different
newspapers as data.
1 1 Our approach shares two characteristics with the two methods just
described: pattern comparison by analysis of the patterns respec-
tive arguments, and use of nonparallel corpora as a data source.
0 0 However, extraction methods are not easily extended to generation
methods.
1 1 One problem is that their templates often only match small frag-
ments of a sentence.
1 1 While this is appropriate for other applications, deciding whether
to use a given template to generate a paraphrase requires informa-
tion about the surrounding context provided by the entire sentence.
? ? ?
Table 2: Part of the annotation for N03-1003 with
respect to two of its references ?Lin and Pan-
tel (2001)? (the first column) ?Shinyama et al
(2002)? (the second column). Cs indicate explicit
citations, 1s indicate implicit citations and 0s are
none.
tences.
3.1 Annotation Process
We annotated the sentences in each paper from Ta-
ble 1. Each annotation instance in our setting cor-
responds to a paper-reference pair, and is a vec-
tor in which each dimension corresponds to a sen-
tence and is marked with a C if it explicitly cites
the reference, and with a 1 if it implicitly talks
about it. All other sentences are marked with 0s.
Table 2 shows a portion of two separate annota-
tion instances of N03-1003 corresponding to two
of its references. Our annotation has resulted in
203 annotation instances each corresponding to
one paper-reference pair. The goal of this work
is to automatically identify all context sentences,
which are marked as ?1?.
3.1.1 Inter-judge Agreement
We also asked a neutral annotator5 to annotate
two of our datasets that are marked with ? in Ta-
ble 1. For each paper-reference pair, the annotator
was provided with a vector in which explicit cita-
5Someone not involved with the paper but an expert in
NLP.
ACL-ID vector size # Annotations ?
N07-1025? 153 21 0.889 ? 0.30
N03-2016? 92 8 0.853 ? 0.35
Table 3: Average ? coefficient as inter-judge
agreement for annotations of two sets
tions were already marked with Cs. The annota-
tion guidelines instructed the annotator to look at
each explicit citation sentence, and read up to 15
sentences before and after, then mark context sen-
tences around that sentence with 1s. Next, the 29
annotation instances done by the external annota-
tor were compared with the corresponding anno-
tations that we did, and the Kappa coefficient (?)
was calculated. The ? statistic is formulated as
? =
Pr(a)? Pr(e)
1? Pr(e)
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that an-
notators agree by chance if each annotator is ran-
domly assigning categories. To calculate ?, we ig-
nored all explicit citations (since they were pro-
vided to the external annotator) and used the bi-
nary categories (i.e., 1 for context sentences, and
0 otherwise) for all other sentences. Table 3 shows
the annotation vector size (i.e., number of sen-
tences), number of annotation instances (i.e., num-
ber of references), and average ? for each set. The
average ? is above 0.85 in both cases, suggest-
ing that the annotation process has a low degree
of subjectivity and can be considered reliable.
3.2 Analysis
In this section we describe our analysis. First,
we look at the number of explicit citations each
reference has received in a paper. Figure 1 (a)
shows the histogram corresponding to this distri-
bution. It indicates that the majority of references
get cited in only 1 sentence in a scientific arti-
cle, while the maximum being 9 in our collected
dataset with only 1 instance (i.e., there is only 1
reference that gets cited 9 times in a paper). More-
over, the data exhibits a highly positive-skewed
distribution. This is illustrated on a log-log scale
in Figure 1 (b). This highly skewed distribution
indicates that the majority of references get cited
only once in a citing paper. The very small number
of citing sentences can not make a full inventory of
the contributions of the cited paper, and therefore,
extracting explicit citations alone without context
557
gap size 0 1 2 4 9 10 15 16
instance 273 14 2 1 2 1 1 1
Table 4: The distribution of gaps in the annotated
data
sentences may result in information loss about the
contributions of the cited paper.
1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
120
140
cit
100 101
10?3
10?2
10?1
100
cit
p(cit)
 
 
alpha = 3.13; D=0.02
a b
Figure 1: (a) Histogram of the number of differ-
ent citations to each reference in a paper. (b) The
distribution observed for the number of different
citations on a log-log scale.
Next, we investigate the distance between con-
text sentences and the closest citations. For each
context sentence, we find its distance to the clos-
ets context sentence or explicit citation. Formally,
we define the gap to be the number of sentences
between a context sentence (marked with 1) and
the closest context sentence or explicit citation
(marked with either C or 1) to it. For example,
the second column of Table 2 shows that there is a
gap of size 1 in the 9th sentence in the set of con-
text and citation sentences about Shinyama et al
(2002). Table 4 shows the distribution of gap sizes
in the annotated data. This observation suggests
that the majority of context sentences directly oc-
cur after or before a citation or another context
sentence. However, it shows that gaps between
sentences describing a cited paper actually exist,
and a proposed method should have the capability
to capture them.
4 Proposed Method
In this section we propose our methodology that
enables us to identify the context information of a
cited paper. Particularly, the task is to assign a bi-
nary label XC to each sentence Si from a paper S,
where XC = 1 shows a context sentence related
to a given cited paper, C. To solve this problem
we propose a systematic way to model the net-
work level relationship between consecutive sen-
tences. In summary, each sentence is represented
with a node and is given two scores (context, non-
context), and we update these scores to be in har-
mony with the neighbors? scores.
A particular class of graphical models known
as Markov Random Fields (MRFs) are suited for
solving inference problems with uncertainty in ob-
served data. The data is modeled as an undirected
graph with two types of nodes: hidden and ob-
served. Observed nodes represent values that are
known from the data. Each hidden node xu, cor-
responding to an observed node yu, represents the
true state underlying the observed value. The state
of a hidden node is related to the value of its cor-
responding observed node as well as the states of
its neighboring hidden nodes.
The local Markov property of an MRF indi-
cates that a variable is conditionally independent
on all other variables given its neighbors: xv ?
? xV \cl(v)|xne(v), where ne(v) is the set of neigh-
bors of v, and cl(v) = {v} ? ne(v) is the closed
neighborhood of v. Thus, the state of a node is as-
sumed to statistically depend only upon its hidden
node and each of its neighbors, and independent
of any other node in the graph given its neighbors.
Dependencies in an MRF are represented using
two functions: Compatibility function (?) and Po-
tential function (?). ?uv(xc, xd) shows the edge
potential of an edge between two nodes u, v of
classes xc and xd. Large values of ?uv would
indicate a strong association between xc and xd
at nodes u, v. The Potential function, ?i(xc, yc),
shows the statistical dependency between xc and
yc at each node i assumed by the MRF model.
In order to find the marginal probabilities of
xis in a MRF we can use Belief Propagation
(BP) (Yedidia et al, 2003). If we assume the yis
are fixed and show ?i(xi, yi) by ?i(xi), we can
find the joint probability distribution for unknown
variables xi as
p({x}) = 1
Z
?
ij
?ij(xi, xj)
?
i
?i(xi)
In the BP algorithm a set of new variables m is
introduced where mij(xj) is the message passed
from i to j about what state xj should be in. Each
message, mij(xj), is a vector with the same di-
mensionality of xj in which each dimension shows
i?s opinion about j being in the corresponding
class. Therefore each message could be consid-
ered as a probability distribution and its compo-
nents should sum up to 1. The final belief at a
558
Figure 2: The illustration of the message updating
rule. Elements that make up the message from a
node i to another node j: messages from i?s neigh-
bors, local evidence at i, and propagation function
between i, j summed over all possible states of
node i.
node i, in the BP algorithm, is also a vector with
the same dimensionality of messages, and is pro-
portional to the local evidence as well as all mes-
sages from the node?s neighbors:
bi(xi)? k?i(xi)
?
j?ne(i)
mji(xi) (1)
where k is the normalization factor of the be-
liefs about different classes. The message passed
from i to j is proportional to the propagation func-
tion between i, j, the local evidence at i, and all
messages sent to i from its neighbors except j:
mij(xj)?
?
xi
?i(xi)?ij(xi, xj)
?
k?ne(i)\j
mki(xi) (2)
Figure 2 illustrates the message update rule.
Convergence can be determined based on a va-
riety of criteria. It can occur when the maximum
change of any message between iteration steps is
less than some threshold. Convergence is guaran-
teed for trees but not for general graphs. However,
it typically occurs in practice (McGlohon et al,
2009). Upon convergence, belief scores are deter-
mined by Equation 1.
4.1 MRF construction
To find the sentences from a paper that form the
context information of a given cited paper, we
build an MRF in which a hidden node xi and
an observed node yi correspond to each sentence.
The structure of the graph associated with the
MRF is dependent upon the validity of a basic as-
sumption. This assumption indicates that the gen-
eration of a sentence (in form of its words) only
(a) (b)
Figure 3: The structure of the MRF constructed
based on the independence of non-adjacent sen-
tences; (a) left, each sentence is independent on
all other sentences given its immediate neighbors.
(b) right, sentences have dependency relationship
with each other regardless of their position.
depends on its surrounding sentences. Said dif-
ferently, each sentence is written independently of
all other sentences given a number of its neigh-
bors. This local dependence assumption can result
in a number of different MRFs, each built assum-
ing a dependency between a sentence and all sen-
tences within a particular distance. Figure 3 shows
the structure of the two MRFs at either extreme of
the local dependence assumption. In Figure 3 a,
each sentence only depends on one following and
one preceding sentence, while Figure 3 b shows
an MRF in which sentences are dependent on each
other regardless of their position. We refer to the
former by BP1, and to the latter by BPn. Gen-
erally, we use BPi to denote an MRF in which
each sentence is connected to i sentences before
and after.
?ij(xc, xd) xd = 0 xd = 1
xc = 0 0.5 0.5
xc = 1 1? Sij Sij
Table 5: The compatibility function ? between
any two nodes in the MRFs from the sentences in
scientific papers
4.2 Compatibility Function
The compatibility function of an MRF represents
the association between the hidden node classes.
A node?s belief to be in class 1 is its probability to
be included in the context. The belief of a node i,
about its neighbor j to be in either classes is as-
sumed to be 0.5 if i is in class 0. In other words, if
a node is not part of the context itself, we assume
559
it has no effect on its neighbors? classes. In con-
trast, if i is in class 1 its belief about its neighbor
j is determined by their mutual lexical similarity.
If this similarity is close to 1 it indicates a stronger
tie between i, j. However, if i, j are not similar,
i?s probability of being in class 1, should not af-
fect that of j?s. To formalize this assumption we
use the sigmoid of the cosine similarity of two sen-
tences to build ?. More formally, we define S to
be
Sij =
1
1 + e?cosine(i,j)
The sigmoid function obtains a value of 0.5 for
a cosine of 0 indicating that there is no bias in the
association of the two sentences. The matrix in Ta-
ble 5 shows the compatibility function built based
on the above arguments.
4.3 Potential Function
The node potential function of an MRF can incor-
porate some other features observable from data.
Here, the goal is to find all sentences that are about
a specific cited paper, without having explicit cita-
tions. To build the node potential function of the
observed nodes, we use some sentence level fea-
tures. First, we use the explicit citation as an im-
portant feature of a sentence. This feature can af-
fect the belief of the corresponding hidden node,
which can in turn affect its neighbors? beliefs. For
a given paper-reference pair, we flag (with a 1)
each sentence that has an explicit citation to the
reference.
The second set of features that we are inter-
ested in are discourse-based features. In particu-
lar we match each sentence with specific patterns
and flag those that match. The first pattern is a bi-
gram in which the first term matches any of ?this;
that; those; these; his; her; their; such; previ-
ous?, and the second term matches any of ?work;
approach; system; method; technique; result; ex-
ample?. The second pattern includes all sentences
that start with ?this; such?.
Finally, the similarity of each sentence to the
reference is observable from the data and can be
used as a sentence-level feature. Intuitively, if a
sentence has higher similarity with the reference
paper, it should have a higher potential of being
in class 1 or C. The flag of each sentence here is
a value between 0 and 1 and is determined by its
cosine similarity to the reference. Once the flags
for each sentence, Si are determined, we calculate
normalized fi as the unweighted linear combina-
tion of individual features. Based on fis, we com-
pute the potential function, ?, as shown in Table 6.
?i(xc, yc) xc = 0 xc = 1
1? fi fi
Table 6: The node potential function ? for each
node in the MRFs from the sentences in scientific
papers is built using the sentences? flags computed
using sentence level features.
5 Experiments
The intrinsic evaluation of our methodology
means to directly compare the output of our
method with the gold standards obtained from the
annotated data. Our methodology finds the sen-
tences that cite a reference implicitly. Therefore
the output of the inference method is a vector, ?,
of 1?s and 0?s, whereby a 1 at element i means
that sentence i in the source document is a con-
text sentence about the reference while a 0 means
an explicit citation or neither. The gold standard
for each paper-reference pair, ? (obtained from the
annotated vectors in Section 3.1 by changing all
Cs to 0s), is also a vector of the same format and
dimensionality.
Precision, recall, and F? for this task can be de-
fined as
p = ? ? ?
? ? 1 ; r =
? ? ?
? ? 1 ; F? =
(1 + ?2)p ? r
?2p + r (3)
where 1 is a vector of 1?s with the same dimen-
sionality and ? is a non-negative real number.
5.1 Baseline Methods
The first baseline that we use is an IR-based
method. This baseline, B1, takes explicit citations
as an input but use them to find context sentences.
Given a paper-reference pair, for each explicit ci-
tation sentence, marked with C, B1 picks its pre-
ceding and following sentences if their similarities
to that sentence is greater than a cutoff (the median
of all such similarities), and repeats this for neigh-
boring sentences of newly marked sentences. In-
tuitively, B1 tries to find the best chain (window)
around citing sentences.
As the second baseline, we use the hand-crafted
discourse based features used in MRF?s potential
function. Particularly, this baseline, B2, marks
560
paper B1 B2 SVM BP1 BP4 BPn
P08-2026 0.441 0.237 0.249 0.470 0.613 0.285
N07-1025 0.388 0.102 0.124 0.313 0.466 0.138
N07-3002 0.521 0.339 0.232 0.742 0.627 0.315
P06-1101 0.125 0.388 0.127 0.649 0.889 0.193
P06-1116 0.283 0.104 0.100 0.307 0.341 0.130
W06-2933 0.313 0.100 0.176 0.338 0.413 0.160
P05-1044 0.225 0.100 0.060 0.172 0.586 0.094
P05-1073 0.144 0.100 0.144 0.433 0.518 0.171
N03-1003 0.245 0.249 0.126 0.523 0.466 0.125
N03-2016 0.100 0.181 0.224 0.439 0.482 0.185
Table 7: Average F?=3 for similarity based baseline (B1), discourse-based baseline (B2), a supervised
method (SVM) and three MRF-based methods.
each sentence that is within a particular distance
(4 in our experiments) of an explicit citation and
matches one of the two patterns mentioned in Sec-
tion 4.3. After marking all such sentences, B2
also marks all sentences between them and the
closest explicit citation, which is no farther than
4 sentences away. This baseline helps us under-
stand how effectively this sentence level feature
can work in the absence of other features and the
network structure.
Finally, we use a supervised method, SVM,
to classify sentences as context/non-context. We
use 4 features to train the SVM model. These
4 features comprise the 3 sentence level features
used in MRF?s potential function (i.e., similar-
ity to reference, explicit citation, matching certain
regular-expressions) and a network level feature:
distance to the closes explicit citation. For each
source paper, P , we use all other source papers
and their source-reference annotation instances to
train a model. We then use this model to clas-
sify all instances in P . Although the number of
references and thus source-reference pairs are dif-
ferent for different papers, this can be considered
similar to a 10-fold cross validation scheme, since
for each source paper the model is built using all
source-reference pairs of all other 9 papers.
We compare these baselines with 3 MRF-based
systems each with a different assumption about in-
dependence of sentences. BP1 denotes an MRF
in which each sentence is only connected to 1 sen-
tence before and after. In BP4 locality is more
relaxed and each sentence is connected to 4 sen-
tences on each sides. BPn denotes an MRF in
which all sentences are connected to each other
regardless of their position in the paper.
Table 7 shows F?=3 for our experiments and
shows how BP4 outperforms the other methods
on average. The value 4 may suggest the fact that
although sentences might be independent of dis-
tant sentences, they depend on more than one sen-
tence on each side.
The final experiment we do to intrinsically eval-
uate the MRF-base method is to compare differ-
ent sentence-level features. The first feature used
to build the potential function is explicit citations.
This feature does not directly affect context sen-
tences (i.e., it affects the marginal probability of
context sentences through the MRF network con-
nections). Therefore, we do not alter this fea-
ture in comparing different features. However, we
look at the effect of the second and the third fea-
tures: hand-crafted regular expression-based fea-
tures and similarity to the reference. For each pa-
per, we use BP4 to perform 3 experiments: two in
absence of each feature and one including all fea-
tures. Figure 4 shows the average F?=3 for each
experiment. This plot shows that the features lead
to better results when used together.
6 Impact on Survey Generation
We also performed an extrinsic evaluation of
our context extraction methodology. Here we
show how context sentences add important survey-
worthy information to explicit citations. Previous
work that generate surveys of scientific topics use
the text of citation sentences alone (Mohammad
et al, 2009; Qazvinian and Radev, 2008). Here,
we show how the surveys generated using citations
and their context sentences are better than those
generated using citation sentences alone.
We use the data from (Mohammad et al, 2009)
561
... Naturally, our current work on question answering for the reading comprehension task is most related to those of
(Hirschman et al , 1999; Charniak et al , 2000; Riloffand Thelen, 2000 ; Wang et al , 2000). In fact, all of this
body of work as well as ours are evaluated on the same set of test stories, and are developed (or trained) on the
same development set of stories. The work of (Hirschman et al , 1999) initiated this series of work, and it reported
an accuracy of 36.3% on answering the questions in the test stories. Subsequently, the work of (Riloffand Thelen ,
2000) and (Chaxniak et al , 2000) improved the accuracy further to 39.7% and 41%, respectively. However, all
of these three systems used handcrafted, deterministic rules and algorithms...
...The cross-model comparison showed that the performance ranking of these models was: U-SVM > PatternM
> S-SVM > Retrieval-M. Compared with retrieval-based [Yang et al 2003], pattern-based [Ravichandran et al 2002
and Soubbotin et al 2002], and deep NLP-based [Moldovan et al 2002, Hovy et al 2001; and Pasca et al 2001]
answer selection, machine learning techniques are more effective in constructing QA components from scratch. These
techniques suffer, however, from the problem of requiring an adequate number of handtagged question-answer
training pairs. It is too expensive and labor intensive to collect such training pairs for supervised machine
learning techniques ...
... As expected, the definition and person-bio answer types are covered well by these resources. The web has
been employed for pattern acquisition (Ravichandran et al , 2003), document retrieval (Dumais et al , 2002), query
expansion (Yang et al , 2003), structured information extraction, and answer validation (Magnini et al , 2002). Some
of these approaches enhance existing QA systems, while others simplify the question answering task, allowing a
less complex approach to find correct answers ...
Table 8: A portion of the QA survey generated by LexRank using the context information.
Figure 4: Average F?=3 for BP4 employing dif-
ferent features.
that contains two sets of cited papers and corre-
sponding citing sentences, one on Question An-
swering (QA) with 10 papers and the other on De-
pendency Parsing (DP) with 16 papers. The QA
set contains two different sets of nuggets extracted
by experts respectively from paper abstracts and
citation sentences. The DP set includes nuggets
extracted only from citation sentences. We use
these nugget sets, which are provided in form of
regular expressions, to evaluate automatically gen-
erated summaries. To perform this experiment we
needed to build a new corpus that includes con-
text sentences. For each citation sentence, BP4 is
used on the citing paper to extract the proper con-
text. Here, we limit the context size to be 4 on
each side. That is, we attach to a citing sentence
any of its 4 preceding and following sentences if
citation survey context survey
QA
CT nuggets 0.416 0.634
AB nuggets 0.397 0.594
DP
CT nuggets 0.324 0.379
Table 9: Pyramid F?=3 scores of automatic
surveys of QA and DP data. The QA surveys
are evaluated using nuggets drawn from citation
texts (CT), or abstracts (AB), and DP surveys are
evaluated using nuggets from citation texts (CT).
BP4 marks them as context sentences. Therefore,
we build a new corpus in which each explicit ci-
tation sentence is replaced with the same sentence
attached to at most 4 sentence on each side.
After building the context corpus, we use
LexRank (Erkan and Radev, 2004) to generate 2
QA and 2 DP surveys using the citation sentences
only, and the new context corpus explained above.
LexRank is a multidocument summarization sys-
tem, which first builds a cosine similarity graph of
all the candidate sentences. Once the network is
built, the system finds the most central sentences
by performing a random walk on the graph. We
limit these surveys to be of a maximum length of
1000 words. Table 8 shows a portion of the sur-
vey generated from the QA context corpus. This
example shows how context sentences add mean-
ingful and survey-worthy information along with
citation sentences. Table 9 shows the Pyramid
F?=3 score of automatic surveys of QA and DP
562
data. The QA surveys are evaluated using nuggets
drawn from citation texts (CT), or abstracts (AB),
and DP surveys are evaluated using nuggets from
citation texts (CT). In all evaluation instances the
surveys generated with the context corpora excel
at covering nuggets drawn from abstracts or cita-
tion sentences.
7 Conclusion
In this paper we proposed a framework based on
probabilistic inference to extract sentences that
appear in the scientific literature, and which are
about a secondary source, but which do not con-
tain explicit citations to that secondary source.
Our methodology is based on inference in an MRF
built using the similarity of sentences and their
lexical features. We show, by numerical exper-
iments, that an MRF in which each sentence is
connected to only a few adjacent sentences prop-
erly fits this problem. We also investigate the use-
fulness of such sentences in generating surveys of
scientific literature. Our experiments on generat-
ing surveys for Question Answering and Depen-
dency Parsing show how surveys generated using
such context information along with citation sen-
tences have higher quality than those built using
citations alone.
Generating fluent scientific surveys is difficult
in absence of sufficient background information.
Our future goal is to combine summarization
and bibliometric techniques towards building au-
tomatic surveys that employ context information
as an important part of the generated surveys.
8 Acknowledgments
The authors would like to thank Arzucan ?Ozgu?r
from University of Michigan for annotations.
This paper is based upon work supported by the
National Science Foundation grant ?iOPENER: A
Flexible Framework to Support Rapid Learning in
Unfamiliar Research Domains?, jointly awarded
to University of Michigan and University of Mary-
land as IIS 0705832. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessar-
ily reflect the views of the National Science Foun-
dation.
References
Shannon Bradshaw. 2002. Reference Directed Index-
ing: Indexing Scientific Literature in the Context of
Its Use. Ph.D. thesis, Northwestern University.
Shannon Bradshaw. 2003. Reference directed index-
ing: Redeeming relevance for subject search in ci-
tation indexes. In Proceedings of the 7th European
Conference on Research and Advanced Technology
for Digital Libraries.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan, David States, and Dragomir R. Radev. 2008.
Blind men and elephants: What do citation sum-
maries tell us about a research article? Journal of
the American Society for Information Science and
Technology, 59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009.
Automatic extraction of citation contexts for re-
search paper summarization: A coreference-chain
based approach. In Proceedings of the 2009 Work-
shop on Text and Citation Analysis for Scholarly
Digital Libraries, pages 88?95, Suntec City, Sin-
gapore, August. Association for Computational Lin-
guistics.
Mary McGlohon, Stephen Bay, Markus G. Anderle,
David M. Steier, and Christos Faloutsos. 2009.
Snare: a link analytic system for graph labeling and
risk detection. In KDD ?09: Proceedings of the 15th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 1265?1274.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In SI-
GIR ?05: Proceedings of the 28th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 472?479.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In SI-
GIR ?07: Proceedings of the 30th annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 311?318.
Donald A. Metzler. 2007. Automatic feature selection
in the markov random field model for information
retrieval. In CIKM ?07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 253?262.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
563
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 584?592, Boulder, Colorado, June.
Association for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Oku-
mura, and Suguru Saito. 2004a. Bilingual presri:
Integration of multiple research paper databases. In
Proceedings of RIAO 2004, pages 195?211, Avi-
gnon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Oku-
mura. 2004b. Classification of research papers us-
ing citation links and citation types: Towards au-
tomatic review article generation. In Proceedings
of the 11th SIG Classification Research Workshop,
pages 117?134, Chicago, USA.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In COLING 2008, Manchester, UK.
Dragomir R. Radev, Pradeep Muthukrishnan, and Va-
hed Qazvinian. 2009. The ACL anthology network
corpus. In ACL workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries.
Matteo Romanello, Federico Boschetti, and Gregory
Crane. 2009. Citations in the digital library of clas-
sics: Extracting canonical references by using con-
ditional random fields. In Proceedings of the 2009
Workshop on Text and Citation Analysis for Schol-
arly Digital Libraries, pages 80?87, Suntec City,
Singapore, August. Association for Computational
Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summarizing
scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function.
In Proceedings of the EMNLP, Sydney, Australia,
July.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and
Affect in Text: Theory and Applications, pages 159?
170.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2003. Understanding belief propagation and
its generalizations. pages 239?269.
564
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 500?509,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Coherent Citation-Based Summarization of Scientific Papers
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In citation-based summarization, text written
by several researchers is leveraged to identify
the important aspects of a target paper. Previ-
ous work on this problem focused almost ex-
clusively on its extraction aspect (i.e. selecting
a representative set of citation sentences that
highlight the contribution of the target paper).
Meanwhile, the fluency of the produced sum-
maries has been mostly ignored. For exam-
ple, diversity, readability, cohesion, and order-
ing of the sentences included in the summary
have not been thoroughly considered. This re-
sulted in noisy and confusing summaries. In
this work, we present an approach for produc-
ing readable and cohesive citation-based sum-
maries. Our experiments show that the pro-
posed approach outperforms several baselines
in terms of both extraction quality and fluency.
1 Introduction
Scientific research is a cumulative activity. The
work of downstream researchers depends on access
to upstream discoveries. The footnotes, end notes,
or reference lists within research articles make this
accumulation possible. When a reference appears in
a scientific paper, it is often accompanied by a span
of text describing the work being cited.
We name the sentence that contains an explicit
reference to another paper citation sentence. Cita-
tion sentences usually highlight the most important
aspects of the cited paper such as the research prob-
lem it addresses, the method it proposes, the good
results it reports, and even its drawbacks and limita-
tions.
By aggregating all the citation sentences that cite
a paper, we have a rich source of information about
it. This information is valuable because human ex-
perts have put their efforts to read the paper and sum-
marize its important contributions.
One way to make use of these sentences is creat-
ing a summary of the target paper. This summary
is different from the abstract or a summary gener-
ated from the paper itself. While the abstract rep-
resents the author?s point of view, the citation sum-
mary is the summation of multiple scholars? view-
points. The task of summarizing a scientific paper
using its set of citation sentences is called citation-
based summarization.
There has been previous work done on citation-
based summarization (Nanba et al, 2000; Elkiss et
al., 2008; Qazvinian and Radev, 2008; Mei and Zhai,
2008; Mohammad et al, 2009). Previous work fo-
cused on the extraction aspect; i.e. analyzing the
collection of citation sentences and selecting a rep-
resentative subset that covers the main aspects of the
paper. The cohesion and the readability of the pro-
duced summaries have been mostly ignored. This
resulted in noisy and confusing summaries.
In this work, we focus on the coherence and read-
ability aspects of the problem. Our approach pro-
duces citation-based summaries in three stages: pre-
processing, extraction, and postprocessing. Our ex-
periments show that our approach produces better
summaries than several baseline summarization sys-
tems.
The rest of this paper is organized as follows. Af-
ter we examine previous work in Section 2, we out-
line the motivation of our approach in Section 3.
Section 4 describes the three stages of our summa-
rization system. The evaluation and the results are
presented in Section 5. Section 6 concludes the pa-
per.
500
2 Related Work
The idea of analyzing and utilizing citation informa-
tion is far from new. The motivation for using in-
formation latent in citations has been explored tens
of years back (Garfield et al, 1984; Hodges, 1972).
Since then, there has been a large body of research
done on citations.
Nanba and Okumura (2000) analyzed citation
sentences and automatically categorized citations
into three groups using 160 pre-defined phrase-
based rules. They also used citation categoriza-
tion to support a system for writing surveys (Nanba
and Okumura, 1999). Newman (2001) analyzed
the structure of the citation networks. Teufel et
al. (2006) addressed the problem of classifying ci-
tations based on their function.
Siddharthan and Teufel (2007) proposed a method
for determining the scientific attribution of an arti-
cle by analyzing citation sentences. Teufel (2007)
described a rhetorical classification task, in which
sentences are labeled as one of Own, Other, Back-
ground, Textual, Aim, Basis, or Contrast according
to their role in the authors argument. In parts of our
approach, we were inspired by this work.
Elkiss et al (2008) performed a study on citation
summaries and their importance. They concluded
that citation summaries are more focused and con-
tain more information than abstracts. Mohammad
et al (2009) suggested using citation information to
generate surveys of scientific paradigms.
Qazvinian and Radev (2008) proposed a method
for summarizing scientific articles by building a sim-
ilarity network of the citation sentences that cite
the target paper, and then applying network analy-
sis techniques to find a set of sentences that covers
as much of the summarized paper facts as possible.
We use this method as one of the baselines when we
evaluate our approach. Qazvinian et al (2010) pro-
posed a citation-based summarization method that
first extracts a number of important keyphrases from
the set of citation sentences, and then finds the best
subset of sentences that covers as many keyphrases
as possible. Qazvinian and Radev (2010) addressed
the problem of identifying the non-explicit citing
sentences to aid citation-based summarization.
3 Motivation
The coherence and readability of citation-based
summaries are impeded by several factors. First,
many citation sentences cite multiple papers besides
the target. For example, the following is a citation
sentence that appeared in the NLP literature and
talked about Resnik?s (1999) work.
(1) Grefenstette and Nioche (2000) and Jones
and Ghani (2000) use the web to generate cor-
pora for languages where electronic resources are
scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
The first fragment of this sentence describes dif-
ferent work other than Resnik?s. The contribution
of Resnik is mentioned in the underlined fragment.
Including the irrelevant fragments in the summary
causes several problems. First, the aim of the sum-
marization task is to summarize the contribution of
the target paper using minimal text. These frag-
ments take space in the summary while being irrel-
evant and less important. Second, including these
fragments in the summary breaks the context and,
hence, degrades the readability and confuses the
reader. Third, the existence of irrelevant fragments
in a sentence makes the ranking algorithm assign a
low weight to it although the relevant fragment may
cover an aspect of the paper that no other sentence
covers.
A second factor has to do with the ordering of the
sentences included in the summary. For example,
the following are two other citation sentences for
Resnik (1999).
(2) Mining the Web for bilingual text (Resnik, 1999) is
not likely to provide sufficient quantities of high quality
data.
(3) Resnik (1999) addressed the issue of language
identification for finding Web pages in the languages of
interest.
If these two sentences are to be included in the
summary, the reasonable ordering would be to put
the second sentence first.
Thirdly, in some instances of citation sentences,
the reference is not a syntactic constituent in the sen-
501
tence. It is added just to indicate the existence of
citation. For example, in sentence (2) above, the ref-
erence could be safely removed from the sentence
without hurting its grammaticality.
In other instances (e.g. sentence (3) above), the
reference is a syntactic constituent of the sentence
and removing it makes the sentence ungrammatical.
However, in certain cases, the reference could be re-
placed with a suitable pronoun (i.e. he, she or they).
This helps avoid the redundancy that results from re-
peating the author name(s) in every sentence.
Finally, a significant number of citation sentences
are not suitable for summarization (Teufel et al,
2006) and should be filtered out. The following
sentences are two examples.
(4) The two algorithms we employed in our depen-
dency parsing model are the Eisner parsing (Eisner,
1996) and Chu-Lius algorithm (Chu and Liu, 1965).
(5) This type of model has been used by, among others,
Eisner (1996).
Sentence (4) appeared in a paper by Nguyen et al
(2007). It does not describe any aspect of Eisner?s
work, rather it informs the reader that Nguyen et al
used Eisner?s algorithm in their model. There is no
value in adding this sentence to the summary of Eis-
ner?s paper. Teufel (2007) reported that a significant
number of citation sentences (67% of the sentences
in her dataset) were of this type.
Likewise, the comprehension of sentence (5) de-
pends on knowing its context (i.e. its surrounding
sentences). This sentence alone does not provide
any valuable information about Eisner?s paper and
should not be added to the summary unless its con-
text is extracted and included in the summary as
well.
In our approach, we address these issues to
achieve the goal of improving the coherence and the
readability of citation-based summaries.
4 Approach
In this section we describe a system that takes a sci-
entific paper and a set of citation sentences that cite
it as input, and outputs a citation summary of the
paper. Our system produces the summaries in three
stages. In the first stage, the citation sentences are
preprocessed to rule out the unsuitable sentences and
the irrelevant fragments of sentences. In the sec-
ond stage, a number of citation sentences that cover
the various aspects of the paper are selected. In the
last stage, the selected sentences are post-processed
to enhance the readability of the summary. We de-
scribe the stages in the following three subsections.
4.1 Preprocessing
The aim of this stage is to determine which pieces of
text (sentences or fragments of sentences) should be
considered for selection in the next stage and which
ones should be excluded. This stage involves three
tasks: reference tagging, reference scope identifica-
tion, and sentence filtering.
4.1.1 Reference Tagging
A citation sentence contains one or more references.
At least one of these references corresponds to the
target paper. When writing scientific articles, au-
thors usually use standard patterns to include point-
ers to their references within the text. We use pattern
matching to tag such references. The reference to
the target is given a different tag than the references
to other papers.
The following example shows a citation sentence
with all the references tagged and the target refer-
ence given a different tag.
In <TREF>Resnik (1999)</TREF>, <REF>Nie,
Simard, and Foster (2001)</REF>, <REF>Ma and
Liberman (1999)</REF>, and <REF>Resnik and
Smith (2002)</REF>, the Web is harvested in search of
pages that are available in two languages.
4.1.2 Identifying the Reference Scope
In the previous section, we showed the importance
of identifying the scope of the target reference; i.e.
the fragment of the citation sentence that corre-
sponds to the target paper. We define the scope of
a reference as the shortest fragment of the citation
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed.
To find such a fragment, we use a simple yet ade-
quate heuristic. We start by parsing the sentence us-
ing the link grammar parser (Sleator and Temperley,
502
1991). Since the parser is not trained on citation sen-
tences, we replace the references with placeholders
before passing the sentence to the parser. Figure 1
shows a portion of the parse tree for Sentence (1)
(from Section 1).
Figure 1: An example showing the scope of a target ref-
erence
We extract the scope of the reference from the
parse tree as follows. We find the smallest subtree
rooted at an S node (sentence clause node) and con-
tains the target reference node. We extract the text
that corresponds to this subtree if it is grammati-
cal. Otherwise, we find the second smallest subtree
rooted at an S node and so on. For example, the
parse tree shown in Figure 1 suggests that the scope
of the reference is:
Resnik (1999) describes a method for mining the web for
bilingual texts.
4.1.3 Sentence Filtering
The task in this step is to detect and filter out unsuit-
able sentences; i.e., sentences that depend on their
context (e.g. Sentence (5) above) or describe the
own work of their authors, not the contribution of
the target paper (e.g Sentence (4) above). Formally,
we classify the citation sentences into two classes:
suitable and unsuitable sentences. We use a ma-
chine learning technique for this purpose. We ex-
tract a number of features from each sentence and
train a classification model using these features. The
trained model is then used to classify the sentences.
We use Support Vector Machines (SVM) with linear
kernel as our classifier. The features that we use in
this step and their descriptions are shown in Table 1.
4.2 Extraction
In the first stage, the sentences and sentence frag-
ments that are not useful for our summarization task
are ruled out. The input to this stage is a set of cita-
tion sentences that are believed to be suitable for the
summary. From these sentences, we need to select
a representative subset. The sentences are selected
based on these three main properties:
First, they should cover diverse aspects of the pa-
per. Second, the sentences that cover the same as-
pect should not contain redundant information. For
example, if two sentences talk about the drawbacks
of the target paper, one sentence can mention the
computation inefficiency, while the other criticize
the assumptions the paper makes. Third, the sen-
tences should cover as many important facts about
the target paper as possible using minimal text.
In this stage, the summary sentences are selected
in three steps. In the first step, the sentences are clas-
sified into five functional categories: Background,
Problem Statement, Method, Results, and Limita-
tions. In the second step, we cluster the sen-
tences within each category into clusters of simi-
lar sentences. In the third step, we compute the
LexRank (Erkan and Radev, 2004) values for the
sentences within each cluster. The summary sen-
tences are selected based on the classification, the
clustering, and the LexRank values.
4.2.1 Functional Category Classification
We classify the citation sentences into the five cat-
egories mentioned above using a machine learning
technique. A classification model is trained on a
number of features (Table 2) extracted from a la-
beled set of citation sentences. We use SVM with
linear kernel as our classifier.
4.2.2 Sentence Clustering
In the previous step we determined the category
of each citation sentence. It is very likely that
sentences from the same category contain similar or
overlapping information. For example, Sentences
(6), (7), and (8) below appear in the set of citation
503
Feature Description
Similarity to the target paper The value of the cosine similarity (using TF-IDF vectors) between the citation sentence and the target paper.
Headlines The section in which the citation sentence appeared in the citing paper. We recognize 10 section types such
as Introduction, Related Work, Approach, etc.
Relative position The relative position of the sentence in the section and the paragraph in which it appears
First person pronouns This feature takes a value of 1 if the sentence contains a first person pronoun (I, we, our, us, etc.), and 0
otherwise.
Tense of the first verb A sentence that contains a past tense verb near its beginning is more likely to be describing previous work.
Determiners Demonstrative Determiners (this, that, these, those, and which) and Alternative Determiners (another, other).
The value of this feature is the relative position of the first determiner (if one exists) in the sentence.
Table 1: The features used for sentence filtering
Feature Description
Similarity to the sections of the target paper The sections of the target paper are categorized into five categories: 1) Introduction, Moti-
vation, Problem Statement. 2) Background, Prior Work, Previous Work, and Related Work.
3) Experiments, Results, and Evaluation. 4) Discussion, Conclusion, and Future work. 5)
All other headlines. The value of this feature is the cosine similarity (using TF-IDF vectors)
between the sentence and the text of the sections of each of the five section categories.
Headlines This is the same feature that we used for sentence filtering in Section 4.1.3.
Number of references in the sentence Sentences that contain multiple references are more likely to be Background sentences.
Verbs We use all the verbs that their lemmatized form appears in at least three sentences that belong
to the same category in the training set. Auxiliary verbs are excluded. In our annotated dataset,
for example, the verb propose appeared in 67 sentences from the Methodology category, while
the verbs outperform and achieve appeared in 33 Result sentences.
Table 2: The features used for sentence classification
sentences that cite Goldwater and Griffiths? (2007).
These sentences belong to the same category (i.e
Method). Both Sentences (6) and (7) convey the
same information about Goldwater and Griffiths
(2007) contribution. Sentence (8), however, de-
scribes a different aspect of the paper methodology.
(6) Goldwater and Griffiths (2007) proposed an
information-theoretic measure known as the Variation of
Information (VI)
(7) Goldwater and Griffiths (2007) propose using the
Variation of Information (VI) metric
(8) A fully-Bayesian approach to unsupervised POS
tagging has been developed by Goldwater and Griffiths
(2007) as a viable alternative to the traditional maximum
likelihood-based HMM approach.
Clustering divides the sentences of each cate-
gory into groups of similar sentences. Following
Qazvinian and Radev (2008), we build a cosine sim-
ilarity graph out of the sentences of each category.
This is an undirected graph in which nodes are sen-
tences and edges represent similarity relations. Each
edge is weighted by the value of the cosine similarity
(using TF-IDF vectors) between the two sentences
the edge connects. Once we have the similarity net-
work constructed, we partition it into clusters using
a community finding technique. We use the Clauset
algorithm (Clauset et al, 2004), a hierarchical ag-
glomerative community finding algorithm that runs
in linear time.
4.2.3 Ranking
Although the sentences that belong to the same clus-
ter are similar, they are not necessarily equally im-
portant. We rank the sentences within each clus-
ter by computing their LexRank (Erkan and Radev,
2004). Sentences with higher rank are more impor-
tant.
4.2.4 Sentence Selection
At this point we have determined (Figure 2), for each
sentence, its category, its cluster, and its relative im-
portance. Sentences are added to the summary in
order based on their category, the size of their clus-
ters, then their LexRank values. The categories are
504
Figure 2: Example illustrating sentence selection
ordered as Background, Problem, Method, Results,
then Limitations. Clusters within each category are
ordered by the number of sentences in them whereas
the sentences of each cluster are ordered by their
LexRank values.
In the example shown in Figure 2, we have three
categories. Each category contains several clusters.
Each cluster contains several sentences with differ-
ent LexRank values (illustrated by the sizes of the
dots in the figure.) If the desired length of the sum-
mary is 3 sentences, the selected sentences will be
in order S1, S12, then S18. If the desired length is 5,
the selected sentences will be S1, S5, S12, S15, then
S18.
4.3 Postprocessing
In this stage, we refine the sentences that we ex-
tracted in the previous stage. Each citation sentence
will have the target reference (the author?s names
and the publication year) mentioned at least once.
The reference could be either syntactically and se-
mantically part of the sentence (e.g. Sentence (3)
above) or not (e.g. Sentence (2)). The aim of this
refinement step is to avoid repeating the author?s
names and the publication year in every sentence.
We keep the author?s names and the publication year
only in the first sentence of the summary. In the
following sentences, we either replace the reference
with a suitable personal pronoun or remove it. The
reference is replaced with a pronoun if it is part of
the sentence and this replacement does not make the
sentence ungrammatical. The reference is removed
if it is not part of the sentence. If the sentence con-
tains references for other papers, they are removed if
this doesn?t hurt the grammaticality of the sentence.
To determine whether a reference is part of the
sentence or not, we again use a machine learning
approach. We train a model on a set of labeled sen-
tences. The features used in this step are listed in
Table 3. The trained model is then used to classify
the references that appear in a sentence into three
classes: keep, remove, replace. If a reference is to
be replaced, and the paper has one author, we use
?he/she? (we do not know if the author is male or
female). If the paper has two or more authors, we
use ?they?.
5 Evaluation
We provide three levels of evaluation. First, we eval-
uate each of the components in our system sepa-
rately. Then we evaluate the summaries that our
system generate in terms of extraction quality. Fi-
nally, we evaluate the coherence and readability of
the summaries.
5.1 Data
We use the ACL Anthology Network (AAN) (Radev
et al, 2009) in our evaluation. AAN is a collection
of more than 16000 papers from the Computational
Linguistics journal, and the proceedings of the ACL
conferences and workshops. AAN provides all cita-
tion information from within the network including
the citation network, the citation sentences, and the
citation context for each paper.
We used 55 papers from AAN as our data. The
papers have a variable number of citation sentences,
ranging from 15 to 348. The total number of cita-
tion sentences in the dataset is 4,335. We split the
data randomly into two different sets; one for evalu-
ating the components of the system, and the other for
evaluating the extraction quality and the readability
of the generated summaries. The first set (dataset1,
henceforth) contained 2,284 sentences coming from
25 papers. We asked humans with good background
in NLP (the area of the annotated papers) to provide
two annotations for each sentence in this set: 1) label
the sentence as Background, Problem, Method, Re-
sult, Limitation, or Unsuitable, 2) for each reference
in the sentence, determine whether it could be re-
placed with a pronoun, removed, or should be kept.
505
Feature Description
Part-of-speech (POS) tag We consider the POS tags of the reference, the word before, and the word after. Before passing the
sentence to the POS tagger, all the references in the sentence are replaced by placeholders.
Style of the reference It is common practice in writing scientific papers to put the whole citation between parenthesis
when the authors are not a constitutive part of the enclosing sentence, and to enclose just the year
between parenthesis when the author?s name is a syntactic constituent in the sentence.
Relative position of the reference This feature takes one of three values: first, last, and inside.
Grammaticality Grammaticality of the sentence if the reference is removed/replaced. Again, we use the Link
Grammar parser (Sleator and Temperley, 1991) to check the grammaticality
Table 3: The features used for author name replacement
Each sentence was given to 3 different annotators.
We used the majority vote labels.
We use Kappa coefficient (Krippendorff, 2003) to
measure the inter-annotator agreement. Kappa coef-
ficient is defined as:
Kappa =
P (A)? P (E)
1? P (E)
(1)
where P (A) is the relative observed agreement
among raters and P (E) is the hypothetical proba-
bility of chance agreement.
The agreement among the three annotators on dis-
tinguishing the unsuitable sentences from the other
five categories is 0.85. On Landis and Kochs(1977)
scale, this value indicates an almost perfect agree-
ment. The agreement on classifying the sentences
into the five functional categories is 0.68. On the
same scale this value indicates substantial agree-
ment.
The second set (dataset2, henceforth) contained
30 papers (2051 sentences). We asked humans with
a good background in NLP (the papers topic) to gen-
erate a readable, coherent summary for each paper in
the set using its citation sentences as the source text.
We asked them to fix the length of the summaries
to 5 sentences. Each paper was assigned to two hu-
mans to summarize.
5.2 Component Evaluation
Reference Tagging and Reference Scope Iden-
tification Evaluation: We ran our reference tag-
ging and scope identification components on the
2,284 sentences in dataset1. Then, we went through
the tagged sentences and the extracted scopes, and
counted the number of correctly/incorrectly tagged
(extracted)/missed references (scopes). Our tagging
- Bkgrnd Prob Method Results Limit.
Precision 64.62% 60.01% 88.66% 76.05% 33.53%
Recall 72.47% 59.30% 75.03% 82.29% 59.36%
F1 68.32% 59.65% 81.27% 79.04% 42.85%
Table 4: Precision and recall results achieved by our cita-
tion sentence classifier
component achieved 98.2% precision and 94.4% re-
call. The reference to the target paper was tagged
correctly in all the sentences.
Our scope identification component extracted the
scope of target references with good precision
(86.4%) but low recall (35.2%). In fact, extracting
a useful scope for a reference requires more than
just finding a grammatical substring. In future work,
we plan to employ text regeneration techniques to
improve the recall by generating grammatical sen-
tences from ungrammatical fragments.
Sentence Filtering Evaluation: We used Sup-
port Vector Machines (SVM) with linear kernel as
our classifier. We performed 10-fold cross validation
on the labeled sentences (unsuitable vs all other cat-
egories) in dataset1. Our classifier achieved 80.3%
accuracy.
Sentence Classification Evaluation: We used
SVM in this step as well. We also performed 10-
fold cross validation on the labeled sentences (the
five functional categories). This classifier achieved
70.1% accuracy. The precision and recall for each
category are given in Table 4
Author Name Replacement Evaluation: The
classifier used in this task is also SVM. We per-
formed 10-fold cross validation on the labeled sen-
tences of dataset1. Our classifier achieved 77.41%
accuracy.
506
Produced using our system
There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical tech-
niques, e.g. constraint-based techniques and transformation-based techniques. A thorough removal of ambiguity requires a syntactic
process. A rule-based tagger described in Voutilainen (1995) was equipped with a set of guessing rules that had been hand-crafted using
knowledge of English morphology and intuitions. The precision of rule-based taggers may exceed that of the probabilistic ones. The
construction of a linguistic rule-based tagger, however, has been considered a difficult and time-consuming task.
Produced using Qazvinian and Radev (2008) system
Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint Grammar work
(Karlsson et al , 1995; Voutilainen, 1995b; Voutilainen et al , 1992; Voutilainen and Tapanainen, 1993), where a large number of
hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context.
Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). A rule-based tagger described
in (Voutilainen, 1995) is equipped with a set of guessing rules which has been hand-crafted using knowledge of English morphology
and intuition. Older versions of EngCG (using about 1,150 constraints) are reported ( butilainen et al 1992; Voutilainen and HeikkiUi
1994; Tapanainen and Voutilainen 1994; Voutilainen 1995) to assign a correct analysis to about 99.7% of all words while each word in
the output retains 1.04-1.09 alternative analyses on an average, i.e. some of the ambiguities remait unresolved. We evaluate the resulting
disambiguated text by a number of metrics defined as follows (Voutilainen, 1995a).
Table 5: Sample Output
5.3 Extraction Evaluation
To evaluate the extraction quality, we use dataset2
(that has never been used for training or tuning any
of the system components). We use our system to
generate summaries for each of the 30 papers in
dataset2. We also generate summaries for the pa-
pers using a number of baseline systems (described
in Section 5.3.1). All the generated summaries were
5 sentences long. We use the Recall-Oriented Un-
derstudy for Gisting Evaluation (ROUGE) based on
the longest common substrings (ROUGE-L) as our
evaluation metric.
5.3.1 Baselines
We evaluate the extraction quality of our system
(FL) against 7 different baselines. In the first base-
line, the sentences are selected randomly from the
set of citation sentences and added to the sum-
mary. The second baseline is the MEAD summa-
rizer (Radev et al, 2004) with all its settings set
to default. The third baseline is LexRank (Erkan
and Radev, 2004) run on the entire set of citation
sentences of the target paper. The forth baseline is
Qazvinian and Radev (2008) citation-based summa-
rizer (QR08) in which the citation sentences are first
clustered then the sentences within each cluster are
ranked using LexRank. The remaining baselines are
variations of our system produced by removing one
component from the pipeline at a time. In one vari-
ation (FL-1), we remove the sentence filtering com-
ponent. In another variation (FL-2), we remove the
sentence classification component; so, all the sen-
tences are assumed to come from one category in the
subsequent components. In a third variation (FL-3),
the clustering component is removed. To make the
comparison of the extraction quality to those base-
lines fair, we remove the author name replacement
component from our system and all its variations.
5.3.2 Results
Table 6 shows the average ROUGE-L scores (with
95% confidence interval) for the summaries of the
30 papers in dataset2 generated using our system
and the different baselines. The two human sum-
maries were used as models for comparison. The
Human score reported in the table is the result of
comparing the two human summaries to each others.
Statistical significance was tested using a 2-tailed
paired t-test. The results are statistically significant
at the 0.05 level.
The results show that our approach outperforms
all the baseline techniques. It achieves higher
ROUGE-L score for most of the papers in our test-
ing set. Comparing the score of FL-1 to the score
of FL shows that sentence filtering has a significant
impact on the results. It also shows that the classifi-
cation and clustering components both improve the
extraction quality.
5.4 Coherence and Readability Evaluation
We asked human judges (not including the authors)
to rate the coherence and readability of a number
of summaries for each of dataset2 papers. For
each paper we evaluated 3 summaries. The sum-
507
- Human Random MEAD LexRank QR08
ROUGE-L 0.733 0.398 0.410 0.408 0.435
- FL-1 FL-2 FL-3 FL -
ROUGE-L 0.475 0.511 0.525 0.539 -
Table 6: Extraction Evaluation
Average Coherence Rating
Number of summaries
Human FL QV08
1? coherence <2 0 9 17
2? coherence <3 3 11 12
3? coherence <4 16 9 1
4? coherence ?5 11 1 0
Table 7: Coherence Evaluation
mary that our system produced, the human sum-
mary, and a summary produced by Qazvinian and
Radev (2008) summarizer (the best baseline - after
our system and its variations - in terms of extrac-
tion quality as shown in the previous subsection.)
The summaries were randomized and given to the
judges without telling them how each summary was
produced. The judges were not given access to the
source text. They were asked to use a five point-
scale to rate how coherent and readable the sum-
maries are, where 1 means that the summary is to-
tally incoherent and needs significant modifications
to improve its readability, and 5 means that the sum-
mary is coherent and no modifications are needed to
improve its readability. We gave each summary to 5
different judges and took the average of their ratings
for each summary. We used Weighted Kappa with
linear weights (Cohen, 1968) to measure the inter-
rater agreement. The Weighted Kappa measure be-
tween the five groups of ratings was 0.72.
Table 7 shows the number of summaries in each
rating range. The results show that our approach sig-
nificantly improves the coherence of citation-based
summarization. Table 5 shows two sample sum-
maries (each 5 sentences long) for the Voutilainen
(1995) paper. One summary was produced using our
system and the other was produced using Qazvinian
and Radev (2008) system.
6 Conclusions
In this paper, we presented a new approach for
citation-based summarization of scientific papers
that produces readable summaries. Our approach in-
volves three stages. The first stage preprocesses the
set of citation sentences to filter out the irrelevant
sentences or fragments of sentences. In the second
stage, a representative set of sentences are extracted
and added to the summary in a reasonable order. In
the last stage, the summary sentences are refined to
improve their readability. The results of our exper-
iments confirmed that our system outperforms sev-
eral baseline systems.
Acknowledgments
This work is in part supported by the National
Science Foundation grant ?iOPENER: A Flexible
Framework to Support Rapid Learning in Unfamil-
iar Research Domains?, jointly awarded to Univer-
sity of Michigan and University of Maryland as
IIS 0705832, and in part by the NIH Grant U54
DA021519 to the National Center for Integrative
Biomedical Informatics.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this paper are those of
the authors and do not necessarily reflect the views
of the supporters.
References
Aaron Clauset, M. E. J. Newman, and Cristopher Moore.
2004. Finding community structure in very large net-
works. Phys. Rev. E, 70(6):066111, Dec.
Jacob Cohen. 1968. Weighted kappa: Nominal scale
agreement provision for scaled disagreement or partial
credit. Psychological Bulletin, 70(4):213 ? 220.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
508
Klaus H. Krippendorff. 2003. Content Analysis: An In-
troduction to Its Methodology. Sage Publications, Inc,
2nd edition, December.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159?174, March.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
M. E. J. Newman. 2001. The structure of scientific
collaboration networks. Proceedings of the National
Academy of Sciences of the United States of America,
98(2):404?409, January.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Daniel D. K. Sleator and Davy Temperley. 1991. Parsing
english with a link grammar. In In Third International
Workshop on Parsing Technologies.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
509
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098?1108,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning From Collective Human Behavior to
Introduce Diversity in Lexical Choice
Vahed Qazvinian
Department of EECS
University of Michigan
Ann Arbor, MI
vahed@umich.edu
Dragomir R. Radev
School of Information
Department of EECS
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We analyze collective discourse, a collective
human behavior in content generation, and
show that it exhibits diversity, a property of
general collective systems. Using extensive
analysis, we propose a novel paradigm for de-
signing summary generation systems that re-
flect the diversity of perspectives seen in real-
life collective summarization. We analyze 50
sets of summaries written by human about the
same story or artifact and investigate the diver-
sity of perspectives across these summaries.
We show how different summaries use vari-
ous phrasal information units (i.e., nuggets) to
express the same atomic semantic units, called
factoids. Finally, we present a ranker that em-
ploys distributional similarities to build a net-
work of words, and captures the diversity of
perspectives by detecting communities in this
network. Our experiments show how our sys-
tem outperforms a wide range of other docu-
ment ranking systems that leverage diversity.
1 Introduction
In sociology, the term collective behavior is used to
denote mass activities that are not centrally coordi-
nated (Blumer, 1951). Collective behavior is dif-
ferent from group behavior in the following ways:
(a) it involves limited social interaction, (b) mem-
bership is fluid, and (c) it generates weak and un-
conventional norms (Smelser, 1963). In this paper,
we focus on the computational analysis of collective
discourse, a collective behavior seen in interactive
content contribution and text summarization in on-
line social media. In collective discourse each in-
dividual?s behavior is largely independent of that of
other individuals.
In social media, discourse (Grosz and Sidner,
1986) is often a collective reaction to an event. One
scenario leading to collective reaction to a well-
defined subject is when an event occurs (a movie is
released, a story occurs, a paper is published) and
people independently write about it (movie reviews,
news headlines, citation sentences). This process of
content generation happens over time, and each per-
son chooses the aspects to cover. Each event has
an onset and a time of death after which nothing is
written about it. Tracing the generation of content
over many instances will reveal temporal patterns
that will allow us to make sense of the text gener-
ated around a particular event.
To understand collective discourse, we are inter-
ested in behavior that happens over a short period
of time. We focus on topics that are relatively well-
defined in scope such as a particular event or a single
news event that does not evolve over time. This can
eventually be extended to events and issues that are
evolving either in time or scope such as elections,
wars, or the economy.
In social sciences and the study of complex sys-
tems a lot of work has been done to study such col-
lective systems, and their properties such as self-
organization (Page, 2007) and diversity (Hong and
Page, 2009; Fisher, 2009). However, there is little
work that studies a collective system in which mem-
bers individually write summaries.
In most of this paper, we will be concerned with
developing a complex systems view of the set of col-
lectively written summaries, and give evidence of
1098
the diversity of perspectives and its cause. We be-
lieve that out experiments will give insight into new
models of text generation, which is aimed at model-
ing the process of producing natural language texts,
and is best characterized as the process of mak-
ing choices between alternate linguistic realizations,
also known as lexical choice (Elhadad, 1995; Barzi-
lay and Lee, 2002; Stede, 1995).
2 Prior Work
In summarization, a number of previous methods
have focused on diversity. (Mei et al, 2010) in-
troduce a diversity-focused ranking methodology
based on reinforced random walks in information
networks. Their random walk model introduces the
rich-gets-richer mechanism to PageRank with rein-
forcements on transition probabilities between ver-
tices. A similar ranking model is the Grasshopper
ranking model (Zhu et al, 2007), which leverages
an absorbing random walk. This model starts with
a regular time-homogeneous random walk, and in
each step the node with the highest weight is set
as an absorbing state. The multi-view point sum-
marization of opinionated text is discussed in (Paul
et al, 2010). Paul et al introduce Compar-
ative LexRank, based on the LexRank ranking
model (Erkan and Radev, 2004). Their random walk
formulation is to score sentences and pairs of sen-
tences from opposite viewpoints (clusters) based on
both their representativeness of the collection as well
as their contrastiveness with each other. Once a lex-
ical similarity graph is built, they modify the graph
based on cluster information and perform LexRank
on the modified cosine similarity graph.
The most well-known paper that address diver-
sity in summarization is (Carbonell and Goldstein,
1998), which introduces Maximal Marginal Rele-
vance (MMR). This method is based on a greedy
algorithm that picks sentences in each step that are
the least similar to the summary so far. There are
a few other diversity-focused summarization sys-
tems like C-LexRank (Qazvinian and Radev, 2008),
which employs document clustering. These papers
try to increase diversity in summarizing documents,
but do not explain the type of the diversity in their in-
puts. In this paper, we give an insightful discussion
on the nature of the diversity seen in collective dis-
course, and will explain why some of the mentioned
methods may not work under such environments.
In prior work on evaluating independent contri-
butions in content generation, Voorhees (Voorhees,
1998) studied IR systems and showed that rele-
vance judgments differ significantly between hu-
mans but relative rankings show high degrees of sta-
bility across annotators. However, perhaps the clos-
est work to this paper is (van Halteren and Teufel,
2004) in which 40 Dutch students and 10 NLP re-
searchers were asked to summarize a BBC news re-
port, resulting in 50 different summaries. Teufel
and van Halteren also used 6 DUC1-provided sum-
maries, and annotations from 10 student participants
and 4 additional researchers, to create 20 summaries
for another news article in the DUC datasets. They
calculated the Kappa statistic (Carletta, 1996; Krip-
pendorff, 1980) and observed high agreement, indi-
cating that the task of atomic semantic unit (factoid)
extraction can be robustly performed in naturally oc-
curring text, without any copy-editing.
The diversity of perspectives and the unprece-
dented growth of the factoid inventory also affects
evaluation in text summarization. Evaluation meth-
ods are either extrinsic, in which the summaries are
evaluated based on their quality in performing a spe-
cific task (Spa?rck-Jones, 1999) or intrinsic where the
quality of the summary itself is evaluated, regardless
of any applied task (van Halteren and Teufel, 2003;
Nenkova and Passonneau, 2004). These evaluation
methods assess the information content in the sum-
maries that are generated automatically.
Finally, recent research on analyzing online so-
cial media shown a growing interest in mining news
stories and headlines because of its broad appli-
cations ranging from ?meme? tracking and spike
detection (Leskovec et al, 2009) to text summa-
rization (Barzilay and McKeown, 2005). In sim-
ilar work on blogs, it is shown that detecting top-
ics (Kumar et al, 2003; Adar et al, 2007) and sen-
timent (Pang and Lee, 2004) in the blogosphere can
help identify influential bloggers (Adar et al, 2004;
Java et al, 2006) and mine opinions about prod-
ucts (Mishne and Glance, 2006).
1Document Understanding Conference
1099
3 Data Annotation
The datasets used in our experiments represent two
completely different categories: news headlines, and
scientific citation sentences. The headlines datasets
consist of 25 clusters of news headlines collected
from Google News2, and the citations datasets have
25 clusters of citations to specific scientific papers
from the ACL Anthology Network (AAN)3. Each
cluster consists of a number of unique summaries
(headlines or citations) about the same artifact (non-
evolving news story or scientific paper) written by
different people. Table 1 lists some of the clusters
with the number of summaries in them.
ID type Name Story/Title #
1 hdl miss Miss Venezuela wins miss universe?09 125
2 hdl typhoon Second typhoon hit philippines 100
3 hdl russian Accident at Russian hydro-plant 101
4 hdl redsox Boston Red Sox win world series 99
5 hdl gervais ?Invention of Lying? movie reviewed 97
? ? ? ? ? ? ? ? ?
25 hdl yale Yale lab tech in court 10
26 cit N03-1017 Statistical Phrase-Based Translation 172
27 cit P02-1006 Learning Surface Text Patterns ... 72
28 cit P05-1012 On-line Large-Margin Training ... 71
29 cit C96-1058 Three New Probabilistic Models ... 66
30 cit P05-1033 A Hierarchical Phrase-Based Model ... 65
? ? ? ? ? ? ? ? ?
50 cit H05-1047 A Semantic Approach to Recognizing ... 7
Table 1: Some of the annotated datasets and the number
of summaries in each of them (hdl = headlines; cit = cita-
tions)
3.1 Nuggets vs. Factoids
We define an annotation task that requires explicit
definitions that distinguish between phrases that rep-
resent the same or different information units. Un-
fortunately, there is little consensus in the literature
on such definitions. Therefore, we follow (van Hal-
teren and Teufel, 2003) and make the following dis-
tinction. We define a nugget to be a phrasal infor-
mation unit. Different nuggets may all represent
the same atomic semantic unit, which we call as a
factoid. In the following headlines, which are ran-
domly extracted from the redsox dataset, nuggets
are manually underlined.
red sox win 2007 world series
boston red sox blank rockies to clinch world series
2news.google.com
3http://clair.si.umich.edu/clair/anthology/
boston fans celebrate world series win; 37 arrests re-
ported
These 3 headlines contain 9 nuggets, which rep-
resent 5 factoids or classes of equivalent nuggets.
f1 : {red sox, boston, boston red sox}
f2 : {2007 world series, world series win, world series}
f3 : {rockies}
f4 : {37 arrests}
f5 : {fans celebrate}
This example suggests that different headlines on
the same story written independently of one an-
other use different phrases (nuggets) to refer to the
same semantic unit (e.g., ?red sox? vs. ?boston? vs.
?boston red sox?) or to semantic units corresponding
to different aspects of the story (e.g., ?37 arrests? vs.
?rockies?). In the former case different nuggets are
used to represent the same factoid, while in the latter
case different nuggets are used to express different
factoids. This analogy is similar to the definition of
factoids in (van Halteren and Teufel, 2004).
The following citation sentences to Koehn?s work
suggest that a similar phenomenon also happens in
citations.
We also compared our model with pharaoh (Koehn et al
2003).
Koehn et al(2003) find that
phrases longer than three words improve per-
formance little.
Koehn et al(2003) suggest limiting phrase length
to three words or less.
For further information on these parameter settings,
confer (koehn et al 2003).
where the first author mentions ?pharaoh? as a
contribution of Koehn et al but the second and third
use different nuggets to represent the same contribu-
tion: use of trigrams. However, as the last citation
shows, a citation sentence, unlike news headlines,
may cover no information about the target paper.
The use of phrasal information as nuggets is an es-
sential element to our experiments, since some head-
line writers often try to use uncommon terms to re-
fer to a factoid. For instance, two headlines from the
redsox cluster are:
Short wait for bossox this time
Soxcess started upstairs
1100
Following these examples, we asked two anno-
tators to annotate all 1, 390 headlines, and 926 ci-
tations. The annotators were asked to follow pre-
cise guidelines in nugget extraction. Our guidelines
instructed annotators to extract non-overlapping
phrases from each headline as nuggets. Therefore,
each nugget should be a substring of the headline
that represents a semantic unit4.
Previously (Lin and Hovy, 2002) had shown that
information overlap judgment is a difficult task for
human annotators. To avoid such a difficulty, we
enforced our annotators to extract non-overlapping
nuggets from a summary to make sure that they are
mutually independent and that information overlap
between them is minimized.
Finding agreement between annotated well-
defined nuggets is straightforward and can be cal-
culated in terms of Kappa. However, when nuggets
themselves are to be extracted by annotators, the
task becomes less obvious. To calculate the agree-
ment, we annotated 10 randomly selected head-
line clusters twice and designed a simple evalua-
tion scheme based on Kappa5. For each n-gram,
w, in a given headline, we look if w is part of any
nugget in either human annotations. If w occurs
in both or neither, then the two annotators agree
on it, and otherwise they do not. Based on this
agreement setup, we can formalize the ? statistic
as ? = Pr(a)?Pr(e)1?Pr(e) where Pr(a) is the relative ob-
served agreement among annotators, and Pr(e) is
the probability that annotators agree by chance if
each annotator is randomly assigning categories.
Table 2 shows the unigram, bigram, and trigram-
based average ? between the two human annotators
(Human1, Human2). These results suggest that
human annotators can reach substantial agreement
when bigram and trigram nuggets are examined, and
has reasonable agreement for unigram nuggets.
4 Diversity
We study the diversity of ways with which human
summarizers talk about the same story or event and
explain why such a diversity exists.
4Before the annotations, we lower-cased all summaries and
removed duplicates
5Previously (Qazvinian and Radev, 2010) have shown high
agreement in human judgments in a similar task on citation an-
notation
Average ?
unigram bigram trigram
Human1 vs. Human2
0.76? 0.4 0.80? 0.4 0.89? 0.3
Table 2: Agreement between different annotators in terms
of average Kappa in 25 headline clusters.
100 101 102
10?2
10?1
100
Pr(X ?
 
c)
c
headlines
 
 Pr(X ? c)
100 101 102
10?2
10?1
100
Pr(X ?
 
c)
c
citations
 
 Pr(X ? c)
Figure 1: The cumulative probability distribution for the
frequency of factoids (i.e., the probability that a factoid
will be mentioned in c different summaries) across in
each category.
4.1 Skewed Distributions
Our first experiment is to analyze the popularity of
different factoids. For each factoid in the annotated
clusters, we extract its count, X , which is equal to
the number of summaries it has been mentioned in,
and then we look at the distribution of X . Fig-
ure 1 shows the cumulative probability distribution
for these counts (i.e., the probability that a factoid
will be mentioned in at least c different summaries)
in both categories.
These highly skewed distributions indicate that a
large number of factoids (more than 28%) are only
mentioned once across different clusters (e.g., ?poor
pitching of colorado? in the redsox cluster), and
that a few factoids are mentioned in a large number
of headlines (likely using different nuggets). The
large number of factoids that are only mentioned in
one headline indicates that different summarizers in-
crease diversity by focusing on different aspects of
a story or a paper. The set of nuggets also exhibit
similar skewed distributions. If we look at individ-
ual nuggets, the redsox set shows that about 63
(or 80%) of the nuggets get mentioned in only one
headline, resulting in a right-skewed distribution.
The factoid analysis of the datasets reveals two
main causes for the content diversity seen in head-
lines: (1) writers focus on different aspects of the
story and therefore write about different factoids
1101
(e.g., ?celebrations? vs. ?poor pitching of col-
orado?). (2) writer use different nuggets to represent
the same factoid (e.g., ?redsox? vs. ?bosox?). In the
following sections we analyze the extent at which
each scenario happens.
100 101 102 1030
200
400
600
800
1000
number of summaries
Inve
ntor
y si
ze
headlines
 
 NuggetsFactoids
100 101 102 1030
50
100
150
200
250
300
350
number of summaries
Inve
ntor
y si
ze
citations
 
 NuggetsFactoids
Figure 2: The number of unique factoids and nuggets ob-
served by reading n random summaries in all the clusters
of each category
4.2 Factoid Inventory
The emergence of diversity in covering different fac-
toids suggests that looking at more summaries will
capture a larger number of factoids. In order to ana-
lyze the growth of the factoid inventory, we perform
a simple experiment. We shuffle the set of sum-
maries from all 25 clusters in each category, and then
look at the number of unique factoids and nuggets
seen after reading nth summary. This number shows
the amount of information that a randomly selected
subset of n writers represent. This is important to
study in order to find out whether we need a large
number of summaries to capture all aspects of a
story and build a complete factoid inventory. The
plot in Figure 4.1 shows, at each n, the number of
unique factoids and nuggets observed by reading n
random summaries from the 25 clusters in each cat-
egory. These curves are plotted on a semi-log scale
to emphasize the difference between the growth pat-
terns of the nugget inventories and the factoid inven-
tories6.
This finding numerically confirms a similar ob-
servation on human summary annotations discussed
in (van Halteren and Teufel, 2003; van Halteren
and Teufel, 2004). In their work, van Halteren and
Teufel indicated that more than 10-20 human sum-
maries are needed for a full factoid inventory. How-
ever, our experiments with nuggets of nearly 2, 400
independent human summaries suggest that neither
the nugget inventory nor the number of factoids will
be likely to show asymptotic behavior. However,
these plots show that the nugget inventory grows at
a much faster rate than factoids. This means that a
lot of the diversity seen in human summarization is
a result of the so called different lexical choices that
represent the same semantic units or factoids.
4.3 Summary Quality
In previous sections we gave evidence for the diver-
sity seen in human summaries. However, a more
important question to answer is whether these sum-
maries all cover important aspects of the story. Here,
we examine the quality of these summaries, study
the distribution of information coverage in them,
and investigate the number of summaries required
to build a complete factoid inventory.
The information covered in each summary can be
determined by the set of factoids (and not nuggets)
and their frequencies across the datasets. For exam-
ple, in the redsox dataset, ?red sox?, ?boston?, and
?boston red sox? are nuggets that all represent the
same piece of information: the red sox team. There-
fore, different summaries that use these nuggets to
refer to the red sox team should not be seen as very
different.
We use the Pyramid model (Nenkova and Pas-
sonneau, 2004) to value different summary factoids.
Intuitively, factoids that are mentioned more fre-
quently are more salient aspects of the story. There-
fore, our pyramid model uses the normalized fre-
quency at which a factoid is mentioned across a
dataset as its weight. In the pyramid model, the in-
dividual factoids fall in tiers. If a factoid appears in
more summaries, it falls in a higher tier. In princi-
ple, if the term wi appears |wi| times in the set of
6Similar experiment using individual clusters exhibit similar
behavior
1102
headlines it is assigned to the tier T|wi|. The pyra-
mid score that we use is computed as follows. Sup-
pose the pyramid has n tiers, Ti, where tier Tn is
the top tier and T1 is the bottom. The weight of
the factoids in tier Ti will be i (i.e. they appeared
in i summaries). If |Ti| denotes the number of fac-
toids in tier Ti, and Di is the number of factoids in
the summary that appear in Ti, then the total factoid
weight for the summary is D =
?n
i=1 i ? Di. Ad-
ditionally, the optimal pyramid score for a summary
is Max =
?n
i=1 i? |Ti|. Finally, the pyramid score
for a summary can be calculated as
P =
D
Max
Based on this scoring scheme, we can use the an-
notated datasets to determine the quality of individ-
ual headlines. First, for each set we look at the vari-
ation in pyramid scores that individual summaries
obtain in their set. Figure 3 shows, for each clus-
ter, the variation in the pyramid scores (25th to 75th
percentile range) of individual summaries evaluated
against the factoids of that cluster. This figure in-
dicates that the pyramid score of almost all sum-
maries obtain values with high variations in most of
the clusters For instance, individual headlines from
redsox obtain pyramid scores as low as 0.00 and
as high as 0.93. This high variation confirms the pre-
vious observations on diversity of information cov-
erage in different summaries.
Additionally, this figure shows that headlines gen-
erally obtain higher values than citations when con-
sidered as summaries. One reason, as explained be-
fore, is that a citation may not cover any important
contribution of the paper it is citing, when headlines
generally tend to cover some aspects of the story.
High variation in quality means that in order to
capture a larger information content we need to read
a greater number of summaries. But how many
headlines should one read to capture a desired level
of information content? To answer this question,
we perform an experiment based on drawing random
summaries from the pool of all the clusters in each
category. We perform a Monte Carlo simulation, in
which for each n, we draw n random summaries,
and look at the pyramid score achieved by reading
these headlines. The pyramid score is calculated us-
ing the factoids from all 25 clusters in each cate-
gory7. Each experiment is repeated 1, 000 times to
find the statistical significance of the experiment and
the variation from the average pyramid scores.
Figure 4.3 shows the average pyramid scores over
different n values in each category on a log-log
scale. This figure shows how pyramid score grows
and approaches 1.00 rapidly as more randomly se-
lected summaries are seen.
100 101 102 103
10?2
10?1
100
number of summaries
Py
ram
id S
cor
e
 
 
headlines
citations
Figure 4: Average pyramid score obtained by reading n
random summaries shows rapid asymptotic behavior.
5 Diversity-based Ranking
In previous sections we showed that the diversity
seen in human summaries could be according to dif-
ferent nuggets or phrases that represent the same fac-
toid. Ideally, a summarizer that seeks to increase di-
versity should capture this phenomenon and avoid
covering redundant nuggets. In this section, we use
different state of the art summarization systems to
rank the set of summaries in each cluster with re-
spect to information content and diversity. To evalu-
ate each system, we cut the ranked list at a constant
length (in terms of the number of words) and calcu-
late the pyramid score of the remaining text.
5.1 Distributional Similarity
We have designed a summary ranker that will pro-
duce a ranked list of documents with respect to the
diversity of their contents. Our model works based
on ranking individual words and using the ranked
list of words to rank documents that contain them.
In order to capture the nuggets of equivalent se-
mantic classes, we use a distributional similarity of
7Similar experiment using individual clusters exhibit similar
results
1103
00.2
0.4
0.6
0.8
1
abo
rtio
n
am
az
on
bab
ies
bur
ger
co
lom
bia
en
gla
nd
ger
vai
s
goo
gle
irel
and
ma
ine
me
rcu
ry
mis
s
mo
nke
y
mo
za
rt
no
bel prie
st
ps3
slim
ra
dia
tion
re
dso
x
ru
ss
ian
sc
ien
tist
so
upy
sw
ede
n
typ
hoo
n
yal
e
A0
0_1
023
A0
0_1
043
A0
0_2
024
C0
0_1
072
C9
6_1
058
D0
3_1
017
D0
4_9
907
H0
5_1
047
H0
5_1
079
J04
_40
02
N0
3_1
017
N0
4_1
033
P0
2_1
006
P0
3_1
001
P0
5_1
012
P0
5_1
013
P0
5_1
014
P0
5_1
033
P9
7_1
003
P9
9_1
065
W0
0_0
403
W0
0_0
603
W0
3_0
301
W0
3_0
510
W0
5_1
203
Py
ram
id S
cor
e
 
 
headlines
citations
Figure 3: The 25th to 75th percentile pyramid score range in individual clusters
words that is inspired by (Lee, 1999). We represent
each word by its context in the cluster and find the
similarity of such contexts. Particularly, each word
wi is represented by a bag of words, `i, that have a
surface distance of 3 or smaller to wi anywhere in
the cluster. In other words, `i contains any word that
co-occurs with wi in a 4-gram in the cluster. This
bag of words representation of words enables us to
find the word-pair similarities.
sim(wi, wj) =
~`
i ? ~`j
?
|~`i|| ~`j |
(1)
We use the pair-wise similarities of words in each
cluster, and build a network of words and their simi-
larities. Intuitively, words that appear in similar con-
texts are more similar to each other and will have a
stronger edge between them in the network. There-
fore, similar words, or words that appear in similar
contexts, will form communities in this graph. Ide-
ally, each community in the word similarity network
would represent a factoid. To find the communities
in the word network we use (Clauset et al, 2004), a
hierarchical agglomeration algorithm which works
by greedily optimizing the modularity in a linear
running time for sparse graphs.
The community detection algorithm will assign
to each word wi, a community label Ci. For each
community, we use LexRank to rank the words us-
ing the similarities in Equation 1, and assign a score
to each word wi as S(wi) =
Ri
|Ci|
, where Ri is the
rank of wi in its community, and |Ci| is the number
of words that belong to Ci. Figure 5.1 shows part
police
second
soxcelebrations red jumpbaseball
unhappy
sweeps
pitching
hittingarrest
victorytitle dynasty
fan poorer
2nd
poor
glory
Pajek
Figure 5: Part of the word similarity graph in the redsox
cluster
of the word similarity graph in the redsox cluster,
in which each node is color-coded with its commu-
nity. This figure illustrates how words that are se-
mantically related to the same aspects of the story
fall in the same communities (e.g., ?police? and ?ar-
rest?). Finally, to rank sentences, we define the score
of each document Dj as the sum of the scores of its
words.
pds(Dj) =
?
wi?Dj
S(wi)
Intuitively, sentences that contain higher ranked
words in highly populated communities will have a
smaller score. To rank the sentences, we sort them
in an ascending order, and cut the list when its size
is greater than the length limit.
5.2 Other Methods
5.2.1 Random
For each cluster in each category (citations and
headlines), this method simply gets a random per-
1104
mutations of the summaries. In the headlines
datasets, where most of the headlines cover some
factoids about the story, we expect this method to
perform reasonably well since randomization will
increase the chances of covering headlines that fo-
cus on different factoids. However, in the citations
dataset, where a citing sentence may cover no infor-
mation about the cited paper, randomization has the
drawback of selecting citations that have no valuable
information in them.
5.2.2 LexRank
LexRank (Erkan and Radev, 2004) works by first
building a graph of all the documents (Di) in a
cluster. The edges between corresponding nodes
(di) represent the cosine similarity between them is
above a threshold (0.10 following (Erkan and Radev,
2004)). Once the network is built, the system finds
the most central sentences by performing a random
walk on the graph.
p(dj) = (1? ?)
1
|D|
+ ?
?
di
p(di)P (di ? dj) (2)
5.2.3 MMR
Maximal Marginal Relevance (MMR) (Carbonell
and Goldstein, 1998) uses the pairwise cosine simi-
larity matrix and greedily chooses sentences that are
the least similar to those already in the summary. In
particular,
MMR = argminDi?D?A
[
maxDj?A Sim(Di, Dj)
]
where A is the set of documents in the summary,
initialized to A = ?.
5.2.4 DivRank
Unlike other time-homogeneous random walks
(e.g., PageRank), DivRank does not assume that
the transition probabilities remain constant over
time. DivRank uses a vertex-reinforced random
walk model to rank graph nodes based on a diversity
based centrality. The basic assumption in DivRank
is that the transition probability from a node to other
is reinforced by the number of previous visits to the
target node (Mei et al, 2010). Particularly, let?s as-
sume pT (u, v) is the transition probability from any
node u to node v at time T . Then,
pT (di, dj) = (1? ?).p
?(dj) + ?.
p0(di, dj).NT (dj)
DT (di)
(3)
whereNT (dj) is the number of times the walk has
visited dj up to time T and
DT (di) =
?
dj?V
p0(di, dj)NT (dj) (4)
Here, p?(dj) is the prior distribution that deter-
mines the preference of visiting vertex dj . We try
two variants of this algorithm: DivRank, in which
p?(dj) is uniform, and DivRank with priors in
which p?(dj) ? l(Dj)?? , where l(Dj) is the num-
ber of the words in the document Dj and ? is a pa-
rameter (? = 0.8).
5.2.5 C-LexRank
C-LexRank is a clustering-based model in which
the cosine similarities of document pairs are used to
build a network of documents. Then the the network
is split into communities, and the most salient doc-
uments in each community are selected (Qazvinian
and Radev, 2008). C-LexRank focuses on finding
communities of documents using their cosine simi-
larity. The intuition is that documents that are more
similar to each other contain similar factoids. We ex-
pect C-LexRank to be a strong ranker, but incapable
of capturing the diversity caused by using different
phrases to express the same meaning. The reason is
that different nuggets that represent the same factoid
often have no words in common (e.g., ?victory? and
?glory?) and won?t be captured by a lexical measure
like cosine similarity.
5.3 Experiments
We use each of the systems explained above to rank
the summaries in each cluster. Each ranked list is
then cut at a certain length (50 words for headlines,
and 150 for citations) and the information content
in the remaining text is examined using the pyramid
score.
Table 3 shows the average pyramid score achieved
by different methods in each category. The method
based on the distributional similarities of words out-
performs other methods in the citations category. All
methods show similar results in the headlines cate-
gory, where most headlines cover at least 1 factoid
about the story and a random ranker performs rea-
sonably well. Table 4 shows top 3 headlines from
3 rankers: word distributional similarity (WDS), C-
LexRank, and MMR. In this example, the first 3
1105
Method
headlines citations Mean
pyramid 95% C.I. pyramid 95% C.I.
R 0.928 [0.896, 0.959] 0.716 [0.625, 0.807] 0.822
MMR 0.930 [0.902, 0.960] 0.766 [0.684, 0.847] 0.848
LR 0.918 [0.891, 0.945] 0.728 [0.635, 0.822] 0.823
DR 0.927 [0.900, 0.955] 0.736 [0.667, 0.804] 0.832
DR(p) 0.916 [0.884, 0.949] 0.764 [0.697, 0.831] 0.840
C-LR 0.942 [0.919, 0.965] 0.781 [0.710, 0.852] 0.862
WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872
R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; C-
LR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence In-
terval
Table 3: Comparison of different ranking systems
Method Top 3 headlines
WDS
1: how sweep it is
2: fans celebrate red sox win
3: red sox take title
C-LR
1: world series: red sox sweep rockies
2: red sox take world series
3: red sox win world series
MMR
1:red sox scale the rockies
2: boston sweep colorado to win world series
3: rookies respond in first crack at the big time
C-LR=C-LexRank; WDS=Word Distributional Similarity
Table 4: Top 3 ranked summaries of the redsox cluster
using different methods
headlines produced by WDS cover two important
factoids: ?red sox winning the title? and ?fans cel-
ebrating?. However, the second factoid is absent in
the other two.
6 Conclusion and Future Work
Our experiments on two different categories of
human-written summaries (headlines and citations)
showed that a lot of the diversity seen in human
summarization comes from different nuggets that
may actually represent the same semantic informa-
tion (i.e., factoids). We showed that the factoids ex-
hibit a skewed distribution model, and that the size
of the nugget inventory asymptotic behavior even
with a large number of summaries. We also showed
high variation in summary quality across different
summaries in terms of pyramid score, and that the
information covered by reading n summaries has a
rapidly growing asymptotic behavior as n increases.
Finally, we proposed a ranking system that employs
word distributional similarities to identify semanti-
cally equivalent words, and compared it with a wide
range of summarization systems that leverage diver-
sity.
In the future, we plan to move to content from
other collective systems on Web. In order to gen-
eralize our findings, we plan to examine blog com-
ments, online reviews, and tweets (that discuss the
same URL). We also plan to build a generation sys-
tem that employs the Yule model (Yule, 1925) to de-
termine the importance of each aspect (e.g. who,
when, where, etc.) in order to produce summaries
that include diverse aspects of a story.
Our work has resulted in a publicly available
dataset 8 of 25 annotated news clusters with nearly
1, 400 headlines, and 25 clusters of citation sen-
tences with more than 900 citations. We believe that
this dataset can open new dimensions in studying di-
versity and other aspects of automatic text genera-
tion.
7 Acknowledgments
This work is supported by the National Science
Foundation grant number IIS-0705832 and grant
number IIS-0968489. Any opinions, findings, and
conclusions or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect the views of the supporters.
References
Eytan Adar, Li Zhang, Lada A. Adamic, and Rajan M.
Lukose. 2004. Implicit structure and the dynamics of
8http://www-personal.umich.edu/?vahed/
data.html
1106
Blogspace. In WWW?04, Workshop on the Weblogging
Ecosystem.
Eytan Adar, Daniel S. Weld, Brian N. Bershad, and
Steven S. Gribble. 2007. Why we search: visualiz-
ing and predicting user behavior. In WWW?07, pages
161?170, New York, NY, USA.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
lexical choice via multiple-sequence alignment. In
Proceedings of the ACL-02 conference on Empirical
methods in natural language processing - Volume 10,
EMNLP ?02, pages 164?171.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Comput. Linguist., 31(3):297?328.
Herbert Blumer. 1951. Collective behavior. In Lee, Al-
fred McClung, Ed., Principles of Sociology.
Jaime G. Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In SIGIR?98, pages
335?336.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70(6).
Michael Elhadad. 1995. Using argumentation in text
generation. Journal of Pragmatics, 24:189?220.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Len Fisher. 2009. The Perfect Swarm: The Science of
Complexity in Everyday Life. Basic Books.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12:175?204, July.
Lu Hong and Scott Page. 2009. Interpreted and
generated signals. Journal of Economic Theory,
144(5):2174?2196.
Akshay Java, Pranam Kolari, Tim Finin, and Tim Oates.
2006. Modeling the spread of influence on the blogo-
sphere. In WWW?06.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan, and
Andrew Tomkins. 2003. On the bursty evolution of
blogspace. In WWW?03, pages 568?576, New York,
NY, USA.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th annual meeting of the
Association for Computational Linguistics on Compu-
tational Linguistics, pages 25?32.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ?09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497?506.
Chin-Yew Lin and Eduard Hovy. 2002. Manual and au-
tomatic evaluation of summaries. In ACL-Workshop
on Automatic Summarization.
Qiaozhu Mei, Jian Guo, and Dragomir Radev. 2010. Di-
vrank: the interplay of prestige and diversity in infor-
mation networks. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 1009?1018.
Gilad Mishne and Natalie Glance. 2006. Predicting
movie sales from blogger sentiment. In AAAI 2006
Spring Symposium on Computational Approaches to
Analysing Weblogs (AAAI-CAAW 2006).
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Scott E. Page. 2007. The Difference: How the Power of
Diversity Creates Better Groups, Firms, Schools, and
Societies. Princeton University Press.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL?04, Morristown,
NJ, USA.
Michael Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, pages 66?76.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Neil J. Smelser. 1963. Theory of Collective Behavior.
Free Press.
Karen Spa?rck-Jones. 1999. Automatic summarizing:
factors and directions. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in automatic text summa-
rization, chapter 1, pages 1 ? 12. The MIT Press.
Manfred Stede. 1995. Lexicalization in natural language
generation: a survey. Artificial Intelligence Review,
(8):309?336.
Hans van Halteren and Simone Teufel. 2003. Examin-
ing the consensus between human summaries: initial
experiments with factoid analysis. In Proceedings of
1107
the HLT-NAACL 03 on Text summarization workshop,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
Hans van Halteren and Simone Teufel. 2004. Evaluating
information content by factoid analysis: human anno-
tation and stability. In EMNLP?04, Barcelona.
Ellen M. Voorhees. 1998. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
In SIGIR ?98: Proceedings of the 21st annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 315?323.
G. Udny Yule. 1925. A mathematical theory of evo-
lution, based on the conclusions of dr. j. c. willis,
f.r.s. Philosophical Transactions of the Royal Society
of London. Series B, Containing Papers of a Biological
Character, 213:21?87.
Xiaojin Zhu, Andrew Goldberg, Jurgen Van Gael, and
David Andrzejewski. 2007. Improving diversity in
ranking using absorbing random walks. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 97?104.
1108
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 592?597,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Identifying the Semantic Orientation of Foreign Words
Ahmed Hassan
EECS Department
University of Michigan
Ann Arbor, MI
hassanam@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI
amjbara@umich.edu
Rahul Jha
EECS Department
University of Michigan
Ann Arbor, MI
rahuljha@umich.edu
Dragomir Radev
EECS Department and School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present a method for identifying the pos-
itive or negative semantic orientation of for-
eign words. Identifying the semantic orienta-
tion of words has numerous applications in the
areas of text classification, analysis of prod-
uct review, analysis of responses to surveys,
and mining online discussions. Identifying
the semantic orientation of English words has
been extensively studied in literature. Most of
this work assumes the existence of resources
(e.g. Wordnet, seeds, etc) that do not exist
in foreign languages. In this work, we de-
scribe a method based on constructing a mul-
tilingual network connecting English and for-
eign words. We use this network to iden-
tify the semantic orientation of foreign words
based on connection between words in the
same language as well as multilingual connec-
tions. The method is experimentally tested us-
ing a manually labeled set of positive and neg-
ative words and has shown very promising re-
sults.
1 Introduction
A great body of research work has focused on iden-
tifying the semantic orientation of words. Word po-
larity is a very important feature that has been used
in several applications. For example, the problem
of mining product reputation from Web reviews has
been extensively studied (Turney, 2002; Morinaga
et al, 2002; Nasukawa and Yi, 2003; Popescu and
Etzioni, 2005; Banea et al, 2008). This is a very
important task given the huge amount of product re-
views written on the Web and the difficulty of man-
ually handling them. Another interesting applica-
tion is mining attitude in discussions (Hassan et al,
2010), where the attitude of participants in a discus-
sion is inferred using the text they exchange.
Due to its importance, several researchers have
addressed the problem of identifying the semantic
orientation of individual words. This work has al-
most exclusively focused on English. Most of this
work used several language dependent resources.
For example Turney and Littman (2003) use the en-
tire English Web corpus by submitting queries con-
sisting of the given word and a set of seeds to a
search engine. In addition, several other methods
have used Wordnet (Miller, 1995) for connecting se-
mantically related words (Kamps et al, 2004; Taka-
mura et al, 2005; Hassan and Radev, 2010).
When we try to apply those methods to other lan-
guages, we run into the problem of the lack of re-
sources in other languages when compared to En-
glish. For example, the General Inquirer lexicon
(Stone et al, 1966) has thousands of English words
labeled with semantic orientation. Most of the lit-
erature has used it as a source of labeled seeds or
for evaluation. Such lexicons are not readily avail-
able in other languages. Another source that has
been widely used for this task is Wordnet (Miller,
1995). Even though other Wordnets have been built
for other languages, their coverage is very limited
when compared to the English Wordnet.
In this work, we present a method for predicting
the semantic orientation of foreign words. The pro-
592
Figure 1: Sparse Foreign Networks are connected to
Dense English Networks. Dashed nodes represent la-
beled positive and negative seeds.
posed method is based on creating a multilingual
network of words that represents both English and
foreign words. The network has English-English
connections, as well as foreign-foreign connections
and English-foreign connections. This allows us to
benefit from the richness of the resources built for
the English language and in the meantime utilize
resources specific to foreign languages. Figure 1
shows a multilingual network where a sparse foreign
network and a dense English network are connected.
We then define a random walk model over the multi-
lingual network and predict the semantic orientation
of any given word by comparing the mean hitting
time of a random walk starting from it to a positive
and a negative set of seed English words.
We use both Arabic and Hindi for experiments.
We compare the performance of several methods us-
ing the foreign language resources only and the mul-
tilingual network that has both English and foreign
words. We show that bootstrapping from languages
with dense resources such as English is useful for
improving the performance on other languages with
limited resources.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work.
We define our problem and explain our approach in
Section 3. Results and discussion are presented in
Section 4. We conclude in Section 5.
2 Related Work
The problem of identifying the polarity of individual
words is a well-studied problem that attracted sev-
eral research efforts in the past few years. In this
section, we survey several methods that addressed
this problem.
The work of Hatzivassiloglou and McKeown
(1997) is among the earliest efforts that addressed
this problem. They proposed a method for identify-
ing the polarity of adjectives. Their method is based
on extracting all conjunctions of adjectives from a
given corpus and then they classify each conjunc-
tive expression as either the same orientation such
as ?simple and well-received? or different orienta-
tion such as ?simplistic but well-received?. Words
are clustered into two sets and the cluster with the
higher average word frequency is classified as posi-
tive.
Turney and Littman (2003) identify word polar-
ity by looking at its statistical association with a set
of positive/negative seed words. They use two sta-
tistical measures for estimating association: Point-
wise Mutual Information (PMI) and Latent Seman-
tic Analysis (LSA). Co-occurrence statistics are col-
lected by submitting queries to a search engine. The
number of hits for positive seeds, negative seeds,
positives seeds near the given word, and negative
seeds near the given word are used to estimate the
association of the given word to the positive/negative
seeds.
Wordnet (Miller, 1995), thesaurus and co-
occurrence statistics have been widely used to mea-
sure word relatedness by several semantic orienta-
tion prediction methods. Kamps et al (2004) use the
length of the shortest-path in Wordnet connecting
any given word to positive/negative seeds to iden-
tify word polarity. Hu and Liu (2004) use Word-
net synonyms and antonyms to bootstrap from words
with known polarity to words with unknown polar-
ity. They assign any given word the label of its syn-
onyms or the opposite label of its antonyms if any of
them are known.
Kanayama and Nasukawa (2006) used syntactic
features and context coherency, defined as the ten-
dency for same polarities to appear successively,
to acquire polar atoms. Takamura et al (2005)
proposed using spin models for extracting seman-
tic orientation of words. They construct a network
of words using gloss definitions, thesaurus and co-
occurrence statistics. They regard each word as an
electron. Each electron has a spin and each spin has
a direction taking one of two values: up or down.
593
Two neighboring spins tend to have the same orien-
tation from an energetic point of view. Their hypoth-
esis is that as neighboring electrons tend to have the
same spin direction, neighboring words tend to have
similar polarity. Hassan and Radev (2010) use a ran-
dom walk model defined over a word relatedness
graph to classify words as either positive or negative.
Words are connected based on Wordnet relations as
well as co-occurrence statistics. They measure the
random walk mean hitting time of the given word to
the positive set and the negative set. They show that
their method outperforms other related methods and
that it is more immune to noisy word connections.
Identifying the semantic orientation of individ-
ual words is closely related to subjectivity analy-
sis. Subjectivity analysis focused on identifying
text that presents opinion as opposed to objective
text that presents factual information (Wiebe, 2000).
Some approaches to subjectivity analysis disregard
the context phrases and words appear in (Wiebe,
2000; Hatzivassiloglou and Wiebe, 2000; Banea
et al, 2008), while others take it into considera-
tion (Riloff and Wiebe, 2003; Yu and Hatzivas-
siloglou, 2003; Nasukawa and Yi, 2003; Popescu
and Etzioni, 2005).
3 Approach
The general goal of this work is to mine the seman-
tic orientation of foreign words. We do this by cre-
ating a multilingual network of words. In this net-
work two words are connected if we believe that they
are semantically related. The network has English-
English, English-Foreign and Foreign-Foreign con-
nections. Some of the English words will be used as
seeds for which we know the semantic orientation.
Given such a network, we will measure the mean
hitting time in a random walk starting at any given
word to the positive set of seeds and the negative set
of seeds. Positive words will be more likely to hit the
positive set faster than hitting the negative set and
vice versa. In the rest of this section, we define how
the multilingual word network is built and describe
an algorithm for predicting the semantic orientation
of any given word.
3.1 Multilingual Word Network
We build a network G(V,E) where V = Ven ? Vfr
is the union of a set of English and foreign words.
E is a set of edges connecting nodes in V . There
are three types of connections: English-English con-
nections, Foreign-Foreign connections and English-
Foreign connections.
For the English-English connections, we use
Wordnet (Miller, 1995). Wordnet is a large lexical
database of English. Words are grouped in synsets
to express distinct concepts. We add a link between
two words if they occur in the same Wordnet synset.
We also add a link between two words if they have a
hypernym or a similar-to relation.
Foreign-Foreign connections are created in a sim-
ilar way to the English connections. Some other lan-
guages have lexical resources based on the design of
the Princeton English Wordnet. For example: Euro
Wordnet (EWN) (Vossen, 1997), Arabic Wordnet
(AWN) (Elkateb, 2006; Black and Fellbaum, 2006;
Elkateb and Fellbaum, 2006) and the Hindi Word-
net (Narayan et al, 2002; S. Jha, 2001). We also use
co-occurrence statistics similar to the work of Hatzi-
vassiloglou and McKeown (1997).
Finally, to connect foreign words to English
words, we use a foreign to English dictionary. For
every word in a list of foreign words, we look up
its meaning in a dictionary and add an edge between
the foreign word and every other English word that
appeared as a possible meaning for it.
3.2 Semantic Orientation Prediction
We use the multilingual network we described above
to predict the semantic orientation of words based
on the mean hitting time to two sets of positive and
negative seeds. Given the graph G(V,E), we de-
scribed in the previous section, we define the transi-
tion probability from node i to node j by normaliz-
ing the weights of the edges out from i:
P (j|i) = Wij/
?
k
Wik (1)
The mean hitting time h(i|j) is the average num-
ber of steps a random walker, starting at i, will take
to enter state j for the first time (Norris, 1997). Let
the average number of steps that a random walker
starting at some node i will need to enter a state
594
k ? S be h(i|S). It can be formally defined as:
h(i|S) =
{
0 i ? S
?
j?V pij ? h(j|S) + 1 otherwise
(2)
where pij is the transition probability between
node i and node j.
Given two lists of seed English words with known
polarity, we define two sets of nodes S+ and S?
representing those seeds. For any given word w, we
calculate the mean hitting time between w and the
two seed sets h(w|S+) and h(w|S?). If h(w|S+)
is greater than h(w|S?), the word is classified as
negative, otherwise it is classified as positive. We
used the list of labeled seeds from (Hatzivassiloglou
and McKeown, 1997) and (Stone et al, 1966). Sev-
eral other similarity measures may be used to predict
whether a given word is closer to the positive seeds
list or the negative seeds list (e.g. average shortest
path length (Kamps et al, 2004)). However hit-
ting time has been shown to be more efficient and
more accurate (Hassan and Radev, 2010) because it
measures connectivity rather than distance. For ex-
ample, the length of the shortest path between the
words ?good? and ?bad? is only 5 (Kamps et al,
2004).
4 Experiments
4.1 Data
We used Wordnet (Miller, 1995) as a source of syn-
onyms and hypernyms for linking English words in
the word relatedness graph. We used two foreign
languages for our experiments Arabic and Hindi.
Both languages have a Wordnet that was constructed
based on the design the Princeton English Wordnet.
Arabic Wordnet (AWN) (Elkateb, 2006; Black and
Fellbaum, 2006; Elkateb and Fellbaum, 2006) has
17561 unique words and 7822 synsets. The Hindi
Wordnet (Narayan et al, 2002; S. Jha, 2001) has
56,928 unique words and 26,208 synsets.
In addition, we used three lexicons with words la-
beled as either positive or negative. For English, we
used the General Inquirer lexicon (Stone et al, 1966)
as a source of seed labeled words. The lexicon con-
tains 4206 words, 1915 of which are positive and
2291 are negative. For Arabic and Hindi we con-
structed a labeled set of 300 words for each language
0
10
20
30
40
50
60
70
80
90
100
Arabic Hindi
SO-PMI HT-FR HT-FR+EN
Figure 2: Accuracy of the proposed method and baselines
for both Arabic and Hindi.
for use in evaluation. Those sets were labeled by two
native speakers of each language. We also used an
Arabic-English and a Hindi-English dictionaries to
generate Foreign-English links.
4.2 Results and Discussion
We performed experiments on the data described in
the previous section. We compare our results to
two baselines. The first is the SO-PMI method de-
scribed in (Turney and Littman, 2003). This method
is based on finding the semantic association of any
given word to a set of positive and a set of negative
words. It can be calculated as follows:
SO-PMI(w) = log
hitsw,pos ? hitsneg
hitsw,neg ? hitspos
(3)
where w is a word with unknown polarity,
hitsw,pos is the number of hits returned by a com-
mercial search engine when the search query is the
given word and the disjunction of all positive seed
words. hitspos is the number of hits when we
search for the disjunction of all positive seed words.
hitsw,neg and hitsneg are defined similarly. We used
7 positive and 7 negative seeds as described in (Tur-
ney and Littman, 2003).
The second baseline constructs a network of for-
eign words only as described earlier. It uses mean
hitting time to find the semantic association of any
given word. We used 10 fold cross validation for this
experiment. We will refer to this system as HT-FR.
Finally, we build a multilingual network and use
the hitting time as before to predict semantic orien-
595
tation. We used the English words from (Stone et
al., 1966) as seeds and the labeled foreign words
for evaluation. We will refer to this system as
HT-FR + EN.
Figure 2 compares the accuracy of the three meth-
ods for Arabic and Hindi. We notice that the
SO-PMI and the hitting time based methods per-
form poorly on both Arabic and Hindi. This is
clearly evident when we consider that the accuracy
of the two systems on English was 83% and 93% re-
spectively (Turney and Littman, 2003; Hassan and
Radev, 2010). This supports our hypothesis that
state of the art methods, designed for English, per-
form poorly on foreign languages due to the limited
amount of resources available in foreign languages
compared to English. The figure also shows that the
proposed method, which combines resources from
both English and foreign languages, performs sig-
nificantly better. Finally, we studied how much im-
provement is achieved by including links between
foreign words from global Wordnets. We found out
that it improves the performance by 2.5% and 4%
for Arabic and Hindi respectively.
5 Conclusions
We addressed the problem of predicting the seman-
tic orientation of foreign words. All previous work
on this task has almost exclusively focused on En-
glish. Applying off-the-shelf methods developed for
English to other languages does not work well be-
cause of the limited amount of resources available
in foreign languages compared to English. We pro-
posed a method based on the construction of a multi-
lingual network that uses both language specific re-
sources as well as the rich semantic relations avail-
able in English. We then use a model that computes
the mean hitting time to a set of positive and neg-
ative seed words to predict whether a given word
has a positive or a negative semantic orientation.
We showed that the proposed method can predict
semantic orientation with high accuracy. We also
showed that it outperforms state of the art methods
limited to using language specific resources.
Acknowledgments
This research was funded in part by the Office
of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the ofcial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Elkateb S. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Black, W. and C. Fellbaum. 2006. Introducing the
arabic wordnet project. In Third International Word-
Net Conference.
Black. W. Rodriguez H Alkhalifa M. Vossen P. Pease A.
Elkateb, S. and C. Fellbaum. 2006. Building a word-
net for arabic. In Fifth International Conference on
Language Resources and Evaluation.
Black W. Vossen P. Farwell D. Rodrguez H. Pease A.
Alkhalifa M. Elkateb, S. 2006. Arabic wordnet and
the challenges of arabic. In Arabic NLP/MT Confer-
ence.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD?04, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In EMNLP?06, pages 355?363.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39?41.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi, and
Toshikazu Fukushima. 2002. Mining product reputa-
tions on the web. In KDD?02, pages 341?349.
596
Dipak Narayan, Debasri Chakrabarti, Prabhakar Pande,
and P. Bhattacharyya. 2002. An experience in build-
ing the indo wordnet - a wordnet for hindi. In First
International Conference on Global WordNet.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
P. Pande P. Bhattacharyya S. Jha, D. Narayan. 2001. A
wordnet for hindi. In International Workshop on Lexi-
cal Resources in Natural Language Processing.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL?02, pages 417?424.
P. Vossen. 1997. Eurowordnet: a multilingual database
for information retrieval. In DELOS workshop on
Cross-language Information Retrieval.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
597
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 121?126,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Clairlib: A Toolkit for Natural Language Processing, Information Retrieval,
and Network Analysis
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper we present Clairlib, an open-
source toolkit for Natural Language Process-
ing, Information Retrieval, and Network Anal-
ysis. Clairlib provides an integrated frame-
work intended to simplify a number of generic
tasks within and across those three areas. It
has a command-line interface, a graphical in-
terface, and a documented API. Clairlib is
compatible with all the common platforms and
operating systems. In addition to its own func-
tionality, it provides interfaces to external soft-
ware and corpora. Clairlib comes with a com-
prehensive documentation and a rich set of tu-
torials and visual demos.
1 Introduction
The development of software packages and code li-
braries that implement algorithms and perform tasks
in scientific areas is of great advantage for both re-
searchers and educators. The availability of these
tools saves the researchers a lot of the time and the
effort needed to implement the new approaches they
propose and conduct experiments to verify their hy-
potheses. Educators also find these tools useful in
class demonstrations and for setting up practical pro-
gramming assignments and projects for their stu-
dents.
A large number of systems have been developed
over the years to solve problems and perform tasks
in Natural Language Processing, Information Re-
trieval, or Network Analysis. Many of these sys-
tems perform specific tasks such as parsing, Graph
Partitioning, co-reference resolution, web crawling
etc. Some other systems are frameworks for per-
forming generic tasks in one area of focus such as
NLTK (Bird and Loper, 2004) and GATE (Cun-
ningham et al, 2002) for Natural Language Pro-
cessing; Pajek (Batagelj and Mrvar, 2003) and
GUESS (Adar, 2006) for Network Analysis and Vi-
sualization; and Lemur1 for Language Modeling and
Information Retrieval.
This paper presents Clairlib, an open-source
toolkit that contains a suit of modules for generic
tasks in Natural Language Processing (NLP), Infor-
mation Retrieval (IR), and Network Analysis (NA).
While many systems have been developed to address
tasks or subtasks in one of these areas as we have
just mentioned, Clairlib provides one integrated en-
vironment that addresses tasks in the three areas.
This makes it useful for a wide range of applications
within and across the three domains.
Clairlib is designed to meet the needs of re-
searchers and educators with varying purposes and
backgrounds. For this purpose, Clairlib provides
three different interfaces to its functionality: a
graphical interface, a command-line interface, and
an application programming interface (API).
Clairlib is developed and maintained by the Com-
putational Linguistics and Information Retrieval
(CLAIR) group at the University of Michigan. The
first version of Clairlib was released in the year
2007. It has been heavily developed since then until
it witnessed a qualitative leap by adding the Graphi-
cal Interface and many new features to the latest ver-
sion that we are presenting here.
Clairlib core modules are written in Perl. The
GUI was written in Java. The Perl back-end and the
Java front-end are efficiently tied together through a
communication module. Clairlib is compatible with
1http://www.lemurproject.org/
121
all the common platforms and operating systems.
The only requirements are a Perl interpreter and Java
Runtime Environment (JRE).
Clairlib has been used in several research projects
to implement systems and conduct experiments. It
also has been used in several academic courses.
The rest of this paper is organized as follows. In
Section 2, we describe the structure of Clairlib. In
Section 3, we present its functionality. Section 4
presents some usage examples. We conclude in Sec-
tion 5.
2 System Overview
Clairlib consists of three main components: the core
library, the command-line interface, and the graph-
ical user interface. The three components were de-
signed and connected together in a manner that aims
to achieve simplicity, integration, and ease of use. In
the following subsections, we briefly describe each
of the three components.
2.1 Modules
The core of Clairlib is a collection of more than 100
modules organized in a shallow hierarchy, each of
which performs a specific task or implements a cer-
tain algorithm. A set of core modules define the data
structures and perform the basic processing tasks.
For example, Clair::Document defines a data struc-
ture for holding textual data in various formats, and
performs the basic text processing tasks such as tok-
enization, stemming, tag stripping, etc.
Another set of modules perform more specific
tasks in the three areas of focus (NLP, IR, and NA).
For example, Clair::Bio::GIN::Interaction is de-
voted to protein-protein interaction extraction from
biomedical text.
A third set contains modules that interface Clair-
lib to external tools. For example, Clair::Utils::Parse
provides an interface to Charniak parser (Charniak,
2000), Stanford parser (Klein and Manning, 2003),
and Chunklink2.
Each module has a well-defined API. The API is
oriented to developers to help them write applica-
tions and build systems on top of Clairlib modules;
and to researchers to help them write applications
and setup custom experiments for their research.
2http://ilk.uvt.nl/team/sabine/chunklink/README.html
2.2 Command-line Interface
The command-line interface provides an easy access
to many of the tasks that Clairlib modules imple-
ment. It provides more than 50 different commands.
Each command is documented and demonstrated in
one or more tutorials. The function of each com-
mand can be customized by passing arguments with
the command. For example, the command
partition.pl -graph graph.net -method GirvanNewman -n 4
uses the GrivanNewman algorithm to divide a
given graph into 4 partitions.
2.3 Graphical User Interface
The graphical user interface (GUI) is an impor-
tant feature that has been recently added to Clairlib
and constituted a quantum leap in its development.
The main purpose of the GUI is to make the rich
set of Clairlib functionalities easier to access by a
larger number of users from various levels and back-
grounds especially students and users with limited or
no programming experience.
It is also intended to help students do their assign-
ments, projects, and research experiments in an in-
teractive environment. We believe that visual tools
facilitate understanding and make learning a more
enjoyable experience for many students. Focusing
on this purpose, the GUI is tuned for simplicity and
ease of use more than high computational efficiency.
Therefore, while it is suitable for small and medium
scale projects, it is not guaranteed to work efficiently
for large projects that involve large datasets and re-
quire heavy processing. The command-line inter-
face is a better choice for large projects.
The GUI consists of three components: the Net-
work Editor/Visualizer/Analyzer, the Text Proces-
sor, and the Corpus Processor. The Network com-
ponent allows the user to 1) build a new network
using a set of drawing and editing tools, 2) open
existing networks stored in files in several different
formats, 3) visualize a network and interact with it,
4) compute different statistics for a network such as
diameter, clustering coefficient, degree distribution,
etc., and 5) perform several operations on a network
such as random walk, label propagation, partition-
ing, etc. This component uses the open source li-
brary, JUNG3 to visualize networks. Figure 1 shows
3http://jung.sourceforge.net/
122
Figure 1: A screenshot for the network visualization component of Clairlib
a screenshot for the Network Visualizer.
The Text Processing component allows users to
process textual data published on the internet or im-
ported from a file stored on the disk. It can process
data in plain, html, or PDF format. Most of the text
processing capabilities implemented in Clairlib core
library are available through this component. Fig-
ure 2 shows a screenshot of the text processing com-
ponent.
The Corpus Processing component allows users
to build a corpus of textual data out of a collection
of files in plain, HTML, or PDF format; or by crawl-
ing a website. Several tasks could be performed on
a corpus such as indexing, querying, summarization,
information extraction, hyperlink network construc-
tion, etc.
Although these components can be run indepen-
dently, they are very integrated and designed to eas-
ily interact with each other. For example, a user can
crawl a website using the Corpus component, then
switch to the Text Processing component to extract
the text from the web documents and stem all the
words, then switch back to the Corpus component
to build a document similarity graph. The graph can
then be taken to the Network component to be visu-
alized and analyzed.
2.4 Documentation
Clairlib comes with an extensive documentation.
The documentation contains the installation infor-
mation for different platforms, a description of all
Clairlib components and modules, and a lot of usage
examples. In addition to this documentation, Clair-
lib provides three other resources:
API Reference
The API Reference provides a complete descrip-
tion of each module in the library. It describes each
subroutine, the task it performs, the arguments it
takes, the value it returns, etc. This reference is use-
ful for developers who want to use Clairlib modules
in their own applications and systems. The API Ref-
erence is published on the internet.
Tutorials
Tutorials teach users how to use Clairlib by ex-
amples. Each tutorial addresses a specific task and
provides a set of instructions to complete the task
using Clairlib command-line tools or its API.
Visual Demos
Visual demos target the users of the graphical in-
terface. The demos visually show how to start the
GUI and how to use its components to perform sev-
eral tasks.
123
Figure 2: A screenshot for the text processing component of Clairlib
3 Functionality
Clairlib provides modules and tools for a broad spec-
trum of tasks. Most of the functionalities are native
to Clairlib. Some functionalities, however, are im-
ported from other open-source packages or external
software. This section lists the main functionalities
categorized by their areas.
3.1 Natural Language Processing
NLP functionalities include Tokenization, Sen-
tence Segmentation, Stemming, HTML Tags Strip-
ping, Syntactic Parsing, Dependency Parsing,
Part-of-Speech Tagging, Document Classification,
LexRank, Summarization, Synthetic Corpus Gen-
eration, N-grams Extraction, XML Parsing, XML
Tree Building, Text Similarity, Political Text Analy-
sis, and Protein Name Tagging.
3.2 Information Retrieval
IR functionalities include Web Crawling, Indexing,
TF-IDF, PageRank, Phrase Based Retrieval, Fuzzy
OR Queries, Latent Semantic Indexing, Web Search,
Automatic Link Extraction, and Protein-Protein In-
teraction Extraction.
3.3 Network Analysis
Network Analysis functionalities include Network
Statistics, Random Network Generation, Network
Visualization, Network Partitioning, Community
Finding, Random Walks, Flow Networks, Signed
Networks, and Semi-supervised Graph-based Clas-
sification. Network Statistics include Centralities,
Clustering Coefficient, Shortest Paths, Diameter,
Triangles, Triplets, etc.
Some of these functionalities are implemented us-
ing several approaches. For example, Clairlib have
implementations for 5 graph partitioning algorithms.
This makes Clairlib a useful tool for conducting ex-
periments for comparative studies.
4 Uses of Clairlib
The diverse set of domains that Clairlib covers and
the different types of interfaces it provides make it
suitable for use in many contexts. In this section, we
highlight some of its uses.
Education
Clairlib contains visual tools that instructors can use
to do class demonstrations to help their students un-
derstand the basic concepts and the algorithms they
face during their study. For example, the random
walk simulator can be used to teach the students how
random walk works by showing a sample network
and then walk randomly step-by-step through it and
show the students how the probabilities change after
each step.
It can also be used to create assignments of vary-
ing levels of difficulty and different scopes. Instruc-
124
tors may ask their students to do experiments with a
dataset using Clairlib, write applications that use the
API, extend an existing module, or contribute new
modules to Clairlib. One example could be to ask
the students to a build a simple information retrieval
system that indexes a collection of documents and
executes search queries on it.
Clairlib has been used to create assignments and
projects in NLP and IR classes at the University of
Michigan and Columbia University. The experience
was positive for both the instructors and the stu-
dents. The instructors were able to design assign-
ments that cover several aspects of the course and
can be done in a reasonable amount of time. The stu-
dents used the API to accomplish their assignments
and projects. This helped them focus on the impor-
tant concepts rather than diving into fine program-
ming details.
Research
Clairlib contains implementations for many algo-
rithms and approaches that solve common problems.
It also comes with a number of corpora and anno-
tated datasets. This makes it a good resource for re-
searchers to build systems and conduct experiments.
Clairlib was successfully used in several research
projects. Examples include Political Text Analy-
sis (Hassan et al, 2008), Scientific Paper Summa-
rization (Qazvinian and Radev, 2009), Blog Net-
works Analysis (Hassan et al, 2009), Protein In-
teraction Extraction (Ozgur and Radev, 2009),
and Citation-Based Summarization (Abu-Jbara and
Radev, 2011).
4.1 Examples
In this subsection, we present some examples where
Clairlib has been used.
Example: Protein-Protein Interaction
Extraction
This is an example of a project that builds an
information extraction system and uses Clairlib as
its main processing component (Ozgur and Radev,
2009). This system is now part of a larger bioinfor-
matics project, NCIBI.
The system uses Clairlib to process a biomedical
article: 1) splits it into sentences using the segmen-
tation module, 2) parses each sentence using the in-
terface to the Stanford Dependency Parser, 3) tags
the protein names, 4) extracts protein-protein inter-
actions using a specific Clairlib module devoted to
this task, and then 5) it builds a protein interaction
network in which nodes are proteins and edges rep-
resent interaction relations. Figure 3 shows an ex-
ample protein interaction network extracted from the
abstracts of a collection of biomedical articles from
PubMed. This network is then analyzed to compute
node centralities and the basic network statistics.
Example: Scientific Paper Summarization Using
Citation Networks
This is an example of a research work that
used Clairlib to implement an approach and con-
duct experiments to support the research hypothe-
sis. Qazvinian and Radev (2009) used Clairlib to
implement their method for citation-based summa-
rization. Given a set of sentences that cite a paper,
they use Clairlib to 1) construct a cosine similarity
network out of these sentences, 2) find communities
of similar sentences using Clairlib community find-
ing module, 3) run Clairlib LexRank module to rank
the sentences, 4) extract the sentence with the high-
est rank from each community, and finally 5) return
the set of extracted sentences as a summary para-
graph.
Example: Text Classification
This is an example of a teaching assignment that
was used in an introductory course on information
retrieval at the University of Michigan. Students
were given the 20-newsgroups corpus (a large set
of news articles labeled by their topic and split into
training and testing sets) and were asked to use
Clairlib API to: 1) stem the text of the documents,
2) convert each document into a feature vector based
on word frequencies, 2) train a multi-class Percep-
tron or Naive Bayes classifier on the documents in
the training set, and finally 3) classify the documents
in the testing set using the trained classifier.
5 Conclusions
Clairlib is a broad-coverage toolkit for Natural Lan-
guage Processing, Information Retrieval, and Net-
work Analysis. It provides a simple, integrated, in-
teractive, and extensible framework for education
and research uses. It provides an API, a command-
125
Figure 3: Clairlib used to construct and analyze a protein network extracted from biomedical articles
line interface, and graphical user interface for the
convenience of users with varying purposes and
backgrounds. Clairlib is well-documented, easy to
learn, and simple to use. It has been tested for vari-
ous types of tasks in various environments.
Clairlib is an open source project and we welcome
all the contributions. Readers who are interested in
contributing to Clairlib are encouraged to contact the
authors.
Acknowledgements
We would like to thank Mark Hodges, Anthony
Fader, Mark Joseph, Joshua Gerrish, Mark Schaller,
Jonathan dePeri, Bryan Gibson, Chen Huang, Arzu-
can Ozgur, and Prem Ganeshkumar who contributed
to the development of Clairlib.
This work was supported in part by grants
R01-LM008106 and U54-DA021519 from the US
National Institutes of Health, U54 DA021519,
IDM 0329043, DHB 0527513, 0534323, and
0527513 from the National Science Foundation, and
W911NF-09-C-0141 from IARPA.
References
R. Gaizauskas, P. J. Rodgers and K. Humphreys 2001.
Visual Tools for Natural Language Processing. Jour-
nal of Visual Languages and Computing, Volume 12,
Issue 4, Pages 375-412.
Arzucan Ozgor and Dragomir Radev 2009. Supervised
classification for extracting biomedical events. Pro-
ceedings of the BioNLP?09 Workshop Shared Task on
Event Extraction at NAACL-HLT, Boulder, Colorado,
USA, pages 111-114
Ahmed Hassan, Dragomir R. Radev, Junghoo Cho, Am-
ruta Joshi. 2009. Content Based Recommendation
and Summarization in the Blogosphere. ICWSM-
2009.
Vahed Qazvinian, Dragomir Radev. 2008. Scientific
Paper Summarization Using Citation Summary Net-
works. COLING 2008.
Ahmed Hassan, Anthony Fader, Michael Crespin, Kevin
Quinn, Burt Monroe, Michael Colaresi and Dragomir
Radev. 2008. Tracking the Dynamic Evolution of Par-
ticipants Salience in a Discussion. COLING 2008.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. Proceedings of NAACL-2000.
Dan Klein and Christopher Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of ACL-2003.
Amjad Abu-Jbara and Dragomir Radev 2011. Coher-
ent Citation-based Summarization of Scientific Papers
Proceedings of ACL-2011.
H. Cunningham and D. Maynard and K. Bontcheva and
V. Tablan 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications Proceedings of ACL-2002, Philadelphia.
Steven Bird and Edward Loper. 2004. NLTK: The Natu-
ral Language Toolkit Proceedings of ACL-2004.
V. Batagelj and A. Mrvar 2003. Pajek - Analysis and
Visualization of Large Networks Springer, Berlin.
Eytan Adar. 2006. GUESS: A Language and Interface
for Graph Exploration CHI 2006.
126
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 399?409,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Subgroup Detection in Ideological Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Center for Computational Learning Systems
Columbia University
New York, NY, USA
mdiab@ccls.columbia.edu
Pradeep Dasigi
Department of Computer Science
Columbia University
New York, NY, USA
pd2359@columbia.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
The rapid and continuous growth of social
networking sites has led to the emergence of
many communities of communicating groups.
Many of these groups discuss ideological and
political topics. It is not uncommon that the
participants in such discussions split into two
or more subgroups. The members of each sub-
group share the same opinion toward the dis-
cussion topic and are more likely to agree with
members of the same subgroup and disagree
with members from opposing subgroups. In
this paper, we propose an unsupervised ap-
proach for automatically detecting discussant
subgroups in online communities. We analyze
the text exchanged between the participants of
a discussion to identify the attitude they carry
toward each other and towards the various as-
pects of the discussion topic. We use attitude
predictions to construct an attitude vector for
each discussant. We use clustering techniques
to cluster these vectors and, hence, determine
the subgroup membership of each participant.
We compare our methods to text clustering
and other baselines, and show that our method
achieves promising results.
1 Introduction
Online forums discussing ideological and political
topics are common1. When people discuss a dis-
puted topic they usually split into subgroups. The
members of each subgroup carry the same opinion
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
toward the discission topic. The member of a sub-
group is more likely to show positive attitude to the
members of the same subgroup, and negative atti-
tude to the members of opposing subgroups.
For example, let us consider the following two
snippets from a debate about the enforcement of a
new immigration law in Arizona state in the United
States:
(1) Discussant 1: Arizona immigration law is good.
Illegal immigration is bad.
(2) Discussant 2: I totally disagree with you. Ari-
zona immigration law is blatant racism, and quite
unconstitutional.
In (1), the writer is expressing positive attitude
regarding the immigration law and negative attitude
regarding illegal immigration. The writer of (2) is
expressing negative attitude towards the writer of
(1) and negative attitude regarding the immigration
law. It is clear from this short dialog that the writer
of (1) and the writer of (2) are members of two
opposing subgroups. Discussant 1 is supporting the
new law, while Discussant 2 is against it.
In this paper, we present an unsupervised ap-
proach for determining the subgroup membership of
each participant in a discussion. We use linguistic
techniques to identify attitude expressions, their po-
larities, and their targets. The target of attitude could
be another discussant or an entity mentioned in the
discussion. We use sentiment analysis techniques
to identify opinion expressions. We use named en-
399
tity recognition and noun phrase chunking to iden-
tify the entities mentioned in the discussion. The
opinion-target pairs are identified using a number of
syntactic and semantic rules.
For each participant in the discussion, we con-
struct a vector of attitude features. We call this vec-
tor the discussant attitude profile. The attitude pro-
file of a discussant contains an entry for every other
discussant and an entry for every entity mentioned
in the discission. We use clustering techniques to
cluster the attitude vector space. We use the clus-
tering results to determine the subgroup structure of
the discussion group and the subgroup membership
of each participant.
The rest of this paper is organized as follows. Sec-
tion 2 examines the previous work. We describe the
data used in the paper in Section 2.4. Section 3
presents our approach. Experiments, results and
analysis are presented in Section 4. We conclude
in Section 5
2 Related Work
2.1 Sentiment Analysis
Our work is related to a huge body of work on sen-
timent analysis. Previous work has studied senti-
ment in text at different levels of granularity. The
first level is identifying the polarity of individual
words. Hatzivassiloglou and McKeown (1997) pro-
posed a method to identify the polarity of adjec-
tives based on conjunctions linking them. Turney
and Littman (2003) used pointwise mutual infor-
mation (PMI) and latent semantic analysis (LSA)
to compute the association between a given word
and a set of positive/negative seed words. Taka-
mura et al (2005) proposed using a spin model to
predict word polarity. Other studies used Word-
Net to improve word polarity prediction (Hu and
Liu, 2004a; Kamps et al, 2004; Kim and Hovy,
2004; Andreevskaia and Bergler, 2006). Hassan
and Radev (2010) used a random walk model built
on top of a word relatedness network to predict the
semantic orientation of English words. Hassan et
al. (2011) proposed a method to extend their random
walk model to assist word polarity identification in
other languages including Arabic and Hindi.
Other work focused on identifying the subjectiv-
ity of words. The goal of this work is to deter-
mine whether a given word is factual or subjective.
We use previous work on subjectivity and polar-
ity prediction to identify opinion words in discus-
sions. Some of the work on this problem classi-
fies words as factual or subjective regardless of their
context (Wiebe, 2000; Hatzivassiloglou and Wiebe,
2000; Banea et al, 2008). Some other work no-
ticed that the subjectivity of a given word depends
on its context. Therefor, several studies proposed
using contextual features to determine the subjec-
tivity of a given word within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
The second level of granularity is the sentence
level. Hassan et al (2010) presents a method for
identifying sentences that display an attitude from
the text writer toward the text recipient. They de-
fine attitude as the mental position of one partici-
pant with regard to another participant. A very de-
tailed survey that covers techniques and approaches
in sentiment analysis and opinion mining could be
found in (Pang and Lee, 2008).
2.2 Opinion Target Extraction
Several methods have been proposed to identify
the target of an opinion expression. Most of the
work have been done in the context of product re-
views mining (Hu and Liu, 2004b; Kobayashi et
al., 2007; Mei et al, 2007; Stoyanov and Cardie,
2008). In this context, opinion targets usually refer
to product features (i.e. product components or at-
tributes, as defined by Liu (2009)). In the work of
Hu and Liu (2004b), they treat frequent nouns and
noun phrases as product feature candidates. In our
work, we extract as targets frequent noun phrases
and named entities that are used by two or more dif-
ferent discussants. Scaffidi et al (2007) propose a
language model approach to product feature extrac-
tion. They assume that product features are men-
tioned more often in product reviews than they ap-
pear in general English text. However, such statistics
may not be reliable when the corpus size is small.
In another related work, Jakob and
Gurevych (2010) showed that resolving the
anaphoric links in the text significantly improves
opinion target extraction. In our work, we use
anaphora resolution to improve opinion-target
400
Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established
federal law. All states should enact such a law.
Participant B commented on A?s
post:
I support the law because the federal government is either afraid or indifferent to the issue. Arizona
has the right and the responsibility to protect the people of the State of Arizona. If this requires a
possible slight inconvenience to any citizen so be it.
Participant C commented on B?s
post:
That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction
of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start
trading Constitutional rights for ?security?, then you?ve lost.
Table 1: Example posts from the Arizona Immigration Law thread
pairing as shown in Section 3 below.
2.3 Community Mining
Previous work also studied community mining in so-
cial media sites. Somasundaran and Wiebe (2009)
presents an unsupervised opinion analysis method
for debate-side classification. They mine the web
to learn associations that are indicative of opinion
stances in debates and combine this knowledge with
discourse information. Anand et al (2011) present
a supervised method for stance classification. They
use a number of linguistic and structural features
such as unigrams, bigrams, cue words, repeated
punctuation, and opinion dependencies to build a
stance classification model. This work is limited to
dual sided debates and defines the problem as a clas-
sification task where the two debate sides are know
beforehand. Our work is characterized by handling
multi-side debates and by regarding the problem as
a clustering problem where the number of sides is
not known by the algorithm. This work also uti-
lizes only discussant-to-topic attitude predictions for
debate-side classification. Out work utilizes both
discussant-to-topic and discussant-to-discussant at-
titude predictions.
In another work, Kim and Hovy (2007) predict
the results of an election by analyzing discussion
threads in online forums that discuss the elections.
They use a supervised approach that uses unigrams,
bigrams, and trigrams as features. In contrast, our
work is unsupervised and uses different types infor-
mation. Moreover, although this work is related to
ours at the goal level, it does not involve any opinion
analysis.
Another related work classifies the speakers side
in a corpus of congressional floor debates, using
the speakers final vote on the bill as a labeling
for side (Thomas et al, 2006; Bansal et al, 2008;
Yessenalina et al, 2010). This work infers agree-
ment between speakers based on cases where one
speaker mentions another by name, and a simple al-
gorithm for determining the polarity of the sentence
in which the mention occurs. This work shows that
even with the resulting sparsely connected agree-
ment structure, the MinCut algorithm can improve
over stance classification based on textual informa-
tion alone. This work also requires that the de-
bate sides be known by the algorithm and it only
identifies discussant-to-discussant attitude. In our
experiments below we show that identifying both
discussant-to-discussant and discussant-to-topic at-
titudes achieves better results.
2.4 Data
In this section, we describe the datasets used in
this paper. We use three different datasets. The
first dataset (politicalforum, henceforth) consists of
5,743 posts collected from a political forum2. All
the posts are in English. The posts cover 12 dis-
puted political and ideological topics. The discus-
sants of each topic were asked to participate in a
poll. The poll asked them to determine their stance
on the discussion topic by choosing one item from a
list of possible arguments. The list of participants
who voted for each argument was published with
the poll results. Each poll was accompanied by a
discussion thread. The people who participated in
the poll were allowed to post text to that thread to
justify their choices and to argue with other partic-
ipants. We collected the votes and the discussion
thread of each poll. We used the votes to identify
the subgroup membership of each participant.
The second dataset (createdebate, henceforth)
comes from an online debating site 3. It consists of
2http://www.politicalforum.com
3http://www.createdebate.com
401
Source Topic Question #Sides #Posts #Participants
Politicalforum
Arizona Immigration Law Do you support Arizona in its decision to enact their
Immigration Enforcement law?
2 738 59
Airport Security Should we pick muslims out of the line and give ad-
ditional scrutiny/screening?
4 735 69
Vote for Obama Will you vote for Obama in the 2012 Presidential
elections?
2 2599 197
Createdebate
Evolution Has evolution been scientifically proved? 2 194 98
Social networking sites It is easier to maintain good relationships in social
networking sites such as Facebook.
2 70 31
Abortion Should abortion be banned 3 477 70
Wikipedia
Ireland Misleading description of Irland island partition 3 40 10
South Africa Goverment Was the current form of South African government
born in May 1910?
3 23 5
Oil Spill Obama?s response to gulf oil spill 3 30 12
Table 2: Example threads from our three datasets
30 debates containing a total of 2,712 posts. Each
debate is about one topic. The description of each
debate states two or more positions regarding the de-
bate topic. When a new participant enters the discus-
sion, she explicitly picks a position and posts text to
support it, support a post written by another partici-
pant who took the same position, or to dispute a post
written by another participant who took an opposing
position. We collected the discussion thread and the
participant positions for each debate.
The third dataset (wikipedia, henceforth) comes
from the Wikipedia4 discussion section. When a
topic on Wikipedia is disputed, the editors of that
topic start a discussion about it. We collected 117
Wikipeida discussion threads. The threads contains
a total of 1,867 posts.
The politicalforum and createdebate datasets are
self labeled as described above. To annotate the
Wikipedia data, we asked an expert annotator (a
professor in sociolinguistics who is not one of the
authors) to read each of the Wikipedia discussion
threads and determine whether the discussants split
into subgroups in which case he was asked to deter-
mine the subgroup membership of each discussant.
Table 2 lists few example threads from our three
datasets. Table 1 shows a portion of discussion
thread between three participants about enforcing a
new immigration law in Arizona. This thread ap-
peared in the polictalforum dataset. The text posted
by the three participants indicates that A?s position
4http://www.wikipedia.com
is with enforcing the law, that B agrees with A, and
that C disagrees with both. This means that A and B
belong to the same opinion subgroup, while belongs
to an opposing subgroup.
We randomly selected 6 threads from our datasets
(2 from politicalforum, 2 from createdebate, and 2
from Wikipedia) and used them as development set.
This set was used to develop our approach.
3 Approach
In this section, we describe a system that takes a
discussion thread as input and outputs the subgroup
membership of each discussant. Figure 1 illustrates
the processing steps performed by our system to de-
tect subgroups. In the following subsections we de-
scribe the different stages in the system pipeline.
3.1 Thread Parsing
We start by parsing the thread to identify posts, par-
ticipants, and the reply structure of the thread (i.e.
who replies to whom). In the datasets described in
Section 2.4, all this information was explicitly avail-
able in the thread. We tokenize the text of each post
and split it into sentences using CLAIRLib (Abu-
Jbara and Radev, 2011).
3.2 Opinion Word Identification
The next step is to identify the words that express
opinion and determine their polarity (positive or
negative). Lehrer (1974) defines word polarity as
the direction the word deviates to from the norm. We
402
use OpinionFinder (Wilson et al, 2005a) to identify
polarized words and their polarities.
The polarity of a word is usally affected by
the context in which it appears. For example, the
word fine is positive when used as an adjective and
negative when used as a noun. For another example,
a positive word that appears in a negated context
becomes negative. OpinionFinder uses a large set of
features to identify the contextual polarity of a given
polarized word given its isolated polarity and the
sentence in which it appears (Wilson et al, 2005b).
Snippet (3) below shows the result of applying this
step to snippet (1) above (O means neutral; POS
means positive; NEG means negative).
(3) Arizona/O Immigration/O law/O good/POS ./O
Illegal/O immigration/O bad/NEG ./O
3.3 Target Identification
The goal of this step is to identify the possible tar-
gets of opinion. A target could be another discus-
sant or an entity mentioned in the discussion. When
the target of opinion is another discussant, either the
discussant name is mentioned explicitly or a second
person pronoun is used to indicate that the opinion
is targeting the recipient of the post. For example,
in snippet (2) above the second person pronoun you
indicates that the opinion word disagree is targeting
Discussant 1, the recipient of the post.
The target of opinion can also be an entity
mentioned in the discussion. We use two methods to
identify such entities. The first method uses shallow
parsing to identify noun groups (NG). We use the
Edinburgh Language Technology Text Tokenization
Toolkit (LT-TTT) (Grover et al, 2000) for this pur-
pose. We consider as an entity any noun group that
is mentioned by at least two different discussants.
We replace each identified entity with a unique
placeholder (ENTITYID). For example, the noun
group Arizona immigration law is mentioned by
Discussant 1 and Discussant 2 in snippets 1 and 2
above respectively. Therefore, we replace it with a
placehold as illustrated in snippets (4) and (5) below.
(4) Discussant 1: ENTITY1 is good. Illegal im-
NER NP Chunking
Barack Obama the Republican nominee
Middle East the maverick economists
Bush conservative ideologues
Bob McDonell the Nobel Prize
Iraq Federal Government
Table 3: Some of the entities identified using NER and
NP Chunking in a discussion thread about the US 2012
elections
migration is bad.
(5) Discussant 2: I totally disagree with you. ENTITY1
is blatant racism, and quite unconstitutional.
We only consider as entities noun groups that
contain two words or more. We impose this require-
ment because individual nouns are very common
and regarding all of them as entities will introduce
significant noise.
In addition to this shallow parsing method, we
also use named entity recognition (NER) to identify
more entities. We use the Stanford Named Entity
Recognizer (Finkel et al, 2005) for this purpose. It
recognizes three types of entities: person, location,
and organization. We impose no restrictions on the
entities identified using this method. Again, we re-
place each distinct entity with a unique placeholder.
The final set of entities identified in a thread is the
union of the entities identified by the two aforemen-
tioned methods. Table 3
Finally, a challenge that always arises when
performing text mining tasks at this level of gran-
ularity is that entities are usually expressed by
anaphorical pronouns. Previous work has shown
that For example, the following snippet contains
an explicit mention of the entity Obama in the first
sentence, and then uses a pronoun to refer to the
same entity in the second sentence. The opinion
word unbeatable appears in the second sentence
and is syntactically related to the pronoun He.
In the next subsection, it will become clear why
knowing which entity does the pronoun He refers to
is essential for opinion-target pairing.
(6) It doesn?t matter whether you vote for Obama.
403
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Opinion Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Target Identification 
? Anaphora resolution 
? Identify named entities 
? Identify Frequent noun 
phrases. 
? Identify mentions of 
other discussants 
Opinion-Target Pairing 
? Dependency Rules 
 
 
 
Discussant Attitude 
Profiles (DAPs)  
 
 
 
Clustering 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text. 
? Split posts into sentences 
 
Figure 1: An overview of the subgroups detection system
He is unbeatable.
Jakob and Gurevych (2010) showed experi-
mentally that resolving the anaphoric links in the
text significantly improves opinion target extraction.
We use the Beautiful Anaphora Resolution Toolkit
(BART) (Versley et al, 2008) to resolve all the
anaphoric links within the text of each post sepa-
rately. The result of applying this step to snippet (6)
is:
(6) It doesn?t matter whether you vote for Obama.
Obama is unbeatable.
Now, both mentions of Obama will be recog-
nized by the Stanford NER system and will be
identified as one entity.
3.4 Opinion-Target Pairing
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. We use the Stanford Parser (Klein and
Manning, 2003) to generate the dependency parse
tree of each sentence in the thread. An opinion word
and a target form a pair if they stratify at least one
of our dependency rules. Table 4 illustrates some
of these rules 5. The rules basically examine the
types of the dependencies on the shortest path that
connect the opinion word and the target in the de-
pendency parse tree. It has been shown in previous
work on relation extraction that the shortest depen-
dency path between any two entities captures the in-
formation required to assert a relationship between
them (Bunescu and Mooney, 2005).
If a sentence S in a post written by participant
Pi contains an opinion word OPj and a target TRk,
and if the opinion-target pair satisfies one of our de-
pendency rules, we say that Pi expresses an attitude
towards TRk. The polarity of the attitude is deter-
mined by the polarity of OPj . We represent this as
Pi
+
? TRk if OPj is positive and Pi
?
? TRk if OPj
is negative.
It is likely that the same participant Pi express
sentiment toward the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi
m+
???
n?
TRk where m (n) is the
number of times Pi expressed positive (negative) at-
titude toward TRk.
3.5 Discussant Attitude Profile
We propose a representation of discussantsa?ttitudes
towards the identified targets in the discussion
thread. As stated above, a target could be another
discussant or an entity mentioned in the discussion.
5The code will be made publicly available at the time of
publication
404
ID Rule In Words Example
R1 OP ? nsubj ? TR The target TR is the nominal subject of the opinion
word OP
ENTITY1TR is goodOP .
R2 OP ? dobj ? TR The target T is a direct object of the opinion OP I hateOP ENTITY2TR
R3 OP ? prep ? ? TR The target TR is the object of a preposition that
modifies the opinion word OP
I totally disagreeOP with youTR.
R4 TR? amod? OP The opinion is an adjectival modifier of the target The badOP ENTITY3TR is spreading lies
R5 OP ? nsubjpass? TR The target TR is the nominal subject of the passive
opinion word OP
ENTITY4TR is hatedOP by everybody.
R6 OP ? prep ? ? poss? TR The opinion word OP connected through a prep ?
relation as in R2 to something possessed by the
target TR
The main flawOP in yourTR analysis is
that it?s based on wrong assumptions.
R7 OP ? dobj ? poss? TR The target TR possesses something that is the direct
object of the opinion word OP
I likeOP ENTITY5TR?s brilliant ideas.
R8 OP ? csubj ? nsubj ? TR The opinon word OP is a causal subject of a phrase
that has the target TR as its nominal subject
What ENTITY6TR announced was
misleadingOP .
Table 4: Examples of the dependency rules used for opinion-target pairing.
Our representation is a vector containing numeri-
cal values. The values correspond to the counts of
positive/negative attitudes expressed by the discus-
sant toward each of the targets. We call this vector
the discussant attitude profile (DAP). We construct a
DAP for every discussant. Given a discussion thread
with d discussants and e entity targets, each attitude
profile vector has n = (d + e) ? 3 dimensions. In
other words, each target (discussant or entity) has
three corresponding values in the DAP: 1) the num-
ber of times the discussant expressed positive atti-
tude toward the target, 2) the number of times the
discussant expressed a negative attitude towards the
target, and 3) the number of times the the discussant
interacted with or mentioned the target. It has to be
noted that these values are not symmetric since the
discussions explicitly denote the source and the tar-
get of each post.
3.6 Clustering
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups according to their opinion.
4 Evaluation
In this section, we present several levels of evalu-
ation of our system. First, we compare our sys-
tem to baseline systems. Second, we study how the
choice of the clustering algorithm impacts the re-
sults. Third, we study the impact of each component
in our system on the performance. All the results
reported in this section that show difference in the
performance are statistically significant at the 0.05
level (as indicated by a 2-tailed paired t-test). Be-
fore describing the experiments and presenting the
results, we first describe the evaluation metrics we
use.
4.0.1 Evaluation Metrics
We use two evaluation metrics to evaluate sub-
groups detection accuracy: Purity and Entropy. To
compute Purity (Manning et al, 2008), each clus-
ter is assigned the class of the majority vote within
the cluster, and then the accuracy of this assignment
is measured by dividing the number of correctly as-
signed members by the total number of instances. It
can be formally defined as:
purity(?, C) =
1
N
?
k
max
j
|?k ? cj | (1)
where ? = {?1, ?2, ..., ?k} is the set of clusters
and C = {c1, c2, ..., cJ} is the set of classes. ?k is
interpreted as the set of documents in ?k and cj as
405
the set of documents in cj . The purity increases as
the quality of clustering improves.
The second metric is Entropy. The Entropy of a
cluster reflects how the members of the k distinct
subgroups are distributed within each resulting clus-
ter; the global quality measure is computed by aver-
aging the entropy of all clusters:
Entropy = ?
j? nj
n
i?
P (i, j)? log2P (i, j)
(2)
where P (i, j) is the probability of finding an ele-
ment from the category i in the cluster j, nj is the
number of items in cluster j, and n the total num-
ber of items in the distribution. In contrast to purity,
the entropy decreases as the quality of clustering im-
proves.
4.1 Comparison to Baseline Systems
We compare our system (DAPC) that was described
in Section 3 to two baseline methods. The first base-
line (GC) uses graph clustering to partition a net-
work based on the interaction frequency between
participants. We build a graph where each node
represents a participant. Edges link participants if
they exchange posts, and edge weights are based on
the number of interactions. We tried two methods
for clustering the resulting graph: spectral partition-
ing (Luxburg, 2007) and a hierarchical agglomera-
tion algorithm which works by greedily optimizing
the modularity for graphs (Clauset et al, 2004).
The second baseline (TC) is based on the premise
that the member of the same subgroup are more
likely to use vocabulary drawn from the same lan-
guage model. We collect all the text posted by each
participant and create a tf-idf representations of the
text in a high dimensional vector space. We then
cluster the vector space to identify subgroups. We
use k-means (MacQueen, 1967) as our clustering
algorithm in this experiment (comparison of vari-
ous clustering algorithms is presented in the next
subsection). The distances between vectors are
Eculidean distances. Table 5 shows that our sys-
tem performs significantly better the baselines on the
three datasets in terms of both the purity (P ) and the
entropy (E) (notice that lower entropy values indi-
cate better clustering). The values reported are the
Method Createdebate Politicalforum Wikipedia
P E P E P E
GC - Spectral 0.50 0.85 0.50 0.88 0.49 0.89
GC - Hierarchical 0.48 0.86 0.47 0.89 0.49 0.87
TC - kmeans 0.51 0.84 0.49 0.88 0.52 0.85
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 5: Comparison to baseline systems
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC - EM 0.63 0.71 0.61 0.82 0.63 0.61
DAPC - FF 0.63 0.70 0.60 0.83 0.64 0.59
DAPC - kmeans 0.64 0.68 0.61 0.80 0.66 0.55
Table 6: Comparison of different clustering algorithms
average results of the threads of each dataset. We
believe that the baselines performed poorly because
the interaction frequency and the text similarity are
not key factors in identifying subgroup structures.
Many people would respond to people they disagree
with more, while others would mainly respond to
people they agree with most of the time. Also, peo-
ple in opposing subgroups tend to use very similar
text when discussing the same topic and hence text
clustering does not work as well.
4.2 Choice of the clustering algorithm
We experimented with three different clustering al-
gorithms: expectation maximization (EM), and k-
means (MacQueen, 1967), and FarthestFirst (FF)
(Hochbaum and Shmoys, 1985; Dasgupta, 2002).
As we did in the previous subsection, we use
Eculidean distance to measure the distance between
vectors All the system (DAP) components are in-
cluded as described in Section 3. The purity and
entropy values using each algorithm are shown in
Table 6. Although k-means seems to be performing
slightly better than other algorithms, the differences
in the results are not significant. This indicates that
the choice of the clustering algorithm does not have
a noticeable impact on the results. We also exper-
imented with using Manhattan distance and cosine
similarity instead of Euclidean distance to measure
the distance between attitude vectors. We noticed
that the choice of the distance does not have signifi-
cant impact on the results as well.
406
4.3 Component Evaluation
In this subsection, we evaluate the impact of the dif-
ferent components in the pipeline on the system per-
formance. We do that by removing each component
from the pipeline and measuring the change in per-
formance. We perform the following experiments:
1) We run the full system with all its components
included (DAPC). 2) We run the system and in-
clude only discussant-to-discussant attitude features
in the attitude vectors (DAPC-DD). 3) We include
only discussant-to-entity attitude features in the atti-
tude vectors (DAPC-DE). 4) We include only senti-
ment features in the attitude vector; i.e. we exclude
the interaction count features (DAPC-SE). 5) We in-
clude only interaction count features to the attitude
vector; i.e. we exclude sentiment features (DAPC-
INT). 6) We skip the anaphora resolution step in the
entity identification component (DAPC-NO AR). 7)
We only use named entity recognition to identify en-
tity targets; i.e. we exclude the entities identified
through noun phrasing chunking (DAPC-NER). 8)
Finally, we only noun phrase chunking to identify
entity targets (DAPC-NP). In all these experiments
k-means is used for clustering and the number of
clusters is set as explained in the previous subsec-
tion.
The results show that all the components in the
system contribute to better performance of the sys-
tem. We notice from the results that the performance
of the system drops significantly if sentiment fea-
tures are not included. This is result corroborates
our hypothesis that interaction features are not suffi-
cient factors for detecting rift in discussion groups.
Including interaction features improve the perfor-
mance (although not by a big difference) because
they help differentiate between the case where par-
ticipants A and B never interacted with each other
and the case where they interact several time but
never posted text that indicate difference in opin-
ion between them. We also notice that the perfor-
mance drops significantly in DAPC-DD and DAPC-
DD which also supports our hypotheses that both
the sentiment discussants show toward one another
and the sentiment they show toward the aspects of
the discussed topic are important for the task. Al-
though using both named entity recognition (NER)
and noun phrase chunking achieves better results, it
Method Createdebate Politicalforum Wikipedia
P E P E P E
DAPC 0.64 0.68 0.61 0.80 0.66 0.55
DAPC-DD 0.59 0.77 0.57 0.86 0.62 0.61
DAPC-DE 0.60 0.69 0.58 0.84 0.58 0.78
DAPC-SE 0.62 0.70 0.60 0.83 0.61 0.62
DAPC-INT 0.54 0.88 0.52 0.91 0.57 0.85
DAPC-NO AR 0.62 0.72 0.60 0.84 0.64 0.60
DAPC-NER 0.61 0.71 0.58 0.86 0.63 0.59
DAPC-NP 0.63 0.75 0.59 0.84 0.65 0.62
Table 7: Impact of system components on the perfor-
mance
can also be noted from the results that NER con-
tributes more to the system performance. Finally,
the results support Jakob and Gurevych (2010) find-
ings that anaphora resolution aids opinion mining
systems.
5 Conclusions
In this paper, we presented an approach for subgroup
detection in ideological discussions. Our system
uses linguistic analysis techniques to identify the at-
titude the participants of online discussions carry to-
ward each other and toward the aspects of the discus-
sion topic. Attitude prediction as well as interaction
frequency to construct an attitude vector for each
participant. The attitude vectors of discussants are
then clustered to form subgroups. Our experiments
showed that our system outperforms text clustering
and interaction graph clustering. We also studied the
contribution of each component in our system to the
overall performance.
Acknowledgments
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
407
References
Amjad Abu-Jbara and Dragomir Radev. 2011. Clairlib:
A toolkit for natural language processing, information
retrieval, and network analysis. In Proceedings of the
ACL-HLT 2011 System Demonstrations, pages 121?
126, Portland, Oregon, June. Association for Compu-
tational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Workshop
on Computational Approaches to Subjectivity and Sen-
timent Analysis (WASSA 2.011), pages 1?9, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
wordnet for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL?06.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Aaron Clauset, Mark E. J. Newman, and Cristopher
Moore. 2004. Finding community structure in very
large networks. Phys. Rev. E, 70:066111.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147?1154.
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255.
Ahmed Hassan, Amjad AbuJbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 592?597, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING, pages 299?305.
Hochbaum and Shmoys. 1985. A best possible heuristic
for the k-center problem. Mathematics of Operations
Research, 10(2):180?184.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In KDD?04, pages 168?177.
Minqing Hu and Bing Liu. 2004b. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263?268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten De Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In National Insti-
tute for, pages 1115?1118.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In COLING, pages 1367?1373.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423?430.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL.
Adrienne Lehrer. 1974. Semantic fields and lezical struc-
ture. North Holland, Amsterdam and New York.
408
Bing Liu. 2009. Web Data Mining: Exploring Hyper-
links, Contents, and Usage Data (Data-Centric Sys-
tems and Applications). Springer, 1st ed. 2007. corr.
2nd printing edition, January.
Ulrike Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17:395?416, December.
J. B. MacQueen. 1967. Some methods for classification
and analysis of multivariate observations. In L. M. Le
Cam and J. Neyman, editors, Proc. of the fifth Berkeley
Symposium on Mathematical Statistics and Probabil-
ity, volume 1, pages 281?297. University of California
Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 171?180, New York, NY,
USA. ACM.
Soo min Kim and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In In EMNLP-
CoNLL 2007.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03: Proceedings of the 2nd
international conference on Knowledge capture, pages
70?77.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In HLT-
EMNLP?05, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
226?234, Suntec, Singapore, August. Association for
Computational Linguistics.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In In Col-
ing.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05, pages 133?140.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In In Proceedings
of EMNLP, pages 327?335.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-
aofeng Yang, and Alessandro Moschitti. 2008. Bart:
A modular toolkit for coreference resolution. In Pro-
ceedings of the ACL-08: HLT Demo Session, pages
9?12, Columbus, Ohio, June. Association for Compu-
tational Linguistics.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of Ar-
tificial Intelligence, pages 735?740.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjectiv-
ity analysis. In Proceedings of HLT/EMNLP on Inter-
active Demonstrations, HLT-Demo ?05, pages 34?35,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Vancou-
ver, Canada.
Ainur Yessenalina, Yisong Yue, and Claire Cardie. 2010.
Multi-level structured models for document-level sen-
timent classification. In In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03, pages 129?136.
409
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 133?138,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Subgroup Detector: A System for Detecting Subgroups in Online
Discussions
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
We present Subgroup Detector, a system
for analyzing threaded discussions and
identifying the attitude of discussants towards
one another and towards the discussion
topic. The system uses attitude predictions to
detect the split of discussants into subgroups
of opposing views. The system uses an
unsupervised approach based on rule-based
opinion target detecting and unsupervised
clustering techniques. The system is open
source and is freely available for download.
An online demo of the system is available at:
http://clair.eecs.umich.edu/SubgroupDetector/
1 Introduction
Online forums discussing ideological and political
topics are common1. When people discuss a con-
troversial topic, it is normal to see situations of both
agreement and disagreement among the discussants.
It is even not uncommon that the big group of dis-
cussants split into two or more smaller subgroups.
The members of each subgroup have the same opin-
ion toward the discission topic. The member of a
subgroup is more likely to show positive attitude to
the members of the same subgroup, and negative at-
titude to the members of opposing subgroups. For
example, consider the following snippet taken from
a debate about school uniform
1www.politicalforum.com, www.createdebate.com,
www.forandagainst.com, etc
(1) Discussant 1: I believe that school uniform is a
good idea because it improves student attendance.
(2) Discussant 2: I disagree with you. School uniform
is a bad idea because people cannot show their person-
ality.
In (1), the writer is expressing positive attitude
regarding school uniform. The writer of (2) is ex-
pressing negative attitude (disagreement) towards
the writer of (1) and negative attitude with respect
to the idea of school uniform. It is clear from this
short dialog that the writer of (1) and the writer of
(2) are members of two opposing subgroups. Dis-
cussant 1 supports school uniform, while Discussant
2 is against it.
In this demo, we present an unsupervised system
for determining the subgroup membership of each
participant in a discussion. We use linguistic tech-
niques to identify attitude expressions, their polar-
ities, and their targets. We use sentiment analy-
sis techniques to identify opinion expressions. We
use named entity recognition, noun phrase chunk-
ing and coreference resolution to identify opinion
targets. Opinion targets could be other discussants
or subtopics of the discussion topic. Opinion-target
pairs are identified using a number of hand-crafted
rules. The functionality of this system is based on
our previous work on attitude mining and subgroup
detection in online discussions.
This work is related to previous work in the areas
of sentiment analysis and online discussion mining.
Many previous systems studied the problem of iden-
133
tifying the polarity of individual words (Hatzivas-
siloglou and McKeown, 1997; Turney and Littman,
2003). Opinionfinder (Wilson et al, 2005) is a sys-
tem for mining opinions from text. SENTIWORD-
NET (Esuli and Sebastiani, 2006) is a lexical re-
source in which each WordNet synset is associated
to three numerical scores Obj(s), Pos(s) and Neg(s),
describing how objective, positive, and negative the
terms contained in the synset are. Dr Sentiment (Das
and Bandyopadhyay, 2011) is an online interactive
gaming technology used to crowd source human
knowledge to build an extension of SentiWordNet.
Another research line focused on analyzing on-
line discussions. For example, Lin et al (2009)
proposed a sparse coding-based model that simul-
taneously models the semantics and the structure
of threaded discussions. Shen et al (2006) pro-
posed a method for exploiting the temporal and lex-
ical similarity information in discussion streams to
identify the reply structure of the dialog. Many sys-
tems addressed the problem of extracting social net-
works from discussions (Elson et al, 2010; McCal-
lum et al, 2007). Other related sentiment analy-
sis systems include MemeTube (Li et al, 2011), a
sentiment-based system for analyzing and display-
ing microblog messages; and C-Feel-It (Joshi et al,
2011), a sentiment analyzer for micro-blogs.
In the rest of this paper, we describe the system
architecture, implementation, usage, and its evalua-
tion.
2 System Overview
Figure 1 shows a block diagram of the system com-
ponents and the processing pipeline. The first com-
ponent is the thread parsing component which takes
as input a discussion thread and parses it to iden-
tify posts, participants, and the reply structure of the
thread. The second component in the pipeline pro-
cesses the text of posts to identify polarized words
and tag them with their polarity. The list of polar-
ity words that we use in this component has been
taken from the OpinionFinder system (Wilson et al,
2005).
The polarity of a word is usually affected by the
context in which it appears. For example, the word
fine is positive when used as an adjective and neg-
ative when used as a noun. For another example, a
positive word that appears in a negated context be-
comes negative. To address this, we take the part-
of-speech (POS) tag of the word into consideration
when we assign word polarities. We require that the
POS tag of a word matches the POS tag provided in
the list of polarized words that we use. The negation
issue is handled in the opinion-target pairing step as
we will explain later.
The next step in the pipeline is to identify the can-
didate targets of opinion in the discussion. The tar-
get of attitude could be another discussant, an entity
mentioned in the discussion, or an aspect of the dis-
cussion topic. When the target of opinion is another
discussant, either the discussant name is mentioned
explicitly or a second person pronoun (e.g you, your,
yourself) is used to indicate that the opinion is tar-
geting the recipient of the post.
The target of opinion could also be a subtopic or
an entity mentioned in the discussion. We use two
methods to identify such targets. The first method
depends on identifying noun groups (NG). We con-
sider as an entity any noun group that is mentioned
by at least two different discussants. We only con-
sider as entities noun groups that contain two words
or more. We impose this requirement because in-
dividual nouns are very common and considering
all of them as candidate targets will introduce sig-
nificant noise. In addition to this shallow pars-
ing method, we also use named entity recognition
(NER) to identify more targets. The named en-
tity tool that we use recognizes three types of en-
tities: person, location, and organization. We im-
pose no restrictions on the entities identified using
this method.
A challenge that always arises when perform-
ing text mining tasks at this level of granularity
is that entities are usually expressed by anaphori-
cal pronouns. Jakob and Gurevych (2010) showed
experimentally that resolving the anaphoric links
134
Discussion 
Thread 
?.??. 
?.??. 
?.??. 
Opinion Identification 
? Identify polarized words 
? Identify the contextual 
polarity of each word 
 
 
Target Identification 
? Anaphora resolution 
? Identify named entities 
? Identify Frequent noun 
phrases. 
? Identify mentions of 
other discussants 
Opinion-Target Pairing 
? Dependency Rules 
 
 
 
Discussant Attitude 
Profiles (DAPs)  
 
 
 
Clustering 
Subgroups 
 
 
 
 
 
Thread Parsing 
? Identify posts 
? Identify discussants 
? Identify the reply 
structure 
? Tokenize text. 
? Split posts into sentences 
 
Figure 1: A block diagram illustrating the processing pipeline of the subgroup detection system
in text significantly improves opinion target extrac-
tion. Therefore, we use co-reference resolution tech-
niques to resolve all the anaphoric links in the dis-
cussion thread.
At this point, we have all the opinion words and
the potential targets identified separately. The next
step is to determine which opinion word is target-
ing which target. We propose a rule based approach
for opinion-target pairing. Our rules are based on
the dependency relations that connect the words in
a sentence. An opinion word and a target form a
pair if the dependency path between them satisfies
at least one of our dependency rules. Table 1 illus-
trates some of these rules. The rules basically exam-
ine the types of dependency relations on the shortest
path that connect the opinion word and the target in
the dependency parse tree. It has been shown in pre-
vious work on relation extraction that the shortest
dependency path between any two entities captures
the information required to assert a relationship be-
tween them (Bunescu and Mooney, 2005). If a sen-
tence S in a post written by participant Pi contains
an opinion word OPj and a target TRk, and if the
opinion-target pair satisfies one of our dependency
rules, we say that Pi expresses an attitude towards
TRk. The polarity of the attitude is determined by
the polarity of OPj . We represent this as Pi
+
? TRk
if OPj is positive and Pi
?
? TRk if OPj is nega-
tive. Negation is handled in this step by reversing
the polarity if the polarized expression is part of a
neg dependency relation.
It is likely that the same participant Pi expresses
sentiment towards the same target TRk multiple
times in different sentences in different posts. We
keep track of the counts of all the instances of posi-
tive/negative attitude Pi expresses toward TRk. We
represent this as Pi
m+
???
n?
TRk where m (n) is the
number of times Pi expressed positive (negative) at-
titude toward TRk.
Now, we have information about each discussant
attitude. We propose a representation of discus-
santsa?ttitudes towards the identified targets in the
discussion thread. As stated above, a target could
be another discussant or an entity mentioned in the
discussion. Our representation is a vector contain-
ing numerical values. The values correspond to the
counts of positive/negative attitudes expressed by
the discussant toward each of the targets. We call
this vector the discussant attitude profile (DAP). We
construct a DAP for every discussant. Given a dis-
cussion thread with d discussants and e entity tar-
gets, each attitude profile vector has n = (d+ e) ? 3
dimensions. In other words, each target (discussant
or entity) has three corresponding values in the DAP:
1) the number of times the discussant expressed pos-
itive attitude toward the target, 2) the number of
times the discussant expressed a negative attitude to-
wards the target, and 3) the number of times the the
discussant interacted with or mentioned the target.
It has to be noted that these values are not symmet-
135
ID Rule In Words
R1 OP ? nsubj ? TR The target TR is the nominal subject of the opinion word OP
R2 OP ? dobj ? TR The target T is a direct object of the opinion OP
R3 OP ? prep ? ? TR The target TR is the object of a preposition that modifies the opinion word OP
R4 TR? amod? OP The opinion is an adjectival modifier of the target
R5 OP ? nsubjpass? TR The target TR is the nominal subject of the passive opinion word OP
R6 OP ? prep ? ? poss? TR The opinion word OP connected through a prep ? relation as in R2 to something pos-
sessed by the target TR
R7 OP ? dobj ? poss? TR The target TR possesses something that is the direct object of the opinion word OP
R8 OP ? csubj ? nsubj ? TR The opinon word OP is a causal subject of a phrase that has the target TR as its nominal
subject.
Table 1: Examples of the dependency rules used for opinion-target pairing.
ric since the discussions explicitly denote the source
and the target of each post.
At this point, we have an attitude profile (or vec-
tor) constructed for each discussant. Our goal is to
use these attitude profiles to determine the subgroup
membership of each discussant. We can achieve this
goal by noticing that the attitude profiles of discus-
sants who share the same opinion are more likely to
be similar to each other than to the attitude profiles
of discussants with opposing opinions. This sug-
gests that clustering the attitude vector space will
achieve the goal and split the discussants into sub-
groups based on their opinion.
3 Implementation
The system is fully implemented in Java. Part-of-
speech tagging, noun group identification, named
entity recognition, co-reference resolution, and de-
pendency parsing are all computed using the Stan-
ford Core NLP API.2 The clustering component
uses the JavaML library3 which provides implemen-
tations to several clustering algorithms such as k-
means, EM, FarthestFirst, and OPTICS.
The system requires no installation. It, however,
requires that the Java Runtime Environment (JRE)
be installed. All the dependencies of the system
come bundled with the system in the same package.
The system works on all the standard platforms.
The system has a command-line interface that
2http://nlp.stanford.edu/software/corenlp.shtml
3http://java-ml.sourceforge.net/
provides full access to the system functionality. It
can be used to run the whole pipeline to detect sub-
groups or any portion of the pipeline. For example,
it can be used to tag an input text with polarity or to
identify candidate targets of opinion in a given in-
put. The system behavior can be controlled by pass-
ing arguments through the command line interface.
For example, the user can specify which clustering
algorithm should be used.
To facilitate using the system for research pur-
poses, the system comes with a clustering evaluation
component that uses the ClusterEvaluator package.4.
If the input to the system contains subgroup labels,
it can be run in the evaluation mode in which case
the system will output the scores of several different
clustering evaluation metrics such as purity, entropy,
f-measure, Jaccard, and RandIndex. The system also
has a Java API that can be used by researchers to de-
velop other systems using our code.
The system can process any discussion thread that
is input to it in a specific format. The format of
the input and output is described in the accompa-
nying documentation. It is the user responsibility
to write a parser that converts an online discussion
thread to the expected format. However, the sys-
tem package comes with two such parsers for two
different discussion sites: www.politicalforum.com
and www.createdebate.com.
The distribution also comes with three datasets
4http://eniac.cs.qc.cuny.edu/andrew/v-
measure/javadoc/index.html
136
Figure 2: A screenshot of the online demo
(from three different sources) comprising a total of
300 discussion threads. The datasets are annotated
with the subgroup labels of discussants.
Finally, we created a web interface to demonstrate
the system functionality. The web interface is in-
tended for demonstration purposes only. No web-
service is provided. Figure 2 shows a screenshots of
the web interface. The online demo can be accessed
at http://clair.eecs.umich.edu/SubgroupDetector/
4 Evaluation
In this section, we give a brief summary of the sys-
tem evaluation. We evaluated the system on discus-
sions comprising more than 10,000 posts in more
than 300 different topics. Our experiments show that
the system detects subgroups with promising accu-
racy. The average clustering purity of the detected
subgroups in the dataset is 0.65. The system signif-
icantly outperforms baseline systems based on text
clustering and discussant interaction frequency. Our
experiments also show that all the components in the
system (such as co-reference resolution, noun phrase
chunking, etc) contribute positively to the accuracy.
5 Conclusion
We presented a demonstration of a discussion min-
ing system that uses linguistic analysis techniques to
predict the attitude the participants in online discus-
sions forums towards one another and towards the
different aspects of the discussion topic. The system
is capable of analyzing the text exchanged in dis-
cussions and identifying positive and negative atti-
tudes towards different targets. Attitude predictions
are used to assign a subgroup membership to each
participant using clustering techniques. The sys-
tem predicts attitudes and identifies subgroups with
promising accuracy.
References
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724?731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Amitava Das and Sivaji Bandyopadhyay. 2011. Dr sen-
timent knows everything! In Proceedings of the ACL-
HLT 2011 System Demonstrations, pages 50?55, Port-
land, Oregon, June. Association for Computational
Linguistics.
137
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 138?147,
Uppsala, Sweden, July.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiword-
net: A publicly available lexical resource for opinion
mining. In In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC06, pages
417?422.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In EACL?97, pages 174?181.
Niklas Jakob and Iryna Gurevych. 2010. Using anaphora
resolution to improve opinion target identification in
movie reviews. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 263?268, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Aditya Joshi, Balamurali AR, Pushpak Bhattacharyya,
and Rajat Mohanty. 2011. C-feel-it: A sentiment ana-
lyzer for micro-blogs. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 127?132, Port-
land, Oregon, June. Association for Computational
Linguistics.
Cheng-Te Li, Chien-Yuan Wang, Chien-Lin Tseng, and
Shou-De Lin. 2011. Memetube: A sentiment-based
audiovisual system for analyzing and displaying mi-
croblog messages. In Proceedings of the ACL-HLT
2011 System Demonstrations, pages 32?37, Portland,
Oregon, June. Association for Computational Linguis-
tics.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09,
pages 131?138.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orientation
from association. ACM Transactions on Information
Systems, 21:315?346.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: a system for subjectivity analysis. In
HLT/EMNLP - Demo.
138
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 249?254,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Random Walk Factoid Annotation for Collective Discourse
Ben King Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
benking@umich.edu
rahuljha@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Robert Mankoff ?
The New Yorker Magazine
New York, NY
bob mankoff
@newyorker.com
Abstract
In this paper, we study the problem of au-
tomatically annotating the factoids present
in collective discourse. Factoids are in-
formation units that are shared between
instances of collective discourse and may
have many different ways of being realized
in words. Our approach divides this prob-
lem into two steps, using a graph-based
approach for each step: (1) factoid dis-
covery, finding groups of words that corre-
spond to the same factoid, and (2) factoid
assignment, using these groups of words
to mark collective discourse units that con-
tain the respective factoids. We study this
on two novel data sets: the New Yorker
caption contest data set, and the crossword
clues data set.
1 Introduction
Collective discourse tends to contain relatively
few factoids, or information units about which the
author speaks, but many nuggets, different ways
to speak about or refer to a factoid (Qazvinian and
Radev, 2011). Many natural language applications
could be improved with good factoid annotation.
Our approach in this paper divides this problem
into two subtasks: discovery of factoids, and as-
signment of factoids. We take a graph-based ap-
proach to the problem, clustering a word graph to
discover factoids and using random walks to as-
sign factoids to discourse units.
We also introduce two new datasets in this pa-
per, covered in more detail in section 3. The
New Yorker cartoon caption dataset, provided
by Robert Mankoff, the cartoon editor at The
New Yorker magazine, is composed of reader-
submitted captions for a cartoon published in the
magazine. The crossword clue dataset consists
?Cartoon Editor, The New Yorker magazine
Figure 1: The cartoon used for the New Yorker
caption contest #331.
of word-clue pairs used in major American cross-
word puzzles, with most words having several
hundred different clues published for it.
The term ?factoid? is used as in (Van Halteren
and Teufel, 2003), but in a slightly more abstract
sense in this paper, denoting a set of related words
that should ideally refer to a real-world entity, but
may not for some of the less coherent factoids.
The factoids discovered using this method don?t
necessarily correspond to the factoids that might
be chosen by annotators.
For example, given two user-submitted cartoon
captions
? ?When they said, ?Take us to your leader,? I
don?t think they meant your mother?s house,?
? and ?You?d better call your mother and tell
her to set a few extra place settings,?
a human may say that they share the factoid called
?mother.? The automatic methods however, might
say that these captions share factoid3, which is
identified by the words ?mother,? ?in-laws,? ?fam-
ily,? ?house,? etc.
The layout of this paper is as follows: we review
related work in section 2, we introduce the datasets
249
in detail in section 3, we describe our methods in
section 4, and report results in section 5.
2 Related Work
The distribution of factoids present in text collec-
tions is important for several NLP tasks such as
summarization. The Pyramid Evaluation method
(Nenkova and Passonneau, 2004) for automatic
summary evaluation depends on finding and an-
notating factoids in input sentences. Qazvinian
and Radev (2011) also studied the properties of
factoids present in collective human datasets and
used it to create a summarization system. Hennig
et al (2010) describe an approach for automati-
cally learning factoids for pyramid evaluation us-
ing a topic modeling approach.
Our random-walk annotation technique is sim-
ilar to the one used in (Hassan and Radev, 2010)
to identify the semantic polarity of words. Das
and Petrov (2011) also introduced a graph-based
method for part-of-speech tagging in which edge
weights are based on feature vectors similarity,
which is like the corpus-based lexical similarity
graph that we construct.
3 Data Sets
We introduce two new data sets in this paper, the
New Yorker caption contest data set, and the cross-
word clues data set. Though these two data sets are
quite different, they share a few important char-
acteristics. First, the discourse units tend to be
short, approximately ten words for cartoon cap-
tions and approximately three words for crossword
clues. Second, though the authors act indepen-
dently, they tend to produce surprisingly similar
text, making the same sorts of jokes, or referring
to words in the same sorts of ways. Thirdly, the
authors often try to be non-obvious: obvious jokes
are often not funny, and obvious crossword clues
make a puzzle less challenging.
3.1 New Yorker Caption Contest Data Set
The New Yorker magazine holds a weekly con-
test1 in which they publish a cartoon without
a caption and solicit caption suggestions from
their readers. The three funniest captions are se-
lected by the editor and published in the follow-
ing weeks. Figure 1 shows an example of such
a cartoon, while Table 1 shows examples of cap-
tions, including its winning captions. As part of
1http://www.newyorker.com/humor/caption
I don?t care what planet they are from, they can pass on the
left like everyone else.
I don?t care what planet they?re from, they should have the
common courtesy to dim their lights.
I don?t care where he?s from, you pass on the left.
If he wants to pass, he can use the right lane like everyone
else.
When they said, ?Take us to your leader,? I don?t think they
meant your mother?s house.
They may be disappointed when they learn that ?our leader?
is your mother.
You?d better call your mother and tell her to set a few extra
place settings.
If they ask for our leader, is it Obama or your mother?
Which finger do I use for aliens?
I guess the middle finger means the same thing to them.
I sense somehow that flipping the bird was lost on them.
What?s the Klingon gesture for ?Go around us, jerk??
Table 1: Captions for contest #331. Finalists are
listed in italics.
this research project, we have acquired five car-
toons along with all of the captions submitted in
the corresponding contest.
While the task of automatically identifying the
funny captions would be quite useful, it is well be-
yond the current state of the art in NLP. A much
more manageable task, and one that is quite impor-
tant for the contest?s editor is to annotate captions
according to their factoids. This allows the orga-
nizers of the contest to find the most frequently
mentioned factoids and select representative cap-
tions for each factoid.
On average, each cartoon has 5,400 submitted
captions, but for each of five cartoons, we sam-
pled 500 captions for annotation. The annotators
were instructed to mark factoids by identifying
and grouping events, objects, and themes present
in the captions, creating a unique name for each
factoid, and marking the captions that contain each
factoid. One caption could be given many differ-
ent labels. For example, in cartoon #331, such fac-
toids may be ?bad directions?, ?police?, ?take me
to your leader?, ?racism?, or ?headlights?. After
annotating, each set of captions contained about
60 factoids on average. On average a caption was
annotated with 0.90 factoids, with approximately
80% of the discourse units having at least one fac-
toid, 20% having at least two, and only 2% hav-
ing more than two. Inter-annotator agreement was
moderate, with an F1-score (described more in
section 5) of 0.6 between annotators.
As van Halteren and Teufel (2003) also found
250
0 20 40 600
20
40
60
(a)
0 5 10 15 20 250
50
100
150
(b)
Figure 2: Average factoid frequency distributions
for cartoon captions (a) and crossword clues (b).
0 100 200 300 400 5000
20
40
60
(a)
0 100 200 300 400 5000
5
10
(b)
Figure 3: Growth of the number of unique factoids
as the size of the corpus grows for cartoon captions
(a) and crossword clues (b).
when examining factoid distributions in human-
produced summaries, we found that the distribu-
tion of factoids in the caption set for each car-
toon seems to follow a power law. Figure 2 shows
the average frequencies of factoids, when ordered
from most- to least-frequent. We also found a
Heap?s law-type effect in the number of unique
factoids compared to the size of the corpus, as in
Figure 3.
3.2 Crossword Clues Data Set
Clues in crossword puzzles are typically obscure,
requiring the reader to recognize double mean-
ings or puns, which leads to a great deal of diver-
sity. These clues can also refer to one or more
of many different senses of the word. Table 2
shows examples of many different clues for the
word ?tea?. This table clearly illustrates the differ-
ence between factoids (the senses being referred
to) and nuggets (the realization of the factoids).
The website crosswordtracker.com col-
lects a large number of clues that appear in dif-
ferent published crossword puzzles and aggregates
them according to their answer. From this site, we
collected 200 sets of clues for common crossword
answers.
We manually annotated 20 sets of crossword
clues according to their factoids in the same fash-
ion as described in section 3.1. On average each
set of clues contains 283 clues and 15 different
factoids. Inter-annotator agreement on this dataset
was quite high with an F1-score of 0.96.
Clue Sense
Major Indian export drink
Leaves for a break? drink
Darjeeling, e.g. drink
Afternoon social event
4:00 gathering event
Sympathy partner film
Mythical Irish queen person
Party movement political movement
Word with rose or garden plant and place
Table 2: Examples of crossword clues and their
different senses for the word ?tea?.
4 Methods
4.1 Random Walk Method
We take a graph-based approach to the discovery
of factoids, clustering a word similarity graph and
taking the resulting clusters to be the factoids. Two
different graphs, a word co-occurrence graph and
a lexical similarity graph learned from the corpus,
are compared. We also compare the graph-based
methods against baselines of clustering and topic
modeling.
4.1.1 Word Co-occurrence Graph
To create the word co-occurrence graph, we create
a link between every pair of words with an edge
weight proportional to the number of times they
both occur in the same discourse unit.
4.1.2 Corpus-based Lexical Similarity Graph
To build the lexical similarity graph, a lexical sim-
ilarity function is learned from the corpus, that
is, from one set of captions or clues. We do this
by computing feature vectors for each lemma and
using the cosine similarity between these feature
vectors as a lexical similarity function. We con-
struct a word graph with edge weights propor-
tional to the learned similarity of the respective
word pairs.
We use three types of features in these feature
vectors: context word features, context part-of-
speech features, and spelling features. Context
features are the presence of each word in a win-
dow of five words (two words on each side plus the
word in question). Context part-of-speech features
are the part-of-speech labels given by the Stan-
ford POS tagger (Toutanova et al, 2003) within
the same window. Spelling features are the counts
of all character trigrams present in the word.
Table 3 shows examples of similar word pairs
from the set of crossword clues for ?tea?. From
251
Figure 4: Example of natural clusters in a subsection of the word co-occurrence graph for the crossword
clue ?astro?.
Word pair Sim.
(white-gloves, white-glove) 0.74
(may, can) 0.57
(midafternoon, mid-afternoon) 0.55
(company, co.) 0.46
(supermarket, market) 0.53
(pick-me-up, perk-me-up) 0.44
(green, black) 0.44
(lady, earl) 0.39
(kenyan, indian) 0.38
Table 3: Examples of similar pairs of words as cal-
culated on the set of crossword clues for ?tea?.
this table, we can see that this method is able
to successfully identify several similar word pairs
that would be missed by most lexical databases:
minor lexical variations, such as ?pick-me-up? vs.
?perk-me-up?; abbreviations, such as ?company?
and ?co.?; and words that are similar only in this
context, such as ?lady? and ?earl? (referring to
Lady Grey and Earl Grey tea).
4.1.3 Graph Clustering
To cluster the word similarity graph, we use the
Louvain graph clustering method (Blondel et al,
2008), a hierarchical method that optimizes graph
modularity. This method produces several hierar-
chical cluster levels. We use the highest level, cor-
responding to the fewest number of clusters.
Figure 4 shows an example of clusters found
in the word graph for the crossword clue ?as-
tro?. There are three obvious clusters, one for the
Houston Astros baseball team, one for the dog in
the Jetsons cartoon, and one for the lexical prefix
?astro-?. In this example, two of the clusters are
connected by a clue that mentions multiple senses,
?Houston ballplayer or Jetson dog?.
4.1.4 Random Walk Factoid Assignment
After discovering factoids, the remaining task is
to annotate captions according to the factoids they
contain. We approach this problem by taking ran-
dom walks on the word graph constructed in the
previous sections, starting the random walks from
words in the caption and measuring the hitting
times to different clusters.
For each discourse unit, we repeatedly sam-
ple words from it and take Markov random walks
starting from the nodes corresponding to the se-
lected and lasting 10 steps (which is enough to en-
sure that every node in the graph can be reached).
After 1000 random walks, we measure the aver-
age hitting time to each cluster, where a cluster is
considered to be reached by the random walk the
first time a node in that cluster is reached. Heuris-
tically, 1000 random walks was more than enough
to ensure that the factoid distribution had stabi-
lized in development data.
The labels that are applied to a caption are the
labels of the clusters that have a sufficiently low
hitting time. We perform five-fold cross valida-
tion on each caption or set of clues and tune the
threshold on the hitting time such that the aver-
age number of labels per unit produced matches
the average number of labels per unit in the gold
annotation of the held-out portion.
For example, a certain caption may have the fol-
lowing hitting times to the different factoid clus-
ters:
factoid1 0.11
factoid2 0.75
factoid3 1.14
factoid4 2.41
If the held-out portion has 1.2 factoids per cap-
tion, it may be determined that the optimal thresh-
252
old on the hitting times is 0.8, that is, a threshold
of 0.8 produces 1.2 factoids per caption in the test-
set on average. In this case factoid1 and factoid2
would be marked for this caption, since the hitting
times fall below the threshold.
4.2 Clustering
A simple baseline that can act as a surrogate for
factoid annotation is clustering of discourse units,
which is equivalent to assigning exactly one fac-
toid (the name of its cluster) to each discourse
unit. As our clustering method, we use C-Lexrank
(Qazvinian and Radev, 2008), a method that has
been well-tested on collective discourse.
4.3 Topic Model
Topic modeling is a natural way to approach the
problem of factoid annotation, if we consider the
topics to be factoids. We use the Mallet (McCal-
lum, 2002) implementation of Latent Dirichlet Al-
location (LDA) (Blei et al, 2003). As with the ran-
dom walk method, we perform five-fold cross val-
idation, tuning the threshold for the average num-
ber of labels per discourse unit to match the aver-
age number of labels in the held-out portion. Be-
cause LDA needs to know the number of topics
a priori, we set the number of topics to be equal
to the true number of factoids. We also use the
average number of unique factoids in the held-out
portion as the number of LDA topics.
5 Evaluation and Results
We evaluate this task in a way similar to pairwise
clustering evaluation methods, where every pair of
discourse units that should share at least one fac-
toid and does is a true positive instance, every pair
that should share a factoid and does not is a false
negative, etc. From this we are able to calculate
precision, recall, and F1-score. This is a reason-
able evaluation method, since the average number
of factoids per discourse unit is close to one. Be-
cause the factoids discovered by this method don?t
necessarily match the factoids chosen by the an-
notators, it doesn?t make sense to try to measure
whether two discourse units share the ?correct?
factoid.
Tables 4 and 5 show the results of the various
methods on the cartoon captions and crossword
clues datasets, respectively. On the crossword
clues datasets, the random-walk-based methods
are clearly superior to the other methods tested,
whereas simple clustering is more effective on the
Method Prec. Rec. F1
LDA 0.318 0.070 0.115
C-Lexrank 0.131 0.347 0.183
Word co-occurrence graph 0.115 0.348 0.166
Word similarity graph 0.093 0.669 0.162
Table 4: Performance of various methods annotat-
ing factoids for cartoon captions.
Method Prec. Rec. F1
LDA 0.315 0.067 0.106
C-Lexrank 0.702 0.251 0.336
Word co-occurrence graph 0.649 0.257 0.347
Word similarity graph 0.575 0.397 0.447
Table 5: Performance of various methods annotat-
ing factoids for crossword clues.
cartoon captions dataset.
In some sense, the two datasets in this paper
both represent difficult domains, ones in which
authors are intentionally obscure. The good re-
sults acheived on the crossword clues dataset in-
dicate that this obscurity can be overcome when
discourse units are short. Future work in this
vein includes applying these methods to domains,
such as newswire, that are more typical for sum-
marization, and if necessary, investigating how
these methods can best be applied to domains with
longer sentences.
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
Vincent D Blondel, Jean-Loup Guillaume, Renaud
Lambiotte, and Etienne Lefebvre. 2008. Fast un-
folding of communities in large networks. Journal
of Statistical Mechanics: Theory and Experiment,
2008(10):P10008.
Dipanjan Das and Slav Petrov. 2011. Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600?609.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 395?403. As-
sociation for Computational Linguistics.
Leonhard Hennig, Ernesto William De Luca, and Sahin
Albayrak. 2010. Learning summary content units
with topic modeling. In Proceedings of the 23rd
253
International Conference on Computational Lin-
guistics: Posters, COLING ?10, pages 391?399,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method.
Vahed Qazvinian and Dragomir R Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 689?696. Association for Computational Lin-
guistics.
Vahed Qazvinian and Dragomir R Radev. 2011.
Learning from collective human behavior to intro-
duce diversity in lexical choice. In Proceedings of
the 49th annual meeting of the association for com-
putational linguistics: Human language techolo-
gies, pages 1098?1108.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
Hans Van Halteren and Simone Teufel. 2003. Exam-
ining the consensus between human summaries: ini-
tial experiments with factoid analysis. In Proceed-
ings of the HLT-NAACL 03 on Text summarization
workshop-Volume 5, pages 57?64. Association for
Computational Linguistics.
254
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572?577,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A System for Summarizing Scientific Topics Starting from Keywords
Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI, USA
rahuljha@umich.edu
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
Department of EECS and
School of Information
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we investigate the problem
of automatic generation of scientific sur-
veys starting from keywords provided by
a user. We present a system that can take
a topic query as input and generate a sur-
vey of the topic by first selecting a set
of relevant documents, and then selecting
relevant sentences from those documents.
We discuss the issues of robust evalua-
tion of such systems and describe an eval-
uation corpus we generated by manually
extracting factoids, or information units,
from 47 gold standard documents (surveys
and tutorials) on seven topics in Natural
Language Processing. We have manually
annotated 2,625 sentences with these fac-
toids (around 375 sentences per topic) to
build an evaluation corpus for this task.
We present evaluation results for the per-
formance of our system using this anno-
tated data.
1 Introduction
The rise of the number of publications in all sci-
entific fields is making it more and more difficult
to get quickly acquainted with the new develop-
ments in a new area. One way to wade through this
huge amount of scholarly information is to consult
topical surveys written by experts in an area. For
example, for machine translation, one might read
(Lopez, 2008)1. Such surveys can be very help-
ful when available, but unfortunately, may not be
available for all areas. Additionally, the manual
surveys quickly go out of date within a few years
of publication as additional papers are published
in the field.
1Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv. 40, 3, Article 8
Thus, a system that can generate such surveys
automatically would be a useful tool. Short sum-
maries in the form of abstracts are available for
individual papers, but no such information is avail-
able for scientific topics. In this paper, we ex-
plore strategies for generating and evaluating such
surveys of scientific topics automatically starting
from a phrase representing a topic area. We evalu-
ate our system on a set of topics in the field of Nat-
ural Language Processing. In earlier work, (Teufel
and Moens, 2002) have examined the problem
of summarizing scientific articles using rhetorical
analysis of sentences. Nanba and Okumura (1999)
have also discussed the problem of generating sur-
veys of multiple papers. Mohammad et al (2009)
presented experiments on generating surveys of
scientific topics starting from papers to be summa-
rized. More recently, Hoang and Kan (2010) have
presented initial results on automatically generat-
ing related work section for a target paper by tak-
ing a hierarchical topic tree as an input.
In this paper, we tackle the more challenging
problem of summarizing a topic starting from a
topic query. Our system takes as an input a string
describing the topic area, selects the relevant pa-
pers from a corpus of papers, and then selects sen-
tences from the citing sentences to these papers to
generate a survey of the topic. A sample output of
our system for the topic of ?Word Sense Disam-
biguation? is shown in Figure 1.
2 Candidate Document Selection
Given a query representing the topic to be sum-
marized, our first task is to find the set of rele-
vant documents from the corpus. The simplest
way to do this for a corpus of scientific publica-
tions is to do a query search using exact match or
a standard TF*IDF system such Lucene, rank the
documents using either citation counts or pager-
ank in the bibliometric citation network, and se-
lect the top n documents. However, comparing
572
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a
tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al , 1998), and semi-supervised sense disambiguation
(Yarowsky, 1995).
Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al , 1993) to train statistical
classifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model.
Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation.
Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002;
Mihalcea and Moldovan, 2001; Yarowsky et al , 2001).
For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al , 1991; Dagan, 1991; Dagan and Itai, 1994;
Ide, 2000; Resnik and Yarowsky, 1999).
Figure 1: A sample output survey of our system on the topic of ?Word Sense Disambiguation? produced
by paper selection using Restricted Expansion and sentence selection using Lexrank. In our evaluations,
this survey achieved a pyramid score of 0.82 and Unnormalized RU score of 0.31.
Document selection algorithm CG5 CG10 CG20
Title match sorted with citation count 1.82 2.75 3.29
Title match sorted with pagerank 1.77 2.55 3.34
Citation expansion sorted with citation
count 0.53 1.20 2.29
Citation expansion sorted with pagerank 0.20 0.78 1.99
TF*IDF ranked 0.14 0.14 0.56
TF*IDF sorted with citation count 0.44 2.25 3.18
TF*IDF sorted with pagerank 1.54 2.22 2.85
Restricted Expansion 2.52 3.91 6.01
Table 1: Comparison of different methods for
document selection by measuring the Cumulative
Gain (CG) of top 5, 10 and 20 results.
the results of these techniques with the papers cov-
ered by gold standard surveys on a few topics, we
found that some important papers are missed by
these simple approaches. One reason for this is
that early papers in a field might use non-standard
terms in the absence of a stable, accepted termi-
nology. Some early Word Sense Disambiguation
papers, for example, refer to the problem as Lex-
ical Ambiguity Resolution. Additionally, papers
might use alternative forms or abbreviations of
topics in their titles and abstracts, e.g. for input
query ?Semantic Role Labelling?, papers such as
(Dahlmeier et al, 2009) titled ?Joint Learning of
Preposition Senses and Semantic Roles of Prepo-
sitional Phrases? and (Che and Liu, 2010) titled
?Jointly Modeling WSD and SRL with Markov
Logic? might be missed.
To find these papers, we add a simple heuristic
called Restricted Expansion. In this method, we
first create a base set B, by finding papers with an
exact match to the query. This is a high precision
set since a paper with a title that contains the ex-
act query phrase is very likely to be relevant to the
topic. We then find additional papers by expand-
ing in the citation network around B, that is, by
finding all the papers that are cited by or cite the
papers in B, to create an extended set E. From
this combined set (B ?E), we create a new set F
by filtering out the set of papers that are not cited
by or cite a minimum threshold tinit of papers in
B. If the total number of papers is lower than fmin
or higher than fmax, we iteratively increase or de-
crease t till fmin ? |F | ? fmax. This method
allows us to increase our recall without losing pre-
cision. The values for our current experiments are:
tinit = 5, fmin = 150, fmax = 250.
Authors Year Size
Surveys
ACL Wiki 2012 4
Roberto Navigli 2009 68
Eneko Agirre; Philip Edmonds 2006 28
Xiaohua Zhou; Hyoil Han 2005 6
Nancy Ide; Jean Vronis 1998 41
Tutorials
Sanda Harabagiu 2011 45
Diana McCarthy 2011 120
Philipp Koehn 2008 17
Rada Mihalcea 2005 186
Table 2: The set of surveys and tutorials col-
lected for the topic of ?Word Sense Disambigua-
tion?. Sizes for surveys are expressed in number
of pages, sizes for tutorials are expressed in num-
ber of slides.
To evaluate different methods of candidate doc-
ument selection, we use Cumulative Gain (CG),
where the weight for each paper is estimated by
the fraction of surveys it appears in. Table 1
shows the average Cumulative Gain of top 5, 10
and 20 documents for each of eight methods we
tried. Restricted Expansion outperformed every
other method. Once we obtain a set of papers to
be summarized, we select the top n most cited pa-
pers in the document set as the papers to be sum-
marized, and extract the set of citing sentences S
from all the papers in the document set to these n
papers. S is the input for our sentence selection
algorithms, described in Section 4.
573
Factoid S1 S2 S3 S4 S5 T1 T2 T3 T4 Factoid Weight
definition of wsd X X X X X X X X X 9
wordnet X X X X X X X X 8
knowledge based wsd X X X X X X X 7
supervised wsd X X X X X X X 7
senseval X X X X X X X 7
definition of word senses X X X X X X 7
knowledge based wsd using machine readable dictionaries X X X X X X 6
unsupervised wsd X X X X X X 6
bootstrapping algorithms X X X X X X 6
supervised wsd using decision lists X X X X X X 6
Table 3: Top 10 factoids for the topic of ?Word Sense Disambiguation? and their distribution across
various data sources.
3 Evaluation Data for Survey Generation
We use the ACL Anthology Network (AAN) as the
corpus for our experiments (Radev et al, 2013).
We built a factoid inventory for seven topics in
NLP based on manual written surveys in the fol-
lowing way. For each topic, we found at least 3
recent tutorials and 3 recent surveys on the topic
and extracted the factoids that are covered in each
of them. Table 2 shows the complete list of ma-
terial collected for the topic of ?Word Sense Dis-
ambiguation?. We found around 80 factoids per
topic on an average. Once the factoids were ex-
tracted, each factoid was assigned a weight based
on the number of documents it appears in, and any
factoids with weight one were removed. Table 3
shows the top ten factoids in the topic of Word
Sense Disambiguation along with their distribu-
tion across the different surveys and tutorials and
final weight.
For each of the topics, we used the method de-
scribed in Section 2 to create a candidate docu-
ment set and extracted the candidate citing sen-
tences to be used as the input for the content se-
lection component. Each sentence in each topic
was then annotated by a human judge against the
factoid list for that topic. A sentence is allowed
to have zero or more than one factoid. The human
assessors were graduate students in Computer Sci-
ence who have taken a basic ?Natural Language
Processing? course or an equivalent course. On an
average, 375 citing sentences were annotated for
each topic, with 2,625 sentences being annotated
in total. We present all our experimental results on
this large annotated corpora which is also available
for download 2.
4 Content Models
Once we have the set of input sentences, our sys-
tem must select the sentences that should be part
2http://clair.si.umich.edu/corpora/survey data/
of the survey. For this task, we experimented with
three content models, described below.
4.1 Centroid
The centroid of a set of documents is a set of words
that are statistically important to the cluster of doc-
uments. Centroid based summarization of a docu-
ment set involves first creating the centroid of the
documents, and then judging the salience of each
document based on its similarity to the centroid
of the document set. In our case, the input citing
sentences represent the documents from which we
extract the centroid. We use the centroid imple-
mentation from the publicly available summariza-
tion toolkit, MEAD (Radev et al, 2004).
4.2 Lexrank
LexRank (Erkan and Radev, 2004) is a network
based content selection algorithm that works by
first building a graph of all the documents in a
cluster. The edges between corresponding nodes
represent the cosine similarity between them.
Once the network is built, the algorithm computes
the salience of sentences in this graph based on
their eigenvector centrality in the network.
4.3 C-Lexrank
C-Lexrank is another network based content selec-
tion algorithm that focuses on diversity (Qazvinian
and Radev, 2008). Given a set of sentences, it first
creates a network using these sentences and then
runs a clustering algorithm to partition the net-
work into smaller clusters that represent different
aspects of the paper. The motivation behind the
clustering is to include more diverse facts in the
summary.
5 Experiments and Results
To do an evaluation of our different content selec-
tion methods, we first select the documents using
our Restricted Expansion method, and then pick
574
Topic Rand Cent LR C-LR
Summarization 0.68 0.61 0.91 0.82
Question Answering 0.52 0.50 0.65 0.56
Word Sense Disambiguation 0.78 0.73 0.82 0.76
Named Entity Recognition 0.90 0.90 0.94 0.94
Sentiment Analysis 0.75 0.78 0.77 0.78
Semantic Role Labeling 0.78 0.79 0.88 0.94
Dependency Parsing 0.67 0.38 0.71 0.53
Average 0.72 0.68 0.81? 0.76
Table 4: Results of pyramid evaluation for each
of the three methods and the random baseline on
each topic.
the citing sentences to be used as the input to the
summarization module as described in Section 2.
Given this input, we generate 500 word summaries
for each of the seven topics using the four meth-
ods: Centroid, Lexrank, C-Lexrank and a random
baseline.
For each summary, we compute two evaluation
metrics. The first is the Pyramid score (Nenkova
and Passonneau, 2004) computed by treating the
factoids as Summary Content Units (SCU?s). The
Pyramid scores for each summary is shown in Ta-
ble 4. The second metric is an Unnormalized Rel-
ative Utility score (Radev and Tam, 2003), com-
puted using the factoid scores of sentences based
on the method presented in (Qazvinian, 2012). We
call this Unnormalized RU since we are not able to
normalize the scores with human generated gold
summaries. The results for Unnormalized RU are
shown in Table 5. The parameter ? is the RU
penalty for including a redundant sentence sub-
sumed by an earlier sentence. If the summary
chooses a sentence si with score worig that is sub-
sumed by an earlier summary sentence, the score
is reduced as wsubsumed = (? ? worig). We ap-
proximate subsumption by marking a sentence sj
as being subsumed by si if Fj ? Fi, where Fi and
Fj are sets of factoids covered in each sentence.
Topic Rand Cent LR C-LR
Summarization 0.16 0.57 0.29 0.17
Question Answering 0.32 0.39 0.48 0.30
Word Sense Disambiguation 0.28 0.33 0.31 0.30
Named Entity Recognition 0.36 0.38 0.34 0.31
Sentiment Analysis 0.23 0.34 0.48 0.33
Semantic Role Labeling 0.11 0.17 0.16 0.21
Dependency Parsing 0.16 0.05 0.30 0.15
Average 0.23 0.32 0.34? 0.25
Table 5: Results of Unnormalized Relative Utility
evaluation for the three methods and random base-
line using ? = 0.5.
The reason for the relatively high scores for the
random baseline is that our process to select the
initial set of sentences eliminates many bad sen-
tences. For example, for a subset of 5 topics,
the total input set contains 1508 sentences, out of
which 922 of the sentences (60%) have at least one
factoid. This makes it highly likely to pick good
content sentences even when we are picking sen-
tences at random.
We find that the Lexrank method outperforms
other sentence selection methods on both evalua-
tion metrics. The higher performance of Lexrank
compared to Centroid is consistent with earlier
published results (Erkan and Radev, 2004). The
reason for the low performance of C-Lexrank as
compared to Lexrank on this data set can be at-
tributed to the fact that the input sentence set is
derived from a much more diverse set of papers
which can have a high diversity in lexical choice
when describing the same factoid. Thus simple
lexical similarity is not enough to find good clus-
ters in this sentence set.
The lower Unnormalized RU scores compared
to Pyramid scores indicate that we are selecting
sentences containing highly weighted factoids, but
we do not select the most informative sentences
that contain a large number of factoids. This
also shows that we select some redundant factoids,
since Unnormalized RU contains a penalty for re-
dundancy. This is again, explained by the fact
that the simple lexical diversity based model in C-
Lexrank is not able to detect the same factoids be-
ing present in two sentences. Despite these short-
comings, our system works quite well in terms
of content selection for unseen topics, Figure 2
shows the top 5 sentences for the query ?Condi-
tional Random Fields?.
6 Conclusion and Future Work
In this paper, we described a pipeline for the gen-
eration of scientific surveys starting from a topic
query. Our system is divided into two components.
The first component finds the set of papers from
the corpus relevant to the query using a simple
heuristic called Restricted Expansion. The second
component selects sentences from these papers to
generate a survey of the topic. One of the main
contributions of this work is a manually annotated
data set for evaluating both the tasks. We collected
47 gold standard documents (surveys and tutori-
als) on seven topics in Natural Language Process-
ing and extracted factoids for each topic. Each
factoid is given an importance score based on the
number of gold standard documents it appears in.
575
In recent years, conditional random fields (CRFs) (Lafferty et al , 2001)
have shown success on a number of natural language processing (NLP)
tasks, including shallow parsing (Sha and Pereira, 2003), named entity
recognition (McCallum and Li, 2003) and information extraction from
research papers (Peng and McCallum, 2004).
In natural language processing, two aspects of CRFs have been
investigated sufficiently: one is to apply it to new tasks, such as named
entity recognition (McCallum and Li, 2003; Li and McCallum, 2003;
Settles, 2004), part-of-speech tagging (Lafferty et al, 2001), shallow
parsing (Sha and Pereira, 2003), and language modeling (Roark et al,
2004); the other is to exploit new training methods for CRFs, such as
improved iterative scaling (Lafferty et al, 2001), L-BFGS (McCallum,
2003) and gradient tree boosting (Dietterich et al, 2004)
NP chunks are very similar to the ones of Ramshaw and Marcus (1995).
CRFs have shown empirical successes recently in POS tagging (Lafferty
et al , 2001), noun phrase segmentation (Sha and Pereira, 2003) and
Chinese word segmentation (McCallum and Feng, 2003)
CRFs have been successfully applied to a number of real-world tasks,
including NP chunking (Sha and Pereira, 2003), Chinese word
segmentation (Peng et al, 2004), information extraction (Pinto et al,
2003; Peng and McCallum, 2004), named entity identification (McCallum
and Li, 2003; Settles, 2004), and many others.
Figure 2: A sample output survey produced by
our system on the topic of ?Conditional Random
Fields? using Restricted Expansion and Lexrank.
Additionally, we manually annotated 2,625 input
sentences, about 375 sentences per topic, with the
factoids extracted from the gold standard docu-
ments for each topic. Using this corpus, we pre-
sented experimental results for the performance of
our document selection component and three sen-
tence selection strategies.
Our results indicate three main directions for
future work. We plan to look at better models
of diversity in sentence selection, since methods
based on simple lexical similarity do not seem to
work well. The low factoid recall shown by low
unnormalized RU scores suggests integrating the
full text of papers with citation based summaries
which might help us find factoids such as topic
definitions that are unlikely to be present in citing
sentences. A final goal would be to improve the
readability and coherence of our system output.
Acknowledgments
We thank Vahed Qazvinian, Wanchen Lu, Ben
King, and Shiwali Mohan for extremely useful
discussions and help with the data annotation.
This research is supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
References
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ?10,
pages 427?435, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 584?592, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the 16th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-99), pages 926?931.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics - Human Language Technologies (HLT-
NAACL ?04).
Vahed Qazvinian and Dragomir R. Radev. 2008.
Scientific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING-08), Manchester, UK.
Vahed Qazvinian. 2012. Using Collective Discourse
to Generate Surveys of Scientific Paradigms. Ph.D.
thesis.
Dragomir R. Radev and Daniel Tam. 2003. Sum-
marization evaluation using relative utility. In
CIKM2003, pages 508?511.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
576
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources
and Evaluation, pages 1?26.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
577
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 829?835,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identifying Opinion Subgroups in Arabic Online Discussions
Amjad Abu-Jbara
Department of EECS
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Mona Diab
Department of Computer Science
George Washington University
Washington DC, USA
mtdiab@gwu.edu
Ben King
Department of EECS
University of Michigan
Ann Arbor, MI, USA
benking@umich.edu
Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we use Arabic natural lan-
guage processing techniques to analyze
Arabic debates. The goal is to identify
how the participants in a discussion split
into subgroups with contrasting opinions.
The members of each subgroup share the
same opinion with respect to the discus-
sion topic and an opposing opinion to
the members of other subgroups. We
use opinion mining techniques to identify
opinion expressions and determine their
polarities and their targets. We opinion
predictions to represent the discussion in
one of two formal representations: signed
attitude network or a space of attitude vec-
tors. We identify opinion subgroups by
partitioning the signed network represen-
tation or by clustering the vector space
representation. We evaluate the system us-
ing a data set of labeled discussions and
show that it achieves good results.
1 Introduction
Arabic is one of the fastest growing languages
on the internet. The number of internet users in
the Arab region grew by 2500% over the past 10
years. As of January 2012, the number of Arabic-
speaking internet users was 86 millions. The re-
cent political and civic movements in the Arab
World resulted in a revolutionary growth in the
number of Arabic users on social networking sites.
For example, Arabic is the fastest growing lan-
guage in Twitter history 1.
This growth in the presence of Arab users on
social networks and all the interactions and dis-
cussions that happen among them led to a huge
amount of opinion-rich Arabic text being avail-
able. Analyzing this text could reveal the different
viewpoints of Arab users with respect to the topics
that they discuss online.
When a controversial topic is discussed, it is
normal for the discussants to adopt different view-
points towards it. This usually causes rifts in dis-
cussion groups and leads to the split of the dis-
cussants into subgroups with contrasting opinions.
Our goal in this paper is to use natural language
processing techniques to detect opinion subgroups
in Arabic discussions. Our approach starts by
identifying opinionated (subjective) text and deter-
mining its polarity (positive, negative, or neutral).
Next, we determine the target of each opinion ex-
pression. The target of opinion can be a named
entity mentioned in the discussion or an aspect of
the discussed topic. We use the identified opinion-
target relations to represent the discussion in one
of two formal representations. In the first repre-
sentation, each discussant is represented by a vec-
tor that encodes all his or her opinion information
towards the discussion topic. In the second repre-
sentation, each discussant is represented by a node
in a signed graph. A positive edge connects two
discussants if they have similar opinion towards
the topic, otherwise the sign of the edge is nega-
1http://semiocast.com/publications/
2011_11_24_Arabic_highest_growth_on_
Twitter
829
tive. To identify opinion subgroups, we cluster the
vector space (the first representation) or partition
the signed network (the second representation).
We evaluate this system using a data set of Ara-
bic discussions collected from an Arabic debating
site. We experiment with several variations of the
system. The results show that the clustering the
vector space representation achieves better results
than partitioning the signed network representa-
tion.
2 Previous Work
Our work is related to a large body of research on
opinion mining and sentiment analysis. Pang &
Lee (2008) and Liu & Zhang (2012) wrote two re-
cent comprehensive surveys about sentiment anal-
ysis and opinion mining techniques and applica-
tions.
Previous work has proposed methods for iden-
tifying subjective text that expresses opinion
and distinguishing it from objective text that
presents factual information (Wiebe, 2000; Hatzi-
vassiloglou and Wiebe, 2000a; Banea et al, 2008;
Riloff and Wiebe, 2003).
Subjective text may express positive, negative,
or neutral opinion. Previous work addressed the
problem of identifying the polarity of subjective
text (Hatzivassiloglou and Wiebe, 2000b; Hassan
et al, 2010; Riloff et al, 2006). Many of the pro-
posed methods for text polarity identification de-
pend on the availability of polarity lexicons (i.e.
lists of positive and negative words). Several ap-
proaches have been devised for building such lex-
icons (Turney and Littman, 2003; Kanayama and
Nasukawa, 2006; Takamura et al, 2005; Hassan
and Radev, 2010). Other research efforts focused
on identifying the holders and the targets of opin-
ion (Zhai et al, 2010; Popescu and Etzioni, 2007;
Bethard et al, 2004).
Opinion mining and sentiment analysis tech-
niques have been used in various applications.
One example of such applications is identifying
perspectives (Grefenstette et al, 2004; Lin et al,
2006) which is most similar to the topic of this
paper. For example, in (Lin et al, 2006), the au-
thors experiment with several supervised and sta-
tistical models to capture how perspectives are ex-
pressed at the document and the sentence levels.
Laver et al (2003) proposed a method for extract-
ing perspectives from political texts. They used
their method to estimate the policy positions of po-
litical parties in Britain and Ireland, on both eco-
nomic and social policy dimensions.
Somasundaran and Wiebe (2009) present an un-
supervised opinion analysis method for debate-
side classification. They mine the web to learn
associations that are indicative of opinion stances
in debates and combine this knowledge with dis-
course information. Anand et al (2011) present a
supervised method for stance classification. They
use a number of linguistic and structural fea-
tures such as unigrams, bigrams, cue words, re-
peated punctuation, and opinion dependencies to
build a stance classification model. In previous
work, we proposed a method that uses participant-
to-participant and participant-to-topic attitudes to
identify subgroups in ideological discussions us-
ing attitude vector space clustering (Abu-Jbara and
Radev, 2012). In this paper, we extend this method
by adding latent similarity features to the attitude
vectors and applying it to Arabic discussions. In
another previous work, our group proposed a su-
pervised method for extracting signed social net-
works from text (Hassan et al, 2012a). The
signed networks constructed using this method
were based only on participant-to-participant at-
titudes that are expressed explicitly in discussions.
We used this method to extract signed networks
from discussions and used a partitioning algo-
rithm to detect opinion subgroups (Hassan et al,
2012b). In this paper, we extend this method by
using participant-to-topic attitudes to construct the
signed network.
Unfortunately, not much work has been done
on Arabic sentiment analysis and opinion min-
ing. Abbasi et al (2008) applies sentiment anal-
ysis techniques to identify and classify document-
level opinions in text crawled from English and
Arabic web forums. Hassan et al (2011) pro-
posed a method for identifying the polarity of non-
English words using multilingual semantic graphs.
They applied their method to Arabic and Hindi.
Abdul-Mageed and Diab (2011) annotated a cor-
pus of Modern Standard Arabic (MSA) news text
for subjectivity at the sentence level. In a later
work (2012a), they expanded their corpus by la-
830
beling data from more genres using Amazon Me-
chanical Turk. Abdul-Mageed et al (2012a) de-
veloped SAMAR, a system for subjectivity and
Sentiment Analysis for Arabic social media gen-
res. We use this system as a component in our
approach.
3 Approach
In this section, we present our approach to de-
tecting opinion subgroups in Arabic discussions.
We propose a pipeline that consists of five com-
ponents. The input to the pipeline is a discussion
thread in Arabic language crawled from a discus-
sion forum. The output is the list of participants
in the discussion and the subgroup membership of
each discussant. We describe the components of
the pipeline in the following subsections.
3.1 Preprocessing
The input to this component is a discussion thread
in HTML format. We parse the HTML file to iden-
tify the posts, the discussants, and the thread struc-
ture. We transform the Arabic content of the posts
and the discussant names that are written in Arabic
to the Buckwalter encoding (Buckwalter, 2004).
We use AMIRAN (Diab, 2009), a system for pro-
cessing Arabic text, to tokenize the text and iden-
tify noun phrases.
3.2 Identifying Opinionated Text
To identify opinion-bearing text, we start from the
word level. We identify the polarized words that
appear in text by looking each word up in a lexicon
of Arabic polarized words. In our experiments, we
use Sifat (Abdul-Mageed and Diab, 2012b), a lex-
icon of 3982 Arabic adjectives labeled as positive,
negative, or neutral.
The polarity of a word may be dependant on
its context (Wilson et al, 2005). For example,
a positive word that appears in a negated context
should be treated as expressing negative opinion
rather than positive. To identify the polarity of
a word given the sentence it appears in, we use
SAMAR (Abdul-Mageed et al, 2012b), a system
for Subjectivity and Sentiment Analysis for Ara-
bic social media genres. SAMAR labels a sen-
tence that contains an opinion expression as pos-
itive, negative, or neutral taking into account the
context of the opinion expression. The reported
accuracy of SAMAR on different data sets ranges
between 84% and 95% for subjectivity classifica-
tion and 65% and 81% for polarity classification.
3.3 Identifying Opinion Targets
In this step, we determine the targets that the opin-
ion is expressed towards. We treat as an opin-
ion target any noun phrase (NP) that appears in
a sentence that SAMAR labeled as polarized (pos-
itive or negative) in the previous step. To avoid
the noise that may result from including all noun
phrases, we limit what we consider as an opinion
target, to the ones that appear in at least two posts
written by two different participants. Since, the
sentence may contain multiple possible targets for
every opinion expression, we associate each opin-
ion expression with the target that is closest to it in
the sentence. For each discussant, we keep track
of the targets mentioned in his/her posts and the
number of times each target was mentioned in a
positive/negative context.
3.4 Latent Textual Similarity
If two participants share the same opinion, they
tend to focus on similar aspects of the discus-
sion topic and emphasize similar points that sup-
port their opinion. To capture this, we follow
previous work (Guo and Diab, 2012; Dasigi et
al., 2012) and apply Latent Dirichelet Allocation
(LDA) topic models to the text written by the dif-
ferent participants. We use an LDA model with
100 topics. So, we represent all the text written
in the discussion by each participant as a vector
of 100 dimensions. The vector of each participant
contains the topic distribution of the participant, as
produced by the LDA model.
3.5 Subgroup Detection
At this point, we have for every discussant the tar-
gets towards which he/she expressed explicit opin-
ion and a 100-dimensions vector representing the
LDA distribution of the text written by him/her.
We use this information to represent the discussion
in two representations. In the first representation,
each discussant is represented by a vector. For ev-
ery target identified in steps 3 of the pipeline, we
add three entries in the vector. The first entry holds
the total number of times the target was mentioned
by the discussant. The second entry holds the
831
 ??? ???? ???? ?? ?????? ????? ???? ?????? ???? ???? ?????
?????? ??????? ?????? ?????? ?? ????? ???? ??? ???? ?? ??????? 
 ?????? ???? ??? ???? ?????? ??????? ????? ??? ???? ?????? ???????
 ?? ?? ?????? ???? ???? ???? ???? ??????? ???? ??????? ??????
 ????? ??? ?????? ????? ?? ????? ???? ??  ??????? ?? ?????
???? ?? ????? ?????? ?????? ??? ??? ?? ???? ?? ???? ??? ???? 
 ???? ??? ??? ???  ?????????? ???? ???? ??? ??? ?? ????
 ???????? ?????? ?? ?????? ??????? ?? ?? ???? ???????
 ?????? ???????? ???????? ??? ????? ?????? ?? ?????
????? ??? ???? ??? ???? ???????? ??? 
(a) 
(c) (b) 
Figure 1: An example debate taken from our dataset. (a) is the discussion topic. (b) and (c) are two posts
expressing contrasting viewpoints with respect to the topic.
number of times the target was mentioned in a pos-
itive context. The third entry holds the number of
target mentions in a negative context. We also add
to this vector the 100 topic entries from the LDA
vector of that discussant. So, if the number of tar-
gets identified in step 3 of the pipeline is t then
the number of entries in the discussant vector is
3 ? t+ 100.
To identify opinion subgroups, we cluster
the vector space. We experiment with several
clustering algorithms including K-means (Mac-
Queen, 1967), Farthest First (FF) (Hochbaum and
Shmoys, 1985; Dasgupta, 2002), and Expectation
Maximization (EM) (Dempster et al, 1977).
The second representation is a signed network
representation. In this representation, each dis-
cussant is represented by a node in a graph. Two
discussants are connected by an edge if they both
mention at least one common target in their posts.
If a discussant mentions a target multiple times in
different contexts with different polarities, the ma-
jority polarity is assumed as the opinion of this
discussant with respect to this target. A positive
sign is assigned to the edge connecting two discus-
sants if the number of targets that they have simi-
lar opinion towards is greater than the targets that
they have opposing opinion towards, otherwise a
negative sign is assigned to the edge.
To identify subgroups, we use a signed net-
work partitioning algorithm to partition the net-
work. Each resulting partition constitutes a sub-
group. Following (Hassan et al, 2012b), we use
the Dorian-Mrvar (1996) algorithm to partition the
signed network. The optimization criterion aims
to have dense positive links within groups and
dense negative links between groups.
The optimization function is as follows:
F (C) = ?? |NEG|+ (1? ?)? |POS| (1)
where C is the clustering under evaluation,
|NEG| is the number of negative links between
nodes in the same subgroup, |POS| is the number
of positive links between nodes in different sub-
groups, and ? is a parameter that specifies impor-
tance of the two terms. We set ? to 0.5 in all our
experiments.
Clusters are selected such that P (C) is mini-
mum. The best clustering that minimizes P (C) is
found by moving nodes around clusters in a greedy
way starting with a random clustering. To han-
dle the possibility of finding a local minima, the
whole process is repeated k times with random
restarts and the clustering with the minimum value
of P (C) is returned. We set k to 3 in all our ex-
periments.
4 Data
We use data from an Arabic discussion forum
called Naqeshny.2. Naqeshny is a platform for
two-sided debates. The debate starts when a per-
son asks a question (e.g. which political party do
you support?) and gives two possible answers or
positions. The registered members of the web-
site who are interested in the topic participate in
the debate by selecting a position and then post-
ing text to support that position and dispute the
2www.Naqeshny.com
832
opposing position. This means that the data set is
self-labeled for subgroup membership. Since the
tools used in our system are trained on Modern
Standard Arabic (MSA) text, we selected debates
that are mostly MSA. The data set consists of 36
debates comprising a total of 711 posts written by
326 users. The average number of posts per dis-
cussion is 19.75 and the average number of partic-
ipants per discussion is 13.08. Figure 1 shows an
example from the data.
5 Experiments and Results
We use three metrics to evaluate the resulting sub-
groups: Purity (Manning et al, 2008), Entropy,
and F-measure. We ran several variations of the
system on the data set described in the previous
section. In one variation, we use the signed net-
work partitioning approach to detect subgroups.
In the other variations, we use the vector space
clustering approach. We experiment with differ-
ent clustering algorithms. We also run two experi-
ments to evaluate the contribution of both opinion-
target counts and latent similarity features on the
clustering accuracy. In one run, we use target-
opinion counts only. In the other run, we use latent
similarity features only. EM was used as the clus-
tering algorithm in these two runs. Table 1 shows
the results. All the results have been tested for sta-
tistical significance using a 2-tailed paired t-test.
The differences between the results of the different
methods shown in the table are statistically signif-
icant at the 0.05 level. The results show that the
clustering approach achieves better results than the
signed network partitioning approach. This can be
explained by the fact that the vector representa-
tion is a richer representation and encodes all the
discussants? opinion information explicitly. The
results also show that Expectation Maximization
achieves significantly better results than the other
clustering algorithms that we experimented with.
The results also show that both latent text similar-
ity and opinion-target features are important and
contribute to the performance.
6 Conclusion
In this paper, we presented a system for iden-
tifying opinion subgroups in Arabic online dis-
cussions. The system uses opinion and text sim-
System Purity F-Measure Entropy
Signed Network 0.71 0.67 0.68
Clustering - K-means 0.72 0.70 0.67
Clustering - EM 0.77 0.76 0.50
Clustering - FF 0.72 0.69 0.70
Opinion-Target Only 0.67 0.65 0.72
Text Similarity Only 0.64 0.65 0.74
Table 1: Comparison of the different variations of
the proposed approach
ilarity features to encode discussants? opinions.
Two approaches were explored for detecting sub-
groups. The first approach clusters a space of dis-
cussant opinion vectors. The second approach par-
titions a signed network representation of the dis-
cussion. Our experiments showed that the former
approach achieves better results. Our experiments
also showed that both opinion and similarity fea-
tures are important.
Acknowledgements
This research was funded in part by the Office of
the Director of National Intelligence, Intelligence
Advanced Research Projects Activity. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the of?cial views or poli-
cies of IARPA, the ODNI or the U.S. Government.
The authors would like to thank Basma Siam for
her help with collecting the data.
References
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Trans. Inf. Syst., 26(3):12:1?12:34,
June.
Muhammad Abdul-Mageed and Mona Diab. 2011.
Subjectivity and sentiment annotation of modern
standard arabic newswire. In Proceedings of the
5th Linguistic Annotation Workshop, pages 110?
118, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Muhammad Abdul-Mageed and Mona Diab. 2012a.
Awatif: A multi-genre corpus for modern standard
arabic subjectivity and sentiment analysis. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Ugur Dogan, Bente
Maegaard, Joseph Mariani, Jan Odijk, and Stelios
833
Piperidis, editors, Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, may. European
Language Resources Association (ELRA).
Muhammad Abdul-Mageed and Mona Diab. 2012b.
Toward building a large-scale arabic sentiment lexi-
con. In Proceedings of the 6th International Global
Word-Net Conference, Matsue, Japan.
Muhammad Abdul-Mageed, Sandra Ku?bler, and Mona
Diab. 2012a. Samar: a system for subjectivity
and sentiment analysis of arabic social media. In
Proceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis,
WASSA ?12, pages 19?28, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Muhammad Abdul-Mageed, Sandra Kuebler, and
Mona Diab. 2012b. Samar: A system for subjec-
tivity and sentiment analysis of arabic social me-
dia. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 19?28, Jeju, Korea, July. Associa-
tion for Computational Linguistics.
Amjad Abu-Jbara and Dragomir Radev. 2012. Sub-
group detection in ideological discussions. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Jeju, Korea, July. The Associa-
tion for Computational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2.011), pages 1?9,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC?08.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Auto-
matic extraction of opinion propositions and their
holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text, page 2224.
Tim Buckwalter. 2004. Issues in arabic orthography
and morphology analysis. In Proceedings of the
Workshop on Computational Approaches to Arabic
Script-based Languages, Semitic ?04, pages 31?34,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Sanjoy Dasgupta. 2002. Performance guarantees for
hierarchical clustering. In 15th Annual Conference
on Computational Learning Theory, pages 351?363.
Springer.
Pradeep Dasigi, Weiwei Guo, and Mona Diab. 2012.
Genre independent subgroup detection in online dis-
cussion threads: A study of implicit attitude us-
ing textual latent semantics. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 65?69, Jeju Island, Korea, July. Association
for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. JOURNAL OF THE ROYAL STATIS-
TICAL SOCIETY, SERIES B, 39(1):1?38.
Mona Diab. 2009. Second generation tools (amira
2.0): Fast and robust tokenization, pos tagging, and
base phrase chunking. In Khalid Choukri and Bente
Maegaard, editors, Proceedings of the Second Inter-
national Conference on Arabic Language Resources
and Tools, Cairo, Egypt, April. The MEDAR Con-
sortium.
Patrick Doreian and Andrej Mrvar. 1996. A partition-
ing approach to structural balance. Social Networks,
18(2):149?168.
Gregory Grefenstette, Yan Qu, James G Shanahan, and
David A Evans. 2004. Coupling niche browsers
and affect analysis for an opinion mining applica-
tion. In Proceedings of RIAO, volume 4, pages 186?
194. Citeseer.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
864?872, Jeju Island, Korea, July. Association for
Computational Linguistics.
Ahmed Hassan and Dragomir Radev. 2010. Identify-
ing text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir
Radev. 2010. What?s with the attitude?: identi-
fying sentences with attitude in online discussions.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1245?1255.
Ahmed Hassan, Amjad Abu-Jbara, Rahul Jha, and
Dragomir Radev. 2011. Identifying the semantic
orientation of foreign words. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, HLT ?11, pages 592?
597, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012a. Extracting signed social networks
from text. In Workshop Proceedings of TextGraphs-
7: Graph-based Methods for Natural Language Pro-
cessing, pages 6?14, Jeju, Republic of Korea, July.
Association for Computational Linguistics.
834
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012b. Signed attitude networks: Predict-
ing positive and negative links using linguistic anal-
ysis. In Submitted to the Conference on Emprical
Methods in Natural Language Processing, Jeju, Ko-
rea, July. The Association for Computational Lin-
guistics.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000a.
Effects of adjective orientation and gradability on
sentence subjectivity. In COLING, pages 299?305.
Vasileios Hatzivassiloglou and Janyce M Wiebe.
2000b. Effects of adjective orientation and grad-
ability on sentence subjectivity. In Proceedings of
the 18th conference on Computational linguistics-
Volume 1, pages 299?305. Association for Compu-
tational Linguistics.
Hochbaum and Shmoys. 1985. A best possible heuris-
tic for the k-center problem. Mathematics of Oper-
ations Research, 10(2):180?184.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. In EMNLP?06, pages
355?363.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(02):311?331.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
pages 109?116. Association for Computational Lin-
guistics.
Bing Liu and Lei Zhang. 2012. A survey of opin-
ion mining and sentiment analysis. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 415?463. Springer US.
J. B. MacQueen. 1967. Some methods for classifi-
cation and analysis of multivariate observations. In
L. M. Le Cam and J. Neyman, editors, Proc. of the
fifth Berkeley Symposium on Mathematical Statistics
and Probability, volume 1, pages 281?297. Univer-
sity of California Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schtze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Orena Etzioni. 2007. Extract-
ing product features and opinions from reviews. In
Natural language processing and text mining, pages
9?28. Springer.
Ellen Riloff and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP?03, pages 105?112.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis.
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, pages
440?448. Association for Computational Linguis-
tics.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226?234, Suntec, Singapore, August.
Association for Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In ACL?05, pages 133?140.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence and
Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 735?740.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05, Van-
couver, Canada.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of the
23rd International Conference on Computational
Linguistics, pages 1272?1280. Association for Com-
putational Linguistics.
835
Heterogeneous Networks and Their Applications: Scientometrics, Name
Disambiguation, and Topic Modeling
Ben King, Rahul Jha
Department of EECS
University of Michigan
Ann Arbor, MI
{benking,rahuljha}@umich.edu
Dragomir R. Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor, MI
radev@umich.edu
Abstract
We present heterogeneous networks as a way to
unify lexical networks with relational data. We
build a unified ACL Anthology network, tying
together the citation, author collaboration, and
term-cooccurence networks with affiliation and
venue relations. This representation proves to
be convenient and allows problems such as name
disambiguation, topic modeling, and the mea-
surement of scientific impact to be easily solved
using only this network and off-the-shelf graph
algorithms.
1 Introduction
Graph-based methods have been used to great ef-
fect in NLP, on problems such as word sense disam-
biguation (Mihalcea, 2005), summarization (Erkan
and Radev, 2004), and dependency parsing (McDon-
ald et al., 2005). Most previous studies of networks
consider networks with only a single type of node,
and in some cases using a network with a single type
of node can be an oversimplified view if it ignores
other types of relationships.
In this paper we will demonstrate heterogeneous
networks, networks with multiple different types of
nodes and edges, along with several applications of
them. The applications in this paper are not pre-
sented so much as robust attempts to out-perform the
current state-of-the-art, but rather attempts at being
competitive against top methods with little effort be-
yond the construction of the heterogeneous network.
Throughout this paper, we will use the data from
the ACL Anthology Network (AAN) (Bird et al.,
2008; Radev et al., 2013), which contains additional
metadata relationships not found in the ACL Anthol-
ogy, as a typical heterogeneous network. The results
in this paper should be generally applicable to other
heterogeneous networks.
1.1 Heterogeneous AAN schema
We build a heterogeneous graph G(V,E) from
AAN, where V is the set of vertices and E is the
set of edges connecting vertices. A vertex can be
one of five semantic types: {paper, author, venue,
institution, term}. An edge can also be one of five
types, each connecting different types of vertices:
? author ? [writes] ? paper
? paper ? [cites] ? paper
? paper ? [published in] ? venue1
? author ? [affiliated with] ? institution2
? paper ? [contains] ? term
All of this data, except for the terms, is available
for all papers in the 2009 release of AAN. Terms are
extracted from titles by running TextRank (Mihal-
cea and Tarau, 2004) on NP-chunks from titles and
manually filtering out bad terms.
We show the usefulness of this representation
in several applications: the measurement of scien-
tific impact (Section 2), name disambiguation (Sec-
tion 3), and topic modeling (Section 4). The hetero-
geneous network representation provides a simple
framework for combining lexical networks (like the
term co-occurence network) with metadata relations
from a source like AAN and allows us to begin to
develop NLP-aware methods for problems like sci-
entometrics and name disambiguation, which are not
usually framed in an NLP perspective.
1For a joint meeting of venues A and B publishing a paper
x, two edges (x,A) and (x,B) are created.
2Author-affiliation edges are weighted according to the
number of papers an author has published from an institution.
1
Transactions of the Association for Computational Linguistics, 2 (2014) 1?14. Action Editor: Lillian Lee.
Submitted 3/2013; Revised 6/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
2 Scientific Impact Measurement
The study of scientometrics, which attempts to
quantify the scientific impact of papers, authors, etc.
has received much attention recently, even within
the NLP community. In the past few years, there
have been many proposed measures of scientific im-
pact based on relationships between entities. Intu-
itively, a model that can take into account many dif-
ferent types of relationships between entities should
be able to measure scientific impact more accu-
rately than simpler measures like citation counts or
h-index.
We propose using Pagerank on the heterogeneous
AAN (Page et al., 1999) to measure scientific impact.
Since changes in the network schema can affect the
relative rankings between different types of entities,
this method is probably not appropriate for compar-
ing entities of two different types against each other.
But between nodes of the same type, this measure is
an appropriate (and as we will show, accurate) way
to compare impacts.
We see this method as a first logical step in the
direction of heterogeneous network-based sciento-
metrics. This method could easily be extended to
use a directed schema (Kurland and Lee, 2005) or a
schema that is aware of the lexical content of citation
sentences, such as sentiment-based signed networks
(Hassan et al., 2012).
Determining the intrinsic quality of scientific im-
pact measures can be difficult since there is no
way to collect gold standard measurements for real-
world entities. Previous studies have attempted to
show that their measures give high scores to a few
known high-impact entities, e.g. Nobel prize win-
ners (Hirsch, 2005), or have performed a statistical
component analysis to find the most important mea-
sures in a group of related statistics (Bollen et al.,
2009). Our approach, instead, is to generate real-
istic data from synthetic entities whose impacts are
known.
We had considered alternative formulations that
did not rely on synthetic data, but each of them
presented problems. When we attempted manual
prominence annotation for AAN data, the inter-
judge agreement (measured by Spearman correla-
tion) in our experiments ranged from decent (0.9
in the case of institutions) to poor (0.3 for authors)
to nearly random (0.03 for terms), far too low to
use in most cases. We also considered evaluating
prominence measures by their ability to predict fu-
ture citations to an entity. Citations are often used
as a proxy for impact, but our measurements have
found that correlation between past citations and fu-
ture citations is too high for citation prediction to be
a meaningful evaluation3.
2.1 Creating a synthetic AAN
In network theory, a common technique for testing
network algorithms when judgments of real-world
data are expensive or impossible to obtain is to test
the algorithm on a synthetic network. To create such
a synthetic network, the authors define a simple, but
realistic generative process by which the real-world
networks of interest may arise. The properties of
the network are measured to ensure that it replicates
certain observable behaviors of the real-world net-
work. They can then test network algorithms to see
how well they are able to recover the hidden param-
eters that generated the synthetic network. (Pastor-
Satorras and Vespignani, 2001; Clauset et al., 2009;
Karrer and Newman, 2011)
We take a two-step approach to generating this
synthetic data, first generating entities with known
impacts, and second, linking these entities together
according to their latent impacts. Our heuristic is
that high impact entities should be linked to other
high impact entities and vice-versa. As in the net-
work theory literature, we must show that this data
reflects important properties observed in the true
AAN.
One such property is that the number of citations
per paper follows a power law distribution (Redner,
1998). We observe this behavior in AAN along with
several other small-world behaviors, such as a small
diameter, a small average shortest path length, and a
high clustering coefficient in the coauthorship graph.
We strive to replicate these properties in our syn-
thetic data.
3Most existing impact measurements require access to at
least one year?s worth of citation information. The Spearman
correlation between the number of citations received after one
year and after five years is 0.79 with correlation between suc-
cessive years as high as 0.99. Practically this means that the
measures that best correlate with citations after five years are
exactly those that best correlate with citations after one year.
2
Since scientific impact measures attempt to quan-
tify the true impact of entities, we can use these mea-
sures to help understand how the true impact mea-
sures are distributed across different entities. In fact,
citation counts, being a good estimate of impact, can
be used to generate these latent impact variables for
each entity. For each type of entity (papers, authors,
institutions, venues, and terms), we create a latent
impact by sampling from the appropriate citation
count distribution. After sampling, all the impacts
are normalized to fall in the [0, 1] interval, with the
highest-impact entity of each type having a latent
impact of 1. Additive smoothing is used to avoid
having an impact of 0.
Once we have created the entities, our method
for placing edges is most similar to the Erdo?s-
Re?yni method for creating random graphs (Erdo?s
and Re?nyi, 1960), in which edges are distributed
uniformly at random between pairs of vertices. In-
stead of distributing links uniformly, links between
entities are sampled proportionally to I(a)I(b)(1 ?
(I(a) ? I(b))2), where I(x) is the latent impact of
entity x.
We tried several other formulae that failed to
replicate the properties of the real AAN. The
I(a)I(b) part of the formula above reflects a pref-
erence for nodes of any type to connect with high-
impact entities (e.g., major conferences receive
many submissions even though most submissions
will be rejected), but the 1 ? (I(a) ? I(b))2 part
also reflects the reality that entities of similar promi-
nence are most likely to attach to each other (e.g.,
well-known authors publish in major conferences,
while less well-known authors may publish mostly
in lesser-known workshops).
Using this distribution, we randomly sample links
between papers and authors; authors and institu-
tions; papers and venues; and papers and terms. The
only exception to this was paper-to-paper citation
links, for which we did not expect this same be-
havior to apply, as low-impact papers regularly cite
high-impact papers, but not vice-versa. To model ci-
tations, we selected citing papers uniformly at ran-
dom and cited papers in proportion to their impacts.
(Albert and Baraba?si, 2002)
Finally, we generated a network equal in size to
AAN, that is, with the exact same numbers of pa-
pers, authors, etc. and the exact same number of
Relationship True value Synth. value
Paper-citations power
law coeff. 1.82 2.12
Diameter 9 8
Avg. shortest path 4.27 4.05
Collaboration network
clustering coeff. 0.34 0.26
Table 1: Network properties of the synthetic AAN
compared with the true AAN.
paper-author links, paper-venue links, etc. Table 1
compares the observed properties of the true AAN
with the observed properties of this synthetic version
of AAN. None of the statistics are exact matches, but
when building random graphs, it is not uncommon
for measures to differ by many orders of magnitude,
so a model that has measures that are on the same
order of magnitude as the observed data is generally
considered to be a decent model (Newman and Park,
2003).
2.2 Measuring impact on the synthetic AAN
This random network is, of course, still imperfect
in some regards. First of all, it has no time aspect,
so it is not possible for impact to change over time,
which means we cannot test against some impact
measures that have a time component like CiteR-
ank (Maslov and Redner, 2008). Second, there are
some constraints present in the real world that are
not enforced here. Because the edges are randomly
selected, some papers have no venues, while others
have multiple venues. There is also nothing to en-
force certain consistencies, such as authors publish-
ing many papers from relatively few institutions, or
repeatedly collaborating with the same authors.
We had also considered using existing random
graph models such as the Baraba?si-Albert model
(Baraba?si and Albert, 1999), which are known to
produce graphs that exhibit power law behavior.
These models, however, do not provide a way to re-
spect the latent impacts of the entities, as they add
links in proportion only to the number of existing
links a node has.
We measure the quality of impact measures by
comparing ranked lists: the ordering of the entities
3
Paper measure Agreement
Heterogeneous network Pagerank 0.773
Citation network Pagerank 0.558
Citation count 0.642
Author measure Agreement
Heterogeneous network Pagerank 0.461
Coauthorship network Pagerank 0.244
h-index (Hirsch, 2005) 0.292
Aggregated citation count 0.236
i10-index 0.235
Institution measure Agreement
Heterogeneous network Pagerank 0.373
h-index (Mitra, 2006) 0.334
Aggregated citation count 0.327
Venue measure Agreement
Heterogeneous network Pagerank 0.449
h-index (Braun et al., 2006) 0.425
Aggregated citation count 0.370
Impact factor 0.092
Venue citation network Pagerank (Bollen
et al., 2006) 0.366
Table 2: Agreement of various impact measures
with the true latent impact.
by their true (but hidden) impact against their order-
ing according to the impact measure. The agree-
ment between these lists is measured by Kendall?s
Tau. Table 2 compares several well-known impact
measures with our impact measure, Pagerank cen-
trality on the heterogeneous AAN network. We find
that some popular methods, such as h-index (Hirsch,
2005) are too coarse to accurately capture much
of the underlying variation. There is a version of
Kendall?s Tau that accounts for ties, and while this
metric slightly helps the coarser measures, Pagerank
on the heterogeneous network is still the clear win-
ner.
When comparing different ordering methods, it
is natural to wonder which of entities the orderings
disagree on. In general, non-heterogeneous mea-
sures like h-index or collaboration network Pager-
ank, which only focus on one type of relationship
can suffer when the entity in question has an impor-
tant relationship of another type. For example, if an
author is highly cited, but mostly works alone, his
1985 1990 1995 2000 2005 2010
20
40
60
80
100
120
Re
lat
ive
Pa
ge
ran
k
ACL
EMNLP
COLING
NAACL
Figure 1: Evolution of conference impacts. The y-
axis measures relative Pagerank, the entity?s Pager-
ank relative to the average Pagerank in that year.
contribution would be undervalued in the collabo-
ration network, but would be more accurate in the
heterogeneous network.
The majority of the differences between the im-
pact measures, though, tend to be in how they han-
dle entities of low prominence. It seems that, for the
most part, there is relatively little disagreement in
the orderings of high-impact entities between differ-
ent impact measures. That is, most highly prominent
entities tend to be highly rated by most measures.
But when an author or a paper, for example, only has
one or two citations, it can be advantageous to look
at more types of relationships than just citations.
The paper may be written by an otherwise prominent
author, or published at a well-known venue, and hav-
ing many types of relations at its disposal can help a
method like heterogeneous network Pagerank better
distinguish between two low-prominence entities.
2.3 Top-ranked entities according to
heterogeneous network PageRank
Table 3 shows the papers, authors, institutions,
venues, and terms that received the highest Pager-
ank in the heterogeneous AAN. It is obvious that the
top-ranked entities in this network are not simply the
most highly cited entities.
This ranking also does not have any time bias
toward the entities that are currently prominent, as
some of the top authors were more prolific in previ-
ous decades than at the current time. We also see
this effect with COLING, which for many of the
early years, is the only venue in the ACL Anthology.
4
Top Papers Top Authors Top Institutions Top Venues TopTerms
? Building A Large Annotated Corpus OfEnglish: The Penn Treebank 4 15 Jun?ichi Tsujii 4 8
Carnegie Mellon
University 4 1 COLING ? translation
? The Mathematics Of Statistical MachineTranslation: Parameter Estimation 4 7
Aravind K.
Joshi 4 1
University of
Edinburgh 5 1 ACL 4 3 speech
? Attention, Intentions, And The Structure OfDiscourse 4 18
Ralph
Grishman 5 2
University of
Pennsylvania 4 2 HLT 5 1 parsing
? A Maximum Entropy Approach To NaturalLanguage Processing 4 75 Hitoshi Isahara 5 2
Massachusetts
Institute of
Technology
4 4 EACL 5 1 machinetranslation
? BLEU: a Method for Automatic Evaluationof Machine Translation 4 20
Yuji
Matsumoto 4 12
Saarland
University 4 7 LREC 4 3 generation
? A Maximum-Entropy-Inspired Parser 4 7 Kathleen R.McKeown 5 2
IBM T.J. Watson
Research Center ? NAACL 4 3 evaluation
4 2 A Stochastic Parts Program And NounPhrase Parser For Unrestricted Text 4 13 Eduard Hovy 4 39 CNRS 5 3 EMNLP 4 6 grammar
5 1 A Systematic Comparison of VariousStatistical Alignment Models 4 10
Christopher D.
Manning 4 26
University of
Tokyo 5 5
Computational
Linguistics 4 16 dialogue
4 4
Transformation-Based Error-Driven
Learning and Natural Language Processing:
a Case Study in Part-of-Speech Tagging
4 93 Yorick Wilks 5 4 StanfordUniversity 4 4 IJCNLP 4 10
knowl-
edge
4 1 A Maximum Entropy Model forPart-of-Speech Tagging 5 9 Hermann Ney 4 3 BBN Technologies 4 1
Workshop on
Speech and
Natural
Language
4 1 discourse
Table 3: The entities of each type receiving the highest scores from the heterogeneous network Pagerank
impact measure along with their respective changes in ranking when compared to a simple citation count
measure.
One possible way to address this is to use a narrower
time window when creating the graph, such as only
including edges from the previous five years. We
apply this technique in the following section.
2.4 Entity impact evolution
The heterogeneous graph formalism also provides a
natural way to study the evolution of impact over
time, as in (Hall et al., 2008), but at a much finer
granularity. Hall et al. measured the year-by-year
prominence of statistical topics, but we can measure
year-by-year prominence for any entity in the graph.
To measure the evolution of impacts over the
years, we iteratively create year-by-year versions of
the heterogeneous AAN. Each of these graphs con-
tains all entities along with all edges occurring in a
five year window. Due to space, we cannot com-
prehensively exhibit this technique and the data it
produces, but as a brief example, in Figure 1, we
show how the impacts of some major NLP confer-
ences changes over time.
The graph shows that NAACL and EMNLP have
been steadily gaining prominence since their intro-
ductions, but also shows that ACL has had to make
up a lot of ground since 1990 to surpass COLING.
We also notice that all the major conferences have
grown in impact since 2005, and believe that as the
field continues to grow, the major conferences will
continue to become more and more important.
3 Name Disambiguation
We frame network name disambiguation in a link
prediction setting (Taskar et al., 2003; Liben-Nowell
and Kleinberg, 2007). The problems of name dis-
ambiguation and link prediction share many char-
acteristics, and we have found that if two ambigu-
ous name nodes are close enough to be selected by a
link-prediction method, then they likely correspond
to the same real-world author.
We intend to show that the heterogeneous biblio-
graphic network can be used to better disambiguate
author names than the author collaboration network.
The heterogeneous network for this problem con-
tains papers, authors, terms, venues, and institutions.
We compare several well-known network similarity
measures from link prediction by transforming the
5
Network Distance Measure Precision Recall F1-score Rand index Purity NMI
Heterogeneous Truncated Commute Time 0.59 0.78 0.63 0.63 0.71 0.43
Heterogeneous Shortest Path 0.90 0.79 0.83 0.87 0.94 0.76
Heterogeneous PropFlow 0.89 0.83 0.84 0.87 0.93 0.77
Coauthorship Truncated Commute Time 0.47 0.80 0.54 0.47 0.60 0.18
Coauthorship Shortest Path 0.54 0.73 0.60 0.61 0.67 0.31
Coauthorship PropFlow 0.57 0.76 0.64 0.66 0.71 0.43
Coauthorship GHOST 0.89 0.60 0.69 0.81 0.94 0.63
Table 4: Performance of different networks and distance measures on the author name disambiguation task.
The performance measures are averaged over the sets of two, three, and four authors. Rand index is from
(Rand, 1971) and NMI is an abbreviation for normalized mutual information (Strehl and Ghosh, 2003)
similarities to distances and inducing clusters of au-
thors based on these distances.
We compare three distance measures: shortest
path, truncated commute time (Sarkar et al., 2008),
and PropFlow (Lichtenwalter et al., 2010). Short-
est path distance can be a useful metric for author
disambiguation because it is small when two am-
biguous nodes are neighbors in the graph or share
a neighbor. Its downside is that it only considers one
path between nodes, the shortest, and cannot take
advantage of the fact that there may be many short
paths between two nodes.
Truncated commute time is a variant of commute
time where all paths longer than some threshold are
truncated. The truncation threshold l should be set
such that no semantically meaningful path is trun-
cated. We use a value of ten for l in the heteroge-
neous graph and three in the coauthorship graph4.
The advantage of truncated commute time over or-
dinary commute time is simpler calculation, as no
paths longer than l need be considered. The down-
side of this method is that large branching factors
tend to lead to less agreement between commute
time and truncated commute time.
PropFlow is a quantity that measures the proba-
bility that a non-intersecting random walk starting at
node a reaches node b in l steps or fewer, where l is
again a threshold. As before, l should be a bound on
the length of semantically meaningful paths, so we
use the same values for l as with truncated commute
time. Of course, PropFlow is not a metric, which is
4This is a standard coauthorship graph with the edge weights
equal to the number of publications shared between authors.
The heterogeneous network does not have author-to-author
links, as authors are linked by paper nodes.
required for some clustering methods. We use the
following equation to transform PropFlow to a met-
ric: d(a, b) = 1PropF low(a,b) ? 1.
With each of the distance measures, we apply
the same clustering method: partitioning around
medoids, with the number of clusters automatically
determined using the gap statistic method (Tibshi-
rani et al., 2001). We create the null distribution
needed for the gap statistic method by many itera-
tions of randomly sampling distances from the com-
plete distance matrix between all nodes in the graph.
The gap statistic method automatically selects the
number of clusters from two, three, or four author
clusters.
We compare our methods against GHOST (Fan et
al., 2011), a high-performance author disambigua-
tion method based on the coauthorship graph.
3.1 Data
To generate name disambiguation data, we use the
pseudoword method of (Gale et al., 1992). Specif-
ically, we choose two or more completely random
authors and conflate them by giving all instances
of both authors the same name. We let each paper
written by this pseudoauthor be an instance to be
clustered. The clusters produced by any author dis-
ambiguation method can then be compared against
the papers actually written by each of the two au-
thors. This method, of course, relies on having all of
the underlying authors completely disambiguated,
which AAN provides.
This method is used to create 100 distambiguation
sets with two authors, 100 for three authors, and 100
for four authors.
6
3.2 Results
Table 4 shows the performance of author name dis-
ambiguation with different networks and distance
metrics. F1-score is the measure that is most of-
ten used to compare author disambiguation methods.
Both PropFlow and shortest path similarity on the
heterogeneous network perform quite well accord-
ing this measure, as well as the other reported mea-
sures. While comparable recall can be achieved us-
ing only the coauthorship graph, the heterogeneous
graph allows for much higher precision.
4 Random walk topic model
Here we present a topic model based entirely on
graph random walks. This method is not truly a
statistical model as there are no statistical parame-
ters being learned, but rather a topic-discovery and
-assignment method, attempting to solve the same
problem as statistical topic models such as proba-
bilistic latent semantic analysis (pLSA) (Hofmann,
1999) or latent Dirichlet allocation (LDA) (Blei et
al., 2003). In the absence of better terminology, we
use the name random walk topic model.
While this method does not have the robust math-
ematical foundation that statistical topic models pos-
sess, in its favor it has modularity, simplicity, and
interpretability. This language model is modular as
it completely separates the discovery of topics from
the association of topics with entities. It is sim-
ple because it requires only a clustering algorithm
and random walk algorithms, instead of complex in-
ference algorithms. The method also does not re-
quire any modification if the topology of the net-
work changes, whereas statistical models may need
an entirely different inference procedure if, e.g., au-
thor topics are desired in addition to paper topics.
Thirdly this method is easily interpretable with top-
ics provided by clustering in the word-relatedness
graph and topic association based on random walks
from entities to topics.
4.1 Topics from word graph clustering
From the set of ACL anthology titles, we create
two graphs: (1) a word relatedness graph by cre-
ating a weighted link between each pair of words
corresponding to the PropFlow (Lichtenwalter et al.,
2010) measure between them on the full heteroge-
neous graph and (2) a word co-occurence graph by
creating a weighted link between each pair of words
corresponding to the number of titles in which both
words occur.
Both of these graphs are then clustered using
Graph Factorization Clustering (GFC). GFC is a soft
clustering algorithm for graphs that models graph
edges as a mixture of latent node-cluster association
variables. (Yu et al., 2006)
Given a word graph G with vertices V and ad-
jacency matrix [w]ij , GFC attempts to fit a bipar-
tite graph K(V,U) with adjacency matrix [b]ij onto
this data, with the m nodes of U representing the
clusters. Whereas in G, similarity between two
words i and j can be measured with wij , we can
similarly measure their similarity in K with w?ij =?m
p=1
bipbjp
?p where ?p =
?n
i=1 bip is the degree of
vertex p ? U .
Essentially the bipartite graph attempts to approx-
imate the transition probability between i and j inG
with the sum of transition probabilities from i to j
through any of the m nodes in U . Yu, et al. (2006)
present an algorithm for minimizing the divergence
distance `(X,Y) =?ij(xijlog xijyij ? xij + yij) be-
tween [w]ij and [w?]ij .
We run GFC with this distance metric and m =
100 clusters on the word graph until convergence
(change in log-likelihood < 0.1%). After conver-
gence, the nodes in U become the clusters and the
weights bip (constrained to sum to 1 for each clus-
ter) become the topic-word association scores.
Examples of some topics found by this method
are shown in Table 5. From manual inspection of
these topics, we found them to be very much like
topics created by statistical topic models. We find
instances of all the types of topics listed in (Mimno
et al., 2011): chained, intruded, random, and unbal-
anced. For an evaluation of these topics see Sec-
tion 4.3.1.
4.2 Entity-topic association
To associate entities with topics, we first create
the heterogeneous network as in previous sections,
adding links between papers and their title words,
along with links between words and the topics that
were discovered in the previous section. Word-topic
links are also weighted according to the weights
7
Word sense induction sense disambiguation word induction unsupervised clustering senses based similarity chinese
CRFs + their applications entity named recognition random conditional fields chinese entities biomedical segmentation
Dependency parsing parsing dependency projective probabilistic incremental deterministic algorithm data syntactic trees
Tagging models tagging model latent markov conditional random parsing unsupervised segmentation
Multi-doc summarization summarization multi document text topic based query extractive focused summaries
Chinese word segmentation word segmentation chinese based alignment character tagging bakeoff model crf
Lexical semantics lexical semantic distributional similarity wordnet resources lexicon acquistion semantics representation
Cross-lingual IR cross lingual retrieval document language linguistic multi person multilingual coreference
Generation for summar. sentence based compression text summarization ordering approach ranking generation
Spoken language speech recognition automatic prosodic tagging spontaneous news broadcast understanding conversational
French function words de la du des le automatique analyse une en pour
Question answering question answering system answer domain retrieval web based open systems
Unsupervised learning unsupervised discovery learning induction knowledge graph acquisition concept clustering pattern
SVMs for NLP support vector machines errors space classification correcting word parsing detecting
MaxEnt models entropy maximum approach based attachment model models phrase prepositional disambiguation
Dialogue systems dialogue spoken systems human conversational multi interaction dialogues utterances multimodal
Semantic role-labeling semantic role labeling parsing syntactic features ill dependency formed framenet
SMT based translation machine statistical phrase english approach learning reordering model
Coreference resolution resolution coreference anaphora reference pronoun ellipsis ambiguity resolving approach pronominal
Semi- and weak-supervision learning supervised semi classification active data clustering approach graph weakly
Information retrieval based retrieval similarity models semantic space model distance measures document
Discourse discourse relations structure rhetorical coherence temporal representation text connectives theory
CFG parsing context free grammars parsing linear probabilistic rewriting grammar systems optimal
Min. risk train. and decod. minimum efficient training error rate translation risk bayes decoding statistical
Phonology phoneme conversion letter phonological grapheme rules applying transliteration syllable sound
Sentiment sentiment opinion reviews classification mining polarity analysis predicting product features
Neural net speech recog. speech robust recognition real network time neural networks language environments
Finite state methods state finite transducers automata weighted translation parsing incremental minimal construction
Mechanical Turk mechanical turk automatic evaluation amazon techniques data articles image scientific
Table 5: Top 10 words for several topics created by the co-occurence random walk topic model. The left
column is a manual label.
Topic 59 Topic 82
translation 0.1953 parsing 0.1715
machine 0.1802 dependency 0.1192
statistical 0.0784 projective 0.0138
Machine Translation 0.0018 K-best Spanning Tree Parsing 0.0025
Better Hypothesis Testing for Statistical
Machine Translation: Controlling for
Optimizer Instability
0.0016 Pseudo-Projective Dependency Parsing 0.0024
Filtering Antonymous, Trend- Contrasting, and
Polarity-Dissimilar Distributional Paraphrases
for Improving Statistical Machine Translation
0.0015 Shift-Reduce Dependency DAG Parsing 0.0017
Knight, Kevin 0.0083 Nivre, Joakim 0.0120
Koehn, Philipp 0.0074 Johnson, Mark 0.0085
Ney, Hermann 0.0072 Nederhof, Mark-Jan 0.0064
RWTH Aachen University 0.0212 Vaxjo University 0.0113
Carnegie Mellon University 0.0183 Brown University 0.0107
University of Southern California 0.0177 University of Amsterdam 0.0094
Workshop on Statistical Machine Translation 0.0590 ACL 0.0512
EMNLP 0.0270 EMNLP 0.0259
COLING 0.0173 CoNLL 0.0223
Table 6: Examples of entities associated with selected topics.
8
determined by GCF. We then simply take random
walks from topics to entities and measure the pro-
portion at which the random walk arrives at each en-
tity of interest. These proportions become the entity-
topic association scores.
For example, if we wanted to find the authors
most associated with topic 12, we would take a num-
ber of random walks (say 50,000) starting at topic
12 and terminating as soon as the random walk first
reaches an author node. Measuring the proportion
at which random walks arrive at each allows us to
compute an association score between topic 12 and
each author.
A common problem in random walks on large
graphs is that the walk can easily get ?lost? between
two nodes that should be very near by taking a just
a few steps in the wrong direction. To keep the ran-
dom walks from taking these wrong steps, we adjust
the topology of the network using directed links to
keep the random walks moving in the ?right? direc-
tion. We design the graph such that if we desire a
random walk from nodes of type s to nodes of type t,
the random walk will never be able to follow an out-
going link that does not decrease its distance from
the nodes of t.
As shown in section 2.3, there are certain nodes at
which a random walk (like Pagerank) arrives at more
often than others simply because of their positions in
the graph. This suggests that there may be stationary
random walk distributions over entities, which we
would need to adjust for in order to find the most
significant entities for a topic.
Indeed this is what we do find. As an example, if
we sample topics uniformly and take random walks
to author nodes, by chance we end up at Jun?ichi
Tsujii on 0.3% of random walks, Eduard Hovy on
0.2% of walks, etc. These values are about 1000
times greater than would be expected at random.
To adjust for this effect, when we take a random
walk from a topic x to an entity type t, we subtract
out this stationary distribution for t, which corre-
sponds to the proportion of random walks that end
at any particular entity of type t by chance, and not
by virtue of the fact that the walk started at topic x.
The resulting distribution yields the entities of t that
are most significantly associated with topic x. Ta-
ble 6 gives examples of the most significant entities
for a couple of topics.
?200 ?150 ?100 ?50
RW-cooc
RW-sim
RTM
LDA
Coherence
Figure 2: Distribution of topic coherences for the
four topic models.
4.3 Topic Model Evaluation
We provide two separate evaluations in this section,
one of the topics alone, and one extrinstic evaluation
of the entire paper-topic model. The variants of ran-
dom walk topic models are compared against LDA
and the relational topic model (RTM), each with 100
topics (Chang and Blei, 2010). As RTM allows only
a single type of relationship between documents, we
use citations as the inter-document relationships.
4.3.1 Topic Coherence
The coherence of a topic is evaluated using the co-
herence metric introduced in (Mimno et al., 2011).
Given the top M words V (t) = (v(t)1 , ..., v(t)M ) for a
topic t, the coherence of that topic can be calculated
with the following formula:
C(t;V (t)) =
M?
m=2
m?1?
l=1
log
(
D(v(t)m , v(t)l ) + 1
D(v(t)l )
)
,
where D(v) is the number of documents contain-
ing v and D(v, v?) is the number of documents con-
taining both v and v?.
This measure of coherence is highly correlated
with manual annotations of topic quality, with a
higher coherence score corresponding to a more co-
herent, higher quality topic. After calculating the co-
herence for each of the 100 topics for RTM and the
random-walk topic model, the average coherence for
RTM topics was -135.2 and the average coherence
for word-similarity random walk topics was -122.2,
with statistical significance at p < 0.01. Figure 2
demonstrates this, showing that the word similarity-
based random walk method generates several highly
coherent topics. The average coherence for the LDA
and the co-occurence random walk model were sig-
nificantly lower.
9
4.3.2 Extrinsic Evaluation
One difficulty in evaluating this random-walk
topic model intrinsically against a statistical topic
model like RTM is that existing evaluation measures
assume certain statistical properties of the topic, for
example, that the topics are generated according to a
Dirichlet prior. Because of this, we choose instead to
evaluate this topic model extrinsically with a down-
stream application. We choose an information re-
trieval application, returning a ranked list of similar
documents, given a reference document.
We evaluate five different methods: citation-
RTM, LDA, the two versions of the random-walk
topic model, and a simple word vector similarity
baseline. Similarity between documents with the
topic models are determined by cosine similarity be-
tween the topic vectors of the two documents. Word
vector similarity determines the similarity between
documents by taking the cosine similarity of their
word vectors. From these similarity scores, a ranked
list is produced.
The document set for this task is the set of all pa-
pers appearing at ACL between 2000 and 2011. The
top 10 results returned by each method are pooled
and manually evaluated with a relevance score be-
tween 1 and 10. Thirty such result sets were manu-
ally annotated. We then evaluate each method ac-
cording to its discounted cumulative gain (DCG)
(Ja?rvelin and Keka?la?inen, 2000).
Performance of these methods is summarized in
Table 7. The co-occurence-based random walk topic
model performed comparably with the best per-
former at this task, LDA, and there was no signifi-
cant difference between the two at p < 0.05.
Going forward, an important problem is to rec-
oncile the co-occurence- and word-similarity-based
formulations of this topic model, as the two formu-
lations perform very differently in our two evalua-
tions. Heuristically, the co-occurence model seems
to create good human-readable topics, while the
word-similarity model creates topics that are more
mathematically-coherent, but less human-readable.
5 Related Work
Heterogeneous networks have been studied in a
number of different fields, such as biology (Sio-
son, 2005), transportation networks (Lozano and
Method DCG
Word vector 1.345 ? 0.007
LDA 3.302 ? 0.008
RTM 3.058 ? 0.011
Random-walk (cooc) 3.295 ? 0.006
Random-walk (sim) 2.761 ? 0.007
Table 7: DCG Performance of the various topic
models and baselines on the related document find-
ing task. A 95% confidence interval is provided.
Storchi, 2002), social networks (Lambiotte and Aus-
loos, 2006), and bibliographic networks (Sun et al.,
2011). These networks are also sometimes known
by the name complex networks or multimodal net-
works, but both these terms have other connotations.
We prefer ?heterogeneous networks? as used by Sun
et al. (2009).
There has also been some study of these networks
in general, in community detection (Murata, 2010),
clustering (Long et al., 2008; Sun et al., 2012), and
data mining (Muthukrishnan et al., 2010), but there
has not yet been any comprehensive study. Recently,
NLP has seen several uses of heterogeneous net-
works (though not by that name) for use with label
propagation algorithms (Das and Petrov, 2011; Spe-
riosu et al., 2011) and random walks (Toutanova et
al., 2004; Kok and Brockett, 2010).
Several authors have proposed the idea of using
network centrality measures to rank the impacts of
journals, authors, papers, etc. (Bollen et al., 2006;
Bergstrom et al., 2008; Chen et al., 2007; Liu et al.,
2005), and it has even been proposed that central-
ity can be applicable in bipartite networks (Zhou et
al., 2007). We propose that Pagerank on any gen-
eral heterogeneous network is appropriate for creat-
ing ranked lists for each type of entity. Most previ-
ous papers also lack a robust evaluation, demonstrat-
ing agreement with previous methods or with some
external awards or recognitions. We use a random
graph that replicates the properties of the real-world
network to show that Pagerank on the heterogeneous
network outperforms other methods.
Name disambiguation has been studied in a num-
ber of different settings, including graph-based set-
tings. It is common to use the coauthorship graph
(Kang et al., 2009; Fan et al., 2011), but authors
10
have also used lexical similarity graphs (On and Lee,
2007), citation graphs (McRae-Spencer and Shad-
bolt, 2006), or social networks (Malin, 2005). Al-
most all graph methods are unsupervised.
There have been some topic models developed
specifically for relational data (Wang et al., 2006;
Airoldi et al., 2008), but both of these models have
limitations in the types of relational data they are
able to model. The group topic model described in
(Wang et al., 2006) is able to create stronger topics
by considering associations between words, events,
and entities, but is very coarse in the way it han-
dles the behavior of entities, and does not generalize
to multiple different types of entities. The stochas-
tic blockmodel of (Airoldi et al., 2008) can create
blocks of similar entities in a graph and is general
in the types of graphs it can handle, but produces
less meaningful results on graphs that have specific
schemas.
6 Conclusion and Future Directions
In this paper, we present a heterogeneous net-
work treatment of the ACL Anthology Network and
demonstrate several applications of it. Using only
off-the-shelf graph algorithms with a single data rep-
resentation, the heterogeneous AAN, we are able to
very easily build a scientific impact measure that is
more accurate than existing measures, an author dis-
ambiguation system better than existing graph-based
author disambiguation systems, and a random-walk-
based topic model that is competitive with statistical
topic models.
While there are many other tasks, such as citation-
based summarization, that could likely be ap-
proached using this framework with the appropri-
ate addition of new types of nodes into the hetero-
geneous AAN network, there are even some poten-
tial synergies between the tasks described in this pa-
per that have yet to be explored. For example, we
may consider that the methods of the author disam-
biguation or topic modeling tasks could be to find
the highest-impact papers associated with a term (for
survey generation, perhaps) or high-impact authors
associated with a workshop?s topic (to select good
reviewers for it). We believe that heterogeneous
graphs are a flexible framework that will allow re-
searchers to find simple, flexible solutions for a va-
riety of problems.
Acknowledgments
This research is supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of
Interior National Business Center (DoI/NBC) contract
number D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmen-
tal purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions con-
tained herein are those of the authors and should not be
interpreted as necessarily representing the official poli-
cies or endorsements, either expressed or implied, of
IARPA, DoI/NBC, or the U.S. Government.
References
Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg,
and Eric P. Xing. 2008. Mixed membership stochastic
blockmodels. The Journal of Machine Learning Re-
search, 9:1981?2014.
Re?ka Albert and Albert-La?szlo? Baraba?si. 2002. Statisti-
cal mechanics of complex networks. Reviews of mod-
ern physics, 74(1):47.
A.L. Baraba?si and R. Albert. 1999. Emergence of scal-
ing in random networks. Science, 286(5439):509?512.
Carl T. Bergstrom, Jevin D. West, and Marc A. Wiseman.
2008. The eigenfactor metrics. The Journal of Neuro-
science, 28(45):11433?11434.
Steven Bird, Robert Dale, Bonnie J Dorr, Bryan Gib-
son, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir R Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational lin-
guistics. In Proc. of the 6th International Conference
on Language Resources and Evaluation Conference
(LREC08), pages 1755?1759.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993?1022.
Johan Bollen, Marko A. Rodriguez, and Herbert Van
de Sompel. 2006. Journal status. CoRR,
abs/cs/0601030.
Johan Bollen, Herbert Van de Sompel, Aric Hagberg, and
Ryan Chute. 2009. A principal component analysis of
39 scientific impact measures. PloS one, 4(6):e6022.
Tibor Braun, Wolfgang Gla?nzel, and Andra?s Schubert.
2006. A hirsch-type index for journals. Scientomet-
rics, 69(1):169?173.
Jonathan Chang and David M Blei. 2010. Hierarchical
relational models for document networks. The Annals
of Applied Statistics, 4(1):124?150.
11
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with googles pagerank
algorithm. Journal of Informetrics, 1(1):8?15.
Aaron Clauset, Cosma Rohilla Shalizi, and Mark EJ
Newman. 2009. Power-law distributions in empirical
data. SIAM review, 51(4):661?703.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
Paul Erdo?s and Alfre?d Re?nyi. 1960. On the evolution of
random graphs. Magyar Tud. Akad. Mat. Kutato? Int.
Ko?zl, 5:17?61.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text sum-
marization. J. Artif. Intell. Res. (JAIR), 22:457?479.
Xiaoming Fan, Jianyong Wang, Xu Pu, Lizhu Zhou, and
Bing Lv. 2011. On graph-based name disambigua-
tion. J. Data and Information Quality, 2(2):10:1?
10:23, February.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Work on statistical methods for word
sense disambiguation. In Working Notes of the AAAI
Fall Symposium on Probabilistic Approaches to Natu-
ral Language, volume 54, page 60.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
363?371. ACL.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev.
2012. Extracting signed social networks from text.
TextGraphs-7, page 6.
Jorge E. Hirsch. 2005. An index to quantify an indi-
vidual?s scientific research output. Proceedings of the
National Academy of Sciences of the United states of
America, 102(46):16569.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 50?57. ACM.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2000. IR evalua-
tion methods for retrieving highly relevant documents.
In Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in in-
formation retrieval, pages 41?48. ACM.
In-Su Kang, Seung-Hoon Na, Seungwoo Lee, Hanmin
Jung, Pyung Kim, Won-Kyung Sung, and Jong-Hyeok
Lee. 2009. On co-authorship for author disam-
biguation. Information Processing & Management,
45(1):84?97.
Brian Karrer and Mark EJ Newman. 2011. Stochas-
tic blockmodels and community structure in networks.
Physical Review E, 83(1):016107.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 145?153. ACL.
Oren Kurland and Lillian Lee. 2005. Pagerank without
hyperlinks: Structural reranking using links induced
by language models. In SIGIR ?05.
Renaud Lambiotte and Marcel Ausloos. 2006. Collabo-
rative tagging as a tripartite network. Computational
Science?ICCS 2006, pages 1114?1117.
David Liben-Nowell and Jon Kleinberg. 2007. The link-
prediction problem for social networks. Journal of the
American society for information science and technol-
ogy, 58(7):1019?1031.
R.N. Lichtenwalter, J.T. Lussier, and N.V. Chawla. 2010.
New perspectives and methods in link prediction. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 243?252. ACM.
Xiaoming Liu, Johan Bollen, Michael L. Nelson, and
Herbert Van de Sompel. 2005. Co-authorship net-
works in the digital library research community. Infor-
mation processing & management, 41(6):1462?1480.
Bo Long, Zhongfei Zhang, and Tianbing Xu. 2008.
Clustering on complex graphs. In Proc. the 23rd Conf.
AAAI 2008.
Angelica Lozano and Giovanni Storchi. 2002. Shortest
viable hyperpath in multimodal networks. Transporta-
tion Research Part B: Methodological, 36(10):853?
874.
Bradley Malin. 2005. Unsupervised name disambigua-
tion via social network similarity. In Workshop on
Link Analysis, Counterterrorism, and Security, vol-
ume 1401, pages 93?102.
Sergei Maslov and Sidney Redner. 2008. Promise
and pitfalls of extending google?s pagerank algorithm
to citation networks. The Journal of Neuroscience,
28(44):11103?11105.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Duncan M. McRae-Spencer and Nigel R. Shadbolt.
2006. Also by the same author: Aktiveauthor, a cita-
tion graph approach to name disambiguation. In Pro-
ceedings of the 6th ACM/IEEE-CS joint conference on
Digital libraries, pages 53?54. ACM.
12
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, vol-
ume 4, pages 404?411. Barcelona, Spain.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
HLT-EMNLP, pages 411?418. ACL.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011. Op-
timizing semantic coherence in topic models. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 262?272. ACL.
Panchanan Mitra. 2006. Hirsch-type indices for rank-
ing institutions scientific research output. Current Sci-
ence, 91(11):1439.
Tsuyoshi Murata. 2010. Detecting communities from
tripartite networks. In Proceedings of the 19th inter-
national conference on World wide web, pages 1159?
1160. ACM.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over mul-
tiple graphs for similarity learning. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on,
pages 374?383. IEEE.
Mark E.J. Newman and Juyong Park. 2003. Why social
networks are different from other types of networks.
Physical Review E, 68(3):036122.
Byung-Won On and Dongwon Lee. 2007. Scalable name
disambiguation using multi-level graph partition. In
Proceedings of the 7th SIAM international conference
on data mining, pages 575?580.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
bringing order to the web.
Romualdo Pastor-Satorras and Alessandro Vespignani.
2001. Epidemic spreading in scale-free networks.
Physical review letters, 86(14):3200?3203.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The ACL
anthology network corpus. Language Resources and
Evaluation, pages 1?26.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846?850.
S. Redner. 1998. How popular is your paper? an empir-
ical study of the citation distribution. The European
Physical Journal B-Condensed Matter and Complex
Systems, 4(2):131?134.
P. Sarkar, A.W. Moore, and A. Prakash. 2008. Fast incre-
mental proximity search in large graphs. In Proceed-
ings of the 25th international conference on Machine
learning, pages 896?903. ACM.
Allan A. Sioson. 2005. Multimodal networks in biology.
Ph.D. thesis, Virginia Polytechnic Institute and State
University.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter polarity classification
with label propagation over lexical links and the fol-
lower graph. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 53?63, Edin-
burgh, Scotland, July. ACL.
Alexander Strehl and Joydeep Ghosh. 2003. Cluster
ensembles?a knowledge reuse framework for com-
bining multiple partitions. The Journal of Machine
Learning Research, 3:583?617.
Yizhou Sun, Jiawei Han, Peixiang Zhao, Zhijun Yin,
Hong Cheng, and Tianyi Wu. 2009. Rankclus: inte-
grating clustering with ranking for heterogeneous in-
formation network analysis. In Proceedings of the
12th International Conference on Extending Database
Technology: Advances in Database Technology, pages
565?576. ACM.
Yizhou Sun, Rick Barber, Manish Gupta, and Jiawei Han.
2011. Co-author relationship prediction in heteroge-
neous bibliographic networks.
Yizhou Sun, Charu C. Aggarwal, and Jiawei Han. 2012.
Relation strength-aware clustering of heterogeneous
information networks with incomplete attributes. Pro-
ceedings of the VLDB Endowment, 5(5):394?405.
Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne
Koller. 2003. Link prediction in relational data. In
Neural Information Processing Systems, volume 15.
Robert Tibshirani, Guenther Walther, and Trevor Hastie.
2001. Estimating the number of clusters in a data
set via the gap statistic. Journal of the Royal Sta-
tistical Society: Series B (Statistical Methodology),
63(2):411?423.
Kristina Toutanova, Christopher D Manning, and An-
drew Y Ng. 2004. Learning random walk models
for inducing word dependency distributions. In Pro-
ceedings of the twenty-first international conference
on Machine learning, page 103. ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2006. Group and topic discovery from relations and
their attributes. Technical report, DTIC Document.
Kai Yu, Shipeng Yu, and Volker Tresp. 2006. Soft
clustering on graphs. Advances in Neural Information
Processing Systems, 18:1553.
Ding Zhou, Sergey A. Orshanskiy, Hongyuan Zha, and
C. Lee Giles. 2007. Co-ranking authors and docu-
ments in a heterogeneous network. In Data Mining,
2007. ICDM 2007. Seventh IEEE International Con-
ference on, pages 739?744. IEEE.
13
14
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 328?334,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UMichigan: A Conditional Random Field Model for Resolving the Scope of
Negation
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
In this paper, we present a system for de-
tecting negation in English text. We address
three tasks: negation cue detection, negation
scope resolution and negated event identifi-
cation. We pose these tasks as sequence la-
beling problems. For each task, we train a
Conditional Random Field (CRF) model on
lexical, structural, and syntactic features ex-
tracted from labeled data. The models are
trained and tested using the dataset distributed
with the *sem Shared Task 2012 on resolving
the scope and focus of negation. The system
detects negation cues with 90.98% F1 mea-
sure (94.3% and 87.88% recall). It identifies
negation scope with 82.70% F1 on token-by-
token level and 64.78% F1 on full scope level.
Negated events are detected with 51.10% F1
measure.
1 Introduction
Negation is a linguistic phenomenon present in all
languages (Tottie, 1993; Horn, 1989). The seman-
tic function of negation is to transform an affirma-
tive statement into its opposite meaning. The auto-
matic detection of negation and its scope is a prob-
lem encountered in a wide range of natural language
processing applications including, but not limited to,
data mining, relation extraction, question answering,
and sentiment analysis. For example, failing to ac-
count for negation may result in giving wrong an-
swers in question answering systems or in the pre-
diction of opposite sentiment in sentiment analysis
systems.
The occurrence of negation in a sentence is deter-
mined by the presence of a negation cue. A nega-
tion cue is a word, a phrase, a prefix, or a postfix
that triggers negation. Scope of negation is the part
of the meaning that is negated (Huddleston and Pul-
lum, 2002). The negated event is the event or the en-
tity that the negation indicates its absence or denies
its occurrence. For example, in the sentence below
never is the negation cue. The scope is enclosed in
square brackets. The negated event is underlined.
[Andrew had] never [liked smart phones],
but he received one as a gift last week and
started to use it.
Negation cues and scopes may be discontinuous.
For example, the negation cue neither ... nor is dis-
continuous.
In this chapter, we present a system for automat-
ically detecting negation cues, negated events, and
negation scopes in English text. The system uses
conditional random field (CRF) models trained on
labeled sentences extracted from two classical En-
glish novels. The CRF models are trained using lex-
ical, structural, and syntactic features. The experi-
ments show promising results.
This paper is organized as follows. Section 2 re-
views previous work. Section 3 describes the data.
Section 4 describes the CRFs models. Section 5
presents evaluation, results, and discussion.
2 Previous Work
Most research on negation has been done in the
biomedical domain (Chapman et al, 2001; Mutalik
et al, 2001; Kim and Park, 2006; Morante et al,
328
Token Lemma POS Syntax Cue 1 Scope 1 Event 1 Cue 2 Scope 2 Event 2
She She PRP (S(NP*) - She - - - -
would would MD (VP* - would - - - -
not not RB * not - - - - -
have have VB (VP* - have - - - -
said say VBD (VP* - said - - - -
? ? ? (SBAR(S(NP* - ? - - - -
Godspeed Godspeed NNP * - Godspeed - - - -
? ? ? *) - ? - - - -
had have VBD (VP* - had - - had -
it it PRP (ADVP* - it - - it -
not not RB *) - not - not - -
been be VBN (VP* - been - - been -
so so RB (ADVP*)))))))) - so - - so -
. . . *) - - - - - -
Table 1: Example sentence annotated for negation following sem shared task 2012 format
2008a; Morante and Daelemans, 2009; Agarwal and
Yu, 2010; Morante, 2010; Read et al, 2011), mostly
on clinical reports. The reason is that most NLP re-
search in the biomedical domain is interested in au-
tomatically extracting factual relations and pieces of
information from unstructured data. Negation detec-
tion is important here because information that falls
in the scope of a negation cue cannot be treated as
facts.
Chapman et al (2001) proposed a rule-based al-
gorithm called NegEx for determining whether a
finding or disease mentioned within narrative med-
ical reports is present or absent. The algorithm
uses regular-expression-based rules. Mutalik et
al. (2001) developed another rule based system
called Negfinder that recognizes negation patterns
in biomedical text. It consists of two components:
a lexical scanner, lexer that uses regular expres-
sion rules to generate a finite state machine, and a
parser. Morante (2008b) proposed a supervised ap-
proach for detecting negation cues and their scopes
in biomedical text. Their system consists of two
memory-based engines, one that decides if the to-
kens in a sentence are negation signals, and another
one that finds the full scope of these negation sig-
nals.
Negation has been also studied in the context of
sentiment analysis (Wilson et al, 2005; Jia et al,
2009; Councill et al, 2010; Heerschop et al, 2011;
Hogenboom et al, 2011). Wiegand et al (2010) sur-
veyed the recent work on negation scope detection
for sentiment analysis. Wilson et al (2005) studied
the contextual features that affect text polarity. They
used a machine learning approach in which nega-
tion is encoded using several features. One feature
checks whether a negation expression occurs in a
fixed window of four words preceding the polar ex-
pression. Another feature accounts for a polar pred-
icate having a negated subject. They also have dis-
ambiguation features to handle negation words that
do not function as negation cues in certain contexts,
e.g. not to mention and not just.
Jia et al (2009) proposed a rule based method to
determine the polarity of sentiments when one or
more occurrences of a negation term such as not ap-
pear in a sentence. The hand-crafted rules are ap-
plied to syntactic and dependency parse tree repre-
sentations of the sentence.
Hogenboom et al (2011) found that applying a
simple rule that considers two words, following a
negation keyword, to be negated by that keyword,
to be effective in improving the accuracy of senti-
ment analysis in movie reviews. This simple method
yields a significant increase in overall sentiment
classification accuracy and macro-level F1 of 5.5%
and 6.2%, respectively, compared to not accounting
for negation.
This work is characterized by addressing three
tasks at once: negation cue detection, negated
event identification, and negation scope resolution.
Our proposed approach uses a supervised graphical
probabilistic model trained using labeled data.
329
3 Data
We use the dataset distributed by the organizers of
the *sem Shared Task 2012 on resolving the scope
and focus of negation. This dataset includes two sto-
ries by Conan Doyle, The Hound of the Baskervilles,
The Adventures of Wisteria Lodge. All occur-
rences of negation are annotated accounting for
negation expressed by nouns, pronouns, verbs, ad-
verbs, determiners, conjunctions and prepositions.
For each negation cue, the negation cue and scope
are marked, as well as the negated event (if any ex-
ists). The annotation guidelines follow the proposal
of Morante et al (2011)1. The data is split into three
sets: a training set containing 3,644 sentences, a de-
velopment set containing 787 sentences, and a test-
ing set containing 1,089 sentences. The data is pro-
vided in CoNLL format. Each line corresponds to a
token and each annotation is provided in a column;
empty lines indicate end of sentences. The provided
annotations are:
? Column 1: chapter name
? Column 2: sentence number within chapter
? Column 3: token number within sentence
? Column 4: word
? Column 5: lemma
? Column 6: part-of-speech
? Column 7: syntax
? Columns 8 to last:
? If the sentence has no negations, column
8 has a ?***? value and there are no more
columns.
? If the sentence has negations, the annota-
tion for each negation is provided in three
columns. The first column contains the
word or part of the word (e.g., morpheme
?un?), that belongs to the negation cue.
The second contains the word or part of
the word that belongs to the scope of the
negation cue. The third column contains
the word or part of the word that is the
1http://www.clips.ua.ac.be/sites/default/files/ctrs-n3.pdf
Token Lemma Punc. Cat. POS Label
Since Since 0 OTH IN O
we we 0 PRO PRP O
have have 0 VB VBP O
been be 0 VB VBN O
so so 0 ADVB RB O
unfortunate unfortunate 0 ADJ JJ PRE
as as 0 ADVB RB O
to to 0 OTH TO O
miss miss 0 VB VB O
him him 0 PRO PRP O
and and 0 OTH CC O
have have 0 VB VBP O
no no 0 OTH DT NEG
notion notion 0 NOUN NN O
of of 0 OTH IN O
his his 0 PRO PRP$ O
errand errand 0 NOUN NN O
, , 1 OTH , O
this this 0 OTH DT O
accidental accidental 0 ADJ JJ O
souvenir souvenir 0 NOUN NN O
becomes become 0 VB VBZ O
of of 0 OTH IN O
importance importance 0 NOUN NN O
. . 1 OTH . O
Table 2: Example sentence labeled for negation cue de-
tection
negated event or property. It can be the
case that no negated event or property are
marked as negated.
Table 1 shows an example of an annotated sen-
tence that contains two negation cues.
4 Approach
The problem that we are trying to solve can be split
into three tasks. The first task is to detect negation
cues. The second task is to identify the scope of each
detected negation cue. The third task is to identify
the negated event. We use a machine learning ap-
proach to address these tasks. We train a Condi-
tional Random Field (CRF) (Lafferty et al, 2001)
model on lexical, structural, and syntactic features
extracted from the training dataset. In the following
subsections, we describe the CRF model that we use
for each task.
4.1 Negation Cue Detection
Negation cues are lexical elements that indicate the
existence of negation in a sentence. From lexical
330
point of view, negation cues can be divided into four
categories:
1. Prefix (i.e. in-, un-, im-, il-, dis-). For example,
un- in unsuitable) is a prefix negation cue.
2. Postfix (i.e. -less). for example, -less in
careless.
3. Multi-word negation cues such as neither...nor,
rather than, by no means, etc.
4. Single word negation cues such as not, no,
none, nobody, etc.
The goal of this task is to detect negation cues.
We pose this problem as a sequence labeling task.
The reason for this choice is that some negation cues
may not indicate negation in some contexts. For
example, the negation cue not in the phrase not to
mention does not indicate negation. Also, as we saw
above, some negation cues may consist of multiple
words, some of them are continuous and others are
discontinuous. Treating the task as a sequence label-
ing problem help model the contextual factors that
affect the function of negation cues. We train a CRF
model using features extracted from the sentences of
the training dataset. The token level features that we
train the model on are:
? Token: The word or the punctuation mark as it
appears in the sentence.
? Lemma: The lemmatized form of the token.
? Part-Of-Speech tag: The part of speech tag of
the token.
? Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
? Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
? Starts with negation prefix: This feature takes
the value 1 if the token is a word that starts with
un-, in-, im-, il-, or dis- and 0 otherwise.
? Ends with negation postfix: This feature takes
the value 1 if the token is a word that ends with
-less and 0 otherwise.
The CRF model that we use considers at each to-
ken the features of the current token, the two pre-
ceding tokens, and the two proceeding tokens. The
model also uses token bigrams and trigrams, and
part-of-speech tag bigrams and trigrams as features.
The labels are 5 types: ?O? for tokens that are
not part of any negation cue; ?NEG? for single
word negation cues; ?PRE? for prefix negation cue;
?POST? for postfix negation cue; and ?MULTI-
NEG? for multi-word negation cues. Table 2 shows
an example labeled sentence.
At testing time, if a token is labeled ?NEG? or
?MULTI-NEG? the whole token is treated as a nega-
tion cue or part of a negation cue respectively. If a
token is labeled as ?PRE? or ?POST?, a regular ex-
pression is used to determine the prefix/postfix that
trigged the negation.
4.2 Negation Scope Detection
Scope of negation is the sequence of tokens (can
be discontinuous) that expresses the meaning that
is meant to be negated by a negation cue. A sen-
tence may contain zero or more negation cues. Each
negation cue has its own scope. It is possible that
the scope of two negation cues overlap. We use
each negation instance (i.e. each negation cue and
its scope) as one training example. Therefore, a
sentence that contains two negation cues provides
two training examples. We train a CRF model on
features extracted from all negation instances in the
training dataset. The features that we use are:
? Token: The word or the punctuation mark as it
appears in the sentence.
? Lemma: The lemmatized form of the token.
? Part-Of-Speech tag: The part of speech tag of
the token.
? Part-Of-Speech tag category: Part-of-speech
tags reduced into 5 categories: Adjec-
tive (ADJ), Verb (VB), Noun (NN), Adverb
(ADVB), Pronoun (PRO), and other (OTH).
331
? Is punctuation mark: This feature takes the
value 1 if the token is a punctuation mark and 0
otherwise.
? Type of negation cue: Possible types are:
?NEG? for single word negation cues; ?PRE?
for prefix negation cue; ?POST? for postfix
negation cue; and ?MULTI? for multi-word
negation cues.
? Relative position: This feature takes the value
1 if the token position in the sentence is be-
fore the position of the negation cue, 2 if the
token position is after the position of the nega-
tion cue, and 3 if the token is the negation cue
itself.
? Distance: The number of tokens between the
current token and the negation cue.
? Same segment: This feature takes the value 1
if this token and the negation cue fall in the
segment in the sentence. The sentence is seg-
mented by punctuation marks.
? Chunk: This feature takes the value NP-B (VP-
B) if this token is the first token of a noun (verb)
phrase, NP-I (VP-I) if it is inside a noun (verb)
phrase, NP-E (VP-E) if it is the last token of a
noun (verb) phrase.
? Same chunk: This feature takes the value 1 if
this token and the negation cue fall in the same
chunk (noun phrase or verb phrase).
? Is negation: This feature takes the value 1 if
this token is a negation cue, and 0 otherwise.
? Syntactic distance: The number of edges in the
shortest path that connects the token and the
negation in the syntactic parse tree.
? Common ancestor node: The type of the node
in the syntactic parse tree that is the least com-
mon ancestor of this token and the negation cue
token.
The CRF model considers the features of 4 tokens
to the left and to the right at each position. It also
uses bigram and trigram combinations of some of
the features.
At testing time a few postprocessing rules are
used to fix sure labels if they were labeled incor-
rectly. For example, if a word starts with a prefix
negation cue, the word itself (without the prefix) is
always part of the scope and it is also the negated
event.
4.3 Negated Event Identification
It is possible that a negation cue comes associated
with an event. A negation has an event if it oc-
curs in a factual context. The dataset that we use
was labeled for negated events whenever one exists.
We used the same features described in the previous
subsection to train a CRF model for negated event
identification. We have also tried to use one CRF
model for both scope resolution and negated event
identification, but we noticed that using two sepa-
rate models results in significantly better results for
both tasks.
5 Evaluation
We use the testing set described in Section 3 to eval-
uate the system. The testing set contains 1089 sen-
tences 235 of which contains at least one negation.
We use the standard precision, recall, and f-
measure metrics to evaluate the system. We perform
the evaluation on different levels:
1. Cues: the metrics are computed only for cue
detection.
2. Scope (tokens): the metrics are calculated at to-
ken level. If a sentence has 2 scopes, one with
5 tokens and another with 4, the total number
of scope tokens is 9.
3. Scope (full): the metrics are calculated at the
full scope level. Both the negation cue and
the whole scope should be correctly identified.
If a sentence contains 2 negation cues, then 2
scopes are checked. We report two values here
one the requires the cue match correctly and
one that does not.
4. Negated Events: the metrics are computed only
for negated events identification (apart from
negation cue and scope).
332
Variant A
gold system tp fp fn precision recall F1
Cues 264 250 232 14 32 94.31 87.88 90.98
Scope (cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (no cue match) 249 227 126 14 123 90.00 50.60 64.78
Scope (tokens - no cue match) 1805 1716 1456 260 349 84.85 80.66 82.70
Negated (no cue match) 173 183 70 70 64 50.00 52.24 51.10
Full negation: 264 250 75 14 189 84.27 28.41 42.49
Variant B
gold system tp fp fn precision recall F1
Cues : 264 250 232 14 32 92.80 87.88 90.27
Scope (cue match): 249 227 126 14 123 55.51 50.60 52.94
Scope (no cue match): 249 227 126 14 123 55.51 50.60 52.94
Negated (no cue match): 173 183 70 70 64 38.25 52.24 44.16
Full negation : 264 250 75 14 189 30.00 28.41 29.18
# Sentences 1089
# Negation sentences 235
# Negation sentences with errors 171
% Correct sentences 83.47
% Correct negation sentences 27.23
Table 3: Results of negation cue, negated event, and negation scope detection
5. Full negation: the metrics are computed for all
the three tasks at once and requiring everything
to match correctly.
For cue, scope and negated event to be correct,
both the tokens and the words or parts of words have
to be correctly identified. The final periods in abbre-
viations are disregarded. If gold has value ?Mr.? and
system ?Mr?, system is counted as correct. Also,
punctuation tokens are *not* taken into account for
evaluation.
Two variants of the metrics are computed. In the
first variant (A), precision is calculated as tp / (tp +
fp) and recall is calculated as tp / (tp + fn) where tp
is the count of true positive labels, fp is the count
of false positive labels, and fn is the count of false
negative labels. In variant B, the precision is calcu-
lated differently, using the formula precision = tp /
system.
Table 3 shows the results of our system.
6 Error Analysis
The system used no external resources outside the
training data. This means that the system recognizes
only negation cues that appeared in the training set.
This was the first source of error. For example, the
word unacquainted that starts with the negation pre-
fix un has never been seen in the training data. In-
tuitively, if no negation cue is detected, the system
does not attempt to produce scope levels. This prob-
lem can be overcome by using a lexicon of negation
words and those words that can be negated by adding
a negation prefix to them.
We noticed in several occasions that scope detec-
tion accuracy can be improved if some simple rules
can be imposed after doing the initial labeling us-
ing the CRF model (but we have not actually imple-
mented any such rules in the system). For example,
the system can require all the tokens that belong to
the same chunk (noun group, verb group, etc.) all
have the same label (e.g. the majority vote label).
The same thing could be also applied on the segment
rather than the chunk level where the boundaries of
segments are determined by punctuation marks.
7 Conclusion
We presented a supervised system for identifying
negation in English sentences. The system uses
three CRF trained models. One model is trained for
negation cue detection. Another model is trained
for negated event identification. A third one is
trained for negation scope identification. The mod-
els are trained using features extracted from a la-
beled dataset. Our experiments show that the system
achieves promising results.
333
References
Shashank Agarwal and Hong Yu. 2010. Biomedi-
cal negation scope detection with conditional random
fields. Journal of the American Medical Informatics
Association, 17(6):696?701.
Wendy Webber Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001. A
simple algorithm for identifying negated findings and
diseases in discharge summaries. Journal of Biomedi-
cal Informatics, pages 301?310.
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not: learn-
ing to classify the scope of negation for improved sen-
timent analysis. In Proceedings of the Workshop on
Negation and Speculation in Natural Language Pro-
cessing, NeSp-NLP ?10, pages 51?59, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Bas Heerschop, Paul van Iterson, Alexander Hogenboom,
Flavius Frasincar, and Uzay Kaymak. 2011. Analyz-
ing sentiment in a large set of web data while account-
ing for negation. In AWIC, pages 195?205.
Alexander Hogenboom, Paul van Iterson, Bas Heerschop,
Flavius Frasincar, and Uzay Kaymak. 2011. Deter-
mining negation scope and strength in sentiment anal-
ysis. In SMC, pages 2589?2594.
Laurence R. Horn. 1989. A natural history of nega-
tion / Laurence R. Horn. University of Chicago Press,
Chicago :.
Rodney D. Huddleston and Geoffrey K. Pullum. 2002.
The Cambridge Grammar of the English Language.
Cambridge University Press, April.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM con-
ference on Information and knowledge management,
CIKM ?09, pages 1827?1830, New York, NY, USA.
ACM.
Jung-Jae Kim and Jong C. Park. 2006. Extracting con-
trastive information from negation patterns in biomed-
ical literature. 5(1):44?60, March.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Roser Morante and Walter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. Pro-
ceedings of the Workshop on BioNLP BioNLP 09,
(June):28.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008a. Learning the scope of negation in
biomedical texts. Proceedings of the Conference on
Empirical Methods in Natural Language Processing
EMNLP 08, (October):715?724.
Roser Morante, Anthony Liekens, and Walter Daele-
mans. 2008b. Learning the scope of negation in
biomedical texts. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 715?724, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Roser Morante, Sarah Schrauwen, and Walter Daele-
mans. 2011. Annotation of negation cues and their
scope. Technical report.
Roser Morante. 2010. Descriptive analysis of negation
cues in biomedical texts. Language Resources And
Evaluation, pages 1?8.
P. G. Mutalik, A. Deshpande, and P. M. Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: a quantitative
study using the UMLS. Journal of the American Med-
ical Informatics Association : JAMIA, 8(6):598?609.
Jonathon Read, Erik Velldal, Stephan Oepen, and Lilja
vrelid. 2011. Resolving speculation and negation
scope in biomedical articles with a syntactic con-
stituent ranker. In Proceedings of the Fourth Inter-
national Symposium on Languages in Biology and
Medicine, Singapore.
Gunnel Tottie. 1993. Negation in English Speech and
Writing: A Study in Variation. Language, 69(3):590?
593.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A sur-
vey on the role of negation in sentiment analysis. In
Proceedings of the Workshop on Negation and Spec-
ulation in Natural Language Processing, NeSp-NLP
?10, pages 60?68, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
334
Proceedings of the TextGraphs-6 Workshop, pages 42?50,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Simultaneous Similarity Learning and Feature-Weight Learning for
Document Clustering
Pradeep Muthukrishnan
Dept of CSE,
University of Michigan
mpradeep@umich.edu
Dragomir Radev
School of Information,
Dept of CSE,
University of Michigan
radev@umich.edu
Qiaozhu Mei
School of Information,
Dept of CSE,
University of Michigan
qmei@umich.edu
Abstract
A key problem in document classification and
clustering is learning the similarity between
documents. Traditional approaches include
estimating similarity between feature vectors
of documents where the vectors are computed
using TF-IDF in the bag-of-words model.
However, these approaches do not work well
when either similar documents do not use the
same vocabulary or the feature vectors are not
estimated correctly.
In this paper, we represent documents and
keywords using multiple layers of connected
graphs. We pose the problem of simultane-
ously learning similarity between documents
and keyword weights as an edge-weight regu-
larization problem over the different layers of
graphs. Unlike most feature weight learning
algorithms, we propose an unsupervised algo-
rithm in the proposed framework to simulta-
neously optimize similarity and the keyword
weights. We extrinsically evaluate the perfor-
mance of the proposed similarity measure on
two different tasks, clustering and classifica-
tion. The proposed similarity measure out-
performs the similarity measure proposed by
(Muthukrishnan et al, 2010), a state-of-the-
art classification algorithm (Zhou and Burges,
2007) and three different baselines on a vari-
ety of standard, large data sets.
1 Introduction
The recent upsurge in the amount of text available
due to the widespread growth of the Internet has led
to the need for large scale, efficient Machine Learn-
ing (ML), Information Retrieval (IR) tools for text
mining. At the heart of many of the ML, IR algo-
rithms is the need for a good similarity measure be-
tween documents. For example, a better similarity
measure almost always leads to better performance
in tasks like document classification, clustering, etc.
Traditional approaches represent documents with
many keywords using a simple feature vector de-
scription. Then, similarity between two documents
is estimated using the dot product between their
corresponding vectors. However, such similarity
measures do not use all the keywords together and
hence, suffer from problems due to sparsity. There
are two major issues in computing similarity be-
tween documents
? Similar documents may not use the same vo-
cabulary.
? Estimating feature weights or weight of a key-
word to the document it is contained in.
For example, consider two publications, X and
Y , in the field of Machine Learning. Let X be a
paper on clustering while Y is on classification. Al-
though the two publications use very different vo-
cabulary, they are semantically similar. Keyword
weights are mostly estimated using the frequency of
the keyword in the document. For example, TF-IDF
based scoring is the most commonly used approach
to compute keyword weights to compute similarity
between documents. However, suppose publications
X and Y mention the keyword ??Machine Learn-
ing?? only once. Although, they are mentioned only
once in the text of the document, for the purposes
of computing semantic similarity between the docu-
42
ments, it would be beneficial to give it a high key-
word weight.
A commonly used approach to estimate seman-
tic similarity between documents is to use an ex-
ternal knowledge source like WordNet (Pedersen
et al, 2004). However, these approaches are do-
main dependent and language dependent. If docu-
ment similarity can not be estimated accurately us-
ing just the text, there have been approaches incor-
porating multiple sources of similarity like link sim-
ilarity, authorship similarity between publications
(Bach et al, 2004; Cortes et al, 2009). (Muthukr-
ishnan et al, 2010) also uses multiple sources of
similarity. In addition to improving similarity es-
timates between documents, it also improves sim-
ilarity estimates between keywords. Co-clustering
(Dhillon et al, 2003) based approaches are used
to alleviate problems due to the sparsity and high-
dimensionality of the data. In co-clustering, the key-
words and the documents are simultaneously clus-
tered by exploiting the duality between them. How-
ever, the approach relies solely on the keyword dis-
tributions to cluster the documents and vice-versa.
It does not take into account the inherent similar-
ity between the keywords (documents) when cluster-
ing the documents (keywords). Also, co-clustering
takes as input the weight of all keywords to corre-
sponding documents.
2 Motivation
First, we explain how similarity learning and fea-
ture weight learning can mutually benefit from each
other using an example. For example, consider the
following three publications in the field of Machine
Translation, (Brown et al, 1990; Gale and Church,
1991; Marcu and Wong, 2002)
Clearly, all the papers belong to the field of Ma-
chine Translation but (Gale and Church, 1991) con-
tains the phrase ??Machine Translation?? only once
in the entire text. However, we can learn to attribute
some similarity between (Brown et al, 1990) and
the second publication using the text in (Marcu and
Wong, 2002). The keywords ??Bilingual Corpora??
and ??Machine Translation?? co-occur in the text in
(Marcu andWong, 2002) which makes the keywords
themselves similar. Now we can attribute some sim-
ilarity between the (Brown et al, 1990) and (Marcu
andWong, 2002) publication since they contain sim-
ilar keywords. This shows how similarity learning
can benefit from important keywords.
Now, assume that ??Machine Translation?? is an
important keyword (high keyword weight) for the
first and third publication while ??Bilingual Cor-
pora?? is an important keyword for the second pub-
lication. We explained how to infer similarity be-
tween the first and second publication using the third
publication as a bridge. Using the newly learned
similarity measure, we can infer that ??Bilingual
Corpora?? is an important keyword for the sec-
ond publication since a similar keyword (??Machine
Translation??) is an important keyword for similar
publications.
Let documents Di and Dj contain keywords Kik
and Kjl. Then intuitively, the similarity between
two documents should be jointly proportional to
? The similarity between keywords Kik and Kjl
? The weights of Kik to Di and Kjl to Dj .
Similarly the weight of a keyword Kik to docu-
ment Di should be jointly proportional to
? The similarity between documents Di and Dj .
? The similarity between keyphrases Kik and
Kjl and weight of Kjl to Dj .
The major contributions of the paper are given be-
low,
? A rich representation model for representing
documents with associated keywords for effi-
ciently estimating document similarity..
? A regularization framework for joint feature
weight (keyword weight) learning and similar-
ity learning.
? An unsupervised algorithm in the proposed
framework to efficiently learn similarity be-
tween documents and the weights of keywords
for each document in a set of documents.
In the next two sections, we formalize and ex-
ploit this observation to jointly optimize similarity
between documents and weight of keywords to doc-
uments in a principled way.
43
3 Problem Formulation
We assume that a set of keywords have been ex-
tracted for the set of documents to be analyzed. The
setup is very general: Documents are represented
by the set of candidate keywords. In addition to
that, we have crude initial similarities estimated
between documents and also between keywords
and the weights of keywords to documents. The
similarities and keyword weights are represented
using two layers of graphs. We formally define the
necessary concepts,
Definition 1: Documents and corresponding
keywords
We have a set of N documents D =
{d1, d2, . . . , dN}. Each document, di has a set
of mi keywords Ki = {ki1, ki2, . . . , kimi}
Definition 2: Document Similarity Graph
The document similarity graph, G1 = (V1, E1),
consists of the set of documents as nodes and the
edge weights represent the initial similarity between
the documents.
Definition 3: Keyword Similarity Graph
The keyword similarity graph, G2 = (V2, E2), con-
sists of the set of keywords as nodes and the edge
weights represent the initial similarity between the
keywords.
The document similarity graph and the keyword
similarity graph can be considered as two layers of
graphs which are connected by the function defined
below
Definition 4: keyword Weights (KW)
There exists an edge between di and kij for 1 ? j ?
mi. Let Z represent the keyword weighting func-
tion, i.e, Zdi,kij represents the weight of keyword
kij t document di.
4 Regularization Framework
?(w,Z) = ?0 ? ISC(w,w?) + ?1 ? IKC(Z,Z?)
+?2 ?KS(w,Z) + ?3 ? SK(Z,w) (1)
where ?0 + ?1 + ?2 + ?3 = 1.
ISC refers to Initial Similarity Criterion and IKC
refers to Initial Keyword weight Criterion. They are
defined as follows
ISC(w,w?) =
?
u,v?G1
(wu,v ? w?u,v)
2 (2)
IKC(Z,Z?) =
?
u?G1,v?G2
(Zu,v ? Z?u,v)
2 (3)
KS refers toKeyword based Similarity and SK refers
to Similarity induced Keyword weight. They are de-
fined as follows
KS(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
Zu1,u2Zv1,v2
(wu1,v1 ? wu2,v2)
2 (4)
and
SK(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
wu1,v1wu2,v2
(Zu1,u2 ? Zv1,v2)
2 (5)
Then the task is to minimize the objective function.
The objective function consists of four parts. The
first and second parts are initial similarity criterion
and initial keyword criterion. They ensure that the
optimized edge weights are close to the initial edge
weights. The weights ?0 and ?1 ensure that the op-
timized weights are close to the initial weights, in
other words, they represent the confidence level in
initial weights.
The significance of the third and the fourth parts
of the objective function are best explained by a sim-
ple example. Consider two graphs, G1 and G2. Let
G1 be the graph containing publications as nodes
and edge weights representing initial similarity val-
ues. Let G2 be the graph corresponding to keywords
and edge weights represent similarity between key-
words. There is an edge from a node u1 in G1 to a
node v1 in G2 if the publication corresponding to u1
contains the keyword corresponding to v1.
According to this example, minimizing the key-
word weight induced similarity part corresponds to
updating similarity values between keywords in pro-
portion to weights of the keywords to the respective
documents they are contained in and the similarity
between the documents. keyword weight induced
similarity part also helps updating similarity values
44
between documents in proportion to weights of key-
words they contain and the similarity between the
contained keywords.
Minimizing the similarity induced keyword part
corresponds to updating keyword weights in propor-
tion to the following
? Similarity between v1 and other keywords v2 ?
G2
? Keyword weight of v2 to documents u2 ? G1
? Similarity between u1 and u2
Therefore, even if frequency of a keyword such
as ??Machine Translation?? in a publication is not
high, it can achieve a high keyword weight if it con-
tains many other similar keywords such as ??Bilin-
gual Corpora?? and ??Word alignment??.
5 Efficient Algorithm
We seek to minimize the objective function using
Alternating Optimization (AO) (Bezdek and Hath-
away, 2002), an approximate optimization method.
Alternating optimization is an iterative procedure for
minimizing (or maximizing) the function f(x) =
f(X1, X2, . . . , Xt) jointly over all variables by al-
ternating restricted minimizations over the individ-
ual subsets of variables X1, . . . , Xt.
In this optimization method, we partition the set
of variables into a set of mutually exclusive, exhaus-
tive subsets. We iteratively perform minimizations
over each subset of variables while maintaining the
other subsets of variables fixed. Formally, let the set
of real-valued variables be X = {X1, X2, . . . , XN}
be partitioned into m subsets, {Y1, Y2, . . . , Ym}.
Also, let si = |Yi|. Then we begin with the ini-
tial set of values {Y 01 , Y20, . . . , Ym0} and make re-
stricted minimizations of the following form,
min
Yi?Rsi
{f(Y1r+1, . . . , Yi?1r+1, Yi, Yi+1r, . . . , Ymr)}
(6)
where i = 1, 2, . . . ,m. The underline notation Yj
indicates that the subset of variables Yj are fixed
with respect to Yi. In the context of our prob-
lem, we update each edge weight while maintaining
other edge weights to be a constant. Then the prob-
lem boils down to the minimization problem over a
single edge weight. For example, let us solve the
minimization problem for edge weight correspond-
ing to (ui, vj) where ui, vj ? G1 (The case where
ui, vj ? G2 is analogous). Clearly the objective
function is a convex function in w(ui, vj). The par-
tial derivative of the objective function with respect
to the edge weight is given below,
??(w,Z)
?wui,vj
= 2?0(wui,vj ? w?ui,vj )
+2?2 ?
?
u2,v2?G2
(wui,vj ? wu2,v2)Zu1,u2Zvj ,v2
+?3 ?
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wui,vjwu2,v2
. (7)
To minimize the above function, we set the partial
derivative to zero which gives us the following ex-
pression,
wuj ,vk =
1
C1
(?0w?ui,vj +
?2
?
u2,v2?G2
Zui,u2 wu2,v2 Zvj ,v2)(8)
where
C1 = ?0 + ?2
?
u2,v2?G2
Zui,u2 Zvj ,v2
+?3
2
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wu2,v2
Similarly, we can derive the update equation for
keyword weights, Zui,uj as below,
Zui,uj =
1
C2
(?1Z?ui,uj +
?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2 Zv1,v2)
(9)
where,
C2 = ?1 + ?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2
+?2
2
?
v1?G1
?
v2?G2
(wui,v1 ? wuj ,v2)
2Zv1,v2
45
The similarity score between two nodes is propor-
tional to the similarity between nodes in the other
layer. For example, the similarity between two doc-
uments (keywords) is proportional to the similarity
between the keywords the documents they contain
(the documents they are contained in). C plays the
role of a normalization constant. Therefore, for sim-
ilarity between two nodes to be high, it is required
that they not only contain a lot of similar nodes in
the other graph but the similar nodes need to be im-
portant to the two target nodes.
Similarly, a particular keyword will have a high
weight to a document if similar keywords have high
weights to similar documents. Also, it is neces-
sary that the similarity between the keywords and
the documents are high.
It can be shown that equations 8 and 9 converge
q? linearly since the minimization problem is con-
vex in each of the variables individually and hence
has a global and unique minimizer (Bezdek and
Hathaway, 2002).
5.1 Layered Random Walk Interpretation
The above algorithm has a very nice intuitive inter-
preation in terms of random walks over the two dif-
ferent graphs. Assume the initial weights are transi-
tion probability values after the graphs are normal-
ized so that each row of the adjacency matrices sums
to 1. Then the similarity between two nodes u and v
in the same graph is computed as sum of two parts.
The first part is ?0 times the initial similarity. This
is necessary so that the optimized similarity values
are not too far away from the initial similarity val-
ues. The second part corresponds to the probability
of a random walk of length 3 starting at u and reach-
ing v through two intermediate nodes from the other
graph.
The semantics of the random walk depends on
whether u, v are documents or keywords. For exam-
ple, if the two nodes are documents, then the simi-
larity between two documents d1 and d2 is the prob-
ability of random walk starting at document d1 and
then moving to a keyword k1 and then moving to
keyword k2 and then to document d2. Note that key-
words k1 and k2 can be the same keyword which
accounts for similarity between documents because
they contain the same keyword.
Also, note that second and higher order depen-
dencies are also taken into account by this algo-
rithm. That is, two papers may become similar be-
cause they contain two keywords which are con-
nected by a path in the keyword graph, whose length
is greater than 1. This is due to the iterative nature
of the algorithm. For example, keywords ??Machine
Translation?? and ??Bilingual corpora?? occur often
together and hence any co-occurrence based simi-
larity measure will assign a high initial similarity
value. Hence two publications which contain these
words will be assigned a non-zero similarity value
after a single iteration. Also, ??Bilingual corpora??
and ??SMT?? (abbreviation for Statistical Machine
Translation) can have a high initial similarity value
which enables assiging a high similarity value be-
tween ??Machine Translation?? and ??SMT??. This
leads to a chain effect as the number of iterations in-
creases which helps assign non-zero similarity val-
ues between semantically similar documents even if
they do not contain common keywords.
6 Experiments
It is very hard to evaluate similarity measures in iso-
lation. Thus, most of the algorithms to compute sim-
ilarity scores are evaluated extrinsically, i.e, the sim-
ilarity scores are used for an external task like clus-
tering or classification and the performance in the
external task is used as the performance measure for
the similarity scores. This also helps demonstrate
the different applications of the computed similar-
ity measure. Thus, we perform a variety of differ-
ent experiments on standard data sets to illustrate
the improved performance of the proposed similar-
ity measure. There are three natural variants of the
algorithm,
? Unified: We compare against the edge-weight
regularization algorithm proposed in (Muthukr-
ishnan et al, 2010). The algorithm has the
same representation as our algorithm but the
optimization is strictly defined over the edge
weights in the two layers of the graph, wij?s
and not on the keyword weights. Therefore,
Zij are maintained constant throughout the al-
gorithm.
? Unified-binary: In this variant, we initialize the
keyword weights to 1, i.e, Zij = 1 whenever
document i contains the keyword j.
46
ACL-ID Paper Title Research Topic
W05-0812 Improved HMM Alignment Models for Languages With Scarce
Resources
Machine Translation
P07-1111 A Re-Examination of Machine Learning Approaches for Sentence-
Level MT Evaluation
Machine Translation
P03-1054 Accurate Unlexicalized Parsing Dependency Parsing
P07-1050 K-Best Spanning Tree Parsing Dependency Parsing
P88-1020 Planning Coherent Multi-Sentential Text Summarization
Table 1: Details of a few sample papers classified according to research topic
? Unified-TFIDF: We initialize the keyword
weights to the TFIDF scores, Zij is set to the
TFIDF score of keyword j for document i.
Experiment Set I: We compare our similarity mea-
sure against other similarity measures in the context
of classification. We also compare against a state
of the art classification algorithm which uses differ-
ent similarity measures due to different feature types
without integrating them into one single similarity
measure. Specifically, we compare our algorithm
against three other similarity baselines in the context
of classification which are listed below.
? Content Similarity: Similarity is computed us-
ing just the feature vector representation using
just the text. We use cosine similarity after pre-
processing each document into a tf.idf vector
for the AAN data set. For all other data sets,
we use the cosine similarity on the binary fea-
ture vector representation that is available.
? Link Similarity: Similarity is computed using
only the links (citations, in the case of publica-
tions). To compute link similarity, we use the
node similarity algorithm proposed by (Harel
and Koren, 2001) using a random walk of
length 3 on the link graph.
? Linear combination: The content similarity
(CS) and link similarity (LS) between docu-
ments x and y are combined in a linear fashion
as ?CS(x, y)+(1??)LS(x, y). We tried dif-
ferent values of ? and report only the best accu-
racy that can be achieved using linear combina-
tion of similarity measures. Note that this is an
upper bound on the accuracy of Multiple Ker-
nel Learning with the restriction of the combi-
nation being affine.
We also compare our algorithm against the follow-
ing algorithms SC-MV: We compare our algorithm
against the spectral classification algorithm for data
with multiple views (Zhou and Burges, 2007). The
algorithm tries to classify data when multiple views
of the data are available. The multiple views are rep-
resented using multiple homogeneous graphs with a
common vertex set. In each graph, the edge weights
represent similarity between the nodes computed us-
ing a single feature type. For our experiments, we
used the link similarity graph and the content simi-
larity graph as described above as the two views of
the same data
We use a semi-supervised graph classification al-
gorithm (Zhu et al, 2003) to perform the classifica-
tion.
Experiment Set II: We illustrate the improved
performance of our similarity measure in the con-
text of clustering. We compare our similarity mea-
sure against the three similarity baselines mentioned
above. We use a spectral graph clustering algorithm
proposed in (Dhillon et al, 2007) to perform the
clustering.
We performed our experiments on three different
data sets. The three data sets are explained below.
? AAN Data: The ACL Anthology is a collec-
tion of papers from the Computational Lin-
guistics journal as well as proceedings from
ACL conferences and workshops and includes
15, 160 papers. To build the ACL Anthology
Network (AAN), (Radev et al, 2009) manu-
ally performed some preprocessing tasks in-
cluding parsing references and building the net-
work metadata, the citation, and the author col-
laboration networks. The full AAN includes
the raw text of all the papers in addition to full
citation and collaboration networks.
We chose a subset of papers in 3 topics (Ma-
47
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  10  20  30  40  50  60  70
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(a) AAN
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(b) Cornell
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(c) Texas
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45  50
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(d) Washington
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(e) Wisconsin
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 50  100  150  200  250  300  350  400  450  500
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(f) Cora
Figure 1: Classification Accuracy on the different data sets. The number of points labeled is plotted along
the x-axis and the y-axis shows the classification accuracy on the unlabeled data.
chine Translation, Dependency Parsing, Sum-
marization) from the ACL anthology. These
topics are three main research areas in Natural
Language Processing (NLP). Specifically, we
collected all papers which were cited by pa-
pers whose titles contain any of the following
phrases, ??Dependency Parsing??, ??Machine
Translation??, ??Summarization??. From this
list, we removed all the papers which contained
any of the above phrases in their title because
48
this would make the clustering task easy. The
pruned list contains 1190 papers. We manually
classified each paper into four classes (Depen-
dency Parsing, Machine Translation, Summa-
rization, Other) by considering the full text of
the paper. The manually cleaned data set con-
sists of 275Machine Translation papers, 73 De-
pendency Parsing papers and 32 Summariza-
tion papers. Table 1 lists a few sample papers
from each class.
WebKB(Sen et al, 2008): The data set con-
sists of a subset of the original WebKB data set.
The corpus consists of 877 web pages collected
from four different universities. Each web page
is represented by a 0/1-valued word vector with
1703 unique words after stemming and remov-
ing stopwords. All words with document fre-
quency less than 10 were removed.
Cora(Sen et al, 2008): The Cora dataset con-
sists of 2708 scientific publications classified
into one of seven classes. The citation network
consists of 5429 links. Each publication in the
dataset is described by a 0/1-valued word vec-
tor indicating the absence/presence of the cor-
responding word from the dictionary. The dic-
tionary consists of 1433 unique words.
For all the data sets, we constructed two graphs,
the kewyord feature graph and the link similarity
graph. The keyword feature layer graph, Gf =
(Vf , Ef , wf ) is a weighted graph where Vf is the
set of all features. The edge weight between key-
words fi and fj represents the similarity between
the features. The edge weights are initialized to the
cosine similarity between their corresponding doc-
ument vectors. The link similarity graph, Go =
(Vo, Eo, wo) is another weighted graph where Vo
is the set of objects. The edge weight represents
the similarity between the documents and is initial-
ized to the similarity between the documents due to
the link structure. The link similarity between two
documents is computed using the similarity mea-
sure proposed by (Harel and Koren, 2001) on the
citation graph. We also performed experiments by
initializing the similarity between documents to the
keyword similarity. Although, our algorithm still
outperforms other algorithms and the baselines (not
shown due to space restrictions), the accuracy using
citation similarity is higher.
7 Results and Discussion
Figure 1 shows the accuracy of the classification ob-
tained using different similarity measures. It can be
seen that the proposed algorithm (both the variants)
performs much better than other similarity measures
by a large margin. The algorithm performs much
better when more information is provided in the
form of TF-IDF scores. We attribute this to the
rich representation of the data. In our algorithm, the
data is represented as a set of heterogeneous graphs
(layers) which are connected together instead of the
normal feature vector representation. Thus, we can
leverage on the similarity between the keywords and
the objects (documents) to iteratively improve sim-
ilarity in both layers. Whereas, in the case of the
algorithm in (Zhou and Burges, 2007) all the graphs
are isolated homogeneous graphs. Hence there is no
information transfer across the different graphs.
For the clustering task, we use Normalized Mu-
tual Information (NMI) (Strehl and Ghosh, 2002)
between the ground truth clusters and the outputted
clustering as the measure of clustering accuracy.
Table 2 shows the Normalized Mutual Informa-
tion scores obtained by the different similarity mea-
sures on the different data sets.
8 Conclusion
In this paper, we have proposed a novel approach
to compute similarity between documents and key-
words iteratively. We formalized the problem of
similarity estimation as an optimization problem in-
duced by a regularization framework over edges in
multiple graphs. We propose an efficient, iterative
algorithm based on Alternating Optimization (AO)
which has a neat, intuitive interpretation in terms
of random walks over multiple graphs. We demon-
strated the improved performance of the proposed
algorithm over many different baselines and a state-
of-the-art classifcation algorithm and a similarity
measure which uses the same information as given
to our algorithm.
49
Similarity Measure AAN Texas Wisconsin Washington Cornell Cora
Content Similarity (Cosine) 0.66 0.34 0.42 0.59 0.63 0.48
Link Similarity 0.45 0.49 0.39 0.52 0.56 0.52
Linear Combination 0.69 0.54 0.46 0.54 0.68 0.54
Unified Similarity 0.78 0.69 0.54 0.66 0.72 0.64
Unified Similarity-Binary 0.80 0.68 0.56 0.69 0.74 0.66
Unified Similarity-TFIDF 0.84 0.70 0.60 0.72 0.78 0.70
Table 2: Normalized Mutual Information scores of the different similarity measures on the different data
sets
References
Francis R. Bach, Gert R. G. Lanckriet, and Michael I.
Jordan. 2004. Multiple kernel learning, conic duality,
and the smo algorithm. In Proceedings of the twenty-
first international conference on Machine learning,
ICML ?04, pages 6?, New York, NY, USA. ACM.
James Bezdek and Richard Hathaway. 2002. Some notes
on alternating optimization. In Nikhil Pal and Michio
Sugeno, editors, Advances in Soft Computing AFSS
2002, volume 2275 of Lecture Notes in Computer Sci-
ence, pages 187?195. Springer Berlin.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics.
Corinna Cortes, Mehryar. Mohri, and Afshin Ros-
tamizadeh. 2009. Learning non-linear combinations
of kernels. In In NIPS.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-theoretic co-
clustering. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?03, pages 89?98, New York, NY,
USA. ACM.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors
a multilevel approach. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 29(11):1944?
1957, November.
William A. Gale and Kenneth Ward Church. 1991. A
program for aligning sentences in bilingual corpora.
In In Proceedings of ACL.
David Harel and Yehuda Koren. 2001. On clustering us-
ing random walks. In Foundations of Software Tech-
nology and Theoretical Computer Science 2245, pages
18?41. Springer-Verlag.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of EMNLP.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over multiple
graphs for similarity learning. In In ICDM.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In In Proceedings of the ACL Workshop on Nat-
ural Language Processing and Information Retrieval
for Digital Libraries.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
Alexander Strehl and Joydeep Ghosh. 2002. Cluster en-
sembles: a knowledge reuse framework for combining
partitionings. In Eighteenth national conference on
Artificial intelligence, pages 93?98, Menlo Park, CA,
USA. American Association for Artificial Intelligence.
Dengyong Zhou and Christopher J. C. Burges. 2007.
Spectral clustering and transductive learning with mul-
tiple views. In ICML ?07, pages 1159?1166, New
York, NY, USA.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In ICML 2003, pages 912?
919.
50
Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1?12,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Rediscovering ACL Discoveries Through the Lens of ACL Anthology
Network Citing Sentences
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Abstract
The ACL Anthology Network (AAN)1 is a
comprehensive manually curated networked
database of citations and collaborations in the
field of Computational Linguistics. Each cita-
tion edge in AAN is associated with one or
more citing sentences. A citing sentence is
one that appears in a scientific article and con-
tains an explicit reference to another article. In
this paper, we shed the light on the usefulness
of AAN citing sentences for understanding re-
search trends and summarizing previous dis-
coveries and contributions. We also propose
and motivate several different uses and appli-
cations of citing sentences.
1 Introduction
The ACL Anthology2 is one of the most success-
ful initiatives of the Association for Computational
Linguistics (ACL). It was initiated by Steven Bird
in 2001 and is now maintained by Min-Yen Kan. It
includes all papers published by ACL and related or-
ganizations as well as the Computational Linguistics
journal over a period of four decades.
The ACL Anthology Network (AAN) is another
successful initiative built on top of the ACL Anthol-
ogy. It was started in 2007 by our group (Radev
et al, 2009) at the University of Michigan. AAN
provides citation and collaboration networks of the
articles included in the ACL Anthology (excluding
book reviews). AAN also includes rankings of pa-
pers and authors based on their centrality statistics
1http://clair.si.umich.edu/anthology/
2http://www.aclweb.org/anthology-new/
in the citation and collaboration networks. It also
includes the citing sentences associated with each
citation link. These sentences were extracted auto-
matically using pattern matching and then cleaned
manually. Table 1 shows some statistics of the cur-
rent release of AAN.
The text surrounding citations in scientific publi-
cations has been studied and used in previous work.
Nanba and Okumura (1999) used the term citing
area to refer to citing sentences. They define the cit-
ing area as the succession of sentences that appear
around the location of a given reference in a scien-
tific paper and has connection to it. They proposed
a rule-based algorithm to identify the citing area of
a given reference. In (Nanba et al, 2000) they use
their citing area identification algorithm to identify
the purpose of citation (i.e. the author?s reason for
citing a given paper.)
Nakov et al (2004) use the term citances to refer
to citing sentences. They explored several different
uses of citances including the creation of training
and testing data for semantic analysis, synonym set
creation, database curation, summarization, and in-
formation retrieval.
Other previous studies have used citing sentences
in various applications such as: scientific paper
summarization (Elkiss et al, 2008; Qazvinian and
Radev, 2008; Mei and Zhai, 2008; Qazvinian et al,
2010; Qazvinian and Radev, 2010; Abu-Jbara and
Radev, 2011a), automatic survey generation (Nanba
et al, 2000; Mohammad et al, 2009), and citation
function classification (Nanba et al, 2000; Teufel
et al, 2006; Siddharthan and Teufel, 2007; Teufel,
2007).
1
Number of papers 18,290
Number of authors 14,799
Number of venues 341
Number of paper citations 84,237
Citation network diameter 22
Collaboration network diameter 15
Number of citing sentences 77,753
Table 1: Statistics of AAN 2011 release
In this paper, we focus on the usefulness of the
citing sentences included in AAN. We propose sev-
eral uses of citing sentences such as analyzing the
trends of research, understanding the impact of re-
search and how this impact changes over time, sum-
marizing the contributions of a researcher, summa-
rizing the discoveries in a certain research field,
and providing high quality data for Natural Lan-
guage Processing tasks. In the rest of this paper
we present some of these ideas and provide exam-
ples from AAN to demonstrate their applicability.
Some of these ideas have been explored in previous
work, but we believe that they still need further ex-
ploration. However, most of the ideas are novel to
our knowledge. We present our ideas in the follow-
ing sections.
2 Temporal Analysis of Citations
The interest in studying citations stems from the fact
that bibliometric measures are commonly used to es-
timate the impact of a researcher?s work (Borgman
and Furner, 2002; Luukkonen, 1992). Several pre-
vious studies have performed temporal analysis of
citation links (Amblard et al, 2011; Mazloumian et
al., 2011; Redner, 2005) to see how the impact of
research and the relations between research topics
evolve overtime. These studies focused on observ-
ing how the number of incoming citations to a given
article or a set of related articles change over time.
However, the number of incoming citations is often
not the only factor that changes with time. We be-
lieve that analyzing the text of citing sentences al-
lows researchers to observe the change in other di-
mensions such as the purpose of citation, the polarity
of citations, and the research trends. The following
subsections discuss some of these dimensions.
Comparison Contrast/Comparison in Results, Method, or
Goals
Basis Author uses cited work as basis or starting point
Use Author uses tools, algorithms, data, or defini-
tions
Description Neutral description of cited work
Weakness Limitation or weakness of cited work
Table 2: Annotation scheme for citation purpose
2.1 Temporal Analysis of Citation Purpose
Teufel et al (2006) has shown that the purpose of
citation can be determined by analyzing the text of
citing sentences. We hypothesize that performing
a temporal analysis of the purpose for citing a pa-
per gives a better picture about its impact. As a
proof of concept, we annotated all the citing sen-
tences in AAN that cite the top 10 cited papers from
the 1980?s with citation purpose labels. The labels
we used for annotation are based on Teufel et al?s
annotation scheme and are described in Table 2. We
counted the number of times the paper was cited
for each purpose in each year since its publication
date. This analysis revealed interesting observations
about the paper impacts. We will discuss these ob-
servations in Section 2.3. Figure 1 shows the change
in the ratio of each purpose with time for Shieber?s
(1985) work on parsing.
2.2 Temporal Analysis of Citation Polarity
The bibliometric measures that are used to estimate
the impact of research are often computed based on
the number of citations it received. This number is
taken as a proxy for the relevance and the quality of
the published work. It, however, ignores the fact that
citations do not necessarily always represent posi-
tive feedback. Many of the citations that a publica-
tion receives are neutral citations, and citations that
represent negative criticism are not uncommon. To
validate this intuition, we annotated about 2000 cit-
ing sentences from AAN for citation polarity. We
found that only 30% of citations are positive, 4.3%
are negative, and the rest are neutral. In another pub-
lished study, Athar (2011) annotated 8736 citations
from AAN with their polarity and found that only
10% of citations are positive, 3% are negative and
the rest were all neutral. We believe that consider-
ing the polarity of citations when conducting tem-
poral analysis of citations gives more insight about
2
-10
0
10
20
30
40
50
60
70
80
1985-1987 1987-1989 1989-1991 1991-1993 1993-1995 1995-1997 1997-1999 1999-2001 2001-2003 2007-2009
comparison
basis
using
weakness
descriptive
Figure 1: Change in the citation purpose of Shieber (1985) paper
0
20
40
60
80
100
120 NeutralPosivtiveNegative
Figure 2: Change in the polarity of the sentences citing
Church (1988) paper
how the way a published work is perceived by the re-
search community over time. As a proof of concept,
we annotated the polarity of citing sentences for the
top 10 cited papers in AAN that were published in
the 1980?s. We split the year range of citations into
two-year slots and counted the number of positive,
negative, and neutral citations that each paper re-
ceived during that time slot. We observed how the
ratios of each category changed overtime. Figure 2
shows the result of this analysis when applied to the
work of Kenneth Church (1988) on part-of-speech
tagging.
2.3 Predict Emergence of New Techniques or
Decline of Impact of Old Techniques.
The ideas discussed in Sections 2.1 and 2.2 and the
results illustrated in Figures 1 and 2 suggest that
studying the change in citation purpose and cita-
tion polarity allow us to predict the emergence of
new techniques or the decline in impact of old tech-
niques. For example, the analysis illustrated in Fig-
ure 2 shows that the work of Ken Church (1988)
on part-of-speech tagging received significant posi-
tive feedback during the 1990s and until early 2000s
before it started to receive more negative feedback.
This probably can be explained by the emergence
of better statistical models for part-of-speech (POS)
tagging (e.g. Conditional Random Fields (Lafferty
et al, 2001)) that outperformed Church?s approach.
However, as indicated by the neutral citation curve,
Church?s work continued to be cited as a classical
pioneering research on the POS tagging task, but
not as the state-of-the-art approach. Similar anal-
ysis can be applied to the change in citation purpose
of Shieber (1985) as illustrated in Figure 1
2.4 Study the Dynamics of Research
In recent research, Gupta and Manning (2011) con-
ducted a study that tries to understand the dynamics
of research in computational linguistics (CL). They
analyzed the abstracts of CL papers included in the
ACL Anthology Reference Corpus. They extracted
the contributions, the domain of application, and the
3
apply propose extend system
Abstracts 1368 2856 425 5065
Citing Sentences 2534 3902 917 6633
Table 3: Comparison of trigger word occurrences in ab-
stracts vs citing sentences.
techniques and tools used in each paper. They com-
bined this information with pre-calculated article-to-
community assignments to study the influence of a
community on others in terms of techniques bor-
rowed and the maturing of some communities to
solve problems from other domains. We hypothe-
size that conducting such an analysis using the cit-
ing sentences of papers instead of (or in combination
with) abstracts leads to a more accurate picture of
research dynamics and the interaction between dif-
ferent research communities. There are several intu-
itions that support this hypothesis.
First, previous research (Elkiss et al, 2008) has
shown that the citing sentences that cite a paper are
more focused and more concise than the paper ab-
stract, and that they consistently contain additional
information that does not appear in abstracts. This
means that additional characteristics of a paper can
be extracted from citing sentences that cannot be
extracted from abstracts. To verify this, we com-
pared abstracts vs citing sentences (within AAN)
in terms of the number of occurrences of the trig-
ger words that Gupta and Manning (2011) deemed
to be indicative of paper characteristics (Table 3).
All the abstracts and citing sentences included in
the 2011 release of AAN were used to get these
numbers. The numbers clearly show that the trig-
ger words appear more frequently in the set of cit-
ing sentences of papers than they do in the paper
abstracts. We also found many papers that none of
the trigger words appeared in their abstracts, while
they do appear in their citing sentences. This sug-
gests that more paper properties (contributions, tech-
niques used, etc.) could be extracted from citations
than from abstracts.
Second, while the contributions included in an ab-
stract are the claims of the paper author(s), the con-
tributions highlighted in citing sentences are collec-
tively deemed to be important by peer researchers.
This means that the contributions extracted from ci-
Rank
word 1980s 1990s 2000s
grammar 22 71 123
model 75 72 26
rules 77 89 148
statistical - 69 74
syntax 257 1018 683
summarization - 880 359
Table 4: Ranks of selected keywords in citing sentences
to papers published in 80s, 90s and 2000s
tations are more important from the viewpoint of the
community and are likely to reflect research trends
more accurately.
We performed another simple experiment that
demonstrates the use of citing sentences to track the
changes in the focus of research. We split the set of
citing sentences in AAN into three subsets: the set
of citing sentences that cite papers from 1980s, the
set of citing sentences that cite papers from 1990s,
and the set of citing sentences that cite papers from
2000s. We counted the frequencies of words in each
of the three sets. Then, we ranked the words in each
set by the decreasing order of their frequencies. We
selected a number of keywords and compared their
ranks in the three year ranges. Some of these key-
words are listed in Table 4. This analysis shows, for
example, that there was more focus on ?grammar? in
the computational linguistics research in the 1980s
then this focus declined with time as indicated by the
lower rank of the keyword ?grammar? in the 1990s
and 2000s. Similarly, rule based methods were pop-
ular in the 1980s and 1990s but their popularity de-
clined significantly in the 2000s.
3 Scientific Literature Summarization
Using Citing Sentences
The fact that citing sentences cover different aspects
of the cited paper and highlight its most important
contributions motivates the idea of using citing sen-
tences to summarize research. The comparison that
Elkiss et al (2008) performed between abstracts and
citing sentences suggests that a summary generated
from citing sentences will be different and proba-
bly more concise and informative than the paper
abstract or a summary generated from the full text
of the paper. For example, Table 5 shows the ab-
stract of Resnik (1999) and 5 selected sentences that
cite it in AAN. We notice that citing sentences con-
4
tain additional facts that are not in the abstract, not
only ones that summarize the paper contributions,
but also those that criticize it (e.g., the last citing
sentence in the Table).
Previous work has explored this research direc-
tion. Qazvinian and Radev (2008) proposed a
method for summarizing scientific articles by build-
ing a similarity network of the sentences that cite
it, and then applying network analysis techniques to
find a set of sentences that covers as much of the
paper facts as possible. Qazvinian et al (2010) pro-
posed another summarization method that first ex-
tracts a number of important key phrases from the
set of citing sentences, and then finds the best sub-
set of sentences that covers as many key phrases as
possible.
These works focused on analyzing the citing sen-
tences and selecting a representative subset that cov-
ers the different aspects of the summarized article.
In recent work, Abu-Jbara and Radev (2011b) raised
the issue of coherence and readability in summaries
generated from citing sentences. They added a pre-
processing and postprocessing steps to the summa-
rization pipeline. In the preprocessing step, they use
a supervised classification approach to rule out ir-
relevant sentences or fragments of sentences. In the
postprocessing step, they improve the summary co-
herence and readability by reordering the sentences,
removing extraneous text (e.g. redundant mentions
of author names and publication year).
Mohammed et al (2009) went beyond single pa-
per summarization. They investigated the useful-
ness of directly summarizing citation texts in the
automatic creation of technical surveys. They gen-
erated surveys from a set of Question Answering
(QA) and Dependency Parsing (DP) papers, their ab-
stracts, and their citation texts. The evaluation of the
generated surveys shows that both citation texts and
abstracts have unique survey-worthy information. It
is worth noting that all the aforementioned research
on citation-based summarization used the ACL An-
thology Network (AAN) for evaluation.
4 Controversy Identification
Some arguments and claims made by researchers
may get disputed by other researchers (Teufel,
1999). The following are examples of citing
sentences that dispute previous work.
(1) Even though prior work (Teufel et al, 2006) argues that citation
text is unsuitable for summarization, we show that in the framework
of multi-document survey creation, citation texts can play a crucial role.
(2) Mining the Web for bilingual text (Resnik, 1999) is not
likely to provide sufficient quantities of high quality data.
In many cases, it is useful to know which ar-
guments were confirmed and accepted by the
research community and which ones where dis-
puted or even rejected. We believe that analyzing
citation text helps identify these contrasting views
automatically.
5 Comparison of Different Techniques
Citing sentences that compare different tech-
niques or compare the techniques proposed by
the author to previous work are common. The fol-
lowing sentences are examples of such comparisons.
(3) In (Zollmann et al, 2008), an interesting comparison be-
tween phrase-based, hierarchical and syntax-augmented models is
carried out, concluding that hierarchical and syntax-based models
slightly outperform phrase-based models under large data conditions
and for sufficiently non-monotonic language pairs.
(4) Brill?s results demonstrate that this approach can outper-
form the Hidden Markov Model approaches that are frequently used
for part-of-speech tagging (Jelinek, 1985; Church, 1988; DeRose,
1988; Cutting et al, 1992; Weischedel et al, 1993), as well as showing
promise for other applications.
(5) Our highest scores of 90.8% LP and 90.5% LR outperform
the scores of the best previously published parser by Charniak (2000)
who obtains 90.1% for both LP and LR.
Extracting such comparisons from citations can be
of great benefit to researchers. It will allow them
to quickly determine which technique works better
for their tasks. To verify that citation text could
be a good source for extracting comparisons, we
created a list of words and phrases that are usually
used to express comparisons and counted their
frequency in AAN citing sentences. We found, for
example, that the word compare (at its variations)
5
Abstract STRAND (Resnik, 1998) is a language-independent system for automatic discovery of text in parallel translation on the World
Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders
of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus
comprising 2491 English-French document pairs, approximately 1.5 million words per language.
Selected
Citing
Sentences
Many research ideas have exploited the Web in unsupervised or weakly supervised algorithms for natural language processing
(e.g. , Resnik (1999))
Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest.
In Resnik (1999), the Web is harvested in search of pages that are available in two languages, with the aim of building parallel
corpora for any pair of target languages.
The STRAND system of (Resnik, 1999), uses structural markup information from the pages, without looking at their content, to
attempt to align them.
Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.
Table 5: Comparison of the abstract and a selected set of sentences that cite Resnik (1999) work
appears in about 4000 sentences, and that the words
outperform and contrast each appears in about 1000
citing sentences.
6 Ontology Creation
It is useful for researchers to know which tasks
and research problems are important, and what
techniques and tools are usually used with them.
Citation text is a good source of such information.
For example, sentence (6) below shows three
different techniques (underlined) that were used to
extend tools and resources that were created for
English so that they work for other languages. For
another example, sentence (7) shows different tasks
in which re-ranking has been successfully applied.
These relations can be easily extracted from citing
sentences and can be possibly used to build an
ontology of tasks, methods, tools, and the relations
between them.
(6) Another strain of research has sought to exploit resources and tools
in some languages (especially English) to construct similar resources
and tools for other languages, through heuristic projection (Yarowsky
and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett
and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011;
McDonald et al, 2011) or inference (Smith and Smith, 2004).
(7) (Re)rankers have been successfully applied to numerous
NLP tasks, such as parse selection (Osborne and Baldridge, 2004;
Toutanova et al, 2004), parse reranking (Collins and Duffy, 2002;
Charniak and Johnson, 2005), question-answering (Ravichandran et
al., 2003).
7 Paraphrase Extraction
It is common that multiple citing sentences high-
light the same facts about a cited paper. Since these
sentences were written by different authors, they
often use different wording to describe the cited
paper facts. This motivates the idea of using citing
sentences to create data sets for paraphrase extrac-
tion. For example, sentences (8) and (9) below both
cite (Turney, 2002) and highlight the same aspect
of Turney?s work using slightly different wordings.
Therefore, sentences (8) and (9) can be considered
paraphrases of each other.
(8) In (Turney, 2002), an unsupervised learning algorithm was
proposed to classify reviews as recommended or not recommended
by averaging sentiment annotation of phrases in reviews that contain
adjectives or adverbs.
(9) For example, Turney (2002) proposes a method to classify
reviews as recommended/not recommended, based on the average
semantic orientation of the review.
The paraphrase annotation of citing sentences
consists of manually labeling which sentence
consists of what facts. Then, if two citing sentences
consist of the same set of facts, they are labeled
as paraphrases of each other. For example, if a
paper has 50 sentences citing it, this gives us a
paraphrasing data set that consists of 50*49 = 2450
pairs. As a proof of concept, we annotated 25 papers
from AAN using the annotation method described
above. This data set consisted of 33,683 sentence
pairs of which 8,704 are paraphrases.
The idea of using citing sentences to create data
sets for paraphrase extraction was initially suggested
6
by Nakov et al (2004) who proposed an algorithm
that extracts paraphrases from citing sentences us-
ing rules based on automatic named entity annota-
tion and the dependency paths between them.
8 Scientific Article Classification
Automatic classification of scientific articles is one
of the important tasks for creating publication
databases. A variety of machine learning algorithms
have been proposed for this task. Many of these
methods perform the classification based on the title,
the abstract, or the full text of the article. Some other
methods used citation links in addition to content to
make classification decisions. Cao and Gao (2005)
proposed a two-phase classification system. The
system first applies a content-based statistical clas-
sification method which is similar to general text
classification. In the second phase, the system uses
an iterative method to update the labels of classified
instances using citation links. A similar approach
is also proposed by Zhang et al (2006). These ap-
proaches use citation links only to improve classifi-
cation decisions that were made based on content.
We hypothesize that using the text of citing sen-
tences in addition to citation structure and content
leads to more accurate classification than using the
content and citation links only.
9 Terminology Translation
Citing sentences can also be used to improve
machine translation systems by using citing sen-
tences from different languages to build parallel
corpus of terms and their translations. This can
be done by identifying articles written in different
languages that cite a common target paper, then
extracting the citing sentences from each paper.
Word alignment techniques can then be applied to
the text surrounding the reference to the common
target paper. The aligned words from each source
can then be extracted and used as translations of the
same term. Sentences (10) and (11) below illustrate
how the application of this proposed method can
identify that the underlined terms in sentence 10
(Spanish) and sentence 11 (English) are translations
of each other.
(10) Spanish: Se comprobo? que la agrupacio?n por bloques
ofrec??a mejores resultados que, la introduccio?n de vocabulario (Hearst,
1997) o las cadenas le?xicas (Hearst, 1994) y, por tanto, es la que se ha
utilizado en la segunda fase del algoritmo.
(11) English: This can be done either by analyzing the number
of overlapping lexical chains (Hearst, 1994) or by building a
short-range and long-range language model (Beeferman et al, 1999).
10 Other Uses of Citing Sentences
Nakov et al (2004) proposed several other uses of
citing sentences. First, they suggested using them as
a source for unannotated comparable corpora. Such
comparable corpora can be used in several applica-
tions such as paraphrase extraction as we showed
earlier. They also noticed that the scientific liter-
ature is rife with abbreviations and synonyms, and
hence, citing sentences referring to the same article
may allow synonyms to be identified and recorded.
They also proposed using citing sentences to build
a model of the different ways used to express a re-
lationship between two entities. They hypothesized
that this model can help improve both relation ex-
traction and named entity recognition systems. Fi-
nally, they proposed improving the indexing and
ranking of publications by considering, in addition
to the content of the publication, the text of citing
sentences that cite it and their contexts.
11 Summarizing 30 years of ACL
Discoveries Using Citing Sentences
The ACL Anthology Corpus contains all the pro-
ceedings of the Annual Meeting of the Association
of Computational Linguistics (ACL) since 1979. All
the ACL papers and their citation links and citing
sentences are included in the ACL Anthology Net-
work (ACL). In this section, we show how citing
sentences can be used to summarize the most im-
portant contributions that have been published in the
ACL conference since 1979. We selected the most
cited papers in each year and then manually picked a
citing sentence that cites a top cited and describes it
contribution. It should be noted here that the citation
counts we used for ranking papers reflect the number
of incoming citations the paper received only from
the venues included in AAN. To create the summary,
we used citing sentences that has the reference to the
cited paper in the beginning of the sentence. This is
7
1979 Carbonell (1979) discusses inferring the meaning of new words.
1980 Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the gram-
mar.
1981 Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order
logic.
1982 Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to
attachment decisions based on syntactic considerations only.
1983 Grosz et al (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse
and the choices of referring expressions.
1984 Karttunen (1984) provides examples of feature structures in which a negation operator might be useful.
1985 Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley?s algorithm by using
restriction to do top-down filtering.
1986 Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of
the neighborhood.
1987 Brennan et al (1987) propose a default ordering on transitions which correlates with discourse coherence.
1988 Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals,
and questions show initiative, while answers and acknowledgements do not.
1989 Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows.
1990 Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs.
1991 Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and
monetary information.
1992 Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity
or crossentropy are very different.
1993 Pereira et al (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic.
1994 Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries pro-
duced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics.
1995 Yarowsky (1995) describes a ?semi-unsupervised? approach to the problem of sense disambiguation of words, also using a set of
initial seeds, in this case a few high quality sense annotations.
1996 Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
1997 Collins (1997)?s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of lan-
guages: English (Collins, 1999), Czech (Collins et al , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins,
2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic.
1998 Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus
using a parsed corpus.
1999 Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of
each other.
2000 Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models.
2001 Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and
proposed a tree to string model for alignment.
2002 BLEU (Papineni et al, 2002) was devised to provide automatic evaluation of MT output.
2003 Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear
MT models.
2004 Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier
separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the
subjective texts into positive and negative.
2005 Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation.
2006 Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees.
2007 Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model
size.
2008 Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could poten-
tially incorporate exponential number of k-best list.
2009 Mintz et al (2009) uses Freebase to provide distant supervision for relation extraction.
2010 Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical
phrase-based system.
Table 6: A citation-based summary of the important contributions published in ACL conference proceedings since
1979. The top cited paper in each year is found and one citation sentence is manually picked to represent it in the
summary.
8
because such citing sentences are often high-quality,
concise summaries of the cited work. Table 6 shows
the summary of the ACL conference contributions
that we created using citing sentences.
12 Conclusion
We motivated and discussed several different uses
of citing sentences, the text surrounding citations.
We showed that citing sentences can be used to an-
alyze the dynamics of research and observe how it
trends. We also gave examples on how analyzing
the text of citing sentences can give a better under-
standing of the impact of a researcher?s work and
how this impact changes over time. In addition, we
presented several different applications that can ben-
efit from citing sentences such as scientific literature
summarization, identifying controversial arguments,
and identifying relations between techniques, tools
and tasks. We also showed how citing sentences can
provide high-quality for NLP tasks such as informa-
tion extraction, paraphrase extraction, and machine
translation. Finally, we used AAN citing sentences
to create a citation-based summary of the important
contributions included in the ACL conference publi-
cation in the past 30 years.
References
Amjad Abu-Jbara and Dragomir Radev. 2011a. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amjad Abu-Jbara and Dragomir Radev. 2011b. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 500?509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
F. Amblard, A. Casteigts, P. Flocchini, W. Quattrocioc-
chi, and N. Santoro. 2011. On the temporal analysis
of scientific network evolution. In Computational As-
pects of Social Networks (CASoN), 2011 International
Conference on, pages 169 ?174, oct.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81?87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Christine L. Borgman and Jonathan Furner. 2002. Schol-
arly communication and bibliometrics. ANNUAL RE-
VIEW OF INFORMATION SCIENCE AND TECH-
NOLOGY, 36(1):2?72.
Susan E. Brennan, Marilyn W. Friedman, and Carl J. Pol-
lard. 1987. A centering approach to pronouns. In
Proceedings of the 25th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 155?162,
Stanford, California, USA, July. Association for Com-
putational Linguistics.
Minh Duc Cao and Xiaoying Gao. 2005. Combin-
ing contents and citations for scientific document clas-
sification. In Proceedings of the 18th Australian
Joint conference on Advances in Artificial Intelligence,
AI?05, pages 143?152, Berlin, Heidelberg. Springer-
Verlag.
Jaime G. Carbonell. 1979. Towards a self-extending
parser. In Proceedings of the 17th Annual Meeting of
the Association for Computational Linguistics, pages
3?7, La Jolla, California, USA, June. Association for
Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan, June. Association for Computational
Linguistics.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1443?1452, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of the 27th Annual Meeting of the
Association for Computational Linguistics, pages 76?
83, Vancouver, British Columbia, Canada, June. Asso-
ciation for Computational Linguistics.
Kenneth Ward Church. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In Pro-
ceedings of the Second Conference on Applied Natural
Language Processing, pages 136?143, Austin, Texas,
USA, February. Association for Computational Lin-
guistics.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184?191, Santa
Cruz, California, USA, June. Association for Compu-
tational Linguistics.
9
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain, July.
Association for Computational Linguistics.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51?62.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th Annual Meeting of the As-
sociation for Computational Linguistics, pages 177?
184, Berkeley, California, USA, June. Association for
Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1983. Providing a unified account of definite
noun phrases in discourse. In Proceedings of the 21st
Annual Meeting of the Association for Computational
Linguistics, pages 44?50, Cambridge, Massachusetts,
USA, June. Association for Computational Linguis-
tics.
Sonal Gupta and Christopher Manning. 2011. Analyz-
ing the dynamics of research by extracting key as-
pects of scientific papers. In Proceedings of 5th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 1?9, Chiang Mai, Thailand, November.
Asian Federation of Natural Language Processing.
Marti A. Hearst. 1994. Multi-paragraph segmentation
expository text. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 9?16, Las Cruces, New Mexico, USA,
June. Association for Computational Linguistics.
George E. Heidorn. 1982. Experience with an easily
computed metric for ranking alternative parses. In
Proceedings of the 20th Annual Meeting of the As-
sociation for Computational Linguistics, pages 82?84,
Toronto, Ontario, Canada, June. Association for Com-
putational Linguistics.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In Proceedings of the
28th Annual Meeting of the Association for Compu-
tational Linguistics, pages 268?275, Pittsburgh, Penn-
sylvania, USA, June. Association for Computational
Linguistics.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Association for Computational Linguistics.
Megumi Kameyama. 1986. A property-sharing con-
straint in centering. In Proceedings of the 24th Annual
Meeting of the Association for Computational Linguis-
tics, pages 200?206, New York, New York, USA, July.
Association for Computational Linguistics.
Lauri Karttunen. 1984. Features and values. In Proceed-
ings of the 10th International Conference on Compu-
tational Linguistics and 22nd Annual Meeting of the
Association for Computational Linguistics, pages 28?
33, Stanford, California, USA, July. Association for
Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ?01, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics, Volume 2, pages 768?774, Mon-
treal, Quebec, Canada, August. Association for Com-
putational Linguistics.
Yang (1) Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 609?616, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Terttu Luukkonen. 1992. Is scientists? publishing be-
haviour rewardseeking? Scientometrics, 24:297?319.
10.1007/BF02017913.
Amin Mazloumian, Young-Ho Eom, Dirk Helbing, Sergi
Lozano, and Santo Fortunato. 2011. How citation
boosts promote scientific paradigm shifts and nobel
prizes. PLoS ONE, 6(5):e18975, 05.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL-08: HLT, pages 816?824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1003?1011,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
10
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584?592, Boulder, Colorado, June. Association
for Computational Linguistics.
Robert C. Moore. 1981. Problems in logical form. In
Proceedings of the 19th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 117?124,
Stanford, California, USA, June. Association for Com-
putational Linguistics.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In In Proceedings of the SI-
GIR04 workshop on Search and Discovery in Bioin-
formatics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ?99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926?931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447, Hong Kong, October. As-
sociation for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computational
Linguistics (ACL?04), Main Volume, pages 271?278,
Barcelona, Spain, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Pro-
ceedings of the 31st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 183?190,
Columbus, Ohio, USA, June. Association for Compu-
tational Linguistics.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689?696, Manchester, UK, August. Coling 2008
Organizing Committee.
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555?564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895?903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ?09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54?61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526, College Park, Maryland, USA, June. Asso-
ciation for Computational Linguistics.
Sidney Redner. 2005. Citation statistics from 110 years
of physical review. Physics Today, 58(6):49?54.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 527?
534, College Park, Maryland, USA, June. Association
for Computational Linguistics.
Stuart M. Shieber. 1985. Using restriction to ex-
tend parsing algorithms for complex-feature-based
formalisms. In Proceedings of the 23rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 145?152, Chicago, Illinois, USA, July. Associ-
ation for Computational Linguistics.
11
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 1999. Argumentative zoning: Informa-
tion extraction from scientific text. Technical report.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
Ralph M. Weischedel and John E. Black. 1980. If the
parser fails. In Proceedings of the 18th Annual Meet-
ing of the Association for Computational Linguistics,
pages 95?95, Philadelphia, Pennsylvania, USA, June.
Association for Computational Linguistics.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogues. In Proceedings of the
26th Annual Meeting of the Association for Computa-
tional Linguistics, pages 123?130, Buffalo, New York,
USA, June. Association for Computational Linguis-
tics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523?530, Toulouse, France, July.
Association for Computational Linguistics.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 189?196,
Cambridge, Massachusetts, USA, June. Association
for Computational Linguistics.
M. Zhang, X. Gao, M.D. Cao, and Yuejin Ma. 2006.
Neural networks for scientific paper classification.
In Innovative Computing, Information and Control,
2006. ICICIC ?06. First International Conference on,
volume 2, pages 51 ?54, 30 2006-sept. 1.
12
Proceedings of the TextGraphs-7 Workshop at ACL, pages 6?14,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Extracting Signed Social Networks From Text
Ahmed Hassan
Microsoft Research
Redmond, WA, USA
hassanam@microsoft.com
Amjad Abu-Jbara
EECS Department
University of Michigan
Ann Arbor, MI, USA
amjbara@umich.edu
Dragomir Radev
EECS Department
University of Michigan
Ann Arbor, MI, USA
radev@umich.edu
Abstract
Most of the research on social networks has al-
most exclusively focused on positive links be-
tween entities. There are much more insights
that we may gain by generalizing social net-
works to the signed case where both positive
and negative edges are considered. One of the
reasons why signed social networks have re-
ceived less attention that networks based on
positive links only is the lack of an explicit
notion of negative relations in most social net-
work applications. However, most such appli-
cations have text embedded in the social net-
work. Applying linguistic analysis techniques
to this text enables us to identify both positive
and negative interactions. In this work, we
propose a new method to automatically con-
struct a signed social network from text. The
resulting networks have a polarity associated
with every edge. Edge polarity is a means for
indicating a positive or negative affinity be-
tween two individuals. We apply the proposed
method to a larger amount of online discus-
sion posts. Experiments show that the pro-
posed method is capable of constructing net-
works from text with high accuracy. We also
connect out analysis to social psychology the-
ories of signed network, namely the structural
balance theory.
1 Introduction
A great body of research work has focused on so-
cial network analysis. Social network analysis plays
a huge role in understanding and improving so-
cial computing applications. Most of this research
has almost exclusively focused on positive links be-
tween individuals (e.g. friends, fans, followers,
etc.). However, if we carefully examine the relation-
ships between individuals in online communities,
we will find out that limiting links to positive inter-
actions is a very simplistic assumption. It is true that
people show positive attitude by labeling others as
friends, and showing agreement, but they also show
disagreement, and antagonism toward other mem-
bers of the online community. Discussion forums
are one example that makes it clear that considering
both positive and negative interactions is essential
for understanding the rich relationships that develop
between individuals in online communities.
If considering both negative and positive interac-
tions will provide much more insight toward under-
standing the social network, why did most of pre-
vious work only focus on positive interactions? We
think that one of the main reasons behind this is the
lack of a notion for explicitly labeling negative re-
lations. For example, most social web applications
allow people to mark others as friends, like them,
follow them, etc. However, they do not allow people
to explicitly label negative relations with others.
Previous work has built networks from discus-
sions by linking people who reply to one another.
Even though, the mere fact that X replied to Y ?s
post does show an interaction, it does not tell us any-
thing about the type of that interaction. In this case,
the type of interaction is not readily available; how-
ever it may be mined from the text that underlies
the social network. Hence, if we examine the text
exchanged between individuals, we may be able to
come up with conclusions about, not only the exis-
tence of an interaction, but also its type.
In this work, we apply Natural Language Pro-
cessing techniques to text correspondences ex-
changed between individuals to identify the under-
6
lying signed social structure in online communities.
We present and compare several algorithms for iden-
tifying user attitude and for automatically construct-
ing a signed social network representation. We ap-
ply the proposed methods to a large set of discussion
posts. We evaluate the performance using a manu-
ally labeled dataset.
The input to our algorithm is a set of text corre-
spondences exchanged between users (e.g. posts or
comments). The output is a signed network where
edges signify the existence of an interaction between
two users. The resulting network has polarity asso-
ciated with every edge. Edge polarity is a means for
indicating a positive or negative affinity between two
individuals.
The proposed method was applied to a very large
dataset of online discussions. To evaluate our auto-
mated procedure, we asked human annotators to ex-
amine text correspondences exchanged between in-
dividuals and judge whether their interaction is pos-
itive or negative. We compared the edge signs that
had been automatically identified to edges manually
created by human annotators.
We also connected our analysis to social psychol-
ogy theories, namely the Structural Balance The-
ory (Heider, 1946). The balance theory has been
shown to hold both theoretically (Heider, 1946) and
empirically (Leskovec et al, 2010b) for a variety
of social community settings. Showing that it also
holds for our automatically constructed network fur-
ther validates our results.
The rest of the paper is structured as follows. In
section 2, we review some of the related prior work
on mining sentiment from text, mining online dis-
cussions, extracting social networks from text, and
analyzing signed social networks. We define our
problem and explain our approach in Section 3. Sec-
tion 4 describes our dataset. Results and discussion
are presented in Section 5. We present a possible
application for the proposed approach in Section 6.
We conclude in Section 7.
2 Related Work
In this section, we survey several lines of research
that are related to our work.
2.1 Mining Sentiment from Text
Our general goal of mining attitude from one in-
dividual toward another makes our work related to
a huge body of work on sentiment analysis. One
such line of research is the well-studied problem
of identifying the of individual words. In previ-
ous work, Hatzivassiloglou and McKeown (1997)
proposed a method to identify the polarity of ad-
jectives based on conjunctions linking them in a
large corpus. Turney and Littman (2003) used sta-
tistical measures to find the association between a
given word and a set of positive/negative seed words.
Takamura et al (2005) used the spin model to ex-
tract word semantic orientation. Finally, Hassan and
Radev (2010) use a random walk model defined over
a word relatedness graph to classify words as either
positive or negative.
Subjectivity analysis is yet another research line
that is closely related to our general goal of mining
attitude. The objective of subjectivity analysis is to
identify text that presents opinion as opposed to ob-
jective text that presents factual information (Wiebe,
2000). Prior work on subjectivity analysis mainly
consists of two main categories: subjectivity of a
phrase or word is analyzed regardless of the context
(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000;
Banea et al, 2008), or within its context (Riloff and
Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Na-
sukawa and Yi, 2003; Popescu and Etzioni, 2005).
Hassan et al (2010) presents a method for identify-
ing sentences that display an attitude from the text
writer toward the text recipient. Our work is dif-
ferent from subjectivity analysis because we are not
only interested in discriminating between opinions
and facts. Rather, we are interested in identifying
the polarity of interactions between individuals. Our
method is not restricted to phrases or words, rather it
generalizes this to identifying the polarity of an in-
teraction between two individuals based on several
posts they exchange.
2.2 Mining Online Discussions
Our use of discussion threads as a source of data
connects us to some previous work on mining
online discussions. Lin et al (2009) proposed
a sparse coding-based model that simultaneously
models semantics and structure of threaded discus-
7
sions. Huang et al (2007) learn SVM classifiers
from data to extract (thread-title, reply) pairs. Their
objective was to build a chatbot for a certain do-
main using knowledge from online discussion fo-
rums. Shen et al (2006) proposed three clustering
methods for exploiting the temporal information in
discussion streams, as well as an algorithm based on
linguistic features to analyze discourse structure in-
formation.
2.3 Extracting Social Networks from Text
Little work has been done on the front of extracting
social relations between individuals from text. El-
son et al (2010) present a method for extracting so-
cial networks from nineteenth-century British nov-
els and serials. They link two characters based on
whether they are in conversation or not. McCal-
lum et al (2007) explored the use of structured data
such as email headers for social network construc-
tion. Gruzd and Hyrthonthwaite (2008) explored the
use of post text in discussions to study interaction
patterns in e-learning communities.
Our work is related to this line of research because
we employ natural language processing techniques
to reveal embedded social structures. Despite sim-
ilarities, our work is uniquely characterized by the
fact that we extract signed social networks from text.
2.4 Signed Social Networks
Most of the work on social networks analysis has
only focused on positive interactions. A few recent
papers have taken the signs of edges into account.
Brzozowski et al (2008) study the positive and
negative relationships between users of Essembly.
Essembly is an ideological social network that dis-
tinguishes between ideological allies and nemeses.
Kunegis et al (2009) analyze user relationships in
the Slashdot technology news site. Slashdot allows
users of the website to tag other users as friends or
foes, providing positive and negative endorsements.
Leskovec et al (2010c) study signed social networks
generated from Slashdot, Epinions, and Wikipedia.
They also connect their analysis to theories of signed
networks from social psychology. A similar study
used the same datasets for predicting positive and
negative links given their context (Leskovec et al,
2010a). Other work addressed the problem of clus-
tering signed networks by taking both positive and
negative edges into consideration (Yang et al, 2007;
Doreian and Mrvar, 2009).
All this work has been limited to analyzing a
handful of datasets for which an explicit notion of
both positive and negative relations exists. Our work
goes beyond this limitation by leveraging the power
of natural language processing to automate the dis-
covery of signed social networks using the text em-
bedded in the network.
3 Approach
The general goal of this work is to mine attitude be-
tween individuals engaged in an online discussion.
We use that to extract a signed social network rep-
resenting the interactions between different partici-
pants. Our approach consists of several steps. In
this section, we will explain how we identify senti-
ment at the word level (i.e. polarity), at the sentence
level (i.e. attitude), and finally generalize over this
to find positive/negative interactions between indi-
viduals based on their text correspondences.
The first step toward identifying attitude is to
identify polarized words. Polarized words are very
good indicators of subjective sentences and hence
we their existence will be highly correlated with the
existence of attitude. The method we use for identi-
fying word polarity is a Random Walk based method
over a word relatedness graph (Hassan and Radev,
2010).
The following step is to move to the sentence level
by examining different sentences to find out which
sentences display an attitude from the text writer to
the recipient. We train a classifier based on several
sources of information to make this prediction (Has-
san et al, 2010). We use lexical items, polarity tags,
part-of-speech tags, and dependency parse trees to
train a classifier that identifies sentences with atti-
tude.
Finally, we build a network connecting partici-
pants based on their interactions. We use the predic-
tions we made both at the word and sentence levels
to associate a sign to every edge.
3.1 Identified Positive/Negative Words
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
8
indicates the direction the word deviates from the
norm (Lehrer, 1974). Past work has demonstrated
that polarized words are very good indicators of
subjective sentences (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). We use a Random Walk
based method to identify the semantic orientation
of words (Hassan and Radev, 2010). We construct
a graph where each node represents a word/part-
of-speech pair. We connect nodes based on syn-
onyms, hypernyms, and similar-to relations from
WordNet (Miller, 1995). For words that do not
appear in WordNet, we use distributional similar-
ity (Lee, 1999) as a proxy for word relatedness.
We use a list of words with known polarity (Stone
et al, 1966) to label some of the nodes in the graph.
We then define a random walk model where the set
of nodes correspond to the state space, and transi-
tion probabilities are estimated by normalizing edge
weights. We assume that a random surfer walks
along the word relatedness graph starting from a
word with unknown polarity. The walk continues
until the surfer hits a word with a known polarity.
Seed words with known polarity act as an absorb-
ing boundary for the random walk. We calculate the
mean hitting time (Norris, 1997) from any word with
unknown polarity to the set of positive seeds and the
set of negative seeds. If the absolute difference of
the two mean hitting times is below a certain thresh-
old, the word is classified as neutral. Otherwise, it
is labeled with the class that has the smallest mean
hitting time.
3.2 Identifying Attitude from Text
The first step toward identifying attitude is to iden-
tify words with positive/negative semantic orienta-
tion. The semantic orientation or polarity of a word
indicates the direction the word deviates from the
norm (Lehrer, 1974). We use OpinionFinder (Wil-
son et al, 2005a) to identify words with positive
or negative semantic orientation. The polarity of a
word is also affected by the context where the word
appears. For example, a positive word that appears
in a negated context should have a negative polarity.
Other polarized words sometimes appear as neutral
words in some contexts. Hence, we use the method
described in (Wilson et al, 2005b) to identify the
contextual polarity of words given their isolated po-
larity. A large set of features is used for that purpose
including words, sentences, structure, and other fea-
tures.
Our overall objective is to find the direct attitude
between participants. Hence after identifying the se-
mantic orientation of individual words, we move on
to predicting which polarized expressions target the
addressee and which are not.
Sentences that show an attitude are different from
subjective sentences. Subjective sentences are sen-
tences used to express opinions, evaluations, and
speculations (Riloff and Wiebe, 2003). While ev-
ery sentence that shows an attitude is a subjective
sentence, not every subjective sentence shows an at-
titude toward the recipient. A discussion sentence
may display an opinion about any topic yet no atti-
tude.
We address the problem of identifying sentences
with attitude as a relation detection problem in a su-
pervised learning setting (Hassan et al, 2010). We
study sentences that use second person pronouns and
polarized expressions. We predict whether the sec-
ond person pronoun is related to the polarized ex-
pression or not. We regard the second person pro-
noun and the polarized expression as two entities
and try to learn a classifier that predicts whether the
two entities are related or not. The text connecting
the two entities offers a very condensed represen-
tation of the information needed to assess whether
they are related or not. For example the two sen-
tences ?you are completely unqualified? and ?you
know what, he is unqualified ...? show two differ-
ent ways the words ?you?, and ?unqualified? could
appear in a sentence. In the first case the polarized
word unqualified refers to the word you. In the sec-
ond case, the two words are not related. The se-
quence of words connecting the two entities is a
very good predictor for whether they are related or
not. However, these paths are completely lexicalized
and consequently their performance will be limited
by data sparseness. To alleviate this problem, we
use higher levels of generalization to represent the
path connecting the two tokens. These representa-
tions are the part-of-speech tags, and the shortest
path in a dependency graph connecting the two to-
kens. We represent every sentence with several rep-
resentations at different levels of generalization. For
example, the sentence your ideas are very inspiring
will be represented using lexical, polarity, part-of-
9
speech, and dependency information as follows:
LEX: ?YOUR ideas are very POS?
POS: ?YOUR NNS VBP RB JJ POS?
DEP: ?YOUR poss nsubj POS?
3.2.1 A Text Classification Approach
In this method, we treat the problem as a topic
classification problem with two topics: having pos-
itive attitude and having negative attitude. As we
are only interested in attitude between participants
rather than sentiment in general, we restrict the text
we analyze to sentences that contain mentions of the
addressee (e.g. name or second person pronouns).
A similar approach for sentiment classification has
been presented in (Pang et al, ).
We represent text using the popular bag-of-words
approach. Every piece of text is represented using
a high dimensional feature space. Every word is
considered a feature. The tf-idf weighting schema
is used to calculate feature weights. tf, or term fre-
quency, is the number of time a term t occurred in
a document d. idf, or inverse document frequency,
is a measure of the general importance of the term.
It is obtained by dividing the total number of doc-
uments by the number of documents containing the
term. The logarithm of this value is often used in-
stead of the original value.
We used Support Vector Machines (SVMs) for
classification. SVM has been shown to be highly
effective for traditional text classification. We used
the SVM Light implementation with default param-
eters (Joachims, 1999). All stop words were re-
moved and all documents were length normalized
before training.
The set of features we use are the set of unigrams,
and bigrams representing the words, part-of-speech
tags, and dependency relations connecting the two
entities. For example the following features will be
set for the previous example:
YOUR ideas, YOUR NNS, YOUR poss,
poss nsubj, ...., etc.
We use Support Vector Machines (SVM) as a
learning system because it is good with handling
high dimensional feature spaces.
3.3 Extracting the Signed Network
In this subsection, we describe the procedure we
used to build the signed network given the compo-
nents we described in the previous subsections. This
procedure consists of two main steps. The first is
building the network without signs, and the second
is assigning signs to different edges.
To build the network, we parse our data to identify
different threads, posts and senders. Every sender is
represented with a node in the network. An edge
connects two nodes if there exists an interaction be-
tween the corresponding participants. We add a di-
rected edge A? B, if A replies to B?s posts at least
n times in m different threads. We set m, and n to
2 in most of our experiments. The interaction infor-
mation (i.e. who replies to whom) can be extracted
directly from the thread structure.
Once we build the network, we move to the more
challenging task in which we associate a sign with
every edge. We have shown in the previous section
how sentences with positive and negative attitude
can be extracted from text. Unfortunately the sign
of an interaction cannot be trivially inferred from the
polarity of sentences. For example, a single negative
sentence written by A and directed to B does not
mean that the interaction between A and B is neg-
ative. One way to solve this problem would be to
compare the number of negative sentences to posi-
tive sentences in all posts between A and B and clas-
sify the interaction according to the plurality value.
We will show later, in our experiment section, that
such a simplistic method does not perform well in
predicting the sign of an interaction.
As a result, we decided to pose the problem
as a classical supervised learning problem. We
came up with a set of features that we think are
good predictors of the interaction sign, and we
train a classifier using those features on a labeled
dataset. Our features include numbers and percent-
ages of positive/negative sentences per post, posts
per thread, and so on. A sentence is labeled as posi-
tive/negative if a relation has been detected in this
sentence between a second person pronoun and a
positive/negative expression. A post is considered
positive/negative based on the majority of relations
detected in it. We use two sets of features. The first
set is related to A only or B only. The second set
10
Participant Features
Number of posts per month for A (B)
Percentage of positive posts per month for A (B)
Percentage of negative posts per month for A (B)
gender
Interaction Features
Percentage/number of positive (negative) sentences per post
Percentage/number of positive (negative) posts per thread
Discussion Topic
Table 1: Features used by the Interaction Sign Classifier.
is related to the interactions between A and B. The
features are outlined in Table 1.
4 Data
Our data consists of a large amount of discussion
threads collected from online discussion forums. We
collected around 41, 000 threads and 1.2M posts
from the period between the end of 2008 and the end
of 2010. All threads were in English and had 5 posts
or more. They covered a wide range of topics in-
cluding: politics, religion, science, etc. The data was
tokenized, sentence-split, and part-of-speech tagged
with the OpenNLP toolkit. It was parsed with the
Stanford parser (Klein and Manning, 2003).
We randomly selected 5300 posts (having approx-
imately 1000 interactions), and asked human anno-
tators to label them. Our annotators were instructed
to read all the posts exchanged between two partic-
ipants and decide whether the interaction between
them is positive or negative. We used Amazon Me-
chanical Turk for annotations. Following previous
work (Callison-Burch, 2009; Akkaya et al, 2010),
we took several precautions to maintain data in-
tegrity. We restricted annotators to those based in
the US to maintain an acceptable level of English
fluency. We also restricted annotators to those who
have more than 95% approval rate for all previous
work. Moreover, we asked three different annota-
tors to label every interaction. The label was com-
puted by taking the majority vote among the three
annotators. We refer to this data as the Interactions
Dataset.
The kappa measure between the three groups of
annotations was 0.62. To better assess the quality
of the annotations, we asked a trained annotator to
label 10% of the data. We measured the agreement
between the expert annotator and the majority label
from the Mechanical Turk. The kappa measure was
Class Pos. Neg. Weigh. Avg.
TP Rate 0.847 0.809 0.835
FP Rate 0.191 0.153 0.179
Precision 0.906 0.71 0.844
Recall 0.847 0.809 0.835
F-Measure 0.875 0.756 0.838
Accuracy - - 0.835
Table 2: Interaction sign classifier evaluation.
0.69.
We trained the classifier that detects sentences
with attitude (Section 3.1) on a set of 4000 manu-
ally annotated sentences. None of this data overlaps
with the dataset described earlier. A similar annota-
tion procedure was used to label this data. We refer
to this data as the Sentences Dataset.
5 Results and Discussion
We performed experiments on the data described
in the previous section. We trained and tested the
sentence with attitude detection classifiers described
in Section 3.1 using the Sentences Dataset. We
also trained and tested the interaction sign classi-
fier described in Section 3.3 using the Interactions
Dataset. We build one unsigned network from ev-
ery topic in the data set. This results in a signed
social network for every topic (e.g. politics, eco-
nomics,etc.). We decided to build a network for ev-
ery topic as opposed to one single network because
the relation between any two individuals may vary
across topics. In the rest of this section, we will de-
scribe the experiments we did to assess the perfor-
mance of the sentences with attitude detection and
interaction sign prediction steps.
In addition to classical evaluation, we evaluate our
results using the structural balance theory which has
been shown to hold both theoretically (Heider, 1946)
and empirically (Leskovec et al, 2010b). We val-
idate our results by showing that the automatically
extracted networks mostly agree with the theory.
5.1 Identifying Sentences with Attitude
We tested this component using the Sentences
Dataset described in Section 4. In a 10-fold cross
validation mode, the classifier achieves 80.3% accu-
racy, 81.0% precision, %79.4 recall, and 80.2% F1.
11
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 Balanced Triangles Balanced Triangles (Random)
Figure 1: Percentage of balanced triangles in extracted
network vs. random network.
5.2 Interaction Sign Classifier
We used the relation detection classifier described in
Section 3.2 to find sentences with positive and neg-
ative attitude. The output of this classifier was used
to compute the the features described in Section 3.3,
which were used to train a classifier that predicts the
sign of an interaction between any two individuals.
We used Support Vector Machines (SVM) to train
the sign interaction classifier. We report several per-
formance metrics for them in Table 2. All results
were computed using 10 fold cross validation on the
labeled data. To better assess the performance of
the proposed classifier, we compare it to a baseline
that labels the relation as negative if the percentage
of negative sentences exceeds a particular threshold,
otherwise it is labeled as positive. The thresholds
was empirically evaluated using a separate develop-
ment set. The accuracy of this baseline is only 71%.
We evaluated the importance of the features listed
in Table 1 by measuring the chi-squared statistic for
every feature with respect to the class. We found
out that the features describing the interaction be-
tween the two participants are more informative than
the ones describing individuals characteristics. The
later features are still helpful though and they im-
prove the performance by a statistically significant
amount. We also noticed that all features based on
percentages are more informative than those based
on count. The most informative features are: per-
centage of negative posts per tread, percentage of
negative sentences per post, percentage of positive
posts per thread, number of negative posts, and dis-
cussion topic.
5.3 Structural Balance Theory
The structural balance theory is a psychological the-
ory that tries to explain the dynamics of signed so-
cial interactions. It has been shown to hold both the-
oretically (Heider, 1946) and empirically (Leskovec
et al, 2010b). In this section, we study the agree-
ment between the theory and the automatically ex-
tracted networks. The theory has its origins in the
work of Heider (1946). It was then formalized in a
graph theoretic form in (Cartwright and Harary, ).
The theory is based on the principles that ?the friend
of my friend is my friend?, ?the enemy of my friend
is my enemy?, ?the friend of my enemy is my en-
emy?, and variations on these.
There are several possible ways in which trian-
gles representing the relation of three people can be
signed. The structural balance theory states that tri-
angles that have an odd number of positive signs (+
+ + and + - -) are balanced, while triangles that have
an even number of positive signs (- - - and + + -) are
not.
Even though the structural balance theory posits
some triangles as unbalanced, that does not elimi-
nate the chance of their existence. Actually, for most
observed signed structures for social groups, exact
structural balance does not hold (Doreian and Mr-
var, 1996). Davis (1967) developed the theory fur-
ther into the weak structural balance theory. In this
theory, he extended the structural balance theory to
cases where there can be more than two such mu-
tually antagonistic subgroups. Hence, he suggested
that only triangles with exactly two positive edges
are implausible in real networks, and that all other
kinds of triangles should be permissible.
In this section, we connect our analysis to the
structural balance theory. We compare the predic-
tions of edge signs made by our system to the struc-
tural balance theory by counting the frequencies of
different types of triangles in the predicted network.
Showing that our automatically constructed network
agrees with the structural balance theory further val-
idates our results.
We compute the frequency of every type of trian-
gle for ten different topics. We compare these fre-
quencies to the frequencies of triangles in a set of
random networks. We shuffle signs for all edges on
every network keeping the fractions of positive and
12
0.1
0.15
0.2
0.25
0.3
0.35
Figure 2: Percentage of negative edges across topics.
negative edges constant.
We repeat shuffling for 1000 times. Every time,
we compute the frequencies of different types of tri-
angles. We find that the all-positive triangle (+++)
is overrepresented in the generated network com-
pared to chance across all topics. We also see that
the triangle with two positive edges (++?), and the
all-negative triangle (? ? ?) are underrepresented
compared to chance across all topics. The trian-
gle with a single positive edge is slightly overrep-
resented in most but not all of the topics compared
to chance. This shows that the predicted networks
mostly agree with the structural balance theory. In
general, the percentage of balanced triangles in the
predicted networks is higher than in the shuffled net-
works, and hence the balanced triangles are signif-
icantly overrepresented compared to chance. Fig-
ure 1 compares the percentage of balanced triangles
in the predicted networks and the shuffled networks.
This proves that our automatically constructed net-
work is similar to explicit signed networks in that
they both mostly agree with the balance theory.
6 Application: Dispute Level Prediction
There are many applications that could benefit from
the signed network representation of discussions
such as community finding, stance recognition, rec-
ommendation systems, and disputed topics identifi-
cation. In this section, we will describe one such
application.
Discussion forums usually respond quickly to
new topics and events. Some of those topics usu-
ally receive more attention and more dispute than
others. We can identify such topics and in general
measure the amount of dispute every topic receives
using the extracted signed network. We computed
the percentage of negative edges to all edges for ev-
ery topic. We believe that this would act as a mea-
sure for how disputed a particular topic is. We see,
from Figure 2, that ?environment?, ?science?, and
?technology? topics are among the least disputed
topics, whereas ?terrorism?, ?abortion? and ?eco-
nomics? are among the most disputed topics. These
findings are another way of validating our predic-
tions. They also suggest another application for this
work that focuses on measuring the amount of dis-
pute different topics receive. This can be done for
more specific topics, rather than high level topics as
shown here, to identify hot topics that receive a lot
of dispute.
7 Conclusions
In this paper, we have shown that natural language
processing techniques can be reliably used to extract
signed social networks from text correspondences.
We believe that this work brings us closer to un-
derstanding the relation between language use and
social interactions and opens the door to further re-
search efforts that go beyond standard social net-
work analysis by studying the interplay of positive
and negative connections. We rigorously evaluated
the proposed methods on labeled data and connected
our analysis to social psychology theories to show
that our predictions mostly agree with them. Finally,
we presented potential applications that benefit from
the automatically extracted signed network.
References
Cem Akkaya, Alexander Conrad, Janyce Wiebe, and
Rada Mihalcea. 2010. Amazon mechanical turk for
subjectivity word sense disambiguation. In CSLDAMT
?10.
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources. In
LREC?08.
Michael J. Brzozowski, Tad Hogg, and Gabor Szabo.
2008. Friends and foes: ideological social network-
ing. In SIGCHI.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using amazon?s mechan-
ical turk. In EMNLP ?9, EMNLP ?09.
Dorwin Cartwright and Frank Harary. Structure balance:
A generalization of heiders theory. Psych. Rev.
J. A. Davis. 1967. Clustering and structural balance in
graphs. Human Relations.
Patrick Doreian and Andrej Mrvar. 1996. A partitioning
approach to structural balance. Social Networks.
13
Patrick Doreian and Andrej Mrvar. 2009. Partitioning
signed social networks. Social Networks.
David Elson, Nicholas Dames, and Kathleen McKeown.
2010. Extracting social networks from literary fiction.
In ACL 2010, Uppsala, Sweden.
Anatoliy Gruzd and Caroline Haythornthwaite. 2008.
Automated discovery and analysis of social networks
from threaded discussions. In (INSNA).
Ahmed Hassan and Dragomir Radev. 2010. Identifying
text polarity using random walks. In ACL?10.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude?: identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing.
V. Hatzivassiloglou and K. McKeown. 1997. Predicting
the semantic orientation of adjectives. In EACL?97.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000. Ef-
fects of adjective orientation and gradability on sen-
tence subjectivity. In COLING.
Fritz Heider. 1946. Attitudes and cognitive organization.
Journal of Psychology.
J. Huang, M. Zhou, and D. Yang. 2007. Extracting chat-
bot knowledge from online discussion forums. In IJ-
CAI?07.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical. MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL?03.
Je?ro?me Kunegis, Andreas Lommatzsch, and Christian
Bauckhage. 2009. The slashdot zoo: mining a social
network with negative edges. In WWW ?09.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL-1999.
A. Lehrer. 1974. Semantic fields and lezical structure.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010a. Predicting positive and negative links in online
social networks. In WWW ?10.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010b. Signed networks in social media. In CHI 2010.
Jure Leskovec, Daniel Huttenlocher, and Jon Kleinberg.
2010c. Signed networks in social media. In Proceed-
ings of the 28th international conference on Human
factors in computing systems, pages 1361?1370, New
York, NY, USA.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
and Wei Wang. 2009. Simultaneously modeling se-
mantics and structure of threaded discussions: a sparse
coding approach and its applications. In SIGIR ?09.
Andrew McCallum, Xuerui Wang, and Andre?s Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. J. Artif. Int. Res., 30:249?272, October.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM.
Tetsuya Nasukawa and Jeonghee Yi. 2003. Sentiment
analysis: capturing favorability using natural language
processing. In K-CAP ?03.
J. Norris. 1997. Markov chains. Cambridge University
Press.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up?: sentiment classification using machine
learning techniques. In EMNLP. Association for Com-
putational Linguistics.
A. Popescu and O. Etzioni. 2005. Extracting product fea-
tures and opinions from reviews. In HLT-EMNLP?05.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In EMNLP?03.
Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.
2006. Thread detection in dynamic text message
streams. In SIGIR ?06, pages 35?42.
Philip Stone, Dexter Dunphy, Marchall Smith, and Daniel
Ogilvie. 1966. The general inquirer: A computer ap-
proach to content analysis. The MIT Press.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In ACL?05.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic orientation from asso-
ciation. Transactions on Information Systems.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
evaluative and speculative language. In Proceedings
of the Second SIGdial Workshop on Discourse and Di-
alogue, pages 1?10.
Janyce Wiebe. 2000. Learning subjective adjectives
from corpora. In AAAI-IAAI.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patwardhan.
2005a. Opinionfinder: a system for subjectivity anal-
ysis. In HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP?05.
Bo Yang, William Cheung, and Jiming Liu. 2007. Com-
munity mining from signed social networks. IEEE
Trans. on Knowl. and Data Eng., 19(10).
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In EMNLP?03.
14
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82?88,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Experimental Results on the Native Language Identification Shared Task
Amjad Abu-Jbara, Rahul Jha, Eric Morley, Dragomir Radev
Department of EECS
University of Michigan
Ann Arbor, MI, USA
[amjbara, rahuljha, eamorley, radev]@umich.edu
Abstract
We present a system for automatically iden-
tifying the native language of a writer. We
experiment with a large set of features and
train them on a corpus of 9,900 essays writ-
ten in English by speakers of 11 different lan-
guages. our system achieved an accuracy of
43% on the test data, improved to 63% with
improved feature normalization. In this paper,
we present the features used in our system, de-
scribe our experiments and provide an analysis
of our results.
1 Introduction
The task of Native Language Identification (NLI)
is the task of identifying the native language of a
writer or a speaker by analyzing their writing in
English. Previous work in this area shows that
there are several linguistic cues that can be used
to do such identification. Based on their native
language, different speakers tend to make different
kinds of errors pertaining to spelling, punctuation,
and grammar (Garfield, 1964; Wong and Dras, 2009;
Kochmar, 2011). We describe the complete set of
features we considered in Section 4. We evaluate
different combinations of these features, and differ-
ent ways of normalizing them in Section 5.
There are many possible applications for an NLI
system, as noted by Kochmar (2011): finding the
origins of anonymous text; error correction in var-
ious tasks including speech recognition, part-of-
speech tagging, and parsing; and in the field of sec-
ond language acquisition for identifying learner dif-
ficulties. We are most interested in statistical ap-
proaches to this problem because it may point to-
wards fruitful avenues of research in language and
sound transfer, which are how people apply knowl-
edge of their native language, and its phonology
and orthography, respectively, to a second language.
For example, Tsur and Rappoport (2007) found that
character bigrams are quite useful for NLI, which
led them to suggest that second language learners?
word choice may in part be driven by their native
languages. Analysis of such language and sound
translation patterns might be useful in understand-
ing the process of language acquisition in humans.
2 Previous Work
The work presented in this paper was done as part
of the NLI shared task (Tetreault et al, 2013), which
is the first time this problem has been the subject
of a shared task. However, several researchers have
investigated NLI and similar problems. Authorship
attribution, a related problem, has been well stud-
ied in the literature, starting from the seminal work
on disputed Federalist Papers by Mosteller and Wal-
lace (1964). The goal of authorship attribution is
to assign a text to one author from a candidate set
82
of authors. This technique has many applications,
and has recently been used to investigate terrorist
communication (Abbasi and Chen, 2005) and dig-
ital crime (Chaski, 2005). The goal of NLI some-
what similar to authorship attribution, in that NLI
attempts to distinguish between candidate commu-
nities of people who share a common cultural and
linguistic background, while authorship attribution
distinguishes between candidate individuals.
In the earliest treatment of this problem, Koppel
et al (2005) used stylistic text features to identify
the native language of an author. They used features
based on function words, character n-grams and er-
rors and idiosyncrasies such as spelling errors and
non-standard syntactic constructions. They exper-
imented on a dataset with essays written by non-
native English speakers from five countries, Russia,
Czech Republic, Bulgaria, France and Spain, with
258 instances from each dataset. They trained a
multi-class SVM model using the above features and
reported 10-fold cross validation accuracy of 80.2%.
Tsur and Rappoport (2007) studied the problem
of NLI with a focus on language transfer, i.e. how
a seaker?s native language affects the way in which
they acquire a second language, an important area in
Second Language Acquisition research. Their fea-
ture analysis showed that character bigrams alone
can lead to a classification accuracy of about 66%
in a 5-class task. They concluded that the choice of
words people make when writing in a second lan-
guage is highly influenced by the phonology of their
native language.
Wong and Dras (2009) studied syntactic errors de-
rived from contrastive analysis as features for NLI.
They used the five languages from Koppel et al
(2008) along with Chinese and Japanese, but did not
find an improvement in classification accuracy by
adding error features based on contrastive analysis.
Later, Wong and Dras (2011) studied a more gen-
eral set of syntactic features and showed that adding
these features improved the accuracy significantly.
They also investigated classification models based
on LDA (Wong et al, 2011), but did not find them
to be useful overall. They did, however, notice that
some of the topics were capturing information that
would be useful for identifying particular native lan-
guages. They also proposed the use of adaptor gram-
mars (Johnson et al, 2007), which are a generaliza-
tion of probabilistic context-free grammars, to cap-
ture collocational pairings. In a later paper, Wong
et al explored the use of adapter grammars in de-
tail (Wong et al, 2012) and showed that an exten-
sion of adaptor grammars to discover collocations
beyond lexical words can produce features useful for
the NLI task.
3 Dataset
The experiments for this paper were performed us-
ing the TOEFL11 dataset (Blanchard et al, 2013)
provided as part of the shared task. The dataset con-
tains essays written in English from native speakers
of 11 languages (Arabic, Chinese, French, German,
Hindi, Italian, Japanese, Korean, Spanish, Telugu,
and Turkish). The corpus contains 12,099 essays per
language sampled evenly from 8 prompts or topics.
This dataset was designed specifically to support the
task of NLI and addresses some of the shortcom-
ings of earlier datasets used for research in this area.
Specifically, the dataset has been carefully selected
in order to maintain consistency in topic distribu-
tions, character encodings and annotations across
the essays from different native languages. The data
was split into three data sets: a training set com-
prising 9,900 essays, a development set comprising
1,100 essays, and a test set comprising 1,100 essays.
4 Approach
We addressed the problem as a supervised, multi-
class classification task. We trained a Support Vector
Machine (SVM) classifier on a set of lexical, syntac-
tic and dependency features extracted from the train-
ing data. We computed the minimum and maximum
values for each of the features and normalized the
values by the range (max - min). Here we describe
the features in turn.
83
Character and Word N-grams Tsur and Rap-
poport (2007) found that character bigrams were
useful for NLI, and they suggested that this may be
due to the writer?s native language influencing their
choice of words. To reflect this, we compute features
using both characters and word N-grams. For char-
acters, we consider 2,3 and 4-grams, with padding
characters at the beginning and end of each sentence.
The features are generated over the entire training
data, i.e., every n-gram occurring in the training data
is used as a feature. Similarly, we create features
with 1,2 and 3-grams of words. Each word n-gram
is used as a separate feature. We explore both binary
features for each character or word n-gram, as well
as normalized count features.
Part-Of-Speech N-grams Several investigations,
for example those conducted by Kochmar (2011)
and Wong and Dras (2011), have found that part-of-
speech tags can be useful for NLI. Therefore we in-
clude part-of-speech (POS) n-grams as features. We
parse the sentences with the Stanford Parser (Klein
and Manning, 2003) and extract the POS tags. We
use binary features describing the presence or ab-
sence of POS bigrams in a document, as well as nu-
merical features describing their relative frequency
in a document.
Function Words Koppel et al (2005) found that
function words can help identify someone?s native
language. To this end, we include a categorical fea-
ture for the presence of function words that are in-
cluded in list of 321 function words.
Use of punctuation Based on our experience
with speakers of native languages, as well as
Kochmar?s (2011) observations of written English
produced by Germanic and Romance language
speakers, we suspect that speakers of different native
languages use punctuation in different ways, pre-
sumably based on the punctuation patterns in their
native language. For example, comma placement
differs between German and English, and neither
Chinese nor Japanese requires a full stop at the end
of every sentence. To capture these kinds of patterns,
we create two features for each essay: the number of
punctuation marks used per sentence, and the num-
ber of punctuation marks used per word.
Number of Unique Stems Speakers of different
native languages might differ in the amount of vo-
cabulary they use when communicating in English.
We capture this by counting the number of unique
stems in each essay and using this as an additional
feature. The hypothesis here is that depending on the
similarity of the native language with English, the
presence of common words, and other cultural cues,
people with different native language might have ac-
cess to different amounts of vocabulary.
Misuse of Articles We count instances in which
the number of an article is inconsistent with the as-
sociated noun. To do so, we fist identify all the det
dependency relations in the essay. We then com-
pute the ratio of det relations between ?a? or ?an?
and a plural noun (NNS), to all det relations. We
also count the ratio of det relations between ?a? or
?an? and an uncountable noun, to all det relations.
We do this using a list of 288 uncountable nouns.1
Capitalization The writing systems of some lan-
guages in the data set, for example Telugu, do not
include capitalization. Furthermore, other languages
may use capitalization quite differently from En-
glish, for example German, in which all nouns are
capitalized, and French, in which nationalities are
not. Character capitalization mistakes may be com-
mon in the text written by the speakers of such lan-
guages. We compute the ratio of words with at least
two letters that are written in all caps to identify ex-
cessive capitalization. We also count the relative fre-
quency of capitalized words that appear in the mid-
dle of a sentence that are not tagged as proper nouns
by the part of speech tagger.
Tense and Aspect Frequency Verbal tense and
aspect systems vary widely between languages. En-
glish has obligatory tense (past, present, future) and
1http://www.englishclub.com/vocabulary/nouns-
uncountable-list.htm
84
aspect (imperfect, perfect, progressive) marking on
verbs. Other languages, for example French, may
require verbs to be marked for tense, but not as-
pect. Still other languages, for example Chinese,
may use adverbials and temporal phrases to com-
municate temporal and aspectual information. To
attempt to capture some of the ways learners of En-
glish may be influenced by their native language?s
system of tense and aspect, we compute two fea-
tures. First, we compute the relative frequency of
each tense and aspect in the article from the counts
of each verb POS tags (ex. VB, VBD, VBG). We
also compute the percentage of sentences that con-
tain verbs of different tenses or aspect, again using
the verb POS tags.
Missing Punctuation We compute the relative
frequency of sentences that include an introductory
phrase (e.g. however, furthermore, moreover) that is
not followed by a comma. We also count the relative
frequency of sentences that start with a subordinat-
ing conjunction (e.g. sentences starting with if, after,
before, when, even though, etc.), but do not contain
a comma.
Average Number of Syllables We count the num-
ber of syllables per word and the ratio of words with
three or more syllables. To count the number of syl-
lables in a word, we used a perl module that esti-
mates the number of syllables by applying a set of
hand-crafted rules.2.
Arc Length We calculate several features pertain-
ing to dependency arc length and direction. We
parse each sentence separately, using the Stanford
Dependency Parser, and then compute a single value
for each of these features for each document. First,
we simply compute the percentage of arcs that point
left or right (PCTARCL, PCTARCR). We also com-
pute the minumum, maximum, and mean depen-
dency arc length, ignoring arc direction. We also
compute similar features for typed dependencies:
the minimum, maximum, and mean dependency arc
2http://search.cpan.org/dist/Lingua-EN-
Syllable/Syllable.pm
length for each typed dependency; and the percent-
age of arcs for each typed dependency that go to the
left or right.
Downtoners and Intensifiers We compute three
features to describe the use of downtoners, and three
for intensifiers in each document. First, we count the
number of downtoners or intensifiers in a given doc-
ument.3 We normalize this count by the number of
tokens, types, and sentences in the document to yield
the three features capturing the use of downtoners or
intensifiers.
Production Rules We compute a set of features to
describe the relative frequency of production rules
in a given document. First, we parse each sentence
using the Stanford Parser, using the default English
PCFG (Klein and Manning, 2003). We then count
all non-terminal production rules in a given docu-
ment, and report the relative frequency of each pro-
duction rule in that document.
Subject Agreement We count the number of sen-
tences in which there appears to be a mistake in sub-
ject agreement. To do this, we first identify nsubj
and nsubjpass dependency relationships. Of these
dependencies, we count ones meeting the following
criteria as mistakes: a third person singular present
tense verb with a nominal that is not third person
singular, and a third person singular subject with a
present tense verb not marked as third person sin-
gular. We then normalize the count of errors by the
total number of nsubj and nsubj pass dependencies
in the document, and the number of sentences in the
document to produce two features.
Words per Sentence We compute both the num-
ber of tokens per line and the number of types per
3The words we count as downtoners are: ?almost?, ?alot?,
?a lot?, ?barely?, ?a bit?, ?fairly?, ?hardly?, ?just?, ?kind of?,
?least?, ?less?, ?merely?, ?mildly?, ?nearly?, ?only?, ?partially?,
?partly?, ?practically?, ?rather?, ?scarcely?, ?sort of?, ?slightly?,
and ?somewhat?. The intensifiers are: ?a good deal?, ?a great
deal?, ?absolutely?, ?altogether?, ?completely?,?enormously?,
?entirely?, ?extremely?, ?fully?, ?greatly?, ?highly?, ?intensely?,
?more?, ?most?, ?perfectly?, ?quite?, ?really?, ?so?, ?strongly?,
?super?, ?thoroughly?, ?too?, ?totally?, ?utterly?, and ?very?.
85
line.
Topic Scores We construct an unsupervised topic
model for all of the documents using Mallet (Mc-
Callum, 2002) with 100 topics, dirichlet hyperpa-
rameter reestimation every 10 rounds, and all other
options set to default values. We then use the topic
weights as features.
Passive Constructions We count the number of
times an author uses passive constructions by count-
ing the number of nsubjpass dependencies in each
document. We normalize this count in two ways to
produce two different features: dividing by the num-
ber of sentences, and dividing by the total number of
nsubj and nsubjpass dependencies.
5 Experiments and Results
We used weka (Hall et al, 2009) and libsvm (Chang
and Lin, 2011) to run the experiments. The classi-
fication was done using an SVM classifier. We ex-
perimented with different SVM kernels and different
values for the cost parameter. The best performance
was achieved with a linear kernel and cost = 0.001.
We trained the model using the combination of the
training and the development sets. We submitted the
output of the system to the NLI shared task work-
shop. Our system achieved 43.3% accuracy. Table 1
shows the confusion matrix and the precision, recall,
and F-measure for each language. After the NLI
submission deadline, we noticed that we our system
was not handling the normalization of the features
properly which resulted in the poor performance.
After fixing the problem, our system achieved 63%
accuracy on both test data and 10-fold cross valida-
tion on the entire data.
6 Analysis
We did feature analysis on the training and devel-
opment data sets using the Chi-squared test. Our
feature analysis shows that the most important fea-
tures for the classifier were topic models, charac-
ter n-grams of all orders, word unigrams and bi-
grams, POS bigrams, capitalization features, func-
tion words, production rules, and arc length. These
results are consistent with those presented in previ-
ous work done on this task.
Looking at the confusion matrix in Figure 1, we
see that Korean and Japanese were the most com-
monly confused pair of languages. Hindi and Tel-
ugu, two languages from the Indian subcontinent,
were also often confused. To analyze this further,
we did another experiment by training just a binary
classifier on Korean and Japanese using the exact
same feature set as earlier. We achieved a 10-fold
cross validation accuracy of 83.3% on this classifi-
cation task. Thus, given just these two languages,
we were able to obtain high classification accuracy.
This suggests that a potentially fruitful strategy for
NLI systems might be to fuse often-confused pairs,
such as Korean/Japanese and Hindi/Telugu, into sin-
gleton classes for the initial run, and then run a sec-
ond classifier to do a more fine grained classification
within these higher level classes.
When doing feature analysis for these two lan-
guages, we found that the character bigrams rep-
resenting the country names were some of the top
features used for classification. For example ?Kor?
occurred as a trigram frequently in essays from na-
tive language speakers of Korean. Based on this, we
designed a small experiment where we created fea-
tures corresponding to the country name associated
with each native language, e.g., ?Korea?, ?China?,
?India?, ?France?, etc. For Arabic, we used a list of
22 countries where Arabic is spoken. Just using this
feature, we obtained a 10-fold cross validation accu-
racy of 21.3% on the development set. This suggests
that in certain genres, one may be able to leverage in-
formation conveying geographical and demographic
attributes for NLI.
7 Conclusion
In this paper, we presented a supervised system for
the task of Native Language Identification. We de-
scribe and motivate several features for this task
and report results of supervised classification using
these features on a test data set consisting of 11 lan-
86
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measure
ARA 41 7 8 3 6 2 3 5 10 7 8 44.6% 41.0% 42.7%
CHI 6 38 5 2 2 8 15 8 3 3 10 40.0% 38.0% 39.0%
FRE 8 6 43 8 1 14 2 4 6 1 7 39.1% 43.0% 41.0%
GER 3 3 10 49 4 9 1 7 6 0 8 54.4% 49.0% 51.6%
HIN 5 2 6 9 34 0 3 1 3 32 5 47.9% 34.0% 39.8%
ITA 5 3 10 5 1 52 2 1 17 0 4 46.0% 52.0% 48.8%
JPN 3 11 0 1 1 3 49 26 1 1 4 37.4% 49.0% 42.4%
KOR 2 6 6 1 1 2 35 40 1 1 5 38.1% 40.0% 39.0%
SPA 4 6 14 1 1 17 6 2 38 0 11 40.9% 38.0% 39.4%
TEL 9 7 3 4 18 2 2 2 2 48 3 51.1% 48.0% 49.5%
TUR 6 6 5 7 2 4 13 9 6 1 41 38.7% 41.0% 39.8%
Accuracy = 43.0%
Table 1: The results of our original submission to the NLI shared task on the test set. These results reflect the
performance of the system that does not normalize the features properly
guages provided as part of the NLI shared task. We
found that our classifier often confused two pairs
of languages that are spoken near one another, but
are linguistically unrelated: Hindi/Telugu and Ko-
rean/Japanese. We found that we could obtain high
classification accuracy on these two pairs of lan-
guages using a binary classifier trained on just these
pairs. During our feature analysis, we also found
that certain features that happened to convey geo-
graphical and demographic information were also
informative for this task.
References
Ahmed Abbasi and Hsinchun Chen. 2005. Apply-
ing authorship analysis to extremist-group web fo-
rum messages. IEEE Intelligent Systems, 20(5):67?75,
September.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27.
Carole E. Chaski. 2005. Who?s at the keyboard: Au-
thorship attribution in digital evidence investigations.
International Journal of Digital Evidence, 4:2005.
Eugene Garfield. 1964. Can citation indexing be auto-
mated?
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for speci-
fying compositional nonparametric bayesian models.
Advances in neural information processing systems,
19:641.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Associ-
ation for Computational Linguistics.
Ekaterina Kochmar. 2011. Identification of a Writer?s
Native Langauge by Error Analysis. Ph.D. thesis.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proceedings of the eleventh ACM
SIGKDD international conference on Knowledge dis-
covery in data mining, pages 624?628, Chicago, IL.
ACM.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2008. Computational methods in authorship attribu-
tion. Journal of the American Society for information
Science and Technology, 60(1):9?26.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Frederick Mosteller and David L. Wallace. 1964. Infer-
ence and Disputed Authorship: The Federalist Papers.
Addison-Wesley, Reading, Mass.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
87
on Innovative Use of NLP for Building Educational
Applications, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Com-
putational Language Acquisition, CACLA ?07, pages
9?16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
Analysis and Native Language Identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop 2009, pages 53?61, Sydney, Aus-
tralia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
Parse Structures for Native Language Identification.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600?1610, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic Modeling for Native Language Identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124, Canberra, Australia, December.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2012. Exploring Adaptor Grammars for Native Lan-
guage Identification. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 699?709, Jeju Island, Korea,
July. Association for Computational Linguistics.
88
Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Introducing Computational Concepts in a Linguistics Olympiad
Patrick Littell
Department of Linguistics
University of British Columbia
Vancouver, BC V6T1Z4, Canada
littell@interchange.ubc.ca
Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
lsl@cs.cmu.edu
Jason Eisner
Computer Science Department
Johns Hopkins University
Baltimore, MD 21218, USA
jason@cs.jhu.edu
Dragomir R. Radev
Department of EECS
School of Information
and Department of Linguistics
University of Michigan
radev@umich.edu
Abstract
Linguistics olympiads, now offered
in more than 20 countries, provide
secondary-school students a compelling
introduction to an unfamiliar field. The
North American Computational Lin-
guistics Olympiad (NACLO) includes
computational puzzles in addition to
purely linguistic ones. This paper ex-
plores the computational subject matter
we seek to convey via NACLO, as well
as some of the challenges that arise
when adapting problems in computational
linguistics to an audience that may have
no background in computer science,
linguistics, or advanced mathematics.
We present a small library of reusable
design patterns that have proven useful
when composing puzzles appropriate for
secondary-school students.
1 What is a Linguistics Olympiad?
A linguistics olympiad (LO) (Payne and Derzhan-
ski, 2010) is a puzzle contest for secondary-school
students in which contestants compete to solve
self-contained linguistics problem sets. LOs have
their origin in the Moscow Traditional Olympiad
in Linguistics, established in 1965, and have since
spread around the world; an international contest
(http://www.ioling.org) has been held
yearly since 2003.
In an LO, every problem set is self-contained,
so no prior experience in linguistics is necessary
to compete. In fact, LO contests are fun and re-
warding for exactly this reason: by the end of the
contest, contestants are managing to read hiero-
glyphics, conjugate verbs in Swahili, and perform
other amazing feats. Furthermore, they have ac-
complished this solely through their own analyti-
cal abilities and linguistic intuition.
Based on our experience going into high
schools and presenting our material, this ?linguis-
tic? way of thinking about languages almost al-
ways comes as a novel surprise to students. They
largely think about languages as collections of
known facts that you learn in classes and from
books, not something that you can dive into and
figure out for yourself. This is a hands-on antidote
to the common public misconception that linguists
are fundamentally polyglots, rather than language
scientists, and students come out of the experience
having realized that linguistics is a very different
field (and hopefully a more compelling one) than
they had assumed it to be.
2 Computational Linguistics at the LO
Our goal, since starting the North American
Computational Linguistics Olympiad (NACLO) in
2007 (Radev et al, 2008), has been to explore how
this LO experience can be used to introduce stu-
dents to computational linguistics. Topics in com-
putational linguistics have been featured before in
LOs, occasionally in the Moscow LO and with
some regularity in the Bulgarian LO.
Our deliberations began with some trou-
bling statistics regarding enrollments in computer
science programs (Zweben, 2013). Between
2003 and 2007 enrollments in computer science
dropped dramatically. This was attributed in part
to the dip in the IT sector, but it also stemmed in
18
part from a perception problem in which teenagers
view computer science careers as mundane and
boring: ?I don?t want to be Dilbert,1 sitting in a
cubicle programming payroll software my whole
life.? This is an unrealistically narrow percep-
tion of the kinds of problems computer scientists
tackle, and NACLO began in part as a way to pub-
licize to teenagers that many interesting problems
can be approached using computational methods.
Although enrollments are not yet back to the
2003 levels, there has been a sharp increase since
2007 (Zweben, 2013). The resurgence can be at-
tributed in part to the strength of the IT sector, but
also to the realization that computer science is rel-
evant to almost every area of science and technol-
ogy (Thibodeau, 2013). NACLO aims to be part
of this trend by showing students that computer
science is used in studying fascinating problems
related to human language.
Even ?traditional? LO puzzles are inherently
computational in that they require pattern recog-
nition, abstraction, generalization, and establish-
ing and pruning a solution space. However, we
also want to teach computational linguistics more
explicitly. NACLO puzzles have featured a wide
variety of topics in computational linguistics and
computer science; they may focus on the applica-
tion itself, or on concepts, tools, and algorithms
that underlie the applications. Broadly, computa-
tional LO topics fall into three types, summarized
below.
2.1 Technological applications
NACLO has included puzzles on technologies that
most people are familiar with, including spell
checking, information retrieval, machine transla-
tion, document summarization, and dialogue sys-
tems. In a typical applications puzzle, the contes-
tants would discover how the application works,
how it handles difficult cases, or what its limita-
tions are. In ?Summer Eyes? (Radev and Hester-
berg, 2009), the contestant discovers the features
that are used for selecting sentences in a sum-
marization program, including the position of a
sentence in the article, the number of words the
sentence shares with the title, etc. In ?Spring-
ing Up Baby? (Srivastava and Bender, 2008) and
?Running on MT? (Somers, 2011), contestants ex-
plore word sense disambiguation in the context of
1An engineer in the eponymous American comic strip,
Dilbert has a famously dysfunctional workplace and unre-
warding job.
machine translation, while ?Tiger Tale? (Radev,
2011) highlights some realistic sources of knowl-
edge for machine translation such as cognates
and cross-language syntactic similarities. ?Thorny
Stems? (Breck, 2008) and ?A fox among the h?
(Littell, 2012b) introduce stemming.
2.2 Formal grammars and algorithms
Some puzzles introduce the formal tools of com-
putational linguistics and linguistic concepts that
are important in computational linguistics, of-
ten in a whimsical way. For example, ?Sk8
Parsr? (Littell, 2009) introduces shift-reduce pars-
ing by means of a hypothetical skateboarding
video game. ?Aw-TOM-uh-tuh? (Littell, 2008)
introduces a finite-state machine that determines
which strings form legal words in the Rotokas
language. ?Orwellspeak? (Eisner, 2009) asks
solvers to modify a simple context-free grammar,
and then to discover that a 4-gram model can-
not model this language without precision or re-
call errors. ?Twodee? (Eisner, 2012) invents a
two-dimensional writing system, shown below, as
a vehicle for helping students discover parsing
ambiguity?and production ambiguity?without
the full formal apparatus of grammars, nontermi-
nals, or tree notation.
?The Little Engine That Could. . . Read? (Littell
and Pustejovsky, 2012) explores quantifier mono-
tonicity, while ?Grice?s Grifter Gadgets? (Boyd-
Graber, 2013) covers Grice?s maxims as part of the
specification of a computerized game assistant.
2.3 Computational concepts
NACLO puzzles have also introduced computa-
tional concepts that go beyond computational lin-
guistics. ?Texting, Texting, One Two Three? (Lit-
tell, 2010b) and ?The Heads and Tails of Huff-
man? (DeNero, 2013) introduce data compression.
?One, Two, Tree? (Smith et al, 2012) introduces
the Catalan numbers and other recurrences via bi-
nary bracketing of ambiguous compound nouns.
19
?Nok-nok? (Fink, 2009) introduces Levenshtein
distance by describing a hypothetical typing tutor
for very bad spellers.
3 The Challenge of Writing
Computational Problems
To achieve our goals, it becomes necessary to
write computational linguistics puzzles in such a
way that they are self-contained, requiring no prior
experience in linguistics, computer science, or ad-
vanced math. This has proven very difficult, but
not impossible, and in the past seven years we have
managed to learn a lot about how to (and how not
to) write them.
Perhaps the hardest part of writing any LO puz-
zle is that authors have to remove themselves from
their knowledge and experience: to forget techni-
cal definitions of ?phrase? or ?noun? or ?string? or
?function,? and to forget the facts and insights and
history that formed our modern understanding of
these. This is doubly hard when it comes to puz-
zles involving computational methods. The ability
to write an algorithm that a computer could actu-
ally interpret is a specialized skill that we learned
through education, and it is very, very hard to back
up and imagine what it would be like to not be
able to think like this. (It is almost like trying to
remember what it was like to not be able to read?
not simply not knowing a particular alphabet or
language, but not even understanding how reading
would work.)
Here is an illustration of an interesting but
nonetheless inappropriate LO puzzle:
Here are fourteen English compound
words:
birdhouse housework
blackbird tablespoon
blackboard teacup
boardroom teaspoon
boathouse workhouse
cupboard workroom
houseboat worktable
Even if you didn?t know any English, you
could probably determine by looking at
this list which English words were used
to make up the compounds: ?black?,
?bird?, ?board?, etc...
How would you do this if you were a
computer?
This task, although potentially appropriate for a
programming competition, is inappropriate for an
LO: the intended task requires some prior knowl-
edge about what computers can and cannot do.
Note that nowhere in the puzzle itself are the prop-
erties of this imaginary computer specified. It is
assumed that the solver knows roughly the state of
modern computing machinery and what kinds of
instructions it can execute.
Imagine for a moment what a right answer to
this puzzle would look like, and then picture what
a wrong answer might look like. Your right answer
was probably an algorithm that could run on an ab-
stract computer with capabilities very much like
real computers. The wrong answer probably made
incorrect assumptions about what sorts of opera-
tions computers are capable of, or treated enor-
mously complex operations as if they were primi-
tive.2
The problem with the above puzzle is that it is
very open-ended, and in the absence of a large
body of shared knowledge between the author and
the solver, the solver cannot know what it is the
author wants or when they have solved it to the
author?s satisfaction.
In order to avoid this, it is best to set up the puz-
zle so that the ?search space? for possible answers
is relatively constrained, and the ?win? conditions
are clear. Ideally, if a contestant has solved a puz-
zle, they should know they have solved it, and thus
be able to move on confidently to the next puz-
zle.3 In this respect, LO puzzles are akin to cross-
word puzzles, problems from other Olympiads, or
online puzzle games. This feeling of accomplish-
ment is key to the kind of rewarding learning ex-
perience that have made LOs so successful.
4 Design Patterns for CL Puzzles
Over the years, we have found several reliable
strategies for turning ideas and topics from com-
putational linguistics into solvable, rewarding puz-
2Keep in mind that today?s contestants were born in the
late 1990s. They are unlikely to even remember a world with-
out ubiquitous Internet and powerful natural language search.
Their conception of ?what computers basically do? is not nec-
essarily going to be the same as those of us who encountered
computers when they were still recognizable as a kind of so-
phisticated calculator.
3This is not to say, however, that only those who solve a
puzzle in its entirety should feel accomplished or rewarded.
The best puzzles often contain layers of mysteries: it may be
that only a few will solve every mystery in the puzzle, but
most contestants come away with the satisfaction of having
discovered something.
20
zles.
Not every computational puzzle makes use of
these?some are entirely unique?but many do.
In addition, these strategies are not mutually ex-
clusive; many computational puzzles utilize sev-
eral of these at once. For example, a ?Broken Ma-
chine? puzzle may then present the solver with a
?Troublemaker? task, or an ?Assembly Required?
machine may, upon assembly, turn out to be a
?Broken? one.
4.1 Assembly Required
The solver is presented with a task to complete,
and also a partially specified algorithm for doing
so. The partial specification illustrates the de-
sired formal notation and the model of computa-
tion. But it may be missing elements, or the or-
dering or relationship between the elements is un-
clear, or some other aspect of the system remains
unfinished. The solver is asked to complete the
system so that it performs the appropriate task or
produces the appropriate outputs.
For example, NACLO 2008 included a puzzle
on stemming, ?Thorny Stems? (Breck, 2008), in
which contestants help develop an algorithm to
isolate the stems of various words. In this puzzle,
the solver is not required to invent an algorithm
ex nihilo; this would merely have rewarded those
who already understand algorithms, not introduce
algorithmic thinking to neophytes. Instead, the
overall structure of the intended algorithm (an or-
dered sequence of if-thens) is made explicit, and
the solver?s task is to fill in the details:
Rule 1: If a word ends in , then
replace with to form the
stem.
Rule 2: If a word ends in , then
replace with to form the
stem.
In another puzzle from the same contest, ?Aw-
TOM-uh-tuh? (Littell, 2008), the solver must
complete an unfinished finite-state automaton so
that it performs a language recognition task. The
solver is given a brief introduction to FSAs and a
simple sample FSA, and then given an incomplete
FSA whose labels lack edges. The solver?s task is
to place the labels on the correct edges so that the
FSA accepts certain inputs and rejects others.
Other examples of the ?Assembly Required?
pattern can be found in the puzzles ?Sk8 Parsr?
(Littell, 2009), ?The Heads and Tails of Huff-
man? (DeNero, 2013), and ?BrokEnglish!? (Lit-
tell, 2011).
4.2 Black Box
The solver is presented with the inputs to a system
and the outputs, and must work out how the system
generated the outputs. Unlike in the ?Assembly
Required? pattern, little or no information about
the algorithm is provided to the solver; the solver?s
fundamental task is to characterize this unknown
algorithm as thoroughly as possible.
For example, NACLO 2010 featured a puzzle
on Huffman text compression, ?Texting, Texting,
One Two Three? (Littell, 2010b), in which an un-
specified algorithm converts strings of letters to
strings of numbers:
Testing testing = 33222143224142341-
1222143224142341331
Does anyone copy = 33233322143131-
42343324221124232342343331
Working out the basic number-letter correspon-
dences is relatively straightforward, but the real
puzzle is working out the rationale behind these
correspondences. Some of the answers require let-
ters (like ?r? and ?x?) that do not occur anywhere
in the data, but can be deduced once the system as
a whole is fully understood.
NACLO 2009 featured a puzzle on Levenshtein
distance, ?Nok-nok!? (Fink, 2009), that also
used this pattern. In it, a spell-checker is rat-
ing how well (or poorly) a user has spelled a word.
21
User Input Correct word Output
owll owl ?almost right?
ples please ?quite close?
reqird required ?quite close?
plez please ?a bit confusing?
mispeln misspelling ?very confusing?
The solver?s task is to work out the algorithm suf-
ficiently to predict how the system would respond
to novel inputs.
Other examples of the ?Black Box? pattern can
be found in ?The Deschamps Codice? (Piperski,
2012) and ?The Little Engine that Could. . . Read?
(Littell and Pustejovsky, 2012).
Depending on the intended algorithm, the
?Black Box? pattern may or may not be appro-
priate. This pattern works best when the nature
of the transformation between input and output is
relatively straightforward and the purpose of the
transformation is relatively clear. In the Huff-
man coding puzzle, for example, the nature of
the transformation is entirely obvious (replace let-
ters with number sequences) and thus the solution
space of the puzzle is relatively constrained (figure
out which letters correspond to which number se-
quences and then try to figure out why). In the
spell-checking puzzle, the purpose of the trans-
formation is easily understood, giving the solver
a head start on figuring out which features of the
input the algorithm might be considering.
When the nature of the transformation is less
obvious?for example, the generation of numbers
of unclear significance, rating some unknown as-
pect of a text passage??Black Box? is not as ap-
propriate as the other patterns. The potential prob-
lem is that not only must the solver come up with
an algorithm on their own, they must come up with
the same algorithm the author did. Given a com-
plicated algorithm, even small implementation de-
tails may lead to very different outputs, so a solver
can even have found a basically correct solution
but nevertheless not managed to produce the in-
tended outputs.
In such cases, the ?Assembly Required? or
?Broken Machine? patterns are potentially more
appropriate.
4.3 Broken Machine
The solver is presented with a system that purports
to perform a particular task, but actually fails on
particular inputs. The solver is tasked with fig-
uring out what went wrong and, potentially, fixing
the system so that it works. In some cases, the sys-
tem simply has an error in it; in others, the system
is correct but cannot handle certain difficult cases.
NACLO has featured a wide variety of broken
machines, often with humorous outputs. ?Help my
Camera!? (Bender, 2009) features a dialogue sys-
tem that could not correctly resolve pronoun refer-
ences:
Human: ?There?s this restaurant on
Bancroft that?s supposed to be really
good that I heard about from my mother.
Can you help me find it??
Computer: ?Where did you last see your
mother??
?BrokEnglish!? (Littell, 2011) features a run-
away script that replaced certain ISO 639-1 codes
with language names:
Hebrewy, ChamorRomanianrICHebre-
wcHebrewnlandic! whEnglish you
get a FrEnglishcHebrewe momEnglisht,
cHebrewck out thICHebrewcHebrewn-
landic niCHebrewcHebrewn little pRo-
maniangram i wRomaniante.
Solvers are then tasked with determining why
this script produced such a bizarre output, and ad-
ditionally tasked with determining in what order
the replacements had to have occurred in order to
get this exact output.
?Orwellspeak? (Eisner, 2009) involves a
context-free grammar that produces sentences
that were grammatically correct but counter to the
ideals of a fictional totalitarian Party. The solver
must rewrite the grammar so that only ?correct?
thoughts can be uttered. In the second part of the
puzzle, the solver must show that Markov models
would be inherently broken.
Other examples of ?Broken Machines? are ?The
Lost Tram? (Iomdin, 2007), ?Sk8 Parsr? (Lit-
tell, 2009), ?A fox among the h? (Littell, 2012b),
?The Little Engine that Could. . . Read? (Littell and
Pustejovsky, 2012), and ?Grice?s Grifter Gadgets?
(Boyd-Graber, 2013).
4.4 Troublemaker
The solver is presented with a system and some
sample inputs and outputs, and must discover an
input that causes the system to fail, or produce out-
puts that are strange, suboptimal, or have some un-
usual property.
22
Few puzzles make use of only the ?Trouble-
maker? pattern. Many are basically ?Assembly
Required? or ?Broken Machine? puzzles that use a
?Troublemaker? task to get the contestant thinking
about the ways in which the system is limited or
imperfect. They are also often creative?the con-
testant usually invents their own inputs?and thus
can serve as a refreshing change of pace.4
NACLO 2009 featured a ?Broken Machine?
puzzle about shift-reduce parsing (?Sk8 Parsr?)
(Littell, 2009), couched in terms of a fictional
skateboarding videogame. The solver is given an
algorithm by which button presses are transformed
into skateboard trick ?combos? like those shown
below, but many well-formed ?combos? cannot
correctly be parsed due to a shift-reduce conflict.
The solver is given an example of one such class
of inputs, and then asked to discover other classes
of inputs that likewise fail.
?Troublemaker? puzzles are not always
couched in terms of bugs. ?This problem is pretty
// easy? (Radev, 2007a) asks solvers to construct
eye-catching garden path sentences. In the
Huffman text compression puzzle detailed above
(?Texting, Texting, One Two Three?) (Littell,
2010b), a ?Troublemaker? task is introduced to
get contestants thinking about the limits of com-
pression. Although the compression algorithm
is not ?broken? in any way, any compression
algorithm will ?fail? on some possible input and
return an output longer than the input, and the
solver is tasked to discover such an input.
?Troublemaker? tasks can also be found in
?Grammar Rules? (Schalley and Littell, 2013) and
?Yesbot? (Mitkov and Littell, 2013).
4If the ?Troublemaker? task asks for an input with a par-
ticular formal property (i.e., a sentence generated or not gen-
erated from a particular grammar), automated grading scripts
can determine the correctness of the answer without human
intervention. This means that contestants can get a chance
to enter ?creative? answers even in large contests (like the
NACLO Open Round) that utilize automatic grading.
4.5 Jabberwock
Not all puzzle types revolve around abstract ma-
chines. Another recurring puzzle type, the ?Jab-
berwock?, involves asking the solver to puzzle out
the syntactic or semantic properties of unknown
words. Often these words are nonsense words, but
this puzzle type can also work on natural language
data. To perform this task, solvers often have to
use the same methods that a computer would.
?We are all molistic in a way? (Radev, 2007b)
asks solvers to infer the polarity of various non-
sense adjectives based on a series of sentences.5
The teacher is danty and cloovy.
Mary is blitty but cloovy.
Strungy and struffy, Diane was a plea-
sure to watch.
Even though weasy, John is strungy.
Carla is blitty but struffy.
The solver must work out from sentences such
as these whether words like ?danty? and ?weasy?
have positive or negative associations. In doing so,
the solver has essentially constructed and solved a
semi-supervised learning problem.
In ?Gelda?s House of Gelbelgarg? (Littell,
2010a), solvers are presented with a page of fab-
ricated restaurant reviews for an entirely fictional
cuisine:
?A hidden gem in Lower Uptown! Get
the fa?rsel-fo?rsel with gorse-weebel and
you?ll have a happy stomach for a week.
And top it off with a flebba of sweet-
bolger while you?re at it!?
5The list given here includes a subset of the examples used
in the real puzzle in 2007.
23
?I found the food confusing and disori-
enting. Where is this from? I randomly
ordered the fa?rsel-fo?rsel and had to send
them back!?
Using various grammatical cues (article and pro-
noun choice, ?less? vs. ?fewer?, etc.), solvers have
to sort the items into things most likely to be dis-
crete, countable objects, things most likely to be
liquids or masses, and things most likely to be con-
tainers or measures.
This type of puzzle often violates the common
LO restriction on using nonsense words and made-
up languages, but it is not always possible to base
this sort of puzzle on a completely unfamiliar lan-
guage. Many ?Jabberwock? puzzles involve infer-
ring syntactic or semantic information about un-
known words in an otherwise known language.
The two puzzles above therefore require contes-
tants to consult their own intuitions about English.
These puzzles would have been entirely different
(and prohibitively difficult) if the language had
been completely unfamiliar.
Other Jabberwock puzzles include ?Tiger Tale?
(Radev, 2011) and ?Cat and Mouse Story? (Littell,
2012a).
4.6 Combinatorial Problems
Some puzzles effectively force the solver to design
and run an algorithm, to get an answer that would
be too difficult to compute by brute force. Such
puzzles involve computational thinking. But since
the solver only has to give the output of the algo-
rithm, there is no need to agree on a type of com-
puting device or a notation for writing algorithms
down.
Such puzzles include combinatorial tasks that
involve the counting, maximization, or existence
of linguistic objects. They require mathematical
and algorithmic skills (just as in math or program-
ming competitions), and demonstrate how these
skills apply to linguistics or NLP.
Portions of ?One, Two, Tree? (Smith et
al., 2012) and ?Twodee? (Eisner, 2012) require
solvers to count all ways to parse a sentence, or
to count all sentences of a certain type. Because
the counts are large, the solver must find the pat-
tern, which involves writing down a closed-form
formula such as 2n or a more complex dynamic
programming recurrence.
5 Conclusions
Researchers and teachers from the ACL commu-
nity are invited to contact the NACLO organizing
committee at naclo14org@umich.edu6 with
their ideas for new puzzles or new types of puz-
zles. All of the past puzzles and solutions can
be browsed at http://www.naclo.cs.cmu.
edu/practice.html. In general, puzzles in
Round 1 each year should be easier and automat-
ically gradable. Puzzles in Round 2 permit more
involved questions and answers; this is a smaller
contest in which the top Round 1 scorers (usu-
ally, the top 10 percent) can qualify for the Inter-
national Linguistic Olympiad.
Thus far, NACLO?s computational puzzles have
reached at least 6,000 students at more than 150
testing sites7 in the U.S. and Canada, as well as at
least 10,000 students in the three other English-
language countries that share LO puzzles with
NACLO.
We observe that most computational puzzles do
not need obscure languages, staying on the contes-
tant?s home turf of English and technology. This
does not mean, however, that the computational
puzzles are purely formal and lack linguistic con-
tent. Some of them in fact probe subtle facts about
English (the introspective method in linguistics),
and some of them cover areas of linguistics that
are underserved by traditional LO puzzles. Tra-
ditional LO puzzles instead ask the solver to sort
out vocabulary and basic morphophonological or
orthographic patterns in a mystery language (the
fieldwork method in linguistics). Students who en-
joy ?top-down? thinking or who are deeply inter-
ested in ?how to do things with words? may prefer
the former kind of puzzle.
Competitions are popular in many North Amer-
ican high schools, perhaps in part as a way to im-
press college admissions officers. We have ex-
ploited this to give students a taste of our inter-
disciplinary field before they choose a college ma-
jor. Some students may be specifically attracted to
NACLO by the word ?computational? or the word
?linguistics,? or may be intrigued by their juxta-
position. Many NACLO participants reveal that
they had started to study linguistics on their own
before encountering NACLO, and have welcomed
6Or nacloXXorg@umich.edu, where XX is the last
two digits of the calendar year of the upcoming February.
7NACLO tests have been given at more than 100 high
schools and more than 50 university sites; the latter are open
to students from all local high schools.
24
NACLO as an outlet for their enthusiasm and a
place where they can interact with other students
who have the same interests.
NACLO?s past puzzles remain freely available
on the web for anyone who is interested. Two
volumes of NACLO-style puzzles (most of them
from real competitions), edited by program chair
Dragomir Radev, have recently been published by
Springer (Radev, 2013a; Radev, 2013b). Adult
hobbyists and home-schooled students may dis-
cover computational linguistics through encoun-
tering these puzzles. Avid LO contestants use
them to prepare for upcoming contests. Finally,
high school and college teachers can use them
as the basis of whole-class or small-group class-
room activities that expose students to computa-
tional thinking.
Acknowledgments
We would like to thank the National Science Foun-
dation for supporting NACLO through the fol-
lowing grants: IIS0633871, BCS1137828, and
IIS0838848. We also express our gratitude to NSF
program managers Tatiana Korelsky, Terry Lan-
gendoen, and Joan Maling for their effort in ini-
tiating and maintaining NACLO. The Linguistic
Society of America and the North American Chap-
ter of the Association for Computational Linguis-
tics provide ongoing support. Other sponsors, vol-
unteers, and problem writers are too numerous to
name. They are listed on the contest booklets each
year, which can be found on the NACLO web site:
http://www.naclo.cs.cmu.edu.
References
Emily Bender. 2009. Help my camera! In
North American Computational Linguistics
Olympiad 2009. http://www.naclo.cs.cmu.edu/
assets/problems/naclo09F.pdf.
Jordan Boyd-Graber. 2013. Grice?s grifter gad-
gets. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
Eric Breck. 2008. Thorny stems. In North Amer-
ican Computational Linguistics Olympiad 2008.
http://www.naclo.cs.cmu.edu/assets/problems/
NACLO08h.pdf.
John DeNero. 2013. The heads and tails of Huff-
man. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Jason Eisner. 2009. Orwellspeak. In North Amer-
ican Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09M.pdf.
Jason Eisner. 2012. Twodee. In North
American Computational Linguistics Olympiad
2013. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Eugene Fink. 2009. Nok-nok! In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09B.pdf.
Boris Iomdin. 2007. The lost tram. In North Amer-
ican Computational Linguistics Olympiad 2007.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo07 f.pdf.
Patrick Littell and James Pustejovsky. 2012.
The little engine that could. . . read. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Patrick Littell. 2008. Aw-TOM-uh-tuh. In North
American Computational Linguistics Olympiad
2008. http://www.naclo.cs.cmu.edu/assets/
problems/NACLO08i.pdf.
Patrick Littell. 2009. Sk8 parsr. In North Ameri-
can Computational Linguistics Olympiad 2009.
http://www.naclo.cs.cmu.edu/assets/problems/
naclo09G.pdf.
Patrick Littell. 2010a. Gelda?s house of gelbel-
garg. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/A.pdf.
Patrick Littell. 2010b. Texting, texting, one two
three. In North American Computational Linguis-
tics Olympiad 2010. http://www.naclo.cs.cmu.edu/
problems2010/E.pdf.
Patrick Littell. 2011. BrokEnglish! In North Amer-
ican Computational Linguistics Olympiad 2011.
http://www.naclo.cs.cmu.edu/problems2011/E.pdf.
Patrick Littell. 2012a. Cat and mouse story. In North
American Computational Linguistics Olympiad
2012. http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND1.pdf.
Patrick Littell. 2012b. A fox among the
h. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Ruslan Mitkov and Patrick Littell. 2013. Grammar
rules. In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND2.pdf.
25
Thomas E. Payne and Ivan Derzhanski. 2010. The lin-
guistics olympiads: Academic competitions in lin-
guistics for secondary school students. In Kristin
Denham and Anne Lobeck, editors, Linguistics at
school. Cambridge University Press.
Alexander Piperski. 2012. The Deschamps
codice. In North American Computational Linguis-
tics Olympiad 2012. http://www.naclo.cs.cmu.edu/
problems2012/NACLO2012ROUND2.pdf.
Dragomir Radev and Adam Hesterberg. 2009. Sum-
mer eyes. In North American Computational Lin-
guistics Olympiad 2009. http://www.naclo.cs.cmu.
edu/assets/problems/naclo09E.pdf.
Dragomir R. Radev, Lori Levin, and Thomas E.
Payne. 2008. The North American Computa-
tional Linguistics Olympiad (NACLO). In Proceed-
ings of the Third Workshop on Issues in Teaching
Computational Linguistics, pages 87?96, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics. http://www.aclweb.org/anthology/W/W08/
W08-0211.
Dragomir Radev. 2007a. This problem is pretty //
easy. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 h.pdf.
Dragomir Radev. 2007b. We are all molistic in a
way. In North American Computational Linguis-
tics Olympiad 2007. http://www.naclo.cs.cmu.edu/
assets/problems/naclo07 a.pdf.
Dragomir Radev. 2011. Tiger tale. In North American
Computational Linguistics Olympiad 2011. http://
www.naclo.cs.cmu.edu/problems2011/F.pdf.
Dragomir Radev, editor. 2013a. Puzzles in Logic,
Languages, and Computation: The Green Book.
Springer: Berlin.
Dragomir Radev, editor. 2013b. Puzzles in Logic, Lan-
guages, and Computation: The Red Book. Springer:
Berlin.
Andrea Schalley and Patrick Littell. 2013. Grammar
rules! In North American Computational Linguis-
tics Olympiad 2013. http://www.naclo.cs.cmu.edu/
2013/NACLO2013ROUND1.pdf.
Noah Smith, Kevin Gimpel, and Jason Eisner.
2012. One, two, tree. In North American
Computational Linguistics Olympiad 2012.
http://www.naclo.cs.cmu.edu/problems2012/
NACLO2012ROUND2.pdf.
Harold Somers. 2011. Running on MT. In North
American Computational Linguistics Olympiad
2011. http://www.naclo.cs.cmu.edu/problems2011/
A.pdf.
Ankit Srivastava and Emily Bender. 2008. Springing
up baby. In North American Computational Lin-
guistics Olympiad 2008. http://www.naclo.cs.cmu.
edu/assets/problems/prob08b.pdf.
Patrick Thibodeau. 2013. Computer science en-
rollments soared last year, rising 30%, March.
http://www.computerworld.com/s/article/9237459/
Computer science enrollments soared last year
rising 30 .
Stuart Zweben. 2013. Computing degree and enroll-
ment trends, March. http://cra.org/govaffairs/blog/
wp-content/uploads/2013/03/CRA Taulbee CS
Degrees and Enrollment 2011-12.pdf.
26
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146?154,
Dublin, Ireland, August 23 2014.
Experiments in Sentence Language Identification with Groups of Similar
Languages
Ben King
Department of EECS
University of Michigan
Ann Arbor
benking@umich.edu
Dragomir Radev
Department of EECS
School of Information
University of Michigan
Ann Arbor
radev@umich.edu
Steven Abney
Department of Linguistics
University of Michigan
Ann Arbor
abney@umich.edu
Abstract
Language identification is a simple problem that becomes much more difficult when its usual
assumptions are broken. In this paper we consider the task of classifying short segments of text in
closely-related languages for the Discriminating Similar Languages shared task, which is broken
into six subtasks, (A) Bosnian, Croatian, and Serbian, (B) Indonesian and Malay, (C) Czech
and Slovak, (D) Brazilian and European Portuguese, (E) Argentinian and Peninsular Spanish,
and (F) American and British English. We consider a number of different methods to boost
classification performance, such as feature selection and data filtering, but we ultimately find that
a simple na??ve Bayes classifier using character and word n-gram features is a strong baseline that
is difficult to improve on, achieving an average accuracy of 0.8746 across the six tasks.
1 Introduction
Language identification constitutes the first stage of many NLP pipelines. Before applying tools trained
on specific languages, one must determine the language of the text. It is also is often considered to be a
solved task because of the high accuracy of language identification methods in the canonical formulation
of the problem with long monolingual documents and a set of mostly dissimilar languages to choose
from. We consider a different setting with much shorter text in the form of single sentences drawn from
very similar languages or dialects.
This paper describes experiments related to and our submissions to the Discriminating Similar Lan-
guages (DSL) shared task. This shared task has six subtasks, each a classification task in which a sentence
must be labeled as belonging to a small set of related languages:
? Task A: Bosnian vs. Croatian vs. Serbian
? Task B: Indonesian vs. Malay
? Task C: Czech vs. Slovak
? Task D: Brazilian vs. European Portuguese
? Task E: Argentinian vs. Peninsular Spanish
? Task F: American vs. British English
The first three tasks involve classes that could be rightly called separate languages or dialects. The
classes of each of the final three tasks have high mutual intelligibility and are so similar that some
linguists may not even classify them as separate dialects. We will use the term ?language variant? to
refer to such classes.
In this paper we experiment with several types of methods aimed at improving the classification ac-
curacy of these tasks: machine learning methods, data pre-processing, feature selection, and additional
training data. We find that a simple na??ve Bayes classifier using character and word n-gram features is
a strong baseline that is difficult to improve on. Because this paper covers so many different types of
methods, its format eschews the standard ?Results? section, instead providing comparisons of methods
as they are presented.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
146
2 Related Work
Recent directions in language identification have included finer-grained language identification (King
and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs
(Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely
related languages.
Language identification for closely related languages has been considered by several researchers,
though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguish-
ing Croatian from Serbian and Slovenian is explored by Ljube?si?c et al. (2007), who used a list of most
frequent words along with a Markov model and a word blacklist, a list of words that are not allowed
to appear in a certain language. A similar approach was later used by Tiedemann and Ljube?si?c (2012)
to distinguish Bosnian, Croatian, and Serbian. They further develop the idea of a blacklist classifier,
loosening the binary restriction of the earlier work?s blacklist and considering the frequencies of words
rather than their absolute counts. This blacklist classifier is able to outperform a na??ve Bayes classifier
with large amounts of training data. They also find training on parallel data to be important, as it al-
lows the machine learning methods to pick out features relating to the differences between the languages
themselves, rather than learning differences in domain.
Zampieri et al. consider classes that would be most often classified as language varieties rather than
separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al.,
2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and
Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity
between a document and a corpus according to the size of the intersection between the sets of types in
each.
A similar, but somewhat different problem of automatically identifying lexical variants between
closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they
are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch.
3 Data
This paper?s training data and evaluation data both come from the DSL corpus collection (DSLCC)
(Tan et al., 2014). We use the training section of this data for training and the development section for
evaluation. The training section consists of 18,000 labeled instances per class, while the development
section has 2,000 labeled instances per class.
In order to try to increase classifier accuracy (and to avoid the problems with the task F training
data), we decided to collect additional training data for each open-class task. For each task, we collected
newspaper text from the appropriate websites for each of the 2?3 languages. We used regular expressions
to split the text into sentences, and created a set of rules to filter out strings that were unlikely to be good
sentences. Because the pages on the newspaper websites tended to have some boilerplate text, we collated
all the sentences and only kept one copy of each sentence.
Task Language/Dialect Newspaper Sentences Words
A
Bosnian Nezavisne Novine 175,741 3,250,648
Croatian Novi List 231,271 4,591,318
Serbian Ve?cernje Novosti 239,390 5,213,507
B
Indonesian Kompas 114,785 1,896,138
Malay Berita Harian 36,144 695,597
C
Czech Den??k 160,972 2,432,393
Slovak Denn??k SME 62,908 970,913
D
Brazilian Portuguese O Estado de S. Paulo 558,169 11,199,168
European Portuguese Correio da Manh?a 148,745 2,979,904
E
Argentinian Spanish La Naci?on 333,246 7,769,941
Peninsular Spanish El Pa??s 195,897 4,329,480
F
American English The New York Times 473,350 10,491,641
British English The Guardian 971,097 20,288,294
Table 1: Sources and amounts of training data collected for the open track for each task.
147
In order to create balanced training data, for each task we downsampled the number of sentences of
the larger collection(s) to match the number of sentences in the smaller collection. For example, we
downsampled the British English collection to 473,350 sentences and combined it with the American
English sentences to create the training data for English. Figure 1 shows results of training using this
external data.
3.1 Features
We use many types of features that have been found to be useful in previous language identification
work: word unigrams, word bigrams, and character n-grams (2 ? n ? 6). Character n-grams are simply
substrings of the sentence and may include in addition to letters, whitespace, punctuation, digits, and
anything else that might be in the sentence. Words, for the purpose of word unigrams and bigrams, are
simply maximal tokens not containing any punctuation, digit, or whitespace.
When instances are encoded into feature vectors, each feature has a value equal to the number of times
it occured in the corresponding sentence, so the majority of features have a value of 0 for any given
instance, but it is possible for a feature to occur multiple times in a sentence and have a value greater
than 1.0 in the feature vector. Table 2 below compares the performance of a na??ve Bayes classifier using
each of the different feature groups below.
Word Character
Task All 1 2 2 3 4 5 6
Bosnian/Croatian/Serbian 0.9348 0.9290 0.8183 0.7720 0.8808 0.9412 0.9338 0.9323
Indonesian/Malay 0.9918 0.9943 0.9885 0.8545 0.9518 0.9833 0.9908 0.9930
Czech/Slovak 0.9998 1.0000 0.9985 0.9980 0.9998 0.9998 1.0000 1.0000
Portuguese 0.9535 0.9468 0.9493 0.7935 0.8888 0.9318 0.9468 0.9570
Spanish 0.8623 0.8738 0.8625 0.7673 0.8273 0.8513 0.8610 0.8660
English 0.4970 0.4948 0.5005 0.4825 0.4988 0.5010 0.5048 0.4993
Average 0.8732 0.8731 0.8529 0.7780 0.8412 0.8681 0.8729 0.8746
Table 2: Accuracies compared for different sets of features compared. The classifier used here is na??ve
Bayes.
4 Methods
Our baseline method against which we compare all other models is a na??ve Bayes classifier using word
unigram features trained on the DSL-provided training data. The methods we compare to it can be
broken into three classes: other machine learning methods, feature selection methods, and data filtering
methods.
The classification pipeline used here has the following stages: (1) data filtering, (2) feature extraction,
(3) feature selection, (4) training, and (5) classification.
4.1 Machine Learning Methods
We will use the following notation throughout this section. An instance x, that is, a sentence to be
classified, with a corresponding class label y is encoded into a feature vector f(x), where each entry
is an integer denoting how many times the feature corresponding to that entry?s index occurred in the
sentence. The class label here is a language and it?s drawn from a small set y ? Y .
In addition to the na??ve Bayes classifier, we also experiment with two versions of logistic regression
and a support vector machine classifier. The MALLET machine learning library implementations are
used for the first three classifiers (McCallum, 2002) and SVMLight is used for the fourth (Joachims, ).
Na??ve Bayes A na??ve Bayes classifier models the class label as an independent combination of input
features.
148
P (y|f(x)) =
1
P (f(x))
P (y)
n
?
i=1
P (f(x)
i
|y) (1)
As na??ve Bayes is a generative classifier, it has been shown to be able to outperform discriminative
classifiers when the number of training instances is small compared to the number of features (Ng and
Jordan, 2002). This classifier is additionally advantageous in that it has a simple closed-form solution
for maximizing its log likelihood.
Logistic Regression A logistic regression classifier is a discriminative classifier whose parameters are
encoded in a vector ?. The conditional probability of a class label over an instance (x, y) is modeled as
follows:
P (y|x; ?) =
1
Z(x; ?)
exp {f(x, y) ? ?} ; Z(x, ?) =
?
y?Y
exp {f(x, y) ? ?} (2)
The parameter vector ? is commonly estimated by maximizing the log-likelihood of this function over
the set of training instances (x, y) ? T in the following way:
? = argmax
?
?
(x,y)?T
logP (y
i
|x
i
; ?)? ?R(?) (3)
The term R(?) above is a regularization term. It is common for such a classifier to overfit the pa-
rameters to the training data. To keep this from happening, a regularization term can be added which
keeps the parameters in ? from growing too large. Two common choices for this function are L2 and L1
normalization:
R
L2
= ||?||
2
2
=
n
?
i=1
?
2
i
, R
L1
= ||?||
1
=
n
?
i=1
|?
i
| (4)
L2 regularization is well-grounded theoretically, as it is equivalent to a model with a Gaussian prior
on the parameters (Rennie, 2004). But L1 regularization has a reputation for enforcing sparsity on the
parameters. In fact, it has been shown to be quite effective when the number of irrelevant dimensions is
greater than the number of training examples, which we expect to be the case with many of the tasks in
this paper (Ng, 2004).
Support Vector Machines A support vector machine (SVM) is a type of linear classifier that attempts
to find a boundary that linearly separates the training data with the maximum possible margin. SVMs
have been shown to be a very efficient and high accuracy method to classify data across a wide variety
of different types of tasks (Tsochantaridis et al., 2004).
Table 3 below compares these machine learning methods. Because of its consistently good perfor-
mance across tasks, we use a na??ve Bayes classifier throughout the rest of the paper.
4.2 Feature Selection Methods
We expect that the majority of features are not relevant to the classification task, and so we experimented
with several methods of feature selection, both manual and automatic.
Information Gain As a fully automatic method of feature extraction, we used information gain to
score features according to their expected usefulness. Information gain (IG) is an information theoretic
concept that (colloquially) measures the amount of knowledge about the class label that is gained by
having access to a specific feature. If f is the occurence an individual feature and
?
f the non-occurence
of a feature, we measure its information gain by the following formula:
G(f) = P (f)
?
?
?
y?Y
P (y|f)logP (y|f)
?
?
+ P (
?
f)
?
?
?
y?Y
logP (y|
?
f)logP (y|
?
f)
?
?
(5)
149
Task
Logistic
Regression
(L2-norm)
Logistic
Regression
(L1-norm)
Na??ve Bayes SVM
Bosnian/Croatian/Serbian 0.9138 0.9135 0.9290 0.9100
Indonesian/Malay 0.9878 0.9810 0.9943 0.9873
Czech/Slovak 0.9983 0.9958 1.0000 0.9985
Portuguese 0.9383 0.9368 0.9468 0.9325
Spanish 0.8843 0.8770 0.8738 0.8768
English 0.5000 0.4945 0.4948 0.4958
Average 0.8704 0.8648 0.8731 0.8668
Table 3: Comparison of different machine learning methods using word unigram features on the six
tasks.
To reduce the number of features being used in classification (and to hopefully remove irrelevant
features), we choose the 10,000 features with the highest IG scores. IG considers each feature indepen-
dently, so it is possible that redundant feature sets could be chosen. For example, it might happen that
both the quadrigram ther and the trigram the score highly according to IG and are both selected, even
though they are highly correlated with one another.
Parallel Text Feature Selection Because IG feature selection often seemed to choose features more
related to differences in domain than to differences in language (see Table 7), we wanted to try to isolate
features that are specific to language differences. It has been shown in previous work that training on
parallel text can help to isolate language differences since the domains of the languages are identical
(Tiedemann and Ljube?si?c, 2012). For each of the tasks,
1
we use translations of the complete Bible as a
parallel corpus, running IG feature selection exactly as above. Table 4 below gives more details about
the texts used.
Task Language/Dialect Bible
B
Indonesian Alkitab dalam Bahasa Indonesia Masa Kini
Malay 2001 Today?s Malay Version
C
Czech Cesk?y studijn?? preklad
Slovak Slovensk?y Ekumenick?y Biblia
D
Brazilian Portuguese a B
?
IBLIA para todos
European Portuguese Almeida Revista e Corrigida (Portugal)
E
Argentinian Spanish La Palabra (versi?on hispanoamericana)
Peninsular Spanish La Palabra (versi?on espa?nola)
F
American English New International Version
British English New International Version Anglicized
Table 4: Bibles used as parallel corpora for feature selection.
Manual Feature Selection We also used manual feature selection, selecting features to use in the clas-
sifiers from lists published on Wikipedia comparing the two languages. Of course some of the features in
lists like these are features that are quite difficult to detect using NLP (especially before the language has
been identified) such as characteristic passive or genitive constructions. But there are many features that
we are able to detect and use in a list of manually selected features, such as character n-grams relating
to morphology and spelling and word n-grams relating to vocabulary differences.
Table 5 below compares these feature selection methods on each task. Since the manual feature selec-
tion suggested all types of features, including character n-gram and word unigram and bigram features,
the experiments in this section use all features described in Section 3.1. The results show that any type
of feature selection consistently hurts performance, though IG hurts the least, and it should be noted
that in certain cases with other machine learning methods, IG feature selection actually yielded better
1
excluding Task A, for which we were unable to find a Bible in Latin-script Serbian or any Bible in Bosnian
150
performance than all features. That the feature selection methods designed to isolate language-specific
features performed so poorly is one indicator that the labeled data has additional differences that are not
tied to the languages themselves. We discuss this idea further in Section 5.
Task No feature selection IG Parallel Manual
Bosnian/Croatian/Serbian 0.9348 0.9300 ? 0.6328
Indonesian/Malay 0.9918 0.9768 0.8093 0.8485
Czech/Slovak 0.9998 0.9995 0.9940 0.8118
Portuguese 0.9535 0.9193 0.7215 0.6888
Spanish 0.8623 0.8310 0.5210 0.7023
English 0.4970 0.4978 0.5020 0.5053
Average 0.8732 0.8590 ? 0.6982
Table 5: Comparison of manual and automatic feature selection methods. IG and parallel feature selec-
tion both use the 10,000 features with the highest IG scores.
4.3 Data Filtering Methods
English Word Removal In looking through the training data for the non-English tasks, we observed
that it was not uncommon for sentences in these languages to contain English words and phrases. Be-
cause foreign words should be independent of the language/dialect used, English words included in the
sentences for other tasks should just be noise that, if removed will improve classification performance.
For each of the non-English tasks (A, B, C, D, and E), we create a new training set for identifying
English/non-English words by mixing together 1,000 random English words with 10,000 random task-
language words. The imbalance in the classes is a compromise, approximating the actual proportions in
the test without leading to a degenerate classifier. Because English and the other classes are so dissimilar,
the performance of the English word classifier is very insensitive to the actual ratio. From this data, we
train a na??ve Bayes classifier using character 3-grams, 4-grams, and 5-grams.
We manually labeled the words of 150 sentences from the five non-English tasks in order to evaluate
the English word classifier. Across the five tasks, the precision was 0.76 and the recall was 0.66, leading
to an F1-score of 0.70. Any words labeled as English by the classifier were removed from the sentence
and it was passed on to the feature extraction, classification, and training stages.
Named Entity Removal We also observed another common class of word that could potentially act
as a noise source: named entities. Across all the languages listed studied here, it is common for named
entities to begin with a capital letter. Lacking named entity recognizers for all the languages here, we
instead used the property of having an initial capital letter as a surrogate for recognizing a word as a
named entity. Because all the languaes studied here also have the convention of capitalizing the first
word of a sentence, we remove all words beginning with a capital letter except for the first and pass this
abridged sentence on to the feature extraction, classification, and training stages.
Task No data filtering
English Word
Removal
Named Entity
Removal
Bosnian/Croatian/Serbian 0.9138 0.9105 0.9003
Indonesian/Malay 0.9878 0.9885 0.9778
Czech/Slovak 0.9983 0.9980 0.9973
Portuguese 0.9383 0.9365 0.9068
Spanish 0.8843 0.8835 0.8555
English 0.5000 0.5000 0.5050
Average 0.8704 0.8695 0.8571
Table 6: Comparison of data filtering methods using word unigram features on the six tasks.
151
(A)
0 0.2 0.4 0.6 0.8 1
?10
5
0.4
0.6
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(B)
0 0.5 1 1.5 2 2.5
?10
4
0.5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(C)
0 0.5 1 1.5
?10
5
0.6
0.7
0.8
0.9
1
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(D)
0 0.2 0.4 0.6 0.8 1
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(E)
0 0.5 1 1.5
?10
5
0.5
0.6
0.7
0.8
0.9
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
(F)
0 0.5 1 1.5 2
?10
5
0.5
0.6
0.7
0.8
Training Instances per Class
A
c
c
u
r
a
c
y
DSL
external
external (CV)
Figure 1: Learning curves for the six tasks as the number of training instances per language is varied.
The line marked ?DSL? is the learning curve for the DSL-provided training data evaluated against the
developement data. The line marked ?external? is our external newspaper training data evaluated against
the development data. The line marked ?external (CV)? is our external training data evaluated using
10-fold cross-validation.
152
Bosnian/Croatian/Serbian Indonesian/Malay Czech/Slovak Portuguese Spanish English
da bisa sa Portugal the I
kako berkata se R Rosario you
sa kerana aj euros han The
kazao karena ako Brasil euros said
takode daripada ve cento Argentina Obama
rekao saat pre governo PP your
evra dari pro Lusa Fe If
tijekom beliau ktor?e PSD Rajoy that
posle selepas s?u Ele Espa?na but
posto bahwa ktor?y Governo Madrid It
Table 7: The ten word-unigram features given the highest weight by information gain feature selection
for each of the six tasks.
5 Discussion
Across many of the tasks, there was evidence that performance was tied more strongly to domain-specific
features of the two classes rather than to language- (or language-variant-) specific features. For example,
Table 7 shows the best word-unigram features selected by information gain feature selection for each of
the tasks. The Portuguese, Spanish, and English tasks specifically have as many of their most important
features named entities and other non-language specific features.
It seems that for many of the tasks, it is easier to distinguish the subject matter written about than it is to
distinguish the languages/dialects themselves. With Portuguese, for example, Brazilian dialect speakers
were much more likely to discuss places in Brazil and mention Brazilian reais (currency, abbreviated
as R), while European speakers mentioned euros, places in Portugal, and discussed Portuguese politics.
While there are definite linguistic differences between Brazilian and European Portuguese, these seem
to be less pronounced than the superficial differences in subject matter.
Practically, this is not necessarily a bad thing for this shared task, as the domain information gives extra
clues that allow the task to be completed with higher accuracy than would otherwise be possible. This
would become problematic if one wanted to apply a classifier trained on this data to general domains,
where the classifier may not be able to rely on the speaker talking about a certain subject matter. To
address this, the classifier would either need to focus on features specific to the language pair itself or
would need to be trained on data that spanned many domains.
Further evidence of domain overfitting comes from the fact that the larger training sets drawn from
newspaper text were not able to improve performance on the development set over the provided training
data, which is presumably drawn from the same collection as the development data. Figure 1 shows
learning curves for each of the six tasks. Though all the external text is self-consistent (cross-validation
results in high accuracy), in none of the cases does training on a large amount of external data allow the
classifier to exceed the accuracy achieved by training on the DSL data.
6 Conclusion
In this paper we experimented with several methods for classification of sentences in closely-related lan-
guages for the DSL shared task. Our analysis showed that, when dealing with closely related languages,
the task of classifying text according to its language was difficult to untie from the taks of classifying
other text characteristics, such as the domain. Across all our types of methods, we found that a na??ve
Bayes classifier using character n-gram, word unigram, and word bigram features was a strong baseline.
In future work, we would like to try to improve on these results by incorporating features that try to
capture syntactic relationships. Certainly some of the pairs of languages considered here are close enough
that they could be chunked, tagged, or parsed before knowing exactly which variety they belong to. This
would allow for the inclusion of features related to transitivity, agreement, complementation, etc. For
example, in British English, the verb ?provide? is monotransitive, but ditransitive in American English. It
is unclear how much features like these would improve accuracy, but it is likely that they would ultimately
be necessary to improve classification of similar languages to human levels of performance.
153
References
Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identi-
fication for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language
in Social Media, pages 65?74. Association for Computational Linguistics.
Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: Overcoming
the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195?215.
Chu-Ren Huang and Lung-Hao Lee. 2008. Contrastive approach towards text source classification based on
top-bag-of-word similarity. pages 404?410.
Thorsten Joachims. Svmlight: Support vector machine. http://svmlight. joachims. org/.
Ben King and Steven Abney. 2013. Labeling the languages of words in mixed-language documents using weakly
supervised methods. In Proceedings of NAACL-HLT, pages 1110?1119.
Nikola Ljube?si?c, Nives Mikeli?c, and Damir Boras. 2007. Language identication: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces, pages
541?546.
Marco Lui, Jey Han Lau, and Timothy Baldwin. 2014. Automatic detection and language identification of multi-
lingual documents. Transactions of the Association for Computational Linguistics, 2:27?40.
Andrew K. McCallum. 2002. Mallet: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Andrew Y Ng and Michael I Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic
regression and naive bayes. Advances in neural information processing systems, 2:841?848.
Andrew Y Ng. 2004. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of the
twenty-first international conference on Machine learning, page 78. ACM.
Dong-Phuong Nguyen and A Seza Dogruoz. 2013. Word level language identification in online multilingual
communication. Association for Computational Linguistics.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16(4):469?491.
Jason Rennie. 2004. On l2-norm regularization and the gaussian prior.
http://people.csail.mit.edu/jrennie/writing.
Liling Tan, Marcos Zampieri, Nikola Ljube?sic, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The dsl corpus collection. In Proceedings of The 7th Workshop on
Building and Using Comparable Corpora (BUCC).
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
COLING, pages 2619?2634.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector ma-
chine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international
conference on Machine learning, page 104. ACM.
Marcos Zampieri and Binyam Gebrekidan. 2012. Automatic identification of language varieties: The case of
portuguese. In Proceedings of KONVENS, pages 233?237.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2012. Classifying pluricentric languages:
Extending the monolingual model. In Proceedings of the Fourth Swedish Language Technlogy Conference
(SLTC2012), pages 79?80.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos
distribution for the identification of spanish varieties. Proceedings of TALN2013, Sable dOlonne, France, pages
580?587.
154
