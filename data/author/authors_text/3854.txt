Proceedings of ACL-08: HLT, pages 701?709,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Improving Search Results Quality by Customizing Summary Lengths
Michael Kaisser
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
m.kaisser@sms.ed.ac.uk
Marti A. Hearst
UC Berkeley
102 South Hall
Berkeley, CA 94705
hearst@ischool.berkeley.edu
John B. Lowe
Powerset, Inc.
475 Brannan St.
San Francisco, CA 94107
johnblowe@gmail.com
Abstract
Web search engines today typically show re-
sults as a list of titles and short snippets that
summarize how the retrieved documents are
related to the query. However, recent research
suggests that longer summaries can be prefer-
able for certain types of queries. This pa-
per presents empirical evidence that judges
can predict appropriate search result summary
lengths, and that perceptions of search result
quality can be affected by varying these result
lengths. These findings have important impli-
cations for search results presentation, espe-
cially for natural language queries.
1 Introduction
Search results listings on the web have become stan-
dardized as a list of information summarizing the
retrieved documents. This summary information is
often referred to as the document?s surrogate (Mar-
chionini et al, 2008).
In older search systems, such as those used in
news and legal search, the document surrogate typ-
ically consisted of the title and important metadata,
such as date, author, source, and length of the article,
as well as the document?s manually written abstract.
In most cases, the full text content of the document
was not available to the search engine and so no ex-
tracts could be made.
In web search, document surrogates typically
show the web page?s title, a URL, and information
extracted from the full text contents of the docu-
ment. This latter part is referred to by several dif-
ferent names, including summary, abstract, extract,
and snippet. Today it is standard for web search en-
gines to show these summaries as one or two lines
of text, often with ellipses separating sentence frag-
ments. However, there is evidence that the ideal re-
sult length is often longer than the standard snippet
length, and that furthermore, result length depends
on the type of answer being sought.
In this paper, we systematically examine the ques-
tion of search result length preference, comparing
different result lengths for different query types. We
find evidence that desired answer length is sensitive
to query type, and that for some queries longer an-
swers are judged to be of higher quality.
In the following sections we summarize the re-
lated work on result length variation and on query
topic classification. We then describe two studies. In
the first, judges examined queries and made predic-
tions about the expected answer types and the ideal
answer lengths. In the second study, judges rated
answers of different lengths for these queries. The
studies find evidence supporting the idea that differ-
ent query types are best answered with summaries
of different lengths.
2 Related Work
2.1 Query-biased Summaries
In the early days of the web, the result summary
consisted of the first few lines of text, due both to
concerns about intellectual property, and because of-
ten that was the only part of the full text that the
search engines retained from their crawls. Eventu-
ally, search engines started showing what are known
variously as query-biased summaries, keyword-in-
701
context (KWIC) extractions, and user-directed sum-
maries (Tombros and Sanderson, 1998). In these
summaries, sentence fragments, full sentences, or
groups of sentences that contain query terms are ex-
tracted from the full text. Early versions of this idea
were developed in the Snippet Search tool (Peder-
sen et al, 1991) and the Superbook tool?s Table-of-
Contents view (Egan et al, 1989).
A query-biased summary shows sentences that
summarize the ways the query terms are used within
the document. In addition to showing which subsets
of query terms occur in a retrieved document, this
display also exposes the context in which the query
terms appear with respect to one another.
Research suggests that query-biased summaries
are superior to showing the first few sentences from
documents. Tombros & Sanderson (1998), in a
study with 20 participants using TREC ad hoc data,
found higher precision and recall and higher sub-
jective preferences for query-biased summaries over
summaries showing the first few sentences. Simi-
lar results for timing and subjective measurements
were found by White et al (2003) in a study with
24 participants. White et al (2003) also describe
experiments with different sentence selection mech-
anisms, including giving more weight to sentences
that contained query words along with text format-
ting.
There are significant design questions surround-
ing how best to formulate and display query-biased
summaries. As with standard document summariza-
tion and extraction, there is an inherent trade-off
between showing long, informative summaries and
minimizing the screen space required by each search
hit. There is also a tension between showing short
snippets that contain all or most of the query terms
and showing coherent stretches of text. If the query
terms do not co-occur near one another, then the ex-
tract has to become very long if full sentences and
all query terms are to be shown. Many web search
engine snippets compromise by showing fragments
instead of sentences.
2.2 Studies Comparing Results Lengths
Recently, a few studies have analyzed the results of
varying search summary length.
In the question-answering context (as opposed to
general web search), Lin et al (2003) conducted
a usability study with 32 computer science students
comparing four types of answer context: exact an-
swer, answer-in-sentence, answer-in-paragraph, and
answer-in-document. To remove effects of incorrect
answers, they used a system that produced only cor-
rect answers, drawn from an online encyclopedia.
Participants viewed answers for 8 question scenar-
ios. Lin et al (2003) found no significant differ-
ences in task completion times, but they did find dif-
ferences in subjective responses. Most participants
(53%) preferred paragraph-sized chunks, noting that
a sentence wasn?t much more information beyond
the exact answer, and a full document was often-
times too long. That said, 23% preferred full docu-
ments, 20% preferred sentences, and one participant
preferred exact answer, thus suggesting that there is
considerable individual variation.
Paek et al (2004) experimented with showing dif-
fering amounts of summary information in results
listings, controlling the study design so that only
one result in each list of 10 was relevant. For half
the test questions, the target information was visi-
ble in the original snippet, and for the other half, the
participant needed to use their mouse to view more
information from the relevant search result. They
compared three interface conditions:
(i) a standard search results listing, in which a
mouse click on the title brings up the full text
of the web page,
(ii) ?instant? view, for which a mouseclick ex-
panded the document summary to show addi-
tional sentences from the document, and those
sentences contained query terms and the an-
swer to the search task, and
(iii) a ?dynamic? view that responded to a mouse
hover, and dynamically expanded the summary
with a few words at a time.
Eleven out of 18 participants preferred instant
view over the other two views, and on average all
participants produced faster and more accurate re-
sults with this view. Seven participants preferred dy-
namic view over the others, but many others found
this view disruptive. The dynamic view suffered
from the problem that, as the text expanded, the
mouse no longer covered the selected results, and
702
so an unintended, different search result sometimes
started to expand. Notably, none of the participants
preferred the standard results listing view.
Cutrell & Guan (2007), compared search sum-
maries of varying length: short (1 line of text),
medium (2-3 lines) and long (6-7 lines) using search
engine-produced snippets (it is unclear if the sum-
mary text was contiguous or included ellipses).
They also compared 6 navigational queries (where
the goal is to find a website?s homepage), with 6 in-
formational queries (e.g., ?find when the Titanic set
sail for its only voyage and what port it left from,?
?find out how long the Las Vegas monorail is?). In
a study with 22 participants, they found that partic-
ipants were 24 seconds faster on average with the
long view than with the short and medium view. The
also found that participants were 10 seconds slower
on average with the long view for the navigational
tasks. They present eye tracking evidence which
suggests that on the navigational task, the extra text
distracts the eye from the URL. They did not re-
port on subjective responses to the different answer
lengths.
Rose et al (2007) varied search results summaries
along several dimensions, finding that text choppi-
ness and sentence truncation had negative effects,
and genre cues had positive effects. They did not
find effects for varying summary length, but they
only compared relatively similar summary lengths
(2 vs. 3 vs. 4 lines long).
2.3 Categorizing Questions by Expected
Answer Types
In the field of automated question-answering, much
effort has been expended on automatically deter-
mining the kind of answer that is expected for a
given question. The candidate answer types are
often drawn from the types of questions that have
appeared in the TREC Question Answering track
(Voorhees, 2003). For example, the Webclopedia
project created a taxonomy of 180 types of ques-
tion targets (Hovy et al, 2002), and the FALCON
project (Harabagiu et al, 2003) developed an an-
swer taxonomy with 33 top level categories (such
as PERSON, TIME, REASON, PRODUCT, LOCA-
TION, NUMERICAL VALUE, QUOTATION), and
these were further refined into an unspecified num-
ber of additional categories. Ramakrishnan et al
(2004) show an automated method for determining
expected answer types using syntactic information
and mapping query terms to WordNet.
2.4 Categorizing Web Queries
A different line of research is the query log cate-
gorization problem. In query logs, the queries are
often much more terse and ill-defined than in the
TREC QA track, and, accordingly, the taxonomies
used to classify what is called the query intent have
been much more general.
In an attempt to demonstrate how information
needs for web search differ from the assumptions
of pre-web information retrieval systems, Broder
(2002) created a taxonomy of web search goals, and
then estimated frequency of such goals by a com-
bination of an online survey (3,200 responses, 10%
response rate) and a manual analysis of 1,000 query
from the AltaVista query logs. This taxonomy has
been heavily influential in discussions of query types
on the Web.
Rose & Levinson (2004) followed up on Broder?s
work, again using web query logs, but developing
a taxonomy that differed somewhat from Broder?s.
They manually classified a set of 1,500 AltaVista
search engine log queries. For two sets of 500
queries, the labeler saw just the query and the re-
trieved documents; for the third set the labeler also
saw information about which item(s) the searcher
clicked on. They found that the classifications that
used the extra information about clickthrough did
not change the proportions of assignments to each
category. Because they did not directly compare
judgments with and without click information on
the same queries, this is only weak evidence that
query plus retrieved documents is sufficient to clas-
sify query intent.
Alternatively, queries from web query logs can be
classified according to the topic of the query, inde-
pendent of the type of information need. For ex-
ample, a search involving the topic of weather can
consist of the simple information need of looking
at today?s forecast, or the rich and complex infor-
mation need of studying meteorology. Over many
years, Spink & Jansen et al (2006; 2007) have man-
ually analyzed samples of query logs to track a num-
ber of different trends. One of the most notable is
the change in topic mix. As an alternative to man-
703
ual classification of query topics, Shen et al (2005)
described an algorithm for automatically classifying
web queries into a set of pre-defined topics. More re-
cently, Broder et al (2007) presented a highly accu-
rate method (around .7 F-score) for classifying short,
rare queries into a taxonomy of 6,000 categories.
3 Study Goals
Related work suggests that longer results are prefer-
able, but not for all query types. The goal of our
efforts was to determine preferred result length for
search results, depending on type of query. To do
this, we performed two studies:
1. We asked a set of judges to categorize a large
set of web queries according to their expected
preferred response type and expected preferred
response length.
2. We then developed high-quality answer pas-
sages of different lengths for a subset of these
queries by selecting appropriate passages from
the online encyclopedia Wikipedia, and asked
judges to rate the quality of these answers.
The results of this study should inform search in-
terface designers about what the best presentation
format is.
3.1 Using Mechanical Turk
For these studies, we make use of a web service of-
fered by Amazon.com called Mechanical Turk, in
which participants (called ?turkers?) are paid small
sums of money in exchange for work on ?Human
Intelligence tasks? (HITs).1 These HITs are gener-
ated from an XML description of the task created
by the investigator (called a ?requester?). The par-
ticipants can come from any walk of life, and their
identity is not known to the requesters. We have in
past work found the results produced by these judges
to be of high quality, and have put into place vari-
ous checks to detect fraudulent behavior. Other re-
searchers have investigated the efficacy of language
1Website: http://www.mturk.com. For experiment 1, ap-
proximately 38,000 HITs were completed at a cost of about
$1,500. For experiment 2, approximately 7,300 HITs were
completed for about $170. Turkers were paid between $.01 and
$.05 per HIT depending on task complexity; Amazon imposes
additional charges.
1. Person(s)
2. Organization(s)
3. Time(s) (date, year, time span etc.)
4. Number or Quantity
5. Geographic Location(s) (e.g., city, lake, address)
6. Place(s) (e.g.,?the White House?, ?at a supermar-
ket?)
7. Obtain resource online (e.g., movies, lyrics, books,
magazines, knitting patterns)
8. Website or URL
9. Purchase and product information
10. Gossip and celebrity information
11. Language-related (e.g., translations, definitions,
crossword puzzle answers)
12. General information about a topic
13. Advice
14. Reason or Cause, Explanation
15. Yes/No, with or without explanation or evidence
16. Other
17. Unjudgable
Table 1: Allowable responses to the question: ?What sort
of result or results does the query ask for?? in the first
experiment.
1. A word or short phrase
2. A sentence
3. One or more paragraphs (i.e. at least several sen-
tences)
4. An article or full document
5. A list
6. Other, or some combination of the above
Table 2: Allowable responses to the question: ?How long
is the best result for this query?? in the first experiment.
annotation using this service and have found that the
results are of high quality (Su et al, 2007).
3.2 Estimating Ideal Answer Length and Type
We developed a set of 12,790 queries, drawn from
Powerset?s in house query database which con-
tains representative subsets of queries from different
search engines? query logs, as well as hand-edited
query sets used for regression testing. There are a
disproportionally large number of natural language
queries in this set compared with query sets from
typical keyword engines. Such queries are often
complete questions and are sometimes grammatical
fragments (e.g., ?date of next US election?) and so
are likely to be amenable to interesting natural lan-
guage processing algorithms, which is an area of in-
704
Figure 1: Results of the first experiment. The y-axis shows the semantic type of the predicted answer, in the same
order as listed in Table 1; the x-axis shows the preferred length as listed in Table 2. Three bars with length greater
than 1,500 are trimmed to the maximum size to improve readability (GeneralInfo/Paragraphs, GeneralInfo/Article,
and Number/Phrase).
terest of our research. The average number of words
per query (as determined by white space separation)
was 5.8 (sd. 2.9) and the average number of char-
acters (including punctuation and white space) was
32.3 (14.9). This is substantially longer than the cur-
rent average for web search query, which was ap-
proximately 2.8 in 2005 (Jansen et al, 2007); this is
due to the existence of natural language queries.
Judges were asked to classify each query accord-
ing to its expected response type into one of 17 cat-
egories (see Table 1). These categories include an-
swer types used in question answering research as
well as (to better capture the diverse nature of web
queries) several more general response types such
as Advice and General Information. Additionally,
we asked judges to anticipate what the best result
length would be for the query, as shown in Table 2.
Each of the 12,790 queries received three assess-
ments by MTurk judges. For answer types, the
number of times all three judges agreed was 4537
(35.4%); two agreed 6030 times (47.1%), and none
agreed 2223 times (17.4%). Not surprisingly, there
was significant overlap between the label General-
Info and the other categories. For answer length
estimations, all three judges agreed in 2361 cases
(18.5%), two agreed in 7210 cases (56.4%) and none
3219 times (25.2%).
Figure 1 summarizes expected length judgments
by estimated answer category. Distribution of the
length categories differs a great deal across the in-
dividual expected response categories. In general,
the results are intuitive: judges preferred short re-
sponses for ?precise queries? (e.g., those asking for
numbers) and they preferred longer responses for
queries in broad categories like Advice or Gener-
alInfo. But some results are less intuitive: for ex-
ample, judges preferred different response lengths
for queries categorized as Person and Organization
? in fact for the latter the largest single selection
made was List. Reviewing the queries for these
two categories, we note that most queries about or-
ganizations in our collection asked for companies
705
length type average std dev
Word or Phrase 38.1 25.8
Sentence 148.1 71.4
Paragraph 490.5 303.1
Section 1574.2 1251.1
Table 3: Average number of characters for each answer
length type for the stimuli used in the second experiment.
(e.g. ?around the world travel agency?) and for
these there usually is more than one correct answer,
whereas the queries about persons (?CEO of mi-
crosoft? ) typically only had one relevant answer.
The results of this table show that there are some
trends but not definitive relationships between query
type (as classified in this study) and expected answer
length. More detailed classifications might help re-
solve some of the conflicts.
3.3 Result Length Study
The purpose of the second study was twofold: first,
to see if doing a larger study confirms what is hinted
at in the literature: that search result lengths longer
than the standard snippet may be desirable for at
least a subset of queries. Second, we wanted to
see if judges? predictions of desirable results lengths
would be confirmed by other judges? responses to
search results of different lengths.
3.3.1 Method
It has been found that obtaining judges? agree-
ment on intent of a query from a log can be difficult
(Rose and Levinson, 2004; Kellar et al, 2007). In
order to make the task of judging query relevance
easier, for the next phase of the study we focused
on only those queries for which all three assessors
in the first experiment agreed both on the category
label and on the estimated ideal length. There were
1099 such high-confidence queries, whose average
number of words was 6.3 (2.9) and average number
of characters was 34.5 (14.3).
We randomly selected a subset of the high-
agreement queries from the first experiment and
manually excluded queries for which it seemed ob-
vious that no responses could be found in Wikipedia.
These included queries about song lyrics, since in-
tellectual property restrictions prevent these being
posted, and crossword puzzle questions such as ?a
four letter word for water.?
The remaining set contained 170 queries. MTurk
annotators were asked to find one good text passage
(in English) for each query from the Wikipedia on-
line encyclopedia. They were also asked to subdi-
vide the text of this answer into each of the following
lengths: a word or phrase, a sentence, a paragraph,
a section or an entire article.2 Thus, the shorter an-
swer passages are subsumed by the longer ones.
Table 3 shows the average lengths and standard
deviations of each result length type. Table 4 con-
tains sample answers for the shorter length formats
for one query. For 24 of the 170 queries the annota-
tors could not find a suitable response in Wikipedia,
e.g., ?How many miles between NY and Milwau-
kee?? We collected two to five results for each of the
remaining 146 queries and manually chose the best
of these answer passages. Note that, by design, all
responses were factually correct; they only differed
in their length.
Ten MTurk judges saw each query/answer length
pair, and for each of these, were told: ?Below you
see a search engine query and a possible response.
We would like you to give us your opinion about the
response. We are especially interested in the length
of the response. Is it suitable for the query? Is there
too much or not enough information? Please rate the
response on a scale from 0 (very bad response) to 10
(very good response).? There were 124 judges in to-
tal; of these, 16 did more than 146 HITs, meaning
they saw the same query more than one time (but
with different lengths). Upon examination of the re-
sults, we determined that two of these high-volume
judges were not trying to do the task properly, and so
we dropped their judgments from the final analysis.
3.3.2 Results
Our results show that judges prefer results of dif-
ferent lengths, depending on the query. The re-
sults also suggest that judges? estimates of a pre-
ferred result length in the first experiment are ac-
curate predictors when there is strong agreement
among them. Figure 2 shows in four diagrams
2Note the slight difference between the length categories in
the first and second experiment: The List and Other options
were dropped for the second experiment because we wanted to
concentrate on textual length. Additionally, to provide more
than one option between Sentence and Article, the category
One or more paragraphs was split up into two: (One) Para-
graph and (One) Section.
706
query Who was the first person to scale K2?
Paragraph An Italian expedition finally succeeded in ascending to the summit of K2 on July 31, 1954.
The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni. The team included a Pakistani mem-
ber, Colonel Muhammad Ata-ullah. He had been a part of an earlier 1953 American expe-
dition which failed to make the summit because of a storm which killed a key climber, Art
Gilkey. On the expedition also was the famous Italian climber Walter Bonatti. He proved
vital to the expeditions success in that he carried vital oxygen to 26,600ft for Lacedelli
and Compagnoni. His dramatic bivouac, at that altitude with the equipment, wrote another
chapter in the saga of Himalayan climbing.
Sentence The expedition was led by Ardito Desio, although the two climbers who actually reached
the top were Lino Lacedelli and Achille Compagnoni.
Phrase Lino Lacedelli and Achille Compagnoni
Table 4: Sample answers of differing lengths used as input for the second study. Note that the shorter answers are
contained in the longer ones. For the full article case, judges were asked to follow a hyperlink to an article.
Figure 2: Results of the second experiment, where each query/answer-length pair was assessed by 8?10 judges using
a scale of 0 (?very bad?) to 10 (?very good?). Marks indicate means and standard errors. The top left graph shows
responses of different lengths for queries that were classified as best answered with a phrase in the first experiment.
The upper right shows responses for queries predicted to be best answered with a sentence, lower left for best answered
with one or more paragraphs and lower right for best answered with an article.
707
Slope Std. Error p-value
Phrase -0.850 0.044 < 0.0001
Sentence -0.550 0.050 < 0.0001
Paragraph 0.328 0.049 < 0.0001
Article 0.856 0.053 < 0.0001
Table 5: Results of unweighted linear regression on the
data for the second experiment, which was separated into
four groups based on the predicted preferred length.
how queries assigned by judges to one of the four
length categories from the first experiment were
judged when presented with responses of the five
answer lengths from the second experiment. The
graphs show the means and standard error of the
judges? scores across all queries for each predicted-
length/presented-length combination.
In order to test whether these results are signifi-
cant we performed four separate linear regressions;
one for each of the predicted preferred length cat-
egories. The snippet length, the independent vari-
able, was coded as 1-5, shortest to longest. The
score for each query-snippet pair is the dependent
variable. Table 5 shows that for each group there is
evidence to reject the null hypothesis that the slope
is equal to 0 at the 99% confidence level. High
scores are associated with shorter snippet lengths
for queries with predicted preferred length phrase
or sentence and also with longer snippet lengths for
queries with predicted preferred length paragraphs
or article. These associations are strongest for the
queries with the most extreme predicted preferred
lengths (phrase and article).
Our results also suggest the intuition that the best
answer lengths do not form strictly distinct classes,
but rather lie on a continuum. If the ideal response is
from a certain category (e.g., a sentence), returning a
result from an adjacent category (a phrase or a para-
graph) is not strongly penalized by judges, whereas
retuning a result from a category further up or down
the scale (an article) is.
One potential drawback of this study format is
that we do not show judges a list of results for
queries, as is standard in search engines, and so they
do not experience the tradeoff effect of longer results
requiring more scrolling if the desired answer is not
shown first. However, the earlier results of Cutrell &
Guan (2007) and Paek et al (2004) suggest that the
preference for longer results occurs even in contexts
that require looking through multiple results. An-
other potential drawback of the study is that judges
only view one relevant result; the effects of showing
a list of long non-relevant results may be more neg-
ative than that of showing short non-relevant results;
this study would not capture that effect.
4 Conclusions and Future Work
Our studies suggest that different queries are best
served with different response lengths (Experi-
ment 1), and that for a subset of especially clear
queries, human judges can predict the preferred re-
sult lengths (Experiment 2). The results furthermore
support the contention that standard results listings
are too short in many cases, at least assuming that
the summary shows information that is relevant for
the query. These findings have important implica-
tions for the design of search results presentations,
suggesting that as user queries become more expres-
sive, search engine results should become more re-
sponsive to the type of answer desired. This may
mean showing more context in the results listing, or
perhaps using more dynamic tools such as expand-
on-mouseover to help answer the query in place.
The obvious next step is to determine how to au-
tomatically classify queries according to their pre-
dicted result length and type. For classifying ac-
cording to expected length, we have run some initial
experiments based on unigram word counts which
correctly classified 78% of 286 test queries (on 805
training queries) into one of three length bins. We
plan to pursue this further in future work. For classi-
fying according to type, as discussed above, most
automated query classification for web logs have
been based on the topic of the query rather than on
the intended result type, but the question answering
literature has intensively investigated how to pre-
dict appropriate answer types. It is likely that the
techniques from these two fields can be productively
combined to address this challenge.
Acknowledgments. This work was supported in
part by Powerset, Inc., and in part by Microsoft Re-
search through the MSR European PhD Scholarship
Programme. We would like to thank Susan Gruber
and Bonnie Webber for their helpful comments and
suggestions.
708
References
A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi, V. Josi-
fovski, and T. Zhang. 2007. Robust classification of
rare queries using web knowledge. Proceedings of SI-
GIR 2007.
A. Broder. 2002. A taxonomy of web search. ACM
SIGIR Forum, 36(2):3?10.
E. Cutrell and Z. Guan. 2007. What Are You Looking
For? An Eye-tracking Study of Information Usage in
Web Search. Proceedings of ACM SIGCHI 2007.
D.E. Egan, J.R. Remde, L.M. Gomez, T.K. Landauer,
J. Eberhardt, and C.C. Lochbaum. 1989. Formative
design evaluation of Superbook. ACM Transactions
on Information Systems (TOIS), 7(1):30?57.
S.M. Harabagiu, S.J. Maiorano, and M.A. Pasca. 2003.
Open-domain textual question answering techniques.
Natural Language Engineering, 9(03):231?267.
E. Hovy, U. Hermjakob, and D. Ravichandran. 2002.
A question/answer typology with surface text patterns.
Proceedings of the second international conference on
Human Language Technology Research, pages 247?
251.
B.J. Jansen and Spink. 2006. How are we searching
the World Wide Web? A comparison of nine search
engine transaction logs. Information Processing and
Management, 42(1):248?263.
B.J. Jansen, A. Spink, and S. Koshman. 2007. Web
searcher interaction with the Dogpile.com metasearch
engine. Journal of the American Society for Informa-
tion Science and Technology, 58(5):744?755.
M. Kellar, C. Watters, and M. Shepherd. 2007. A Goal-
based Classification of Web Information Tasks. JA-
SIST, 43(1).
J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz,
and D.R. Karger. 2003. What Makes a Good Answer?
The Role of Context in Question Answering. Human-
Computer Interaction (INTERACT 2003).
G. Marchionini, R.W. White, and Marchionini. 2008.
Find What You Need, Understand What You Find.
Journal of Human-Computer Interaction (to appear).
T. Paek, S.T. Dumais, and R. Logan. 2004. WaveLens:
A new view onto internet search results. Proceedings
on the ACM SIGCHI Conference on Human Factors in
Computing Systems, pages 727?734.
J. Pedersen, D. Cutting, and J. Tukey. 1991. Snippet
search: A single phrase approach to text access. Pro-
ceedings of the 1991 Joint Statistical Meetings.
G. Ramakrishnan and D. Paranjpe. 2004. Is question an-
swering an acquired skill? Proceedings of the 13th
international conference on World Wide Web, pages
111?120.
D.E. Rose and D. Levinson. 2004. Understanding user
goals in web search. Proceedings of the 13th interna-
tional conference on World Wide Web, pages 13?19.
D.E. Rose, D. Orr, and R.G.P. Kantamneni. 2007. Sum-
mary attributes and perceived search quality. Pro-
ceedings of the 16th international conference on World
Wide Web, pages 1201?1202.
D. Shen, R. Pan, J.T. Sun, J.J. Pan, K. Wu, J. Yin,
and Q. Yang. 2005. Q2C@UST: our winning solu-
tion to query classification in KDDCUP 2005. ACM
SIGKDD Explorations Newsletter, 7(2):100?110.
Q. Su, D. Pavlov, J. Chow, and W. Baker. 2007. Internet-
Scale Collection of Human-Reviewed Data. Proceed-
ings of WWW 2007.
A. Tombros and M. Sanderson. 1998. Advantages of
query biased summaries in information retrieval. Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 2?10.
E.M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
R.W. White, J. Jose, and I. Ruthven. 2003. A task-
oriented study on the influencing effects of query-
biased summarisation in web searching. Information
Processing and Management, 39(5):707?733.
709
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 32?35,
Columbus, June 2008. c?2008 Association for Computational Linguistics
The QuALiM Question Answering Demo:
Supplementing Answers with Paragraphs drawn from Wikipedia
Michael Kaisser
School of Informatics
University of Edinburgh
M.Kaisser@sms.ed.ac.uk
Abstract
This paper describes the online demo of the
QuALiM Question Answering system. While
the system actually gets answers from the web
by querying major search engines, during pre-
sentation answers are supplemented with rel-
evant passages from Wikipedia. We believe
that this additional information improves a
user?s search experience.
1 Introduction
This paper describes the online demo of
the QuALiM1 Question Answering system
(http://demos.inf.ed.ac.uk:8080/qualim/). We
will refrain from describing QuALiM?s answer
finding strategies?our work on QuALiM has been
described in several papers in the last few years,
especially Kaisser and Becker (2004) and Kaisser et
al. (2006) are suitable to get an overview over the
system?but concentrate on one new feature that was
developed especially for this web demo: In order
to improve user benefit, answers are supplemented
with relevant passages from the online encyclopedia
Wikipedia. We see two main benefits:
1. Users are presented with additional information
closely related to their actual information need
and thus of potential high interest.
2. The returned text passages present the answer
in context and thus help users to validate the
answer?there always will be the odd case where
a system returns a wrong result.
1for Question Answering with Linguistic Methods
Historically, our system is web-based, receiving
its answers by querying major search engines and
post processing their results. In order to satisfy
TREC requirements?which require participants to
return the ID of one document from the AQUAINT
corpus that supports the answer itself (Voorhees,
2004)?we already experimented with answer projec-
tion strategies in our TREC participations in recent
years. For this web demo we use Wikipedia instead
of the AQUAINT corpus for several reasons:
1. QuALiM is an open domain Question Answer-
ing system and Wikipedia is an ?open domain?
Encyclopedia; it aims to cover all areas of inter-
est as long as they are of some general interest.
2. Wikipedia is a free online encyclopedia. Other
than the AQUAINT corpus, there are no legal
problems when using it for a public demo.
3. Wikipedia is frequently updated, whereas the
AQUAINT corpus remains static and thus con-
tains a lot of outdated information.
Another advantage of Wikipedia is that the in-
formation contained is much more structured. As
we will see, this structure can be exploited to im-
prove performance when finding answers or?as in
our case?projecting answers.
2 How Best to Present Answers?
In the fields of Question Answering and Web
Search, the issue how answers/results should be pre-
sented is a vital one. Nevertheless, as of today, the
majority of QA system?which a few notable excep-
tions, e.g. MIT?s START (Katz et al, 2002)?are
32
Figure 1: Screenshot of QuALiM?s response to the question ?How many Munros are there in Scotland?? The green
bar to the left indicates that the system is confident to have found the right answer, which is shown in bold: ?284?.
Furthermore, one Wikipedia paragraph which contains additional information of potential interest to the user is dis-
played. In this paragraph the sentence containing the answer is highlighted. This display of context also allows the
user to validate the answer.
still experimental and research-oriented and typi-
cally only return the answer itself. Yet it is highly
doubtful that this is the best strategy.
Lin et al (2003) performed a study with
32 computer science students comparing four
types of answer context: exact answer, answer-
in-sentence, answer-in-paragraph, and answer-in-
document. Since they were interested in interface
design, they worked with a system that answered
all questions correctly. They found that 53% of all
participants preferred paragraph-sized chunks, 23%
preferred full documents, 20% preferred sentences,
and one participant preferred exact answer.
Web search engines typically show results as a
list of titles and short snippets that summarize how
the retrieved document is related to the query terms,
often called query-biased summaries (Tombros and
Sanderson, 1998). Recently, Kaisser et al (2008)
conducted a study to test whether users would pre-
fer search engine results of different lengths (phrase,
sentence, paragraph, section or article) and whether
the optimal response length could be predicted by
human judges. They find that judges indeed pre-
fer different response lengths for different types of
queries and that these can be predicted by other
judges.
In this demo, we opted for a slightly different, yet
related approach: The system does not decide on
one answer length, but always presents a combina-
tion of three different lengths to the user (see Figure
1): The answer itself (usually a phrase), is presented
in bold. Additionally, a paragraph relating the an-
swer to the question is shown, and in this paragraph
one sentence containing the answer is highlighted.
Note also, that each paragraph contains a link that
takes the user to the Wikipedia article, should he/she
want to know more about the subject. The intention
behind this mode of presentation is to prominently
display the piece of information the user is most in-
terested in, but also to present context information
and to furthermore provide options for the user to
find out more about the topic, should he/she want to.
3 Finding Supportive Wikipedia
Paragraphs
We use Lucene (Hatcher and Gospodnetic?, 2004) to
index the publically available Wikipedia dumps (see
http://download.wikimedia.org/). The text inside the
dump is broken down into paragraphs and each para-
graph functions as a Lucene document. The data of
each paragraph is stored in three fields: Title, which
contains the title of the Wikipedia article the para-
graph is from, Headers, which lists the title and all
section and subsection headings indicating the posi-
tion of the paragraph in the article and Text, which
stores the text of the article. An example can be seen
33
in Table 1.
Title ?Tom Cruise?
Headers ?Tom Cruise/Relationships and personal
life/Katie Holmes?
Text ?In April 2005, Cruise began dating
Katie Holmes ... the couple married in
Bracciano, Italy on November 18, 2006.?
Table 1: Example of Lucene index fields used.
As mentioned, QuALiM finds answers by query-
ing major search engines. After post processing, a
list of answer candidates, each one associated with a
confidence value, is output. For the question ?Who
is Tom Cruise married to??, for example, we get:
81.0: "Katie Holmes"
35.0: "Nicole Kidman"
The way we find supporting paragraphs for these
answers is probably best explained by giving an
example. Figure 3 shows the Lucene query we
use for the mentioned question and answer can-
didates. (The numbers behind the terms indicate
query weights.) As can be seen, we initially build
two separate queries for the Headers and the Text
fields (compare Table 1). In a later processing step,
both queries are combined into a single query us-
ing Lucene?s MultipleFieldQueryCreator
class. Note also that both answer candidates (?Katie
Holmes? and ?Nicole Kidman?) are included in this
one query. This is done because of speed issues: In
our setup, each query takes up roughly two seconds
of processing time. The complexity and length of
a query on the other hand has very little impact on
speed.
The type of question influences the query building
process in a fundamental manner. For the question
?When was Franz Kafka born?? and the correct an-
swer ?July 3, 1883?, for example, it is reasonable
to search for an article with title ?Franz Kafka? and
to expect the answer in the text on that page. For
the question ?Who invented the automobile?? on
the other hand, it is more reasonable to search the
information on a page called ?Karl Benz? (the an-
swer to the question). In order to capture this be-
haviour we developed a set of rules that for differ-
ent type of questions, increases or decreases con-
stituents? weights in either the Headers or the Text
field.
Additionally, during question analysis, certain
question constituents are marked as either Topic or
Focus (see Moldovan et al, (1999)). For the earlier
example question ?Tom Cruise? becomes the Topic
while ?married? is marked Focus2. These also influ-
ence constituents? weights in the different fields:
? Constituents marked as Topic are generally ex-
pected to be found in the Headers field. After
all, the topic marks what the question is about.
In a similar manner, titles and subtitles help to
structure an article, assisting the user to navi-
gate to the place where the relevant informa-
tion is most likely to be found: A paragraph?s
titles and subtitles indicate what the paragraph
is about.
? Constituents marked as Focus are generally ex-
pected to be found in the text, especially if they
are verbs. The focus indicates what the ques-
tion asks for, and such information can usually
rather be expected in the text than in titles or
subtitles.
Figure 3 also shows that, if we recognize named
entities (especially person names) in the question or
answer strings, we once include each named entity
as a quoted string and additionally add the words
it contains separately. This is to boost documents
which contain the complete name as used in the
question or the answer, but also to allow documents
which contain variants of these names, e.g. ?Thomas
Cruise Mapother IV?.
The formula to determine the exact boost factor
for each query term is complex and a matter of on-
going development. It additionally depends on the
following criteria:
? Named entities receive a higher weight.
? Capitalized words or constituents receive a
higher weight.
? The confidence value associated with the an-
swer candidate influences the boost factor.
? Whether a term originates from the question or
an answer candidate influences its weight in a
different manner for the header and text fields.
2With allowing verbs to be the Focus, we slightly depart
from the traditional definition of the term.
34
Header query:
"Tom Cruise"?10 Tom?5 Cruise?5 "Katie Holmes"?5 Katie?2.5 Holmes2.?5
"Nicole Kidman"?4.3 Nicole?2.2 Kidman?2.2
Text query:
married?10 "Tom Cruise"?1.5 Tom?4.5 Cruise?4.5 "Katie Holmes"?3 Katie?9 Holmes?9
"Nicole Kidman"?2.2 Nicole?6.6 Kidman?6.6
Figure 2: Lucene Queries used to find supporting documents for the ?Who is Tom Cruise married to??
and the two answers ?Katie Holmes? and ?Nicole Kidman?. Both queries are combined using Lucene?s
MultipleFieldQueryCreator class.
4 Future Work
Although QuALiM performed well in recent TREC
evaluations, improving precision and recall will of
course always be on our agenda. Beside this we cur-
rently focus on increasing processing speed. At the
time of writing, the web demo runs on a server with
a single 3GHz Intel Pentium D dual core processor
and 2Gb SDRAM. At times, the machine is shared
with other demos and applications. This makes re-
liable figures about speed difficult to produce, but
from our log files we can see that users usually wait
between three and twelve seconds for the system?s
results. While this is okay for a research demo, it
definitely would not be fast enough for a commer-
cial product. Three factors contribute with roughly
equal weight to the speed issue:
1. Search engine?s APIs usually do not return re-
sults as fast as their web interfaces built for hu-
man use do. Google for example has a built-in
one second delay for each query asked. The
demo usually sends out between one and four
queries per question, thus getting results from
Google alone takes between one and four sec-
onds.
2. All received results need to be post-processed,
the most computing heavy step here is parsing.
3. Finally, the local (8.3 GB big) Wikipedia index
needs to be queried, which roughly takes two
seconds per query.
We are currently looking into possibilities to im-
prove all of the above issues.
Acknowledgements
This work was supported by Microsoft Research
through the European PhD Scholarship Programme.
References
Erik Hatcher and Otis Gospodnetic?. 2004. Lucene in
Action. Manning Publications Co.
Michael Kaisser and Tilman Becker. 2004. Question An-
swering by Searching Large Corpora with Linguistic
Methods. In The Proceedings of the 2004 Edition of
the Text REtrieval Conference, TREC 2004.
Michael Kaisser, Silke Scheible, and Bonnie Webber.
2006. Experiments at the University of Edinburgh for
the TREC 2006 QA track. In The Proceedings of the
2006 Edition of the Text REtrieval Conference, TREC
2006.
Michael Kaisser, Marti Hearst, and John Lowe. 2008.
Improving Search Result Quality by Customizing
Summary Lengths. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics.
Boris Katz, Jimmy Lin, and Sue Felshin. 2002. The
START multimedia information system: Current tech-
nology and future directions. In Proceedings of the In-
ternational Workshop on Multimedia Information Sys-
tems (MIS 2002).
Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,
David Huynh, Boris Katz, and David R. Karger. 2003.
What Makes a Good Answer? The Role of Context in
Question Answering. Human-Computer Interaction
(INTERACT 2003).
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada
Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. LASSO: A tool for surfing the an-
swer net. In Proceedings of the Eighth Text Retrieval
Conference (TREC-8).
A. Tombros and M. Sanderson. 1998. Advantages of
query biased summaries in information retrieval. Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 2?10.
Ellen M. Voorhees. 2004. Overview of the TREC 2003
Question Answering Track. In The Proceedings of the
2003 Edition of the Text REtrieval Conference, TREC
2003.
35
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 41?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Question Answering based on Semantic Roles
Michael Kaisser Bonnie Webber
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW
Scotland
m.kaisser@sms.ed.ac.uk, bonnie@inf.ed.ac.uk
Abstract
This paper discusses how lexical resources
based on semantic roles (i.e. FrameNet,
PropBank, VerbNet) can be used for Ques-
tion Answering, especially Web Question
Answering. Two algorithms have been im-
plemented to this end, with quite different
characteristics. We discuss both approaches
when applied to each of the resources and a
combination of these and give an evaluation.
We argue that employing semantic roles can
indeed be highly beneficial for a QA system.
1 Introduction
A large part of the work done in NLP deals with
exploring how different tools and resources can be
used to improve performance on a task. The quality
and usefulness of the resource certainly is a major
factor for the success of the research, but equally so
is the creativity with which these tools or resources
are used. There usually is more than one way to
employ these, and the approach chosen largely de-
termines the outcome of the work.
This paper illustrates the above claims with re-
spect to three lexical resources ? FrameNet (Baker
et al, 1998), PropBank (Palmer et al, 2005) and
VerbNet (Schuler, 2005) ? that convey information
about lexical predicates and their arguments. We de-
scribe two new and complementary techniques for
using these resources and show the improvements to
be gained when they are used individually and then
together. We also point out problems that must be
overcome to achieve these results.
Compared with WordNet (Miller et al, 1993)?
which has been used widely in QA?FrameNet, Prop-
Bank and VerbNet are still relatively new, and there-
fore their usefulness for QA has still to be proven.
They offer the following features which can be used
to gain a better understanding of questions, sen-
tences containing answer candidates, and the rela-
tions between them:
? They all provide verb-argument structures for a
large number of lexical entries.
? FrameNet and PropBank contain semantically
annotated sentences that exemplify the under-
lying frame.
? FrameNet contains not only verbs but also lex-
ical entries for other part-of-speeches.
? FrameNet provides inter-frame relations that
can be used for more complex paraphrasing to
link the question and answer sentences.
In this paper we describe two methods that use
these resources to annotate both questions and sen-
tences containing answer candidates with seman-
tic roles. If these annotations can successfully be
matched, an answer candidate can be extracted. We
are able, for example, to give a complete frame-
semantic analysis of the following sentences and to
recognize that they all contain an answer to the ques-
tion ?When was Alaska purchased??:
The United States purchased Alaska in 1867.
Alaska was bought from Russia in 1867.
In 1867, Russia sold Alaska to the United States.
The acquisition of Alaska by the United States
in 1867 is known as ?Seward?s Folly.
41
The first algorithm we present uses the three
lexical resources to generate potential answer-
containing templates. While the templates contain
holes ? in particular, for the answer ? the parts that
are known can be used to create exact quoted search
queries. Sentences can then be extracted from the
output of the search engine and annotated with re-
spect to the resource being used. From this, an an-
swer candidate (if present) can be extracted. The
second algorithm analyzes the dependency structure
of the annotated example sentences in FrameNet and
PropBank. It then poses rather abstract queries to the
web, but can in its candidate sentence analysis stage
deal with a wider range of syntactic possibilities. As
we will see, the two algorithms are nicely comple-
mentary.
2 Method 1: Question Answering by
Natural Language Generation
The first method implemented uses the data avail-
able in the resources to generate potential answer
sentences to the question. While at least one com-
ponent of such a sentence (the answer) is yet un-
known, the remainder of the sentence can be used to
query a web search engine. The results can then be
analyzed, and if they match the originally-proposed
answer sentence structure, an answer candidate can
be extracted.
The first step is to annotate the question with its
semantic roles. For this task we use a classical se-
mantic role labeler combined with a rule-based ap-
proach. Keep in mind that our task is to annotate
questions, not declarative sentences. This is impor-
tant for several reasons:
1. The role labeler we use is trained on FrameNet
and PropBank data, i.e. mostly on declarative
sentences, whose syntax often differs consider-
ably from the sytax of questions. Aa a result,
the training and test set differ substantially in
nature.
2. Questions tend to be shorter and simpler syn-
tactically than declarative sentences?especially
those occurring in news corpora.
3. Questions contain one semantic role that has to
be annotated but which is not or is only implic-
itly (through the question word) mentioned ?
the answer.
Because of these reasons and especially because
many questions tend to be gramatically simple, we
found that a few simple rules can help the question
annotation process dramatically. We rely on Mini-
Par (Lin, 1998) to find the question?s head verb, e.g.
?purchase? for ?Who purchased YouTube?? (In the
following we will often refer to this question to il-
lustrate our approach.) We then look up all entries
in one of the resources, and for FrameNet and Prop-
Bank we simplify the annotated sentences until we
achieve a set of abstract frame structures, similar to
those in VerbNet. By doing this we intentionally re-
move certain levels of information that were present
in the original data, i.e. tense, voice, mood and nega-
tion. (In a later step we will reintroduce some of it.)
Here is what we find in FrameNet for ?purchase?:
Buyer[Subj,NP] VERB Goods[Obj,NP]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Seller[Dep,PP-from]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Money[Dep,PP-for]
Buyer[Subj,NP] VERB Goods[Obj,NP]
Recipient[Dep,PP-for]
...
A syntactic analysis of the question (also obtained
from MiniPar) shows that ?Who? is the (deep) sub-
ject and ?YouTube?, the (deep) object. The first of
the above frames fits this analysis best, because it
lists only the two roles with the desired grammatical
functions. By mapping the question analysis to this
frame, we can assign the roles Goods to ?YouTube?
and Buyer to ?Who?. From this we can conclude
that the question asks for the Buyer role.
An additional point suitable to illustrate why a
few simple rules can achieve in many cases more
that a statistical classifier, are When- and Where-
questions. Here, the hint that leads to the correct de-
tection of the answer role lies in the question word,
which is of course not present in the answer sen-
tence. Furthermore, the answer role in an answer
sentence will usually be realized as a PP with a to-
tally different dependency path than the one of ques-
tion?s question word. In contrast, a rule that states
that whenever a temporal or location question is de-
tected the answer role becomes, in FrameNet terms,
Place or Time, respectively, is very helpful here.
Once the role assignment is complete, we use
all abstract frames which contain the roles found in
the question to generate potential answer templates.
42
This is also the point where we reintroduce tense and
voice information:1 If the question was asked in the
a past tense, we will now create from each abstract
frame, all surface realizations in all past tenses, both
in active and passive voice. If we had used the an-
notated data directly without the detour over the ab-
stract frames, we would have difficulty sorting out
negated sentences, those in undesired moods and
those in unsuitable tenses. In contrast our approach
guarantees that all possible tenses in both voices are
generated, and no meaning-altering information like
mood and negation is present. For the example given
above we would create inter alia the following an-
swer templates:
ANSWER[NP] purchased YouTube
YouTube was purchased by ANSWER[NP]
ANSWER[NP] had purchased YouTube
...
The part (or parts) of the templates that are
known are quoted and sent to a search en-
gine. For the second example, this would be
"YouTube was purchased by". From the snippets
returned by the search engine, we extract candi-
date sentences and match them against the abstract
frame structure from which the queries were origi-
nally created. In this way, we annotate the candidate
sentences and are now able to identify the filler of
the answer role. For example, the above query re-
turns ?On October 9, 2006, YouTube was purchased
by Google for an incredible US$1.65 billion?, from
which we can extract ?Google?, because it is the NP
filling the buyer role.
So far, we have mostly discussed questions whose
answer role is an argument of the head verb. How-
ever, for questions like ?When was YouTube pur-
chased?? this assumption does not hold. Here, the
question asks for an adjunct. This is an important
difference for at least three reasons:
1. FrameNet and VerbNet do not or only sparsely
annotate peripheral adjuncts. (PropBank how-
ever does.)
2. In English, the position of adjuncts varies much
more than those of arguments.
3. In English, different kinds of adjuncts can oc-
cupy the same position in a sentence, although
naturally not at the same time.
1While we strip off mood and negation during the creation
of the abstract frames, we have not yet reintroduced them.
The following examples illustrate point 2:
YouTube was purchased by Google on October 9.
On October 9, YouTube was purchased by Google.
YouTube was purchased on October 9 by Google.
All variations are possible, although they may dif-
fer in frequency. PPs conveying other peripheral ad-
juncts ( e.g. ?for $1.65 billion?) could replace all the
above temporals PPs, or they could be added at other
positions.
The special behavior of these types of questions
has not only to be accounted for when annotating
the question with semantic roles, but also and when
creating and processing potential answer sentences.
We use an abstract frame structure like the following
to create the queries:
Buyer[Subj,NP,unknown]
VERB Goods[Obj,NP,"YouTube"]
While this lacks a role for the answer, we
can still use it to create, for example, the query
"has purchased YouTube". When sentences re-
turned from the search engine are then matched
against the abstract structure, we can extract all PPs
directly before the Buyer role, between the Buyer
role and the verb and directly behind the Goods role.
Then we can check all these PPs on their semantic
types and keep only those that match the answer type
of the question (if any).
3 Making use of FrameNet Frames and
Inter-Frame Relations
The method presented so far can be used with all
three resources. But FrameNet goes a step further
than just listing verb-argument structures: It orga-
nizes all of its lexical entries in frames2, with rela-
tions between frames that can be used for a wider
paraphrasing and inference. This section will ex-
plain how we make use of these relations.
The purchase.v entry is organized in a frame
called Commerce buy which also contains the
entries for buy.v and purchase ((act)).n. Both
these entries are annotated with the same frame
elements as purchase.v. This makes it possible to
formulate alternative answer templates, for exam-
ple: YouTube was bought by ANSWER[NP] and
2Note the different meaning of frame in FrameNet and Prop-
Bank/VerbNet respectively.
43
ANSWER[NP-Genitive] purchase of YouTube.
The latter example illustrates that we can also
generate target paraphrases with heads which are
not verbs. Handling these is usually easier than
sentences based on verbs, because no tense/voice
information has to be introduced.
Furthermore, frames themselves can stand in
different relations. The frame Commerce goods-
transfer, for example, relates both to the already
mentioned Commerce buy frame and to Com-
merce sell in an is perspectivized in relation. The
latter contains the lexical entries retail.v, retailer.n,
sale.n, sell.v, vend.v and vendor.n. Again, the
frame elements used are the same as for pur-
chase.v. Thus we can now create answer templates
like YouTube was sold to ANSWER[NP]. Other
templates created from this frame seem odd, e.g.
YouTube has been retailed to ANSWER[NP].
because the verb ?to retail? usually takes mass-
products as its object argument and not a company.
But FrameNet does not make such fine-grained
distinctions. Interestingly, we did not come across
a single example in our experiments where such
a phenomenon caused an overall wrong answer.
Sentences like the one above will most likely not be
found on the web (just because they are in a narrow
semantic sense not well-formed). Yet even if we
would get a hit, it probably would be a legitimate to
count the odd sentence ?YouTube had been retailed
to Google? as evidence for the fact that Google
bought YouTube.
4 Method 2: Combining Semantic Roles
and Dependency Paths
The second method we have implemented com-
pares the dependency structure of example sentences
found in PropBank and FrameNet with the depen-
dency structure of candidate sentences. (VerbNet
does not list example sentences for lexical entries,
so could not be used here.)
In a pre-processing step, all example sentences in
PropBank and FrameNet are analyzed and the de-
pendency paths from the head to each of the frame
elements are stored. For example, in the sentence
?The Soviet Union has purchased roughly eight mil-
lion tons of grain this month? (found in PropBank),
?purchased? is recognized as the head, ?The So-
viet Union? as ARG0, ?roughly eight million tons of
grain? as ARG1, and ?this month? as an adjunct of
type TMP. The stored paths to each are as follows:
headPath = ? i
role = ARG0, paths = {?s, ?subj}
role = ARG1, paths = {?obj}
role = TMP, paths = {?mod}
This says that the head is at the root, ARG0 is at both
surface subject (s) and deep subject (subj) position3,
ARG1 is the deep object (obj), and TMP is a direct
adjunct (mod) of the head.
Questions are annotated as described in Section 2.
Sentences that potentially contain answer candidates
are then retrieved by posing a rather abstract query
consisting of key words from the question. Once
we have obtained a set of candidate-containing sen-
tences, we ask the following questions of their de-
pendency structures compared with those of the ex-
ample sentences from PropBank4:
1a Does the candidate-containing sentence share
the same head verb as the example sentence?
1b Do the candidate sentence and the example sen-
tence share the same path to the head?
2a In the candidate sentence, do we find one or
more of the example?s paths to the answer role?
2b In the candidate sentence, do we find all of the
example?s paths to the answer role?
3a Can some of the paths for the other roles be
found in the candidate sentence?
3b Can all of the paths for the other roles be found
in the candidate sentence?
4a Do the surface strings of the other roles par-
tially match those of the question?
4b Do the surface strings of the other roles com-
pletely match those of the question?
Tests 1a and 2a of the above are required criteria:
If the candidate sentence does not share the same
head verb or if we can find no path to the answer
role, we exclude it from further processing.
3MiniPar allows more than one path between nodes due, for
example, to traces. The given example is MiniPar?s way of in-
dicating that this is a sentence in active voice.
4Note that our proceeding is not too different from what a
classical role labeler would do: Both approaches are primarily
based on comparing dependency paths. However, a standard
role labeler would not take tests 3a, 3b, 4a and 4b into account.
44
Each sentence that passes steps 1a and 2a is
assigned a weight of 1. For each of the remaining
tests that succeeds, we multiply that weight by
2. Hence a candidate sentence that passes all the
tests is assigned a weight 64 times higher than a
candidate that only passes tests 1a and 2a. We take
this as reasonable, as the evidence for having found
a correct answer is indeed very weak if only tests 1a
and 2a succeeded and very high if all tests succeed.
Whenever condition 2a holds, we can extract an
answer candidate from the sentence: It is the phrase
that the answer role-path points to. All extracted
answers are stored together with their weights, if
we retrieve the same answer more than once, we
simple add the new weight to the old ones. After
all candidate sentences have been compared with
all pre-extracted structures, the ones that do not
show the correct semantic type are removed. This
is especially important for answers that are realized
as adjuncts, see Section 2. We choose the answer
candidate with the highest score as the final answer.
We now illustrate this method with respect to our
question ?Who purchased YouTube?? The roles as-
signment process produces this result: ?YouTube?
is ARG1 and the answer is ARG0. From the web
we retrieve inter alia the following sentence: ?Their
aim is to compete with YouTube, which Google re-
cently purchased for more than $1 billion.? The de-
pendency analysis of the relevant phrases is:
headPath = ?i?i?pred?i?mod?pcom-n?rel?i
phrase = ?Google?, paths = {?s, ?subj}
phrase = ?which?, paths = {?obj}
phrase = ?YouTube?, paths = {?i?rel}
phrase = ?for more than $1 billion?, paths = {?mod}
If we annotate this sentence by using the analy-
sis from the above example sentence (?The Soviet
Union has purchased ...?) we get the following (par-
tially correct) role assignment: ?Google? is ARG0,
?which? is ARG1, ?for more than $1 billion? is TMP.
The following table shows the results of the 8 tests
described above:
1a OK
1b ?
2a OK
2b OK
3a OK
3b OK
4a ?
4b ?
Test 1a and 2a succeeded, so this sentence is as-
signed an initial weight of 1. However, only three
other tests succeed as well, so its final weight is
8. This rather low weight for a positive candi-
date sentence is due to the fact that we compared
it against a dependency structure which it only par-
tially matched. However, it might very well be the
case that another of the annotated sentences shows a
perfect fit. In such a case this comparison would
result in a weight of 64. If these were the only
two sentences that produce a weight of 1 or greater,
the final weight for this answer candidate would be
8 + 64 = 72.
5 Evaluation
We choose to evaluate our experiments with the
TREC 2002 QA test set because test sets from 2004
and beyond contain question series that pose prob-
lems that are separate from the research described
in this paper. While we participated in TREC 2004,
2005 and 2006, with an anaphora-resolution com-
ponent that performed quite well, we feel that if
one wants to evaluate a particular method, adding an
additional module, unrelated to the actual problem,
can distort the results. Additionally, because we are
searching for answers on the web rather than in the
AQUAINT corpus, we do not distinguish between
supported and unsupported judgments.
Of the 500 questions in the TREC 2002 test set,
236 have be as their head verb. As the work de-
scribed here essentially concerns verb semantics,
such questions fall outside its scope. Evaluation
has thus been carried out on only the remaining 264
questions.
For the first method (cf. Section 2), we evaluated
system accuracy separately for each of the three re-
sources, and then together, obtaining the following
values:
FrameNet PropBank VerbNet combined
0.181 0.227 0.223 0.261
For the combined run we looked up the verb
in all three resources simultaneously and all en-
tries from every resource were used. As can
be seen, PropBank and VerbNet perform equally
well, while FrameNet?s performance is significantly
lower. These differences are due to coverage issues:
FrameNet is still in development, and further ver-
sions with a higher coverage will be released. How-
ever, a closer look shows that coverage is a problem
for all of the resources. The following table shows
the percentage of the head verbs that were looked
45
up during the above experiments based on the 2002
question set, that could not be found (not found). It
also lists the percentage of lexical entries that con-
tain no annotated sentences (s = 0), five or fewer
(s <= 5), ten or fewer (s <= 10), or more than
50 (s > 50). Furthermore, the table lists the aver-
age number of lexical entries found per head verb
(avg senses) and the average number of annotated
sentences found per lexical entry (avg sent). 5
FrameNet PropBank
not found 11% 8%
s = 0 41% 7%
s <= 5 48% 35%
s <= 10 57% 45%
s > 50 8% 23%
avg senses 2.8 4.4
avg sent. 16.4 115.0
The problem with lexical entires only containing
a small number of annotated sentences is that these
sentences often do not exemplify common argument
structures, but rather seldom ones. As a solution to
this coverage problem, we experimented with a cau-
tious technique for expanding coverage. Any head
verb, we assumed displays the following three pat-
terns:
intransitive: [ARG0] VERB
transitive: [ARG0] VERB [ARG1]
ditransitive: [ARG0] VERB [ARG1] [ARG2]
During processing, we then determined whether
the question used the head verb in a standard in-
transitive, transitive or ditransitive way. If it did,
and that pattern for the head verb was not contained
in the resources, we temporarily added this abstract
frame to the list of abstract frames the system used.
This method rarely adds erroneous data, because the
question shows that such a verb argument structure
exists for the verb in question. By applying this tech-
nique, the combined performance increased from
0.261 to 0.284.
In Section 2 we reported on experiments that
make use of FrameNet?s inter-frame relations. The
next table lists the results we get when (a) using only
the question head verb for the reformulations, (b) us-
ing the other entries in the same frame as well, (c)
using all entries in all frames to which the starting
5As VerbNet contains no annotated sentences, it is not listed.
Note also, that these figures are not based on the resources in
total, but on the head verbs we looked up for our evaluation.
frame is related via the Inheritance, Perspective on
and Using relations (by using only those frames
which show the same frame elements).
(a) only question head verb 0.181
(b) all entries in frame 0.204
all entries in related frames(c) (with same frame elements) 0.215
Our second method described in Section 4, can
only be used with FrameNet and PropBank, because
VerbNet does not give annotated example sentences.
Here are the results:
FrameNet PropBank
0.030 0.159
Analysis shows that PropBank dramatically out-
performs FrameNet for three reasons:
1. PropBank?s lexicon contains more entries.
2. PropBank provides many more example sen-
tences for each entry.
3. FrameNet does not annotate peripheral ad-
juncts, and so does not apply to When- or
Where-questions.
The methods we have described above are com-
plementary. When they are combined so that when
method 1 returns an answer it is always chosen
as the final one, and only if method 1 did not
return an answer were the results from method
2 used, we obtain a combined accuracy of 0.306
when only using PropBank. When using method 1
with all three resources and our cautious coverage-
extension strategy, with all additional reformulations
that FrameNet can produce and method 2, using
PropBank and FrameNet, we achieve an accuracy of
0.367.
We also evaluated how much increase the de-
scribed approaches based on semantic roles bring to
our existing QA system. This system is completly
web-based and employs two answer finding strate-
gies. The first is based on syntactic reformulation
rules, which are similar to what we described in sec-
tion 2. However, in contrast to the work described
in this paper, these rules are manually created. The
second strategy uses key words from the question as
queries, and looks for frequently occuring n-grams
in the snippets returned by the search engine. The
system received the fourth best result for factoids in
TREC 2004 (Kaisser and Becker, 2004) (where both
46
just mentioned approaches are described in more de-
tail) and TREC 2006 (Kaisser et al, 2006), so it in
itself is a state-of-the-art, high performing QA sys-
tem. We observe an increase in performance by 21%
over the mentioned baseline system. (Without the
components based on semantic roles 130 out of 264
questions are answered correct, with these compo-
nents 157.)
6 Related Work
So far, there has been little work at the intersection
of QA and semantic roles. Fliedner (2004) describes
the functionality of a planned system based on the
German version of FrameNet, SALSA, but no so far
no paper describing the completed system has been
published.
Novischi and Moldovan (2006) use a technique
that builds on a combination of lexical chains and
verb argument structures extracted from VerbNet to
re-rank answer candidates. The authors? aim is to
recognize changing syntactic roles in cases where
an answer sentence shows a head verb different from
the question (similar to work described here in Sec-
tion 2). However, since VerbNet is based on the-
matic rather than semantic roles, there are problems
in using it for this purpose, illustrated by the follow-
ing VerbNet pattern for buy and sell:
[Agent] buy [Theme] from [Source]
[Agent] sell [Recipient] [Theme]
Starting with the sentence ?Peter bought a guitar
from Johnny?, and mapping the above roles for buy
to those for sell, the resulting paraphrase in terms
of sell would be ?Peter sold UNKNOWN a guitar?.
That is, there is nothing blocking the Agent role of
buy being mapped to the Agent role of sell, nor any-
thing linking the Source role of buy to any role in
sell. There is also a coverage problem: The authors
report that their approach only applies to 15 of 230
TREC 2004 questions. They report a performance
gain of 2.4% (MMR for the top 50 answers), but it
does not become clear whether that is for these 15
questions or for the complete question set.
The way in which we use the web in our first
method is somewhat similar to (Dumais et al, 2002).
However, our system allows control of verb argu-
ment structures, tense and voice and thus we can
create a much larger set of reformulations.
Regarding our second method, two papers de-
scribe related ideas: Firstly, in (Bouma et al, 2005)
the authors describe a Dutch QA system which
makes extensive use of dependency relations. In a
pre-processing step they parsed and stored the full
text collection for the Dutch CLEF QA-task. When
their system is asked a question, they match the de-
pendency structure of the question against the de-
pendency structures of potential answer candidates.
Additionally, a set of 13 equivalence rules allows
transformations of the kind ?the coach of Norway,
Egil Olsen? ? ?Egil Olsen, the coach of Norway?.
Secondly, Shen and Klakow (2006) use depen-
dency relation paths to rank answer candidates. In
their work, a candidate sentence supports an answer
if relations between certain phrases in the candidate
sentence are similar to the corresponding ones in the
question.
Our work complements that described in both
these papers, based as it is on a large collection of
semantically annotated example sentences: We only
require a candidate sentence to match one of the an-
notated example sentences. This allows us to deal
with a much wider range of syntactic possibilities, as
the resources we use do not only document verb ar-
gument structures, but also the many ways they can
be syntactically realized.
7 Discussion
Both methods presented in this paper employ se-
mantic roles but with different aims in mind: The
first method focuses on creating obvious answer-
containing sentences. Because in these sentences,
the head and the semantic roles are usually adjacent,
it is possible to create exact search queries that will
lead to answer candidates of a high quality. Our
second method can deal with a wider range of syn-
tactic variations but here the link to the answer sen-
tences? surface structure is not obvious, thus no ex-
act queries can be posed.
The overall accuracy we achieved suggests that
employing semantic roles for question answering is
indeed useful. Our results compare nicely to re-
cent TREC evaluation results. This is an especially
strong point, because virtually all high performing
TREC systems combine miscellaneous strategies,
which are already know to perform well. Because
47
the research question driving this work was to deter-
mine how semantic roles can benefit QA, we deliber-
ately designed our system to only build on semantic
roles. We did not chose to extend an already exist-
ing system, using other methods with a few features
based on semantic roles.
Our results are convincing qualitatively as well as
quantitavely: Detecting paraphrases and drawing in-
ferences is a key challenge in question answering,
which our methods achieve in various ways:
? They both recognize different verb-argument
structures of the same verb.
? Method 1 controls for tense and voice: Our sys-
tem will not take a future perfect sentence for
an answer to a present perfect question.
? For method 1, no answer candidates altered by
mood or negation are accepted.
? Method 1 can create and recognize answer sen-
tences, whose head is synonymous or related in
meaning to the answers head. In such transfor-
mations, we are also aware of potential changes
in the argument structure.
? The annotated sentences in the resources en-
ables method 2 to deal with a wide range of
syntactic phenomena.
8 Conclusion
This paper explores whether lexical resources like
FrameNet, PropBank and VerbNet are beneficial for
QA and describes two different methods in which
they can be used. One method uses the data in these
resources to generate potential answer-containing
sentences that are searched for on the web by using
exact, quoted search queries. The second method
uses only a keyword-based search, but it can anno-
tate a larger set of candidate sentences. Both meth-
ods perform well solemnly and they nicely comple-
ment each other. Our methods based on semantic
roles alone achieves an accuracy of 0.39. Further-
more adding the described features to our already
existing system boosted accuracy by 21%.
Acknowledgments
This work was supported by Microsoft Research
through the European PhD Scholarship Programme.
References
Colin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL.
Gosse Bouma, Jori Mur, Gertjan van Noord, Lonneke
van der Plas, and Jo?rg Tiedemann. 2005. Question
Answering for Dutch using Dependency Relations. In
Proceedings of the CLEF 2005 Workshop.
Susan Dumais, Michele Bankom, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web Question Answering: Is
More Always Better? Proceedings of UAI 2003.
Gerhard Fliedner. 2004. Towards Using FrameNet for
Question Answering. In Proceedings of the LREC
2004 Workshop on Building Lexical Resources from
Semantically Annotated Corpora.
Michael Kaisser and Tilman Becker. 2004. Question An-
swering by Searching Large Corpora with Linguistic
Methods. In The Proceedings of the 2004 Edition of
the Text REtrieval Conference, TREC 2004.
Michael Kaisser, Silke Scheible, and Bonnie Webber.
2006. Experiments at the University of Edinburgh for
the TREC 2006 QA track. In The Proceedings of the
2006 Edition of the Text REtrieval Conference, TREC
2006.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An On-Line Lexical Database.
Adrian Novischi and Dan Moldovan. 2006. Question
Answering with Lexical Chains Propagating Verb Ar-
guments. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Karin Kipper Schuler. 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
Dan Shen and Dietrich Klakow. 2006. Exploring Corre-
lation of Dependency Relation Paths for Answer Ex-
traction. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the ACL.
48
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 88?98,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Answer Sentence Retrieval by Matching Dependency Paths
Acquired from Question/Answer Sentence Pairs
Michael Kaisser
AGT Group (R&D) GmbH
Ja?gerstr. 41, 10117 Berlin, Germany
mkaisser@agtgermany.com
Abstract
In Information Retrieval (IR) in general
and Question Answering (QA) in particu-
lar, queries and relevant textual content of-
ten significantly differ in their properties
and are therefore difficult to relate with tra-
ditional IR methods, e.g. key-word match-
ing. In this paper we describe an algorithm
that addresses this problem, but rather than
looking at it on a term matching/term re-
formulation level, we focus on the syntac-
tic differences between questions and rele-
vant text passages. To this end we propose
a novel algorithm that analyzes dependency
structures of queries and known relevant
text passages and acquires transformational
patterns that can be used to retrieve rele-
vant textual content. We evaluate our algo-
rithm in a QA setting, and show that it out-
performs a baseline that uses only depen-
dency information contained in the ques-
tions by 300% and that it also improves per-
formance of a state of the art QA system
significantly.
1 Introduction
It is a well known problem in Information Re-
trieval (IR) and Question Answering (QA) that
queries and relevant textual content often signif-
icantly differ in their properties, and are therefore
difficult to match with traditional IR methods. A
common example is a user entering words to de-
scribe their information need that do not match
the words used in the most relevant indexed doc-
uments. This work addresses this problem, but
shifts focus from words to syntactic structures of
questions and relevant pieces of text. To this end,
we present a novel algorithm that analyses the de-
pendency structures of known valid answer sen-
tence and from these acquires patterns that can be
used to more precisely retrieve relevant text pas-
sages from the underlying document collection.
To achieve this, the position of key phrases in the
answer sentence relative to the answer itself is an-
alyzed and linked to a certain syntactic question
type. Unlike most previous work that uses depen-
dency paths for QA (see Section 2), our approach
does not require a candidate sentence to be similar
to the question in any respect. We learn valid de-
pendency structures from the known answer sen-
tences alone, and therefore are able to link a much
wider spectrum of answer sentences to the ques-
tion.
The work in this paper is presented and eval-
uated in a classical factoid Question Answering
(QA) setting. The main reason for this is that
in QA suitable training and test data is available
in the public domain, e.g. via the Text REtrieval
Conference (TREC), see for example (Voorhees,
1999). The methods described in this paper how-
ever can also be applied to other IR scenarios, e.g.
web search. The necessary condition for our ap-
proach to work is that the user query is somewhat
grammatically well formed; this kind of queries
are commonly referred to as Natural Language
Queries or NLQs.
Table 1 provides evidence that users indeed
search the web with NLQs. The data is based on
two query sets sampled from three months of user
logs from a popular search engine, using two dif-
ferent sampling techniques. The ?head? set sam-
ples queries taking query frequency into account,
so that more common queries have a proportion-
ally higher chance of being selected. The ?tail?
query set samples only queries that have been is-
88
Set Head Tail
Query # 15,665 12,500
how 1.33% 2.42%
what 0.77% 1.89%
define 0.34% 0.18%
is/are 0.25% 0.42%
where 0.18% 0.45%
do/does 0.14% 0.30%
can 0.14% 0.25%
why 0.13% 0.30%
who 0.12% 0.38%
when 0.09% 0.21%
which 0.03% 0.08%
Total 3.55% 6.86%
Table 1: Percentages of Natural Language queries in
head and tail search engine query logs. See text for
details.
sued less that 500 times during a three months pe-
riod and it disregards query frequency. As a result,
rare and frequent queries have the same chance of
being selected. Doubles are excluded from both
sets. Table 1 lists the percentage of queries in
the query sets that start with the specified word.
In most contexts this indicates that the query is a
question, which in turn means that we are dealing
with an NLQ. Of course there are many NLQs that
start with words other than the ones listed, so we
can expect their real percentage to be even higher.
2 Related Work
In IR the problem that queries and relevant tex-
tual content often do not exhibit the same terms is
commonly encountered. Latent Semantic Index-
ing (Deerwester et al 1900) was an early, highly
influential approach to solve this problem. More
recently, a significant amount of research is ded-
icated to query alteration approaches. (Cui et al
2002), for example, assume that if queries con-
taining one term often result in the selection of
documents containing another term, then a strong
relationship between the two terms exist. In their
approach, query terms and document terms are
linked via sessions in which users click on doc-
uments that are presented as results for the query.
(Riezler and Liu, 2010) apply a Statistical Ma-
chine Translation model to parallel data consist-
ing of user queries and snippets from clicked web
documents and in such a way extract contextual
expansion terms from the query rewrites.
We see our work as addressing the same fun-
damental problem, but shifting focus from query
term/document term mismatch to mismatches ob-
served between the grammatical structure of Nat-
ural Language Queries and relevant text pieces. In
order to achieve this we analyze the queries? and
the relevant contents? syntactic structure by using
dependency paths.
Especially in QA there is a strong tradition
of using dependency structures: (Lin and Pan-
tel, 2001) present an unsupervised algorithm to
automatically discover inference rules (essentially
paraphrases) from text. These inference rules are
based on dependency paths, each of which con-
nects two nouns. Their paths have the following
form:
N:subj:V?find?V:obj:N?solution?N:to:N
This path represents the relation ?X finds a solu-
tion to Y? and can be mapped to another path rep-
resenting e.g. ?X solves Y.? As such the approach
is suitable to detect paraphrases that describe the
relation between two entities in documents. How-
ever, the paper does not describe how the mined
paraphrases can be linked to questions, and which
paraphrase is suitable to answer which question
type.
(Attardi et al 2001) describes a QA system
that, after a set of candidate answer sentences
have been identified, matches their dependency
relations against the question. Questions and
answer sentences are parsed with MiniPar (Lin,
1998) and the dependency output is analyzed in
order to determine whether relations present in a
question also appear in a candidate sentence. For
the question ?Who killed John F. Kennedy?, for
example an answer sentence is expected to con-
tain the answer as subject of the verb ?kill?, to
which ?John F. Kennedy? should be in object re-
lation.
(Cui et al 2005) describe a fuzzy depen-
dency relation matching approach to passage re-
trieval in QA. Here, the authors present a statis-
tical technique to measure the degree of overlap
between dependency relations in candidate sen-
tences with their corresponding relations in the
question. Question/answer passage pairs from
TREC-8 and TREC-9 evaluations are used as
training data. As in some of the papers mentioned
earlier, a statistical translation model is used, but
this time to learn relatedness between paths. (Cui
et al 2004) apply the same idea to answer ex-
89
traction. In each sentences returned by the IR
module, all named entities of the expected answer
types are treated as answer candidates. For ques-
tions with an unknown answer type, all NPs in
the candidate sentence are considered. Then those
paths in the answer sentence that are connected
to an answer candidate are compared against the
corresponding paths in the question, in a similar
fashion as in (Cui et al 2005). The candidate
whose paths show the highest matching score is
selected. (Shen and Klakow, 2006) also describe
a method that is primarily based on similarity
scores between dependency relation pairs. How-
ever, their algorithm computes the similarity of
paths between key phrases, not between words.
Furthermore, it takes relations in a path not as in-
dependent from each other, but acknowledges that
they form a sequence, by comparing two paths
with the help of an adaptation of the Dynamic
Time Warping algorithm (Rabiner et al 1991).
(Molla, 2006) presents an approach for the ac-
quisition of question answering rules by apply-
ing graph manipulation methods. Questions are
represented as dependency graphs, which are ex-
tended with information from answer sentences.
These combined graphs can then be used to iden-
tify answers. Finally, in (Wang et al 2007), a
quasi-synchronous grammar (Smith and Eisner,
2006) is used to model relations between ques-
tions and answer sentences.
In this paper we describe an algorithm that
learns possible syntactic answer sentence formu-
lations for syntactic question classes from a set of
example question/answer sentence pairs. Unlike
the related work described above, it acknowledges
that a) a valid answer sentence?s syntax might
be very different for the question?s syntax and b)
several valid answer sentence structures, which
might be completely independent from each other,
can exist for one and the same question.
To illustrate this consider the question ?When
was Alaska purchased?? The following four sen-
tences all answer the given question, but only the
first sentence is a straightforward reformulation of
the question:
1. The United States purchased Alaska in 1867
from Russia.
2. Alaska was bought from Russia in 1867.
3. In 1867, the Russian Empire sold the Alaska
territory to the USA.
4. The acquisition of Alaska by the United
States of America from Russia in 1867 is
known as ?Seward?s Folly?.
The remaining three sentences introduce vari-
ous forms of syntactic and semantic transforma-
tions. In order to capture a wide range of possible
ways on how answer sentences can be formulated,
in our model a candidate sentence is not evalu-
ated according to its similarity with the question.
Instead, its similarity to known answer sentences
(which were presented to the system during train-
ing) is evaluated. This allows to us to capture a
much wider range of syntactic and semantic trans-
formations.
3 Overview of the Algorithm
Our algorithm uses input data containing pairs of
the following:
NLQs/Questions NLQs that describe the users?
information need. For the experiments car-
ried out in this paper we use questions from
the TREC QA track 2002-2006.
Relevant textual content This is a piece of text
that is relevant to the user query in that it
contains the information the user is search-
ing for. In this paper, we use sentences ex-
tracted from the AQUAINT corpus (Graff,
2002) that contain the answer to the given
TREC question.
In total, the data available to us for our experi-
ments consists of 8,830 question/answer sentence
pairs. This data is publicly available, see (Kaisser
and Lowe, 2008). The algorithm described in this
paper has three main steps:
Phrase alignment Key phrases from the ques-
tion are paired with phrases from the answer
sentences.
Pattern creation The dependency structures of
queries and answer sentences are analyzed
and patterns are extracted.
Pattern evaluation The patterns discovered in
the last step are evaluated and a confidence
score is assigned to each.
The acquired patterns can then be used during
retrieval, where a question is matched against the
antecedents describing the syntax of the question.
90
Input: (a) Query: ?When was Alaska purchased??
(b) Answer sentence: ?The acquisition of Alaska happened in 1867.?
Step 1: Question is segmented into key phrases and stop words:
When[1]+was[2]+NP[3]+VERB[4]
Step 2: Key question phrases are aligned with key answer sentence phrases:
[3]Alaska ? Alaska
[4]purchased ? acquisition
ANSWER ? 1867
Step 3: A pre-computed parse tree of the answer sentence is loaded:
1: The (the, DT, 2) [det]
2: acquisition (acquisition, NN, 5) [nsubj]
3: of (of, IN, 2) [prep]
4: Alaska (Alaska, IN, 2) [pobj]
5: happened (happen, VBD, null) [ROOT]
6: in (in, IN, 5) [prep]
7: 1867 (1867, CD, 6) [pobj]
Step 4: Dependency paths from key question phrases to the answer are computed:
Alaska?1867: ?pobj?prep?nsubj?prep?pobj
acquisition?1867: ?nsubj?prep?pobj
Step 5: The resulting pattern is stored:
Query: When[1]+was[2]+NP[3]+VERB[4]
Path 3: ?pobj?prep?nsubj?prep?pobj
Path 4: ?nsubj?prep?pobj
Figure 1: The pattern creation algorithm exemplified in five key steps for the query ?When was Alaska pur-
chased?? and the answer sentence ?The acquisition of Alaska happened in 1867.?
Note that one question can potentially match sev-
eral patterns. The consequents contain descrip-
tions of grammatical structures of potential an-
swer sentences that can be used to identify and
evaluate candidate sentences.
4 Phrase Alignment
The goal of this processing step is to align phrases
from the question with corresponding phrases
from the answer sentences in the training data.
Consider the following example:
Query: ?When was the Alaska territory pur-
chased??
Answer sentence: ?The acquisition of what
would become the territory of Alaska took place
in 1867.?
The mapping that has to be achieved is:
Query Answer Sentence
phrase phrase
?Alaska territory? ?territory of Alaska?
?purchased? ?acquisition?
ANSWER ?1867?
In our approach, this is a two step process.
First we align on a word level, then the output
of the word alignment process is used to iden-
tify and align phrases. Word Alignment is im-
portant in many fields of NLP, e.g. Machine
Translation (MT) where words in parallel, bilin-
gual corpora need to be aligned, see (Och and
Ney, 2003) for a comparison of various statisti-
cal alignment models. In our case however we
are dealing with a monolingual alignment prob-
lem which enables us to exploit clues not available
for bilingual alignment: First of all, we can expect
many query words to be present in the answer sen-
tence, either with the exact same surface appear-
ance or in some morphological variant. Secondly,
there are tools available that tell us how semanti-
cally related two words are, most notably Word-
Net (Miller et al 1993). For these reasons we im-
plemented a bespoke alignment strategy, tailored
towards our problem description.
This method is described in detail in (Kaisser,
2009). The processing steps described in the
next sections build on its output. For reasons of
brevity, we skip a detailed explanations in this pa-
per and focus only on its key part: the alignment
of words with very different surface structures.
For more details we would like to point the reader
to the aforementioned work.
In the above example, the alignment of ?pur-
91
chased? and ?acquisition? is the most problem-
atic, because the surface structures of the two
words clearly are very different. For such cases
we experimented with a number of alignment
strategies based on WordNet. These approaches
are similar in that each picks one word that has to
be aligned from the question at a time and com-
pares it to all of the non-stop words in the answer
sentence. Each of the answer sentence words is
assigned a value between zero and one express-
ing its relatedness to the question word. The
highest scoring word, if above a certain thresh-
old, is selected as the closest semantic match.
Most of these approaches make use of Word-
Net::Similarity, a Perl software package that mea-
sures semantic similarity (or relatedness) between
a pair of word senses by returning a numeric value
that represents the degree to which they are sim-
ilar or related (Pedersen et al 2004). Addition-
ally, we developed a custom-built method that as-
sumes that two words are semantically related if
any kind of pointer exists between any occurrence
of the words root form in WordNet. For details of
these experiments, please refer to (Kaisser, 2009).
In our experiments the custom-built method per-
formed best, and was therefore used for the exper-
iments described in this paper. The main reasons
for this are:
1. Many of the measures in the Word-
Net::Similarity package take only hyponym/
hypernym relations into account. This makes
aligning word of different parts of speech
difficult or even impossible. However, such
alignments are important for our needs.
2. Many of the measures return results, even if
only a weak semantic relationship exists. For
our purposes however, it is beneficial to only
take strong semantic relations into account.
5 Pattern Creation
Figure 1 details our algorithm in its five key steps.
In step 1 and 2 key phrases from the question are
aligned to the corresponding phrases in the an-
swer sentence, see Section 4 of this paper. Step
3 is concerned with retrieving the parse tree for
the answer sentence. In our implementation all
answer sentences in the training set have for per-
formance reasons been parsed beforehand with
the Stanford Parser (Klein and Manning, 2003b;
Klein and Manning, 2003a), so at this point they
are simply loaded from file. Step 4 is the key step
in our algorithm. From the previous steps, we
know where the key constituents from the ques-
tion as well as the answer are located in the an-
swer sentence. This enables us to compute the
dependency paths in the answer sentences? parse
tree that connect the answer with the key con-
stituents. In our example the answer is ?1867?
and the key constituents are ?acquisition? and
?Alaska.? Knowing the syntactic relationships
(captured by their dependency paths) between the
answer and the key phrases enables us to capture
one syntactic possibility of how answer sentences
to queries of the form When+was+NP+VERB can
be formulated.
As can be seen in Step 5 a flat syntactic ques-
tion representation is stored, together with num-
bers assigned to each constituent. The num-
bers for those constituents for which alignments
in the answer sentence were sought and found
are listed together with the resulting dependency
paths. Path 3 for example denotes the path from
constituent 3 (the NP ?Alaska?) to the answer. If
no alignment could be found for a constituent,
null is stored instead of a path. Should two or
more alternative constituents be identified for one
question constituent, additional patterns are cre-
ated, so that each contains one of the possibilities.
The described procedure is repeated for all ques-
tion/answer sentence pairs in the training set and
for each, one or more patterns are created.
It is worth to note that many TREC ques-
tions are fairly short and grammatically sim-
ple. In our training data we for exam-
ple find 102 questions matching the pattern
When[1]+was[2]+NP[3]+VERB[4], which
together list 382 answer sentences, and thus 382
potentially different answer sentence structures
from which patterns can be gained. As a result,
the amount of training examples we have avail-
able, is sufficient to achieve the performance de-
scribed in Section 7. The algorithm described in
this paper can of course also be used for more
complicated NLQs, although in such a scenario a
significantly larger amount of training data would
have to be used.
6 Pattern Evaluation
For each created pattern, at least one match-
ing example must exists: the sentence that was
92
used to create it in the first place. However, we
do not know how precise each pattern is. To
this end, an additional processing step between
pattern creation and application is needed: pat-
tern evaluation. Similar approaches to ours have
been described in the relevant literature, many
of them concerned with bootstrapping, starting
with (Ravichandran and Hovy, 2002). The gen-
eral purpose of this step is to use the available
data about questions and their correct answers to
evaluate how often each created pattern returns a
correct or an incorrect result. This data is stored
with each pattern and the result of the equation,
often called pattern precision, can be used during
retrieval stage. Pattern precision in our case is de-
fined as:
p =
#correct + 1
#correct + #incorrect + 2
(1)
We use Lucene to retrieve the top 100 para-
graphs from the AQUAINT corpus by issuing a
query that consists of the query?s key words and
all non-stop words in the answer. Then, all pat-
terns are loaded whose antecedent matches the
query that is currently being processed. After that,
constituents from all sentences in the retrieved
100 paragraphs are aligned to the query?s con-
stituents in the same way as for the sentences dur-
ing pattern creation, see Section 5. Now, the paths
specified in these patterns are searched for in the
paragraphs? parse trees. If they are all found,
it is checked whether they all point to the same
node and whether this node?s surface structure is
in some morphological form present in the answer
strings associated with the question in our train-
ing data. If this is the case a variable in the pat-
tern named correct is increased by 1, otherwise
the variable incorrect is increased by 1. After the
evaluation process is finished the final version of
the pattern given as an example in Figure 1 now
is:
Query: When[1]+was[2]+NP[3]+VERB[4]
Path 3: ?pobj?prep?nsubj?prep?pobj
Path 4: ?nsubj?prep?pobj
Correct: 15
Incorrect: 4
The variables correct and incorrect are used
during retrieval, where the score of an answer can-
didate ac is the sum of all scores of all matching
patterns p:
score(ac) =
n?
i=1
score(pi) (2)
where
score(pi) =
{
correcti+1
correcti+incorrecti+2
if match
0 no match
(3)
The highest scoring candidate is selected.
We would like to explicitly call out one prop-
erty of our algorithm: It effectively returns two
entities: a) a sentence that constitutes a valid
response to the query, b) the head node of a
phrase in that sentence that constitutes the answer.
Therefore the algorithm can be used for sentence
retrieval or for answer retrieval. It depends on
the application which of the two behaviors is de-
sired. In the next section, we evaluate its answer
retrieval performance.
7 Experiments & Results
This section provides an evaluation of the algo-
rithm described in this paper. The key questions
we seek to answer are the following:
1. How does our method perform when com-
pared to a baseline that extracts dependency
paths from the question?
2. How much does the described algorithm im-
prove performance of a state-of-the-art QA
system?
3. What is the effect of training data size on per-
formance? Can we expect that more training
data would further improve the algorithm?s
performance?
7.1 Evaluation Setup
We use all factoid questions in TREC?s QA test
sets from 2002 to 2006 for evaluation for which
a known answer exists in the AQUAINT corpus.
Additionally, the data in (Lin and Katz, 2005) is
used. In this paper the authors attempt to identify
a much more complete set of relevant documents
for a subset of TREC 2002 questions than TREC
itself. We adopt a cross validation approach for
our evaluation. Table 4 shows how the data is split
into five folds.
In order to evaluate the algorithm?s patterns we
need a set of sentences to which they can be ap-
plied. In a traditional QA system architecture,
93
Test Number of Correct Answer Sentences
Mean Med
set = 0 <= 1 <= 3 <= 5 <= 10 <= 25 <= 50 >= 75 >= 90 >= 100
2002 0.203 0.396 0.580 0.671 0.809 0.935 0.984 0.0 0.0 0.0 6.86 2.0
2003 0.249 0.429 0.627 0.732 0.828 0.955 0.997 0.003 0.003 0.0 5.67 2.0
2004 0.221 0.368 0.539 0.637 0.799 0.936 0.985 0.0 0.0 0.0 6.51 3.0
2005 0.245 0.404 0.574 0.665 0.777 0.912 0.987 0.0 0.0 0.0 7.56 2.0
2006 0.241 0.389 0.568 0.665 0.807 0.920 0.966 0.006 0.0 0.0 8.04 3.0
Table 2: Fraction of sentences that contain correct answers in Evaluation Set 1 (approximation).
Test Number of Correct Answer Sentences
Mean Med
set = 0 <= 1 <= 3 <= 5 <= 10 <= 25 <= 50 >= 75 >= 90 >= 100
2002 0.0 0.074 0.158 0.235 0.342 0.561 0.748 0.172 0.116 0.060 33.46 21.0
2003 0.0 0.099 0.203 0.254 0.356 0.573 0.720 0.161 0.090 0.031 32.88 19.0
2004 0.0 0.073 0.137 0.211 0.328 0.598 0.779 0.142 0.069 0.034 30.82 20.0
2005 0.0 0.163 0.238 0.279 0.410 0.589 0.759 0.141 0.097 0.069 30.87 17.0
2006 0.0 0.125 0.207 0.281 0.415 0.596 0.727 0.173 0.122 0.088 32.93 17.5
Table 3: Fraction of sentences that contain correct answers in Evaluation Set 2 (approximation).
Fold
Training Data Test Data
sets used # set #
1 T03, T04, T05, T06 4565 T02 1159
2 T02, T04, T05, T06, Lin02 6174 T03 1352
3 T02, T03, T05, T06, Lin02 6700 T04 826
4 T02, T03, T04, T06, Lin02 6298 T05 1228
5 T02, T03, T04, T05, Lin02 6367 T06 1159
Table 4: Splits into training and tests sets of the data
used for evaluation. T02 stands for TREC 2002 data
etc. Lin02 is based on (Lin and Katz, 2005). The #
rows show how many question/answer sentence pairs
are used for training and for testing.
see e.g. (Prager, 2006; Voorhees, 2003), the docu-
ment or passage retrieval step performs this func-
tion. This step is crucial to a QA system?s per-
formance, because it is impossible to locate an-
swers in the subsequent answer extraction step if
the passages returned during passage retrieval do
not contain the answer in the first place. This also
holds true in our case: the patterns cannot be ex-
pected to identify a correct answer if none of the
sentences used as input contains the correct an-
swer. We therefore use two different evaluation
sets to evaluate our algorithm:
1. The first set contains for each question all
sentences in the top 100 paragraphs returned
by Lucene when using simple queries made
up from the question?s key words. It cannot
be guaranteed that answers to every question
are present in this test set.
2. For the second set, the query additionally list
all known correct answers to the question as
parts of one OR operator. This increases the
chance that the evaluation set actually con-
tains valid answer sentences significantly.
In order to provide a quantitative characteriza-
tion of the two evaluation sets we estimated the
number of correct answer sentences they contain.
For each paragraph it was determined whether it
contained one of the known answer strings and
at least of one of the question key words. Ta-
bles 2 and 3 show for each evaluation set how
many answers on average it contains per ques-
tion. The column ?= 0? for example shows the
fraction of questions for which no valid answer
sentence is contained in the evaluation set, while
column ?>= 90? gives the fraction of questions
with 90 or more valid answer sentences. The last
two columns show mean and median values.
7.2 Comparison with Baseline
As pointed out in Section 2 there is a strong tra-
dition of using dependency paths in QA. Many
relevant papers describe algorithms that analyze
a question?s grammatical structure and expect
to find a similar structure in valid answer sen-
tences, e.g. (Attardi et al 2001), (Cui et al 2005)
or (Bouma et al 2005) to name just a few. As
already pointed out, a major contribution of our
work is that we do not assume this similarity. In
our approach valid answer sentences are allowed
to have grammatical structures that are very dif-
ferent from the question and also very different
from each other. Thus it is natural to compare our
approach against a baseline that compares can-
didate sentences not against patterns that were
gained from question/answer sentence pairs, but
from questions alone. In order to create these pat-
terns, we use a small trick: During the Pattern
Creation step, see Section 5 and Figure 1, we re-
94
place the answer sentences in the input file with
the questions, and assume that the question word
indicates the position where the answer should be
located.
Test Q Qs with > 1 Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 147 50 0.117 0.156
2003 354 237 76 22 0.062 0.093
2004 204 142 74 26 0.127 0.183
2005 319 214 97 46 0.144 0.215
2006 352 208 85 31 0.088 0.149
Sum 1658 1122 452 176 0.106 0.156
Table 5: Performance based on evaluation set 1.
Test Q Qs with > 1 Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 239 133 0.310 0.414
2003 354 237 149 88 0.248 0.371
2004 204 142 119 65 0.319 0.458
2005 319 214 161 92 0.288 0.429
2006 352 208 139 84 0.238 0.403
Sum 1658 1122 807 462 0.278 0.411
Table 6: Performance based on evaluation set 2.
Tables 5 and 6 show how our algorithm per-
forms on evaluation sets 1 and 2, respectively. Ta-
bles 7 and 8 show how the baseline performs on
evaluation sets 1 and 2, respectively. The tables?
columns list the year of the TREC test set used,
the number of questions in the set (we only use
questions for which we know that there is an an-
swer in the corpus), the number of questions for
which one or more patterns exist, how often at
least one pattern returned the correct answer, how
often we get an overall correct result by taking
all patterns and their confidence values into ac-
count, accuracy@1 of the overall system, and ac-
curacy@1 computed only for those questions for
which we have at least one pattern available (for
all other questions the system returns no result.)
As can be seen, on evaluation set 1 our method
outperforms the baseline by 300%, on evaluation
set 2 by 311%, taking accuracy if a pattern exists
as a basis.
Test Q Qs with Min one Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 43 14 0.033 0.044
2003 354 237 28 10 0.028 0.042
2004 204 142 19 6 0.029 0.042
2005 319 214 21 7 0.022 0.033
2006 352 208 20 7 0.020 0.034
Sum 1658 1122 131 44 0.027 0.039
Table 7: Baseline performance based on evaluation set
1.
Many of the papers cited earlier that use an ap-
proach similar to our baseline approach of course
report much better results than Tables 7 and 8.
This however is not too surprising as the approach
Test Q Qs with Min one Overall Accuracy Acc. if
set number patterns correct correct overall pattern
2002 429 321 77 37 0.086 0.115
2003 354 237 39 26 0.073 0.120
2004 204 142 25 15 0.074 0.073
2005 319 214 38 18 0.056 0.084
2006 352 208 34 16 0.045 0.077
Sum 1658 1122 213 112 0.068 0.100
Table 8: Baseline performance based on evaluation set
2.
described in this paper and the baseline approach
do not make use of many techniques commonly
used to increase performance of a QA system, e.g.
TF-IDF fallback strategies, fuzzy matching, man-
ual reformulation patterns etc. It was a deliberate
decision from our side not to use any of these ap-
proaches. After all, this would result in an ex-
perimental setup where the performance of our
answer extraction strategy could not have been
observed in isolation. The QA system used as a
baseline in the next section makes use of many of
these techniques and we will see that our method,
as described here, is suitable to increase its per-
formance significantly.
7.3 Impact on an existing QA System
Tables 9 and 10 show how our algorithm in-
creases performance of our QuALiM system, see
e.g. (Kaisser et al 2006). Section 6 in this pa-
per describes via formulas 2 and 3 how answer
candidates are ranked. This ranking is combined
with the existing QA system?s candidate ranking
by simply using it as an additional feature that
boosts candidates proportionally to their confi-
dence score. The difference between both tables
is that the first uses all 1658 questions in our test
sets for the evaluation, whereas the second con-
siders only those 1122 questions for which our
system was able to learn a pattern. Thus for Table
10 questions which the system had no chance of
answering due to limited training data are omitted.
As can be seen, accuracy@1 increases by 4.9% on
the complete test set and by 11.5% on the partial
set.
Note that the QA system used as a baseline is
at an advantage in at least two respects: a) It has
important web-based components and as such has
access to a much larger body of textual informa-
tion. b) The algorithm described in this paper is an
answer extraction approach only. For paragraph
retrieval we use the same approach as for evalu-
ation set 1, see Section 7.1. However, in more
than 20% of the cases, this method returns not
95
a single paragraph that contains both the answer
and at least one question keyword. In such cases,
the simple paragraph retrieval makes it close to
impossible for our algorithm to return the correct
answer.
Test Set QuALiM QASP combined increase
2002 0.503 0.117 0.524 4.2%
2003 0.367 0.062 0.390 6.2%
2004 0.426 0.127 0.451 5.7%
2005 0.373 0.144 0.389 4.2%
2006 0.341 0.088 0.358 5.0%
02-06 0.405 0.106 0.425 4.9%
Table 9: Top-1 accuracy of the QuALiM system on its
own and when combined with the algorithm described
in this paper. All increases are statistically significant
using a sign test (p < 0.05).
Test Set QuALiM QASP combined increase
2002 0.530 0.156 0.595 12.3%
2003 0.380 0.093 0.430 13.3%
2004 0.465 0.183 0.514 10.6%
2005 0.388 0.214 0.421 8.4%
2006 0.385 0.149 0.428 11.3%
02-06 0.436 0.157 0.486 11.5%
Table 10: Top-1 accuracy of the QuALiM system on
its own and when combined with the algorithm de-
scribed in this paper, when only considering questions
for which a pattern could be acquired from the training
data. All increases are statistically significant using a
sign test (p < 0.05).
7.4 Effect of Training Data Size
We now assess the effect of training data size on
performance. Tables 5 and 6 presented earlier
show that an average of 32.2% of the questions
have no matching patterns. This is because the
data used for training contained no examples for a
significant subset of question classes. It can be ex-
pected that, if more training data would be avail-
able, this percentage would decrease and perfor-
mance would increase. In order to test this as-
sumption, we repeated the evaluation procedure
detailed in this section several times, initially us-
ing data from only one TREC test set for train-
ing and then gradually adding more sets until all
available training data had been used. The results
for evaluation set 2 are presented in Figure 2. As
can be seen, every time more data is added, per-
formance increases. This strongly suggests that
the point of diminishing returns, when adding ad-
ditional training data no longer improves perfor-
mance is not yet reached.
Figure 2: Effect of the amount of training data on sys-
tem performance
8 Conclusions
In this paper we present an algorithm that acquires
syntactic information about how relevant textual
content to a question can be formulated from a
collection of paired questions and answer sen-
tences. Other than previous work employing de-
pendency paths for QA, our approach does not as-
sume that a valid answer sentence is similar to the
question and it allows many potentially very dif-
ferent syntactic answer sentence structures. The
algorithm is evaluated using TREC data, and it
is shown that it outperforms an algorithm that
merely uses the syntactic information contained
in the question itself by 300%. It is also shown
that the algorithm improves the performance of a
state-of-the-art QA system significantly.
As always, there are many ways how we could
imagine our algorithm to be improved. Combin-
ing it with fuzzy matching techniques as in (Cui et
al., 2004) or (Cui et al 2005) is an obvious direc-
tion for future work. We are also aware that in or-
der to apply our algorithm on a larger scale and in
a real world setting with real users, we would need
a much larger set of training data. These could
be acquired semi-manually, for example by using
crowd-sourcing techniques. We are also thinking
about fully automated approaches, or about us-
ing indirect human evidence, e.g. user clicks in
search engine logs. Typically users only see the
title and a short abstract of the document when
clicking on a result, so it is possible to imagine a
scenario where a subset of these abstracts, paired
with user queries, could serve as training data.
96
References
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi, and Alessandro Tommasi.
2001. PIQASso: Pisa Question Answering System.
In Proceedings of the 2001 Edition of the Text RE-
trieval Conference (TREC-01).
Gosse Bouma, Jori Mur, and Gertjan van Noord. 2005.
Reasoning over Dependency Relations for QA. In
Proceedings of the IJCAI workshop on Knowledge
and Reasoning for Answering Questions (KRAQ-
05).
Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying
Ma. 2002. Probabilistic query expansion using
query logs. In 11th International World Wide Web
Conference (WWW-02).
Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and
Min-Yen Kan. 2004. National University of Sin-
gapore at the TREC-13 Question Answering Main
Task. In Proceedings of the 2004 Edition of the Text
REtrieval Conference (TREC-04).
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question Answering Pas-
sage Retrieval Using Dependency Relations. In
Proceedings of the 28th ACM-SIGIR International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR-05).
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1900.
Indexing by Latent Semantic Analysis. Journal of
the American society for information science, 41(6).
David Graff. 2002. The AQUAINT Corpus of English
News Text.
Michael Kaisser and John Lowe. 2008. Creating a
Research Collection of Question Answer Sentence
Pairs with Amazon?s Mechanical Turk. In Proceed-
ings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC-08).
Michael Kaisser, Silke Scheible, and Bonnie Webber.
2006. Experiments at the University of Edinburgh
for the TREC 2006 QA track. In Proceedings of
the 2006 Edition of the Text REtrieval Conference
(TREC-06).
Michael Kaisser. 2009. Acquiring Syntactic and
Semantic Transformations in Question Answering.
Ph.D. thesis, University of Edinburgh.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate Unlexicalized Parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics (ACL-03).
Dan Klein and Christopher D. Manning. 2003b. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Informa-
tion Processing Systems 15.
Jimmy Lin and Boris Katz. 2005. Building a Reusable
Test Collection for Question Answering. Journal of
the American Society for Information Science and
Technology (JASIST).
Dekang Lin and Patrick Pantel. 2001. Discovery of
Inference Rules for Question-Answering. Natural
Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Pars-
ing Systems.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993.
Introduction to WordNet: An On-Line Lexical
Database. Journal of Lexicography, 3(4):235?244.
Diego Molla. 2006. Learning of Graph-based
Question Answering Rules. In Proceedings of
HLT/NAACL 2006 Workshop on Graph Algorithms
for Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::Similarity - Measur-
ing the Relatedness of Concepts. In Proceedings
of the Nineteenth National Conference on Artificial
Intelligence (AAAI-04).
John Prager. 2006. Open-Domain Question-
Answering. Foundations and Trends in Information
Retrieval, 1(2).
L. R. Rabiner, A. E. Rosenberg, and S. E. Levin-
son. 1991. Considerations in Dynamic Time Warp-
ing Algorithms for Discrete Word Recognition. In
Proceedings of IEEE Transactions on Acoustics,
Speech and Signal Processing.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning Surface Text Patterns for a Question An-
swering System. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-02).
Stefan Riezler and Yi Liu. 2010. Query Rewriting
using Monolingual Statistical Machine Translation.
Computational Linguistics, 36(3).
Dan Shen and Dietrich Klakow. 2006. Exploring Cor-
relation of Dependency Relation Paths for Answer
Extraction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL (COLING/ACL-06).
David A. Smith and Jason Eisner. 2006. Quasisyn-
chronous grammars: Alignment by Soft Projec-
tion of Syntactic Dependencies. In Proceedings of
the HLTNAACL Workshop on Statistical Machine
Translation.
Ellen M. Voorhees. 1999. Overview of the Eighth
Text REtrieval Conference (TREC-8). In Pro-
ceedings of the Eighth Text REtrieval Conference
(TREC-8).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 Question Answering Track. In Proceedings of
the 2003 Edition of the Text REtrieval Conference
(TREC-03).
97
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A Qua-
sisynchronous Grammar for QA. In Proceedings of
EMNLP-CoNLL 2007.
98
