Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257?266,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Clustering to Find Exemplar Terms for Keyphrase Extraction
Zhiyuan Liu, Peng Li, Yabin Zheng, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, pengli09, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrases are widely used as a brief
summary of documents. Since man-
ual assignment is time-consuming, vari-
ous unsupervised ranking methods based
on importance scores are proposed for
keyphrase extraction. In practice, the
keyphrases of a document should not only
be statistically important in the docu-
ment, but also have a good coverage of
the document. Based on this observa-
tion, we propose an unsupervised method
for keyphrase extraction. Firstly, the
method finds exemplar terms by leverag-
ing clustering techniques, which guaran-
tees the document to be semantically cov-
ered by these exemplar terms. Then the
keyphrases are extracted from the doc-
ument using the exemplar terms. Our
method outperforms sate-of-the-art graph-
based ranking methods (TextRank) by
9.5% in F1-measure.
1 Introduction
With the development of Internet, information on
the web is emerging exponentially. How to effec-
tively seek and manage information becomes an
important research issue. Keyphrases, as a brief
summary of a document, provide a solution to help
organize, manage and retrieve documents, and are
widely used in digital libraries and information re-
trieval.
Keyphrases in articles of journals and books
are usually assigned by authors. However,
most articles on the web usually do not have
human-assigned keyphrases. Therefore, automatic
keyphrase extraction is an important research task.
Existing methods can be divided into supervised
and unsupervised approaches.
The supervised approach (Turney, 1999) re-
gards keyphrase extraction as a classification task.
In this approach, a model is trained to determine
whether a candidate term of the document is a
keyphrase, based on statistical and linguistic fea-
tures. For the supervised keyphrase extraction
approach, a document set with human-assigned
keyphrases is required as training set. However,
human labelling is time-consuming. Therefore, in
this study we focus on unsupervised approach.
As an example of an unsupervised keyphrase
extraction approach, the graph-based ranking (Mi-
halcea and Tarau, 2004) regards keyphrase extrac-
tion as a ranking task, where a document is repre-
sented by a term graph based on term relatedness,
and then a graph-based ranking algorithm is used
to assign importance scores to each term. Existing
methods usually use term cooccurrences within a
specified window size in the given document as an
approximation of term relatedness (Mihalcea and
Tarau, 2004).
As we know, none of these existing works
gives an explicit definition on what are appropri-
ate keyphrases for a document. In fact, the existing
methods only judge the importance of each term,
and extract the most important ones as keyphrases.
From the observation of human-assigned
keyphrases, we conclude that good keyphrases
of a document should satisfy the following
properties:
1. Understandable. The keyphrases are un-
derstandable to people. This indicates the
extracted keyphrases should be grammatical.
For example, ?machine learning? is a gram-
matical phrase, but ?machine learned? is not.
2. Relevant. The keyphrases are semantically
relevant with the document theme. For ex-
ample, for a document about ?machine learn-
ing?, we want the keyphrases all about this
theme.
3. Good coverage. The keyphrases should
257
cover the whole document well. Sup-
pose we have a document describing ?Bei-
jing? from various aspects of ?location?,
?atmosphere? and ?culture?, the extracted
keyphrases should cover all the three aspects,
instead of just a partial subset of them.
The classification-based approach determines
whether a term is a keyphrase in isolation, which
could not guarantee Property 3. Neither does the
graph-based approach guarantee the top-ranked
keyphrases could cover the whole document. This
may cause the resulting keyphrases to be inappro-
priate or badly-grouped.
To extract the appropriate keyphrases for a doc-
ument, we suggest an unsupervised clustering-
based method. Firstly the terms in a document are
grouped into clusters based on semantic related-
ness. Each cluster is represented by an exemplar
term, which is also the centroid of each cluster.
Then the keyphrases are extracted from the docu-
ment using these exemplar terms.
In this method, we group terms based on se-
mantic relatedness, which guarantees a good cov-
erage of the document and meets Property 2 and
3. Moreover, we only extract the keyphrases in ac-
cordance with noun group (chunk) patterns, which
guarantees the keyphrases satisfy Property 1.
Experiments show that the clustering-based
method outperforms the state-of-the-art graph-
based approach on precision, recall and F1-
measure. Moreover, this method is unsupervised
and language-independent, which is applicable in
the web era with enormous information.
The rest of the paper is organized as follows.
In Section 2, we introduce and discuss the re-
lated work in this area. In Section 3, we give an
overview of our method for keyphrase extraction.
From Section 4 to Section 7, the algorithm is de-
scribed in detail. Empirical experiment results are
demonstrated in Section 8, followed by our con-
clusions and plans for future work in Section 9.
2 Related Work
A straightforward method for keyphrase extrac-
tion is to select keyphrases according to frequency
criteria. However, the poor performance of this
method drives people to explore other methods. A
pioneering achievement is carried out in (Turney,
1999), as mentioned in Section 1, a supervised ma-
chine learning method was suggested in this paper
which regards keyphrase extraction as a classifi-
cation task. In this work, parameterized heuristic
rules are combined with a genetic algorithm into a
system for keyphrase extraction. A different learn-
ing algorithm, Naive Bayes method, is applied in
(Frank et al, 1999) with improved results on the
same data used in (Turney, 1999). Hulth (Hulth,
2003; Hulth, 2004) adds more linguistic knowl-
edge, such as syntactic features, to enrich term
representation, which significantly improves the
performance. Generally, the supervised methods
need manually annotated training set, which may
sometimes not be practical, especially in the web
scenario.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becom-
ing the most widely used unsupervised approach
for keyphrase extraction. The work in (Litvak
and Last, 2008) applies HITS algorithm on the
word graph of a document under the assumption
that the top-ranked nodes should be the document
keywords. Experiments show that classification-
based supervised method provides the highest key-
word identification accuracy, while the HITS al-
gorithm gets the highest F-measure. Work in
(Huang et al, 2006) also considers each document
as a term graph where the structural dynamics of
these graphs can be used to identify keyphrases.
Wan and Xiao (Wan and Xiao, 2008b) use a
small number of nearest neighbor documents to
provide more knowledge to improve graph-based
keyphrase extraction algorithm for single docu-
ment. Motivated by similar idea, Wan and Xiao
(Wan and Xiao, 2008a) propose to adopt cluster-
ing methods to find a small number of similar doc-
uments to provide more knowledge for building
word graphs for keyword extraction. Moreover,
after our submission of this paper, we find that
a method using community detection on seman-
tic term graphs is proposed for keyphrase extrac-
tion from multi-theme documents (Grineva et al,
2009). In addition, some practical systems, such
as KP-Miner (Elbeltagy and Rafea, 2009), also
do not need to be trained on a particular human-
annotated document set.
In recent years, a number of systems are de-
veloped for extracting keyphrases from web docu-
ments (Kelleher and Luz, 2005; Chen et al, 2005),
email (Dredze et al, 2008) and some other spe-
cific sources, which indicates the importance of
keyphrase extraction in the web era. However,
258
none of these previous works has overall consid-
eration on the essential properties of appropriate
keyphrases mentioned in Section 1.
We should also note that, although the preci-
sion and recall of most current keyphrase extrac-
tors are still much lower compared to other NLP-
tasks, it does not indicate the performance is poor
because even different annotators may assign dif-
ferent keyphrases to the same document. As de-
scribed in (Wan and Xiao, 2008b), when two anno-
tators were asked to label keyphrases on 308 doc-
uments, the Kappa statistic for measuring inter-
agreement among them was only 0.70.
3 Algorithm Overview
The method proposed in this paper is mainly in-
spired by the nature of appropriate keyphrases
mentioned in Section 1, namely understandable,
semantically relevant with the document and high
coverage of the whole document.
Let?s analyze the document describing ?Bei-
jing? from the aspects of ?location?, ?atmosphere?
and ?culture?. Under the bag-of-words assump-
tion, each term in the document, except for func-
tion words, is used to describe an aspect of the
theme. Based on these aspects, terms are grouped
into different clusters. The terms in the same clus-
ter are more relevant with each other than with
the ones in other clusters. Taking the terms ?tem-
perature?, ?cold? and ?winter? for example, they
may serve the aspect ?atmosphere? instead of ?lo-
cation? or some other aspects when talking about
?Beijing?.
Based on above description, it is thus reason-
able to propose a clustering-based method for
keyphrase extraction. The overview of the method
is:
1. Candidate term selection. We first filter out
the stop words and select candidate terms for
keyphrase extraction.
2. Calculating term relatedness. We use some
measures to calculate the semantic related-
ness of candidate terms.
3. Term clustering. Based on term relatedness,
we group candidate terms into clusters and
find the exemplar terms of each cluster.
4. From exemplar terms to keyphrases. Fi-
nally, we use these exemplar terms to extract
keyphrases from the document.
In the next four sections we describe the algo-
rithm in detail.
4 Candidate Term Selection
Not all words in a document are possible to be se-
lected as keyphrases. In order to filter out the noisy
words in advance, we select candidate terms using
some heuristic rules. This step proceeds as fol-
lows. Firstly the text is tokenized for English or
segmented into words for Chinese and other lan-
guages without word-separators. Then we remove
the stop words and consider the remaining single
terms as candidates for calculating semantic relat-
edness and clustering.
In methods like (Turney, 1999; Elbeltagy and
Rafea, 2009), candidate keyphrases were first
found using n-gram. Instead, in this method, we
just find the single-word terms as the candidate
terms at the beginning. After identifying the ex-
emplar terms within the candidate terms, we ex-
tract multi-word keyphrases using the exemplars.
5 Calculating Term Relatedness
After selecting candidate terms, it is important to
measure term relatedness for clustering. In this pa-
per, we propose two approaches to calculate term
relatedness: one is based on term cooccurrence
within the document, and the other by leveraging
human knowledge bases.
5.1 Cooccurrence-based Term Relatedness
An intuitive method for measuring term relat-
edness is based on term cooccurrence relations
within the given document. The cooccurrence
relation expresses the cohesion relationships be-
tween terms.
In this paper, cooccurrence-based relatedness is
simply set to the count of cooccurrences within a
window of maximum w words in the whole doc-
ument. In the following experiments, the window
size w is set from 2 to 10 words.
Each document can be regarded as a word se-
quence for computing cooccurrence-based relat-
edness. There are two types of word sequence
for counting term cooccurrences. One is the origi-
nal word sequence without filtering out any words,
and the other is after filtering out the stop words
or the words with specified part-of-speech (POS)
tags. In this paper we select the first type because
each word in the sequence takes important role for
measuring term cooccurrences, no matter whether
259
it is a stop word or something else. If we filter
out some words, the term relatedness will not be
as precise as before.
In experiments, we will investigate how the
window size influences the performance of
keyphrase extraction.
5.2 Wikipedia-based Term Relatedness
Many methods have been proposed for measuring
the relatedness between terms using external re-
sources. One principled method is leveraging hu-
man knowledge bases. Inspired by (Gabrilovich
and Markovitch, 2007), we adopt Wikipedia, the
largest encyclopedia collected and organized by
human on the web, as the knowledge base to mea-
sure term relatedness.
The basic idea of computing term related-
ness by leveragting Wikipedia is to consider each
Wikipedia article as a concept. Then the se-
mantic meaning of a term could be represented
as a weighted vector of Wikipedia concepts, of
which the values are the term?s TFIDF within cor-
responding Wikipedia articles. We could com-
pute the term relatedness by comparing the con-
cept vectors of the terms. Empirical evaluations
confirm that the idea is effective and practical
for computing term relatedness (Gabrilovich and
Markovitch, 2007).
In this paper, we select cosine similarity, Eu-
clidean distance, Point-wise Mutual Information
and Normalized Google Similarity Distance (Cili-
brasi and Vitanyi, 2007) for measuring term relat-
edness based on the vector of Wikipedia concepts.
Denote the Wikipedia-concept vector of the
term t
i
as C
i
= {c
i1
, c
i2
, ..., c
iN
}, where N in-
dicates the number of Wikipedia articles, and c
ik
is the TFIDF value of w
i
in the kth Wikipedia ar-
ticle. The cosine similarity is defined as
cos(i, j) =
C
i
? C
j
?C
i
??C
j
?
(1)
The definition of Euclidean distance is
euc(i, j) =
?
?
?
?
N
?
k=1
(c
ik
? c
jk
)
2 (2)
Point-wise Mutual Information (PMI) is a com-
mon approach to quantify relatedness. Here we
take three ways to measure term relatedness using
PMI. One is based on Wikipedia page count,
pmi
p
(i, j) = log
2
N ? p(i, j)
p(i) ? p(j)
(3)
where p(i, j) is the number of Wikipedia articles
containing both t
i
and t
j
, while p(i) is the number
of articles which contain t
i
. The second is based
on the term count in Wikipedia articles,
pmi
t
(i, j) = log
2
T ? t(i, j)
t(i) ? t(j)
(4)
where T is the number of terms in Wikipedia,
t(i, j) is the number of t
i
and t
j
occurred adja-
cently in Wikipedia, and t(i) is the number of t
i
in
Wikipedia. The third one is a combination of the
above two PMI ways,
pmi
c
(i, j) = log
2
N ? pt(i, j)
p(i) ? p(j)
(5)
where pt(i, j) indicates the number of Wikipedia
articles containing t
i
and t
j
as adjacency. It is ob-
vious that pmi
c
(i, j) ? pmi
p
(i, j), and pmi
c
(i, j)
is more strict and accurate for measuring related-
ness.
Normalized Google Similarity Distance (NGD)
is a new measure for measuring similarity between
terms proposed by (Cilibrasi and Vitanyi, 2007)
based on information distance and Kolmogorov
complexity. It could be applied to compute term
similarity from the World Wide Web or any large
enough corpus using the page counts of terms.
NGD used in this paper is based on Wikipedia ar-
ticle count, defined as
ngd(i, j) =
max(log p(i), log p(j)) ? logp(i, j)
logN ? min(logp(i), logp(j))
(6)
where N is the number of Wikipedia articles used
as normalized factor.
Once we get the term relatedness, we could then
group the terms using clustering techniques and
find exemplar terms for each cluster.
6 Term Clustering
Clustering is an important unsupervised learning
problem, which is the assignment of objects into
groups so that objects from the same cluster are
more similar to each other than objects from dif-
ferent clusters (Han and Kamber, 2005). In this
paper, we use three widely used clustering algo-
rithms, hierarchical clustering, spectral clustering
and Affinity Propagation, to cluster the candidate
terms of a given document based on the semantic
relatedness between them.
260
6.1 Hierarchical Clustering
Hierarchical clustering groups data over a variety
of scales by creating a cluster tree. The tree is a
multilevel hierarchy, where clusters at one level
are joined as clusters at the next level. The hier-
archical clustering follows this procedure:
1. Find the distance or similarity between every
pair of data points in the dataset;
2. Group the data points into a binary and hier-
archical cluster tree;
3. Determine where to cut the hierarchical tree
into clusters. In hierarchical clustering, we
have to specify the cluster number m in ad-
vance.
In this paper, we use the hierarchical cluster-
ing implemented in Matlab Statistics Toolbox.
Note that although we use hierarchical clustering
here, the cluster hierarchy is not necessary for the
clustering-based method.
6.2 Spectral Clustering
In recent years, spectral clustering has become one
of the most popular modern clustering algorithms.
Spectral clustering makes use of the spectrum of
the similarity matrix of the data to perform dimen-
sionality reduction for clustering into fewer di-
mensions, which is simple to implement and often
outperforms traditional clustering methods such as
k-means. Detailed introduction to spectral cluster-
ing could be found in (von Luxburg, 2006).
In this paper, we use the spectral clustering tool-
box developed by Wen-Yen Chen, et al (Chen et
al., 2008) 1. Since the cooccurrence-based term
relatedness is usually sparse, the traditional eigen-
value decomposition in spectral clustering will
sometimes get run-time error. In this paper, we
use the singular value decomposition (SVD) tech-
nique for spectral clustering instead.
For spectral clustering, two parameters are re-
quired to be set by the user: the cluster number
m, and ? which is used in computing similarities
from object distances
s(i, j) = exp(
?d(i, j)
2
2?
2
) (7)
where s(i, j) and d(i, j) are the similarity and dis-
tance between i and j respectively.
1The package could be accessed via http://www.cs.
ucsb.edu/
?
wychen/sc.html.
6.3 Affinity Propagation
Another powerful clustering method, Affinity
Propagation, is based on message passing tech-
niques. AP was proposed in (Frey and Dueck,
2007), where AP was reported to find clusters with
much lower error than those found by other meth-
ods. In this paper, we use the toolbox developed
by Frey, et al 2.
Detailed description of the algorithm could be
found in (Frey and Dueck, 2007). Here we intro-
duced three parameters for AP:
? Preference. Rather than requiring prede-
fined number of clusters, Affinity Propaga-
tion takes as input a real number p for each
term, so that the terms with larger p are more
likely to be chosen as exemplars, i.e., cen-
troids of clusters. These values are referred
to as ?preferences?. The preferences are usu-
ally be set as the maximum, minimum, mean
or median of s(i, j), i 6= j.
? Convergence criterion. AP terminates if (1)
the local decisions stay constant for I
1
itera-
tions; or (2) the number of iterations reaches
I
2
. In this work, we set I
1
to 100 and I
2
to
1, 000.
? Damping factor. When updating the mes-
sages, it is important to avoid numerical os-
cillations by using damping factor. Each
message is set to ? times its value from the
previous iteration plus 1 ? ? times its pre-
scribed updated value, where the damping
factor ? is between 0 and 1. In this paper we
set ? = 0.9.
7 From Exemplar Terms to Keyphrases
After term clustering, we select the exemplar
terms of each clusters as seed terms. In Affinity
Propagation, the exemplar terms are directly ob-
tained from the clustering results. In hierarchical
clustering, exemplar terms could also be obtained
by the Matlab toolbox. While in spectral cluster-
ing, we select the terms that are most close to the
centroid of a cluster as exemplar terms.
As reported in (Hulth, 2003), most manually
assigned keyphrases turn out to be noun groups.
Therefore, we annotate the document with POS
2The package could be accessed via http://www.
psi.toronto.edu/affinitypropagation/.
261
tags using Stanford Log-Linear Tagger 3, and then
extract the noun groups whose pattern is zero or
more adjectives followed by one or more nouns.
The pattern can be represented using regular ex-
pressions as follows
(JJ) ? (NN |NNS|NNP )+
where JJ indicates adjectives and various forms
of nouns are represented using NN , NNS and
NNP . From these noun groups, we select the
ones that contain one or more exemplar terms to
be the keyphrases of the document.
In this process, we may find single-word
keyphrases. In practice, only a small fraction of
keyphrases are single-word. Thus, as a part of
postprocessing process, we have to use a frequent
word list to filter out the terms that are too com-
mon to be keyphrases.
8 Experiment Results
8.1 Datasets and Evaluation Metric
The dataset used in the experiments is a collec-
tion of scientific publication abstracts from the In-
spec database and the corresponding manually as-
signed keyphrases 4. The dataset is used in both
(Hulth, 2003) and (Mihalcea and Tarau, 2004).
Each abstract has two kinds of keyphrases: con-
trolled keyphrases, restricted to a given dictionary,
and uncontrolled keyphrases, freely assigned by
the experts. We use the uncontrolled keyphrases
for evaluation as proposed in (Hulth, 2003) and
followed by (Mihalcea and Tarau, 2004).
As indicated in (Hulth, 2003; Mihalcea and
Tarau, 2004), in uncontrolled manually assigned
keyphrases, only the ones that occur in the cor-
responding abstracts are considered in evaluation.
The extracted keyphrases of various methods and
manually assigned keyphrases are compared after
stemming.
In the experiments of (Hulth, 2003), for her su-
pervised method, Hulth splits a total of 2, 000 ab-
stracts into 1, 000 for training, 500 for validation
and 500 for test. In (Mihalcea and Tarau, 2004),
due to the unsupervised method, only the test set
was used for comparing the performance of Tex-
tRank and Hulth?s method.
3The package could be accessed via http://http://
nlp.stanford.edu/software/tagger.shtml.
4Many thanks to Anette Hulth for providing us the dataset.
For computing Wikipedia-based relatedness,
we use a snapshot on November 11, 2005 5. The
frequent word list used in the postprocessing step
for filtering single-word phrases is also computed
from Wikipedia. In the experiments of this pa-
per, we add the words that occur more than 1, 000
times in Wikipedia into the list.
The clustering-based method is completely un-
supervised. Here, we mainly run our method on
test set and investigate the influence of relatedness
measurements and clustering methods with differ-
ent parameters. Then we compare our method
with two baseline methods: Hulth?s method and
TextRank. Finally, we analyze and discuss the per-
formance of the method by taking the abstract of
this paper as a demonstration.
8.2 Influence of Relatedness Measurements
We first investigate the influence of semantic re-
latedness measurements. By systematic experi-
ments, we find that Wikipedia-based relatedness
outperforms cooccurrence-based relatedness for
keyphrase extraction, though the improvement is
not significant. In Table 1, we list the perfor-
mance of spectral clustering with various related-
ness measurements for demonstration. In this ta-
ble, the w indicates the window size for counting
cooccurrences in cooccurrence-based relatedness.
cos, euc, etc. are different measures for com-
puting Wikipedia-based relatedness which we pre-
sented in Section 5.2.
Table 1: Influence of relatedness measurements
for keyphrase extraction.
Parameters Precision Recall F1-measure
Cooccurrence-based Relatedness
w = 2 0.331 0.626 0.433
w = 4 0.333 0.621 0.434
w = 6 0.331 0.630 0.434
w = 8 0.330 0.623 0.432
w = 10 0.333 0.632 0.436
Wikipedia-based Relatedness
cos 0.348 0.655 0.455
euc 0.344 0.634 0.446
pmi
p
0.344 0.621 0.443
pmi
t
0.344 0.619 0.442
pmi
c
0.350 0.660 0.457
ngd 0.343 0.620 0.442
5The dataset could be get from http://www.cs.
technion.ac.il/
?
gabr/resources/code/
wikiprep/.
262
We use spectral clustering here because it out-
performs other clustering techniques, which will
be shown in the next subsection. The results in Ta-
ble 1 are obtained when the cluster number m =
2
3
n, where n is the number of candidate terms ob-
tained in Section 5. Besides, for Euclidean dis-
tance and Google distance, we set ? = 36 of For-
mula 7 to convert them to corresponding similari-
ties, where we get the best result when we conduct
different trails with ? = 9, 18, 36, 54, though there
are only a small margin among them.
As shown in Table 1, although the method using
Wikipedia-based relatedness outperforms that us-
ing cooccurrence-based relatedness, the improve-
ment is not prominent. Wikipedia-based related-
ness is computed according to global statistical in-
formation on Wikipedia. Therefore it is more pre-
cise than cooccurrence-based relatedness, which is
reflected in the performance of the keyphrase ex-
traction. However, on the other hand, Wikipedia-
based relatedness does not catch the document-
specific relatedness, which is represented by the
cooccurrence-based relatedness. It will be an in-
teresting future work to combine these two types
of relatedness measurements.
From this subsection, we conclude that, al-
though the method using Wikipedia-based related-
ness performs better than cooccurrence-based one,
due to the expensive computation of Wikipedia-
based relatedness, the cooccurrence-based one is
good enough for practical applications.
8.3 Influence of Clustering Methods and
Their Parameters
To demonstrate the influence of clustering meth-
ods for keyphrase extraction, we fix the relat-
edness measurement as Wikipedia-based pmi
c
,
which has been shown in Section 8.2 to be the best
relatedness measurement.
In Table 2, we show the performance of three
clustering techniques for keyphrase extraction.
For hierarchical clustering and spectral clustering,
the cluster number m are set explicitly as the pro-
portion of candidate terms n, while for Affinity
Propagation, we set preferences as the minimum,
mean, median and maximum of s(i, j) to get dif-
ferent number of clusters, denoted as min, mean,
median and max in the table respectively.
As shown in the table, when cluster number m
is large, spectral clustering outperforms hierarchi-
cal clustering and Affinity Propagation. Among
Table 2: Influence of clustering methods for
keyphrase extraction.
Parameters Precision Recall F1-measure
Hierarchical Clustering
m =
1
4
n 0.365 0.369 0.367
m =
1
3
n 0.365 0.369 0.367
m =
1
2
n 0.351 0.562 0.432
m =
2
3
n 0.346 0.629 0.446
m =
4
5
n 0.340 0.657 0.448
Spectral Clustering
m =
1
4
n 0.385 0.409 0.397
m =
1
3
n 0.374 0.497 0.427
m =
1
2
n 0.374 0.497 0.427
m =
2
3
n 0.350 0.660 0.457
m =
4
5
n 0.340 0.679 0.453
Affinity Propagation
p = max 0.331 0.688 0.447
p = mean 0.433 0.070 0.121
p = median 0.422 0.078 0.132
p = min 0.419 0.059 0.103
these methods, only Affinity Propagation under
some parameters performs poorly.
8.4 Comparing with Other Algorithms
Table 3 lists the results of the clustering-based
method compared with the best results reported
in (Hulth, 2003; Mihalcea and Tarau, 2004) on
the same dataset. For each method, the table lists
the total number of assigned keyphrases, the mean
number of keyphrases per abstract, the total num-
ber of correct keyphrases, and the mean number of
correct keyphrases. The table also lists precision,
recall and F1-measure. In this table, hierarchical
clustering, spectral clustering and Affinity Propa-
gation are abbreviated by ?HC?, ?SC? and ?AP?
respectively.
The result of Hulth?s method listed in this ta-
ble is the best one reported in (Hulth, 2003) on the
same dataset. This is a supervised classification-
based method, which takes more linguistic fea-
tures in consideration for keyphrase extraction.
The best result is obtained using n-gram as candi-
date keyphrases and adding POS tags as candidate
features for classification.
The result of TextRank listed here is the best
one reported in (Mihalcea and Tarau, 2004) on the
same dataset. To obtain the best result, the authors
built an undirected graph using window w = 2
on word sequence of the given document, and ran
263
Table 3: Comparison results of Hulth?s method, TextRank and our clustering-based method.
Assigned Correct
Method Total Mean Total Mean Precision Recall F1-measure
Hulth?s 7,815 15.6 1,973 3.9 0.252 0.517 0.339
TextRank 6,784 13.7 2,116 4.2 0.312 0.431 0.362
HC 7,303 14.6 2,494 5.0 0.342 0.657 0.449
SC 7,158 14.3 2,505 5.0 0.350 0.660 0.457
AP 8,013 16.0 2,648 5.3 0.330 0.697 0.448
PageRank on it.
In this table, the best result of hierarchical clus-
tering is obtained by setting the cluster number
m =
2
3
n and using Euclidean distance for comput-
ing Wikipedia-based relatedness. The parameters
of spectral clustering are the same as in last sub-
section. For Affinity Propagation, the best result
is obtained under p = max and using Wikipedia-
based Euclidean distance as relatedness measure.
From this table, we can see clustering-
based method outperforms TextRank and Hulth?s
method. For spectral clustering, F1-measure
achieves an approximately 9.5% improvement as
compared to TextRank.
Furthermore, since the clustering-based method
is unsupervised, we do not need any set for train-
ing and validation. In this paper, we also carry out
an experiment on the whole Hulth?s dataset with
2, 000 abstracts. The performance is similar to
that on 500 abstracts as shown above. The best
result is obtained when we use spectral clustering
by setting m = 2
3
n with Wikipedia-based pmi
c
relatedness, which is the same in 500 abstracts. In
this result, we extract 29, 517 keyphrases, among
which 9, 655 are correctly extracted. The preci-
sion, recall and F1-measure are 0.327, 0.653 and
0.436 respectively. The experiment results show
that the clustering-based method is stable.
8.5 Analysis and Discussions
From the above experiment results, we can see the
clustering-based method is both robust and effec-
tive for keyphrase extraction as an unsupervised
method.
Here, as an demonstration, we use spectral clus-
tering and Wikipedia-based pmi
c
relatedness to
extract keyphrases from the abstract of this pa-
per. The extracted stemmed keyphrases under var-
ious cluster numbers are shown in Figure 1. In
this figure, we find that when m = 1
4
n,
1
3
n,
1
2
n,
the extracted keyphrases are identical, where the
exemplar terms under m = 1
3
n are marked in
boldface. We find several aspects like ?unsuper-
vised?, ?exemplar term? and ?keyphrase extrac-
tion? are extracted correctly. In fact, ?clustering
technique? in the abstract should also be extracted
as a keyphrase. However, since ?clustering? is
tagged as a verb that ends in -ing, which disagrees
the noun group patterns, thus the phrase is not
among the extracted keyphrases.
When m = 2
3
n, the extracted keyphrases
are noisy with many single-word phrases. As
the cluster number increases, more exemplar
terms are identified from these clusters, and more
keyphrases will be extracted from the document
based on exemplar terms. If we set the cluster
number to m = n, all terms will be selected as
exemplar terms. In this extreme case, all noun
groups will be extracted as keyphrases, which
is obviously not proper for keyphrase extraction.
Thus, it is important for this method to appropri-
ately specify the cluster number.
In the experiments, we also notice that frequent
word list is important for keyphrase extraction.
Without the list for filtering, the best F1-measure
will decrease by about 5 percent to 40%. How-
ever, the solution of using frequent word list is
somewhat too simple, and in future work, we plan
to investigate a better combination of clustering-
based method with traditional methods using term
frequency as the criteria.
9 Conclusion and Future Work
In this paper, we propose an unsupervised
clustering-based keyphrase extraction algorithm.
This method groups candidate terms into clus-
ters and identify the exemplar terms. Then
keyphrases are extracted from the document based
on the exemplar terms. The clustering based on
term semantic relatedness guarantees the extracted
keyphrases have a good coverage of the document.
Experiment results show the method has a good ef-
264
Figure 1: Keyphrases in stemmed form extracted
from this paper?s abstract.
Keyphrases when m = 1
4
n,
1
3
n,
1
2
n
unsupervis method; various unsupervis rank
method; exemplar term; state-of-the-art
graph-bas rank method; keyphras; keyphras
extract
Keyphrases when m = 2
3
n
unsupervis method; manual assign; brief sum-
mari; various unsupervis rank method; exem-
plar term; document; state-of-the-art graph-bas
rank method; experi; keyphras; import score;
keyphras extract
fectiveness and robustness, and outperforms base-
lines significantly.
Future work may include:
1. Investigate the feasibility of clustering di-
rectly on noun groups;
2. Investigate the feasibility of combining
cooccurrence-based and Wikipedia-based re-
latedness for clustering;
3. Investigate the performance of the method on
other types of documents, such as long arti-
cles, product reviews and news;
4. The solution of using frequent word list
for filtering out too common single-word
keyphrases is undoubtedly simple, and we
plan to make a better combination of
the clustering-based method with traditional
frequency-based methods for keyphrase ex-
traction.
Acknowledgments
This work is supported by the National 863 Project
under Grant No. 2007AA01Z148 and the Na-
tional Science Foundation of China under Grant
No. 60621062. The authors would like to thank
Anette Hulth for kindly sharing her datasets.
References
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase extrac-
tion for web pages. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 277?278.
Wen Y. Chen, Yangqiu Song, Hongjie Bai, Chih J. Lin,
and Edward Chang. 2008. Psc: Paralel spectral
clustering. Submitted.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Transactions on
Knowledge and Data Engineering, 19(3):370?383.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th international conference on Intelligent user in-
terfaces, pages 199?206.
S. Elbeltagy and A. Rafea. 2009. Kp-miner: A
keyphrase extraction system for english and arabic
documents. Information Systems, 34(1):132?144.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668?673.
Brendan J J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Ex-
tracting key terms from noisy and multi-theme docu-
ments. In Proceedings of the 18th international con-
ference on World wide web, pages 661?670. ACM
New York, NY, USA.
Jiawei Han and Micheline Kamber. 2005. Data Min-
ing: Concepts and Techniques, second edition. Mor-
gan Kaufmann.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275?284.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216?223.
A. Hulth. 2004. Reducing false positives by expert
combination in automatic keyword indexing. Re-
cent Advances in Natural Language Processing III:
Selected Papers from RANLP 2003, page 367.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence.
265
Marina Litvak and Mark Last. 2008. Graph-based
keyword extraction for single-document summariza-
tion. In Proceedings of the workshop Multi-source
Multilingual Information Extraction and Summa-
rization, pages 17?24.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Peter D. Turney. 1999. Learning to Extract Keyphrases
from Text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
U. von Luxburg. 2006. A tutorial on spectral clus-
tering. Technical report, Max Planck Institute for
Biological Cybernetics.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labrank: Towards a collaborative approach to single-
document keyphrase extraction. In Proceedings of
COLING, pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
855?860.
266
Coling 2010: Poster Volume, pages 710?718,
Beijing, August 2010
Fast-Champollion: A Fast and Robust
Sentence Alignment Algorithm
Peng Li and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
pengli09@gmail.com, sms@tsinghua.edu.cn
Ping Xue
The Boeing Company
ping.xue@boeing.com
Abstract
Sentence-level aligned parallel texts are
important resources for a number of nat-
ural language processing (NLP) tasks and
applications such as statistical machine
translation and cross-language informa-
tion retrieval. With the rapid growth
of online parallel texts, efficient and ro-
bust sentence alignment algorithms be-
come increasingly important. In this
paper, we propose a fast and robust
sentence alignment algorithm, i.e., Fast-
Champollion, which employs a combi-
nation of both length-based and lexicon-
based algorithm. By optimizing the pro-
cess of splitting the input bilingual texts
into small fragments for alignment, Fast-
Champollion, as our extensive experi-
ments show, is 4.0 to 5.1 times as fast
as the current baseline methods such as
Champollion (Ma, 2006) on short texts
and achieves about 39.4 times as fast on
long texts, and Fast-Champollion is as ro-
bust as Champollion.
1 Introduction
Sentence level aligned parallel corpora are very
important resources for NLP tasks including ma-
chine translation, cross-language information re-
trieval and so on. These tasks typically require
support by large aligned corpora. In general, the
more aligned text we have, the better result we
achieve. Although there is a huge amount of bilin-
gual text on the Internet, most of them are either
only aligned at article level or even not aligned
at all. Sentence alignment is a process mapping
sentences in the source text to their correspond-
ing units in the translated text. Manual sentence
alignment operation is both expensive and time-
consuming, and thus automated sentence align-
ment techniques are necessary. A sentence align-
ment algorithm for practical use should be (1)
fast enough to process large corpora, (2) robust
enough to tackle noise commonly present in the
real data, and (3) effective enough to make as few
mistakes as possible.
Various sentence alignment algorithms have
been proposed, which generally fall into three
types: length-based, lexicon-based, and the hy-
brid of the above two types. Length-based algo-
rithms align sentences according to their length
(measured by character or word). The first length-
based algorithm was proposed in (Brown et al,
1991). This algorithm is fast and has a good per-
formance if there is minimal noise (e.g., sentence
or paragraph omission) in the input bilingual texts.
As this algorithm does not use any lexical infor-
mation, it is not robust. Lexicon-based algorithms
are usually more robust than the length-based al-
gorithm, because they use the lexical information
from source and translation lexicons instead of
solely sentence length to determine the transla-
tion relationship between sentences in the source
text and the target text. However, lexicon-based
algorithms are slower than length-based sentence
alignment algorithms, because they require much
more expensive computation. Typical lexicon-
based algorithms include (Ma, 2006; Chen, 1993;
Utsuro et al, 1994; Melamed, 1996). Sentence
length and lexical information are also combined
to achieve more efficient algorithms in two ways.
One way is to use both sentence length and lex-
710
ical information together to determine whether
two sentences should be directly aligned or not
(Simard et al, 1993; Wu, 1994). The other way is
to produce a rough alignment based on sentence
length (and possibly some lexical information at
the same time), and then build more precise align-
ment by using more effective lexicon-based algo-
rithms (Moore, 2002; Varga et al, 2005). But both
of the two ways suffer from high computational
cost and are not fast enough for processing large
corpora.
Lexical information is necessary for improving
robustness of a sentence alignment algorithm, but
use of lexical information will introduce higher
computational cost and cause a lower speed. A
common fact is that the shorter the text is, the
less combination possibilities it would introduce
and the less computational cost it would need. So
if we can first split the input bilingual texts into
small aligned fragments reliably with a reasonable
amount of computational cost, and then further
align these fragments one by one, we can speed
up these algorithms remarkably. This is the main
idea of our algorithm Fast-Champollion.
The rest of this paper is organized as fol-
lows: Section 2 presents formal definitions of sen-
tence alignment problem, and briefly reviews the
length-based sentence alignment algorithm and
Champollion algorithm; Section 3 proposes the
Fast-Champollion algorithm. Section 4 shows the
experiment results; and Section 5 is the conclu-
sion.
2 Definitions and Related Work
2.1 Definitions and Key Points
A segment is one or more consecutive sen-
tence(s). A fragment consists of one segment of
the source text (denoted by S) and one segment of
the target text (denoted by T ), and a fragment can
be further divided into one or more beads. A bead
represents a group of one or more sentences in
the source text and the corresponding sentence(s)
in the target text, denoted by Ai = (SAi;TAi) =
(Sai?1+1, Sai?1+2, ? ? ? , Sai ;Tbi?1+1, Tbi?1+2, ? ? ? ,
Tbi), where Si and Tj are the ith and jth sentence
of S and T respectively.
In practice, we rarely encounter crossing align-
ment, e.g., sentences Si and Sj of the source lan-
guage are aligned to the sentences Tj and Ti of
the target language respectively. But much more
effort has to be taken for an algorithm to process
crossing alignment well. So we do not consider
crossing alignment here.
In addition, only a few type of beads are fre-
quently observed in the real world. As it can save
significantly in terms of computational cost and it
would not do significant harm to algorithm with-
out considering rare bead types, a common prac-
tice for designing sentence alignment algorithms
is to only consider the frequently observed types
of beads. Following this practice, we only con-
sider beads of 1-to-0, 0-to-1, 1-to-1, 1-to-2, 2-to-
1, 1-to-3, 3-to-1, 1-to-4, 4-to-1 and 2-to-2 types in
our algorithm, where n-to-m means the bead con-
sists of n sentence(s) of the source language and
m sentence(s) of the target language.
2.2 Length-based Sentence Alignment
Algorithm
Length-based sentence alignment algorithm was
first proposed in (Brown et al, 1991). This algo-
rithm captures the idea that long or short sentences
tend to be translated into long or short sentences.
A probability is produced for each bead based on
the sentence length, and a dynamic programming
algorithm is used to search for the alignment with
the highest probability, which is treated as the best
alignment.
This algorithm is fast and can produce good
alignment when the input bilingual texts do not
contain too much noise, but it is not robust, be-
cause it only uses the sentence length information.
When there is too much noise in the input bilin-
gual texts, sentence length information will be no
longer reliable.
2.3 Champollion Aligner
Champollion aligner was proposed in (Ma, 2006).
It borrows the idea of tf-idf value, which is widely
used in information retrieval, to weight term1 pair
similarity. Greater weight is assigned to the less
frequent translation term pairs, because these term
1Here terms are not limited to linguistic words, but also
can be tokens like ?QX6800?
711
pairs have much stronger evidence for two seg-
ments to be aligned. For any two segments, a sim-
ilarity is assigned based on the term pair weight,
sentence number and sentence length. And the dy-
namic programming algorithm is used to search
for the alignment with the greatest total similarity.
This alignment is treated as the best alignment.
Champollion aligner can produce good align-
ment even on noisy input as reported in (Ma,
2006). Its simplicity and robustness make it a
good candidate for practical use. But this aligner
is slow. Because its time complexity is O(n2) and
it has to look up the dictionary multiple times in
each step of the dynamic programming algorithm,
which needs higher computational cost.
3 Fast-Champollion Algorithm
In this section we propose a new sentence align-
ment algorithm: Fast-Champollion. Its basis
is splitting the input bilingual texts into small
aligned fragments and then further aligning them
one by one to reduce running time while maintain-
ing Champollion-equivalent (or better) alignment
quality; it takes the advantages of both length-
based and lexicon-based algorithms to the maxi-
mum extent. The outline of the algorithm is that
first the length-based splitting module is used to
split the input bilingual texts into aligned frag-
ments, and then the components of each of these
fragments will be identified and aligned by a
Champollion-based algorithm. The details are de-
scribed in the following sections.
3.1 Length-based Splitting Module
Although length-based sentence alignment algo-
rithm is not robust enough, it can produce rough
alignment very fast with a certain number of re-
liably translated beads. Length-based splitting
module is designed to select these reliably trans-
lated beads to be used for delimiting and splitting
the input bilingual texts into fragments. These
beads will be referred to as anchor beads in the
remaining sections.
There are four steps in this module as described
below in detail.
Step 1: decide whether to skip step 2-4 or not
When there is too much noise in the input bilin-
gual texts, the percentage of reliably translated
beads in the alignment produced by the length-
based algorithm will be very low. In this case, we
will skip step 2 through 4.
An evidence for such a situation is that the
difference between the sentence numbers of the
source and target language is too big. Suppose
NS and NT are the number of sentences of the
source and target language respectively. We spec-
ify r = |NS ? NT |/min{NS , NT } as a measure
of the difference, where min means minimum. If
r is bigger than a threshold, we say the difference
is too big. In our experiments, the threshold is set
as 0.4 empirically.
Step 2: align the input texts using
length-based algorithm
In this step, length-based sentence alignment
algorithm is used to align the input bilingual texts.
Brown, et al (1991) models the process of sen-
tence alignment as two steps. First, a bead is gen-
erated according to a fixed probability distribution
over bead types, and then sentence length in the
bead is generated according to this model: for the
0-to-1 and 1-to-0 type of beads, it is assumed that
the sentence lengths are distributed according to
a probability distribution estimated from the data.
For other type of beads, the lengths of sentences of
the source language are generated independently
from the probability distribution for the 0-to-1 and
1-to-0 type of beads, and the total length of sen-
tences of the target language is generated accord-
ing to a probability distribution conditioned on the
total length of sentences of the source language.
For a bead Ai = (SAi;TAi), lSAi and lTAi are
the total lengths of sentences in SAi and TAi re-
spectively, which are measured by word2. Brown,
et al (1991) assumed this conditioned probability
distribution is
Prob(lTAi |lSAi) = ? exp
(
?(?i ? ?)
2
2?2
)
,
where ?i = log(lTAi/lSAi) and ? is a normal-
ization factor. Moore (2002) assumed the condi-
2For Chinese, word segmentation should be done first to
identify words.
712
tioned probability distribution is
Prob(lTAi |lSAi) =
exp (?lSAir) (lSAir)lTAi
lTAi !
,
where r is the ratio of the mean length of sen-
tences of the target language to the mean length
of sentences of the source language. We tested the
two models on our development corpus and the re-
sult shows that the first model performs better, so
we choose the first one.
Step 3: determine the anchor beads
In this step, the reliably translated beads in
the alignment produced by the length-based algo-
rithm in Step 2 will be selected as anchor beads.
The length-based algorithm can generate a
probability for each bead it produces. So a triv-
ial way is to choose the beads with a probability
above certain threshold as anchor beads. But as
pointed out before, when there is too much noise,
the alignment produced by the length-based algo-
rithm is no longer reliable, and so is it with the
probability. A fact is that if we select a non-
translated bead as an anchor bead, we will split
the input bilingual texts into wrong fragments and
may cause many errors. So we have to make de-
cision conservatively in this step and we decide to
use lexical information instead of the probability
to determine the anchor beads.
For a bead Ai = (SAi;TAi), the proportion of
translation term-pairs is a good measure for de-
termine whether this bead is reliably translated
or not. In addition, use of local information will
also be greatly helpful. To explain the use of ?lo-
cal information?, let?s define the fingerprint of a
sentence first. Suppose we have a sequence of
sentences S1, S2, ? ? ? , Sm, and W (Si)is the set of
distinct words in Si, then the fingerprint of Si is
f(Si) = W (Si)?W (Si?1)?W (Si+1),
and specially
f(S1) = W (S1)?W (S2),
f(Sm) = W (Sm)?W (Sm?1).
The fingerprints of SAi and TAi, denoted by
f(SAi) and f(TAi), are the unions of all the fin-
gerprints of sentences in SAi and TAi respectively.
As you can see, the fingerprint of a sentence is the
set of words in the sentence that do not appear in
the adjacent sentence(s), and thus can distinguish
this sentence from its neighbors. So fingerprint
is also a good measure. By combining these two
measures together, we can select out more reliably
translated beads.
For a word w, we use dD(w) to denote the set
of all its translations in a bilingual dictionary D,
and use tD(w) to denote the union of {w} and
dD(w), i.e., tD(w) = {w} ? dD(w). Given two
sets of words A and B. We say a word w of A is
translated by B if either one of its translations in
the dictionary D or the word itself appears in B,
i.e., tD(w)?B 6= ?. The set of all the words of A
that are translated by B is:
hD(A,B) = {w ? A and tD(w) ?B 6= ?}.
Then the proportion of terms in A that are trans-
lated by B is
rD(A,B) =
|hD(A,B)|
|A| .
We specify the proportion of translation term
pairs in a bead, denoted as arD(Ai), to be
min{rD(W (SAi),W (TAi)), rD(W (TAi),W (SAi))},
where W (SAi) and W (TAi) are the sets of dis-
tinct words in SAi and TAi respectively. Also we
specify the proportion of translation term-pairs
in the fingerprint, denoted as frD(Ai), to be
min{rD(f(SAi), f(TAi)), rD(f(TAi), f(SAi))}.
Given thresholds THar and THfr, a bead is
selected as an anchor bead when arD(Ai) and
frD(Ai) are not smaller than THar and THfr
respectively. We will show that Fast-Champollion
algorithm is not sensitive to THar and THfr to
some extent in Section 4.2.
Step 4: split the input bilingual texts
The anchor beads determined in Step 3 are used
to split the input texts into fragments. The ending
location of each anchor bead is regarded as a split-
ting point, resulting in two fragments.
3.2 Aligning Fragments with Champollion
Aligner
The similarity function used by Champollion
aligner is defined as follows. Given two (source
713
and target language) groups of sentences in a
fragment, denoted by GS=S1, S2,? ? ? ,Sm and
GT=T1, T2,? ? ? ,Tn, suppose there are k pairs
of translated terms in GS and GT denoted
by (ws1, wt1),(ws2, wt2),? ? ? ,(wsk, wtk), where
wsi is in GS and wti is in GT . For each pair of
the translated terms (wsi, wti), define idtf(wsi)
to be
Total # of terms in the whole document
# occurrences of wsi in GS
,
and define
stf(wsi, wti) = min{stf(wsi), stf(wti)},
where stf(wsi) and stf(wti) are the frequency
of wsi and wti in GS and GT respectively. The
similarity between GS and GT is defined as
k?
i=1
log (idtf(wsi)? stf(wsi, wti))
?alignment penalty
?length penalty,
where alignment penalty is 1 for 1-to-1 align-
ment type of beads and a number between 0 and 1
for other type of beads, length penalty is a func-
tion of the total sentence lengths of GS and GT .
The reason for choosing Champollion aligner
instead of other algorithms will be given in Sec-
tion 4.2. And another question is how idtf values
should be calculated. idtf is used to estimate how
widely a term is used. An intuition is that idtf
will work better if the texts are longer, because
if the texts are short, most words will have a low
frequency and will seem to only appear locally. In
Fast-Champollion, we calculate idtf according to
the whole document instead of each fragment. In
this way, a better performance is achieved.
3.3 Parameter Estimation
A development corpus is used to estimate the pa-
rameters needed by Fast-Champollion.
For the length-based algorithm, there are five
parameters that need to be estimated. The first one
is the probability distribution over bead types. The
ratio of different types of beads in the develop-
ment corpus is used as the basis for the estimation.
The second and third parameters are the proba-
bility distributions over the sentence length of the
source language and the target language. These
distributions are estimated as the distributions ob-
served from the input bilingual texts. That is to
say, these two distributions will not be the same
for different bilingual input texts. The forth and
fifth are ? and ?. They are estimated as the mean
and variance of ?i over the development corpus.
For Champollion aligner, alignment penalty
and length penalty need to be determined. Be-
cause the Perl version of Champollion aligner3
is well developed, we borrow the two definitions
from it directly.
4 Experiments
4.1 Datasets and Evaluation Metrics
We have two English-Chinese parallel corpora,
one for the development purpose and one for the
testing purpose. Both of the two corpora are col-
lected from the Internet and are manually aligned.
The development corpus has 2,004 beads.
Given the space constraint, detailed information
about the development corpus is omitted here.
The testing corpus contains 26 English-Chinese
bilingual articles collected from the Internet, in-
cluding news reports, novels, science articles,
television documentary subtitles and the record of
government meetings. There are 9,130 English
sentences and 9,052 Chinese sentences in these ar-
ticles4. The number of different type of beads in
the golden standard answer is shown in Table 1.
Type Number Percentage(%)
1:1 7275 83.19
1:2 2:1 846 9.67
1:3 3:1 77 0.88
1:4 4:1 16 0.18
2:2 32 0.37
1:0 0:1 482 5.51
others 17 0.19
total 8745 100.00
Table 1: Types of beads in the golden standard
Both the Fast-Champollion algorithm and the
Champollion aligner need a bilingual dictionary
and we supply the same bidirectional dictionary to
3http://champollion.sourceforge.net
4The definition of ?sentence? is slightly different from the
common sense here. We also treat semicolon and colon as the
end of a sentence.
714
them in the following evaluations. This dictionary
contains 45,439 pair of English-Chinese transla-
tion terms.
We use four commonly used measures for eval-
uating the performance of a sentence alignment
algorithm, which are the running time,
Precision = |GB ? PB||PB| ,
Recall = |GB ? PB||GB| ,
and
F1-measure = 2? Precision?RecallPrecision+Recall ,
where GB is the set of beads in the golden stan-
dard, and PB is the set of beads produced by the
algorithm.
All the following experiments are taken on a PC
with an Intel QX6800 CPU and 8GB memory.
4.2 Algorithm Design Issues
Why Choose Champollion?
We compared Champollion aligner with two
other sentence alignment algorithms which also
make use of lexical information. And the result
is shown in Table 2. ?Moore-1-to-1? and ?Moore-
all? are corresponding to the algorithm proposed
in (Moore, 2002). The difference between them is
how Recall is calculated. Moore?s algorithm can
only output 1-to-1 type of beads. For ?Moore-1-
to-1?, we only consider beads of 1-to-1 type in
the golden standard when calculating Recall, but
all types of beads are considered for ?Moore-all?.
The result suggests that ignoring the beads that are
not of 1-to-1 type does have much negative effect
on the overall performance of Moore?s algorithm.
Our goal is to design a general purpose sentence
alignment algorithm that can process frequently
observed types of beads. So Moore?s algorithm is
not a good choice. Hunalign refers to the hunalign
algorithm proposed in (Varga et al, 2005). The re-
sources provided to Champollion aligner and hu-
nalign algorithm are the same in the test, but hu-
nalign algorithm?s performance is much lower. So
hunalign algorithm is not a good choice either.
Champollion algorithm is simple and has a high
overall performance. So it is a better choice for
us.
Aligner Precision Recall F1-measure
Champollion 0.9456 0.9546 0.9501
Moore-1-to-1 0.9529 0.9436 0.9482
Moore-all 0.9529 0.7680 0.8505
Hunalign 0.8813 0.9037 0.8923
Table 2: The performance of different aligners on
the development corpus
The Effect of THar and THfr
THar and THfr are two thresholds for select-
ing anchor beads in Step 3 of length-based split-
ting module. In order to investigate the effect of
these two thresholds on the performance of Fast-
Champollion, we run Fast-Champollion on the de-
velopment corpus with different THar and THfr.
Both THar and THfr vary from 0 to 1 with step
0.05. And the running time and F1-measure are
shown in Figure 1 and Figure 2 respectively.
0 0.5
1
00.5
10
50100
150200
TH arTHfr
Runnin
g time(s)
Figure 1: The running time corresponding to dif-
ferent THar and THfr
0 0.5
1
00.5
10.8
0.850.9
0.951
TH arTHfr
F1?me
asure
Figure 2: The F1-measure corresponding to dif-
ferent THar and THfr
715
From Figure 1 and Figure 2, we see that for a
large range of the possible values of THar and
THfr, the running time of Fast-Champollion in-
creases slowly while F1-measure are nearly the
same. In other words, Fast-Champollion are not
sensitive to THar and THfr to some extent. So
making choice for the exact values of THar and
THfr becomes simple. And we use 0.5 for both
of them in the following experiments.
4.3 Performance of Fast-Champollion
We use three baselines in the following evalua-
tions. One is an implementation of the length-
based algorithm in Java, one is a re-implemented
Champollion aligner in Java according to the Perl
version, and the last one is Fast-Champollion-
Recal. Fast-Champollion-Recal is the same as
Fast-Champollion except that it calculates idtf
values according to the fragments themselves in-
dependently instead of the whole document, and
the Java versions of the length-based algorithm
and Champollion aligner are used for evaluation.
Performance on Texts from the Internet
Table 3 shows the performance of Fast-
Champollion and the baselines on the testing cor-
pus. The result shows that Fast-Champollion
achieves slightly better performance than Fast-
Champollion-Recal. The running time of Cham-
pollion is about 2.6 times longer than Fast-
Champollion with lower Precision, Recall and
F1-measure. It should be pointed out that Fast-
Champollion achieves better Precision, Recall and
F1-measure than Champollion does because the
splitting process may split the regions hard to
align into different fragments and reduces the
chance for making mistakes. Because of the noise
in the corpus, the F1-measure of the length-based
algorithm is low. This result suggests that Fast-
Champollion is fast, robust and effective enough
for aligning texts from the Internet.
Robustness of Fast-Champollion
In order to make a more precise investigation
on the robustness of Fast-Champollion against
noise, we made the following evaluation. First
we manually removed all the 1-to-0 and 0-to-
1 type of beads from the testing corpus to pro-
duce a clean corpus. This corpus contains 8,263
beads. Then we added 8263?n% 1-to-0 or 0-to-
1 type of beads to this corpus at arbitrary posi-
tions to produce a series of noisy corpora, with
n having the values of 5,10,...,100. Finally we
ran Fast-Champollion algorithm and the baselines
on these corpora respectively and the results are
shown in Figure 3 and Figure 4, which indi-
cate that for Fast-Champollion, when n increases
1, Precision drops 0.0021, Recall drops 0.0038
and F1-measure drops 0.0030 on average, which
are very similar to those of Champollion, but
Fast-Champollion is 4.0 to 5.1 times as fast as
Champollion. This evaluation proves that Fast-
Champollion is robust against noise and is a more
reasonable choice for practical use.
0 20 40 60 80 100050
100150200
250300
Noisy Level
Running tim
e(s)
 
 
Time of Fast?ChampollionTime of Fast?Champllion?RecalTime of ChampollionTime of length?based aligner
Figure 3: Running Time of Fast-Champollion,
Fast-Champollion-Recal, Champollion and the
length-based algorithm
0 20 40 60 80 10000.2
0.40.6
0.81
 
 
X: 100Y: 0.5427
Noisy Level
F1 of FCF1 of FCRF1 of CF1 of LP of FCP of FCRP of CP of LR of FCR of FCRR of CR of L
Figure 4: Precision (P), Recall (R) and F1-
measure (F1) of Fast-Champollion (FC), Fast-
Champollion-Recall (FCR), Champollion (C) and
the length-base algorithm (L)
Performance on Long Texts
In order to test the scalability of Fast-
Champollion algorithm, we evaluated it on long
texts. We merged all the articles in the testing cor-
716
Aligner Precision Recall F1-measure Running time(s)
Fast-Champollion 0.9458 0.9408 0.9433 48.0
Fast-Champollion-Recal 0.9470 0.9373 0.9421 45.4
Champollion 0.9450 0.9385 0.9417 173.5
Length-based 0.8154 0.7878 0.8013 11.3
Table 3: Performance on texts from the Internet
Aligner Precision Recall F1-measure Running time(s)
Fast-Champollion 0.9457 0.9418 0.9437 51.5
Fast-Champollion-Recall 0.9456 0.9362 0.9409 50.7
Champollion 0.9464 0.9412 0.9438 2029.0
Length-based 0.8031 0.7729 0.7877 23.8
Table 4: Performance on long text
pus into a single long ?article?. Its length is com-
parable to that of the novel of Wuthering Heights.
Table 4 shows the evaluation results on this long
article. Fast-Champollion is about 39.4 times as
fast as Champollion with slightly lower Precision,
Recall and F1-measure, and is just about 1.2 times
slower than the length-based algorithm, which has
much lower Precision, Recall and F1-measure. So
Fast-Champollion is also applicable for long text,
and has a significantly higher speed.
4.4 Evaluation of the Length-based Splitting
Module
The reason for Fast-Champollion can achieve rel-
atively high speed is that the length-based split-
ting module can split the bilingual input texts into
many small fragments reliably. We investigate the
fragments produced by the length-based splitting
module when aligning the long article used in Sec-
tion 4.3. The length-based splitting module splits
the long article at 1,993 places, and 1,972 seg-
ments are correct. The numbers of Chinese and
English segments with no more than 30 Chinese
and English sentences are shown in Figure 5. As
there are only 27 and 29 segments with more than
30 sentences for Chinese and English respectively,
we omit them in the figure. We can conclude that
although the length-based splitting module is sim-
ple, it is efficient and reliable.
5 Conclusion and Future Work
In this paper we propose a new sentence align-
ment algorithm Fast-Champollion. It reduces the
running time by first splitting the bilingual input
texts into small aligned fragments and then further
aligning them one by one. The evaluations show
0 5 10 15 20 25 300
200
400
600
800
Number of Sentences Contained in a Segment
Numbe
r
 
 
English SegmentChinese Segment
Figure 5: Numbers of Chinese/English segments
with no more than 30 Chinese/English sentences
that Fast-Champollion is fast, robust and effective
enough for practical use, especially for aligning
large amount of bilingual texts or long bilingual
texts.
Fast-Champollion needs a dictionary for align-
ing sentences, and shares the same problem of
Champollion aligner as indicated in (Ma, 2006),
that is the precision and recall will drop as the
size of the dictionary decreases. So how to build
bilingual dictionaries automatically is an impor-
tant task for improving the performance of Fast-
Champollion in practice, and is a critical problem
for applying Fast-Champollion on language pairs
without a ready to use dictionary.
Acknowledgement
This research is supported by the Boeing-
Tsinghua Joint Research Project ?Robust Chi-
nese Word Segmentation and High Performance
English-Chinese Bilingual Text Alignment?.
717
References
Brown, Peter F., Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of the 29th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 169?
176, Berkeley, California, USA, June. Association
for Computational Linguistics.
Chen, Stanley F. 1993. Aligning sentences in bilin-
gual corpora using lexical information. In Proceed-
ings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 9?16, Colum-
bus, Ohio, USA, June. Association for Computa-
tional Linguistics.
Ma, Xiaoyi. 2006. Champollion: A robust paral-
lel text sentence aligner. In Proceedings of LREC-
2006: Fifth International Conference on Language
Resources and Evaluation, pages 489?492.
Melamed, I. Dan. 1996. A geometric approach to
mapping bitext correspondence. In Proceedings of
the First Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?12.
Moore, Robert C. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
the 5th Conference of the Association for Machine
Translation in the Americas on Machine Transla-
tion: From Research to Real Users, pages 135?144,
London, UK. Springer-Verlag.
Simard, Michel, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilingual
corpora. In Proceedings of the 1993 Conference of
the Centre for Advanced Studies on Collaborative
Research, pages 1071?1082. IBM Press.
Utsuro, Takehito, Hiroshi Ikeda, Masaya Yamane, Yuji
Matsumoto, and Makoto Nagao. 1994. Bilingual
text matching using bilingual dictionary and statis-
tics. In Proceedings of the 15th Conference on
Computational Linguistics, pages 1076?1082, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
Nagy V. 2005. Parallel corpora for medium den-
sity languages. In Proceedings of the RANLP 2005,
pages 590?596.
Wu, Dekai. 1994. Aligning a parallel english-chinese
corpus statistically with lexical criteria. In Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, pages 80?87, Las
Cruces, New Mexico, USA, June. Association for
Computational Linguistics.
718
Coling 2010: Poster Volume, pages 1131?1139,
Beijing, August 2010
Using Clustering to Improve Retrieval Evaluation without 
Relevance Judgments 
Zhiwei Shi 
Institute of Computing Technology 
Chinese Academy of Science 
shizhiwei@ict.ac.cn
Peng Li 
Institute of Computing Technology 
Chinese Academy of Science 
lipeng01@ict.ac.cn
Bin Wang 
Institute of Computing Technology 
Chinese Academy of Science 
wangbin@ict.ac.cn
Abstract
Retrieval evaluation without relevance 
judgments is a hard but also very mean-
ingful work. In this paper, we use clus-
tering technique to improve the per-
formance of judgment free retrieval 
evaluation. By using one system to rep-
resent all the systems that are similar to 
it, we can largely reduce the negative ef-
fect of similar retrieval results in Re-
trieval evaluation. Experimental results 
demonstrated that our method outper-
formed all the previous judgment free 
evaluation methods significantly. Its 
overall average performance outper-
formed the best previous result by 
20.5%. Besides, our work is a general 
framework that can be applied to any 
other judgment free evaluation method 
for performance improvement. 
1 Introduction 
Generally, to compare the effectiveness of in-
formation retrieval systems, we need to prepare 
a test collection composed of a set of documents, 
a set of query topics, and a set of relevance 
judgments indicating which documents are rele-
vant to which topics. Among these requirements, 
relevance judgment is the most human resource 
exhausting and time consuming part. It even 
becomes infeasible when the test collection is 
extremely large. To address this problem, the 
TREC conferences used a pooling technology 
(Voorhees and Harman, 1999), where the top n
(e.g., n=100) documents retrieved by each par-
ticipating system are collected into a pool and 
then only the documents in the pool are judged 
for system comparison. Zobel (1998) has shown 
that this pooling method leads to reliable results 
in term of determining the effectiveness of re-
trieval systems and their relative rankings. Yet, 
the relevance determination process is still very 
resource intensive especially when the test col-
lection reaches or exceeds terabyte, or much 
more queries are included. More seriously, 
when we change to a new document collection, 
we have to redo the entire evaluation process.  
There are two possible solutions to the prob-
lem above, evaluation with incomplete rele-
vance judgments and evaluation without rele-
vance judgments. The former is well studied.  
Many well designed ranking methods with in-
complete judgments were carried out. Two of 
them, Minimal Test Collection (MTC) method 
(Carterette et al, 2006) and Statistical evalua-
tion (statMAP) method (Aslam et al, 2006), 
even got practical application in the Million 
Query (1MQ) track in TREC 2007 (Allan et al,
2007), and achieved satisfactory evaluation per-
formance. The latter is comparatively less stud-
ied. Only a few papers concentrate on the issue 
of evaluating retrieval systems without rele-
vance judgments. In Section 2 of this paper, we 
will briefly review some representative methods. 
We will see what they are and how they work.  
1131
In this paper, we focus our effort on the re-
trieval evaluation without relevance judgments. 
Although ?blind? evaluation is really a hard 
problem and its evaluation performance is far 
less than that of methods with incomplete 
judgments, it is undeniable that non-judgment 
evaluation has its own advantages. In some 
cases, relevance judgments are non-attainable. 
For example, when researchers compare their 
novel retrieval algorithms to existing methods, 
or search for optimal parameters of their algo-
rithms, or conduct data fusion in a dynamic en-
vironment, relevance judgment usually seems 
impossible. Besides, to construct a good evalua-
tion method without relevance judgments, re-
searchers need to mine the retrieval results thor-
oughly, and try to find laws that indicate the 
correlation between the effectiveness of a sys-
tem and features of its retrieval result. These 
laws are not only useful for ?blind? evaluation 
methods but also valuable for evaluation meth-
ods with incomplete judgments. 
One of the useful laws for ?blind? evaluation 
methods is Authority Effect (Spoerri, 2005). Yet 
it always ruined by multiple similar results. 
In this work, we use clustering technique to 
solve this problem. By selecting one system to 
represent all the systems that are similar to it, 
we can largely reduce the negative effect of 
similar retrieval results. Details of this method 
will be presented Section 3. Experimental re-
sults, which are reported in Section 4, also veri-
fied that our idea is feasible and effective. Our 
method outperformed all the previous judgment 
free evaluation methods on every test bed.  The 
overall average performance outperformed the 
best previous result by 20.5%. Finally, we con-
clude our work in Section 5. 
2 Related Work 
In 2001, Soboroff et al (2001) firstly proposed 
the concept of evaluating retrieval systems in 
the absence of relevance judgments. They 
generated a set of pseudo-relevance judgments 
by randomly selecting and declaring some 
documents from the pool of top 100 documents 
as relevant. This set of pseudo-relevance 
judgments (instead of a set of human relevance 
judgments) was then used to determine the 
effectiveness of the retrieval systems. Four 
versions of this random pseudo-relevance 
method were designed and tested on data from 
the ad hoc track in TREC 3, 5, 6, 7 and 8. They 
were simple random pseudo-relevance method, 
the variant with duplicate documents, the 
variant with Shallow pools and the variant with 
Exact-fraction sampling. All their resulting 
system assessments and rankings were well 
correlated with actual TREC rankings, and the 
variant with duplicate documents in pools got 
the best performance, with an average Kendall?s 
tau value 0.50 over the data of TREC 3, 5, 6, 7 
and 8. 
Soboroff et al?s idea came from two results 
in retrieval evaluation. One is that incomplete 
judgments do not harm evaluation results 
greatly. Zobel?s (1998) research had showed 
that the results obtained using pooling technol-
ogy were quite reliable given a pool depth of 
100. He also found that even though the pool 
depth was limited to 10, the relative perform-
ance among systems changed little, although 
actual precision scores did change for some sys-
tems. The other is that partially incorrect rele-
vance judgments do not harm evaluation results 
greatly. Voorhees (1998) ascertained that de-
spite a low average overlap between assessment 
sets, and wide variation in overlap among par-
ticular topics, the relative rankings of systems 
remained largely unchanged across the different 
sets of relevance judgments. These two points 
are bases of Soboroff et al?s random pseudo-
relevance method, and give explanation to the 
result that their rankings were positively related 
to that of the actual TRECs. As a matter of fact, 
the two points are bases of all the retrieval 
evaluation methods without or with incomplete 
relevance judgments. 
Aslam and Savell (2003) devised a method to 
measure the relative retrieval effectiveness of 
systems through system similarity computation. 
In their work, the similarity between two re-
trieval systems was the ratio of the number of 
documents in their intersection and union. Each 
system was scored by the average similarity 
between it and all other systems. This measure-
ment produced results that were highly corre-
lated with the random pseudo-relevance method. 
Aslam and Savell hypothesized that this was 
caused by ?tyranny of the masses? effect, and 
these two related methods were assessing the 
systems based on ?popularity? instead of ?per-
formance?. The analysis by Spoerri (2005) sug-
1132
gested that the ?popularity? effect was caused by 
considering all the runs submitted by a retrieval 
system, instead of only selecting one run per 
system. Our later experimental results will show 
that this point of view is partially correct. The 
?popularity? effect could not be avoided com-
pletely by only selecting one run per system. 
This is indeed a hard problem for all the evalua-
tion methods without relevance judgments. 
Wu and Crestani (2003) developed multiple 
?reference count? based methods to rank re-
trieval systems. They made the distinction be-
tween an ?original? document and its duplicates 
in all other lists, called the ?reference? docu-
ments, when computing a document?s score. A 
system?s score is the (weighted) sum of the 
scores of its ?original? documents. Several ver-
sions of reference count method were carried 
out and tested. The basic method (Basic) scored 
each ?original? document by the number of its 
?reference? documents. The first variant (V1) 
assigned different weights to ?reference? docu-
ments based on their ranking positions. The 
second variant (V2) assigned different weights 
to the ?original? document based on its ranking 
position. The third variant (V3) assigned differ-
ent weights to both the ?original? documents and 
the ?reference? documents based on their rank-
ing positions. The fourth variant (V4) was simi-
lar to V3, except that it normalized the weights 
to ?reference? documents. Wu and Crestani?s 
method output similar evaluation performance 
to that of the random pseudo-relevance method. 
Their work also showed that the similarity be-
tween the multiple runs submitted by the same 
retrieval system affected the ranking process. If 
only one run was selected for any of the partici-
pant system for any query, for 3-9 systems, V3 
outperformed random pseudo-relevance method 
by 45.6%; for 10-15 systems, random pseudo-
relevance method outperformed V3 by 6.5%. 
Nuray and Can (2006) introduced a method 
to rank retrieval systems automatically using 
data fusion. Their method consists of two parts. 
One is selecting systems for data fusion, and the 
other is selecting documents as pseudo relevant 
documents as the fusion result. In the former 
part, they hypothesized that systems returning 
documents different from the majority could 
provide better discrimination among the docu-
ments and systems. In return, this could lead to 
a more accurate pseudo relevant documents and 
more accurate rankings. To find proper systems, 
they introduced the ?bias? concept for system 
selection. In their work, bias was 1 minus the 
similarity between a system and the majority, 
where the similarity is a normalized dot product 
of two vectors. In the latter part, Nuray and Can 
tested three criterions, namely Rank position, 
Borda count and Condorcet. Experimental re-
sults on data from TREC 3, 5, 6 and 7 showed 
that bias plus Condorcet got the best evaluation 
results and it outperformed the reference count 
method and random pseudo relevance method 
greatly. 
More recently, Spoerri (2007) proposed a 
method using the structure of overlap between 
search results to rank retrieval systems. This 
method provides us a new view on how to rank 
retrieval systems without relevance judgments. 
He used local statistics of retrieval results as 
indicators of relative effectiveness of retrieval 
systems. Concretely, if there are N systems to be 
ranked, N groups are constructed randomly with 
the constraint that each group contains five sys-
tems and each system will appear in five groups; 
then the percentages of a system?s documents 
not found by other systems (Single%) as well as 
the difference between the percentages of docu-
ments found by a single system and all five sys-
tems (Single%-AllFive%) are calculated as in-
dicators of relative effectiveness respectively. 
Spoerri found that these two local statistics were 
highly and negatively correlated with the mean 
average precision and precision at 1000 scores 
of the systems. By utilizing the two statistics to 
rank systems from subsets of TREC 3, 6, 7 and 
8, Spoerri obtained appealing evaluation results. 
The overlap structure of the top 50 documents 
were sufficient to rank retrieval systems and 
produced the best results, which outperformed 
previous attempts to rank retrieval systems 
without relevance judgments significantly. 
So far, we have reviewed 5 representatives of 
non-judgment evaluation methods. All these 
methods faced the same serious problem: simi-
lar runs harmed the effectiveness of ranking 
process. Different methods handled this prob-
lem differently. Aslam and Savell (2003) called 
this the ?tyranny of the masses? and provided no 
solution. Wu and Crestani (2003) addressed this 
problem by selecting only one run for any of the 
participant system for any query. Nuray and 
Can (2006) selected systems that were less simi-
1133
lar to the majority for data fusion. Spoerri (2007) 
performed his method on a selected subset of all 
the systems. All these treatments led to evalua-
tion performance improvement. Yet we will say 
it could be improved more. In the next section, 
we will present a new solution to this problem. 
Its performance is examined in Section 4. 
3 Using Clustering to Improve Re-
trieval Evaluation without Relevance 
Judgments
3.1 Problem
As we reviewed in Section 2, previous research 
had shown that incomplete relevance judgments 
and partially incorrect relevance judgments do 
not harm retrieval evaluation greatly. This is 
why pooling technique can lead to reliable 
retrieval evaluation results. It is also the 
theoretical foundation of evaluation without 
relevance judgments.
Besides, non-judgments methods armed with 
more laws inside retrieval results. These laws 
indicate the correlation between retrieval effec-
tiveness of a system and features in its retrieval 
results. One of the most important laws used in 
non-judgments evaluation is Authority Effect 
(Spoerri, 2005): document, which is retrieval by 
more systems, is more likely being relevant. 
Unfortunately, similar retrieval results ruined 
this law. Aslam and Savell (2003) called this the 
?tyranny of the masses?. So, how to alleviate the 
negative effect of similar retrieval results is a 
big issue in non-judgments evaluation.  
3.2 Solution
Generally, our solution to the ?tyranny of the 
masses? is removing similar systems by cluster-
ing. The whole process is as follows: 
Firstly, all systems to be evaluated are clus-
tered into several subsets. 
Secondly, for each subset, one system is se-
lected as a representative. 
Thirdly, all the information used for system 
evaluation comes from these representatives. 
Finally, score every system according to the 
information collected in the previous step.
This is the general framework of our method-
ology. Notice that, in the third step, only se-
lected systems contribute to the information 
required for system evaluation. So we can elimi-
nate the negative effect caused by similar re-
trieval results. 
This solution can be applied to any method of 
retrieval evaluation without relevance judg-
ments. To illustrate how to apply it to a retrieval 
evaluation method, we will describe using clus-
tering to improve Average System Similarity, 
which is proposed by Aslam and Savell (2003), 
in detail as an example. 
3.3 Average System Similarity Based on 
Clustering
In Aslam and Savell?s (2003) method, each sys-
tem is evaluated based on a criterion named Av-
erage System Similarity. The average system 
similarity of a given system S0 is calculated ac-
cording to formula (1). 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SS
SSSysSim
n
S
(1)
where n is the number of systems to be evalu-
ated, and similarity between two systems S and 
S0, SysSim(S, S0), is calculated based on for-
mula (2). 
21
21
21 RetRet
RetRet
),(SysSim
?
?
 SS (2)
where Reti indicates the set of documents re-
turned by System i (i = 1, 2). 
When applying clustering technique to the 
system similarity method, we need to define an 
equivalence relation first. 
Definition 1 (System Equivalence): Suppose 
that all systems are clustered into m clusters 
namely C1, C2, ?, Cm. Two systems S1 and S2
are equivalent if and only if there exists k (1 ? k
? m) so that S1?Ck and S2?Ck.
kk CSCSmkk
iff
SS
??dd
 
21
21
,,1,
(3)
Given the definition of System Equivalence, 
we get the average system similarity based on 
clustering as follows: 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SR
SRSysSim
m
S
(4)
where m is the number of clusters and R is the 
representative system of a cluster. 
1134
Replacing formula (1) with formula (4), we 
get the retrieval evaluation method Average 
System Similarity Based on Clustering, shortly 
ASSBC.
There are two important issues for ASSBC 
that need to be addressed. Issue 1: How to select 
representative system from a cluster? Issue 2: 
How to decide the number of clusters we need? 
Before we address Issue 1, we introduce an-
other definition, Cluster Similarity. 
Definition 2 (Cluster Similarity): for any 
given two clusters C1 and C2, with their respec-
tive representative systems S1 and S2, the cluster 
similarity between C1 and C2 is the system simi-
larity between S1 and S2.
),(SysSim),(ClusterSim 2121 SSCC  (5)
Now we come to selecting representative sys-
tems for clusters. Here, we utilize a hierarchical 
bottom up clustering technique. The entire clus-
tering process is as follows. 
Initially, each system forms a cluster.
Loop Until the number of clusters is m 
Two most similar clusters merge, and 
one of their representatives with higher 
average system similarity survives as 
the representative of the new cluster. 
End Loop. 
In the initial step, since every cluster contains 
only one system, the representative system is 
unquestionable. Within each loop, two represen-
tative systems of the old clusters are candidates 
of the new cluster, and the one with higher score, 
which means higher retrieval performance, be-
comes the representative of the new cluster. 
For Issue 2, technically, how to decide the 
number of clusters is always a problem for clus-
tering. Yet, we do not have to rush in the deci-
sion. Let us examine the evaluation perform-
ance on different values of m first. 
4 Experiments 
In this section, we will illustrate the evaluation 
performance of Average System Similarity 
Based on Clustering vs. different values of m.
Before we come to the experimental results, we 
would like to make some details clear first. 
4.1 Some Clarification 
4.1.1 Dataset
We perform our experiments on the ad hoc tasks 
of TREC-3, -5, -6 and -7. Most existing works 
on retrieval evaluation without judgments are 
tested on these tasks. To make a direct compari-
son with these work mentioned in Section 2 
later, we also choose these tasks as our test bed. 
4.1.2 Performance Measurement 
One of the measures of retrieval effectiveness 
used by TREC is mean non-interpolated average 
precision (MAP). Since average precision is 
based on much more information than other ef-
fectiveness measures such as R-precision or 
P(10) and known to be a more powerful and 
more stable effectiveness measure (Buckley and 
Voorhees, 2000), we utilize MAP as the effec-
tive measurement of retrieval systems in our 
experiments. 
The correlation of the ranking with our pro-
posed methods, as well as other methods, to the 
TREC official rankings is measured using the 
Spearman?s rank correlation coefficient. One 
reason is that it suits better for evaluating corre-
lation between ratio sequences, e.g. MAP, than 
Kendall?s tau. The other reason is that we can 
directly compare our results with those of pre-
vious attempts reviewed in Section 2, since 
most of them provided Spearman?s rank correla-
tion coefficient results. 
4.1.3 Substitute for Number of Clusters 
TREC Runs 
3 40 
5 61 
6 74 
7 103 
Table 1. Number of TREC runs 
As we know, the number of systems (runs) var-
ies in different TREC dataset (see Table 1 for 
details). Instead of examining the evaluation 
performance variation when absolute number of 
clusters m changes, we illustrate the evaluation 
performance vs. the percentage of m. Actually, 
for the sake of convenience, we will plot the 
correlation of our method to the TREC official 
rankings vs. the percentage of systems removed 
from the representative group in the following 
subsection.
1135
4.2 Experimental results 
Figure 1-4 show the plots of the correlation 
of our method to the TREC official rankings vs. 
the percentage of systems removed from the 
representative group on TREC-3, -5, -6 and -7 
respectively. The percentage of systems re-
moved goes from 0 to 85%, where 0 means no 
system removed and represents the original Av-
erage System Similarity method, and 85% is an 
up bound in our experiments. The horizontal 
line indicates the original performance. The 
tagged number on the curve says when the per-
formance curve reaches its peak and the peak 
value.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1897?1907, Dublin, Ireland, August 23-29 2014.
A Neural Reordering Model for Phrase-based Translation
Peng Li
?
Yang Liu
?
Maosong Sun
?
Tatsuya Izuha
?
Dakun Zhang
?
?
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
?
Toshiba Corporation Corporate Research & Development Center
tatsuya.izuha@toshiba.co.jp
?
Toshiba (China) R&D Center
zhangdakun@toshiba.com.cn
Abstract
While lexicalized reordering models have been widely used in phrase-based translation systems,
they suffer from three drawbacks: context insensitivity, ambiguity, and sparsity. We propose a
neural reordering model that conditions reordering probabilities on the words of both the current
and previous phrase pairs. Including the words of previous phrase pairs significantly improves
context sensitivity and reduces reordering ambiguity. To alleviate the data sparsity problem, we
build one classifier for all phrase pairs, which are represented as continuous space vectors. Ex-
periments on the NIST Chinese-English datasets show that our neural reordering model achieves
significant improvements over state-of-the-art lexicalized reordering models.
1 Introduction
Reordering plays a crucial role in phrase-based translation (Koehn et al., 2003; Och and Ney, 2004).
While local reordering can be directly memorized in phrases, modeling reordering at a phrase level still
remains a major challenge: it can be cast as a travelling salesman problem and proves to be NP-complete
(Knight, 1999; Zaslavskiy et al., 2009).
The past decade has witnessed the rapid development of phrase reordering models (e.g., (Och et al.,
2004; Tillman, 2004; Zens et al., 2004; Xiong et al., 2006; Al-Onaizan and Papineni, 2006; Koehn et
al., 2007; Galley and Manning, 2008; Feng et al., 2010; Green et al., 2010; Bisazza and Federico, 2012;
Cherry, 2013), just to name a few). Among them, lexicalized reordering models (Tillman, 2004; Koehn
et al., 2007; Galley and Manning, 2008) have been widely used in practical phrase-based systems. Un-
like the distance-based reordering model (Koehn et al., 2003) that only penalizes phrase displacements
in terms of the degree of nonmonotonicity, lexicalized reordering models introduce reordering probabil-
ities conditioned on the words of each phrase pair. They often distinguish between three orientations
with respect to the previous phrase pair: monotone, swap, and discontinuous. As lexicalized reordering
models capture the phenomenon that some words are far more likely to be displaced than others, they
outperform unlexicalized reordering models substantially.
Despite their apparent success in statistical machine translation, lexicalized reordering models suffer
from the following three drawbacks:
1. Context insensitivity. Lexicalized reordering models determine the orientations only depending on
the words of current phrase pairs. In fact, a phrase pair usually has different orientations in different
contexts. It is important to include more contexts to improve the expressive power of reordering
models.
2. Ambiguity. Short phrase pairs, which are observed in the training data more frequently, usually have
multiple orientations. We observe that about 92.4% of one-word Chinese-English phrase pairs are
ambiguous. This makes it hard to decide which orientation should be properly used in decoding.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1897
Figure 1: Ambiguity in phrase reordering. The phrase pair ??yingyun?, ?business?? is labeled with
different orientations in different contexts: (a) monotone, (b) swap, and (c) discontinuous. Lexicalized
reordering models use fixed probability distributions (e.g., 17.50% for M, 1.59% for S, and 80.92% for
D) in decoding even though the surrounding contexts keep changing.
3. Sparsity. Lexicalized reordering models maintain a reordering probability distribution for each
phrase pair. As most long phrase pairs that are capable of memorizing local word selection and
reordering only occur once in the training data, maximum likelihood estimation can hardly train the
models accurately.
In this work, we propose a neural reordering model for phrase-based translation. The contribution is
twofold. Firstly, unlike conventional lexicalized reordering models, the neural reordering model condi-
tions reordering probabilities on the words of both the current and previous phrase pairs. Including the
words of previous phrase pairs significantly improves context sensitivity and reduces reordering ambi-
guity. Secondly, to alleviate the data sparsity problem, we build a neural classifier for all phrase pairs,
which are represented as continuous space vectors. Experiments on the NIST Chinese-English datasets
show that our neural reordering model achieves significant improvements over state-of-the-art lexicalized
models.
2 Lexicalized Reordering Models
The lexicalized reordering models (Tillman, 2004; Koehn et al., 2007; Galley and Manning, 2008) have
become the de facto standard in modern phrase-based systems. These models are called lexicalized
because they condition reordering probabilities on the words of each phrase pair. Depending on the
relationship between the current and previous phrase pairs, lexicalized reordering models often define
orientations to classify different reordering patterns.
More formally, we use f = {
?
f
1
, . . . ,
?
f
n
} to denote a sequence of source phrases, e = {e?
1
, . . . , e?
n
}
to denote the phrase sequence on the target side, and a = {a
1
, . . . , a
n
} to denote the alignment be-
tween source and target phrases. A source phrase
?
f
a
i
and a target phrase e?
i
form a phrase pair. Lex-
icalized reordering models aim to estimate the conditional probability of a sequence of orientations
o = {o
1
, . . . , o
n
}:
P (o|f , e,a) =
n
?
i=1
P (o
i
|f , e?
1
, . . . , e?
i
, a
1
, . . . , a
i
) (1)
where each o
i
takes values over a set of predefined orientations. For simplicity, current lexicalized
1898
model
source phrase length
1 2 3 4 5 6 7
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) 92.74 54.01 24.09 14.40 10.78 8.47 6.95
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
a
i?1
, a
i?1
, a
i
) 21.72 5.22 2.63 1.48 0.98 0.67 0.54
Table 1: Percentages of phrase pairs that have multiple orientations. Including previous phrase pairs in
modeling significantly reduces the reordering ambiguity for the M/S/D orientations. For example, while
92.74% of 1-word Chinese-English phrase pairs have multiple orientations observed in the training data,
the ratio dramatically drops to 21.72% if the orientations are conditioned on both the current and previous
phrase pairs.
reordering models use orientations conditioned only on a
i?1
and a
i
:
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
, a
i?1
, a
i
) (2)
The most widely used orientations are monotone (M), swap (S), and discontinuous (D):
1
o
i
=
?
?
?
M if a
i
? a
i?1
= 1
S if a
i
? a
i?1
= ?1
D if |a
i
? a
i?1
| =? 1
(3)
As lexicalized reordering models maintain a reordering probability distribution for each phrase pair,
it is hard to accurately learn reordering probabilities for long phrase pairs that are usually observed only
once in the training data. On the contrary, short phrase pairs that occur in the training data for many times
tend to be ambiguous. For example, as shown in Figure 1, a Chinese-English phrase pair ??yingyun?,
?business?? is observed to have different orientations in different contexts.
It is unreasonable to use fixed reordering probability distributions in decoding as the surrounding
contexts keep changing. Previous study shows that considering more contexts into reordering modeling
improves translation performance (Khalilov and Simaan, 2010). Therefore, we need a more powerful
mechanism to include more contexts, resolve the reordering ambiguity, and reduce the data sparsity.
3 A Neural Reordering Model
3.1 The Model
Intuitively, conditioning reordering probabilities on the words of both the current and previous phrase
pairs will significantly reduce both reordering ambiguity and context insensitivity. The new reordering
model is given by
P (o|f , e,a) ?
n
?
i=1
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) (4)
where ?
?
f
a
i?1
, e?
i?1
? is the previous phrase pair.
Including the previous phrase pairs improves the context sensitivity. For example, given a phrase pair
??yingyun?, ?business??, its orientation is more likely to be monotone if it is preceded by a noun phrase
pair such as ??xinyongka?, ?credit card??. On the contrary, the probability of the discontinuous orienta-
tion is higher if the previous phrase pairs contain verbs such as ??gaishan?, ?improve??. Therefore, the
new model is capable of capturing the phenomenon that the orientation of a phrase pair depends on its
surrounding contexts.
Another advantage of including previous phrase pairs is the reduction of reordering ambiguity. As
shown in Table 1, 92.74% of 1-word Chinese-English phrase pairs have multiple orientations (i.e., M, S,
1
There are many variants of lexicalized reordering models depending on the model type, orientation, directionality, lan-
guage, and collapsing. See http://www.statmt.org/moses/?n=FactoredTraining.BuildReorderingModel for more details.
1899
and D) observed in the training data. The ratio decreases with the increase of phrase length. In contrast,
the new model is much less ambiguous (e.g., the ratio of ambiguous one-word phrase pairs dramatically
drops to 21.72%) as it is conditioned on both the current and previous phrase pairs.
Unfortunately, including more contexts in modeling also increases the data sparsity. We observe that
about 90% of reordering examples (i.e., the current and previous phrase pairs) are observed only once in
the training data. As a result, it is more difficult to train lexicalized reordering models accurately using
maximum likelihood estimation.
To alleviate the data sparsity problem, we use the following two strategies:
1. Reordering as classification. Instead of maintaining a reordering probability distribution for each
phrase pair, we build a reordering classifier for all phrase pairs (Xiong et al., 2006; Li et al., 2013).
This significantly reduces data sparsity by considering all occurrences of extracted phrase pairs as
training examples. We find that 500, 000 reordering examples suffice to train a robust classifier
(Section 4.5).
2. Continuous space representation. Instead of using a symbolic representation of phrases, we use
a continuous space representation that treats a phrase as a dense real-valued vector (Socher et al.,
2011b; Li et al., 2013). Consider two phrases ?in London? and ?in Centara Grand?. It is usually
easy to predict the orientations of ?in London? because it might be observed in the training data for
many times. This is not the case for ?in Centara Grand? as it might occur only once. However, if
the two phrases happen to have very similar continuous space representations, ?in Centara Grand?
is likely to have a similar reordering probability distribution with ?in London?.
To generate vector space representation for phrases, we follow Socher et al. (2011a) to use recursive
autoencoders. Given two words w
1
and w
2
, suppose their vector space representations are c
1
and c
2
.
The vector space representation p of the two-word phrase {w
1
, w
2
} can be computed using a two-layer
neural network:
p = g
(1)
(W
(1)
[c
1
; c
2
] + b
(1)
) (5)
where [c
1
; c
2
] ? R
2n
is the concatenation of c
1
and c
2
, W
(1)
? R
n?2n
is a weight matrix, b
(1)
is a bias
vector, and g
(1)
is an element-wise activation function.
In order to measure how well p represents c
1
and c
2
, they can be reconstructed using another two-layer
neural network:
[c
?
1
; c
?
2
] = g
(2)
(W
(2)
p + b
(2)
) (6)
where c
?
1
? R
n
and c
?
2
? R
n
are reconstructed vectors of c
1
and c
2
, W
(2)
? R
2n?n
is a weight matrix,
b
(2)
? R
n
is a bias vector, and g
(2)
is an element-wise activation function. The reconstruction error can
be measured by comparing c
1
and c
2
with c
?
1
and c
?
2
. This process runs recursively in a bottom-up style
to obtain the vector space representation of a multi-word phrase (Socher et al., 2011a). Socher et al.
(2011a) find that minimizing the norms of hidden layers leads to the reduction of reconstruction error in
an undesirable way. Therefore, we normalize p such that ||p||
2
= 1.
Treating phrase reordering as a classification problem, we propose a neural reordering classifier that
takes the current and previous phrase pairs as input. The neural network consists of four recursive
autoencoders and a softmax layer. The input of the classifier are the previous phrase pair and the current
phrase pair. Four recursive autoencoders are used to transform the four phrases (i.e.,
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
)
into vectors. Then, these vectors are fed to the softmax layer to predict reordering orientations. Note that
the recursive autoencoders for the same language share with the same parameters. Our neural network is
similar to that of Li et al. (2013). The major difference is that Li et al. (2013) need to compute vector
space representation for variable-sized blocks ranging from words to sentences on the fly both in training
and decoding. In contrast, we only need to compute vectors for phrases with up to 7 words in the training
phase, which makes our approach simpler and more scalable to large data.
1900
Formally, given the previous phrase pair ?
?
f
a
i?1
, e?
i?1
?, the current phrase pair ?
?
f
i
, e?
i
? and the orienta-
tion o
i
, the reordering probability is computed as
P (o
i
|
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
, a
i?1
, a
i
) = g(W
o
c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) + b
o
), (7)
where W
o
is a weight matrix, b
o
is a bias vector, c(
?
f
a
i
, e?
i
,
?
f
a
i?1
, e?
i?1
) is the concatenation of the vectors
of the four phrases.
2
Following Och (2003), we use a linear model in our decoder with conventional features (e.g., trans-
lation probabilities and n-gram language model). The neural reordering model is incorporated into the
discriminative framework as an additional feature.
3.2 Training
Training the neural reordering model involves minimizing the following two kinds of errors:
? Reconstruction error: It measures how well the computed vector space representations represent
the input vectors. It is defined as the average reconstruction error of all the parent nodes in the trees
formed during computing the vector space representation for all the phrases in the training data.
? Classification error: It measures how well the resulting classifier predicts the reordering orienta-
tions. It is defined as the average cross-entropy errors of all the training examples.
In our experiments, the objective function is a linear interpolation of the reconstruction error and the
classification error.
Following Socher et al. (2011b), we use L-BFGS (Liu and Nocedal, 1989) to optimize the parameters.
At the beginning of each iteration, a binary tree for each phrase is constructed using a greedy algorithm
(Socher et al., 2011b).
3
With these trees fixed, the partial derivatives with respect to parameters are
computed via the backpropagation through structures algorithm (Goller and Kuchler, 1996).
When optimizing the parameters of the softmax layer, the training procedure keeps the parameters of
the recursive autoencoders and word embedding matrices fixed. The corresponding error function is the
classification error as described above. We also use L-BFGS to optimize the parameters and the standard
error backpropagation algorithm (Rumelhart et al., 1986) to compute the derivatives.
3.3 Decoding
As the vector space representation of a phrase is calculated based on all the words in the phrase, using
the neural reordering model complicates the conditions for risk-free hypothesis recombination (Koehn et
al., 2003). Therefore, many hypotheses are not likely to be recombined if the neural reordering model
is directly integrated in decoding, making the decoder to only explore in a much smaller search space.
4
Therefore, we use Moses to generate search graphs and then use hypergraph reranking (Huang and
Chiang, 2007; Huang, 2008) to find most probable derivations using the neural reordering model.
4 Experiments
4.1 Data Preparation
We evaluate our reordering model on Chinese-English translation. The training corpus consists of 1.23M
sentence pairs with 32.1M Chinese words and 35.4M English words. A 4-gram language model was
trained on the Xinhua portion of the English GIGAWORD corpus using KenLM (Heafield, 2011), which
contains 398.6M words. We used the NIST 2006 MT Chinese-English dataset as the development set,
and NIST 2002-2005, 2008 MT Chinese-English datasets as the test sets. Case-insensitive BLEU is used
2
In practice, as suggested by Socher et al. (2011b), we feed the four average vectors of the vectors present in each recursive
autoencoders to the softmax layer. Taking ?resident population? as an example, there are three vectors in the binary tree used
by the corresponding recursive autoencoder, denoted as x?
1
, x?
2
and x?
3
. The average vector is computed as x? =
1
3
?
3
i=1
x?
i
.
3
As phrases in phrase-based translation are not necessarily syntactic constituents, we do not use parse trees in this work.
4
Experimental results show that we can only achieve comparable performance with Moses by integrating neural reordering
model directly in decoding.
1901
Model Orientation MT06 MT02 MT03 MT04 MT05 MT08
distance N/A 29.56 31.40 31.27 31.34 29.98 23.87
word
M/S/D 30.19 32.03 31.86 32.09 30.55 24.20
left/right 30.17 31.98 31.52 31.98 30.19 24.30
phrase
M/S/D 30.24 32.35 31.85 32.00 30.78 24.33
left/right 29.57 32.64 31.53 31.90 30.70 24.28
hierarchical
M/S/D 30.46 32.52 31.89 32.09 30.39 24.11
left/right 30.03 32.13 31.59 31.91 30.21 24.41
neural
M/S/D 30.68 32.19 31.94 32.20 30.81 24.71
left/right 31.03** 33.03** 32.48** 32.52** 31.11* 25.20**
Table 2: Comparison of distance-based, lexicalized, and neural reordering models in terms of case-
insensitive BLEU-4 scores. ?distance? denotes the distance-based reordering model (Koehn et al., 2003),
?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes the phrase-based
lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-based reordering
model (Galley and Manning, 2008), and ?neural? denotes our model. The ?left? and ?right? orientations
only considers whether the current source phrase is on the left of the previous source phrase or not. We
use ?*? to highlight the result that is significantly better than the best baseline (highlighted in italic)
at p < 0.05 level and ?**? at p < 0.01 level. The neural model does not work well for the M/S/D
orientations due to the non-separability problem (Section 4.3).
as the evaluation metric. As a trade-off between expressive power and computational cost, we set the
dimension of the word embedding vectors to 25.
5
Both g
(1)
and g
(2)
are set to tanh(?). The other
hyperparameters are optimized via random search (Bergstra and Bengio, 2012).
4.2 Comparison of Distance-based, Lexicalized, and Neural Reordering Models
We compare three kinds of reordering models with increasing expressive power:
1. distance-based model: penalizing phrase displacements proportionally to the amount of nonmono-
tonicity (Koehn et al., 2003);
2. lexicalized models: conditioning the reordering probabilities on the current phrase pairs. The ori-
entations can be determined with respect to words (Tillman, 2004), phrases (Koehn et al., 2007), or
hierarchical phrases (Galley and Manning, 2008);
3. neural model: conditioning the reordering probabilities on both the current and previous phrase
pairs.
For lexicalized and neural models, we further distinguish between two kinds of orientation sets:
{monotone, swap, discontinuous} and {left, right}. The left/right orientations only consider whether
the current source phrase is on the left of the previous source phrase or not. Therefore, swap and
discontinuous-left are merged into left while monotone and discontinuous-right into right.
All these reordering models are tested using Moses (Koehn et al., 2007), except that the neural model
needs an additional hypergraph reranking procedure (Section 3.3). Implemented using Java, it takes the
reranker 0.748 second to rerank a hypergraph on average.
Table 2 shows the case-insensitive BLEU-scores of distance-based, lexicalized, and neural reordering
models on the NIST Chinese-English datasets. ?distance? denotes the distance-based reordering model
(Koehn et al., 2003), ?word? denotes the word-based lexicalized model (Tillman, 2004), ?phrase? denotes
the phrase-based lexicalized model (Koehn et al., 2007), ?hierarchical? denotes the hierarchical phrase-
based reordering model (Galley and Manning, 2008), and ?neural? denotes our model.
5
We find that the dimensions of vectors do not have a significant impact on translation performance. For efficiency, we set
the dimension to 25.
1902
Figure 2: The non-separability problem for the neural reordering model. Given an aligned Chinese-
English sentence pair, the unaligned Chinese word ?de? makes a big difference in determining M/S/D
orientations. In (a), ?de? is included in the previous source phrase and thus the orientation is monotone.
In (b), however, it is not included in the previous source phrase and the orientation is discontinuous. In
our neural reordering model, ?liu wan de? and ?liu wan? have very similar vector space representations
yet different orientations (i.e., M and D). In other words, training examples labeled with M, S, D are
prone to be mixed with each other in the vector space. Therefore, it is difficult to find a hyperplane to
separate M, S and D examples in the high-dimensional space.
We find that lexicalized reordering models obtain significant improvements over the distance-based
model, which indicates that conditioning reordering probabilities on the words of the current phrase
pairs does improve the expressive power. Our neural model using left/right orientations significantly
outperforms all variants of lexicalized models. We use ?*? to highlight the result that is significantly
better than the best baseline (highlighted in italic) at p < 0.05 level and ?**? at p < 0.01 level. This
suggests that conditioning reordering probabilities on the words of current and previous phrase pairs is
helpful for resolving reordering ambiguities and reducing context insensitivity.
4.3 The Non-Separability Problem
In Table 2, the neural model using the M/S/D orientations fails to outperform lexicalized models signifi-
cantly. One possible reason is that the neural model suffers from the non-separability problem due to the
M/S/D orientations.
As shown in Figure 2, given an aligned Chinese-English sentence pair, the unaligned Chinese function
word ?de? makes a big difference in determining M/S/D orientations. In (a), ?de? is included in the
previous source phrase and thus the orientation is monotone. In (b), however, ?de? is not included in the
previous source phrase and the orientation is discontinuous. In our neural reordering model, ?liu wan
de? and ?liu wan? have very similar vector space representations yet different orientations (i.e., M and
D). In other words, training examples labeled with M, S, D are prone to be mixed with each other in
the vector space. Therefore, it is difficult to find a hyperplane to separate M, S and D examples in the
high-dimensional space.
Fortunately, we find that using the left/right orientations can alleviate this problem. As the left/right
orientations only consider whether the current source phrase is on the left of the previous source phrase
or not, unaligned source words will not change orientations. For example, both Figure 2(a) and 2(b) are
identified as the right orientation.
As a result, using left/right orientations in the neural reordering model not only has a higher classifi-
cation accuracy (85%) over using the M/S/D orientations (69%), but also achieves higher BLEU scores
on all NIST datasets systematically.
4.4 The Effect of Distortion Limit
Figure 3 shows the performance of the lexicalized model and our neural model with various distortion
limits. The lexicalized model is the word-based model with M/S/D orientations. The neural model uses
left/right orientations. The neural model consistently outperforms the lexicalized model, especially for
large distortion limits. This finding suggests that the neural model is superior to lexicalized models in
predicting long-distance reordering.
1903
2 4 6 822
23
24
25
Distortion Limit
BLEU
 
 
neurallexicalized
Figure 3: BLEU with various distortion limits.
# examples Accuracy BLEU
100,000 83.55 30.92
200,000 84.40 31.03
300,000 84.55 31.01
400,000 84.95 30.93
500,000 85.25 31.27
3,000,000 85.55 31.03
Table 3: Effect of training corpus size.
Vectors MT06 MT02 MT03 MT04 MT05 MT08
ours 31.03 33.03 32.48 32.52 31.11 25.20
word2vec 30.44 32.28 32.00 32.07 30.24 24.54
Table 4: Comparison of neural reordering models trained based on word vectors produced by our model
(ours) and word2vec (Mikolov et al., 2013).
4.5 The Effect of Training Corpus Size
Table 3 shows the classification accuracy and translation performance with various number of randomly
sampled reordering examples for training the neural classifier. The classification accuracy and transla-
tion performance generally rise as the number of reordering example increases.
6
Surprisingly, both the
classification accuracy and translation performance of using 500,000 reordering examples are close to
using 3,000,000 reordering examples, suggesting that a relatively small amount of reordering examples
are enough for training a robust classifier.
4.6 Learned Vector Space Representations
We randomly sampled 200,000 English phrases and found 999 clusters according to the vector space
representations computed by recursive autoencoders using the k-means algorithm (MacQueen, 1967).
The distance between two phrases is calculated by the Euclidean distance between their vector space
representations.
Figure 4 shows 10 of the 999 clusters. An interesting finding is that phrase pairs that are close in the
vector space share with similar reordering patterns rather than semantic similarity. For example, ?by
june 1? and ?within the agencies? have similar distributions on the left/right orientations but are totally
unrelated in terms of meaning. As a result, the vector representations of words trained using unlabeled
data hardly helps in training the neural reordering model. Table 4 shows the results when we replace
the word vectors of our model with those trained using word2vec (Mikolov et al., 2013). The recursive
autoencoders and the classifier are retrained. The performance of the neural reordering model trained in
this way drops significantly, which confirms our analysis.
5 Related Work
Reordering as classification is a common way to alleviate the data sparsity problem. Xiong et al. (2006)
use a maximum entropy model to predict whether to merge two blocks in a straight or an inverted order
in their ITG decoder. Nguyen et al. (2009) build a similar model for hierarchical phrase reordering
models (Galley and Manning, 2008). Green et al. (2010) and Yahyaei and Monz (2010) predict finer-
grained distance bins instead. Another direction is to learn sparse reordering features and create more
flexible distributions (Cherry, 2013). Although these models are effective, feature engineering is a major
challenge. In contrast, our neural reordering model is capable of learning features automatically.
6
The reason why the BLEU scores oscillate slightly on the training set is that classification accuracy is not directly correlated
with BLEU scores. Optimizing the neural reordering model directly with respect to BLEU score may further improve the
performance. We leave this for future work.
1904
but is willing toeconomy is required to
range of services to said his visit is to
is making use of
june 18, 2001late 2011
as detention centergroup all togethertake care of oldby june 1
and complete by end 1998
or other economicand for other
within the agencies
Figure 4: Phrase clusters as calculated by the Euclidean distance in the vector space. English phrases
that have similar reordering probability distributions rather than similar semantic similarity fall into one
cluster.
Along another line, n-gram-based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al.,
2013) treat translation as Markov chains over minimal translation units (Mari`no et al., 2006; Durrani et
al., 2013) or operations (Durrani et al., 2011) directly. Although naturally leveraging both the source and
target side contexts, these approaches still face the data sparsity problem.
Our work is closely related to Li et al. (2013). The major difference is that Li et al. (2013) need to
compute vector space representation for variable-sized blocks ranging from words to sentences on the
fly both in training and decoding. In contrast, we only need to compute vectors for phrases with up to 7
words in the training phase, which makes our approach simpler and more scalable to large data.
6 Conclusion
We have shown that surrounding context is effective for resolving reordering ambiguities in phrase-based
models. As the data sparseness problem is the major challenge for using context in reordering models,
we propose to use a single classifier based on recursive autoencoders to predict reordering orientations.
Experimental results show that our neural reordering model outperforms the state-of-the-art lexicalized
reordering models significantly and consistently across all the NIST datasets under various settings.
There are a few future directions we plan to explore. First, as the machine translation system and neu-
ral classifier are trained separately, the neural network training only has an indirect effect on translation
quality. Jointly training the machine translation system and neural classifier is an interesting topic. Sec-
ond, it is interesting to develop more efficient models to leverage larger contexts to resolve reordering
ambiguities. Third, we plan to extend our work to other translation models such as syntax-based and
n-gram based models (Mari`no et al., 2006; Durrani et al., 2011; Durrani et al., 2013). Finally, as we cast
phrase reordering as two-category classification problem (i.e, left vs. right), it is interesting to intersect
structured SVM (Tsochantaridis et al., 2005) with neural networks to develop a large margin training
algorithm for our neural reordering model.
Acknowledgements
This research is supported by the 973 Program (No. 2014CB340501), the National Natural Science
Foundation of China (No. 61331013), the 863 Program (No. 2012AA011102), Toshiba Corporation
Corporate Research & Development Center, and the Singapore National Research Foundation under its
International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme.
1905
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages 529?536.
James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine
Learning Research, 13(1):281?305.
Arianna Bisazza and Marcello Federico. 2012. Modified distortion matrices for phrase-based statistical machine
translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 478?487.
Colin Cherry. 2013. Improved reordering for phrase-based translation using sparse features. In Proceedings of
the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, pages 22?31.
Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrat-
ed reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 1045?1054.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can Markov models
over minimal translation units help phrase-based SMT? In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pages 399?405.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics:
Posters, pages 285?293.
Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model.
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 848?856.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Proceedings of 1996 IEEE International Conference on Neural Networks (Vol-
ume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Manning. 2010. Improved models of distortion cost for statis-
tical machine translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics, pages 867?875.
Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the EMNLP 2011
Sixth Workshop on Statistical Machine Translation, pages 187?197.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151.
Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages
586?594.
Maxim Khalilov and Khalil Simaan. 2010. Source reordering using maxent classifiers and supertags. In Proceed-
ings of The 14th Annual Conference of the European Association for Machine Translation, pages 292?299.
Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics,
25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for ITG-based translation. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577.
1906
DongC. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math-
ematical Programming, 45(1-3):503?528.
James MacQueen. 1967. Some methods for classification and analysis of multivariate observations. In Proceed-
ings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281?297.
Jos?e B. Mari`no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527?549.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen, and Thai Phuong Nguyen. 2009. Improving a lexicalized
hierarchical reordering model using maximum entropy. In Proceedings of The twelfth Machine Translation
Summit (MT Summit XII).
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004: Main
Proceedings, pages 161?168.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics, pages 160?167.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1986. Learning internal representations by
error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1:
Foundations, pages 318?362.
Richard Socher, Eric H. Huang, Jeffrey Pennin, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of Advances in Neural
Information Processing Systems 24, pages 801?809.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Processing, pages 151?161.
Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings of the
Human Language Technology Conference of the North American Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004: Short Papers, pages 101?104.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453?1484.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical
machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and
44th Annual Meeting of the Association for Computational Linguistics, pages 521?528.
Sirvan Yahyaei and Christof Monz. 2010. Dynamic distortion in a discriminative reordering model for statistical
machine translation. In Proceedings of the 7th International Workshop on Spoken Language Translation, pages
353?360.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Cancedda. 2009. Phrase-based statistical machine translation
as a traveling salesman problem. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 333?341.
Richard Zens, Hermann Ney, Taro Watanabe, and Eiichiro Sumita. 2004. Reordering constraints for phrase-
based statistical machine translation. In Proceedings of the 20th International Conference on Computational
Linguistics, pages 205?211.
1907
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1137?1146,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Generating Aspect-oriented Multi-Document Summarization with
Event-aspect model
Peng Li1 and Yinglin Wang1 and Wei Gao2and Jing Jiang3
1 Department of Computer Science and Engineering, Shanghai Jiao Tong University
2 Department of Systems Engineering and Engineering Management, Chinese University of Hong Kong
3 School of Information Systems, Singapore Management University
{lipeng, ylwang@sjtu.edu.cn} {wgao@se.cuhk.edu.hk} {jingjiang@smu.edu.sg}
Abstract
In this paper, we propose a novel approach to
automatic generation of aspect-oriented sum-
maries from multiple documents. We first de-
velop an event-aspect LDA model to cluster
sentences into aspects. We then use extend-
ed LexRank algorithm to rank the sentences
in each cluster. We use Integer Linear Pro-
gramming for sentence selection. Key features
of our method include automatic grouping of
semantically related sentences and sentence
ranking based on extension of random walk
model. Also, we implement a new sentence
compression algorithm which use dependency
tree instead of parser tree. We compare our
method with four baseline methods. Quantita-
tive evaluation based on Rouge metric demon-
strates the effectiveness and advantages of our
method.
1 Introduction
In recent years, there has been much interest in
the task of multi-document summarization. In this
paper, we study the task of automatically generat-
ing aspect-oriented summaries from multiple docu-
ments. The goal of aspect-oriented summarization
is to present the most important content to the us-
er in a condensed form and a well-organized struc-
ture to satisfy the user?s needs. A summary should
follow a readable structure and cover all the aspect-
s users are interested in. For example, a summary
about natural disasters should include aspects about
what happened, when/where it happened, reasons,
damages, rescue efforts, etc. and these aspects may
be scattered in multiple articles written by different
news agencies. Our goal is to automatically collect
aspects and construct summaries from multiple doc-
uments.
Aspect-oriented summarization can be used in
many scenarios. First of all, it can be used to gener-
ateWikipedia-like summary articles, especially used
to generate introduction sections that summarizes
the subject of articles before the table of contents
and other elaborate sections. Second, opinionat-
ed text often contains multiple viewpoints about an
issue generated by different people. Summarizing
these multiple opinions can help people easily di-
gest them. Furthermore, combined with search en-
gines and question&answering systems, we can bet-
ter organize the summary content based on aspects
to improve user experience.
Despite its usefulness, the problem of modeling
domain specific aspects for multi-document summa-
rization has not been well studied. The most relevant
work is by (Haghighi and Vanderwende, 2009) on
exploring content models for multi-document sum-
marization. They proposed a HIERSUM model for
finding the subtopics or aspects which are combined
by using KL-divergence criterion for selecting rel-
evant sentences. They introduced a general con-
tent distribution and several specific content distri-
butions to discover the topic and aspects for a s-
ingle document collection. However, the aspects
may be shared not only across documents in a sin-
gle collection, but also across documents in different
topic-related collections. Their model is conceptual-
ly inadequate for simultaneously summarizing mul-
tiple topic-related document collections. Further-
more, their sentence selection method based on KL-
divergence cannot prevent redundancy across differ-
ent aspects.
In this paper, we study how to overcome these
1137
limitations. We hypothesize that comparatively
summarizing topics across similar collections can
improve the effectiveness of aspect-oriented multi-
document summarization. We propose a novel
extraction-based approach which consists of four
main steps listed below:
Sentence Clustering: Our goal in this step is to
automatically identify the different aspects and clus-
ter sentences into aspects (See Section 2). We sub-
stantially extend the entity-aspect model in (Li et al,
2010) for generating general sentence clusters.
Sentence Ranking: In this step, we use an exten-
sion of LexRank algorithm proposed by (Paul et al,
2010) to score representative sentences in each clus-
ter (See Section 3).
Sentence Compression: In this step, we aim to
improve the linguistic quality of the summaries by
simplifying the sentence expressions. We prune sen-
tences using grammatical relations defined on de-
pendency trees for recognizing important clauses
and removing redundant subtrees (See Section 4).
Sentence Selection: Finally, we select one com-
pressed version of the sentences from each aspec-
t cluster. We use Integer Linear Programming
(ILP) algorithm, which optimizes a global objective
function, for sentence selection (McDonald, 2007;
Gillick and Favre, 2009; Sauper and Barzilay, 2009)
(See Section 5).
We evaluate our method using TAC2010 Guided
Summarization task data sets1 (Section 6). Our eval-
uation shows that our method obtains better ROUGE
recall score compared with four baseline methods,
and it also achieve reasonably high-quality aspec-
t clusters in terms of purity.
2 Sentence Clustering
In this step, our goal is to discover event aspects con-
tained in a document set and cluster sentences in-
to aspects. Here we substantially extend the entity-
aspect model in Li et al (2010) and refer to it as
event-aspect model. The main difference between
our event-aspect model and entity-aspect model is
that we introduce an additional layer of event topics
and the separation of general and specific aspects.
1http://www.nist.gov/tac/2010/
Summarization/
Our extension is based upon the following ob-
servations. For example, specific events like
?Columbine Massacre? and ?Malaysia Resort Ab-
duction? can be related to the ?Attack? topic. Each
event consists of multiple articles written by dif-
ferent news agencies. Interesting aspects may in-
clude ?what happened, when, where, perpetrators,
reasons, who affected, damages and countermea-
sures,? etc2. We compared the ?Columbine Mas-
sacre? and ?Malaysia Resort Abduction? data set-
s and found 5 different kinds of words in the text:
(1) stop words that occur frequently in any docu-
ment collection; (2) general content words describ-
ing ?damages? or ?countermeasures? aspect of at-
tacks; (3) specific content words describing ?what
happened?, ?who affected? or ?where? aspect of the
concrete event; (4) background words describing the
general topic of ?Attack?; (5) words that are local to
a single document and do not appear across different
documents. Table 1 shows four sentences related to
two major aspects. We found that the entity-aspect
model does not have enough capacity to cluster sen-
tences into aspects (See Section 6). So we introduce
additional layer to improve the effectiveness of sen-
tence clustering. We also found that their one aspect
per sentence assumption is not very strong in this
scenario. Although a sentence may belong to a sin-
gle general aspect, it still contains multiple specific
aspect words like second sentence in Table 1. There-
fore, We assume that each sentence belongs to both
a general aspect and a specific aspect.
2.1 Event-Aspect Model
Stop words can be ignored by LDA model because
they can be easily identified using a standard stop
word list. Suppose that for a given event topic, there
are in total C specific events for which we need to
simultaneously generate summaries. We can assume
four kinds of unigram language models (i.e. multi-
nomial word distributions). For each event topic,
there is a background model ?B that generates words
commonly used in all documents, and there are AG
general aspect models ?ga (1 ? ga ? AG), where
AG is the number of general aspects. For each spe-
cific event in a topic, there are AS specific aspect
2http://www.nist.gov/tac/2010/
Summarization/Guided-Summ.2010.guidelines.
html
1138
countermeasures
Police/GA are/S close/B to/S identifying/GA someone/B responsible/GA
for/S the/S attack/B .
Investigators/GA do/S not/S know/B how/S many/S suspects/SA
they/S are/S looking/B for/S, but/S reported/B progress/B toward/S
identifying/GA one/S of/S the/S bombers/SA .
what happened, when, where
During/S the/S morning/SA rush/D hour/D on/S July/SA 7/SA terrorists/B
exploded/SA bombs/SA on/S three/D London/SA subway/D trains/SA and/S a/S
double-decker/D bus/SA .
Four/D coordinated/B bombings/SA struck/B central/B London/SA on/SA
July/SA 7/SA, three/D in/S subway/D cars/SA and/S one/D on/S a/S bus/SA .
Table 1: Four sentences on ?COUNTERMEASURES? and ?What, When, Where? aspects from the ?Attack? topic. S:
stop word. B: background word. GA: general aspect word. SA: specific aspect word. D: document word.
models ?sa (1 ? sa ? AS), where AS is the num-
ber of specific aspects, and also there are D doc-
ument models ?d (1 ? d ? D), where D is the
number of documents in this collection. We assume
that these word distributions have a uniform Dirich-
let prior with parameter ?.
We introduce a level distribution ? that control-
s whether we choose a word from ?ga or ?sa. ?
is sampled from Beta(?0, ?1) distribution. We also
introduce an aspect distribution ? that controls how
often a general or a specific aspect occurs in the col-
lection, where ? is sampled from another Dirichlet
prior with parameter ?. There is also a multinomi-
al distribution ? that controls in each sentence how
often we encounter a background word, a document
word, or an aspect word. ? has a Dirichlet prior with
parameter ?.
Let Sd denote the number of sentences in docu-
ment d, Nd,s denote the number of words (after stop
word removal) in sentence s of document d, and
wd,s,n denote the n?th word in this sentence. We
introduce hidden variables zgad,s and zsad,s to indicatethat a sentence s of document d belongs to which
general or specific aspects . We introduce hidden
variables yd,s,n for each word to indicate whether a
word is generated from the background model, the
document model, or the aspect model. We also intro-
duce hidden variables ld,s,n to indicate whether the
n?th word in sentence s of document d is generated
from the general aspect model. Figure 1 describes
the process of generating the whole document col-
lection. The plate notation of the model is shown in
Figure 2. Note that the values of ?0, ?1, ?1, ?2, ?
and ? are fixed. The number of general and specific
aspects AG and AS are also empirically set.
Given a document collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assignmen-
t of zgad,s, zsad,s, yd,s,n and ld,s,n that maximizes dis-tribution p(z,y, l|w;?, ?, ?, ?), where z, y, l and w
represent the set of all z, y, l andw variables, respec-
tively. With the assignment, sentences are naturally
clustered into aspects, and words are labeled as ei-
ther a background word, a document word, a general
aspect word or a specific aspect word.
Inference can be done with Gibbs sampling,
which is commonly used in LDA models (Griffiths
and Steyvers, 2004).
In our experiments, we set ?1 = 5, ?2 = 3,
? = 0.01, ? = 20, ?1 = 10 and ?2 = 10. We
run 100 burn-in iterations through all documents in
a collection to stabilize the distribution of z and y
before collecting samples. We take 10 samples with
a gap of 10 iterations between two samples, and av-
erage over these 10 samples to get the estimation for
the parameters.
After estimating all the distributions, we can find
the values of each zgad,s and zsad,s that gives us sen-tences clustered into general and specific aspects.
3 Sentence Ranking
In this step, we want to order the clustered sen-
tences so that the representative sentences can be
ranked higher in each aspect. Inspired by Paul et
al. (2010), we use an extended LexRank algorithm
to obtain top ranked sentences. LexRank (Erkan and
Radev, 2004) algorithm defines a random walk mod-
1139
1. Draw ?1 ? Dir(?1), ?2 ? Dir(?2), ? ? Dir(?)
Draw ? ? Beta(?0, ?1)
2. For each event topic, there is a background model
?B, and there are general aspect ga, where 1 ?
ga ? AG
(a) draw ?B ? Dir(?)
(b) draw ?ga ? Dir(?)
3. For each document collection, there are specific
aspect sa, where 1 ? sa ? AS
(a) draw ?sa ? Dir(?)
4. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zga ? Multi(?1)
ii. draw zsa ? Multi(?2)
iii. for each word n = 1, . . . , Nd,s
A. draw ld,s,n ? Binomial(?)
B. draw yd,s,n ? Multi(?)
C. drawwd,s,n ? Multi(?B) if yd,s,n =
1, wd,s,n ? Multi(?d) if yd,s,n = 2,
wd,s,n ? Multi(?z
sa
d,s) if yd,s,n =
3 and ld,s,n = 1 or wd,s,n ?
Multi(?z
ga
d,s) if yd,s,n = 3 and
ld,s,n = 0
Figure 1: The document generation process.
el on top of a graph that represents sentences to be
summarized as nodes and their similarities as edges.
The LexRank score of a sentence gives the expected
probability that a random walk will visit that sen-
tence in the long run. A variant is called continu-
ous LexRank improved LexRank by making use of
the strength of the similarity links. The continuous
LexRank score can be computed using the following
formula:
L(u) = dN + (1 ? d)
?
v?adj[u]
p(u|v)L(v)
whereL(u) is the LexRank value of sentence u,N is
the total number of nodes in the graph, d is a damp-
ing factor for the convergence of the method, and
p(u|v) is the jumping probability between sentence
u and its neighboring sentence v. p(u|v) is defined
using content similarity function sim(u, v) between
two sentences:
T
yd? d
SD sd ,wSA?
?
C
?
pi ?
?
gaz saz
l
?
GA
B?
1? 2?
1? 2?
Figure 2: The event-aspect model.
p(u|v) = sim(u, v)?
z?adj[v] sim(z, v)
The major extension is to modify this jumping
probability so as to favor visiting representative sen-
tences. More specifically, we scale sim(u, v) by the
likelihood that the two sentences represent the same
general aspect ga or specific aspect sa:
sim?(u, v) = sim(u, v)[
AG?
ga=1
P (ga|u)P (ga|v)
+
AS?
sa=1
P (sa|u)P (sa|v)]
where the value P (ga|u) and P (sa|u) can be
computed by our event-aspect model. We define
sim(u, v) as the tf ? idf weighted cosine similar-
ity between two sentences.
We found that sentence ranking is better con-
ducted before the compression because the pre-
compressed sentences are more informative and the
similarity function in LexRank can be better off with
the complete information.
4 Sentence Compression
It has been shown that sentence compression can
improve linguistic quality of summaries (Zajic et
al., 2007; Gillick et al, 2010). Commonly used
?Syntactic parse and trim? approach may produce
poor compression results. For example, given the
sentence ?We have friends whose children go to
Columbine, the freshman said?, the procedure tries
to remove the clause ?the freshman said? from the
parse tree by using the ?SBAR? label to locate the
1140
clause, and will result in ?whose children go to
Columbine?, which is not adequate. Furthermore,
some important temporal modifier, numeric modifier
and clausal complement need to be retained because
they reflect content aspects of the summary. There-
fore, we propose the ?dependency parse and trim?
approach, which prunes sentences based on depen-
dency tree representations, using English grammati-
cal relations to recognize clauses and remove redun-
dant structures. Table 2 shows two examples by re-
moving redundant auxiliary clauses. Below is the
sentence compression procedure:
1. Select possible subtree root nodes using gram-
matical relations, such as clausal complement,
complementizer, or parataxis 3.
2. Decide which subtree root node can be the root
of clause. If this root contains maximum num-
ber of child nodes and the collection of all child
edges include object or auxiliary relations, it is
selected as the root node.
3. Remove redundant modifiers such as adverbial-
s, relative clause modifiers and abbreviations,
participials and infinitive modifiers.
4. Traverse the subtrees and generate all possible
compression alternatives using the subtree root
node, then keep the top two longest sub sen-
tences.
5. Drop the sub sentences shorter than 5 words.
5 Sentence Selection
After sentence pruning, we prepare for the final
event summary generation process. In this step, we
select one compressed version of the sentence from
each aspect cluster. To avoid redundancy between
aspects, we use Integer Linear Programming to opti-
mize a global objective function for sentence selec-
tion. Inspired by (Sauper and Barzilay, 2009), we
formulate the optimization problem based on sen-
tence ranking information. More specifically, we
3The parataxis relation is a relation between the main verb
of a clause and other sentential elements, such as a sentential
parenthetical, colon, or semicolon
Original Compressed
When rescue workers
arrived, they said, on-
ly one of his limbs was
visible.
When rescue workers
arrived, only one of his
limbs was visible.
Two days earlier, a
massacre by two s-
tudents at Columbine
High, whose teams are
called the Rebels, left
15 people dead and
dozens wounded.
Two days earlier, a
massacre by two stu-
dents at Columbine
High, left 15 peo-
ple dead and dozens
wounded.
Table 2: Example compressed sentences.
would like to select exactly one compressed sen-
tence which receives the highest possible ranking s-
core from each aspect cluster subject to a series of
constraints, such as redundancy and length. We em-
ployed lp solver 4, an efficient mixed integer pro-
gramming solver using the Branch-and-Bound algo-
rithm to select sentences.
Assume that there are in total K aspects in an
event topic. For each aspect j, there are in total R
ranked sentences. The variables Sjl is a binary indi-
cator of the sentence. That is, Sjl= 1 if the sentence
is included in the final summary, and Sjl = 0 other-
wise. l is the ranked position of the sentence in this
aspect cluster.
Objective Function
Top ranked sentences are the most relevant corre-
sponding to the related aspects which we want to in-
clude in the final summary. Thus we try to minimize
the ranks of the sentences to improve the overall re-
sponsiveness.
min(
K?
j=1
Rj?
l=1
l ? Sjl)
Exclusivity Constraints
To prevent redundancy in each aspect, we just
choose one sentence from each general or specific
aspect cluster. The constraint is formulated as fol-
lows:
Rj?
l=1
Sjl = 1 ?j ? {1 . . .K}
4http://lpsolve.sourceforge.net/5.5/
1141
Redundancy Constraints
We also want to prevent redundancy across differ-
ent aspects. If sentence-similarity sim(sjl, sj?l?) be-
tween sentence sjl and sj?l? is above 0.5, then we
drop the pair and choose one sentence ranked higher
from the pair otherwise. This constraint is formulat-
ed as follows:
(Sjl + Sj?l?) ? sim(sjl, sj?l?) ? 0.5
?j, j? ? {1 . . .K}?l ? {1 . . . Rj}?l? ? {1 . . . Rj?}
Length Constraints
We add this constraint to ensure that the length of
the final summary is limited to L words.
K?
j=1
Rj?
l=1
lenjl ? Sjl ? L
where lenjl is the length of Sjl.
6 Evaluation
In order to systematically evaluate our method, we
want to check (1) whether the whole system is effec-
tive, which means to quantitatively evaluate summa-
ry quality, and (2) whether individual components
like clustering and compression algorithms are use-
ful.
6.1 Data
We use TAC2010 Summarization task data set for
the summary content evaluation. This data set pro-
vides 46 events. Each event falls into a predefined
event topic. Each specific event includes an even-
t statement and 20 relevant newswire articles which
have been divided into 2 sets: Document Set A and
Document Set B. Each document set has 10 docu-
ments, and all the documents in Set A chronologi-
cally precede the documents in Set B. We just use
document Set A for our task. Assessors wrote mod-
el summaries for each event, so we can compare
our automatic generated summaries with the model
summaries. We combine topic related data sets to-
gether, then these data sets simultaneously annotated
by our Event-aspect model. After labeling process,
we run sentence ranking, compression and selection
module to get final aspect-oriented summarizations.
6.2 Quality of summary
We use the ROUGE (Lin and Hovy, 2003) metric for
measuring the summarization system performance.
Ideally, a summarization criterion should be more
recall oriented. So the average recall of ROUGE-
1, ROUGE-2, ROUGE-SU4, ROUGE-W-1.2 and
ROUGE-L were computed by running ROUGE-
1.5.5 with stemming but no removal of stop word-
s. We compare our method with the following four
baseline methods.
Baseline 1
In this baseline, we try to compare different sen-
tence clustering algorithms in the multi-document
summarization scenario. First, we use CLUTO 5 to
do K-means clustering. Then we try entity-aspect
model proposed by Li et al (2010) to do sentence
clustering. Entity-aspect model is similar with ?HI-
ERSUM? content model proposed by Haghighi and
Vanderwende (2009). We use the same ranking,
compression, and selection components to generate
aspect-oriented summaries for comparison.
Baseline 2
In this baseline, we compare our method with
traditional ranking and selection summary genera-
tion framework (Erkan and Radev, 2004; Nenkova
and Vanderwende, 2005) to show that our sentence
clustering component is necessary in aspect-oriented
summarization system. Also we want check whether
sentence ranking combined with greedy based sen-
tence selection can prevent redundancy effective-
ly. We follow LexRank based sentence ranking
combined with greedy sentence selection methods.
We implement two greedy algorithms (Zhang et al,
2008; Paul et al, 2010). One is to select the top
ranked sentence simultaneously by removing 10 re-
dundant neighbor sentences from the sentence sim-
ilarity graph if the summary length is less then 100
words. This is repeated until the graph cannot be
partitioned. The similarity graph building threshold
is 0.3, damping factor is 0.2 and error tolerance for
Power Method in LexRank is 0.1. The other is to se-
lect top ranked sentences as long as the redundancy
score (similarity) between a candidate sentence and
5http://glaros.dtc.umn.edu/gkhome/cluto/
cluto/overview
1142
current summary is under 0.5. This is repeated until
the summary reaches a 100 word length limit.
Baseline 3
In this baseline, we compare our ILP based sen-
tence selection with KL-divergence based sentence
selection. The KL-divergence formula we use is be-
low,
KL(PS ||QD) =
?
w
P (w) log P (w)Q(w)
where P (S) is the empirical unigram distribution of
the candidate summary S, and Q(D) is the unigram
distribution of document collection D. We only re-
placed our selection method with the KL-divergence
selection method. Other parts are the same. After
ranking sentences for each aspect, we add the sen-
tence with the highest ranking score from each as-
pect sentence cluster as long as the KL-divergence
between candidate and current summary does not
decrease. This is repeated until the summary reach-
es a 100 word length limit. To our knowledge, this
is the first work to directly compare Integer Lin-
ear Programming based sentence selection with KL-
divergence based sentence selection in summariza-
tion generation framework.
Baseline 4
In this baseline, we directly compare our method
with ?HIERSUM? proposed by (Haghighi and Van-
derwende, 2009). As in Baseline 1, we use entity-
aspect model to approximate ?HIERSUM? mod-
el. We replace unigram distribution of P (w) in
KL-divergence with learned distribution estimated
by ?HIERSUM? model. The KL-divergence based
greedy sentence selection algorithm is similar to
Baseline 3.
For fair comparison, Baselines 1, 2, 3 and 4 use
the same sentence compression algorithm and have
the summary length no more then 100 words. In
Table 3, we show the average ROUGE recall of 46
summaries generated by our method and four base-
line methods. We can see that our method gives
better Rouge recall measures then the four baseline
methods. For BL-1, we can see that LDA-based sen-
tence clustering is better then k-means. For BL-2,
we can see that traditional ranking plus greedy selec-
tion summary generation framework is not suitable
for the aspect-oriented summarization task. More
specifically, greedy-based sentence selection can not
prevent redundancy effectively. BL-3 evaluation re-
sults showed that ILP-based sentence selection is
better then KL-divergence selection in terms of pre-
venting redundancy across different aspects. The
measurement performance between BL-3 and BL-
4 is close. They use the same KL-divergence based
sentence selection, but topic model they use are d-
ifferent, and also BL-3 has a sentence ranking pro-
cess. The Rouge recall of our method is better than
BL-4. It is expected because our event-aspect mod-
el can better find the aspects and also prove that
our LexRank based sentence ranking combined with
ILP-based sentence selection can prevent redundan-
cy.
Due to TAC2010 summarization community just
compute ROUGE-2 and ROUGE-SU4 metrics for
participants, our ROUGE-2 metric ranked 11 out
of 23, ROUGE-SU4 metric ranked 12 out of 23.
They use MEAD6 as their baseline approach. The
ROUGE-2 score of our approach achieve 0.06508
higher than MEAD?s 0.05929. The ROUGE-SU4 s-
core of our approach achieve 0.10146 higher than
MEAD?s 0.09112. Many systems that get high-
er performances leverage domain knowledge bases
like Wikipedia or training data, but we didn?t. The
advantage of our method is that we generate sum-
maries with totally unsupervised framework and this
approach is domain adaptive.
6.3 Quality of aspect-oriented sentence clusters
To judge the quality of the aspect-oriented sentence
clusters, we ask the human judges to group the
ground truth sentences based on the aspect related-
ness in each event topic. We then compute the pu-
rity of the automatically generated clusters against
the human judged clusters. The results are shown
in Table 4. In our experiments, we set the number
of general aspect clusters AG is 5 and specific as-
pect clusters AS is 3. We can see from Table 4 that
our generated aspect clusters can achieve reasonably
good performance.
6http://www.summarization.com/mead/
1143
Rouge Average Recall
Method ROUGE-1 ROUGE-2 ROUGE-SU4 ROUGE-W-1.2 ROUGE-L
BL-1 k-means 0.21895 0.03689 0.06644 0.06683 0.19208
entity-aspect 0.26082 0.05082 0.08286 0.08055 0.22976
BL-2 greedy 1 0.27802 0.04872 0.08302 0.08488 0.24426
greedy 2 0.27898 0.04723 0.08275 0.08500 0.24430
BL-3 KL-Div 0.29286 0.05369 0.09117 0.08827 0.25100
BL-4 HIERSUM 0.28736 0.05502 0.08932 0.08923 0.25285
Without compression 0.30563 0.05983 0.09513 0.09468 0.25487
Our Method 0.32641 0.06508 0.10146 0.09998 0.28610
Table 3: ROUGE evaluation results on TAC2010 Summarization data sets
Category A Purity
Accidents and Natural Disasters 7 0.613
Attacks 8 0.658
Health and Safety 5 0.724
Endangered Resources 4 0.716
Investigations and Trials 6 0.669
Table 4: The true numbers of aspects as judged by the
human annotator (A), and the purity of the clusters.
Category Average Score
Accidents and Natural Disasters 2.4
Attacks 2.3
Health and Safety 2.6
Endangered Resources 2.5
Investigations and Trials 2.4
Table 5: The average score of each event topic.
6.4 Quality of sentence compression
To judge the quality of the dependency tree based
sentence compression algorithm, we ask the human
judges to choose 20 sentences from each event top-
ic then score them. The judges follow 3-point scale
to score each compressed sentence: 1 means poor,
2 means barely acceptable, and 3 means good. We
then compute the average scores. The results are
shown in Table 5. To evaluate the effectiveness of
sentence compression component, we conduct the
system without sentence compression component,
then compare it with our system. In Table 3, we
can see that sentence compression can improve the
system performance.
7 Related Work
Our event-aspect model is related to a number of
previous extensions of LDA models. Chemudugun-
ta et al (2007) proposed to introduce a background
topic and document-specific topics. Our background
and document language models are similar to theirs.
However, they still treat documents as bags of words
rather then sets of sentences as in our models. Titov
and McDonald (2008) exploited the idea that a short
paragraph within a document is likely to be about
the same aspect. The way we separate words in-
to stop words, background words, document word-
s and aspect words bears similarity to that used
in (Daume? III and Marcu, 2006; Haghighi and Van-
derwende, 2009). Paul and Girju (2010) proposed a
topic-aspect model for simultaneously finding topic-
s and aspects. The most related extension is entity-
aspect model proposed by Li et al (2010). The main
difference between event-aspect model and entity-
aspect model is our model further consider aspect
granularity and add a layer to model topic-related
events.
Filippova and Strube (2008) proposed a depen-
dency tree based sentence compression algorithm.
Their approach need a large corpus to build language
model for compression, whereas we prune depen-
dency tree using grammatical rules.
Paul et al (2010) proposed to modify LexRank
algorithm using their topic-aspect model. But their
task is to summarize contrastive viewpoints in opin-
ionated text. Furthermore, they use a simple greedy
approach for constructing summary.
McDonald (2007) proposed to use Integer Linear
Programming framework in multi-document sum-
1144
marization. And Sauper and Barzilay (2009) use in-
teger linear programming framework to automatical-
ly generate Wikipedia articles. There is a fundamen-
tal difference between their method and ours. They
used trained perceptron algorithm for ranking ex-
cerpts, whereas we give an extended LexRank with
integer linear programming to optimize sentence se-
lection for our aspect-oriented multi-document sum-
marization.
8 Conclusions and Future Work
In this paper, we study the task of automatically
generating aspect-oriented summary from multiple
documents. We proposed an event-aspect model
that can automatically cluster sentences into aspect-
s. We then use an extension of the LexRank algo-
rithm to rank sentences. We took advantage of the
output generated by the event-aspect model to mod-
ify jumping probabilities so as to favor visiting rep-
resentative sentence. We also proposed dependen-
cy tree compression algorithm to prune sentence for
improving linguistic quality of the summaries. Fi-
nally we use Integer Linear Programming Frame-
work to select aspect relevant sentences. We con-
ducted quantitative evaluation using standard test
data sets. We found that our method gave overal-
l better ROUGE scores than four baseline methods,
and the new sentence clustering and compression al-
gorithm are robust.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply more linguistic knowl-
edge to improve the quality of sentence compres-
sion. Currently the sentence compression algorith-
m may generate meaningless subtrees. It is rela-
tively hard to decide which clause is redundant in
terms of summarization. Second, we may explore
more domain knowledge to improve the quality of
aspect-oriented summaries. For example, we know
that the ?who-affected? aspect is related to person,
and ?when, where? are related to Time and Location.
we can import Name Entity Recognition to anno-
tate these phrases and then help locate relevant sen-
tences. Third, we want to extend our event-aspect
model to simultaneously find topics and aspects.
Acknowledgments
This work was supported by the National Nat-
ural Science Foundation of China (NSFC No.
60773088), the National High-tech R&D Program
of China (863 Program No. 2009AA04Z106), and
the Key Program of Basic Research of Shanghai
Municipal S&T Commission (No. 08JC1411700).
References
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific aspects
of documents with a probabilistic topic model. In Ad-
vances in Neural Information Processing Systems 19,
pages 241?248.
Hal. Daume? III and Daniel. Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 305?312.
Association for Computational Linguistics.
Gu?nes. Erkan and Dragomir Radev. 2004. LexRank:
Graph-based lexical centrality as salience in text sum-
marization. Journal of Artificial Intelligence Re-
search, 22(1):457?479.
K. Filippova and M. Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference, pages 25?32. Association for Computa-
tional Linguistics.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
gauge Processing, pages 10?18.
Dan Gillick, Benoit Favre, D. Hakkani-Tur, B. Bohnet,
Y. Liu, and S. Xie. 2010. The icsi/utd summarization
system at tac 2009. In Proceedings of the Second Text
Analysis Conference, Gaithersburg, Maryland, USA:
National Institute of Standards and Technology.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National A-
cademy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
A. Haghighi and L. Vanderwende. 2009. Exploring con-
tent models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics on
ZZZ, pages 362?370. Association for Computational
Linguistics.
1145
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gen-
erating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the Joint Conference of the 48th Annual Meeting of the
ACL. Association for Computational Linguistics.
C.Y. Lin and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 71?78. Association for Computation-
al Linguistics.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. Advances in
Information Retrieval, pages 557?564.
A. Nenkova and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Michael J. Paul and Roxana Girju. 2010. A two-
dimensional topic-aspect model for discovering multi-
faceted topics. In In AAAI-2010: Twenty-Fourth Con-
ference on Artificial Intelligence.
Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010. Summarizing contrastive viewpoints in opin-
ionated text. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?10, pages 66?76, Morristown, NJ, USA.
Association for Computational Linguistics.
Christina Sauper and Regina Barzilay. 2009. Automati-
cally generating wikipedia articles: A structure-aware
approach. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 208?216, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ivan Titov and Ryan McDonald. 2008. Modeling online
reviews with multi-grain topic models. In Proceeding
of the 17th International Conference on World Wide
Web, pages 111?120.
D. Zajic, B.J. Dorr, J. Lin, and R. Schwartz. 2007. Multi-
candidate reduction: Sentence compression as a tool
for document summarization tasks. Information Pro-
cessing & Management, 43(6):1549?1570.
Jin. Zhang, Xueqi. Cheng, and Hongbo. Xu. 2008. GSP-
Summary: a graph-based sub-topic partition algorith-
m for summarization. In Proceedings of the 4th Asi-
a information retrieval conference on Information re-
trieval technology, pages 321?334. Springer-Verlag.
1146
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Recursive Autoencoders for ITG-based Translation
Peng Li, Yang Liu and Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
pengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cn
Abstract
While inversion transduction grammar (ITG)
is well suited for modeling ordering shifts
between languages, how to make applying
the two reordering rules (i.e., straight and
inverted) dependent on actual blocks being
merged remains a challenge. Unlike previous
work that only uses boundary words, we pro-
pose to use recursive autoencoders to make
full use of the entire merging blocks alter-
natively. The recursive autoencoders are ca-
pable of generating vector space representa-
tions for variable-sized phrases, which enable
predicting orders to exploit syntactic and se-
mantic information from a neural language
modeling?s perspective. Experiments on the
NIST 2008 dataset show that our system sig-
nificantly improves over the MaxEnt classifier
by 1.07 BLEU points.
1 Introduction
Phrase-based models (Koehn et al, 2003; Och and
Ney, 2004) have been widely used in practical ma-
chine translation (MT) systems due to their effec-
tiveness, simplicity, and applicability. First, as se-
quences of consecutive words, phrases are capable
of memorizing local word selection and reorder-
ing, making them an effective mechanism for trans-
lating idioms or translations with word insertions
or omissions. Moreover, n-gram language models
can be seamlessly integrated into phrase-based de-
coders since partial translations grow left to right
in decoding. Finally, phrase-based systems can be
applicable to most domains and languages, espe-
cially for resource-scarce languages without high-
accuracy parsers.
However, as phrase-based decoding casts transla-
tion as a string concatenation problem and permits
arbitrary permutations, it proves to be NP-complete
(Knight, 1999). Therefore, phrase reordering mod-
eling has attracted intensive attention in the past
decade (e.g., Och et al, 2004; Tillman, 2004; Zens
et al, 2004; Al-Onaizan and Papineni, 2006; Xiong
et al, 2006; Koehn et al, 2007; Galley and Man-
ning, 2008; Feng et al, 2010; Green et al, 2010;
Bisazza and Federico, 2012; Cherry, 2013).
Among them, reordering models based on inver-
sion transduction grammar (ITG) (Wu, 1997) are
one of the important ongoing research directions.
As a formalism for bilingual modeling of sentence
pairs, ITG is particularly well suited to predicting
ordering shifts between languages. As a result, a
number of authors have incorporated ITG into left-
to-right decoding to constrain the reordering space
and reported significant improvements (e.g., Zens et
al., 2004; Feng et al, 2010). Along another line,
Xiong et al (2006) propose a maximum entropy
(MaxEnt) reordering model based on ITG. They use
the CKY algorithm to recursively merge two blocks
(i.e., a pair of source and target strings) into larger
blocks, either in a straight or an inverted order. Un-
like lexicalized reordering models (Tillman, 2004;
Koehn et al, 2007; Galley and Manning, 2008) that
are defined on individual bilingual phrases, the Max-
Ent ITG reordering model is a two-category classi-
fier (i.e., straight or inverted) for two arbitrary bilin-
gual phrases of which the source phrases are adja-
cent. This potentially alleviates the data sparseness
567
problem since there are usually a large number of
reordering training examples available (Xiong et al,
2006). As a result, the MaxEnt ITG model and its
extensions (Xiong et al, 2008; Xiong et al, 2010)
have achieved competing performance as compared
with state-of-the-art phrase-based systems.
Despite these successful efforts, the ITG reorder-
ing classifiers still face a major challenge: how to
extract features from training examples (i.e., a pair
of bilingual strings). It is hard to decide which words
are representative for predicting reordering, either
manually or automatically, especially for long sen-
tences. As a result, Xiong et al (2006) only use
boundary words (i.e., the first and the last words in
a string) to predict the ordering. What if we look
inside? Is it possible to avoid manual feature engi-
neering and learn semantic representations from the
data?
Fortunately, the rapid development of intersect-
ing deep learning with natural language processing
(Bengio et al, 2003; Collobert and Weston, 2008;
Collobert et al, 2011; Glorot et al, 2011; Bordes et
al., 2011; Socher et al, 2011a; Socher et al, 2011b;
Socher et al, 2011c; Socher et al, 2012; Bordes et
al., 2012; Huang et al, 2012; Socher et al, 2013;
Hermann and Blunsom, 2013) brings hope for alle-
viating this problem. In these efforts, natural lan-
guage words are represented as real-valued vectors,
which can be naturally fed to neural networks as in-
put. More importantly, it is possible to learn vec-
tor space representations for multi-word phrases us-
ing recursive autoencoders (Socher et al, 2011c),
which opens the door to leveraging semantic repre-
sentations of phrases in reordering models from a
neural language modeling point of view.
In this work, we propose an ITG reordering clas-
sifier based on recursive autoencoders. The neu-
ral network consists of four autoencoders (i.e., the
first source phrase, the first target phrase, the sec-
ond source phrase, and the second target phrase)
and a softmax layer. The recursive autoencoders,
which are trained on reordering examples extracted
from word-aligned bilingual corpus, are capable
of producing vector space representations for arbi-
trary multi-word strings in decoding. Therefore,
our model takes the whole phrases rather than only
boundary words into consideration when predict-
ing phrase permutations. Experiments on the NIST
2008 dataset show that our system significantly im-
proves over the MaxEnt classifier by 1.07 in terms
of case-insensitive BLEU score.
2 Recursive Autoencoders for ITG-based
Translation
2.1 Inversion Transduction Grammar
Inversion transduction grammar (ITG) (Wu, 1997)
is a formalism for synchronous parsing of bilingual
sentence pairs. Xiong et al (2006) apply bracketing
transduction grammar (BTG), which is a simplified
version of ITG, to phrase-based translation using the
following production rules:
X ? [X1, X2] (1)
X ? ?X1, X2? (2)
X ? f/e (3)
where X is a block that consists of a pair of source
and target strings, f is a source phrase, and e is a tar-
get phrase. X1 and X2 are two neighboring blocks
of which the two source phrases are adjacent. While
rule (1) merges two target phrases in a straight or-
der, rule (2) merges in an inverted order. Besides
these two reordering rules, rule (3) is a lexical rule
that translates a source phrase f into a target phrase
e. This is exactly a bilingual phrase used in conven-
tional phrase-based systems.
An ITG derivation, which consists of a sequence
of production rules, explains how a sentence pair is
generated simultaneously. Figure 1 shows an ITG
derivation for a Chinese sentence and its English
translation. We distinguish between two types of
blocks:
1. atomic blocks: blocks generated by applying
lexical rules,
2. composed blocks: blocks generated by apply-
ing reordering rules.
In Figure 1, the sentence pair is segmented into
five atomic blocks:
X0,3,0,3 : wo you yi ge? I have a
X3,5,5,6 : cong mei you? never
X5,8,6,8 : jian guo de? seen before
X8,10,3,5 : nv xing peng you? female friend
X10,11,8,9 : .? .
568
(1) X0,11,0,9 ? [X0,10,0,8, X10,11,8,9]
(2) X0,10,0,8 ? [X0,3,0,3, X3,10,3,8]
(3) X0,3,0,3 ? wo you yi ge / I have a
(4) X3,10,3,8 ? ?X3,8,5,8, X8,10,3,5?
(5) X3,8,5,8 ? [X3,5,5,6, X5,8,6,8]
(6) X3,5,5,6 ? cong mei you / never
(7) X5,8,6,8 ? juan guo de / seen before
(8) X8,10,3,5 ? nv xing peng you/ female friend
(9) X10,11,8,9 ? . / .
Figure 1: An ITG derivation for a Chinese sentence and its translation. We useXi,j,k,l = ?f
j
i , e
l
k? to represent a block.
Our neural ITG reordering model first assigns vector space representations to single words and then produces vectors
for phrases using recursive autoencoders, which form atomic blocks. The atomic blocks are recursively merged into
composed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously.
The neural classifier makes decisions at each node using the vectors of all its descendants.
569
where X3,5,5,6 indicates that the block consists of a
source phrase spanning from position 3 to position 5
(i.e., ?cong mei you?) and a target phrase spanning
from position 5 to position 6 (i.e., ?never?). More
formally, a block Xi,j,k,l = ?f
j
i , e
l
k? is a pair of a
source phrase f ji = fi+1 . . . fj and a target phrase
elk = ek+1 . . . el. Obviously, these atomic blocks
are generated by lexical rules.
Two blocks of which the source phrases are adja-
cent can be merged into a larger one in two ways:
concatenating the target phrases in a straight order
using rule (1) or in an inverted order using rule (2).
For example, atomic blocks X3,5,5,6 and X5,8,6,8 are
merged into a composed block X3,8,5,8 in a straight
order, which is further merged with an atomic block
X8,10,3,5 into another composed block X3,10,3,8 in
an inverted order. This process recursively proceeds
until the entire sentence pair is generated.
The major challenge of applying ITG to machine
translation is to decide when to merge two blocks
in a straight order and when in an inverted order.
Therefore, the ITG reordering model can be seen as
a two-category classifier P (o|X1, X2), where o ?
{straight, inverted}.
A naive way is to assign fixed probabilities to two
reordering rules, which is referred to as flat model
by Xiong et al (2006):
P (o|X1, X2) =
{
p o = straight
1? p o = inverted
(4)
The drawback of the flat model is ignoring the
actual blocks being merged. Intuitively, different
blocks should have different preferences between
the two orders.
To alleviate this problem, Xiong et al (2006) pro-
pose a maximum entropy (MaxEnt) classifier:
P (o|X1, X2) =
exp(? ? h(o,X1, X2))
?
o? exp(? ? h(o
?, X1, X2))
(5)
where h(?) is a vector of features defined on the
blocks and the order, ? is a vector of feature weights.
While MaxEnt is a flexible and powerful frame-
work for including arbitrary features, feature engi-
neering becomes a major challenge for the MaxEnt
classifier. Xiong et al (2006) find that boundary
words (i.e., the first and the last words in a string)
are informative for predicting reordering. Actually,
Figure 2: A recursive autoencoder for multi-word strings.
The example is adapted from (Socher et al, 2011c). Blue
and grey nodes are original and reconstructed ones, re-
spectively.
it is hard to decide which internal words in a long
composed blocks are representative and informa-
tive. Therefore, they only use boundary words as
the main features.
However, it seems not enough to just consider
boundary words and ignore all internal words when
making order predictions, especially for long sen-
tences.1 Indeed, Xiong et al (2008) find that the
MaxEnt classifier with boundary words as features
is prone to make wrong predictions for long com-
posed blocks. As a result, they have to impose a hard
constraint to always prefer merging long composed
blocks in a monotonic way.
Therefore, it is important to consider more than
boundary words to make more accurate reordering
predictions. We need a new mechanism to achieve
this goal.
2.2 Recursive Autoencoders
2.2.1 Vector Space Representations for Words
In neural networks, a natural language word is
represented as a real-valued vector (Bengio et al,
2003; Collobert and Weston, 2008). For example,
we can use [0.1 0.8 0.4]T to represent ?female? and
1Strictly speaking, the ITG reordering model is not a phrase
reordering model since phrase pairs are only the atomic blocks.
Instead, it is defined to work on arbitrarily long strings because
composed blocks become larger and larger until the entire sen-
tence pair is generated.
570
Figure 3: A neural ITG reordering model. The binary classifier makes decisions based on the vector space representa-
tions of the source and target sides of merging blocks.
[0.7 0.1 0.5]T to represent ?friend?. Such vector
space representations enable natural language words
to be fed to neural networks as input.
Formally, we denote each word as a vector x ?
Rn. These word vectors are then stacked into a word
embedding matrix L ? Rn?|V |, where |V | is the vo-
cabulary size. Given a sentence that is an ordered list
ofmwords, each word has an associated vocabulary
index k into the word embedding matrix L that we
use to retrieve the word?s vector space representa-
tion. This look-up operation can be seen as a simple
projection layer:
xi = Lbk ? Rn (6)
where bk is a binary vector which is zero in all posi-
tions except for the kth index.
In Figure 1, we assume n = 3 for simplicity and
can retrieve vectors for Chinese and English words
from two embedding matrices, respectively.
2.2.2 Vector Space Representations for
Multi-Word Strings
To apply neural networks to ITG-based transla-
tion, it is important to generate vector space repre-
sentations for atomic and composed blocks.
For example, since the vector of ?female? is
[0.1 0.8 0.4]T and the vector of ?friend? is
[0.7 0.1 0.5]T , what is the vector of the phrase ?fe-
male friend?? If we denote ?female friend? as p
(i.e., parent), ?female? as c1 (i.e., the first child),
and ?friend? as c2 (i.e., the second child), this can
be done by applying a function f (1):
p = f (1)(W (1)[c1; c2] + b
(1)) (7)
where [c1; c2] ? R2n?1 is the concatenation of c1
and c2, W (1) ? Rn?2n is a parameter matrix, b(1) ?
Rn?1 is a bias term, and f (1) is an element-wise ac-
tivation function such as tanh(?), which is used in
our experiments.
Note that the resulting vector for the parent is also
an n-dimensional vector, e.g, [0.6 0.9 0.2]T . The
same neural network can be recursively applied to
two strings until the vector of the entire sentence is
generated. As ITG derivation builds a binary parse
tree, the neural network can be naturally integrated
into CKY parsing.
To assess how well the learned vector p represents
its children, we can reconstruct the children in a
reconstruction layer:
[c?1; c
?
2] = f
(2)(W (2)p+ b(2)) (8)
where c?1 and c
?
2 are the reconstructed children,W
(2)
is a parameter matrix for reconstruction, b(2) is a bias
term for reconstruction, and f (2) is an element-wise
activation function, which is also set as tanh(?) in
our experiments. Similarly, the same reconstruction
neural network can be applied to each node in an
ITG parse.
These neural networks are called recursive au-
toencoders (Socher et al, 2011c). Figure 2 illus-
trates an application of a recursive autoencoder to a
571
binary tree. The blue and grey nodes are the original
and reconstructed nodes, respectively. The autoen-
coder is re-used at each node of the tree. The bi-
nary tree is composed of a set of triplets in the form
of (p ? c1 c2), where p is a parent vector and c1
and c2 are children vectors of p. Each child can be
either an input word vector or a multi-word vector.
Therefore, the tree in Figure 2 can be represented as
three triplets: (y1 ? x1 x2), (y2 ? y1 x3), and
(y3 ? y2 x4).
In Figure 1, we use recursive autoencoders to gen-
erate vector space representations for Chinese and
English phrases, which form the atomic blocks for
further block merging.
2.2.3 A Neural ITG Reordering Model
Once the vectors for blocks are generated, it is
straightforward to introduce a neural ITG reorder-
ing model. As shown in Figure 3, the neural net-
work consists of an input layer and a softmax layer.
The input layer is composed of the vectors of the
first source phrase, the first target phrase, the second
source phrase, and the second target phrase. Note
that all phrases in the same language use the the
same recursive autoencoder. The softmax layer out-
puts the probabilities of the two merging orders:
P (o|X1, X2) =
exp(g(o,X1, X2))
?
o? exp(g(o
?, X1, X2))
(9)
g(o,X1, X2) = f(W oc(X1, X2) + bo) (10)
where o ? {straight, inverted}, W o ? R1?4n
is a parameter matrix, bo ? R is a bias term, and
c(X1, X2) ? R4n?1 is the concatenation of the vec-
tors of the four phrases.
3 Training
There are three sets of parameters in our recursive
autoencoders:
1. ?L: word embedding matrix L for both source
and target languages (Section 2.2.1);
2. ?rec: recursive autoencoder parameter matrices
W (1), W (2) and bias terms b(1), b(2) for both
source and target languages (Section 2.2.2);
3. ?reo: neural ITG reordering model parameter
matrix W o and bias term bo (Section 2.2.3).
All these parameters are learned automatically from
the training data. For clarity, we will use ? to denote
all these parameters in the rest of the paper.
For training word embedding matrix, there are
two settings commonly used. In the first setting,
the word embedding matrix is initialized randomly.
This works well in a supervised scenario, in which
a neural network updates the matrix in order to op-
timize some task-specific objectives (Collobert et
al., 2011; Socher et al, 2011c). In the second set-
ting, the word embedding matrix is pre-trained us-
ing an unsupervised neural language model (Bengio
et al, 2003; Collobert and Weston, 2008) with huge
amount of unlabeled data. In this work, we prefer to
the first setting because the word embedding matri-
ces can be trained to minimize errors with respect to
reordering modeling.
There are two kinds of errors involved
1. reconstruction error: how well the learned
vector space representations represent the cor-
responding strings?
2. reordering error: how well the classifier pre-
dicts the merging order?
As described in Section 2.2.2, the input vector
c1 and c2 of a recursive autoencoder can be recon-
structed using Eq. 8 as c?1 and c
?
2. We use Euclidean
distance between the input and the reconstructed
vectors to measure the reconstruction error:
Erec([c1; c2]; ?) =
1
2
?
?[c1; c2]? [c
?
1; c
?
2]
?
?2 . (11)
Given a sentence, there are exponentially many
ways to obtain its vector space representation. Note
that each way corresponds to a binary tree like Fig-
ure 2. To find a binary tree with minimal reconstruc-
tion error, we follow Socher et al (2011c) to use a
greedy algorithm. Taking Figure 2 as an example,
the greedy algorithm begins with computing the re-
construction error Erec(?) for each pair of consecu-
tive vectors, i.e., Erec([x1;x2]; ?), Erec([x2;x3]; ?)
and Erec([x3;x4]; ?). Suppose Erec([x1;x2]; ?) is
the smallest, the algorithm will replace x1 and x2
with their vector representation y1 produced by the
recursive autoencoder. Then, the algorithm evalu-
ates Erec([y1;x3]; ?) and Erec([x3;x4]; ?) and re-
peats the above replacing steps until only one vector
572
remains. Socher et al (2011c) find that the greedy
algorithm runs fast without significant loss in perfor-
mance as compared with CKY-style algorithms.
Given a training example set S = {ti =
(oi, X1i , X
2
i )}, the average reconstruction error on
the source side on the training set is defined as
Erec,s(S; ?) =
1
Ns
?
i
?
p?T ?R(ti,s)
Erec([p.c1, p.c2]; ?)
(12)
where T ?R(ti, s) denotes all the intermediate nodes
on the source side in binary trees, Ns is the num-
ber of these intermediate nodes, and p.ck is the kth
child vector of p. The average reconstruction error
on the target side, denoted by Erec,t(S; ?), can be
computed in a similar way.
Therefore, the reconstruction error is defined as
Erec(S; ?) = Erec,s(S; ?) + Erec,t(S; ?). (13)
Given a training example ti = (oi, X1i , X
2
i ), we
assume the probability distribution dti for its label
is [1, 0] when oi = straight, and [0, 1] when oi =
inverted. Then the cross-entropy error is
Ec(ti; ?) = ?
?
o
dti(o) log
(
P?(o|X
1, X2)
)
(14)
where o ? {straight, inverted}. As a result, the
reordering error is defined as
Ereo(S; ?) =
1
|S|
?
i
Ec(ti; ?). (15)
Therefore, the joint training objective function is
J = ?Erec(S; ?)+(1??)Ereo(S; ?)+R(?) (16)
where ? is a parameter used to balance the prefer-
ence between reconstruction error and reordering er-
ror, R(?) is the regularizer and defined as 2
R(?) =
?L
2
??L?
2 +
?rec
2
??rec?
2 +
?reo
2
??reo?
2 .
(17)
As Socher et al (2011c) stated, a naive way for
lowering the reconstruction error is to make the
magnitude of the hidden layer very small, which is
2The bias terms b(1), b(2) and bo are not regularized. We do
not exclude them from the equation explicitly just for clarity.
not desirable. In order to prevent such behavior, we
normalize all the output vectors of the hidden layers
to have length 1 in the same way as (Socher et al,
2011c). Namely we set p = p||p|| after computing p
as in Eq. 7, and c?1 =
c?1
||c?1||
, c?2 =
c?2
||c?2||
in Eq. 8.
Following Socher et al (2011c), we use L-BFGS
to estimate the parameters with respect to the joint
training objective. Given a set of parameters, we
construct binary trees for all the phrases using the
greedy algorithm. The derivatives for these fixed
binary trees can be computed via backpropagation
through structures (Goller and Kuchler, 1996).
4 Experiments
4.1 Data Preparation
We evaluated our system on Chinese-English trans-
lation. The training corpus contains 1.23M sen-
tence pairs with 32.1M Chinese words and 35.4M
English words. We used SRILM (Stolcke, 2002)
to train a 4-gram language model on the Xinhua
portion of the GIGAWORD corpus, which con-
tains 398.6M words. We used the NIST 2006 MT
Chinese-English dataset as the development set and
NIST 2008 dataset as the test set. The evaluation
metric is case-insensitive BLEU. Because of the ex-
pensive computational cost for training our neural
ITG reordering model, only the reordering exam-
ples extracted from about 1/5 of the entire parallel
training corpus were used to train our neural ITG re-
ordering model.
For the neural ITG reordering model, we set the
dimension of the word embedding vectors to 25 em-
pirically, which is a trade-off between computational
cost and expressive power. We use the early stop-
ping principle to determine when to stop L-BFGS.
The hyper-parameters ?, ?L, ?rec and ?reo are op-
timized by random search (Bergstra and Bengio,
2012). As preliminary experiments show that classi-
fication accuracy has a high correlation with BLEU
score, we optimize these hyper-parameters with re-
spect to classification accuracy instead of BLEU
to reduce computational cost. We randomly select
400,000 reordering examples as training set, 500 as
development set, and another 500 as test set. The
numbers of straight and inverted reordering exam-
ples in the development/test set are set to be equal
to avoid biases. We draw ? uniformly from 0.05
573
System NIST 2006 (tune) NIST 2008
maxent 30.40 23.75
neural 31.61* 24.82*
Table 1: BLEU scores on the NIST 2006 and 2008
datasets. *: significantly better (p < 0.01). ?maxent?
denotes the baseline maximum entropy system and ?neu-
ral? denotes our recursive autoencoder system.
length > = <
[1, 10] 43 121 57
[11, 20] 181 67 164
[21, 30] 170 11 152
[31, 40] 105 3 90
[41, 50] 69 1 53
[51, 119] 40 0 30
Table 2: Number of sentences that our system has a
higher (>), equal (=) or lower (<) sentence-level BLEU-
4 score on the NIST 2008 dataset.
to 0.3, and ?L, ?rec, ?reo exponentially from 10?8
to 10?2. We use the following hyper-parameters in
our experiments: ? = 0.11764, ?L = 7.59 ? 10?5,
?rec = 1.30? 10?5 and ?reo = 3.80? 10?4. 3
The baseline system is a re-implementation of
(Xiong et al, 2006). Our system is different from the
baseline by replacing the MaxEnt reordering model
with a neural model. Both the systems have the same
pruning settings: the threshold pruning parameter is
set to 0.5 and the histogram pruning parameter to
40. For minimum-error-rate training, both systems
generate 200-best lists.
4.2 MT Evaluation
Table 1 shows the case-insensitive BLEU-4 scores
of the baseline system and our system on the devel-
opment and test sets. Our system outperforms the
baseline system by 1.21 BLEU points on the de-
velopment set and 1.07 on the test set. Both the
differences are statistically significant at p = 0.01
level (Riezler and Maxwell, 2005).
Table 2 shows the number of sentences that our
system has a higher (>), equal (=) or lower (<)
BLEU score on the NIST 2008 dataset. We find that
our system is superior to the baseline system for long
3The choice of ? is very important for achieving high BLEU
scores. We tried a number of intervals and found that the clas-
sification accuracy is most stable in the interval [0.100,0.125].
5 10 15 20 25 30 351
90
95
100
88 Length
Accur
acy (%
)
 
 
neuralmaxent
Figure 4: Comparison of reordering classification accu-
racies between the MaxEnt and neural classifiers over
varying phrase lengths. ?Length? denotes the sum of the
lengths of two source phrases in a reordering example.
Our classifier (neural) outperforms the MaxEnt classi-
fier (maxent) consistently, especially for predicting long-
distance reordering.
# of examples NIST 2006 (tune) NIST 2008
100,000 30.88 23.78
200,000 30.75 23.89
400,000 30.80 24.35
800,000 31.01 24.45
6,004,441 31.61 24.82
Table 3: The effect of reordering training data size on
BLEU scores. The BLEU scores rise with the increase of
training data size. Due to the computational cost, we only
used 1/5 of the entire bilingual corpus to train our neural
reordering model.
sentences.
Figure 4 compares classification accuracies of the
neural and MaxEnt classifiers. ?Length? denotes the
sum of the lengths of two source phrases in a re-
ordering example. For each length, we randomly se-
lect 200 unseen reordering examples to calculate the
classification accuracy. Our classifier outperforms
the baseline consistently, especially for long com-
posed blocks.
Xiong et al (2008) find that the performance of
the baseline system can be improved by forbidding
inverted reordering if the phrase length exceeds a
pre-defined distortion limit. This heuristic increases
the BLEU score of the baseline system significantly
to 24.46 but is still significantly worse (p < 0.05)
than our system without the heuristic. We find that
imposing this heuristic fails to improve our system
574
cluster 1 cluster 2 cluster 3 cluster 4 cluster 5
1.18 works for alternative duties these people who of the three
accessibility verify on one-day conference the reasons why on the fundamental
wheelchair tunnels from armed groups the story of how over the entire
candies transparency in chinese language works the system which through its own
cough opinion at eating habits the trend towards with the best
Table 4: Words and phrases that are close in the Euclidean space. The words and phrases in the same cluster have
similar behaviors from a reordering point of view rather than relatedness, suggesting that the vector representations
produced by the recursive autoencoders are helpful for capturing reordering regularities.
significantly. One possible reason is that there is
limited room for improvement as our system makes
fewer wrong predictions for long composed blocks.
The above results suggest that our system does go
beyond using boundary words and make a better use
of the merging blocks by using vector space repre-
sentations.
Table 3 shows the effect of training dataset size
on BLEU scores. We find that BLEU scores on both
the development and test sets rise with the increase
of the training dataset size. As the training process is
very time-consuming, only the reordering examples
extracted from 1/5 of the entire parallel training cor-
pus are used in our experiments to train our model.
Obviously, with more efficient training algorithms,
making full use of all the reordering examples ex-
tracted from the entire corpus will result in better
results. We leave this for future work.
4.3 Qualitative Analysis on Vector
Representations
Table 4 shows a number of words and phrases that
are close (measured by Euclidean distance) in the
n-dimensional space. We randomly select about
370K target side phrases used in our experiments
and cluster them into 983 clusters using k-means al-
gorithm (MacQueen, 1967). The distance between
two phrases are measured by the Euclidean distance
between their vector representations. As shown in
Table 4, cluster 1 mainly consists of nouns, clus-
ter 2 mainly contains verb/noun+preposition struc-
tures, cluster 3 contains compound phrases, cluster
4 consists of phrases which should be followed by
a clause, and cluster 5 mainly contains the begin-
ning parts of prepositional phrases that tend to be
followed by a noun phrase or word. We find that
the words and phrases in the same cluster have sim-
ilar behaviors from a reordering point of view rather
than relatedness. This indicates that the vector rep-
resentations produced by the recursive autoencoders
are helpful for capturing reordering regularities.
5 Conclusion
We have presented an ITG reordering classifier
based on recursive autoencoders. As recursive au-
toencoders are capable of producing vector space
representations for arbitrary multi-word strings in
decoding, our neural ITG system achieves an ab-
solute improvement of 1.07 BLEU points over the
baseline on the NIST 2008 Chinese-English dataset.
There are a number of interesting directions we
would like to pursue in the near future. First, re-
placing the MaxEnt classifier with a neural one re-
defines the conditions for risk-free hypothesis re-
combination. We find that the number of hypothe-
ses that can be recombined reduces in our system.
Therefore, we plan to use forest reranking (Huang,
2008) to alleviate this problem. Second, it is in-
teresting to follow Socher et al (2013) to combine
linguistically-motivated labels with recursive neural
networks. Another problem with our system is that
the decoding speed is much slower than the baseline
system because of the computational overhead intro-
duced by RAEs. It is necessary to investigate more
efficient decoding algorithms. Finally, it is possible
to apply our method to other phrase-based and even
syntax-based systems.
Acknowledgments
This research is supported by the 863 Program un-
der the grant No. 2012AA011102, by the Boeing
Tsinghua Joint Research Project on Language Pro-
cessing (Agreement TBRC-008-SDB-2011 Phase 3
575
(2013)), by the Singapore National Research Foun-
dation under its International Research Centre @
Singapore Funding Initiative and administered by
the IDM Programme Office, and by a Research Fund
No. 20123000007 from Tsinghua MOE-Microsoft
Joint Laboratory. Many thanks go to Chunyang Liu
and Chong Kuang for their great help for setting up
the computing platform. We also thank Min-Yen
Kan, Meng Zhang and Yu Zhao for their insightful
discussions.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion models for statistical machine translation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
529?536, Sydney, Australia, July.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155, March.
James Bergstra and Yoshua Bengio. 2012. Random
search for hyper-parameter optimization. The Jour-
nal of Machine Learning Research, 13(1):281?305,
February.
Arianna Bisazza and Marcello Federico. 2012. Modi-
fied distortion matrices for phrase-based statistical ma-
chine translation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 478?487, Jeju Is-
land, Korea, July.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured embed-
dings of knowledge bases. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelli-
gence, pages 301?306.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words and
meaning representations for open-text semantic pars-
ing. In International Conference on Artificial Intelli-
gence and Statistics, pages 127?135.
Colin Cherry. 2013. Improved reordering for phrase-
based translation using sparse features. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 22?31,
Atlanta, Georgia, June.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. In Proceed-
ings of the 25th International Conference on Machine
Learning, pages 160?167.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural lan-
guage processing (almost) from scratch. Journal of
Machine Learning Research, 12:2493?2537.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics: Posters, pages 285?293, Beijing, China, August.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 848?856, Honolulu, Hawaii, October.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Proceed-
ings of the 28th International Conference on Machine
Learning (ICML-11), pages 513?520.
Christoph Goller and Andreas Kuchler. 1996. Learning
task-dependent distributed representations by back-
propagation through structure. In Proceedings of 1996
IEEE International Conference on Neural Networks
(Volume:1), volume 1, pages 347?352.
Spence Green, Michel Galley, and Christopher D. Man-
ning. 2010. Improved models of distortion cost for
statistical machine translation. In Proceedings of Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 867?875,
Los Angeles, California, June.
Karl Moritz Hermann and Phil Blunsom. 2013. The role
of syntax in vector space models of compositional se-
mantics. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 894?904, Sofia, Bulgaria,
August.
Eric Huang, Richard Socher, Christopher Manning, and
Andrew Ng. 2012. Improving word representations
via global context and multiple word prototypes. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 873?882, Jeju Island, Korea, July.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 586?594, Columbus, Ohio, June.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
576
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June.
James MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of the Fifth Berkeley Symposium on Mathe-
matical Statistics and Probability, volume 1.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics: HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA, May.
Stefan Riezler and John T. Maxwell. 2005. On some
pitfalls in automatic evaluation and significance test-
ing for MT. In Proceedings of the ACL Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 57?
64, Ann Arbor, Michigan, June.
Richard Socher, Eric H. Huang, Jeffrey Pennin, An-
drew Y. Ng, and Christopher D. Manning. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Proceedings of Advances
in Neural Information Processing Systems 24, pages
801?809.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-
pher D. Manning. 2011b. Parsing natural scenes and
natural language with recursive neural networks. In
Proceedings of the 26th International Conference on
Machine Learning (ICML), pages 129?136.
Richard Socher, Jeffrey Pennington, Eric H. Huang, An-
drew Y. Ng, and Christopher D. Manning. 2011c.
Semi-supervised recursive autoencoders for predicting
sentiment distributions. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151?161, Edinburgh, Scot-
land, UK., July.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1201?
1211, Jeju Island, Korea, July.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compositional
vector grammars. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 455?465, Sofia,
Bulgaria, August.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
vol. 2, pages 901?904, September.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the Association for Com-
putational Linguistics: HLT-NAACL 2004: Short Pa-
pers, pages 101?104, Boston, Massachusetts, USA,
May.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 521?528, Sydney,
Australia, July.
Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,
and Shouxun Lin. 2008. Refinements in BTG-based
statistical machine translation. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing: Volume-I, pages 505?512.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li. 2010.
Linguistically annotated reordering: Evaluation and
analysis. Computational Linguistics, 36(3):535?568,
September.
Richard Zens, Hermann Ney, Taro Watanabe, and Ei-
ichiro Sumita. 2004. Reordering constraints for
phrase-based statistical machine translation. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics, pages 205?211, Geneva,
Switzerland, Aug 23?Aug 27.
577
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640?649,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Generating Templates of Entity Summaries
with an Entity-Aspect Model and Pattern Mining
Peng Li1 and Jing Jiang2 and Yinglin Wang1
1Department of Computer Science and Engineering, Shanghai Jiao Tong University
2School of Information Systems, Singapore Management University
{lipeng,ylwang}@sjtu.edu.cn jingjiang@smu.edu.sg
Abstract
In this paper, we propose a novel approach
to automatic generation of summary tem-
plates from given collections of summary
articles. This kind of summary templates
can be useful in various applications. We
first develop an entity-aspect LDA model
to simultaneously cluster both sentences
and words into aspects. We then apply fre-
quent subtree pattern mining on the depen-
dency parse trees of the clustered and la-
beled sentences to discover sentence pat-
terns that well represent the aspects. Key
features of our method include automatic
grouping of semantically related sentence
patterns and automatic identification of
template slots that need to be filled in. We
apply our method on five Wikipedia entity
categories and compare our method with
two baseline methods. Both quantitative
evaluation based on human judgment and
qualitative comparison demonstrate the ef-
fectiveness and advantages of our method.
1 Introduction
In this paper, we study the task of automatically
generating templates for entity summaries. An en-
tity summary is a short document that gives the
most important facts about an entity. In Wikipedia,
for instance, most articles have an introduction
section that summarizes the subject entity before
the table of contents and other elaborate sections.
These introduction sections are examples of en-
tity summaries we consider. Summaries of enti-
ties from the same category usually share some
common structure. For example, biographies of
physicists usually contain facts about the national-
ity, educational background, affiliation and major
contributions of the physicist, whereas introduc-
tions of companies usually list information such
as the industry, founder and headquarter of the
company. Our goal is to automatically construct
a summary template that outlines the most salient
types of facts for an entity category, given a col-
lection of entity summaries from this category.
Such kind of summary templates can be very
useful in many applications. First of all, they
can uncover the underlying structures of summary
articles and help better organize the information
units, much in the same way as infoboxes do in
Wikipedia. In fact, automatic template genera-
tion provides a solution to induction of infobox
structures, which are still highly incomplete in
Wikipedia (Wu and Weld, 2007). A template
can also serve as a starting point for human edi-
tors to create new summary articles. Furthermore,
with summary templates, we can potentially ap-
ply information retrieval and extraction techniques
to construct summaries for new entities automati-
cally on the fly, improving the user experience for
search engine and question answering systems.
Despite its usefulness, the problem has not been
well studied. The most relevant work is by Fila-
tova et al (2006) on automatic creation of domain
templates, where the defintion of a domain is sim-
ilar to our notion of an entity category. Filatova
et al (2006) first identify the important verbs for
a domain using corpus statistics, and then find fre-
quent parse tree patterns from sentences contain-
ing these verbs to construct a domain template.
There are two major limitations of their approach.
First, the focus on verbs restricts the template pat-
terns that can be found. Second, redundant or
related patterns using different verbs to express
the same or similar facts cannot be grouped to-
gether. For example, ?won X award? and ?re-
ceived X prize? are considered two different pat-
terns by this approach. We propose a method that
can overcome these two limitations. Automatic
template generation is also related to a number of
other problems that have been studied before, in-
640
cluding unsupervised IE pattern discovery (Sudo
et al, 2003; Shinyama and Sekine, 2006; Sekine,
2006; Yan et al, 2009) and automatic generation
of Wikipedia articles (Sauper and Barzilay, 2009).
We discuss the differences of our work from exist-
ing related work in Section 6.
In this paper we propose a novel approach to
the task of automatically generating entity sum-
mary templates. We first develop an entity-aspect
model that extends standard LDA to identify clus-
ters of words that can represent different aspects
of facts that are salient in a given summary col-
lection (Section 3). For example, the words ?re-
ceived,? ?award,? ?won? and ?Nobel? may be
clustered together from biographies of physicists
to represent one aspect, even though they may ap-
pear in different sentences from different biogra-
phies. Simultaneously, the entity-aspect model
separates words in each sentence into background
words, document words and aspect words, and
sentences likely about the same aspect are natu-
rally clustered together. After this aspect identi-
fication step, we mine frequent subtree patterns
from the dependency parse trees of the clustered
sentences (Section 4). Different from previous
work, we leverage the word labels assigned by the
entity-aspect model to prune the patterns and to
locate template slots to be filled in.
We evaluate our method on five entity cate-
gories using Wikipedia articles (Section 5). Be-
cause the task is new and thus there is no stan-
dard evaluation criteria, we conduct both quanti-
tative evaluation using our own human judgment
and qualitative comparison. Our evaluation shows
that our method can obtain better sentence patterns
in terms of f1 measure compared with two baseline
methods, and it can also achieve reasonably good
quality of aspect clusters in terms of purity. Com-
pared with standard LDA and K-means sentence
clustering, the aspects identified by our method are
also more meaningful.
2 The Task
Given a collection of entity summaries from the
same entity category, our task is to automatically
construct a summary template that outlines the
most important information one should include in
a summary for this entity category. For example,
given a collection of biographies of physicists, ide-
ally the summary template should indicate that im-
portant facts about a physicist include his/her ed-
Aspect Pattern
ENT received his phd from ? university
1 ENT studied ? under ?
ENT earned his ? in physics from university of
?
ENT was awarded the medal in ?
2 ENT won the ? award
ENT received the nobel prize in physics in ?
ENT was ? director
3 ENT was the head of ?
ENT worked for ?
ENT made contributions to ?
4 ENT is best known for work on ?
ENT is noted for ?
Table 1: Examples of some good template patterns
and their aspects generated by our method.
ucational background, affiliation, major contribu-
tions, awards received, etc.
However, it is not clear what is the best repre-
sentation of such templates. Should a template
comprise a list of subtopic labels (e.g. ?educa-
tion? and ?affiliation?) or a set of explicit ques-
tions? Here we define a template format based on
the usage of the templates as well as our obser-
vations from Wikipedia entity summaries. First,
since we expect that the templates can be used by
human editors for creating new summaries, we use
sentence patterns that are human readable as basic
units of the templates. For example, we may have
a sentence pattern ?ENT graduated from ? Uni-
versity? for the entity category ?physicist,? where
ENT is a placeholder for the entity that the sum-
mary is about, and ??? is a slot to be filled in. Sec-
ond, we observe that information about entities of
the same category can be grouped into subtopics.
For example, the sentences ?Bohr is a Nobel lau-
reate? and ?Einstein received the Nobel Prize? are
paraphrases of the same type of facts, while the
sentences ?Taub earned his doctorate at Prince-
ton University? and ?he graduated from MIT? are
slightly different but both describe a person?s ed-
ucational background. Therefore, it makes sense
to group sentence patterns based on the subtopics
they pertain to. Here we call these subtopics the
aspects of a summary template.
Formally, we define a summary template to be a
set of sentence patterns grouped into aspects. Each
sentence pattern has a placeholder for the entity to
be summarized and possibly one or more template
slots to be filled in. Table 1 shows some sentence
patterns our method has generated for the ?physi-
cist? category.
641
2.1 Overview of Our Method
Our automatic template generation method con-
sists of two steps:
Aspect Identification: In this step, our goal is
to automatically identify the different aspects or
subtopics of the given summary collection. We si-
multaneously cluster sentences and words into as-
pects, using an entity-aspect model extended from
the standard LDA model that is widely used in
text mining (Blei et al, 2003). The output of this
step are sentences clustered into aspects, with each
word labeled as a stop word, a background word,
a document word or an aspect word.
Sentence Pattern Generation: In this step, we
generate human-readable sentence patterns to rep-
resent each aspect. We use frequent subtree pat-
tern mining to find the most representative sen-
tence structures for each aspect. The fixed struc-
ture of a sentence pattern consists of aspect words,
background words and stop words, while docu-
ment words become template slots whose values
can vary from summary to summary.
3 Aspect Identification
At the aspect identification step, our goal is to dis-
cover the most salient aspects or subtopics con-
tained in a summary collection. Here we propose
a principled method based on a modified LDA
model to simultaneously cluster both sentences
and words to discover aspects.
We first make the following observation. In en-
tity summaries such as the introduction sections
of Wikipedia articles, most sentences are talk-
ing about a single fact of the entity. If we look
closely, there are a few different kinds of words in
these sentences. First of all, there are stop words
that occur frequently in any document collection.
Second, for a given entity category, some words
are generally used in all aspects of the collection.
Third, some words are clearly associated with the
aspects of the sentences they occur in. And finally,
there are also words that are document or entity
specific. For example, in Table 2 we show two
sentences related to the ?affiliation? aspect from
the ?physicist? summary collection. Stop words
such as ?is? and ?the? are labeled with ?S.? The
word ?physics? can be regarded as a background
word for this collection. ?Professor? and ?univer-
sity? are clearly related to the ?affiliation? aspect.
Finally words such as ?Modena? and ?Chicago?
are specifically associated with the subject enti-
ties being discussed, that is, they are specific to
the summary documents.
To capture background words and document-
specific words, Chemudugunta et al (2007)
proposed to introduce a background topic and
document-specific topics. Here we borrow their
idea and also include a background topic as well
as document-specific topics. To discover aspects
that are local to one or a few adjacent sentences but
may occur in many documents, Titov and McDon-
ald (2008) proposed a multi-grain topic model,
which relies on word co-occurrences within short
paragraphs rather than documents in order to dis-
cover aspects. Inspired by their model, we rely
on word co-occurrences within single sentences to
identify aspects.
3.1 Entity-Aspect Model
We now formally present our entity-aspect model.
First, we assume that stop words can be identified
using a standard stop word list. We then assume
that for a given entity category there are three
kinds of unigram language models (i.e. multino-
mial word distributions). There is a background
model ?B that generates words commonly used
in all documents and all aspects. There are D
document models ?d (1 ? d ? D), where D
is the number of documents in the given sum-
mary collection, and there are A aspect models ?a
(1 ? a ? A), where A is the number of aspects.
We assume that these word distributions have a
uniform Dirichlet prior with parameter ?.
Since not all aspects are discussed equally fre-
quently, we assume that there is a global aspect
distribution ? that controls how often each aspect
occurs in the collection. ? is sampled from another
Dirichlet prior with parameter ?. There is also a
multinomial distribution pi that controls in each
sentence how often we encounter a background
word, a document word, or an aspect word. pi has
a Dirichlet prior with parameter ?.
Let Sd denote the number of sentences in doc-
ument d, Nd,s denote the number of words (after
stop word removal) in sentence s of document d,
and wd,s,n denote the n?th word in this sentence.
We introduce hidden variables zd,s for each sen-
tence to indicate the aspect a sentence belongs to.
We also introduce hidden variables yd,s,n for each
word to indicate whether a word is generated from
the background model, the document model, or
the aspect model. Figure 1 shows the process of
642
Venturi/D is/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Modena/D ./S
He/S was/S a/S professor/A of/S physics/B at/S the/S University/A of/S
Chicago/D until/S 1982/D ./S
Table 2: Two sentences on ?affiliation? from the ?physicist? entity category. S: stop word. B: background
word. A: aspect word. D: document word.
1. Draw ? ? Dir(?), ?B ? Dir(?), pi ? Dir(?)
2. For each aspect a = 1, . . . , A,
(a) draw ?a ? Dir(?)
3. For each document d = 1, . . . , D,
(a) draw ?d ? Dir(?)
(b) for each sentence s = 1, . . . , Sd
i. draw zd,s ? Multi(?)
ii. for each word n = 1, . . . , Nd,s
A. draw yd,s,n ? Multi(pi)
B. draw wd,s,n ? Multi(?B) if yd,s,n = 1,
wd,s,n ? Multi(?d) if yd,s,n = 2, or
wd,s,n ? Multi(?zd,s) if yd,s,n = 3
Figure 1: The document generation process.
y z
?pi
? ?
?
? A
dSD sdN ,
B?
?
w
Figure 2: The entity-aspect model.
generating the whole document collection. The
plate notation of the model is shown in Figure 2.
Note that the values of ?, ? and ? are fixed. The
number of aspects A is also manually set.
3.2 Inference
Given a summary collection, i.e. the set of all
wd,s,n, our goal is to find the most likely assign-
ment of zd,s and yd,s,n, that is, the assignment that
maximizes p(z,y|w;?, ?, ?), where z, y and w rep-
resent the set of all z, y and w variables, respec-
tively. With the assignment, sentences are natu-
rally clustered into aspects, and words are labeled
as either a background word, a document word, or
an aspect word.
We approximate p(y, z|w;?, ?, ?) by
p(y,z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i), where ??B,
{??d}Dd=1, {??a}Aa=1, ?? and p?i are estimated using
Gibbs sampling, which is commonly used for
inference for LDA models (Griffiths and Steyvers,
2004). Due to space limit, we give the formulas
for the Gibbs sampler below without derivation.
First, given sentence s in document d, we sam-
ple a value for zd,s given the values of all other z
and y variables using the following formula:
p(zd,s = a|z?{d,s},y,w)
? C
A
(a) + ?
CA(?) +A?
?
?V
v=1
?E(v)
i=0 (Ca(v) + i+ ?)?E(?)
i=0 (Ca(?) + i+ V ?)
.
In the formula above, z?{d,s} is the current aspect
assignment of all sentences excluding the current
sentence. CA(a) is the number of sentences assigned
to aspect a, and CA(?) is the total number of sen-
tences. V is the vocabulary size. Ca(v) is the num-
ber of times word v has been assigned to aspect
a. Ca(?) is the total number of words assigned to
aspect a. All the counts above exclude the current
sentence. E(v) is the number of times word v oc-
curs in the current sentence and is assigned to be
an aspect word, as indicated by y, and E(?) is the
total number of words in the current sentence that
are assigned to be an aspect word.
We then sample a value for yd,s,n for each word
in the current sentence using the following formu-
las:
p(yd,s,n = 1|z,y?{d,s,n}) ?
Cpi(1) + ?
Cpi(?) + 3?
?
CB(wd,s,n) + ?
CB(?) + V ?
,
p(yd,s,n = 2|z,y?{d,s,n}) ?
Cpi(2) + ?
Cpi(?) + 3?
?
Cd(wd,s,n) + ?
Cd(?) + V ?
,
p(yd,s,n = 3|z,y?{d,s,n}) ?
Cpi(3) + ?
Cpi(?) + 3?
?
Ca(wd,s,n) + ?
Ca(?) + V ?
.
In the formulas above, y?{d,s,n} is the set of all y
variables excluding yd,s,n. Cpi(1), Cpi(2) and Cpi(3) are
the numbers of words assigned to be a background
word, a document word, or an aspect word, respec-
tively, and Cpi(?) is the total number of words. CB
and Cd are counters similar to Ca but are for the
background model and the document models. In
all these counts, the current word is excluded.
With one Gibbs sample, we can make the fol-
lowing estimation:
643
??Bv =
CB(v) + ?
CB(?) + V ?
, ??dv =
Cd(v) + ?
Cd(?) + V ?
, ??av =
Ca(v) + ?
Ca(?) + V ?
,
??a =
CA(a) + ?
CA(?) +A?
, p?it =
Cpi(t) + ?
Cpi(?) + 3?
(1 ? t ? 3).
Here the counts include all sentences and all
words.
In our experiments, we set ? = 5, ? = 0.01 and
? = 20. We run 100 burn-in iterations through all
documents in a collection to stabilize the distri-
bution of z and y before collecting samples. We
found that empirically 100 burn-in iterations were
sufficient for our data set. We take 10 samples with
a gap of 10 iterations between two samples, and
average over these 10 samples to get the estima-
tion for the parameters.
After estimating ??B, {??d}Dd=1, {??a}Aa=1, ?? and p?i,
we find the values of each zd,s and yd,s,n that max-
imize p(y, z|w; ??B, {??d}Dd=1, {??a}Aa=1, ??, p?i). This as-
signment, together with the standard stop word list
we use, gives us sentences clustered into A as-
pects, where each word is labeled as either a stop
word, a background word, a document word or an
aspect word.
3.3 Comparison with Other Models
A major difference of our entity-aspect model
from standard LDA model is that we assume each
sentence belongs to a single aspect while in LDA
words in the same sentence can be assigned to
different topics. Our one-aspect-per-sentence as-
sumption is important because our goal is to clus-
ter sentences into aspects so that we can mine
common sentence patterns for each aspect.
To cluster sentences, we could have used a
straightforward solution similar to document clus-
tering, where sentences are represented as feature
vectors using the vector space model, and a stan-
dard clustering algorithm such as K-means can
be applied to group sentences together. However,
there are some potential problems with directly ap-
plying this typical document clustering method.
First, unlike documents, sentences are short, and
the number of words in a sentence that imply its
aspect is even smaller. Besides, we do not know
the aspect-related words in advance. As a result,
the cosine similarity between two sentences may
not reflect whether they are about the same aspect.
We can perform heuristic term weighting, but the
method becomes less robust. Second, after sen-
tence clustering, we may still want to identify the
the aspect words in each sentence, which are use-
ful in the next pattern mining step. Directly taking
the most frequent words from each sentence clus-
ter as aspect words may not work well even af-
ter stop word removal, because there can be back-
ground words commonly used in all aspects.
4 Sentence Pattern Generation
At the pattern generation step, we want to iden-
tify human-readable sentence patterns that best
represent each cluster. Following the basic idea
from (Filatova et al, 2006), we start with the parse
trees of sentences in each cluster, and apply a
frequent subtree pattern mining algorithm to find
sentence structures that have occurred at least K
times in the cluster. Here we use dependency parse
trees.
However, different from (Filatova et al, 2006),
the word labels (S, B, D and A) assigned by the
entity-aspect model give us some advantages. In-
tuitively, a representative sentence pattern for an
aspect should contain at least one aspect word. On
the other hand, document words are entity-specific
and therefore should not appear in the generic tem-
plate patterns; instead, they correspond to tem-
plate slots that need to be filled in. Furthermore,
since we work on entity summaries, in each sen-
tence there is usually a word or phrase that refers
to the subject entity, and we should have a place-
holder for the subject entity in each pattern.
Based on the intuitions above, we have the fol-
lowing sentence pattern generation process.
1. Locate subject entities: In each sentence, we
want to locate the word or phrase that refers to the
subject entity. For example, in a biography, usu-
ally a pronoun ?he? or ?she? is used to refer to
the subject person. We use the following heuristic
to locate the subject entities: For each summary
document, we first find the top 3 frequent base
noun phrases that are subjects of sentences. For
example, in a company introduction, the phrase
?the company? is probably used frequently as a
sentence subject. Then for each sentence, we first
look for the title of the Wikipedia article. If it oc-
curs, it is tagged as the subject entity. Otherwise,
we check whether one of the top 3 subject base
noun phrases occurs, and if so, it is tagged as the
subject entity. Otherwise, we tag the subject of the
sentence as the subject entity. Finally, for the iden-
tified subject entity word or phrase, we replace the
label assigned by the entity-aspect model with a
644
professor_A
is_SENT a_S
physics_B university_A
?the_S
nsubj
cop
det
prep_of
det
prep_at
prep_of
Figure 3: An example labeled dependency parse
tree.
new label E.
2. Generate labeled parse trees: We parse each
sentence using the Stanford Parser1. After parsing,
for each sentence we obtain a dependency parse
tree where each node is a single word and each
edge is labeled with a dependency relation. Each
word is also labeled with one of {E, S, B, D,
A}. We replace words labeled with E by a place-
holder ENT, and replace words labeled with D by
a question mark to indicate that these correspond
to template slots. For the other words, we attach
their labels to the tree nodes. Figure 3 shows an
example labeled dependency parse tree.
3. Mine frequent subtree patterns: For the set
of parse trees in each cluster, we use FREQT2, a
software that implements the frequent subtree pat-
tern mining algorithm proposed in (Zaki, 2002), to
find all subtrees with a minimum support of K.
4. Prune patterns: We remove subtree patterns
found by FREQT that do not contain ENT or any
aspect word. We also remove small patterns that
are contained in some other larger pattern in the
same cluster.
5. Covert subtree patterns to sentence patterns:
The remaining patterns are still represented as sub-
trees. To covert them back to human-readable sen-
tence patterns, we map each pattern back to one of
the sentences that contain the pattern to order the
tree nodes according to their original order in the
sentence.
In the end, for each summary collection, we ob-
tain A clusters of sentence patterns, where each
cluster presumably corresponds to a single aspect
or subtopic.
1http://nlp.stanford.edu/software/
lex-parser.shtml
2http://chasen.org/?taku/software/
freqt/
Category D S Sd
min max avg
US Actress 407 1721 1 21 4
Physicist 697 4238 1 49 6
US CEO 179 1040 1 24 5
US Company 375 2477 1 36 6
Restaurant 152 1195 1 37 7
Table 3: The number of documents (D), total
number of sentences (S) and minimum, maximum
and average numbers of sentences per document
(Sd) of the data set.
5 Evaluation
Because we study a non-standard task, there is no
existing annotated data set. We therefore created a
small data set and made our own human judgment
for quantitative evaluation purpose.
5.1 Data
We downloaded five collections of Wikipedia ar-
ticles from different entity categories. We took
only the introduction sections of each article (be-
fore the tables of contents) as entity summaries.
Some statistics of the data set are given in Table 3.
5.2 Quantitative Evaluation
To quantitatively evaluate the summary templates,
we want to check (1) whether our sentence pat-
terns are meaningful and can represent the corre-
sponding entity categories well, and (2) whether
semantically related sentence patterns are grouped
into the same aspect. It is hard to evaluate both
together. We therefore separate these two criteria.
5.2.1 Quality of sentence patterns
To judge the quality of sentence patterns without
looking at aspect clusters, ideally we want to com-
pute the precision and recall of our patterns, that
is, the percentage of our sentence patterns that are
meaningful, and the percentage of true meaningful
sentence patterns of each category that our method
can capture. The former is relatively easy to obtain
because we can ask humans to judge the quality of
our patterns. The latter is much harder to com-
pute because we need human judges to find the set
of true sentence patterns for each entity category,
which can be very subjective.
We adopt the following pooling strategy bor-
rowed from information retrieval. Assume we
want to compare a number of methods that each
can generate a set of sentence patterns from a sum-
mary collection. We take the union of these sets
645
of patterns generated by the different methods and
order them randomly. We then ask a human judge
to decide whether each sentence pattern is mean-
ingful for the given category. We can then treat
the set of meaningful sentence patterns found by
the human judge this way as the ground truth, and
precision and recall of each method can be com-
puted. If our goal is only to compare the different
methods, this pooling strategy should suffice.
We compare our method with the following two
baseline methods.
Baseline 1: In this baseline, we use the same
subtree pattern mining algorithm to find sentence
patterns from each summary collection. We also
locate the subject entities and replace them with
ENT. However, we do not have aspect words or
document words in this case. Therefore we do not
prune any pattern except to merge small patterns
with the large ones that contain them. The pat-
terns generated by this method do not have tem-
plate slots.
Baseline 2: In the second baseline, we apply a
verb-based pruning on the patterns generated by
the first baseline, similar to (Filatova et al, 2006).
We first find the top-20 verbs using the scoring
function below that is taken from (Filatova et al,
2006), and then prune patterns that do not contain
any of the top-20 verbs.
s(vi) = N(vi)?
vj?V N(vj)
? M(vi)D ,
where N(vi) is the frequency of verb vi in the
collection, V is the set of all verbs, D is the total
number of documents in the collection, and M(vi)
is the number of documents in the collection that
contains vi.
In Table 4, we show the precision, recall and f1
of the sentence patterns generated by our method
and the two baseline methods for the five cate-
gories. For our method, we set the support of
the subtree patterns K to 2, that is, each pattern
has occurred in at least two sentences in the cor-
responding aspect cluster. For the two baseline
methods, because sentences are not clustered, we
use a larger support K of 3; otherwise, we find
that there can be too many patterns. We can see
that overall our method gives better f1 measures
than the two baseline methods for most categories.
Our method achieves a good balance between pre-
cision and recall. For BL-1, the precision is high
but recall is low. Intuitively BL-1 should have a
higher recall than our method because our method
Category B Purity
US Actress 4 0.626
Physicist 6 0.714
US CEO 4 0.674
US Company 4 0.614
Restaurant 3 0.587
Table 5: The true numbers of aspects as judged
by the human annotator (B), and the purity of the
clusters.
does more pattern pruning than BL-1 using aspect
words. Here it is not the case mainly because we
used a higher frequency threshold (K = 3) to se-
lect frequent patterns in BL-1, giving overall fewer
patterns than in our method. For BL-2, the preci-
sion is higher than BL-1 but recall is lower. It is
expected because the patterns of BL-2 is a subset
of that of BL-1.
There are some advantages of our method that
are not reflected in Table 4. First, many of our pat-
terns contain template slots, which make the pat-
tern more meaningful. In contrast the baseline pat-
terns do not contain template slots. Because the
human judge did not give preference over patterns
with slots, both ?ENT won the award? and ?ENT
won the ? award? were judged to be meaningful
without any distinction, although the former one
generated by our method is more meaningful. Sec-
ond, compared with BL-2, our method can obtain
patterns that do not contain a non-auxiliary verb,
such as ?ENT was ? director.?
5.2.2 Quality of aspect clusters
We also want to judge the quality of the aspect
clusters. To do so, we ask the human judge to
group the ground truth sentence patterns of each
category based on semantic relatedness. We then
compute the purity of the automatically generated
clusters against the human judged clusters using
purity. The results are shown in Table 5. In our
experiments, we set the number of clusters A used
in the entity-aspect model to be 10. We can see
from Table 5 that our generated aspect clusters can
achieve reasonably good performance.
5.3 Qualitative evaluation
We also conducted qualitative comparison be-
tween our entity-aspect model and standard LDA
model as well as a K-means sentence clustering
method. In Table 6, we show the top 5 fre-
quent words of three sample aspects as found by
our method, standard LDA, and K-means. Note
that although we try to align the aspects, there is
646
Category
Method US Actress Physicist US CEO US Company Restaurant
BL-1 precision 0.714 0.695 0.778 0.622 0.706
recall 0.545 0.300 0.367 0.425 0.361
f1 0.618 0.419 0.499 0.505 0.478
BL-2 precision 0.845 0.767 0.829 0.809 1.000
recall 0.260 0.096 0.127 0.167 0.188
f1 0.397 0.17 0.220 0.276 0.316
Ours precision 0.544 0.607 0.586 0.450 0.560
recall 0.710 0.785 0.712 0.618 0.701
f1 0.616 0.684 0.643 0.520 0.624
Table 4: Quality of sentence patterns in terms of precision, recall and f1.
Method Sample Aspects
1 2 3
Our university prize academy
entity- received nobel sciences
aspect ph.d. physics member
model college awarded national
degree medal society
Standard physics nobel physics
LDA american prize institute
professor physicist research
received awarded member
university john sciences
K-means physics physicist physics
university american academy
institute physics sciences
work university university
research nobel new
Table 6: Comparison of the top 5 words of three
sample aspects using different methods.
no correspondence between clusters numbered the
same but generated by different methods.
We can see that our method gives very mean-
ingful aspect clusters. Standard LDA also gives
meaningful words, but background words such
as ?physics? and ?physicist? are mixed with as-
pect words. Entity-specific words such as ?john?
also appear mixed with aspect words. K-means
clusters are much less meaningful, with too many
background words mixed with aspect words.
6 Related Work
The most related existing work is on domain tem-
plate generation by Filatova et al (2006). There
are several differences between our work and
theirs. First, their template patterns must contain a
non-auxiliary verb whereas ours do not have this
restriction. Second, their verb-centered patterns
are independent of each other, whereas we group
semantically related patterns into aspects, giving
more meaningful templates. Third, in their work,
named entities, numbers and general nouns are
treated as template slots. In our method, we ap-
ply the entity-aspect model to automatically iden-
tify words that are document-specific, and treat
these words as template slots, which can be poten-
tially more robust as we do not rely on the quality
of named entity recognition. Last but not least,
their documents are event-centered while ours are
entity-centered. Therefore we can use heuristics to
anchor our patterns on the subject entities.
Sauper and Barzilay (2009) proposed a frame-
work to learn to automatically generate Wikipedia
articles. There is a fundamental difference be-
tween their task and ours. The articles they gen-
erate are long, comprehensive documents consist-
ing of several sections on different subtopics of
the subject entity, and they focus on learning the
topical structures from complete Wikipedia arti-
cles. We focus on learning sentence patterns of the
short, concise introduction sections of Wikipedia
articles.
Our entity-aspect model is related to a num-
ber of previous extensions of LDA models.
Chemudugunta et al (2007) proposed to intro-
duce a background topic and document-specific
topics. Our background and document language
models are similar to theirs. However, they still
treat documents as bags of words rather than sets
of sentences as in our model. Titov and McDon-
ald (2008) exploited the idea that a short paragraph
within a document is likely to be about the same
aspect. Our one-aspect-per-sentence assumption
is a stricter than theirs, but it is required in our
model for the purpose of mining sentence patterns.
The way we separate words into stop words, back-
ground words, document words and aspect words
bears similarity to that used in (Daume? III and
Marcu, 2006; Haghighi and Vanderwende, 2009),
but their task is multi-document summarization
while ours is to induce summary templates.
647
7 Conclusions and Future Work
In this paper, we studied the task of automati-
cally generating templates for entity summaries.
We proposed an entity-aspect model that can auto-
matically cluster sentences and words into aspects.
The model also labels words in sentences as either
a stop word, a background word, a document word
or an aspect word. We then applied frequent sub-
tree pattern mining to generate sentence patterns
that can represent the aspects. We took advan-
tage of the labels generated by the entity-aspect
model to prune patterns and to locate template
slots. We conducted both quantitative and qualita-
tive evaluation using five collections of Wikipedia
entity summaries. We found that our method gave
overall better template patterns than two baseline
methods, and the aspect clusters generated by our
method are reasonably good.
There are a number of directions we plan to pur-
sue in the future in order to improve our method.
First, we can possibly apply linguistic knowledge
to improve the quality of sentence patterns. Cur-
rently the method may generate similar sentence
patterns that differ only slightly, e.g. change of a
preposition. Also, the sentence patterns may not
form complete, meaningful sentences. For exam-
ple, a sentence pattern may contain an adjective
but not the noun it modifies. We plan to study
how to use linguistic knowledge to guide the con-
struction of sentence patterns and make them more
meaningful. Second, we have not quantitatively
evaluated the quality of the template slots, because
our judgment is only at the whole sentence pattern
level. We plan to get more human judges and more
rigorously judge the relevance and usefulness of
both the sentence patterns and the template slots.
It is also possible to introduce certain rules or con-
straints to selectively form template slots rather
than treating all words labeled with D as template
slots.
Acknowledgments
This work was done during Peng Li?s visit to the
Singapore Management University. This work
was partially supported by the National High-tech
Research and Development Project of China (863)
under the grant number 2009AA04Z106 and the
National Science Foundation of China (NSFC) un-
der the grant number 60773088. We thank the
anonymous reviewers for their helpful comments.
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling general and specific as-
pects of documents with a probabilistic topic model.
In Advances in Neural Information Processing Sys-
tems 19, pages 241?248.
Hal Daume? III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 305?312.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of do-
main templates. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 207?214.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl. 1):5228?5235.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 362?370.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In Proceedings of 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 731?738.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 304?311.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic IE pattern acquisition. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 224?
231.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
648
Proceeding of the 17th International Conference on
World Wide Web, pages 111?120.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying Wikipedia. In Proceedings of the 16th
ACM Conference on Information and Knowledge
Management, pages 41?50.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining Wikipedia texts using
information from the Web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1021?1029.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. In Proceedings of the 8th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 71?80.
649
