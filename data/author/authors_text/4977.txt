Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 489?496
Manchester, August 2008
PNR2: Ranking Sentences with Positive and Negative Reinforcement 
for Query-Oriented Update Summarization 
 
 Abstract 
Query-oriented update summarization is 
an emerging summarization task very 
recently. It brings new challenges to the 
sentence ranking algorithms that require 
not only to locate the important and 
query-relevant information, but also to 
capture the new information when 
document collections evolve. In this 
paper, we propose a novel graph based 
sentence ranking algorithm, namely PNR2, 
for update summarization. Inspired by the 
intuition that ?a sentence receives a 
positive influence from the sentences that 
correlate to it in the same collection, 
whereas a sentence receives a negative 
influence from the sentences that 
correlates to it in the different (perhaps 
previously read) collection?, PNR2 
models both the positive and the negative 
mutual reinforcement in the ranking 
process. Automatic evaluation on the 
DUC 2007 data set pilot task 
demonstrates the effectiveness of the 
algorithm.  
 
1 Introduction 
The explosion of the WWW has brought with it a 
vast board of information. It has become virtually 
impossible for anyone to read and understand 
large numbers of individual documents that are 
abundantly available. Automatic document 
summarization provides an effective means to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
manage such an exponentially increased 
collection of information and to support 
information seeking and condensing goals.  
The main evaluation forum that provides 
benchmarks for researchers working on 
document summarization to exchange their ideas 
and experiences is the Document Understanding 
Conferences (DUC). The goals of the DUC 
evaluations are to enable researchers to 
participate in large-scale experiments upon the 
standard benchmark and to increase the 
availability of appropriate evaluation techniques. 
Over the past years, the DUC evaluations have 
evolved gradually from single-document 
summarization to multi-document summarization 
and from generic summarization to query-
oriented summarization. Query-oriented multi-
document summarization initiated in 2005 aims 
to produce a short and concise summary for a 
collection of topic relevant documents according 
to a given query that describes a user?s particular 
interests. 
Previous summarization tasks are all targeted 
on a single document or a static collection of 
documents on a given topic. However, the 
document collections can change (actually grow) 
dynamically when the topic evolves over time. 
New documents are continuously added into the 
topic during the whole lifecycle of the topic and 
normally they bring the new information into the 
topic. To cater for the need of summarizing a 
dynamic collection of documents, the DUC 
evaluations piloted update summarization in 2007. 
The task of update summarization differs from 
previous summarization tasks in that the latter 
aims to dig out the salient information in a topic 
while the former cares the information not only 
salient but also novel.  
Up to the present, the predominant approaches 
in document summarization regardless of the 
nature and the goals of the tasks have still been 
built upon the sentence extraction framework. 
Li Wenjie1, Wei Furu1,2, Lu Qin1, He Yanxiang2 
1Department of Computing 
The Hong Kong Polytechnic University, HK 
{csfwei, cswjli, csluqin} 
@comp.polyu.edu.hk 
2Department of Computer Science 
and Technology, Wuhan University, China 
{frwei, yxhe@whu.edu.cn} 
489
Under this framework, sentence ranking is the 
issue of most concern. In general, two kinds of 
sentences need to be evaluated in update 
summarization, i.e. the sentences in an early (old) 
document collection A (denoted by SA) and the 
sentences in a late (new) document collection B 
(denoted by SB). Given the changes from SA to SB, 
an update summarization approach may be 
concerned about four ranking issues: (1) rank SA 
independently; (2) re-rank SA after SB comes; (3) 
rank SB independently; and (4) rank SB given that 
SA is provided. Among them, (4) is of most 
concern. It should be noting that both (2) and (4) 
need to consider the influence from the sentences 
in the same and different collections.  
In this study, we made an attempt to capture 
the intuition that  
?A sentence receives a positive influence from 
the sentences that correlate to it in the same 
collection, whereas a sentence receives a 
negative influence from the sentences that 
correlates to it in the different collection.? 
We represent the sentences in A or B as a text 
graph constructed using the same approach as 
was used in Erkan and Radev (2004a, 2004b). 
Different from the existing PageRank-like 
algorithms adopted in document summarization, 
we propose a novel sentence ranking algorithm, 
called PNR2 (Ranking with Positive and Negative 
Reinforcement). While PageRank models the 
positive mutual reinforcement among the 
sentences in the graph, PNR2 is capable of 
modeling both positive and negative 
reinforcement in the ranking process.  
The remainder of this paper is organized as 
follows. Section 2 introduces the background of 
the work presented in this paper, including 
existing graph-based summarization models, 
descriptions of update summarization and time-
based ranking solutions with web graph and text 
graph. Section 3 then proposes PNR2, a sentence 
ranking algorithm based on positive and negative 
reinforcement and presents a query-oriented 
update summarization model. Next, Section 4 
reports experiments and evaluation results. 
Finally, Section 5 concludes the paper. 
2 Background and Related Work 
2.1 Previous Work in Graph-based 
Document Summarization 
Graph-based ranking algorithms such as 
Google?s PageRank (Brin and Page, 1998) and 
Kleinberg?s HITS (Kleinberg, 1999) have been 
successfully used in the analysis of the link 
structure of the WWW. Now they are springing 
up in the community of document summarization. 
The major concerns in graph-based 
summarization researches include how to model 
the documents using text graph and how to 
transform existing web page ranking algorithms 
to their variations that could accommodate 
various summarization requirements. 
Erkan and Radev (2004a and 2004b) 
represented the documents as a weighted 
undirected graph by taking sentences as vertices 
and cosine similarity between sentences as the 
edge weight function. An algorithm called 
LexRank, adapted from PageRank, was applied 
to calculate sentence significance, which was 
then used as the criterion to rank and select 
summary sentences. Meanwhile, Mihalcea and 
Tarau (2004) presented their PageRank variation, 
called TextRank, in the same year. Besides, they 
reported experimental comparison of three 
different graph-based sentence ranking 
algorithms obtained from Positional Power 
Function, HITS and PageRank (Mihalcea and 
Tarau, 2005). Both HITS and PageRank 
performed excellently. 
Likewise, the use of PageRank family was also 
very popular in event-based summarization 
approaches (Leskovec et al, 2004; Vanderwende 
et al, 2004; Yoshioka and Haraguchi, 2004; Li et 
al., 2006). In contrast to conventional sentence-
based approaches, newly emerged event-based 
approaches took event terms, such as verbs and 
action nouns and their associated named entities 
as graph nodes, and connected nodes according 
to their co-occurrence information or semantic 
dependency relations. They were able to provide 
finer text representation and thus could be in 
favor of sentence compression which was 
targeted to include more informative contents in a 
fixed-length summary. Nevertheless, these 
advantages lied on appropriately defining and 
selecting event terms.  
All above-mentioned representative work was 
concerned with generic summarization. Later on, 
graph-based ranking algorithms were introduced 
in query-oriented summarization too when this 
new challenge became a hot research topic 
recently. For example, a topic-sensitive version 
of PageRank was proposed in (OtterBacher et al, 
2005). The same algorithm was followed by Wan 
et al (2006) and Lin et al (2007) who further 
investigated on its application in query-oriented 
update summarization.  
490
2.2 The DUC 2007 Update Summarization 
Task Description 
The DUC 2007 update summarization pilot task 
is to create short (100 words) multi-document 
summaries under the assumption that the reader 
has already read some number of previous 
documents. Each of 10 topics contains 25 
documents. For each topic, the documents are 
sorted in chronological order and then partitioned 
into three collections, ?A?, ?B? and ?C?. The 
participants are then required to generate (1) a 
summary for ?A?; (2) an update summary for 
?B? assuming documents in ?A? have already 
been read; and (3) an update summary for ?C? 
assuming documents in ?A? and ?B? have 
already been read. Growing out of the DUC 2007, 
the Text Analysis Conference (TAC) 2008 
planed to keep only the DUC 2007 task (1) and 
(2). 
Each topic collection in the DUC 2007 (will 
also in the TAC 2008) is accompanied with a 
query that describes a user?s interests and focuses. 
System-generated summaries should include as 
many responses relevant to the given query as 
possible. Here is a query example from the DUC 
2007 document collection ?D0703A?.  
<topic> 
<num> D0703A </num> 
<title> Steps toward introduction of the 
Euro. </title> 
<narr> Describe steps taken and worldwide 
reaction prior to introduction of the Euro on 
January 1, 1999. Include predictions and 
expectations reported in the press. </narr> 
</topic>                                          [D0703A] 
Update summarization is definitely a time-
related task. An appropriate ranking algorithm 
must be the one capable of coping with the 
change or the time issues.  
2.3 Time-based Ranking Solutions with 
Web Graph and Text Graph 
Graph based models in document summarization 
are inspired by the idea behind web graph models 
which have been successfully used by current 
search engines. As a matter of fact, adding time 
dimension into the web graph has been 
extensively studied in recent literature. 
Basically, the evolution in the web graph stems 
from (1) adding new edges between two existing 
nodes; (2) adding new nodes in the existing graph 
(consequently adding new edges between the 
existing nodes and the new nodes or among the 
new nodes); and (3) deleting existing edges or 
nodes. Berberich et al (2004 and 2005) 
developed two link analysis methods, i.e. T-Rank 
Light and T-Rank, by taking into account two 
temporal aspects, i.e. freshness (i.e. timestamp of 
most recent update) and activity (i.e. update rates) 
of the pages and the links. They modeled the web 
as an evolving graph in which each nodes and 
edges (i.e. web pages and hyperlinks) were 
annotated with time information. The time 
information in the graph indicated different kinds 
of events in the lifespan of the nodes and edges, 
such as creation, deletion and modifications. 
Then they derived a subgraph of the evolving 
graph with respect to the user?s temporal interest. 
Finally, the time information of the nodes and the 
edges were used to modify the random walk 
model as was used in PageRank. Specifically, 
they used it to modify the random jump 
probabilities (in both T-Rank Light and T-Rank) 
and the transition probabilities (in T-Rank only).  
Meanwhile, Yu et al (2004 and 2005) 
introduced a time-weighted PageRank, called 
TimedPageRank, for ranking in a network of 
scientific publications. In their approach, 
citations were weighted based on their ages. Then 
a post-processing step decayed the authority of a 
publication based on the publication?s age. Later, 
Yang et al (2007) proposed TemporalRank, 
based on which they computed the page 
importance from two perspectives: the 
importance from the current web graph snapshot 
and the accumulated historical importance from 
previous web graph snapshot. They used a kinetic 
model to interpret TemporalRank and showed it 
could be regarded as a solution to an ordinary 
differential equation.  
In conclusion, Yu et al tried to cope with the 
problem that PageRank favors over old pages 
whose in-degrees are greater than those of new 
pages. They worked on a static single snapshot of 
the web graph, and their algorithm could work 
well on all pages in the web graph. Yang et al, 
on the other hand, worked on a series of web 
graphs at different snapshots. Their algorithm 
was able to provide more robust ranking of the 
web pages, but could not alleviate the problem 
carried by time dimension at each web graph 
snapshot. This is because they directly applied 
the original PageRank to rank the pages. In other 
words, the old pages still obtained higher scores 
while the newly coming pages still got lower 
scores. Berberich et al focused their efforts on 
the evolution of nodes and edges in the web 
graph. However, their algorithms did not work 
491
when the temporal interest of the user (or query) 
was not available.  
As for graph based update summarization, 
Wan (2007) presented the TimedTextRank 
algorithm by following the same idea presented 
in the work of Yu et al Given three collections of 
chronologically ordered documents, Lin et al 
(2007) proposed to construct the TimeStamped 
graph (TSG) graph by incrementally adding the 
sentences to the graph. They modified the 
construction of the text graph, but the ranking 
algorithm was the same as the one proposed by 
OtterBacher et al  
Nevertheless, the text graph is different from 
the web graph. The evolution in the text graph is 
limited to the type (2) in the web graph. The 
nodes and edges can not be deleted or modified 
once they are inserted. In other words, we are 
only interested in the changes caused when new 
sentences are introduced into the existing text 
graph. As a result, the ideas from Berberich et al 
cannot be adopted directly in the text graph. 
Similarly, the problem in web graph as stated in 
the work of Yu et al (i.e. ?new pages, which may 
be of high quality, have few or no in-links and 
are left behind.?) does not exist in the text graph 
at all. More precisely, the new coming sentences 
are equally treated as the existing sentences, and 
the degree (in or out) of the new sentences are 
also equally accumulated as the old sentences. 
Directly applying the ideas from the work of Yu 
et al does not always make sense in the text 
graph. Recall that the main task for sentence 
ranking in update summarization is to rank SB 
given SA. So the idea from Yang et al is also not 
applicable.  
In fact, the key points include not only 
maximizing the importance in the current new 
document collection but also minimizing the 
redundancy to the old document collection when 
ranking the sentences for update summarization. 
Time dimension does contribute here, but it is not 
the only way to consider the changes. Unlike the 
web graph, the easily-captured content 
information in a text graph can provide additional 
means to analyze the influence of the changes.  
To conclude the previous discussions, adding 
temporal information to the text graph is different 
from it in the web graph. Capturing operations 
(such as addition, deletion, modification of web 
pages and hyperlinks) is most concerned in the 
web graph; however, prohibiting redundant 
information from the old documents is the most 
critical issue in the text graph. 
3 Positive and Negative Reinforcement 
Ranking for Update Summarization 
Existing document summarization approaches 
basically follow the same processes: (1) first 
calculate the significance of the sentences with 
reference to the given query with/without using 
some sorts of sentence relations; (2) then rank the 
sentences according to certain criteria and 
measures; (3) finally extract the top-ranked but 
non-redundant sentences from the original 
documents to create a summary. Under this 
extractive framework, undoubtedly the two 
critical processes involved are sentence ranking 
and sentence selection. In the following sections, 
we will first introduce the sentence ranking 
algorithm based on ranking with positive and 
negative reinforcement, and then we present the 
sentence selection strategy. 
3.1 Ranking with Positive and Negative 
Reinforcement (PNR2) 
Previous graph-based sentence ranking 
algorithms is capable to model the fact that a 
sentence is important if it correlates to (many) 
other important sentences. We call this positive 
mutual reinforcement. In this paper, we study two 
kinds of reinforcement, namely positive and 
negative reinforcement, among two document 
collections, as illustrated in Figure 1.  
 
Figure 1 Positive and Negative Reinforcement 
In Figure 1, ?A? and ?B? denote two document 
collections about the same topics (?A? is the old 
document collection, ?B? is the new document 
collection), SA and SB denote the sentences in 
?A? and ?B?. We assume: 
1. SA performs positive reinforcement on its 
own internally; 
2. SA performs negative reinforcement on SB 
externally; 
3. SB performs negative reinforcement on SA 
externally;  
4. SB performs positive reinforcement on its 
own internally. 
Positive reinforcement captures the intuition 
that a sentence is more important if it associates 
to the other important sentences in the same 
collection. Negative reinforcement, on the other 
hand, reflects the fact that a sentence is less 
A B + + 
- 
- 
492
important if it associates to the important 
sentences in the other collection, since such a 
sentence might repeat the same or very similar 
information which is supposed to be included in 
the summary generated for the other collection.  
Let RA and RB denote the ranking of the 
sentences in A and B, the reinforcement can be 
formally described as 
??
??
?
?+??+??=
?+??+??=
+
+
B
k
BBB
k
ABA
k
B
A
k
BAB
k
AAA
k
A
pRMRMR
pRMRMR
r
r
2
)(
2
)(
2
)1(
1
)(
1
)(
1
)1(
???
???
 (1) 
where the four matrices MAA, MBB, MAB and MBA 
are the affinity matrices of the sentences in SA, in 
SB, from SA to SB and from SB to SA. 
??
?
??
?
=
22
11
??
??
W  is a weight matrix to balance the 
reinforcement among different sentences. Notice 
that 0, 21 <??  such that they perform negative 
reinforcement. Ap
r
 and Bp
r
 are two bias vectors, 
with 1,0 21 << ??  as the damping factors. [ ]
1
1
?
=
n
A n
pr , where n is the order of MAA. Bp
r
 is 
defined in the same way. We will further define 
the affinity matrices in section 3.2 later. With the 
above reinforcement ranking equation, it is also 
true that 
1. A sentence in SB correlates to many new 
sentences in SB is supposed to receive a high 
ranking from RB, and 
2. A sentence in SB correlates to many old 
sentences in SA is supposed to receive a low 
ranking from RB. 
Let [ ]TBA RRR =  and [ ]TBA ppp rrr ??= 21 ?? , then 
the above iterative equation (1) corresponds to 
the linear system, 
( ) pRMI r=??                            (2) 
where, ??
?
??
?
=
BBBA
ABAA
MM
MM
M
22
11
??
??
. 
Up to now, the PNR2 is still query-independent. 
That means only the content of the sentences is 
considered. However, for the tasks of query-
oriented summarization, the reinforcement should 
obviously bias to the user?s query. In this work, 
we integrate query information into PNR2 by 
defining the vector pr  as ( )qsrelp ii |=r , where 
( )qsrel i |  denotes the relevance of the sentence si 
to the query q. 
To guarantee the solution of the linear system 
Equation (2), we make the following two 
transformations on M. First M is normalized by 
columns. If all the elements in a column are zero, 
we replace zero elements with n1  (n is the total 
number of the elements in that column). Second, 
M is multiplied by a decay factor ? ( 10 <<? ), 
such that each element in M is scaled down but 
the meaning of M will not be changed.  
Finally, Equation (2) is rewritten as, 
( ) pRMI r=??? ?                        (3) 
The matrix ( )MI ???  is a strictly diagonally 
dominant matrix now, and the solution of the 
linear system Equation (3) exists.  
3.2 Sentence Ranking based on PNR2 
We use the above mentioned PNR2 framework to 
rank the sentences in both SA and SB 
simultaneously. Section 3.2 defines the affinity 
matrices and presents the ranking algorithm. 
The affinity (i.e. similarity) between two 
sentences is measured by the cosine similarity of 
the corresponding two word vectors, i.e.  
[ ] ( )ji sssimjiM ,, =                     (4) 
where ( )
ji
ji
ji
ss
ss
sssim rr
rr
?
?
=,
. However, when 
calculating the affinity matrices MAA and MBB, the 
similarity of a sentence to itself is defined as 0, 
i.e. 
[ ] ( )
??
?
=
?
= ji
jisssimjiM ji
             0
,
,              (5) 
Furthermore, the relevance of a sentence to the 
query q is defined as 
( )
qs
qs
qsrel
i
i
i rr
rr
?
?
=,                     (6) 
Algorithm 1. RankSentence(SA, SB, q) 
Input: The old sentence set SA, the new 
sentence set SB, and the query q. 
Output: The ranking vectors R of SA and SB. 
1: Construct the affinity matrices, and set the 
weight matrix W; 
2: Construct the matrix ( )MIA ??= ? .  
3: Choose (randomly) the initial non-negative 
vectors TR ]11[)0( L= ; 
4: 0?k , 0?? ; 
5: Repeat 
6:     ( )? ?< >++ ??= ij ij kjijkjiji
ij
k
i RaRap
a
R )()1()1( 1 r ; 
7:     ( ))()1(max kk RR ??? + ; 
8:  )1( +kR is normalized such that the maximal 
element in )1( +kR is 1. 
493
9:     1+? kk ; 
10: Until ?<? 1; 
11: )(kRR ? ; 
12: Return. 
Now, we are ready to adopt the Gauss-Seidel 
method to solve the linear system Equation (3), 
and an iterative algorithm is developed to rank 
the sentences in SA and SB. 
After sentence ranking, the sentences in SB 
with higher ranking will be considered to be 
included in the final summary.  
3.3 Sentence Selection by Removing 
Redundancy 
When multiple documents are summarized, the 
problem of information redundancy is more 
severe than it is in single document 
summarization. Redundancy removal is a must. 
Since our focus is designing effective sentence 
ranking approach, we apply the following simple 
sentence selection algorithm. 
Algorithm 2. GenerateSummary(S, length) 
Input: sentence collection S (ranked in 
descending order of significance) and length 
(the given summary length limitation) 
Output: The generated summary ?  
{}?? ; 
?l length; 
For i ?  0 to |S| do 
     threshold ? ( )( )??ssssim i   ,max ; 
     If threshold <= 0.92 do 
          isU??? ; 
          ll ? - ( )islen ;  
          If ( l <= 0) break; 
     End 
End 
Return ? . 
 
4 Experimental Studies 
4.1 Data Set and Evaluation Metrics 
The experiments are set up on the DUC 2007 
update pilot task data set. Each collection of 
documents is accompanied with a query 
description representing a user?s information 
need. We simply focus on generating a summary 
for the document collection ?B? given that the 
                                                 
1
 ?  is a pre-defined small real number as the 
convergence threshold. 
2
 In fact, this is a tunable parameter in the algorithm. 
We use the value of 0.9 by our intuition. 
user has read the document collection ?A?, which 
is a typical update summarization task.  
Table 1 below shows the basic statistics of the 
DUC 2007 update data set. Stop-words in both 
documents and queries are removed 3  and the 
remaining words are stemmed by Porter 
Stemmer 4 . According to the task definition, 
system-generated summaries are strictly limited 
to 100 English words in length. We incrementally 
add into a summary the highest ranked sentence 
of concern if it doesn?t significantly repeat the 
information already included in the summary 
until the word limitation is reached. 
 A B 
Average number of documents 10 10 
Average number of sentences 237.6 177.3 
Table 1. Basic Statistics of DUC2007 Update Data Set 
As for the evaluation metric, it is difficult to 
come up with a universally accepted method that 
can measure the quality of machine-generated 
summaries accurately and effectively. Many 
literatures have addressed different methods for 
automatic evaluations other than human judges. 
Among them, ROUGE5 (Lin and Hovy, 2003) is 
supposed to produce the most reliable scores in 
correspondence with human evaluations. Given 
the fact that judgments by humans are time-
consuming and labor-intensive, and more 
important, ROUGE has been officially adopted 
for the DUC evaluations since 2005, like the 
other researchers, we also choose it as the 
evaluation criteria. 
In the following experiments, the sentences 
and the queries are all represented as the vectors 
of words. The relevance of a sentence to the 
query is calculated by cosine similarity. Notice 
that the word weights are normally measured by 
the document-level TF*IDF scheme in 
conventional vector space models. However, we 
believe that it is more reasonable to use the 
sentence-level inverse sentence frequency (ISF) 
rather than document-level IDF when dealing 
with sentence-level text processing. This has 
been verified in our early study. 
4.2 Comparison of Positive and Negative 
Reinforcement Ranking Strategy 
The aim of the following experiments is to 
investigate the different reinforcement ranking 
strategies. Three algorithms (i.e. PR(B), 
                                                 
3
 A list of 199 words is used to filter stop-words. 
4
 http://www.tartarus.org/~martin/PorterStemmer. 
5
 ROUGE version 1.5.5 is used. 
494
PR(A+B), PR(A+B/A)) are implemented as 
reference. These algorithms are all based on the 
query-sensitive LexRank (OtterBacher et al, 
2005). The differences are two-fold: (1) the 
document collection(s) used to build the text 
graph are different; and (2) after ranking, the 
sentence selection strategies are different. In 
particular, PR(B) only uses the sentences in ?B? 
to build the graph, and the other two consider the 
sentences in both ?A? and in ?B?. Only the 
sentences in ?B? are considered to be selected in 
PR(B) and PR(A+B/A), but all the sentences in 
?A? and ?B? have the same chance to be selected 
in PR(A+B). Only the sentences from B are 
considered to be selected in the final summaries 
in PNR2 as well. In the following experiments, 
the damping factor is set to 0.85 in the first three 
algorithms as the same in PageRank. The weight 
matrix W is set to ??
?
??
?
?
?
15.0
5.01
 in the proposed 
algorithm (i.e. PNR2) and 5.021 == ?? . We have 
obtained reasonable good results with the decay 
factor ?  between 0.3 and 0.8. So we set it to 0.5 
in this paper.  
Notice that the three PageRank-like graph-
based ranking algorithms can be viewed as only 
the positive reinforcement among the sentences is 
considered, while both positive and negative 
reinforcement are considered in PNR2 as 
mentioned before. Table 2 below shows the 
results of recall scores of ROUGE-1, ROUGE-2 
and ROUGE-SU4 along with their 95% 
confidential internals within square brackets.  
 ROUGE 
-1 
ROUGE 
-2 
ROUGE-
SU4 
PR(B) 0.3323 [0.3164,0.3501] 
0.0814 
[0.0670,0.0959] 
0.1165 
0.1053,0.1286] 
PR(A+B) 0.3059 [0.2841,0.3256] 
0.0746 
[0.0613,0.0893] 
0.1064 
[0.0938,0.1186] 
PR(A+B/A) 0.3376 [0.3186,0.3572] 
0.0865 
[0.0724,0.1007] 
0.1222 
[0.1104,0.1304] 
PNR2 0.3616 [0.3464,0.3756] 
0.0895 
[0.0810,0.0987] 
0.1291 
[0.1208,0.1384] 
Table 2. Experiment Results 
We come to the following three conclusions. 
First, it is not surprising that PR(B) and 
PR(A+B/A) outperform PR(A+B), because the 
update task obviously prefers the sentences from 
the new documents (i.e. ?B?). Second, 
PR(A+B/A) outperforms PR(B) because the 
sentences in ?A? can provide useful information 
in ranking the sentences in ?B?, although we do 
not select the sentences ranked high in ?A?. Third, 
PNR2 achieves the best performance. PNR2 is 
above PR(A+B/A) by 7.11% of ROUGE-1, 
3.47% of ROUGE-2, and 5.65% of ROUGE-SU4. 
This result confirms the idea and algorithm 
proposed in this work. 
4.3 Comparison with DUC 2007 Systems 
Twenty-four systems have been submitted to the 
DUC for evaluation in the 2007 update task. 
Table 3 compares our PNR2 with them. For 
reference, we present the following representative 
ROUGE results of (1) the best and worst 
participating system performance, and (2) the 
average ROUGE scores (i.e. AVG). We can then 
easily locate the positions of the proposed models 
among them. 
 PNR2 Mean Best / Worst 
ROUGE-1 0.3616 0.3262 0.3768/0.2621 
ROUGE2 0.0895 0.0745 0.1117/0.0365 
ROUGE-SU4 0.1291 0.1128 0.1430/0.0745 
Table 3. System Comparison 
4.4 Discussion 
In this work, we use the sentences in the same 
sentence set for positive reinforcement and 
sentences in the different set for negative 
reinforcement. Precisely, the old sentences 
perform negative reinforcement over the new 
sentences while the new sentences perform 
positive reinforcement over each other. This is 
reasonable although we may have a more 
comprehensive alternation. Old sentences may 
express old topics, but they may also express 
emerging new topics. Similarly, new sentences 
are supposed to express new topics, but they may 
also express the continuation of old topics. As a 
result, it will be more comprehensive to classify 
the whole sentences (both new sentences and old 
sentences together) into two categories, i.e. old 
topics oriented sentences and new topic oriented 
sentences, and then to apply these two sentence 
sets in the PNR2 framework. This will be further 
studied in our future work. 
Moreover, in the update summarization task, 
the summary length is restricted to about 100 
words. In this situation, we find that sentence 
simplification is even more important in our 
investigations. We will also work on this issue in 
our forthcoming studies. 
5 Conclusion 
In this paper, we propose a novel sentence 
ranking algorithm, namely PNR2, for update 
summarization. As our pilot study, we simply 
assume to receive two chronologically ordered 
document collections and evaluate the summaries 
495
generated for the collection given later. With 
PNR2, sentences from the new (i.e. late) 
document collection perform positive 
reinforcement among each other but they receive 
negative reinforcement from the sentences in the 
old (i.e. early) document collection. Positive and 
negative reinforcement are concerned 
simultaneously in the ranking process. As a result, 
PNR2 favors the sentences biased to the sentences 
that are important in the new collection and 
meanwhile novel to the sentences in the old 
collection. As a matter of fact, this positive and 
negative ranking scheme is general enough and 
can be used in many other situations, such as 
social network analysis etc. 
 
Acknowledgements 
The research work presented in this paper was 
partially supported by the grants from RGC of 
HKSAR (Project No: PolyU5217/07E), NSF of 
China (Project No: 60703008) and the Hong 
Kong Polytechnic University (Project No: A-
PA6L). 
 
References 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2004. G.T-Rank: Time-Aware Authority 
Ranking. In Algorithms and Models for the Web-
Graph: Third International Workshop, WAW, pp 
131-141. 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2005. Time-Aware Authority Ranking. 
Journal of Internet Mathematics, 2(3): 301-332. 
Klaus Lorenz Berberich. 2004. Time-aware and 
Trend-based Authority Ranking. Master Thesis, 
Saarlandes  University, Germany. 
Sergey Brin and Lawrence Page. 1998. The Anatomy 
of a Large-scale Hypertextual Web Search Engine. 
Computer Networks and ISDN Systems, 30(1-
7):107-117. 
Gunes Erkan and Dragomir R. Radev. 2004a. 
LexPageRank: Prestige in Multi-Document Text 
Summarization, in Proceedings of EMNLP, pp365-
371. 
Gunes Erkan and Dragomir R. Radev. 2004b. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization, Journal of Artificial 
Intelligence Research 22:457-479. 
Jon M. Kleinberg. 1999. Authoritative Sources in 
Hyperlinked Environment, Journal of the ACM, 
46(5):604-632. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of 
Document Semantic Graphs for Document 
Summarization, in Proceedings of LinkKDD 
Workshop, pp133-138. 
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Intra- 
and Inter-Event Relevance, in Proceedings of 
ACL/COLING, pp369-376. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics, in Proceedings of HLT-
NAACL, pp71-78. 
Ziheng Lin, Tat-Seng Chua, Min-Yen Kan, Wee Sun 
Lee, Long Qiu, and Shiren Ye. 2007. NUS at DUC 
2007: Using Evolutionary Models for Text. In 
Proceedings of Document Understanding 
Conference (DUC) 2007. 
Rada Mihalcea and Paul Tarau. 2004. TextRank - 
Bringing Order into Text, in Proceedings of 
EMNLP, pp404-411. 
Rada Mihalcea. 2004. Graph-based Ranking 
Algorithms for Sentence Extraction, Applied to 
Text Summarization, in Proceedings of ACL 
(Companion Volume). 
Jahna OtterBacher, Gunes Erkan, Dragomir R. Radev. 
2005. Using Random Walks for Question-focused 
Sentence Retrieval, in Proceedings of 
HLT/EMNLP, pp915-922. 
Lucy Vanderwende, Michele Banko and Arul 
Menezes. 2004. Event-Centric Summary 
Generation, in Working Notes of DUC 2004. 
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2006. 
Using Cross-Document Random Walks for Topic-
Focused Multi-Document Summarization, in 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
pp1012-1018. 
Xiaojun Wan. 2007. TimedTextRank: Adding the 
Temporal Dimension to Multi-document 
Summarization. In Proceedings of 30th ACM 
SIGIR, pp 867-868. 
Lei Yang, Lei Qi, Yan-Ping Zhao, Bin Gao, and Tie-
Yan Liu. 2007. Link Analysis using Time Series of 
Web Graphs. In Proceedings of CIKM?07. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information, in Working Notes of 
NTCIR-4. 
Philip S. Yu, Xin Li, and Bing Liu. 2004. On the 
Temporal Dimension of Search. In Proceedings of 
the 13th International World Wide Web Conference 
on Alternate Track Papers and Posters, pp 448-449. 
Philip S. Yu, Xin Li, and Bing Liu. 2005. Adding the 
Temporal Dimension to Search ? A Case Study in 
Publication Search. In Proceedings of the 2005 
IEEE/WIC/ACM International Conference on Web 
Intelligence. 
496
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1033?1040
Manchester, August 2008
Chinese Term Extraction Using Minimal Resources 
Yuha
School of C
Science and Technology,  
Harbin In
Techn
Harbin 15
1983yang@gmail.com 
Qin L
Department of Computing,  
The Hon
Polytechn
Hong Ko
csluqin@comp.polyu.e
du.
Tieju
School of C
Science and Technology,  
Harbin In
Techn
Harbin 1
tjzhao@mtlab.hit.edu
.
 
ct 
This pap
term extraction nimal resources. 
A term candidate extraction algorithm is 
proposed to i tures of the 
1 
Ter st 
fun  domain. Term 
                                                
ng Yang 
omputer  
stitute of  
ology, 
0001, China 
u 
g Kong  
ic University, 
ng, China 
hk 
n Zhao 
omputer  
stitute of  
ology, 
50001, China 
cn 
Abstra
er presents a new approach for 
using mi
dentify fea
relatively stable and domain independent 
term delimiters rather than that of the 
terms. For term verification, a link 
analysis based method is proposed to 
calculate the relevance between term 
candidates and the sentences in the 
domain specific corpus from which the 
candidates are extracted. The proposed 
approach requires no prior domain 
knowledge, no general corpora, no full 
segmentation and minimal adaptation for 
new domains. Consequently, the method 
can be used in any domain corpus and it 
is especially useful for resource-limited 
domains. Evaluations conducted on two 
different domains for Chinese term 
extraction show quite significant 
improvements over existing techniques 
and also verify the efficiency and relative 
domain independent nature of the 
approach. Experiments on new term 
extraction also indicate that the approach 
is quite effective for identifying new 
terms in a domain making it useful for 
domain knowledge update. 
Introduction 
ms are the lexical units to represent the mo
damental knowledge of a
 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
extraction is 
knowledge acq
an essential task in domain 
uisition which
lexicon update, domain onto
etc. Term extraction involves tw
tes by unithood calculation 
s a valid term. The second 
endent features of domain 
ter
s. 
Ot
 can be used for 
logy construction, 
o steps. The first 
step extracts candida
to qualify a string a
step verifies them through termhood measures 
(Kageura and Umino, 1996) to validate their 
domain specificity.  
Existing techniques extract term candidates 
mainly by two kinds of statistic based measures 
including internal association (e.g. Schone and 
Jurafsky, 2001) and context dependency (e.g. 
Sornlertlamvanich et al, 2000). These techniques 
are also used in Chinese term candidate 
extraction (e.g. Luo and Sun, 2003; Ji and Lu, 
2007). Domain dep
ms are used in a weighted manner to identify 
term boundaries. However, these algorithms 
always face the dilemma that fewer features are 
not enough to identify terms from non-terms 
whereas more features lead to more conflicts 
among selected features in a specific instance.  
Most term verification techniques use features 
on the difference in distribution of a term 
occurred within a domain and across domains, 
such as TF-IDF (Salton and McGill, 1983; Frank, 
1999) and Inter-Domain Entropy (Chang, 2005). 
Limited distribution information on term 
candidates in different documents are far from 
enough to distinguish terms from non-term
her researches attempted to use more direct 
information. The therm verification algorithm, 
TV_ConSem, proposed in (Ji and Lu, 2007) for 
Chinese calculate the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a large domain lexicon 
and relies heavily on both the size and the quality 
of the lexicon. Some supervised learning 
1033
approaches have been applied to protein/gene 
name recognition (Zhou et al, 2005) and Chinese 
new word identification (Li et al, 2004) using 
SVM classifiers (Vapnik, 1995) which also 
require large domain corpora and annotations, 
and intensive training is needed for a new domain. 
Current term extraction techniques (e.g. Frank 
et al, 1999; Chang, 2005; Ji and Lu, 2007) suffer 
from three major problems. The first problem is 
that these algorithms cannot identify certain 
kinds of terms such as the ones that have less 
statistical significance. The second problem is 
their dependency on full segmentation for 
Chinese text which is particularly vulnerable to 
ha
idates and the sentences in domain 
sp
s (terms for short) are more likely to 
be domain substantives. Words immediate before 
s, called predecessors and 
es
conne . These predecessors and 
Ch
ndle domain specific data (Huang et al, 2007). 
The third problem is their dependency on some a 
priori domain knowledge such as a domain 
lexicon making it difficult to be applied to a new 
domain.  
In this work, the proposed algorithm extracts 
candidates by identifying the relatively stable and 
domain independent term boundary markers 
instead of looking for features associated with the 
term candidate themselves. Furthermore, a novel 
algorithm for term verification is proposed using 
link analysis to calculate the relevance between 
term cand
ecific corpus to validate their domain 
specificity.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the 
performance evaluation. Section 4 is the 
conclusion. 
2 Methodology 
2.1 Delimiters Based Term Candidate 
Extraction 
Generally speaking, sentences are constituted by 
substantives and functional words. Domain 
specific term
and after these term
succ sors of the terms, are likely to be either 
functional words or other general substantives 
cting terms
successors can be considered as markers of terms, 
and are referred to as term delimiters in this 
paper. In contrast to terms, delimiters are 
relatively stable and domain independent. Thus, 
they can be extracted more easily. Instead of 
looking for features associated with terms as in 
other works, this paper looks for features 
associated with term delimiters. That is, term 
delimiters are identified first. Words between 
delimiters are then taken as term candidates.  
The proposed delimiter identification based 
algorithm, referred to as TCE_DI (Term 
Candidate Extraction ? Delimiter Identification), 
extracts term candidates from a domain corpus 
by using a delimiter list, referred to as the DList. 
Given a DList, the algorithm TCE_DI itself is 
straight forward. For a given character string CS 
(CS = C1C2?Cn) shown in Figure 1, where Ci is a 
inese character. Suppose there are two 
delimiters D1 = Ci1?Cil and D2 = Cj1?Cjm in CS 
where D1 ? DList and D2 ? DList. The string CS 
is then segmented to five substrings: C1?Cib, 
Ci1?Cil, Cia?Cjb, Cj1?Cjm, and Cja?Cn. Since 
Ci1?Cil and Cj1?Cjm are delimiters, C1?Cib, 
Cia?Cjb, and Cja?Cn are regarded as term 
candidates as labeled by TC1, TC2 and TC3 in 
Figure 1, respectively. If there is no delimiter 
contained in CS, the whole string C1C2?Cn is 
regarded as one term candidate. 
Figure 1. Paradigm of Term Candidate Extraction 
DList can be obtained either from a delimiter 
training corpus or from a given  o l stop w rd ist. 
Given a delimiter training corpus, CorpusTraining, 
normally a domain specific corpus, and a domain 
lexicon Lexicon, DList can be obtained based on 
the following algorithm, referred to as DList_Ext 
St
withou
a stop  experts or from a 
ge
(DelimiterList Extraction Algorithm).   
ep 1: For each term Ti in Lexicon, mark Ti in 
CorpusTraining as a non-divisible lexical unit.  
Step 2: Segment remaining text in CorpusTraining.  
Step 3: Extracts predecessors and successors of 
all Ti as delimiter candidates. 
Step 4: Remove delimiter candidates that are 
contained in a Ti in Lexicon. 
Step 5: Rank delimiter candidates by frequency 
and the top NDI number of items are 
considered delimiters. 
The DList_Ext algorithm basically use known 
terms in a domain specific Lexicon to find the 
delimiters. It can be shown in the experiments 
later that Lexicon does not need to be 
comprehensive. Even if a small training corpus, 
CorpusTraining, is not available in a language 
t sufficient domain specific NLP resources, 
-word list produced by
neral corpus can serve as DList directly 
without using the DList_Ext algorithm. 
1034
2.2 Link Analysis Based Term Verification 
In a domain corpus, some sentences are domain 
relevant sentences which contain more domain 
specific information whereas others are general 
sentences which contain less domain information. 
A domain specific term is more likely to be 
contained in domain relevant sentences, which 
means that domain relevant sentences and 
ai  
, w(pn)
l numb
dom n specific terms have a mutually
reinforcing relationship. A novel algorithm, 
referred to as TV_LinkA (Term Verification ? 
Link Analysis) based the Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 1997) 
originally proposed for information retrieval, is 
proposed using link analysis to calculate the 
relevance between term candidates and the 
sentences in domain specific corpora for term 
verification.   
In TV_LinkA, a node p can either be a sentence 
or a term candidate. If a term candidate TermC is 
contained in a sentence Sen of the corpus 
CorpusExtract where the candidates were extracted, 
there is a directional link from Sen to TermC. This 
way, a graph for the candidates and the sentences 
in CorpusExtract can be constructed and the links 
between them indicate their relationships. A good 
hub  Corpu in sExtract is a sentence that contains 
many good authorities; a good authority is a term 
candidate that is contained in many good hubs. 
Each node p is associated with a non-negative 
authority weight Apw )(  and a non-negative hub 
weight Hpw )( . Link analysis in TV_LinkA 
makes use of the relationship between hubs and 
authorities via an iterative process to maintain 
and update authority/hub weights for each node 
of the graph.  
Let VA denote the authority vector (w(p1)A, 
w(p2)A,?, w(pn)A)  and VH denote the hub vector 
(w(p1)H, w(p2)H,? H), where n is the sum of 
the tota er of sentences and the total 
number of term candidates. Given weights VA and 
VH with a directional link p?q, the I operation(an 
in-pointer to a node) and the O operation(an out-
pointer to a node) update w(q)A and w(p)H as 
follows. 
I operation: ?
??
=
Eqp
HA w(p)w(q)          (1) 
O operation: ?
??
e calculated as follows. 
For i 
Apply the I operation to ( ), 
o . 
factor
=
Eqp
AH w(q)w(p)         (2) 
Let k be the iteration termination parameter and z 
be the vector (1, 1, 1,?, 1) , and VA and VH are 
initialized to AV0  = 
HV0  = z. Hubs and authorities 
can then b
= 1, 2,?, k 
A
iV 1- ,
H
iV 1-
btaining new AiV '
Apply the O operation to ( AiV ' ,
H
iV 1- ), 
obtaining new HiV ' . 
Normalize iV '  by dividing the 
A
normalization  ? 2)'( A(p)w  to 
'  by dividing the 
obtain AiV . 
Normalize V Hi
normalization factor ? 2)'( H(p)w  to 
End 
R
In sExt , term candidates with high 
authority in 
dom terms wh
high uments are m
likel
on this observation, the termhood of each 
candidate term TermC, denoted as TermhoodC, is 
calculated according to formula (3) defined 
be
obtain iV . 
eturn ( AkV , 
H
kV ) 
 Corpu ract
H
a few documents are likely to be 
ain specific ereas candidates with 
 authority in many doc ore 
y to be commonly used general words. Based 
low. 
)log()(
Cj
A
jC DF
D
w(C)Termhood ?=      (3) 
where Ajw(C)  is the authority of TermC in a 
document Dj of CorpusExtract, |D| is the total 
number of documents in CorpusExtract and DFC is 
the total number of documents in which TermC 
occurs. Term s
termhood val
C  are then ranked according to their 
ues TermhoodC, and the top ranked 
NTCList candidates are considered terms. NT
an algorithm parameter to be determined 
entally
e two sets of non-overlapping 
academic papers in the IT domain and 
CorpusIT_Small is identical to the corpus used in 
TV_ConSem(Ji and Lu, 2007). CorpusLegal_Small is 
a complete set of official Chinese criminal law 
articles. CorpusLegal_Large includes the complete set 
CList is 
experim . 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
algorithms for Chinese, experiments are 
conducted on four corpora of two different 
domains as listed in Table 1. CorpusIT_Small and 
CorpusIT_Large ar
1035
of ficial Chinese constitutional lof aw articles and 
d
Economics/Finance law articles (http://www.law-
lib.com/). Three domain lexicons used in the 
experiments are detailed in Table 2. LexiconIT is 
obtained according to the term extraction 
algorithm (Ji and Lu, 2007) with manual 
verification. LexiconLegal is extracted from 
CorpusLegal_Small by manual verification too. 
Because legal text covers a lot of different areas 
such finance, science, advertisement, etc., the 
actually legal specific terms are relatively small 
in size. LexiconPKU contains a total of 144K 
manually verified IT terms supplied by the 
Institute of Computational Linguistics, Peking 
University. LexiconPKU, is used as the standard 
term set for evaluation on the IT domain. 
CorpusIT_Small and LexiconIT are used to obtain the 
delimiter list of IT domain, DListIT. 
CorpusLegal_Small and LexiconLegal are used to 
obtain the elimiter list of legal domain, 
DListLegal. CorpusIT_Large and CorpusLegal_Large are 
used as open test data to evaluate the proposed 
algorithms in IT domain and legal domain, 
respectively.  
Corpus Domain Size Text type
CorpusIT_Small IT 77K Academic 
papers 
CorpusIT_Large IT 6.64M Academic 
papers 
Corpus 
Legal_Small
Legal 344K Law  
article 
Corpus 
Legal_Large
Legal 1.04M Law  
article 
Table 1. Different Corpora Used for Experiments 
Lexicon Domain Size Source 
LexiconIT IT 3,337 Corpus 
IT_Small
Lex 3
 
iconLegal Legal 94 Corpus 
Legal_Small
LexiconPKU IT 144K PKU 
Table 2. D rent Le se
xperime
rify that the approach works with a 
op word list without delimiter extraction, 
rd list, , is d a ence 
 the 494 general purpose stop words 
web www n) thou  
. 
 in the IT 
alua e 
follow formula: 
iffe
E
xicons U
nts 
d for 
To ve
simple st
a stop wo DListSW also use s refer
by taking
downloaded from a Chinese NLP resource 
site (
ion
.nlp.org.c wi t any
modificat
The performance of the algorithm
domain is ev ted by precision according to th
TCList
NewLexicon N+
TE N
N
recis =
where tes in 
term candidate xtracted by an 
ev
e verification of all the new terms 
is 
arked them as correct terms. As 
there is no reasonably large standard l
list available, the evaluation of the leg
p ion            (4) 
NTCList is the number of term candida
list TCList e
aluated algorithm, NLexicon denotes the number 
of term candidates in TCList contained in 
LexiconPKU, NNew denotes the number of extracted 
term candidates that are not in LexiconPKU, yet 
are considered correct. Thus, NNew is the number 
of newly discovered terms with respect to 
LexiconPKU. Th
carried out manually by two experts 
independently. A new term is considered correct 
if both experts m
egal term 
al domain 
in terms of precision is conducted manually. No 
evaluation on new term extraction is conducted. 
To evaluate the ability of the algorithms in 
identify new terms in the IT domain, another 
measurement is applied to the IT corpus against 
LexiconPKU based on the following formula: 
TCList
New
NTE N
N
R =                          (5) 
where TCList and NNew are the same as given in 
formula (4). A higher RNTE indicates that more 
extracted terms are outside of LexiconPKU and are 
thus considered new terms. This is similar to the 
measurements of out of vocabulary (OOV) in 
Chinese segmentation. A higher RNTE indicates 
the algorithm can be useful for domain 
knowledge update including lexicon expansion. 
3.2 Evaluation on Term Extraction 
For comparison, a statistical based term 
candidate extraction algorithm, TCE_SEF&CV 
with the best performance in 
using both internal association and external 
e 
; one is a 
(Ji and Lu, 2007) 
strength, is used as the reference algorithm for 
the evaluation of TCE_DI. A statistics based term 
verification algorithm, TV_ConSem (Ji and Lu, 
2007) using semantic information within a 
context window is used for the evaluation of 
TV_LinkA. LexiconPKU is also used in 
TV_ConSem. Two popular methods integrated 
without division of candidate extraction and 
verification steps are used for comparison. Th
first one is based on TF-IDF (Salton and McGill, 
1983 Frank et al, 1999). The second 
supervised learning approach based on a SVM 
classifier, SVMlight (Joachims, 1999). The 
features used by SVMlight are shown in Table 3. 
Two training sets are constructed for the SVM 
classifier. The first one includes 3,337 positive 
examples (LexiconIT) and 5,950 negative 
examples extracted from CorpusIT_Small. The 
second one includes 394 positive examples 
1036
(LexiconLegal) and 28,051 negative examples 
extracted from CorpusLegal_Small.  
No. Feature Explanation 
1 Percentage of the Chinese characters 
occurred in LexiconDomain
2 Frequency in the domain corpus 
3 Frequency in the general corpus 
4 Part of speech 
5 The length of Chinese characters in 
the candidate 
6 The length of non-Chinese 
characters in the candidate 
7 Contextual evidence 
Table 3. Features Used in the SVM Classifier 
perforFigure 2 shows the mance of the 
proposed TCE_ for term 
ex tio s 
for IT do IT
TCE_DIl tracted 
de iter  NDI = 
50 esp SW
word list 
DI and TV_LinkA 
trac n compared to the reference algorithm
main using CorpusIT_Large. TCE_DI  and 
egal indicate TCE_DI using ex
lim  lists DListIT and DListLegal with
I  simply uses the stop 0, r ectively. TCE_D
DListSW. 
0 1000 2000 3000 4000 5000
40
45
50
55
60
65
70
75
80
85
90
95
100
Pr
ec
is
io
n
Extracted Terms (N
TCList
)
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI +TV_L
SW
inkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 2 Performance of Different Algorithms on 
IT Domain 
As shown in Figure 2, term extraction based 
on TCE_DIIT combined with TV_LinkA gives the 
best performance. It achieves 75.4% precision 
when the number of extracted terms NTCList 
reaches 5,000. The performance is 9.6% and 
29.4% higher in precision compared to TF-IDF 
and TCE_SEF&CV combined with TV_ConSem, 
respectively. These translate to improvements of 
ver 14.8% and 63.9%, respectively.  
When applying the same TV_LinkA algorithm 
for term verification, TCE_DI using different 
delimiter lists provide 24% better performance on 
average compared to the TCE_SEF&CV 
algorithm which translates to improvement of 
over 47%. The result from using delimiters of 
legal domain (DListLegal) to data in IT domain (as 
shown in TCE_DIlegal) is better on average than 
using a simple general stop word list. It should be 
ver, that TCE_DISW still performs 
much better than the reference algorithms, which 
means that delimiter based term candidate 
extraction algorithm can improve performance 
even without any domain specific training. When 
applying the same TCE_DIIT algorithm in term 
candidate extraction, TV_LinkA provides 10% 
higher performance compared to the TV_ConSem 
TV_LinkA using o word list without 
an
of
precision of o
noted, howe
algorithm which translates to improvement of 
over 15.3%. It is important to point out that 
nly the stop 
y domain specific knowledge performs better 
than TV_ConSem using a large domain lexicon. 
In other words, delimiter based extraction with 
link analysis use much less resources and still 
improve performance of TV_ConSem. 
The performance of TCE_DIIT or 
TCE_SEF&CV combined with TV_ConSem have 
an upward trend when more terms are extracted 
which seems to be against intuition. The principle 
 the TV_ConSem algorithm is that a candidate 
is considered a valid term if a majority of its 
context words already appear in the domain 
lexicon. General words are more likely to be 
ranked on top because they are commonly used 
which explains the low performance of 
TV_ConSem in the lower range of NTCList. When 
NTCList increases, more domain terms are included. 
Thus, there is an upward trend in precision. But, 
the upward trend reverts at around 4,500 because 
the measurement in percentage is too low to 
distinguish valid terms from non-term candidates.  
It is also interesting to point out that the simple 
TF-IDF algorithm which was rarely used in 
Chinese term extraction performs as well as the 
SVM classifier. The main reason is that the test 
corpus consists of academic papers. So, many 
terms are consistent and repeated a lot of times in 
different documents which accords with the idea 
of TF-IDF. Thus, TF-IDF performs relatively 
well because of the high-quality domain corpus. 
However, TF-IDF, as a statistics based algorithm 
suffers from similar problem as others based on 
1037
statistics. Thus it does not perform as well as the 
proposed TCE_DI and TV_LinkA algorithms. 
ac
Figure 3 shows that the proposed algorithms 
achieve similar performance on the legal domain. 
TCE_DILegal combined with TV_LinkA perform 
the best. The result from using IT domain 
delimiters (DListIT) in legal domain as shown in 
TCE_DIIT is better on average than using the 
general purpose stop list. This further proves that 
extracted delimiter list even from a different 
domain can be more effective than a general stop 
word list. When applying the same TV_LinkA 
algorithm for term verification, TCE_DI using 
different delimiter lists are better than all the 
reference algorithms. Without large lexicon in 
Chinese legal domain, the TV_ConSem algorithm 
does not even work.  TV_LinkA using no prior 
domain knowledge for term verification still 
hieves similar improvement compared to that 
of the IT domain where a comprehensive domain 
lexicon is available. 
70
80
90
100
0 1000 2000 3000 4000 5000
40
50
60
Extracted Terms (N
TCList
)
Pr
ec
is
io
n
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TF-IDF
 SVM
 Figure 3. Performance of Different Algorithms 
on Legal Domain 
There are three main reasons for the 
performance improvements of the proposed 
TCE_DI and TV_LinkA algorithms. Firstly, the 
delimiters which are mainly functional words (e. 
g. ???(at/in), ???(or)) and general substantive 
(e.g. ???(be), ????(adopt)) can be extracted 
easily and are effective term boundary markers 
since they are quite domain independent and 
stable. Secondly, the granularity of domain 
specific terms extracted the proposed algorithm is 
much larger than words obtained by word 
segmentation. This keeps many noisy strings out 
of the term candidate set. Thus, the proposed 
delimiter based algorithm performs much better 
over segmentation based statistical methods. 
Thirdly, the proposed approach is not as sensitive 
to term frequency as other statistical based 
approaches because term candidates are 
identified without regards to the frequencies of 
the candidates. In the TV_LinkA algorithm, terms 
are verified by calculating the relevance between 
candidates and the sentences instead of the 
distributions of terms in different types of 
documents. Terms having low frequencies can be 
identified as long as they are in domain relevant 
sentences whereas in the previous approaches 
including TF-IDF, terms with less statistical 
significance are weeded out. For example, a long 
IT term ????????? (Hierarchical storage 
system) with a low frequency of 6 is extracted 
using the proposed approach. It cannot be 
i  
information is is term cannot 
be extracte
dentified by TF-IDF since the statistical
not significant. Th
d by the segmentation based 
algorithms either because general segmentor split 
long terms into pieces making them difficult to 
be reunited using term extraction techniques.  
It is interesting to know that the proposed 
approach not only achieves the best performance 
for both domains, it also achieves second best 
when using extracted delimiters from a different 
domain. The results confirm that delimiters are 
quite stable across domains and the relevance 
between candidates and sentences are efficient 
for distinguishing terms from non-terms in 
different domains. In fact, the proposed approach 
can be applied to different domains with minimal 
training or no training if resources are limited. 
3.3 Evaluation on New Term Extraction 
As LexiconPKU is the only ready-to-use domain 
lexicon, the evaluation on new term extraction is 
conducted on CorpusIT_Large only. Figure 4 shows 
the evaluation of the proposed algorithms 
compared to the reference algorithms in terms of 
RNTE, the ratio of new terms among all identified 
terms.  
It can be seen that the proposed algorithms 
TCE_DIIT combined with TV_LinkA is basically 
the top performer throughout the range. It can 
identify 4% (with respect to TCE_SEF&CV 
+TV_ConSem) to 27% (with respect to TF-IDF) 
more new terms when NTCList reaches 5,000 which 
translate to improvements of over 9% to 170%, 
respectively. The second best performer is 
TCE_DIlegal combined with TV_LinkA using 
delimiters of legal domain. In fact, it only 
underperforms in the lower range of NTCList 
1038
compared to TCE_DIIT. When NTCList reaches 
5,000, their performance is basically the same. 
However, the TCE_DISW algorithm using s
context words occur in the domain lexicon than 
that of other terms. Thus, new terms are actually 
ranked higher than other terms in TV_ConSem 
which explains its higher ability to identify new 
terms in the low range of NTCList. However, its 
performance drops in the high range of NTCList 
because the influence of context words 
diminishes in terms of percentage in the domain 
lexicon to distinguish terms from non-terms. 
Figure 4 also shows that TF-IDF and SVM 
perform the worst in new term extraction 
compared to other algorithms. TF-IDF has 
relatively low ability to identify new terms since 
new terms are not widely used and they do not 
repeat a lot of times in many documents. As  
SVM  is sensitive to training data, it is naturally 
not adaptive to new terms. 
All current Chinese term extraction algorithms 
rely on segmentation with comprehensive lexical 
knowledge and y
a  
problem. T xtraction 
pa
top 
wo
in 
mi ion 
 a 
rds performs much worse than using extracted 
delimiter lists as shown for TCE_DIIT and 
TCE_DIlegal. In the TCE_DI algorithm, character 
strings are split by delimiters and the remained 
parts are taken as term candidates. Generally 
speaking, if a new term contains a delimiter or a 
stop word as its component, it cannot be 
identified correctly. Consequently, if a new term 
contains a stop word as its component, it cannot 
be extracted correctly using TCE_DISW.  
However, new terms are less likely to conta
deli ters because the delimiter extract
algorithm DList_Ext would not consider
component as a delimiter if it is contained in a 
term in LexiconDomain. Consequently, TCE_DISW 
is less adaptive to domain specific data compared 
to TCE_DIIT and TCE_DIlegal. That is also why 
TCE_DISW picks up new terms much more slowly. 
0 1000 2000 3000 4000 5000
0
10
20
30
40
50
Pe
rc
en
ta
ge
 o
f N
ew
 T
er
m
s
Extracted Terms (N
TCList
)
 TCE_DI +TV_LinkA
IT
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 4. Performance of Different Algorithms 
for New Term Extraction 
It is interesting to know that TCE_DIIT 
combined with TV_ConSem identifies more new 
terms in the low range of NTCList. In the 
TV_ConSem algorithm, the major information 
used for term verification is the percentage of the 
context words appear in the domain lexicon. As 
discussed earlier in Section 3.2, TV_ConSem 
ranks commonly used general words higher than 
others which leads to the low precision of 
TV_ConSem for term extraction. A new term 
faces a similar scenario because more of its 
et Chinese segmentation 
lgorithms have the OOV (out of vocabulary)
his makes Chinese term e
rticularly vulnerable to new term extraction. 
The proposed approach, on the other hand, is 
based on delimiters which is more stable, domain 
independent, and OOV independent. Figure 4 
shows that TCE_DI and TV_LinkA using minimal 
training from different domains can extract much 
more new terms than previous techniques. In fact, 
the proposed approach can serve as a much better 
tool to identify new domain terms and can be 
quite effective for domain lexicon expansion. 
4 Conclusion 
In conclusion, this paper presents a robust term 
extraction approach using minimal resources. It 
includes a delimiter based algorithm for term 
candidate extraction and a link analysis based 
algorithm for term verification. The proposed 
approach is not sensitive to term frequency as the 
previous works. It requires no prior domain 
knowledge, no general corpora, no full 
segmentation, and minimal adaptation for new 
domains.  
Experiments for term extraction are conducted 
on IT domain and legal domain, respectively. 
Evaluations indicate that the proposed approach 
has a number of advantages. Firstly, the proposed 
approach can improve precision of term 
extraction quite significantly. Secondly, the fact 
that the proposed approach achieves the best 
performance on two different domains verifies its 
domain independent nature. The proposed 
approach using delimiters extracted from a 
1039
different domain also achieves the second best 
performance which indicates that the delimiters 
are quite stable and domain independent. The 
proposed approach still performs much better 
than the reference algorithms when using a 
general purpose stop word list, which means that 
the proposed approach can improve performance 
well even as a completely unsupervised approach 
without any training. Consequently, the results 
demonstrate that the proposed approach can be 
applied to different domains easily even without 
he proposed approach is 
R
 August 2002. 
a. 2002. A measure of term 
ed on the number of co-
inese Word 
Segmentation: Tokenization, Character 
n, or Wordbreak Identification. In 
Ka K., and B. Umino. 1996. Methods of 
Kl
onment. In Proceedings of the 9th 
Ji 
2 ? 74. 
Lu
 Measures. In 
M
e Identification and Semantic 
Na
Sc rafsky D. 2001. Is Knowledge-free 
So
ord 
Vl
Zh
training. Thirdly, t
particularly good for identifying new terms so 
that it can serve as an effective tool for domain 
lexicon expansion. 
Acknowledgements 
This work was done while the first author was 
working at the Hong Kong Polytechnic 
University supported by CERG Grant B-Q941 
and Central Research Grant: G-U297.
eferences 
Chang Jing-Shin. 2005. Domain Specific Word 
Extraction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Learning: 
64-71. 
Chien LF. 1999. Pat-tree-based adaptive keyphrase 
extraction for intelligent Chinese information 
retrieval. Information Processing and Management, 
vol.35: 501-521. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. 
Domain-specific Keyphrase Extraction. In 
Proceedings of 16th International Joint Conference 
on Artificial Intelligence IJCAI-99: 668-673. 
Feng Haodi, Kang Chen, Xiaotie Deng , and Weimin 
Zheng, 2004. Accessor variety criteria for Chinese 
word extraction. Computational Linguistics, 
30(1):75-93. 
Hiroshi Nakagawa, and Tatsunori Mori. 2002. A 
simple but powerful automatic term extraction 
method. In COMPUTERM-2002 Proceedings of 
the 2nd International Workshop on Computational 
Term: 29-35. Taiwan,
Hisamitsu T., and Y. Niw
representativeness bas
occurring salient words. In Proceedings of the 19th 
COLING, 2002. 
Huang Chu-Ren, Petr ?Simon, Shu-Kai Hsieh, and 
Laurent Pr?evot. 2007. Rethinking Ch
Classificatio
Proceedings of the ACL 2007 Demo and Poster 
Sessions: 69?72. Joachims T. 2000. Estimating the 
Generalization Performance of a SVM Efficiently. 
In Proceedings of the International Conference on 
Machine Learning, Morgan Kaufman, 2000. 
geura 
automatic term recognition: a review. Term 
3(2):259-289. 
einberg J. 1997. Authoritative sources in a 
hyperlinked envir
ACM-SIAM Symposium on Discrete Algorithms: 
668-677. New Orleans, America, January 1997. 
Luning, and Qin Lu. 2007. Chinese Term Extraction 
Using Window-Based Contextual Information. In 
Proceedings of CICLing 2007, LNCS 4394: 6
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proceedings of the 1st 
International Joint Conference on Natural 
Language Processing ( IJCNL P2004): 723-732. 
Hainan Island, China, March 2004. 
o Shengfen, and Maosong Sun. 2003. Two-
Character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing: 24-30. 
cDonald, David D. 1993. Internal and External 
Evidence in th
Categorization of Proper Names. In Proceedings of 
the Workshop on Acquisition of Lexical 
Knowledge from Text, pages 32--43, Columbus, 
OH, June. Special Interest Group on the Lexicon of 
the Association for Computational Linguistics. 
sreen AbdulJaleel and Yan Qu. 2005. Domain 
Term Extraction and Structuring via Link Analysis. 
In Proceedings of the AAAI '05 Workshop on Link 
Analysis: 39-46. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
hone, P. and Ju
Induction of Multiword Unit Dictionary Headwords 
a solved problem? In Proceedings of EMNLP2001. 
rnlertlamvanich V., Potipiti T., and Charoenporn T. 
2000. Automatic Corpus-based Thai W
Extraction with the C4.5 Learning Algorithm. In 
Proceedings of COLING 2000. 
adimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
ou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text 
using an Ensemble of Classifiers. BMC 
Bioinformatics 2005, 6(Suppl 1):S7. 
1040
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 414 ? 425, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Preliminary Work on Classifying Time Granularities 
of Temporal Questions 
Wei Li1, Wenjie Li1, Qin Lu1, and Kam-Fai Wong2  
1
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{cswli, cswjli, csluqin}@comp.polyu.edu.hk 
2
 Department of Systems Engineering, the Chinese University of Hong Kong, 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
Abstract. Temporal question classification assigns time granularities to tempo-
ral questions ac-cording to their anticipated answers. It is very important for an-
swer extraction and verification in the literature of temporal question answer-
ing. Other than simply distinguishing between "date" and "period", a more fine-
grained classification hierarchy scaling down from "millions of years" to "sec-
ond" is proposed in this paper. Based on it, a SNoW-based classifier, combining 
user preference, word N-grams, granularity of time expressions, special patterns 
as well as event types, is built to choose appropriate time granularities for the 
ambiguous temporal questions, such as When- and How long-like questions. 
Evaluation on 194 such questions achieves 83.5% accuracy, almost close to 
manually tagging accuracy 86.2%. Experiments reveal that user preferences 
make significant contributions to time granularity classification. 
1   Introduction 
Temporal questions, such as the questions with the interrogatives ?when?, ?how long? 
and ?which year?, seek for the occurrence time of the events or the temporal attributes 
of the entities. Temporal question classification plays an important role in the litera-
ture of question answering and temporal information processing. In the evaluation of 
TREC 10 Question-Answering (QA) track [1], more than 10% of questions in the test 
question corpus are temporal questions. Different from TREC QA track, Workshop 
TERQAS (http://www.timeml.org/terqas/) particularly investigated on temporal ques-
tion answering instead of a general one. It focused on temporal and event recognition 
in question answering systems and paid great attention to temporal relations among 
states, events and time expressions in temporal questions. TimeML (http://www.ti-
meml.org), a temporal information (e.g. time expression, tense & aspect) annotation 
standard, has also been used for temporal question answering in this workshop [2]. 
Correct understanding of a temporal question will greatly help extracting and verify-
ing its answers and certainly improve the performance of any question answering 
system. Look at the following examples. 
[Ea]. What is the birthday of Abraham Lincoln? 
[Eb]. When did the Neanderthal man live? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 415 
In a general question answering system, the question classifier commonly classifies 
temporal questions into two classes, i.e. ?date? and ?period?. With such a system, the 
above two questions are both assigned a ?date?. Whereas it is natural for the question 
[Ea] to be answered with a particular data (e.g. ?12/02/1809?), it is not the case for 
question [Eb], because a proper answer could be ?35,000 years ago?. However, if it is 
known that the time granularity concerned is ?thousands of years?, answer extraction 
turn to be more targeted. The need for a more fine-grained classification is obvious. 
Although there were different question classification hierarchies, as reported 
[3,4,12,13,14], few inclined to introducing the classification hierarchy (e.g. ?year?, 
?month? and ?day?) which could give a clearer direction to guide answer extraction 
and verification of temporal questions. In the following, we try to find out whether 
temporal questions can be further classified into finer time granularity and how to 
classify them. 
By examining a temporal question corpus consisting of 348 questions, 293 of 
which are gathered from UIUC question answering labelled data (http://l2r.cs. 
uiuc.edu/~cogcomp/Data/QA/QC), and the rest 55 from TREC 10 test corpus, we find 
two different cases. On the one hand, some questions are very straightforward in ex-
pressing the time granularities of the answers expected, e.g. the questions beginning 
with ?which year? or ?for how many years?. On the other hand, some questions are 
not so obvious, e.g. the questions headed by ?when? or ?for how long?. We call such 
questions ambiguous questions. Not surprisingly, the ambiguous When- and How 
long-like questions account for a large proportion in this temporal question corpus, 
i.e. 197 from 348 in total. 
We further investigate on those 197 ambiguous questions in order to find out 
whether they can be classified into finer time granularity. Three experimenters are 
requested to tag a time granularity to each question independently1. Answers are not 
provided. The tag with two agreements is taken as the time granularity class of the 
corresponding temporal question. Otherwise the tag ?UNKNOWN? is assigned. Ref-
erence answers for the questions are extracted from AltaVista Web Search 
(http://www.altavista.com). Comparing the time granularities tagged manually with 
those provided by the reference answers, we find that only 27 out of 197 questions are 
incorrectly tagged, in other words, the manually tagging accuracy is 86.2%. Errors 
exist though, the relatively high agreement between users? tagging and reference 
answers lights the hope of automatically determining the time granularities of tempo-
ral questions. 
Analysing the tagging results, it is revealed that the tagging errors arouse from 
three sources: insufficient world knowledge, different speaking habits and different 
expected information granularity among human. See the following examples: 
[Ec]. When did the Neanderthal man live?   
User: year; Ref.: thousands of years 
[Ed]. How long is human gestation?  
User: month; Ref.: week 
[Ee]. When was the first Wall Street Journal published?   
User: year; Ref.: day 
                                                          
1
  The granularity hierarchy and the tagging principle will be detailed later. 
416 W. Li et al 
For question [Ec], the time granularity should be ?thousands of years?, rather than 
?year?. This error could be corrected if one knows that Neanderthal man existed 
35,000 years ago. The time granularity of question [Ed] should be ?week?, but not 
?month? in accordance with the habit. For question [Ee], users? tag is ?year?, different 
from the reference answer?s tag ?day?. However, both granularities are acceptable in 
commonsense, because the different users may want coarser or finer information. This 
observation suggests that incorporating question context, world knowledge, and 
speaking habits would help determine the time granularities of temporal questions. 
In this paper, we propose a fine-grained temporal question classification scheme, 
i.e. time granularity hierarchy, consisting of sixteen non-exclusive classes and scaling 
down from ?millions of years? to ?second?. The SNoW-based classifier is then built 
to combine linguistic features (including word N-grams, granularity of time expres-
sions and special patterns), user preferences and event types, and assign one of the 
sixteen classes to each temporal question. In our work, user preference, which charac-
terizes world knowledge and speaking habits, is estimated by means of the time 
granularities of the entities and/or events involved. The SNoW-based classifier 
achieves 83.5% accuracy, almost close to 86.2% of manually tagging accuracy. Ex-
periments also show that user preference makes a great contribution to time granular-
ity classification. 
The rest of this paper is organized as follows. In the next section various related 
works in this literature are introduced. In Sect. 3, we demonstrate the time granularity 
hierarchy and principles. User preference is fully investigated in Sect. 4. Feature de-
sign is depicted in Sect. 5. Time granularity classifiers are introduced in Sect. 6 and 
the experiment results are presented in Sect. 7. We finally conclude this paper in the 
last section. 
2   Related Works 
In TREC QA track, almost every QA system joining in the evaluation has a question 
classification module. This makes question classification a hot topic. Questions can be 
classified from several aspects. Most classification hierarchies [3,4,12,13,14] adopt the 
anticipated answer types as its classification criteria. Abney et al [4] gave a coarse 
classification hierarchy with seven classes (person, location, etc.). Hovy et al [13] 
introduced a finer classification with forty-seven classes manually constructed from 
17,000 practical questions. Li et al [3] proposed a two-level classification hierarchy, a 
coarser one with six classes and a finer one with fifty classes. In all these classification 
hierarchies, temporal questions are simply classified into two classes, i.e. ?date? and 
?period?. Some works classified temporal questions from other aspects. In [2], a tem-
poral question classification hierarchy is proposed according to the temporal relation 
among state, event and time expression. In [5], temporal questions are classified into 
three types with regard to question structure: non-temporal, simple and complex. Diaz 
F. et al [6] did an interesting work on the statistics of the number of topics along time-
line. According to whether questions or topics have a clear distribution along timeline, 
they can be classified into three types: atemporal, temporal clear and temporal ambigu-
ous.  Focusing on ambiguous temporal questions, e.g. when and how long-like ques-
tions, we introduce a classification hierarchy in terms of the anticipated answer types. 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 417 
It is an extension of two classes ?date? and ?period? and includes sixteen non-
exclusive classes scaling down from ?millions of years? to ?second?. 
Related to the work of features design, Li et al [3] built the question classifier 
based on three types of features, including surface text (e.g. N-grams), syntactic fea-
tures (e.g. part-of-speech and name entity tags), and semantic related words (words 
that often occur with a specific question class). Later works of Li et al [10] intro-
duced semantic information and world knowledge from external resources such as 
WordNet. In this paper, we introduce a new feature, user preference, which is ex-
pected to imply the world knowledge in time granularity in the experiment. User 
preference is estimated from statistics with which Diaz F. et al [6] determine whether 
a question is temporal ambiguous or not. E. Saquete et al [5] suggested that questions 
had different structures, i.e. non-temporal, simple and complex, which is helpful to 
handle questions more orderly. It gives us inspiration to use question focus, i.e. 
whether a question is event-based or entity-based.  
Many machine-learning methods have been used in question classification, such 
as language model [7], SNoW [3,10], maximum entropy [15] and support vector ma-
chine [8,9]. In our experiments, language model is selected as the baseline model, and 
SNoW is selected to tackle to the large feature space and build the classifier. In fact, 
SNoW has already been used in many other fields, such as text categorization, word 
sense disambiguation and even facial feature detection. 
3   Time Granularity Hierarchy and Tagging Principles 
In traditional question answering systems, only two question types are time-related, i.e. 
?date? and ?period?. For the reasons explained in Sect. 1, we propose a more detailed 
temporal question classification scheme, namely time granularity hierarchy scaling 
down from ?millions of years? to ?second? in order to facilitate answer extraction and 
verification. The initial time granularity hierarchy includes the following twelve 
classes: ?second?, ?minute?, ?hour?, ?day?, ?week?, ?month?, ?season?, ?year?, ?dec-
ade?, ?century?,  ?thousands of years? and ?millions of years?.  
Granularity ?weekday? is added to the initial hierarchy because some temporal 
questions favor ?weekday? instead of ?day?, although both of them indicate one day. 
Some questions favour a region of time granularity. Look at the following examples. 
[Ef]. What time of year has the most air travel? 
[Eg]. What time of day did Emperor Hirohito die? 
For [Ef] question, its time granularity could be ?season?, ?month? or even ?day?; and 
for question [Eg], the time granularity could be ?hour? or ?minute?. We can only 
determine that their time granularities are less than ?year? or ?day? respectively, but 
cannot go any further. Such situations only occur to time granularity ?year? and 
?day?, so we expand the original classification hierarchy by adding another two types: 
?less than day?, ?less than year?. Besides, the questions asking for festivals are classi-
fied into ?special date?.  
Up to now, the time granularity hierarchy has sixteen classes. The less frequent 
temporal measures, such as ?microsecond? and ?billions of years? are ignored. As 
mentioned above, the class ?less than day? overlaps several granularities, e.g. ?hour? 
and ?minute?, so the time granularity hierarchy we proposed is non-exclusive.  
418 W. Li et al 
In reality, some temporal questions can be answered in several different time 
granularities. For example, question ?when was Abraham Lincoln born??,  its answers 
can be a ?day? (?12/02/1809?) or a ?year? (?1809?). To resolve this confliction, we 
adopt two principles for time granularity annotation.  
[Pa]. Assign the minimum time granularity we can determine to a given temporal 
question if several time granularities are applicable. 
[Pb]. Select the time granularity with regard to speaking habits or user preferences.  
When the two principles conflict to each other, principle [Pb] takes the priority. With 
principle [Pa], time granularity of the above question can only be ?day?. 
4   User Preference 
In general, temporal questions have two different focuses: entity-based and event-based. 
[a]. Entity-based question: temporal interrogative words + (be) + entity, e.g. 
?When was the World War II?? 
[b]. Event-based question: temporal interrogatives + event, e.g. ?When did 
Mount St. Helen last have a significant eruption?? 
Time granularities of entities (or events) have great significance to those of entity-
based (or event-based) temporal questions. So, in the following, we make estimation 
of the time granularities of entities and events from statistics, based on the intuition 
that some entities or events may favor certain types of time granularities, which is 
called user preference here.  
4.1   Estimation of Time Granularities of Entities and Events 
4.1.1   Time Granularity of Entities 
The time granularity of the entity is derived by counting the co-occurrences of the 
entity and time granularities. The statistics is gathered from AltaVista Web Search. 
The sentences containing both the entity and time expressions are extracted from the 
first one hundred results returned by AltaVista with the entity as the searching key-
word. The probability P of a time granularity class tgi on the occurrence of the entity 
is calculated as the following Equation (1).  
)(#
)(#)|(
entity
entitytg
entitytgP ii
?
=
  )|(max)( entitytgPArgentityTG itgi=          (1) 
#( ) is the number of the sentences containing the expressions between the parenthe-
sis. TG(entity) represents the time granularity of the entity. 
4.1.2   Time Granularity of Events 
The time granularities of the events are not directly extracted as what is done to the 
entities, because they have little chance to be reused on the observation that there are 
rarely two identical events in a question corpus. As an alternative, the time granularity 
of an event is estimated from a sequence of entity-verb-entity? approximating the 
event. The time granularity of the verb is determined as Equation (1) by substituting 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 419 
?verb? for ?entity?. We choose two strategies for the estimation: maximum product 
and one-win-all.  
Maximum  product: )'|()|()|(1)|( entitytgPverbtgPentitytgP
Z
eventtgP iiii =
 
                                 )|(max)( eventtgPArgeventTG itgi=                                        (2) 
TG(event) represents time granularity of event. Z is used for normalization. 
One-win-all: )}'|(),|(),|({max)( entitytgPverbtgPentitytgPArgeventTG iiitgi=               (3) 
Equation (1) is smoothed in order to avoid 0 values in Equation (2). 
                          
i
i
i tgttw
wtg
wtgP =
+
+?
= )(#
1)(#)|(                           (4) 
t is the number of the time granularity classes, w is either an entity or a verb. 
4.1.3   Experiment: Evaluating the Estimation 
In the 197 ambiguous questions, 12 questions are entity-based, and the rest 185 ques-
tions are event-based. If all the 197 questions are arbitrarily assigned a tag ?year?, the 
tagging accuracy is 48.2%. 
For each entity-based or event-based question, the time granularity of the entity or 
event within it are assumed as the time granularity of the question. Compared with the 
time granularity of the reference answer, for the entity-based questions, we achieve 
75% accuracy; for the event-based question, the accuracy of maximum product strat-
egy and one-win-all strategy are 67.0% and 64.3% respectively. It seems that maxi-
mum product strategy is more effective than one-win-all strategy in this application. 
With maximum product strategy, the overall accuracy on all the 197 ambiguous ques-
tions is 67.4%. Notice that the accuracy of arbitrarily tagging is only 48.2%, so the 
estimation of the time granularities of the entities and the events is useful for deter-
mining the time granularities of temporal questions. 
4.2   Distribution of the Time Granularity of Entities and Events 
4.2.1   Observation of Distribution 
In the experiments of estimation, we find that some entities or events tend to favor 
only one certain time granularity, some others tend to favor several time granularities, 
and the rest may have a uniform distribution almost on every time granularity. 
-1 0 1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
Season
Week
Pr
o
p o
rti
o n
(%
)
Time Granularity of "gestation"
   
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
60
Century
Decade
Year
Month
Day
Pr
o
po
rti
o
n(%
)
Time Granularity of "Lincoln born"
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
Year
Month
Weekday
Day
Hour
Pr
op
or
tio
n
(%
)
Time Granularity of "take place"
 
(a)   (b)   (c) 
Fig. 1. Distribution of the time granularities of the entities and events 
420 W. Li et al 
In Fig. 1(a), time granularity ?day? takes a preponderant proportion, i.e. more than 
80%, in the distribution of ?gestation?, which is called single-peak-distribution. In 
Fig. 1(b), both ?day? and ?year? take a large proportion, so ?Lincoln born? is multi-
peak-distributed. In Fig. 1(c), for ?take place?, all the time granularities almost take a 
similar proportion and it is a uniform distribution. 
4.2.2   Experiments on Distribution 
Assume an entity (or event) E, its possible time granularities {tgi, i=1,?t} and the 
corresponding probabilities {Pi, i=1,?t} (calculated by Equation 1 and 2).  
       ?= i iPt1? ;  ?= i iPId ),( ? ; ?
??
?
>
??
?
=
i
i
i P
P
PI
0
1),(                        (5) 
d is the number of time granularities tgi with higher probability Pi than average prob-
ability ? . For simplicity, distribution DE of the time granularity of E is determined as 
follows, 
                                              
3
31
1
>
?<
=
??
??
?
=
d
d
d
Uniform
Multi
Single
DE
                                                  (6)  
Observing the experiment results in Sect. 4.1.3, 88.7%, 56.3% and 18.9% accuracy 
are achieved on the questions within which the time granularities of the entities or 
events are estimated to be single-peak-, multi-peak-, and uniform-distributed respec-
tively. So whether the estimated time granularity of the entity or event is single-peak-, 
multi-peak-, or uniform-distributed highlights the confidence on the estimation, which 
can be taken as a feature associated with the estimation of the time granularities. 
5   Feature Design 
As described in the above section, estimation of the time granularities of the entities 
and the events is useful for determining the time granularities of temporal questions; 
whether a question is entity-based or not and the distribution of time granularities of 
the entities and events within the questions will also be taken as associated features. 
These three features are named user preference feature in total. Besides, another four 
types of features are considered. 
Word N-grams 
Word N-grams feature, e.g. unigram and bigram is the most straightforward feature 
and commonly used in question classification. In general question classification, uni-
gram ?when? indicates a temporal question. In temporal question classification, uni-
gram ?birthday? always implies a ?day? while bigram ?when ? born? is a strong 
evidence of the time granularity ?day?. From this aspect, word N-grams also reflect 
user preference on time granularity. 
Granularity of Time Expressions 
Time expressions are common in temporal questions, e.g. ?July 11, 1998? and date 
modifier ?1998? in ?1998 Superbowl?. We take the granularities of time expressions 
as features, for example, 
TG(?in 1998?) = ?year?  TG(?July 11, 1998?) = ?day? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 421 
Granularities of time expressions impose the constraints on the time granularities of 
temporal questions. If there is a time expression whose time granularity is tg in a 
temporal question, time granularity of this question can not be tg. For example, ques-
tion ?When is the 1998 SuperBowl??, its time granularity can not be ?Year?, i.e. the 
time granularity of  ?1998?. 
Special Patterns 
In word N-gram features, words are equally processed, however, some special words 
combining with the verbs or the temporal connectives (e.g. ?when?, ?before? and 
?since?) will produce special patterns and affect the time granularities of temporal 
questions. Look at the following examples. 
[Eh]. Since when hasn?t John Sununu been able to fly on government planes for 
personal business? 
[Ei]. What time of the day does Michael Milken typically wake up? 
For question [Eh], the temporal preposition ?since? combined with ?when? highlights 
that this question is seeking for a beginning point time, which implies a finer time 
granularity; for question [Ei], ?typically? combined with verb ?wake up? indicates a 
generally occurred event, and implies that its time granularity could be ?less than 
day? or ?less than year?. 
Event Types 
In general, there are four event types: states, activities, accomplishments, and 
achievements. States and activities favour larger time granularities, while accom-
plishments and achievements favour smaller ones. For example, the activity ?stay? 
will favour larger time granularity than the accomplishment event ?take place?.  
6   Classifier Building 
In this work, we choose the Sparse Network of Winnow (SNoW) model as the time 
granularity classifier and compare it with a commonly used Language Model (LM) 
classifier. 
6.1   Language Model (LM) 
As language model has already been used in question classification [7], it is taken as 
the baseline model in the experiments. Language model mainly combines two types 
of features, i.e. unigram and bigram. Given a temporal question Q, its time granularity 
TG(Q) is calculated by Equation (7). 
              ?? =
=
+
=
=
?+=
nj
j jji
mj
j jitg wwtgPwtgPArgQTG i 1 11 )|()1()|(max)( ??              (7)  
w represents words. m and n are the numbers of unigrams and bigrams in questions 
respectively. ?  assigns different weights to unigrams and bigrams. In the experiment, 
best accuracy is achieved when 7.0=?  (see Sect. 7.3.1). 
6.2   Sparse Network of Winnow (SNoW) 
SNoW is a learning framework and applicable to the tasks with a very large number 
of features. It selects active features by updating weights of features, and learns a 
422 W. Li et al 
linear function from a corpus consisting of positive and negative examples. Let 
Ac={i1, ?, im} be the set of features that are active and linked to target class c. Let si 
be the real valued strength associated with feature in the example. Then the example?s 
class is c if and only if, 
                                                   ?
?
?
Aci
ciic sw ?,                                                     (8) 
icw , is weight of feature i connected with class c, which is learned from the training 
corpus. SNoW has already been used in question classification [3,10] and good results 
are reported. As mentioned in Sect. 5, five types of features are selected for our task. 
They are altogether counted to more than ten thousand features. Since it is a large 
feature set, SNoW is a good choice.  
7   Experiments 
7.1   Setup 
In this 348-question-corpus (see Sect. 1), time granularities of 151 questions are 
straightforward, while those of the rest 197 questions are ambiguous. For the sixteen 
time granularity classes, we only consider ten classes including more than four ques-
tions. Questions with unconsidered time granularity classes excluded, the question cor-
pus has 339 questions in total, 145 for training and 194 for testing. As a result, the task 
is to learn a model from the 145-question training corpus and classify questions in the 
194-question test corpus into ten classes: ?second?, ?minute?, ?hour?, ?day?, ?week-
day?, ?week?, ?month?, ?season?, ?year? and ?century?. The SNoW classifier is 
downloaded from UIUC (http://l2r.cs.uiuc.edu/~cogcomp/download.php?key=SNOW). 
7.2   Evaluation Criteria 
The primary evaluation standard is accuracy1, i.e. the proportion of the correct classi-
fied questions out of the test questions (see Equation 9). However, if a question seek-
ing for a finer time granularity, e.g. ?day?, has been incorrectly determined as a 
coarser one, e.g. ?year?, it should also be taken as partly correct, which is reflected in 
accuracy2 (see Equation 10).  
                                         
)(#
)(#
1 test
correctAccuracy =                                                  (9) 
#( ) is number of questions. 
       
)(#
)(
2 test
QRR
Accuracy i i?=
??
??
?
>
<
=
+?
=
)()'(
)()'(
)()'(
)1)()'((1
0
1
)(
QQ
QQ
QQ
QQ tgRtgR
tgRtgR
tgRtgR
tgRtgR
QRR
        (10) 
Qtg  and 'Qtg  are the reference and classification result respectively. )( QtgR is the 
rank of the time granularity class Qtg , scaling down from ?millions of years? to 
?second?. Rank of ?second? is 1, while rank of ?year? is 9. The ranks of the last three 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 423 
time granularities, i.e. ?special date?, ?less than day? and ?less than year? are 14, 15 
and 16 respectively. Likewise, )'( QtgR is the rank of 'Qtg .  
7.3   Experimental Results and Analysis 
In the experiments, language model is taken as the baseline model. Performance of 
SNoW-based classifier will be compared with that of language model. Different com-
binations of features are tested in SNoW-based classifier and their performances are 
investigated. 
7.3.1   LM Classifier 
The LM classifier takes two types of features: unigram and bigram. Experiment re-
sults are presented in Fig. 2.  
Accuracy varies with different feature weight ?  and best accuracy (accuracy1 
68.0% and accuracy2 68.9%) achieves when ? =0.7. Accuracy when ? =1.0 is higher 
than that when ? =0. It indicates that, in the framework of language model, unigrams 
achieves better performance than bigrams, which accounts from the sparseness of 
bigram features. 
-0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
45
50
55
60
65
70
 Accuracy1
 Accuracy2
Ac
cu
ra
cy
(%
)
?
 
Fig. 2. Accuracy of LM classifier. Data in circle is the best performance achieved. 
7.3.2   SNOW Classifier 
Our SNoW classifier requires binary features. We then encode each feature with an 
integer label. When a feature is observed in a question, its label will appear in the 
extracted feature set of this question. There are six types of features: 15 user prefer-
ences (10 for the estimation of time granularities, 3 for the estimation distributions, 
and 2 for question focuses) (F1), 951 unigrams (F2), 9277 bigrams (F3), 10 granularity 
of time expressions (F4), 14 special patterns (F5), and 4 event types (F6). Although the 
number of all features is more than ten thousand, the features in one question are no 
more than twenty in general. Accuracies of SNoW classifier on 194 test questions are 
presented in Table 1. It shows that simply using unigram features, SNoW classifier 
has already achieved better accuracy than LM classifier (accuracy1: 69.5% vs. 68.0%; 
accuracy2: 70.3% vs. 68.9%). From this view, SNoW classifier outperforms LM clas-
sifier in handling sparse features. When all the six types of features are used, SNoW 
classifier achieves 83.5% in accuracy1 and 83.9% in accuracy2, almost close to the 
accuracy of user tagging, i.e. 86.2%. 
424 W. Li et al 
Table 1. Accuracy (%) of SNoW classifier 
Feature Set F2 F2, 3 F1~6 
Accuracy1 69.5 72.1 83.5 
Accuracy2 70.3 72.7 83.9 
Table 2. Accuracy1 (%) on different types of time granularities 
TG second minute hour day weekday 
Accuracy1 100 100 100 64.2 100 
TG week month season year century 
Accuracy1 100 60 100 90.5 66.7 
Table 3. Accuracy (%) on combination of different types of features 
Feature Set F2,3 F1,2,3 F2,3,4 F2,3,5 F2,3,6 
Accuracy1 72.1 79.8 73.7 74.7 72.6 
Accuracy2 72.7 80.6 74.7 75.2 73.1 
With all the six types of features, accuracy1 on the questions with different types of 
time granularity is illustrated in Table 2. It reveals that the classification errors mainly 
come from time granularity of ?month?, ?day? and ?century?. Low accuracy on 
?month? and ?century? accounts from absence of enough examples, i.e. examples for 
training and testing both less than five. Many ?day? questions are incorrectly classi-
fied into ?year?, which accounts for the low accuracy on ?day?.  The reason lies in 
that there are more ?year? questions than ?day? questions in the training question 
corpus (116 vs. 56).  
In general, we can extract three F1 features, one F4 feature, less than two F5 fea-
tures, and one F6 feature from one question. It is hard for SNoW classifier to train and 
test independently on each of these types of the features because of the small feature 
number in one example question. However, the numbers of F2 and F3 features in a 
question are normally more than ten. So we take unigrams (F2) and bigrams (F3) as 
the basic feature set. Table 3 presents the accuracy when the rest four types of fea-
tures are added into the basic feature set respectively. As expected user preference 
makes the most significant improvement, 7.82% in accuracy1 and 7.90% in accuracy2. 
Special patterns also play an important role, which makes 2.6% accuracy1 improve-
ment. It is strange that event type makes such a modest improvement (0.5%). After 
analyzing the experimental results, we find that as there are only four event types, it 
makes limited contribution to 10-class time granularity classification. 
8   Conclusion 
Various features for time granularity classification of temporal questions are investi-
gated in this paper. User preference is shown to make a significant contribution  
to classification performance. SNoW classifier, combining user preference, word  
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 425 
N-grams, granularity of time expressions, special patterns and event types, achieves 
83.5% accuracy in classification, close to manually tagging accuracy 86.2%.  
Acknowledgement 
This project is partially supported by Hong Kong RGC CERG (Grant No: 
PolyU5181/03E), and partially by CUHK Direct Grant (No: 2050330).  
References 
1) TREC (ed.): The TREC-8 Question Answering Track Evaluation. Text Retrieval Confer-
ence TREC-8, Gaithersburg, MD (1999) 
2) Radev D. and Sundheim B.: Using TimeML in Question Answering. 
http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/TimeML-use-in-qa-
v1.0.pdf, (2002) 
3) Li, X. and Roth, D.: Learning Question Classifiers. Proceedings of the 19th International 
Conference on Computational Linguistics (2002) 556-562 
4) S. Abney, M. Collins, and A. Singhal: Answer Extraction. Proceedings of the 6th ANLP 
Conference (2000) 296-301 
5) Saquete E., Mart?nez-Barco P., Mu?oz R.: Splitting Complex Temporal Questions for 
Question Answering Systems. Proceedings of the 42nd Annual Meeting of the Association 
for Computational Linguistics (2004) 567-574 
6) Diaz, F. and Jones, R.: Temporal Profiles of Queries. Yahoo! Research Labs Technical 
Report YRL-2004-022 (2004) 
7) Wei Li: Question Classification Using Language Modeling. CIIR Technical Report (2002) 
8) Dell Zhang and Wee Sun Lee: Question Classification Using Support Vector Machines. 
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and 
Development in Information Retrieval (2003) 26-32 
9) Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda: Question Classification 
Using HDAG Kernel. Proceedings of Workshop on Multilingual Summarization and 
Question Answering (2003) 61-68 
10) Li X., Roth D., and Small K.: The Role of Semantic Information in Learning Question 
Classifiers. Proceedings of the International Joint Conference on Natural Language Proc-
essing (2004) 
11) Schilder, Frank & Habel, Christopher: Temporal Information Extraction for Temporal 
Question Answering. In New Directions in Question Answering. Papers from the 2003 
AAAI Spring Symposium TR SS-03-07 (2003) 34-44 
12) Rohini K. Srihari, Wei Li: A Question Answering System Supported by Information Ex-
traction. Proceedings of Association for Computational Linguistics (2000) 166-172 
13) Eduard Hovy, Laurie Geber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran: 
Towards Semantics-Based Answer Pinpointing. Proceedings of the DARPA Human Lan-
guage Technology Conference (2001) 
14) Hermjacob U.: Parsing and Question Classification for Question Answering. Proceedings 
of the Association for Computational Linguists Workshop on Open-Domain Question An-
swering (2001) 17-22 
15) Ittycheriah, Franz M., Zhu W., Ratnaparki A. and Mammone R.: Question Answering Us-
ing Maximum Entropy Components. Proceedings of the North American chapter of the 
Association for Computational Linguistics (2001) 33-39  
 R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 694 ? 706, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
CTEMP: A Chinese Temporal Parser for Extracting  
and Normalizing Temporal Information 
Wu Mingli, Li Wenjie, Lu Qin, and Li Baoli 
Department of Computing, 
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
{csmlwu, cswjli, csluqin, csblli}@polyu.edu.hk 
Abstract. Temporal information is useful in many NLP applications, such as 
information extraction, question answering and summarization. In this paper, 
we present a temporal parser for extracting and normalizing temporal expres-
sions from Chinese texts. An integrated temporal framework is proposed, which 
includes basic temporal concepts and the classification of temporal expressions. 
The identification of temporal expressions is fulfilled by powerful chart-parsing 
based on grammar rules and constraint rules. We evaluated the system on a sub-
stantial corpus and obtained promising results.  
1   Introduction 
Temporal information processing is valuable in many NLP applications, such as in-
formation extraction, machine translation, question-answering and multi-document 
summarization. However, a wide scope of linguistic means, from lexical to syntactic 
phenomena, can represent this information. It is hard to catch the internal temporal 
meanings which are behind surface texts. The potential applications and the flexibil-
ities of temporal representations motivate our research in this direction. 
In this paper, temporal information is defined as the knowledge about time or dura-
tion. This information is crucial for both temporal reasoning and anchoring events on 
the time line. Temporal expressions are defined as chunks of text which convey direct 
or indirect temporal information. TIMEX2 annotating guidelines [4, 6] give good 
descriptions about temporal expressions. According to the guidelines, temporal ex-
pressions include dates, times of day, durations, set-denoting expressions, event-
anchored expressions, and so on. To retrieve the useful temporal information con-
tained in these temporal expressions, we need to identify the extents of temporal ex-
pressions in raw text and then represent temporal information according to some stan-
dard. The two tasks are called temporal extraction and temporal normalization, re-
spectively. We have implemented a full system CTEMP, which consists of two mod-
ules: extractor and normalizer. The two modules fulfill temporal extraction and tem-
poral normalization, respectively. 
A comprehensive temporal framework is investigated to analyze the elements in-
volved in the mapping procedure, from surface text to internal temporal information. 
This framework includes basic temporal objects and relations, the measurement of 
time, and the classification of temporal expressions from Chinese texts. To cope with 
 CTEMP: A Chinese Temporal Parser 695 
 
the flexibilities of the temporal expressions, we have built the temporal parser based 
on chart-parsing and effective constraints. Experiments with respect to a substantial 
corpus show that the temporal parser achieves promising results. We took part in 
TERN 2004 Chinese temporal expression extraction with this temporal parser and our 
performance is the highest in that track.  
The rest of the paper is organized as follows: In Section 2 we give a brief discus-
sion on related works; Section 3 describes the temporal framework, which is the basis 
of the whole temporal parser; extractor and normalizer of the temporal parser are 
discussed in Section 4 and Section 5, respectively; Section 6 gives the description 
about experiments and evaluations. Finally, conclusion and future work are presented 
in Section 7. 
2   Related Work 
Motivated by the potential applications, temporal information processing has ab-
sorbed more attention recently than ever, such as ACL 2001 workshop on temporal 
and spatial information processing, LREC 2002 and TERN 2004 [14]. Mani [10] 
gives a good review about the recent trend. Research works in this area can be classi-
fied into four types: designing annotation scheme for temporal information represen-
tation [4, 6, 12]; developing temporal ontology which covers temporal objects and 
their relationships between each other [2, 7]; Identifying time-stamps of events or 
temporal relationships between events [5, 9]; Identifying and normalizing temporal 
expressions from different languages [1, 3, 8, 11, 13, 15].  
Temporal annotation, temporal ontology and temporal reasoning are not the fo-
cuses in this paper. Among the research works on temporal expression extraction and 
normalization, most of them are based on hand-written rules or machine-learnt rules. 
Mani and Wilson [11] resolve temporal expressions by hand-crafted and machine-
learnt rules. Their focus is resolving temporal expressions, especially indexical ex-
pressions, which designate times that are dependent on the speaker and some refer-
ence time. We concentrate on the procedure of extraction and normalization, and try 
to cover more temporal expressions. Schilder and Habel [13] employ several finite 
state transducers based on hand-written rules to extract and normalize time-denoting 
and event-denoting temporal expressions. Evaluation of the system is presented on a 
small corpus. 
 Vazov [15] identifies temporal expressions based on context constraints and regu-
lar expressions, but temporal expression normalization is not investigated. Estela et al 
[3] present a temporal parser on Spanish based on grammar rules and evaluate the 
tagger on a small corpus. Jang [8] reports a time tagger for Korean based on a human-
edited, automatically-derived dictionary of patterns. The dictionary is induced from 
training data and is used to extract and normalize temporal expressions in texts. Ahn 
et al [1] adopt the task of TERN 2004 evaluation and investigate machine learning 
methods for extraction of temporal expression and rule based methods for normaliza-
tion. However, they focus on Korean and English text respectively and may not con-
sider some characteristics of Chinese language.  
696 M. Wu et al 
 
3   Temporal Framework 
The goal of the temporal parser is to extract and normalize temporal expressions. First 
we should realize the elements involved in this procedure. We propose a temporal 
framework to describe temporal concepts, the measurement and all kinds of temporal 
expressions in the surface text. Our temporal parser is based on this comprehensive 
framework. 
3.1   Basic Objects and Relations 
In the field of time, basic objects are just time and durations. Time is a point or inter-
val on the time line. Given the origin and a measurement, it can be evaluated with a 
real number. If there is no extra specification in Chinese text, the calendar is the Gre-
gorian calendar. Duration is the distance between some two times. We can anchor 
duration by the start time and the end time, or by one of them and the length of the 
duration. However, if duration is referred to just length, it cannot be anchored on the 
time line. In temporal field, relations between objects are also defined. Between two 
times, relations are ?before?, ?same?, ?include?, ?after?. These objects and relation-
ships are internal concepts behind surface text and we hope to fetch them.   
3.2 The Measurement  
To represent lengths on the time line, a measurement should be given. The temporal 
units consist of two types, macro units and micro units, shown in Fig. 1. To represent 
a time, the scope of the numbers which can be combined with temporal units is lim-
ited. ?Century? and ?Year? are two special time units, because only these two time 
units can help to anchor a time concept on the time line. If there is no help from con-
texts, other time units can not anchor a time concept on the time line. These limita-
tions are valuable in normalization of temporal expression. 
M o n th
[1 .. .1 2 ]
C e n tu ry
Y e a r
Q u a r te r
[1 .. .4 ]
W e e k
[1 .. .5 3 ]
D a y
[1 .. .3 1 ]
M in u te
[1 .. .6 0 ]
H o u r
[1 .. .2 4 ]
S e c o n d
[1 .. .6 0 ]
M a c ro  U n i ts
M ic ro  U n i ts
 
Fig. 1. The scheme of time units 
3.3   Representation in Chinese Text  
According to our observation on Chinese texts and the annotation standards of 
TIMEX2 [4, 6] and TIMEX3 [12], temporal expressions can be classified into differ-
ent classes. They are shown in Fig. 2.  
 CTEMP: A Chinese Temporal Parser 697 
 
E x p
M o n th P art
D ay T im e D ate D u ra tio n S e t P o sD ate E v en tA n
T em p W o rd C o m p o site
B asic C a lcu la ted S p ec ia l
Y earP art D ay P art
 
Fig. 2. The classification of temporal expression 
In Chinese, if people do not know the exact number at an inferior time level, they 
may append an imprecise description to denote a position in a larger scope, such as 
?????/the spring of last year?. We named these temporal expressions ?PosDate?. 
These expressions consist of date expressions and imprecise appendix.  
?TempWord? expressions are some Chinese words which contained temporal 
meanings, such as ???/the lunar new year?, ???/now?. ?Composite? expressions 
include basic temporal expressions, calculated expressions and special expressions, 
such as ?1999?4?28?/April 28, 1999?, and ????/after two years? and ?1999 
?? / the fiscal year 1999?. ?Set? expressions denote a set of time and most of them 
are about frequency, such as ???/every year? and ????/every two days?. ?Even-
tAn? expressions are relevant to the times of events, such as ??????/when he 
was speaking?. ?EventAn? expressions can be anchored on the time line only after the 
times of the events are resolved.  
4   Extractor Based on Grammar and Constraints 
The task of extractor is to identify the extents of temporal expressions in the surface 
text. A set of context free grammar rules is designed to describe the basic form of all 
kinds of temporal expressions and a bottom-up chart parser is employed to parse tem-
poral expressions. Word segmentation is a preliminary step in many Chinese NLP 
applications. However, the performance of word segmentation is not perfect and it 
may introduce some extra errors. In our system, each possible combination of Chinese 
characters in a sentence will be looked up, and then all of the constituents are fed into 
the char parser. If the dictionaries are comprehensive enough, then all the possible 
explanations of all the possible combinations of characters can be gotten. Ambiguities 
and overlaps between multiple temporal expressions are left to constraint rules and 
combination rules. 
4.1   Temporal Grammar Rules 
A set of grammar rules is designed for each type of temporal expressions. In order to 
catch more temporal expressions, the grammar rules are given loosely. Some pseudo 
temporal expressions may be introduced and this problem is addressed in the next 
698 M. Wu et al 
 
section.Given these grammar rules, ?15?24?/15:24? and ?15?24?39?/15:24:39? 
can be recognized. In these examples, ??/o?clock?, ??/minute?, ??/second? are all 
constituents of the type ?Time_Unit?. 
Table 1. Grammar rules for DayTime expressions 
No.1.   Exp -> Time_Of_Day 
No.2.   Time_Of_Day -> Time_Base 
No.3.   Time_Base -> Time_Temp + 
No.4.   Time_Temp -> Integer Time_Unit 
     No.5.    Integer -> Digit + 
4.2   Constraint Rules 
There are many complex and variable phenomena in natural language. Even the domain 
is narrowed down to the temporal field, grammar rules are not enough to extract exact 
temporal expressions. There are some pseudo expressions which satisfy grammar rules, 
so constraint rules are designed to specify the true temporal expressions according to the 
context. These constraint rules are developed by analyzing thedata set. 
A constraint rule will be triggered after the right part of the corresponding grammar 
rule is satisfied. If the constraint rule is satisfied, then the grammar rule can be applied; 
otherwise, it cannot be applied. Examples of constraint rules are shown in Table 2 and 
the following two examples show the constraint checking procedure step by step. 
Table 2.  Examples of constraint rules 
Grammar   rule 3:   Time_Base -> Time_Temp + 
Constraint  rule 3:   IF There is only one constituent of the type ?Time_Temp?,  
                                THEN the constituent ?Time_Unit? which is contained in  
                                ?Time_Temp?, should not be ??/ minute?.   
 
Grammar  rule 4:    Time_Temp -> Integer Time_Unit 
Constraint rule 4:    The constituent ?Integer? can not end up with ??/ (a quanti 
                                fier)?. 
 (1)  ??????????????????? 
  (This news agency reported the event very quickly.)   
Step 1. Look up dictionary.    
            [?/Digit] [?/Time_Unit]  
Step 2. Apply the grammar rule No.5. 
            [? /Integer] [?/Time_Unit]   
Step 3. Check constraint rule No.4.  
            Pass. 
Step 4. Apply grammar rule No.4. 
            [??/Time_Temp] 
Step 5. Check constraint rule No.3. 
            Fail and then terminate parsing.       
 CTEMP: A Chinese Temporal Parser 699 
 
(2) ??7?30????????????????? 
(The ballot ended at 7:30 p.m. in Western Virginia and Ohio.) 
Step 1. Look up dictionary.    
            [7/Digit] [?/Time_Unit][3/Digit] [0/Digit][?/Time_Unit] 
Step  2. Apply the grammar rule No.5. 
            [7/Integer] [?/Time_Unit] [30/Integer][?/Time_Unit] 
Step  3. Check constraint rule No.4. 
             Pass. 
Step  4. Apply grammar rule No.4. 
        [7?/Time_Temp] [30?/Time_Temp] 
Step  5. Check constraint rule No.3. 
             Pass. 
Step  6. Apply grammar rule No.3. 
             [7?30?/Time_Base] 
Step  7. Apply grammar rule No.2. 
             [7?30?/Time_Of_Day] 
Step  8. Apply grammar rule No.1. 
             [7?30?/Exp] 
Step  9. Recognize the temporal expression successfully. 
In the first example ???/very? is an adverb and has no temporal meaning. How-
ever the character ??/ten? and ??/minute? can be looked up and satisfy the grammar 
rule. Constraint rules are necessary to filter the pseudo expression. 
4.3   Combination of Temporal Expressions 
Because each possible substring in a sentence is tried, multiple nested, overlap or 
adjacent temporal expressions may exist in the sentence. However, some of these 
expressions are just parts of the optimal answers. So combination is necessary to get 
the integrated temporal expression. After applying grammar rules, if any two temporal 
expressions are nested, overlapped or adjacent, our system will combine them and 
keep the final result. This procedure is shown by the following examples. 
(3) ??????????????? 
     (This train will arrive at Nan Chang next morning.) 
First recognized temporal expressions are [??/tomorrow] and [??/morning]. 
After the combination, the correct answer [????/next morning] will appear. 
(4) ??8???????? 
     (The basketball game starts at 8:00 p.m.) 
First recognized expressions are [??/night], [8?/8:00], [??8?/8:00 p.m.]. 
The final result is [??8?/ 8:00 p.m.]. 
4.4   Temporal/Non-temporal Disambiguation 
Some strings of characters are temporal expressions in given contexts, but in other 
contexts they are not. The context should be browsed to extract the true temporal 
expressions. Some constraint rules are designed to check the context and fulfill dis-
ambiguation. Three kinds of ambiguities are founded. The first kind is the ambiguities 
700 M. Wu et al 
 
caused by numbers, such as example 5. In this case, the expression ?15?10? contains 
temporal information, but in sports news messages it may be a score of a game. The 
second kind is the ambiguities caused by the combination of numbers and time units, 
such as ????. In example 6, the expression ???? just refers to a football team 
member. However, in many news messages it is a date. The third kind is the ambigui-
ties caused by Chinese words, such as ???. In example 7, the expression means ?for-
mer? and its explanations in other contexts may be ?in front of?. 
(5) ??????15?10?????? 
(6) ??6????10????????????? 
(7) ???????????????????????? 
There are multiple explanations for the same one phrase or word, so ambiguities 
may be caused. To discriminate these expressions, heuristics for disambiguation are 
embedded in corresponding constraint rules.  
5   Normalizer Based on Mapping Procedure 
The goal of normalizer is to represent the temporal information of contained in tem-
poral expressions, according to some standard. The normalizer is based on the map-
ping procedure, in which temporal expressions are explained and represented by val-
ues of temporal attributes. In this procedure, the objects number, unit, time and dura-
tion are employed to store and represent temporal information.  
5.1   Introduction to Normalization 
TERN 2004 evaluation [14] is a public evaluation on the extraction and normalization 
of temporal expressions. To evaluate our temporal parser in a real task, we express 
temporal information according to the standard of TERN 2004 evaluation. Any tem-
poral expression will be explained by a possible combination of the values of the six 
attributes. These attributes are described in table 3. 
Table 3. Temporal attributes 
Attribute Function 
VAL Contains the value of a time or duration 
MOD Captures temporal modifications 
SET Designates set-denoting expressions 
ANCHOR_VAL Contains a normalized form of the reference time 
ANCHOR_DIR Capture the relative direction/orientation between VAL 
and ANCHOR_VAL 
NON_SPECIFIC Designates a generic, essensially nonreferencial expression 
5.2   Normalization of Temporal Expressions 
After the procedure of extraction, the chart parser keeps all the applied grammar rules 
and recognized intermediate constituents. Semantic meanings of temporal expressions 
 CTEMP: A Chinese Temporal Parser 701 
 
can be achieved by the explanation of these grammar rules. In this procedure, some 
basic objects, such as ?number?, ?unit?, ?time? and ?duration?, can be employed to 
store and convey temporal information. Applying grammar rules means creations or 
updates of basic temporal objects. Based on our temporal framework, we explain how 
to normalize the temporal expression extracted, i.e. mapping the expressions to the 
values of six attributes. The mapping procedure is different for different kinds of 
temporal expressions. A general description about the mapping procedure is shown in   
Fig. 3. 
D a y T i m e
D a t e
D u r a t i o n
P o s D a t e
S e t
V A L
M O D
A N C H O R _ V A L
A N C H O R _ D I R
S E T
 
Fig. 3. Mapping temporal expressions to attributes 
According to the classification scheme in Section 3.3, all temporal expressions can 
be mapped to the six attributes. The mapping procedures are complicated and selected 
examples are shown in Table 4. It is difficult to tell whether a temporal expression is 
?specific? or not, and few expressions are set a value at this attribute, we do not map 
expression to the attribute ?NON_SPECIFIC?. 
Table 4. Examples of normalization 
Expressions Attributes 
??/now 
val="PRESENT_REF"  
anchor_val="2000-10-05" anchor_dir="AS_OF" 
??8?20?/20: 20 p.m. val="1999-04-26 T20:20" 
???/the next two years val="P2Y"  
anchor_val="2000" anchor_dir="STARTING" 
???/every two days val="P2D" set="YES" 
????/ next afternoon val="2000-10-07TAF" 
?MOD? attribute of temporal expressions may be set as ?YES? if there are some 
modifying descriptions about the expressions, such as ???/about?, ???/before? 
and so on. So any kind of temporal expressions may be mapped on this attribute. 
?Set? expressions can be explained as set of times, such as ???/each year?, or set of 
durations, such as ????/every two years?, so the attributes ?VAL? and ?SET? will 
be filled. ?ANCHOR_VAL? and ?ANCHOR_DIR? refer to reference times and we 
702 M. Wu et al 
 
adopt the publishing times of news articles as the default reference times. Event ex-
pressions are relevant with a specific event and it is hard to represent the exact mean-
ing of them. In our system event expressions are not normalized.  
5.3   Time/Duration Disambiguation 
Sometimes people omit a part of a full temporal expression for convenience in Chi-
nese texts. For example, ?4?/April? and ?97?/ ?97? are used to instead ?2000?4?
/April, 2000? and ?1997?/the year 1997?. However, ?4?/four months? and ?97?
/97 years? are also legal temporal expressions. These temporal expressions are com-
binations of numbers and common time units. The first kind of explanations means 
these expressions are times and the second kind of explanations means they are dura-
tions. To get the correct values of temporal attributes for these temporal expressions, 
disambiguation is necessary. Heuristic rules are employed for disambiguation, which 
are shown in Table 5.   
Table 5.  Some heuristic rules for disambiguation 
IF a 3-digit or four-digit number is combined with the unit ??/year?, THEN this 
expression is time;  
IF a 2-digit number is combined with the unit ??/year? and the number is bigger 
that 70, THEN this expression is time. 
IF a 1-digit number is combined with the unit ??/year?,  THEN this expression is 
duration. 
6   Evaluation and Analysis 
In this section we report the results about evaluating our temporal parser on a manu-
ally annotated corpus, which consist of 457 Chinese news articles. The data collection 
contains 285,746 characters/142,872 words and 4,290 manually annotated temporal 
expressions. We will evaluate the boundaries of expressions and the values of the six 
temporal attributes.  
Table 6. Experiment configuration 
Experiment No. Conditions 
1 No constraints, combination of nested expressions 
2 No constraints, combination of nested, overlapped and 
adjacent expressions 
3 Constraints, combination of nested expressions 
4 Constraints, combination of nested, overlapped and adja-
cent expressions 
In our temporal parser, we embedded constraints to restrict grammar rules. In addi-
tion, we combine the nested, overlapped and adjacent temporal expressions. In Chi-
nese, many temporal expressions contain nested temporal expressions. If we do not 
 CTEMP: A Chinese Temporal Parser 703 
 
combine these nested components into the optimal answer, there will be so many 
mismatched expressions. So the combination of nested temporal expressions is neces-
sary. In the experiments, we try to evaluate two factors: the constraint rules, and the 
combination of overlapped and adjacent temporal expressions. Four experiments are 
set up, which are described in Table 6. Given these conditions, the results of the ex-
periments are shown in Table 7.  
Table 7. Experiment results 
Attributes  NO. 1. NO. 2. NO. 3. NO. 4. 
P 0.717 0.758 0.810 0.856 
R 0.838 0.850 0.830 0.843 TEXT 
F 0.773 0.801 0.820 0.849 
P 0.730 0.750 0.787 0.807 
R 0.693 0.681 0.742 0.732 VAL 
F 0.711 0.714 0.764 0.768 
P 0.563 0.565 0.629 0.626 
R 0.586 0.550 0.616 0.574 MOD 
F 0.574 0.557 0.622 0.599 
P 0.698 0.662 0.879 0.867 
R 0.606 0.589 0.611 0.598 SET 
F 0.649 0.624 0.720 0.707 
P 0.680 0.750 0.681 0.687 
R 0.658 0.681 0.662 0.652 ANCHOR_VAL 
F 0.669 0.714 0.672 0.669 
P 0.724 0.727 0.733 0.737 
R 0.682 0.669 0.694 0.682 ANCHOR_DIR 
F 0.702 0.697 0.713 0.708 
Several related works are designed to extract and normalize temporal expressions, 
however they are about English, Spanish, French, Korea and so on. We take part in 
TERN 2004 evaluation on Chinese temporal expression extraction and achieve the 
highest performance in this task. There is no public result on Chinese temporal ex-
pression normalization, for reference we compare our normalization result of Experi-
ment NO. 4 with the English normalization result in TERN 2004. Our performance is 
medium among their results.  
Table 7 compares the Precision, Recall and F-measure for different attributes in 
different experiments. ?TEXT? means the performance of exact boundaries of tempo-
ral expressions and other attributes are explained in Section 5.1. For attributes 
?TEXT? and ?VAL?, we achieve the highest performance in Experiment 4. The F-
scores are 0.849 and 0.768, respectively. For other attributes, we also achieved nearly 
highest score in Experiment 4. From the trend of performance on these two attributes, 
we can see the constraints and the procedure of combination have positive effects to 
performance of the temporal parser, especially on ?TEXT? and ?VAL?. At the same 
time, the procedure of combination is not significant to other attributes. Based on the 
assumption that two adjacent or overlapped temporal expressions refer to the same 
temporal concept, we combined them. However, the procedure of combination can 
not help to explain the meaning of the expressions.    
704 M. Wu et al 
 
After the evaluation we collect the errors of Experiment NO. 4 and try to summary 
the reasons. Wrong attribute values include missed, incorrect and spurious cases. The 
reason for errors on the attributes ?ANCHOR_VAL? and ?ANCHOR_DIR? is that 
the system did not give correct reference times. Table 8 gives the error distributions 
according to different attributes. From this table, it can be seen that temporal Chinese 
words and events are difficult to extract and normalize.  
Table 8. Error distributions 
Attributes Reasons Number Percentage 
Boundaries of temporal Chinese words  366 37.4% 
Boundaries of events 193 19.7% 
Grammar rules 161 16.4% 
Boundaries of temporal noun phrase 89 9.1% 
Combination procedure  76 7.8% 
Annotation inconsistence 75 7.7% 
TEXT 
Temporal/non-temporal ambiguities 19 1.9% 
Explained semantics 299 27.6% 
Explanation of temporal Chinese word 180 16.6% 
Errors introduced by extraction 177 16.3% 
Specification/generalization characteristic 148 13.7% 
Wrong reference times 122 11.3% 
Annotation inconsistence 80 7.4% 
Point/duration ambiguities 63 5.8% 
VAL 
Explanation of events or noun phrase 14 1.3% 
Errors introduced by extraction 44 33.3% 
Annotation inconsistence 35 26.5% 
Explanation of temporal Chinese word 27 20.5% 
Explained semantics 23 17.4% 
MOD 
Ambiguities 3 2.1% 
Explained semantics 35 81.4% 
Errors introduced by extraction 3 7.0% SET 
Annotation inconsistence 5 11.6% 
7   Conclusion 
In this paper, we present the temporal parser that extract and normalize comprehen-
sive temporal expressions from Chinese texts. We also propose a temporal frame-
work, which include basic temporal objects and relations, the measurement and classi-
fication of temporal expressions. To cope with kinds of temporal expressions, con-
straint rules are employed to retrieve genuine expressions and resolve ambiguities. 
The temporal parser CTEMP is fully implemented, which is based on the chart pars-
ing and constraint checking scheme. We have evaluated the temporal parser on a 
manually annotated corpus and achieved promising results of F-measures of 85.6% on 
extent and 76.8% on value. We took part in TERN-2004 Chinese temporal expression 
extraction with this parser and achieved the highest performance in that track.  
 CTEMP: A Chinese Temporal Parser 705 
 
In our experiments the temporal parser is also evaluated with/without constraints, 
combination of nested and overlapped temporal expressions. We find that constraints 
are significant to the task extraction and normalization. At the same time, combina-
tion has positive influence on the task extraction. Error analysis shows that temporal 
Chinese words and events are more difficult to extract and normalize. To improve the 
performance of extraction, we plan to decide whether to keep any temporal Chinese 
words as a genuine temporal expression automatically according to the contribution of 
the word. We also plan to improve the performance of normalization by more precise 
semantic explanation.  
Acknowledgement 
The work presented in this paper is supported by Research Grants Council of Hong 
Kong (reference number: CERG PolyU 5085/02E and 5181/03E). 
References 
1. Ahn, D., Adafre, S. F., and Rijke, M. de.: Towards Task-Based Temporal Extraction and 
Recognition. Proceedings Dagstuhl Workshop on Annotating, Extracting, and Reasoning 
about Time and Events (2005) 
2. Allen, J. F.: Towards a General Theory of Action and Time. Artificial Intelligence, Vol. 
23, Issue 2, (1984) 123-154 
3. Estela, S., Martinez-Barco, Patricio, and Munoz, R.: Recognizing and Tagging Temporal 
Expressions in Spanish. Workshop on Annotation Standards for Temporal Information in 
Natural Language, LREC 2002 
4. Ferro, L., Gerber, L., Mani, I., Sundheim, B., And Wilson, G.: TIDES 2003 standard for 
the annotation of temporal expressions (2004). timex2.mitre.org 
5. Filatove E. and Hovy E.: Assigning Time-Stamps to Event-Clauses. Proceedings of the 
ACL Workshop on Temporal and Spatial Information Processing, Toulouse (2001), 88-95 
6. Gerber, L., Huang, S., and Wang, X.: TIDES 2003 standard for the annotation of temporal 
expressions. Chinese supplement draft (2004). timex2.mitre.org 
7. Hobbs, J. R. and Pan, F.: An Ontology of Time for the Semantic Web. ACM Transactions 
on Asian Language Information Processing (2004), Vol. 3, Issue 1, 66-85 
8. Jang, S.B., Baldwin, J. and Mani, I.: Automatic TIMEX2 Tagging of Korean News. ACM 
Transactions on Asian Language Information processing (2004), Vol. 3, No. 1, 51-65 
9. Li, W., Wong, K.-F., and Yuan, C.: A Model for Processing Temporal References in Chi-
nese. Proceedings of the ACL 2001 Workshop on Temporal and Spatial Information Proc-
essing (2001) 
10. Mani, I., Pustejovsky, J., and Sundheim, B.: Introduction to the special issue on temporal 
information processing. ACM Transactions on Asian Language Information Processing 
(2004), Vol. 3, Issue 1, 1-10 
11. Mani, I. and Wilson G.: Robust Temporal Processing of News. Proceedings of the 38th 
Annual Meeting of the Association for Computational Linguistics. New Brunswick, New 
Jersey (2000) 
12. Sauri, R., Littman, J., Knippen, B., Gaizauskas, R., Setzer, A., and Pustejovsky, J.: Ti-
meML Annotation Guidelines (2004). cs.brandeis.edu 
706 M. Wu et al 
 
13. Schilder, F. and Habel, C.: From temporal expressions to temporal information: semantic 
tagging of news messages. Proceedings of the ACL 2001 Workshop on Temporal and Spa-
tial Information Processing. Toulouse (2001), 65-72 
14. TERN-2004. http://timex2.mitre.org/tern.html 2005 
15. Vazov N.: A System for Extraction of Temporal Expressions from French Texts based on 
Syntactic and Semantic Constraints. Proceedings of the ACL Workshop on Temporal and 
Spatial Information Processing (2001), 96-103 
Integrating Collocation Features in Chinese Word Sense 
Disambiguation 
Wanyin Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswyli@comp.polyu.e
du.hk 
Qin Lu 
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
csqinlu@comp.polyu.e
du.hk 
Wenjie Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswjli@comp.polyu.ed
u.hk 
 
Abstract 
The selection of features is critical in pro-
viding discriminative information for clas-
sifiers in Word Sense Disambiguation 
(WSD). Uninformative features will de-
grade the performance of classifiers. Based 
on the strong evidence that an ambiguous 
word expresses a unique sense in a given 
collocation, this paper reports our experi-
ments on automatic WSD using collocation 
as local features based on the corpus ex-
tracted from People?s Daily News (PDN) 
as well as the standard SENSEVAL-3 data 
set. Using the Na?ve Bayes classifier as our 
core algorithm, we have implemented a 
classifier using a feature set combining 
both local collocation features and topical 
features. The average precision on the 
PDN corpus has 3.2% improvement com-
pared to 81.5% of the baseline system 
where collocation features are not consid-
ered. For the SENSEVAL-3 data, we have 
reached the precision rate of 37.6% by in-
tegrating collocation features into 
contextual features, to achieve 37% im-
provement  over  26.7% of precision in the 
baseline system. Our experiments have 
shown that collocation features can be used 
to reduce the size of human tagged corpus. 
1 Introduction 
WSD tries to resolve lexical ambiguity which 
refers to the fact that a word may have multiple 
meanings such as the word ?walk? in  ?Walk or 
Bike to school? and ?BBC Education Walk 
Through Time?, or the Chinese word  ???? in  
??????(?local government?) and ?????
????(?He is also partly right?). WSD tries to 
automatically assign an appropriate sense to an 
occurrence of a word in a given context.  
Various approaches have been proposed to deal 
with the word sense disambiguation problem 
including rule-based approaches, knowledge or 
dictionary based approaches, corpus-based ap-
proaches, and hybrid approaches. Among these 
approaches, the supervised corpus-based ap-
proach had been applied and discussed by many 
researches ([2-8]). According to [1], the corpus-
based supervised machine learning methods are 
the most successful approaches to WSD where 
contextual features have been used mainly to 
distinguish ambiguous words in these methods. 
However, word occurrences in the context are 
too diverse to capture the right pattern, which 
means that the dimension of contextual words 
will be very large when all words in the training 
samples are used for WSD [14]. Certain 
uninformative features will weaken the dis-
criminative power of a classifier resulting in a 
lower precision rate. To narrow down the con-
text, we propose to use collocations as contex-
tual information as defined in Section 3.1.2. It is 
generally understood that the sense of an am-
biguous word is unique in a given collocation 
[19]. For example, ???? means ?burden? but 
not ?baggage? when it appears in the collocation 
?????? (? burden of thought?). 
In this paper, we apply a classifier to combine 
the local features of collocations which contain 
the target word with other contextual features to 
discriminate the ambiguous words. The intuition 
is that when the target context captures a collo-
cation, the influence of other dimensions of
87
contextual words can be reduced or even ig-
nored. For example, in the expression ?????
?????? ? (?terrorists burned down the 
gene laboratory?), the influence of contextual 
word ???? (?gene?) should be reduced to work 
on the target word ???? because ?????? is 
a collocation whereas ???? and ???? are not 
collocations even though they do co-occur. Our 
intention is not to generally replace contextual 
information by collocation only. Rather, we 
would like to use collocation as an additional 
feature in WSD. We still make use of other  con-
textual features because of the following reasons. 
Firstly, contextual information is proven to be 
effective for WSD in the previous research 
works. Secondly, collocations may be independ-
ent on the training corpus and a sentence in con-
sideration may not contain any collocation. 
Thirdly, to fix the tie case such as ??????
?????? (?terrorists? gene checking?),  
???? means ?human? when presented in 
the collocation ??????, but ?particle? 
in the collocation ??????.  The primary 
purpose of using collocation in WSD is to im-
prove precision rate without any sacrifices in 
recall rate. We also want to investigate whether 
the use of collocation as an additional feature 
can reduce the size of hand tagged sense corpus. 
 The rest of this paper is organized as follows. 
Section 2 summarizes the existing Word Sense 
Disambiguation techniques based on annotated 
corpora. Section 3 describes the classifier and 
the features in our proposed WSD approach. 
Section 4 describes the experiments and the 
analysis of our results. Section 5 is the conclu-
sion. 
2 Related Work 
Automating word sense disambiguation tasks 
based on annotated corpora have been proposed. 
Examples of supervised learning methods for 
WSD appear in [2-4], [7-8]. The learning algo-
rithms applied including: decision tree, decision-
list [15], neural networks [7], na?ve Bayesian 
learning ([5],[11]) and maximum entropy [10]. 
Among these leaning methods, the most impor-
tant issue is what features will be used to con-
struct the classifier. It is common in WSD to use 
contextual information that can be found in the 
neighborhood of the ambiguous word in training 
data ([6], [16-18]). It is generally true that when 
words are used in the same sense, they have 
similar context and co-occurrence information 
[13]. It is also generally true that the nearby con-
text words of an ambiguous word give more ef-
fective patterns and features values than those 
far from it [12]. The existing methods consider 
features selection for context representation in-
cluding both local and topic features where local 
features refer to the information pertained only 
to the given context and topical features are sta-
tistically obtained from a training corpus. Most 
of the recent works for English corpus including 
[7] and [8], which combine both local and topi-
cal information in order to improve their per-
formance. An interesting study on feature 
selection for Chinese [10] has considered topical 
features as well as local collocational, syntactic, 
and semantic features using the maximum en-
tropy model. In Dang?s [10] work, collocational 
features refer to the local PoS information and 
bi-gram co-occurrences of words within 2 posi-
tions of the ambiguous word. A useful result 
from this work based on (about one million 
words) the tagged People?s Daily News shows 
that adding more features from richer levels of 
linguistic information such as PoS tagging 
yielded no significant improvement (less than 
1%) over using only the bi-gram co-occurrences 
information. Another similar study for Chinese 
[11] is based on the Naive Bayes classifier 
model which has taken into consideration PoS 
with position information and bi-gram templates 
in the local context. The system has a reported 
60.40% in both precision and recall based on the 
SENSEVAL-3 Chinese training data. Even 
though in both approaches, statistically signifi-
cant bi-gram co-occurrence information is used, 
they are not necessarily true collocations.  For 
example, in the express ?????????
????????????, the bi-grams in 
their system are (???,???, ???
?, ????, ?????, ????
??,? ????Some bi-grams such as 
????may have higher frequency but 
may introduce noise when considering it as fea-
tures in disambiguating the sense ?human|?? 
and ?symbol|??? like in the example case of 
?????????. In our system, we do not rely 
on co-occurrence information. Instead, we util-
ize true collocation information (???, ??) 
which fall in the window size of (-5, +5) as fea-
88
tures and the sense of ?human|?? can be de-
cided clearly using this features. The collocation 
information is a pre-prepared collocation list 
obtained from a collocation extraction system 
and verified with syntactic and semantic meth-
ods ([21], [24]).    
Yarowsky [9] used the one sense per collocation 
property as an essential ingredient for an unsu-
pervised Word-Sense Disambiguation algorithm 
to perform bootstrapping algorithm on a more 
general high-recall disambiguation. A few re-
cent research works have begun to pay attention 
to collocation features on WSD. Domminic [19] 
used three different methods called bilingual 
method, collocation method and UMLS (Unified 
Medical Language System) relation based 
method to disambiguate unsupervised English 
and German medical documents. As expected, 
the collocation method achieved a good preci-
sion around 79% in English and 82% in German 
but a very low recall which is 3% in English and 
1% in German. The low recall is due to the na-
ture of UMLS where many collocations would 
almost never occur in natural text.  To avoid this 
problem, we combine the contextual features in 
the target context with the pre-prepared colloca-
tions list to build our classifier.  
3 The Classifier With Topical Contex-
tual and Local Collocation Features 
3.1 The Feature Set 
As stated early, an important issue is what fea-
tures will be used to construct the classifier in 
WSD. Early researches have proven that using 
lexical statistical information, such as bi-gram 
co-occurrences was sufficient to produce close 
to the best results [10] for Chinese WSD. In-
stead of including bi-gram features as part of 
discrimination features, in our system, we con-
sider both topical contextual features as well as 
local collocation features. These features are 
extracted form the 60MB human sense-tagged 
People?s Daily News with segmentation infor-
mation.  
3.1.1 Topical Contextual Features 
Niu [11] proved in his experiments that Na?ve 
Bayes classifier achieved best disambiguation 
accuracy with small topical context window size 
(< 10 words).  We follow their method and set 
the contextual window size as 10 in our system.  
Each of the Chinese words except the stop 
words inside the window range will be consid-
ered as one topical feature. Their frequencies are 
calculated over the entire corpus with respect to 
each sense of an ambiguous word w.  The sense 
definitions are obtained from HowNet. 
3.1.2 Local Collocation Features 
We chose collocations as the local features. A 
collocation is a recurrent and conventional fixed 
expression of words which holds syntactic and 
semantic relations [21]. Collocations can be 
classified as fully fixed collocations, fixed col-
locations, strong collocations and loose colloca-
tions. Fixed collocations means the appearance 
of one word implies the co-occurrence of an-
other one such as ?????? (?burden of his-
tory?), while strong collocations allows very 
limited substitution of the components, for ex-
ample, ?????? (?local college?), or ? ???
?? (?local university?). The sense of ambiguous 
words can be uniquely determined in these two 
types of collocations, therefore are the colloca-
tions applied in our system. The sources of the 
collocations will be explained in Section 4.1. 
In both Niu [11] and Dang?s [10] work, topical 
features as well as the so called collocational 
features were used. However, as discussed in 
Section 2, they both used bi-gram co-
occurrences as the additional local features. 
However, bi-gram co-occurrences only indicate 
statistical significance which may not actually 
satisfy the conceptual definition of collocations. 
Thus instead of using co-occurrences of bi-
grams, we take the true bi-gram collocations 
extracted from our system and use this data to 
compare with bi-gram co-occurrences to test the 
usefulness of collocation for WSD. The local 
features in our system make use of the colloca-
tions using the template (wi, w) within a window 
size of ten (where i = ? 5). For example, ???
?????????? (?Government 
departments and local government commanded 
that?) fits the bi-gram collocation template (w, 
w1) with the value of (????). During the 
training and the testing processes, the counting 
of frequency value of the collocation feature will 
be increased by 1 if a collocation containing the 
ambiguous word occurs in a sentence. To have a 
good analysis on collocation features, we have 
also developed an algorithm using lonely 
adjacent bi-gram as locals features(named Sys-
89
adjacent bi-gram as locals features(named Sys-
tem A)  and another using collocation as local 
features(named System B). 
3.2 The Collocation Classifier 
We consider all the features in the features set F 
= Ft ?Fl = {f1, f2,  ? , fm } as independent, where 
Ft stands for the topical contextual features set, 
and Fl stands for the local collocation features 
set. For an ambiguous word w with n senses, let 
Sw = {ws1, ws2,  ? , wsn } be the sense set. For 
the contextual features, we directly apply the 
Na?ve Bayes algorithm using Add-Lambda 
Smoothing to handle unknown words: 
 
)|(log)(log)(1 sij
Ff
sisi wfpwpwscore
tj
?
?
+=   
(1) 
For each sense siw of an ambiguous word w:
 
)(
)()(
wfreq
wfreqwp sisi =                       (2) 
For each contextual feature fj respects to each 
sense siw of w : 
),(
),(
)|(
si
Ff
t
sij
sij wffreq
wffreq
wfp
tt
?
?
=   (3) 
To integrate the local collocation feature fj ? Fl  
with respect to each sense siw  of w, we use the 
follows formula: 
)()()( 21 sisisi wscorewscorewscore ?+= ?  (4) 
 
where ? is tuned from experiments (Section 4.5), 
score1( siw ) refers the score of the topical con-
textual features based on formula (1) and 
score2( siw ) refers the score of collocation fea-
tures with respect to the sense sjw  of w defined 
below. 
?
?
=
lj Ff
sjjsi wfwscore )|()(2 ?           (5) 
where ?(fj| sjw ) = 1 for fj ? Fl if the collocation 
occurs in the local context. Otherwise this term 
is set as 0. 
Finally, we choose the right skw so that 
)(maxarg sks wscores k=        (6) 
4 Experimental Results 
We have designed a set of experiments to com-
pare the classifier with and without the colloca-
tion features. In system A, the classifier is built 
with local bi-gram features and topical contex-
tual features. The classifier in system B is con-
structed from combining the local collocation 
features with topical features. 
4.1 Preparation the Data Set 
We have selected 20 ambiguous words from 
nouns and verbs with the sense number as 4 in 
average. The sense definition is taken from 
HowNet [22]. To show the effect of the algo-
rithm, we try to choose words with high degree 
of ambiguity, high frequency of use [23], and 
high frequency of constructing collocations. The 
selection of these 20 words is not completely 
random although within each criterion class we 
do try to pick word randomly. 
Based on the 20 words, we extracted 28,000 
sentences from the 60 MB People?s Daily News 
with segmentation information as our train-
ing/test set which is then manually sense-tagged.  
The collocation list is constructed from a 
combination of a digital collocation dictionary, a 
return result from a collocation automatic ex-
traction system [21], and a hand collection from 
the People?s Daily News. As we stated early, the 
sense of ambiguous words in the fixed colloca-
tions and strong collocations can be decided 
uniquely although they are not unique in loose 
collocations. For example, the ambiguous word 
???? in the collocation ??????? may 
have both the sense of ?appearance|??? or 
?reputation|???. Therefore, when labeling the 
sense of collocations, we filter out the ones 
which cannot uniquely determine the sense of 
ambiguous words inside. However, this does not 
mean that loose collocations have no contribu-
tion in WSD classification. We simply reduce its 
weight when combining it with the contextual 
features compared with the fixed and strong col-
locations. The sense and collocation distribution 
over the 20 words on the training examples can 
be found in Table 1. 
Table 1. Sense and Collocation Distribution of the 20 tar-
get words in the training corpus 
Am. 
W 
T# S1 
co# 
S2 
co# 
S3 
co# 
S4 
co# 
S5 
co# 
S6 
co# 
90
?? 31 1  1 
30 
10 NA  
  
?? 499 479  324 
18  
0 0 0 
NA  
?? 944 908  129 
1  
1 
17 
10 
18  
0 
0 NA 
?? 409 3  2 
389  
171 
17 
0 NA 
  
?? 110 3  0 
101 
36 
6  
9 NA 
  
?? 41 3  0 
37  
6 
1  
0 NA 
  
?? 4885 26  0 
34  
0 
72  
0 
4492 
1356 
261 
1 NA 
?? 3508 7  0 
48  
4 
3194 
1448 
259 
194 
NA  
?? 348 312  117 
22 
11 
14  
4 NA 
  
?? 4438 3983 721 
33  
10 
123  
37 
153 
123 
102 
23 
44 
5 
?? 1987 1712 723 
274 
10 NA  
  
?? 83 36  14 
47  
4 00 NA 
  
?? 995 168  108 
827 
513 NA  
  
?? 31 11  3 
20  
11 NA  
  
?? 2725 227 1772 
498 
49 
102 
424 
1898 
201 
NA  
?? 592 1  0 
208 
63 
367 
124 16 1 
NA  
?? 1155 756  571 
399 
135 NA  
  
?? 2792 691  98 
1765 
113 
336  
29 0 
NA  
?? 2460 82  63 
36 
11 
1231 
474 
877 
103 
NA  
?? 125 11  0 
64  
0 
15  
3 
32 
 4 
3  
0 NA 
T#: total number of sentences contain the ambiguous word 
s1- s6: sense no; co#: number of collocations in each sense 
4.2 The Effect of Collocation Features 
We recorded 6 trials with average precision over 
six-fold validation for each word. Their average 
precision for the six trials in the system A, and B 
can be found in Table 2 and Table 3. From Ta-
ble 3, regarding to precision, there are 16 words 
have improved and 4 words remained the same 
in the system B. The results from the both sys-
tem confirmed that collocation features do im-
prove the precision. Note that 4 words have the 
same precision in the two systems, which fall 
into two cases. In the first case, it can be seen 
that these words already have very high preci-
sion in the system A (over 93%) which means 
that one sense dominates all other senses. In this 
case, the additional collation information is not 
necessary. In fact, when we checked the inter-
mediate outputs, the score of the candidate 
senses of the ambiguous words contained in the 
collocations get improved. Even though, it 
would not change the result. Secondly, no collo-
cation appeared in the sentences which are 
tagged incorrectly in the system A. This is con-
firmed when we check the error files. For exam-
ple, the word ???? with the sense as ???? 
(?closeness?) appeared in 4492 examples over 
the total 4885 examples (91.9%). In the mean 
time, 99% of collocation in its collocation list 
has the same sense of ??? ? (?closeness?). 
Only one collocation ????? has the sense of 
??? ? (?power?). Therefore, the collocation 
features improved the score of sense ??? ? 
which is already the highest one based on the 
contextual features.  
As can be seen from Table 3, the collocation 
features work well for the sparse data. For ex-
ample, the word ???? in the training corpus 
has only one example with the sense ??? (?hu-
man?), the other 30 examples all have the sense 
???? (?management?). Under this situation, 
the topical contextual features failed to identify 
the right sense for the only appearance of the 
sense ??? (?human?) in the training instance 
??????????????????. How-
ever, it can be correctly identified in the system 
B because the appearance of the collocation ??
??????. 
To well show the effect of collocations on 
the accuracy of classifier for the task of WSD, 
we also tested both systems on SENSEVAL-3 
data set, and the result is recorded in the Table 4. 
From the difference in the relative improvement 
of both data sets, we can see that collocation 
features work well when the statistical model is 
not sufficiently built up such as from a small 
corpus like SENSEVAL-3. Actually, in this case, 
the training examples appear in the corpus only 
once or twice so that the parameters for such 
sparse training examples may not be accurate to 
forecast the test examples, which convinces us 
that collocation features are effective on han-
dling sparse training data even for unknown 
words. Fig. 1 shows the precision comparison in 
the system A, and B on SENVESAL-3. 
Table 2.  Average Precision (5/6 training, 1/6 test) of 
system A on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 .83 .972 
?? .90 .97 1.00 1.00 .97 .98 .972 
?? .97 .96 .96 .92 .98 .96 .958 
91
?? .94 .94 .97 .92 .97 .97 .951 
?? 1.00 1.00 .77 .94 .88 1.00 .932 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .93 .94 .89 .91 .89 .90 .91 
?? .94 .93 .86 .93 .89 .87 .903 
?? .83 .94 .89 .90 .88 .94 .897 
?? .86 .88 .92 .84 .82 .87 .865 
?? .92 .84 .92 .76 .84 .72 .833 
?? .84 .83 .88 .82 .88 .71 .827 
?? .80 .60 .80 .20 1.00 1.00 .733 
?? .68 .72 .67 .77 .70 .68 .703 
?? .51 .67 .47 .60 .68 .59 .586 
?? .70 .63 .66 .64 .64 .64 .652 
?? .57 .74 .55 .64 .72 .67 .648 
?? .65 .58 .66 .64 .54 .47 .58 
?? .55 .50 .45 .45 .45 .64 .507 
Total Average Precision 0.815 
Table 3. Average Precision (5/6 training, 1/6 test) of 
system B on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 1.00 1.00 
?? .90 .97 1.00 1.00 .97 .98 .970 
?? .96 .98 .97 .96 .98 .96 .968 
?? .94 .94 .97 .94 .97 .98 .957 
?? 1.00 1.00 .77 .94 .88 1.00 .931 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .92 .95 .92 .92 .91 .91 .922 
?? .94 .94 .86 .93 .91 .87 .908 
?? .80 .95 .89 .93 .89 .94 .902 
?? .87 .88 .92 .84 .83 .91 .875 
?? .84 1.00 .92 .76 .84 .77 .855 
?? .88 .86 .89 .84 .90 .74 .852 
?? 1.00 .80 .80 .20 1.00 1.00 .800 
?? .69 .72 .68 .79 .75 .72 .725 
?? .69 .76 .73 .74 .82 .79 .755 
?? .58 .59 .70 .67 .64 .59 .628 
?? .68 .67 .66 .63 .65 .63 .653 
?? .65 .68 .71 .61 .70 .69 .673 
?? .60 .55 .54 .54 .54 .64 .568 
Total Average Precision 0.840 
Table 4.  Average Precision of System A & B on 
SENSEVAL-3 Data Set 
Amb. 
Word 
Total 
S 
Ave. Prec. in 
Sys A 
Ave. Prec. 
in Sys B 
?? 48 .207 .290 
?? 20 .742 .742 
? 49 .165 .325 
? 25 .325 .325 
?? 36 .260 .373 
?? 30 .167 .267 
?? 30 .192 .392 
?? 36 .635 .635 
? 57 .238 .275 
?? 36 .327 .385 
?? 31 .100 .322 
? 40 .358 .442 
?? 40 .308 .308 
? 76 .110 .123 
? 28 .308 .475 
?667. 500. 30 ? 
? 42 .165 .260 
? 57 .037 .422 
?? 28 .833 .103 
Total Ave. 
Precision .276 .376 
Fig. 1. The precision comparison in system A, and B based 
on SENSEVAL-3 
 
4.3 The Effect of Collocations on the Size 
of Training Corpus Needed 
Hwee [21] stated that a large-scale, human 
sense-tagged corpus is critical for a supervised 
learning approach to achieve broad coverage 
and high accuracy WSD. He conducted a thor-
ough study on the effect of training examples on 
the accuracy of supervised corpus based WSD. 
As the result showed, WSD accuracy continues 
to climb as the number of training examples in-
creases. Similarly, we have tested the system A, 
and B with the different size of training corpus 
based on the PDN corpus we prepared. Our ex-
periment results shown in Fig 2 follow the same 
fact.  The purpose we did the testing is that we 
hope to disclose the effect of collocations on the 
size of training corpus needed. From Fig 2, we 
can see by using the collocation features, the 
precision of the system B has increased slower 
along with the growth of training examples than 
the precision of the system A.  The result is rea-
sonable because with collocation feature, the 
statistical contextual information over the entire 
corpus becomes side effect. Actually, as can be 
seen from Fig. 2, after using collocation features 
92
in the system B, even we use 1/6 corpus as train-
ing, the precision is still higher than we use 5/6 
train corpus in the system A. 
Fig. 2. The precision variation respect to the size of   train-
ing corpus in system A, and B based on PDN corpus 
 
4.4 Investigation of Sense Distribution on 
the Effect of Collocation Features 
To investigate the sense distribution on the ef-
fect of collocation features, we selected the am-
biguous words with the number of sense varied 
from 2 to 6. In each level of the sense number, 
the words are selected randomly. Table 5 shows 
the effect of sense distribution on the effect of 
collocation features. From the table, we can see 
that the collocation features work well when the 
sense distribution is even for a particular am-
biguous word under which case the classifier 
may get confused. 
Table 5.  The Effect of Sense Distribution on the Effect of 
collocation Features 
Amb. 
word 
Prec. 
Wihtout 
coll 
Prec. 
With  
coll 
Sense 
# 
Sense 
Distri. 
?? .972 1 2 97% * 
?? .97 .97 4 96% * 
?? .957 .968 5 96% * 
?? .951 .957 3 95% * 
?? .931 .931 3 92% * 
?? .927 .927 3 90% * 
?? .925 .925 5 92% * 
?? .915 .922 4 91% * 
?? .903 .908 3 90% * 
?? .902 .902 6 90% * 
?? .865 .875 2 86% o 
?? .833 .855 3 ^ 
?? .823 .852 2 83% o 
?? .733 .8 2 ^ 
?? .706 .725 4 ^ 
?? .65 .653 4 ^ 
?? .618 .755 4 ^ 
?? .582 .628 2 ^ 
?? .563 .673 4 ^ 
?? .507 .568 5 ^ 
     *: over 90% samples fall in one dominate sense 
     ^: Even distribution over all senses  
     o: 83% to 86% samples fall in one dominate sense 
4.5 The Test of ? 
We have conducted a set of experiments based 
on both the PDN corpus and SENSEVLA-3 data 
to set the best value of ? for the formula (4) de-
scribed in Section 3.2. The best start value of ? 
is tested based on the precision rate which is 
shown in Fig. 3. It is shown from the experiment 
that ? takes the start value of 0.5 for both cor-
puses.  
Fig. 3. The best value of ? vs the precision rate 
 
5 Conclusion and the Future Work 
This paper reports a corpus-based Word Sense 
Disambiguation approach for Chinese word us-
ing local collocation features and topical contex-
tual features. Compared with the base-line 
systems in which a Na?ve Bayes classifier is 
constructed by combining the contextual fea-
tures with the bi-gram features, the new system 
achieves 3% precision improvement in average 
in Peoples? Daily News corpus and 10% im-
provement in SENSEVAL-3 data set. Actually, 
it works very well when disambiguating the 
sense with sparse distribution over the entire 
corpus under which the statistic calculation 
prone to identify it incorrectly. In the same time, 
because disambiguating using collocation fea-
93
tures does not need statistical calculation, it 
makes contribution to reduce the size of human 
tagged corpus needed which is critical and time 
consuming in corpus based approach.  
Because different types of collocations may 
play different roles in classifying the sense of an 
ambiguous word, we hope to extend this work 
by integrating collocations with different weight 
based on their types in the future, which may 
need a pre-processing job to categorize the col-
locations automatically. 
6 Acknowledgements 
We would like to present our thanks to the IR 
Laboratory in HIT University of China for shar-
ing their sense number definition automatically 
extracted from HowNet with us. 
References 
1. Hwee Tou Ng, Bin Wang, Yee Seng Chan. Exploiting 
Parallel Texts for Word Sense Disambiguation. ACL-03 
(2003) 
2. Black E.: An experiment in computational discrimina-
tion of English word senses. IBM Journal of Research 
and Development, v.32, n.2, (1988) 185-194 
3. Gale, W. A., Church, K. W. and Yarowsky, D.: A 
method for disambiguating word senses in a large cor-
pus. Computers and the Humanities, v.26, (1993) 415-
439 
4. Leacock, C., Towell, G. and Voorhees, E. M.: Corpus-
based statistical sense resolution. In Proceedings of the 
ARPA Human Languages Technology Workshop (1993) 
5. Leacock, C., Chodorow, M., & Miller G. A..Using Cor-
pus Statistics and WordNet Relations for Sense Identifi-
cation. Computational Linguistics, 24:1, (1998) 147?
165  
6. Sch?tze, H.: Automatic word sense discrimination. 
Computational Linguistics, v.24, n.1, (1998) 97-124 
7. Towell, G. and Voorhees, E. M.: Disambiguating highly 
ambiguous words. Computational Linguistics, v.24, n.1, 
(1998) 125-146 
 8. Yarowsky, D.: Decision lists for lexical ambiguity reso-
lution: Application to accent restoration in Spanish and 
French. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics, (1994) 88-
95 
9. Yarowsky, D.: Unsupervised word sense disambiguation 
rivaling supervised methods. In Proceedings of the An-
nual Meeting of the Association for Computational Lin-
guistics, (1995)189-196 
 10. Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D., 
Simple Features for Chinese Word Sense Disambigua-
tion. In Proc. of COLING (2002) 
11. Zheng-Yu Niu, Dong-Hong Ji, Chew Lim Tan, Opti-
mizing Feature Set for Chinese Word Sense Disam-
biguation. To appear in Proceedings of the 3rd 
International Workshop on the Evaluation of Systems 
for the Semantic Analysis of Text (SENSEVAL-3). 
Barcelona, Spain (2004) 
12. Chen, Jen Nan and Jason S. Chang, A Concept-based 
Adaptive Approach to Word SenseDisambiguation, 
Proceedings of 36th Annual Meeting of the Association 
for Computational Linguistics and 17th International 
Conference on Computational linguistics. 
COLING/ACL-98 (1998) 237-243 
13.  Rigau, G., J. Atserias and E. Agirre, Combining Unsu-
pervised Lexical Knowledge Methods for Word Sense 
Disambiguation, Proceedings of joint 35th Annual 
Meeting of the Association for Computational Linguis-
tics and 8th Conference of the European Chapter of the 
Association for Computational Linguistics 
(ACL/EACL?97), Madrid, Spain (1997) 
14. Jong-Hoon Oh, and Key-Sun Choi, C02-1098.: Word 
Sense Disambiguation using Static and Dynamic Sense 
Vectors. COLING (2002) 
15. Yarowsky, D., Hierarchical Decision Lists for Word 
Sense Disambiguation. Computers and the Humanities, 
34(1-2), (2000) 179?186 
16. Agirre, E. and G. Rigau (1996) Word Sense Disam-
biguation using Conceptual Density, Proceedings of 
16th International Conference on Computational Lin-
guistics. Copenhagen, Denmark, COLING (1996) 
17. Escudero, G., L. M?rquez and G. Rigau, Boosting Ap-
plied to Word Sense Disambiguation. Proceedings of 
the 11th European Conference on Machine Learning 
(ECML 2000) Barcelona, Spain. 2000. Lecture Notes in 
Artificial Intelligence 1810. R. L. de M?ntaras and E. 
Plaza (Eds.). Springer Verlag (2000) 
18. Gruber, T. R., Subject-Dependent Co-occurrence and 
Word Sense Disambiguation. Proceedings of 29th An-
nual Meeting of the Association for Computational Lin-
guistics (1991) 
19. Dominic Widdows, Stanley Peters, Scott Cederberg, 
Chiu-Ki Chan, Diana Steffen, Paul Buitelaar, Unsuper-
vised Monolingual and Bilingual Word-Sense Disam-
biguation of Medical Documents using UMLS. 
Appeared in Natural Language Processing in Biomedi-
cine,. ACL 2003 Workshop, Sapporo, Japan (2003) 9?
16 
20. Hwee Tou Ng., Getting serious about word sense dis-
ambiguation. In Proceedings of the ACL SIGLEX 
Workshop on Tagging Text with Lexical Seman-
tics:Why, What, and How? (1997) 1?7 
21. Ruifeng Xu , Qin Lu, and Yin Li, An automatic Chi-
nese Collocation Extraction Algorithm Based On Lexi-
cal Statistics. In Proceedings of the NLPKE Workshop 
(2003) 
22.  D. Dong and Q. Dong, HowNet. 
   http://www.keenage.com, (1991) 
23.  Chih-Hao Tsai, 
 http://technology.chtsai.org/wordlist/, (1995-2004) 
24. Q. Lu, Y. Li, and R. F. Xu, Improving Xtract for Chi-
nese Collocation Extraction.  Proceedings of IEEE In-
ternational Conference on Natural Language Processing 
and Knowledge Engineering, Beijing (2003) 
 
 
94
67
68
69
70
71
72
73
74
75
Preliminary Chinese Term Classification 
for Ontology Construction 
Gaoying Cui, Qin Lu, Wenjie Li 
Department of Computing, 
Hong Kong Polytechnic University 
{csgycui, csluqin, cswjli}@comp.polyu.edu.hk 
 
 
Abstract 
An ontology can be seen as a representa-
tion of concepts in a specific domain. Ac-
cordingly, ontology construction can be re-
garded as the process of organizing these 
concepts. If the terms which are used to la-
bel the concepts are classified before build-
ing an ontology, the work of ontology con-
struction can proceed much more easily. 
Part-of-speech (PoS) tags usually carry 
some linguistic information of terms, so 
PoS tagging can be seen as a kind of pre-
liminary classification to help constructing 
concept nodes in ontology because features 
or attributes related to concepts of different 
PoS types may be different. This paper pre-
sents a simple approach to tag domain 
terms for the convenience of ontology con-
struction, referred to as Term PoS (TPoS) 
Tagging. The proposed approach makes 
use of segmentation and tagging results 
from a general PoS tagging software to pre-
dict tags for extracted domain specific 
terms. This approach needs no training and 
no context information. The experimental 
results show that the proposed approach 
achieves a precision of 95.41% for ex-
tracted terms and can be easily applied to 
different domains. Comparing with some 
existing approaches, our approach shows 
that for some specific tasks, simple method 
can obtain very good performance and is 
thus a better choice. 
Keywords: ontology construction, part-of-
speech (PoS) tagging, Term PoS (TPoS) 
tagging. 
1 Introduction 
Ontology construction has two main issues 
including the acquisition of domain concepts and 
the acquisition of appropriate taxonomies of these 
concepts. These concepts are labeled by the terms 
used in the domain which are described by 
different attributes. Since domain specific terms 
(terminology) are labels of concepts among other 
things, terminology extraction is the first and the 
foremost important step of domain concept 
acquisition. Most of the existing algorithms in 
Chinese terminology extraction only produce a list 
of terms without much linguistic information or 
classification information (Yun Li and Qiangjun 
Wang, 2001; Yan He et al, 2006; Feng Zhang et 
al., 2006). This fact makes it difficult in ontology 
construction as the fundamental features of these 
terms are missing. The acquisition of taxonomies is 
in fact the process of organizing domain specific 
concepts. These concepts in an ontology should be 
defined using a subclass hierarchy by assigning 
and defining properties and by defining 
relationship between concepts etc. (Van Rees, R., 
2003). These methods are all concept descriptions. 
The linguistic information associated with domain 
terms such as PoS tags and semantic classification 
information of terms can also make up for the 
concept related features which are associated with 
concept labels. Terms with different PoS tags 
usually carry different semantic information. For 
example, a noun is usually a word naming a thing 
or an object. A verb is usually a word denoting an 
action, occurrence or state of existence, which are 
all associated with a time and a place. Thus in 
ontology construction, noun nodes and verb nodes 
should be described using different attributes with 
different discriminating characters. With this 
information, extracted terms can then be classified 
The 6th Workshop on Asian Languae Resources, 2008
17
accordingly to help in ontology construction and 
retrieval work. Thus PoS tags can help identify the 
different features needed for concept representation 
in domain ontology construction.  
It should be pointed out that Term PoS (TPoS) 
tagging is different from the general PoS tagging 
tasks. It is designed to do PoS tagging for a given 
list of terms extracted from some terminology ex-
traction algorithms such as those presented in 
(Luning Ji et al, 2007). The granularity of general 
PoS tagging is smaller than what is targeted in this 
paper because terms representing domain specific 
concepts are more likely to be compound words 
and sometimes even phrases, such as ?????
? ?(file manager),  ????? ?(description of 
concurrency), etc.. Even though current general 
word segmentation and PoS tagging can achieve 
precision of 99.6% and 97.58%, respectively 
(Huaping Zhang et al, 2003),   its performance for 
domain specific corpus is much less satisfactory 
(Luning Ji et al, 2007), which is why terminology 
extraction algorithms need to be developed. 
In this paper, a very simple but effective method 
is proposed for TPoS tagging which needs no train-
ing process or even context information. This 
method is based on the assumption that every term 
has a headword. For a given list of domain specific 
terms which are segmented and each word in the 
term already has a PoS tag, the TPoS tagging algo-
rithm then identifies the position of the headword 
and take the tag of the headword as the tag of the 
term. Experiments show that this method is quite 
effective in giving good precision and minimal 
computing time. 
The remaining of this paper is organized as fol-
lows. Section 2 reviews the related work. Section 3 
gives the observations to the task and correspond-
ing corpus, then presents our method for TPOS 
tagging. Section 4 gives the evaluation details and 
discussions on the proposed method and reference 
methods. Section 5 concludes this paper. 
2 Related Work 
Although TPoS tagging is different from general 
PoS tagging, the general POS tagging methods are 
worthy of referencing. There are a lot of existing 
POS tagging researches which can be classified 
into following categories in general. Natural ideas 
of solving this problem were to make use of the 
information from the words themselves. A number 
of features based on prefixes and suffixes and 
spelling cues like capitalization were adopted in 
these researches (Mikheev, A, 1997; Brants, Thor-
sten, 2000; Mikheev, A, 1996). Mikheev presented 
a technique for automatically acquiring rules to 
guess possible POS tags for unknown words using 
their starting and ending segments (Mikheev, A, 
1997). Through an unsupervised process of rule 
acquisition, three complementary sets of word-
guessing rules would be induced from a general 
purpose lexicon and a raw corpus: prefix morpho-
logical rules, suffix morphological rules and end-
ing-guessing rules (Mikheev, A, 1996). Brants 
used the linear interpolation of fixed length suffix 
model for word handling in his POS tagger, named 
TnT. For example, an English word ending in the 
suffix ?able was very likely to be an adjective 
(Brants, Thorsten, 2000). 
Some existing methods are based on the analysis 
of word morphology. They exploited more features 
besides morphology or took morphology as sup-
plementary means (Toutanova et al, 2003; Huihsin 
Tseng et al, 2005; Samuelsson, Christer, 1993). 
Toutanova et al demonstrated the use of both pre-
ceding and following tag contexts via a depend-
ency network representation and made use of some 
additional features such as lexical features includ-
ing jointly conditioning on multiple consecutive 
words and other fine-grained modeling of word 
features (Toutanova et al, 2003). Huihisin et al 
proposed a variety of morphological word features, 
such as the tag sequence features from both left 
and right side of the current word for POS tagging 
and implemented them in a Maximum Entropy 
Markov model (Huihsin Tseng et al, 2005). 
Samuelsson used n-grams of letter sequences end-
ing and starting each word as word features. The 
main goal of using Bayesian inference was to in-
vestigate the influence of various information 
sources, and ways of combining them, on the abil-
ity to assign lexical categories to words. The 
Bayesian inference was used to find the tag as-
signment T with highest probability P(T|M, S) 
given morphology M (word form) and syntactic 
context S (neighboring tags) (Samuelsson, Christer, 
1993). 
Other researchers inclined to regard this POS 
tagging work as a multi-class classification prob-
lem. Many methods used in machine learning, such 
The 6th Workshop on Asian Languae Resources, 2008
18
as Decision Tree, Support Vector Machines (SVM) 
and k-Nearest-Neighbors (k-NN), were used for 
guessing possible POS tags of words (G. Orphanos 
and D. Christodoulakis, 1999; Nakagawa T, 2001; 
Maosong Sun et al, 2000). Orphanos and Christo-
doulakis presented a POS tagger for Modern Greek 
and focused on a data-driven approach for the in-
duction of decision trees used as disambiguation or 
guessing devices (G. Orphanos and D. Christodou-
lakis, 1999). The system was based on a high-
coverage lexicon and a tagged corpus capable of 
showing off the behavior of all POS ambiguity 
schemes and characteristics of words. Support 
Vector Machine is a widely used (or effective) 
classification approach for solving two-class pat-
tern recognition problems. Selecting appropriate 
features and training effective classifiers are the 
main points of SVM method. Nakagawa et al used 
substrings and surrounding context as features and 
achieve high accuracy in POS tag prediction (Na-
kagawa T, 2001). Furthermore, Sun et alpresented 
a POS identification algorithm based on k-nearest-
neighbors (k-NN) strategy for Chinese word POS 
tagging. With the auxiliary information such as 
existing tagged lexicon, the algorithm can find out 
k nearest words which were mostly similar with the 
word need tagging (Maosong Sun et al, 2000). 
3 Algorithm Design 
As pointed out earlier, TPoS tagging is different 
from the general PoS tagging tasks. In this paper, it 
is assumed that a terminology extraction algorithm 
has already obtained the PoS tags of individual 
words. For example, in the segmented and tagged 
sentence ????/n ??/n ?/v ?/f ?/w ??/n 
??/d ?/v ???/a ??/n ?/f ??/v ?/w?(In 
computer graphics, objects are usually represented 
as polygonal meshes.), the term ??????? 
(polygonal meshes) has been segmented into two 
individual words and tagged as ???? /a? 
(polygonal /a) and ??? /n? (meshes /n). The 
terminology extraction algorithm would identify 
these two words  ????/a? and ???/n? as a 
single term in a specific domain. The proposed 
algorithm is to determine the PoS of this single 
term ??????? (polygonal meshes), thus the 
algorithm is referred to as TPoS tagging. It can be 
seen that the general purpose PoS tagging and term 
PoS tagging assign tags at different granularity. In 
principle, the context information of terms can help 
TPoS tagging and the individual PoS tags may be 
good choices as classification features. 
The proposed TPoS tagging algorithm consists 
of two modules. The first module is a terminology 
extraction preprocessing module. The second 
module carries out the TPoS tag assignment. In the 
terminology extraction module, if the result of ter-
minology extraction algorithm is a list of terms 
without PoS tags, a general purpose segmenter 
called ICTCLAS1 will be used to give PoS tags to 
all individual words. ICTCLAS is developed by 
Chinese Academy of Science, the precision of 
which is 97.58% on tagging general words 
(Huaping Zhang et al, 2003). Then the output of 
this module is a list of terms, referred to as Term-
List, using algorithms such as the method de-
scribed in (Luning Ji et al, 2007). 
In this paper, two simple schemes for the term 
PoS tag assignment module are proposed. The first 
scheme is called the blind assignment scheme. It 
simply assigns the noun tag to every term in the 
TermList. This is based on the assumption that 
most of the terms in a specific domain represent 
certain concepts that are most likely to be nouns. 
Result from this blind assignment scheme can be 
considered as the baseline or the worse case sce-
nario. Even in general domain, it is observed that 
nouns are in the majority of Chinese words with 
more than 50% among all different PoS tags (Hui 
Wang, 2006).  
The second scheme is called head-word-driven 
assignment scheme. Theoretically, it will take the 
tag of the head word of one term as the tag of the 
whole term. But here it simply takes the tag of the 
last word in a term. This is based on the assump-
tion that each term has a headword which in most 
cases is the last word in a term (Hui Wang, 2006). 
One additional experiment has been done to verify 
this assumption. A manually annotated Chinese 
shallow Treebank in general domain is used for the 
statistic work (Ruifeng Xu et al, 2005). There are 
9 different structures of Chinese phrases, (Yunfang 
Wu et al, 2003), but only 3 of them do not have 
their head words in the tail, which are about 6.56% 
from all phrases. Following the examples earlier, 
   
1 Copyright ? Institute of Computing Technology, Chinese 
Academy of Sciences 
The 6th Workshop on Asian Languae Resources, 2008
19
the term ????/a ??/n? (polygonal /a meshes 
/n) will be assigned the tag ?/n? because the last 
word is labeled ?/n?. 
There are a lot of semanteme tags at the end of a 
term. For example, ?/ng? presents single character 
postfix of a noun. But it would be improper if a 
term is tagged as ?/ng?. For example, the term ??
?? ? (decision-making machine) contains two 
segments as  listed with two components ???/n? 
and ??/ng?. It is obvious that ????/ng? is in-
appropriate. Thus the head-word-driven assign-
ment scheme also includes some rules to correct 
this kind of problems. As will be discussed in the 
experiment, the current result of TPoS tagging is 
based on 2 simple induction rules applied in this 
algorithm. 
4 Experiments and Discussions 
The domain corpus used in this work contains 
16 papers selected from different Chinese IT jour-
nals between 1998 and 2000 with over 1,500,000 
numbers of characters. They cover topics in IT, 
such as electronics, software engineering, telecom, 
and wireless communication. The same corpus is 
used by the terminology extraction algorithm de-
veloped in (Luning Ji et al, 2007). In the domain 
of IT, two TermLists are used for the experiment. 
TermList1 is a manually collected and verified 
term list from the selected corpus containing a total 
of 3,343 terms. TermList1 is also referred to as the 
standard answer set to the corpus for evaluation 
purposes. TermList2 is produced by running the 
terminology extraction algorithm in (Van Rees, R, 
2003). TermList2 contains 2,660 items out of 
which 929 of them are verified as terminology and 
1,731 items are not considered terminology ac-
cording to the standard answer above. 
To verify the validity of the proposed method to 
different domains, a term list containing 366 legal 
terms obtained from Google searching results for 
??????? ?(complete dictionary of legal 
terms) is selected for comparison, which is named 
TermList3. 
4.1 Experiment on the Blind Assignment 
Scheme 
The first experiment is designed to examine the 
proportion of nouns in TermList1 and TermList3, 
to validate of the assumption of the blind assign-
ment scheme. In first part of this experiment, all 
the 3,343 terms in TermList1 are tagged as nouns. 
The result shows that the precision of the blind 
assignment scheme is between 78.79% and 84.77%. 
The reason for the range is that there are about 200 
terms in TermList1 which can be considered either 
as nouns, gerunds, or even verbs without reference 
to context. For example, the term ???????
?? (?remote access of local area network? or ?re-
mote access to local area network?) and the term 
???? (polarization or polarize), can be consid-
ered either as nouns if they are regarded as courses 
of events or as verbs if they refer to the actions for 
completing certain work. The specific type is de-
pendent on the context which is not provided with-
out the use of a corpus. However, the experiment 
result does show that in a specific domain, there is 
a much higher percentage of terms that are nouns 
than other tags in general (Hui Wang, 2006). As to 
TermList3, the precision of blind assignment is 
between 65.57% and 70.77% (19 mixed ones). 
TermList2 is the result of a terminology extraction 
algorithm and there are non-term items in the ex-
traction result, so the blind assignment scheme is 
not applied on TermList2. The blue colored bars 
(lighter color) in Figure 1 shows the result of 
TermList1 and TermList3 using the blind assign-
ment scheme which gives the two worst result 
compared to our proposed approach to be dis-
cussed in Section 4.2 
4.2 Experiments on the Head-Word-Driven 
Assignment Scheme 
The experiment in this section was designed to 
validate the proposed head-word-driven assign-
ment scheme. The same experiment is conducted 
on the three term lists respectively, as shown in 
Figure 1 in purple color (darker color). The preci-
sion for assigning TPoS tags to TermList1 is 
93.45%.  By taking the result from a terminology 
extraction algorithm without regards to its potential 
error propagations, the precision of the head-word-
driven assignment scheme for TermList2 is 
94.32%. For TermList3, the precision of PoS tag 
assignment is 90.71%. By comparing to the blind 
assignment scheme, this algorithm has reasonably 
good performance for all three term list with 
precision of over 90%. It also gives 8.7% and 
19.9% improvement for TermList1 and TermList3, 
respectively, as compared to the blind assignment 
The 6th Workshop on Asian Languae Resources, 2008
20
scheme, a reasonably good improvement without a 
heavy cost.  However, there are some abnormali-
ties in these results. Supposedly, TermList1 is a 
hand verified term list in the IT domain and thus its 
result should have less noise and thus should per-
form better than TermList2, which is not the case 
as shown in Figure 1. 
Figure 1 Performance of the Two Assignment 
Schemes on the Three Term Lists 
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
T1 T2 T3
Term Lists
Pr
ec
isi
on
blind assignment
head-driven
assignment
 
By further analyzing the error result, for exam-
ple for TermList1, among these 3,343 terms, about 
219 were given improper tags, such as the term ??
??? (Graphics). In this example, two individual 
words, ???/n? and ??/v?, form a term. So the 
output was ????/v? for taking the tag of the last 
segment. It was a wrong tag because the whole 
term was a noun. In fact, the error is caused by the 
general word PoS tagging algorithm because with-
out context, the most likely tagging of ???, a se-
manteme, is a verb. This kind of errors in se-
manteme tagging appeared in the results of all 
three term lists with 169 from TermList1, 29 from 
TermList2 and 12 from TermList3, respectively. 
This was a kind of errors which can be corrected 
by applying some simple induction rules. For ex-
ample, for all semantemes with multiple tags (in-
cluding noun as in the example), the rule can be 
?tagging terms with noun suffixes as nouns?. For 
example, terms ???/n ?/q? (reform-through-
labor camp) and ????/n ??/n ?/v? (com-
puter graphics) were given different tags using the 
head-word-driven assignment scheme. They were 
assigned as: ????/q? and ???????/v? 
which can be corrected by this rule.   Another kind 
of mistake is related to the suffix tags such as ?/ng? 
(noun suffix) and ?/vg?(verb suffix). For examples, 
???/n ??/n ?/ng? (intellectual property tri-
bunal) and ???/n ?/vg? (data set) will be tagged 
as ?????? /ng? and ???? /vg?, respec-
tively, which are obviously wrong. So, the simple 
rule of ?tagging terms with ?/ng? and ?/vg? to ?/n? 
is applied. The performance of TPoS tag assign-
ment after applying these two fine tuning induction 
rules are shown in Table 1 below. 
Table 1 Influence of Induction Rules on Different 
Term Lists 
Term 
Lists 
Precision 
of tagging
Precision 
after add-
ing induc-
tion rule 
Improve-
ment 
Percentage
TermList1 93.45% 97.03% 3.83% 
TermList2 94.32% 95.41% 1.16% 
TermList3 90.71% 93.99% 3.62% 
It is obvious that with the use of fine tuning us-
ing induction rules, the results are much better. In 
fact the result for TermList1 reached 97.03% 
which is quite close to PoS tagging of general do-
main data. The abnormality also disappeared as the 
performance of TermList1 has the best result. The 
improvement to TermList2 (1.16%) is not as obvi-
ous as that for TermList1 and TermList3, which 
are 3.83% and 3.62%, respectively. This, however, 
is reasonable as TermList2 is produced directly 
from a terminology extraction algorithm using a 
corpus, thus, the results are noisier. 
Further analysis is then conducted on the result 
of TermList2 to examine the influence of non-term 
items to this term list. The non-term items are 
items that are general words or items cannot be 
considered as terminology according to the stan-
dard answer sheet. For example, neither of the 
terms ???? (problem) and ??????? (pat-
tern training is) were considered as terms because 
the former was a general word, and the latter 
should be considered as a fragment rather than a 
word. In fact, in 2,660 items extracted by the algo-
rithm as terminology, only 929 of them are indeed 
terminology (34.92%), and rest of them do not 
qualify as domain specific terms. The result of this 
analysis is listed in Table 2. 
The 6th Workshop on Asian Languae Resources, 2008
21
Table 2 Data Distribution Analysis on TermList2 
Without Induction 
Rules 
Induction Rules 
Applied  
correct 
terms precision 
correct 
terms precision 
Terms 
(929)  879 94.62%  898 96.66%
Non-terms 
(1,731) 1,630 94.17% 1,640 94.74% 
Total 
(2,660) 2,509 94.32% 2,538 95.41% 
Results show that 31 and 50 from the 929 cor-
rect terms were assigned improper PoS tags using 
the proposed algorithm with and without the induc-
tions rules, respectively. That is, the precisions for 
correct data are comparable to that of TermList1 
(93.45% and 97.03%, respectively). For the non-
terms, 91 items and 101 items from 1,731 items 
were assigned improper tags with and without the 
induction rules, respectively. Even though the pre-
cisions for terms and non-terms without using the 
induction rules are quite the same (94.62% vs. 
94.17%), the improvement for the non-terms using 
the induction rules are much less impressive than 
that for the terms. This is the reason for the rela-
tively less impressed performance of induction 
rules for TermList2.  It is interesting to know that, 
even though the performance of the terminology 
extraction algorithm is quite poor with precision of 
only around 35% (929 out of 2,666 terms), it does 
not affect too much on the performance of the 
TPoS proposed in this paper. This is mainly be-
cause the items extracted are still legitimate words, 
compounds, or phrases which are not necessarily 
domain specific. 
The proposed algorithm in this paper use mini-
mum resources. They need no training process and 
even no context information. But the performance 
of the proposed algorithm is still quite good and 
can be directly used as a preparation work for do-
main ontology construction because of its presion 
of over 95%. Other PoS tagging algorithms reach 
good performance in processing general words. 
For example, a k-nearest-neighbors strategy to 
identify possible PoS tags for Chinese words can 
reach 90.25% for general word PoS tagging 
(Maosong Sun et al, 2000). Another method based 
on SVM method on English corpus can reach 
96.9% in PoS tagging known and unknown words 
(Nakagawa T, 2001). These results show that pro-
posed method in this paper is comparable to these 
general PoS tagging algorithms in magnitude. Of 
course, one main reason of this fact is the differ-
ence in its objectives. The proposed method is for 
the PoS tagging of domain specific terms which 
have much less ambiguity than tagging of general 
text. Domain specific terms are more likely to be 
nouns and there are some rules in the word-
formation patterns while general PoS tagging algo-
rithms usually need training process in which large 
manually labeled corpora would be involved. Ex-
periment results also show that this simple method 
can be applied to data in different domains. 
5 Conclusion and Future Work 
In this paper, a simple but effective method for 
assigning PoS tags to domain specific terms was 
presented. This is a preliminary classification work 
on terms. It needs no training process and not even 
context information. Yet it obtains a relatively 
good result. The method itself is not domain de-
pendent, thus it is applicable to different domains. 
Results show that in certain applications, a simple 
method may be more effective under similar cir-
cumstances. The algorithm can still be investigated 
over the use of more induction rules. Some context 
information, statistics of word/tag usage can also 
be explored. 
Acknowledgments 
This project is partially supported by CERG 
grants (PolyU 5190/04E and PolyU 5225/05E) and 
B-Q941 (Acquisition of New Domain Specific 
Concepts and Ontology Update). 
References 
Yun Li, Qiangjun Wang. 2001. Automatic Term Extrac-
tion in the Field of Information Technology. In the 
proceedings of The Conference of 20th Anniversary 
for Chinese Information Processing Society of China.  
Yan He, Zhifang Sui, Huiming Duan, and Shiwen Yu. 
2006. Term Mining Combining Term Component 
Bank. In Computer Engineering and Applications. 
Vol.42 No.33,4--7. 
Feng Zhang, Xiaozhong Fan, and Yun Xu. 2006. Chi-
nese Term Extraction Based on PAT Tree. Journal of 
Beijing Institute of Technology. Vol. 15, No. 2. 
Van Rees, R. 2003. Clarity in the Usage of the Terms 
Ontology, Taxonomy and Classification. CIB73. 
The 6th Workshop on Asian Languae Resources, 2008
22
Mikheev, A. 1997. Automatic Rule Induction. for Un-
known Word Guessing. In Computational Lingusitics 
Vol. 23(3), ACL.  
Toutanova, Kristina, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In proceedings of HLT-NAACL. 
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005.Morphological Features Help POS Tag-
ging of Unknown Words across Language Varieties. 
In proceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing. 
Samuelsson, Christer. 1993. Morphological Tagging 
Based Entirely on Bayesian Inference. In proceedings 
of NCCL 9. 
Brants, Thorsten. 2000. TnT: A Statistical Part-of-
Speech Tagger. In proceedings of ANLP 6. 
G. Orphanos, and D. Christodoulakis. 1999. POS Dis-
ambiguation and Unknown Word Guessing with De-
cision Trees. In proceedings of EACL?99, 134--141. 
H Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In proceedings of International 
Conference on New Methods in Language Processing.  
Maosong Sun, Dayang Shen, and Changning Huang. 
1997. CSeg& Tag1.0: a practical word segmenter 
and POS tagger for Chinese texts. In proceedings of 
the fifth conference on applied natural language 
processing.  
Ying Liu. 2002. Analysing Chinese with Rule-based 
Method Combined with Statistic-based Method. In 
Computer Engineering and Applications, Vol.7. 
Mikheev, A. 1996. Unsupervised Learning of Word-
Category Guessing Rules. In proceedings of ACL-96.  
Nakagawa T, Kudoh T, and Matsumoto Y. 2001. Un-
known Word Guessing and Part-of-Speech Tagging 
Using Support Vector Machines. In proceedings of 
NLPPRS 6, 325--331. 
Maosong Sun, Zhengping Zuo, and B K, TSOU. 2000. 
Part-of-Speech Identification for Unknown Chinese 
Words Based on K-Nearest-Neighbors Strategy. In 
Chinese Journal of Computers. Vol.23 No.2: 166--
170. 
Luning Ji, Mantai Sum, Qin Lu, Wenjie Li, Yirong 
Chen. 2007. Chinese Terminology Extraction using 
Window-based Contextual Information. In proceed-
ings of CICLING. 
Huaping Zhang et al 2003. HHMM-based Chinese 
Lexical Analyzer ICTCLAS. Second SIGHAN work-
shop affiliated with 41th ACL, 184--187. Sapporo 
Japan. 
Hui Wang. Last checked: 2007-08-04. Statistical studies 
on Chinese vocabulary (???????? ). 
http://www.huayuqiao.org/articles/wanghui/wanghui
06.doc. The date of publication is unknown from the 
online source. 
Ruifeng Xu, Qin Lu, Yin Li and Wanyin Li. 2005. The 
Design and Construction of the PolyU Shallow Tree-
bank. International Journal of Computational Lin-
guistics and Chinese Language Processing, V.10 N.3. 
Yunfang Wu, Baobao Chang and Weidong Zhan. 2003. 
Building Chinese-English Bilingual Phrase Database. 
Page 41-45, Vol. 4. 
The 6th Workshop on Asian Languae Resources, 2008
23
The 6th Workshop on Asian Languae Resources, 2008
24
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Extractive Summarization using Inter- and Intra- Event Relevance 
 
Wenjie Li, Mingli Wu and Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University 
{cswjli,csmlwu,csluqin}@comp
.polyu.edu.hk 
Wei Xu and Chunfa Yuan 
Department of Computer Science and 
Technology, Tsinghua University 
{vivian00,cfyuan}@mail.ts
inghua.edu.cn 
 
 
 
Abstract 
Event-based summarization attempts to 
select and organize the sentences in a 
summary with respect to the events or 
the sub-events that the sentences de-
scribe. Each event has its own internal 
structure, and meanwhile often relates to 
other events semantically, temporally, 
spatially, causally or conditionally. In 
this paper, we define an event as one or 
more event terms along with the named 
entities associated, and present a novel 
approach to derive intra- and inter- event 
relevance using the information of inter-
nal association, semantic relatedness, 
distributional similarity and named en-
tity clustering. We then apply PageRank 
ranking algorithm to estimate the sig-
nificance of an event for inclusion in a 
summary from the event relevance de-
rived. Experiments on the DUC 2001 
test data shows that the relevance of the 
named entities involved in events 
achieves better result when their rele-
vance is derived from the event terms 
they associate. It also reveals that the 
topic-specific relevance from documents 
themselves outperforms the semantic 
relevance from a general purpose 
knowledge base like Word-Net. 
 
 
1. Introduction 
Extractive summarization selects sentences 
which contain the most salient concepts in 
documents. Two important issues with it are 
how the concepts are defined and what criteria 
should be used to judge the salience of the con-
cepts. Existing work has typically been based on 
techniques that extract key textual elements, 
such as keywords (also known as significant 
terms) as weighed by their tf*idf score, or con-
cepts (such as events or entities) with linguistic 
and/or statistical analysis. Then, sentences are 
selected according to either the important textual 
units they contain or certain types of inter-
sentence relations they hold.  
Event-based summarization which has e-
merged recently attempts to select and organize 
sentences in a summary with respect to events or 
sub-events that the sentences describe. With re-
gard to the concept of events, people do not 
have the same definition when introducing it in 
different domains. While traditional linguistics 
work on semantic theory of events and the se-
mantic structures of verbs, studies in 
information retrieval (IR) within topic detection 
and tracking framework look at events as 
narrowly defined topics which can be 
categorized or clustered as a set of related 
documents (TDT). IR events are broader (or to 
say complex) events in the sense that they may 
include happenings and their causes, 
consequences or even more extended effects. In 
the information extraction (IE) community, 
events are defined as the pre-specified and struc-
tured templates that relate an action to its 
participants, times, locations and other entities 
involved (MUC-7). IE defines what people call 
atomic events. Regardless of their distinct perspectives, peo-
ple all agree that events are collections of activi-
ties together with associated entities. To apply 
the concept of events in the context of text sum-
marization, we believe it is more appropriate to 
consider events at the sentence level, rather than 
at the document level. To avoid the complexity 
of deep semantic and syntactic processing, we 
complement the advantages of statistical 
techniques from the IR community and struc-
tured information provided by the IE community. 
369
 We propose to extract semi-structured events 
with shallow natural language processing (NLP) 
techniques and estimate their importance for 
inclusion in a summary with IR techniques. 
Though it is most likely that documents nar-
rate more than one similar or related event, most 
event-based summarization techniques reported 
so far explore the importance of the events inde-
pendently. Motivated by this observation, this 
paper addresses the task of event-relevance 
based summarization and explores what sorts of 
relevance make a contribution. To this end, we 
investigate intra-event relevance, that is action-
entity relevance, and inter-event relevance, that 
is event-event relevance. While intra-event rele-
vance is measured with frequencies of the asso-
ciated events and entities directly, inter-event 
relevance is derived indirectly from a general 
WordNet similarity utility, distributional simi-
larity in the documents to be summarized, 
named entity clustering and so on. Pagerank 
ranking algorithm is then applied to estimate the 
event importance for inclusion in a summary 
using the aforesaid relevance.  
The remainder of this paper is organized as 
follows. Section 2 introduces related work. Sec-
tions 3 introduces our proposed event-based 
summarization approaches which make use of 
intra- and inter- event relevance. Section 4 pre-
sents experiments and evaluates different ap-
proaches. Finally, Section 5 concludes the paper. 
2. Related Work 
Event-based summarization has been investi-
gated in recent research. It was first presented in 
(Daniel, Radev and Allison, 2003), who treated 
a news topic in multi-document summarization 
as a series of sub-events according to human 
understanding of the topic. They determined the 
degree of sentence relevance to each sub-event 
through human judgment and evaluated six ex-
tractive approaches. Their paper concluded that 
recognizing the sub-events that comprise a sin-
gle news event is essential for producing better 
summaries. However, it is difficult to automati-
cally break a news topic into sub-events.  
Later, atomic events were defined as the rela-
tionships between the important named entities 
(Filatova and Hatzivassiloglou, 2004), such as 
participants, locations and times (which are 
called relations) through the verbs or action 
nouns labeling the events themselves (which are 
called connectors). They evaluated sentences 
based on co-occurrence statistics of the named 
entity relations and the event connectors in-
volved. The proposed approach claimed to out-
perform conventional tf*idf approach. Appar-
ently, named entities are key elements in their 
model. However, the constraints defining events 
seemed quite stringent.  
The application of dependency parsing, 
anaphora and co-reference resolution in recog-
nizing events were presented involving NLP and 
IE techniques more or less (Yoshioka and Hara-
guchi, 2004), (Vanderwende, Banko and Mene-
zes, 2004) and (Leskovec, Grobelnik and Fral-
ing, 2004). Rather than pre-specifying events, 
these efforts extracted (verb)-(dependent rela-
tion)-(noun) triples as events and took the triples 
to form a graph merged by relations.  
As a matter of fact, events in documents are 
related in some ways. Judging whether the sen-
tences are salient or not and organizing them in 
a coherent summary can take advantage from 
event relevance. Unfortunately, this was ne-
glected in most previous work. Barzilay and La-
pata (2005) exploited the use of the distribu-
tional and referential information of discourse 
entities to improve summary coherence. While 
they captured text relatedness with entity transi-
tion sequences, i.e. entity-based summarization, 
we are particularly interested in relevance be-
tween events in event-based summarization. 
Extractive summarization requires ranking 
sentences with respect to their importance. 
Successfully used in Web-link analysis and 
more recently in text summarization, Google?s 
PageRank (Brin and Page, 1998) is one of the 
most popular ranking algorithms. It is a kind of 
graph-based ranking algorithm deciding on the 
importance of a node within a graph by taking 
into account the global information recursively 
computed from the entire graph, rather than re-
lying on only the local node-specific infor-
mation. A graph can be constructed by adding a 
node for each sentence, phrase or word. Edges 
between nodes are established using inter-
sentence similarity relations as a function of 
content overlap or grammatically relations be-
tween words or phrases.  
The application of PageRank in sentence ex-
traction was first reported in (Erkan and Radev, 
2004). The similarity between two sentence 
nodes according to their term vectors was used 
to generate links and define link strength. The 
same idea was followed and investigated exten-
370
 sively (Mihalcea, 2005). Yoshioka and Haragu-
chi (2004) went one step further toward event-
based summarization. Two sentences were 
linked if they shared similar events. When tested 
on TSC-3, the approach favoured longer sum-
maries. In contrast, the importance of the verbs 
and nouns constructing events was evaluated 
with PageRank as individual nodes aligned by 
their dependence relations (Vanderwende, 2004; 
Leskovec, 2004).  
Although we agree that the fabric of event 
constitutions constructed by their syntactic rela-
tions can help dig out the important events, we 
have two comments. First, not all verbs denote 
event happenings. Second, semantic similarity 
or relatedness between action words should be 
taken into account. 
3. Event-based Summarization 
3.1. Event Definition and Event Map 
Events can be broadly defined as ?Who did 
What to Whom When and Where?. Both lin-
guistic and empirical studies acknowledge that 
event arguments help characterize the effects of 
a verb?s event structure even though verbs or 
other words denoting event determine the se-
mantics of an event. In this paper, we choose 
verbs (such as ?elect?) and action nouns (such as 
?supervision?) as event terms that can character-
ize or partially characterize actions or incident 
occurrences. They roughly relate to ?did What?. 
One or more associated named entities are con-
sidered as what are denoted by linguists as event 
arguments. Four types of named entities are cur-
rently under the consideration. These are <Per-
son>, <Organization>, <Location> and <Date>. 
They convey the information of ?Who?, 
?Whom?, ?When? and ?Where?. A verb or an 
action noun is deemed as an event term only 
when it presents itself at least once between two 
named entities. 
Events are commonly related with one an-
other semantically, temporally, spatially, caus-
ally or conditionally, especially when the docu-
ments to be summarized are about the same or 
very similar topics. Therefore, all event terms 
and named entities involved can be explicitly 
connected or implicitly related and weave a 
document or a set of documents into an event 
fabric, i.e. an event graphical representation (see 
Figure 1). The nodes in the graph are of two 
types. Event terms (ET) are indicated by rectan-
gles and named entities (NE) are indicated by 
ellipses. They represent concepts rather than 
instances. Words in either their original form or 
morphological variations are represented with a 
single node in the graph regardless of how many 
times they appear in documents. We call this 
representation an event map, from which the 
most important concepts can be pick out in the 
summary. 
 
 
 
Figure 1 Sample sentences and their graphical representation 
 
 
The advantage of representing with separated 
action and entity nodes over simply combining 
them into one event or sentence node is to pro-
vide a convenient way for analyzing the rele-
vance among event terms and named entities 
either by their semantic or distributional similar-
ity. More importantly, this favors extraction of 
concepts and brings the conceptual compression 
available. 
We then integrate the strength of the connec-
tions between nodes into this graphical model in 
terms of the relevance defined from different 
perspectives. The relevance is indicated by 
),( ji nodenoder , where inode  and jnode  repre-
sent two nodes, and are either event terms ( iet ) 
or named entities ( jne ). Then, the significance 
of each node, indicated by )( inodew , is calcu-
<Organization> America Online </Organization> was to buy <Organization> 
Netscape </Organization> and forge a partnership with <Organization> Sun 
</Organization>, benefiting all three and giving technological independence 
from <Organization> Microsoft </Organization>. 
371
 lated with PageRank ranking algorithm. Sec-
tions 3.2 and 3.3 address the issues of deriving 
),( ji nodenoder  according to intra- or/and inter- 
event relevance and calculating )( inodew  in de-
tail. 
3.2 Intra- and Inter- Event Relevance 
We consider both intra-event and inter-event 
relevance for summarization. Intra-event rele-
vance measures how an action itself is associ-
ated with its associated arguments. It is indi-
cated as ),( NEETR  and ),( ETNER  in Table 1 
below. This is a kind of direct relevance as the 
connections between actions and arguments are 
established from the text surface directly. No 
inference or background knowledge is required. 
We consider that when the connection between 
an event term iet  and a named entity jne  is 
symmetry, then TNEETRETNER ),(),( = . Events 
are related as explained in Section 2. By means 
of inter-event relevance, we consider how an 
event term (or a named entity involved in an 
event) associate to another event term (or an-
other named entity involved in the same or dif-
ferent events) syntactically, semantically and 
distributionally. It is indicated by ),( ETETR or 
),( NENER in Table 1 and measures an indirect 
connection which is not explicit in the event 
map needing to be derived from the external 
resource or overall event distribution. 
 Event Term 
(ET) 
Named En-
tity (NE) 
Event Term (ET) ),( ETETR  ),( NEETR  
Named Entity (NE) ),( ETNER  ),( NENER
Table 1 Relevance Matrix 
The complete relevance matrix is: 
??
???
?=
),(),(
),(),(
NENERETNER
NEETRETETR
R  
The intra-event relevance ),( NEETR can be 
simply established by counting how many times 
iet  and jne  are associated, i.e.  
),(),( jijiDocument neetfreqneetr =  (E1) 
One way to measure the term relevance is to 
make use of a general language knowledge base, 
such as WordNet (Fellbaum 1998). Word-
Net::Similarity is a freely available software 
package that makes it possible to measure the 
semantic relatedness between a pair of concepts, 
or in our case event terms, based on WordNet 
(Pedersen, Patwardhan and Michelizzi, 2004). It 
supports three measures. The one we choose is 
the function lesk. 
),(),(),( jijijiWordNet etetlesketetsimilarityetetr ==
      (E2) 
Alternatively, term relevance can be meas-
ured according to their distributions in the speci-
fied documents. We believe that if two events 
are concerned with the same participants, occur 
at same location, or at the same time, these two 
events are interrelated with each other in some 
ways. This observation motivates us to try deriv-
ing event term relevance from the number of 
name entities they share. 
|)()(|),( jijiDocument etNEetNEetetr ?=  (E3) 
Where )( ietNE is the set of named entities iet  
associate. | | indicates the number of the ele-
ments in the set. The relevance of named entities 
can be derived in a similar way. 
|)()(|),( jijiDocument neETneETnener ?=  (E4) 
The relevance derived with (E3) and (E4) are 
indirect relevance. In previous work, a cluster-
ing algorithm, shown in Figure 2, has been pro-
posed (Xu et al 2006) to merge the named en-
tity that refer to the same person (such as 
Ranariddh, Prince Norodom Ranariddh and Presi-
dent Prince Norodom Ranariddh). It is used for 
co-reference resolution and aims at joining the 
same concept into a single node in the event 
map. The experimental result suggests that 
merging named entity improves performance in 
some extend but not evidently. When applying 
the same algorithm for clustering all four types 
of name entities in DUC data, we observe that 
the name entities in the same cluster do not al-
ways refer to the same objects, even when they 
are indeed related in some way. For example, 
?Mississippi? is a state in the southeast United 
States, while ?Mississippi River? is the second-
longest rever in the United States and flows 
through ?Mississippi?. 
Step1: Each name entity is represented by 
ikiii wwwne ...21= , where iw  is the ith 
word in it. The cluster it belongs to, in-
dicated by )( ineC , is initialled by 
ikii www ...21 itself.  
Step2: For each name entity  
           ikiii wwwne ...21=  
For each name entity 
372
 jljjj wwwne ...21= , if )( ineC  is a 
sub-string of )( jneC , then 
)()( ji neCneC = . 
Continue Step 2 until no change occurs. 
Figure 2 The algorithm proposed to merge the 
named entities 
Location Person Date Organization
Mississippi 
 
Professor Sir 
Richard 
Southwood 
first six 
months of 
last year 
Long Beach 
City Council 
Sir Richard 
Southwood 
San Jose City 
Council 
Mississippi 
River 
Richard 
Southwood 
last year 
City Council 
Table 2 Some results of the named entity 
merged 
It therefore provides a second way to measure 
named entity relevance based on the clusters 
found. It is actually a kind of measure of lexical 
similarity. 
??
?=
otherwise      ,0
cluster same in the are ,      ,1
),( jijiCluster
nene
nener
     (E5) 
In addition, the relevance of the named enti-
ties can be sometimes revealed by sentence con-
text. Take the following most frequently used 
sentence patterns as examples: 
 
Figure 3 The example patterns  
Considering that two neighbouring name enti-
ties in a sentence are usually relevant, the fol-
lowing window-based relevance is also experi-
mented with. 
??
?=
otherwise      ,0
size  windowspecified-pre a within are ,      1,
),(
ji
jiPattern
nene
nener
     (E6) 
3.3 Significance of Concepts 
The significance score, i.e. the weight 
)( inodew  of each inode , is then estimated recur-
sively with PageRank ranking algorithm which 
assigns the significance score to each node ac-
cording to the number of nodes connecting to it 
as well as the strength of their connections. The 
equation calculating )( inodew using PageRank 
of a certain inode  is shown as follows. 
)
),(
)(
...
),(
)(
...
),(
)(()1()(
1
1
ti
t
ji
j
i
i
nodenoder
nodew
nodenoder
nodew
nodenoder
nodewddnodew
+++
++?=
 (E7) 
In (E7), jnode ( tj ,...2,1= , ij ? ) are the 
nodes linking to inode . d is the factor used to 
avoid the limitation of loop in the map structure. 
It is set to 0.85 experimentally. The significance 
of each sentence to be included in the summary 
is then obtained from the significance of the 
events it contains. The sentences with higher 
significance are picked up into the summary as 
long as they are not exactly the same sentences. 
We are aware of the important roles of informa-
tion fusion and sentence compression in sum-
mary generation. However, the focus of this pa-
per is to evaluate event-based approaches in ex-
tracting the most important sentences. Concep-
tual extraction based on event relevance is our 
future direction. 
4. Experiments and Discussions 
To evaluate the event based summarization ap-
proaches proposed, we conduct a set of experi-
ments on 30 English document sets provide by 
the DUC 2001 multi-document summarization 
task. The documents are pre-processed with 
GATE to recognize the previously mentioned 
four types of name entities. On average, each set 
contains 10.3 documents, 602 sentences, 216 
event terms and 148.5 name entities. 
To evaluate the quality of the generated 
summaries, we choose an automatic summary 
evaluation metric ROUGE, which has been used 
in DUCs. ROUGE is a recall-based metric for 
fixed length summaries. It bases on N-gram co-
occurrence and compares the system generated 
summaries to human judges (Lin and Hovy, 
2003). For each DUC document set, the system 
creates a summary of 200 word length and pre-
sent three of the ROUGE metrics: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on longest common sub-
sequence weighed by the length) in the follow-
ing experiments and evaluations.  
We first evaluate the summaries generated 
based on ),( NEETR  itself. In the pre-evaluation 
experiments, we have observed that some fre-
<Person>, a-position-name of <Organization>, 
does something. 
<Person> and another <Person> do something. 
373
 quently occurring nouns, such as ?doctors? and 
?hospitals?, by themselves are not marked by 
general NE taggers. But they indicate persons, 
organizations or locations. We compare the 
ROUGE scores of adding frequent nouns or not 
to the set of named entities in Table 3. A noun is 
considered as a frequent noun when its fre-
quency is larger than 10. Roughly 5% improve-
ment is achieved when high frequent nouns are 
taken into the consideration. Hereafter, when we 
mention NE in latter experiments, the high fre-
quent nouns are included. 
),( NEETR  NE Without High 
Frequency Nouns 
NE With High 
Frequency Nouns
ROUGE-1 0.33320 0.34859 
ROUGE-2 0.06260 0.07157 
ROUGE-W 0.12965 0.13471 
Table 3 ROUGE scores using ),( NEETR  itself 
Table 4 below then presents the summariza-
tion results by using ),( ETETR  itself. It com-
pares two relevance derivation approaches, 
WordNetR  and DocumentR . The topic-specific rele-
vance derived from the documents to be summa-
rized outperforms the general purpose Word-Net 
relevance by about 4%. This result is reasonable 
as WordNet may introduce the word relatedness 
which is not necessary in the topic-specific 
documents. When we examine the relevance 
matrix from the event term pairs with the high-
est relevant, we find that the pairs, like ?abort? 
and ?confirm?, ?vote? and confirm?, do reflect 
semantics (antonymous) and associated (causal) 
relations to some degree.  
),( ETETR  Semantic Rele-
vance from 
Word-Net 
Topic-Specific 
Relevance from 
Documents 
ROUGE-1 0.32917 0.34178 
ROUGE-2 0.05737 0.06852 
ROUGE-W 0.11959 0.13262 
Table 4 ROUGE scores using ),( ETETR  itself 
Surprisingly, the best individual result is from 
document distributional similarity DocumentR  
),( NENE  in Table 5. Looking more closely, we 
conclude that compared to event terms, named 
entities are more representative of the docu-
ments in which they are included. In other words, 
event terms are more likely to be distributed 
around all the document sets, whereas named 
entities are more topic-specific and therefore 
cluster in a particular document set more. Ex-
amples of high related named entities in rele-
vance matrix are ?Andrew? and ?Florida?, 
?Louisiana? and ?Florida?. Although their rele-
vance is not as explicit as the same of event 
terms (their relevance is more contextual than 
semantic), we can still deduce that some events 
may happen in both Louisiana and Florida, or 
about Andrew in Florida. In addition, it also 
shows that the relevance we would have ex-
pected to be derived from patterns and clustering 
can also be discovered by ),( NENERDocument . 
The window size is set to 5 experimentally in 
window-based practice.  
),( NENER Relevance 
from 
Documents
Relevance 
from 
Clustering 
Relevance 
from Window-
based Context
ROUGE-1 0.35212 0.33561 0.34466 
ROUGE-2 0.07107 0.07286 0.07508 
ROUGE-W 0.13603 0.13109 0.13523 
Table 5 ROUGE scores using ),( NENER  itself 
Next, we evaluate the integration of 
),( NEETR , ),( ETETR  and ),( NENER . As 
DUC 2001 provides 4 different summary sizes 
for evaluation, it satisfies our desire to test the 
sensibility of the proposed event-based summa-
rization techniques to the length of summaries. 
While the previously presented results are 
evaluated on 200 word summaries, now we 
move to check the results in four different sizes, 
i.e. 50, 100, 200 and 400 words. The experi-
ments results show that the event-based ap-
proaches indeed prefer longer summaries. This 
is coincident with what we have hypothesized. 
For this set of experiments, we choose to inte-
grate the best method from each individual 
evaluation presented previously. It appears that 
using the named entities relevance which is de-
rived from the event terms gives the best 
ROUGE scores in almost all the summery sizes. 
Compared with the results provided in (Filatova 
and Hatzivassiloglou, 2004) whose average 
ROUGE-1 score is below 0.3 on the same data 
set, the significant improvement is revealed. Of 
course, we need to test on more data in the fu-
ture. 
),( NENER 50 100 200 400 
ROUGE-1 0.22383 0.28584 0.35212 0.41612
ROUGE-2 0.03376 0.05489 0.07107 0.10275
ROUGE-W 0.10203 0.11610 0.13603 0.13877
),( NEETR 50 100 200 400 
ROUGE-1 0.22224 0.27947 0.34859 0.41644
ROUGE-2 0.03310 0.05073 0.07157 0.10369
ROUGE-W 0.10229 0.11497 0.13471 0.13850
),( ETETR 50 100 200 400 
374
 ROUGE-1 0.20616 0.26923 0.34178 0.41201
ROUGE-2 0.02347 0.04575 0.06852 0.10263
ROUGE-W 0.09212 0.11081 0.13262 0.13742
),( NEETR + 
),( ETETR + 
),( NENER  
 
50 
 
100 
 
200 
 
400 
ROUGE-1 0.21311 0.27939 0.34630 0.41639
ROUGE-2 0.03068 0.05127 0.07057 0.10579
ROUGE-W 0.09532 0.11371 0.13416 0.13913
Table 6 ROUGE scores using complete R matrix 
and with different summary lengths 
As discussed in Section 3.2, the named enti-
ties in the same cluster may often be relevant but 
not always be co-referred. In the following last 
set of experiments, we evaluate the two ways to 
use the clustering results. One is to consider 
them as related as if they are in the same cluster 
and derive the NE-NE relevance with (E5). The 
other is to merge the entities in one cluster as 
one reprehensive named entity and then use it in 
ET-NE with (E1). The rationality of the former 
approach is validated. 
 Clustering is 
used to derive 
NE-NE 
Clustering is used to 
merge entities and 
then to derive ET-NE
ROUGE-1 0.34072 0.33006 
ROUGE-2 0.06727 0.06154 
ROUGE-W 0.13229 0.12845 
Table 7 ROUGE scores with regard to how to 
use the clustering information 
5. Conclusion 
In this paper, we propose to integrate event-
based approaches to extractive summarization. 
Both inter-event and intra-event relevance are 
investigated and PageRank algorithm is used to 
evaluate the significance of each concept (in-
cluding both event terms and named entities). 
The sentences containing more concepts and 
highest significance scores are chosen in the 
summary as long as they are not the same sen-
tences.  
To derive event relevance, we consider the 
associations at the syntactic, semantic and con-
textual levels. An important finding on the DUC 
2001 data set is that making use of named entity 
relevance derived from the event terms they as-
sociate with achieves the best result. The result 
of 0.35212 significantly outperforms the one 
reported in the closely related work whose aver-
age is below 0.3. We are interested in the issue 
of how to improve an event representation in 
order to build a more powerful event-based 
summarization system. This would be one of our 
future directions. We also want to see how con-
cepts rather than sentences are selected into the 
summary in order to develop a more flexible 
compression technique and to know what char-
acteristics of a document set is appropriate for 
applying event-based summarization techniques.  
 
Acknowledgements 
The work presented in this paper is supported 
partially by Research Grants Council on Hong 
Kong (reference number CERG PolyU5181/03E) 
and partially by National Natural Science Foun-
dation of China (reference number: NSFC 
60573186). 
 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL 2003, pp71-78. 
Christiane Fellbaum. 1998, WordNet: An Electronic 
Lexical Database. MIT Press. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive summarization. In Pro-
ceedings of ACL 2004 Workshop on Summariza-
tion, pp104-111.  
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of Docu-
ment Semantic Graphs for Document Summariza-
tion. In LinkKDD 2004.  
Lucy Vanderwende, Michele Banko and Arul Mene-
zes. 2004. Event-Centric Summary Generation. In 
Working Notes of DUC 2004. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information. In Working Notes 
of NTCIR-4, Tokyo. 
MUC-7. http://www-nlpir.nist.gov/related_projects/ 
muc/proceeings/ muc_7_toc.html 
Naomi Daniel, Dragomir Radev and Timothy Allison. 
2003. Sub-event based Multi-document Summari-
zation. In Proceedings of the HLT-NAACL 2003 
Workshop on Text Summarization, pp9-16. 
375
 Page Lawrence, Brin Sergey, Motwani Rajeev and 
Winograd Terry. 1998. The PageRank Citation 
Ranking: Bring Order to the Web. Technical Re-
port, Stanford University. 
Rada Mihalcea. 2005. Language Independent Extrac-
tive Summarization. ACL 2005 poster. 
Regina Barzilay and Michael Elhadad. 2005. Model-
ling Local Coherence: An Entity-based Approach. 
In Proceedings of ACL, pp141-148. 
TDT. http://projects.ldc.upenn.edu/TDT. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measur-
ing the Relatedness of Concepts. In Proceedings of 
AAAI, pp25-29. 
Wei Xu, Wenjie Li, Mingli Wu, Wei Li and Chunfa 
Yuan. 2006. Deriving Event Relevance from the 
Ontology Constructed with Formal Concept 
Analysis, in Proceedings of CiCling?06, pp480-
489. 
 
376
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89?92,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Novel Feature-based Approach to Chinese Entity Relation Extraction
 
Wenjie Li1, Peng Zhang1,2, Furu Wei1, Yuexian Hou2 and Qin Lu1
1Department of Computing 2School of Computer Science and Technology
The Hong Kong Polytechnic University, Hong Kong Tianjin University, China 
{cswjli,csfwei,csluqin}@comp.polyu.edu.hk {pzhang,yxhou}@tju.edu.cn 
 
 
Abstract 
Relation extraction is the task of finding 
semantic relations between two entities from 
text. In this paper, we propose a novel 
feature-based Chinese relation extraction 
approach that explicitly defines and explores 
nine positional structures between two entities. 
We also suggest some correction and inference 
mechanisms based on relation hierarchy and 
co-reference information etc. The approach is 
effective when evaluated on the ACE 2005 
Chinese data set. 
1 Introduction 
Relation extraction is promoted by the ACE program. 
It is the task of finding predefined semantic relations 
between two entities from text. For example, the 
sentence ?Bill Gates is the chairman and chief 
software architect of Microsoft Corporation? conveys 
the ACE-style relation ?ORG-AFFILIATION? 
between the two entities ?Bill Gates (PER)? and 
?Microsoft Corporation (ORG)?.  
The task of relation extraction has been extensively 
studied in English over the past years. It is typically 
cast as a classification problem. Existing approaches 
include feature-based and kernel-based classification. 
Feature-based approaches transform the context of 
two entities into a liner vector of carefully selected 
linguistic features, varying from entity semantic 
information to lexical and syntactic features of the 
context. Kernel-based approaches, on the other hand, 
explore structured representation such as parse tree 
and dependency tree and directly compute the 
similarity between trees. Comparably, feature-based 
approaches are easier to implement and achieve much 
success. 
In contrast to the significant achievements 
concerning English and other Western languages, 
research progress in Chinese relation extraction is 
quite limited. This may be attributed to the different 
characteristic of Chinese language, e.g. no word 
boundaries and lack of morphologic variations, etc. In 
this paper, we propose a character-based Chinese 
entity relation extraction approach that complements 
entity context (both internal and external) character 
N-grams with four word lists extracted from a 
published Chinese dictionary. In addition to entity 
semantic information, we define and examine nine 
positional structures between two entities. To cope 
with the data sparseness problem, we also suggest 
some correction and inference mechanisms according 
to the given ACE relation hierarchy and co-reference 
information. Experiments on the ACE 2005 data set 
show that the positional structure feature can provide 
stronger support for Chinese relation extraction. 
Meanwhile, it can be captured with less effort than 
applying deep natural language processing. But 
unfortunately, entity co-reference does not help as 
much as we have expected. The lack of necessary 
co-referenced mentions might be the main reason. 
2 Related Work 
Many approaches have been proposed in the literature 
of relation extraction. Among them, feature-based and 
kernel-based approaches are most popular. 
Kernel-based approaches exploit the structure of 
the tree that connects two entities. Zelenko et al(2003) 
proposed a kernel over two parse trees, which 
recursively matched nodes from roots to leaves in a 
top-down manner. Culotta and Sorensen (2004) 
extended this work to estimate similarity between 
augmented dependency trees. The above two work 
was further advanced by Bunescu and Mooney (2005) 
who argued that the information to extract a relation 
between two entities can be typically captured by the 
shortest path between them in the dependency graph. 
Later, Zhang et al(2006) developed a composite 
kernel that combined parse tree kernel with entity 
kernel and Zhou et al(2007) experimented with a 
context-sensitive kernel by automatically determining 
context-sensitive tree spans.  
In the feature-based framework, Kambhatla (2004) 
employed ME models to combine diverse lexical, 
syntactic and semantic features derived from word, 
entity type, mention level, overlap, dependency and 
parse tree. Based on his work, Zhou et al(2005) 
89
further incorporated the base phrase chunking 
information and semi-automatically collected country 
name list and personal relative trigger word list. Jiang 
and Zhai (2007) then systematically explored a large 
space of features and evaluated the effectiveness of 
different feature subspaces corresponding to sequence, 
syntactic parse tree and dependency parse tree. Their 
experiments showed that using only the basic unit 
features within each feature subspace can already 
achieve state-of-art performance, while over-inclusion 
of complex features might hurt the performance. 
Previous approaches mainly focused on English 
relations. Most of them were evaluated on the ACE 
2004 data set (or a sub set of it) which defined 7 
relation types and 23 subtypes. Although Chinese 
processing is of the same importance as English and 
other Western language processing, unfortunately few 
work has been published on Chinese relation 
extraction. Che et al(2005) defined an improved edit 
distance kernel over the original Chinese string 
representation around particular entities. The only 
relation they studied is PERSON-AFFLIATION. The 
insufficient study in Chinese relation extraction drives 
us to investigate how to find an approach that is 
particularly appropriate for Chinese. 
3 A Chinese Relation Extraction Model 
Due to the aforementioned reasons, entity relation 
extraction in Chinese is more challenging than in 
English. The system segmented words are already not 
error free, saying nothing of the quality of the 
generated parse trees. All these errors will 
undoubtedly propagate to the subsequent processing, 
such as relation extraction. It is therefore reasonable to 
conclude that kernel-based especially tree-kernel 
approaches are not suitable for Chinese, at least at 
current stage. In this paper, we study a feature-based 
approach that basically integrates entity related 
information with context information. 
3.1 Classification Features  
The classification is based on the following four types 
of features. 
z Entity Positional Structure Features  
We define and examine nine finer positional 
structures between two entities (see Appendix). They 
can be merged into three coarser structures. 
z Entity Features 
Entity types and subtypes are concerned.  
z Entity Context Features 
These are character-based features. We consider 
both internal and external context. Internal context 
includes the characters inside two entities and the 
characters inside the heads of two entities. External 
context involves the characters around two entities 
within a given window size (it is set to 4 in this study). 
All the internal and external context characters are 
transformed to Uni-grams and Bi-grams. 
z Word List Features 
Although Uni-grams and Bi-grams should be able 
to cover most of Chinese words given sufficient 
training data, many discriminative words might not be 
discovered by classifiers due to the severe sparseness 
problem of Bi-grams. We complement character- 
based context features with four word lists which are 
extracted from a published Chinese dictionary. The 
word lists include 165 prepositions, 105 orientations, 
20 auxiliaries and 25 conjunctions. 
3.2 Correction with Relation/Argument 
Constraints and Type/Subtype Consistency Check 
An identified relation is said to be correct only when 
its type/subtype (R) is correct and at the same time its 
two arguments (ARG-1 and ARG-2) must be of the 
correct entity types/subtypes and of the correct order. 
One way to improve the previous feature-based 
classification approach is to make use of the prior 
knowledge of the task to find and rectify the incorrect 
results. Table 1 illustrates the examples of possible 
relations between PER and ORG. We regard possible 
relations between two particular types of entity 
arguments as constraints. Some relations are 
symmetrical for two arguments, such as PER_ 
SOCIAL.FAMILY, but others not, such as ORG_AFF. 
EMPLOYMENT. Argument orders are important for 
asymmetrical relations.  
 PER ORG 
PER PER_SOCIAL.BUS, PER_SOCIAL.FAMILY, ? 
ORG_AFF.EMPLOYMENT, 
 ORG_AFF.OWNERSHIP, ? 
ORG  PART_WHOLE.SUBSIDIARY, ORG_AFF.INVESTOR/SHARE, ?
Table 1 Possible Relations between ARG-1 and ARG-2 
Since our classifiers are trained on relations instead 
of arguments, we simply select the first (as in adjacent 
and separate structures) and outer (as in nested 
structures) as the first argument. This setting works at 
most of cases, but still fails sometimes. The correction 
works in this way. Given two entities, if the identified 
type/subtype is an impossible one, it is revised to 
NONE (it means no relation at all). If the identified 
type/subtype is possible, but the order of arguments 
does not consist with the given relation definition, the 
order of arguments is adjusted.  
Another source of incorrect results is the 
inconsistency between the identified types and 
subtypes, since they are typically classified separately. 
90
This type of errors can be checked against the 
provided hierarchy of relations, such as the subtypes 
OWNERSHIP and EMPLOYMENT must belong to 
the ORG_AFF type. There are existing strategies to 
deal with this problem, such as strictly bottom-up (i.e. 
use the identified subtype to choose the type it belongs 
to), guiding top-down (i.e. to classify types first and 
then subtypes under a certain type). However, these 
two strategies lack of interaction between the two 
classification levels. To insure consistency in an 
interactive manner, we rank the first n numbers of the 
most likely classified types and then check them 
against the classified subtype one by one until the 
subtype conforms to a type. The matched type is 
selected as the result. If the last type still fails, both 
type and subtype are revised to NONE. We call this 
strategy type selection. Alternatively, we can choose 
the most likely classified subtypes, and check them 
with the classified type (i.e. subtype selection 
strategy). Currently, n is 2. 
3.2 Inference with Co-reference Information and 
Linguistic Patterns 
Each entity can be mentioned in different places in 
text. Two mentions are said to be co-referenced to one 
entity if they refers to the same entity in the world 
though they may have different surface expressions. 
For example, both ?he? and ?Gates? may refer to ?Bill 
Gates of Microsoft?. If a relation ?ORG- 
AFFILIATION? is held between ?Bill Gates? and 
?Microsoft?, it must be also held between ?he? and 
?Microsoft?. Formally, given two entities E1={EM11, 
EM12, ?, EM1n} and E2={EM21, EM22, ?, EM2m} (Ei 
is an entity, EMij is a mention of Ei), it is true that 
R(EM11, EM21)? R(EM1l, EM2k). This nature allows 
us to infer more relations which may not be identified 
by classifiers.  
Our previous experiments show that the 
performance of the nested and the adjacent relations is 
much better than the performance of other structured 
relations which suffer from unbearable low recall due 
to insufficient training data. Intuitively we can follow 
the path of ?Nested ? Adjacent ? Separated ? 
Others? (Nested, Adjacent and Separated structures 
are majority in the corpus) to perform the inference. 
But soon we have an interesting finding. If two related 
entities are nested, almost all the mentions of them are 
nested. So basically inference works on ?Adjacent ? 
Separated??. 
When considering the co-reference information, we 
may find another type of inconsistency, i.e. the one 
raised from co-referenced entity mentions. It is 
possible that R(EM11, EM21) ? R(EM12, EM22) when R 
is identified based on the context of EM. Co-reference 
not only helps for inference but also provides the 
second chance to check the consistency among entity 
mention pairs so that we can revise accordingly. As the 
classification results of SVM can be transformed to 
probabilities with a sigmoid function, the relations of 
lower probability mention pairs are revised according 
to the relation of highest probability mention pairs. 
The above inference strategy is called coreference- 
based inference. Besides, we find that pattern-based 
inference is also necessary. The relations of adjacent 
structure can infer the relations of separated structure 
if there are certain linguistic indicators in the local 
context. For example, given a local context ?EM1 and 
EM2 located EM3?, if the relation of EM2 and EM3 has 
been identified, EM1 and EM3 will take the relation 
type/subtype that EM2 and EM3 holds. Currently, the 
only indicators under consideration are ?and? and ?or?. 
However, more patterns can be included in the future. 
4 Experimental Results 
The experiments are conducted on the ACE 2005 
Chinese RDC training data (with true entities) where 6 
types and 18 subtypes of relations are annotated. We 
use 75% of it to train SVM classifiers and the 
remaining to evaluate results.  
The aim of the first set of experiments is to examine 
the role of structure features. In these experiments, a 
?NONE? class is added to indicate a null type/subtype. 
With entity features and entity context features and 
word list features, we consider three different 
classification contexts: (1), only three coarser 
structures 1 , i.e. nested, adjacent and separated, are 
used as feature, and a classifier is trained for each 
relation type and subtype; (2) similar to (1) but all nine 
structures are concerned; and (3) similar to (2) but the 
training data is divided into 9 parts according to 
structure, i.e. type and subtype classifiers are trained 
on the data with the same structures. The results 
presented in Table 2 show that 9-structure is much 
more discriminative than 3-structure. Also, the 
performance can be improved significantly by 
dividing training data based on nine structures. 
Type / Subtype Precision Recall F-measure 
3-Structure 0.7918/0.7356 0.3123/0.2923 0.4479/0.4183
9-Structure 0.7533/0.7502 0.4389/0.3773 0.5546/0.5021
9-Structure_Divide 0.7733/0.7485 0.5506/0.5301 0.6432/0.6209
Table 2 Evaluation on Structure Features 
Structure Positive Class Negative Class Ratio 
Nested 6332 4612 1 : 0.7283
Adjacent 2028 27100 1 : 13.3629
                                                     
1 Nine structures are combined to three by merging (b) and (c) to (a), (e) 
and (f) to (d), (h) and (i) to (g). 
91
Separated 939 79989 1 : 85.1853
Total 9299 111701 1 : 12.01 
Table 3 Imbalance Training Class Problem 
In the experiments, we find that the training class 
imbalance problem is quite serious, especially for the 
separated structure (see Table 3 above where 
?Positive? and ?Negative? mean there exists a relation 
between two entities and otherwise). A possible 
solution to alleviate this problem is to detect whether 
the given two entities have some relation first and if 
they do then to classify the relation types and subtypes 
instead of combining detection and classification in 
one process. The second set of experiment is to 
examine the difference between these two 
implementations. Against our expectation, the 
sequence implementation does better than the 
combination implementation, but not significantly, as 
shown in Table 4 below.  
Type / Subtype Precision Recall F-measure 
Combination 0.7733/0.7485 0.5506/0.5301 0.6432/0.6206
Sequence 0.7374/0.7151 0.5860/0.5683 0.6530/0.6333
Table 4 Evaluation of Two Detection and Classification Modes 
Based on the sequence implementation, we set up 
the third set of experiments to examine the correction 
and inference mechanisms. The results are illustrated 
in Table 5. The correction with constraints and 
consistency check is clearly contributing. It improves 
F-measure 7.40% and 6.47% in type and subtype 
classification respectively. We further compare four 
possible consistency check strategies in Table 6 and 
find that the strategies using subtypes to determine or 
select types perform better than top down strategies. 
This can be attributed to the fact that correction with 
relation/argument constraints in subtype is tighter than 
the ones in type.  
Type / Subtype Precision Recall F-measure 
Seq. + Cor. 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Seq. + Cor. + Inf. 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 5 Evaluation of Correction and Inference Mechanisms 
Type / Subtype Precision Recall F-measure 
Guiding Top-Down 0.7644/0.7853 0.6074/0.5783 0.6770/0.6661
Subtype Selection 0.8069/0.7738 0.6065/0.5817 0.6925/0.6641
Strictly Bottom-Up 0.8120/0.7798 0.6146/0.5903 0.6996/0.6719
Type Selection 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Table 6 Comparison of Different Consistency Check Strategies 
Finally, we provide our findings from the fourth set 
of experiments which looks at the detailed 
contributions from four feature types. Entity type 
features themselves do not work. We incrementally 
add the structures, the external contexts and internal 
contexts, Uni-grams and Bi-grams, and at last the 
word lists on them. The observations are: Uni-grams 
provide more discriminative information than 
Bi-grams; external context seems more useful than 
internal context; positional structure provides stronger 
support than other individual recognized features such 
as entity type and context; but word list feature can not 
further boost the performance.  
Type / Subtype Precision Recall F-measure 
Entity Type + Structure 0.7288/0.6902 0.4876/0.4618 0.5843/0.5534
+ External (Uni-) 0.7935/0.7492 0.5817/0.5478 0.6713/0.6321
+ Internal (Uni-) 0.8137/0.7769 0.6113/0.5836 0.6981/0.6665
+ Bi- (Internal & External) 0.8144/0.7828 0.6141/0.5902 0.7002/0.6730
+ Wordlist 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 6 Evaluation of Feature and Their Combinations 
5 Conclusion 
In this paper, we study feature-based Chinese relation 
extraction. The proposed approach is effective on the 
ACE 2005 data set. Unfortunately, there is no result 
reported on the same data so that we can compare. 
6 Appendix: Nine Positional Structures  
 
Acknowledgments 
This work was supported by HK RGC (CERG PolyU5211/05E) 
and China NSF (60603027). 
References 
Razvan Bunescu and Raymond Mooney. 2005. A Shortest Path 
Dependency Tree Kernel for Relation Extraction, In Proceedings of 
HLT/EMNLP, pages 724-731.  
Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for 
Relation Extraction, in Proceedings of ACL, pages 423-429. 
Jing Jiang, Chengxiang Zhai. 2007. A Systematic Exploration of the 
Feature Space for Relation Extraction. In proceedings of 
NAACL/HLT, pages 113-120. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic 
Features with Maximum Entropy Models for Extracting Relations. 
In Proceedings of ACL, pages 178-181.  
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of Machine 
Learning Research 3:1083-1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite 
Kernel to Extract Relations between Entities with both Flat and 
Structured Features, in Proceedings of COLING/ACL, pages 
825-832. 
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring 
Various Knowledge in Relation Extraction. In Proceedings of ACL, 
pages 427-434. 
GuoDong Zhou, Min Zhang, Donghong Ji and Qiaoming Zhu. 2007. 
Tree Kernel-based Relation Extraction with Context-Sensitive 
Structured Parse Tree Information. In Proceedings of EMNLP, 
pages 728-736. 
Wanxiang Che et al 2005. Improved-Edit-Distance Kernel for Chinese 
Relation Extraction. In Proceedings of IJCNLP, pages 132-137. 
92
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 113?116,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
An Integrated Multi-document Summarization Approach based on 
Word Hierarchical Representation 
You Ouyang, Wenji Li, Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University  
{csyouyang,cswjli,csluqin}@comp.polyu.edu.hk 
 
Abstract 
This paper introduces a novel hierarchical 
summarization approach for automatic multi-
document summarization. By creating a 
hierarchical representation of the words in the 
input document set, the proposed approach is 
able to incorporate various objectives of multi-
document summarization through an 
integrated framework. The evaluation is 
conducted on the DUC 2007 data set. 
1 Introduction and Background 
Multi-document summarization requires creating 
a short summary from a set of documents which 
concentrate on the same topic. Sometimes an 
additional query is also given to specify the 
information need of the summary. Generally, an 
effective summary should be relevant, concise 
and fluent. It means that the summary should 
cover the most important concepts in the original 
document set, contain less redundant information 
and should be well-organized.  
Currently, most successful multi-document 
summarization systems follow the extractive 
summarization framework. These systems first 
rank all the sentences in the original document 
set and then select the most salient sentences to 
compose summaries for a good coverage of the 
concepts. For the purpose of creating more 
concise and fluent summaries, some intensive 
post-processing approaches are also appended on 
the extracted sentences. For example, 
redundancy removal (Carbonell and Goldstein, 
1998) and sentence compression (Knight and 
Marcu, 2000) approaches are used to make the 
summary more concise. Sentence re-ordering 
approaches (Barzilay et al, 2002) are used to 
make the summary more fluent. In most systems, 
these approaches are treated as independent steps. 
A sequential process is usually adopted in their 
implementation, applying the various approaches 
one after another. 
In this paper, we suggest a new summarization 
framework aiming at integrating multiple 
objectives of multi-document summarization. 
The main idea of the approach is to employ a 
hierarchical summarization process which is 
motivated by the behavior of a human 
summarizer. While the document set may be 
very large in multi-document summarization, the 
length of the summary to be generated is usually 
limited. So there are always some concepts that 
can not be included in the summary. A natural 
thought is that more general concepts should be 
considered first. So, when a human summarizer 
faces a set of many documents, he may follow a 
general-specific principle to write the summary. 
The human summarizer may start with finding 
the core topic in a document set and write some 
sentences to describe this core topic. Next he 
may go to find the important sub-topics and 
cover the subtopics one by one in the summary, 
then the sub-sub-topics, sub-sub-sub-topics and 
so on. By this process, the written summary can 
convey the most salient concepts. Also, the 
general-specific relation can be used to serve 
other objectives, i.e. diversity, coherence and etc.  
Motivated by this experience, we propose a 
hierarchical summarization approach which 
attempts to mimic the behavior of a human 
summarizer. The approach includes two phases. 
In the first phase, a hierarchical tree is 
constructed to organize the important concepts in 
a document set following the general-to-specific 
order. In the second phase, an iterative algorithm 
is proposed to select the sentences based on the 
constructed hierarchical tree with consideration 
of the various objectives of multi-document 
summarization. 
2 Word Hierarchical  Representation 
2.1 Candidate Word Identification 
As a matter of fact, the concepts in the original 
document set are not all necessary to be included 
in the summary. Therefore, before constructing 
the hierarchical representation, we first conduct a 
113
filtering process to remove the unnecessary 
concepts in the document set in order to improve 
the accuracy of the hierarchical representation. In 
this study, concepts are represented in terms of 
words. Two types of unnecessary words are 
considered. One is irrelevant words that are not 
related to the given query. The other is general 
words that are not significant for the specified 
document set. The two types of words are 
filtered through two features, i.e. query-
relevance and topic-specificity.  
The query-relevance of a word is defined as 
the proportion of the number of sentences that 
contains both the word and at least one query 
word to the number of sentences that contains the 
word. If a feature value is large, it means that the 
co-occurrence rate of the word and the query is 
high, thus it is more related to the query. The 
topic-specificity of a word is defined as the 
entropy of its frequencies in different document 
sets. If the feature value is large, it means that the 
word appears uniformly in document sets, so its 
significance to a specified document set is low. 
Thus, the words with very low query-relevance 
or with very high topic-specificity are filtered 
out1. 
2.2 Word Relation Identification and 
Hierarchical Representation 
To construct a hierarchical representation for the 
words in a given document set, we follow the 
idea introduced by Lawrie et al (2001) who use 
the subsuming relation to express the general-to-
specific structure of a document set. A 
subsumption is defined as an association of two 
words if one word can be regarded as a sub-
concept of the other one. In our approach, the 
pointwise mutual information (PMI) is used to 
identify the subsumption between words. 
Generally, two words with a high PMI is 
regarded as related. Using the identified relations, 
the word hierarchical tree is constructed in a top-
bottom manner. Two constraints are used in the 
tree construction process: 
(1) For two words related by a subsumption 
relation, the one which appears more frequently 
in the document set serves as the parent node in 
the tree and the other one serves as the child 
node. 
(2) For a word, its parent node in the hierarchical 
tree is defined as the most related word, which is 
identified by PMI.  
                                                 
1 Experimental thresholds are used on the evaluated data.  
2 http://duc.nist.gov/ 
The construction algorithm is detailed below. 
Algorithm 1: Hierarchical Tree Construction 
1: Sort the identified key words by their 
frequency in the document set in descending 
order, denoted as T = {t1, t2 ,?, tn} 
2: For each ti, i from 1 to n, find the most 
relevant word tj from all the words before ti in T, 
as Ti = {t1, t2 ,?, ti-1}. Here the relevance of two 
words is calculated by their PMI, i.e. 
)()(
*),(
log),(
ji
ji
ji tfreqtfreq
Nttfreq
ttPMI   
If the coverage rate of word ti by word tj 
2.0
)(
),(
)|( t 
i
ji
ji tfreq
ttfreq
ttP , ti is regarded as 
being subsumed by tj. Here freq(ti) is the 
frequency of ti in the document set and  freq(ti, 
ti) is the co-occurrence of ti and tj in the same 
sentences of the document set. N is the total 
number of tokens in the document set. 
4: After all the subsumption relations are found, 
the tree is constructed by connecting the related 
words from the first word t1. 
An example of a tree fragment is demonstrated 
below. The tree is constructed on the document 
set D0701A from DUC 20072, the query of this 
document set is ?Describe the activities of 
Morris Dees and the Southern Poverty Law 
Center?. 
 
3 Summarization based on Word 
Hierarchical Representation 
3.1 Word Significance Estimation 
In order to include the most significant concepts 
into the summary, before using the hierarchical 
tree to create an extract, we need to estimate the 
significance of the words on the tree first. 
Initially, a rough estimation of the significance of 
a word is given by its frequency in the document 
set. However, this simple frequency-based 
measure is obviously not accurate. One thing we 
observe from the constructed hierarchical tree is 
that a word which subsumes many other words is 
usually very important, though it may not appear 
Center 
Dee Law group 
Morris hatePoverty Southern
lawyer organizationcivil Klan
114
frequently in the document set. The reason is that 
the word covers many key concepts so it is 
dominant in the document set. Motivated by this, 
we develop a bottom-up algorithm which 
propagates the significance of the child nodes in 
the hierarchical tree backward to their parent 
nodes to boost the significance of nodes with 
many descendants. 
Algorithm 2: Word Scoring Theme 
1: Set the initial score of each word in T as its 
log-frequency, i.e. score(ti) =log freq(ti). 
2: For ti from n to 1, propagate an importance 
score to its parent node par(ti) (if exists) 
according to their relevance, i.e. score(par(ti)) = 
score(par(ti)) + log freq(ti, par(ti)).  
3.2 Sentence Selection  
Based on the word hierarchical tree and the 
estimated word significance, we propose an 
iterative algorithm to select sentences which is 
able to integrate the multiple objectives for 
composing a relevant, concise and fluent 
summary. The algorithm follows a general-to-
specific order to select sentences into the 
summary. In the implementation, the idea is 
carried out by following a top-down order to 
cover the words in the hierarchical tree. In the 
beginning, we consider several ?seed? words 
which are in the top-level of the tree (these 
words are regarded as the core concepts in the 
document set). Once some sentences have been 
extracted according to these ?seed? words, the 
algorithm moves to down-level words through 
the subsumption relations between the words. 
Then new sentences are added according to the 
down-level words and the algorithm continues 
moving to lower levels of the tree until the whole 
summary is generated. For the purpose of 
reducing redundancy, the words already covered 
by the extracted sentences will be ignored while 
selecting new sentences. To improve the fluency 
of the generated summary, after a sentence is 
selected, it is inserted to the position according to 
the subsumption relation between the words of 
this sentence and the sentences which are already 
in the summary. The detailed process of the 
sentence selection algorithm is described below. 
Algorithm 3: Summary Generation  
1: For the words in the hierarchical tree, set the 
initial states of the top n words3 as ?activated? 
and the states of other words as ?inactivated?. 
2: For all the sentences in the document set, 
                                                 
3 n is set to 3 experimentally on the evaluation data set. 
select the sentence with the largest score 
according to the ?activated? word set. The 
score of a sentence s is defined as 
? )(|| 1)( itscoressscore  where ti is a word 
belongs to s and the state of ti should be 
?activated?. | s | is the number of words in s. 
3: For the selected sentence sk, the subsumption 
relations between it and the existing sentences 
in the current summary are calculated and the 
most related sentence sl is selected. sk is then 
inserted to the position right behind sl. 
4: For each word ti belongs to the selected 
sentence sk, set its state to ?inactivated?; for 
each word tj which is subsumed by ti, set its 
state to ?activated?. 
5: Repeat step 2-4 until the length limit of the 
summary is exceeded. 
4 Experiment  
Experiments are conducted on the DUC 2007 
data set which contains 45 document sets. Each 
document set consists of 25 documents and a 
topic description as the query. In the task 
definition, the length of the summary is limited 
to 250 words. In our summarization system, pre-
processing includes stop-word removal and word 
stemming (conducted by GATE4). 
One of the DUC evaluation methods, ROUGE 
(Lin and Hovy, 2003), is used to evaluate the 
content of the generated summaries. ROUGE is a 
state-of-the-art automatic evaluation method 
based on N-gram matching between system 
summaries and human summaries. In the 
experiment, our system is compared to the top 
systems in DUC 2007. Moreover, a baseline 
system which considers only the frequencies of 
words but ignores the relations between words is 
included for comparison. Table 1 below shows 
the average recalls of ROUGE-1, ROUGE-2 and 
ROUGE-SU4 over the 45 DUC 2007document 
sets. In the experiment, the proposed 
summarization system outperforms the baseline 
system, which proves the benefit of considering 
the relations between words. Also, the system 
ranks the 6th among the 32 submitted systems in 
DUC 2007. This shows that the proposed 
approach is competitive. 
  ROUGE-1 ROUGE-2 ROUGE-SU4
S15 0.4451 0.1245 0.1771 
S29 0.4325 0.1203 0.1707 
S4 0.4342 0.1189 0.1699 
S24 0.4526 0.1179 0.1759 
                                                 
4 http://gate.ac.uk/ 
115
S13 0.4218 0.1117 0.1644 
Ours 0.4257 0.1110 0.1608 
Baseline 0.4088 0.1040 0.1542 
Table 1. ROUGE Evaluation Results 
To demonstrate the advantage of the proposed 
approach, i.e. its ability to incorporate multiple 
summarization objectives, the fragments of the 
generated summaries on the data set D0701A are 
also provided below as a case study. 
The summary produced by our system 
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists. 
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala. 
Dees, founder of the Southern Poverty Law 
Center, has won a series of civil right suits against 
the Ku Klux Klan and other racist organizations in 
a campaign to drive them out of business. 
In 1987, Dees won a $7 million verdict against a 
Ku Klux Klan organization over the slaying of a 
19-year-old black man in Mobile, Ala. 
The summary produced by the baseline system
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala.  
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists.  
The Southern Poverty Law Center previously 
recorded a 20-percent increase in hate groups 
from 1996 to 1997.  
The verdict was obtained by lawyers for the 
Southern Poverty Law Center, a nonprofit 
organization in Birmingham, Ala. 
Comparing the generated summaries of the 
two systems, we can see that the summary 
generated by the proposed approach is better in 
coherence and fluency since these factors are 
considered in the integrated summarization 
framework. Various summarization approaches, 
i.e. sentence ranking, redundancy removal and 
sentence re-ordering, are all implemented in the 
sentence selection algorithm based on the word 
hierarchical tree. However, we also observe that 
the proposed approach fails to generate better 
summaries on some document sets. The main 
problem is that the quality of the constructed 
hierarchical tree is not always satisfied. In the 
proposed summarization approach, we mainly 
rely on the PMI between the words to construct 
the hierarchical tree. However, a single PMI-
based measure is not enough to characterize the 
word relation. Consequently the constructed tree 
can not always well represent the concepts for 
some document sets. Another problem is that the 
two constraints used in the tree construction 
algorithm are not always right in real data. So we 
regard developing better tree construction 
approaches as of primary importance. Also, there 
are other places which can be improved in the 
future, such as the word significance estimation 
and sentence inserting algorithms. Nevertheless, 
we believe that the idea of incorporating the 
multiple summarization objectives into one 
integrated framework is meaningful and worth 
further study. 
5 Conclusion  
We introduced a summarization framework 
which aims at integrating various summarization 
objectives. By constructing a hierarchical tree 
representation for the words in the original 
document set, we proposed a summarization 
approach for the purpose of generating a relevant, 
concise and fluent summary. Experiments on 
DUC 2007 showed the advantages of the 
integrated framework.  
Acknowledgments 
The work described in this paper was partially 
supported by Hong Kong RGC Projects (No. 
PolyU 5217/07E) and partially supported by The 
Hong Kong Polytechnic University internal 
grants (A-PA6L and G-YG80). 
 
References  
R. Barzilay, N. Elhadad, and K. R. McKeown. 2002. 
Inferring strategies for sentence ordering in 
multidocument news summarization. Journal of 
Artificial Intelligence Research, 17:35-55, 2002. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering 
Documents and Producing Summaries. In 
Proceedings of ACM SIGIR 1998, pp 335-336. 
K. Knight and D. Marcu. 2000. Statistics-based 
summarization --- step one: Sentence compression. 
In Proceeding of The American Association for 
Artificial Intelligence Conference (AAAI-2000), 
pp 703-710. 
D. Lawrie, W. B. Croft and A. Rosenberg. 2001. 
Finding topic words for hierarchical 
summarization. In Proceedings of ACM SIGIR 
2001, pp 349-357. 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurance statistics. 
In Proc. of HLT-NAACL 2003, pp 71-78. 
116
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Chinese Term Extraction Using Different Types of Relevance 
 
 
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
1School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin 150001, China 
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn 
2Department of Computing,  
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based 
method. Different types of relevance are used 
separately or jointly for term verification. The 
proposed approach requires no prior domain 
knowledge and no adaptation for new domains. 
Consequently, the method can be used in any 
domain corpus and it is especially useful for 
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese 
term extraction show significant improve-
ments over existing techniques and also verify 
the efficiency and relative domain independent 
nature of the approach. 
1 Introduction 
Terms are the lexical units to represent the most 
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge 
acquisition which can be used for lexicon update, 
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts 
candidates by unithood calculation to qualify a 
string as a valid term. The second step verifies 
them through termhood measures (Kageura and 
Umino, 1996) to validate their domain specificity.  
Many previous studies are conducted on term 
candidate extraction. Other tasks such as named 
entity recognition, meaningful word extraction 
and unknown word detection, use techniques 
similar to that for term candidate extraction. But, 
their focuses are not on domain specificity. This 
study focuses on the verification of candidates by 
termhood calculation.  
Relevance between term candidates and docu-
ments is the most popular feature used for term 
verification such as TF-IDF (Salton and McGill, 
1983; Frank, 1999) and Inter-Domain Entropy 
(Chang, 2005), which are all based on the hy-
pothesis that ?if a candidate occurs frequently in 
a few documents of a domain, it is likely a term?. 
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from 
non-terms. There are also attempts to use prior 
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and 
Lu, 2007) calculates the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a domain lexicon whose 
size and quality have great impact on the per-
formance of the algorithm. Some supervised 
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al, 2005) 
and Chinese new word identification (Li et al, 
2004) using SVM classifiers (Vapnik, 1995) 
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and 
sentences by using the link analysis approach 
based on the HITS algorithm to achieve better 
performance. 
In this work, a new feature on the relevance 
between different term candidates is integrated 
with other features to validate their domain 
specificity. The relevance between candidate 
terms may be useful to identify domain specific 
terms based on two assumptions. First, terms are 
more likely to occur with other terms in order to 
express domain information. Second, term can-
didates extracted from domain corpora are likely 
213
to be domain specific. Previous work by (e.g. Ji 
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain 
lexicon. In this study, the relevance between 
term candidates are iteratively calculated by 
graphs using link analysis algorithm to avoid the 
dependency on prior domain knowledge.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and 
presents the future plans. 
2 Methodology 
This study assumes the availability of term can-
didates since the focus is on term verification by 
termhood calculation. Three types of relevance 
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence 
relevance, CS; and the candidates to document 
relevance, CD. Terms are then verified by using 
different types of relevance. 
2.1 Relevance between Term Candidates 
Based on the assumptions that term candidates 
are likely to be used together in order to repre-
sent a particular domain concept, relevance of 
term candidates can be represented by graphs in 
a domain corpus. In this study, CC is defined as 
their co-occurrence in the same sentence of the 
domain corpus. For each document, a graph of 
term candidates is first constructed. In the graph, 
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence, 
two directional links between TC1 to TC2 are 
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed 
which means long terms can be linked to their 
components if the components are also candi-
dates.  
After graph construction, the term candidate 
relevance, CC, is then iteratively calculated using 
the PageRank algorithm (Page et al 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to 
other nodes, it is more likely to be a salient node. 
The algorithm assigns the significance score to 
each node according to the number of nodes link-
ing to it as well as the significance of the nodes. 
The PageRank calculation PR of a node A is 
shown as follows:  
)
)(
)(
...
)(
)(
)(
)(
()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPR
ddAPR ++++?=
(1) 
where B1, B2,?, Bt are all nodes linked to node A; 
C(Bi) is the number of outgoing links from node 
Bi; d is the factor to avoid loop trap in the 
graphic structure. d is set to 0.85 as suggested in 
(Page et al, 1998). Initially, all PR weights are 
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of 
each term candidate in the domain specific cor-
pus is then derived based on the significance of 
other candidates it co-occurred with. The CC 
weight of term candidate TCi is given by its PR 
value after k iterations, a parameter to be deter-
mined experimentally. 
2.2 Relevance between Term Candidates 
and Sentences 
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance 
between term candidate and sentences, referred 
to as CS, is calculated using the TV_HITS (Term 
Verification ? HITS) algorithm proposed in 
(Yang et al, 2008) based on  Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 
1997). In TV_HITS, a good hub in the domain 
corpus is a sentence that contains many good 
authorities; a good authority is a term candidate 
that is contained in many good hubs.  
In TV_HITS, a node p can either be a sentence 
or a term candidate. If a term candidate TC is 
contained in a sentence Sen of the domain corpus, 
there is a directional link from Sen to TC. 
TV_HITS then makes use of the relationship be-
tween candidates and sentences via an iterative 
process to update CS weight for each TC.  
Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote the 
authority vector and VH(w(p1)H, w(p2)H,?, w(pn)H) 
denote the hub vector. VA and VH are initialized 
to (1, 1,?, 1). Given weights VA and VH with a 
directional link p?q, w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a 
node) and the O operation(an out-pointer to a 
node) shown as follows. The CS weight of term 
candidate TCi is given by its w(q)A value after 
iteration. 
I operation:          (2) ?
??
=
Eqp
HA w(p)w(q)
O operation:         (3) ?
??
=
Eqp
AH w(q)w(p)
2.3 Relevance between Term Candidates 
and Documents 
The relevance between term candidates and 
documents is used in many term extraction algo-
214
rithms. The relevance is measured by the TF-IDF 
value according to the following equations: 
)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4) 
)
)(
log()(
i
i TCDF
D
TCIDF =             (5) 
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi) 
is the number of documents in which TCi occurs 
at least once, |D| is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse 
document frequency which can be calculated 
from the document frequency. 
2.4 Combination of Relevance 
To evaluate the effective of the different types of 
relevance, they are combined in different ways in 
the evaluation. Term candidates are then ranked 
according to the corresponding termhood values 
Th(TC) and the top ranked candidates are con-
sidered terms.  
For each document Dj in the domain corpus 
where a term candidate TCi occurs, there is CCij 
weight and a CSij weight. When features CC and 
CS are used separately, termhood ThCC(TCi) and 
ThCS(TCi) are calculated by averaging CCij and 
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi) 
denotes the ranking position of TCi.  
)(TCR)(TCR
)(TCTh
iCSiCC
iCSCC
11 +=+    (6) 
)log()()(
Cj
ijiCDCC DF
D
CCTCTh ?=+     (7) 
)log()()(
Cj
ijiCDCS DF
D
CSTCTh ?=+     (8) 
)(TCR)(TCR
TCTh
iCDCSiCDCC
iCDCSCC
++
++ += 11)( (9) 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal., 
respectively. CorpusIT includes academic papers 
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the 
complete set of official Chinese constitutional 
law articles and Economics/Finance law articles 
of 1.04M in size (http://www.law-lib.com/).  
For comparison to previous work, all term 
candidates are extracted from the same domain 
corpora using the delimiter based algorithm 
TCE_DI (Term Candidate Extraction ? Delimiter 
Identification) which is efficient according to 
(Yang et al, 2008). In TCE_DI, term delimiters 
are identified first. Words between delimiters are 
then taken as term candidates. 
The performances are evaluated in terms of 
precision (P), recall (R) and F-value (F). Since 
the corpora are relatively large, sampling is used 
for evaluation based on fixed interval of 1 in 
each 10 ranked results. The verification of all the 
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set 
of correct terms which are manually verified 
from the extracted terms by different methods is 
constructed as the standard answer. The answer 
set is certainly not complete. But it is useful as a 
performance indication for comparison since it is 
fair to all algorithms. 
3.2 Evaluation on Term Extraction 
For comparison, three reference algorithms are 
used in the evaluation. The first algorithm is 
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al, 2008). 
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight 
(Joachims, 1999). Internal and external features 
are used by SVMlight. The third algorithm is the 
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except 
SVMlight. Two training sets containing thousands 
of positive and negative examples from IT do-
main and legal domain are constructed for the 
SVM classifier. The training and testing sets are 
not overlapped. 
Table 1 and Table 2 show the performance of 
the proposed algorithms using different features 
for IT domain and legal domain, respectively. 
The algorithm using CD alone is the same as the 
TF-IDF algorithm. The algorithm using CS and 
CD is the TV_LinkA algorithm.  
Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 63.6 49.5 55.6 
CC 47.1 36.5 41.2 
CS 65.6 51 57.4 
CD(TF-IDF) 64.8 50.4 56.7 
CC+CS 80.4 62.5 70.3 
CC+CD 49 38.1 42.9 
CS+CD 
(TV_LinkA) 
75.4 58.6 66 
CC+CS+CD 82.8 64.4 72.4 
Table 1. Performance on IT Domain 
215
 Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 60.1 54.2 57.3 
CC 45.2 40.3 42.6 
CS 70.5 40.1 51.1 
CD(TF-IDF) 59.4 52.9 56 
CC+CS 64.2 49.9 56.1 
CC+CD 48.4 43.1 45.6 
CS+CD 
(TV_LinkA) 
67.4 60.1 63.5 
CC+CS+CD 70.2 62.6 66.2 
Table 2. Performance on Legal Domain 
Table 1 and Table 2 show that the proposed 
algorithms achieve similar performance on both 
domains. The proposed algorithm using all three 
features (CC+CS+CD) performs the best. The 
results confirm that the proposed approach are 
quite stable across domains and the relevance 
between candidates are efficient for improving 
performance of term extraction in different do-
mains. The algorithm using CC only does not 
achieve good performance. Neither does CC+CS. 
The main reason is that the term candidates used 
in the experiments are extracted using the 
TCE_DI algorithm which can extract candidates 
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and 
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between 
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD 
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for 
the legal domain. This is because the noise data 
are eliminated by CS and CD, and CC help to 
identify additional terms that may not be statisti-
cally significant.  
4 Conclusion and Future Work 
In conclusion, this paper exploits the relevance 
between term candidates as an additional feature 
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and 
no adaptation for new domains. Experiments for 
term extraction are conducted on IT domain and 
legal domain, respectively. Evaluations indicate 
that the proposed algorithm using different types 
of relevance achieves the best performance in 
both domains without training.  
In this work, only co-occurrence in a sentence 
is used as the relevance between term candidates. 
Other features such as syntactic relations can 
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to 
apply this approach to other languages such as 
English. 
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University 
(PolyU CRG G-U297) 
References 
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of 
16th Int. Joint Conf. on AI,  IJCAI-99: 668-673. 
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int 
Conf. on Machine Learning, Morgan Kaufman, 
2000. 
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289. 
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM 
Symposium on Discrete Algorithms: 668-677. New 
Orleans, America, January 1997. 
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information. 
In Proc. of CICLing 2007, LNCS 4394: 62 ? 74. 
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proc. of the 1st Int.Joint 
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide 
Web Conf, Brisbane, Australia, April 1998, 107-
117. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese 
Term Extraction Using Minimal Resources. The 
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040. 
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7. 
216
Tutorial Abstracts of ACL-IJCNLP 2009, page 1,
Suntec, Singapore, 2 August 2009. c?2009 ACL and AFNLP
Fundamentals of Chinese Language Processing 
 
Chu-Ren Huang 
Dept. of Chinese and Bilingual Studies 
Hong Kong polytechnic University  
Churen.huang@inet.polyu.edu.hk
Qin Lu 
Department of Computing 
Hong Kong Polytechnic University 
csluqin@comp.polyu.edu.hk 
 
 
1 Introduction 
This tutorial gives an introduction to the funda-
mentals of Chinese language processing for text 
processing. Today, more and more Chinese in-
formation are available in electronic form and 
over the internet. Computer processing of Chi-
nese text requires the understanding of both the 
language itself and the technology to handle 
them. This tutorial is targeted for both Chinese 
linguists who are interested in computational 
linguistics and computer scientists who are inter-
ested in research on processing Chinese.  
2 Content Overview 
This tutorial consists of two parts. The first part 
overviews the grammar of the Chinese language 
from a language processing perspective based on 
naturally occurring data. The second part over-
views Chinese specific processing issues and 
corresponding computational technologies. 
The grammar introduced is a descriptive 
grammar of general-purpose, present-day stan-
dard Mandarin Chinese, which is fast becoming 
an internationally spoken language. Real exam-
ples of actual language use will be illustrated 
based on a data driven and corpus based ap-
proach so that its links to computational linguis-
tic approaches for computer processing are natu-
rally bridged in. A number of important Chinese 
NLP resources are also presented. On the tech-
nology side, the tutorial mainly covers Chinese 
word segmentation and Part-of-Speech tagging. 
Word segmentation problem has to deal with 
some Chinese language unique problems such as 
unknown word detection and named entity rec-
ognition which are the emphasis of this tutorial.  
3 Tutorial Outline  
Part 1: Highlights of Chinese Grammar for NLP 
1.1 Preliminaries: Orthography and writing 
conventions 
 
1.2 Basic unit of processing: word or character? 
a. Word-forms vs. character forms 
 b. Word-senses vs. character-senses 
1.3 Part-of-Speech: important issues in defin-
ing word classes 
1.4 Word formation: from affixation to com-
pounding  
1.5 Unique constructions and challenges 
a. Classifier-noun agreement 
b. Separable compounds (or ionization) 
 c. ?Verbless? Constructions 
1.6. Chinese NLP resources  
 
Part 2: Text Processing 
2.1 Lexical processing 
 a. Segmentation 
 b. Disambiguation 
 c. Unknown word detection 
 d. Named Entity Recognition 
2.2 Syntactic processing 
 a. Issues in PoS tagging 
 b. Hidden Markov Models 
2.3 NLP Applications 
References  
Academia Sinica Balance Corpus of Mandarin Chi-
nese.  http://www.sinica.edu.tw/SinicaCorpus/  
Chao, Y. R. 1968. A Grammar of Spoken Chinese. 
Berkeley: University of California Press. 
Huang, C.-R., K.-j. Chen and B. K. T'sou. 1996.  
Readings in Chinese Natural Language Processing. 
Journal of Chinese Linguistics Monograph Series 
No. 9.  Berkeley: POLA. 
T'sou, B. K. 2004. Chinese Language Processing at 
the Dawn of the 21st Century. In C.-R. Huang and 
W. Lenders. Eds. Computational Linguistics and 
Beyond. Pp. 189-206. Taipei: AcademiaSinica. 
Miao, S.Q., Wei, Z.H. 2007, Chinese Text Informa-
tion Processing Principles and Applications (In 
Chinese). Tsinghua University Press.  
 
1
 Decomposition for ISO/IEC 10646 Ideographic Characters 
LU Qin, CHAN Shiu Tong, LI Yin, LI Ngai Ling 
Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{csluqin, cstchan, csyinli, csnlli}@comp.polyu.edu.hk 
 
Abstract 
Ideograph characters are often formed by some 
smaller functional units, which we call character 
components. These character components can be 
ideograph radicals, ideographs proper, or some 
pure components which must be used with 
others to form characters. Decomposition of 
ideographs can be used in many applications. It 
is particularly important in the study of Chinese 
character formation, phonetics and semantics.  
However, the way a character is decomposed 
depends on the definition of components as well 
as the decomposition rules. The 12 Ideographic 
Description Characters (IDCs) introduced in ISO 
10646 are designed to describe characters using 
components. The Hong Kong SAR Government 
recently published two sets of glyph standards 
for ISO10646 characters. The standards, being 
the first of its kind, make use of character 
decomposition to specify a character glyph using 
its components. In this paper, we will first 
introduce the IDCs and how they can be used 
with components to describe two dimensional 
ideograph characters in a linear fashion. Next we 
will briefly discuss the basic references and 
character decomposition rules. We will then 
describe the data structure and algorithms to 
decompose Chinese characters into components 
and, vice versa. We have also implemented our 
database and algorithms as an internet 
application, called  the Chinese Character 
Search System, available at website 
http://www.iso10646hk.net/. With this tool, 
people can easily search characters and 
components in ISO 10646. 
  
Introduction 
ISO/IEC 10646 (ISO 10646) in its current 
version, contains more than 27,000 Han 
characters, or ideograph characters as it is called, 
in its basic multilingual plane and another 
40,000 in the second plane[1-2]. The complete 
set of ideograph repertoire includes Han 
characters in all national/regional standards as 
well as all characters from the Kang Xi 
Dictionary( ) and other major 
references. In almost all the current encoding 
systems including ISO 10646 and Unicode, each 
Han character is treated as a separate unique 
symbol and given a separate code point.  This 
single character encoding method has some 
serious drawbacks. Consider most of the 
alphabet-based languages, such as English, even 
though new words are created quite frequently, 
the alphabet itself is quite stable. Thus the newly 
adopted words do not have any impact on 
coding standards. When new Han characters are 
created, they must be assigned a new code point, 
thus all codesets supporting Han characters must 
leave space for extension. As there is no formal 
rule to limit the formation of new Han characters, 
the standardization process for code point 
assignment can be potentially endless. On the 
other hand, new Han characters are almost 
always be created using some existing character 
components which can be existing radicals, 
characters proper, or pure components which are 
not used alone as characters. If we can use coded 
components to describe a new character, we can 
potentially eliminate the standardization process. 
Han characters can be considered as a two 
dimensional encoding of components. The same 
set of components when used in different 
relative positions can form different characters. 
For example the two components  and  
can form two different characters:   
depending on the relative positions of the two 
components. However, the current internal code 
point assignments in no way can reveal the 
relationship of the these characters with respect 
to their component characters. Because of the 
limitation of the encoding system, people have 
to put a lot of efforts to develop different input 
methods. Searching for characters with similar 
shapes are also quite difficult. The 12 
 Ideographic Description Characters (IDCs) were 
introduced in ISO 10646 in the code range of 
2FF0 - 2FFB to describe the relative positions of 
components as shown in Table 1. Each IDC 
symbol shows a typical ideograph character 
composition structure. For example,  
(U+2FF0) indicates that a character is formed by 
two components, one on the left-hand side and 
one on the right-hand side. All IDCs except 
U+2FF2 and U+2FF3 have cardinality of two 
because the decomposition requires two 
components only.  Details of these symbols can 
be found in Annex F of ISO 10646 2nd Edition 
[1] and in John Jenkens' report [3]. 
 
 
Smbl 
 
Code 
point  
 
Name in ISO 10646 
 
Cardi- 
nality 
 
Label 
 2FF0 IDC LEFT TO RIGHT IDC2 A 
 2FF1 IDC ABOVE TO BELOW 
IDC2 B 
 2FF2 IDC LEFT TO MIDDLE AND RIGHT 
IDC3 K 
 2FF3 IDC ABOVE TO MIDDLE AND 
BELOW 
IDC3 L 
 2FF4 IDC FULL SURROUND 
IDC2 I 
 2FF5 IDC SURROUND FROM ABOVE 
IDC2 F 
 2FF6 IDC SURROUND FROM BELOW 
IDC2 G 
 2FF7 IDC SURROUND FROM LEFT 
IDC2 H 
 2FF8 IDC SURROUND FROM UPPER LEFT 
IDC2 D 
 2FF9 IDC SURROUND FROM UPPER RIGHT 
IDC2 C 
 2FFA IDC SURROUND FROM LOWER LEFT 
IDC2 E 
 2FFB IDC OVERLAID IDC2 J 
Table 1. The 12 Ideograph Description 
Characters 
 
The IDCs can be used to describe not only 
unencoded characters, but also coded characters 
to reveal their internal structures and 
relationships among components. Thus 
applications for using these structural symbols 
can be quite useful. In fact the most common 
applications are in electronic dictionaries and 
on-line education [4]. 
 
In this paper, however, we introduce a new 
application where the IDCs and components are 
used in the standardization of Han character 
glyphs. As we all know that ISO 10646 is a 
character standard, which allows different glyph 
styles for the same character and different 
regions can develop different glyph styles to suit 
their own needs. The ideographic repertoire in 
ISO 10646 has a so called Horizontal Extension, 
where each coded ideograph character is listed 
under the respective CJKV columns. The glyph 
of each character can be different under different 
columns because ISO 10646 is a character 
standard, not a glyph standard. We normally call 
these different glyphs as variants. For example, 
the character bone   can take three different 
forms(variants):  
 
   
HK Mainland Taiwan 
 
Even with the ISO 10646 horizontal extensions, 
people in Hong Kong still get confused as to 
which styles to use, as only some characters in 
the Hong Kong style deviate from both G 
column(mainland China) and T column(Taiwan). 
Consequently, the Hong Kong SAR Government 
has decided to  develop the Hong Kong glyph 
standards for ISO 10646 which can serve as a 
reference guide for font vendors when 
developing products for Hong Kong. The 
standards, being the first of its kind, makes uses 
of character decomposition to specify a 
character glyph using its components. 
 
The rest of the paper is organized as follows. 
Section 1 gives the rationale for the use of 
character components, the references and 
decomposition rules. Section 2 describes the 
data structure and algorithms to decompose 
Chinese characters into components and, vice 
versa. Section 3 discusses performance 
considerations and Section 4 is the conclusion. 
 
1. Character Decomposition Rules 
At the beginning of the glyph standardization, 
one important requirement was agreed by the 
working group, namely, extensibility. That is, the 
specifications should be easily extended by 
adding more characters into later versions of the 
ISO/IEC 10646, which we refer to as the new 
characters. The specifications should also not 
contain any internal inconsistency, or 
inconsistency in relation to the ISO/IEC 10646?s 
 source standards. In order to satisfy both 
consistency requirements, we have concluded 
that listing every character in ISO/IEC 10646 is 
not desirable. Instead, we decided to produce the 
specifications by giving the correct glyphs of 
character components based on a common 
assumption that if a component or a character is 
written in a certain way, all other characters 
using it as a component should also write it in 
the same way. For example if the character 
?bone?  (U+9AA8) is written in a certain 
way, all characters using ?bone? as a component, 
such as ? ? (U+6ED1) and ? ? (U+9ABC), 
should have the bone ? ? component follow 
the same style. In this way, the specification can 
be extended very easily for all new characters 
using bone ? ? as a component. In other words, 
we can assume that component glyphs are 
standardized for general usage. By using 
components to describe a character, we can also 
avoid inconsistency. That is, by avoid listing all 
characters with bone, ? ? as a component, we 
do not need to be concerned about producing 
inconsistent glyphs in the specifications. This is 
important because the working group does not 
have any font vendor as a member, because of 
an implicit rule that was specified by the 
Government of the HKSAR to avoid any 
potential conflict of interest. The glyph style is 
mostly based on the book published by the Hong 
Kong Institute of Education in 2000[5] 
 
In principle, for producing glyph specifications, 
we have to produce a concrete, minimal, and 
unique list of basic components. In order to 
achieve this, we need to have a set of rules to 
decompose the characters systematically. In our 
work, we have used the GF 3001-1997 [6] as our 
major component reference. The following is a 
brief description of the rules. (For a detailed 
description, please refer to the paper ?The Hong 
Kong Glyph Specifications for ISO 10646's 
Ideographic Characters?[7].) 
? Use GF 3001-1997 specifications as the 
basis to construct a set of primary 
components. Components for simplified 
Chinese are removed. The shapes are 
modified to match the glyph style for 
Hong Kong. 
? Characters are decomposed into 
components according to their structure 
and etymological origin. 
? In some cases, an ?ad-hoc? 
decomposition occurs if the 
etymological origin and its glyph shape 
are not consistent, or the etymological 
origin is not clear, or to avoid defining 
additional components. 
? Characters are not decomposed if it 
appears in GF 3001-1997 as a 
component. 
? Detached parts can be further 
decomposed. 
? Merely touched parts that do not have 
overlapping or crossing can be 
decomposed. 
? In some cases, we do not decompose 
some components to prevent the 
components from getting too small. 
? In some cases, a single component will 
be distinguished as two different 
components. This is the concept of 
variant or related component. 
This set of rules, together with 644 basic 
components and the set of intermediate 
components defined, enables us to decompose 
Chinese characters that appear in the first 
version ISO 10646 with 20,902 characters, Ext. 
A in the second version of ISO 10646[1] and 
Hong Kong Suplementary Character Set [8-9].  
The 644 basic components play a very important 
role because they form all the Chinese characters 
in our scope. 
 
In order to describe the position relationship 
amongst components in a character, we have 
used the 12 Ideographic Description Characters 
(IDC) in ISO/IEC 10646 Part1:2000 in the range 
from 2FF0 to 2FFB, and defined an extra IDC 
?M? (which indicates that a particular 
component is a basic component and will not be 
further decomposed), as shown in Table 1. Every 
character can be decomposed into up to three 
components depending on the cardinality of the 
IDC used.  
 
Each Character is decomposed according to the 
following definition: 
 
Character = IDC2 CC(1)  CC(2)  
| IDC3 CC(1) CC(2) CC(3) 
| M 
where 
IDC2 ? (2FF0 ? 2FFB)  
CC(i)  is a set of character components 
and i indicates its position in the 
sequence  
 M is a special symbol indicating 
Character will not be further 
decomposed 
 
By our definition, a CC can be formed by three 
subsets: (1) coded radicals, (2) coded 
components and ideographs proper, and (3) 
intermediate components that are not coded in 
ISO 10646. The intermediate components are 
maintained by our system only. The 
decomposition result is stored in the database. 
Conceptually, every entry in the database can be 
treated as a Chinese component, having a data 
structure described above. 
 
2. Decomposition/Formation Algorithms 
As mentioned above, the decomposition 
database only gives information on how a 
character is decomposed in a minimal way. 
However, some characters have nested 
components. For instance, the character ??? 
can be decomposed into two components: ??? 
and ???, but ??? being a character can be 
further decomposed into two components. In 
order to handle nesting and finding components 
to the most elementary form(no further 
decomposition), we have defined the 
decomposition and formation algorithms.  
There are mainly two algorithms, one for the 
decomposition of a character into a set of 
components(the algorithm is called 
Char-to-Compnt) , another one for the formation 
of a set of characters from a component ( the 
algorithm is called Compn-to-Charr). 
Let x be the seed (x = starting character for 
search); 
Stop = false 
WHILE NOT  stop DO 
      IF Struct_Symbol(CD[x]) = ?M? 
        Stop = True 
     ELSE 
LCmp ={ cc[x] ?  CC } 
ENDWHILE 
 
Figure 1. Pseudo-code of ?Char-to-Compnt? 
 
Both algorithms are very similar. They 
recursively retrieve all characters/components 
appearing in the decomposition database by 
using the characters/components themselves as a 
seed, but their directions of retrieval are opposite 
to each other. In the ?Char-to-Compnt?, the 
decomposition goes from its current level down, 
one level at a time, until no more decomposition 
can be done. Figure 1 the pseudo code of the 
algorithm for one level only and they can be 
done recursively to find all components of a 
character. Table 2 shows the entries related to 
the character ???. Notice that the number of 
components for ??? is not two, but 4 because 
one of the components ??? can be further 
decomposed into two more components. 
 
Character IDC Comp1 Comp2 Comp3 
? B ? ?  
? A ? ?  
? M    
? M    
? M    
Table 2. Component Entries of character 
??? 
 
On the other hand, the ?Compnt-to-Char? 
algorithm searches from its current level up until 
no more character can be found using the current 
component. Figure 2 shows the pseudo code of 
the upward search algorithm where x is 
considered the seed to start the search and the 
variable contains all characters formed using the 
current component x.  
 
Let x be the seed (x = starting component for 
search); 
Stop = false 
Char_List ={ x} 
WHILE NOT  stop DO 
      IF No Change to Char_List 
              Stop = True 
     ELSE 
FOR each x in Char_List 
Char_List = Char_List ?{ Char[x]} 
     ENDFOR 
 
ENDWHILE 
 
Figure 2. Pseudo-code of ?Compnt-to-Char? 
 
Character IDC Comp1 Comp2 Comp3 
? M    
? B ? ?  
? A ? ?  
? A ? ?  
? A ? ?  
?     
Table 3. Example character entries of  
component ???   
  
Table 3 shows some of the search results 
involving the component  ???.  Note that the  
result not only find the character ???, but also 
the characters using ??? as components as well.  
 
Further more, due to the fact that there are two 
IDCs with cardinality of three, the 
decomposition is not unique.  Based Han 
characters formation rules, some characters 
should be decomposed into two components first 
before considering further decomposition.  For 
instance, ??? should be decomposed into ??? 
and ??? whereas ??? should be decomposed 
into ??? and ???.  However, for upward 
search we certainly want the character ??? to 
be found if the search component is ?? ?. 
Therefore, in addition to using the most reason 
decomposition at the first level, we also 
maintain different decompositions for 
applications where character formation rule are 
less important. In other words, we also provided 
composition and decompositions independent of 
certain particular character formal rules. Again 
taking the character ??? as an examples, its 
components should not only be ??? and ???, 
but also  ???, ???, ??? as well as ??? and 
? ?.  In fact, in our system,  ?? ?  is 
decomposed into ???, ??? and ? ? as shown 
in Table 4. The ?Char-to-Compnt? algorithm 
will take the relative positions of the 
components into consideration based on the IDC 
defined in each entry to find other three possible 
components  ???, ??? and ???.  This can 
be done because the combination of ??? and 
??? will form ???; similarly ??? and ? ? 
will form ???;, and ??? and ? ?will form 
???. Note that in the first two cases of the OR 
clause, ??? and ??? will be identified. In the 
third case of the OR  clause, the character  
??? will be identified. You may argue the 
validity of the third case of the OR clause, but 
for the character ????, finding the component 
??? would be very important.    
Character IDC Comp1 Comp2 Comp3 
? K ? ?  
? A ? ?  
? A ?   
? A ?   
Table 4 An example of handling a character 
with three components 
 
The  basic principle of the algorithm, as shown 
in Figure 3,  is that if we see a character with 
an IDC {K} or {L}, or an IDC of a character 
that can be transformed to IDC {K} or {L}, we 
will try to use its components to form characters. 
 
Let x be a Chinese component (x = cc); 
Let LCsub be the list of sub-components c; 
IF x[structure] = IDC{K} THEN 
LCsub = c : c[structure] = IDC{A}AND 
c[component(1)] = x[component(1)] AND 
c[component(2)] = x[component(2)] or 
c[component(2)] = x[component(2)] AND 
c[component(3)] = x[component(3)] or 
c[component(1)] = x[component(1)] AND 
c[component(3)] = x[component(3)] 
END 
**the same algorithm works when x[structure] = 
IDC{L}, then the result c[structure] will become 
IDC{B} 
Figure 4 Pseudo-code for handling a 
character with three components 
 
Let s be the seed (s = cc); 
Let r be the result component; 
if s[structure] = IDC{A} 
if s[component(1)][ structure] = IDC{A} then 
r = IDC{K} + 
s[component(1)][component(1)] + 
s[component(1)][component(2)] + 
s[component(2)] 
else if s[component(2)][ structure] = IDC{A} 
then 
r = IDC{K} + s[component(1)] + 
s[component(2)][component(1)] + 
s[component(2)][component(2)] 
end 
end 
**this algorithm also works when s[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 4 Pseudo-code of For the Split Step 
 
In many cases, we still want to maintain the 
characters in the right decomposition, e.g, to 
decompose them into two components first and 
then further decompose if needed. Take another  
character ??? as an example. Suppose it is only 
decomposed into two components (??? and 
???). This makes the search more complex. In 
order to simplify the search, we need to go 
through an internal step which we call the Split 
Step to decompose the character into three 
 components before we allow for component to 
character search. The pseudo code for the Split 
Step is shown in Figure 4. The generated result 
is shown in Table 5.  
 
Character IDC Comp1 Comp2 Comp3 
? A ? ?  
? A ? ?  
 
? K ? ? ? 
Table 5. An example Output of the Split Step 
 
For some characters like ???, the Split Step 
must consider the component  ?? ? in the 
middle as an insertion into the character ???. 
We use similar handling to decompose  ??? 
into  ???, ??? and ???, with an IDC {K}. In 
order to find a character with the component 
??? such as ??? , we need additional algorithm 
to locate components that are potentially being 
split to the two sides with an inserted component. 
We try to decompose a component into two 
sub-components if their IDC is ?A? or ?B?. 
Once we get the two sub-components, we try to 
make different combinations to see if there are 
any characters with an IDC {K} or {L} that 
contain the two sub-components as shown in 
Figure 5. 
 
Let x be a Chinese character (x = cc); 
Let Clst be the list of results c; 
if x[structure] = IDC{A} then 
Clst = c : c[structure] = IDC{K} and 
((c[component(1)] = x[component(1)] and 
c[component(2)] = x[component(2)] ) or 
(c[component(2)] = x[component(1)] and 
c[component(3)] = x[component(2)]) or 
(c[component(1)] = x[component(1)] and  
c[component(3)] = x[component(2)])) 
end 
**this algorithm also works when x[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 5. Pseudo-code of finding inserted 
component 
 
3. Performance Evaluation 
Since the algorithms have to do excessive search 
for many combinations in many levels 
recursively, performance becomes a very 
important issue especially if we want to make 
this for public access through the internet. 
However, since the decomposition is static, it 
does not need to be done in real time. as the 
search doesn?t need to be done online, In other 
words, searching of the same data will always 
give the same result unless the decomposition 
rules or algorithms are changed. Consequently, 
we built two pre-searched tables to store the 
results of both ?Compnt-to-Char? algorithm and 
the ?Char-to-Compnt?algorithm. Once we have 
the pre-searched tables, we can totally avoid the 
recursive search. Instead, the search result can 
be directly retrieved in a single tuple. This 
results in much better performance both in terms 
of usage of CPU time and I/O usage. 
 
Character Pre-searched result 
? ? ? ? ? ? ?  
? ? ? ? ? ? 
??  
Table 6. Examples of pre-searched results of 
?Cha-to-Compnt?Algorithm 
 
Character Pre-searched result 
? ? ? ? ? ? ? ? ? (total 
5481 characters) 
? ? ? ? ? ? ? ? (total 44 
characters) 
??  
Table 7. Examples of pre-searched results of 
?Component to Character? 
Table 6 and table 7 shows some samples of the 
pre-searched tables for the downward search and 
the upward search, respectively. 
  
Although the advanced control algorithms can 
retrieve most Chinese characters correctly, they 
also return some components that do not make 
much sense. For example, the character ??? has 
a structure of IDC{B}, and components ??? 
and ?? ?. However, when it is eventually 
decomposed into  ???, ??? and ???. Using 
the algorithm ?Char-to-Compnt?, the component 
??? will also be returned, even though ??? has 
no cognate relationship with the character ???. 
We can take into consideration of only a subset 
of characters that can be split in character 
formation, such as ??? and ???. This way, the 
insertion components will only be considered for 
these characters.   
 
4. Conclusion 
In this paper, we focus on the algorithms of 
character decomposition and formation. The 
results can be used for the standardization of 
 computer fonts, glyphs, or relevant language 
resources. We have implemented a Chinese 
Character Search System based on the result of 
this standardization work. We can use this search 
system to look for character decomposition or 
formation results. The system comes with many 
handy and useful features. It provides a lot of 
useful information on Chinese characters, such 
as the code for various encodings, and 
pronunciations. A stand-alone version is also 
built. The actual implementation of these 
algorithms and of the database helps people to 
get information about Chinese characters very 
quickly. It further facilitates researchers? work in 
related areas. For more information on the 
system, please visit the website 
http://www.iso10646hk.net. 
 
Acknowledgement 
The project is partially supported by the Hong 
Kong SAR Government(Project code: ITF 
AF/212/99) and  the Hong Kong Polytechnic 
University(Project Code: Z044). 
 
References 
[1] ISO/IEC, ?ISO/IEC 10646-1 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2000 
[2] ISO/IEC, ?ISO/IEC 10646-2 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2001 
[3] John Jenkins, "New Ideographs in Unicode 
3.0 and Beyond", Proceedings of the 15th 
International Unicode Conference C15, 
San Jose, California, Sept. 1-2, 1999 
[4] Dept. of Education(Taiwan), ?Dictionary 
of Chinese Character Variants Version 2?, 
Dept. of Education, Taiwan, 2000 
[5] ???????, ????????, (?
???????), ???????
? , ????? ( LEE Hok-ming as 
Chief Editor, Common Character 
Glyph Table 2nd Edition, Hong Kong 
Institute of Education, 2000) 
[6] GF3001-1997???????????? 
?????? ?????GB13000.1?
?????????, ????????
???, 1997? 12?. 
 
[7] Lu Qin, The Hong Kong Glyph 
Specifications for ISO 10646?s Ideographic 
Characters. 21st International Unicode 
Conference, Dublin, Ireland, May 2002 
[8] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set?, HKSARG, September 28, 
1999 
[9] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set ? 2001 ? , HKSARG, 
December 31, 2001 
 
 
  
A Unicode based Adaptive Segmentor 
Q. Lu, S. T. Chan, R. F. Xu, T. S. Chiu 
Dept. Of Computing, 
The Hong Kong Polytechnic University, 
Hung Hom, Hong Kong 
{csluqin,csrfxu}@comp.polyu.edu.hk 
B. L. Li, S. W. Yu 
The Institute of Computational Linguistics, 
Peking University, 
Beijing, China 
{libi,yusw}@pku.edu.cn 
 
Abstract 
This paper presents a Unicode based 
Chinese word segmentor. It can handle 
Chinese text in Simplified, Traditional, or 
mixed mode. The system uses the strategy 
of divide-and-conquer to handle the 
recognition of personal names, numbers, 
time and numerical values, etc in the pre-
processing stage. The segmentor further 
uses tagging information to work on 
disambiguation. Adopting a modular 
design approach, different functional parts 
are separately implemented using 
different modules and each module 
tackles one problem at a time providing 
more flexibility and extensibility. Results 
show that with added pre-processing 
modules and accessorial modules, the 
accuracy of the segmentor is increased 
and the system is easily adaptive to 
different applications. 
1 Introduction 
The most difficult problem in Chinese word 
segmentation is due to overlapping ambiguities [1-
2]. The recognition of names, foreign names, and 
organizations are quite unique for Chinese. Some 
systems can already achieve very high accuracy [3], 
but they heavily rely on manual work in getting the 
system to be trained to work certain language 
environment. However, for many applications, we 
need to look at the cost to achieve high accuracy. 
In a competitive environment, we also need to 
have systems that are quickly adaptive to new 
requirements with limited resources available. 
In this paper, we report a Unicode based Chinese 
word segmentor. The segmentor can handle 
Chinese text in Simplified, Traditional, or mixed 
mode where internally only one dictionary is 
needed. The system uses the strategy of divide-
and-conquer to handle the recognition of personal 
names, numbers, time and numerical values. The 
system has a built-in new word extractor that can 
extract new words from running text, thus save 
time on training and getting the system quickly 
adaptive to new language environment. The 
Bakeoff results in the open text for our system in 
all categories have shown that it works reasonably 
good for all different corpora. 
The rest of the paper is organized as follows. 
Section 2 presents our system design objectives 
and components. Section 3 discusses more 
implementation details. Section 4 gives some 
performance evaluations. Section 5 is the 
conclusion. 
2 Design Objectives and Components 
With the wide use of Unicode based operating 
systems such as Window 2000 and Window XP, 
we now see more and more text data written in 
both the Simplified form and the Traditional form 
to co-exist on the same system. It is also likely that 
text written in mixed mode. Because of this reality, 
the first design objective of this system is its ability 
to handle the segmentation of Chinese text written 
in either Simplified Chinese, Traditional Chinese, 
or mixed mode.  As an example, we should be able 
to segment the same sentence in different forms 
such as the example given below:   
 
The second design objective is to adopt the 
modular design approach where different 
functional parts are separately implemented using 
independent modules and each module tackles one 
problem at a time. Using this modular approach, 
we can isolate problems and fine tune each module 
with minimal effect on other modules in the system. 
  
Special features like adding new rules or new 
dictionary can be easily done without affecting 
other modules. Consequently, the system is more 
flexible and can be easily extended.  
The third design objective of the system is to make 
the segmentor adaptive to different application 
domains. We consider it having more practical 
value if the segmentor can be easily trained using 
some semi-automatic process to work in different 
domains and work well for text with different 
regional variations. We consider it essential that 
the segmentor has tools to help it to obtain regional 
related information quickly even if annotated 
corpora are not available. For instance, when it 
runs text from Hong Kong, it must be able to 
recognize the personal names such as  if 
such a name(quadra-gram) appears in the text often. 
 
Figure 1. System components 
Figure 1 shows the two major components, the 
segmentor and data manager. The segmentor is the 
core component of the system. It has a pre-
processor, the kernel, and a post-processor. As the 
system has to maintain a number of tables such as 
the dictionaries, family name list, etc., a separate 
component called data manager is responsible in 
handling the maintenance of these data.  The pre-
processor has separate modules to handle 
paragraphs, ASCII code, numbers, time, and 
proper names including personal names, place and 
organizational names, and foreign names. The 
kernel supports different segmentation algorithms. 
It is the application or user?s choice to invoke the 
preferred segmentation algorithms that at current 
time include the basic maximum matching and 
minimum matching in both forward and backward 
mode. These can also be used to build more 
complicated algorithms later on. In addition, the 
system provides segmentation using part-of-speech 
tagging information to help resolve ambiguity. The 
post-processor applies morphological rules which 
cannot be easily applied using a dictionary.   
The data manager helps to maintain the knowledge 
base in the system. It also has an accessory 
software called the new word extractor which can 
collect statistical information based on character 
bi-grams, tri-grams and quadra-grams to semi-
automatically extract words and names so that they 
can be used by the segmentor to improve 
performance especially when switching to a new 
domain. Another characteristic of this segmentor is 
that it provides tagging information for segmented 
text. The tagging information can be optionally 
omitted if not needed by an application. 
3 Implementation Details 
The basic dictionary of this system was provided 
by Peking University [4] and we also used the 
tagging data from [4]. The data structure for our 
dictionaries are very similar to that discussed in [5]. 
As our program needs to handle both Simplified 
and Traditional Chinese characters, Unicode is the 
only solution for dealing with more than one script 
at the same time. 
Even though it is our design objective to support 
both Simplified and Traditional Chinese, we do not 
want to keep two different sets of dictionaries for 
Simplified and Traditional Chinese. Even if two 
versions are kept, it would not serve well for text 
in mixed mode. For example, Traditional Chinese 
word of ?the day after tomorrow? should be , 
and for Simplified Chinese, it should be . 
However sometimes we can see the word  
appears in a Traditional Chinese text. We cannot 
say that it is wrong because the sentence is still 
semantically correct especially in Unicode 
environment. Therefore the segmentor should be 
able to segment those words correctly such as in 
the examples: ? ?, and in ?  
?. We must also deal with dictionary 
maintenance related to Chinese variants. For 
example, characters  are variants, so are 
. 
Data manager Segmentor 
Pre-
Processor 
Kernel 
Post 
Processor 
New Word
Extractor
Knowledge-
base 
  
In order to keep the dictionary maintenance simple, 
our system uses a single dictionary which only 
keeps the so called canonical form of a word. In 
our system, the canonical form of a word is its 
?simplified form?.  We quoted the word 
?simplified? because only certain characters have 
simplified forms such as  to , but for  
, there is no simplified form. In the case of 
variants, we simply choose one of them as the 
canonical character.  The canonical characters are 
maintained in the traditional-simplified character 
conversion table as well as in a variant table.  
Whenever a new word, item, is added into the 
dictionary, it must be added using a function 
CanonicalConversion(), which takes item as an 
input. During segmentation, the corresponding 
dictionary look up function will first convert the 
token to its canonical form before looking up in the 
dictionary.  
The personal name recognizers (separate for 
Chinese names and foreign names) use the 
maximum-likelihood algorithm with consideration 
of commonly used Chinese family names, given 
names, and foreign name characters. It works for 
Chinese names of length up to 5 characters. In the 
following examples you can see that our system 
successfully recognized the name . This 
is done using our algorithm, not by putting her 
name in our dictionary: 
 
Organization names and place names are 
recognized mainly using special purpose 
dictionaries. The segmentor uses tagging 
information to help resolve ambiguity. The 
disambiguation is mostly based on rules such as  
p + (n + f) -> p + n + f 
which would word to correct 
  
For efficiency reasons, our system uses only about 
20 rules. The system is flexible enough for new 
rules to be added to improve performance.  
The new word extractor is an accessory program to 
extract new words from running text based on 
statistical data which can either be grabbed from 
the internet or collected from other sources. The 
basic statistical data include bi-gram frequency, tri-
gram frequency, and quadra-gram frequencies. In 
order to further example whether a bi-gram, say  
, is indeed a word, we further collect forward 
conditional frequency of  , and 
the back-ward conditional frequency of , 
. For an i-gram token, we also 
use the (i+1)-gram statistics to eliminate those i-
grams that are only a part of (i+1) ? gram word.  
For instance, if the frequency of bi-gram  is 
very close to the frequency of tri-gram , it 
is less likely that  is a word. Of course, 
whether  is a word depends on quadra-gram 
results.  Using the statistical result, a set of rules 
was applied to these i-grams to eliminate entries 
that are not considered new words. Minimal 
manual work is required to identify whether the 
remaining candidates are new words. Before words 
are added into the dictionary, part-of-speech 
information are added manually (although not 
necessary) before using the canonical function. 
The following table shows examples of bi-grams 
which are found by the new word extractor using 
one year Hong Kong Commercial Daily News data. 
 
 
 
4 Performance Evaluation 
The valuation metrics used in [6] were adopted 
here. 
1
3
N
N
recall =     (1) 
 
2
3
N
Npresicion =     (2) 
  
precisionrecall
precisionrecallrecallprecisionF +
??= 2),(1   (3) 
where N  1  denotes the number of words in the 
annotated corpus, N 2 denotes the number of words 
identified by the segmentation algorithm , and N 3 is 
the number of words correctly identified. 
We participated in the open tests for all four 
corpora. The results are shown in the following 
table. 
The worst performance in the 4 tests were for the 
CTB(UPenn) data. From the observation from the 
testing data, we found that the main problem with 
have with CTB data is the difference in word 
granularity. To confirm our observation, we have 
done an analysis of combining errors and 
overlapping errors. The results show that the ratios 
of combining errors in all the error types are 
0.8425(AS), 0.87684(CTB), 0.82085(HK), and 
0.77102(PK). The biggest problem we have with 
AS data, on the other hand is due to out of 
vocabulary mistakes. Even though our new word 
extractor can help us to reduce this problem, but 
we have not trained our system using data from 
Taiwan.  Our best performance was on PK data 
because we used a very similar dictionary. The 
additional training of data for HK was done using 
one year Commercial Daily( ). 
The following table summarizes the execution 
speed of our program for the 4 different 
sources: 
Data No. of 
chars 
Processi
ng Time 
(sec.) 
Processin
g Rate 
(char/sec) 
Segmentat
ion Rate 
(char/sec) 
AS 18,743 4.703 3,985 7,641 
CTB 62,332 10.110 6,165 7,930 
HK 57,432 10.329 5,560 7,109 
PK 28,458 4.829 5,893 10,970 
 
The program initialization needs around 2.25 
seconds mainly to load the dictionaries and other 
data into the memory before the segmentation can 
start. If we only count the segmentation time, the 
rate of segmentation on the average is around 
7,500 characters for the first three corpora. It 
seems that the processing speed for Peking U. data 
is faster. This may be because the dictionaries we 
used are closer to the PK system, thus it would 
take less time to work on disambiguation.  
5 Conclusion 
In this paper, design and algorithms of a general-
purposed Unicode based segmentor is proposed. It 
is able to process Simplified and Traditional 
Chinese appear in the same text. Sophisticated pre-
processing and other auxiliary modules help 
segmenting text more accurately. User interactions 
and modules can be easily added with the help of 
its modular design. A built-in new word extractor 
is also implemented for extracting new words from 
running text. It saves much time on training and 
thus it can be quickly adapted to new environments. 
Acknowledgement 
We thank the PI of ITF Grant by ITC of 
HKSARG (ITS/024/01) entitled: Towards Cost-
Effective E-business in the News Media & 
Publishing Industry for the use of HK Commercial 
Daily. 
References 
[1] Automatic Segmentation and Tagging for Chinese 
Text ( ) , K.Y. Liu, 
Commercial Press, 2000 
[2] Segmentation Issues in Chinese Information 
Processing,  (C.N. Huang Issue No. 
1, 1997) 
[3] The design and Implementation of a Modern General 
Purpose Segmentation System (B. Lou,  R. Song, W.L. 
Li, and Z.Y. Luo, Journal of Chinese Information 
Processing, Issue No. 5, 2001) 
[4] (Institute of 
Computational Linguistics, Peking Univ., 2002) 
[5] 
  Journal of Chinese information 
processing vol. 14, no. 1, 2001) 
[6] Chinese Word Segmentation and Information 
Retrieval, Palmer D., and Burger J., In AAAI 
Symposium Cross-Language Text and Speech 
Retrieval 1997 
  
Using Synonym Relations In Chinese Collocation Extraction 
Wanyin Li 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
Qin Lu 
Department of Computing,  
The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csluqin @comp.polyu.edu.hk 
Ruifeng Xu 
Department of Computing, The Hong Kong Polytechnic University,  
Hung Hom, Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
 
 
Abstract 
A challenging task in Chinese collocation 
extraction is to improve both the precision and 
recall rate. Most lexical statistical methods 
including Xtract face the problem of unable to 
extract  collocations with lower frequencies than 
a given threshold. This paper presents a method 
where HowNet is used to find synonyms using a 
similarity function. Based on such synonym 
information, we have successfully extracted 
synonymous collocations which normally cannot 
be extracted using the lexical statistical 
approach. We applied synonyms mapping to 
each headword to extract more synonymous 
word bi-grams. Our evaluation over 60MB 
tagged corpus shows that we can extract 
synonymous collocations that occur with very 
low frequency, sometimes even for collocations 
that occur only once in the training set. 
Comparing to a collocation extraction system 
based on Xtract, we have reached the precision 
rate of 43% on word bi-grams for a set of 9 
headwords, almost 50% improvement from 
precision rate of 30% in the Xtract system. 
Furthermore, it  improves the recall rate of word 
bi-gram collocation extraction by 30%. 
1 Introduction 
A Chinese collocation is a recurrent and 
conventional expression of words which  holds 
syntactic and semantic relations.  A widely adopted 
definition given by Benson (Benson 1990) stated 
that ?a collocation is an arbitrary and recurrent 
word combination.?  For example, we say ?warm 
greetings? rather than ?hot greetings?, ?broad 
daylight? rather than ?bright daylight?.  Similarly, 
in Chinese ? ? ? ? ? ? are three nouns 
with similar meanings, however, we  say 
? ? rather than ? ?, 
? ?rather than ? ?.   
 
Study in collocation extraction using lexical 
statistics has gained some insights to the issues 
faced in collocation extraction (Church and Hanks 
1990, Smadja 1993, Choueka 1993, Lin 1998). As 
the lexical statistical approach is developed based 
on the ?recurrence? property of collocations, only 
collocations with reasonably good recurrence can 
be extracted. Collocations with low occurrence 
frequency cannot be extracted, thus affecting the 
recall rate. The precision rate using the lexical 
statistics approach can reach around 60% if both 
word bi-gram extraction and n-gram extractions 
are taking into account (Smadja 1993, Lin 1997 
and Lu et al 2003). The low precision rate is 
mainly due to the low precision rate of word bi-
gram extractions as only about 30% - 40% 
precision rate can be achieved for word bi-grams.  
In this paper, we propose a different approach to 
find collocations with low recurrences. The main 
idea is to make use of synonym relations to extract 
synonymous collocations. Lin (Lin 1997) 
described a distributional hypothesis that if two 
words have similar set of collocations, they are 
probably similar. In HowNet, Liu Qun (Liu et al 
2002) defined the word similarity as two words 
that can substitute each other in the context and 
keep the sentence consistent in syntax and 
semantic structure. That means, naturally, two 
similar words are very close to each other and they 
can be used in place of the other in certain context. 
For example, we may either say  ? ?or ? ? 
as  and are semantically close to each 
other. We apply this lexical phenomenal after the 
lexical statistics based extractor to find the low 
frequency synonymous collocations, thus 
increasing recall rate.  
 
  
  The rest of this paper is organized as follows. 
Section 2 describes related existing collocation 
extraction techniques based on both lexical 
statistics and synonymous collocation. Section 3 
describes our approach on collocation extraction. 
Section 4 evaluates the proposed method. Section 5 
draws our conclusion and presents possible future  
work. 
2 Related Work 
Methods have proposed to extract collocations 
based on lexical statistics. Choueka (Choueka 
1993) applied quantitative selection criteria based 
on frequency threshold to extract adjacent n-grams 
(including bi-grams). Church and Hanks (Church 
and Hanks 1990) employed mutual information to 
extract both adjacent and distant bi-grams that tend 
to co-occur within a fixed-size window. But the 
method did not extend to extract n-grams. Smadja 
(Smadja 1993) proposed a statistical model by 
measuring the spread of the distribution of co-
occurring pairs of words with higher strength. This 
method successfully extracted both adjacent and 
distant bi-grams and n-grams. However, the 
method failed to extract bi-grams with lower 
frequency. The precision rate on bi-grams 
collocation is very low, only around high 20% and 
low 30%. Even though, it is difficult to measure 
recall rate in collocation extraction (almost no 
report on recall estimation), It is understood that 
low occurrence collocations cannot be extracted. 
Our research group has further applied the Xtract 
system to Chinese (Lu et al 2003) by adjusting the 
parameters to optimize the algorithm for Chinese 
and a new weighted algorithm was developed 
based on mutual information to acquire word bi-
grams with one higher frequency word and one 
lower frequency word. The result has achieved an 
estimated 5% improvement in recall rate and a 
15% improvement in precision comparing to the 
Xtract system. 
All of the above techniques do not take 
advantage of the wide range of lexical resources 
available including synonym information. Pearce 
(Pearce 2001) presented a collocation extraction 
technique that relies on a mapping from a word to 
its synonyms for each of its senses. The underlying 
intuitions is that if the difference between the 
occurrence counts of one synonyms pair with 
respect to a particular word was at least two, then 
this was deemed sufficient to consider them as a 
collocation. To apply this approach, knowledge in 
word (concept) semantics and relations to other 
words must be available such as the use of 
WordNet. Dagan (Dagan 1997) applied similarity-
based smoothing method to solve the problem of 
data sparseness in statistical natural language 
processing. The experiments conducted in his later 
works showed that this method achieved much 
better results than back-off smoothing methods in 
word sense disambiguation. Similarly, Hua Wu 
(Wu and Zhou 2003) applied synonyms 
relationship between two different languages to 
automatically acquire English synonymous 
collocation. This is the first time that the concept 
synonymous collocation is proposed. A side 
intuition raised here is that nature language is full 
of synonymous collocations. As many of them 
have low occurrences, they are failed to be 
retrieved by lexical statistical methods. Even 
though there are Chinese synonym dictionaries, 
such as  ( Tong Yi Ci Lin), the 
dictionaries lack structured knowledge and 
synonyms are too loosely defined to be used for 
collocation extraction.  
HowNet developed by Dong et al(Dong and 
Dong 1999) is the best publicly available resource 
on Chinese semantics. By making use of semantic 
similarities of words, synonyms can be defined by 
the closeness of their related concepts and the 
closeness can be calculated. In Section 3, we 
present our method to extract synonyms from 
HowNet and using synonym relations to further 
extract collocations. 
Sun (Sun 1997) did a preliminary Quantitative 
analysis on Chinese collocations based on their 
arbitrariness, recurrence and the syntax structure. 
The purpose of this study is to help differentiate if 
a collocation is true or not according to the 
quantitative factors. By observing the existence of  
synonyms information in natural language use, we 
consider it possible to identify different types of 
collocations using more semantic and syntactic 
information available. We discuss the basic ideas 
in section 5.. 
3 Our Approach 
Our method of extracting Chinese collocations 
consists of three steps.  
Step 1: Take the output of any lexical statistical 
algorithm which extracts word bi-gram 
collocations. The data is then sorted 
according to each headword , Wh, with its co-
word, Wc, listed. 
Step 2: For each headword Wh used to extract bi-
grams, we acquire its synonyms based on a 
similarity function using HowNet. Any word 
in HowNet having similarity value over a 
threshold value is chosen as a synonym 
headword Ws for additional extractions. 
Step 3: For each synonym headword, Ws, and the 
co-word Wc of Wh, as its synonym, if the bi-
gram (Ws , Wc) is not in the output of the 
  
lexical statistical algorithm in Step one, take 
this bi-gram (Ws , Wc) as a collocation if the 
pair co-occurs in the corpus by additional 
search to the corpus. 
3.1 Structure of HowNet 
Different from WordNet or other synonyms 
dictionary, HowNet describes words as a set of 
concepts  and each concept is described by a 
set of primitives . The following lists for the 
word , one of its corresponding concepts 
 
 
In the above record, DEF is where the primitives 
are specified. DEF contains up to four types  of 
primitives: the basic independent primitive   
, the other independent 
primitive , the relation primitive  
, and the symbol primitive , 
where the basic independent primitive and the 
other independent primitive are used to indicate the 
semantics of a concept and the others are used to 
indicate syntactical relationships. The similarity 
model described in the next subsection will 
consider both of these relationships.  
The primitives are linked by a hierarchical tree 
to indicate the parent-child relationships of the 
primitives as shown in the following example:  
 
 
 
This hierarchical structure provides a way to link 
one concept with any other concept in HowNet, 
and the closeness of concepts can be simulated by 
the distance between two concepts. 
3.2 Similarity Model Based on HowNet 
Liu Qun (Liu 2002) defined word similarity as 
two words which can substitute each other in the 
same context and still maintain the sentence 
consistent syntactically and semantically. This is 
very close to our definition of synonyms. Thus we 
directly used their similarity function, which is 
stated as follows.  
A word in HowNet is defined as a set of 
concepts and each concept is represented by 
primitives.  Thus, HowNet can be described by W, 
a collection of n words, as: 
 W = { w1, w2, ? wn}Each word wi is, in 
turn, described by a set of concepts S as:  
 Wi = { Si1, Si2,?Six}, 
And, each concept Si  is, in turn, described by a 
set of primitives: 
 Si  = { pi1, pi2 ?piy } 
For each word pair, w1 and w2, the similarity 
function is defined by 
  )1(),(max),( 21...1,..121 jimjni SSSimwwSim ===     
where S1i is the list of concepts associated with W1 
and S2j is the list of concepts associated with W2.  
As any concept Si is presented by its primitives, 
the similarity of primitives for any p1, and  p2 of 
the same type, can be expressed by the following 
formula: 
 ?
?
+= ),(),( 2121 ppDis
ppSim     (2) 
where ?  is an adjustable parameter set to 1.6, 
and ),( 21 ppDis is the path length between p1 and 
p2 based on the semantic tree structure. The above 
formula where ? is a constant does not indicate 
explicitly the fact that the depth of a pair of nodes 
in the tree affects their similarity. For two pairs of 
nodes (p1 ,  p2) and  (p3 ,  p4) with the same distance,  
the deeper the depth is, the more commonly shared 
ancestros they would have which should be 
semantically closer to each other. In following two 
tree structures, the pair of nodes (p1, p2) in the left 
tree should be more similar than (p3 ,  p4)  in the 
right tree. 
 
                root 
 
 
 
 
 
                             p2 
 
             p1 
                      
                     root 
 
 
 
 
 
                          P3 
 
 
                    P4 
 
 
  
 
To indicate this observation,  ?  is modified as a 
function of tree depths of the nodes using the 
formula  ? =min(d(p1), d(p2)) . Consequently, the 
formula (2) is rewritten as formular (2?) during the 
experiment. 
 
))(),(min(),(
))(),(min(),(
2121
21
21 pdpdppDis
pdpdppSim +=
    (2?) 
 
where d(pi) is the depth of node pi  in the tree . The 
comparison of calculating the word similarity by 
applying the formula (2) and  (2?) is shown in 
Section 4.4. 
 
 Based on the DEF description in HowNet, 
different primitive types play different roles only 
some are directly related to semantics. To make 
use of both the semantic and syntactic information 
included in HowNet to describe a word, the 
similarity of two concepts should take into 
consideration of all primitive types with weighted 
considerations and thus the formula is defined as 
)3(),(),( 21
1
4
1
21 jj
i
j
j
i
i ppSimSSSim ??
==
= ?   
where ?i is a weighting factor given in (Liu 
2002) with the sum of ?1 + ?2 + ?3 + ?4 being 1? 
and ?1 ? ?2 ? ?3 ? ?4. The distribution of the 
weighting factors is given for each concept a priori 
in HowNet to indicate the importance of primitive 
pi in defining the corresponding concept S. 
 
3.3 Collocation Extraction 
In order to extract collocations from a corpus, 
and to obtain result for Step 1 of our algorithm, we 
used the collocation extraction algorithm 
developed by the research group at the Hong Kong 
Polytechnic University(Lu et al 2003). The 
extraction of bi-gram collocation is based on the 
English Xtract(Smaja 1993) with improvements. 
Based on the three Steps mentioned earlier, we will 
present the extractions in each step in the 
subsections. 
 
3.3.1 Bi-gram Extraction 
Based on the lexical statistical model proposed 
by Smadja in Xtract on extracting English 
collocations, an improved algorithm was 
developed for Chinese collocation by our research 
group and the system is called CXtract. For easy of 
understanding, we will explain the algorithm 
briefly here. According to Xtract, word 
cooccurence is denoted by a tripplet (wh, wi, d)  
where wh is a given headword, wi is a co-word 
appeared in the corpus in a distance d within the 
window of [-5, 5]. The frequency fi of the co-word   
wi   in the window of [-5, 5] is defined as: 
?
?=
=
5
5
,
j
jii ff    (4) 
where  fi, j   is the frequency of the co-word at distance 
j in the corpus within the window. The average 
frequency of  fi , denoted by if , is given by 
10/
5
5
,?
?=
=
j
jii ff    (5) 
Then, the average frequency f , and the standard 
deviation ? are defined by 
?
=
=
n
i
ifn
f
1
1
;  2
1
)(1?
=
?=
n
i
i ffn
?  (6) 
The Strength of the co-occurrence for the pair 
(wh, wi,), denoted by ki, is defined by 
?
ffk ii
?= ?   (7) 
Furthermore, the Spread of (wh, wi,),, denoted as 
Ui, which characterizes the distribution of  wi 
around  wh is define as: 
10
)( 2,? ?
=
iji
i
ff
U ;    (8) 
To eliminate the bi-grams with unlikely co-
occurrence, the following sets of threshold values 
is defined: 
0:1 K
ff
kC ii ??= ?    (9) 
0:2 UUC i ?     (10) 
)(:3 1, iiji UKffC ?+?   (11) 
However, the above statistical model given by 
Smadja fails to extract the bi-grams with a much 
higher frequency of wh but a relatively low 
frequency word of wi,,  For example,  in the bi-
gram , freq ( ) is much lower than the 
freq ( ). Therefore, we further defined a 
weighted mutual information to extract this kind of 
bi-grams: 
  ,
)(
),w(
0
h R
wf
wfR
i
i
i ?=      (12)  
As a result, the system should return a list of 
triplets (wh, wi, d), where  (wh, wi,) is considered 
collocations.  
  
3.3.2 Synonyms Set 
For each given headword wh, before taking it as 
an input to extract its bi-grams directly, we fist 
apply the similarity formula described in Equation 
(1) to generate a set of synonyms headwords Wsyn: 
 
   }),(:{ ?>= shssyn wwSimwW                             (13) 
Where 0 <? <1 is an algorithm parameter which 
is adjusted based on experience. We set it as 0.85 
from the experiment because we would like to 
balance the strength of the synonyms relationship 
and the coverage of the synonyms set. The setting 
of the parameter ? < 0.85 weaks the similarity 
strength of the extracted synonyms. For example, 
for a given collocation ? ?, that is unlikely 
to include the candidates ? ?, ? ?, 
? ?.  On the other hand, by setting the 
parameter ? > 0.85 will limit the coverage of the 
synonyms set and hence lose valuable synonyms. 
For example, for a given bi-gram ? ?, we 
hope to include the candidate synonymous 
collocations such as  ? ?, ? ?, 
? ?. We will show the test of ?  in the 
section 4.2. 
This synonyms headwords set provides the 
possibility to extract the synonymous collocation 
with the lower frequency that failed to be extracted 
by lexical statistic. 
3.3.3 Synonymous Collocations 
A phenomenal among the collocations in natural 
language is that there are many synonymous 
collocations exist. For example, ?switch on light? 
and ?turn on light?, ? ? and ? ?. 
Due to the domain specification of the corpus, 
some of the synonymous collocations may fail to 
be extracted by the lexical statistic model because 
of their lower frequency. Based on this 
observation, this paper takes a further step. The 
basic idea is for a bi-gram collocation (wh, wc, d ) 
we select the synonyms ws of wh with the 
maximum similarity respect to all the concepts 
contained by wh, we deem (ws, wc, d ) as a 
collocation if its occurrence is greater than 1 in the 
corpus. There are similar works discussed by 
Pearce (Pearce 2001). . 
For a given collocation (ws, wc,, d), if ws ? Wsyn, 
then we deem the triple (ws, wc,, d) as a 
synonymous collocation with respect to the 
collocation (wh, wc,, d) if the co-occurrence of (ws, 
wc, , d) in the corpus is greater than one. Therefore, 
we define the collection of synonymous 
collocations Csyn as: 
}1),,(:),,{( >= dwwFreqdwwC cscssyn           (14) 
where  ws ? Wsyn. 
4 Evaluation 
The performance of collocation is normally 
evaluated by precision and recall as defined below. 
nsCollocatioextractedofnumbertotal
nsCollocatioExtractedcorrectofnumberprecision
    
    = (15) 
nsCollocatioactualofnumbertotal
nsCollocatioExtractedcorrectofnumberrecall
    
    =  (16) 
To evaluate the performance of our approach, we 
conducted a set of experiments based on 9 selected 
headwords. A baseline system using only lexical 
statistics given in 3.3.1 is used to get a set of 
baseline data called Set A. The output using our 
algorithm is called Set B. Results are checked by 
hand for validation on what is true collocation and 
what is not a true collocation. 
 
Table 1. Sample table for the true collocation 
with headword ? ?  
 
Table 2. Sample table for the bi-grams that are 
not true collocations  
  
Table 1 shows samples of extracted word bi-grams 
using our algorithm that are considered 
synonymous collocations for the headword ? ?. 
Table 2 shows extracted bi-grams by our algorithm 
that are not considered true collocations. 
4.1 Test Set 
Our experiment is based on a corpus of six 
months tagged People Daily with 11 millions 
number of words. For word bi-gram extractions, 
we consider only content words, thus headwords 
are selected from noun, verb and adjective only. 
For evaluation purpose, we selected randomly 3 
nouns, 3 verbs and 3 adjectives with frequency of 
low, medium and high. Thus, in Step 1 of the 
algorithm, 9 headwords were  used to extract bi-
gram collocations from the corpus, and 253 pairs 
of collocations were extracted. Evaluation by hand 
has identified 77 true collocations in Set A test set. 
The overall precision rate is 30% (see Table 3).  
 
 Noun+Verb
+Adjective 
Headword 9 
Extracted Bi-grams 253 
True collocations using 
lexical statistics only 
77 
Precision rate 30% 
 Table 3: Statistics in test set for set A 
 
Using Step 2 of our algorithm, where ?=0.85 is 
used, we have obtained 55 synonym headwords 
(include the 9 headwords). Out of these 55 
synonyms, 614 bi-gram pairs were then extracted 
from the lexical statistics based algorithm, in 
which 179 are consider true collocations. Then, by 
applying Step 3 of our algorithm, we extracted an 
additional 201 bi-gram pairs, among them, 178 are 
considered true collocations. Therefore, using our 
algorithm, the overall precision rate has achieved 
43%, an improvement of almost 50%. The data is 
summarized in Table 4. 
 n., v, and adj. 
Synonyms headword 55 
Bi-grams (lexical statistics) 614 
Non-synonym collocations 
(lexical statistics only) 
179 
Extracted synonym 
collocations Step 2 
201 
True synonym collocations 
using Step 2 
178 
Overall precision rate 43% 
Table 4: Statistics in test set for mode B 
4.2 The choice of ? 
We also conducted a set of experiments to 
choose the best value for the similarity function?s 
threshold ?. We tested the best value of ? with both 
the precision rate and the estimated recall rate 
using the so called remainder bi-grams. The 
remainder bi-grams is the total number of bi-grams 
extracted by the algorithm. When precision goes 
up, the size of the result is smaller, which in a way 
is an indicator of less recalled collocations. Figure 
1 shows the precision rate and the estimated recall 
rate in testing the value of ?. 
 
Figure 1. Precision Rate vs. value of ? 
From Figure 1, it is obvious that at ?=0.85 the 
recall rate starts to drop more drastically without 
much incentive for precision. 
 
 Extracted Bi-
grams using 
lexical 
statistics 
Extracted 
Synonyms 
Collocations 
using Step 2 
(1.2,1.4,12) 465 328 
(1.4,1.4,12) 457 304 
(1.4,1.6,12) 394 288 
(1.2,1.2,12) 513 382 
(1.2,1.2,14) 503 407 
(1.2,1.2,16) 481 413 
      Table 5: Value of (K0, K1, U0). 
4.3 The test of (K0, K1, U0) 
The original threshold for CXtract is (1.2, 1.2, 12) 
for the parameters (K0, K1, U0). However, with 
synonyms collocations, we have also conducted 
some experiments to see whether the parameters 
should be adjusted. Table 5 shows the statistics to 
test the value of (K0, K1, U0). The similarity 
threshold ? was fixed at 0.85 throughout the 
experiments. 
  
The experimental shows that varying the value of 
(k0, k1) does not bring any benefit to our algorithm. 
However, increasing the value of u0 did improve 
the extraction of synonymous collocations. Figure 
2 shows that U0 =14 is a good trade-off for the 
precision rate and the remainder Bi-grams. The basic 
meaning behind the result is reasonable. According to 
Smadja, U0 defined in the formula (8) represents the 
co-occurrence distribution of the candidate 
collocation (wh, wc) in the position of d (-5 ? d ? 
5). For a true collocation (wh, wc,, d), its co-
occurrence  in the position d is much higher than in 
other positions which leads to a peak in the co-
occurrence distribution. Therefore, it is selected by 
the statistical algorithm based on the formula (10). 
Based on the physical meaning behind, one way to 
improve the precision rate is to increase the value of  
the threshold U0.  A side effect to an increased  value 
of U0  is that the recall is decreased because some  
true collocations do not meet the condition of co-
occurrence greater than U0. Step 2 of the new 
algorithm regains some  true collocations lost 
because of a higher U0. in Step 1.  
               Figure 2. Precision Rate vs. Value of U0 
 
4.4 The comparison of similarity calculation 
based on formula  (2) and (2?) 
Table 6 shows the similarity value given by 
formula (2) where ?  is a constant given the value 
1.6 and by formula (2?) where ? is replaced by a 
function of the depths of the nodes. Results show 
that (2?) is more fine tuned and reflects the nature 
of the data better. For example, and 
are more similar than and . 
 and are much similar but not the same. 
 
 
Table 6: comparison of similarity calculation 
5 Conclusion and Further Work 
In this paper, we have presented a method to 
extract bi-gram collocations using lexical statistics 
model with synonyms information. Our method 
reaches the precision rate of 43% for the tested data. 
Comparing to the precision of 30% using lexical 
statistics only, our improvement is close to 50%. In 
additional, the recall improved 30%. The contribution 
is that we have made use of synonym information 
which is plentiful in the natural language use and it 
works well to supplement the shortcomings of lexical 
statistical method.  
Manning claimed that the lack of valid 
substitution for a synonym is a characteristics of 
collocations in general (Manning and Schutze 
1999). To extend our work, we consider the use of 
synonym information can be further applied to 
help identify collocations of different types.  
Our preliminary study has suggested that 
collocation can be classified into 4 types:   
Type 0 Collocation:  Fully fixed collocation 
which include some idioms, proverbs and sayings 
such as ? ? ? ? and so on.  
Type 1 Collocation:  Fixed collocation in which 
the appearance of one word implies the co-
occurrence of another one such as ? ?.  
Type 2 Collocation: Strong collocation which 
allows very limited substitution of the components, 
for example, ? ?, ? ?, 
? ? and so on.  
Type 3 Collocation: Normal collocation which 
allows more substitution of the components, 
however a limitation is still required. For example, 
? ? ? ? ? ? 
? ? . 
  
By using synonym information and define 
substitutability, we can validate whether 
collocations are fixed collocations, strong 
collocations with very limited substitutions, or 
general collocations that can be substituted more 
freely. 
6 Acknowledgements 
Our great thanks to Dr. Liu Qun of the Chinese 
Language Research Center of Peking University for 
letting us share their data structure in the Synonyms 
Similarity Calculation. This work is partially 
supported by the Hong Kong Polytechnic 
University (Project Code A-P203) and CERG 
Grant (Project code 5087/01E) 
References  
M. Benson, 1990. Collocations and General 
Purpose Dictionaries. International Journal of 
Lexicography, 3(1): 23-35 
Y. Choueka, 1993. Looking for Needles in a 
Haystack or Locating Interesting Collocation 
Expressions in Large Textual Database. 
Proceedings of RIAO Conference on User-
oriented Content-based Text and Image 
Handling: 21-24, Cambridge. 
K. Church, and P. Hanks, 1990. Word Association 
Norms, Mutual Information,and Lexicography. 
Computational Linguistics, 6(1): 22-29. 
I. Dagan, L. Lee, and F. Pereira. 1997. Similarity-
based method for word sense disambiguation. 
Proceedings of the 35th Annual Meeting of 
ACL: 56-63, Madrid, Spain. 
Z. D. Dong and Q. Dong. 1999. Hownet, 
http://www.keenage.com 
D. K. Lin, 1997. Using Syntactic Dependency as 
Local Context to Resolve Word Sense Ambiguity. 
Proceedings of ACL/EACL-97: 64-71, Madrid, 
Spain 
Q. Liu, 2002. The Word Similarity Calculation on 
<<HowNet>>. Proceedings of 3rd Conference 
on Chinese lexicography, TaiBei 
Q. Lu, Y. Li, and R. F. Xu, 2003. Improving Xtract 
for Chinese Collocation Extraction.  Proceedings 
of IEEE International Conference on Natural 
Language Processing and Knowledge 
Engineering, Beijing 
C. D. Manning and H. Schutze, 1999. Foundations 
of Statistical Natural Language Processing. The 
MIT Press, Cambridge, Massachusetts  
D. Pearce, 2001. Synonymy in Collocation 
Extraction. Proceedings of NAACL'01 
Workshop on Wordnet and Other Lexical 
Resources: Applications, Extensions and 
Customizations 
F. Smadja, 1993. Retrieving collocations from text: 
Xtract. Computational Linguistics, 19(1): 143-
177 
H. Wu, and M. Zhou, 2003. Synonymous 
Collocation Extraction Using Translation 
Information. Proceeding of the 41st Annual 
Meeting of ACL 
D. K. Lin, 1998. Extracting collocations from text 
corpora. In Proc. First Workshop on 
Computational Terminology, Montreal, Canada. 
M. S. Sun, C. N. Huang and J. Fang, 1997. 
Preliminary Study on Quantitative Study on 
Chinese Collocations. ZhongGuoYuWen, No.1, 
29-38, (in Chinese). 
 
 
 The Construction of A Chinese Shallow Treebank 
Ruifeng Xu 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csrfxu@comp.polyu.edu.hk 
Qin Lu 
Dept. Computing, 
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong  
csluqin@comp.polyu.edu.hk 
Yin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
csyinli@comp.polyu.edu.hk 
Wanyin Li 
Dept. Computing,  
The Hong Kong Polytechnic University, 
Kowloon, Hong Kong 
cswyli@comp.polyu.edu.hk 
 
Abstract 
This paper presents the construction of a 
manually annotated Chinese shallow Treebank, 
named PolyU Treebank. Different from 
traditional Chinese Treebank based on full 
parsing, the PolyU Treebank is based on 
shallow parsing in which only partial syntactical 
structures are annotated. This Treebank can be 
used to support shallow parser training, testing 
and other natural language applications. 
Phrase-based Grammar, proposed by Peking 
University, is used to guide the design and 
implementation of the PolyU Treebank. The 
design principles include good resource sharing, 
low structural complexity, sufficient syntactic 
information and large data scale. The design 
issues, including corpus material preparation, 
standard for word segmentation and POS 
tagging, and the guideline for phrase bracketing 
and annotation, are presented in this paper. 
Well-designed workflow and effective 
semiautomatic and automatic annotation 
checking are used to ensure annotation accuracy 
and consistency. Currently, the PolyU Treebank 
has completed the annotation of a 
1-million-word corpus. The evaluation shows 
that the accuracy of annotation is higher than 
98%. 
1 Introduction 
A Treebank can be defined as a syntactically 
processed corpus. It is a language resource  
containing annotations of information at various 
linguistic levels such as words, phrases, clauses and 
sentences to form a ?bank of linguistic trees?. There 
are many Treebanks built for different languages 
such as the Penn Treebank (Marcus 1993), ICE-GB 
(Wallis 2003), and so on. The Penn Chinese 
Treebank is an important resource (Xia et al 2000; 
Xue et al 2002). Its annotation is based on 
Head-driven Phrase Structure Grammar (HPSG). 
The corpus of 100,000 Chinese words has been 
manually annotated with a strict quality assurance 
process. Another important work is the Sinica 
Treebank at the Academic Sinica, Taiwan ( Chen et 
al. 1999; Chen et al 2003). Information-based Case 
Grammar (ICG) was selected as the language 
framework. A head-driven chart parser was 
performed to do phrase bracketing and annotating. 
Then, manual post-editing was conducted. 
According to the report, The Sinica Treebank  
contains 38,725 parsed trees with 329,532 words.  
Most reported Chinese Treebanks, including the 
two above, are based on full parsing which requires 
complete syntactical analysis including determining 
syntactic categories of words, locating chunks that 
can be nested, finding relations between phrases and 
resolving the attachment ambiguities. The output of 
full parsing is a set of complete syntactic trees. 
Automatic full parsing, however, is difficult to 
achieve good performance. Shallow parsing (or 
partial parsing) is usually defined as a parsing 
process aiming to provide a limited amount of local 
syntactic information such as non-recursive noun 
phrases, V-O structures and S-V structures etc. Since 
shallow parsing can recognize the backbone of a 
sentence more effectively and accurately with lower 
cost, people has in recent years started to work using 
results from shallow parsing. A shallow parsed 
Treebank can be used to extract information for 
different applications especially for training shallow 
parsers. 
Different from full parsing, annotation to a 
shallow Treebank is only targeted at certain local 
structures in a sentence. The depth of ?shallowness?  
and the scope of annotation vary from different 
reported work. Thus, two issues in shallow Treebank 
annotation is (1) what information and (2) to what 
depths the syntactic information should be annotated. 
Generally speaking, the degree of ?shallowness? and 
the syntactical labeling are determined by the 
requirement of the serving applications. The choice 
of full parsing or shallow parsing is dependent on 
the need of the application including resources and 
 the capability of system to be developed (Xia et al 
2000; Chen et al 2000; Li et al 2003). Currently, 
there is no large-scale shallow annotated Treebank 
available as a publicly resource for training and 
testing.  
In this paper, we present a manually annotated 
shallow Treebank, called the PolyU Treebank. It is 
targeted to contain 1-million-word contemporary 
Chinese text. The whole work on the PolyU 
Treebank follows the Phrase-based Grammar 
proposed by Peking University (Yu et al 1998). In 
this language framework, a phrase, lead by a lexical 
word(or sometimes called a content word) as a head, 
is considered the basic syntactical unit in a Chinese 
sentence. The building of the PolyU Treebank was 
originally designed as training data for a shallow 
parser used for Chinese collocation extraction. From 
linguistics viewpoint, a collocation occurs only in 
words within a phrase, or between the headwords of 
related phrases (Zhang and Lin 1992). Therefore, the 
use of syntactic information is naturally considered 
an effective way to improve the performance of 
collocation extraction systems. The typical problems 
like doctor-nurse (Church and Hanks 1990) could be 
avoided by using such information. When 
employing syntactical information in collocation 
extraction, we restrict ourselves to identify the stable 
phrases in the sentences with certain levels of 
nesting. Thus it has motivated us to produce a 
shallow Treebank. 
A natural way to obtain a shallow Treebank is 
through extracting shallow structures from a fully 
parsed Treebank. Unfortunately, all the available 
fully parsed Treebank, such as the Penn Treebank 
and the Sinica Treebank, are annotated using 
different grammars than our chosen Phrase-based 
Grammar. Also, the sizes of these Treebank are 
much smaller in scale to be useful for training our 
shallow parser. 
This paper presents the most important design 
issues of the PolyU Treebank and the quality control 
mechanisms. The rest of this paper is organized as 
follows. Section 2 introduces the overview and 
design principles.  Section 3 to Section5, present 
the design issues on corpus material preparation, the 
standard for word segmentation and POS tagging, 
and the guideline for phrase bracketing and labeling, 
respectively. Section 6 discusses the quality 
assurance mechanisms including a carefully 
designed workflow, parallel annotation, and 
automatic and semi-automatic post-annotation 
checking. Section 7 gives the current progress and 
future work. 
2 Overview and Design Principles 
The objective of this project is to manually 
construct a large shallow Treebank with high 
accuracy and consistency.  
The design principles of The PolyU Treebank are: 
high resource sharing ability, low structural 
complexity, sufficient syntactic information and 
large data scale. First of all, the design and 
construction of The PolyU Treebank aims to provide 
as much a general purpose Treebank as possible so 
that different applications can make use of it as a 
NLP resource. With this objective, we chose to 
follow the well-known Phrase-based Grammar as 
the framework for annotation as this grammar is 
widely accepted by Chinese language researchers, 
and thus our work can be easily understood and 
accepted.  
Due to the lack of word delimitation in Chinese, 
word segmentation must be performed before any 
further syntactical annotation. High accuracy of 
word segmentation is very important for this project. 
In this project, we chose to use the segmented and 
tagged corpus of People Daily annotated by the 
Peking University. The annotated corpus contains 
articles appeared in the People Daily Newspaper in 
1998. The segmentation is based on the guidelines, 
given in the Chinese national standard GB13715, 
(Liu et al 1993) and the POS tagging specification 
was developed according to the ?Grammatical 
Knowledge-base of contemporary Chinese?. 
According to the report from Peking University, the 
accuracy of this annotated corpus in terms of 
segmentation and POS tagging are 99.9% and 99.5%, 
respectively (Yu et al 2001). The use of such mature 
and widely adopted resource can effectively reduce 
our cost, ensure syntactical annotation quality. With 
consistency in segmentation, POS, and syntactic 
annotation, the resulting Treebank can be readily 
shared by other researchers as a public resource. 
The second design principle is low structural 
complexity. That means, the annotation framework 
should be clear and simple, and the labeled syntactic 
and functional information should be commonly 
used and accepted. Considering the characteristics of 
shallow annotation, our project has focused on the 
annotation of phrases and headwords while the 
sentence level syntax are ignored.  
Following the framework of Phrase-based 
Grammar, a base-phrase is regarded as the smallest 
unit where a base-phrase is defined as a ?stable? and 
?simple? phrase without nesting components. Study 
on Chinese syntactical analysis suggests that phrases 
should be the fundamental unit instead of words in a 
sentence. This is because, firstly, the usage of 
Chinese words is very flexible. A word may have 
different POS tags serving for different functions in 
sentences. On the contrary, the use of Chinese 
phrases is much more stable. That is, a phrase has 
very limited functional use in a sentence. Secondly, 
the construction rules of Chinese phrases are nearly 
 the same as that of Chinese sentences. Therefore, the 
analysis of phrases can help identifying POS and 
grammatical functions of words. Naturally, it should 
be regarded as the basic syntactical unit. Usually, a 
base-phrase is driven by a lexical word as its 
headword. Examples of base-phrases include base 
NP, base VP and so on, such as the sample shown 
below. 
  
Using base-phrases as the start point, nested levels 
of phrases are then identified, until the maximum 
phrases (will be defined later) are identified. Since 
we do not intend to provide full parsing information, 
there has to be a limit on the level of nesting. For 
practical reasons, we choose to limit the nesting of 
brackets to 3 levels. That means, the depth of our 
shallow parsed Treebank will be limited to 3. This 
restriction can limit the structural complexity to a 
manageable level.  
Our nested bracketing is not strictly bottom up. 
That is we do not simply extend from base-phrase 
and move up until the 3rd level. Instead, we first 
identify the maximal-phrase which is used to 
identify the backbone of the sentence. The 
maximal-phrase provides the framework under 
which the base-phrases of up to 2 levels can be 
identified. The principles for the identification of 
scope and depth of phrase bracketing are briefly 
explained below and the operating procedure is 
indicated by the given order in which these 
principles are presented. More details is given in 
Section 5. 
Step 1: Annotation of maximal-phrase which is 
the shortest word sequence of maximally 
spanning non-overlapping edges which plays a 
distinct semantic role of a predicate. A 
maximal-phrase contains two or more lexical 
words. 
Step 2: Annotation of base-phrases within a 
maximal-phrase. In case a base-phrase and a 
maximal-phrase are identical and the 
maximal-phrase is already bracketed in Step 1, no 
bracketing is done in this step. For each identified 
base-phrase, its headword will be marked. 
Step 3: Annotation of next level of bracketing, 
called mid-phrase which is expended from a 
base-phrase. A mid-phrase is annotated only if it is 
deemed necessary. The process starts from the 
identified base-phrase. One more level of 
syntactical structure is then bracketed if it exists 
within the maximal-phrase.   
  
The third design principle is to provide sufficient 
syntactical information for natural language 
application even though shallow annotation does not 
necessarily contain complete syntactic information 
at sentence level. Some past research in Chinese 
shallow parsing were on single level base-phrases 
only (Sun 2001). However, for certain applications, 
such as for collocation extraction, identification of 
base-phrases only are not very useful. In this project, 
we have decided to annotate phrases within three 
levels of nesting within a sentence. For each phrase, 
a label is be given to indicate its syntactical 
information, and an optional semantic or structural 
label is given if applicable. Furthermore, the 
headword of a base-phrase is annotated. We believe 
these information are sufficient for many natural 
language processing research work and it is also 
manageable for this project within its working 
schedule. 
Fourthly, aiming to support practical language 
processing, a reasonably large annotated Treebank is 
expected. Studies on English have shown that 
Treebank of word size 500K to 1M is reasonable for 
syntactical structure analysis (Leech and Garside 
1996). In consideration of the resources available 
and the reference of studies on English, we have set 
out our Treebank size to be one million words. We 
hope such a reasonably large-scale data can 
effectively support some language research, such as  
collocation extraction.  
We chose to use the XML format to record the 
annotated data. Other information such as original 
article related information (author, date, etc.), 
annotator name, and other useful information are 
also given through the meta-tags provided by XML. 
All the meta-tags can be removed by a program to 
recover the original data. 
We have performed a small-scale experiment to 
compare the annotation cost of shallow annotation 
and full annotation (followed Penn Chinese 
Treebank specification) on 500 Chinese sentences 
by the same annotators. The time cost in shallow 
annotation is only 25% of that for full annotation. 
Meanwhile, due to the reduced structural complexity 
in shallow annotation, the accuracy of first pass 
shallow annotation is much higher than full 
annotation. 
3 Corpus Materials Preparation 
The People Daily corpus, developed by PKU, 
consists of more than 13k articles totaling 5M words. 
As we need one million words for our Treebank, we 
have selected articles covering different areas in 
different time span to avoid duplications due to 
short-lived events and news topics. Our selection 
takes each day?s news as one single unit, and then  
several distant dates are randomly selected among 
the whole 182 days in the entire collection.  We 
have also decided to keep the original articles? 
structures and topics indicators as they may be 
useful for some applications. 
 4 Word Segmentation and Part-of-Speech 
Tagging 
The articles selected from PKU corpus are already 
segmented into words following the guidelines 
given in GB13715. The annotated corpus has a basic 
lexicon of over 60,000 words. We simply use this 
segmentation without any change and the accuracy 
is claimed to be 99.9%.  
Each word in the PKU corpus is given a POS tag.  
In this tagging scheme, a total of 43 POS tags are 
listed (Yu et al 2001).  Our project takes the PKU 
POS tags with only notational changes explained as 
follows: 
The morphemes tags including Ag (Adjectives 
morphemes), Bg, Dg, Ng, Mg, Rg, Tg, Qg, and Ug 
are re-labeled as lowercase letters, ag, bg, dg, ng, mg, 
rg, tg, qg and ug, respectively. This modification is 
to ensure consistent labeling in our system where the 
lower cases are used to indicate word-level tags and 
upper cases are used to indicate phrase-level labels. 
5 Phrase Bracketing and Annotation 
Phrase bracketing and annotation is the core part 
of this project. Not only all the original annotated 
files are converted to XML files, results of our 
annotations are also given in XML form. The meta 
tags provided by XML are very helpful for further 
processing and searching to the annotated text. . 
Note that in our project, the basic phrasal analysis 
looks at the context of a clause, not a sentence. Here, 
the term clause refers the text string ended by some 
punctuations including comma (,), semicolon (;), 
colon (:), or period (.). Certain punctuation marks 
such as ? ?, ?<?, and ?>? are not considered clause 
separators. For example,  
  
is considered having two clauses and thus will be 
bracketed separately. It should be pointed out that he 
set of Chinese punctuation marks are different from 
that of English and their usage can also be different. 
Therefore, an English sentence and their Chinese 
translation may use different punctuation marks.  
For example, the sentence 
 
is the translation of the English ?Tom, John, and 
Jack go back to school together? , which uses ? ? 
rather than comma(,) to indicate parallel structures, 
and is thus considered one clause.   
Each clause will then be processed according to 
the principles discussed in Section 2. The symbols 
?[? and ?]? are used to indicate the left and right 
boundaries of a phrase. The right bracket is 
appended with syntactic labels as described in the 
general form of [Phrase]SS-FF, where SS is a 
mandatory syntactic label such as NP(noun phrase) 
and AP(adjective phrase), and FF is an optional label 
indicating internal structures and semantic functions 
such as BL(parallel), SB(a noun is the object of verb 
within a verb phrase). A total of  21 SS labels and 
20 FF labels are given in our phrase annotation 
specification. For example, the functional label BL 
identifies parallel components in a phrase as 
indicated in the example .  
As in another example shown below,  
 
the phrase  is a verb phrase, thus it is 
labeled as VP. Furthermore, the verb phrase can be 
further classified as a verb-complement type. Thus 
an additional SBU function label is marked. We 
should point out that since the FF labels are not 
syntactical information and are thus not expected to 
be used by any shallow parsers. The FF labels carry 
structural and/or semantic information which are of 
help in annotation. We consider it useful for other 
applications and thus decide to keep them in the 
Treebank. Appendix 1 lists all the FF labels used in 
the annotation. 
 
5.1  Identification of Maximal-phrase:  
The maximal-phrases are the main syntactical 
structures including subject, predicate, and objects in 
a clause. Again, maximal-phrase is defined as the 
phrase with the maximum spanning non-overlapping 
length, and it is a predicate playing a distinct 
semantic role and containing more than one lexical 
word. That means a maximal-phrase contains at least 
one base-phrase. As this is the first stage in the 
bracketing process, no nesting should occur. In the 
following annotated sentence, 
 (Eg.1) 
there are two separate maximal-phrases, 
, and 
. Note 
that  is considered a base-phrase, but not a 
maximal-phrase because it contains only one lexical 
word. Unlike many annotations where the object of 
a sentence is included as a part of the verb phrase, 
we treat them as separate maximal-phrases both due 
to our requirement and also for reducing nesting. 
If a clause is completely embedded in a larger 
clause, it is considered a special clause and given a 
special name called an internal clause .  We will 
bracket such an internal clause as a maximal phrase 
with the tag ?IC? as shown in the following example, 
 
 
5.2  Annotation of Base-phrases:  
A base-phrase is the phrase with stable, close and 
simple structure without nesting components. 
Normally a base-phrase contains a lexical word as 
 headword. Taking the  maximal-phrase 
in 
Eg.1 as an example,  and 
, are base-phrases in this 
maximal-phrase. Thus, the sentence is annotated as 
 
  
In fact, and are also 
base-phrases.  is not bracketed because it is a 
single lexical word as a base-phrase without any 
ambiguity and it is thus by default not being 
bracketed. is not further 
bracketed because it overlaps with a maximal-phrase. 
Our annotation principle here is that if a base-phrase 
overlaps with a maximal-phrase, it will not be 
bracketed twice.  
The identification of base-phrase is done only 
within an already identified maximal-phrase. In 
other words, if a base-phrase is identified, it must be 
nested inside a maximal-phrase or at most overlaps 
with it. It should be pointed out that the 
identification of a base-phrase is the most 
fundamental and most important goal of Treebank 
annotation. The identification of maximal-phrases 
can be considered as parsing a clause using a 
top-down approach. On the other hand, the 
identification of a base-phrase is a bottom up 
approach to find the most basic units within a 
maximal-phrase.  
 
5.3  Mid-Phrase Identification:  
Due to the fact that sometimes there may be more 
syntactic structures between the base-phrases and 
maximal-phrases, this step uses base-phrase as the 
starting point to further identify one more level of 
the syntactical structure in a maximal-phrase. Takes 
Eg.1 as an example, it is further annotated as 
 
where the underlined text shows the additional 
annotation. 
As we only limit our nesting to three levels, any 
further nested phrases will be ignored. The 
following sentence shows the result of our 
annotation with three levels of nesting:  
  
However, a full annotation should have 4 levels of 
nesting as shown below. The underlined text is the 
4th level annotation skipped by our system. 
 
 
5.4  Annotation of Headword 
In our system, a ?#? tag will be appended after a 
word to indicate that it is a headword of the 
base-phrase. Here, a headword must be a lexical 
word rather than a function word.  
In most cases, a headword stays in a fixed position 
of a base-phrase. For example, the headword of a 
noun phrase is normally the last noun in this phrase. 
Thus, we call this position the default position. If a 
headword is in the default position, annotation is not 
needed. Otherwise, a ?#? tag is used to indicate the 
headword. 
For example, in a clause, 
,  
 is a verb phrase, and the headword 
of the phrase is , which is not in the default 
position of a verb phrase. Thus, this phrase is further 
annotated as:  
  
Note that  is also a headword, but since it 
is in the default position, no explicit annotation is 
needed. 
6 Annotation and Quality Assurance 
Our research team is formed by four people at the 
Hong Kong Polytechnic University, two linguists 
from Beijing Language and Culture University and 
some research collaborators from Peking University. 
Furthermore, the annotation work has been 
conducted by four post-graduate students in 
language studies and computational linguistics from 
the Beijing Language and Culture University.  
The annotation work is conducted in 5 separate 
stages to ensure quality output of the annotation 
work. The preparation of annotation specification 
and corpus selection was done in the first stage. 
Researchers in Hong Kong invited two linguists 
from China to come to Hong Kong to prepare for the 
corpus collection and selection work. A thorough 
study on the reported work in this area was 
conducted. After the project scope was defined, the 
SS labels and the FF labels were then defined. A 
Treebank specification was then documented.  The 
Treebank was given the name PolyU Treebank to 
indicate that it is produced at the Hong Kong 
Polytechnic University. In order to validate the 
specifications drafted, all the six members first 
manually annotated 10k-word material, separately. 
The outputs were then compared, and the problems 
and ambiguities occurred were discussed and 
consolidated and named Version 1.0. Stage 1 took 
about 5 months to complete. Details of the 
specification can be downloaded from the project 
website www.comp.polyu.edu.hk/~cclab. 
In Stage 2, the annotators in Beijing were then 
involved. They had to first study the specification 
and understand the requirement of the annotation. 
Then, the annotators under the supervision of a team 
member in Stage 1 annotated 20k-word materials 
together and discussed the problems occurred. 
 During this two-month work, the annotators were 
trained to understand the specification. The 
emphasis at this stage was to train the annotators? 
good understanding of the specification as well as 
consistency by each annotator and consistency by 
different annotators. Further problems occurred in 
the actual annotation practice were then solved and 
the specification was also further refined or 
modified.  
In Stage 3, which took about 2 months, each 
annotator was  assigned 40k-word material each in 
which 5k-words material were duplicate annotated 
to all the annotators. Meanwhile, the team members 
in Hong Kong also developed a post-annotation 
checking tool to verify the annotation format, phrase 
bracketing, annotation tags, and phrase marks to 
remove ambiguities and mistakes. Furthermore, an 
evaluation tool was built to check the consistency of 
annotation output. The detected annotation errors 
were then sent back to the annotators for discussion 
and correction. Any further problems occurred were 
submitted for group discussion and minor 
modification on the specification was also done. 
In stage 4, each annotator was dispatched with one 
set of 50k-word material each time. For each 
distribution, 15k-word data in each set were 
distributed to more than two annotators in duplicates 
so that for any three annotators, there would be 5K 
duplicated materials. When the annotators finished 
the first pass annotation, we used the post-annotation 
checking tool to do format checking in order to 
remove the obvious annotation errors such as wrong 
tag annotation and cross bracketing. However, it was 
quite difficult to check the difference in annotation 
due to different interpretation of a sentence. What 
we did was to make use of the annotations done on 
the duplicate materials to compare for consistency. 
When ambiguity or differences were identified, 
discussions were conducted and a result used by the 
majority would be chosen as the accepted result. The 
re-annotated results were regarded as the Golden 
Standard to evaluate the accuracy of annotation and 
consistency between different annotators. The 
annotators were required to study this Golden 
Standard and go back to remove  similar mistakes. 
The annotated 50k data was accepted only after this. 
Then, a new 50k-word materials was distributed and 
repeated in the same way. During this stage, the 
ambiguous and out-of-tag-set phrase structures were 
marked as OT for further process. The annotation 
specification was not modified in order to avoid 
frequent revisit to already annotated data. About 4 
months were spent on this stage. 
In Stage 5, all the members and annotators were 
grouped and discuss the OT cases. Some typical new 
phrase structure and function types were appended 
in the specification and thus the final formal 
annotation specification was established. Using this 
final specification, the annotators had to go back to 
check their output, modify the mistakes and 
substitute the OT tags by the agreed tags. Currently, 
the project was already in Stage 5 with 2 months of 
work finished. A further 2 months was expected to 
complete this work. 
Since it is impossible to do all the checking and 
analysis manually, a series of checking and 
evaluating tools are established. One of the tools is 
to check the consistency between text corpus files 
and annotated XML files including checking the 
XML format, the filled XML header, and whether 
the original txt material is being altered by accident. 
This program ensures that the XML header 
information is correctly filled and during annotation 
process, no additional mistakes are introduced due to 
typing errors.  
Furthermore, we have developed and trained a 
shallow parser using the Golden Standard data. This 
shallow parser is performed on the original text data, 
and its output and manually annotated result are 
compared for verification to further remove errors 
Now, we are in the process of developing an 
effective analyzer to evaluate the accuracy and 
consistency for the whole annotated corpus. For the 
exactly matched bracketed phrases, we check 
whether the same phrase labels are given. Abnormal 
cases will be manually checked and confirmed. Our 
final goal is to ensure the bracketing can reach 99% 
accuracy and consistency. 
7 Current Progress and Future Work 
As mentioned earlier, we are now in Stage 5 of the 
annotation. The resulting annotation contains 2,639 
articles selected from PKU People Daily corpus. 
These articles contains 1, 035, 058 segmented 
Chinese words, with on average, around 394 words 
in each article. There are a total of 284, 665 
bracketed phrases including nested phrases. A 
summary of the different SS labels used are given in 
Table 1. 
 
Table 1. Statistics of annotated syntactical phrases 
 
For each bracketed phrase, if its FF label does not 
fit into the corresponding default pattern, (like for 
the noun phrase(NP), the default grammatical 
structure is that the last noun in the phrase is the 
headword and other components are the modifiers, 
using PZ tags), its FF labels should then be 
explicitly labeled. The statistics of annotated FF tags 
 are listed in Table 2.  
 
Table 2. Statistics of function and structure tags 
 
For the material annotated by multiple annotators 
as duplicates, the evaluation program has reported 
that the accuracy of phrase annotation is higher than 
99.5% and the consistency between different 
annotators is higher than 99.8%. As for other 
annotated materials, the quality evaluation program 
preliminarily reports the accuracy of phrase 
annotation is higher than 98%. Further checking and 
evaluation work are ongoing to ensure the final 
overall accuracy achieves 99%. 
Up to now, the FF labels of 5,255 phrases are 
annotated as OT. That means about 1.8% (5,255 out 
of a total of 284,665) of them do not fit into any 
patterns listed in Table 2. Most of them are proper 
noun phrase, syntactically labeled as PP. We are 
investigating these cases and trying to identify 
whether some of them can be in new function and 
structure patterns and give a new label. 
It is also our intention to further develop our tools 
to improve the automatic annotation analysis and 
evaluation program to find out the potential 
annotation error and inconsistency. Other 
visualization tools are also being developed to 
support keyword searching, context indexing, and 
annotation case searching. Once we complete Stage 
5, we intend to make the PolyU Treebank data 
available for public access.  Furthermore, we are 
developing a shallow parser and using The PolyU 
Treebank as training and testing data. 
 
Acknowledgement 
 
This project is partially supported by the Hong Kong 
Polytechnic University (Project Code A-P203) and 
CERG Grant (Project code 5087/01E) 
References  
Baoli Li, Qin Lu and Yin Li. 2003. Building a 
Chinese Shallow Parsed Treebank for Collocation 
Extraction, Proceedings of CICLing 2003: 
402-405 
Fei Xia, et al 2000. Developing Guidelines and 
Ensuring Consistency for Chinese Text Annotation 
Proceedings of LREC-2000, Greece 
Feng-yi Chen, et al 1999. Sinica Treebank, 
Computational Linguistics and Chinese Language 
Processing, 4(2):183-204 
G. N. Leech, R.Garside. 1996. Running a grammar 
factory: the production of syntactically analyzed 
corpora or ?treebanks?, Johansson and Stenstron. 
Honglin Sun, 2001. A Content Chunk Parser for 
Unrestricted Chinese Text, Ph.D Thesis, Peking 
University, 2001 
Keh-jiann Chen et al 2003. Building and Using 
Parsed Corpora (Anne Abeill? ed. s) KLUWER, 
Dordrecht 
Kenneth Church, and Patrick Hanks. 1990. Word 
association norms, mutual information, and 
lexicography, Computational Linguistics, 16(1): 
22-29 
Marcus, M. et al 1993. Building a Large Annotated 
Corpus of English: The Penn Treebank, 
Computational Linguistics, 19(1): 313-330. 
Nianwen Xue, et al 2002. Building a Large-Scale 
Annotated Chinese Corpus, Proceedings of 
COLING 2002, Taipei, Taiwan 
Sean Wallis, 2003. Building and Using Parsed 
Corpora (Anne Abeill? eds) KLUWER, Dordrecht 
Shiwen Yu, et al 1998. The Grammatical 
Knowledge- base of contemporary Chinese: a 
complete specification. Tsinghua University Press, 
Beijing, China 
Shiwen Yu, et al 2001. Guideline of People Daily 
Corpus Annotation, Technical report, Beijing 
University 
Shoukang Zhang and Xingguang Lin, 1992. 
Collocation Dictionary of Modern Chinese 
Lexical Words, Business Publisher, China 
Yuan Liu, et al 1993. Segmentation standard for 
Modern Chinese Information Processing and 
automatic segmentation methodology. Tsinghua 
University Press, Beijing, China 
  
Appendix 1 The structural and semantic FF labels  
 
 
Appendix 2 Example of an Annotated Article  
 
 
 
Proceedings of the Linguistic Annotation Workshop, pages 61?68,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Chinese Collocations with Multi Information 
 
Ruifeng Xu1,     Qin Lu1,     Kam-Fai Wong2,    Wenjie Li1 
? 
?1 Department of Computing,                       2 Department of Systems Engineering and 
?                                                  Engineering Management 
 The Hong Kong Polytechnic University,        The Chinese University of Hong Kong,   
?Kowloon, Hong Kong                                      N.T., Hong Kong 
{csrfxu,csluqin,cswjli}@comp.polyu.edu.hk  kfwong@se.cuhk.edu.hk 
? 
 
Abstract 
This paper presents the design and construc-
tion of an annotated Chinese collocation bank 
as the resource to support systematic research 
on Chinese collocations. With the help of 
computational tools, the bi-gram and n-gram 
collocations corresponding to 3,643 head-
words are manually identified. Furthermore, 
annotations for bi-gram collocations include 
dependency relation, chunking relation and 
classification of collocation types. Currently, 
the collocation bank annotated 23,581 bi-
gram collocations and 2,752 n-gram colloca-
tions extracted from  a 5-million-word corpus. 
Through statistical analysis on the collocation 
bank, some characteristics of Chinese bi-
gram collocations are examined which is es-
sential to collocation research, especially for 
Chinese. 
1 Introduction 
Collocation is a lexical phenomenon in which two 
or more words are habitually combined and com-
monly used in a language to express certain seman-
tic meaning. For example, in Chinese, people will 
say ??-?? (historical baggage) rather than ?
? -?? (historical luggage) even though ??
(baggage) and ?? (luggage) are synonymous. 
However, no one can argue why ?? must collo-
cate with??. Briefly speaking, collocations are 
frequently used word combinations. The collocated 
words always have syntactic or semantic relations 
but they cannot be generated directly by syntactic 
or semantic rules. Collocation can bring out differ-
ent meanings a word can carry and it plays an in-
dispensable role in expressing the most appropriate 
meaning in a given context. Consequently, colloca-
tion knowledge is widely employed in natural lan-
guage processing tasks such as word sense disam-
biguation, machine translation, information re-
trieval and natural language generation (Manning 
et al 1999).  
Although the importance of collocation is well 
known, it is difficult to compile a complete collo-
cation dictionary. There are some existing corpus 
linguistic researches on automatic extraction of 
collocations from electronic text (Smadja 1993; 
Lin 1998; Xu and Lu 2006). These techniques are 
mainly based on statistical techniques and syntactic 
analysis. However, the performances of automatic 
collocation extraction systems are not satisfactory 
(Pecina 2005). A problem is that collocations are 
word combinations that co-occur within a short 
context, but not all such co-occurrences are true 
collocations. Further examinations is needed to 
filter out pseudo-collocations once co-occurred 
word pairs are identified.  A collocation bank with 
true collocations annotated is naturally an indis-
pensable resource for collocation research. (Kosho 
et al 2000) presented their works of collocation 
annotation on Japanese text. Also, the Turkish 
treebank, (Bedin 2003) included collocation anno-
tation as one step in its annotation. These two col-
location banks provided collocation identification 
and co-occurrence verification information. (Tutin 
2005) used shallow analysis based on finite state 
transducers and lexicon-grammar to identify and 
annotate collocations in a French corpus. This col-
location bank further provided the lexical functions 
of the collocations. However to this day, there is 
no reported Chinese collocation bank available. 
61
In this paper, we present the design and con-
struction of a Chinese collocation bank (acrony-
med CCB). This is the first attempt to build a 
large-scale Chinese collocation bank as a Chinese 
NLP resource with multiple linguistic information 
for each collocation including:  (1) annotating the 
collocated words for each given headword; (2) dis-
tinguishing n-gram and bi-gram collocations for 
the headword; (3) for bi-gram collocations, CCB 
provides their syntactic dependencies, chunking 
relation and classification of collocation types 
which is proposed by (Xu and Lu 2006). In addi-
tion, we introduce the quality assurance mecha-
nism used for CCB. CCB currently contains for 
3,643 common headwords taken from ?The Dic-
tionary of Modern Chinese Collocations? (Mei 
1999) with 23,581 unique bi-gram collocations and 
2,752 unique n-gram collocations extracted from a 
five-million-word segmented and chunked Chinese 
corpus (Xu and Lu, 2005). 
The rest of this paper is organized as follows. 
Section 2 presents some basic concepts. Section 3 
describes the annotation guideline. Section 4 de-
scribes the practical issues in the annotation proc-
ess including corpus preparation, headword prepa-
ration, annotation flow, and the quality assurance 
mechanism. Section 5 gives current status of CCB 
and characteristics analysis of the annotated collo-
cations. Section 6 concludes this paper. 
2 Basic Concepts 
Although collocations are habitual expressions in 
natural language use and they can be easily under-
stood by people, a precise definition of collocation 
is still far-reaching (Manning et al 1999). In this 
study, we define a collocation as a recurrent and 
conventional expression of two or more content 
words that holds syntactic and semantic relation. 
Content words in Chinese include noun, verb, ad-
jective, adverb, determiner, directional word, and 
gerund. Collocations with only two words are 
called bi-gram collocations and others are called n-
gram collocations. 
From a linguistic view point, collocations have a 
number of characteristics. Firstly, collocations are 
recurrent as they are of habitual use. Collocations 
occur frequently in similar contexts and they ap-
pear in certain fixed patterns. However, they can-
not be described by the same set of syntactic or 
semantic rules. Secondly, free word combinations 
which can be generated by linguistic rules are 
normally considered compositional.  In contrast, 
collocations should be limited compositional 
(Manning et al 1999) and they usually carry addi-
tional meanings when used as a collocation. 
Thirdly, collocations are also limited substitutable 
and limited modifiable. Limited substitutable here 
means that a word cannot be freely substituted by 
other words with similar linguistic functions in the 
same context such as synonyms. Also, many collo-
cations cannot be modified freely by adding modi-
fiers or through grammatical transformations. 
Lastly, collocations are domain-dependent (Smadja 
1993) and language-dependent.  
3 Annotation Guideline Design 
The guideline firstly determines the annotation 
strategy. 
(1) The annotation of CCB follows the head-
word-driven strategy. The annotation uses selected 
headwords as the starting point. In each circle, the 
collocations corresponding to one headword are 
annotated. Headword-driven strategy makes a 
more efficient annotation as it is helpful to estimate 
and compare the relevant collocations. 
(2) CCB is manually annotated with the help of 
automatic estimation of computational features, i.e. 
semi-automatic software tools are used to generate 
parsing and chunking candidates and to estimate 
the classification features. These data are present to 
the annotators for determination. The use of assis-
tive tools is helpful to produce accurate annota-
tions with efficiency. 
The guideline also specifies the information to 
be annotated and the labels used in the annotation. 
For a given headword, CCB annotates both bi-
gram collocations and n-gram collocations. Con-
sidering the fact that n-gram collocations consist-
ing of continuous significant bi-grams as a whole 
and, the n-gram annotation is based on the identifi-
cation and verification of bi-gram word combina-
tions and is prior to the annotation of bi-gram col-
locations.  
For bi-gram annotation, which is the major in-
terest  in collocation research, three kinds of in-
formation are annotated. The first one is the syn-
tactic dependency of the headword and its co-word 
in a bi-gram collocation . A syntactic dependency 
normally consists of one word as the governor (or 
head), a dependency type and another word serves 
62
as dependent (or modifier) (Lin 1998).Totally, 10 
types of dependencies are annotated in CCB. They 
are listed in Table 1 below. 
 
 Dependency Description Example 
ADA Adjective and its adverbial modifier ??/d ??/a  greatly painful 
ADV Predicate and its adverbial modifier in 
which the predicate serves as head 
??/ad ??/v heavily strike 
AN Noun and its adjective modifier ??/a ??/n lawful incoming 
CMP Predicate and its complement in which 
the predicate serves as head 
??/v??/v ineffectively treat
NJX Juxtaposition structure ??/a??/a fair and reasonable
NN Noun and its nominal modifier ??/n ??/n personal safety 
SBV Predicate and its subject ??/n ??/v property transfer 
VO Predicate and its object in which the 
predicate serves as head 
??/v ??/n change mechanism
VV Serial verb constructions which indi-
cates that there are serial actions  
??/v ??/v trace and report 
OT Others   
Table 1. The dependency categories 
The second one is the syntactic chunking informa-
tion (a chunk is defined as a minimum non-nesting 
or non-overlapping phrase) (Xu and Lu, 2005). 
Chunking information identifies all the words for a 
collocation within the context of an enclosed 
chunk. Thus, it is a way to identify its proper con-
text at the most immediate syntactic structure. 11 
types of syntactic chunking categories given in (Xu 
and 2006) are used as listed in Table 2.  
 
 Description Examples 
BNP Base noun phrase [??/n ??/n]NP     market economy 
BAP Base adjective phrase  [??/a??/a]BAP   fair and reasonable
BVP Base verb phrase [??/a??/v]BVP   successfully start 
BDP Base adverb phrase [?/d ??/d]BDP      no longer 
BQP Base quantifier phrase [?? /m ? /q]BQP ?? /n several thou-
sand soldiers 
BTP Base time phrase [??/t ??/t]BTP 8:00 in the morning 
BFP Base position phrase [??/ns ???/f]BFP Northeast of Mon-
golia 
BNT Name of an organization [??/ns ??/n]BNT Yantai University 
BNS Name of a place [??/ns ??/ns]BNS Tongshan, Jiangsu 
Province 
BNZ Other proper noun phrase [???/nr?/n]BNZ The Nobel Prize 
BSV S-V structure [??/n ??/a]BSV  territorial integrity 
Table 2. The chunking categories 
The third one is the classification of collocation 
types. Collocations cover a wide spectrum of ha-
bitual word combinations ranging from idioms to 
free word combinations. Some collocations are 
very rigid and some are more flexible. (Xu and Lu 
2006) proposed a scheme to classify collocations 
into four types according to the internal association 
of collocations including compositionality, non-
substitutability, non-modifiability, and statistical 
significance. They are,  
Type 0: Idiomatic Collocation 
Type 0 collocations are fully non-compositional 
as its meaning cannot be predicted from the mean-
ings of its components such as???? (climbing 
a tree to catch a fish, which is a metaphor for a 
fruitless endeavour). Some terminologies are also 
Type 0 collocations such as ?  ?(Blue-tooth ) 
which refers to a wireless communication protocol. 
Type 0 collocations must have fixed forms. Their 
components are non-substitutable and non-
modifiable allowing no syntactic transformation 
and no internal lexical variation. This type of col-
locations has very strong internal associations and 
co-occurrence statistics is not important.  
Type 1: Fixed Collocation 
Type 1 collocations are very limited composi-
tional with fixed forms which are non-substitutable 
and non-modifiable. However, this type can be 
compositional. None of the words in a Type 1 col-
location can be substituted by any other words to 
retain the same meaning such as in??/n ???
/n (diplomatic immunity). Finally, Type 1 colloca-
tions normally have strong co-occurrence statistics 
to support them. 
Type 2: Strong Collocation 
Type 2 collocations are limitedly compositional. 
They allow very limited substitutability. In other 
words, their components can only be substituted by 
few synonyms and the newly generated word com-
binations have similar meaning, e.g., ??/v ??
/n (alliance formation) and ??/v ??/n (alliance 
formation). Furthermore, Type 2 collocations al-
low limited modifier insertion and the order of 
components must be maintained. Type2 colloca-
tions normally have strong statistical support.  
Type 3: Loose Collocation 
Type 3 collocations have loose restrictions. 
They are nearly compositional. Their components 
may be substituted by some of their synonyms and 
the newly generated word combinations usually 
have very similar meanings. Type 3 collocations 
are modifiable meaning that they allow modifier 
insertions. Type 3 collocations have weak internal 
associations and they must have statistically sig-
nificant co-occurrence.  
The classification represents the strength of in-
ternal associations of collocated words. The anno-
tation of these three kinds of information is essen-
tial to all-rounded characteristic analysis of collo-
cations. 
63
4 Annotation of CCB 
4.1 Data Preparation 
CCB is based on the PolyU chunk bank (Xu and 
Lu, 2005) which contains chunking information on 
the People?s Daily corpus with both segmentation 
and part-of-speech tags. The accuracies of word 
segmentation and POS tagging are claimed to be 
higher than 99.9% and 99.5%, respectively (Yu et 
al. 2001). The use of this popular and accurate raw 
resource helped to reduce the cost of annotation 
significantly, and ensured maximal sharing of our 
output.  
The set of 3, 643 headwords are selected from 
?The Dictionary of Modern Chinese Collocation? 
(Mei 1999) among about 6,000 headwords in the 
dictionary. The selection  was based both on the 
judgment by linguistic experts as well as the statis-
tical information that they are commonly used. 
4.2 Corpus Preprocessing 
The CCB annotations are represented in XML. 
Since collocations are practical word combinations 
and word is the basic unit in collocation research, a 
preprocessing module is devised to transfer the 
chunked sentences in the PolyU chunk bank to 
word sequences with the appropriate labels to indi-
cate the corresponding chunking information. This 
preprocessing module indexes the words and 
chunks in the sentences and encodes the chunking 
information of each word in two steps. Consider 
the following sample sentence extracted from the 
PolyU chunk bank: 
??/v[??/n??/n]BNP?/u[??/n??/n??
/an ]BNP 
(ensure life and property safety of the people) 
The first step in preprocessing is to index each 
word and the chunk in the sentence by giving in-
cremental word ids and chunk ids from left to right. 
That is,, 
[W1]??/v [W2]??/n [W3]??/n [W4]?/u  
[W5]??/n [W6]??/n [W7]??/an [C1]BNP [C2]BNP 
where, [W1] to [W7] are the words and [C1] to [C2] 
are chunks although chunking positions are not 
included in this step. One Chinese word may occur 
in a sentence for more than one times, the unique 
word ids are helpful to avoid ambiguities in the 
collocation annotation on these words. 
The second step is to represent the chunking in-
formation of each word. Chunking boundary in-
formation is labeled by following initial/final rep-
resentation scheme. Four labels, O/B/I/E, are used 
to mark the isolated words outsides any chunks, 
chunk-initial words, words in the middle of chunks, 
and chunk-final words, respectively. Finally, a la-
bel H is used to mark the identified head of chunks 
and N to mark the non-head words. 
The above sample sentence is then transferred to 
a sequence of words with labels as shown below, 
<labeled> [W1][O_O_N][O]??/v [W2][B_BNP_N][C1]
??/n [W3][E_BNP_H][C1]??/n [W4][O_O_N][O]?/u 
[W5][B_BNP_N][C2]??/n [W6][I_BNP_N][C2]??/n 
[W7][E_BNP_N][C2]??/an </labeled> 
For each word, the first label is the word ID. The 
second one is a hybrid tag for describing its chunk-
ing status. The hybrid tags are ordinal with respect 
to the chunking status of boundary, syntactic cate-
gory and head, For example, B_BNP_N indicates 
that current word is the beginning or a BNP and 
this word is not the head of this chunk. The third 
one is the chunk ID if applicable. For the word out 
of any chunks, a fixed chunk ID O is given. 
4.3 Collocation Annotation 
Collocation annotation is conducted on one head-
word at a time. For a given headword, an annota-
tors examines its context to determine if its co-
occurred word(s) forms a collocation with it and if 
so, also annotate the collocation?s dependency, 
chunking and classification information. The anno-
tation procedure, requires three passes. We use a 
headword ??/an (safe), as an illustrative exam-
ple.  
Pass 1. Concordance and dependency identifica-
tion 
In the first pass, the concordance of the given 
headword is performed. Sentences containing the 
headwords are obtained, e.g.  
S1: ??/v [??/v  ??/an]BVP  ?/u  ??/n 
(follow the principles for ensuring the safety) 
S2: ??/v [??/n ??/n]BNP ?/u[??/n ??/n ?
?/an]BNP 
(ensure life and property safety of people) 
S3: ??/v  ??/ns  [??/an  ??/v]BVP 
(ensure the flood pass through Yangzi River safely) 
With the help of an automatic dependency pars-
er, the annotator determines all syntactically and 
semantically dependent words in the chunking con-
text of the observing headword. The annotation 
output of S1 is given below in which XML tags are 
used for the dependency annotation.  
S1:<sentence>??/v [??/v  ??/an]BVP  ?/u  ??/n 
64
<labeled> [W1][O_O_N][O]??/v [W2][B_BVP_H][C1]
??/v [W3][E_BNP_N][C1]??/an [W4][O_O_N][O]?
/u  [W5][O_O_N][O]??/n </labeled> 
<dependency no="1" observing="??/an" head="??
/v" head_wordid="W2" head_chunk ="B_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="E_BVP_N" 
modifer_chunkid="C1" relation="VO" > </dependency> 
</sentence> 
Dependency of word combination is annotated 
with the tag <dependency> which includes the fol-
lowing attributes: 
-<dependency> indicates an identified depend-
ency   
-no is the id of identified dependency within cur-
rent sentence according to ordinal sequence 
-observing indicates the current observing 
headword 
-head indicates the head of the identified word 
dependency 
-head_wordid is the word id of the head 
-head_chunk is the hybrid tags for labeling the 
chunking information of the head 
-head_chunkid is the chunk id of the head 
-modifier indicates the modifier of the identified 
dependency 
-modifier_wordid is the word id of the modifier 
-modifier_chunk is the hybrid tags for labeling 
chunking information of the modifier 
-modifier_chunkid is the chunk id of the modi-
fier 
-relation gives the syntactic dependency rela-
tions labeled according to the dependency labels 
listed in Table 1. 
In S1 and S2, the word combination ??/v??
/an has direct dependency, and in S3, such a de-
pendency does not exist as??/v only determines
??/v and ??/an depends on ??/v. The qual-
ity of CCB highly depends on the accuracy of de-
pendency annotation. This is very important for 
effective characteristics analysis of collocations 
and for the collocation extraction algorithms.  
Pass 2. N-gram collocations annotation 
It is relatively easy to identify n-gram colloca-
tions since an n-gram collocation is of habitual and 
recurrent use of a series of bi-grams. This means 
that n-gram collocations can be identified by find-
ing consecutive occurrence of significant bi-grams 
in certain position. In the second pass, the annota-
tors focus on the sentences where the headword 
has more than one dependency. The percentage of 
all appearances of each dependent word at each 
position around the headword is estimated with the 
help of a program (Xu and Lu, 2006). Finally, 
word dependencies frequently co-occurring in con-
secutive positions in a fixed order are extracted as 
n-gram collocations.   
For the headword, an n-gram collocation??/n 
? ? /n ? ? /an is identified since the co-
occurrence percentage of dependency??/-NN-?
?/an and dependency??/n-NN-??/an is 0.74 
is greater than a empirical threshold suggest in (Xu 
and Lu, 2006). This n-gram is annotated in S2 as 
follows: 
<ncolloc observing="??/an" w1="??/n" w2="??/n" 
w3="??/an" start_wordid="5"> </ncolloc> 
where, 
-<ncolloc> indicates an n-gram collocation  
-w1, w2,..wn give the components of the n-gram 
collocation according to the ordinal sequence.  
-start_wordid  indicates the word id of the first 
component of the n-gram collocation. 
Since n-gram collocation is regarded as a whole, 
its internal dependencies are ignored in the output 
file of pass 2. That is, if the dependencies of sev-
eral components are associated with an n-gram 
collocation in one sentence, the n-gram collocation 
is annotated and these dependencies are filtered out 
so as not to disturb the bi-gram dependencies.  
Pass 3. Bi-gram collocations annotation 
In this pass, all the word dependencies are ex-
amined to identify bi-gram collocations. Further-
more, if a dependent word combination is regarded 
as a collocation by the annotators, it will be further 
labeled based on the type determined. The identifi-
cation is based on expert knowledge combined 
with the use of several computational features as 
discussed in (Xu and Lu, 2006). 
An assistive tool is developed to estimate the 
computational features. We use the program to ob-
tain feature data based on two sets of data. The 
first data set is the annotated dependencies in the 
5-million-word corpus which is obtained through 
Pass 1 and Pass 2 annotations. Because the de-
pendent word combinations are manually identi-
fied and annotated in the first pass, the statistical 
significance is helpful to identify whether the word 
combination is a collocation and to determine its 
type. However, data sparseness problem must be 
considered since 5-million-word is not large 
enough. Thus, another set of statistical data are 
65
collected from a 100-million segmented and tagged 
corpus (Xu and Lu, 2006). With this large corpus, 
data sparseness is no longer a serious problem. But, 
the collected statistics are quite noisy since they 
are directly retrieved from text without any verifi-
cation. By analyzing the statistical features from 
both sets, the annotator can use his/her professional 
judgment to determine whether a bi-gram is a col-
location and its collocation type. 
In the example sentences, two collocations are 
identified. Firstly, ??/an ??/v is classified as a 
Type 1 collocation as they have only one peak co-
occurrence, very low substitution ratio and their 
co-occurrence order nearly never altered. Secondly, 
??/v ??/an is identified as a collocation. They 
have frequent co-occurrences and they are always 
co-occurred in fixed order among the verified de-
pendencies. However, their co-occurrences are dis-
tributed evenly and they have two peak co-
occurrences. Therefore, ??/v ??/an is classi-
fied as a Type 3 collocation. These bi-gram collo-
cations are annotated as illustrated below, 
<bcolloc observing="??/an" col="??/v" head="??
/v" type= "1" relation="ADV">  
<dependency no="1" observing="??/an" head="??/v" 
head_wordid="W4" head_chunk ="E_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="B_BVP_N" 
modifer_chunkid="C1" relation="ADV" 
></dependency></bcolloc> 
where, 
-<bcolloc> indicates a bi-gram collocation. 
-col is for  the collocated word. 
-head indicates the head of an identified colloca-
tion 
-type is the classified collocation type. 
-relation gives the syntactic dependency rela-
tions of this bi-gram collocation. 
Note that the dependency annotations within the 
bi-gram collocations are reserved. 
4.4 Quality Assurance 
The annotators of CCB are three post-graduate stu-
dents majoring in linguistics. In the first annotation 
stage, 20% headwords of the whole set was anno-
tated in duplicates by all three of them. Their out-
puts were checked by a program. Annotated collo-
cation including classified dependencies and types 
accepted by at least two annotators are reserved in 
the final data as the Golden Standard while the 
others are considered incorrect. The inconsisten-
cies between different annotators were discussed to 
clarify any misunderstanding in order to come up 
with the most appropriate annotations. In the sec-
ond annotation stage, 80% of the whole annota-
tions were then divided into three parts and sepa-
rately distributed to the annotators with 5% dupli-
cate headwords were distributed blindly. The du-
plicate annotation data were used to estimate the 
annotation consistency between annotators.  
5 Collocation Characteristic Analysis 
5.1 Progress and Quality of CCB 
Up to now, the first version of CCB is completed. 
We have obtained 23,581 unique bi-gram colloca-
tions and 2,752 unique n-gram collocations corre-
sponding to the 3,643 observing headwords. 
Meanwhile, their occurrences in the corpus are an-
notated and verified. With the help of a computer 
program, the annotators manually classified bi-
gram collocations into three types. The numbers of 
Type 0/1, Type 2 and Type 3 collocations are 152, 
3,982 and 19,447, respectively.  
For the 3,643 headwords in The Dictionary of 
Modern Chinese Collocations (Mei 1999) with 
35,742 bi-gram collocations,  20,035 collocations 
appear in the corpus. We call this collection as 
Mei?s Collocation Collection (MCC). There are 
19,967 common entries in MCC and CCB, which 
means 99.7% collocations in MCC appear in CCB 
indicating a good linguistic consistency. Further-
more, 3,614 additional collocations are found in 
CCB which enriches the static collocation diction-
ary.
 
5.2 Dependencies Numbers Statistics of Col-
locations 
Firstly, we study the statistics of how many types 
of dependencies a bi-gram collocation may have. 
The numbers of dependency types with respect to 
different collocation types are listed in Table 3.  
 
Collocations 1 type 2 types >2 types Total 
Type 0/1 152 0 0 152 
Type 2 3970 12 0 3982 
Type 3 17282 2130 35 19447 
Total 21404 2142 35 23581 
Table 3. Collocation classification versus number 
of dependency types 
66
It is observed that about 90% bi-gram collocations 
have only one dependency type. This indicates that 
a collocation normally has only one fixed syntactic 
dependency. It is also observed that about 10% bi-
gram collocations have more than one dependency 
type, especially Type 3 collocations. For example, 
two types of dependencies are identified in the bi-
gram collocation ??/an-??/n. They are ??
/an-AN-??/n (a safe nation) which indicates the 
dependency of a noun and its nominal modifier 
where ??/n serves as the head, and??/n-NN-
?? /an (national security) which indicates the 
dependency of a noun and its nominal modifier 
where ??/an serves as the head. It is attributed to 
the fact that the use of Chinese words is flexible. A 
Chinese word may support different part-of-speech. 
A collocation with different dependencies results 
in different distribution trends and most of these 
collocations are classified as Type 3. On the other 
hand, Type 0/1 and Type 2 collocations seldom 
have more than one dependency type. 
5.3 Syntactic Dependency Statistics of Collo-
cations 
The statistics of the 10 types of syntactic depend-
encies with respect to different types of bi-gram 
collocations are shown in Table 4. No. is the num-
ber of collocations with a given dependency type  
D and a given collocation type T. The percentage 
of No. among all collocations with the same collo-
cation type T is labeled as P_T, and the percentage 
of No. among all of the collocations with the same 
dependency D is labeled as P_D. 
 
 Type 0/1  Type 2  Type 3  Total 
 No. P_T P_D No. P_T P_D No. P_T P_D No. P_T
ADA 1 0.7 0.1 212 5.3 11.5 1637 7.6 88.5 1850 7.2
ADV 9 5.9 0.3 322 8.1 11.2 2555 11.8 88.5 2886 11.2
AN 20 13.2 0.4 871 21.8 15.4 4771 22.0 84.3 5662 22.0
CMP 12 7.9 2.2 144 3.6 26.9 379 1.8 70.8 535 2.1
NJX 8 5.3 3.2 42 1.1 16.9 198 0.9 79.8 248 1.0
NN 44 28.9 0.9 1036 25.9 21.6 3722 17.2 77.5 4802 18.6
SBV 4 2.6 0.2 285 7.1 11.1 2279 10.5 88.7 2568 10.0
VO 26 17.1 0.5 652 16.3 12.5 4545 21.0 87.0 5223 20.2
VV 3 2.0 0.2 227 5.7 13.4 1464 6.8 86.4 1694 6.6
OT 25 16.4 7.7 203 5.1 62.5 97 0.4 29.8 325 1.3
Total 152 100.0 0.6 3994 100.0 15.5 21647 100.0 83.9 25793 100.0
Table 4. The statistics of collocations with dif-
ferent collocation type and dependency 
 
Corresponding to 23,581 bi-gram collocations, 
25,793 types of dependencies are identified (some 
collocations have more than one types of depend-
ency). In which, about 82% belongs to five major 
dependency types. They are AN, VO, NN, ADV and 
SBV. It is note-worthy that the percentage of NN 
collocation is much higher than that in English. 
This is because nouns are more often used in paral-
lel to serve as one syntactic component in Chinese 
sentences than in English. 
The percentages of Type 0/1, Type 2 and Type 3 
collocations in CCB are 0.6%, 16.9% and 82.5%, 
respectively. However, the collocations with dif-
ferent types of dependencies have shown their own 
characteristics with respect to different collocation 
types. The collocations with CMP, NJX and NN 
dependencies on average have higher percentage to 
be classified into Type 0/1 and Type 2 collocations. 
This indicates that CMP, NJX and NN collocations 
in Chinese are always used in fixed patterns and 
these kinds of collocations are not freely modifi-
able and substitutable. In the contrary, many ADV 
and AN collocations are classified as Type 3. This 
is partially due to the special usage of auxiliary 
words in Chinese.  Many AN Chinese collocations 
can be inserted by a meaningless auxiliary word?
/u and many ADV Chinese collocations can be in-
serted by an auxiliary word?/u. This means that 
many AN and ADV collocations can be modified 
and thus, they always have two peak co-
occurrences. Therefore, they are classified as Type 
3 collocations. 7.7% and 62.5% of the collocations 
with dependency OT are classified as Type 0/1 and 
Type2 collocations, respectively. Such percentages 
are much higher than the average. This is attributed 
by the fact that some Type 0/1 and Type 2 colloca-
tions have strong semantic relations rather than 
syntactic relations and thus their dependencies are 
difficult to label. 
5.4 Chunking Statistics of Collocations 
The chunking characteristic for the collocations 
with different types and different dependencies are 
examined. In most cases, Type 0/1/2 collocations 
co-occur within one chunk or between neighboring 
chunks. Therefore, their chunking characteristics 
are not discussed in detail. The percentage of the 
occurrences of Type 3 collocations with different 
chunking distances are given in Table 5. If a collo-
cation co-occurs within one chunk, the chunking 
distance is 0. If a collocation co-occurs between 
neighboring chunks, or between neighboring words, 
or between a word and a neighboring chunk, the 
chunking distance is 1, and so on. 
67
 
 ADA ADV AN CMP NJX NN SBV VO VV OT
0 chunk 56.8 53.1 65.7 48.5 70.2 62.4 46.5 41.1 47.2 86.4
1 chunk 38.2 43.7 28.5 37.2 15.4 27.9 41.2 35.7 41.1 13.5
2 chunks 5.0 3.2 3.7 14.2 14.4 9.7 11.0 17.6 9.6 0.1
>2chunks 0.0 0.0 2.1 0.1 0.0 0.0 1.3 5.6 2.1 0.0
Table 5. Chunking distances of Type 3 collocations  
 
It is shown that the co-occurrence of collocations 
decreases with increased chunking distance. Yet, 
the behavior for decrease is different for colloca-
tions with different dependencies. Generally speak-
ing, the ADA, ADV, CMP, NJX, NN and OT collo-
cations seldom co-occur cross two words or two 
chunks. Furthermore, the occurrences of AN, NJX 
and OT collocations quickly drops when the 
chunking distance is greater than 0, i.e. these col-
locations tends to co-occur within the same chunk. 
In the contrary, the co-occurrences of ADA, ADV, 
CMP, SBV and VV collocations corresponding to 
chunking distance equals 0 and 1 decrease steadily. 
It means that these four kinds of collocations are 
more evenly distributed within the same chunk or 
between neighboring words or chunks. The occur-
rences of VO collocations corresponding to chunk-
ing distance from 0 to 3 with a much flatter reduc-
tion. This indicates that a verb may govern its ob-
ject in a long range. 
6 Conclusions 
This paper describes the design and construction of 
a manually annotated Chinese collocation bank. 
Following a set of well-designed annotation guide-
line, the collocations corresponding to 3,643 
headwords are identified from a chunked five-
million word corpus. 2,752 unique n-gram colloca-
tions and 23,581 unique bi-gram collocations are 
annotated. Furthermore, each bi-gram collocation 
is annotated with its syntactic dependency informa-
tion, classification information and chunking in-
formation. Based on CCB, characteristics of collo-
cations with different types and different depend-
encies are examined. The obtained result is essen-
tial for improving research related to Chinese col-
location. Also, CCB may be used as a standard an-
swer set for evaluating the performance of differ-
ent collocation extraction algorithms. In the future, 
collocations of all unvisited headwords will be an-
notated to produce a complete 5-million-word Chi-
nese collocation bank. 
Acknowledgement 
This research is supported by The Hong Kong 
Polytechnic University (A-P203), CERG Grant 
(5087/01E) and Chinese University of Hong Kong 
under the Direct Grant Scheme project (2050330) 
and Strategic Grant Scheme project (4410001). 
References 
Bedin N. et al 2003. The Annotation Process in the 
Turkish Treebank. In Proc. 11th Conference of the 
EACL-4th Linguistically Interpreted Corpora Work-
shop- LINC. 
Kosho S. et al 2000. Collocations as Word Co-
occurrence Restriction Data - An Application to 
Japanese Word Processor. In Proc. Second Interna-
tional Conference on Language Resources and 
Evaluation 
Lin D.K. 1998. Extracting collocations from text cor-
pora. In Proc. First Workshop on Computational 
Terminology, Montreal  
Manning, C.D., Sch?tze, H. 1999: Foundations of Sta-
tistical Natural Language Processing, MIT Press  
Mei J.J. 1999. Dictionary of Modern Chinese Colloca-
tions, Hanyu Dictionary Press  
Pecina P. 2005. An Extensive Empirical Study of Collo-
cation Extraction Methods. In Proc. 2005 ACL Stu-
dent Research Workshop. 13-18 
Smadja. F. 1993. Retrieving collocations from text: 
Xtract, Computational Linguistics. 19. 1.  143-177 
Tutin A. 2005. Annotating Lexical Functions in Corpora: 
Showing Collocations in Context. In Proc. 2nd Inter-
national Conference on the Meaning ? Text Theory 
Xu R. F. and Lu Q. 2005. Improving Collocation Ex-
traction by Using Syntactic Patterns, In Proc. IEEE 
International Conference on Natural Language Proc-
essing and Knowledge Engineering. 52-57 
Xu, R.F. and Lu, Q. 2006. A Multi-stage Chinese Col-
location Extraction System. Lecture Notes in Com-
puter Science, Vol. 3930, Springer-Verlag. 740-749 
Yu S.W. et al 2001. Guideline of People?s Daily Cor-
pus Annotation, Technical Report, Peking University  
 
 
68
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 185?188,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extractive Summarization Based on Event Term Clustering 
Maofu Liu1,2, Wenjie Li1, Mingli Wu1 and Qin Lu1 
 
1Department of Computing 
The Hong Kong Polytechnic University 
{csmfliu, cswjli, csmlwu, 
csluqin}@comp.polyu.edu.hk 
2College of Computer Science and Technology 
Wuhan University of Science and Technology 
mfliu_china@hotmail.com 
 
Abstract 
Event-based summarization extracts and 
organizes summary sentences in terms of 
the events that the sentences describe. In 
this work, we focus on semantic relations 
among event terms. By connecting terms 
with relations, we build up event term 
graph, upon which relevant terms are 
grouped into clusters. We assume that each 
cluster represents a topic of documents. 
Then two summarization strategies are 
investigated, i.e. selecting one term as the 
representative of each topic so as to cover 
all the topics, or selecting all terms in one 
most significant topic so as to highlight the 
relevant information related to this topic. 
The selected terms are then responsible to 
pick out the most appropriate sentences 
describing them. The evaluation of 
clustering-based summarization on DUC 
2001 document sets shows encouraging 
improvement over the well-known 
PageRank-based summarization. 
1 Introduction 
Event-based extractive summarization has emerged 
recently (Filatova and Hatzivassiloglou, 2004). It 
extracts and organizes summary sentences in terms 
of the events that sentences describe.  
We follow the common agreement that event 
can be formulated as ?[Who] did [What] to [Whom] 
[When] and [Where]? and ?did [What]? denotes 
the key element of an event, i.e. the action within 
the formulation. We approximately define the 
verbs and action nouns as the event terms which 
can characterize or partially characterize the event 
occurrences. 
Most existing event-based summarization 
approaches rely on the statistical features derived 
from documents and generally associated with 
single events, but they neglect the relations among 
events. However, events are commonly related 
with one another especially when the documents to 
be summarized are about the same or very similar 
topics. Li et al(2006) report that the improved 
performance can be achieved by taking into 
account of event distributional similarities, but it 
does not benefit much from semantic similarities. 
This motivated us to further investigate whether 
event-based summarization can take advantage of 
the semantic relations of event terms, and most 
importantly, how to make use of those relations. 
Our idea is grouping the terms connected by the 
relations into the clusters, which are assumed to 
represent some topics described in documents. 
In the past, various clustering approaches have 
been investigated in document summarization. 
Hatzivassiloglou et al(2001) apply clustering 
method to organize the highly similar paragraphs 
into tight clusters based on primitive or composite 
features. Then one paragraph per cluster is selected 
to form the summary by extraction or by 
reformulation. Zha (2002) uses spectral graph 
clustering algorithm to partition sentences into 
topical groups. Within each cluster, the saliency 
scores of terms and sentences are calculated using 
mutual reinforcement principal, which assigns high 
salience scores to the sentences that contain many 
terms with high salience scores. The sentences and 
key phrases are selected by their saliency scores to 
generate the summary. The similar work based on 
topic or event is also reported in (Guo and Stylios, 
2005).
The granularity of clustering units mentioned 
above is rather coarse, either sentence or paragraph. 
In this paper, we define event term as clustering 
185
unit and implement a clustering algorithm based on 
semantic relations. We extract event terms from 
documents and construct the event term graph by 
linking terms with the relations. We then regard a 
group of closely related terms as a topic and make 
the following two alterative assumptions:  
(1) If we could find the most significant topic as 
the main topic of documents and select all terms in 
it, we could summarize the documents with this 
main topic.  
(2) If we could find all topics and pick out one 
term as the representative of each topic, we could 
obtain the condensed version of topics described in 
the documents.  
Based on these two assumptions, a set of cluster 
ranking, term selection and ranking and sentence 
extraction strategies are developed. The remainder 
of this paper is organized as follows. Section 2 
introduces the proposed extractive summarization 
approach based on event term clustering. Section 3 
presents experiments and evaluations. Finally, 
Section 4 concludes the paper. 
2 Summarization Based on Event Term 
Clustering 
2.1 Event Term Graph 
We introduce VerbOcean (Chklovski and Pantel, 
2004), a broad-coverage repository of semantic 
verb relations, into event-based summarization. 
Different from other thesaurus like WordNet, 
VerbOcean provides five types of semantic verb 
relations at finer level. This just fits in with our 
idea to introduce event term relations into 
summarization. Currently, only the stronger-than 
relation is explored. When two verbs are similar, 
one may denote a more intense, thorough, 
comprehensive or absolute action. In the case of 
change-of-state verbs, one may denote a more 
complete change. This is identified as the stronger-
than relation in (Timothy and Patrick, 2004). In 
this paper, only stronger-than is taken into account 
but we consider extending our future work with 
other applicable relations types.  
The event term graph connected by term 
semantic relations is defined formally as 
, where V is a set of event terms and E 
is a set of relation links connecting the event terms 
in V. The graph is directed if the semantic relation 
has the characteristic of the asymmetric. Otherwise, 
it is undirected. Figure 1 shows a sample of event 
term graph built from one DUC 2001 document set. 
It is a directed graph as the stronger-than relation 
in VerbOcean exhibits the conspicuous asymmetric 
characteristic. For example, ?fight? means to 
attempt to harm by blows or with weapons, while 
?resist? means to keep from giving in. Therefore, a 
directed link from ?fight? to ?resist? is shown in 
the following Figure 1.  
),( EVG =
Relations link terms together and form the event 
term graph. Based upon it, term significance is 
evaluated and in turn sentence is judged whether to 
be extracted in the summary. 
 
Figure 1. Terms connected by semantic relations 
2.2 Event Term Clustering 
Note that in Figure 1, some linked event terms, 
such as ?kill?, ?rob?, ?threaten? and ?infect?, are 
semantically closely related. They may describe 
the same or similar topic somehow. In contrast, 
?toler?, ?resist? and ?fight? are clearly involved in 
another topic; although they are also reachable 
from ?kill?. Based on this observation, a clustering 
algorithm is required to group the similar and 
related event terms into the cluster of the topic.  
In this work, event terms are clustered by the 
DBSCAN, a density-based clustering algorithm 
proposed in (Easter et al 1996). The key idea 
behind it is that for each term of a cluster the 
neighborhood of a given radius has to contain at 
least a minimum number of terms, i.e. the density 
in the neighborhood has to exceed some threshold. 
By using this algorithm, we need to figure out 
appropriate values for two basic parameters, 
namely, Eps (denoting the searching radius from 
each term) and MinPts (denoting the minimum 
number of terms in the neighborhood of the term). 
We assign one semantic relation step to Eps since 
there is no clear distance concept in the event term 
186
graph. The value of Eps is experimentally set in 
our experiments. We also make some modification 
on Easter?s DBSCAN in order to accommodate to 
our task.  
Figure 2 shows the seven term clusters 
generated by the modified DBSCAN clustering 
algorithm from the graph in Figure 1. We represent 
each cluster by the starting event term in bold font.  
fight
resist
consider
expect
announce
offer
list public
accept
honor
publish study
found
place
prepare
toler
pass
fear
threaten
kill
feel suffer
live
survive
undergo
ambush
rob
infect
endure
run
moverush
report
investigate
file
satisfy
please
manage
accept
Figure 2. Term clusters generated from Figure 1 
2.3 Cluster Ranking 
The significance of the cluster is calculated by  
? ??
? ??
=
CC Ct
t
Ct
ti
i ii
ddCsc /)(  
where  is the degree of the term t  in the term 
graph. C  is the set of term clusters obtained by the 
modified DBSCAN clustering algorithm and  is 
the ith one. Obviously, the significance of the 
cluster is calculated from global point of view, i.e. 
the sum of the degree of all terms in the same 
cluster is divided by the total degree of the terms in 
all clusters. 
td
iC
2.4 Term Selection  and Ranking 
Representative terms are selected according to the 
significance of the event terms calculated within 
each cluster (i.e. from local point of view) or in all 
clusters (i.e. from global point of view) by  
LOCAL:  or ?
?
=
ict
tt ddtst /)(
GLOBAL:  ? ?
? ?
=
Cc ct
tt
i i
ddtst /)(
Then two strategies are developed to select the 
representative terms from the clusters.  
(1) One Cluster All Terms (OCAT) selects all 
terms within the first rank cluster. The selected 
terms are then ranked according to their 
significance.  
(2) One Term All Cluster (OTAC) selects one 
most significant term from each cluster. Notice that 
because terms compete with each other within 
clusters, it is not surprising to see )()( 21 tsttst <  
even when , . To 
address this problem, the representative terms are 
ranked according to the significance of the clusters 
they belong to.  
)()( 21 csccsc > ),( 2211 ctct ??
2.5 Sentence Evaluation and Extraction 
A representative event term may associate to more 
than one sentence. We extract only one of them as 
the description of the event. To this end, sentences 
are compared according to the significance of the 
terms in them. MAX compares the maximum 
significance scores, while SUM compares the sum 
of the significance scores. The sentence with either 
higher MAX or SUM wins the competition and is 
picked up as a candidate summary sentence. If the 
sentence in the first place has been selected by 
another term, the one in the second place is chosen. 
The ranks of these candidates are the same as the 
ranks of the terms they are selected for. Finally, 
candidate sentences are selected in the summary 
until the length limitation is reached. 
3 Experiments 
We evaluate the proposed approaches on DUC 
2001 corpus which contains 30 English document 
sets. There are 431 event terms on average in each 
document set. The automatic evaluation tool, 
ROUGE (Lin and Hovy, 2003), is run to evaluate 
the quality of the generated summaries (200 words 
in length). The tool presents three values including 
unigram-based ROUGE-1, bigram-based ROUGE-
2 and ROUGE-W which is based on longest 
common subsequence weighted by the length. 
Google?s PageRank (Page and Brin, 1998) is 
one of the most popular ranking algorithms. It is 
also graph-based and has been successfully applied 
in summarization. Table 1 lists the result of our 
implementation of PageRank based on event terms. 
We then compare it with the results of the event 
term clustering-based approaches illustrated in 
Table 2. 
 PageRank  
ROUGE-1 0.32749 
187
ROUGE-2 0.05670 
ROUGE-W 0.11500 
Table 1. Evaluations of PageRank-based 
Summarization 
LOCAL+OTAC MAX SUM 
ROUGE-1 0.32771 0.33243
ROUGE-2 0.05334 0.05569
ROUGE-W 0.11633 0.11718
GLOBAL+OTAC MAX SUM 
ROUGE-1 0.32549 0.32966
ROUGE-2 0.05254 0.05257
ROUGE-W 0.11670 0.11641
LOCAL+OCAT MAX SUM 
ROUGE-1 0.33519 0.33397
ROUGE-2 0.05662 0.05869
ROUGE-W 0.11917 0.11849
GLOBAL+OCAT MAX SUM 
ROUGE-1 0.33568 0.33872
ROUGE-2 0.05506 0.05933
ROUGE-W 0.11795 0.12011
Table 2. Evaluations of Clustering-based  
Summarization 
The experiments show that both assumptions are 
reasonable. It is encouraging to find that our event 
term clustering-based approaches could outperform 
the PageRank-based approach. The results based 
on the second assumption are even better. This 
suggests indeed there is a main topic in a DUC 
2001 document set. 
4 Conclusion 
In this paper, we put forward to apply clustering 
algorithm on the event term graph connected by 
semantic relations derived from external linguistic 
resource. The experiment results based on our two 
assumptions are encouraging. Event term 
clustering-based approaches perform better than 
PageRank-based approach. Current approaches 
simply utilize the degrees of event terms in the 
graph. In the future, we would like to further 
explore and integrate more information derived 
from documents in order to achieve more 
significant results using the event term clustering-
based approaches. 
Acknowledgments 
The work described in this paper was fully 
supported by a grant from the Research Grants 
Council of the Hong Kong Special Administrative 
Region, China (Project No. PolyU5181/03E). 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram 
Cooccurrence Statistics. In Proceedings of HLT/ 
NAACL 2003, pp71-78. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In 
Proceedings of ACL 2004 Workshop on 
Summarization, pp104-111. 
Hongyuan Zha. 2002. Generic Summarization and 
keyphrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of the 25th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, 2002. pp113-120.   
Lawrence Page and Sergey Brin, Motwani Rajeev 
and Winograd Terry. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Martin Easter, Hans-Peter Kriegel, J?rg Sander, et al 
1996. A Density-Based Algorithm for Discovering 
Clusters in Large Spatial Databases with Noise. In 
Proceedings of the 2nd International Conference 
on Knowledge Discovery and Data Mining, Menlo 
Park, CA, 1996. 226-231.  
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Timothy Chklovski and Patrick Pantel. 2004. 
VerbOcean: Mining the Web for Fine-Grained 
Semantic Verb Relations. In Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing, 2004. 
Vasileios Hatzivassiloglou, Judith L. Klavans, 
Melissa L. Holcombe, et al 2001. Simfinder: A 
Flexible Clustering Tool for Summarization. In 
Workshop on Automatic Summarization, NAACL, 
2001. 
Wenjie Li, Wei Xu, Mingli Wu, et al 2006. 
Extractive Summarization using Inter- and Intra- 
Event Relevance. In Proceedings of ACL 2006, 
pp369-376. 
Yi Guo and George Stylios. 2005. An intelligent 
summarization system based on cognitive 
psychology. Journal of Information Sciences, 
Volume 174, Issue 1-2, Jun. 2005, pp1-36. 
188
Coling 2010: Poster Volume, pages 665?673,
Beijing, August 2010
Combining Constituent and Dependency Syntactic Views for  
Chinese Semantic Role Labeling 
Shiqi Li1, Qin Lu2, Tiejun Zhao1, Pengyuan Liu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Department of Computing, 
The Hong Kong Polytechnic University 
csluqin@comp.polyu.edu.hk 
3Institute of Computational Linguistics, 
Peking University 
liupengyuan@pku.edu.cn 
 
Abstract 
This paper presents a novel feature-
based semantic role labeling (SRL) 
method which uses both constituent 
and dependency syntactic views. Com-
paring to the traditional SRL method 
relying on only one syntactic view, the 
method has a much richer set of syn-
tactic features. First we select several 
important constituent-based and de-
pendency-based features from existing 
studies as basic features. Then, we pro-
pose a statistical method to select dis-
criminative combined features which 
are composed by the basic features. 
SRL is achieved by using the SVM 
classifier with both the basic features 
and the combined features. Experimen-
tal results on Chinese Proposition Bank 
(CPB) show that the method outper-
forms the traditional constituent-based 
or dependency-based SRL methods. 
1 Introduction 
Semantic role labeling (SRL) is a major me-
thod in current semantic analysis which is im-
portant to NLP applications. The SRL task is 
to identify semantic roles (or arguments) of 
each predicate and then label them with their 
functional tags, such as 'Arg0' and 'ArgM' in 
PropBank (Palmer et al, 2005), or 'Agent' and 
'Patient' in FrameNet (Baker et al, 1998).  
The significance of syntactic analysis in 
SRL has been proven by (Gildea and Palmer, 
2002; Punyakanok et al, 2005), and syntactic 
parsing has been applied by almost all current 
studies. In terms of syntactic representations, 
the SRL approaches are mainly divided into 
three categories: constituent-based, chunk-
based and dependency-based. Constituent-
based SRL has been studied intensively with 
satisfactory results. Chunk-based SRL has 
been found to be less effective than the con-
stituent-based by (Punyakanok et al, 2005). In 
recent years, the dependency-based SRL has 
been greatly promoted by the CoNLL shared 
tasks on semantic parsing (Hajic et al, 2009). 
However, there is not much research on com-
bined use of different syntactic views (Pradhan 
et al, 2005), on the feature level of SRL.  
This paper introduces a novel method for 
Chinese SRL utilizing both constituent-based 
and dependency-based features. The method 
takes constituent as the basic unit of argument 
and adopts the labeling of PropBank. It follows 
the prevalent feature-based SRL methods to 
first turn predicate-argument pairs into flat 
structures by well-defined linguistic features, 
and then uses machine learning methods to 
predict the semantic labels. The method also 
involves two classification phases: semantic 
role identification (SRI) and semantic role 
classification (SRC). In addition, a heuristic-
based pruning preprocessing (Xue and Palmer, 
2004) is used to filter out a lot of apparently 
inappropriate constituents at the beginning.  
665
And it has been widely reported that, in fea-
ture-based SRL, the performance can be im-
proved by adding several combined features 
each of which is composed by two single fea-
tures (Xue and Palmer, 2004; Toutanova et al, 
2005; Zhao et al, 2009). Thus, in this work, 
we exploit combined use of both constituent-
based and dependency-based features in addi-
tion to using features of singular types of syn-
tactic view. We propose a statistical method to 
select effective combined features using both 
constituent-based and dependency-based fea-
tures to make full use of two syntactic views. 
2 Related Work 
In recent years, many advances have been 
made on SRL using singular syntactic view, 
such as constituent (Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Surdeanu et al, 2007), 
dependency (Hacioglu, 2004; Johansson and 
Nugues, 2008; Zhao et al, 2009), and CCG 
(Chen and Rambow, 2003; Boxwell et al 
2009). However, there are few studies on the 
use of multiple syntactic views. We briefly 
review the relevant studies of SRL using 
multiple syntactic views as follows. 
Pradhan et al (2005) built three semantic 
role labelers using constituent, dependency and 
chunk syntactic views, and then heuristically 
combined them at the output level. The method 
was further improved in Pradhan et al (2008) 
which trains two semantic role labelers for 
constituents and dependency separately, and 
then uses the output of the two systems as ad-
ditional features in another labeler using chunk 
parsing. The result shows an improvement to 
each labeler alone. A possible reason for the 
improvement is that the errors caused by dif-
ferent syntactic parsers are compensated. Yet, 
the features of different syntactic views can 
hardly complement each other in labeling. And 
the complexity of using multiple syntactic 
parsers is extremely high. Hacioglu (2004) 
proposed a SRL method to combine constitu-
ent and dependency syntactic views where the 
dependency parses are ob-tained through auto-
matic mapping of constitu-ent parses. It uses 
the constituent parses to get candidates and 
then, the dependency parses to label them.  
Boxwell et al (2009) proposed a SRL me-
thod using features of three syntactic views: 
CCG, CFG and dependency. It primarily uses 
CCG-based features associated with 4 CFG-
based and 2 dependency-based features. The 
combination of these syntactic views leads to a 
substantial performance improvement. Nguyen 
et al (2009) proposed a composite kernel 
based on both constituent and dependency syn-
tactic views and achieved a significant im-
provement in a relation extraction application. 
3 
Compared to related work, the proposed me-
thod integrates the constituent and dependency 
views in a collaborative manner. First, we de-
fine a basic feature set containing features 
from constituent and dependency syntactic 
views. Then, to make better use of two syntac-
tic views, we introduce a statistical method to 
select effective combined features from the 
basic feature set. Finally we use both the basic 
features and the combined features to identify 
and label arguments. One of the drawbacks of 
the related work is the considerable complexity 
caused by multiple syntactic parsing processes. 
In our method, the cost of syntactic parsing 
will increase only slightly as we derive de-
pendency parsing from constituent parsing us-
ing a constituent-to-dependency converter in-
stead of using an additional dependency parser. 
In our method, the feature set used for SRL 
consists of two parts: the basic feature set and 
the combined feature set built upon the basic 
feature set. The basic feature set can be further 
divided into constituent-based features and 
dependency-based features. Constituent fea-
tures focus on hierarchical relations between 
multi-word constituents whereas dependency 
features focus on dependencies between indi-
vidual words, as shown in Figure 1. Take the 
predicate '??' (increased) as an example, in 
Figure 1(a), the NP constituent '?????' 
(China's position) is labeled as 'Arg0'. The ar-
gument and the predicate are connected by the 
path of node types: 'NP-IP-VP-VP'. But in 
Figure 1(b), the individual word '??' (posi-
tion) is labeled as 'Arg0'. And the connection 
between the argument and the predicate is only 
one edge with the relation 'nsubj', which is 
more explicit than the path in the constituent 
structure. So the two syntactic views can com-
plement each other on different linguistic units. 
Design Principle and Basic Features 
666
3.1 Constituent-Based Features 
As a prevalent syntactic feature set for SRL, 
constituent-based features have been 
extensively studied by many researchers. In 
this work, we simply take 26 constituent-based 
features tested by existing studies, and add 8 
new features define by us. Firstly, the 26 
constituent-based features used by others are: 
y The seven "standard" features: predicate (c1), 
path (c2), phrase type (c3), position (c4), 
voice (c5), head word (c6) and predicate 
subcategorization (c7) features proposed by 
(Gildea and Jurafsky, 2002). 
y Syntactic frame (c8) feature from (Xue and 
Palmer, 2004). 
y Head word POS (c9), partial path (c10), 
first/last word in constituent (c11/c12), 
first/last POS in constituent (c13/c14), 
left/right sibling constituent (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree dis-
tance (c21) and temporal cue words (c22) 
features from (Pradhan et al, 2004). 
y Predicate POS (c23), argument's parent 
constituent (c24), argument's parent con-
stituent head (c25) and argument's parent 
constituent POS (c26) inspired by (Pradhan 
et al, 2004). 
Secondly, the 8 new features that we define 
are (we take the 'Arg0' node in Figure 1(a) as 
the example to illustrate them): 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue words, similar to the temporal 
cue words (c22). This feature is defined to 
distinguish the arguments with the 'ArgM-
LOC' type from others. 
y POS pattern of argument's children (c28): 
the left-to-right chain of the POS tags of the 
argument's children, e.g. 'NR-DEG-NN'. 
y Phrase type pattern of argument's children 
(c29): the left-to-right chain of the phrase 
type labels of the argument's children, simi-
lar with the POS pattern of argument's chil-
dren (c28), e.g. 'DNP-NP'. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child, e.g. 'IP-NP'. 
y Type of LCA and right child (c31): The 
phrase type of the LCA combined with its 
right child, e.g. 'IP-VP'. 
Three features: bag of words of path (c32), 
bag of words of POS pattern (c33) and bag of 
words of type pattern (c34), for generalizing 
three sparse features: path (c2), POS pattern of 
argument's children (c28) and phrase type pat-
tern of argument's children (c29) by the bag-
of-words representation. 
3.2 Dependency-Based Features 
The dependency parse can effectively repre-
sent the head-dependent relationship between 
words, yet, it lacks constituent information. If 
we want to label constituents using depend-
ency-based features, we should firstly map 
each constituent to one or more appropriate 
words in the dependency tree. In this paper, we 
use the head word of a constituent to represent 
the constituent in the dependency parses.  
The selection method of dependency-based 
features is similar to the method of constitu-
ent-based features. The 35 selected dependen-
cy-based features include: 
y Predicate/Argument relation type (d1/d2), 
relation path (d3), POS pattern of predi-
cate?s children (d4) and relation pattern of 
predicate?s children (d5) features from (Ha-
cioglu, 2004). 
y Child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21) and left/right sibling 
relation (d22/d23) features as described in 
(Johansson and Nugues, 2008). 
y Dep-exists (d24) and dep-type (d25) features 
from (Boxwell et al, 2009). 
y POS path (d26), POS path length (d27), REL 
path length (d28) from (Che et al, 2008). 
y High/low support verb (d29/d30), high/low 
support noun (d31/d32) features from (Zhao 
et al, 2009). 
y  LCA?s word/POS/relation (d33/d34/d35) 
inspired by (Toutanova et al, 2005). 
To maintain the consistency between two 
syntactic views, the dependency parses are 
generated by a constituent-to-dependency con-
verter (Marneffe et al, 2006), which is suitable 
for semantic analysis as it retrieves the seman-
tic head rather than the general syntactic head, 
using a set of modified Bikel's head rules.  
667
4 Selection of Combined Features 
The combined features, each of which consists 
of two different basic features, have proven to 
be positive for SRL. Several combined features 
have been widely used in SRL, such as 'predi-
cate+head word' and 'position+voice'. But to 
our knowledge, there is no prior report about 
the selection method of combined features for 
SRL. The common entropy-based criteria are 
invalid here because the combined features 
always take lots of distinct values. And the 
greedy method is too complicated to be practi-
cal due to the large number of combinations. 
In this paper, we define two statistical crite-
ria to efficiently estimate the classification per-
formance of each combined feature on the cor-
pus. Inspired by Fisher Linear Discriminant 
Analysis (FLDA) (Fisher, 1938) in which the 
separation of two classes is defined as the ratio 
of the variance between the classes to the vari-
ance within the classes, namely larger ratio can 
lead to better separation between two classes, 
and the discriminant plane can be achieved by 
maximizing the separation. Therefore, in this 
paper, we adopt the ratio of inter-class distance 
to intra-class distance to measure to what ex-
tent a combined feature can partition the data.  
Initially, the feature set contains only the N  
basic features. We construct one combined 
feature abf  at each iteration by combining two 
basic features af  and bf , where , [1, ]a b N?  
and a b? . We push abf  into the feature set and 
take it as the 1N + th feature. Then, all the 
training instances are represented by feature 
vectors using the new feature set, and we then 
quantize the feature vectors of positive and 
negative data orderly to keep their intrinsic 
statistical difference. If the training dataset is 
denoted as :{ , }pos negD D D , then the separation 
criterion, namely the ratio of inter-class to in-
tra-class distance for feature if  can be given as 
( ) ( , )
( , )
i
fi
f pos neg
i
pos neg
InterDist D D
g f
IntraDist D D
=
 
(1)
where the inter-class and the intra-class dis-
tance between posD  and negD  for feature if  are 
specified by (2) and (3), respectively. 
( )2( , ) ( ) ( )
i i if pos neg f pos f neg
InterDist D D Mean D Mean D= ? (2)
2 2( , ) ( ) ( )f fi iif pos neg pos negIntraDist D D S D S D= + (3)
( )
if
Mean D  in (2) and ( )
if
S D  in (3) repre-
sents the sample mean and the corresponding 
sample standard deviation of feature if  in 
dataset D  as given in (4) and (5). 
( )
( )
| |i
x D
f
x i
Mean D
D
?=
?
, [1, 1]i N? +  (4)
( )2( ) ( )
( )
i
i
f
x D
f
Mean D x i
S D
N
?
?
=
?
, [1, 1]i N? +
(5)
Essentially, the inter-class distance reflects 
the distance between the center of positive da-
taset and the center of negative dataset, and the 
intra-class distance indicates the intensity of all 
instances relative to the corresponding center. 
Therefore, larger ratio will lead to a better par-
tition for a feature, as has been pointed out by 
FLDA. In order to compare the ratio between 
different combined features, we further stan-
dardize the value of ( )ig f  by computing its z-
score ( )iZ f  which indicates how many stan-
dard deviations between a sample and its mean, 
as given in (6). 
( ) ( )
( ) i ii
G
g f g f
Z f
S
?=  (6)
where ( )ig f  represents the sample mean as 
given in (7), and GS  represents the sample 
standard deviation of the sequence ( )ig f  
where i  ranges from 1 to N+1 as given in (8).  
1
1
( )
( )
1
N
i
i
i
g f
g f
N
+
== +
?
, [1, 1]i N? +  
(7)
1
2
1
( ( ) ( ))
N
i i
i
G
g f g f
S
N
+
=
?
=
?
, [1, 1]i N? +  
(8)
After figuring out the ( )aZ f  and ( )bZ f  for 
the basic feature af  and bf , and ( )abZ f  for the 
combined feature abf  by (6), we define the 
other criterion, namely the improvement 
( )abI f  of the combined feature, as the smaller 
difference between the z-score of the combined 
668
feature and its two corresponding basic fea-
tures as given in (9). 
( )( ) ( ) Max ( ),  ( )ab ab a bI f Z f Z f Z f= ?  (9)
Finally, the combined feature with a nega-
tive ( )abI f  value is eliminated. Then, we will 
rank the combined features in terms of their z-
score, and use the top N of them for later clas-
sification. The selection method based on the 
two criteria can effectively filter out combined 
features whose means have no significant dif-
ference between positive and negative data, 
and hence retain the potentially useful com-
bined features for the separation. Meanwhile, it 
has a relatively fast speed when dealing with a 
large number of features in comparison to the 
greedy method due to its simplicity. 
5 Performance Evaluation 
5.1 Experimental Setting 
In our experiments, we adopt the three-step 
strategy proposed by (Xue and Palmer, 2004). 
First, argument candidates are generated from 
the input constituent parse tree using the preva-
lent heuristic-based pruning algorithm in (Xue 
and Palmer, 2004). Then, each predicate-
argument pair is converted to a flat feature 
structure by which the similarity between two 
instances can be easily measured. Finally we 
employ the Support Vector Machines (SVM) 
classifier to identify and classify the arguments. 
It is noteworthy that we use the same basic 
features, but different combined features for 
the identification and classification of argu-
ments. We present the result comparison be-
tween using gold-standard parsing and auto-
matic parsing, and also offer an analysis of the 
contribution of the combined features.  
To evaluate the proposed method and com-
pare it with others, we use the most commonly 
used corpus in Chinese SRL, Chinese Proposi-
tion Bank (CPB) version 1.0, as the dataset. 
The CPB corpus contains 760 documents, 
10,364 sentences, 37,183 target predicates and 
88,134 arguments. In this paper, we focus on 
six main types of semantic roles: Arg0, Arg1, 
Arg2, ArgM-ADV, ArgM-LOC and ArgM-
TMP. The number of semantic roles of the six 
types accounted for 95% of all the semantic 
roles in CPB. For SRC, we use the one-versus-
all approach, in which six SVMs will be 
trained to separate each semantic type from the 
remaining types. We divide the corpus into 
three parts: the first 99 documents 
(chtb_001.fid to chtb_099.fid) serve as the test 
data, the last 32 documents (chtb_900.fid to 
chtb_931.fid) serve as the development data 
and the left 629 documents (chtb_100.fid to 
chtb_899.fid) serve as the training data.  
We use the SVM-Light Toolkit version 6.02 
(Joachims, 1999) for the implementation of 
SVM, and use the Stanford Parser version 1.6 
(Levy and Manning, 2003) as the constituent 
parser and the constituent-to-dependency con-
verter. In classifications, we employ the linear 
kernel for SVM and set the regularization pa-
rameter to the default value which is the recip-
rocal of the average Euclidean norm of training 
data. The performance metrics are: accuracy 
(A), precision (P), recall (R) and F-score (F). 
5.2 Combined Feature Selection 
First, we select the combined features for clas-
sifications of SRI and SRC using the method 
described in Section 4 on the training data with 
gold-standard parse trees. Due to the limit of 
this paper, we only list the top-10 combined 
features for SRI and SRC for the 6 different 
types, as shown in Table 1 in which each com-
bined feature is expressed by the IDs of its two 
basic features with a plus sign between them. 
Rank SRI ARG0 ARG1 ARG2 ADV LOC TMP
1 c1+c6 c1+c6 c1+c6 c1+c6 c1+c6 c5+c27 c1+c6 
2 c1+d3 c32+c30 c30+d31 c1+d1 c30+d27 c9+d17 c22+c27
3 d25+d14 c7+c6 c30+d32 c1+c7 c30+d28 c9+d13 c7+c6 
4 c4+d25 c1+c2 c5+c30 c7+c6 c1+c11 c9+c2 d26+d27
5 d25+d22 c1+c12 c30+d24 c1+c5 c24+d33 c23+c27 d26+d28
6 d25+d20 c23+c6 c30+c21 c1+c23 c30+d25 c9+c20 c23+d26
7 d25+d21 c1+c3 c5+c4 c23+c6 c24+d9 c14+c32 c5+d26 
8 d25+d18 c10+d35 c1+c10 c1+c3 c27+c2 c14+c10 d26+d31
9 d25+d19 c10+d1 c30+d10 c5+c6 c22+c2 c9+c26 d26+d32
10 d25+d35 c10+d28 c4+c6 c1+d5 c24+d13 c14+c2 c23+c6 
Table 1. Top-10 combined features for SRI and 
SRC ranked by z-score 
Table 1 shows that the commonly used 
combined features, such as 'predicate+head 
word' (c1+c6) and 'position+voice' (c4+c5) 
proposed by (Xue and Palmer, 2004) are also 
included. In particular, the 'predicate+head 
word' feature takes first place in all semantic 
669
categories except LOC, in which the combina-
tion of the new feature 'locational cue words' 
(c27) and the 'voice (c5)' feature performs the 
best. The results also show that the most fre-
quently occurred basic features in the com-
bined set are 'predicate' (c1), 'head word' (c6), 
'type of LCA and left child' (c30), 'dep-type' 
(d25) and 'POS path' (d26). These basic fea-
tures should be more discriminative when 
combined with others. Additionally, we find 
some other latent effective combined features, 
such as 'predicate subcategorization+head 
word' (c7+c6), 'predicate POS+head word' 
(c23+c6) and 'predicate+phrase type' (c1+c3), 
whose performance will be further validated 
and analyzed later in this section. It is obvious 
that the obtained combined features for SRI 
and SRC are different, and the obtained com-
bined features for each type are also different 
as our selection method is based on positive 
and negative data which are completely differ-
ent for each argument type. In SRI phase, we 
will use the combined features for all the six 
semantic types (after removing duplicates). 
Then, we evaluate the performance of SRL 
based on the top-N combined features. The 
preliminary evaluation on the development set 
suggests that the performance becomes stable 
when N exceeds 20. Therefore, we vary the 
value of N to 5, 10 and 20 in the experiments 
to evaluate the performance of combined fea-
tures. Corresponding to the three different val-
ues of N, we finally obtained 28, 60 and 114 
combined features for the SRL, respectively. 
5.3 SRL Using Gold Parses  
To illustrate each component of the method, 
we constructed 6 SRL systems using 6 differ-
ent feature sets: 'Constituent Only' (CO) - uses 
the constituent-based features, as presented in 
Section 3.1; 'Dependency Only' (DO) - uses 
the dependency-based features, as presented in 
Section 3.2; 'CD' - uses both the constituent-
based features and the dependency-based fea-
tures, but no combined features; 'CD+Top5' - 
obtained by adding the top-5 combined fea-
tures to the 'CD' system; and similarly for the 
'CD+Top10' and the 'CD+Top20' systems. And 
'CO' serves as the baseline in our experiments. 
First, we evaluate the performance of SRI 
using the held-out test set with gold-standard 
constituent parse trees. The corresponding de-
pendency parse trees are automatically gener-
ated by the constituent-to-dependency con-
verter included in the Stanford Parser. The 
testing results of the six systems on the SRI 
phase are shown in Table 2. 
System A (%) P (%) R (%) F (%)
CO 97.87 97.04 97.30 97.17
DO 92.76 92.90 84.19 88.33
CD 97.98 97.44 97.25 97.34
CD+Top5 98.12 97.56 97.58 97.57
CD+Top10 98.15 97.61 97.62 97.61
CD+Top20 98.18 97.68 97.64 97.66
Table 2. Results of SRI using gold parses 
It can be seen from Table 2 that 'CD' and 
'CD+Top20' give only slightly improvement 
over 'CO' by less than 1% point. In other words, 
feature combinations do not seem to be very 
effective for SRI. Then we label all recognized 
constituents in the SRI phase with one of the 
six semantic role types. Table 3 displays the F-
score of each semantic type and the overall 
SRC on the test set with gold-standard parses. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 92.40 90.57 59.98 96.25 86.80 98.14 91.23 
DO 90.70 88.22 56.95 94.54 81.23 97.37 89.14 
CD 92.85 91.29 63.35 96.55 87.55 98.32 91.86 
CD+Top5 93.96 92.79 73.48 97.13 88.63 98.31 93.22*1
CD+Top10 94.15 93.23 74.18 97.42 87.17 98.57 93.41*
CD+Top20 94.10 93.19 75.13 97.23 88.05 98.48 93.46*
Table 3. Results of SRC using gold parses  
Table 3 shows that the proposed method 
performs much better in SRC. It improves the 
constituent-based method by more than 2% in 
SRC. The effectiveness of combined features 
can also be clearly seen because the overall F-
scores of the three systems using combined 
features all exceed 93%, significant greater 
than the systems using singular features. The 
improvement is noticeable for all semantic role 
types except the 'TMP' type. It means that the 
dependency parses cannot provide additional 
information to the labeling of this type. The 
results of Table 2 and Table 3 together show 
                                                 
1 The F-score value with an asterisk (*) indicates 
that there is a statistically significant difference 
between this system and the baseline ('CO') using 
the chi-square test (p<0.05). 
670
that our method using combined features can 
effectively improve the performance of SRL 
on the SRC phases, when using gold parses.  
5.4 SRL Using Automatic Parses  
To measure the performance of the algorithm 
in practical conditions, we replicate the above 
experiments using Stanford Parser on the raw 
texts of the test set, without segmentation or 
POS tagging. The dependency parses are also 
generated from the automatic constituent 
parses, as described in Section 5.3. The results 
are shown in Table 4. 
System A (%) P (%) R (%) F (%)
CO 71.54 68.72 70.62 69.66
DO 68.86 65.06 60.68 62.79
CD 73.53 70.63 72.75 71.67*
CD+Top5  73.62 70.69 72.98 71.82*
CD+Top10 73.65 70.71 73.08 71.88*
CD+Top20 73.67 70.70 73.16 71.91*
Table 4. Results of SRI using automatic parses 
Table 4 shows that the proposed method is 
also effective when using automatic parses 
despite the dramatic decrease in F-scores in 
comparison to using gold-standard parses. The 
decline is mainly caused by the heuristic-based 
pruning strategy in which a number of real ar-
guments are pruned when using the constituent 
parses with errors. Further analysis shows that, 
in SRI using gold parses, the ratio of incor-
rectly pruned arguments to the total is less than 
2%, but the ratio jumps to 17% when using 
automatic parses. Next, on the basis of the SRI 
results, we test the performance of SRC using 
the automatic parses, as shown in Table 5. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 89.20 88.90 54.47 93.93 81.80 94.38 88.24
DO 88.79 89.32 50.21 91.27 78.26 93.86 87.63
CD 89.75 89.87 57.71 95.28 84.22 94.71 89.16*
CD+Top5 90.75 90.97 65.64 95.53 84.45 94.45 90.16*
CD+Top10 90.96 91.37 67.25 95.31 84.49 94.61 90.45*
CD+Top20 90.94 91.29 67.42 95.22 84.39 94.65 90.42*
Table 5. Results of SRC using auto parses 
Table 5 shows only a slight decline in com-
parison with the result of using gold-standard 
parses, and it maintains the same trend of per-
formance for each semantic role in the Table 3, 
which proves the validity of the proposed me-
thod when using automatic parses. Table 6 
shows the F-score of the overall SRL on both 
the gold-standard and the automatic parse data. 
System Gold Parse (F%) Auto Parse (F%)
CO 89.29 63.13 
DO 82.69 60.34 
CD 90.01 65.56* 
CD+Top5 91.47* 66.37* 
CD+Top10 91.68* 66.61* 
CD+Top20 91.76* 66.61* 
Table 6. Results of overall SRL 
Table 6 shows that the F-score of the 
'CD+Top20' surpasses that of the 'CO' system 
by more than 2% on the gold parses, and more 
than 3% on the automatic parse. In other words, 
the method using constituent and dependency 
syntactic views performs even more effective 
for the automatic parses. The last three rows of 
Table 6 shows that the top-10 combined fea-
tures perform better than the top-5 features by 
adding 32 more features, but the top-20 com-
bined features obtain similar results to the top-
10 features by adding 54 more features. It sug-
gests that only several salient combined fea-
tures can actually improve the performance.  
5.5 Combined Feature Performance 
To evaluate the performance of each combined 
feature to identify the salient combined fea-
tures for SRL, we rank the 60 combined fea-
tures used by the 'CD+Top10' system on the 
test data with gold-standard parses, according 
to the F-score improvement achieved by each 
combined feature. Here we list the top 20 of 
them which are shown in Table 7.  
Rank Feature ? F(%) Rank Feature ? F(%)
1 c1+c6 0.611 11 c10+d1 0.413
2 c1+c10 0.593 12 c5+d26 0.404
3 c4+c6 0.557 13 c24+d9 0.395
4 c9+c20 0.503 14 d25+d35 0.395
5 c23+c6 0.494 15 c30+d24 0.377
6 c1+c3 0.458 16 c9+c26 0.377
7 c9+d13 0.449 17 c10+d28 0.368
8 c14+c10 0.431 18 c30+d29 0.365
9 c1+c5 0.422 19 c30+d30 0.361
10 c24+d33 0.413 20 c7+c6 0.361
Table 7. Top-20 combined features 
As can be seen from Table 7, a half of com-
bined features are composed by constituent 
671
features only, and the other half contain at least 
one dependency-based feature. This indicates 
that dependency features can be helpful to con-
struct combined features for SRL. Through 
analyzing the performance of each combined 
features, we have obtained some new and ef-
fective combined features which were not rec-
ognized before, such as 'predicate+partial 
path' (c1+c10), 'position+head word' (c4+c6), 
'Head word POS+right sibling POS' (c9+c20). 
Observation from these combined features 
suggests that not all combined features are 
composed by two significant basic features. 
Some not significant ones, such as 'partial 
path' (c10) and 'Head word POS' (c9) can also 
produce salient combined features. 
Furthermore, we find that the relative order 
of the combined features in Table 7 is not ex-
actly consistent with their orders in Table 1. 
The inconsistency indicates that the estimation 
criteria used for combined features selection is 
not perfect. In estimation, the effect of com-
bined features is evaluated simply based on the 
distance between the positive and the negative 
dataset by considering the efficiency. But in 
practice, the effects of them are determined 
through one-by-one classification. 
5.6 Comparison to Other Work 
Finally, we compare the proposed method with 
other four representative Chinese SRL systems. 
First, the 'Xue1' system (Xue and Palmer, 2005) 
is a typical feature-based system using 9 basic 
features, 2 combined features and the Maxi-
mum Entropy (ME) classifier. Second, the 'Liu' 
system (Liu et al 2007) which uses 19 basic 
features, 10 combined features and also the 
ME classifier. Third, the 'Che' (Che, 2008) sys-
tem use a hybrid convolution tree kernel to 
directly measure the similarity between two 
constituent structures. Fourth, the 'Xue2' sys-
tem described in (Xue, 2008), which is similar 
to 'Xue1' on basic framework, but using a new 
feature set. The 'Xue2' system evaluates the 
SRL of the verbal predicates and the nominal-
ized predicates separately, and offers no con-
solidated evaluation in (Xue, 2008). So in the 
comparison, we refer to its performance on the 
verbal predicates and the nominalized predi-
cates as 'Xue21' and 'Xue22'. 
All the four systems mentioned above use 
the constituent as the labeling unit and use the 
CPB corpus as the data set, the same as our 
method. And we use the same training and test 
data splits as in the 'Xue1' and 'Che' systems. 
Table 8 shows the comparison results in terms 
of F-score on both gold parses and auto parses.  
System Gold Parse (F%) Auto Parse (F%)
Xue22 69.6 57.3 
Xue1 91.3 61.3 
Liu 91.31 ? 
Che 91.67 65.42 
Ours 91.76 66.61 
Xue21 92.0 66.8 
Table 8. Comparison to other work 
Table 8 shows that our method performs 
better than the 'Xue1', 'Liu' and 'Che' systems 
on both gold parses and automatic parses. It is 
only slightly worse than the 'Xue21', namely the 
verbal predicates part of the 'Xue2' system. But 
for the other part of the 'Xue2' system for the 
nominalized predicates, namely the 'Xue22', our 
method performs much better than it. The re-
sults further verify the validity of the method. 
6 Conclusions 
This paper presents a novel feature-based SRL 
approach for Chinese. Compared to the tradi-
tional feature-based methods, the method can 
effectively integrate the constituent and the 
dependency syntactic views at the feature level. 
The method provides an effective way to con-
nect two syntactic views by a statistical selec-
tion method of combined features to substan-
tially improve the feature-based SRL method. 
The complexity of the method will not increase 
significantly compared to the method using 
one syntactic view as we use a constituent-to-
dependency conversion rather than additional 
dependency parsing. The effectiveness of the 
method has been proven by the experiments on 
CPB using SVM classifier with linear kernel.  
 
Acknowledgments 
This work is supported by the Key Program of 
National Natural Science Foundation of China 
under Grant No. 60736014, the Key Project of 
the National High Technology Research and 
Development Program of China under Grant 
No. 2006AA010108, and the Hong Kong Poly-
technic University under Grant No. G-U297 
and G-U596.  
672
References  
Collins F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. 
Proceedings of Coling-ACL-1998. 
Stephen A. Boxwell, Dennis Mehay, and Chris 
Brew. 2009. Brutus: A Semantic Role Labeling 
System Incorporating CCG, CFG, and Depend-
ency Features. Proceedings of ACL-2009. 
Wanxiang Che. 2008. Kernel-based Semantic Role 
Labeling. Ph.D. Thesis. Harbin Institute of 
Technology, Harbin, China. 
John Chen and Owen Rambow. 2003. Use of Deep 
Linguistic Features for the Recognition and La-
beling of Semantic Arguments. Proceedings of 
EMNLP-2003. 
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hier-
archical Feature Selection Strategy. Proceedings 
of EMNLP-2008. 
Ronald A. Fisher. 1938. The Statistical Utilization 
of Multiple Measurements. Annals of Eugenics, 
8:376-386. 
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic Labeling of Semantic Roles. Computa-
tional Linguistics, 28(3):245-288. 
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Syntactic Parsing for Predicate Argu-
ment Recognition. Proceedings of ACL-2002. 
Kadri Hacioglu. 2004. Semantic Role Labeling 
Using Dependency Trees. Proceedings of COL-
ING-2004. 
Jan Hajic, Massimiliano Ciaramita, Richard Jo-
hansson, et al The CoNLL-2009 Shared Task: 
Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL-2009. 
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Richard Johansson and Pierre Nugues. 2008. De-
pendency-based Semantic Role Labeling of 
PropBank. Proceedings of EMNLP-2008. 
Roger Levy and Christopher D. Manning. 2003. Is 
it harder to parse Chinese, or the Chinese Tree-
bank. Proceedings of ACL-2003. 
Huaijun Liu, Wanxiang Che, and Ting Liu. 2007. 
Feature Engineering for Chinese Semantic Role 
Labeling. Journal of Chinese Information Proc-
essing, 21(2):79-85. 
Marie-Catherine de Marneffe, Bill MacCartney, 
and Christopher D. Manning. 2006. Generating 
Typed Dependency Parses from Phrase Structure 
Parses. Proceedings of LREC-2006. 
Truc-Vien T. Nguyen, Alessandro Moschitti, and 
Giuseppe Riccardi. 2009. Convolution Kernels 
on Constituent, Dependency and Sequential 
Structures for Relation Extraction. Proceedings 
of EMNLP-2009. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguis-
tics, 31(1):71-106 
Sameer Pradhan, Wayne Waed, Kadri Haciolgu, 
and James H. Martin. 2004. Shallow Semantic 
Parsing using Support Vector Machines. Pro-
ceedings of HLT/NAACL-2004 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2005. 
Semantic Role Labeling Using Different Syntac-
tic Views. Proceedings of ACL-2005. 
Sameer Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards Robust Semantic Role Label-
ing. Computational Linguistics, 34(2): 289-310. 
Vasin Punyakanok, Dan Roth, Wentau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic 
Role Labeling. Proceedings of IJCAI-2005. 
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, 
and Pere R. Comas. 2007. Combination Strate-
gies for Semantic Role Labeling. Journal of 
Artificial Intelligence Research, 29:105-151. 
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. 2005. Joint learning improves 
semantic role labeling. Proceedings of ACL-
2005. 
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing Features for Semantic Role Labeling. Pro-
ceedings of EMNLP-2004. 
Nianwen Xue and Martha Palmer. 2005 Automatic 
semantic role labeling for Chinese verbs. Pro-
ceedings of IJCAI-2005. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009. 
Semantic Dependency Parsing of NomBank and 
PropBank: An Efficient Integrated Approach via 
a Large-scale Feature Selection. Proceedings of 
EMNLP-2009. 
673
Coling 2010: Poster Volume, pages 919?927,
Beijing, August 2010
A Study on Position Information in Document Summarization 
You Ouyang       Wenjie Li       Qin Lu       Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University 
{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hk 
Abstract 
Position information has been proved to 
be very effective in document 
summarization, especially in generic 
summarization. Existing approaches 
mostly consider the information of 
sentence positions in a document, based 
on a sentence position hypothesis that 
the importance of a sentence decreases 
with its distance from the beginning of 
the document. In this paper, we consider 
another kind of position information, i.e., 
the word position information, which is 
based on the ordinal positions of word 
appearances instead of sentence 
positions. An extractive summarization 
model is proposed to provide an 
evaluation framework for the position 
information. The resulting systems are 
evaluated on various data sets to 
demonstrate the effectiveness of the 
position information in different 
summarization tasks. Experimental 
results show that word position 
information is more effective and 
adaptive than sentence position 
information. 
1 Introduction 
Position information has been frequently used in 
document summarization. It springs from 
human?s tendency of writing sentences of 
greater topic centrality at particular positions in 
a document. For example, in newswire 
documents, topic sentences are usually written 
earlier. A sentence position hypothesis is then 
given as: the first sentence in a document is the 
most important and the importance decreases as 
the sentence gets further away from the 
beginning. Based on this sentence position 
hypothesis, sentence position features are 
defined by the ordinal position of sentences. 
These position features have been proved to be 
very effective in generic document 
summarization. In more recent summarization 
tasks, such as query-focused and update 
summarization tasks, position features are also 
widely used.  
Although in these tasks position features may 
be used in different ways, they are all based on 
the sentence position hypothesis. So we regard 
them as providing the sentence position 
information. In this paper, we study a new kind 
of position information, i.e., the word position 
information. The motivation of word position 
information comes from the idea of assigning 
different importance to multiple appearances of 
one word in a document.  
As to many language models such as the bag-
of-words model, it is well acknowledged that a 
word which appears more frequently is usually 
more important. If we take a closer look at all 
the appearances of one word, we can view this 
as a process that the different appearances of the 
same word raise the importance of each other. 
Now let?s also take the order of the appearances 
into account. When reading a document, we can 
view it as a word token stream from the first 
token to the last. When a new token is read, we 
attach more importance to previous tokens that 
have the same lemma because they are just 
repeated by the new token. Inspired by this, we 
postulate a word position hypothesis here: for 
all the appearances of a fixed word, the 
importance of each appearance depends on all 
its following appearances. Therefore, the first 
appearance of a word is the most important and 
the importance decreases with the ordinal 
919
positions of the appearances. Then, a novel kind 
of position features can be defined for the word 
appearances based on their ordinal positions. 
We believe that these word position features 
have some advantages when compared to 
traditional sentence position features. According 
to the sentence position hypothesis, sentence 
position features generally prefer earlier 
sentences in a document. As to the word 
position features that attempt to differentiate 
word appearances instead of sentences, a 
sentence which is not the first one in the 
document may still not be penalized as long as 
its words do not appear in previous sentences. 
Therefore, word position features are able to 
discover topic sentences in deep positions of the 
document. On the other hand, the assertion that 
the first sentence is always the most important is 
not true in actual data. It depends on the writing 
style indeed. For example, some authors may 
like to write some background sentences before 
topic sentences. In conclusion, we can expect 
word position features  to be more adaptive to 
documents with different structures.  
In the study of this paper, we define several 
word position features based on the ordinal 
positions of word appearances. We also develop 
a word-based summarization system to evaluate 
the effectiveness of the proposed word position 
features on a series of summarization data sets. 
The main contributions of our work are: 
(1) representation of word position information, 
which is a new kind of position information in 
document summarization area. 
(2) empirical results on various data sets that 
demonstrate the impact of position information 
in different summarization tasks. 
2 Related Work 
The use of position information in document 
summarization has a long history. In the seminal 
work by (Luhn, 1958), position information was 
already considered as a good indicator of 
significant sentences. In (Edmundson, 1969), a 
location method was proposed that assigns 
positive weights to the sentences to their ordinal 
positions in the document. Position information 
has since been adopted by many successful 
summarization systems, usually in the form of 
sentence position features. For example, Radev 
et al (2004) developed a feature-based system 
MEAD based on word frequencies and sentence 
positions. The position feature was defined as a 
descending function of the sentence position. 
The MEAD system performed very well in the 
generic multi-document summarization task of 
the DUC 2004 competition. Later, position 
information is also applied to more 
summarization tasks. For example, in query-
focused task, sentence position features are 
widely used in learning-based summarization 
systems as a component feature for calculating 
the composite sentence score (Ouyang et al 
2007; Toutanova et al 2007). However, the 
effect of position features alone was not studied 
in these works.  
There were also studies aimed at analyzing 
and explaining the effectiveness of position 
information. Lin and Hovy (1997) provided an 
empirical validation on the sentence position 
hypothesis. For each position, the sentence 
position yield was defined as the average value 
of the significance of the sentences with the 
fixed position. It was observed that the average 
significance at earlier positions was indeed 
larger. Nenkova (2005) did a conclusive 
overview on the DUC 2001-2004 evaluation 
results. It was reported that position information 
is very effective in generic summarization. In 
generic single-document summarization, a lead-
based baseline that simply takes the leading 
sentences as the summary can outperform most 
submitted summarization system in DUC 2001 
and 2002. As in multi-document summarization, 
the position-based baseline system is 
competitive in generating short summaries but 
not in longer summaries. Schilder and 
Kondadadi (2008) analyzed the effectiveness of 
the features that are used in their learning-based 
sentence scoring model for query-focused 
summarization. By comparing the ROUGE-2 
results of each individual feature, it was 
reported that position-based features are less 
effective than frequency-based features. In 
(Gillick et al, 2009), the effect of position 
information in the update summarization task 
was studied. By using ROUGE to measure the 
density of valuable words at each sentence 
position, it was observed that the first sentence 
of newswire document was especially important 
for composing update summaries. They defined 
a binary sentence position feature based on the 
920
observation and the feature did improve the 
performance on the update summarization data. 
3 Methodology 
In the section, we first describe the word-based 
summarization model. The word position 
features are then defined and incorporated into 
the summarization model. 
3.1 Basic Summarization Model 
To test the effectiveness of position information 
in document summarization, we first propose a 
word-based summarization model for applying 
the position information. The system follows a 
typical extractive style that constructs the target 
summary by selecting the most salient sentences.  
Under the bag-of-words model, the 
probability of a word w in a document set D can 
be scaled by its frequency, i.e., p(w)=freq(w)/|D|, 
where freq(w) indicates the frequency of w in D 
and |D| indicates the total number of words in D. 
The probability of a sentence s={w1, ?, wN} is 
then calculated as the product of the word 
probabilities, i.e., p(s)=?i p(wi). Moreover, the 
probability of a summary consisting a set of 
sentences, denoted as S={s1, ?, sM}, can be 
calculated by the product of the sentence 
probabilities, i.e., p(S)=?j p(sj). To obtain the 
optimum summary, an intuitive idea is to select 
the sentences to maximize the overall summary 
probability p(S), equivalent to maximizing 
log(p(S)) = ?j?i log(p(wji)) = ?j?i (logfreq(wji)- 
log|D|) = ?j?i log freq(wji) - |S|?log |D|,  
where wji indicates the ith word in sj and |S| 
indicates the total number of words in S. As to 
practical summarization tasks, a maximum 
summary length is usually postulated. So here 
we just assume that the length of the summary 
is fixed. Then, the above optimization target is 
equivalent to maximizing ?j?i logfreq(wji). 
From the view of information theory, the sum 
can also be interpreted as a simple measure on 
the total information amount of the summary. In 
this interpretation, the information of a single 
word wji is measured by log freq(wji) and the 
summary information is the sum of the word 
information. So the optimization target can also 
be interpreted as including the most informative 
words to form the most informative summary 
given the length limit.  
In extractive summarization, summaries are 
composed by sentence selection. As to the 
above optimization target, the sentence scoring 
function for ranking the sentences should be 
calculated as the average word information, i.e., 
score(s) = ?i log freq(wi) / |s|. 
After ranking the sentences by their ranking 
scores, we can select the sentences into the 
summary by the descending order of their score 
until the length limit is reached. By this process, 
the summary with the largest  p(S) can be 
composed.  
3.2 Word Position Features 
With the above model, word position features 
are defined to represent the word position 
information and are then incorporated into the 
model. According to the motivation, the features 
are defined by the ordinal positions of word 
appearances, based on the position hypothesis 
that earlier appearances of a word are more 
informative. Formally, for the ith appearance 
among the total n appearances of a word w, four 
position features are defined based on i and n 
using different formulas as described below. 
(1) Direct proportion (DP) With the word 
position hypothesis, an intuitive idea is to regard 
the information degree of the first appearance as 
1 and the last one as 1/n, and then let the degree 
decrease linearly to the position i. So we can 
obtain the first position feature defined by the 
direct proportion function, i.e., f(i)=(n-i+1)/n. 
(2) Inverse proportion (IP). Besides the linear 
function, other functions can also be used to 
characterize the relationship between the 
position and the importance. The second 
position feature adopts another widely-used 
function, the inversed proportion function, i.e., 
f(i)=1/i. This measure is similar to the above 
one, but the information degree decreases by the 
inverse proportional function. Therefore, the 
degree decreases more quickly at smaller 
positions, which implies a stronger preference 
for leading sentences. 
(3) Geometric sequence (GS). For the third 
feature, we make an assumption that the degree 
of every appearance is the sum of the degree of 
all the following appearances, i.e., f(i) = f(i+1)+ 
f(i+2)+?+ f(n). It can be easily derived that the 
sequence also satisfies f(i) = 2?f(i-1). That is, the 
information degree of each new appearance is 
921
halved. Then the feature value of the ith 
appearance can be calculated as f(i) = (1/2)i-1.  
(4) Binary function (BF). The final feature is a 
binary position feature that regards the first 
appearance as much more informative than the 
all the other appearances, i.e., f(i)=1, if i=1; ? 
else, where ? is a small positive real number.  
3.3 Incorporating the Position Features  
To incorporate the position features into the 
word-based summarization model, we use them 
to adjust the importance of the word appearance. 
For the ith appearance of a word w, its original 
importance is multiplied by the position feature 
value, i.e., log freq(w)?pos(w, i), where pos(w, i) 
is calculated by one of the four position features 
introduced above. By this, the position feature is 
also incorporated into the sentence scores, i.e., 
score?(s) = ?i [log freq(wi) ? pos(wi)] / |s| 
3.4 Sentence Position Features 
In our study, another type of position features, 
which model sentence position information, is 
defined for comparison with the word position 
features. The sentence position features are also 
defined by the above four formulas. However, 
for each appearance, the definition of i and n in 
the formulas are changed to the ordinal position 
of the sentence that contains this appearance 
and the total number of sentences in the 
document respectively. In fact, the effects of the 
features defined in this way are equivalent to 
traditional sentence position features. Since i 
and n are now defined by sentence positions, the 
feature values of the word tokens in the same 
sentence s are all equal. Denote it by pos(s), and 
the sentence score with the position feature can 
be written as  
score?(s) = ( ?w in slogfreq(w) ? pos(s))/|s|  
= pos(s)?(? logw in s freq(w)/|s|), 
which can just be viewed as the product of the 
original score and a sentence position feature. 
3.5 Discussion 
By using the four functions to measure word or 
sentence position information, we can generate 
a total of eight position features. Among the 
four functions, the importance drops fastest 
under the binary function and the order is BF > 
GS > IP > DP. Therefore, the features based on 
the binary function are the most biased to the 
leading sentences in the document and the 
features based on the direct proportion function 
are the least. On the other hand, as mentioned in 
the introduction, sentence-based features have 
larger preferences for leading sentences than 
word-based position features.  
An example is given below to illustrate the 
difference between word and sentence position 
features. This is a document from DUC 2001. 
1. GENERAL ACCIDENT, the leading British 
insurer, said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as 
much as Dollars 40m.'  
2. Lord Airlie, the chairman who was 
addressing an extraordinary shareholders' 
meeting, said: 'On the basis of emerging 
information, General Accident advise that the 
losses to their US operations arising from 
Hurricane Andrew, which struck Florida and 
Louisiana, might in total reach the level at 
which external catastrophe reinsurance covers 
would become exposed'.  
3. What this means is that GA is able to pass on 
its losses to external reinsurers once a certain 
claims threshold has been breached.  
4. It believes this threshold may be breached in 
respect of Hurricane Andrew claims.  
5. However, if this happens, it would suffer a 
post-tax loss of Dollars 40m (Pounds 20m).  
6. Mr Nelson Robertson, GA's chief general 
manager, explained later that the company has a  
1/2 per cent share of the Florida market.  
7. It has a branch in Orlando.  
8. The company's loss adjusters are in the area 
trying to estimate the losses.  
9. Their guess is that losses to be faced by all 
insurers may total more than Dollars 8bn.  
10. Not all damaged property in the area is 
insured and there have been estimates that the 
storm caused more than Dollars 20bn of 
damage.  
11. However, other insurers have estimated that 
losses could be as low as Dollars 1bn in total. 
12 Mr Robertson said: 'No one knows at this 
time what the exact loss is'. 
For the word ?threshold? which appears 
twice in the document, its original importance is 
log(2), for the appearance of ?threshold? in the 
4th sentence, the modified score based on word 
position feature with the direct proportion 
function is 1/2?log(2). In contrast, the score 
based on sentence position feature with the 
922
same function is 9/12?log(2), which is larger. 
For the appearance of the word ?estimate? in the 
8th sentence, its original importance is log(3) 
(the three boldfaced tokens are regarded as one 
word with stemming). The word-based and 
sentence-based scores are log(3) and 5/12?log(3) 
respectively. So its importance is larger under 
word position feature. Therefore, the system 
with word position features may prefer the 8th 
sentence that is in deeper positions but the 
system with sentence position feature may 
prefer the 4th sentence. As for this document, the 
top 5 sentences selected by sentence position 
feature are {1, 4, 3, 5, 2} and the those selected 
by the word position features are {1, 8, 3, 6, 9}. 
This clearly demonstrates the difference 
between the position features. 
4 Experimental Results 
4.1 Experiment Settings 
We conduct the experiments on the data sets 
from the Document Understanding Conference 
(DUC) run by NIST. The DUC competition 
started at year 2001 and has successfully 
evaluated various summarization tasks up to 
now. In the experiments, we evaluate the 
effectiveness of position information on several 
DUC data sets that involve various 
summarization tasks. One of the evaluation 
criteria used in DUC, the automatic 
summarization evaluation package ROUGE, is 
used to evaluate the effectiveness of the 
proposed word position features in the context 
of document summarization1. The recall scores 
of ROUGE-1 and ROUGE-2, which are based 
on unigram and bigram matching between 
system summaries and reference summaries, are 
adopted as the evaluation criteria.  
In the data sets used in the experiments, the 
original documents are all pre-processed by 
sentence segmentation, stop-word removal and 
word stemming. Based on the word-based 
summarization model, a total of nine systems 
are evaluated in the experiments, including the 
system with the original ranking model (denoted 
as None), four systems with each word position 
feature (denoted as WP) and four systems with 
each sentence position feature (denoted as SP). 
                                                 
1 We run ROUGE-1.5.5 with the parameters ?-x -m -
n 2 -2 4 -u -c 95 -p 0.5 -t 0? 
For reference, the average ROUGE scores of all 
the human summarizers and all the submitted 
systems from the official results of NIST are 
also given (denoted as Hum and NIST 
respectively).  
4.2 Redundancy Removal 
To reduce the redundancy in the generated 
summaries, we use an approach similar to the 
maximum marginal relevance (MMR) approach 
in the sentence selection process (Carbonell and 
Goldstein, 1998). In each round of the sentence 
selection, the candidate sentence is compared 
against the already-selected sentences. The 
sentence is added to the summary only if it is 
not significantly similar to any already-selected 
sentence, which is judged by the condition that 
the cosine similarity between the two sentences 
is less than 0.7. 
4.3 Generic Summarization 
In the first experiment, we use the DUC 2001 
data set for generic single-document 
summarization and the DUC 2004 data set for 
generic multi-document summarization. The 
DUC 2001 data set contains 303 document-
summary pairs; the DUC 2004 data set contains 
45 document sets, with each set consisting of 10 
documents. A summary is required for each 
document set. Here we need to adjust the 
ranking model for the multi-document task, i.e., 
the importance of a word is calculated as its 
total frequency in the whole document set 
instead of a single document. For both tasks, the 
summary length limit is 100 words. 
Table 1 and 2 below provide the average 
ROUGE-1 and ROUGE-2 scores (denoted as R-
1 and R-2) of all the systems. Moreover, we 
used paired two sample t-test to calculate the 
significance of the differences between a pair of 
word and sentence position features. The bolded 
score in the tables indicates that that score is 
significantly better than the corresponding 
paired one. For example, in Table 1, the bolded 
R-1 score of system WP DP means that it is 
significantly better than the R-1 score of system 
SP DP. Besides the ROUGE scores, two 
statistics, the number of ?first sentences 2 ? 
among the selected sentences (FS-N) and the 
                                                 
2 A ?first sentence? is the sentence at the fist position 
of a document.  
923
average position of the selected sentences (A-
SP), are also reported in the tables for analysis.  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.4473 0.1942 301 4.00 
SP DP 0.4396 0.1844 300 3.69 
WP IP 0.4543 0.2023 290 4.30 
SP IP 0.4502 0.1964 303 3.08 
WP GS 0.4544 0.2041 278 4.50 
SP GS 0.4509 0.1974 303 2.93 
WP BF 0.4544 0.2036 253 5.57 
SP BF 0.4239 0.1668 303 9.64 
None 0.4193 0.1626 265 10.06
NIST 0.4445 0.1865 - - 
Hum 0.4568 0.1740 - - 
Table 1. Results on the DUC 2001 data set  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.3728 0.0911 89 4.16 
SP DP 0.3724 0.0908 112 2.68 
WP IP 0.3756 0.0912 108 3.77 
SP IP 0.3690 0.0905 201 1.01 
WP GS 0.3751 0.0916 110 3.67 
SP GS 0.3690 0.0905 201 1.01 
WP BF 0.3740 0.0926 127 3.14 
SP BF 0.3685 0.0903 203 1 
None 0.3550 0.0745 36 10.98
NIST 0.3340 0.0686 - - 
Hum 0.4002 0.0962 - - 
Table 2. Results on the DUC 2004 data set 
 
From Table 1 and Table 2, it is observed that 
position information is indeed very effective in 
generic summarization so that all the systems 
with position features performed better than the 
system None which does not use any position 
information. Moreover, it is also clear that the 
proposed word position features consistently 
outperform the corresponding sentence position 
features. Though the gaps between the ROUGE 
scores are not large, the t-tests proved that word 
position features are significantly better on the 
DUC 2001 data set. On the other hand, the 
advantages of word position features over 
sentence position features are less significant on 
the DUC 2004 data set. One reason may be that 
the multiple documents have provided more 
candidate sentences for composing the summary. 
Thus it is possible to generate a good summary 
only from the leading sentences in the 
documents. According to Table 2, the average-
sentence-position of system SP BF is 1, which 
means that all the selected sentences are ?first 
sentences?. Even under this extreme condition, 
the performance is not much worse. 
The two statistics also show the different 
preferences of the features. Compared to word 
position features, sentence position features are 
likely to select more ?first sentences? and also 
have smaller average-sentence-positions. The 
abnormally large average-sentence-position of 
SP BF in DUC 2001 is because it does not 
differentiate all the other sentences except the 
first one. The corresponding word-position-
based system WP BF can differentiate the 
sentences since it is based on word positions, so 
its average-sentence-position is not that large. 
4.4 Query-focused Summarization 
Since year 2005, DUC has adopted query-
focused multi-document summarization tasks 
that require creating a summary from a set of 
documents to a given query. This task has been 
specified as the main evaluation task over three 
years (2005-2007). The data set of each year 
contains about 50 DUC topics, with each topic 
including 25-50 documents and a query. In this 
experiment, we adjust the calculation of the 
word importance again for the query-focused 
issue. It is changed to the total number of the 
appearances that fall into the sentences with at 
least one word in the query. Formally, given the 
query which is viewed as a set of words 
Q={w1, ?, wT}, a sentence set SQ is defined as 
the set of sentences that contain at least one wi 
in Q. Then the importance of a word w is 
calculated by its frequency in SQ. For the query-
focused task, the summary length limit is 250 
words. 
Table 3 below provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
DUC 2005-2007 data sets. The boldfaced terms 
in the tables indicate the best results in each 
column. According to the results, on query-
focused summarization, position information 
seems to be not as effective as on generic 
summarization. The systems with position 
features can not outperform the system None. In 
fact, this is reasonable due to the requirement 
specified by the pre-defined query. Given the 
query, the content of interest may be in any 
924
position of the document and thus the position 
information becomes less meaningful.  
On the other hand, we find that though the 
systems with word position features cannot 
outperform the system None, it does 
significantly outperform the systems with 
sentence position features. This is also due to 
the role of the query. Since it may refer to the 
specified content in any position of the 
documents, sentence position features are more 
likely to fail in discovering the desired 
sentences since they always prefer leading 
sentences. In contrast, word position features 
are less sensitive to this problem and thus 
perform better. Similarly, we can see that the 
direct proportion (DP), which has the least bias 
for leading sentences, has the best performance 
among the four functions. 
System 2005 2006 2007 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3791 0.0805 0.3909 0.0917 0.4158 0.1135 
SP DP 0.3727 0.0776 0.3832 0.0869 0.4118 0.1103 
WP IP 0.3772 0.0791 0.3830 0.0886 0.4106 0.1121 
SP IP 0.3618 0.0715 0.3590 0.0739 0.3909 0.1027 
WP GS 0.3767 0.0794 0.3836 0.0879 0.4109 0.1119 
SP GS 0.3616 0.0716 0.3590 0.0739 0.3909 0.1027 
WP BF 0.3740 0.0741 0.3642 0.0796 0.3962 0.1037 
SP BF 0.3647 0.0686 0.3547 0.0742 0.3852 0.1013 
NONE 0.3788 0.0791 0.3936 0.0924 0.4193 0.1140 
NIST 0.3353 0.0592 0.3707 0.0741 0.0962 0.3978 
Hum 0.4392 0.1022 0.4532 0.1101 0.4757 0.1402 
Table 3. Results on the DUC 2005 - 2007 data sets 
 
System 2008 A 2008 B 2009 A 2009 B R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3687 0.0978 0.3758 0.1036 0.3759 0.1015 0.3693 0.0922 
SP DP 0.3687 0.0971 0.3723 0.1011 0.3763 0.1031 0.3704 0.0946 
WP IP 0.3709 0.1014 0.3741 0.1058 0.3758 0.1030 0.3723 0.0906 
SP IP 0.3619 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP GS 0.3705 0.1004 0.3732 0.1048 0.3770 0.1051 0.3731 0.0917 
SP GS 0.3625 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP BF 0.3661 0.0975 0.3678 0.0992 0.3720 0.1069 0.3650 0.0936 
SP BF 0.3658 0.0965 0.3674 0.0980 0.3683 0.1043 0.3654 0.0945 
NONE 0.3697 0.0978 0.3656 0.0915 0.3653 0.0934 0.3595 0.0834 
NIST 0.3389 0.0799 0.3192 0.0676 0.3468 0.0890 0.3315 0.0761 
Hum 0.4105 0.1156 0.3948 0.1134 0.4235 0.1249 0.3901 0.1059 
Table 4. Results on the TAC 2008 - 2009 data sets 
 
4.5 Update Summarization 
Since year 2008, the DUC summarization track 
has become a part of the Text Analysis 
Conference (TAC). In the update summarization 
task, each document set is divided into two 
ordered sets A and B. The summarization target 
on set A is the same as the query-focused task in 
DUC 2005-2007. As to the set B, the target is to 
write an update summary of the documents in 
set B, under the assumption that the reader has 
already read the documents in set A. The data 
set of each year contains about 50 topics, and 
each topic includes 10 documents for set A, 10 
documents for set B and an additional query. 
For set A, we follow exactly the same method 
used in section 4.4; for set B, we make an 
additional novelty check for the sentences in B 
with the MMR approach. Each candidate 
sentence for set B is now compared to both the 
selected sentences in set B and in set A to 
925
ensure its novelty. In the update task, the 
summary length limit is 100 words.  
Table 4 above provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
TAC 2008-2009 data sets. The results on set A 
and set B are shown individually. For the task 
on set A which is almost the same as the DUC 
2005-2007 tasks, the results are also very 
similar. A small difference is that the systems 
with position features perform slightly better 
than the system None on these two data sets. 
Also, the difference between word position 
features and sentence position features becomes 
smaller. One reason may be that the shorter 
summary length increases the chance of 
generating good summaries only from the 
leading sentences. This is somewhat similar to 
the results reported in (Nenkova, 2005) that 
position information is more effective for short 
summaries. 
For the update set B, the results show that 
position information is indeed very effective. In 
the results, all the systems with position features 
significantly outperform the system None. We 
attribute the reason to the fact that we are more 
concerned with novel information when 
summarizing update set B. Therefore, the effect 
of the query is less on set B, which means that 
the effect of position information may be more 
pronounced in contrast. On the other hand, 
when comparing the position features, we can 
see that though the difference of the position 
features is quite small, word position features 
are still better in most cases.  
4.6 Discussion 
Based on the experiments, we briefly conclude 
the effectiveness of position information in 
document summarization. In different tasks, the 
effectiveness varies indeed. It depends on 
whether the given task has a preference for the 
sentences at particular positions. Generally, in 
generic summarization, the position hypothesis 
works well and thus the ordinal position 
information is effective. In this case, those 
position features that are more distinctive, such 
as GS and BF, can achieve better performances. 
In contrast, in the query-focused task that relates 
to specified content in the documents, ordinal 
position information is not so useful. Therefore, 
the more distinctive a position feature is, the 
worse performance it leads to. However, in the 
update summarization task that also involves 
queries, position information becomes effective 
again since the role of the query is less 
dominant on the update document set.   
On the other hand, by comparing the sentence 
position features and word position features on 
all the data sets, we can draw an overall 
conclusion that word position features are 
consistently more appreciated. For both generic 
tasks in which position information is effective 
and query-focused tasks in which it is not so 
effective, word position features show their 
advantages over sentence position features. This 
is because of the looser position hypothesis 
postulated by them. By avoiding arbitrarily 
regarding the leading sentences as more 
important, they are more adaptive to different 
tasks and data sets. 
5 Conclusion and Future Work 
In this paper, we proposed a novel kind of word 
position features which consider the positions of 
word appearances instead of sentence positions. 
The word position features were compared to 
sentence position features under the proposed 
sentence ranking model. From the results on a 
series of DUC data sets, we drew the conclusion 
that the word position features are more 
effective and adaptive than traditional sentence 
position features. Moreover, we also discussed 
the effectiveness of position information in 
different summarization tasks. 
In our future work, we?d like to conduct more 
detailed analysis on position information. 
Besides the ordinal positions, more kinds of 
position information can be considered to better 
model the document structures. Moreover, since 
position hypothesis is not always correct in all 
documents, we?d also like to consider a pre-
classification method, aiming at identifying the 
documents for which position information is 
more suitable. 
 
Acknowledgement The work described in 
this paper was supported by Hong Kong RGC 
Projects (PolyU5217/07E). We are grateful to 
professor Chu-Ren Huang for his insightful 
suggestions and discussions with us. 
926
References
Edmundson, H. P.. 1969. New methods in automatic 
Extracting. Journal of the ACM, volume 16, issue 
2, pp 264-285. 
Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B., 
Liu, Y., Xie, S.. 2009. The ICSI/UTD 
Summarization System at TAC 2009. Proceedings 
of Text Analysis Conference 2009.  
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for 
reordering documents and producing summaries. 
Proceedings of the 21st annual international ACM 
SIGIR conference on Research and development 
in information retrieval, pp 335-336. 
Lin, C. and Hovy, E.. 1997. Identifying Topics by 
Position. Proceedings of the fifth conference on 
Applied natural language processing 1997, pp 
283-290. 
Luhn, H. P.. 1958. The automatic creation of 
literature abstracts. IBM J. Res. Develop. 2, 2, pp 
159-165. 
Nenkova. 2005. Automatic text summarization of 
newswire: lessons learned from the document 
understanding conference. Proceedings of the 
20th National Conference on Artificial 
Intelligence, pp 1436-1441. 
Ouyang, Y., Li, S., Li, W.. 2007. Developing 
learning strategies for topic-based summarization. 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp 79-86. 
Radev, D., Jing, H., Sty?s, M. and Tam, D.. 2004. 
Centroid-based summarization of multiple 
documents. Information Processing and 
Management, volume 40, pp 919?938. 
Schilder, F., Kondadadi, R.. 2008. FastSum: fast and 
accurate query-based multi-document 
summarization. Proceedings of the 46th Annual 
Meeting of the Association for Computational 
Linguistics on Human Language Technologies, 
short paper session, pp 205-208. 
Toutanova, K. et al 2007. The PYTHY 
summarization system: Microsoft research at 
DUC 2007. Proceedings of Document 
Understanding Conference 2007.  
 
927
Coling 2010: Poster Volume, pages 1489?1497,
Beijing, August 2010
Sentence Ordering with Event-Enriched Semantics and Two-
Layered Clustering for Multi-Document News Summarization
Renxian Zhang                Wenjie Li                   Qin Lu      
Department of Computing, the Hong Kong Polytechnic University
{csrzhang,cswjli,csluqin}@comp.polyu.edu.hk
Abstract
We propose an event-enriched model to 
alleviate the semantic deficiency 
problem in the IR-style text processing 
and apply it to sentence ordering for 
multi-document news summarization.
The ordering algorithm is built on event 
and entity coherence, both locally and 
globally. To accommodate the event-
enriched model, a novel LSA-integrated 
two-layered clustering approach is 
adopted. The experimental result shows 
clear advantage of our model over 
event-agonistic models.
1 Introduction
One of the crucial steps in multi-document 
summarization (MDS) is information ordering, 
right after content selection and before sentence 
realization (Jurafsky and Martin, 2009:832?
834). Problems with this step are the culprit for 
much of the dissatisfaction with automatic 
summaries. While textual order may guide the 
ordering in single-document summarization, no 
such guidance is available for MDS ordering. 
A sensible solution is ordering sentences by 
enhancing coherence since incoherence is the 
source of disorder. Recent researches in this 
direction mostly focus on local coherence by 
studying lexical cohesion (Conroy et al, 2006) 
or entity overlap and transition (Barzilay and 
Lapata, 2008). But global coherence, i.e., 
coherence between sentence groups with the 
whole text in view, is largely unaccounted for 
and few efforts are made at levels higher than 
entity or word in measuring sentence coherence.
On the other hand, event as a high-level 
construct has proved useful in MDS content 
selection (Filatova and Hatzivassiloglou, 2004; 
Li et al, 2006). But the potential of event in 
summarization has not been fully gauged and 
few publications report using event in MDS 
information ordering. We will argue that event 
is instrumental for MDS information ordering, 
especially multi-document news summarization 
(MDNS). Ordering algorithms based on event 
and entity information outperform those based 
only on entity information.
After related works are surveyed in section 2, 
we will discuss in section 3 the problem of 
semantic deficiency in IR-based text processing, 
which motivates building event information into 
sentence representation. The details of such 
representation are provided in section 4. In 
section 5, we will explicate the ordering 
algorithms, including layered clustering and 
cluster-based ordering. The performance of the 
event-enriched model will be extensively 
evaluated in section 6. Section 7 will conclude 
the work with directions to future work.
2 Related Work
In MDS, information ordering is often realized 
on the sentence level and treated as a coherence 
enhancement task. A simple ordering criterion 
is the chronological order of the events 
represented in the sentences, which is often 
augmented with other ordering criteria such as 
lexical overlap (Conroy et al, 2006), lexical
cohesion (Barzilay et al, 2002) or syntactic 
features (Lapata 2003).
A different way to capture local coherence in 
sentence ordering is the Centering Theory (CT, 
Grosz et al 1995)-inspired entity-transition 
approach, advocated by Barzilay and Lapata 
(2005, 2008). In their entity grid model, 
syntactic roles played by entities and transitions 
between these syntactic roles underlie the 
coherence patterns between sentences and in the 
1489
whole text. An entity-parsed corpus can be used 
to train a model that prefers the sentence 
orderings that comply with the optimal entity 
transition patterns.
Another important clue to sentence ordering 
is the sentence positional information in a 
source document, or ?precedence relation?, 
which is utilized by Okazaki et al (2004) in 
combination with topical clustering.
Those works are all relevant to the current 
work because we seek ordering clues from 
chronological order, lexical cohesion, entity 
transition, and sentence precedence. But we also 
add an important member to the panoply ? event.  
Despite its intuitive and conceptual appeal, 
event is not as extensively used in 
summarization as term or entity. Filatova and 
Hatzivassiloglou (2004) use ?atomic events? as 
conceptual representations in MDS content 
selection, followed by Li et al (2006) who treat 
event terms and named entities as graph nodes 
in their PageRank algorithm. Yoshioka and 
Haraguchi (2004) report an event reference-
based approach to MDS content selection for 
Japanese articles. Although ?sentence 
reordering? is a component of their model, it 
relies merely on textual and chronological order. 
Few published works report using event 
information in MDS sentence ordering.
Our work will represent text content at two 
levels: event vectors and sentence vectors. This 
is close in spirit to Bromberg?s (2006) enriched 
LSA-coherence model, where both sentence and 
word vectors are used to compute a centroid as 
the topic of the text. 
3 Semantic Deficiency in IR-Style Text 
Processing
As automatic summarization traces its root to 
Information Retrieval (IR), it inherits the vector 
space model (VSM) of text representation,
according to which a sentence is treated as a bag 
of words or stoplist-filtered terms. The order or 
relation among the terms is ignored. For 
example,
1a) The storm killed 120,000 people in Jamaica 
and five in the Dominican Republic before moving 
west to Mexico.
1b) [Dominican, Mexico, Jamaica, Republic, five,
kill, move, people, storm, west]
1c) [Dominican Republic, Mexico, Jamaica,
people, storm]
1b) and 1c) are the term-based and entity-
based representations of 1a) respectively. They
only indicate what the sentence is about (i.e., 
some happening, probably a storm, in some 
place that affects people), but ?aboutness? is a 
far cry from informativeness. For instance, no 
message about ?people in which place, Mexico 
or Jamaica, are affected? or ?what moves to 
where? can be gleaned from 1b) although such 
message is clearly conveyed in 1a). In other 
words, the IR-style text representation is 
semantically deficient. 
We argue that a natural text, especially a 
news article, is not only about somebody or 
something. It also tells what happened to 
somebody or something in a temporal-spatial 
manner. A natural approach to meeting the 
?what happened? requirement is to introduce 
event.
4 Event-Enriched SentenceRepresentation 
In summarization, an event is an activity or 
episode associated with participants, time, place, 
and manner. Conceptually, event bridges 
sentence and term/entity and partially fills the 
semantic gap in the sentence representation.
4.1 Event Structure and Extraction
Following (Li et al 2006), we define an event E
as a structured semantic unit consisting of one 
event term Term(E) and a set of event entities 
Entity(E). In the news domain, event terms are 
typically action verbs or deverbal nouns. Light 
verbs such as ?take?, ?give?, etc. (Tan et al,
2006) are removed.
Event entities include named entities and 
high-frequency entities. Named entities denote 
people, locations, organizations, dates, etc. 
High-frequency entities are common nouns or 
NPs that frequently participate in news events. 
Filatova and Hatzivassiloglou (2004) take the 
top 10 most frequent entities and Li et al (2006)
take the entities with frequency > 10. Rather 
than using a fixed threshold, we reformulate 
?high-frequency? as relative statistics based on 
(assumed) Gaussian distribution of the entities 
and consider those with z-score > 1 as candidate 
event entities. 
Event extraction begins with shallow parsing 
and named entity recognition, analyzing each 
1490
sentence S into ordered lists of event terms {t1,
t2, ?}. Low-frequency common entities are 
removed. If a noun is decided to be an event 
term, it cannot be (the head noun of) an entity.
The next step is to identify events with event 
terms and entities. Filatova and 
Hatzivassiloglou (2003) treat events as triplets 
with two event entities sandwiching one 
connector (event term). But the number 
restriction on entities is counterintuitive and is 
dropped in our method. We first identify n + 1
Segi segmented by n event terms tj.
? t1 ? ? tj-1 ? tj ? tj+1 ? ? tn ?
Figure 1. Segments among Event Terms
For each tj, the corresponding event Ej are 
extracted by taking tj and the event entities in its 
nearest entity-containing Segp and Segq.
Ej = [tj, Entity(Segp)?Entity(Segq)]            (Eq. 1)
where p = argmax?????????????(????) ? ? and q
= argmin?????????????(????) ? ? if such p and q
exist. 1d) is the event-extracted result of 1a).
1d) {[killed, [storm, people, Jamaica, Dominican
Republic]], [moving, [people, Jamaica, Dominican
Republic, west, Mexico]]}
From this representation, it is easy to identify 
the two events in sentence 1a) led by the event 
terms ?killed? and ?moving?. Unlike the triplets 
(two named entities and one connector) in 
(Filatova and Hatzivassiloglou 2003), an event 
in our model can have an unlimited number of 
event entities, as is often the real case. 
Moreover, we can tell that the ?killing? involves
?people?, ?storm?, ?Jamaica?, etc. and the 
?moving? involves ?Jamaica?, ?Dominique 
Republic?, etc.
The shallow parsing-based approach is 
admittedly coarse-grade (e.g., ?storm? is 
missing from the ?moving? event), but the 
extracted event-enriched representations help to 
alleviate the semantic deficiency problem in IR.
4.2 Event Relations
The relations between two events include event 
term relation and event entity relation. Two 
events are similar if their event terms are similar 
and/or their event entities are similar. Such
similarities are in turn defined on the word level. 
For event terms, we first find the root verbs of 
deverbal nouns and then measure verb similarity 
by using the fine-grained relations provided by 
VerbOcean (Chklovski and Pantel, 2004), 
which has proved useful in summarization (Liu 
et al, 2007). But unlike (Liu et al, 2007), we 
count in all the verb relations except antonymy
because considering two antonymous verbs as 
similar is counterintuitive. The other four 
relations ? similarity, strength, enablement,
before ? are all considered in our measurement 
of verb similarity. If we denote the normalized 
score of two verbs on relation i as VOi(V1, V2)
with i = 1, 2, 3, 4 corresponding to the above 
four relations, the term similarity of two events
?t(E1, E2) is defined as in Eq. 2, where ? is a 
small number to suppress zeroes. ? = 0.01 if
VOi(V1, V2) = 1 and otherwise ? = 0.
?t(E1, E2) = ?t(Term(E1), Term(E2)) = 1 ?
? (1 ? ? ??(???)???),???)???)) + ????? ) (Eq. 2)
Entity similarity is measured by the shared 
entities between two events. Li et al (2006) 
define entity similarity as the number of shared 
entities, which may unfairly assign high scores 
to events with many entities in our model. So 
we decide to use the normalized result as shown 
in Eq. 3, where ?e(E1, E2) denotes the event 
entity-based similarity between events E1 and E2.
?e(E1, E2) = 
|??????(??)???????(??)|
|??????(??)???????(??)|
(Eq. 3)
?(E1, E2), the score of event similarity, is a 
linear combination of ?t(E1, E2) and ?e(E1, E2).
?(E1, E2) = ?1 ? ?t(E1, E2) + (1 ? ?1) ? ?e(E1, E2) (Eq. 4)
4.3 Statistical Evidence for News Events
In this work, we introduce events as a middle-
layer representation between words and 
sentences under the assumptions that 1) events 
are widely distributed in a text and that 2) they 
are natural clusters of salient information in a 
text. They guarantee the relevance of event to 
our task ? summaries are condensed collections 
of salient information in source documents.
In order to confirm them, we scan the whole 
dataset in our experiment, which consists of 42 
200w human extracts and 39 400w human 
extracts for the DUC 02 multi-document extract 
task. Detailed information about the dataset can 
be found in Section 6. Table 1 lists the statistics.
200w 400w
200w +
400w
Source
Docs
Entity/Sent 8.78 8.48 8.47 6.01
Entity/Word 0.34 0.33 0.33 0.30
Event/Sent 2.43 2.26 2.28 1.42
SegnSegj-1 SegjSeg0
1491
Event/Word 0.09 0.09 0.09 0.07
Sents with
events/Sents
86.9% 85.1% 84.6% 71.3%
Table 1. Statistics from DUC 02 Dataset
There are on average 1.42 events per sentence 
in the source documents, and more than 70% of 
all the sentences contain events. The high event 
density confirms our first assumption about the 
distribution of events. For the 200w+400w 
category consisting of all the human-selected 
sentences, there are on average 2.28 events per
sentence, a 60% increase from the same ratio in 
the source documents. The proportion of event-
containing sentences reaches 84.6%, 13% 
higher than that in the source documents. Such 
is evidence that events count into the extract-
worthiness of sentences, which confirms our 
second assumption about the relevance of 
events to summarization. The data also show 
higher entity density in the extracts than in the 
source documents. As entities are still reliable 
and domain-independent clues of salient content,
we will consider both event and entity in the 
following ordering algorithm.
5 MDS Sentence Ordering with Event 
and Entity Coherence
In this section, we discuss how event can 
facilitate MDS sentence ordering with layered 
clustering on the event and sentence levels and 
then how event and entity information can be 
integrated in a coherence-based algorithm to 
order sentences based on sentence clusters.
5.1 Two-layered Clustering
After sentences are represented as collections of 
events, we need to vectorize events and 
sentences to facilitate clustering and cluster-
based sentence ordering. 
For a document set, event vectorization 
begins with aggregating all the event terms and 
entities in a set of event units (eu). Given m
distinct event terms, n distinct named entities, 
and p distinct high-frequency common entities, 
the m + n + p eu?s are a concatenation of the 
event terms and entities such that eui is an event 
term for 1 ? i ? m, a named entity for m + 1 ? i
? m + n, and a high-frequency entity for m + n +
1 ? i ? m + n + p. The eu?s define the m + n + p
dimensions of an event vector in an eu-by-event 
matrix E = [eij], as shown in Figure 2.
?
?
?
?
?
?
?
??? ? ???
? ? ?
??? ? ???
? ?
????,? ? ????,?
? ?
??????,? ? ??????,??
?
?
?
?
?
?
Figure 2. eu-by-Event Matrix
We further define EntityN(Ej) and EntityH(Ej)
to be the set of named entities and set of high-
frequency entities of Ej. Then,
??(???,???)???)) 1 ? i ? m
eij =
? ??(???,?)?????????(??)
????????(??)?
m + 1 ? i ? m + n
? ??(???,?)?????????(??)
????????(??)?
m + n + 1 ? i ?
m + n + p (Eq. 5)
                     2 w1 is identical to w2
?n(w1, w2) =  1 w1 (w2) is a part of w2 (w1) or they 
are in a hypernymy / holonymy 
relationship
             0 otherwise                          (Eq. 6)
1 w1 is identical to w2
?h(w1, w2) = 0.5 w1 are w2 are synonyms
0 otherwise                       (Eq. 7)
In Eq. 5, ?t(w1, w2) is defined as in Eq. 2.
Both the entity-based ?n(w1, w2) and ?h(w1, w2)
are measured in terms of total equivalence 
(identity) and partial equivalence. For named 
entities, partial equivalence applies to structural 
subsumption (e.g., ?Britain? and ?Great Britain?) 
and hypernymy/holonymy (e.g., ?South Africa? 
and ?Zambia?). For common entities, it applies 
to synonymy (e.g., ?security? and ?safety?). 
Partial equivalence is considered because of the 
lexical variations frequently employed in 
journalist writing. The named entity scores are 
doubled because they represent the essential 
elements of a news story.
Since the events are represented as vectors, 
sentence vectorization based on events is not as 
straightforward as on entities or terms. In this 
work we propose a novel approach of two-
layered clustering for the purpose. The basic 
idea is clustering events at the first layer and 
then using event clusters as a feature to 
vectorize and cluster sentences at the second 
E1, E2, ? Eq
eu1
?
eum
?
eum+n
...
eum+n+p
1492
layer. Hard clustering of events, such as K-
means, not only results in binary values in event 
vectors and data sparseness but also is 
inappropriate. For example, if EC1 clusters 
events all with event terms similar to t* and EC2
clusters events all with event entity sets similar 
to e* (a set), what about event {t*, e*}? 
Assigning it to either EC1 or EC2 is problematic
as it is partially similar to both. So we decide to 
do soft clustering at the first layer.
A well-studied soft clustering technique is the 
Expectation-Maximization (EM) algorithm 
which iteratively estimates the unknown 
parameters in a probability mixture model. We 
assume a Gaussian mixture model for the q
event vectors V1, V2, ?, Vq, with hidden 
variables Hi, initial means Mi, priors ?i, and 
covariance matrix Ci. The E-step is to calculate 
the hidden variables ??
? for each Vt and the M-
step re-estimates the new priors ?i?, means Mi?,
and covariance matrix Ci
?. We iterate the two 
steps until the log-likelihood converges within a 
threshold = 10-6. The performance of the EM 
algorithm is sensitive to the initial means, which 
are pre-computed by a conventional K-means.
In a preliminary study, we found that the 
event vectors display pronounced sparseness. A 
solution to this problem in an effort to leverage 
the latent ?event topics? among eu?s is the 
Latent Semantic Analysis (LSA, Landauer and 
Dumais, 1997) approach. We apply LSA-style 
dimensionality reduction to the eu-by-event 
matrix E by doing Singular Value 
Decomposition (SVD). A problem is with the 
number h of the largest singular values, which 
affects the performance of dimensionality 
reduction. In this work, we adopt a utility-based 
metric to find the best h* by maximizing intra-
cluster similarity (?h) and minimizing inter-
cluster similarity (?h) corresponding to the h-
dimensionality reduction
h* = argmax? ?? ???                               (Eq. 8)
?h is defined as the mean of average cluster 
similarities measured by cosine distance and ?h
is the mean of cluster centroid similarities. 
Because the EM clustering assigns a probability 
to every event vector, we also take those 
probabilities into account when calculating ?h
and ?h.
Based on the EM clustering of events, we 
vectorize a sentence by summing up the 
probabilities of its constituent event vectors 
over all event clusters (ECs) and obtaining an 
EC-by-sentence (Sn) matrix S = [sij].
                     ?
??? ? ???
? ? ?
??? ? ???
?
Figure 3. EC-by-Sentence Matrix
sij = ? P(????????????)????? where ?????? is Er?s vector.
At the sentence layer, hard clustering is 
sufficient because we need definitive, not 
probabilistic, membership information for the 
next step ? sentence ordering. We use K-means 
for the purpose. The LSA-style dimensionality 
reduction is still in order as possible 
performance gain is expected from the 
discovery of latent EC ?topics?. The decision of 
the best dimensionality is the same as before,
except that no probabilities are included.
5.2 Coherence-Based Sentence Ordering
Our ordering algorithm is based on sentence 
clusters, which is designed on the observation
that human writers and summarizers organize 
sentences by blocks (paragraphs). Sentences 
within a block are conceptually close to each 
other and adjacent sentences cohere with each 
other. Local coherence is thus realized within 
blocks. On the other hand, blocks are not 
randomly ordered. Two blocks are put next to 
each other if their contents are close enough to 
ensure text-level coherence. So text-level, or 
global coherence is realized among blocks. 
We believe in MDNS, the block-style 
organization is a sensible strategy taken by 
human extractors to sort sentences from 
different sources. Sentence clusters are 
simulations of such blocks and our ordering 
algorithm will be based on local coherence and 
global coherence described above. 
First we have to pinpoint the leading sentence 
for an extract. Using the heuristic of time and 
textual precedence, we first generate a set of 
possible leading sentences L = {Li} as the 
intersection of the document-leading extract 
sentence set LDoc and the time-leading sentence 
set LTime. Note that |LDoc| = the number of 
documents, LTime is in fact a sentence collection 
of time-leading documents, and LDoc ? LTime ? ?.
S1, S2, ? Sn
EC1
?
ECm
1493
If L is a singleton, finding the leading 
sentence SL is trivial. If not, SL is decided to be 
the sentence in L most similar to all the other 
sentences in the extract sentence set P so that it 
qualifies as a good topic sentence.
SL = argmax???? ? ??????(?? ,??)????\{??} (Eq. 9)
where ??????(??, ??) is the similarity between S1
and S2 in terms of their event similarity ?(S1, S2)
and entity similarity ?(S1, S2). ?(S1, S2) is an 
extended version of ?(E1, E2) (Eq. 4) by 
averaging the ?t(Ei, Ej) and ?e(Ei, Ej) for all (Ei,
Ej) pairs in S1 ?S2.
?(S1, S2) = ?2 ?
? ??(??,??)?????,?????
|?????(??)??????(??)|
+
(1 ? ?2)?
? ??(??,??)?????,?????
|?????(??)??????(??)|
              (Eq. 10)
where Event(S) is the set of all events in S. Next, 
?(S1, S2) is the cosine similarity between their 
entity vectors ?????? and ?????? with entity weights 
constructed according to Eq. 6 and 7. Then,
??????(??, ??) = ?3??(S1, S2) +(1 ? ?3)??(S1, S2) (Eq. 11)
After the leading sentence is determined, we 
identify the leading cluster it belongs to and our 
local coherence-based ordering starts with this 
cluster. We adopt a greedy algorithm, which 
selects each time from the unordered sentence 
set a sentence that best coheres with the 
sentence just selected, called anchor sentence.
Matching each candidate sentence with the 
anchor sentence only in terms of ?????? would 
assume that the sentences are isolated and 
decontextualized. But the anchor sentence did 
not come from nowhere and in order to find its 
best successor, we should also seek clues from 
its source context, which is inspired by the 
?sentence precedence? by Okazaki et al (2004).
More formally, given an anchor sentence Si at 
the end of the ordered sentence list, we select 
the next best sentence Si+1 according to their 
associative similarity and substitutive 
similarity, two crucial measures invented by us.
Associative similarity SimASS(Si, Sj) measures 
how Si and Sj associate with each other in terms 
of their event and entity coherence, which 
almost is ?????????, ???. But to better capture the 
transition between entities and the flow of topic, 
we also consider a topic-continuity score tc(Si,
Sj) according to the Centering Theory. If the 
topic continuity is measured in terms of entity 
change, local coherence can be captured by the 
centering transitions (CB and CP) in adjacent 
sentences. Based on (Taboada and Wiesemann,
2009), we assign 0.2 to the Establish and 
Continue transitions, 0.1 to Smooth Shift and 
Retain, and 0 to other centering transitions.
Since tc(Si, Sj) only applies to entities, it is 
treated as a bonus affiliated to ?(Si, Sj).
??????? ??, ??? = ?4 ? ?(Si, Sj) + (1 ? ?4) ? ?(Si, Sj)
? (1 + tc(Si, Sj))                                                 (Eq. 12)
Substitutive similarity accommodates what 
we earlier emphasized about the ?source context?
of the extracted sentences by measuring to what 
degree Si and Sj resemble each other?s relevant 
source context. More formally, let LC(Si) and 
RC(Si) be the left and right source contexts of Si
respectively, and the substitutive similarity 
SimSUB(Si, Sj) is defined as follows.
??????? ??, S?? = ??????? ??, ??( ??)? +
?????????( ??), S??                                       (Eq. 13)
In this work, we simply take LC(Si) and RC(Si)
to be the left adjacent sentence and right 
adjacent sentence of Si in the source document. 
Note that tc(Si, Sj) does not apply here. In view 
of the chronological order widely accepted in 
MDS ordering, a time penalty, tp(Si, Sj), is used 
to discount the score by 0.8 if Si?s document 
date is later than Sj?s document date. Finally, Eq.
14 summarizes our intra-cluster ordering 
method in a sentence cluster SCk.
Si+1 = argmax??????\{??} ??? ? ??????? ??, ??? +
(1 ? ??) ? ??????? ??, ???? ? ??( ??, ??) (Eq. 14)
After all the sentences in the current sentence 
cluster are ordered, we move on by considering 
the similarity of sentence clusters. Given a 
processed sentence cluster SCi, the next best 
sentence cluster SCi+1 is the one that maximizes 
the cluster similarity SimCLU(SCi, SCj) among 
the set of all clusters U. Since clusters are 
collections of sentences, their similarity is the 
mean of cross-cluster pairwise sentence 
similarities, each calculated according to Eq. 14.
Eq. 15 shows how SCi+1 is computed.
SCi+1=argmax?????\{???}??????(??? , ???) (Eq. 15)
This is how we incorporate (block-style) 
global coherence into MDS sentence ordering. 
Starting from the second chosen sentence 
cluster, we choose the first sentence in the 
current cluster with reference to the last 
sentence in the previous processed cluster and 
apply Eq. 14. We continue the whole process 
until all the extract sentences are ordered.
1494
6 Evaluation
In this section, we report the experimental result 
on the DUC 02 dataset.
6.1 Data
We use the dataset of the DUC 02 
summarization track for MDS because it 
includes an extraction task for which model 
extracts are provided. For every document set, 2 
model extracts are provided each for the 200w 
and 400w length categories. We use 1 randomly 
chosen model extract per document set per 
length category as the gold standard.
We intended to use all the 59 document sets 
on DUC 02 but found that for some categories, 
both model extracts contain material from 
sections such as the title, lead, or even byline.
Those extracts are incompatible with our design 
tailored for news body extracts. Therefore we 
have to filter them and retain only those extracts 
with all units selected from the news body. As a 
result, we collect 42 200w extracts and 39 400w 
extracts as our experimental dataset.
6.2 Peer Orderings
We evaluate the role played by various key 
elements in our approach, including event, topic 
continuity, time penalty, and LSA-style 
dimensionality reduction. In addition, we 
produce a random ordering and a baseline 
ordering according to chronological and textual 
order only. Table 2 lists the 9 peer orderings to 
be evaluated, with their codes.
A Random
B Baseline (time order + textual order)
C Entity only (no LSA)
D Event only (no LSA)
E Entity + Event ? topic continuity (no LSA)
F Entity + Event ? time penalty (no LSA)
G Entity + Event (no LSA)
H Entity + Event (event clustering LSA)
I Entity + Event (event + sentence clustering LSA)
Table 2. Peer Orderings
6.3 Metrics
A popular metric used in sequence evaluation 
is Kendall?s ? (Lapata, 2006), which measures 
ordering differences in terms of the number of 
adjacent sentence inversions necessary to 
convert a test ordering to the reference ordering.
? = 4m/(n(n ? 1))             (Eq. 16)
where m is the number of inversions described 
above and n is the total number of sentences.
The second metric we use is the Average 
Continuity (AC) developed by Bollegala et al
(2006), which captures the intuition that the 
ordering quality can be estimated by the number 
of correctly arranged continuous sentences.
?? = exp( ?
???
? log( ?? + ?)????                (Eq. 17)
where k is the maximum number of continuous 
sentences, ? is a small value in case Pn = 1. Pn,
the proportion of continuous sentences of length 
n in an ordering, is defined as m/(N ? n + 1) 
where m is the number of continuous sentences 
of length n in both the test and reference 
orderings and N is the total number of sentences. 
We set k = 4 and ? = 0.01.
6.4 Result
We empirically determine all the parameters (?
i
)
and produce all the peer orderings. Table 3 lists
the result, where we also show the statistical 
significance between the full model peer
ordering ?I? and all other versions, marked by * 
(p < .05) and ** (p < .01) on a two-tailed t-test.
Peer 
Code
200w 400w
Kendall?s ? AC Kendall?s ? AC
A 0.014** 0.009** -0.019** 0.004**
B 0.387 0.151* 0.259** 0.151*
C 0.369* 0.128* 0.264* 0.156*
D 0.380 0.163 0.270* 0.158*
E 0.375* 0.156* 0.267* 0.157*
F 0.388 0.159* 0.264* 0.157*
G 0.385 0.158* 0.269* 0.162
H 0.384 0.164 0.292* 0.170
I 0.395 0.170 0.350 0.176
Table 3. Evaluation Result
Almost all versions with entity and event 
information outperform the baseline. The LSA-
style dimensionality reduction proves effective 
for our task, as the full model (Peer I) ranks first 
and significantly beats versions without event
information, topic continuity, or LSA. Applying
LSA to both event and sentence clustering is 
better than applying it only to event clustering
(Peer H), which produces unstable results and is 
sometimes outperformed by no-LSA versions
(Peer G).
Event (Peer D) proves to be more valuable 
than entity (Peer C) as the event-only versions 
outperform the entity-only version in all 
categories, which is predicable because events
1495
are high-level constructs that incorporate most 
of the document-level important entities.
When entity is used, extra bonus can be 
gained from topic continuity concerns from CT
(Peer E vs. Peer G) because the centering 
transition effectively captures the coherence 
pattern between adjacent sentences. The effect 
of the chronological order seems less clear (Peer 
F vs. P) as removing it hurts longer extracts 
rather than short extracts. Therefore
chronological clues are more valuable for 
arranging more sentences from the same source 
document.
Our ordering algorithm achieves even better 
result with long extracts because the importance 
of order and coherence grows with text length. 
Measured by Kendall?s ?, the full model 
ordering in the 400w category is significantly
better than all other orderings.
For a qualitative evaluation, we select the 
200w extract d080ae and list all the sentences in 
Figure 4. The event terms are boldfaced and the 
event entities are underlined.
Limited by space, let?s focus on the baseline
(1 2 3 4 5 6), entity-only (3 5 2 4 6 1), and full-
model versions (3 5 4 2 1 6). The news extract 
is about the acquitting of child molesters. Both 
the ?acquitting? and ?molesting? events are 
found in 1) and 3) but only the latter qualifies as
the topic sentence because it contains important 
event entities. Choosing 3) instead of 1) as the 
leading sentence shows the advantage of our 
event-enriched model over the baseline. The
same choice is made by the entity-only version 
because 3) happens to be also entity-intensive. 
In order to see the advantage of the full model 
over the entity-only model, let?s consider 2) and 
4). 2) is chosen by the entity-only model after 5) 
because of the heavy entity overlap between 5) 
a
because of the heavy entity overlap between 5) 
and 2). But semantically, 2) is not as close to 5) 
as 4) because only 4) contains entities for both 
the ?acquitting? (?juror?) and ?molesting?
(?children?) events and intuitively, 4) continues 
the main trial-acquittal event topic but 2) 
supplies only secondary information. We
examined the sentence clusters before the 
ordering and found that 3), 5), and 4) are 
clustered together only by the full model,
leading to better coherence, locally and globally.
7 Conclusion and Future Work
We set out by realizing the semantic deficiency 
of IR and propose a low-cost approach of 
building event semantics into sentence 
representation. Event extraction relies on 
shallow parsing and external knowledge sources. 
Then we propose a novel approach of two-
layered clustering to use event information,
coupled with LSA-style dimensionality
reduction. MDS sentence ordering is guided by 
local and global coherence to simulate the 
block-style writing and is realized by a greedy 
algorithm. The evaluation shows clear 
advantage of our event-enriched model over
baseline and event-agonistic models, 
quantitatively and qualitatively.
The extraction approach can be refined by 
deep parsing and rich verb (frame) semantics. In 
a follow-up project, we will expand our dataset 
and experiment with more data and incorporate 
human evaluation in comparative tasks.
Acknowledgment
The work described in this paper was partially 
supported by a grant from the HK RGC (Project 
Number: PolyU5217/07E).
1) Thursday's acquittals in the McMartin Pre-School molestation case outraged parents who said prosecutors botched it, 
while those on the defense side proclaimed a triumph of justice over hysteria and hype.
2) Originally, there were seven defendants, including Raymond Buckey's sister, Peggy Ann Buckey, and Virginia McMartin, 
the founder of the school, mother of Mrs. Buckey and grandmother of Raymond Buckey.
3) Seven jurors who spoke with reporters in a joint news conference after acquitting Raymond Buckey and his mother, 
Peggy McMartin Buckey, on 52 molestation charges Thursday said they felt some children who testified may have been 
molested _ but not at the family-run McMartin Pre-School.
4) ``The children were never allowed to say in their own words what happened to them,'' said juror John Breese.
5) Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at the 
family-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminal 
trial in history .
6) As it becomes apparent that McMartin cases will stretch out for years to come, parents and the former criminal defendants
alike are trying to resign themselves to the inevitability that the matter may be one they can never leave behind.
Figure 4. Extract sentences of d80ae, 200w
1496
References
Barzilay, R., Elhadad, N., and McKeown, K. 2002. 
Inferring Strategies for Sentence Ordering in 
Multidocument News Summarization. Journal of 
Artificial Intelligence Research, 17:35?55.
Barzilay, R., and Lapata, M. 2005. Modeling Local 
Coherence: An Entity-based Approach. In 
Proceedings of the 43rd Annual Meeting of the 
ACL, 141-148. Ann Arbor.
Barzilay, R., and Lapata, M. 2008. Modeling Local 
Coherence: An Entity-Based Approach. 
Computational Linguistics, 34:1?34.
Bollegala, D, Okazaki, N., and Ishizuka, M. 2006. A 
Bottom-up Approach to Sentence Ordering for 
Multi-document Summarization. In Proceedings 
of the 21st International Conference on 
Computational Linguistics and 44th Annual 
Meeting of the ACL, 385?392. Sydney, Australia.
Bromberg, I. 2006. Ordering Sentences According to 
Topicality. Presented at the Midwest 
Computational Linguistics Colloquium.
Chklovski, T., and Pantel, P. 2004. VerbOcean: 
Mining the Web for Fine-Grained Semantic Verb 
Relations. In Proceedings of Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-04). 11?13. Barcelona, 
Spain.
Conroy, J. M., Schlesinger, J. D., and Goldstein, J. 
2006. CLASSY Tasked Based Summarization: 
Back to Basics. In proceedings of the Document 
Understanding Conference (DUC-06).
Filatova, E., and Hatzivassiloglou, V. 2003. Domain-
independent detection, extraction, and labeling of 
atomic events. In Proceedings of RANLP, 145?
152, Borovetz, Bulgaria.
Filatova, E., and Hatzivassiloglou, V. 2004. Event-
Based Extractive Summarization. In Proceedings 
of the ACL-04, 104?111.
Grosz, B. J., Aravind K. J., and Scott W. 1995. 
Centering: A framework for Modeling the Local 
Coherence of Discourse. Computational 
Linguistics, 21(2):203?225.
Jurafsky D., and Martin, J. H. 2009. Speech and 
Language Processing, Second Edition. Upper 
Saddle River, NJ: Pearson Education International.
Landauer, T., and Dumais, S. 1997. A solution to 
Plato?s problem: The latent semantic analysis 
theory of the acquisition, induction, and 
representation of knowledge. Psychological 
Review, 104.
Lapata, M. 2003. Probabilistic Text Structuring: 
Experiments with Sentence Ordering. In 
Proceedings of the Annual Meeting of ACL, 545-
552. Sapporo, Japan.
Li, W., Wu, M., Lu, Q., Xu, W., and Yuan, C. 2006. 
Extractive Summarization Using Inter- and Intra-
Event Relevance. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL,
369?376. Sydney.
Liu, M., Li, W., Wu, M., and Lu, Q. 2007. Extractive 
Summarization Based on Event Term Clustering. 
In Proceedings of the ACL 2007 Demo and Poster
Sessions, 185?188. Prague.
Okazaki, N., Matsuo, Y., and Ishizuka, M. 2004. 
Improving Chronological Ordering by Precedence 
Relation. In Proceedings of 20th International 
Conference on Computational Linguistics 
(COLING 04), 750?756.
Taboada, M., and Wiesemann, L., Subjects and 
topics in conversation. Journal of Pragmatics
(2009), doi:10.1016/j.pragma.2009.04.009.
Tan, Y. F., Kan, M., and Cui, H. 2006. Extending 
corpus-based identification of light verb 
constructions using a supervised learning 
framework. In Proceedings of the EACL 2006 
Workshop on Multi-word-expressions in a 
multilingual context, 49?56, Trento, Italy.
Yoshioka, M., and Haraguchi, M. 2004. Multiple 
News Articles Summarization Based on Event 
Reference Information. In Working Notes of 
NTCIR-4, Tokyo.
1497
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?42,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Chasing Hypernyms in Vector Spaces with Entropy 
 
Enrico Santus 
Dept. of Chinese and Bilingual Studies 
The Hong Kong Polytechnic 
University, Hong Kong 
e.santus@connect.polyu.hk 
Alessandro Lenci 
CoLing Lab ? Dept. of  Philology, 
Literature, and Linguistics 
University of Pisa, Italy 
alessandro.lenci@ling.unipi.it 
  
Qin Lu 
Dept. of Computing 
The Hong Kong Polytechnic 
University, Hong Kong 
csluqin@comp.polyu.edu.hk 
Sabine Schulte im Walde 
Inst. for Natural Language Processing 
University of Stuttgart 
Germany 
schulte@ims.uni-stuttgart.de 
 
 
 
Abstract 
In this paper, we introduce SLQS, a new 
entropy-based measure for the unsupervised 
identification of hypernymy and its 
directionality in Distributional Semantic 
Models (DSMs). SLQS is assessed through 
two tasks: (i.) identifying the hypernym in 
hyponym-hypernym pairs, and (ii.) 
discriminating hypernymy among various 
semantic relations. In both tasks, SLQS 
outperforms other state-of-the-art measures. 
1 Introduction 
In recent years, Distributional Semantic Models 
(DSMs) have gained much attention in 
computational linguistics as unsupervised 
methods to build lexical semantic representations 
from corpus-derived co-occurrences encoded as 
distributional vectors (Sahlgren, 2006; Turney 
and Pantel, 2010). DSMs rely on the 
Distributional Hypothesis (Harris, 1954) and 
model lexical semantic similarity as a function of 
distributional similarity, which is most 
commonly measured with the vector cosine 
(Turney and Pantel, 2010). DSMs have achieved 
impressive results in tasks such as synonym 
detection, semantic categorization, etc. (Pad? and 
Lapata, 2007; Baroni and Lenci, 2010). 
One major shortcoming of current DSMs is 
that they are not able to discriminate among 
different types of semantic relations linking 
distributionally similar lexemes. For instance, the 
nearest neighbors of dog in vector spaces 
typically include hypernyms like animal, co-
hyponyms like cat, meronyms like tail, together 
with other words semantically related to dog. 
DSMs tell us how similar these words are to dog, 
but they do not give us a principled way to single 
out the items linked by a specific relation (e.g., 
hypernyms). 
Another related issue is to what extent 
distributional similarity, as currently measured 
by DSMs, is appropriate to model the semantic 
properties of a relation like hypernymy, which is 
crucial for Natural Language Processing. 
Similarity is by definition a symmetric notion (a 
is similar to b if and only if b is similar to a) and 
it can therefore naturally model symmetric 
semantic relations, such as synonymy and co-
hyponymy (Murphy, 2003). It is not clear, 
however, how this notion can also model 
hypernymy, which is asymmetric. In fact, it is 
not enough to say that animal is distributionally 
similar to dog. We must also account for the fact 
that animal is semantically broader than dog: 
every dog is an animal, but not every animal is a 
dog. 
38
In this paper, we introduce SLQS, a new 
entropy-based distributional measure that aims to 
identify hypernyms by providing a distributional 
characterization of their semantic generality. We 
assess it with two tasks: (i.) the identification of 
the broader term in hyponym-hypernym pairs 
(directionality task); (ii.) the discrimination of 
hypernymy among other semantic relations 
(detection task). Given the centrality of 
hypernymy, the relevance of the themes we 
address hardly needs any further motivation. 
Improving the ability of DSMs to identify 
hypernyms is in fact extremely important in tasks 
such as Recognizing Textual Entailment (RTE) 
and ontology learning, as well as to enhance the 
cognitive plausibility of DSMs as general models 
of the semantic lexicon. 
2 Related work 
The problem of identifying asymmetric relations 
like hypernymy has so far been addressed in 
distributional semantics only in a limited way 
(Kotlerman et al., 2010) or treated through semi-
supervised approaches, such as pattern-based 
approaches (Hearst, 1992). The few works that 
have attempted a completely unsupervised 
approach to the identification of hypernymy in 
corpora have mostly relied on some versions of 
the Distributional Inclusion Hypothesis (DIH; 
Weeds and Weir, 2003; Weeds et al., 2004), 
according to which the contexts of a narrow term 
are also shared by the broad term. 
One of the first proposed measures 
formalizing the DIH is WeedsPrec (Weeds and 
Weir, 2003; Weeds et al., 2004), which 
quantifies the weights of the features f of a 
narrow term u that are included into the set of 
features of a broad term v: 
???????????, ?? = ? ????????????? ?????????  
where Fx is the set of features of a term x, and 
wx(f) is the weight of the feature f of the term x. 
Variations of this measure have been introduced 
by Clarke (2009), Kotlerman et al. (2010) and 
Lenci and Benotto (2012). 
In this paper, we adopt a different approach, 
which is not based on DIH, but on the hypothesis 
that hypernyms are semantically more general 
than hyponyms, and therefore tend to occur in 
less informative contexts than hypernyms. 
3 SLQS: A new entropy-based measure 
DIH is grounded on an ?extensional? definition 
of the asymmetric character of hypernymy: since 
the class (i.e., extension) denoted by a hyponym 
is included in the class denoted by the hypernym, 
hyponyms are expected to occur in a subset of 
the contexts of their hypernyms. However, it is 
also possible to provide an ?intensional? 
definition of the same asymmetry. In fact, the 
typical characteristics making up the ?intension? 
(i.e., concept) expressed by a hypernym (e.g., 
move or eat for animal) are semantically more 
general than the characteristics forming the 
?intension? of its hyponyms (e.g., bark or has fur 
for dog). This corresponds to the idea that 
superordinate terms like animal are less 
informative than their hyponyms (Murphy, 2002). 
From a distributional point of view, we can 
therefore expect that the most typical linguistic 
contexts of a hypernym are less informative than 
the most typical linguistic contexts of its 
hyponyms. In fact, contexts such as bark and has 
fur are likely to co-occur with a smaller number 
of words than move and eat. Starting from this 
hypothesis and using entropy as an estimate of 
context informativeness (Shannon, 1948), we 
propose SLQS, which measures the semantic 
generality of a word by the entropy of its 
statistically most prominent contexts. 
For every term wi we identify the N most 
associated contexts c (where N is a parameter 
empirically set to 50)1. The association strength 
has been calculated with Local Mutual 
Information (LMI; Evert, 2005). For each 
selected context c, we define its entropy H(c) as: 
                                                          
1
 N=50 is the result of an optimization of the model 
against the dataset after trying the following 
suboptimal values: 5, 10, 25, 75 and 100. 
39
???? = ??????|?? ? ?????????|???
?
???
 
where p(fi|c) is the probability of the feature fi 
given the context c, obtained through the ratio 
between the frequency of <c, fi> and the total 
frequency of c. The resulting values H(c) are 
then normalized in the range 0-1 by using the 
Min-Max-Scaling (Priddy and Keller, 2005): 
Hn(c). Finally, for each term wi we calculate the 
median entropy Ewi of its N contexts: 
??? = ?????? 	???????? 
???  can be considered as a semantic generality 
index for the term wi: the higher ??? , the more 
semantically general wi is. SLQS is then defined 
as the reciprocal difference between the semantic 
generality ??? and ??? of two terms w1 and w2: 
???????, ??? = 1 ? ?????? 
According to this formula, SLQS<0, if ???>???; 
SLQS?0, if ???????; and SLQS>0, if ???<???. 
SLQS is an asymmetric measure because, by 
definition, SLQS(w1,w2)?SLQS(w2,w1) (except 
when w1 and w2 have exactly the same 
generality). Therefore, if SLQS(w1,w2)>0, w1 is 
semantically less general than w2. 
4 Experiments and evaluation 
4.1 The DSM and the dataset 
For the experiments, we used a standard 
window-based DSM recording co-occurrences 
with the nearest 2 content words to the left and 
right of each target word. Co-occurrences were 
extracted from a combination of the freely 
available ukWaC and WaCkypedia corpora (with 
1.915 billion and 820 million words, respectively) 
and weighted with LMI. 
To assess SLQS we relied on a subset of 
BLESS (Baroni and Lenci, 2011), a freely- 
available dataset that includes 200 distinct 
English concrete nouns as target concepts, 
equally divided between living and non-living 
entities (e.g. BIRD, FRUIT, etc.). For each target 
concept, BLESS contains several relata, 
connected to it through one relation, such as co-
hyponymy (COORD), hypernymy (HYPER), 
meronymy (MERO) or no-relation (RANDOM-N).2 
Since BLESS contains different numbers of 
pairs for every relation, we randomly extracted a 
subset of 1,277 pairs for each relation, where 
1,277 is the maximum number of HYPER-related 
pairs for which vectors existed in our DSM. 
4.2 Task 1: Directionality 
In this experiment we aimed at identifying the 
hypernym in the 1,277 hypernymy-related pairs 
of our dataset. Since the HYPER-related pairs in 
BLESS are in the order hyponym-hypernym (e.g. 
eagle-bird, eagle-animal, etc.), the hypernym in 
a pair (w1,w2) is correctly identified by SLQS, if 
SLQS (w1,w2) > 0. Following Weeds et al. (2004), 
we used word frequency as a baseline model. 
This baseline is grounded on the hypothesis that 
hypernyms are more frequent than hyponyms in 
corpora. Table 1 gives the evaluation results: 
 
SLQS WeedsPrec BASELINE 
POSITIVE 1111 805 844 
NEGATIVE 166 472 433 
TOTAL 1277 1277 1277 
PRECISION 87.00% 63.04% 66.09% 
Table 1. Accuracy for Task 1. 
As it can be seen in Table 1, SLQS scores a 
precision of 87% in identifying the second term 
of the test pairs as the hypernym. This result is 
particularly significant when compared to the 
one obtained by applying WeedsPrec (+23.96%). 
As it was also noticed by Geffet and Dagan 
(2005) with reference to a previous similar 
experiment performed on a different corpus 
(Weeds et al., 2004), the WeedsPrec precision in 
this task is comparable to the na?ve baseline. 
SLQS scores instead a +20.91%. 
                                                          
2
 In these experiments, we only consider the BLESS 
pairs containing a noun relatum. 
40
4.3 Task 2: Detection 
The second experiment aimed at 
HYPER test pairs from those linked by other 
types of relations in BLESS (i.e.
and RANDOM-N). To this purpose, we assume
that hypernymy is characterized by two main 
properties: (i.) the hypernym and the hyponym 
are distributionally similar (in the sense of the 
Distributional Hypothesis), and 
hyponym is semantically less general than the 
hypernym. We measured the first property with 
the vector cosine and the second one with 
After calculating SLQS for all the pairs in our 
datasets, we set to zero all the negative values, 
that is to say those in which 
SLQS ? the first term is semantically more 
general than the second one. Then,
SLQS and vector cosine by the
greater the resulting value, the greater the 
likelihood that we are considering a hypernymy
related pair, in which the first word is 
and the second word is a hypernym.
To evaluate the performance
used Average Precision (AP; Kotlerman et al., 
2010), a method derived from Information 
Retrieval that combines precision, relevance 
ranking and overall recall, returning a value that 
ranges from 0 to 1. AP=1 means that all the 
instances of a relation are in the top of the rank
whereas AP=0 means they are in the bottom
is calculated for the four relations we extracted 
from BLESS. SLQS was also compared with 
WeedsPrec and vector cosine
frequency as baseline. Table 2 shows the results
 HYPER COORD MERO
Baseline 0.40 0.51 
Cosine 0.48 0.46 
WeedsPrec 0.50 0.35 
SLQS * 
Cosine 
0.59 0.27 
Table 2. AP values for T
The AP values show the performance
tested measures on the four 
optimal result would be obtained scoring 
HYPER and 0 for the other relations
discriminating 
, MERO, COORD 
d 
(ii.) the 
SLQS. 
? according to 
 we combined 
ir product. The 
-
a hyponym 
 
 of SLQS, we 
, 
. AP 
, again using 
: 
 RANDOM 
0.38 0.17 
0.31 0.21 
0.39 0.21 
0.35 0.24 
ask 2. 
s of the 
relations. The 
1 for 
.  
The product between SLQS
gets the best performance in identifying 
(+0.09 in comparison to 
discriminating it from COORD
WeedsPrec). It also achieves
discriminating MERO (-0.04
On the other hand, it seems to get 
lower precision in discriminating
(+0.03 in comparison to WeedsPrec
reason is that unrelated pairs might also have a 
fairly high semantic generality difference, 
slightly affecting the measure
Figure 1 gives a graphic depiction of the 
performances. SLQS corresponds to the 
line in comparison to the 
borders, grey fill), the vector c
borders) and the baseline (grey fill).
Figure 1. AP values
5 Conclusions and future work
In this paper, we have proposed 
asymmetric distributional measure of semantic 
generality which is able to identify the 
term in a hypernym-hyponym pair
combined with vector cosine
hypernymy from other types of semantic 
relations. The successful performance of 
the reported experiments
hyponyms and hypernyms 
similar, but hyponyms tend to occur in more 
informative contexts than hypernyms.
shows that an ?intensional? 
hypernymy can be pursued in distributional 
terms. This opens up new 
study of semantic relations 
research, SLQS will also be tested on other 
datasets and languages. 
 and vector cosine 
HYPER 
WeedsPrec) and in 
 (-0.08 than 
 better results in 
 than WeedsPrec). 
a slightly 
 RANDOM-N 
). The likely 
?s performance. 
black 
WeedsPrec (black 
osine (grey 
 
 
 for Task 2. 
 
SLQS, a new 
broader 
 and, when 
, to discriminate 
SLQS in 
 confirms that 
are distributionally 
 SLQS 
characterization of 
possibilities for the 
in DSMs. In further 
41
References  
Baroni, Marco and Lenci, Alessandro. 2010. 
?Distributional Memory: A general framework for 
corpus-based semantics?. Computational 
Linguistics, Vol. 36 (4). 673-721. 
Baroni, Marco and Lenci, Alessandro. 2011. ?How 
we BLESSed distributional semantic 
evaluation?. Proceedings of the EMNLP 2011 
Geometrical Models for Natural Language 
Semantics (GEMS 2011) Workshop. Edinburg, UK. 
1-10. 
Clarke, Daoud. 2009. ?Context-theoretic semantics 
for natural language: An overview?. Proceedings 
of the Workshop on Geometrical Models of Natural 
Language Semantics. Athens, Greece. 112-119. 
Evert, Stefan. 2005. The Statistics of Word 
Cooccurrences. Dissertation, Stuttgart University. 
Geffet, Maayan and Dagan, Idan. 2005. ?The 
Distributional Inclusion Hypotheses and Lexical 
Entailment?. Proceedings of 43rd Annual Meeting 
of the ACL. Michigan, USA. 107-114. 
Harris, Zellig. 1954. ?Distributional structure?. Word, 
Vol. 10 (23). 146-162. 
Hearst, Marti A. 1992. ?Automatic Acquisition of 
Hyponyms from Large Text Corpora?. 
Proceedings of the 14th International Conference 
on Computational Linguistics. Nantes, France. 
539-545. 
Kotlerman, Lili, Dagan, Ido, Szpektor, Idan, and 
Zhitomirsky-Geffet, Maayan. 2010. ?Directional 
Distributional Similarity for Lexical Inference?. 
Natural Language Engineering, Vol. 16 (4). 359-
389. 
Lenci, Alessandro and Benotto, Giulia. 2012. 
?Identifying hypernyms in distributional semantic 
spaces?. SEM 2012 ? The First Joint Conference 
on Lexical and Computational Semantics. Montr?al, 
Canada. Vol. 2. 75-79. 
Murphy, Gregory L.. 2002. The Big Book of Concepts. 
The MIT Press, Cambridge, MA. 
Murphy, M. Lynne. 2003. Lexical meaning. 
Cambridge University Press, Cambridge. 
Pad?, Sebastian and Lapata, Mirella. 2007. 
?Dependency-based Construction of Semantic 
Space Models?. Computational Linguistics, Vol. 
33 (2). 161-199. 
Priddy, Kevin L. and Keller, Paul E. 2005. Artificial 
Neural Networks: An Introduction. SPIE Press -
International Society for Optical Engineering, 
October 2005. 
Sahlgren, Magnus. 2006. The Word-Space Model: 
Using distributional analysis to represent 
syntagmatic and paradigmatic relations between 
words in high-dimensional vector spaces. Ph.D. 
dissertation, Department of Linguistics, Stockholm 
University. 
Shannon, Claude E. 1948. ?A mathematical theory of 
communication?. Bell System Technical Journal, 
Vol. 27. 379-423 and 623-656. 
Turney, Peter D. and Pantel, Patrick. 2010. ?From 
Frequency to Meaning: Vector Space Models of 
Semantics?. Journal of Articial Intelligence 
Research, Vol. 37. 141-188. 
Weeds, Julie and Weir, David. 2003. ?A general 
framework for distributional similarity?. 
Proceedings of the 2003 Conference on Empirical 
Methods in Natural Language Processing. Sapporo, 
Japan. 81-88. 
Weeds, Julie, Weir, David and McCarthy, Diana. 
2004. ?Characterising measures of lexical 
distributional similarity?. Proceedings of COLING 
2004. Geneva, Switzerland.1015-1021. 
 
 
 
 
 
42
Feature-Frequency?Adaptive On-line
Training for Fast and Accurate Natural
Language Processing
Xu Sun?
Peking University
Wenjie Li??
Hong Kong Polytechnic University
Houfeng Wang?
Peking University
Qin Lu?
Hong Kong Polytechnic University
Training speed and accuracy are two major concerns of large-scale natural language processing
systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve
the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.
Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time,
which is the target of this work. To reach this target, we present a new training method, feature-
frequency?adaptive on-line training, for fast and accurate training of natural language process-
ing systems. It is based on the core idea that higher frequency features should have a learning rate
that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast
convergence rate. Experiments are conducted based on well-known benchmark tasks, including
named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These
tasks consist of three structured classification tasks and one non-structured classification task,
with binary features and real-valued features, respectively. Experimental results demonstrate
that the proposed method is faster and at the same time more accurate than existing methods,
achieving state-of-the-art scores on the tasks with different characteristics.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: xusun@pku.edu.cn.
?? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: cswjli@comp.polyu.edu.hk.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: wanghf@pku.edu.cn.
? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: csluqin@comp.polyu.edu.hk.
Submission received: 27 December 2012; revised version received: 30 May 2013; accepted for publication:
16 September 2013.
doi:10.1162/COLI a 00193
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Training speed is an important concern of natural language processing (NLP) systems.
Large-scale NLP systems are computationally expensive. In many real-world applica-
tions, we further need to optimize high-dimensional model parameters. For example,
the state-of-the-art word segmentation system uses more than 40 million features (Sun,
Wang, and Li 2012). The heavyNLPmodels together with high-dimensional parameters
lead to a challenging problem onmodel training, whichmay require week-level training
time even with fast computing machines.
Accuracy is another very important concern of NLP systems. Nevertheless, usually
it is quite difficult to build a system that has fast training speed and at the same time
has high accuracy. Typically we need to make a tradeoff between speed and accuracy,
to trade training speed for higher accuracy or vice versa. In this work, we have tried
to overcome this problem: to improve the training speed and the model accuracy at the
same time.
There are twomajor approaches for parameter training: batch and on-line. Standard
gradient descent methods are normally batch training methods, in which the gradient
computed by using all training instances is used to update the parameters of the model.
The batch training methods include, for example, steepest gradient descent, conjugate
gradient descent (CG), and quasi-Newtonmethods like limited-memory BFGS (Nocedal
and Wright 1999). The true gradient is usually the sum of the gradients from each
individual training instance. Therefore, batch gradient descent requires the training
method to go through the entire training set before updating parameters. This is why
batch training methods are typically slow.
On-line learning methods can significantly accelerate the training speed compared
with batch training methods. A representative on-line training method is the stochastic
gradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou
1998; Vishwanathan et al. 2006). The model parameters are updated more frequently
compared with batch training, and fewer passes are needed before convergence. For
large-scale data sets, on-line training methods can be much faster than batch training
methods.
However, we find that the existing on-line training methods are still not good
enough for training large-scale NLP systems?probably because those methods are
not well-tailored for NLP systems that have massive features. First, the convergence
speed of the existing on-line training methods is not fast enough. Our studies show that
the existing on-line training methods typically require more than 50 training passes
before empirical convergence, which is still slow. For large-scale NLP systems, the
training time per pass is typically long and fast convergence speed is crucial. Second,
the accuracy of the existing on-line training methods is not good enough. We want to
further improve the training accuracy. We try to deal with the two challenges at the
same time. Our goal is to develop a new training method for faster and at the same time
more accurate natural language processing.
In this article, we present a new on-line training method, adaptive on-line gradient
descent based on feature frequency information (ADF),1 for very accurate and fast
on-line training of NLP systems. Other than the high training accuracy and fast train-
ing speed, we further expect that the proposed training method has good theoretical
1 ADF source code and tools can be obtained from http://klcl.pku.edu.cn/member/sunxu/index.htm.
564
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
properties. We want to prove that the proposed method is convergent and has a fast
convergence rate.
In the proposed ADF training method, we use a learning rate vector in the on-line
updating. This learning rate vector is automatically adapted based on feature frequency
information in the training data set. Each model parameter has its own learning rate
adapted on feature frequency information. This proposal is based on the simple intu-
ition that a feature with higher frequency in the training process should have a learning
rate that decays faster. This is because a higher frequency feature is expected to be
well optimized with higher confidence. Thus, a higher frequency feature is expected to
have a lower learning rate. We systematically formalize this intuition into a theoretically
sound training algorithm, ADF.
The main contributions of this work are as follows:
r On the methodology side, we propose a general purpose on-line training
method, ADF. The ADF method is significantly more accurate than
existing on-line and batch training methods, and has faster training speed.
Moreover, theoretical analysis demonstrates that the ADF method is
convergent with a fast convergence rate.
r On the application side, for the three well-known tasks, including named
entity recognition, word segmentation, and phrase chunking, the proposed
simple method achieves equal or even better accuracy than the existing
gold-standard systems, which are complicated and use extra resources.
2. Related Work
Our main focus is on structured classification models with high dimensional features.
For structured classification, the conditional random fields model is widely used. To
illustrate that the proposed method is a general-purpose training method not limited to
a specific classification task or model, we also evaluate the proposal for non-structured
classification tasks like binary classification. For non-structured classification, the max-
imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996)
is widely used. Here, we review the conditional random fields model and the related
work of on-line training methods.
2.1 Conditional Random Fields
The conditional random field (CRF) model is a representative structured classification
model and it is well known for its high accuracy in real-world applications. The CRF
model is proposed for structured classification by solving ?the label bias problem?
(Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a global feature vector f, the probability
of a label sequence y conditioned on the observation sequence x is modeled as follows
(Lafferty, McCallum, and Pereira 2001):
P(y|x,w) =
exp {w>f (y,x)}
?
?y? exp {w>f (y? ,x)}
(1)
wherew is a parameter vector.
565
Computational Linguistics Volume 40, Number 3
Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1 . . .n,
parameter estimation is performed by maximizing the objective function,
L(w) =
n
?
i=1
logP(yi|xi,w)? R(w) (2)
The first term of this equation represents a conditional log-likelihood of training
data. The second term is a regularizer for reducing overfitting. We use an L2 prior,
R(w) = ||w||
2
2?2 . In what follows, we denote the conditional log-likelihood of each sample
as logP(yi|xi,w) as `(zi,w). The final objective function is as follows:
L(w) =
n
?
i=1
`(zi,w)?
||w||2
2?2
(3)
2.2 On-line Training
The most representative on-line training method is the SGD method (Bottou 1998;
Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). The SGD method uses a
randomly selected small subset of the training sample to approximate the gradient of
an objective function. The number of training samples used for this approximation is
called the batch size. By using a smaller batch size, one can update the parameters
more frequently and speed up the convergence. The extreme case is a batch size of 1,
and it gives the maximum frequency of updates, which we adopt in this work. In this
case, the model parameters are updated as follows:
wt+1 = wt + ?t?wtLstoch(zi,wt) (4)
where t is the update counter, ?t is the learning rate or so-called decaying rate, and
Lstoch(zi,wt) is the stochastic loss function based on a training sample zi. (More details
of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and
Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying
rate works the best for natural language processing tasks, and it is adopted in our
implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013).
Other well-known on-line training methods include perceptron training (Freund
and Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-
opment/extensions of stochastic gradient descent (e.g., the second-order stochastic
gradient descent training methods like stochastic meta descent) (Vishwanathan et al.
2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent
method requires the computation or approximation of the inverse of the Hessian matrix
of the objective function, which is typically slow, especially for heavily structured classi-
fication models. Usually the convergence speed based on number of training iterations
is moderately faster, but the time cost per iteration is slower. Thus the overall time cost
is still large.
Compared with the related work on batch and on-line training (Jacobs 1988;
Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, and
Singer 2010; McMahan and Streeter 2010), our work is fundamentally different. The
proposedADF trainingmethod is based on feature frequency adaptation, and to the best
of our knowledge there is no prior work on direct feature-frequency?adaptive on-line
566
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
training. Compared with the confidence-weighted (CW) classification method and its
variation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze
2009), the proposed method is substantially different. While the feature frequency
information is implicitly modeled via a complicated Gaussian distribution framework
in Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009),
the frequency information is explicitly modeled in our proposal via simple learning
rate adaptation. Our proposal is more straightforward in capturing feature frequency
information, and it has no need to use Gaussian distributions and KL divergence,
which are important in the CW and AROW methods. In addition, our proposal is a
probabilistic learning method for training probabilistic models such as CRFs, whereas
the CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza,
and Dredze 2009) are non-probabilistic learning methods extended from perceptron-
style approaches. Thus, the framework is different. This work is a substantial extension
of the conference version (Sun, Wang, and Li 2012). Sun, Wang, and Li (2012) focus on
the specific task of word segmentation, whereas this article focuses on the proposed
training algorithm.
3. Feature-Frequency?Adaptive On-line Learning
In traditional on-line optimization methods such as SGD, no distinction is made for
different parameters in terms of the learning rate, and this may result in slow conver-
gence of the model training. For example, in the on-line training process, suppose the
high frequency feature f1 and the low frequency feature f2 are observed in a training
sample and their corresponding parameters w1 and w2 are to be updated via the same
learning rate ?t. Suppose the high frequency feature f1 has been updated 100 times
and the low frequency feature f2 has only been updated once. Then, it is possible that
the weight w1 is already well optimized and the learning rate ?t is too aggressive for
updating w1. Updating the weight w1 with the learning rate ?t may make w1 be far
from the well-optimized value, and it will require corrections in the future updates. This
causes fluctuations in the on-line training and results in slow convergence speed. On
the other hand, it is possible that the weight w2 is poorly optimized and the same learn-
ing rate ?t is too conservative for updating w2. This also results in slow convergence
speed.
To solve this problem, we propose ADF. In spite of the high accuracy and fast
convergence speed, the proposed method is easy to implement. The proposed method
with feature-frequency?adaptive learning rates can be seen as a learning method with
specific diagonal approximation of the Hessian information based on assumptions of
feature frequency information. In this approximation, the diagonal elements of the
diagonal matrix correspond to the feature-frequency?adaptive learning rates. Accord-
ing to the aforementioned example and analysis, it assumes that a feature with higher
frequency in the training process should have a learning rate that decays faster.
3.1 Algorithm
In the proposed ADF method, we try to use more refined learning rates than traditional
SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend
the learning rate scalar to a learning rate vector, which has the same dimension as the
weight vector w. The learning rate vector is automatically adapted based on feature
567
Computational Linguistics Volume 40, Number 3
frequency information. By doing so, each weight has its own learning rate, and we will
show that this can significantly improve the convergence speed of on-line learning.
In the ADF learning method, the update formula is:
wt+1 = wt +?t ? gt (5)
The update term gt is the gradient term of a randomly sampled instance:
gt = ?wtLstoch(zi,wt) = ?wt
{
`(zi,wt)?
||wt||2
2n?2
}
In addition, ?t ? R
f
+ is a positive vector-valued learning rate and ? denotes the
component-wise (Hadamard) product of two vectors.
The learning rate vector ?t is automatically adapted based on feature frequency
information in the updating process. Intuitively, a feature with higher frequency in the
training process has a learning rate that decays faster. This is because a weight with
higher frequency is expected to be more adequately trained, hence a lower learning
rate is preferable for fast convergence. We assume that a high frequency feature should
have a lower learning rate, and a low frequency feature should have a relatively higher
learning rate in the training process.We systematically formalize this idea into a theoret-
ically sound training algorithm. The proposedmethodwith feature-frequency?adaptive
learning rates can be seen as a learning method with specific diagonal approximation
of the inverse of the Hessian matrix based on feature frequency information.
Given awindow size q (number of samples in awindow), we use a vector v to record
the feature frequency. The kth entry vk corresponds to the frequency of the feature k in
this window. Given a feature k, we use u to record the normalized frequency:
u = vk/q
For each feature, an adaptation factor ? is calculated based on the normalized frequency
information, as follows:
? = ?? u(?? ?)
where ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1. Intu-
itively, the upper bound ? corresponds to the adaptation factor of the lowest frequency
features, and the lower bound ? corresponds to the adaptation factor of the highest
frequency features. The optimal values of ? and ? can be tuned based on specific real-
world tasks, for example, via cross-validation on the training data or using held-out
data. In practice, via cross-validation on the training data of different tasks, we found
that the following setting is sufficient to produce adequate performance for most of the
real-world natural language processing tasks: ? around 0.995, and ? around 0.6. This
indicates that the feature frequency information has similar characteristics across many
different natural language processing tasks.
As we can see, a feature with higher frequency corresponds to a smaller scalar via
linear approximation. Finally, the learning rate is updated as follows:
?k ? ??k
568
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
ADF learning algorithm
1: procedure ADF(Z,w, q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample zi at random from the data set Z
5: . v ? UPDATEFEATUREFREQ(v, zi)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATELEARNRATE(?, v)
8: . . v ? 0
9: . g ??wLstoch(zi,w)
10: . w ? w +? ? g
11: . t? t+ 1
12: returnw
13:
14: procedure UPDATEFEATUREFREQ(v, zi)
15: for k ? features used in sample zi
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATELEARNRATE(?, v)
20: for k ? all features
21: . u? vk/q
22: . ?? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1
The proposed ADF on-line learning algorithm. In the algorithm, Z is the training data set; q, c, ?,
and ? are hyper-parameters; q is an integer representing window size; c is for initializing the
learning rates; and ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1.
With this setting, different features correspond to different adaptation factors based
on feature frequency information. Our ADF algorithm is summarized in Figure 1.
The ADF training method is efficient because the only additional computation
(compared with traditional SGD) is the derivation of the learning rates, which is simple
and efficient. As we know, the regularization of SGD can perform efficiently via the opti-
mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007). Similarly,
the derivation of ?t can also perform efficiently via the optimization based on sparse
features. Note that although binary features are common in natural language processing
tasks, the ADF algorithm is not limited to binary features and it can be applied to real-
valued features.
3.2 Convergence Analysis
We want to show that the proposed ADF learning algorithm has good convergence
properties. There are two steps in the convergence analysis. First, we show that the
ADF update rule is a contraction mapping. Then, we show that the ADF training is
asymptotically convergent, and with a fast convergence rate.
To simplify the discussion, our convergence analysis is based on the convex loss
function of traditional classification or regression problems:
L(w) =
n
?
i=1
`(xi, yi,w ? f i)?
||w||2
2?2
569
Computational Linguistics Volume 40, Number 3
where f i is the feature vector generated from the training sample (xi, yi). L(w) is a func-
tion in w ? f i, such as 12 (yi ?w ? f i)2 for regression or log[1+ exp(?yiw ? f i)] for binary
classification.
To make convergence analysis of the proposed ADF training algorithm, we need to
introduce several mathematical definitions. First, we introduce Lipschitz continuity:
Definition 1 (Lipschitz continuity)
A function F : X ? R is Lipschitz continuous with the degree of D if |F(x)? F(y)| ?
D|x? y| for ?x, y ? X . X can be multi-dimensional space, and |x? y| is the distance
between the points x and y.
Based on the definition of Lipschitz continuity, we give the definition of the
Lipschitz constant ||F||Lip as follows:
Definition 2 (Lipschitz constant)
||F||Lip := inf{D where |F(x)? F(y)| ? D|x? y| for ?x, y}
In other words, the Lipschitz constant ||F||Lip is the lower bound of the continuity degree
that makes the function F Lipschitz continuous.
Further, based on the definition of Lipschitz constant, we give the definition of
contraction mapping as follows:
Definition 3 (Contraction mapping)
A function F : X ? X is a contraction mapping if its Lipschitz constant is smaller than
1: ||F||Lip < 1.
Then, we can show that the traditional SGD update is a contraction mapping.
Lemma 1 (SGD update rule is contraction mapping)
Let ? be a fixed low learning rate in SGD updating. If ? ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1,
the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-
tinuity degree 1? ?/?2.
The proof can be extended from the relatedwork on convergence analysis of parallel
SGD training (Zinkevich et al. 2010). The stochastic training process is a one-following-
one dynamic update process. In this dynamic process, if we use the same update rule F,
we havewt+1 = F(wt) andwt+2 = F(wt+1). It is only necessary to prove that the dynamic
update is a contraction mapping restricted by this one-following-one dynamic process.
That is, for the proposed ADF update rule, it is only necessary to prove it is a dynamic
contraction mapping. We formally define dynamic contraction mapping as follows.
Definition 4 (Dynamic contraction mapping)
Given a function F : X ? X , suppose the function is used in a dynamic one-following-
one process: xt+1 = F(xt) and xt+2 = F(xt+1) for ?xt ? X . Then, the function F is a
dynamic contraction mapping if ?D < 1, |xt+2 ? xt+1| ? D|xt+1 ? xt| for ?xt ? X .
We can see that a contraction mapping is also a dynamic contraction mapping, but
a dynamic contraction mapping is not necessarily a contraction mapping. We first show
570
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
that the ADF update rule with a fixed learning rate vector of different learning rates is
a dynamic contraction mapping.
Theorem 1 (ADF update rule with fixed learning rates)
Let ? be a fixed learning rate vector with different learning rates. Let ?max be the max-
imum learning rate in the learning rate vector ?: ?max := sup{?i where ?i ? ?}. Then
if ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, the ADF update rule is a dynamic contraction
mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Further, we need to prove that the ADF update rule with a decaying learning
rate vector is a dynamic contraction mapping, because the real ADF algorithm has a
decaying learning rate vector. In the decaying case, the condition that ?max ? (||x2i || ?
||?y? `(xi, yi, y?)||Lip)?1 can be easily achieved, because ? continues to decay with an
exponential decaying rate. Even if the ? is initialized with high values of learning rates,
after a number of training passes (denoted as T) ?T is guaranteed to be small enough so
that ?max := sup{?i where ?i ? ?T} and ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Without
losing generality, our convergence analysis starts from the pass T and we take ?T as ?0
in the following analysis. Thus, we can show that the ADF update rule with a decaying
learning rate vector is a dynamic contraction mapping:
Theorem 2 (ADF update rule with decaying learning rates)
Let ?t be a learning rate vector in the ADF learning algorithm, which is decaying
over the time t and with different decaying rates based on feature frequency infor-
mation. Let ?t start from a low enough learning rate vector ?0 such that ?max ?
(||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, where ?max is the maximum element in?0. Then, the ADF
update rule with decaying learning rate vector is a dynamic contraction mapping in
Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Based on the connections between ADF training and contraction mapping, we
demonstrate the convergence properties of the ADF training method. First, we prove
the convergence of the ADF training.
Theorem 3 (ADF convergence)
ADF training is asymptotically convergent.
The proof is sketched in Section 5.
Further, we analyze the convergence rate of the ADF training. When we have the
lowest learning rate?t+1 = ??t, the expectation of the obtainedwt is as follows (Murata
1998; Hsu et al. 2009):
E(wt) = w? +
t
?
m=1
(I ??0?mH(w?))(w0 ?w?)
where w? is the optimal weight vector, and H is the Hessian matrix of the objective
function. The rate of convergence is governed by the largest eigenvalue of the function
Ct =
?t
m=1(I ??0?mH(w?)). Following Murata (1998) and Hsu et al. (2009), we can
derive a bound of rate of convergence, as follows.
571
Computational Linguistics Volume 40, Number 3
Theorem 4 (ADF convergence rate)
Assume ? is the largest eigenvalue of the function Ct =
?t
m=1(I ??0?mH(w?)). For the
proposed ADF training, its convergence rate is bounded by ?, and we have
? ? exp {?0???? 1}
where ? is the minimum eigenvalue ofH(w?).
The proof is sketched in Section 5.
The convergence analysis demonstrates that the proposed method with feature-
frequency-adaptive learning rates is convergent and the bound of convergence rate
is analyzed. It demonstrates that increasing the values of ?0 and ? leads to a lower
bound of the convergence rate. Because the bound of the convergence rate is just an
up-bound rather than the actual convergence rate, we still need to conduct automatic
tuning of the hyper-parameters, including ?0 and ?, for optimal convergence rate in
practice. The ADF training method has a fast convergence rate because the feature-
frequency-adaptive schema can avoid the fluctuations on updating the weights of high
frequency features, and it can avoid the insufficient training on updating the weights of
low frequency features. In the following sections, we perform experiments to confirm
the fast convergence rate of the proposed method.
4. Evaluation
Our main focus is on training heavily structured classification models. We evaluate the
proposal on three NLP structured classification tasks: biomedical named entity recogni-
tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking. For the
structured classification tasks, the ADF training is based on the CRF model (Lafferty,
McCallum, and Pereira 2001). Further, to demonstrate that the proposed method is
not limited to structured classification tasks, we also perform experiments on a non-
structured binary classification task: sentiment-based text classification. For the non-
structured classification task, the ADF training is based on themaximum entropymodel
(Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).
4.1 Biomedical Named Entity Recognition (Structured Classification)
The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004
shared task. The task is to recognize five kinds of biomedical named entities, including
DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining
corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential
labeling task with the BIO encoding.
This data set consists of 20,546 training samples (from 2,000 MEDLINE article
abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data
are summarized in Table 1. State-of-the-art systems for this task include Settles (2004),
Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al.
(2009), and Tsuruoka, Tsujii, and Ananiadou (2009).
Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki,
et al. 2009), we use word token?based features, part-of-speech (POS) based features,
and orthography pattern?based features (prefix, uppercase/lowercase, etc.), as listed in
Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package),
the edges features usually contain only the information of yi?1 and yi, and ignore the
572
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 1
Summary of the Bio-NER data set.
#Abstracts #Sentences #Words
Train 2,000 20,546 (10/abs) 472,006 (23/sen)
Test 404 4,260 (11/abs) 96,780 (23/sen)
Table 2
Feature templates used for the Bio-NER task. wi is the current word token on position i. ti is the
POS tag on position i. oi is the orthography mode on position i. yi is the classification label on
position i. yi?1yi represents label transition. A? B represents a Cartesian product between
two sets.
Word Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
Orthography Pattern?based Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi, oioi+1, oi+1oi+2}
?{yi, yi?1yi}
information of the observation sequence (i.e., x). The major reason for this simple real-
ization of edge features in traditional CRF implementation is to reduce the dimension
of features. To improve the model accuracy, we utilize rich edge features following Sun,
Wang, and Li (2012), in which local observation information of x is combined in edge
features just like the implementation of node features. A detailed introduction to rich
edge features can be found in Sun, Wang, and Li (2012). Using the feature templates,
we extract a high dimensional feature set, which contains 5.3? 107 features in total.
Following prior studies, the evaluation metric for this task is the balanced F-score
defined as 2PR/(P+ R), where P is precision and R is recall.
4.2 Chinese Word Segmentation (Structured Classification)
Chinese word segmentation aims to automatically segment character sequences into
word sequences. Chinese word segmentation is important because it is the first step
for most Chinese language information processing systems. Our experiments are based
on the Microsoft Research data provided by The Second International Chinese Word
Segmentation Bakeoff. In this data set, there are 8.8? 104 word-types, 2.4? 106 word-
tokens, 5? 103 character-types, and 4.1? 106 character-tokens. State-of-the-art systems
for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and
Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010),
and Zhao and Kit (2011).
The feature engineering follows previous work on word segmentation (Sun, Wang,
and Li 2012). Rich edge features are used. For the classification label yi and the label
transition yi?1yi on position i, we use the feature templates as follows (Sun, Wang, and
Li 2012):
r Character unigrams located at positions i? 2, i? 1, i, i+ 1, and i+ 2.
573
Computational Linguistics Volume 40, Number 3
r Character bigrams located at positions i? 2, i? 1, i and i+ 1.
r Whether xj and xj+1 are identical, for j = i? 2, . . . , i+ 1.
r Whether xj and xj+2 are identical, for j = i? 3, . . . , i+ 1.
r The character sequence xj,i if it matches a word w ? U, with the constraint
i? 6 < j < i. The item xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected from the training data.
r The character sequence xi,k if it matches a word w ? U, with the constraint
i < k < i+ 6.
r The word bigram candidate [xj,i?1, xi,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
B represents the word bigram dictionary collected from the training data.
r The word bigram candidate [xj,i, xi+1,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
All feature templates are instantiated with values that occurred in training samples.
The extracted feature set is large, and there are 2.4? 107 features in total. Our evaluation
is based on a closed test, and we do not use extra resources. Following prior studies, the
evaluation metric for this task is the balanced F-score.
4.3 Phrase Chunking (Structured Classification)
In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs,
are identified. The phrase chunking data is extracted from the data of the CoNLL-2000
shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936
sentences, and the test set consists of 2,012 sentences. We use the feature templates
based on word n-grams and part-of-speech n-grams, and feature templates are shown
in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8? 105
features in total. State-of-the-art systems for this task include Kudo and Matsumoto
(2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al.
(2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior
studies, the evaluation metric for this task is the balanced F-score.
4.4 Sentiment Classification (Non-Structured Classification)
To demonstrate that the proposed method is not limited to structured classification, we
select a well-known sentiment classification task for evaluating the proposed method
on non-structured classification.
Table 3
Feature templates used for the phrase chunking task. wi, ti, and yi are defined as before.
Word-Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
574
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Generally, sentiment classification classifies user review text as a positive or neg-
ative opinion. This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasks
based on user reviews from Amazon.com. Each subtask is a binary sentiment clas-
sification task based on a specific topic. We use the maximum entropy model for
classification. We use the same lexical features as those used in Blitzer, Dredze, and
Pereira (2007), and the total number of features is 9.4? 105. Following prior work, the
evaluation metric is binary classification accuracy.
4.5 Experimental Setting
As for training, we perform gradient descent with the proposed ADF training method.
To compare with existing literature, we choose four popular training methods, a rep-
resentative batch training method, and three representative on-line training methods.
The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal and
Wright 1999), which is considered to be one of the best optimizers for log-linear models
like CRFs. The on-line training methods include the SGD training method, which we
introduced in Section 2.2, the structured perceptron (Perc) training method (Freund
and Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) training
method (Collins 2002). The structured perceptron method and averaged perceptron
method are non-probabilistic training methods that have very fast training speed due
to the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013). All
training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set
of features.
We also compared the ADF method with the CW method (Dredze, Crammer, and
Pereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009). The CW
and AROW methods are implemented based on the Confidence Weighted Learning
Library.2 Because the current implementation of the CW and AROW methods do not
utilize rich edge features, we removed the rich edge features in our systems to make
more fair comparisons. That is, we removed rich edge features in the CRF-ADF setting,
and this simplified method is denoted as ADF-noRich. The second-order stochastic
gradient descent training methods, including the SMD method (Vishwanathan et al.
2006) and the PSA method (Hsu et al. 2009), are not considered in our experiments
because we find those methods are quite slow when running on our data sets with high
dimensional features.
We find that the settings of q, ?, and ? in the ADF training method are not sensitive
among specific tasks and can be generally set. We simply set q = n/10 (n is the number
of training samples). It means that feature frequency information is updated 10 times
per iteration. Via cross-validation only on the training data of different tasks, we find
that the following setting is sufficient to produce adequate performance for most of
the real-world natural language processing tasks: ? around 0.995 and ? around 0.6.
This indicates that the feature frequency information has similar characteristics across
many different natural language processing tasks.
Thus, we simply use the following setting for all tasks: q = n/10, ? = 0.995, and
? = 0.6. This leaves c (the initial value of the learning rates) as the only hyper-parameter
that requires careful tuning. We perform automatic tuning for c based on the training
data via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, and
the optimal c is chosen based on the best accuracy of cross-validation. Via this automatic
2 http://webee.technion.ac.il/people/koby/code-index.html.
575
Computational Linguistics Volume 40, Number 3
tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, word
segmentation, phrase chunking, and sentiment classification tasks, respectively.
To reduce overfitting, we use an L2 Gaussian weight prior (Chen and Rosenfeld
1999) for the ADF, LBFGS, and SGD training methods. We vary the ? with different
values (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of different
tasks, and finally set ? = 5.0 for all training methods in the Bio-NER task; ? = 5.0 for
all training methods in the word segmentation task; ? = 5.0, 1.0, 1.0 for ADF, SGD,
and LBFGS in the phrase chunking task; and ? = 1.0 for all training methods in the
sentiment classification task. Experiments are performed on a computer with an Intel(R)
Xeon(R) 2.0-GHz CPU.
4.6 Structured Classification Results
4.6.1 Comparisons Based on Empirical Convergence. First, we check the experimental re-
sults of different methods on their empirical convergence state. Because the perceptron
training method (Perc) does not achieve empirical convergence even with a very large
number of training passes, we simply report its results based on a large enough number
of training passes (e.g., 200 passes). Experimental results are shown in Table 4.
As we can see, the proposed ADF method is more accurate than other training
methods, either the on-line ones or the batch one. It is a bit surprising that the ADF
method performs even more accurately than the batch training method (LBFGS). We
notice that some previous work also found that on-line training methods could have
Table 4
Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the
number of passes are decided based on empirical convergence (with score deviation of adjacent
five passes less than 0.01). For the non-convergent case, we simply report the results based on a
large enough number of training passes. As we can see, the ADF method achieves the best
accuracy with the fastest convergence speed.
Bio-NER Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 67.69 70.20 68.92 400 152,811.34
SGD (on-line) 70.91 72.69 71.79 91 76,549.21
Perc (on-line) 65.37 66.95 66.15 200 20,436.69
Avg-Perc (on-line) 68.76 72.56 70.61 37 3,928.01
ADF (proposal) 71.71 72.80 72.25 35 27,490.24
Segmentation Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 97.46 96.86 97.16 102 13,550.68
SGD (on-line) 97.58 97.11 97.34 27 6,811.15
Perc (on-line) 96.99 96.03 96.50 200 8,382.606
Avg-Perc (on-line) 97.56 97.05 97.30 16 716.87
ADF (proposal) 97.67 97.31 97.49 15 4,260.08
Chunking Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 94.57 94.09 94.33 105 797.04
SGD (on-line) 94.48 94.04 94.26 56 903.88
Perc (on-line) 93.66 93.31 93.48 200 543.51
Avg-Perc (on-line) 94.34 94.04 94.19 12 33.45
ADF (proposal) 94.66 94.38 94.52 17 282.17
576
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
better performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, and
Ananiadou 2009; Schaul, Zhang, and LeCun 2012). The ADF training method can
achieve better results probably because the feature-frequency?adaptive training schema
can produce more balanced training of features with diversified frequencies. Traditional
SGD training may over-train high frequency features and at the same time may have
insufficient training of low frequency features. The ADF training method can avoid
such problems. It will be interesting to perform further analysis in future work.
We also performed significance tests based on t-tests with a significance level of
0.05. Significance tests demonstrate that the ADF method is significantly more accurate
than the existing training methods in most of the comparisons, whether on-line or
batch. For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc,
and Avg-Perc are significant. For the word segmentation task, the differences between
ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the phrase chunking
task, the differences between ADF and Perc and Avg-Perc are significant; the differences
between ADF and LBFGS and SGD are non-significant.
Moreover, as we can see, the proposed method achieves a convergence state with
the least number of training passes, and with the least wall-clock time. In general, the
ADF method is about one order of magnitude faster than the LBFGS batch training
method and several times faster than the existing on-line training methods.
4.6.2 Comparisons with State-of-the-Art Systems. The three tasks are well-known bench-
mark tasks with standard data sets. There is a large amount of published research on
those three tasks. We compare the proposed method with the state-of-the-art systems.
The comparisons are shown in Table 5.
As we can see, our system is competitive with the best systems for the Bio-NER,
word segmentation, and NP-chunking tasks. Many of the state-of-the-art systems use
extra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting over
Table 5
Comparing our results with some representative state-of-the-art systems.
Bio-NER Method F-score
(Okanohara et al. 2006) Semi-Markov CRF + global features 71.5
(Hsu et al. 2009) CRF + PSA(1) training 69.4
(Tsuruoka, Tsujii, and Ananiadou 2009) CRF + SGD-L1 training 71.6
Our Method CRF + ADF training 72.3
Segmentation Method F-score
(Gao et al. 2007) Semi-Markov CRF 97.2
(Sun, Zhang, et al. 2009) Latent-variable CRF 97.3
(Sun 2010) Multiple segmenters + voting 96.9
Our Method CRF + ADF training 97.5
Chunking Method F-score
(Kudo and Matsumoto 2001) Combination of multiple SVM 94.2
(Vishwanathan et al. 2006) CRF + SMD training 93.6
(Sun et al. 2008) Latent-variable CRF 94.3
Our Method CRF + ADF training 94.5
577
Computational Linguistics Volume 40, Number 3
multiple models). Thus, it is impressive that our single model?based system without
extra resources achieves good performance. This indicates that the proposed ADF
training method can train model parameters with good generality on the test data.
4.6.3 Training Curves. To study the detailed training process and convergence speed, we
show the training curves in Figures 2?4. Figure 2 focuses on the comparisons between
the ADF method and the existing on-line training methods. As we can see, the ADF
method converges faster than other on-line training methods in terms of both training
passes and wall-clock time. The ADF method has roughly the same training speed per
pass compared with traditional SGD training.
Figure 3 (Top Row) focuses on comparing the ADF method with the CW method
(Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, and
Dredze 2009). Comparisons are based on similar features. As discussed before, the ADF-
noRich method is a simplified system, with rich edge features removed from the CRF-
ADF system. As we can see, the proposed ADF method, whether with or without rich
edge features, outperforms the CWandAROWmethods. Figure 3 (BottomRow) focuses
on the comparisons with different mini-batch (the training samples in each stochastic
update) sizes. Representative results with a mini-batch size of 10 are shown. In general,
we find larger mini-batch sizes will slow down the convergence speed. Results demon-
strate that, compared with the SGD training method, the ADF training method is less
sensitive to mini-batch sizes.
Figure 4 focuses on the comparisons between the ADF method and the batch
training method LBFGS. As we can see, the ADF method converges at least one order
0 20 40 60 80 10066
67
68
69
70
71
72
73
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 1 2 3 4 5 6x 104
66
67
68
69
70
71
72
73
Training Time (sec)
)
 
 
ADFSGDPerc
0 2,000 4,000 6,000 8,000 10,00096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
0 200 400 600 80092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
Figure 2
Comparisons among the ADF method and other on-line training methods. (Top Row)
Comparisons based on training passes. As we can see, the ADF method has the best accuracy
and with the fastest convergence speed based on training passes. (Bottom Row) Comparisons
based on wall-clock time.
578
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
0 20 40 60 80 10060
62
64
66
68
70
72
Bio?NER (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092
93
94
95
96
97
98Segmentation (ADF vs. CW/AROW)
Number of Passes
F?sco
re  (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092.5
93
93.5
94
94.5
95 Chunking (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10068
69
70
71
72
73 Bio?NER (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
0 20 40 60 80 10096.4
96.6
96.8
97
97.2
97.4
97.6
Segmentation (MiniBatch=10)
Number of Passes
F?sco
re  (%)
 
 
ADFSGD
0 20 40 60 80 10093.2
93.4
93.6
93.8
94
94.2
94.4
94.6
Chunking (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
Figure 3
(Top Row) Comparing ADF and ADF-noRich with CW and AROW methods. As we can see,
both the ADF and ADF-noRich methods work better than the CW and AROW methods.
(Bottom Row) Comparing different methods with mini-batch = 10 in the stochastic learning
setting.
magnitude faster than the LBFGS training in terms of both training passes and wall-
clock time. For the LBFGS training, we need to determine the LBFGSmemory parameter
m, which controls the number of prior gradients used to approximate the Hessian
information. A larger value of m will potentially lead to more accurate estimation
of the Hessian information, but at the same time will consume significantly more
memory. Roughly, the LBFGS training consumes m times more memory than the ADF
on-line training method. For most tasks, the default setting of m = 10 is reasonable. We
set m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for the
Bio-NER task due to the shortage of memory for m > 6 cases in this task.
4.6.4 One-Pass Learning Results. Many real-world data sets can only observe the training
data in one pass. For example, some Web-based on-line data streams can only appear
once so that the model parameter learning should be finished in one-pass learning (see
Zinkevich et al. 2010). Hence, it is important to test the performance in the one-pass
learning scenario.
In the one-pass learning scenario, the feature frequency information is computed
?on the fly? during on-line training. As shown in Section 3.1, we only need to have
a real-valued vector v to record the cumulative feature frequency information, which
is updated when observing training instances one by one. Then, the learning rate
vector ? is updated based on the v only and there is no need to observe the training
instances again. This is the same algorithm introduced in Section 3.1 and no change is
required for the one-pass learning scenario. Figure 5 shows the comparisons between
the ADF method and baselines on one-pass learning. As we can see, the ADF method
579
Computational Linguistics Volume 40, Number 3
0 100 200 300 40066
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 100 200 300 40096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. batch)
Number of Passes
F?sco
re  (%)
 
 
ADFLBFGS
0 100 200 300 400
93.6
93.8
94
94.2
94.4
Chunking (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 5 10 15x 104
66
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Training Time (sec)
F?sco
re (%)
 
 
ADFLBFGS
0 1 2 3 4 5x 104
9696.2
96.496.6
96.897
97.297.4
97.6
Segmentation (ADF vs. batch)
Training Time (sec)
F?sco
re  (%)
 
 
ADFLBFGS
0 1,000 2,000 3,00092.5
93
93.5
94
94.5 Chunking (ADF vs. batch)
Training Time (sec)
)
 
 
ADFLBFGS
Figure 4
Comparisons between the ADF method and the batch training method LBFGS. (Top Row)
Comparisons based on training passes. As we can see, the ADF method converges much faster
than the LBFGS method, and with better accuracy on the convergence state. (Bottom Row)
Comparisons based on wall-clock time.
consistently outperforms the baselines. This also reflects the fast convergence speed of
the ADF training method.
4.7 Non-Structured Classification Results
In previous experiments, we showed that the proposed method outperforms existing
baselines on structured classification. Nevertheless, we want to show that the ADF
method also has good performance on non-structured classification. In addition, this
task is based on real-valued features instead of binary features.
Figure 5
Comparisons among different methods based on one-pass learning. As we can see, the ADF
method has the best accuracy on one-pass learning.
580
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 6
Results on sentiment classification (non-structured binary classification).
Accuracy Passes Train-Time (sec)
LBFGS (batch) 87.00 86 72.20
SGD (on-line) 87.13 44 55.88
Perc (on-line) 84.55 25 5.82
Avg-Perc (on-line) 85.04 46 12.22
ADF (proposal) 87.89 30 57.12
Experimental results of different training methods on the convergence state are
shown in Table 6. As we can see, the proposed method outperforms all of the on-line
and batch baselines in terms of binary classification accuracy. Here again we observe
that the ADF and SGD methods outperform the LBFGS baseline.
The training curves are shown in Figure 6. As we can see, the ADF method con-
verges quickly. Because this data set is relatively small and the feature dimension is
much smaller than previous tasks, we find the baseline training methods also have
fast convergence speed. The comparisons on one-pass learning are shown in Fig-
ure 7. Just as for the experiments for structured classification tasks, the ADF method
0 20 40 60 80 10082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Number of Passes
Accur
acy (%
)
 
 
ADFSGDPerc
0 20 40 60 8082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Training Time (sec)
Accur
acy (%
)
 
 
ADFSGDPerc
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Number of Passes
Accur
acy (%
)
 
 
ADFLBFGS
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Training Time (sec)
Accu
racy (%
)
 
 
ADFLBFGS
Figure 6
F-score curves on sentiment classification. (Top Row) Comparisons among the ADF method and
on-line training baselines, based on training passes and wall-clock time, respectively. (Bottom
Row) Comparisons between the ADF method and the batch training method LBFGS, based on
training passes and wall-clock time, respectively. As we can see, the ADF method outperforms
both the on-line training baselines and the batch training baseline, with better accuracy and
faster convergence speed.
581
Computational Linguistics Volume 40, Number 3
Figure 7
One-pass learning results on sentiment classification.
outperforms the baseline methods on one-pass learning, with more than 12.7% error
rate reduction.
5. Proofs
This section gives proofs of Theorems 1?4.
Proof of Theorem 1 Following Equation (5), the ADF update rule is F(wt) := wt+1 =
wt +? ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |F(wt+1)?wt+1|
= |wt+1 +? ? gt+1 ?wt+1|
= |? ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |?maxgt+1|
= |FSGD(wt+1)? FSGD(wt)|
(6)
where ai and bi are the ith elements of the vector ? and gt+1, respectively. FSGD is the
SGD update rule with the fixed learning rate ?max such that ?max := sup{?i where ?i ?
?}. In other words, for the SGD update rule FSGD, the fixed learning rate ?max is derived
from the ADF update rule. According to Lemma 1, the SGD update rule FSGD is a
contraction mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2,
given the condition that ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Hence, it goes to
|FSGD(wt+1)? FSGD(wt)| ? (1? ?max/?2)|wt+1 ?wt| (7)
Combining Equations (6) and (7), it goes to
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
Thus, according to the definition of dynamic contraction mapping, the ADF update rule
is a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree
1? ?max/?2. ut
582
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Proof of Theorem 2 As presented in Equation (5), the ADF update rule is F(wt) :=
wt+1 = wt +?t ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |?t+1 ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |FSGD(wt+1)? FSGD(wt)|
(8)
where ai is the ith element of the vector ?t+1. bi and FSGD are the same as before.
Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because ?max
is the maximum learning rate at the beginning and all learning rates are decreasing
when t is increasing. The proof can be easily derived following the same steps in the
proof of Theorem 1. To avoid redundancy, we do not repeat the derivation. ut
Proof of Theorem 3 Let M be the accumulative change of the ADF weight vectorwt:
Mt :=
?
t?=1,2,...,t
|wt?+1 ?wt? |
To prove the convergence of the ADF, we need to prove the sequence Mt converges as
t??. Following Theorem 2, we have the following formula for the ADF training:
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
where ?max is the maximum learning rate at the beginning. Let d0 := |w2 ?w1| and
q := 1? ?max/?2, then we have:
Mt =
?
t?=1,2,...,t
|wt?+1 ?wt? |
? d0 + d0q+ d0q2 + ? ? ?+ d0qt?1
= d0(1? qt)/(1? q)
(9)
When t??, d0(1? qt)/(1? q) goes to d0/(1? q) because q < 1. Hence, we have:
Mt ? d0/(1? q)
Thus, Mt is upper-bounded. Because we know that Mt is a monotonically increasing
function when t??, it follows that Mt converges when t??. This completes the
proof. ut
583
Computational Linguistics Volume 40, Number 3
Proof of Theorem 4 First, we have
eigen(Ct) =
t
?
m=1
(1??0?m?)
? exp
{
??0?
t
?
m=1
?m
}
Then, we have
0 ?
n
?
j=1
(1? aj) ?
n
?
j=1
e?aj = e?
?n
j=1 aj
This is because 1? aj ? e?aj given 0 ? aj < 1. Finally, because
?t
m=1 ?m ?
?
1?? when
t??, we have
eigen(Ct) ? exp
{
??0?
t
?
m=1
?m
}
? exp
{
??0??
1? ?
}
This completes the proof. ut
6. Conclusions
In this work we tried to simultaneously improve the training speed andmodel accuracy
of natural language processing systems. We proposed the ADF on-line training method,
based on the core idea that high frequency features should result in a learning rate that
decays faster. We demonstrated that the ADF on-line training method is convergent
and has good theoretical properties. Based on empirical experiments, we can state the
following conclusions. First, the ADF method achieved the major target of this work:
faster training speed and higher accuracy at the same time. Second, the ADF method
was robust: It had good performance on several structured and non-structured classifi-
cation tasks with very different characteristics. Third, the ADF method worked well on
both binary features and real-valued features. Fourth, the ADF method outperformed
existing methods in a one-pass learning setting. Finally, our method achieved state-
of-the-art performance on several well-known benchmark tasks. To the best of our
knowledge, our simple method achieved a much better F-score than the existing best
reports on the biomedical named entity recognition task.
Acknowledgments
This work was supported by the National
Natural Science Foundation of China
(no. 61300063, no. 61370117), the Doctoral
Fund of Ministry of Education of China
(no. 20130001120004), a Hong Kong
Polytechnic University internal grant
(4-ZZD5), a Hong Kong RGC Project
(no. PolyU 5230/08E), the National High
Technology Research and Development
Program of China (863 Program,
no. 2012AA011101), and the Major National
Social Science Fund of China (no. 12&ZD227).
This work is a substantial extension of the
conference version presented at ACL 2012
(Sun, Wang, and Li 2012).
584
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
References
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 440?447, Prague.
Bottou, Le?on. 1998. Online algorithms
and stochastic approximations. In
D. Saad, editor. Online Learning and Neural
Networks. Cambridge University Press,
pages 9?42.
Chen, Stanley F. and Ronald Rosenfeld.
1999. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Carnegie
Mellon University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP?02, pages 1?8, Philadelphia, PA.
Crammer, Koby, Alex Kulesza, and Mark
Dredze. 2009. Adaptive regularization of
weight vectors. In NIPS?09, pages 414?422,
Vancouver.
Dredze, Mark, Koby Crammer, and
Fernando Pereira. 2008. Confidence-
weighted linear classification. In
Proceedings of ICML?08, pages 264?271,
Helsinki.
Duchi, John, Elad Hazan, and Yoram Singer.
2010. Adaptive subgradient methods
for online learning and stochastic
optimization. Journal of Machine
Learning Research, 12:2,121?2,159.
Finkel, Jenny, Shipra Dingare, Huy Nguyen,
Malvina Nissim, Christopher Manning,
and Gail Sinclair. 2004. Exploiting context
for biomedical entity recognition: From
syntax to the Web. In Proceedings of
BioNLP?04, pages 91?94, Geneva.
Freund, Yoav and Robert Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gao, Jianfeng, Galen Andrew, Mark Johnson,
and Kristina Toutanova. 2007. A comparative
study of parameter estimation methods for
statistical natural language processing. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics
(ACL?07), pages 824?831, Prague.
Hsu, Chun-Nan, Han-Shen Huang, Yu-Ming
Chang, and Yuh-Jye Lee. 2009. Periodic
step-size adaptation in second-order
gradient descent for single-pass on-line
structured learning. Machine Learning,
77(2-3):195?224.
Jacobs, Robert A. 1988. Increased rates of
convergence through learning rate
adaptation. Neural Networks, 1(4):295?307.
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa
Tsuruoka, and Yuka Tateisi. 2004.
Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of
BioNLP?04, pages 70?75, Geneva.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of NAACL?01, pages 1?8,
Pittsburgh, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence
data. In ICML?01, pages 282?289,
Williamstown, MA.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Flexible text
segmentation with structured multilabel
classification. In Proceedings of HLT/
EMNLP?05, pages 987?994, Vancouver.
McMahan, H. Brendan and Matthew J.
Streeter. 2010. Adaptive bound
optimization for online convex
optimization. In Proceedings of COLT?10,
pages 244?256, Haifa.
Murata, Noboru. 1998. A statistical study
of on-line learning. In D. Saad, editor.
Online Learning in Neural Networks.
Cambridge University Press, pages 63?92.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical optimization. Springer.
Okanohara, Daisuke, Yusuke Miyao,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2006. Improving the scalability of
semi-Markov conditional random
fields for named entity recognition.
In Proceedings of COLING-ACL?06,
pages 465?472, Sydney.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
1996, pages 133?142, Pennsylvania.
Sang, Erik Tjong Kim and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL?00, pages 127?132, Lisbon.
Schaul, Tom, Sixin Zhang, and Yann LeCun.
2012. No more pesky learning rates. CoRR,
abs/1206.1106.
585
Computational Linguistics Volume 40, Number 3
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random
fields and rich feature sets. In Proceedings
of BioNLP?04, pages 104?107, Geneva.
Shalev-Shwartz, Shai, Yoram Singer, and
Nathan Srebro. 2007. Pegasos: Primal
estimated sub-gradient solver for SVM.
In Proceedings of ICML?07, pages 807?814,
Corvallis, OR.
Sperduti, Alessandro and Antonina Starita.
1993. Speed up learning and network
optimization with extended back
propagation. Neural Networks, 6(3):365?383.
Sun, Weiwei. 2010. Word-based and
character-based word segmentation
models: Comparison and combination.
In COLING?10 (Posters), pages 1,211?1,219,
Beijing.
Sun, Xu, Takuya Matsuzaki, and Wenjie Li.
2013. Latent structured perceptrons
for large-scale learning with hidden
information. IEEE Transactions on
Knowledge and Data Engineering,
25(9):2,063?2,075.
Sun, Xu, Takuya Matsuzaki, Daisuke
Okanohara, and Jun?ichi Tsujii. 2009.
Latent variable perceptron algorithm for
structured classification. In Proceedings
of the 21st International Joint Conference
on Artificial Intelligence (IJCAI 2009),
pages 1,236?1,242, Pasadena, CA.
Sun, Xu, Louis-Philippe Morency, Daisuke
Okanohara, and Jun?ichi Tsujii. 2008.
Modeling latent-dynamic in shallow
parsing: A latent conditional model with
improved inference. In Proceedings of
COLING?08, pages 841?848, Manchester.
Sun, Xu, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-
adaptive learning rates for Chinese word
segmentation and new word detection.
In Proceedings of ACL?12, pages 253?262,
Jeju Island.
Sun, Xu, Yaozhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2009. A discriminative
latent variable Chinese segmenter with
hybrid word/character information.
In Proceedings of NAACL-HLT?09,
pages 56?64, Boulder, CO.
Sun, Xu, Yao Zhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2013. Probabilistic Chinese
word segmentation with non-local
information and stochastic training.
Information Processing & Management,
49(3):626?636.
Tseng, Huihsin, Pichuan Chang, Galen
Andrew, Daniel Jurafsky, and Christopher
Manning. 2005. A conditional random
field word segmenter for SIGHAN bakeoff
2005. In Proceedings of the Fourth SIGHAN
Workshop, pages 168?171, Jeju Island.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and
Sophia Ananiadou. 2009. Stochastic
gradient descent training for
l1-regularized log-linear models with
cumulative penalty. In Proceedings of
ACL?09, pages 477?485, Suntec.
Vishwanathan, S. V. N., Nicol N.
Schraudolph, Mark W. Schmidt, and
Kevin P. Murphy. 2006. Accelerated
training of conditional random
fields with stochastic meta-descent.
In Proceedings of ICML?06, pages 969?976,
Pittsburgh, PA.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random fields for
Chinese word segmentation. In Proceedings
of the Human Language Technology
Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196,
New York City.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 840?847,
Prague.
Zhao, Hai, Changning Huang, Mu Li,
and Bao-Liang Lu. 2010. A unified
character-based tagging framework for
Chinese word segmentation. ACM
Transactions on Asian Language Information
Processing, 9(2): Article 5.
Zhao, Hai and Chunyu Kit. 2011. Integrating
unsupervised and supervised word
segmentation: The role of goodness
measures. Information Sciences,
181(1):163?183.
Zinkevich, Martin, Markus Weimer,
Alexander J. Smola, and Lihong Li.
2010. Parallelized stochastic gradient
descent. In Proceedings of NIPS?10,
pages 2,595?2,603, Vancouver.
586
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 860?865,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
___________________  
*Corresponding author 
Cross-lingual Opinion Analysis via Negative Transfer Detection  
Lin Gui1,2, Ruifeng Xu1*, Qin Lu2, Jun Xu1, Jian Xu2, Bin Liu1, Xiaolong Wang1 
1Key Laboratory of Network Oriented Intelligent Computation, Shenzhen Graduate School, 
Harbin Institute of Technology, Shenzhen 518055 
2Department Of Computing, the Hong Kong Polytechnic University 
guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn, 
csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn 
 
Abstract 
Transfer learning has been used in opin-
ion analysis to make use of available lan-
guage resources for other resource scarce 
languages. However, the cumulative 
class noise in transfer learning adversely 
affects performance when more training 
data is used. In this paper, we propose a 
novel method in transductive transfer 
learning to identify noises through the 
detection of negative transfers. Evalua-
tion on NLP&CC 2013 cross-lingual 
opinion analysis dataset shows that our 
approach outperforms the state-of-the-art 
systems. More significantly, our system 
shows a monotonic increase trend in per-
formance improvement when more train-
ing data are used.  
1 Introduction 
Mining opinions from text by identifying their 
positive and negative polarities is an important 
task and supervised learning methods have been 
quite successful. However, supervised methods 
require labeled samples for modeling and the 
lack of sufficient training data is the performance 
bottle-neck in opinion analysis especially for re-
source scarce languages. To solve this problem, 
the transfer leaning method (Arnold et al, 2007) 
have been used to make use of samples from a 
resource rich source language to a resource 
scarce target language, also known as cross lan-
guage opinion analysis (CLOA). 
In transductive transfer learning (TTL) where 
the source language has labeled data and the tar-
get language has only unlabeled data, an algo-
rithm needs to select samples from the unlabeled 
target language as the training data and assign 
them with class labels using some estimated con-
fidence. These labeled samples in the target lan-
guage, referred to as the transferred samples, also 
have a probability of being misclassified. During 
training iterations, the misclassification introduc-
es class noise which accumulates, resulting in a 
so called negative transfer that affects the classi-
fication performance.  
In this paper, we propose a novel method 
aimed at reducing class noise for TTL in CLOA. 
The basic idea is to utilize transferred samples 
with high quality to identify those negative trans-
fers and remove them as class noise to reduce 
noise accumulation in future training iterations. 
Evaluations on NLP&CC 2013 CLOA evalua-
tion data set show that our algorithm achieves the 
best result, outperforming the current state-of-
the-art systems. More significantly, our system 
shows a monotonic increasing trend in perfor-
mance when more training data are used beating 
the performance degradation curse of most trans-
fer learning methods when training data reaches 
certain size. 
The rest of the paper is organized as follows. 
Section 2 introduces related works in transfer 
learning, cross lingual opinion analysis, and class 
noise detection technology. Section 3 presents 
our algorithm. Section 4 gives performance eval-
uation. Section 5 concludes this paper. 
2 Related works 
TTL has been widely used before the formal 
concept and definition of TTL was given in (Ar-
nold, 2007). Wan introduced the co-training 
method into cross-lingual opinion analysis (Wan, 
2009; Zhou et al, 2011), and Aue et al intro-
duced transfer learning into cross domain analy-
sis (Aue, 2005) which solves similar problems. 
In this paper, we will use the terms source lan-
guage and target language to refer to all cross 
lingual/domain analysis. 
Traditionally, transfer learning methods focus 
on how to estimate the confidence score of trans-
ferred samples in the target language or domain 
(Blitzer et al 2006, Huang et al, 2007; Sugiya-
ma et al, 2008, Chen et al 2011, Lu et al, 2011). 
In some tasks, researchers utilize NLP tools such 
as alignment to reduce the bias towards that of 
860
 the source language in transfer learning (Meng et 
al., 2012). However, detecting misclassification 
in transferred samples (referred to as class noise) 
and reducing negative transfers are still an unre-
solved problem. 
There are two basic methods for class noise 
detection in machine learning. The first is the 
classification based method (Brodley and Friedl, 
1999; Zhu et al 2003; Zhu 2004; Sluban et al, 
2010) and the second is the graph based method 
(Zighed et al 2002; Muhlenbach et al 2004; 
Jiang and Zhou, 2004). Class noise detection can 
also be applied to semi-supervised learning be-
cause noise can accumulate in iterations too. Li 
employed Zighed?s cut edge weight statistic 
method in self-training (Li and Zhou, 2005) and 
co-training (Li and Zhou, 2011). Chao used Li?s 
method in tri-training (Chao et al 2008). (Fuku-
moto et al 2013) used the support vectors to de-
tect class noise in semi-supervised learning.  
In TTL, however, training and testing samples 
cannot be assumed to have the same distributions. 
Thus, noise detection methods used in semi-
supervised learning are not directly suited in 
TTL. Y. Cheng has tried to use semi-supervised 
method (Jiang and Zhou, 2004) in transfer learn-
ing (Cheng and Li, 2009). His experiment 
showed that their approach would work when the 
source domain and the target domain share simi-
lar distributions. How to reduce negative trans-
fers is still a problem in transfer learning. 
3 Our Approach 
In order to reduce negative transfers, we pro-
pose to incorporate class noise detection into 
TTL. The basic idea is to first select high quality 
labeled samples after certain iterations as indica-
tor to detect class noise in transferred samples. 
We then remove noisy samples that cause nega-
tive transfers from the current accumulated train-
ing set to retain an improved set of training data 
for the remainder of the training phase. This neg-
ative sample reduction process can be repeated 
several times during transfer learning. Two ques-
tions must be answered in this approach: (1) how 
to measure the quality of transferred samples, 
and (2) how to utilize high quality labeled sam-
ples to detect class noise in training data. 
3.1 Estimating Testing Error 
To determine the quality of the transferred 
samples that are added iteratively in the learning 
process, we cannot use training error to estimate 
true error because the training data and the test-
ing data have different distributions. In this work, 
we employ the Probably Approximately Correct 
(PAC) learning theory to estimate the error 
boundary. According to the PAC learning theory, 
the least error boundary ? is determined by the 
size of the training set m and the class noise rate 
?, bound by the following relation: 
  ?   (   )                      ( ) 
In TTL, m increases linearly, yet ? is multi-
plied in each iteration. This means the signifi-
cance of m to performance is higher at the begin-
ning of transfer learning and gradually slows 
down in later iterations. On the contrary, the in-
fluence of class noise increases. That is why per-
formance improves initially and gradually falls to 
negative transfer when noise accumulation out-
performs the learned information as shown in 
Fig.1. In TTL, transferred samples in both the 
training data and test data have the same distribu-
tion. This implies that we can apply the PAC 
theory to analyze the error boundary of the ma-
chine learning model using transferred data. 
 
Figure 1 Negative transfer in the learning process 
According to PAC theorem with an assumed 
fixed probability ? (Angluin and Laird, 1988), 
the least error boundary ? is given by:   
  ?   (   ? )  ( (   ) )       ( ) 
where N is a constant decided by the hypothesis 
space.  In any iteration during TTL, the hypothe-
sis space is the same and the probability ? is 
fixed. Thus the least error boundary is deter-
mined by the size of the transferred sample m 
and the class noise of transferred samples ?. Ac-
cording to (2), we apply a manifold assumption 
based method to estimate ?. Let T be the number 
of iterations to serve as one period. We then es-
timate the least error boundary before and after 
each T to measure the quality of transferred sam-
ples during each T. If the least error boundary is 
reduced, it means that transferred samples used 
in this period are of high quality and can improve 
the performance. Otherwise, the transfer learning 
algorithm should stop.  
861
 3.2 Estimating Class Noise 
For formula (2) to work, we need to know the 
class noise rate ? to calculate the error boundary. 
Obviously, we cannot use conditional probabili-
ties from the training data in the source language 
to estimate the noise rate ? of the transferred 
samples because the distribution of source lan-
guage is different from that of target language. 
Consider a KNN graph on the transferred 
samples using any similarity metric, for example, 
cosine similarity, for any two connected vertex 
(     )and (     ) in the graph from samples to 
classes, the edge weight is given by: 
       (     )                         ( ) 
Furthermore, a sign function for the two vertices 
(     )and (     ), is defined as: 
    {
          
          
                   ( ) 
According to the manifold assumption, the 
conditional probability  (  |  ) can be approxi-
mated by the frequency of  (     ) which is 
equal to  (     ). In opinion annotations, the 
agreement of two annotators is often no larger 
than 0.8. This means that for the best cases 
 (     )=0.2. Hence     follows a Bernoulli 
distribution with p=0.2 for the best cases in 
manual annotations.  
Let      (     )  be the vertices that are 
connected to the     vertex, the statistical magni-
tude of the     vertex can be defined as: 
   ?                                 ( )  
where j refers to the     vertex that is connected 
to the     vertex.  
From the theory of cut edge statics, we know 
that the expectation of    is: 
    (     )  ?                  ( )  
And the variance of    is: 
  
   (     ) (     )  ?    
 
 ( )  
By the Center Limit Theorem (CLT),    fol-
lows the normal distribution: 
(     )
  
  (   )                    ( )  
To detect the noise rate of a sample (     ) , 
we can use (8) as the null hypothesis to test the 
significant level. Let    denotes probability of 
the correct classification for a transferred sample. 
   should follow a normal distribution,  
   
 
?    
?  
 
(    )
 
   
   
  
           ( )  
Note that experiments (Li and Zhou, 2011; 
Cheng and Li, 2009; Brodley and Friedl, 1999) 
have shown that     is related to the error rate of 
the example (     ), but it does not reflect the 
ground-truth probability in statistics. Hence we 
assume the class noise rate of example (     ) is: 
                              (  ) 
 We take the general significant level of 0.05 
to reject the null hypothesis. It means that if    of 
(     ) is larger than 0.95, the sample will be 
considered as a class noisy sample. Furthermore, 
   can be used to estimate the average class noise 
rate of a transferred samples in (2). 
In our proposed approach, we establish the 
quality estimate period T to conduct class noise 
detection to estimate the class noise rate of trans-
ferred samples. Based on the average class noise 
we can get the least error boundary so as to tell if 
an added sample is of high quality. If the newly 
added samples are of high quality, they can be 
used to detect class noise in transferred training 
data. Otherwise, transfer learning should stop. 
The flow chart for negative transfer is in Fig.2. 
SLS(labeled)
TLS
(unlabeled)
Classifier
Top k
TS
 period 1
TS
period 2
TS
 period n
KNN 
graph
Estimate ?i and ?n 
?n ? ?n-1?
Output SLS and TS 
(period 1 to n-1)
No
Yes
Del te TS
 ?i? 0.95 
period 1 to n-1
Input 
Input 
T iterations per period
Transfer
process
Negative
transfer
detection
Figure 2 Flow charts of negative transfer detection 
In the above flow chart, SLS and TLS refer to 
the source and target language samples, respec-
tively. TS refers to the transferred samples. Let T 
denote quality estimate period T in terms of itera-
tion numbers. The transfer process select k sam-
ples in each iteration. When one period of trans-
fer process finishes, the negative transfer detec-
tion will estimate the quality by comparing and 
either select the new transferred samples or re-
move class noise accumulated up to this iteration. 
4 Experiment 
4.1 Experiment Setting 
The proposed approach is evaluated on the 
NLP&CC 2013 cross-lingual opinion analysis (in 
862
 short, NLP&CC) dataset 1 . In the training set, 
there are 12,000 labeled English Amazon.com 
products reviews, denoted by Train_ENG, and 
120 labeled Chinese product reviews, denoted as 
Train_CHN, from three categories, DVD, BOOK, 
MUSIC. 94,651 unlabeled Chinese products re-
views from corresponding categories are used as 
the development set, denoted as Dev_CHN. In 
the testing set, there are 12,000 Chinese product 
reviews (shown in Table.1). This dataset is de-
signed to evaluate the CLOA algorithm which 
uses Train_CHN, Train_ENG and Dev_CHN to 
train a classifier for Test_CHN. The performance 
is evaluated by the correct classification accuracy 
for each category in Test_CHN2:  
          
                                  
    
 
where c is either DVD, BOOK or MUSIC. 
Team DVD Book Music 
Train_CHN 40 40 40 
Train_ENG 4000 4000 4000 
Dev_CHN 17814 47071 29677 
Test_CHN 4000 4000 4000 
Table.1 The NLP&CC 2013 CLOA dataset 
In the experiment, the basic transfer learning 
algorithm is co-training. The Chinese word seg-
mentation tool is ICTCLAS (Zhang et al 2003) 
and Google Translator3 is the MT for the source 
language. The monolingual opinion classifier is 
SVMlight4, word unigram/bigram features are em-
ployed. 
4.2 CLOA Experiment Results 
Firstly, we evaluate the baseline systems 
which use the same monolingual opinion classi-
fier with three training dataset including 
Train_CHN, translated Train_ENG and their un-
ion, respectively.  
 DVD Book Music Accuracy 
Train_CHN 0.552 0.513 0.500 0.522 
Train_ENG 0.729 0.733 0.722 0.728 
Train_CHN 
+Train_ENG 
0.737 0.722 0.742 0.734 
Table.2 Baseline performances  
It can be seen that using the same method, the 
classifier trained by Train_CHN are on avergage 
20% worse than the English counter parts.The 
combined use of Train_CHN and translated 
Train_ENG, however, obtained similar 
                                                 
1http://tcci.ccf.org.cn/conference/2013/dldoc/evdata03.zip 
2http://tcci.ccf.org.cn/conference/2013/dldoc/evres03.pdf 
3https://translate.google.com 
4http://svmlight.joachims.org/ 
performance to the English counter parts. This 
means the predominant training comes from the 
English training data. 
In the second set of experiment, we compare  
our proposed approach to the official results in 
NLP&CC 2013 CLOA evaluation and the result 
is given in Table 3. Note that in Table 3, the top 
performer of NLP&CC 2013 CLOA evaluation 
is the HLT-HITSZ system(underscored in the 
table), which used the co-training method in 
transfer learning (Gui et al 2013), proving that 
co-training is quite effective for cross-lingual 
analysis. With the additional negative transfer 
detection, our proposed approach achieves the 
best performance on this dataset outperformed 
the top system (by HLT-HITSZ) by a 2.97% 
which translate to 13.1% error reduction im-
provement to this state-of-the-art system as 
shown in the last row of Table 3.     
Team DVD Book Music Accuracy 
BUAA 0.481 0.498 0.503 0.494 
BISTU 0.647 0.598 0.661 0.635 
HLT-HITSZ 0.777 0.785 0.751 0.771 
THUIR 0.739 0.742 0.733 0.738 
SJTU 0.772 0.724 0.745 0.747 
WHU 0.783 0.770 0.760 0.771 
Our approach 0.816 0.801 0.786 0.801 
Error 
Reduction 
0.152 0.072 0.110 0.131 
Table.3 Performance compares with NLP&CC 
2013 CLOA evaluation results 
To further investigate the effectiveness of our 
method, the third set of experiments evaluate the 
negative transfer detection (NTD) compared to 
co-training (CO) without negative transfer 
detection as shown in Table.4 and Fig.3 Here, we 
use the union of Train_CHN and Train_ENG as 
labeled data and Dev_CHN as unlabeled data to 
be transferred in the learning algorithms. 
 DVD Book Music Mean 
NTD 
Best case 0.816 0.801 0.786 0.801 
Best period 0.809 0.798 0.782 0.796 
Mean 0.805 0.795 0.781 0.794 
CO 
Best case 0.804 0.796 0.783 0.794 
Best period 0.803 0.794 0.781 0.792 
Mean 0.797 0.790 0.775 0.787 
Table.4 CLOA performances 
Taking all categories of data, our proposed 
method improves the overall average precision 
(the best cases) from 79.4% to 80.1% when 
compared to the state of the art system which 
translates to error reduction of 3.40% (p-
value?0.01 in Wilcoxon signed rank test). Alt-
hough the improvement does not seem large, our 
863
  
   
Figure 3 Performance of negative transfer detection vs. co-training 
algorithm shows a different behavior in that it 
can continue to make use of available training 
data to improve the system performance. In other 
words, we do not need to identify the tipping 
point where the performance degradation can 
occur when more training samples are used. Our 
approach has also shown the advantage of stable 
improvement.  
In the most practical tasks, co-training based 
approach has the difficulty to determine when to 
stop the training process because of the negative 
transfer. And thus, there is no sure way to obtain 
the above best average precision. On the contrary, 
the performance of our proposed approach keeps 
stable improvement with more iterations, i.e. our 
approach has a much better chance to ensure the 
best performance. Another experiment is con-
ducted to compare the performance of our pro-
posed transfer learning based approach with su-
pervised learning. Here, the achieved perfor-
mance of 3-folder cross validation are given in 
Table 5. 
 DVD Book Music Average 
Supervised 0.833 0.800 0.801 0.811 
Our approach 0.816 0.801 0.786 0.801 
Table.5 Comparison with supervised learning  
The accuracy of our approach is only 1.0% 
lower than the supervised learning using 2/3 of 
Test_CHN. In the BOOK subset, our approach 
achieves match result. Note that the performance 
gap in different subsets shows positive correla-
tion to the size of Dev_CHN. The more samples 
are given in Dev_CHN, a higher precision is 
achieved even though these samples are unla-
beled. According to the theorem of PAC, we 
know that the accuracy of a classifier training 
from a large enough training set with confined 
class noise rate will approximate the accuracy of 
classifier training from a non-class noise training 
set. This experiment shows that our proposed 
negative transfer detection controls the class 
noise rate in a very limited boundary. Theoreti-
cally speaking, it can catch up with the perfor-
mance of supervised learning if enough unla-
beled samples are available. In fact, such an ad-
vantage is the essence of our proposed approach.  
5 Conclusion 
In this paper, we propose a negative transfer 
detection approach for transfer learning method 
in order to handle cumulative class noise and 
reduce negative transfer in the process of transfer 
learning. The basic idea is to utilize high quality 
samples after transfer learning to detect class 
noise in transferred samples. We take cross lin-
gual opinion analysis as the data set to evaluate 
our method. Experiments show that our proposed 
approach obtains a more stable performance im-
provement by reducing negative transfers. Our 
approach reduced 13.1% errors than the top sys-
tem on the NLP&CC 2013 CLOA evaluation 
dataset. In BOOK category it even achieves bet-
ter result than the supervised learning. Experi-
mental results also show that our approach can 
obtain better performance when the transferred 
samples are added incrementally, which in pre-
vious works would decrease the system perfor-
mance. In future work, we plan to extend this 
method into other language/domain resources to 
identify more transferred samples.  
Acknowledgement 
This research is supported by NSFC 61203378, 
61300112, 61370165, Natural Science Founda-
tion of GuangDong S2013010014475, MOE 
Specialized Research Fund for the Doctoral Pro-
gram of Higher Education 20122302120070,  
Open Projects Program of National Laboratory 
of Pattern Recognition?Shenzhen Foundational 
Research Funding JCYJ20120613152557576, 
JC201005260118A, Shenzhen International Co-
operation Research Funding 
GJHZ20120613110641217 and Hong Kong Pol-
ytechnic University Project code Z0EP. 
DVD Book Music 
864
 Reference 
Angluin, D., Laird, P. 1988. Learning from Noisy 
Examples. Machine Learning, 2(4): 343-370. 
Arnold, A., Nallapati, R., Cohen, W. W. 2007. A 
Comparative Study of Methods for Transductive 
Transfer Learning. In Proc. 7th IEEE ICDM Work-
shops, pages 77-82. 
Aue, A., Gamon, M. 2005. Customizing Sentiment 
Classifiers to New Domains: a Case Study, In Proc. 
of t RANLP. 
Blitzer, J., McDonald, R., Pereira, F. 2006. Domain 
Adaptation with Structural Correspondence Learn-
ing. In Proc. EMNLP, 120-128. 
Brodley, C. E., Friedl, M. A. 1999. Identifying and 
Eliminating Mislabeled Training Instances. Journal 
of Artificial Intelligence Research, 11:131-167. 
Chao, D., Guo, M. Z., Liu, Y.,  Li, H. F. 2008.  Partic-
ipatory Learning based Semi-supervised Classifica-
tion. In Proc. of 4th ICNC, pages 207-216. 
Cheng, Y., Li, Q. Y. 2009. Transfer Learning with 
Data Edit. LNAI, pages 427?434. 
Chen, M., Weinberger, K. Q.,  Blitzer, J. C. 2011.  
Co-Training for Domain Adaptation. In Proc. of 
23th NIPS. 
Fukumoto, F., Suzuki, Y., Matsuyoshi, S. 2013. Text 
Classification from Positive and Unlabeled Data 
using Misclassified Data Correction. In Proc. of 
51st ACL, pages 474-478. 
Gui, L., Xu, R.,  Xu, J., et al 2013. A Mixed Model 
for Cross Lingual Opinion Analysis. In CCIS, 400, 
pages 93-104. 
Huang, J., Smola, A., Gretton, A., Borgwardt, K.M., 
Scholkopf, B. 2007. Correcting Sample Selection 
Bias by Unlabeled Data. In Proc. of 19th NIPS,  
pages 601-608. 
Jiang, Y., Zhou, Z. H. 2004. Editing Training Data for 
kNN Classifiers with Neural Network Ensemble. In 
LNCS, 3173,  pages 356-361. 
Li, M., Zhou, Z. H. 2005. SETRED: Self-Training 
with Editing. In Proc. of PAKDD, pages 611-621. 
Li, M., Zhou, Z. H. 2011.  COTRADE: Confident Co-
Training With Data Editing. IEEE Transactions on 
Systems, Man, and Cybernetics?Part B: Cyber-
netics, 41(6):1612-1627. 
Lu, B., Tang, C. H., Cardie, C., Tsou, B. K. 2011. 
Joint Bilingual Sentiment Classification with Un-
labeled Parallel Corpora. In Proc. of 49th ACL, 
pages 320-330. 
Meng, X. F., Wei, F. R., Liu, X. H., et al 2012. 
Cross-Lingual Mixture Model for Sentiment Clas-
sification. In Proc. of 50th ACL, pages 572-581. 
Muhlenbach, F., Lallich, S., Zighed, D. A. 2004. 
Identifying and Handling Mislabeled Instances.  
Journal of Intelligent Information System, 22(1): 
89-109. 
Pan, S. J., Yang, Q. 2010. A Survey on Transfer 
Learning, IEEE Transactions on Knowledge and 
Data Engineering, 22(10):1345-1360. 
Sindhwani, V., Rosenberg, D. S. 2008. An RKHS for 
Multi-view Learning and Manifold Co-
Regularization. In Proc. of 25th  ICML, pages 976?
983. 
Sluban, B., Gamberger, D., Lavra, N. 2010.  Advanc-
es in Class Noise Detection. In Proc.19th ECAI,  
pages 1105-1106. 
Sugiyama, M.,  Nakajima, S., Kashima, H., Buenau, 
P.V., Kawanabe, M. 2008. Direct Importance Es-
timation with Model Selection and its Application 
to Covariate Shift Adaptation. In Proc. 20th NIPS. 
Wan, X. 2009. Co-Training for Cross-Lingual Senti-
ment Classification, In Proc. of the 47th Annual 
Meeting of the ACL and the 4th IJCNLP of the 
AFNLP,  235?243. 
Zhang, H. P., Yu, H. K., Xiong, D. Y., and Liu., Q. 
2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In 2nd SIGHAN workshop affiliated 
with 41th ACL, pages 184-187. 
 Zhou, X., Wan X., Xiao, J. 2011. Cross-Language 
Opinion Target Extraction in Review Texts. In 
Proc. of IEEE 12th ICDM, pages 1200-1205. 
Zhu, X. Q., Wu, X. D., Chen, Q. J. 2003.  Eliminating 
Class Noise in Large Datasets. In Proc. of 12th 
ICML, pages 920-927. 
Zhu, X. Q. 2004. Cost-guided Class Noise Handling 
for Effective Cost-sensitive Learning In Proc. of 4th  
IEEE ICDM,  pages 297-304. 
Zighed, D. A., Lallich, S., Muhlenbach, F. 2002.  
Separability Index in Supervised Learning. In Proc. 
of PKDD, pages 475-487. 
865
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 304?307,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PKU_HIT: An Event Detection System Based on Instances Expansion 
and Rich Syntactic Features 
 
 
Shiqi Li1, Pengyuan Liu2, Tiejun Zhao1, Qin Lu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology, Harbin 150001, China 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Institute of Computational Linguistics, 
Peking University, Beijing 100871, China 
liupengyuan@pku.edu.cn 
3Department of Computing, 
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper describes the PKU_HIT system 
on event detection in the SemEval-2010 
Task. We construct three modules for the 
three sub-tasks of this evaluation. For 
target verb WSD, we build a Na?ve 
Bayesian classifier which uses additional 
training instances expanded from an 
untagged Chinese corpus automatically. 
For sentence SRL and event detection, we 
use a feature-based machine learning 
method which makes combined use of 
both constituent-based and dependency-
based features. Experimental results show 
that the Macro Accuracy of the WSD 
module reaches 83.81% and F-Score of 
the SRL module is 55.71%. 
1 Introduction 
In this paper, we describe the system submitted 
to the SemEval-2010 Task 11 on event detection 
in Chinese news sentences (Zhou, 2010). The 
objective of the task is to detect and analyze 
basic event contents in Chinese news sentences, 
similar to the frame semantic structure extraction 
task in SemEval-2007. However, this task is a 
more complex as it involves three interrelated 
subtasks: (1) target verb word sense 
disambiguation (WSD), (2) sentence semantic 
role labeling (SRL) and (3) event detection (ED).  
Therefore, the architecture of the system that 
we develop for the task consists of three modules: 
WSD, SRL and ED. First, the WSD module is to 
recognize key verbs or verb phrases which 
describe the basic event in a sentence, and then 
select an appropriate situation description 
formula for the recognized key verbs (or verb 
phrases); Then, the SRL module anchors the 
arguments to suitable constituents in the sentence, 
and then label each argument with three 
functional tags, namely constituent type tag, 
semantic role tags and event role tag. Finally, in 
the ED module, complete situation description of 
the sentence can be achieved by combining the 
results of the WSD module and the SRL module. 
For the WSD module, we consider the subtask 
as a general WSD problem. First of all, we 
automatically extract many instances from an 
untagged Chinese corpus using a heuristic rule 
inspired by Yarowsky (1993). Then we train a 
Na?ve Bayesian (NB) classifier based on both the 
extracted instances and the official training data. 
We then use the NB classifier to predict situation 
the description formula and natural explanation 
of each target verb in testing data. 
For the SRL module, we use a rich syntactic 
feature-based learning method. As the state-of-
the-art method in the field of SRL, feature-based 
method represents a predicate-argument structure 
(PAS) by a flat vector using a set of linguistic 
features. Then PAS can be directly classified by 
machine learning algorithms based on the 
corresponding vectors. In feature-based SRL, the 
304
significance of syntactic information in SRL was 
proven by (Punyakanok et al, 2005). In our 
method, we exploit a rich set of syntactic 
features from two syntactic views: constituent 
and dependency. As the two syntactic views 
focus on different syntactic elements, 
constituent-based features and dependency-based 
features can complement each other in SRL to 
some extent. Finally, the ED module can be 
readily implemented by combining the SRL and 
the WSD result using some simply rules.  
2 System Description 
2.1 Target Verb WSD 
The WSD module is based on a simple heuristic 
rule by which we can extract sense-labeled 
instances automatically. The heuristic rule 
assumes that one sense per 3-gram which is 
proposed by us initially through investigating a 
Chinese sense-tagged corpus STC (Wu et al, 
2006). The assumption is similar to the 
celebrated one sense per collocation supposition 
(Yarowsky, 1993), whereas ours has more 
expansibility. STC is an ongoing project which is 
to build a sense-tagged corpus containing sense-
tagged 1, 2 and 3 months of People?s Daily 2000 
now. According to our investigation, given a 
specific 3-gram (w-1wverbw1) to any target verb, 
on average, we expect to see the same label 
95.4% of the time. Based on this observation, we 
consider one sense per 3-gram (w-1wverbw1) or at 
least we can extract instances with this pattern. 
For all the 27 multiple-sense target verbs in 
the official training data, we found their 3-gram 
(w-1wverbw1) and extracted the instances with the 
same 3-gram from a Chinese monolingual corpus 
? the 2001 People?s Daily (about 116M bytes). 
We consider the same 3-gram instances should 
have the same label. Then an additional sense-
labeled training corpus is built automatically in 
expectation of having 95.4% precision at most. 
And this corpus has 2145 instances in total 
(official training data have 4608 instances). 
We build four systems to investigate the effect 
of our instances expansion using the Na?ve 
Bayesian classifier. System configuration is 
shown in Table 1. In column 1, BL means 
baseline, X means instance expansion, 3 and 15 
means the window size. In column 2, wi is the i-
th word relative to the target word, wi-1wi is the 2-
gram of words, wj/j is the word with position 
information (j?[-3,+3]). In the last column, ?O? 
means using only the original training data and 
?O+A? means using both the original and 
additional training data. Syntactic feature and 
parameter optimizing are not used in this module. 
 
System Features Window Size 
Training 
Data 
BL_3 
wi, wi-1wi, wj/j
?3 O 
X_3 ?3 O+A 
BL_15 ?15 O 
X_15 ?15 O+A 
Table 1: The system configuration 
2.2 Sentence SRL and Event Detection 
We use a feature-based machine learning method 
to implement the SRL module in which three 
tags are labeled, namely the semantic role tag, 
the event role tag and the phrase type tag. We 
consider the SRL task as a four-step pipeline: (1) 
parsing which generates a constituent parse tree 
for the input sentence; (2) pruning which filters 
out many apparently impossible constituents 
(Xue and Palmer, 2004); (3) semantic role 
identification (SRI) which identifies the 
constituent that will be the semantic role of a 
predicate in a sentence, and (4) semantic role 
classification (SRC) which determines the type 
of identified semantic role. The machine learning 
method takes PAS as the classification unit 
which consists of a target predicate and an 
argument candidate. The SRI step utilizes a 
binary classifier to determine whether the 
argument candidate in the PAS is a real argument. 
Finally, in the SRC step, the semantic role tag 
and the event role tag of each identified 
argument can be obtained by two multi-value 
classifications on the SRI results. The remaining 
phrase type tag can be directly extracted from the 
constituent parsing tree.  
The selection of the feature set is the most 
important factor for the feature-based SRL 
method. In addition to constituent-based features 
and dependency-based features, we also consider 
WSD-based features. To our knowledge, the 
combined use of constituents-based syntactic 
features and dependency-based syntactic features 
is the first attempts to use them both on the 
feature level of SRL. As a prevalent kind of 
syntactic features for SRL, constituent-based 
features have been extensively studied by many 
researchers. In this module, we use 34 
constituent-based features, 35 dependency-based 
features, and 2 WSD-based features. Among the 
constituent-based features, 26 features are 
manually selected from effective features proven 
by existing SRL studies and 8 new features are 
305
defined by us. Firstly, the 26 constituent-based 
features used by others are: 
y predicate (c1), path (c2), phrase type (c3), 
position (c4), voice (c5), head word (c6), 
predicate subcategorization (c7), syntactic 
frame (c8), head word POS (c9), partial path 
(c10), first/last word (c11/c12), first/last POS 
(c13/c14), left/right sibling type (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree 
distance (c21), temporal cue words (c22), 
Predicate POS (c23), argument's parent 
type(c24), argument's parent head (c25) and 
argument's parent POS (c26). 
And the 8 new features we define are: 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue word.  
y POS pattern of argument (c28): the left-to-
right chain of POS tags of argument's children. 
y Phrase type pattern of argument (c29): the 
left-to-right chain of phrase type labels of 
argument's children. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child. 
y Type of LCA and right child (c31): The phrase 
type of the LCA combined with its right child. 
y Three features: word bag of path (c32), word 
bag of POS pattern (c33) and word bag of type 
pattern (c34), for generalizing three sparse 
features: path (c7), POS pattern argument (c28) 
and phrase type pattern of argument (c29) by 
the bag-of-words representation. 
Secondly, the selection of dependency-based 
features is similar to that of constituent-based 
features. But dependency parsing lacks 
constituent information. If we want to use 
dependency-based features to label constituents, 
we should map a constituent to one or more 
appropriate words in dependency trees. Here we 
use head word of a constituent to represent it in 
dependency parses. The 35 dependency-based 
features we adopt are:  
y Predicate/Argument relation (d1/d2), relation 
path (d3), POS pattern of predicate?s children 
(d4), relation pattern of predicate?s children 
(d5) , child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21), left/right sibling 
relation (d22/d23), dep-exists (d24) and dep-
type (d25), POS path (d26), POS path length 
(d27), relation path length (d28), high/low 
support verb (d29/d30), high/low support noun 
(d31/d32) and LCA?s word/POS/relation 
(d33/d34/d35). 
In this work, the dependency parse trees are 
generated from the constituent parse trees using a 
constituent-to-dependency converter (Marneffe 
et al, 2006). The converter is suitable for 
semantic analysis as it can retrieve the semantic 
head rather than the general syntactic head.  
Lastly, the 2 WSD-based features are: 
y Situation description formula (s1): predicate?s 
situation description formula generated by the 
WSD module. 
y Natural explanation (s2): predicate?s natural 
explanation generated by the WSD module. 
3 Experimental Results and Discussion 
3.1 Target Verb WSD 
System Micro-A (%) Macro-A (%) Rank
BL_3 81.30 83.81 3/7 
X_3 79.82 82.58 4/7 
BL_15 79.23 82.18 5/7 
X_15 77.74 81.42 6/7 
Table 2: Official results of the WSD systems 
Table 2 shows the official result of the WSD 
system. BL_3 with window size three using the 
original training corpus achieves the best result 
in our submission. It indicates the local features 
are more effective in our systems. There are two 
possible reasons why the performances of the X 
system with instance expansion are lower than 
the BL system. First, the additional instances 
extracted based on 3-gram provide a few local 
features but many topical features. But, local 
features are more effective for our systems as 
mentioned above. The local feature related 
information that the classifier gets from the 
additional instances is not sufficient. Second, the 
granularity of the WSD module is too small to be 
distinguished by 3-grams. As a result, the 
additional corpus built upon 3-gram has more 
exceptional instances (noises), and therefore it 
impairs the performance of X_3 and X_15. 
Taking the verb ??? ? (belong to ) as an 
example, it has two senses in the task, but both 
senses have the same natural explanation: ???
?????????? (part of or belong to), 
which is always considered as the sense in 
general SRL. The difference between the two 
senses is in their situation description formulas: 
?partof (x,y)+NULL? vs. ?belongto (x,y)+NULL?.  
306
3.2 Sentence SRL and Event Detection 
In the SRL module, we use the training data 
provided by SemEval-2010 to train the SVM 
classifiers without any external resources. The 
training data contain 4,608 sentences, 100 target 
predicates and 13,926 arguments. We use the 
SVM-Light Toolkit (Joachims, 1999) for the 
implementation of SVM, and use the Stanford 
Parser (Levy and Manning, 2003) as the parser 
and the constituent-to-dependency converter. We 
employ the linear kernel for SVM and set the 
regularization parameter to the default value 
which is the reciprocal of the average Euclidean 
norm of the training data. The evaluation results 
of our SRL module on the official test data are 
shown in Table 3, where ?AB?, ?SR?, ?PT? and 
?ER? represent argument boundary, semantic role 
tag, phrase type tag, and event role tag. 
 
Tag Precision(%) Recall(%) F-Score(%)
AB 73.10 66.83 69.82 
AB+SR 67.44 61.65 64.42 
AB+PT 61.78 56.48 59.01 
AB+ER 69.05 63.12 65.95 
Overall 58.33 53.32 55.71 
Table 3: Official results of the SRL system 
It is clear that ?AB? plays an important role as 
the labeling of the other three tags is directly 
based on it. Through analyzing the results, we 
find that errors in the recognition of ?AB? are 
mainly caused by two factors: the automatic 
constituent parsing and the pruning algorithm. It 
is inevitable that some constituents and 
hierarchical relations are misidentified in 
automatic parsing of Chinese. These errors are 
further enlarged by the heuristic-based pruning 
algorithm because the algorithm is built upon the 
gold-standard paring trees, and therefore a lot of 
real arguments are pruned out when using the 
noisy automatic parses. So the pruning algorithm 
is the current bottleneck of SRL in the evaluation.  
 
System Micro-A (%) Macro-A (%) Rank
BL_3 20.33 20.19 4/7 
X_3 20.05 20.23 5/7 
BL_15 20.05 20.22 6/7 
X_15 20.05 20.14 7/7 
Table 4: Official results of the ED systems 
From the fact that the results of ?AB+SR? and 
?AB+ER? are close to that of ?AB?, it can be 
inferred that the SR and ER results should be 
satisfactory if the errors in ?AB? are not 
propagated. Furthermore, the result of ?AB+PT? 
is low as the phrase types here is inconsistent 
with those in Stanford Parser. The problem 
should be improved by a set of mapping rules. 
Finally, in the ED module, we combine the 
results of WSD and SRL by filling variables of 
the situation description formula obtained by the 
WSD module with the arguments obtained by the 
SRL module according to their event role tags. 
Table 4 shows the final results which are 
generated by combining the results of WSD and 
SRL. Obviously the reduced overall ranking 
comparing to WSD is due to the SRL module. 
4 Conclusions 
In this paper, we propose a modular approach for 
the SemEval-2010 Task on Chinese event 
detection. Our system consists of three modules: 
WSD, SRL and ED. The WSD module is based 
on instances expansion, and the SRL module is 
based on rich syntactic features. Evaluation 
results show that our system is good at WSD, 
semantic role tagging and event role tagging, but 
poor at pruning and boundary detection. In future 
studies, we will modify the pruning algorithm to 
reduce the bottleneck of the current system. 
 
Acknowledgments 
This work is partially supported by the Hong 
Kong Polytechnic University under Grant No. G-
U297 and G-U596, and by the National Natural 
Science Foundation of China under Grant No. 
60736014 and 60803094. 
References  
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank. 
Proceedings of ACL-2003. 
Vasin Punyakanok, Dan Roth, and Wentau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. Proceedings of IJCAI-2005. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen 
Yu. 2006. A Chinese corpus with word sense 
annotation. Proceedings of ICCPOL-2006. 
David Yarowsky. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. 
Qiang Zhou. 2010. SemEval-2010 task 11: Event 
detection in Chinese News Sentences. Proceedings 
of SemEval-2010. 
307
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 524?528,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
PolyUCOMP: Combining Semantic Vectors with Skip bigrams for  
Semantic Textual Similarity 
 
Jian Xu Qin Lu Zhengzhong Liu 
The Hong Kong Polytechnic University 
Department of Computing 
Hung Hom, Kowloon, Hong Kong 
{csjxu, csluqin, hector.liu}@comp.polyu.edu.hk 
 
 
Abstract 
This paper presents the work of the Hong 
Kong Polytechnic University (PolyUCOMP) 
team which has participated in the Semantic 
Textual Similarity task of SemEval-2012. The 
PolyUCOMP system combines semantic vec-
tors with skip bigrams to determine sentence 
similarity. The semantic vector is used to 
compute similarities between sentence pairs 
using the lexical database WordNet and the 
Wikipedia corpus. The use of skip bigram is 
to introduce the order of words in measuring 
sentence similarity.  
1 Introduction 
Sentence similarity computation plays an im-
portant role in text summarization, classification, 
question answering and social network applica-
tions (Lin and Pantel, 2001; Erkan and Radev, 
2004; Ko et al, 2004; Ou et al, 2011).  The 
SemEval 2012 competition includes a task targeted 
at Semantic Textual Similarity (STS) between sen-
tence pairs (Eneko et al, 2012). Given a set of sen-
tence pairs, participants are required to assign to 
each sentence pair a similarity score. 
Because a sentence has only a limited amount of 
content words, it is not easy to determine sentence 
similarities because of the sparseness issue. 
Hatzivassiloglou et al (1999) proposed to use lin-
guistic features as indicators of text similarity to 
address the problem of sparse representation of 
sentences. Mihalcea et al (2006) measured sen-
tence similarity using component words in sen-
tences. Li et al (2006) proposed to incorporate the 
semantic vector and word order to calculate sen-
tence similarity.  
In our approach to the STS task, semantic vector 
is used and the semantic relatedness between 
words is derived from two sources: WordNet and 
Wikipedia. Because WordNet is limited in its cov-
erage, Wikipedia is used as a candidate for deter-
mining word similarity.  
Word order, however, is not considered in se-
mantic vector. As semantic information are coded 
in sentences according to its order of writing, and 
in our systems, content words may not be adjacent 
to each other, we proposed to use skip bigrams to 
represent the structure of sentences. Skip bigrams, 
generally speaking, are pairs of words in a sen-
tence order with arbitrary gap (Lin and Och, 
2004a). Different from the previous skip bigram 
statistics which compare sentence similarities 
through overlapping skip bigrams (Lin and Och, 
2004a), the skip bigrams we used are weighted by 
a decaying factor of the skipping gap in a sentence, 
giving higher scores to closer occurrences of skip 
bigrams. It is reasonable to assume that similar 
sentences should have more overlapping skip bi-
grams, and the gaps in their shared skip bigrams 
should also be similar.  
The rest of this paper is organized as followed. 
Section 2 describes sentence similarity using se-
mantic vectors and the order-sensitive skip bigrams. 
Section 3 gives the performance evaluation. Sec-
tion 4 is the conclusion.   
2 Similarity between Sentences 
Words are used to represent a sentence in the 
vector space model. Semantic vectors are con-
structed for sentence representations with each en-
try corresponding to a word. Since the semantic 
vector does not consider word order, we further 
proposed to use skip bigrams to represent sentence 
structure. Moreover, these skip bigrams are 
524
weighted by a decaying factor based on the so 
called skip distance in the sentence.  
2.1 Sentence similarity using Semantic 
Vector 
Given a sentence pair, S1 and S2, for example, 
S1: Chairman Michael Powell and FCC colleagues at 
the Wednesday hearing. 
S2: FCC chief Michael Powell presides over hearing 
Monday. 
The term set of the vector space is first formed 
by taking only the content words in both sentences, 
T={chairman, chief, colleagues, fcc, hearing, michael, 
monday, powell, presides, wednesday } 
Each entry of the semantic vector corresponds to 
a word in the joint word set (Li et al, 2006). Then, 
the vector for each sentence is formed in two steps: 
For a word both in the term set T and in the sen-
tence, the value for this word entry is set to 1. If a 
word is not in the sentence, the most similar word 
in the sentence will then be identified, and the cor-
responding path similarity value will be assigned 
to this entry. Let T be the term set with a sorted list 
of content words, T=(t1, t2,?, tn). Without loss of 
generality, let a sentence S=(w1 w2?wm) where wj 
is a content word and wj is a word in T. Let the 
vector space of the sentence S be VSs = (v1, v2, ?, 
vn). Then the value of vi is assigned as follows, 
 
where the similarity function SIM(ti, wj) is calcu-
lated according to the path measure (Pedersen et 
al., 2004) using the WordNet, formally defined as, 
),(1),( jiji wtdistwtSIM ?
 
where dist(ti, wj) is the shortest path from  ti, to 
wj by counting nodes in the WordNet taxonomy. 
Based on this, the semantic vectors for the two ex-
ample sentences will be,  
SVS1 = (1, 0.25, 1, 1, 1, 1, 0.33, 1, 0, 1) and 
SVS2 = (0.25, 1, 0, 1, 1, 1, 1, 1, 1, 0.33) 
Based on the two semantic vectors, the cosine 
metric is used to measure sentence similarity. In 
the WordNet, the entry chairman in the joint set is 
most similar to the word chief in sentence S2. In 
practice, however, this entry might be closer to the 
word presides than to the word chief. Therefore, 
we try to obtain the semantic relatedness using the 
Wikipedia for sentence T and find that the entry 
chairman is closest to the word presides. The Wik-
ipedia-based word relatedness utilizes the hyper-
link structure (Milne & Witten, 2008).  It first 
identifies the candidate articles, a and b, that dis-
cuss ti and wj respectively in this case and then 
compute relatedness between these articles, 
|))||,log(min(||)log(|
)log(|))||,log(max(|),( BAW
BABAbarel ?
???
 
where A and B are sets of articles that link to a 
and b. W is the set of all articles in the Wikipedia. 
Finally, two articles that represent ti and wj are se-
lected and their relatedness score is assigned to 
SIM(ti, wj).  
2.2 Sentence Similarity by Skip bigrams 
Skip bigrams are pairs of words in a sentence 
order with arbitrary gaps. They contain the order-
sensitive information between two words. The skip 
bigrams of a sentence are extracted as features 
which will be stacked in a vector space. Each skip 
bigram is weighted by a decaying factor with its 
skip distances in the sentence. To illustrate this, 
consider the following sentences S and T: 
S =  w1 w2 w1 w3 w4   and    T =  w2 w1 w4 w5 w4 
where w denotes a word. It can be used more 
than once in a sentence. Each sentence above has a 
C(5, 2) 1 = 10 skip bigrams. 
The sentence S has the following skip bigrams: 
?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1?, 
?w2w3? , ?w2w4? , ?w1w3?, ?w1w4?, ?w3w4? 
The sentence T has the following skip bigrams: 
? 2w1?, ?w2w4?, ?w2w5?, ?w2w4?, ?w1w4?, 
?w1w5? , ?w1w4? , ?w4w5?, ?w4w4?, ?w5w4? 
In the sentence S, we have two repeated skip bi-
grams ?w1w4? and ?w1w3?. In the sentence T, we 
have ?w2w4? and ?w1w4? repeated twice. In this 
case, the weight of the recurring skip bigrams will 
be increased. Hereafter, vectors for S and T will be 
                                                          
1 Combination: C(5,2)=5!/(2!*3!)=10. 
525
formulated with each entry corresponding to a dis-
tinctive skip bigram.  
VS = (?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1, 
?w2w3?, ?w2w4?, ?w3w4?)? 
VT = (?w2w1?, ?w2w4?, ?w2w5?, ?w1w4?, ?w1w5?, 
?w4w5?, ?w4w4?, ?w5w4?)? 
Now, the question remains how to weight the 
skip bigrams. Given?  as a finite word set, let 
S=w1w2?w|S| be a sentence, wi??and 1?i?|S|. 
A skip bigram of S, denoted by u, is defined by an 
index set I=(i1, i2) of S (1?i1<i2?|S| and u=S[I]). 
The skip distance of S[I] , denoted by du (I), is the 
skip distance of the first word and the second word 
of u, calculated by i2-i1+1. For example, if S is the 
sentence of w1w2w1w3w4 and u = w1w4, then there 
are two index sets, I1=[3,5] and I2=[1,5] such that 
u=S[3,5] and u=S[1,5], and the skip distances of 
S[3,5] and S[1,5] are 3 and 5. The weight of a skip 
bigram u for a sentence S with all its possible oc-
currences, denoted by ( )u S? , is defined as: 
( )
: [ ]
( ) ud Iu I u S IS? ??? ?
 
where ? is the decay factor which penalizes the 
longer skip distance of a skip bigram. By doing so, 
for the sentence S, the complete word set is ?={w1, 
w2, w3,w4}. The weights for the skip bigrams are 
listed in Table 1: 
u 
)(Su?
 u 
)(Su?
 
21ww   2?  12ww  2?  
11ww  3?  32ww  3?  
31ww  24 ?? ?  42ww  4?  
41ww  35 ?? ?  43ww  2?  
Table 1: Skip bigrams and their Weights in S 
In Table 1, if ? is set to 0.25, the weight of the 
skip bigram w1w2 in S is 0.25
2=0.0625, and w1w3 is 
0.254 +0.252=0.064. Similarly, the skip bigrams 
and weights in the sentence T can be obtained. 
With the skip bigram-based vectors, cosine metric 
is then used to compute similarity between S and T. 
3 Experiments 
In the STS task, three training datasets are avail-
able: MSR-Paraphrase, MSR-Video and 
SMTeuroparl (Eneko et al, 2012). The number of 
sentence pairs for three dataset is 750, 750 and 734.  
In the following experiments, Let SWN, SWIKI
 and 
SSKIP denote similarity measures of the vector space 
representation using WordNet, Wikipedia and skip 
bigrams, respectively. The three similarity 
measures are linearly combined as SCOMB: 
SKIPWIKIWNCOMB SSSS ???????? )1( ????  
where ? and ? are weight factors for SWN and 
SWIKI in the range [0,1].  If ? is set to 1, only the 
WordNet-based similarity measure is used; if ? is 0, 
the Wikipedia and skip bigram measures are used.  
Because each dataset has a different representa-
tion for sentences, the parameter configurations for 
them are different. For the word similarity using 
the lexical resource WordNet, the path measure is 
used in experiments. To get word relatedness from 
the English Wikipedia, the Wikipedia Miner tool2 
is used. When computing sentence similarity based 
on the skip bigrams, the decaying factor (DF) must 
be specified beforehand. Hence, parameter config-
urations for the three datasets are listed in Table 2: 
 
Table 2: Parameter Configurations 
In the testing phase, five testing dataset are pro-
vided. In addition to three test datasets drawn from 
the publicly available datasets used in the training 
phase, two surprise datasets are given. They are 
SMTnews and OnWN (Eneko et al, 2012). 
SMTnews has 399 pairs of sentences and OnWN 
contains 750 sentence pairs. The parameter config-
urations for these two surprise datasets are the 
same as those for the dataset MSR-Paraphrase. 
The official scoring is based on Pearson correla-
tion. If the system gives the similarity scores close 
to the reference answers, the system will attain a 
high correlation value. Besides, three other evalua-
tion metrics (ALL, ALLnrm, Mean) based on the 
Pearson correlation are used (Eneko et al, 2012).  
Among the 89 submitted systems, the results of 
our system are given in Table 3: 
Run ALL Rank ALLnrm RankNrm Mean RankMean
PolyUCOMP 0.6528 31 0.7642 59 0.5492 51 
Table 3: Performance using Different Metrics 
                                                          
2 http://wikipedia-miner.cms.waikato.ac.nz/ 
526
Using the ALL metric, our system ranks 31, but 
for ALLnrm and Mean metrics, our system ranking 
is decreased to 59 and 51. In terms of ALL metric, 
our system achieves a medium performance, im-
plying that our system correlates well with human 
assessments. In terms of ALLnrm and Mean met-
rics, our system performance degrades a lot, imply-
ing that our system is not well correlated with the 
reference answer when each dataset is normalized 
into the aggregated dataset using the least square 
error or the weighted mean across the datasets.  
To see how well each of the individual vector 
space models performed on the evaluation sets, we 
experiment on the five datasets using vectors based 
on WordNet, Wikipedia (Wiki), SkipBigram and 
PolyuCOMP (a combination of the three vectors). 
Table 4 gives detailed results of each dataset. 
 
Table 4: Pearson Correlation for each Dataset 
Table 4 shows that after combining three vector 
representations, each dataset obtains the best per-
formance. The WordNet-based approach gives a 
better performance than Wikipedia-based approach 
in MSRvid dataset. The two approaches, however, 
give similar performance in other four datasets. 
This is because the sentences in the MSRvid da-
taset are too short with limited amount of content 
words. It is difficult to capture the meaning of a 
sentence without distinguishing words in consecu-
tive positions. This is why the order-sensitive 
SkipBigram approach gives better performance 
than the other two approaches. For example, 
A woman is playing a game with a man. 
A man is playing piano. 
Using the semantic vectors, we will get high 
similarity scores, but the two sentences are dissimi-
lar. If the skip bigram approach is used, the simi-
larity score between sentences will be 0, which 
correlates with human judgment. In parameter con-
figurations for the MSRvid dataset, higher weight 
(1-0.123-0.01=0.867) is also given to skip bigrams. 
It is interesting to note that the decaying factor for 
this dataset is 1.4 and is not in the range from 0 to 
1 inclusive. This is because higher decaying factor 
helps to capture semantic meaning between words 
that span afar. For example, 
A man is playing a flute. 
A man is playing a bamboo flute. 
In this sentence pair, the second sentence is en-
tailed by the first one. The similarity can be cap-
tured by assigned larger decay factor to weigh the 
skip bigram ?playing flute? in two sentences. 
Hence, if the value of the decay factor is greater 
than 1, the two sentences will become much more 
similar. After careful investigation, these two sen-
tences are similar to a large extent. In this sense, a 
higher decaying factor would help capture the 
meaning between sentence pairs. This is quite dif-
ferent from the other four datasets which focus on 
shared skip bigrams with smaller decaying factor. 
4 Conclusions and Future Work 
In the Semantic Textual Similarity task of 
SemEval-2012, we proposed to combine the se-
mantic vector with the order-sensitive skip bigrams 
to capture the meaning between sentences. First, a 
semantic vector is derived from either the 
WordNet or Wikipedia. The WordNet simulates 
the common human knowledge about word con-
cepts. However, WordNet is limited in its word 
coverage. To remedy this, Wikipedia is used to 
obtain the semantic relatedness between words. 
Second, the proposed approach also considers the 
impact of word order in sentence similarity by us-
ing skip bigrams. Finally, the overall sentence sim-
ilarity is defined as a linear combination of the 
three similarity metrics. However, our system is 
limited in its approaches. In future work, we would 
like to apply machine learning approach in deter-
mining sentence similarity. 
 
527
References  
David Milne , Ian H. Witten. 2008. An Effective, Low-
cost Measure of Semantic Relatedness Obtained from 
Wikipedia Links. In Proceedings of the first AAAI 
Workshop on Wikipedia and Artificial Intelligence 
(WIKIAI'08), Chicago, I.L 
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343-360. 
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on 
Semantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evaluation 
(SemEval 2012), in conjunction with the First Joint 
Conference on Lexical and Computational Semantics 
(*SEM 2012). 
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank: 
Graph-based Lexical Centrality as Salience in Text 
Summarization. Journal of Artificial Intelligence Re-
search, 22: 457?479. 
Lin, Chin-Yew and Franz Josef Och. 2004a. Automatic 
Evaluation of Machine Translation Quality Using 
Longest Common Subsequence and Skip bigram Sta-
tistics. In Proceedings of the 42nd Annual Meeting of 
the Association for Computational Linguistics (ACL 
2004), Barcelona, Spain. 
Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang.  
2011. Transferring Topical Knowledge from Auxilia-
ry Long Text for Short Text Understanding. In: Pro-
ceedings of the 20th ACM Conference on 
Information and Knowledge Management (ACM 
CIKM 2011). Glasgow, UK. 
Rada Mihalcea and Courtney Corley. 2006. Corpus-
based and Knowledge-based Measures of Text Se-
mantic Similarity. In Proceeding of the Twenty-First 
National Conference on Artificial Intelligence and 
the Eighteenth Innovative Applications of Artificial 
Intelligence Conference. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity?Measuring 
the Relatedness of Concepts. In Proceedings of the 
19th National Conference on Artificial Intelligence 
(AAAI, San Jose, CA), pages 144?152. 
Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar 
Eskin. 1999. Detecting Text Similarity over Short 
Passages: Exploring Linguistic Feature Combinations 
via Machine Learning. In Proceeding of Empirical 
Methods in natural language processing and Very 
Large Corpora. 
Youngjoong Ko,  Jinwoo Park, and Jungyun Seo. 2004. 
Improving Text Categorization using the Importance 
of Sentences. Information Processingand Manage-
ment, 40(1): 65?79. 
Yuhua Li, David Mclean, Zuhair B, James D. O'shea 
and Keeley Crockett. 2006. Sentence Similarity 
Based on Semantic Nets and Corpus Statistics. IEEE 
Transactions on Knowledge and Data Engineering, 
18(8), 1138?1149. 
 
 
 
 
528
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 90?95, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
PolyUCOMP-CORE TYPED: Computing Semantic Textual Similarity
using Overlapped Senses
Jian Xu Qin Lu
The Hong Kong Polytechnic University
Department of Computing
Hung Hom, Kowloon, Hong Kong
{csjxu, csluqin}@comp.polyu.edu.hk
Abstract
The Semantic Textual Similarity (STS)
task aims to exam the degree of semantic
equivalence between sentences (Agirre et
al., 2012). This paper presents the work
of the Hong Kong Polytechnic University
(PolyUCOMP) team which has participated
in the STS core and typed tasks of SemEval-
2013. For the STS core task, the PolyUCOMP
system disambiguates words senses using
contexts and then determine sentence
similarity by counting the number of senses
they shared. For the STS typed task, the string
kernel (Lodhi et al, 2002) is used to compute
similarity between two entities to avoid string
variations in entities.
1 Introduction
Sentence similarity computation plays an important
role in text summarization and social network
applications (Erkan et al, 2004; Jin et al, 2011).
The SemEval 2012 competition initiated a task
targeted at Semantic Textual Similarity (STS)
between sentence pairs (Agirre et al, 2012). Given
a set of sentence pairs, participants are required to
assign to each sentence pair a similarity score.
Because a sentence has only a limited amount of
content words, it is difficult to determine sentence
similarities. To solve this problem, Hatzivassiloglou
et al (1999) proposed to use linguistic features
as indicators of text similarity to address the
problem of sparse representation of sentences.
Mihalcea et al (2006) measured sentence similarity
using component words in sentences. Li et al
(2006) proposed to incorporate the semantic vector
and word order to calculate sentence similarity.
Biemann et al (2012) applied the log-linear
regression model by combining the simple string
based measures, for example, word ngrams and
semantic similarity measures, for example, textual
entailment. Similarly, Saric et al (2012) used a
support vector regression model which incorporates
features computed from sentence pairs. The features
are knowledge- and corpus-based word similarity,
ngram overlaps, WordNet augmented word overlap,
syntactic features and so on. Xu et al (2012)
combined semantic vectors with skip bigrams to
determine sentence similarity, whereas the skip
bigrams take into the sequential order between
words.
In our approach to the STS task, words in
sentences are assigned with appropriate senses using
their contexts. Sentence similarity is computed by
calculating the number of shared senses in both
sentences since it is reasonable to assume that
similar sentences should have more overlapping
senses. For the STS-TYPED task, variations
might occur in author names, people involved,
time expression and location. Thus, string kernel
is applied to compute similarity between entities
because it can capture variations between entities.
Moreover, for the event similarity in STS-TYPED
task, semantic relatedness between verbs is derived
the WordNet.
The rest of this paper is structured as follows.
Section 2 describes sentence similarity using sense
overlapping and string kernel. Section 3 gives the
performance evaluation. Section 4 is the conclusion.
90
2 Similarity between Sentences
Words are used to convey meaning in a sentence.
They are tagged with appropriate senses initially and
then sentence similarity is calculated based on the
number of shared senses.
2.1 Sense Overlapping
When comparing word features, we did not compare
their surface equality, but we first conceptualize
these words and then calculate their similarities
based on the hierarchial structure in WordNet. For a
word in a sentence, it will be assigned a WordNet
sense. In this paper, we focus on the Word
Sense Disambiguation (WSD) algorithm taken by
Banerjee and Pederson (2003). They measured the
semantic relatedness between concepts by counting
the shared words in their WordNet glosses.
In WordNet, a word sense is represented by a
synset which has a gloss that defines the concept
that it represents. For example, the words walking,
afoot, ambulate constitute a single synset which has
gloss representations as follows,
walking: the act of traveling by foot
afoot: traveling by foot
ambulate: walk about
To lift the limitations of dictionary glosses which
are fairly short with insufficient vocabulary, we
utilize the glosses of related senses since we assume
that words co-occur in one sentence share related
senses and the more glosses two senses share, the
more similar they are. Therefore, we extract not
only glosses of target synset, but also the glosses
of the hypernym, hyponym, meronym, holonym and
troponym synsets of the target synset to form a
synset context. Finally, we compare the sentence
contexts with different synset contexts to determine
which sense should be assigned to the words.
To disambiguate word senses, a window of
contexts surrounding the the target word is specified
and a set of candidate word senses are extracted for
the content word (noun, verb, adjective) within that
window. Let the current target word index i = 0 that
is,w0, the window size be 2n+1 and?n ? i ? +n.
Let |wi| be the number of senses for word wi and the
jth sense of wi is si,j , where 1 ? j ? |wi|. Next is
to assign an appropriate sense k to the target word.
We achieve this by adding together the relatedness
scores calculated by comparing the senses of the
target word and senses of every non-target word
within the window of context. The sense score for
the current target word w0 is defined as,
Sensek =
n?
i=?n
|wi|?
j=1
relatedness(s0,k, si,j) (1)
The kth sense which has the biggest sense score
will be chosen as the right sense for the target word
w0. Now remains the question of how to define the
relatedness between two synsets. It is defined as,
relatedness(s0,k, si,j) =
score(gloss(s0,k), gloss(si,j))
+score(hype(s0,k), hype(si,j))
+score(hypo(s0,k), hypo(si,j))
+score(hype(s0,k), gloss(si,j))
+score(gloss(s0,k), hype(si,j))
(2)
In Equation 2, the score function counts the
number of overlapping words between two glosses.
However, if there is a phrasal n-word overlap, then
a score of n2 will be assigned, thus encouraging the
longer n-word overlap. Let V denote the set of n-
word overlaps shared between two glosses, the score
is defined as,
score =
?
w?V
?w?2 (3)
where ?w? refers to the number of words in w. In
so doing, we can have corresponding senses for the
sentence Castro celebrates 86th birthday Monday
as follows,
castro/10886929-n celebrate/02490877-v
birthday/15250178-n monday/15163979-n
To find the n-word overlap, we found that
contiguous words in two glosses lie in the diagonal
of a matrix, take the senses walk and afoot for
example, their glosses are,
walking: the act of traveling by foot
afoot: traveling by foot
91
Place the walking glosses in rows and afoot
glosses in columns, we get the matrix representation
in Figure 1,
Figure 1: n-word overlap representation
Figure 1 shows that travel by foot is a continuous
sequence of words shared by two glosses. Steps to
find n-word overlapping are:
(1) Construct a matrix for two sentences;
(2) Get continuous n-word overlapping, n is
greater than 1;
(3) Set the cell values to 0 if they are contained in
continuous n-word.
(4) Get the words (unigrams) which are shared by
two sentences.
Take a b c d and b c a d for example, we will have
the matrix as follows,
b c a d
a 0 0 1 0
b 1 0 0 0
c 0 1 0 0
d 0 0 0 1
Table 1: Matrix representation for two sentences
By the step 2, we will get the b c and its
corresponding cells cell(1,0) and cell(2,1). We then
set the two cells to zero, and obtain an updated
matrix as follows,
b c a d
a 0 0 1 0
b 0 0 0 0
c 0 0 0 0
d 0 0 0 1
Table 2: Updated matrix representation for two sentences
In Table 2, we found that cell(0,2) and cell(3,3)
have values greater than zero. Therefore, a and b
will be extracted the common terms.
This approach can also be applied to find common
n-word overlaps between sentences, for example,
s1: Olli Heinonen, the Head of the International
Atomic Energy Agency delegation to Iran, declared
yesterday that the agency has reached an agreement
with Tehran on the method of conducting the
negotiations pertaining to its nuclear program.
s2: leader of international atomic energy agency
delegation to iran , olli heinonen said yesterday ,
that the agency concluded a mutual understanding
with tehran on the way to manage talks depending
upon its atomic program .
We will have ngrams with n ranging from 1 to 7,
such as,
unigram: of, to, its, program, yesterday
bigram: olli heinonen
trigram: that the agency
four-gram: with tehran on the
seven-gram: international atomic energy agency
delegation to iran
Similarity between two sentences is calculated by
counting the number of overlapped n-words. The
similarity for s1 and s2 is, (1 + 1 + 1 + 1 + 1) +
(2)2 + (3)2 + (4)2 + (7)2 = 83.
2.2 String kernel
For the STS-TYPED task, when comparing whether
people or authors are similar or not, we found that
some entity mentions may have tiny variations, for
example,
E Vincent Harris and E.Vincent Harris
The difference between the entities lies in fact that
the second entity has one more dot. In this case,
string kernel would be a good choice in verifying
they are similar or not. If we consider n=2, we obtain
79-dimensional feature space where the two entities
are mapped in Table 3.
In Table 3, ? is the decay factor, in the range
of [0,1], that penalizes the longer distance of a
subsequence. Formally, string kernel is defined as,
Kn(s, t) =
?
u?
?n
??u(s) ? ?u(t)? (4)
92
ev ei en ? ? ? e. ? ? ? rs is
?(evincentharris) ?2 ?3 + ?13 ?2 + ?4 + ?7 ? ? ? 0 ? ? ? ?3 + ?4 ?2 + ?12
?(e.vincentharris) ?3 ?4 + ?14 ?2 + ?5 + ?8 ? ? ? ?2 ? ? ? ?3 + ?4 ?2 + ?12
Table 3: Feature mapping for two entities
TEAM headlines OnWN FNWN SMT mean rank
RUN1 0.5176 0.1517 0.2496 0.2914 0.3284 77
Table 4: Experimental results for STS-CORE
where
?n is the set of all possible subsequences
of length n. u indicates an item in the set, for
example, the subsequence ev in Table 3. ?u(s) is
the feature mapping of the subsequences in s. In
so doing, we can have similarity between entities in
Table 3 as follows:
Kn(s, t) = ?2? ?3 + (?3 + ?13)? (?4 + ?14) +
? ? ?+(?3+?4)?(?3+?4)+(?2+?12)?(?2+?12)
To avoid enumeration of all subsequences for
similarity measurement, dynamic programming,
similar to the method by Lodhi et al (2002) is used
here for similarity calculation.
3 Experiments
The STS-CORE task is to quantify how similar
two sentences are. We simply use the sense
overlapping approach to compute the similarity.
Since this approach needs to find appropriate senses
for each word based on its contexts. The number
of contextual words is set to 5. Experiments
are conducted on four datasets. They are:
headlines mined from news sources by European
Media, OnWN extracted from from WordNet and
OntoNotes, FNWN from WordNet and FrameNet
and SMT dataset from DARPA GALE HTER and
HyTER. The results of our system (PolyUCOMP-
RUN1) are given in Table 4 ,
Our system achieves rather lower performance
in the OnWN and FNWN datasets. This is because
it is difficult to use contextual terms to find the
correct senses for words in sentences of these two
datasets. Take the two sentences in OnWN dataset
for example,
s1: the act of choosing among alternatives
s2: the act of changing one thing for another
thing.
The valid concepts for the two sentences are:
c1: 06532095-n 05790944-n
c2: 00030358-n 00126264-v 00002452-n
00002452-n
c1 and c2 have no shared senses, resulting in a
zero similarity between s1 and s2. However, s1 and
s2 should have the same meaning. Moreover, in the
FNWN dataset, the sentence lengths are unbalanced,
for example,
s1: there exist a number of different possible
events that may happen in the future. in most cases,
there is an agent involved who has to consider which
of the possible events will or should occur. a salient
entity which is deeply involved in the event may also
be mentioned.
s2: doing as one pleases or chooses;
s1 has 48 tokens with punctuations being
excluded and s2 has only 6 tokens. This would affect
our system performance as well.
For the STS-TYPED task, data set is taken
from Europeana, which provides millions of books,
paintings, films, museum objects and archival
records that have been digitised throughout Europe.
Each item has one line per type, where the type
can be the title of a record, list of subject terms,
textual description of the record, creator of the
record and date of the record. Participating systems
are supposed to compute similarities between semi-
structured items. In this task, we take the strategies
in Table 5,
Jaccard denotes the Jaccard similarity measure.
Stringkernel + Jaccard means that two types
are similar if they share many terms, for example,
93
TEAM general author people time location event subject description mean rank
RUN1 0.4888 0.6940 0.3223 0.3820 0.3621 0.1625 0.3962 0.4816 0.4112 12
RUN2 0.4893 0.6940 0.3253 0.3777 0.3628 0.1968 0.3962 0.4816 0.4155 11
RUN3 0.4915 0.6940 0.3254 0.3737 0.3667 0.2207 0.3962 0.4816 0.4187 10
Table 6: Experimental results for STS-TYPED
Type Strategy
author String kernel
people String kernel + Jaccard
time String kernel + Jaccard
location String kernel + Jaccard
event WordNet + Jaccard
subject Sense overlapping
description Sense overlapping
Table 5: Strategies for computing similarity
location; and string kernel is used to determine
whether two locations are similar or not. For the
type of event, we extract verbs from records and
count the number of shared verbs between two
records. The verb similarity is obtained through
WordNet. The general similarity is equal to the
average of the 7 scores. Also, Stanford CoreNLP
tool1 is used to extract author, date, time, location
and handle part-of-speech tagging.
In this STS-TYPED task, we use string kernel and
WordNet to determine whether two terms are similar
and increase the number of counts if their similarity
exceeds a certain threshold. Therefore, we have
chosen 0.4, 0.5 and 0.6 in a heuristic manner and
obtained three different runs. Experimental results
are given in Table 6.
Since the types of author, subject and
description are not related to either string kernel
or WordNet, their performances remain unchanged
during three runs.
4 Conclusions and Future Work
In the Semantic Textual Similarity task of SemEval-
2013, to capture the meaning between sentences,
we proposed to disambiguate word senses using
contexts and then determine sentence similarity
by counting the senses they shared. First, word
senses are disambiguated by means of the contextual
1http://nlp.stanford.edu/software/corenlp.shtml
words. When determining similarity between two
senses (synsets), n-word overlapping approach is
used for counting the number of shared words
in two glosses. Besides, string kernel is used
to capture similarity between entities to avoid
variations between entities. Our approach is simple
and we will apply regression models to determine
sentence similarity on the basis of these features in
future work.
References
Daniel B., Chris Biemann, Iryna Gurevych and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012), in
conjunction with the First Joint Conference on Lexical
and Computational Semantics (*SEM 2012).
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot
on Semantic Textual Similarity. Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Frane Saric, Goran Glavas, Mladen Karan, Jan Snajder
and Bojana Dalbelo Basia. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. Proceedings
of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), in conjunction with the
First Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Gunes Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research, 22(2004):457?479.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, and Chris Watkins. 2002. Text
Classification using String Kernels. The Journal of
Machine Learning Research, 2(2002):419?444.
Jian Xu, Qin Lu and Zhengzhong Liu. 2012.
PolyUCOMP: Combining Semantic Vectors with
Skip-bigrams for Semantic Textual Similarity.
94
Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang 2011.
Transferring Topical Knowledge from Auxiliary Long
Text for Short Text Understanding. Proceedings of the
20th ACM Conference on Information and Knowledge
Management (ACM CIKM 2011).
Rada Mihalcea and Courtney Corley. 2006. Corpusbased
and Knowledge-based Measures of Text Semantic
Similarity. Proceeding of the Twenty-First National
Conference on Artificial Intelligence and the
Eighteenth Innovative Applications of Artificial
Intelligence Conference..
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
Gloss Overlaps as a Measure of Semantic Relatedness.
Proceedings of the 18th International Joint
Conference on Artificial Intelligence.
Vasileios Hatzivassiloglou, Judith L. Klavans , Eleazar
Eskin. 1999. Detecting Text Similarity over Short
Passages: Exploring Linguistic Feature Combinations
via Machine Learning. Proceeding of Empirical
Methods in natural language processing and Very
Large Corpora.
Yuhua Li, David Mclean, Zuhair B, James D. O?shea
and Keeley Crockett. 2006. Sentence Similarity
Based on Semantic Nets and Corpus Statistics. IEEE
Transactions on Knowledge and Data Engineering,
18(8):1138?1149.
95
