Word Sense Disambiguation
by Relative Selection
Hee-Cheol Seo1, Hae-Chang Rim2, and Myung-Gil Jang1
1 Knowledge Mining Research Team,
Electronics and Telecommunications Research Institute (ETRI),
Daejeon, Korea
{hcseo, mgjang}@etri.re.kr
2 Dept. of Computer Science and Engineering, Korea University,
1, 5-ka, Anam-dong, Seongbuk-Gu, Seoul, 136-701, Korea
rim@nlp.korea.ac.kr
Abstract. This paper describes a novel method for a word sense disam-
biguation that utilizes relatives (i.e. synonyms, hypernyms, meronyms,
etc in WordNet) of a target word and raw corpora. The method disam-
biguates senses of a target word by selecting a relative that most prob-
ably occurs in a new sentence including the target word. Only one co-
occurrence frequency matrix is utilized to efficiently disambiguate senses
of many target words. Experiments on several English datum present
that our proposed method achieves a good performance.
1 Introduction
With its importance, a word sense disambiguation (WSD) has been known as
a very important field of a natural language processing (NLP) and has been
studied steadily since the advent of NLP in the 1950s. In spite of the long study,
few WSD systems are used for practical NLP applications unlike part-of-speech
(POS) taggers and syntactic parsers. The reason is because most of WSD studies
have focused on only a small number of ambiguous words based on sense tagged
corpus. In other words, the previous WSD systems disambiguate senses of just
a few words, and hence are not helpful for other NLP applications because of its
low coverage.
Why have the studies about WSD stayed on the small number of ambiguous
words? The answer is on sense tagged corpus where a few words are assigned to
correct senses. Since the construction of the sense tagged corpus needs a great
amount of times and cost, most of current sense tagged corpora contain a small
number of words less than 100 and the corresponding senses to the words. The
corpora, which have sense information of all words, have been built recently,
but are not large enough to provide sufficient disambiguation information of
the all words. Therefore, the methods based on the sense tagged corpora have
difficulties in disambiguating senses of all words.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 920?932, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Word Sense Disambiguation by Relative Selection 921
In this paper, we proposed a novel WSD method that requires no sense tagged
corpus1 and that identifies senses of all words in sentences or documents, not
a small number of words. Our proposed method depends on raw corpus, which
is relatively very large, and on WordNet [1], which is a lexical database in a
hierarchical structure.
2 Related Works
There are several works for WSD that do not depend on a sense tagged corpus,
and they can be classified into three approaches according to main resources
used: raw corpus based approach [2], dictionary based approach [3,4] and hier-
archical lexical database approach. The hierarchical lexical database approach
can be reclassified into three groups according to usages of the database: gloss
based method [5], conceptual density based method [6,7] and relative based
method [8,9,10]. Since our method is a kind of the relative based method, this
section describes the related works of the relative based method.
[8] introduced the relative based method using International Roget?s The-
saurus as a hierarchical lexical database. His method is conducted as follows: 1)
Get relatives of each sense of a target word from the Roget?s Thesaurus. 2) Col-
lect example sentences of the relatives, which are representative of each sense. 3)
Identify salient words in the collective context and determine weights for each
word. 4) Use the resulting weights to predict the appropriate sense for the target
word occurring in a novel text. He evaluated the method on 12 English nouns,
and showed over than 90% precision. However, the evaluation was conducted on
just a small part of senses of the words, not on all senses of them.
He indicated that a drawback of his method is on the ambiguous relative: just
one sense of the ambiguous relative is usually related to a target word but the
other senses of the ambiguous relatives are not. Hence, a collection of example
sentences of the ambiguous relative includes the example sentences irrelevant
to the target word, which prevent WSD systems from collecting correct WSD
information. For example, an ambiguous word rail is a relative of a meaning bird
of a target word crane at WordNet, but the word rail means railway for the most
part, not the meaning related to bird. Therefore, most of the example sentences
of rail are not helpful for WSD of crane. His method has another problem in
disambiguating senses of a large number of target words because it requires a
great amount of time and storage space to collect example sentences of relatives
of the target words.
[9] followed the method of [8], but tried to resolve the ambiguous relative
problem by using just unambiguous relatives. That is, the ambiguous relative
rail is not utilized to build a training data of the word crane because the word
rail is ambiguous. Another difference from [8] is on a lexical database: they
utilized WordNet as a lexical database for acquiring relatives of target words
1 Strictly speaking, our method utilizes bias of word senses at WordNet, which is
acquired a sense tagged corpus. However, our method does not access a sense tagged
corpus directly. Hence, our method is a kind of a weakly supervised approach.
922 H.-C. Seo, H.-C. Rim, and M.-G. Jang
instead of International Roget?s Thesaurus. Since WordNet is freely available
for research, various kinds of WSD studies based on WordNet can be compared
with the method of [9]. They evaluated their method on 14 ambiguous nouns
and achieved a good performance comparable to the methods based on the sense
tagged corpus. However, the evaluation was conducted on a small part of senses
of the target words like [8].
However, many senses in WordNet do not have unambiguous relatives
through relationships such as synonyms, direct hypernyms, and direct hy-
ponyms.2 A possible alternative is to use the unambiguous relatives in the long
distance from a target word, but the way is still problematic because the longer
the distance of two senses is, the weaker the relationship between them is. In
other words, the unambiguous relatives in the long distance may provide ir-
relevant examples for WSD like ambiguous relatives. Hence, the method has
difficulties in disambiguating senses of words that do not have unambiguous rel-
atives near the target words in the WordNet. The problem becomes more serious
when verbs, which most of the relatives are ambiguous, are disambiguated. Like
[8], the method also has a difficulty in disambiguating senses of many words
because the method collects the example sentences of relatives of many words.
[10] reimplemented the method of [9] using a web, which may be a very
large corpus, in order to collect example sentences. They built training datum
of all noun words in WordNet whose size is larger than 7GB, but evaluated their
method on a small number of nouns of lexical sample task of SENSEVAL-2 as
[8] and [9].
3 Word Sense Disambiguation by Relative Selection
Our method disambiguates senses of a target word in a sentence by selecting
only a relative among the relatives of the target word that most probably occurs
in the sentence. A flowchart of our method is presented in Figure 1 with an
example3: 1) Given a new sentence including a target word, a set of relatives of
the target word is created by looking up in WordNet. 2) Next, the relative that
most probably occurs in the sentence is chosen from the set. In this step, co-
occurrence frequencies between relatives and words in the sentence are used in
order to calculate the probabilities of relatives. Our method does not depend on
the training data, but on co-occurrence frequency matrix. Hence in our method,
it is not necessary to build the training data, which requires too much time and
space. 3) Finally, a sense of the target word is determined as the sense that is
related to the selected relative. In this example, the relative stork is selected with
the highest probability and the proper sense is determined as crane#1, which is
related to the selected relative stork.
2 In this paper, direct hypernyms and direct hyponyms mean parents and children at
a lexical database, respectively.
3 In WordNet 1.7.1, a word crane contains four senses, but in this paper only two
senses (i.e. bird and device) are described in the convenience of description.
Word Sense Disambiguation by Relative Selection 923
A mother crane soon laid an egg.
stork, ibis, flamingo, 
bird, beak, feather, ...
lifting device, elevator,
davit, derrick, ...
Pr(stork | Context ),  Pr(ibis | Context ) , 
...
Pr(davit | Context), Pr(derrick | Context ) 
stork
crane#1 crane#2
crane#1
Sentence
Collect 
Relatives
Calculate 
Probability
Select a 
Relative
Determine 
Sense
Fig. 1. Flowchart of our proposed method
Our method makes use of ambiguous relatives as well as unambiguous rela-
tives unlike [9] and hence overcomes the shortage problem of relatives and also
reduces the problem of ambiguous relatives in [8] by handling relatives separately
instead of putting example sentences of the relatives together into a pool.
3.1 Relative Selection
The selected relative of the i-th target word twi in a sentence C is defined to be
the relative of twi that has the largest co-occurrence probability with the words
in the sentence:
SR(twi, C)
def
= argmax
rij
P (rij |C)P (Srij )W (rij , twi) (1)
where SR is the selected relative, rij is the j-th relative of twi, Srij is a sense of
twi that is related to the relative rij , and W is a weight of rij . The right hand
side of Eq. 1 is logarithmically calculated by Bayesian rule:
argmax
rij
P (rij |C)P (Srij )W (rij , twi)
= argmax
rij
P (C|rij)P (rij)
P (C)
P (Srij )W (rij , twi)
= argmax
rij
P (C|rij)P (rij)P (Srij )W (rij , twi)
= argmax
rij
{logP (C|rij) + logP (rij)
+logP (Srij) + logW (rij , twi)} (2)
924 H.-C. Seo, H.-C. Rim, and M.-G. Jang
The first probability in Eq. 2 is computed under the assumption that words
in C occur independently as follows:
logP (C|rij) ?
n
?
k=1
logP (wk|rij) (3)
where wk is the k-th word in C and n is the number of words in C. The proba-
bility of wk given rij is calculated:
P (wk|rij) =
P (rij , wk)
P (rij)
(4)
where P (rij , wk) is a joint probability of rij and wk, and P (rij) is a probability
of rij .
Other probabilities in Eq. 2 and 4 are computed as follows:
P (rij , wk) =
freq(rij , wk)
CS
(5)
P (rij) =
freq(rij)
CS
(6)
Pr(Srij ) =
0.5 + WNf(Srij)
n ? 0.5 + WNf(twi)
(7)
where freq(rij , wk) is the frequency that rij and wk co-occur in a raw corpus,
freq(rij) is the frequency of rij in the corpus, and CS is a corpus size, which is
the sum of frequencies of all words in the raw corpus. WNf(Srij ) and WNf(twi)
is the frequency of a sense related to rij and twi in WordNet.4 In Eq. 7, 0.5 is
a smoothing factor and n is the number of senses of twi. Finally, in Eq. 2, the
weights of relatives, W (rij , twi), are described in following Section 3.1.
Relative Weight. WordNet provides relatives of words, but all of them are not
useful for WSD. That is to say, it is clear that most of ambiguous relatives may
bring about a problem by providing example sentences irrelevant to the target
word to WSD system as described in the previous section.
However, WordNet as a lexical database is classified as a fine-grained dictio-
nary, and consequently some words are classified into ambiguous words though
the words represent just one sense in the most occurrences. Such ambiguous rela-
tives may be useful for WSD of target words that are related to the most frequent
senses of the ambiguous relatives. For example, a relative bird of a word crane is
an ambiguous word, but it usually represents one meaning, ?warm-blooded egg-
laying vertebrates characterized by feathers and forelimbs modified as wings?,
4 WordNet provides the frequencies of words and senses in a sense tagged corpus (i.e.
SemCor), and WNf is calculated with the frequencies in WordNet. That represents
bias of word senses in WordNet.
Word Sense Disambiguation by Relative Selection 925
which is closely related to crane. Hence, the word bird can be a useful relative of
the word crane though the word bird is ambiguous. But the ambiguous relative
is not useful for other target words that are related to the least frequent senses
of the relatives: that is, a relative bird is never helpful to disambiguate the senses
of a word birdie, which is related to the least frequent sense of the relative bird.
We employ a weighting scheme for relatives in order to identify useful rel-
atives for WSD. In terms of weights of relatives, our intent is to provide the
useful relative with high weights, but the useless relatives with low weights. For
instance, a relative bird of a word crane has a high weight whereas a relative
bird of a word birdie get a low weight.
For the sake of the weights, we calculate similarities between a target word
and its relatives and determine the weight of each relative based on the degree of
the similarity. Among similarity measures between words, the total divergence
to the mean (TDM) is adopted, which is known as one of the best similarity
measures for word similarity [11].
Since TDM estimates a divergence between vectors, not between words,
words have to be represented by vectors in order to calculate the similarity
between the words based on the TDM. We define vector elements as words that
occur more than 10 in a raw corpus, and build vectors of words by counting
co-occurrence frequencies of the words and vector elements.
TDM does measure the divergence between words, and hence a reciprocal of
the TDM measure is utilized as the similarity measure:
Sim(
?
wi,
?
wj) =
1
TDM(
?
wi,
?
wj)
where Sim(
?
wi,
?
wj) represents a similarity between two word vectors,
?
wi and
?
wj .
A weight of a relative is determined by the similarity of a target word and
its relative as follows:
W (rij , twi) = Sim(
?
rij ,
?
twi)
3.2 Co-occurrence Frequency Matrix
In order to select a relative for a target word in a given sentence, we must
calculate probabilities of relatives given the sentence, as described in previous
section. These probabilities as Eq. 5 and 6 can be estimated based on frequencies
of relatives and co-occurrence frequencies between each relative and each word
in the sentence.
In order to acquire the frequency information for calculating the probabilities,
the previous relative based methods constructed a training data by collecting
example sentences of relatives. However, to construct the training data requires
a great amount of time and storage space. What is worse, it is an awful work
to construct training datum of all ambiguous words, whose number is over than
20,000 in WordNet.
Instead, we build a co-occurrence frequency matrix (CFM) from a raw corpus
that contains frequencies of words and word pairs. A value in the i-th row and
926 H.-C. Seo, H.-C. Rim, and M.-G. Jang
j-th column in the CFM represents the co-occurrence frequency of the i-th word
and j-th word in a vocabulary, and a value in the i-th row and the i-th column
represents the frequency of the i-th word.
The CFM is easily built by counting words and word pairs in a raw corpus.
Furthermore, it is not necessary to make a CFM per each ambiguous word since
a CFM contains frequencies of all words including relatives and word pairs.
Therefore, our proposed method disambiguates senses of all ambiguous words
efficiently by referring to only one CFM.
The frequencies in Eq. 5 and 6 can be obtained through a CFM as follows:
freq(wi) = cfm(i, i) (8)
freq(wi, wj) = cfm(i, j) (9)
where wi is a word, and cfm(i, j) represents the value in the i-th row and j-th
column of the CFM, in other word, the frequency that the i-th word and j-th
word co-occur in a raw corpus.
4 Experiments
4.1 Experimental Environment
Experiments were carried out on several English sense tagged corpora: SemCor
and corpora for both lexical sample task and all words task of both SENSEVAL-
2 & -3.5 SemCor [12]6 is a semantic concordance, where all content words (i.e.
noun, verb, adjective, and adverb) are assigned to WordNet senses. SemCor
consists of three parts: brown1, brown2 and brownv. We used all of the three
parts of the SemCor for evaluation.
In our method, raw corpora are utilized in order to build a CFM and to
calculate similarities between words for the sake of the weights of relatives. We
adopted Wall Street Journal corpus in Penn Treebank II [13] and LATIMES cor-
pus in TREC as raw corpora, which contain about 37 million word occurrences.
Our CFM contains frequencies of content words and content word pairs. In
order to identify the content words from the raw corpus, Tree-Tagger [14], which
is a kind of automatic POS taggers, is employed.
WordNet provides various kinds of relationships between words or synsets.
In our experiments, the relatives in Table 1 are utilized according to POSs of
target words. In the table, hyper3 means 1 to 3 hypernyms (i.e. parents, grand-
parents and great-grandparent) and hypo3 is 1 to 3 hyponyms (i.e. children,
grandchildren and great-grandchildren).
5 We did not evaluate on verbs of lexical sample task of SENSEVAL-3 because the
verbs are assigned to senses of WordSmyth, not WordNet.
6 In this paper, SemCor 1.7.1 is adopted.
Word Sense Disambiguation by Relative Selection 927
Table 1. Used Relative types
POS relatives
noun synonym, hyper3, hypo3, antonym, attribute, holonym, meronym, sibling
adjective synonym, antonym, similar to, alsosee, attribute, particle, pertain
verb synonym, hyper2, tropo2, alsosee, antonym, causal, entail, verbgroup
adverb synonyms, antonyms, derived
4.2 Experimental Results
ComparisonwithOtherRelative Based Methods. We tried to compare our
proposed method with the previous relative based methods. However, both of [8]
and [9] didnot evaluate theirmethods onapublicly available data.We implemented
their methods and compared our method with them on the same evaluation data.
When both of the methods are implemented, it is practically difficult to col-
lect example sentences of all target words in the evaluation data. Instead, we
implemented the previous methods to work with our CFM. WordNet was uti-
lized as a lexical database to acquire relatives of target words and the sense
disambiguation modules were implemented by using on Na??ve Bayesian classi-
fier, which [9] adopted though [8] utilized International Roget?s Thesaurus and
other classifier similar to decision lists. Also the bias of word senses, which is
presented at WordNet, is reflected on the implementation in order to be in a
same condition with our method. Hence, the reimplemented methods in this pa-
per are not exactly same with the previous methods, but the main ideas of the
methods are not corrupted. A correct sense of a target word twi in a sentence
C is determined as follows:
Sense(twi, C)
def
= arg max
sij
P (sij |C)Pwn(sij) (10)
where Sense(twi, C) is a sense of twi in C, sij is the j-th sense of twi. Pwn(sij)
is the WordNet probability of sij . The right hand side of Eq. 10 is calculated
logarithmically under the assumption that words in C occur independently:
arg max
sij
P (sij |C)Pwn(sij)
= argmax
sij
P (C|sij)P (sij)
P (C)
Pwn(sij)
= argmax
sij
P (C|sij)P (sij)Pwn(sij)
= argmax
sij
{logP (C|sij) + logP (sij))
+logPwn(sij)}
? argmax
sij
{
n
?
k=1
logP (wk|sij) + logP (sij))
+logPwn(sij)} (11)
928 H.-C. Seo, H.-C. Rim, and M.-G. Jang
where wk is the k-th word in C and n is the number of words in C. In Eq. 11,
we assume independence among the words in C.
Probabilities in Eq. 11 are calculated as follows:
P (wk|sij) =
P (sij , wk)
P (sij)
=
freq(sij , wk)
freq(sij)
(12)
P (sij) =
freq(sij)
CS
(13)
Pwn(sij) =
0.5 + WNf(sij)
n ? 0.5 + WNf(twi)
(14)
where freq(sij , wk) is the frequency that sij and wk co-occur in a corpus,
freq(sij) is the frequency of sij in a corpus, which is the sum of frequencies
of all relatives related to sij . CS means corpus size, which is the sum of frequen-
cies of all words in a corpus. WNf(sij) and WNf(twi) are the frequencies of a
sij and twi in WordNet, respectively, which represent bias of word senses. Eq.
14 is the same with Eq. 7 in Section 3.
Since the training data are built by collecting example sentences of relatives
in the previous works, the frequencies in Eq. 12 and 13 are calculated with our
matrix as follows:
freq(sij , wk) =
?
rl related to sij
freq(rl, wk)
freq(sij) =
?
rl related to sij
freq(rl)
where rl is a relative related to the sense sij . freq(rl, wk) and freq(rl) are the
co-occurrence frequency between rl and wk and the frequency of rl, respectively,
and both frequencies can be obtained by looking up the matrix since the matrix
contains the frequencies of words and word pairs.
The main difference between [8] and [9] is whether ambiguous relatives are
utilized or not. Considering the difference, we implemented the method of [8] to
include the ambiguous relatives into relatives, but the method of [9] to exclude
the ambiguous relatives.
Word Sense Disambiguation by Relative Selection 929
Table 2. Comparison results with previous relative-based methods
S2 LS S3 LS S2 ALL S3 ALL SemCor
All Relatives 38.86% 42.98% 45.57% 51.20% 53.68%
Unambiguous Relatives 27.40% 24.47% 30.73% 33.61% 30.63%
our method 40.94% 45.12% 45.90% 51.35% 55.58%
Table 3. Comparison results with top 3 systems at SENSEVAL
S2 LS S2 ALL S3 ALL
[15] 40.2% 56.9% .
[16] 29.3% 45.1% .
[5] 24.4% 32.8% .
[17] . . 58.3%
[18] . . 54.8%
[19] . . 48.1%
Our method 40.94% 45.12% 51.35%
Table 2 shows the comparison results.7 In the table, All Relatives and Unam-
biguous Relatives represent the results of the reimplemented methods of [8] and
[9], respectively. It is observed in the table that our proposed method achieves
better performance on all evaluation data than the previous methods though the
improvement is not large. Hence, we may have an idea that our method handles
relatives and in particular ambiguous relatives more effectively than [8] and [9].
Compared with [9], [8] obtains a better performance, and the difference be-
tween the performance of them are totally more than 15 % on all of the evaluation
data. From the comparison results, it is desirable to utilize ambiguous relatives
as well as unambiguous relatives.
[10] evaluated their method on nouns of lexical sample task of SENSEVAL-2.
Their method achieved 49.8% recall. When evaluated on the same nouns of the
lexical sample task, our proposed method achieved 47.26%, and the method of
[8] 45.61%, and the method of [9] 38.03%. Compared with our implementations,
[10] utilized a web as a raw corpus that is much larger than our raw corpus, and
employed various kinds of features such as bigram, trigram, part-of-speeches,
etc.8 Therefore, it can be conjectured that a size of a raw corpus and features
play an important role in the performance. We can observe that in our imple-
mentation of the method of [9], the data sparseness problem is very serious since
unambiguous relatives are usually not frequent in the raw corpus. In the web,
the problem seems to be alleviated. Further studies are required for the effects
of various features.
7 Evaluation measure is a recall, which is utilized for evaluating systems at SENSE-
VAL. In the table, S2 means SENSEVAL-2, LS means lexical sample task, and ALL
represents all words task.
8 [10] also utilized the bias information of word senses at WordNet.
930 H.-C. Seo, H.-C. Rim, and M.-G. Jang
Comparison with Systems Participated in SENSEVAL. We also com-
pared our method with the top systems at SENSEVAL that did not use sense
tagged corpora.9 Table 3 shows the official results of the top 3 participating
systems at SENSEVAL-2 & 3 and experimental performance of our method. In
the table, it is observed that our method is ranked in top 3 systems.
5 Conclusions
We have proposed a simple and novel method that determines senses of all
contents words in sentences by selecting a relative of the target words in Word-
Net. The relative is selected by using a co-occurrence frequency between the
relative and the words surrounding the target word in a given sentence. The co-
occurrence frequencies are obtained from a raw corpus, not from a sense tagged
corpus that is often required by other approaches.
We tested the proposed method on SemCor data and SENSEVAL data, which
are publicly available. The experimental results show that the proposed method
effectively disambiguates many ambiguous words in SemCor and in test data
for SENSEVAL all words task, as well as a small number of ambiguous words
in test data for SENSEVAL lexical sample task. Also our method more cor-
rectly disambiguates senses than [8] and [9]. Furthermore, the proposed method
achieved comparable performance with the top 3 ranked systems at SENSEVAL-
2 & 3.
In consequence, our method has two advantages over the previous methods
([8] and [9]): our method 1) handles the ambiguous relatives and unambiguous
relatives more effectively, and 2) utilizes only one co-occurrence matrix for dis-
ambiguating all contents words instead of collecting training data of the content
words.
However, our method did not achieve good performances. One reason of
the low performance is on the relatives irrelevant to the target words. That is,
investigation of several instances which assign to incorrect senses shows that
relatives irrelevant to the target words are often selected as the most probable
relatives. Hence, we will try to devise a filtering method that filters out the useless
relatives before the relative selection phase. Also we will plan to investigate a
large number of tagged instances in order to find out why our method did not
achieve much better performance than the previous works and to detect how
our method selects the correct relatives more precisely. Finally, we will conduct
experiments with various features such as bigrams, trigrams, POSs, etc, which
[10] considered and examine a relationship of a size of a raw corpus and a system
performance.
9 At SENSEVAL, unsupervised systems include the weakly supervised systems though
there are some debates. In this paper, our methods are compared with the systems
that are classified into the unsupervised approach at SENSEVAL.
Word Sense Disambiguation by Relative Selection 931
References
1. Fellbaum, C.: An WordNet Electronic Lexical Database. The MIT Press (1998)
2. Schu?tze, H.: Automatic word sense discrimination. Computational Linguistics 24
(1998) 97?123
3. Lesk, M.: Automatic sense disambiguation using machine readable dictionaries:
How to tell a pine cone from an ice cream cone. In: Proceedings of the 5th annual
international conference on Systems documentation, Toronto, Ontario, Canada
(1986) 24?26
4. Karov, Y., Edelman, S.: Similarity-based word sense disambiguation. Computa-
tional Linguistics 24 (1998) 41?59
5. Haynes, S.: Semantic tagging using WordNet examples. In: Proceedings of
SENSEVAL-2 Second International Workshop on Evaluating Word Sense Disam-
biguation Systems, Toulouse, France (2001) 79?82
6. Agirre, E., Rigau, G.: Word sense disambiguation using conceptual density. In:
Proceedings of COLING?96, Copenhagen Denmark (1996) 16?22
7. Fernandez-Amoros, D., Gonzalo, J., Verdejo, F.: The role of conceptual relations in
word sense disambiguation. In: Proceedings of the 6th International Workshop on
Applications of Natural Language for Information Systems, Madrid, Spain (2001)
87?98
8. Yarowsky, D.: Word-sense disambiguation using statistical models of Roget?s cat-
egories trained on large corpora. In: Proceedings of COLING-92, Nantes, France
(1992) 454?460
9. Leacock, C., Chodorow, M., Miller, G.A.: Using corpus statistics and WordNet
relations for sense identification. Computational Linguistics 24 (1998) 147?165
10. Agirre, E., Martinez, D.: Unsupervised wsd based on automatically retrieved
examples: The importance of bias. In: Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), Barcelona, Spain
(2004)
11. Lee, L.: Similarity-Based Approaches to Natural Language Processing. PhD thesis,
Harvard University, Cambridge, MA (1997)
12. Miller, G.A., Leacock, C., Tengi, R., Bunker, R.: A semantic concordance. In:
Proceedings of the 3 DARPA Workshop on Human Language Technology. (1993)
303?308
13. Marcus, M.P., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated
corpus of english: The penn treebank. Computational Linguistics 19 (1994) 313?
330
14. Schmid, H.: Probabilistic part-of-speech tagging using decision trees. In: Proceed-
ings of the Conference on New Methods in Language Processing, Manchester, UK
(1994)
15. Fernandez-Amoros, D., Gonzalo, J., Verdejo, F.: The UNED systems at
SENSEVAL-2. In: Proceedings of SENSEVAL-2 Second International Work-
shop on Evaluating Word Sense Disambiguation Systems, Toulouse, France (2001)
75?78
16. Litkowski, K.: SENSEVAL-2:overview. In: Proceedings of SENSEVAL-2 Sec-
ond International Workshop on Evaluating Word Sense Disambiguation Systems,
Toulouse, France (2001) 107?110
932 H.-C. Seo, H.-C. Rim, and M.-G. Jang
17. Strapparava, C., Gliozzo, A., Giuliano, C.: Pattern abstraction and term similarity
for word sense disambiguation: Irst at senseval-3. In: Proceedings of SENSEVAL-
3: Third International Workshop on the Evaluation of Systems for the Semantic
Analysis of Text, Barcelona, Spain (2004) 229?234
18. Fernandez-Amoros, D.: Wsd based on mutual information and syntactic patterns.
In: Proceedings of SENSEVAL-3: Third International Workshop on the Evalu-
ation of Systems for the Semantic Analysis of Text, Barcelona, Spain (2004)
117?120
19. Buscaldi, D., Rosso, P., Masulli, F.: The upv-unige-ciaosenso wsd system.
In: Proceedings of SENSEVAL-3: Third International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text, Barcelona, Spain (2004)
77?82
Restoring an Elided Entry Word in a Sentence  
for Encyclopedia QA System  
Soojong Lim 
Speech/Language 
Information Research 
Department 
ETRI, Korea  
isj@etri.re.kr 
Changki Lee 
Speech/Language Informa-
tion Research Department 
ETRI, Korea  
leeck@etri.re.kr 
Myoung-Gil Jang 
Speech/Language Informa-
tion Research Department 
ETRI, Korea  
mgjang@etri.re.kr 
 
 
 
 
 
Abstract 
This paper presents a hybrid model for 
restoring an elided entry word for en-
cyclopedia QA system. In Korean en-
cyclopedia, an entry word is frequently 
omitted in a sentence. If the QA system 
uses a sentence without an entry word, 
it cannot provide a right answer. For 
resolving this problem, we combine a 
rule-based approach with Maximum 
Entropy model to use the merit of each 
approach. A rule-based approach uses 
caseframes and sense classes. The re-
sult shows that combined approach 
gives a 20% increase over our baseline.  
1 Introduction 
Ellipsis is a linguistic phenomenon that peo-
ple omit a word or phrase not to repeat a same 
word or phrase in a sentence or a document. 
Usually, ellipsis involves the use of clauses that 
are not syntactically complete sentences (Allen, 
1995) but the fact does not apply to all cases. An 
ellipsis occurring in encyclopedia documents in 
Korean is an example.  
 
 (Entry word: Kim Daejung) 
 
Korean: ??[gongro]?[ro] 2000?[nyeon]  
????? [nobel pyeonghwasang]?[eul] 
???[batatda].  
English: won the Nobel prize for peace in 
2000 by meritorious deed. 
 
In QA system(Kim et al 2004), it answers a 
question using the predicate-argument relation 
as in the following example.  
 
Korean: 2000?[nyeon]?[e] ????? [no-
belpyeonghwasang ] ?[eul]  ?? [bateun ] 
??[saram]?[eun]? 
English: Who?s the winner of the Nobel prize for 
peace on 2000? 
 
??(subj:??, obj:?????, adv:2000?) 
( batda(subj:saram, obj: nobelpyeonghwasang, 
adv:ichunnyeon) 
win(subj:who, obj:the Nobel prize for peace, 
adv:2000) 
 
Entry word: ??? 
(Entry word: Kim Daejun) 
 
??(subj:NULL(???), obj:?????, 
adv:2000?, ??) 
(batda(subj:NULL(kimdaejung), obj, nobelpyeongh-
wasang, adv: ichunnyeon, gongro) 
win(subj:NULL(Kim Daejung), obj:the Nobel prize 
for peace, adv:2000, deed) 
 
If an entry word of Korean encyclopedia per-
forms a function of a subject or an objects, it is 
frequently omitted in the sentences of the Ko-
rean encyclopedia. If the QA system uses the 
result in the above example, it cannot find who 
won the Nobel prize for peace in the year of 
2000.  We need to restore an entry word as a 
subject or an object to answer a right question.  
In this paper, to overcome this problem, we 
first try to classify entry words in encyclopedia 
into sense classes and determine which sense 
classes are restored to the subjects or the objects. 
Then we use caseframes for determining sense 
215
classes which are not restored using sense 
classes. If there is no caseframes, we use a sta-
tistical method, ME model, for determining 
whether the entry word is restored or not. Be-
cause each approach has both strength and 
weakness, we combine three approaches to 
achieve a better performance.  
2 Related Work 
Ellipsis is a pervasive phenomenon in natural 
languages. While previous work provides im-
portant insight into the abstract syntactic and 
semantic representations that underlie ellipsis 
phenomena, there has been little empirically 
oriented work on ellipsis.  
There are only two similar empirical experi-
ments done for this task. First is Hardt?s algo-
rithm(Hardt, 1997) for detecting VPE in the 
Penn Treebank. It achieves precision levels of 
44% and recall of 53%, giving an F-Measure of 
48% using a simple search technique, which 
relies on the annotation having identified empty 
expressions correctly. Second is Nielsen?s ma-
chine learning techniques(Nielsen, 2003). They 
only try to detect of elliptical verbs using four 
different machine learning techniques,  Trans-
formation-based learning, Maximum entropy 
modeling, Decision Tree Learning, Memory 
Based Learning. It achieves precision levels of 
85.14% and recall of 69.63%, giving an F-
Measure of 76.61%. There are 4 steps: detection, 
identification of antecedents, difficult antece-
dents, resolving antecedents. Because this study 
only concentrates on the detection, a comparison 
with our study is inadequate.  
We combine rule-based techniques with ma-
chine learning technique for using the merit of 
each technique.  
3 Restoring an Elided Entry Word 
We use three kinds of algorithms: A caseframe 
algorithm, an acceptable sense class algorithm, 
and Maximum Entropy (ME) algorithm. For 
knowing a strength and weakness points of each 
algorithm, we do experiments on each algorithm. 
Then we combine algorithms for higher per-
formance.  
Our system answers in three ways: restoring 
an  entry word as a subject, restoring an entry 
word as an object, and does not restore an entry 
word. We evaluate an algorithm in two ways. 
First, we evaluate all answers with precision. 
Second,  we  evaluate just two answers, restor-
ing an entry word as a subject and object, with 
F-measure.  
 
recallprecision
recallprecisionmeasureF
foundwordsentryelidedall
foundwordsentryelidedcorrectprecision
settestinwordsentryelidedall
foundwordsentryelidedcorrectrecall
+
??=?
=
=
2
 
3.1 Using Caseframes 
We use modified caseframes constructed for 
Korean-Chinese machine translation. The format 
of Korean-Chinese machine translation case 
frame is as the following: 
 
A=Sense_code!case_particle verb > Chinese > 
Korean Sentence 
A=??(saram)!?(ga) B=??(jangso)!?(ro) 
?(ga)!?(da) > A 0x53bb:v B [?(geu)[A]?(ga) 
??(bada)[B]?(ro) ??(gada)] 
A=Person!subj B=Location!adv go. 
 
In the caseframe, we only use Sense Class, 
case particle marker, and the verb. The case-
frame used in this research consists of 30,000 
verbs and 153,000 caseframes.  
The sense class used in this research is se-
lected from the nodes of the ETRI Lexical Con-
cept Network for Korean Nouns which consists 
of about 60,000 nodes. (If we include proper 
nouns, the total entry of ETRI Lexical Concept 
Network for Korean Nouns is about 300,000 
nodes).  
First, we analyze a sentence using depend-
ency parser (LIM, 2004), and then we convert a 
result of a parser into the caseframe format. We 
determine to restore an entry word if there is an 
exactly matched caseframe of a target except a 
sense class of an entry word.  
Table 1 shows an example.  
First, we analyze a sentence using depend-
ency parser (LIM, 2004), and then we convert a 
result of a parser into the caseframe format. We 
determine to restore an entry word if there is an 
exactly matched caseframe of a target except a 
sense class of an entry word.  
216
Table 1. An Example of Caserframe Algorithm 
Input Entry word: Along Bay  
Sense: Location 
Sentence: Located in East of Haiphong 
Parsing Locate(subj:NULL, obj:NULL, adv: east
of Haiphong) 
Caseframe of sentence  
direction!e locate 
Matching 24265-2 A=Location!ga B=Location!eseo
C=direction!e 
24265-4 A=Location!ga B=direction!e 
24265-8 A=weather!ga B=direction!e 
24265-12 A=direction!e 
24265-17 A=body!ga B=direction!e 
decision Restoring an entry word as a subject 
 
The result of caseframe algorithm is in table 
2. The result of caseframe algorithm shows that 
it has a high precision but a relatively low recall 
because it is impossible to construct caseframes 
for all sentences.  
 
Table 2. Result of Caseframe Algorithm 
 Subject Object Sum 
Precision 88.16 6.38 56.91 
Recall 59.29 27.28 56.45 
F-measure 70.90 10.34 56.68 
 
3.2 Acceptable Sense Class 
All entry words in the encyclopedia belong to at 
least one sense class. We verify all 444 sense 
classes to see whether they could be restored in 
a sentence.  We set a precision threshold 50% 
and we fix 36 sense classes to ?acceptable sense 
class?. An acceptable sense class is a sense class 
that if an entry word is included in an acceptable 
sense class, we unconditionally restore an entry 
word in a sentence. Our verification tells that 
there is only acceptable sense classes for sub-
jects. Table 3 shows acceptable sense classes. 
 
Table 3. Acceptable Sense Classes 
PERSON, ORGANIZATION, STUDY, WORK, 
LOCATION, ANIMAL, PLANT, ART,  
BUILDING, BUSINESS MATTERS, POSITION,  
SPORTS, CLOTHES, ESTABLISHMENT, 
 PUBLICATION, MEANS of TRANSPORTATION, 
EQUIPMENT, SITUATION, HARDWARE,  
BROADCASTING, HUMAN RACE, EXISTENCE, 
BRANCH, MATERIAL OBJECT, WEAPON,  
EXPLOSIVE, LANGUAGE, FACILITIES,  
ACTION, SYMBOL, TOPOGRAPHY, ROAD,  
ECONOMY, ADVERTISEMENT, EVENT, TOMB
The result of acceptable sense class algo-
rithm is presented in table 4. Because we cannot 
get acceptable sense classes for objects, F-
measure of object is 0.  
 
Table 4.  Result of ASC Algorithm 
 Subject Object Sum 
Precision 58.14 0.0 58.14 
Recall 66.37 0.0 60.48 
F-measure 61.98 0.0 59.29 
 
3.3 Maximum Entropy Modeling 
Maximum entropy modeling uses features, 
which can be complex, to provide a statistical 
model of the observed data which has the high-
est possible entropy, such that no assumptions 
about the data are made. 
 
)(maxarg* pHp =
*p
( pH
Cp?
 
where is the most uniform distribution, C is a set 
of probability distributions under the constraints and 
 is entropy of ) p .  
 
 Ratnaparkhi(Ratnaparkhi 98) makes a strong 
argument for the use of maximum entropy 
modes, and demonstrates their use in a variety of 
NLP tasks.  
The Maximum Entropy Toolkit was used for 
the experiments.1  
Because maximum entropy allows for a wide 
range of features, we can use various features, 
such as lexical feature, POS feature, sense fea-
ture, and syntactic feature. Each feature consists 
of subfeatures: 
 
Lexical feature; 
Verb_lex : lexeme of a target verb  
Verb_e_lex : lexeme of a suffix attatched 
to a target verb 
 
POS feature;  
Verb_pos : pos of a target verb 
Verb_e_pos : pos of a suffix attatch to a 
target verb 
 
Sense feature; 
                                                          
1 Downloadable from  
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.ht
ml 
217
Ti_res_code: where sense of an entry 
word is included in acceptable sense class 
Verb_cf_subj, obj: whether a sense of en-
try word is included in caseframe of a 
targe verb 
Ti_sense : sense class of entry word 
 
Syntactic feature; 
Tree_posi: position of parse tree 
Rel_type: relation type between verbs in 
a sentence 
Sen_subj, sen_obj : existence of subject 
or object 
 
Hybrid feature; 
 Pair =(sense class of entry word, verb) 
 
 Table 5 shows an example of features that 
we use for finding an elided entry word.  
Previous work using ME model adopted dis-
tance-based context for training. Because we use 
syntactic features, we can use not only distance-
based context but also predicate-argument based 
context. The training data for ME algorithm 
consist of verbs in the encyclopedia document 
and their syntactic arguments. Each verb-
arguments set is augmented with the information 
that signifies whether a subject, an object or nei-
ther of them should be restored. For training, we 
use a dependency parser[Lim, 2004]. A preci-
sion of this parser is about 75%. The results of 
ME model algorithm is shown in table 6. The 
results of ME model shows that its score is the 
lowest of all. We guess the reason is that there is 
not enough training data for covering all sense 
classes.  
 
 
Table 5. An Example of Features 
Entry word, 
Sentence 
!TI Cirsotrema perplexam 
!SENSE Animal 
!VERB live 
!SENT lives in a tidal zone 
Lexical 
feature 
verb_lex=??(salda) verb_e_lex=?
(myeo) 
POS feature verb_pos=4 verb_e_pos=24 
Sense fea-
ture 
ti_res_code=1 verb_cf_subj=1  
verb_cf_obj=0 ti_sense=Animal 
Syntactic 
feature 
tree_posi=high rel_type=-1 sen_subj=
0 sen_obj=0 
Hybrid fea-
ture 
pair=(Animal, live) 
 
Table 6. Result of ME Model 
 Subject Object Sum 
Precision 62.50 40.0 60.87 
Recall 35.40 18.18 33.87 
F-measure 45.20 25.00 43.52 
 
3.4 Combining Algorithms 
Different algorithms have different characteris-
tics. For example, the acceptable sense class 
algorithm has relatively high recall but low pre-
cision, while the opposite holds true for the 
caseframe algorithm,  we need to combine algo-
rithms for maximizing advantages of each algo-
rithm. 
First, we combine the acceptable sense class 
algorithm with the ME model. We process the 
problem using the sense class algorithm. Instead 
of applying the algorithm exactly, we use the 
ME model for helping the acceptable sense class 
algorithm. If the acceptable sense class algo-
rithm determines a restoration, we adopt the 
case to ME model. Then if the score of ME 
model is over the negative threshold, we deter-
mine not to restore an entry word.  
Second, we combine the caseframe algorithm 
with the ME model. We process the cases not 
resolved in the first processing time using the 
caseframe algorithm. We try to match case-
frames exactly to sentence with an entry word 
sense code. If we cannot find the exactly match-
ing caseframe, we try matching caseframes par-
tially. In this case, a precision is maybe lower 
than an exact match, we also use the ME model 
for reliability. If the score of ME model is over 
the positive threshold, we determine to restore 
an entry word.  
4 Result and Conclusion 
For ME model, we made a training set 
manually. The training set consists of 2895 sen-
tences: 916 sentences for restoring an entry 
word as a subject, 232 sentences for restoring an 
entry word as an object, 1756 sentences for not 
restoring any. For a test, we randomly selected 
277 sentences.  
We did 6 kinds of experiments. Using Case-
frame algorithm(CF), Acceptable sense class 
algorithm(ASC), ME model(ME) and combine 
ASC with CF(ASC_CF), ASC with ME 
218
(ASC_ME), and ASC with CF and 
ME(ASC_CF_ME). 
 
Table 7. Result of Combined Algorithm 
 Recall Precision F-measure
baseline 100.00 31.64 48.07 
ASC_CF_ME 78.23 60.25 68.07 
ASC_CF 68.55 50.00 57.82 
ASC_ ME 79.03 59.39 67.82 
 
The performance of the methods is calculated 
using recall, precision and F-measure.  
Table 7 and Figure 1 show the performance 
of each experiment.  
Our proposed approach (ASC_CF_ME) 
gives the best results among all experiments, 
with an F-measure of 68.1%, followed closely 
by ASC_ME. This gives a 20% increase over 
our baseline. For testing a portability of our ap-
proach, we experiment the noun phrase ellipsis 
(NPE) detection. The performance of NPE is 
alike an elided entry word. Recall is 69.31, Pre-
cision is 65.05, and F-measure is 67.12. So we 
expect the performance of our approach not to 
drop when applied to NPE or other ellipsis prob-
lem. The results so far are encouraging, and 
show that the approach taken is capable of pro-
ducing a robust and accurate system.  
In this paper, we suggested the approach that 
restores an elided entry word for Encyclopedia 
QA systems combining an acceptable sense 
class algorithm, a caseframe algorithm, and ME 
model.  
For future work, we plan to pursue the fol-
lowing research. First, we will use various ma-
chine learning methods and compare them with 
the ME model. Second, because we plan to ap-
ply this approach in the encyclopedia document, 
we need to design the more general approach to 
use other ellipsis phenomenon. Third, we try to 
find a method for enhancing performance of 
restoring elided entry words as the object.  
References 
James Allen. 1995. Natural Language Under-
standing, Benjamin/Cummings Publishing 
Company, 449~455 
Leif Arda Nielsen. 2003. Using Machine Learn-
ing Techniques for VPE detection, RANLP 03, 
Bulgaria. 
Daniel Hardt. 1997. An empirical approach to 
vp ellipsis, Computational Linguistics, 23(4).  
0
10
20
30
40
50
60
CF AS
C ME
AS
C_C
F
AS
C_M
E
AS
C_C
F_M
E
70
80
90
Precision
Recall
F-Measure
 
Figure 1. Comparison of All Results 
 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural LANGUAGE Ambiguity 
Resolution, Unpublished PhDthesis, University 
of Pennsylvania.  
Lim soojong. 2004. Dependency Relation 
Analysis Using Caseframe for Encyclopedia 
Question-Answering Systems, IECON, Korea. 
H. J. Kim, H. J. Oh, C. H. Lee., et al 2004.  The 
3-step Answer Processing Method for Encyclo-
pedia Question-Answering System: AnyQues-
tion 1.0. The Proceedings of Asia Information 
Retrieval Symposium (AIRS) 309-312 
 
219
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 21?24, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Descriptive Question Answering in Encyclopedia 
 
 
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, Myung-Gil Jang 
Knowledge Mining Research Team 
Electronics and Telecommunications Research Institute (ETRI) 
Daejeon, Korea 
{ohj, forever, jini, mgjang} @ etri.re.kr 
 
 
 
 
Abstract 
Recently there is a need for a QA system to 
answer not only factoid questions but also 
descriptive questions. Descriptive questions 
are questions which need answers that 
contain definitional information about the 
search term or describe some special events. 
We have proposed a new descriptive QA 
model and presented the result of a system 
which we have built to answer descriptive 
questions. We defined 10 Descriptive 
Answer Type(DAT)s as answer types for 
descriptive questions. We discussed how 
our proposed model was applied to the 
descriptive question with some experiments.  
1 Introduction 
Much of effort in Question Answering has focused 
on the ?short answers? or factoid questions, which 
answer questions for which the correct response is 
a single word or short phrase from the answer 
sentence. However, there are many questions 
which are better answer with a longer description 
or explanation in logs of web search 
engines(Voorhees, 2003). In this paper, we 
introduce a new descriptive QA model and present 
the result of a system which we have built to 
answer such questions.  
Descriptive question are questions such as ?Who 
is Columbus??, ?What is tsunami??, or ?Why is 
blood red??, which need answer that contain the 
definitional information about the search term, 
explain some special phenomenon.(i.e. chemical 
reaction) or describe some particular events.  
At the recent works, definitional QA, namely 
questions of the form ?What is X??, is a 
developing research area related with a subclass of 
descriptive questions. Especially in TREC-12 
conference(Voorhees, 2003), they had produced 50 
definitional questions in QA track for the 
competition. The systems in TREC-12(Blair et al 
2003; Katz et al 2004) applied complicated 
technique which was integrated manually 
constructed definition patterns with statistical 
ranking component.  
Some experiments(Cui et al 2004) tried to use 
external resources such as WordNet and Web 
Dictionary associated with a syntactic pattern. 
Further recent work tried to use online knowledge 
bases on web. Domain-specific definitional QA 
systems in the same context of our works have 
been developed. Shiffman et al2001) applied on 
biographical summaries for people with data-
driven method. 
In contrast to former research, we focus on the 
other descriptive question, such as ?why,? ?how,? 
and ?what kind of?. We also present our 
descriptive QA model and its experimental results. 
2 Descriptive QA 
2.1 Descriptive Answer Type 
Our QA system is a domain specific system for 
encyclopedia 1 . One of the characteristics of 
encyclopedia is that it has many descriptive 
sentences. Because encyclopedia contains facts 
about many different subjects or about one 
particular subject explained for reference, there are 
                                                          
1 Our QA system can answer both factoid questions and descriptive questions. In 
this paper, we present only sub system for descriptive QA 
21
many sentences which present definition such as 
?X is Y.? On the other hand, some sentences 
describe process of some special event(i.e. the 1st 
World War) so that it forms particular sentence 
structures like news article which reveal reasons or 
motives of the event. 
We defined Descriptive Answer Type (DAT) as 
answer types for descriptive questions with two 
points of view: what kind of descriptive questions 
are in the use?s frequently asked questions? and 
what kind of descriptive answers can be  
patternized in the our corpus? On the view of 
question, most of user?s frequently asked questions 
are not only factoid questions but also definitional 
questions. Furthermore, the result of analyzing the 
logs of our web site shows that there are many 
questions about ?why?, ?how?, and so on. On the 
other side, descriptive answer sentences in corpus 
show particular syntactic patterns such as 
appositive clauses, parallel clauses, and adverb 
clauses of cause and effect. In this paper, we 
defined 10 types of DAT to reflect these features of 
sentences in encyclopedia.  
Table 1 shows example sentences with pattern 
for each DAT. For instance, ?A tsunami is a large 
wave, often caused by an earthquake.? is an 
example for ?Definition? DAT with pattern of [X is 
Y]. It also can be an example for ?Reason? DAT 
because of matching pattern of [X is caused by Y]. 
 
Table 1: Descriptive Answer Type 
DAT Example/Pattern 
DEFINITION A tsunami is a large wave, often caused by an earthquake. [X is Y] 
FUCTION 
Air bladder is an air-filled structure in many 
fishes that functions to maintain buoyancy or to 
aid in respiration. [ X that function to Y] 
KIND The coins in States are 1 cent, 5 cents, 25 cents, and 100cents. [X are Y1, Y2,.. and Yn] 
METHOD The method that prevents a cold is washing often your hand.[The method that/of X is Y] 
CHARCTER 
Sea horse, characteristically swimming in an 
upright position and having a prehensile tail. [ X 
is characteristically Y] 
OBJECTIVE An automobile used for land transports. [ X used for Y] 
REASON A tsunami is a large wave, often caused by an earthquake. [X is caused by Y] 
COMPONENT 
An automobile usually is composed of 4 wheels, 
an engine, and a steering wheel. [X is composed 
of Y1, Y2,.. and Yn] 
PRINCIPLE 
Osmosis is the principle, transfer of a liquid 
solvent through a semipermeable membrane that 
does not allow dissolved solids to pass. [X is the 
principle, Y] 
ORIGIN 
The Achilles tendon is the name from the 
mythical Greek hero Achilles. [X is the name 
from Y] 
2.2 Descriptive Answer Indexing 
Descriptive Answer indexing process consists of 
two parts: pattern extraction from pre-tagged 
corpus and extraction of DIU(Descriptive Indexing 
Unix) using a pattern matching technique. 
Descriptive answer sentences generally have a 
particular syntactic structure. For instance, 
definitional sentences has patterns such as ?X is 
Y,? ?X is called Y,? and ?X means Y.? In case of 
sentence which classifies something into sub-kinds, 
i.e. ?Our coin are 50 won, 100 won and 500 won.? 
it forms parallel structure like ?X are Y1, Y2,.. and 
Yn?. 
To extract these descriptive patterns, we first 
build initial patterns. We constructed pre-tagged 
corpus with 10 DAT tags, then performed sentence 
alignment by the surface tag boundary. The tagged 
sentences are then processed through part-of-
speech(POS) tagging in the first step. In this stage, 
we can get descriptive clue terms and structures, 
such as ?X is caused by Y? for ?Reason?, ?X was 
made for Y? for ?Function?, and so on.  
In the second step, we used linguistic analysis 
including chunking and parsing to extend initial 
patterns automatically. Initial patterns are too rigid 
because we look up only surface of sentences in the 
first step. If some clue terms appear with long 
distance in a sentence, it can fail to be recognized 
as a pattern. To solve this problem, we added 
sentence structure patterns on each DAT patterns, 
such as appositive clause patterns for ?Definition?, 
parallel clause patterns for ?Kind?, and so on.  
Finally, we generalized patterns to conduct 
flexible pattern matching. We need to group 
patterns to adapt to various variations of terms 
which appear in un-training sentences. Several 
similar patterns under the same DAT tag were 
integrated into regular-expression union which is to 
be formulated automata. For example, ?Definition? 
patterns are represented by [X<NP> be 
called/named/known as Y<NP>]. 
We defined DIU as indexing unit for descriptive 
answer candidate. In DIU indexing stage 
performed pattern matching, extracting DIU, and 
storing our storage. We built a pattern matching 
system based on Finite State Automata(FSA). After 
pattern matching, we need to filtering over-
generated candidates because descriptive patterns 
are naive in a sense. In case of ?Definition?, ?X is 
Y? is matched so many times, that we restrict the 
22
pattern when ?X? and ?Y? under the same meaning 
on our ETRI-LCN for Noun ontology 2 . For 
example, ?Customs duties are taxes that people pay 
for importing and exporting goods[X is Y]? are 
accepted because ?custom duty? is under the ?tax? 
node so they have same meaning. 
DIU consists of Title, DAT tag, Value, V_title, 
Pattern_ID, Determin_word, and Clue_word. Title 
and Value means X and Y in result of pattern 
matching, respectively. Determin_word and 
Clue_word are used to restrict X and Y in the 
retrieval stage, respectively. V_title is 
distinguished from Title by whether X is an entry 
in the encyclopedia or not. Figure 1 illustrated 
result of extracting DIU. 
 
Title: Cold 
?The method that prevents a cold is washing often your hand.?
  
1623: METHOD:[The method that/of X is Y]
The method that [X:prevents a cold] is [Y:washing often your hand] 
 
z Title: Cold 
z DAT tag: METHOD 
z Value: washing often your hand 
z V_title: NONE 
z Pattern_ID: 1623 
z Determin_Word: prevent 
z Clue_Word: wash hand 
Figure 1: Result of DIU extracting 
2.3 Descriptive Answer Retrieval 
Descriptive answer retrieval performs finding DIU 
candidates which are appropriate to user questions 
through query processing. The important role of 
query processing is to catch out <QTitle, DAT> 
pair in the user question. QTitle means the key 
search word in a question. We used LSP pattern3 
for question analysis. Another function of query 
processing is to extract Determin_word or 
Clue_Terms in question in terms of determining 
what user questioned. Figure 2 illustrates the result 
of QDIU(Question DIU). 
 
?How can we prevent a cold? 
    
z QTitle: Cold 
z DAT tag: METHOD 
z Determin_Word: prevent 
Figure 2: Result of Question Analysis 
                                                          
2 LCN: Lexical Concept Network. ETRI-LCN for Noun consists of 120,000 
nouns and 224,000 named entities. 
3 LSP pattern: Lexico-Syntactic Pattern. We built 774 LSP patterns. 
3 Experiments 
3.1 Evaluation of DIU Indexing 
To extract descriptive patterns, we built 1,853 pre-
tagged sentences within 2,000 entries. About 
40%(760 sentences) of all are tagged with 
?Definition, while only 9 sentences were assigned 
to ?Principle?. Table 2 shows the result of extracted 
descriptive patterns using tagged corpus. 408 
patterns are generated for ?Definition? from 760 
tagged sentences, while 938 patterns for ?Function? 
from 352 examples. That means the sentences of 
describing something?s function formed very 
diverse expressions.  
 
Table 2: Result of Descriptive Pattern Extraction 
DAT # of Patterns DAT # of Patterns
DEFINITION 408(22) OBJECTIVE 166(22)
FUCTION 938(26) REASON 38(15)
KIND 617(71) COMPONENT 122(19)
METHOD 104(29) PRINCIPLE 3(3)
CHARCTER 367(20) ORIGIN 491(52)
  Total 3,254(279)
* The figure in ( ) means # of groups of patterns 
Table 3: Result of DIU Indexing 
DAT # of DIUs DAT # of DIUs 
DEFINITION 164,327(55%) OBJECTIVE 9,381(3%)
FUCTION 25,105(8%) REASON 17,647(6%)
KIND 45,801(15%) COMPONENT 12,123(4%)
METHOD 4,903(2%) PRINCIPLE 64(0%)
CHARCTER 10,397(3%) ORIGIN 10,504(3%)
  Total 300,252 
 
Table 3 shows the result of DIU indexing. We 
extracted 300,252 DIUs from the whole 
encyclopedia 4  using our Descriptive Answer 
Indexing process. As expected, most DIUs(about 
55%, 164,327 DIUs) are ?Definition?. We assumed 
that the entries belonging to the ?History? category 
have many sentences about ?Reason? because 
history usually describes some events. However, 
we obtained only 25,110 DIUs(8%) of ?Reason? 
because patterns of ?Reason? have lack of 
expressing syntactic structure of adverb clauses of 
cause and effect. ?Principle? also has same problem 
of lack of patterns so we only 64 DIUs. 
3.2 Evaluation of DIU Retrieval 
To evaluate our descriptive question answering 
method, we used 152 descriptive questions from 
our ETRI QA Test Set 2.05, judged by 4 assessors. 
                                                          
4 Our encyclopedia consists of 163,535 entries and 13 main categories in Korean. 
5 ETRI QA Test Set 2.0 consists of 1,047 <question, answer> pairs including 
both factoid and descriptive questions for all categories in encyclopedia 
23
For performance comparisons, we used Top 1 and 
Top 5 precision, recall and F-score. Top 5 precision 
is a measure to consider whether there is a correct 
answer in top 5 ranking or not. Top 1 measured 
only one best ranked answer. 
For our experimental evaluations we constructed 
an operational system in the Web, named 
?AnyQuestion 2.0.? To demonstrate how 
effectively our model works, we compared to a 
sentence retrieval system. Our sentence retrieval 
system used vector space model for query retrieval 
and 2-poisson model for keyword weighting.  
Table 4 shows that the scores using our proposed 
method are higher than that of traditional sentence 
retrieval system. As expected, we obtained better 
result(0.608) than sentence retrieval system(0.508). 
We gain 79.3% (0.290 to 0.520) increase on Top1 
than sentence retrieval and 19.6%(0.508 to 0.608) 
on Top5. The fact that the accuracy on Top1 has 
dramatically increased is remarkable, in that 
question answering wants exactly only one relevant 
answer.  
Whereas even the recall of sentence retrieval 
system(0.507) is higher than descriptive QA 
result(0.500) on Top5, the F-score(0.508) is lower 
than that(0.608). It comes from the fact that 
sentence retrieval system tends to produce more 
number of candidates retrieved. While sentence 
retrieval system retrieved 151 candidates, our 
descriptive QA method retrieved 98 DIUs under 
the same condition that the number of corrected 
answers of sentence retrieval is 77 and ours is 76. 
 
Table 4: Result of Descriptive QA 
Sentence Retrieval Descriptive QA  
Top l Top 5 Top 1 Top 5 
Retrieved 151 151 98 98
Corrected 44 77 65 76
Precision 0.291 0.510 0.663 0.776
Recall 0.289 0.507 0.428 0.500
F-score 0.290 0.508 0.520
(+79.3%)
0.608
(+19.6%)
 
We further realized that our system has a few 
week points. Our system is poor for inverted 
retrieval which should answer to the quiz style 
questions, such as ?What is a large wave, often 
caused by an earthquake?? Moreover, our system 
depends on initial patterns. For the details, 
?Principle? has few initial patterns, so that it has 
few descriptive patterns. This problem has 
influence on retrieval results, too. 
4 Conclusion 
We have proposed a new descriptive QA model 
and presented the result of a system which we have 
built to answer descriptive questions. To reflect 
characteristics of descriptive sentences in 
encyclopedia, we defined 10 types of DAT as 
answer types for descriptive questions. We 
explained how our system constructed descriptive 
patterns and how these patterns are worked on our 
indexing process. Finally we presented how 
descriptive answer retrieval performed and 
retrieved DIU candidates. We have shown that our 
proposed model outperformed the traditional 
sentence retrieval system with some experiments. 
We obtained F-score of 0.520 on Top1 and 0.680 
on Top5. It showed better results when compared 
with sentence retrieval system on both Top1 and 
Top5. 
Our Further works will concentrate on reducing 
human efforts for building descriptive patterns. To 
achieve automatic pattern generation, we will try to 
apply machine learning technique like the boosting 
algorithm. More urgently, we have to build an 
inverted retrieval method. Finally, we will compare 
with other systems which participated in TREC by 
translating definitional questions of TREC in 
Korean. 
References  
S. Blair-Goldensohn, K. R. McKeown, and A, H, 
Schlaikjer. 2003. A Hybrid Approach for QA Track 
Definitional Questions, Proceedings of the twelve 
Text REtreival Conference(TREC-12), pp. 336-342. 
H. Cui, M-Y. Kan, T-S. Chua, and J. Xian. 2004. A 
Comparative Study on Sentence Retrieval for 
Definitional Question Answering, Proceedings of 
SIGIR 2004 workshop on Information Retrieval 4 
Question Answering(IR4QA). 
B. Katz, M. Bilotti, S. Felshin, et. al. 2004. Answering 
Multiple Questions on a Topic from Heterogeneous 
Resources, Proceedings of the thirteenth Text 
REtreival Conference(TREC-13).  
B. Shiffman, I. Mani, and K.Concepcion. 2001. 
Producing Biographical Summaries: Combining 
Linguistic Resources and Corpus Statistics,  
Proceedings of the European Association for 
Computational Linguistics (ACL-EACL 01).  
Ellen M. Voorhees. 2003. Overview of TREC 2003 
Question Answering Track, Proceedings of the 
twelfth Text REtreival Conference(TREC-12). 
24
AnyQ: Answer Set based Information Retrieval System
Hyo-Jung Oh, Myung-Gil Jang Moon-Soo Chang
Electronics and Telecommunications Department of Software
Research Institute (ETRI) Seokyeong University
Daejeon, Korea Seoul, Korea
{ohj, mgjang}@etri.re.kr cosmos@skuniv.ac.kr
1. Introduction
The goal of Information Retrieval (IR) is finding
answer suited to user question from massive
document collections with satisfied response time.
With the exponential growth of information on the
Web, user is expecting to find answer more fast with
less effort. Current IR systems especially focus on
improving precision the result rather than recall. A
notable trend in IR is to provide more accurate,
immediately usable information as in Question
Answering systems(Q/A) [1] or in some systems
using pre-constructed question/answer document
pairs [2, 3], known ?answer set driven? system.
While traditional search engine uses term indexing,
i.e. tf*idf, answer approaches use syntactic, semantic
and pragmatic knowledge provided expert, i.e.
WordNet[4]. Another difference comes from the fact
that answer approach returns ?answer set? distilled
information need of user as retrieval result, not just
document appeared query terms.
The TREC Q/A track [1, 5, 6] which has
motivated much of the recent work in the field
focuses on fact-based, short-answer question type, e.g.
?Who is Barbara Jordan?? or ?What is Mardi Gras?.
The Q/A runs find an actual answer in TREC
collection, rather than a ranked list of documents, in
response to a question. On the other hand, user
queries in answer set driven system, like
AskJeeves[2], are more implicit and conceptual.
These system was developed targeting the Web [7, 8],
is larger than the TREC Q/A document collection.
Whereas the user gives incomplete query to system,
they need not only answers but related information.
Sometimes the user even has uncertainty what
exactly they need. For example, the user query just
?Paris? is answered by gathering information
including Paris city guide, photographs of Paris, and
so on. To catch information need of user, these
system have pre-defined query pattern and prepared
correct answers belonging to each question. Since it
is still considered difficult, if not impossible, to
capture semantics and pragmatics of sentences in user
queries and documents, such systems require
knowledge bases built manually so that a certain level
of quality can be guaranteed. Needless to say, this
knowledge base construction process is labor-
intensive, typically requiring significant and
continuous human efforts [9].
This paper rests on the both directions: a new type
of IR and its operational experience. Our system,
named ?AnyQ?1, attempts to provide high quality
answer documents to user queries by maintaining a
knowledge base consisting of expected queries and
corresponding answer document. We defined the
semantic category of the answer as attributes and the
1 http://anyq.etri.re.kr in korean
Abstract
The accuracy of IR result continues to grow on
importance as exponential growth of WWW, and
it is therefore increasingly important that
appropriate retrieval technologies be developed
for the web. We explore a new type of IR,
?answer set based IR?, and its operational
experience. Our proposed approach attempts to
provide high quality answer documents to user by
maintaining a knowledge base with expected
queries and corresponding answer document. We
will elaborate on our architecture and the
experimental results.
Keywords: answer set driven IR, attribute-
based classification, automatic knowledge base
construction,.
Figure 1. System architecture of Answer Set based IR
documents associated with each attributes as answer
set. In order to reduce the cost of manually
constructing and maintaining answer sets, we have
devised a new method of automating the answer
document selection process by using the automatic
text categorization, reported ABC(Attribute-Based
Classification)[10].
The rest of the paper is organized as follows.
Section 2 presents overviews of our answer set driven
retrieval system and knowledge base. In Section 3
and 4 elaborates on answer set construction and its
retrieval process. Section 5 details experiment results
for our method. After discussing the limitations of
our approach in Section 6, we conclude by
summarizing our contributions and describing future
works.
2. Answer Set based IR System
2.1. System Overview
Several approaches to find answer using
informative knowledge from expert were reported [1,
2, 3]. Most recent research proposed a new method of
capturing the semantics of the question and then
presents the document as answer, named ?answer set
driven IR. The goal of these systems is to explore
how does map user question into answer document
that might be contain pertinent information. In these
systems, it is crucial to devise a method to construct a
high-quality knowledge base. In our system, we take
a hybrid approach of using a human-generated
concept hierarchy and automatic classification
techniques to make it more feasible to build an
operational system.
Our system analyzes a user query to extract
concept and attribute terms that can be matched
against the knowledge base where a set of answer
documents can be found. As such, it has three parts:
answer set construction, answer set search, answer
presentation, as illustrated in Figure 1. The answer
set construction part, which is seemed indexing part
in traditional IR system, employees both manual and
automatic methods to build the knowledge base. The
answer set search part processes a natural language
query, extracts concepts and attributes, and maps
them to the knowledge base so that the answer
documents associated with the <concept, attribute>
pairs can be retrieve. In the answer presentation part,
the search result is presented with highlighted
paragraphs considered to contain the answer to the
query.
docum ents
Answer Set Construction
M anual Collection
Auto Construction
W ith ABC*
Query Analys is
Query ? AS
M atching
Answer Set Searching
Docum ent Analys is
Answer Presentation
Answ er E xtraction
Answ er
Info rm ation
Natural Langu age
Query
Know ledge Base
Concept
Netw ork
Attribute
Answ er Set
*ABC: A ttribute-Based Classif ication
Figure 2. Concept, Attribute, and Documents
Group of concepts in concept network hierarchy and Distribution of attributes for concepts
Table 1. Distribution of attributes for concepts in an equivalence class(?-relation)
attributes
concepts
D
ef
in
iti
on
Po
lic
y
Pa
y
Co
n
su
lta
tio
n
ca
se
Pr
o
bl
em
s
K
in
ds
Pu
rp
os
e
Cu
rr
en
t
sit
ua
tio
n
R
eg
ul
at
io
ns
M
er
its
Ca
lc
u
la
tio
n
m
et
ho
ds
N
eg
ot
ia
tio
n
#
at
tr
ib
u
te
s
Incentives O O O O O O O O O O 10
Hourly wage O O O O O O 6
Basic salary O O O O 4
Service allowance O O O O O O O O 8
Ability allowance O O O O O O O 7
Bonus O O O O O O O O 8
2.2. Knowledge Base
Our knowledge base consists of three parts: a
concept network, attributes associated with each
concept, and answer set belonging to each <concept,
attribute> pair.
The concept network contains about 50,000
conceptual word2 as in WordNet [11] with 6 lexico-
semantic relations that are used to form a synset
hierarchy. By using the concept network, as already
well-know in the WordNet-related research [1, 4, 6],
a semantic processing of questions becomes possible.
The information mined from concept network guides
process of bridging inference between the query and
the expected answer. Finding a place, i.e. concept
node, in the network for a query can be construed as
understanding the meaning of the query. Attribute set
an intermediary as connecting concept network with
2 Include 14,700 conceptual word in economy domain
answer documents. The answer-set driven retrieval
system maps a user query into one or more concepts
and further down to one or more attributes where
associated documents can be picked up as the answer
set. Attributes play the role in subcategorizing the
documents belonging to the concept node. A set of
attributes chosen for a particular concept specifies
various aspects often mentioned in the documents
bearing the concept and serves as an intermediary
between a concept and high-precision answer
documents. It should be noted that attributes are not
inherently associated with a concept, but found in the
documents addressing the concept. For instance, as in
Figure 2, the concept node for ?angel investment?
would have attributes, ?definition?, ?strategy?,
?prospects? and ?merits?, that are aspects or
characteristics of ?angel investment? often mentioned
in relevant documents.
Figure 2 and table 1 represent groups of concepts
from different levels of the concept network and the
distribution of attributes, showing that some
Financial Activities
SavingsInvestmentTransaction
Angel
InvestmentDistributive
Investment Investment of
Foreigners
Hedge Trade
Cyber
Stock Trade
Off-board
Transaction
Foreign
Currency Deposit
Fixed-Period
Deposit
Joint
Investment
Concept Network
Attributes
Answer Set
Definition Merits ProspectsStrategyDefinition Merits Problems
attributes are shared by some concepts while others
are unique to a concept. The fact that some attributes
are shared by all or most of the concepts belonging to
a higher level concept allow us to assume that related
concepts share the same set of attributes. Another
assumption we employ is that because of the
observation that attributes tend to be found in the
neighborhoods of some concept, the training data for
a particular attribute under a given concept can be
used for the same attribute under another concept.
With these assumptions, we devised a method of
minimizing the training data construction efforts
required for attribute-based classification, which is
essential to select documents to be associated with
<concept, attribute> pairs. In order to re-use the
training data constructed for a particular <concept,
attribute> pair, we define ?-relation between two
concepts. Two concepts are said to have an ?-
relation when the sets of associated attributes are
sufficiently similar to each other. A later section
describes how this relation is used for the knowledge
base construction process.
3. Answer Set Construction
Attributes are defined to exist for concepts
corresponding to categories in subject-based
classification. Documents classified to a concept are
considered to possess one or more attributes that
reveal some characteristics or aspects of a document.
Considering attributes as a different type of
categories, we can define an attribute-based classifier
for documents. While we employ the same learning-
based and rule-based classification techniques for
attribute-based classification, we underscore the way
it is used for automatic knowledge-base construction,
together with traditional subject-based classification.
It works on reducing human efforts dramatically to
knowledge base construction.
Our attribute-based classification method [10],
at least as it is now, is no different from traditional
text classification methods in that it uses training
documents. However, the task of knowledge base
construction for the answer set based retrieval system
calls for unusual requirements. Whereas categories
are pretty much fixed in traditional classification
systems, the number of attributes (i.e. categories) for
a given concept may change in our context. Another
difference comes from the fact that it is not easy to
have a sufficient number of training documents for
each category since there are so many <concept,
attribute> pairs that correspond to categories.
In order to address the issues mentioned above,
we decided to add two additional steps to the
ordinary statistical classification:
- use of pattern rules in conjunction with the
usual learning-based classification
- selective use of words that may not be
specific for attributes
- use of ?-relation
While the first and second were chosen to
improve precision of the classifier, the third was
devised to widen the coverage of <concept, attribute>
pairs for which training documents are provided. In
other words, the use of ?-relation allow us to re-use a
classifier learned from a set of training documents
belonging to a concept for the same attribute class
under a different concept.
To improve upon accuracy of our attribute-based
classifier, we have employed both rule-based and
learning-based approaches. Unlink the case of
subject-based classification, attribute class
boundaries are sometimes hard to detect if only
words are used as features. As such, we decided to
use patterns of word sequences, not just single words.
We have defined rules from train documents, which
express the characteristics of a given attribute class.
Rules may include single words, phrases, sentences,
or even paragraphs. The pattern rules3 are used to
complement the errors made by the machine learning
method, and further it is reused in query processing.
We take the approach of a hybrid system combining
rule-based classification and learning-based
classification, with different weights are applied to
different attributes. Besides we detect that some
terms are not good at discriminate attribute since they
are too specific to concept, whereas these terms are
helpful to classify in concept. Therefore we eliminate
the terms that concept-dependent word which
frequently appearing in a certain concept area.
Another challenging problem in an operational
setting is to define useful attributes to each of the
concept nodes and collect training documents for
each attribute under a concept node. It would be too
expensive and time-consuming to collect a sufficient
number of training documents for all the classes
represented by <concept, attribute> pairs. Currently
the number of classes is more than 250,0004. We need
a method by which we can assign an attribute to a
new document without separate training documents
for that particular <concept, attribute> pair. Our
approach to this problem is to use a special kind of
relation, named ?-relation, defined over the concepts
3 Currently, we define 83 kind of attributes and 1,938 attribute
pattern rule.
4 More precisely, 14700*18, the number of concept nodes times
the average number of attribute number of attributes under each
concept.
Figure 3. Retrieval Process
in the network. The main idea is to build a classifier
for only one concept among the many belonging to
an equivalence class based on the ?-relation, and use
it for other concepts. That is, we only need training
documents for the representative class. Table 1 shows
a distribution of attributes among the concept in ?-
relation. Once the attributes are identified for the first
two concepts, ?Incentives? and ?Hourly wage?, and
training documents are selected for them, all the
attributes except for the last one, ?negotiation?, can
be considered having training documents. If we
define attributes up to the third concept, ?Basic
salary?, all the attributes are covered. The classifier
learned from a single set of documents belonging to
the concept would have the capability of classifying
documents to the attribute classes belonging to other
concepts if the concepts are all ?-related.
We first construct a training document set5 only
for a single concept node representing all those nodes
with the same attributes, using meta-search engine
and document clustering. So we can build a classifier
for those attributes that manifest themselves in the
training set. If some documents fail to be assigned to
an attribute, they are assigned to one of the remaining
attributes. If a concept node other than the
representative node in the equivalence class needs a.
new attribute, we simply look for training documents
for that attribute only. This kind of incremental
process is based on our assumption that although
attributes are associated with individual concept
nodes, they share the common characteristics
regardless of their parent concept nodes. Undoubtedly,
however, this assumption does not always hold.
5 It is only 3% of total amount of training set we needed.
4. Retrieval Process
4.1. Answer Set Search
The main task of answer set search process is
capturing a <concept, attribute> pair from natural
language query, and mapping them to the knowledge
base so that the answer documents can be retrieve.
User query is represented as natural language so that
imply semantic information need, not just single term.
The query processing distinguishes between the main
and additional terms from query. The former covey
the essence of the query, reflected <concept,
attribute> pairs. The latter help to convey the
meaning of the query but can be omitted without
changing the essence of the meaning. The secondary
terms are useful clue for extracting answer sentence
in answer document. Predefined patterns are also
important for query processing. As noted earlier, we
defined attribute pattern rules for improving accuracy
of attribute-based classification. Then we rebuild
these patterns as query-attribute pattern6, expecting
appeared pattern in interrogative form. Query
processing consists of following part: 1) linguistic
analyzing, 2) concept focusing, and 3) determining
attribute, as illustrated in Figure 3.
Given a query, ?what is the problem of angel
investment??, we analyze the sentence structure, such
as conjunction structure and parallel phrases. We
segment complex query into simple sentence. We
distinguish the main terms in query by matching the
longest term in concept network for focusing concept.
To determine attribute of query we first plainly look
6 It was extended 2,170 query-attribute pattern.
?
Knowledge Base
Concept
Network
Attribute
Answer Set
Query Linguistic analyzing
Concept focusing
Determining attribute
Main concept
<concept, attribute> pair
answer set
Highlighted paragraph
Scoring sentence
Selecting candidate
additional query terms
expanded concepts
Expanded answer set
Main attribute
User!
Table 2. Result of Answer Set Construction
Attribute sets
All attributes
(no ?-relations
used)
Pre-selected
attributes
(with ?-relation)
Precision .5025 .6020
Recall .4662 .6696
F-score .4835 .6358(+31.4%)
Time 4 1
Table 3. Result of Answer Set Retrieval
AS based IR Web IR
Total Top 5 Top 5 Top 10
Precision 0.584 0.769 0.291 0.2864
Recall 0.391 0.655 0.291 0.315
F-score 0.468 0.797 0.291 0.3
Highlighting
MRR 0.78
for the term in attribute synset, which included title
attribute representing all those synonym, i.e.
?Problems? is title of set {Warning, danger, abuse,
damage}. If not, we classify the question into one of
83 categories, each of mapped to a particular set of
query-attribute pattern. Our example query map
<angel investment, problem> pair.
After extracting appropriate <concept, attribute>
pair, query expansion is generated, connecting with
related concept in concept network. This expansion is
based on the assessment of similarity between
distances of concept network. The main advantage of
connecting related concept is that the user can be
traverse concept network through semantic path.
Thus continuous search feedback can be possible.
Expanded query map to knowledge base so that the
documents corresponding the <concept, attribute>
pairs can be retrieve as answer set. The results were
ranked using attributed-based classification score in
answer set construction processing.
4.2. Result Presentation: Highlighting
Answer Sentence
Finding answer to a natural language question
involves not only knowing what the answer is but
also where the answer is. The answer set that
produced initial searching step is considered to
include the candidate answer sentence. For detecting
answer sentence, we extract all the possible <concept,
attribute> pairs each sentence in answer document.
The sentence was not include query pairs was discard
so that we can get candidate sentences where answer
is appeared. Similarly query expansion, candidate
sentences calculate score of match additional query
words, which is generated in query processing.
Highest scoring sentence was highlighted including
its former and latter sentence. Right side box in
Figure 3 shows our retrieval process.
5. Experiments
Whereas traditional Q/A and IR system have
competition conference, like TREC, so that they can
start with standard retrieval test collection, to explore
how useful the proposed approach, we evaluate
performance of answer document and candidate
answer sentence. Another difference comes from the
fact that result units for these systems are different.
That is Q/A system returns exactly relevant answer
(50 byte or 250 byte), while IR system returns
document scored by ranking mechanism. Our system
returns ?answer set? distilled semantic knowledge as
retrieval result
5.1. Automatic Answer Set Construction
Before evaluating our retrieval system, we were
interested in knowing how effective and efficient the
proposed knowledge base construction method. We
tested the attribute-based classification for automatic
construction method with 4,599 documents, 120
concepts, and 83 attributes. For performance
comparisons, we used the standard precision, recall,
and F-score [12]. Table 2 shows that the scores for
using ?-relation are higher than that of not using the
relation. We gain a 31.4% increase in F-score and
400% in speed by using the knowledge. The potential
advantage of using the ?-relation is the ability to
minimize the efforts not only required training set
co
th
ch
au
gr
5.
op
A
83
do
av
43
an
pe
qunstruction but also new answer set construction. On
e other hand, a disadvantage is that it has a less
ance to assign new attributes. The result our
tomatic answer set construction was to establish a
ound work for further experiments.
2. Performance of Answer set retrieval
For our experimental evaluations we constructed
erational system in Web, named ?AnyQ?. Our
nyQ system currently consists of 14,700 concepts,
unique attributes, and more than 1.8 million web
cuments in the economy domain for Korean. The
erage number of document under each concept is
.4, the average number of answer document is 25,
d the average number of attribute is 18. To measure
rformance of retrieving answer set, we build 110
ery-relevant answer set, judged by 4 assessor. Our
assessors team with 2 people. For performance
comparisons, we used the P, R, F-score and MRR[5]
for highlighted sentence. All retrieval runs are
completely automatic, starting with queries, retrieve
answer documents, and finally generating a ranked
list of 5 candidate answer sentence.
We build traditional Web IR system on the same
document set for baseline system. The Web IR
system uses 2-poisson model for term indexing and
vector space model for document retrieving. Table 3
summarized the effectiveness of our initial search
step, answer set search. As expected, we obtained
better results progressively as answer set based
approach. The accuracy of Web IR become higher
top10(0.31) to top5(0.291) when we determine more
number of documents retrieved. By contrast, AS
based IR has improvement both precision(0.769) and
recall(0.655) when we assess less number of
documents on top ranked. Even when all documents
was considered(0.468) is higher than Web IR top
10(0.3). It comes from the fact that Web IR retrieves
massive documents appeared term query. But AS
based IR handled prepared answer set. That is, AS
based IR tend to set highly relevant documents on top
result. In other words, answer set based approach can
be easier for user to find information they need with
less effort.
To evaluate highlighted paragraphs, we generate
a ranked list of 5 candidate answer sentences
considered to contain the answer to the query. The
score is 0.78 MRR. As mentioned before, our result is
not the same type as TREC answer. But we can say
that highlighted sentences are helpful to satisfy user
information need.
We further realized that the query pattern as
attribute was not sufficient for finding answer.
Moreover, Korean has characteristic, various
variation of same pattern, its duplicate over the
attributes. It brings the fact that query processing has
ambiguity. Another weakness of our system is that
the accuracy of retrieval depends on knowledge base
granularity. That is, the effectiveness of attribute-
based classification influences whole process of our
approach.
Unfortunately, Our experience cannot compare
with other commercial system since there is no
standard test collection. By the way AskJeeves was
published their accuracy of retrieval is over
30~40%[7], however, this is not absolute contrast.
6. Conclusion
The accuracy of IR result continues to grow on
importance as exponential growth of WWW, and it is
therefore increasingly important that appropriate
retrieval technologies be developed for the web. We
have introduced a new type of IR, ?Answer Set based
IR?, attempts to provide high quality answer
documents to user queries.
In the context of answer set-driven text retrieval,
it is crucial to capture semantics and pragmatics of
sentences in user queries and documents. In our case,
we defined the semantic category of the answer as
attributes, the documents associated with each
attributes as answer set. We attempted to provide
more accurate answers by attaching attributes to
individual concepts in concept network. In order to
construct knowledge bases, a certain level of quality
is guaranteed, we developed a new method for
attributed-based classifier(ABC) and built attribute
pattern for improving accuracy of ABC and query
processing both. In retrieval, we process a natural
language query, extract concepts and attributes, and
map them to the knowledge base so that the answer
documents associated with the <concept, attribute>
pairs can be retrieve.
Our proposed IR ranked highly relevant
document on top result, thus it helps reducing human
efforts dramatically to find answer. By established
operational system, named ?AnyQ?, our experiment
showed realistic possibility of our approach
systematically.
While our experiments were designed carefully,
and comparisons made thoroughly, it has limitations.
Our current work depends on the domain of the
concept network. It is not clear how the proposed
method can be extended to other domains. Our
assumption, reflecting semantics in sentence to
<concept, attribute> pairs, needs to be tested further.
More fundamentally, we need a certain amount of
manual work to initially construct the knowledge
base such as the concept hierarchy and the initial
training documents. We will have to see how the
initial manual process influences the latter processes
and what kind of performance degradation occurs
when smaller efforts are used for the initial
construction.
7. Reference
[1] Dan Moldovan, Sanda Harabagiu, et al ?LCC
Tool for Question Answering?, Proc of Text
Retrieval Conference (TREC-11), November,
2002.
[2] Ask Jeevestm,
http://www.jeevessolutions.com/technology/
[3] M. G. Jang, H. J. Oh, M. S. Chang, et al
?Semantic Based Information Retrieval?, Korea
Information Science Society review, 19(10):7-18,
October 2001.
[4] Marius Pasca and Sanda M. Harabagiu, ?The
Informative Role of WordNet in Open-Domain
Question Answering?, Proc of the NAACL 2001
Workshop on WordNet and Other Lexical
Resource, pp 138-143, CMU, Pittsburge PA,
June 2001
[5] Ellen M. Voorhees, ?Overview of the TREC 2000
Question Answering Track?, Proc of Text
Retrieval Conference (TREC-11), November,
2002
[6] Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin,
?The Use of External Knowledge in Factoid QA?,
Proc of Text Retrieval Conference (TREC-10),
November, 2001
[7] Cody C. T. Kwok, Oren Etzioni, and Daniel S.
Weld, ?Scaling Question Answering to the
Web?, , Proc. of the 10th annual international
ACM WWW10, pp. 150-161, 2001
[8] Susan Dumais, Michele Banko, Eric Brill, Jimmy
Lin, and Andrew Ng, ?Web Question Answering:
Is More Always Better??, Proc. of the 25th
annual international ACM SIGIR ?2002, pp. 291-
298, Tampere, Finland, 2002
[9] Andrew McCallum, Kamal Nigam, et al, ?A
Machine Learning Approach to Building
Domain-Specific Search Engines?, Proc. of the
16th IJCAI Conference, pp 662-667, 1999
[10] Hyo-Jung Oh, Moon-Su Chang, Myung-Gil Jang,
and Sung-Hyon Myaeng, ?Integrating Attribute-
Based Classification for Answer Set
Construction?, Proc. of the 25th annual
international ACM SIGIR ?2002 2nd workshop
on Operational Text Classification, Tampere,
Finland, 2002
[11] Christiane Fellbaum, ?WordNet : An Electronic
Lexical Database?, The MIT press, 1998
[12] Ricardo Baeza-Yates, Berthier Ribeiro-Neto,
?Modern Information Retrieval?, Yiming Yang
and Xin Liu, ?A Re-examination Of Text
Categorization Methods?, pp. 73~98, Addition-
Wesley Published, ACM press New York, 1999.
ABSTRACT 
 
Named entity recognition is important in sophisticated information 
service system such as Question Answering and Text Mining since 
most of the answer type and text mining unit depend on the named 
entity type. Therefore we focus on named entity recognition model in 
Korean. Korean named entity recognition is difficult since each word 
of named entity has not specific features such as the capitalizing 
feature of English. It has high dependence on the large amounts of 
hand-labeled data and the named entity dictionary, even though 
these are tedious and expensive to create. In this paper, we devise 
HMM based named entity recognizer to consider various context 
models. Furthermore, we consider weakly supervised learning 
technique, CoTraining, to combine labeled data and unlabeled data. 
 
Keywords : Korean Named Entity, HMM, Co-Training 
 
1. Introduction 
Named entity(NE) recognition is important for recent 
sophisticated information service such as question answering 
and text mining since it recognizes the words to present the 
core information in text. In particular, the NE recognizer is 
important module in the well-known question answering 
systems such as FALCON, IBM[3][10]. NE recognizer is 
well suited for the recognition of answer type which can be 
equal to the NE type or not. Although an answer is not 
exactly matched to the NE, these two types can be mapping 
to each other by using WordNet[3]. 
NE recognition can be explained with two steps, NE 
detection and NE classification. Whereas NE detection is to 
catch the named entities in the text , NE classification is to 
classify NE into person, organization or location. In Korean, 
NE detection is difficult since each word of name entity has 
not specific features such as the capitalizing feature of 
English. It has high dependence on the large amounts of 
hand-labeled data and the NE dictionary, even though these 
are tedious and expensive to create.  
In the case of NE classification, NE can be classified with 
the clues such as inner word and context word. Although 
these clue words present the feature of NE type, it can be 
used in detecting the NE since the contained word and 
context word can be used in determining the boundary of NE. 
However, the clue words can provoke ambiguity to 
determine the NE type since various NEs  can share  the same 
clue word. Therefore, we devise the statistical model based 
NE recognizer which can unify the detection and 
classification. 
Furthermore, we consider unlabeled data based statis tical 
learning to extend the initial seed data. The weakly 
supervised learning technique is Co-Training method. In this 
paper, we describe the HMM based Korean NE recognition 
and Co-Training method for HMM based boosting. 
 
2. The Problem 
NE dictionary is not enough to cover all of the NEs since 
there are a few types of NEs besides single word. We classify 
Korean NE into three types. The first is single word type, the 
second is compound noun type, and the third is noun phrase 
type. The single word type is usually single noun. The 
second type, compound noun, is composed of a few words 
and affix. The third type can have grammatical morphemes 
besides nouns. The example is described in Figure 1. It 
describes NEs with PERSON(PER), LOCATION (LOC), 
and ORGANIZATION(ORG). In each type, second 
sentence is the result of morphological analysis. 
The example shows the diversity of the Korean NE type. 
In the single word type, if the dictionary has the word, then 
the NE could be detected easily. However, compound noun 
type and noun phrase type require a tremendous number of 
entries in the dictionary. Moreover, in Korean these kinds of 
NE types are used differently in each people . Therefore, it 
Korean Named Entity Recognition using HMM and CoTraining Model 
 
 
Euisok Chung,  Yi-Gyu Hwang,  Myung-Gil Jang 
Electronics and Telecommunications Research Institute 
161, Kajong-Dong, Yusong-Gu, Daejon, 305-350, KOREA 
{eschung, yghwang, mgjang}@etri.re.kr 
 
 
should use the clue word to recognize NE which is shown as 
the compound noun or noun phrase type. This clue word, 
which is context or inner word of NE, can be used in NE 
detection and classification, but we found that the clue word 
can provoke another problem which is the ambiguity; thus 
different types of NEs  can share the same clue word. Which 
means that the clue word dictionary cannot be the unique 
solution. 
 
Single word type 
?????? <ORG> ???<ORG> <PER> ???<PER> ??? 
<ORG> ??/nc ?/sn<ORG> <PER> ?/nc ??/nc<PER> ??/nc ?/jc 
<ORG>jeonnam dae<ORG> <PER>kim - yeong  yong<PER> nun 
Compound noun type 
- 11? <LOC> ?? ??? ??? ?????<LOC> ?? 
11/nn ?/nb <LOC>??/nc ???/nc ???/nc ?????/nc<LOC> ??/jc 
11 il  <LOC>seoul  hannamdong  hayat  grandvolum<LOC> -eseo
Noun phrase type 
?? <LOC> ??? ???? ??<LOC> ?? 
??/nc <LOC>??/nc ?/jm ?/nc ??/pv ?/em ??/nc<LOC> ??/jc 
cafe <LOC> syagal - eui nun - naeri - neun  maeul <LOC> -eseo 
Figure 1.  Named entity example1 
 
Consequently, we suggest three approaches for NE 
recognition. The first is feature dictionary based approach 
which classify the clue words and generate feature types of 
the context or inner word of NE. The second is statistical 
approach which needs named entity tagged corpus and 
context  model to recognize NE. The third is unlabeled data 
based boosting approach since statistical approach, which 
needs hand-labeled NE tagged corpus, cannot avoid data 
sparseness. 
 
3. Related Works 
Statistical approach in NE recognition can be classified 
into supervised learning and weakly supervised learning. 
Supervised learning is based on labeled data. On the other 
hand, weakly supervised method is the learning approach to 
combine labeled data and unlabeled data. From the 
supervised learning point of view, the most representative 
research is HMM based NE recognition. It builds various 
                                                
1 nc(noun), pv(verb), pa(adjective), px(auxiliary verb), 
co(copula), mag(general adverb), mj(conjunctive 
adverb), m(adnoun), xs (noun-derivational suffix), 
xsv(verb-derivational suffix), xsm (adjective-
derivational suffix ), jc(case particle), j(auxiliary 
particle), jj(conjunctive particle), jm(adnom nal Case 
Particle), ep(Prefinal Ending) 
bigram models of NEs and predicts next NE type with the 
previous history, lexical item and NE type. Using simple 
word feature Bikel shows F-measure 90% in English [4]. 
Zhou?s HMM-based chunk tagger approach adopt more 
detailed feature than Bikel?s, and show F-measure 94.3%[5]. 
In this paper, when we designed feature model, we followed 
the HMM-based chunk tagger approach considering the 
property of Korean NE. 
Recently there have been many researches in weakly 
supervised learning technique to combine labeled data and 
unlabeled data. Co-Training method Blum is most famous 
approach to boost the initial learning data with unlabeled 
data[2]. Blum showed that using a large unlabeled data to 
boost performance of a learning algorithm could be used to 
classify web pages when only a small set of labeled 
examples is available [2]. Nigam demonstrated that when 
learning from labeled and unlabeled data, algorithms 
explicitly leveraging a natural independent split of the 
features outperform algorithms that do not[7][8]. Collins and 
Singer showed that the use of unlabeled data can reduce the 
requirements for supervision to just 7 simple ?seed? rules [9]. 
In addition to a heuristic based on decision list learning, they 
also presented a boosting-like framework that builds on ideas 
from Blum[2]. 
 
4. Named entity recognizer 
In this paper, we propose the Korean NE recognizing 
methodology. It is based on the feature model, HMM based 
statistical model and Co-Training based boosting model. 
 
4.1 Feature model 
NE recognition depends on various clues to distinguish 
each type. The inside and outside properties of NE could be 
the clues which can be composed of a few clue class. The 
class consists of ?character feature?, ?named entity dictionary?, 
?inner word?, and ?context word?. It is described in Table 1. 
(1) ?character feature? shows the digit or Chinese feature in 
Korean NE. Digit feature is used to recognize MONEY, 
TIME, and others. (2) ?named entity dictionary? means that 
we should build NE dictionaries. The dictionaries are 
composed of DATE, PERSON, LOCATION, and 
ORGANIZATION. (3) ?inner word? consists of suffix word 
and constituent word of NE. (4) ?context word? is the word 
set adjacent to NE. These feature values are constructed 
manually  and are used with history in annotating NE type. 
As stated above, all the feature classes have ambiguity since 
one feature word can be another feature type. Therefore, we 
cannot recognize NE with only feature dictionaries. 
 
 Type Feature value 
DIGIT  OneDigitNumber,  TwoDigitNumber, 
FourDigitNum 
DIGIT&LETTER  ContainsDigitAndPeriod, ContainsDigitAndLetter 
CHINESE  OneChinese, ThreeChinese, ContainsOneChineseAndLetter 
ALPHA BET  ContainsAlphaAndLetter, AllCapitalization 
character 
feature  
LETTER  TreeLetters 
DATE  DicDATE 
PERSON  DicPERSON 
LOCATION  DicLOC 
named 
entity 
dictionary 
ORGANIZATION  DicORG 
PERCENT  SuffixPERCENT 
MONEY  SuffixMONEY,  SuffixCURRENCY 
TIME  SuffixTIME,  PeriodTIME 
DATE  
SuffixDATE,  WeekDATE, 
SeasonDATE,  PeriodDATE, 
YearDATE,  OthersDATE. 
LOCATION  DistrictSuffixLOC,  SuffixLOC 
inner 
word  
ORGANIZATION  SuffixORG 
PERSON  PositionPERSON,  RelationPERSON, 
JobPERSON 
LOCATION  ClueLOC 
DATE  ClueDATE 
TIME  ClueTIME 
PHONE  CluePHONE 
ORGANIZATION  ClueORG 
PERCENT  CluePERCENT 
MONEY  ClueMONEY 
ADDRESS  ClueADDRESS 
context 
word 
QUANTITY  ClueQUANTITY 
Table 1. Feature type 
 
4.2 HMM based statistical model 
NE recognizer should adopt statistical model since it is 
impossible to construct the dictionary having all of the NE 
entries and moreover, clue word dictionary can provoke 
ambiguity. For instance, proper noun such as person name 
creates everyday. It is unknown word problem. In the feature 
model, some feature  classes can share the same clue words. 
Simply, both DATE and TIME can have the DIGIT 
character feature. Therefore, we adopt statistical model for 
NE recognition. For the statistical approach, we construct NE 
tagged documents, design NE context model, and suggest 
forward-backward view based boosting approach. 
   We build 300 named entity tagged documents in the 
newspaper article domains such as economy, performance 
and travel. The labeled data is  tagged by using tagging tools. 
We attached only NE tags to the text  and do not consider 
morphological information because Korean morphological 
tag is various to the analyzer. Furthermore, we build 
statistical information extractor to learn the NE distributional 
information. The labeled data is used in constructing NE 
statistical data. 
   To build NE context model for statistical NE recognizer, 
we analyze 201 NEs in labeled documents. From the three 
points of view, word feature, inner word feature, and context 
word feature, we analyze NE examples. From the analysis , 
word feature can be classified into single word  and 
compound word. Inner feature has property of full string and 
inner word. From the context word feature, we recognized 
three kinds of features such as root, adjacent morpheme and 
no context. The result of analysis is described in Table 2. 
Finally, we can build four types of NE context model such as 
lexical model, feature model, POS(part of speech) model and 
root model. 
 
Single Noun 129 64.2 % Word 
feature Compound Noun 72 35.8 % 
full string 118 58.7 % Inner 
Word 
Feature Inner word 83 41.3 % 
Left root of a word 14 7.0 % 
Right root of a word 64 31.8 % 
Morpheme left adjacent to 
name entity 38 18.9 % 
Morpheme right adjacent to 
name entity 112 55.7 % 
Context 
Word 
Feature 
no context 35 17.4 % 
Table 2. Named entity analysis 
 
Whereas the rule-based approach needs enormous hand-
crafted rules, statistical model has the advantages of 
simplicity, expansity and robustness in the named entity 
model. The most representative approach of statistical model 
is HMM based approach. To adopt HMM based model, we 
define HMM state and build NE context model which can 
cover various NE contextual information. 
For HMM based approach, HMM state should be 
defined. HMM state is the type of NE constituent. Thus, 
S_LOC is the state wh ich can be the first lexical item of 
LOC typed named entity. C_LOC is the middle state of the 
NE. E_LOC is the final morpheme. In the case of U_LOC, 
single word is the NE word. For example, location name 
?Inchon International Airport? can be tagged with 
?Inchon/S_LOC International /C_LOC Airport/ E_LOC? 
and another named entity ?Seoul? can be labeled with 
?Seoul/U_LOC?. The HMM state is described in Table 5. 
The NE context model is composed of four types such as 
morp(morphology), root, POS and feature . Through this  NE 
context model, the statistical data is learned from the tagged 
corpus, and is used in computing probability of predicting 
HMM state to lexical item. In the case of feature type, 
?Indiana? has ?DicLOC? since it is discovered in LOC 
dictionary and ?professor? is allocated with ?Position-
PERSON? since it is used with position clue word. The 
context model and example is presented in table 3. 
We designed the NE context model with the divided view 
types such as forward/backward views which means left-
right NE context view. In each view type, the probability is 
computed with the product of state transitional probability 
and lexical probability. In forward view, state transitional 
probability is Pr(Si|Si-1, mi-1) which predict ith HMM state Si 
with i-1th state Si-1 and morpheme mi-1, and lexical probability 
is Pr(mi|Si, mi-1) which predict ith morpheme mi with ith state Si 
and morpheme mi-1.  
In table 4, the state of morpheme mi-1 ?? eui?, the state 
Si, is ?NE_U? and next Si-1 is ?-?. Which means that state 
transitional probability can be computed by count(-, NE_U, 
? eui)/count(-, ? eui). In the case of lexical probability, it 
is computed by count(NE_U,? eui, ????? 
stiglitz)/count(NE_U,? eui). The difference of forward 
view and backward view can be explained with state 
transitional probability. Whereas forward view computes 
current state probability with pre-state, backward view 
computes current state probability with next-state. Thus 
forward view considers left contextual information. On the 
other hand, backward view considers right context. 
For the statistical approach, we build labeled data for 
supervised learning and propose four typed NE context 
model which considers left -right contextual information. 
Furthermore, we adopt smoothing model based on the 
modified Kneser-Ney smoothing technique[6] since HMM 
based approach needs smoothing technique for better result. 
After allocating the lexical probability and state transitional 
probability to the HMM state which is coupled with the 
morpheme of sentence, the recognition is processed by 
Viterbi algorithm. Then, NE is recognized in the input 
sentence. 
 
4.3 CoTraining based boosting model 
Co-Training method is most famous approach to boost the 
initial learning data with unlabeled data. In this paper, we 
propose the method to apply Co-Training method to the 
HMM based statistical approach. The main idea is to divide 
view type of the context model into forward view and 
backward view. Simply the forward view?s output, which is 
the result of NE recognition to the unlabeled data, is used for 
input data in the backward view and vise verse. From the 
iteration of the Co-Training procedure, both views  could 
boost each other, which means that the both statistical data 
could increase by using unlabeled data. HMM based 
CoTraining approach is described in Figure 2. 
???? 
(Indiana) 
?? 
(Gary) 
?? 
(Birth) 
? < i-1> 
eui 
????? < i > 
(Stigritz) 
?? < i+1>  
(Professor) 
? 
nun Type 
NE_S NE_E - - NE_U  - - 
POS  Nc nc nc jm nc nc jx 
MORP  ???? ?? ?? ? ????? ?? ? 
ROOT  Root Root Root - Root Root - 
FEATURE  DicLOC - - - - PositionPERSON - 
Table 3. Context model type 
 
Type View type Statistical Model Example 
Forward Pr(si|si-1,mi-1) x Pr(mi|si,mi-1) 
c(-, NE_U, ? eui) / c(-, ? eui) 
x c( NE_U, ?  eui, ????? Stigritz) / c( NE_U, ? eui) 
MORP  
Backward Pr(si|si+1,mi+1) x Pr(mi|si,mi+1) 
c( NE_U, -, ?? professor) / c(-, ?? professor) 
x c( NE_U, ????? Stigritz, ?? professor) / c(NE_U, ?? professor) 
Table 4.  Forward/Backward model for CoTraining 
 
NE type HMM state 
PERSON  S_PER, C_PER, E_PER, U_PER 
LOCATION  S_LOC, C_LOC, E_LOC, U_LOC 
ORGANIZATION  S_ORG, C_ORG, E_ORG, U_ORG  
DATE  S_DATE, C_DATE, E_DATE, U_DATE  
TIME  S_TIME, C_TIME, E_TIME, U_TIME  
PERCENT  S_PERCENT, C_PERCENT, E_PERCENT, U_PERCENT  
MONEY  S_MONEY, C_MONEY, E_MONEY, U_MONEY  
QUANTITY  S_QUANT, C_QUANT, E_QUANT, U_QUANT  
Table 5. HMM state for NE context model 
 
 
Figure 2. HMM based CoTraining approach 
 
 
 (1) CurrentPath is ForwardView, k-th rounds 
(2) Unlabeled text random sampling 
(3) if CurrentPath = ForwardView  
   then  
     (3-1) ForwardView HMM based NE tagging 
   else  
     (3-2) BackwardView HMM based NE tagging 
   endif 
(4) extract n-best tagging result  
(5) if CurrentPath = ForwardView 
   then 
     (5-1) extract BackwardView based statistical data 
from n-best taggin result  
     (5-2) boost Backward data 
     (5-3) CurrentPath = BackwardView 
   else 
     (5-4) extract ForwardView based statistical data  
from n-best taggin result  
     (5-5) boost Forward data 
     (5-6) CurrentPath = ForwardView 
   endif 
(6) if this round is k-th rounds ? 
   then CoTraining exit 
   else goto (2) 
Figure 3.  CoTraining Procedure 
 
The CoTraining algorithm which boosts HMM based NE 
statistical model is described in Figure 3. Here the procedure 
of CoTraining is shown for boosting between divided 
statistical models. In first step, the current view type and the 
number of times of learning round are determined(1) since 
CoTraining approach, which is based on feature redundant 
principle, divides the learning model and reflects one 
learning result to the other learning model. The next step is 
random sampling of unlabeled text data(2).  
After random sampling, the NE statistical data should be 
extracted from the sampling data. At this time the next step 
depends on the current view type which can be Forward-
View or BackwardView. If current learning view is Forward-
View, the next step is forward model based NE tagging 
task(3-1), otherwise, the current learning view is Backward-
View, the next step is backward view based task(3-2). After 
that, from the result of NE tagging, the n-th best tagging 
results are selected(4) and added to the learning data. 
From the first step till now, NE tagging using unlabeled 
data is processed, and the tagging result prepares for boosting 
other view type. If CurrentPath is ForwardView(5), 
backward view data is extracted from the tagging result(5-1) 
and boost backward view data(5-2), and then CurrentPath 
change to BackwardView(5-3). Otherwise, CurrentPath is 
BackwardView(5), forward view data is extracted from the 
tagging result(5-4) and boost forward view data(5-5), and 
then CurrentPath change to ForwardView(5-6). Finally, the 
round is checked whether it is over the pre -defined iteration 
time or not(6). If it pass the time, the procedure ends, 
otherwise the random sampling step repeats. 
 
5. Experiments 
We evaluate named entity recognition with two kinds of 
experiments. One is the performance of named entity 
recognition which unified morp model and feature model to 
learn statistical information. The other experiment is about 
CoTraining performance. 
 
5.1 Name d Entity Recognition Test 
We evaluate NE recognition with morp model and feature 
model since POS statistical data, which is extracted from 
labeled corpus, has some POS tagging error, and root model 
cannot be implemented due to the difficult to determine what 
the root is. Therefore, in this paper we suggest the evaluation 
of the morp model, feature model and morp/feature model 
considering forward/backward view: (1) morp model based 
forward view [M/F], (2) morp model based backward view 
[M/B], (3) morp/feature model based forward view [MF/F], 
(4) morp/feature model based backward view [MF/B], (5) 
morp/feature model based forward/backward view [MF /FB], 
which combination of forward/backward view is based on 
forward-backward algorithm. We give 0.93 weight to morp 
model and give 0.07 weight to feature model. It is optimized 
from many experiments. 
With 300 NE tagged documents, we train the recognizer 
with 270 documents which is compose of 90 economy, 90 
HMM Model 
Forward View 
Backward View 
Statistical 
Data 
Statistical 
Data 
forward View 
output 
backward View 
output 
Unlabeled data 
Labeled 
data 
performance, and 90 travel articles. Other 30 documents, 
which is not used in training, are used as test data. Test data 
has three types such as untrained 10 economical documents 
(N10), untrained 30 documents(N30), and trained 270 
documents(T270).  
The result of the experiment is described in Table 6. From 
the result, we find that the best result of economy 10 test data 
is morp/feature based backward view(MF/B) type with F-
measure 0.67 considering all NE types. If we considered 
only PLO(Person/Location/Organization) type, MF/FB type 
is the best with F-measure 0.77. The first reason that PLO 
recognition is better than other types is that the PLO trained 
data is more abundant. Figure 4 shows that the PLO type is a 
large number in 300 labeled documents. The second reason 
is that the statistical model is not appropriate to some kinds of 
NE types such as DATE, QUANTITY. These type is more 
appropriate when the pattern based approach is adopted. In 
the future, we test the NE recognizer with balanced trained 
data in each NE types. 
PE
R
LOC ORG DATE PC
T
TIME
QUAT
MON
EY
0.00
5.00
10.00
15.00
20.00
25.00
30.00
35.00
40.00
 Figure 4. Distribution of NE types in 300 NE 
labeled documents 
 
5.2 CoTraining Test 
We evaluate Co-Training for the NE recognition using an 
unlabeled economy domain newspaper articles(39,480 
articles). For the training data, we train the NE recognizer 
with 270 labeled articles  and use 10 evaluate articles as test 
data in each Co-Training iteration. In each training step, we 
increase the number of the NE labels from the high ranked 
NE tagging results in proportion to the training data. We test 
CoTraining with morp model which is divided into forward 
view and backward view. After 145 iteration, backward view 
F-measure decreases from 0.615 to 0.6, but forward view F-
measure increases from 0.558 to 0.57.  
 
 
0.52
0.53
0.54
0.55
0.56
0.57
0.58
0.59
0.6
0.61
0.62
0.63
1 15 29 43 57 71 85 99 113 127 141
Morp Model based Backward
Morp Model based Forward
Figure 5. CoTraining result 
 
6. Conclusion 
In this paper, we suggest HMM based NE recognition and 
NAME ENTITY TYPE TOTAL 
  Num 
of 
Test 
Doc. PERSON LOCACTION ORGANIZATION DATE PERCENT TIME QUANTITY MONEY Precision Recall F-measure 
T270 0.97F 0.82F 0.87F 0.79F 0.97F 0.74F 0.71F 0.91F 0.80 0.87 0.84 
N30 0.33F 0.47F 0.33F 0.63F 0.71F 0.51F 0.55F 0.57F 0.40 0.65 0.49 
M/F 
N10 0.56F 0.75F 0.50F 0.51F 0.21F 0.0F 0.0F 0.0F 0.46 0.70 0.55 
T270 0.93F 0.87F 0.85F 0.73F 0.96F 0.68F 0.71F 0.77F 0.80 0.87 0.83 
N30 0.30F 0.47F 0.31F 0.55F 0.71F 0.32F 0.50F 0.42F 0.35 0.63 0.45 
M/B 
N10 0.46F 0.83F 0.47F 0.53F 0.74F 0.0F 0.0F 0.33F 0.51 0.75 0.61 
T270 0.78F 0.70F 0.68F 0.43F 0.28F 0.54F 0.33F 0.31F 0.54 0.74 0.62 
N30 0.67F 0.62F 0.57F 0.33F 0.0F 0.44F 0.44F 0.28F 0.47 0.64 0.54 
MF/F 
N10 0.80F 0.85F 0.64F 0.24F 0.0F 0.0F 0.0F 0.0F 0.54 0.76 0.63 
T270 0.79F 0.68F 0.73F 0.61F 0.94F 0.65F 0.53F 0.68F 0.67 0.70 0.69 
N30 0.71F 0.55F 0.54F 0.53F 0.97F 0.39F 0.45F 0.37F 0.53 0.58 0.55 
MF/B 
N10 0.73F 0.71F 0.68F 0.50F 1.0F 0.0F 0.0F 0.46F 0.69 0.65 0.67 
N30 0.68F 0.55F 0.62F 0.39F 0.0F 0.48F 0.27F 0.06F 0.45 0.58 0.51 MF/FB 
N10 0.81F 0.79F 0.73F 0.38F 0.0F 0.0F 0.0F 0.0F 0.60 0.73 0.66 
Table 6. Named entity recognition evaluation 
 
boosting technique. Through this research,  we met some 
technical problems such as NE context model unification, 
unbalanced labeled corpus, and boosting degradation. (1) We 
consider HMM based NE recognition with four types NE 
context models which are derived from the analysis of NE 
labeled data. However, we cannot unify all of the models in 
unique way since the weighted integration of the models do 
not guarantee the good performance. (2) In using the labeled 
data, we meet the problem that the recognition of NE types 
depends highly on the size of learning data. (3) In HMM 
based CoTraining, the test result shows that high-
performance model enhance low-performance model but 
high-performance model decrease step by step. 
Finally, we conclude that (1) unification issue of various 
context models can be resolved with Maximum Entropy 
model which can combine diverse forms of contextual 
information in a principled manner, (2) unbalanced labeled 
corpus issue may be resolved by gathering contextual 
information independently from the labeled corpus. It makes 
it possible to balance learning data in each type, and (3) 
degradation of the boosting approach is not difficult problem 
since the boosting step in each round can be controlled with 
pre-test. 
 
References 
 
[1] A. Borthwick. A Japanese named entiry recognizer 
constructed by a non-speaker of Japanese. In Proceedings of 
the IREX Workshop, pages 187-193, 1999. 
[2] A. Blum and T. Mitchell. Combining labeled and 
unlabeled data with cotraining. In Proceedings of the 11th 
Annual Conference on Computational Learning Theory, 
pages 92-100, 1998. 
[3] A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi, 
IBM?s Statistical Question Answering System, In 
Proceedings of the Text Retrieval Conference TRECT-9, 
2000. 
[4] D. M. Bikel, S. Miller, R. Schwartz, R. Weishedel, 
Nymble : a high-performance learning named-finder, In 
Proceedings of the Fifth Conference on Applied Natural 
Language Processing, 1997 
[5] G.. Zhou, J. Su, Named Entity Recognition using an 
HMM-based Chunk Tagger, In 40th Annual Meeting of the 
Association for Computational Linguistics, 2002. 
[6] F. James, Modified Kneser-Ney Smoothing of n-gram 
Models. Technical Report TR00-07, RIACS, USRA, 2000. 
[7] K. Nigam and R. Ghani. Analyzing the effectiveness and 
applicability of co-training. In Proceedings of the Ninth 
International Conference on Information and Knowledge 
Management, 2000. 
[8] K. Nigam and R. Ghani. Understanding the Behavior of 
Co-training. In Proceedings of KDD-2000 Workshop on 
Text Mining, 2000. 
[9] M. Collins and Y. Singer. Unsupervised models for 
named entity classification. In Empirical Methods in Natural 
Language Processing and Very Large Corpora, 1999. 
 
[10] S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea, M. 
Surdeanu, R. Bunescu, R. Girju, V. Rus and P. Morarescu, 
FALCON: Boosting Knowledge for Answer Engines, In 
Proceedings of the Text Retrieval Conference TRECT-9, 
2000.  
 
 
