An Empirical Investigation of the Relation Between Discourse Structure and 
Co-Reference 
Dan Cristea 
Department of Computer Science 
University "A.I. Cuza" 
Ia~i, Romania 
dcristea @ infoiasi, m 
Daniel Marcu 
In fo rmat ion  Sc iences  Inst itute and 
Depar tment  of  Computer  Sc ience 
Univers i ty  of  Southern Cal i forn ia  
Los  Angeles ,  CA, USA 
ma twig @ isi. edu 
Nancy Ide 
Department ofComputer Science 
Vassar College 
Poughkeepsie, NY, USA 
ide @ cs. vassal: edu 
Valentin Tablan* 
Department of Computer Science 
University of Sheffield 
United Kingdom 
v. tablan @ sheJ.'field, ac. uk 
Abstract 
We compare the potential of two classes el' linear and hi- 
erarchical models of discourse to determine co-reference 
links and resolve anaphors. The comparison uses a co l  
pus of thirty texts, which were manually annotated for 
co-reference and discourse structure. 
1 Introduction 
Most current anaphora resolution systems implelnent a
pipeline architecture with three modules (Lappin and Le- 
ass, 1994; Mitkov, 1997; Kameyama, 1997). 
1. A COLLF.CT module determines a list of potential 
antecedents (LPA) for each anaphor (l~ronourl, deli- 
nile noun, proper name, etc.) that have the potential 
to resolve it. 
2. A FILTI~,P, module eliminates referees incompatible 
with the anaphor fi'om the LPA. 
3. A PP, EFERENCE module determines the most likely 
antecedent on the basis of an ordering policy. 
In most cases, the COLLECT module determines an LPA 
by enumerating all antecedents in a window o1' text that 
precedes the anaphor under scrutiny (Hobbs, 1978; Lap- 
pin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; 
Ge et al, 1998). This window can be as small as two 
or three sentences or as large as the entire preceding 
text. The FILTEP, module usually imposes emantic on- 
straints by requiring that the anaphor and potential an- 
tecedents have the same number and gendm; that selec- 
tional restrictions are obeyed, etc. The PREFERENCE 
module imposes preferences on potential antecedents 
on the basis of their grammatical roles, parallelism, 
fi'equency, proximity, etc. In some cases, anaphora 
resolution systems implement hese modules explic- 
itly (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 
* On leave fi'om lhe Faculty of Computer Science, University "AI. I. 
Cuza" of lasi. 
1997; Kameyama, 1997). In other cases, these modules 
are integrated by means of statistical (Ge et al, 1998) or 
uncertainty reasoning teclmiques (Mitkov, 1997). 
The fact that current anaphora resolution systems rely 
exclusively on the linear nature of texts in order to de- 
termine the LPA of an anaphor seems odd, given flint 
several studies have claimed that there is a strong rela- 
tion between discourse structure and reference (Sidner, 
1981 ; Grosz and Sidner, 1986; Grosz et al, 1995; Fox, 
1987; Vonk ct al., 1992; Azzam el al., 1998; Hitzcman 
and Pocsio, 1998). These studies claim, on the one hand, 
that the use of referents in naturally occurring texts im- 
poses constraints on the interpretation f discourse; and, 
on the other, that the structure of discourse constrains the 
LPAs to which anaphors can be resolved. The oddness 
of the situation can be explained by lho fac! that both 
groups seem primafilcie m be right. Empirical exper- 
iments studies that employ linear techniques for deter- 
mining the LPAs o1' almphol's report recall and precision 
anaphora resolution results in the range of 80% (Lappin 
and Leass, 1994; Ge ct al., 1998). Empirical experiments 
that investigated the relation between discourse structure 
and reference also claim that by exploiting the structure 
of discourse one has the potential of determining correct 
co-referential links for more than 80% of the referential 
expressions (Fox, 1987; Cristca et al, 1998) although to 
date, no discourse-based anaphora resolution system has 
been implemented. Since no direct comparison of these 
two classes of approaches has been made, it is difficult to 
determine which group is right, and what method is the 
best. 
In this paper, we attempt o Iill this gap by empiri- 
cally comparing the potential of linear and hierarchical 
models el' discourse to correctly establish co-referential 
links in texts, and hence, their potential to correctly re- 
solve anaphors. Since it is likely that both linear- and 
discourse-based anaphora resolution systems can imple- 
ment similar FILTER and PI~,I~FERENCE strategies, we fo- 
cus here only on the strategies that can be used to COL- 
208 
LECT lists of potential antecedents. Specilically, we fo- 
ct, s on deterlnining whether discourse llteories can help 
an anaphora resolution system detemfine LPAs that are 
"better" than the LPAs that can be contputed from a lin- 
ear interpretation f texts. Section 2 outlines the theoreti- 
cal assumptions of otu" empirical investigation. Section 3 
describes our experiment. We conclude with a discussion 
of tile results. 
2 Background 
2.1 Assumptions 
Our approach is based on the following assmnptions: 
I. For each anaphor in a text, an anaphora resolution 
system must produce an LPA that contains a refer- 
ent to which 111e anaphor can be resolvcd. The size 
of this LPA wuies fronl system to system, depend- 
ing on tile theory a system implements. 
2. The smaller the I,PA (while retaining a correct an- 
tecedent), the less likely that errors ill the \]7tI,TH{ 
:(lid PI',V,I;F, RI;,NCI ~, modules wil l  affect the ability of 
a system to select the appropriate referent. 
3. Theory A is better than lheory B for the task of rel- 
erence resolution if theory A produces I J 'As that 
contain more antecedents o which amtphors can be 
corrcclly resolved than theory B, and if the l,l~As 
produced by theory A arc smaller than those pro- 
duccd by theory B. l;or cxaml)lc, if for a given 
anaphor, theory A produces an I,PA thai contains a 
referee to which the anaphor can be resolved, while 
theory B produces an IJ~A that does not contain 
such a re\[eree, theory A is better than theory B. 
Moreover, if for a given anaphor, theory A produces 
an Lt)A wilh two referees and theory B produces an 
LPA with seven rel'crees (each LPA containing a ref- 
eree to which tile anal)her can be resolved), lheory 
A is considered better than theory 11 because it has a 
higher probability of solving that anaphor correctly. 
We consider two classes of models for determining the 
LPAs of anaphors ill a text: 
Linear-k models. This is at class of linear models in 
which the LPAs include all the references foulad in the 
discourse unit under scrutiny and the k discourse units 
that immediately precede it. Linear-0 models an ap- 
proach that assumes that :tll anaphors can be resolved 
intra-unit; Linear- 1 models an approach that cor,'esponds 
roughly to centering (Grosz et al, 1995). Linear-k is con- 
sistent with the asslunl)tions that underlie most current 
anaphora resohltion systems, which look back h units in 
order to resolve an anaphor. 
l) iscourse-V1:k models. In |his class ()1'models, LPAs 
include all lhe refcrentM expressions fotmd in the dis- 
course unit under scrutiny and the k discourse units that 
hierarchically precede it. The units that hierarchically 
precede a given unit are determined according to Veins 
Theory (VT) (Cristea et al, 1998), which is described 
brielly below. 
2.2 Veins Theory 
VT extends and formalizes the relation between dis- 
course  s t ruc ture  and reference proposed by Fox (1987). 
It identilies "veins", i.e., chains of elementary discourse 
units, over discourse structure trees that are built accord- 
ing to the requirements put forth in Rhetorical Structure 
Theo,y (RST) (Mann and Thompson, 1988). 
One of the conjectures ()1' VT is that the vein expres- 
sion of an elementary discourse unit provides a coher- 
ent "abstract" of the discourse fi'agmcnt hat contains 
that unit. As an internally coherent discottrse fragment, 
most ()1' the anaphors and referentM expressions (REst 
in a unit must be resolved to referees that oceul" in the 
text subs:used by the units in tile vein. This conjec- 
ture is consistent with Fox's view (1987) that the units 
that contain referees to which anaphors can be resolved 
are determined by the nuclearity of the discourse units 
thal precede the anaphors and the overall structure of dis- 
course. According to V'I; REs of both satellites and nu- 
clei can access referees of hierarchically preceding nt,- 
cleus nodes. REs of nuclei can mainly access referees of 
preceding nuclei nodes and of directly subordinated, pre- 
ceding satellite nodes. And the interposition ()1' a nucleus 
after a satellite blocks tim accessibility of the satellite for 
all nodes that att'e lovcer in the corresponding discourse 
structure (see (Cristea et al, 1998) for a full delinition). 
Hence, the fundamental intuition unde,lying VT is 
that the RST-spceilie distinction between nuclei and 
satellites constrains the range of referents to which 
anaphors can 19e resolved; in other words, the nucleus- 
satellite distinction induces for each anaphor (and each 
referential expression) a Do,naita of Refcrenlial Acces- 
sibility (DRA). For each anaphor a in a discourse unit 
~z, VT hypothesizes that a can be resolved by examin- 
ing referential expressions that were used in a subset o1' 
the discourse units that precede it; this subset is called 
the DRA of u. For any elcntentary unit u in a text, the 
corresponding DRA is computed autonmtically from the 
rhetorical representation f that text in two steps: 
1. lteads for each node are computed bottom-up over 
the rhetorical representation tree. Heads ()1" elemen- 
tary discottrse traits are the traits themselves. Heads 
of internal nodes, i.e., discourse spans, are con> 
pt, ted by taking the union of the heads of the im- 
mediate child nodes that :ire nuclei. For example, 
for the text in Figure I, whose rhetorical structure is 
shown in Figure 2, the head ()1' span 115,711 is unit 5 
because the head ()t' the inmmdiate nucleus, the ele- 
mentary unit 5, is 5. However, the head of span 116,7\] 
is the list (6,7) because both immediate children are 
nuclei of a multinuclear relation. 
2. Using the results of step 1, Vein expressions are 
eOmlmted top-down lbr each node in the tree. The 
vein of the root is its head. Veins of child nodes 
209 
i. \[Michael D. Casey,\[a top Johnson&Johnson 
...... get, moved teCGe~ Therapy In~,  
a small biotechnology concern here, 
2. to become_~_t>president and ch ieC  
operating officer. \[ 
3. \[Mr. Casey, 46 years old,\] was\[ president of 
J&J's McNeil Pharmaceutical subsidiary,\] 
*t. which was merged with another J&J unit, 
Ortho Pharmaceutical Corp., this year in 
a cost-cutting move. 
5. I,Ir. Casev\[ succeeds M. James Barrett, 50, 
as\[president of ~,~netic Ther-ap~. 
6. Mr. Barrett remains chief executive officer 
7. and becomes chainaan. 
8. \[Mr-\] r. Casey\] said 
9. ~made the move te {\]{e smaller compan~ 
i0. because~saw health care moving toward 
technologies like 
products. 
ll. Ube l ieve  that the field is emerging and i~ 
prepared to break loose, 
12.\[he\[said. 
Figure 1: An example of text and its elementary units. 
The |'eferential expressions surrounded by boxes and el- 
lipses correspond to two distinct co-referential equiv- 
alence classes. Referential expressions urrounded by 
boxes refer to Mr: Casey; those surrounded by ellipses 
refer to Genetic Thercq~y Inc. 
are computed recursively according to tile rules de- 
scribed by Cristea et al(1998). The DRA ot" a unit u 
is given by the units that precede u in 1t~e vein. 
For example, for the text and RST tree in Figures 1 
and 2, the vein expression of unit 3, which contains 
units 1 and 3, suggests that anaphors from unit 3 
should be resolved only to referential expressions 
in units 1 and 3. Because unit 2 is a satellite to 
unit 1, it is considered to be "blocked" to referen- 
tial links fi'om trait 3. In contrast, tile DRA of unit 
9, consisting o1' units 1, 8, and 9, reflects the intu- 
ition that anaphors l?om unit 9 can be resolved only 
to referential ext)ressions fi'om unit 1, which is the 
most important trait in span \[1,7\], and to unit 8, a 
satellite that immediately precedes unit 9. Figure 2 
shows the heads and veins of all internal nodes in 
the rhetorical representation. 
2.3 Comparing models 
The premise underlying out" experiment is that there are 
potentially significant differences in the size of the search 
space rcquired to resolve referential cxpressions when 
using Linear models vs. Discou|'se-VT models. For ex- 
ample, for text and tile RST tree in Figures 1 and 2, the 
Discourse-VT model narrows tlle search space required 
to resolve the a|mphor the smaller company in unit 9. 
According to VT, we look lbr potential antecede|Us for 
the smaller company in the DRA of unit 9, which lists 
units I, 8, and 9. The antecedent Genetic Therapy Inc. 
appears in unit 1 ; therefore, using VT we search back 2 
units (units 8 and 1) to lind a correct antecedent. In con- 
trast, to resolve the same reference using a linear model, 
four units (units 8, 7, 6, and 5) must he examined be- 
fore Genetic The;zq?y is found. Assuming that referen- 
tial links are established as tile text is processed, Genetic 
Therapy would be linked back to pronottn its in unit 2, 
which would in turn be linked to the first occurrence of 
the antecedent,Genetic Therapy Inc., in unit 1, tile an- 
tecedent determined irectly by using V'E 
In general, when hierarchical adjacency is considered, 
an anaphor may be resolved to a referent hat is not the 
closest in a linear interpretation of a text. Simihu'ly, aref- 
erential expression can be linked to a referee that is not 
the closest in a linear interpretation of a text. However, 
this does not create problems because we are focusing 
here only on co-referential relations of identity (see sec- 
tion 3). Since these relations induce equivalence classes 
over tile set of referential expressions in a text, it is suf\[i- 
cient that an anaphor or referential expression is resolved 
to any of the members of the relevant equiw|lence class. 
For example, according to V'I, the referential expression 
MI: Casey in unit 5 in Figm'e 1 can be linked directly 
only to the referee Mr Casey in unit 1, because the DRA 
o1' unit 5 is { 1,5}. By considering the co-referential links 
of the REs in the other units, tile full equivalence class 
can be determined. This is consistent with tile distinction 
between "direct" and "indirect" references discussed by 
Cristea, et al(1998). 
3 The Experiment 
3.1 Materials 
We used thirty newspaper texts whose lengths varied 
widely; the mean o- is 408 words and tile standard e- 
viation/t is 376. Tile texts were annotated manually for 
co-reference r lations of identity (Hirschman and Chin- 
chef, 1997). Tile co-reference relations define equiv- 
alence classes oil the set of all marked referents in a 
text. Tile texts were also manually annotated by Marcu 
et al (1999) with disconrse structures built in the style 
of Mann and Thompson (1988). Each discourse analy- 
sis yielded an average of 52 elementary discourse units. 
See (Hirschman and Chinchor, 1997) and (Marcu et al, 
1999) for details of tile annotation processes. 
210 
H = 1 9 * 
V=lg*  
t I= l  
"?r ... = 1 9 * . . . . . . .  
H=I  L -  - - -  V=lg*  _ . . . . . . . . . .  
H= i |  ~{:-.,, - 
V 1 9 :+:~f- . . . . .  ~'}'-~%l. 3 5 9 * 
1 2 3 4 
H=3 
V=1359 
DF~,= 1 3 
___ - - - - -  - - ~  
_ _ - - -  - - - ___  
I - _  I 
H=5 
- - - - - __  _ 
';,~ = 1 59* 
q _}L___= 6 7 
._~-"-"- \; =xt,,5 67 9 * 
5 //%,,\ 
6 7 
H=9 
"?" = 1 9 * 
I H=9 i . . . .  i la---- . . . . . . . . . . . .  
m 
V= 1 9"  ~ ~---- . . . .  -- i 21-25 
\ [ -  
13-213 
9 191011*  
\" = 1 (g)9  
DRA = 1 11 12 
Figure 2: The I),ST analysis of the text in ligure I. The trcc is rcprescnted using the conventions proposed by Mann 
and Thompson (1988). 
3.2 Compar ing  potent ia l  to es tab l i sh  co - re ferent ia l  
l inks  
3.2.1 Method  
The annotations for co-reference rchttions and rhetorical 
struclure trues for the thirty texts were fused, yielding 
representations that rcllect not  only tile discourse strut- 
lure, but also the co-reference quivalencc lasses spe- 
citic to each tcxl. Based on this information, we cval- 
ualed the potential of each of the two classes (51" mod- 
els discussed in secdon 2 (Linear-k and Discourse-VT-k) 
to correctly establish co-referential links as follows: For 
each model, each k, and each marked referential expres- 
sion o., we determined whether or not tlle corresponding 
LPA (delined over k elementary units) contained a ref- 
eree from the same equiwdence class. For example, for 
the Linear-2 model and referential expression lhe .vmaller 
company in t, nit 9, we estimated whether a co-refercntial 
link could be established between the smaller company 
and another referential expression in units 7, 8, or 9. 
For the Discourse-VT-2 model and the same referential 
expression, we estimated whether a co-referential link 
could bE established between the smaller company and 
another eferential expression in units 1, 8, or 9, which 
correspond to the DRA of unit 9. 
qb enable a fair comparison of the two models, when k 
is la,'ger than the size of the DRA of a given unit, WE ex- 
tend thatDRA using the closest units that precede the unit 
under scrutiny and are not ah'eady in the DRA. Hence, 
for the Linear-3 model and the referential expression the 
smaller conq~any in trait 9, we estimate whether a co- 
referential link can be established between the xmaller 
company and another eferential expression in units 9, 8, 
7, or 6. For tile Discourse-VT-3 model and tile same rcf- 
ermltial expression, we estimate whclher a co-referential 
link can be established between the smaller company and 
another eferential expression in units 9, 8, 1, or 7, which 
correspond I(5 the DRA of mill 9 (unfls 9, 8, and 1) and to 
unit 7, the closest unit preceding unit 9 that is not  ill ils 
I)RA. 
For the l)iscottrse-VT-k models, we assume Ihat the 
Fxtended DRA (EDRA) of size \]c of a unit ~t. (EDRAz: ('~)) 
is given by the lh'st 1 _< k units of a sequence that 
lists, in reverse order, the units of the DRA of '~z plus 
the /c - l units that precede tt but arc not in its DRA. 
For example, \['or the text in Figure 1, the follow- 
ing relations hold: EDRAc,(!)) = 9; EDRAI(C.)) = 
9, 8; EDRA.,(9) = .q,8, I; EI)RA3(9) := 9 ,8 ,1 ,7 ;  
I'~DRA.,I(!)) = !),8, 1,7,6. For Linear-k inodels, the 
EDRAz:(u) is given by u and the k units that immedi- 
ately precede ~t. 
The potential p(M,  a, EDRA,~) (5t' a model M to de- 
termine correct co-referential links with respect o a ref- 
Erential expression a in unit u, given a corresponding 
EDRA of size k (EDRAt.(u)), is assigned the value 1 if 
the EDRA contains a co-referent from the same equiwt- 
lence class as a. Otherwise, p(M, ,, EDRAt~) is assigned 
the value O. The potential p(k4, 6', k) of a model M 
to determine correct co-rEferential links for all referen- 
tial expressions in a corpus of texts C, using EDRAs 
of size k, is computed as the SUlll oF the potentials 
p(M, a.,EI)RA#) of  all referential expressions ct in C'. 
This potential is normalized to a wdue bEtweEn 0 and 
1 by dividing p(k/l, C, k) by the number ot' referential 
211 
expressions in the corpus that have an antecedent. 
By examining the potential of each model to correctly 
determine co-referential expressions for each k, it is pos- 
sible to determine the degree to which an implementa- 
tion of a given approach can contribute to the overall 
efficiency of anaphora resolution systems. That is, if a 
given model has the potential to correctly determine a
significant percentage of co-referential expressions with 
small DRAs, an anaphora resolution system implement- 
ing that model will have to consider fewer options over- 
all. Hence, the probability of error is reduced. 
3.2.2 Results 
The graph in Figure 3 shows the potentials of the Linear- 
k and Discourse-VT-k models to correctly determine co- 
referential links for each k from 1 to 20. The graph in 
Figure 4 represents he same potefftials but focuses only 
on ks in the interval \[2,9\]. As these two graphs how, the 
potentials increase monotonically with k, the VT-k mod- 
els always doing better than the Linear-k models. Even- 
tually, for large ks, the potential performance of the two 
models converges to 100%. 
The graphs in Figures 3 and 4 also suggest resolution 
strategies for implemented systems. For example, the 
graphs suggests that by choosing to work with EDRAs 
of size 7, a discourse-based system has the potential of 
resolving more than 90% of the co-referential links in 
a text correctly. To achieve the same potential, a linear- 
based system needs to look back 8 units. I fa system does 
not look back at all and attempts to resolve co-referential 
links only within the unit under scrutiny (k = 0), it has 
the potential to correctly resolve about 40% of the co- 
referential links. 
To provide a clearer idea of how the two models differ, 
Figure 5 shows, for each k, the value of the Discourse- 
VT-k potentials divided by the value of the Linear-k po- 
tentials. For k = 0, the potentials of both models are 
equal because both use only the unit in focus in order to 
determine co-referential links. For k = 1, the Discourse- 
VT-1 model is about 7% better than the Linear-! model. 
As the value of k increases, the value Discourse-VT- 
k/Linear-k converges to 1. 
In Figures 6 and 7, we display the number of excep- 
tions, i.e., co-referential links that Discourse-VT-k and 
Linear-k models cannot determine correctly. As one 
can see, over the whole corpus, for each k _< 3, the 
Discourse-VT-k models have the potential to determine 
correctly about 100 more co-referential links than the 
Linear-k models. As k increases, the performance of the 
two models converges. 
3,2,3 Statistical significance 
In order to assess the statistical significance of the differ- 
ence between the potentials of the two models to estab- 
lish correct co-referential links, we carried out a Paired- 
Samples T Test for each k. In general, a Paired-Samples 
T Test checks whether the mean of casewise differences 
between two variables differs from 0. For each text in 
I O0 00% 
__~____x.~-.,:..-~'.. . . . . . .  
a0~? oo~??'~ ?- .~:..- ...... 
70.00% 
60 00% ~' 
5000% . 
40O0% ? 
o 
EDRA s i ze  
- - - -  VT-k  . . . . . . .  tmeaf .k  
Figure 3: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (0 < 
k < 20). 
9"5.00% 
90.00"/,, 
85.00% 
800O% 
75.(X~% 
70.007~ 
~ ~ . - ' " ' "  -"'"" --"" .... .. 
J 
1 2 3 4 5 6 7 9 
EDI1A Bt=~ 
? ---13---  VT-k - - . I f , , .  Linear-k 
Figure 4: The potential of Linear-k and Discourse-VT- 
k models to determine correct co-referential links (2 < 
k _< 9). 
the corpus and each k, we determined the potentials of 
both VT-k and Linear-k models to establish correct co- 
referential links in that text. For ks smaller than 4, the 
difference in potentials was statistically significant. For 
example, for k = 3, t = 3.345, df = 29, P = 0.002. For 
values of k larger than or equal to 4, the difference was no 
longer significant. These results are consistent with the 
graphs shown in Figure 3 to 7, which all show that the 
potentials of Discourse-VT-k and Linear-k models con- 
verges to the same value as the value of k increases. 
3.3 Comparing the effort required to establish 
co-referential links 
3.3.1 Method 
The method escribed in section 3.2.1 estimates the po- 
tential of Linear-k and Discourse-VT-k models to deter- 
mine correct co-referential links by treating EDRAs as 
sets. However, from a computational perspective (and 
212 
L'07 / Q 
t .a6  '" 
. . . .  \[ / ", 
t l ' . ,  u 
0.9~ 
o.97 I 
~,oo I "' 
700 c , " ? 
"', o 
'500 " - . -  "~-~ 
400 \ ' - - ,  "'*x ? "--- 12~:~: ...... 
2OO 
~00 
2 3 4 5 6 7 t3 10 
L 
Figure 5: A direct comparison of Discourse-VT-k 
and Linear-VT-k potentials to correctly determine co- 
referential links (0 < k < 20). 
Figure 7: The number of co-referential links that caunot 
be correctly determined by Discourse-VT-k and Linear-k 
models (1 < k < 10). 
14130 
1200 
0 lOOO 
600 
401) 
200 
\ 
- - -~u ,~:  ?~:  :g~: \[t..:~g<_ .~, ~ g :~. . . t~_~.~_~_ , .=~r ,~ ~ 
EORA ~lz~ 
Figure 6: The number of co-referential links that cannot 
be correctly determined by Discourse-VT-k and Linear-k 
models (0 < /~' < 20). 
presumably, from a psycholinguistic perspective as well) 
it also makes sense to compare the effort required by the 
two classes of models to establish correct co-referential 
links. We estimate this effort using a very simple metric 
that assumes that the closer an antecedent is to a cor- 
responding referential expression in the EDRA, the bet- 
ter. Hence, in estimating the effort to establish a co- 
referential link, we treat EDRAs as ordered lists. For ex- 
ample, using the Linear-9 model, to determine the correct 
antecedent of the referential expression the smaller com- 
pany in unit 9 of Figure 1, it is necessary to search back 
through 4 units (to unit 5, which contains the referent Ge- 
netic Therapy). Had unit 5 been Ml: Cassey succeeds M. 
James Barrett, 50, we would have had to go back 8 units 
(to unit 1) in order to correctly resolve the RE the smaller 
company. In contrast, in the Discourse-VT-9 model, we 
go back only 2 units because unit 1 is two units away 
fi'om unit 9 (EDRA:~ (9) = 9, 8, 1,7, 6, 5,4, 3, 2). 
We consider that the effort e(AJ, a, EDRAa.) of a 
model M to determine correct co-referential links with 
respect o one referential, in unit u, given a correspond- 
ing EDRA of size L" (EDRA~.(,)) is given by the number 
of units between u and the first unit in EDRAk(u) that 
contains a co.-referential expression of a. 
The effort e(M, C, k) of a model M to determine cor- 
rect co-referential links for all referential expressions in 
a corpus of texts C using EDRAs of size k was computed 
as the sum of the efforts e(M, a, EDRAk) of all referen- 
tia ! expressions a in C. 
3.3.2 Results 
Figure 8 shows the Discourse-VT-k and Linear-k efforts 
computed over all referential expressions in the corpus 
and all ks. It is possible, for a given referent a and a 
given k, that no co-referential link exists in the units of 
the corresponding EDRAa.. In this case, we consider that 
the effort is equal to k. As a consequence, for small ks 
the effort required to establish co-referential links is sim- 
ilar for both theories, because both can establish only a 
limited number of links. However, as k increases, the 
effort computed over the entire corpus diverges dramat- 
ically: using the Discourse-VT model, the search space 
for co-referential links is reduced by about 800 units for a 
corpus containing roughly 1200 referential expressions. 
3.3.3 Statistical significance 
A Paired-Samples T Test was performed for each k. For 
each text in the corpus and each k, we determined the 
effort of both VT-k and Linear& models to establish cor- 
rect co-referential links in that text. For all ks the dif- 
ference in effort was statistically significant. For exam- 
ple, for k = T, we obtained the values t = 3.5l, df = 
29, P = 0.001. These results are intuitive: because 
EDRAs are treated as ordered lists and not as sets, the 
effect of the discourse structure on establishing correct 
co-referential links is not diminished as/,' increases. 
4 Conclusion 
We analyzed empirically the potentials of discourse and 
linear models of text to determine co-referential links. 
Our analysis suggests that by exploiting the hierarchi- 
cal structure of texts, one can increase the potential 
213 
7000 . . . . . . . . . .  i - ; ; ;~ : ' : - ' ; ;  . . . .  ~ . . . . .  
~ 0 0 -  
k . . . . . . . . . . . . . . . . . . . . . . . . . .  
I000 
tORA ~ z Q  
- - V T ~ e s s  . . . . . . .  ~ o s s  
Figure 8: The effort required by Linear-k and Discourse- 
VT-k models to determine correct co-referential links 
(0< h< 100). 
q 
of natural anguage systems to correctly determine co- 
referential links, which is a requirement for correctly re- 
solving anaphors. If one treats all discourse units in the 
preceding discourse qually, the increase is statistically 
significant only when a discourse-based coreference sys- 
tem looks back at most four discourse units in order to 
establish co-referential links. However, if one assumes 
that proximity plays an important role in establishing co- 
referential links and that referential expressions are more 
likely to be linked to referees that were used recently in 
discourse, the increase is statistically significant no mat- 
ter how many units a discourse-based co-reference sys~ 
tern looks back in order to establish co-referential links. 
Acknowledgements. We are grateful to Lynette 
Hirschman and Nancy Chinchor for making available 
their corpora of co-reference annotations. We are also 
grateful to Graeme Hirst for comments and feedback on 
a previous draft of this paper. 
References 
Saliha Azzam, Kevin Humphreys, and Robert 
Gaizauskas. 1998. Evaluating a focus-based ap- 
proach to anaphora resolution. In Proceedings of 
the 361h Ammal Meeting of the Associatiot~ for 
Computational Linguistics and of the 17th Inter- 
national Conference on Computational Linguistics 
(COLlNG/ACL'98), pages 74-78, Montreal, Canada, 
August 10-14. 
Dan Cristea, Nancy Ide, and Laurent Romary. 1998. 
Veins theory: A model of global discourse cohesion 
and coherence. In Proceedings of the 36th Ammal 
Meeting of the Association Jot" Computational Lin- 
guistics attd of the 17th lntertmtional Conference on 
Computational Linguistics (COLING/ACL'98), pages 
281-285, Montreal, Canada, August. 
Barbara Fox. 1987. Discourse Structure and Anaphora. 
Cambridge Studies in Linguistics; 48. Cambridge Uni- 
versity Press. 
Niyu Oe, John Hale, and Eugene Charniak. 1998. A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Worksho t) on Vet 3, Large Corpora, 
pages 161-170, Montreal, Canada, August 15-16. 
Barbara J. Grosz and Candace L. Sidner. 1986. At- 
tention, intentions, and the structure of discourse. 
Computational Linguistics, 12(3): 175-204, July- 
September. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 
1995. Centering: A framework tbr modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-226, June. 
Lynette Hirschman and Nancy Chinchor, 1997. MUC-7 
Coreference Task Definition, July 13. 
Janet Hitzeman and Massimo Poesio. 1998. Long dis- 
tance pronominalization a d global focus. In Ptv- 
ceedings of the 36th Ammal Meeting of the Associ- 
ation for Computational Linguistics attd of the 17th 
hzternational Conference on Computational Linguis- 
tics (COLING/ACL'98), pages 550-556, Montreal, 
Canada, August. 
Jerry H. Hobbs. 1978. Resolving pronoun references. 
Lingua, 44:311-338. 
Megumi Kameyama. 1997. Recognizing referential 
links: An information extraction perspective. In Pro- 
ceedings of the ACL/EACL'97 Workshop on Opera- 
tional Factors in Practical, Robust Anaphora Resoht- 
tion, pages 46-53. 
Shalom Lappin and Herbert J. Leass. 1994, An algo- 
rithm for pronominal anaphora resolution. Computa- 
tional Linguistics, 20(4):535-561. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical structure theory: Toward a functional the- 
ory of text organization. Text, 8(3):243-28 i. 
Daniel Marcu, Estibaliz Amorrortu, and Magdalena 
Romera. 1999. Experiments in constructing a cor- 
pus of discourse trees. In Ptvceedings of the ACL'99 
Worksho t)on Standards and 7bols for Discout:re Tag- 
ging, pages 48-57, University of Maryland, June 22. 
Ruslan Mitkov. 1997. Factors in anaphora resolution: 
They are not the only things that matter, a case study 
based on two different approaches. In Proceedings of 
the ACL/EACL'97 Workshop on Operational Factors 
in Practical, Robust Anaphora Resolution, pages 14- 
21. 
Candace L. Sidner. 1981. Focusing for interpretation f
pronouns. Computational Linguistics, 7(4):217-231, 
October-December. 
Wietske Vonk, Lettica G.M.M. Hustinx, and Wire tt.G. 
Simons. 1992. The use of referential expressions in 
structuring discourse, l~mguage and Cognitive Pro- 
cesses, 7(3,4):301-333. 
214 
Fine-Grained Word Sense Disambiguation Based on Parallel Corpora,  
Word Alignment, Word Clustering and Aligned Wordnets 
Dan TUFI? 
Institute for Artificial 
Intelligence 
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
tufis@racai.ro 
Radu ION 
Institute for Artificial 
Intelligence  
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
radu@racai.ro 
Nancy IDE  
Department of Computer Science
Vassar College  
Poughkeepsie,  
NY 12604-0520  
USA 
ide@cs.vassar.edu 
Abstract 
The paper presents a method for word sense 
disambiguation based on parallel corpora. The 
method exploits recent advances in word 
alignment and word clustering based on 
automatic extraction of translation equivalents 
and being supported by available aligned 
wordnets for the languages in the corpus. The 
wordnets are aligned to the Princeton 
Wordnet, according to the principles 
established by EuroWordNet. The evaluation 
of the WSD system, implementing the 
method described herein showed very 
encouraging results. The same system used in 
a validation mode, can be used to check and 
spot alignment errors in multilingually 
aligned wordnets as BalkaNet and 
EuroWordNet.  
1 Introduction 
Word Sense Disambiguation (WSD) is well-
known as one of the more difficult problems in 
the field of natural language processing, as noted 
in  (Gale et al 1992; Kilgarriff, 1997; Ide and 
V?ronis, 1998), and others. The difficulties stem 
from several sources, including the lack of means 
to formalize the properties of context that 
characterize the use of an ambiguous word in a 
given sense, lack of a standard (and possibly 
exhaustive) sense inventory, and the subjectivity 
of the human evaluation of such algorithms. To 
address the last problem, (Gale et al 1992) argue 
for upper and lower bounds of precision when 
comparing automatically assigned sense labels 
with those assigned by human judges. The lower 
bound should not drop below the baseline usage 
of the algorithm (in which every word that is 
disambiguated is assigned the most frequent 
sense) whereas the upper bound should not be too 
restrictive? when the word in question is hard to 
disambiguate even for human judges (a measure 
of this difficulty is the computation of the 
agreement rates between human annotators). 
Identification and formalization of the 
determining contextual parameters for a word 
used in a given sense is the focus of WSD work 
that treats texts in a monolingual setting?that is, 
a setting where translations of the texts in other 
languages either do not exist or are not 
considered. This focus is based on the 
assumption that for a given word w and two of its 
contexts C1 and C2, if C1 ? C2 (are perfectly 
equivalent), then w is used with the same sense in 
C1 and C2. A formalized definition of context for 
a given sense would then enable a WSD system 
to accurately assign sense labels to occurrences 
of w in unseen texts. Attempts to characterize 
context for a given sense of a word have 
addressed a variety of factors: 
? Context length: what is the size of the window 
of text that should be considered to determine 
context?  Should it consist of only a few words, 
or include much larger portions of text? 
? Context content: should all context words be 
considered, or only selected words (e.g., only 
words in a certain part of speech or a certain 
grammatical relations to the target word)? Should 
they be weighted based on distance from the 
target or treated as a ?bag of words?? 
? Context formalization: how can context 
information be represented to enable definitions 
of an inter-context equivalence function? Is there 
a single representation appropriate for all words, 
or does it vary according to, for example, the 
word?s part of speech? 
The use of multi-lingual parallel texts 
provides a very different approach to the problem 
of context identification and characterization. 
?Context? now becomes the word(s) by which 
the target word (i.e., the word to be 
disambiguated) is translated in one or more other 
languages. The assumption here is that different 
senses of a word are likely to be lexicalized 
differently in different languages; therefore, the 
translation can be used to identify the correct 
sense of a word. Effectively, the translation 
captures the context as the translator conceived it. 
The use of parallel translations for sense 
disambiguation brings up a different set of issues, 
primarily because the assumption that different 
senses are lexicalized differently in different 
languages is true only to an extent. For instance, 
it is well known that many ambiguities are 
preserved across languages (e.g. the French 
int?r?t and the English interest), especially 
languages that are relatively closely related. This 
raises new questions: how many languages, and 
of which types (e.g., closely related languages, 
languages from different language families), 
provide adequate information for this purpose? 
How do we measure the degree to which 
different lexicalizations provide evidence for a 
distinct sense? 
We have addressed these questions in 
experiments involving sense clustering based on 
translation equivalents extracted from parallel 
corpora (Ide, 199; Ide et al, 2002). Tufi? and Ion 
(2003) build on this work and further describe a 
method to accomplish a ?neutral? labelling for 
the sense clusters in Romanian and English that 
is not bound to any particular sense inventory. 
Our experiments confirm that the accuracy of 
word sense clustering based on translation 
equivalents is heavily dependent on the number 
and diversity of the languages in the parallel 
corpus and the language register of the parallel 
text. For example, using six source languages 
from three language families (Romance, Slavic 
and Finno-Ugric), sense clustering of English 
words was approximately 74% accurate; when 
fewer languages and/or languages from less 
diverse families are used accuracy drops 
dramatically. This drop is obviously a result of 
the decreased chances that two or more senses of 
an ambiguous word in one language will be 
lexicalized differently in another when fewer 
languages, and languages that are more closely 
related, are considered. 
To enhance our results, we have explored the 
use of additional resources, in particular, the 
aligned wordnets in BalkaNet (Tufi? et al 
2004a). BalkaNet  is a European project that is 
developing monolingual wordnets for five Balkan 
languages (Bulgarian, Greek, Romanian Serbian, 
and Turkish) and improving the Czech wordnet 
developed in the EuroWordNet project. The 
wordnets are aligned to the Princeton Wordnet 
(PWN2.0), taken as an interlingual index, 
following the principles established by the 
EuroWordNet consortium. The underlying 
hypothesis in this experiment exploits the 
common intuition that reciprocal translations in 
parallel texts should have the same (or closely 
related) interlingual meanings (in terms of 
BalkaNet, interlingual index (ILI) codes). 
However, this hypothesis is reasonable if the 
monolingual wordnets are reliable and correctly 
linked to the interlingual index (ILI). Quality 
assurance of the wordnets is a primary concern in 
the BalkaNet project, and to this end, the 
consortium developed several methods and tools 
for validation, described in various papers 
authored by BalkaNet consortium members (see 
Proceedings of the Global WordNet Conference, 
Brno, 2004).  
We previously implemented a language-
independent disambiguation program, called 
WSDtool, which has been extended to serve as a 
multilingual wordnet checker and specialized 
editor for error-correction. In (Tufi?, et al, 2004) 
it was demonstrated that the tool detected several 
interlingual alignment errors that had escaped 
human analysis. In this paper, we describe a 
disambiguation experiment that exploits the ILI 
information in the corrected wordnets 
2 Methodology and the algorithm 
Our methodology consists of the following steps: 
1. given a bitext TL1L2 in languages L1 and L2 for 
which there are aligned wordnets, extract all pairs 
of lexical items that are reciprocal 
translations:{<WiL1 WjL2>+} 
2. for each lexical alignment <WiL1 WjL2>, extract 
the ILI codes for the synsets that contain WiL1 and 
WjL2 respectively to yield two lists of ILI codes, 
L1ILI(WiL1) and L2ILI(WjL2) 
3. identify one ILI code common to the 
intersection L1ILI(WiL1) ? L2ILI(WjL2) or a pair of 
ILI codes ILI1? L1ILI(WiL1)  and ILI2? L2ILI(WjL2), 
so that ILI1 and ILI2 are the most similar ILI 
codes (defined below) among the candidate pairs 
(L1ILI(WiL1)?L2ILI(WjL2) [? = Cartesian product]. 
The accuracy of step 1 is essential for the 
success of the validation method. A recent shared 
task evaluation) of different word aligners 
(www.cs.unt.edu/~rada/wpt, organized on the 
occasion of the Conference of the NAACL 
showed that step 1 may be solved quite reliably. 
Our system (Tufi? et al 2003) produced lexicons 
relevant for wordnets evaluation, with an 
aggregated F-measure as high as 84.26%. 
Meanwhile, the word-aligner was further 
improved so that current performance on the 
same data is about 1% better on all scores in 
word alignment and about 2% better in wordnet-
relevant dictionaries. The word alignment 
problem includes cases of null alignment, where 
words in one part of the bitext are not translated 
in the other part; and cases of expression 
alignment, where multiple words in one part of 
the bitext are translated as one or more words in 
the other part. Word alignment algorithms 
typically do not take into account the part of 
speech (POS) of the words comprising a 
translation equivalence pair, since cross-POS 
translations are rather frequent. However, for the 
aligned wordnet-based word sense 
disambiguation we discard both translation pairs 
which do not preserve the POS and null 
alignments. Multiword expressions included in a 
wordnet are dealt with by the underlying 
tokenizer. Therefore, we consider only one-to-
one, POS-preserving alignments. 
Once the translation equivalents were 
extracted, then, for any translation equivalence 
pair <WL1 WL2> and two aligned wordnets, the 
steps 2 and 3 above should ideally identify one 
ILI concept lexicalized by WL1 in language L1 
and by WL2 in language L2. However, due to 
various reasons, the wordnets alignment might 
reveal not the same ILI concept, but two concepts 
which are semantically close enough to license 
the translation equivalence of WL1 and WL2. This 
can be easily generalized to more than two 
languages. Our measure of interlingual concepts 
semantic similarity is based on PWN2.0 
structure. We compute semantic-similarity score 
by formula: 
ss(ILI1, ILI2) = 1/1+k 
where k is the number of links from ILI1 to ILI2 
or from both ILI1 and ILI2 to the nearest common 
ancestor. The semantic similarity score is 1 when 
the two concepts are identical, 0.33 for two sister 
concepts, and 0.5 for mother/daughter, 
whole/part, or concepts related by a single link. 
Based on empirical studies, we decided to set the 
significance threshold of the semantic similarity 
score to 0.33.  Other approaches to similarity 
measures are described in (Budanitsky and Hirst 
2001). 
In order to describe the algorithm for WSD 
based on aligned wordnets let us assume we have 
a parallel corpus containing texts in k+1 
languages (T, L1, L2?Lk), where T is the target 
language and L1, L2?Lk are the source languages 
and monolingual wordnets for each of the k+1 
languages interlinked via an ILI-like structure. 
For each source language and for all occurrences 
of a specific word in the target language T, we 
build a matrix of translation equivalents as shown 
in Table 1 (eqij represents the translation 
equivalent in the ith source language of the jth 
occurrence of the word in the target language).  
 Occ #1 Occ #2 ? Occ #n 
L1 eq11 eq12 ? eq1n 
L2 eq21 eq22 ? eq2n 
? ? ? ? ? 
Lk eqk1 eqk2 ? eqkn 
Table 1. The translation equivalents matrix 
(EQ matrix) 
If the target word is not translated in language Li, 
eqij is represented by the null string.  
The second step transforms the matrix in 
Table 1 to a VSA (Validation and Sense 
Assignment) matrix with the same dimensions 
(Table 2).  
 Occ #1 Occ #2 ? Occ #n 
L1 VSA11  VSA12 ? VSA 1n  
L2 VSA21 VSA22  VSA22 
? ? ? ? ? 
Lk VSAk1 VSAk2 ? VSAkn 
Table 2. The VSA matrix 
Here,  VSAij = LENILI(WEN) ? LiILI(WjLi),, where 
LENILI(WEN) represent the ILI codes of all synsets 
in which the target word WEN occurs, and 
LiILI(WjLi) is the list of ILI-codes for all synsets in 
which the translation equivalent for the jth 
occurrence of WEN occurs. 
If no translation equivalent is found in 
language Li for the jth occurrence of WEN, 
VSA(i,j) is undefined; otherwise, it is a set 
containing 0, 1, or more ILI codes. For undefined 
VSAs, the algorithm cannot determine the sense 
number for the corresponding occurrence of the 
target word. However, it is very unlikely that an 
entire column in Table 2 is undefined, i.e., that 
there is no translation equivalent for an 
occurrence of the target word in any of the source 
languages.  
When VSA(i,j) contains a single ILI code, the 
target occurrence and its translation equivalent 
are assigned the same sense. 
When VSA(i,j) is empty?i.e., when none of 
the senses of the target word corresponds to an 
ILI code to which a sense of the translation 
equivalent was linked--the algorithm selects the 
pair in LENILI(WEN) ? LiILI(WjLi) with the highest 
similarity score. If no pair in LENILI(WEN) ? 
LiILI(WjLi) has a  the semantic similarity score 
above the significance threshold, neither the 
occurrence of the target word nor its translation 
equivalent can be semantically disambiguated; 
but once again, it is extremely rare that there is 
no translation equivalent for an occurrence of the 
target word in any of the source languages. 
In case of ties, the pair corresponding to the 
most frequent sense of the target word in the 
current bitext pair is selected. If this heuristic in 
turn fails, the choice is made in favor of the pair 
corresponding to the lowest PWN2.0 sense 
number for the target word, since PWN senses 
are ordered by frequency.  
When the VSA cell contains two or more ILI-
codes, we have the case of cross-lingual 
ambiguity, i.e., two or more senses are common 
to the target word and the corresponding 
translation equivalent in the ith language.  
2.1 Agglomerative clustering   
As noted before, when VSA(i,j) is undefined, we 
may get the information from a VSA 
corresponding to the same occurrence of the 
target word in a different language. However, this 
demands that aligned wordnets are available for 
all languages in the parallel corpus, and that the 
quality of the inter-lingual linking is high for all 
languages concerned. In cases where we cannot 
fulfill these requirements, we rely on a ?back-
off? method involving sense clustering based on 
translation equivalents, as discussed in (Ide, et 
al., 2002). We apply the clustering method after 
the wordnet-based method has been applied, and 
therefore each cluster containing an 
undisambiguated occurrence of the target word 
will also typically contain several occurrences 
that have already been assigned a sense. We can 
therefore assign the most frequent sense 
assignment in the cluster to previously unlabeled 
occurrences within the same cluster. The 
combined approach has two main advantages: 
? it eliminates reliance only on high-quality, k-1 
aligned wordnets. Indeed, having k+1 languages 
in our corpus, we need only apply the WSD 
method to the aligned wordnets for the target 
language (English in our case) and one source 
language, say Li, and alignment lexicons from the 
target language to every other language in the 
corpus. The WSD procedure in the bilingual 
setting would ensure the sense assignment for 
most of the non-null translation equivalence pairs 
and the clustering algorithm would classify the 
target words which were not translated (or for 
which the word alignment algorithm didn?t find a 
correct translation) in Li based on their 
equivalents in the other k-1 source languages. 
? it can reinforce or modify the sense 
assignment decided by the tie heuristics in case 
of cross-lingual ambiguity. 
To perform the clustering, we derive a set of 
m binary vectors VECT(Lp, TWi) for each source 
language Lp and each target word i occurring m 
times in the corpus. To compute the vectors, we 
first construct a Dictionary Entry List 
DEL(Lp,TWi)={Wj | <TWi, Wj> is a translation 
equivalence pair}, comprising the ordered list of 
all the translation equivalents in the source 
language pL of the target word TW
i. In this part 
of the experiment, the translation equivalents are 
automatically extracted from the parallel corpus 
using a hypothesis testing algorithm described in 
(Tufi? 2002). VECT(Lp,TWik)  specifies which of 
the possible translations of TWi was actually 
used as an equivalent for the kth occurrence of 
TWi. All positions in VECT(Lp,TWik)  are set to 
0 except the bit at position h, which is 1 if the 
translation equivalent (Lp,TWik)=DELh(Lp,TWi). 
The vector for each target word occurrence is 
obtained by concatenating the VECT(Lp,TWik) 
for all k souce languages  and its length is 
?
=
k
1p
i
p  |)TW,DEL(L| . 
We use a Hierarchical Clustering Algorithm 
based on Stolcke?s Cluster2.9 to classify similar 
vectors into sense classes. Stolcke?s algorithm 
generates a clustering tree, the root of which 
corresponds to a baseline clustering (all the 
occurrences are clustered in one sense class) and 
the leaves are single element classes, 
corresponding to each occurrence vector of the 
target word. An interior cut in the clustering tree 
will produce a specific number (say X) of sub-
trees, the roots of which stand for X classes each 
containing the vectors of their leaves. We call an 
interior cut a pertinent cut if X is equal to the 
number of senses TWi has been used throughout 
the entire corpus. One should note that in a 
clustering tree many pertinent cuts could be 
possible. The pertinent cut which corresponds to 
the correct sense clustering of the m occurrences 
of TWi is called a perfect cut.  However, if TWi 
has Y possible senses, it is possible that only a 
subset of the Y senses will be used in an arbitrary 
text. Therefore, a perfect cut in a clustering tree 
cannot be deterministically computed. Instead of 
deriving the clustering tree and guessing at a 
perfect cut, we stop the clustering algorithm 
when Z clusters have been created, where Z is the 
number of senses in which the occurrences of 
TWi have been used in the text in question. 
However, the value of Z is specific to each word 
and depends on the type and size of the text; it 
cannot therefore be computed a priori. In our 
previous work (Tufi? and Ion, 2003), to 
approximate Z we imposed an exit condition for 
the clustering algorithm based on distance 
heuristics. In particular, the algorithm stops when 
the minimal distance between the existing classes 
increases beyond a given threshold level:  
?>+
?+
)1(
)()1(
kdist
kdistkdist                                   (1) 
where dist(k) is the minimal distance between 
two clusters at the k-th  iteration  step and ? is  an 
empirical numerical threshold. Experimentation 
revealed that reasonable results are achieved with 
a value for ? is 0.12. However, although the 
threshold is a parameter for the clustering 
algorithm irrespective of the target words, the 
number of classes the clustering algorithm 
generates (Z) is still dependent on the particular 
target word and the corpus in which it appears. 
By using sense information produced by the 
ILI-similarity approach, the algorithm and its exit 
condition have been modified as described 
below:  
- the sense label of a cluster is given by the 
majority sense of its members as assigned by the 
wordnet-based sense labelling; a cluster 
containing only non-disambiguated occurrences 
has an wild-card sense label;    
- two joinable clusters (that is the clusters with 
the minimal distance and the exit condition (1) 
not satisfied) are joint only when their sense 
labels is the same or one of them has an wild-
card sense label; in this case the wild-card sense 
label is turned into the sense label of the sense-
assigned cluster. Otherwise the next distant 
clusters are tried. 
- the algorithm stops when no clusters can be 
further joined. 
3 The Experiment 
The parallel corpus we used for our experiments 
is based on Orwell?s novel ?Ninety Eighty Four? 
(1984) which has been initially developed by the 
Multext-East consortium. Besides Orwell?s 
original text, the corpus contained professional 
translations in six languages (Bulgarian, Czech, 
Estonian, Hungarian, Romanian and Slovene). 
The Multext-East corpus (and other language 
resources) is maintained by Toma? Erjavec and a 
new release of it may be found at 
http://nl.ijs.si/ME/V3. Later, the parallel corpus 
has been extended with many other new language 
translations. The BalkaNet consortium added 
three new translations to the ?1984? corpus: 
Greek, Serbian and Turkish. Each language text 
is tokenized, tagged and sentence aligned to the 
English original. We extracted from the entire 
parallel corpus only the languages of concern in 
the BalkaNet project (English, Bulgarian, Czech, 
Greek, Romaniann, Serbian and Turkish) and 
further retained only the 1-1 sentence alignments 
between English and all the other languages. This 
way, we built a unique alignment for all the 
languages and, by exploiting the transitivity of 
sentence alignment, we are able to make 
experiments with any combination of languages. 
The BalkaNet version of the ?1984? corpus is 
encoded as a sequence of translation units (TU), 
each containing one sentences per language, so 
that they are reciprocal translations.  In order to 
evaluate both the performance of the WSDtool 
and to assess the accuracy of the interlingual 
linking of the BalkaNet wordnets we selected a 
bag of English target words (nouns and verbs) 
occurring in the corpus. The selection considered 
only polysemous words (at least two senses per 
part of speech) implemented (and ILI linked) in 
all BalkaNet wordnets. There resulted 211 words 
with 1644 occurrences in the English part of the 
parallel corpus. 
Three experts independently sense-tagged all 
the occurrences of the target words and the 
disagreements were negotiated until consensus 
was obtained. The commonly agreed annotation 
represented the Gold Standard (GS) against 
which the WSD algorithm was evaluated. 
Additionally, a number of 13 students, enrolled in 
a Computational Linguistics Master program, 
were asked to manually sense-tag overlapping 
subsets of the same word occurrences.  The 
overlapping ensured that each target word 
occurrence was seen by at least three students. 
Based on the students? annotations, using a 
majority voting, we computed another set of 
comparison data which below is referred to as 
SMAJ (Students MAJority). 
Finally, the same targeted words were 
automatically disambiguated by the WSDtool 
algorithm (ALG) which was run both with and 
without the back-off clustering algorithm.  For 
the basic wordnet-based WSD we used the 
Princeton Wordnet, the Romanian wordnet and 
the English-Romanian translation equivalence 
dictionary. For the back-off clustering we 
extracted a four1 language translation dictionary 
(EN-RO-CZ-BG) based on which we computed 
the initial clustering vectors for all occurrences of 
the target words. 
                                                     
1 Although we used only RO, CZ and BG 
translation texts, nothing prevents us from using any 
other translations, irrespective of whether their 
languages belong or not to the BalkaNet consortium.  
Out of the 211 set of targeted words, with 
1644 occurrences the system could not make a 
decision for 38 (18 %) words with 63 
occurrences (3.83%). Most of these words were 
happax legomena (21) for which neither the 
wordnet-based step not the clustering back-off 
could do anything. Others, were not translated by 
the same part of speech, were wrongly translated 
by the human translator or not translated at all 
(28). Finally, four occurrences remained 
untagged due to the incompleteness of the 
Romanian synsets linked to the relevant concepts 
(that is the four translation equivalents had their 
relevant sense missing from the Romanian 
wordnet). Applying the simple heuristics (SH) 
that says that any unlabelled target occurrence 
receives its most frequent sense, 42 out of 63 of 
them got a correct sense-tag. The table below 
summarizes the results.   
WSD annotation Precision Recall F 
AWN  74.88% 72.01% 73.41%
AWN + C 75.26% 72.38% 73.79%
AWN + C + SH 74.93% 74.93% 74.93%
SMAJ 72.99% 72.99% 72.99%
Table 4. WSD precision recall and F-measure for 
the algorithm based on aligned wordnets (AWN), 
for AWN with clustering (AWN+C) and for 
AWN+C and the simple heuristics 
(AWN+C+SH) and for the students? majority 
voting (SMAJ) 
It is interesting to note that in this experiment 
the students? majority annotation is less accurate 
than the one achieved by the automatic WSD 
annotation in all three variants. This is a very 
encouraging result since it shows that the tedious 
hand-made WSD in building word-sense 
disambiguated corpora for supervised training 
can be avoided. 
4 Conclusion 
Considering the fine granularity of the PWN2.0 
sense inventory, our disambiguation results using 
parallel resources are superior to the state of the 
art in monolingual WSD (with the same sense 
inventory). This is not surprising since the 
parallel texts contain implicit knowledge about 
the sense of an ambiguous word, which has been 
provided by human translators.  The drawback of 
our approach is that it relies on the existence of 
parallel data, which in the vast majority of cases 
is not available. On the other hand, supervised 
monolingual WSD relies on the existence of large 
samples of training data, and our method can be 
applied to produce such data to bootstrap 
monolingual applications. Given that parallel 
resources are becoming increasingly available, in 
particular on the World Wide Web (see for 
instance http://www.balkantimes.com where the 
same news is published in 10 languages), and 
aligned wordnets are being produced for more 
and more languages, it should be possible to 
apply our and similar methods to large amounts 
of parallel data in the not-too-distant future.  
One of the greatest advantages of our 
approach is that it can be used to automatically 
sense-tag corpora in several languages at once. 
That is, if we have a parallel corpus in multiple 
languages (such as the Orwell corpus), 
disambiguation performed on any one of them 
propagates to the rest via the ILI linkage. Also, 
given that the vast majority of words in any given 
language are monosemous (e.g., approximately 
82% of the words in PWN have only one sense), 
the use of parallel corpora in multiple languages 
for WSD offers the potential to significantly 
improve results and provide substantial sense-
annotated corpora for training in a range of 
languages.  
Acknowledgements 
The work reported here was carried within the 
European project BalkaNet, no. IST-2000 29388 
and support from the Romanian Ministry of 
Education and Research. 
References  
Alex. Budanitsky and Graeme Hirst 2001. 
Semantic distance in WordNet: An 
experimental, application-oriented evaluation 
of five measures. Proceedings of the Workshop 
on WordNet and Other Lexical Resources, 
Second meeting of the NAACL, Pittsburgh, 
June. 
William Gale, Ken Church and Dan Yarowsky 
1992. Estimating upper and lower bounds on 
the performance of wordsense disambiguation 
programs. Proceedings of the 30th Annual 
Meeting of ACL, 249-256. 
Adam Kilgarriff 1997. I don't believe in word 
senses. In Computers and the Humanities, 31 
(2): 91-113. 
Nancy Ide and Jean V?ronis 1998. Word Sense 
Disambiguation: The State of the Art. 
Computational Linguistics,24(1): 1-40. 
Nancy Ide, N. 1999. Parallel translations as sense 
discriminators. SIGLEX99: Standardizing 
Lexical Resources, ACL99 Workshop, College 
Park, Maryland, 52-61. 
Nancy Ide, Toma? Erjavec and Dan Tufi? 2002. 
Sense Discrimination with Parallel Corpora. In 
Proceedings of the SIGLEX Workshop on Word 
Sense Disambiguation: Recent Successes and 
Future Directions, 56-60, Philadelphia. 
Andreas Stolcke 1996. ftp.icsi.berkeley.edu/ 
pub/ai/stolcke/software/cluster-2.9.tar.Z/ 
Dan Tufi?. 2002. A cheap and fast way to build 
useful translation lexicons. In Proceedings of 
the 19th International Conference on 
Computational Linguistics, 1030-1036, Taipei.  
Dan Tufi? and Radu Ion. 2003. Word sense 
clustering based on translation equivalence in 
parallel texts; a case study in Romanian. In 
Proceedings of the International Conference on 
Speech and Dialog ? SPED, 13-26, Bucharest.  
Dan Tufi?,  Ana-Maria Barbu and Radu Ion 
2003. A word-alignment system with limited 
language resources. In Proceedings of the 
NAACL 2003 Workshop on Building and Using 
Parallel Texts; Romanian-English Shared 
Task, 36-39, Edmonton.  
Dan Tufi?, Radu Ion and Nancy Ide 2004. Word 
sense disambiguation as a wordnets validation 
method in Balkanet. In Proceedings of the 
LREC?2004, 741-744, Lisbon 
Dan Tufi?, Dan Cristea and Sofia Stamou 2004a. 
BalkaNet: Aims, Methods, Results and 
Perspectives. A General Overview. In D. Tufi? 
(ed): Special Issue on BalkaNet. Romanian 
Journal on Science and Technology of 
Information, 7(3-4):9-44 
A Hierarchical Account of Referential Accessibility
Nancy IDE
Department of Computer Science
Vassar College
Poughkeepsie, New York 12604-0520 USA
ide@cs.vassar.edu
Dan CRISTEA
Department of Computer Science
University ?Al. I. Cuza?
Iasi, Romania
dcristea@infoiasi.ro
Abstract
In this paper, we outline a theory of
referential accessibility called Veins
Theory (VT). We show how VT addresses
the problem of "left satellites", currently a
problem for stack-based models, and show
that VT can be used to significantly reduce
the search space for antecedents. We also
show that VT provides a better model for
determining domains of referential
accessibility, and discuss how VT can be
used to address various issues of structural
ambiguity.
Introduction
In this paper, we outline a theory of referential
accessibility called Veins Theory (VT). We
compare VT to stack-based models based on
Grosz and Sidner's (1986) focus spaces, and
show how VT addresses the problem of "left
satellites", i.e., subordinate discourse segments
that appear prior to their nuclei (dominating
segments) in the linear text. Left-satellites pose
a problem for stack-based models, which
remove subordinate segments from the stack
before pushing a nuclear or dominating
segment, thus rendering them inaccessible. The
percentage of such cases is typically small,
which may account for the fact that their
treatment has been largely overlooked in the
literature, but the phenomenon nonetheless
persists in most texts. We also show how VT
can be used to address various issues of
structural ambiguity.
1 Veins Theory
Veins Theory (VT) extends and formalizes the
relation between discourse structure and
reference proposed by Fox (1987). VT
identifies ?veins? over discourse structure trees
that are built according to the requirements put
forth in Rhetorical Structure Theory (RST)
(Mann and Thompson, 1987). RST structures
are represented as binary trees, with no loss of
information. Veins are computed based on the
RST-specific distinction between nuclei and
satellites; therefore, RST relations labeling
nodes in the tree are ignored. Terminal nodes
in the tree represent discourse units and non-
terminal nodes represent discourse relations.
The fundamental intuition underlying VT is
that the distinction between nuclei and
satellites constrains the range of referents to
which anaphors can be resolved; in other
words, the nucleus-satellite distinction induces
a domain of referential accessibility (DRA) for
each referential expression. More precisely, for
each anaphor a in a discourse unit u , VT
hypothesizes that a can be resolved by
examining referential expressions that were
used in a subset of the discourse units that
precede u; this subset is called the DRA of u.
For any elementary unit u in a text, the
corresponding DRA is computed automatically
from the text's RST tree  in two steps:
1. Heads for each node are computed bottom-
up over the rhetorical representation tree.
Heads of elementary discourse units are
the units themselves. Heads of internal
nodes, i.e., discourse spans, are computed
by taking the union of the heads of the
immediate child nodes that are nuclei. For
example, for the text in Figure 1,1 with the
rhetorical structure shown in Figure 2,2 the
head of span [5,7] is unit 5. Note that the
head of span [6,7] is the list <6,7> because
both immediate children are nuclei.
2 .  Using the results of step 1, Vein
expressions are computed top-down for
each node in the tree, using the following
functions:
-  mark (x), which returns each symbol in a
string of symbols x marked with
parentheses.
-  seq(x,y), which concatenates the labels in
x with the labels in y, left-to-right.
-  simpl(x), which eliminates all marked
symbols from x, if they exist.
The vein of the root is its head. Veins of
child nodes are computed recursively, as
follows:
?  for each nuclear node whose parent
has vein v, if the node has a left non-
nuclear sibling with head h, then the
vein expression is seq(mark(h), v);
otherwise v.
? for each non-nuclear node with head h
whose parent node has vein v, if the
node is the left child of its parent, then
seq(h,v); otherwise, seq(h, simpl(v)).
                                                       
1
 Figure 1 highlights two co-referential equivalence
classes: referential expressions surrounded by
boxes refer to ?Mr. Casey?; those surrounded by
ellipses refer to ?Genetic Therapy Inc.?.
2
 The rhetorical structure is represented using the
conventions proposed by Mann and Thompson
(1988).
One of the conjectures of VT is that the vein
expression of a unit (terminal node), which
includes a chain of discourse units that contain
that unit itself, provides an ?abstract? or
summary of the discourse fragment that
contains that unit. Because it is an internally
coherent piece of discourse, all referential
expressions (REs) in the unit preferentially
find their referees within that sub-text.
Referees that do not appear in the DRA are
possible, but are more difficult to process, both
computationally and cognitively (see Section
2.2). This conjecture expresses the intuition
that potential referees of the REs of a unit
depend on the nuclearity of previous units:
both a satellite and a nucleus can access a
previous nuclear node, a nucleus can only
access another left nuclear node or its own left
satellite, and the interposition of a nucleus
after a satellite blocks the accessibility of the
satellite for any nodes lower in the hierarchy.
1. Michael D. Casey, a top Johnson & Johnson
manager, moved to Genetic Therapy Inc., a
small biotechnology concern here,
2. to become its president and chief operating
officer
3. Mr. Casey, 46, years old, was president of
J&J?s McNeil Pharmaceutical subsidiary,
4. which was merged with another J&J unit,
Ortho Pharmaceutical Corp., this year in a
cost-cutting move.
5. Mr. Casey succeeds M. James Barrett, 50, as
president of Genetic Therapy.
6. Mr. Barrett remains chief executive officer
7. and becomes chairman.
8. Mr. Casey said
9. he made the move to the smaller company
10. because he saw health care moving toward
technologies like the company?s gene
therapy products.
11. I believe that the field is emerging and is
prepared to break loose,
12. he said.
Figure 1: MUC corpus text fragment
The DRA of a unit u is given by the units in
the vein that precede u. For example, for the
text and RST tree in Figures 1 and 2, the vein
expression of unit 3, which contains units 1
and 3, suggests that anaphors from unit 3
should be resolved only to referential
expressions in units 1 and 3. Because unit 2 is
a satellite to unit 1, it is considered to be
?blocked? to referential links from unit 3. In
contrast, the DRA of unit 9, consisting of units
1, 8, and 9, reflects the intuition that anaphors
from unit 9 can be resolved only to referential
expressions from unit 1, which is the most
important unit in span [1,7] and to unit 8, a
satellite that immediately precedes unit 9.
Figure 2 shows the heads and veins of all
internal nodes in the rhetorical representation.
In general, co-referential relations (such as the
identity relation) induce equivalence classes
over the set of referential expressions in a text.
When hierarchical adjacency is considered, an
anaphor may be resolved to a referent that is
not the closest in a linear interpretation of a
text. However, because referential expressions
are organized in equivalence classes, it is
sufficient that an anaphor is resolved to some
member of the set. This is consistent with the
distinction between "direct" and "indirect"
references discussed in (Cristea, et al, 1998).
1 2 3 4
5
6 7 8
9
10
11 12
13-?
??-?
H = 1 9 *
V = 1 9 *
H = 1
V = 1 9 *
H = 9
V = 1 9 *
H = 1
V = 1 9 *
H = 5
V = 1 5 9 *
H = 1
V = 1 9 *
H = 3
V = 1 3 5 9 *
H = 6 7
V = 1 5 6 7 9 *
H = 9
V = 1 9 * 
H = 9
V = 1 9 *
H = 9
V = 1 (8) 9 *
H = 10
V = 1 9 10 *
H = 11
V = 1 9 10 11 *H = 3
V = 1 3 5 9
DRA  = 1 3 H = 9
V = 1 (8) 9
DRA  = 1 8 9 
Figure 2: RST analysis of the text in Figure 1
2 VT and Stack-based Models
Veins Theory claims that references from a
given unit are possible only in its DRA, i.e., that
discourse structure constrains the areas of the
text over which references can be resolved. In
previous work, we compared the potential of
hierarchical and linear models of discourse--i.e.,
approaches that enumerate potential antecedents
in an undifferentiated window of text linearly
preceding the anaphor under scrutiny--to
correctly establish co-referential links in texts,
and hence, their  potential to correctly resolve
anaphors (Cristea, et al, 2000). Our results
showed that by exploiting the hierarchical
discourse structure of texts, one can increase the
potential of natural language systems to correctly
determine co-referential links, which is a
requirement for correctly resolving anaphors. In
general, the potential to correctly determine co-
referential links was greater for VT than for
linear models when one looks back 4 elementary
discourse units. When looking back more than
four units, the linear model was equally
effective.
Here, we compare VT to stack-based models of
discourse structure based on Grosz and Sidner's
(1986) (G&S) focus spaces (e.g., Hahn and
Str?be, 1997; Azzam, et al, 1998). In these
approaches, discourse segments are pushed on
the stack as they are encountered in a linear
traversal of the text. Before a dominating
segment is pushed, subordinate segments that
precede it are popped from the stack.
Antecedents for REs appearing in the segment
on the top of the stack are sought in discourse
segments in the stack below it. Therefore, in
cases where a subordinate segment a precedes a
dominating segment b, a reference to an entity in
a by an RE in b is not resolvable. Special
provision could be made in order to handle such
cases?e.g., subsequently pushing a on top of
b?but this would violate the overall strategy of
resolving REs appearing in segments currently
on the top of the stack.
The special status given to left satellites in VT
addresses this problem. For example, one RST
analysis of (1) proposed by Moser and Moore
(1996) is given in Figure 3. Moser and Moore
note that the relation of an RST nucleus to its
satellite is analogous to the dominates relation
proposed by G&S (see also Marcu, 2000). As a
subordinate segment preceding the segment that
dominates it, the satellite is popped from the
stack before the dominant segment (the nucleus)
is pushed in the stack-based model, and therefore
it is not included among the discourse segments
that are searched to resolve co-references.3
Similarly, the text in (2), taken from the MUC
annotated corpus (Marcu, et al, 1999), was
assigned the RST structure in Figure 4, which
presents the same problem for the stack-based
approach: the referent for this  in C2 is to the
Clinton program in A2, but because it is a
subordinate segment, it is no longer on the stack
when C2 is processed.
(1) A1. George Bush supports big business.
B1. He's sure to veto House Bill 1711.
Figure 3: RST analysis of (1)
                                                       
3
 Note that Moser and Moore (1996) also propose an
informational RST structure for the same text, in
which a ??volitional-cause?? relation holds between
the nucleus a  and the satellite b, thus providing for a
to be on the stack when b is processed.
 (2) A2. Some of the executives also signed letters on
behalf of the Clinton program.
B2. Nearly all of them praised the president for
his efforts to pare the deficit.
C2. This is not necessarily the package I would
design,
D2. said Martin Marietta's Mr. Augustine.
E2. But we have to attack the deficit.
Figure 4: RST analysis of (2)
2.1 Validation
To validate our claim, we examined 23
newspaper texts with widely varying lengths
(mean length = 408 words, standard deviation
376). The texts were annotated manually for co-
reference relations of identity (Hirschman and
Chinchor, 1997). The co-reference relations
define equivalence relations on the set of all
marked references in a text. The texts were also
annotated manually with discourse structures
built in the style of Mann and Thompson (1988).
Each analysis yielded an average of 52
elementary discourse units. Details of the
annotation process are given in (Marcu et al,
1999).
Six percent of all co-references in the corpus are
to left satellites. If only co-references pointing
outside the unit in which they appear (inter-unit
references) are considered, the rate increases to
7.76%. Among these cases, two possibilities
exist: either the reference is unresolvable using
the stack-based method because the unit in
which the referent appears has been popped from
the stack, or the stack-based algorithm finds a
correct referent in an earlier unit that is still on
the stack. Twenty-two percent (2.38% of all co-
referring expressions in the corpus) of the
referents that VT finds in left satellites fall into
B1A1
evidence
A2-B2
background
elaboration-addition
A2 B2
C2-D2-E2
antithesis
C2-D2
attribution
C2 D2
E2
the first category. For example, in text fragment
(3), taken from the MUC corpus, the co-
referential equivalence class for the pronoun he
in C3 includes Saloman Brothers analyst Jeff
Canin in B3 and he in A3. The RST analysis of
this fragment in Figure 5 shows that both A3 and
B3 are left satellites. A stack-based approach
would not find either antecedent for he in C3,
since both A3 and B3 are popped from the stack
before C3 is processed.
(3) A3. Although the results were a little lighter than
the 49 cents a share he hoped for,
B3. Salomon Brothers analyst Jeff Canin said
C3. he was pleased with Sun's gross margins for
the quarter.
Figure 5: RST analysis of (3)
In cases where stack-based approaches find a co-
referent (although not the most recent
antecedent) elsewhere in the stack, it makes
sense to compare the effort required by the two
models to establish correct co-referential links.
That is, we assume that from a computational
perspective (and, presumably a psycholinguistic
one as well), the closer an antecedent is to the
referential expression to be resolved, the better.
We have shown elsewhere (Cristea et al, 2000)
that VT, compared to linear models, requires
significantly less effort for DRAs of any size.
We use a similar strategy here to compute the
effort required by VT and stack-based models.
DRAs for both models are treated as ordered
lists. For example, text fragment (4) reflects the
set of units on the stack at a given point in
processing one of the MUC texts; units D4 and
E4, in brackets, are left satellites and therefore
not available using the stack-based model, but
visible using VT. To determine the correct
antecedent of Mr. Clinton in F4 using the stack-
based model, it is necessary to search back
through 3 units (C4, B4, A4) to find the referent
President Clinton. In contrast, using VT, we
search back only 1 unit to D4.
 (4) A4. A group of top corporate executives urged
Congress to pass President Clinton's deficit-
reduction plan,
B4. declaring that it is superior to the only
apparent alternative: more gridlock.
C4. Some of the executives who attended
yesterday's session weren't a surprise.
  [ D4. Tenneco Inc. Chairman Michael Walsh, for
instance, is a staunch Democrat who
provided an early endorsement for Mr.
Clinton during the presidential campaign.
E4. Xerox Corp.'s Chairman Paul Allaire was
one of the few top corporate chief executive
officers who contributed money to the
Clinton campaign.   ]
F4. And others, such as Atlantic Richfield Co.
Chairman Lodwrick M. Cook and Zenith
Electronics Corp. Chairman Jerry Pearlman,
have also previously voiced their approval of
Mr. Clinton's economic strategy.
We compute the effort e(M,a,DRAk) of a model
M to determine correct co-referential links with
respect to a referential expression a in unit u,
given a DRA of size k (DRAk(u)) is given by the
number of units between u and the first unit in
DRAk that contains a co-referential expression of
a. The effort e(M,C,k) of a model M to determine
correct co-referential links for all referential
expressions in a corpus of texts C using DRAs of
size k is computed as the sum of the efforts
e(M,a,DRAk) of all referential expressions a
where VT finds the co-reference of a in a left
satellite. Since co-referents found in units that
are not left satellites will be identical for both
VT and stack-based models, the difference in
effort between the two models depends only on
co-referents found in left satellites.
Figure 6 shows the VT and stack-based efforts
computed over referential expressions resolved
by VT in left satellites and k = 1 to 12.
Obviously, for a given k and a given referent a,
that no co-reference exists in the units of the
corresponding DRAk  In these cases, we consider
B3-C3
attribution
concession
A3
B3 C3
the effort to be equal to k. As a result, for small k
the effort required to establish co-referential
links is similar for both models, because both
can establish only a limited number of links.
However, as k increases, the effort computed
over the entire corpus diverges, with VT
performing consistently better than the stack-
based model.
Figure 6: Effort required by VT and stack-based
models
Note that in some cases, the stack-based model
performs better than VT, in particular for small
k. This occurs when VT searches back through n
adjacent left satellites, where n > 1, to find a co-
reference, but a co-referent is found using the
stack-based method by searching back m non-
left satellite units, where m < n. This would be
the case, if for instance, VT first found a co-
referent for Mr. Clinton In text (4) in D4 (2 units
away), but the stack-based model found a co-
referent in C4 (1 unit away since the left
satellites are not on the stack).
In our corpus, 15% of the co-references found in
left satellites by VT required less effort using the
stack-based method, whereas VT out-performed
the stack-based method 23% of the time. In the
majority of cases (62%), the two models
required the same level of effort. However, all of
the cases in which the stack-based model
performed better are for small k (k<4), and the
average difference in distance (in units) is 1.25.
In contrast, VT out-performs the stack-based
model for cases ranging over all values of k in
our experiment (1 to 12), and the average
difference in distance is 3.8 units. At k=4, VT
can determine all the co-referents in our corpus,
whereas the stack-based model requires DRAs of
up to 12 units to resolve them all. This accounts
for the marked divergence in effort shown in
Figure 6 as k  increases. So, despite the minor
difference in the percentage of cases where VT
out-performs the stack-based model, VT has the
potential to significantly reduce the search space
for co-referential links.
2.2 Exceptions
We have also examined the exceptions, i.e., co-
referential links that VT and stack-based models
cannot determine correctly. Because of the
equivalence of the stack contents for left-
balanced discourse trees, there is no case in
which the stack-based model finds a referent
where VT does not. There is, however, a number
of referring expressions for which neither VT
nor the stack-based model finds a co-referent. In
the corpus of MUC texts we consider, 12.3% of
inter-unit references fall into this category, or
9.3% of the references in the corpus if we
include intra-unit references.
Table 1 provides a summary of the types of
referring expressions for which co-referents are
not found in our corpus?i.e., no antecedent
exists, or the antecedent appears outside the
DRA.4 We show the percentage of REs in our
corpus for which VT (and the stack-based model
as well, since all units in the DRA computed
according to VT are in the DRA computed using
the stack-based model) fails to find an
antecedent, and the percentage of REs for which
VT finds a co-referent (in a left satellite) but the
stack-based model does not.
                                                       
4
 Our calculations are made based on the RST
analysis of the MUC data, in which we detected a
small number of structural errors. Therefore, the
values given here are not absolute but rather provide
an indication of the relative distribution of RE types.
0
2 0
4 0
6 0
8 0
100
120
1 2 3 4 5 6 7 8 9 1 0 1 1 1 2
DRA length (k)
Nu
m
be
r 
of
 c
o-
re
fs
Stack
VT
We consider four types of REs:
(1) Pragmatic references, which refer to entities
that can be assumed part of general
knowledge, such as the Senate, the key in the
phrase lock them up and throw away the key,
or our in the phrase our streets.
(2) Proper nouns, such as Mr. Gerstner or
Senator Biden.
(3) Common nouns, such as the steelmaker, the
proceeds, or the top job.
(4) Pronouns
Following (Gundel, et al, 1993), we consider
that the evoking power of each of these types of
REs decreases as we move down the list. That is,
pragmatic references are easily understood
without an antecedent; proper nouns and noun
phrases less so, and are typically resolved by
inference over the context. On the other hand,
pronouns have very poor evoking power, and
therefore a message emitter employs them only
when s/he is certain that the structure of the
discourse allows for easy recuperation of the
antecedent in the message receiver's memory.5
Except for the cases where a pronoun can be
understood without an antecedent (e.g., our in
our streets), it is virtually impossible to use a
pronoun to refer to an antecedent that is outside
the DRA.
Type of RE VT Stack-based
pragmatic 56.3% 0.0%
proper nouns 22.7% 26.1%
common nouns 16.0% 39.1%
pronouns 5.0% 34.8%
Table 1: Exceptions for VT and stack-based models
The alignment of the evoking power of
referential expressions with the percentage of
exceptions for both models shows that the
predictions made by VT relative to DRAs are
fundamentally correct--that is, their prevalence
corresponds directly to their respective evoking
                                                       
5
 Ideally, a psycho-linguistic study of reading times to
verify the claim that referees outside the DRA are
more difficult to process would be in order.
powers. On the other hand, the almost equal
distribution of exceptions over RE types for the
stack-based model shows that it is less reliable
for determining DRAs.
Note that in all VT exceptions for pronouns, the
RST attribution relation is involved. Text
fragment (5) and the corresponding RST tree
(Figure 7) shows the typical case:
(5) A5. A spokesman for the company said,
B5. Mr. Bartlett?s promotion reflects the current
emphasis at Mary Kay on international
expansion.
C5. Mr. Bartlett will be involved in developing
the international expansion strategy,
D5. he said
The antecedent for he in D5 is a spokesman for
the company in A5, which, due to the nuclear-
satellite relations, is inaccessible on the vein.
Our results suggest that annotation of attributive
relations needs to be refined, possibly by treating
X said and the attributed quotation as a single
unit. If this were done, the vein expression
would allow appropriate access.
Figure 7: RST analysis of (5)
2.3 Summary
In sum, VT provides a more natural account of
referential accessibility than the stack-based
model. In cases where the discourse structure is
not left-polarized, at least one satellite precedes
its nucleus in the discourse and is therefore its
left sibling in the binary discourse tree. The vein
definition formalizes the intuition that in a
sequence of units a b c, where a and c  are
satellites of b, b can refer to entities in a (its left
satellite), but the subsequent right satellite, c,
cannot refer to a due to the interposition of
nuclear unit b--or, if such a reference exists, it is
A5-B5
elaboration
attribution
A5 B5
C5-D5
attribution
D5C5
harder to process. In stack-based approaches to
referentiality, such configurations pose
problems: because b dominates a, in order to
resolve potential references from b to a, b must
appear below a on the stack even though it is
processed after a. Even if the processing
difficulties are overcome, this situation leads to
the postulation of cataphoric references when a
satellite precedes its nucleus, which is counter-
intuitive.
3 VT and Structural Ambiguity
The fact that VT considers only the nuclear-
satellite distinction and ignores rhetorical
labeling has practical ramifications for anaphora
resolution systems that rely on discourse
structure to determine the DRA for a given RE.
(Marcu, et al, 1999) show that over a corpus of
texts drawn from MUC newspaper texts, the
Wall Street Journal corpus, and the Brown
Corpus, reliable agreement among annotators is
consistently obtained for discourse segmentation
and assignment of nuclear-satellite status, while
agreement on rhetorical labeling was less
reliable (statistically significant for only the
MUC texts). This means that even when there
exist differences in rhetorical labeling, vein
expressions can be computed and used to
determine DRAs.
VT also has ramifications for evaluating the
viability of different structural representations
for a given text, at least for the purposes of
reference resolution. Like syntactic parsing,
discourse parsing typically yields several
interpretations, and one of the a priori tasks for
further analysis of the parsed texts is to choose
one from among potentially several alternative
structures. Marcu (1996) showed that using only
rhetorical relations, as many as five different
structures can be identified for some texts.
Considering intention-based relations can yield
even more alternatives. For anaphora resolution,
the choice of one structure over another may
have significant impact. For example, an RST
tree for (6) using rhetorical relations is given in
Figure 8; Figure 9 shows another RST tree for
the same text, using intention-based relations. If
we compute the vein expressions for both
representations, we see that the vein for segment
C6 in the intentional representation is <A6 B6
C6>, whereas in the rhetorical representation, the
vein is <(B6), C6>. That is, under the constraints
imposed by VT, John  is not available as a
referent for he in C6 in the rhetorical version,
although J o h n  is clearly the appropriate
antecedent. Interestingly, the intention-based
analysis is skewed to the right and thus is a
"better" representation according to the criteria
outlined in (Marcu, 1996); it also eliminates the
left-satellite that was shown to pose problems for
stack-based approaches. It is therefore likely that
the intention-based analysis is "better" for the
purposes of anaphora resolution.
(6) A6.  Tell John to bring the car home by 5.
B6. That way I can get to the store before it
closes.
C6. Then he can finish the bookshelves tonight.
Figure 8: RST tree for text (6), using rhetorical
relations
Figure 9: RST tree for text (6), using intention-based
relations
Conclusion
Veins Theory is based on established notions of
discourse structure: hierarchical organization, as
in the stack-based model and RST's tree
structures, and dominance or nuclear/satellite
motivation
B6-C6
motivation
B6 C6
A6
A6-B6
condition
condition
A6 B6
C6
relations between discourse segments. As such,
VT captures and formalizes intuitions about
discourse structure that run through the current
literature. VT also explicitly recognizes the
special status of the left satellite for discourse
structure, which has not been adequately
addressed in previous work.
In this paper we have shown how VT addresses
the left satellite problem, and how VT can be
used to address various issues of structural
ambiguity. VT predicts that references not
resolved in the DRA of the unit in which it
appears are more difficult to process, both
computationally and cognitively; by looking at
cases where VT fails we determine that this
claim is justified. By comparing the types of
referring expressions for which VT and the
stack-based model fail, we also show that VT
provides a better model for determining DRAs.
Acknowledgements
We thank Daniel Marcu for providing us with
the RST annotated MUC corpus, and Valentin
Tablan for developing part of the software that
enabled us to process the data.
References
Azzam S., Humphreys K. and Gaizauskas R.
(1998). Evaluating a Focus-Based Approach to
Anaphora Resolution. Proceedings of
COLING-ACL?98, 74-78.
Cristea D., Ide N., and Romary L. (1998). Veins
Theory: A Model of Global Discourse
Cohesion and Coherence. Proceedings of
COLING-ACL?98, 281-285.
Cristea D., Ide N., Marc, D., and Tablan V.
(2000).  An Empirical Investigation of the
Relation Between Discourse Structure and Co-
Reference. Proceedings of COLING 2000,
208-214.
Fox B. (1987). Discourse Structure and
Anaphora. Written and Conversational
English. No 48 in Cambridge Studies in
Linguistics, Cambridge University Press.
Grosz B. and Sidner C. (1986). Attention,
Intention and the Structure of Discourse.
Computational Linguistics, 12, 175-204.
Gundel J., Hedberg N. and Zacharski R.  (1993).
Cognitive Status and the Form of Referring
Expressions. Language, 69:274-307.
Hahn U. and Str?be M. (1997). Centering in-the-
large: Computing referential discourse
segments. Proceedings of ACL-EACL?97, 104-
111.
Hirschman L. and Chinchor N. (1997). MUC-7
Co-reference Task Definition.
Mann, W.C. and Thompson S.A. (1988).
Rhetorical structure theory: A theory of text
organization, Text, 8:3, 243-281.
Marcu D., Amorrortu E. and Romera M. (1999).
Experiments in Constructing a Corpus of
Discourse Trees. Proceedings of the ACL?99
Workshop on Standards and Tools for
Discourse Tagging.
Marcu D. (2000). Extending a Formal and
Computational Model of Rhetorical Structure
Theory with Intentional Structures ? la Grosz
and Sidner. Proceedings of COLING 2000,
523-29.
Marcu D. (1999). A Formal and Computational
Synthesis of Grosz and Sidner's and Mann and
Thompson's theories. Workshop on Levels of
Representation in Discourse, 101-108.
Marcu D. (1996). Building Up Rhetorical
Structure Trees. Proceedings of the Thirteenth
National Conference on Artificial Intelligence,
vol. 2, 1069-1074.
Moser M. and Moore J. (1996). Towards a
Synthesis of Two Accounts of Discourse
Structure. Computational Linguistics, 18(4):
537-544.
Sidner C. (1981). Focusing and the Interpretation
of Pronouns. Computational Linguistics,
7:217-231.
A Common Framework for Syntactic Annotation
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY 12604-0520 USA
ide@cs.vassar.edu
Laurent Romary
LORIA/CNRS
Campus Scientifique, B.P. 239
54506 Vandoeuvre-l?s-Nancy, FRANCE
romary@loria.fr
Abstract
It is widely recognized that the
proliferation of annotation schemes
runs counter to the need to re-use
language resources, and that standards
for linguistic annotation are becoming
increasingly mandatory. To answer this
need, we have developed a
representation framework comprised of
an abstract model for a variety of
different annotation types (e.g.,
morpho-syntactic tagging, syntactic
annotation, co-reference annotation,
etc.), which can be instantiated in
different ways depending on the
annotator?s approach and goals. In this
paper we provide an overview of our
representation framework and
demonstrate its applicability to
syntactic annotation. We show how the
framework can contribute to
comparative evaluation and merging of
parser output and diverse syntactic
annotation schemes.
1 Introduction
It is widely recognized that the proliferation of
annotation schemes runs counter to the need to
re-use language resources, and that standards for
linguistic annotation are becoming increasingly
mandatory. In particular, there is a need for a
general framework for linguistic annotation that
is flexible and extensible enough to
accommodate different annotation types and
different theoretical and practical approaches,
while at the same time enabling their
representation in a ?pivot? format that can serve
as the basis for comparative evaluation of parser
output, such as PARSEVAL  (Harrison, et al,
1991), as well as the development of reusable
editing and processing tools.
To answer this need, we have developed a
representation framework comprised of an
abstract model for a variety of different
annotation types (e.g., morpho-syntactic
tagging, syntactic annotation, co-reference
annotation, etc.), which can be instantiated in
different ways depending on the annotator?s
approach and goals. We have implemented both
the abstract model and various instantiations
using XML schemas (Thompson, et al, 2000),
the Resource Definition Framework (RDF)
(Lassila and Swick, 2000) and RDF schemas
(Brickley and Guha, 2000), which enable
description and definition of abstract data
models together with means to interpret, via the
model, information encoded according to
different conventions. The results have been
incorporated into XCES (Ide, et al, 2000a), part
of the EAGLES Guidelines developed by the
Expert Advisory Group on Language
Engineering Standards (EAGLES)1. The XCES
provides a ready-made, standard encoding
format together with a data architecture
designed specifically for linguistically annotated
corpora.
In this paper we provide an overview of our
representation framework and demonstrate its
applicability to syntactic annotation. The
framework has been applied to the
representation of terminology (Terminological
Markup Framework2, ISO project n.16642) and
computational lexicons (Ide, et al, 2000b), thus
demonstrating its general applicability for a
variety of linguistic annotation types. We also
show how the  framework can contribute to
                                                           
1
 http://www.ilc.pi.cnr.it/EAGLES/home.html
2
 http://www.loria.fr/projects/TMF
comparison and merging of diverse syntactic
annotation schemes.
2 Current Practice
At the highest level of abstraction, syntactic
annotation schemes represent the following
kinds of information:
?  Category  in format ion : labeling of
components based on syntactic category
(e.g., noun phrase, prepositional phrase),
syntactic role (subject, object), etc.;
? Dependency information: relations among
components, including constituency
relations, grammatical role relations, etc.
For example, the annotation in Figure 1, drawn
from the Penn Treebank II3 (hereafter, PTB),
uses LISP-like list structures to specify
constituency relations and provide syntactic
category labels for constituents. Some
grammatical roles (subject, object, etc.) are
implicit in the structure of the encoding: for
instance, the nesting of the NP ?the front room?
implies that the NP is the object of the
prepositional phrase, whereas the position of the
NP ?him? following and at the same level as the
VP node implies that this NP is the grammatical
object. Additional processing (or human
intervention) is required to render these relations
explicit. Note that the PTB encoding provides
some explicit information about grammatical
role, in that ?subject? is explicitly labeled
(although its relation to the verb remains
implicit in the structure), but most relations
(e.g., ?object?) are left implicit. Relations
among non-contiguous elements demand a
special numbering mechanism to enable cross-
reference, as in the specification of the NP-SBJ
of the embedded sentence by reference to the
earlier NP-SBJ-1 node.
Although they differ in the labels and in
some cases the function of various nodes in the
tree, most annotation schemes provide a similar
constituency-based representation of relations
among syntactic components (see Abeille,
forthcoming, for a comprehensive survey of
syntactic annotation schemes). In contrast,
dependency  schemes (e.g., Sleator and
Temperley, 1993; Tapanainen and Jarvinen,
1997; Carroll, et al, forthcoming) do not
                                                           
3
 http://www.cis.upenn.edu/treebank
provide a constituency analysis4 but rather
specify grammatical relations among elements
explicitly; for example, the sentence ?Paul
intends to leave IBM? could be represented as
shown in Figure 2, where the predicate is the
relation type, the first argument is the head, the
second the dependent, and additional arguments
may provide category-specific information (e.g.,
introducer for prepositional phrases, etc.).
((S (NP-SBJ-1 Jones)
(VP followed)
(NP him)
(PP-DIR into
  (NP the front room))
,
(S-ADV (NP-SBJ *-1)
  (VP closing
  (NP the door)
(PP behind
(NP him)))))
.))
Figure 1. PTB annotation of ?Jones followed him
into the front room, closing the door behind
him.?
subj(intend,Paul,_)
xcomp(intend,leave,to)
subj(leave,Paul)
dobj(leave,IBM,_)
Figure 2. Dependency annotation according to
Carroll, Minnen, and Briscoe (forthcoming)
3 A Model for Syntactic Annotation
The goal in the XCES is to provide a framework
for annotation that is theory and tagset
independent. We accomplish this by treating the
description of any specific syntactic annotation
scheme as a process involving several
knowledge sources that interact at various
levels. The process allows one to specify, on the
one hand, the informational properties of the
scheme (i.e., its capacity to represent a given
piece of information), and, on the other, the way
the scheme can be instantiated (e.g., as an XML
document). Figure 3 shows the overall
architecture of the XCES framework for
syntactic annotation.
                                                           
4
  So-called ?hybrid systems?  (e.g., Basili, et al, 199;
Grefenstette, 1999) combine constituency analysis and
functional dependencies, usually producing a shallow
constituent parse that brackets major phrase types and
identifying the dependencies between heads of
constituents.
Figure 3. Overall architecture of the XCES annotation framework
Two knowledge sources are used define the
abstract model:
Data Category Registry: Within the framework
of the XCES we are establishing an inventory of
data categories for syntactic annotation, initially
based on the EAGLES Recommendations for
Syntactic Annotation of Corpora (Leech et al,
1996). Data categories are defined using RDF
descriptions that formalize the properties
associated with each. The categories are
organized in a hierarchy, from general to
specific. For example, a general dependent
relation may be defined, which may have one of
the possible values argument or modifier;
argument in turn may have the possible values
subject, object, or complement; etc.5 Note that
RDF descriptions function much like class
definitions in an object-oriented programming
language: they provide, effectively, templates
that describe how objects may be instantiated,
but do not constitute the objects themselves.
Thus, in a document containing an actual
annotation, several objects with the type
argument  may be instantiated, each with a
different value. The RDF schema ensures that
each instantiation of argument is recognized as a
sub-class of dependent and inherits the
appropriate properties.
Structural Skeleton: a domain-dependent
abstract structural framework for syntactic
                                                           
5
 Cf. the hierarchy in Figure 1.1, Caroll, Minnen, and
Briscoe (forthcoming).
General Markup Language
XSLT Script
Dialect
Specification
DATA
CATEGORY
REGISTRY
Virtual
AML
Concrete
AML
Data
Category
Specification
STRUCTURAL
SKELETON
Abstract
XML
encoding
Concrete
XML
encoding
Non-XML
Encoding
Universal Resources
Project Specific Resources
annotations, capable of fully capturing all the
information in a specific annotation scheme. The
structural skeleton for syntactic annotations is
described below in section 12.1.
Two other knowledge sources are used to
define a project-specific format for the
annotation scheme, in terms of its expressive
power and its instantiation in XML:
Data Category Specification (DCS): describes
the set of data categories that can be used within
a given annotation scheme, again using RDF
schema. The DCS defines constraints on each
category, including restrictions on the values
they can take (e.g., "text with markup"; a
"picklist" for grammatical gender, or any of the
data types defined for XML), restrictions on
where a particular data category can appear
(level in the structural hierarchy). The DCS may
include a subset of categories from the DCR
together with application-specific categories
additionally defined in the DCS.  The DCS also
indicates a level of granularity based on the
DCR hierarchy.
Dialect specification: defines, using XML
schemas, XSLT scripts, and XSL style sheets,
the project-specific XML format for syntactic
annotations. The specifications may include:
?  Data category instantiation styles:  Data
categories may be realized in a project-
specific scheme in any of a variety of
formats. For example, if there exists a data
category NounPhrase, this may be realized
as an <NounPhrase> element (possibly
containing additional elements), a typed
element (e.g. <cat type=NounPhrase>), tag
content (e.g., <cat>NounPhrase</cat>), etc.
?  Data category vocabulary styles: Project-
specific formats can utilize names different
from those in the Data Category Registry;
for instance, a DCR specification for
NounPhrase can be expressed as ?NP? or
?SN? (? syntagme nominal?) in the project-
specific format, if desired.
?  Expansion structures: A project-specific
format may alter the structure of the
annotation as expressed using the structural
skeleton. For example, it may be desirable
for processing or other reasons to create two
sub-nodes under a given <struct> node, one
to group features and one to group relations.
The combination of the structural skeleton
and the DCS defines a virtual annotation
markup language (AML). Any information
structure that corresponds to a virtual AML has
a canonical expression as an XML document;
therefore, the inter-operability of different
AMLs is dependent only on their compatibility
at the virtual level. As such, virtual AML is the
hub of the annotation framework: it defines a
lingua franca for syntactic annotations that can
be used to compare and merge annotations, as
well as enable design of generic tools for
visualization, editing, extraction, etc.
The combination of a virtual AML with the
Dialect Specification provides the information
necessary to automatically generate a concrete
AML representation of the annotation scheme,
which conforms to the project-specific format
provided in the Dialect Specification. XSLT
filters translate between the representations of
the annotation in concrete and virtual AML, as
well as between non-XML formats (such as the
LISP-like PTB notation) and concrete AML.6
2.1 The Structural Skeleton
For syntactic annotation, we can identify a
general, underlying model that informs current
practice: specification of constituency relations
(with some set of application-specific names and
properties) among syntactic or grammatical
components (also with a set of application-
specific names and properties), whether this is
modeled with a tree structure or the relations are
given explicitly.
Because of the common use of trees in
syntactic annotation, together with the natural
tree-structure of markup in XML documents, we
provide a structural skeleton for syntactic
markup following this model. The most
important element in the skeleton is the
<struct> element, which represents a node
(level) in the syntax tree. <struct> elements may
be recursively nested at any level to reflect the
structure of the corresponding tree. The <struct>
element has the following attributes:
                                                           
6
  Strictly speaking, an application-specific format could be
translated directly into the virtual AML, eliminating the
need for the intermediary concrete AML format. However,
especially for existing formats, it is typically more
straightforward to perform the two-step process.
?  type : specifies the node label (e.g., ?S?,
?NP?, etc.) or points to an object in another
document that provides the value. This
allows specifying complex data items as
annotations. It also enables generating a
single instantiation of an annotation value in
a separate document that can be referenced as
needed.
?  xlink : points to the data to which the
annotation applies. In the XCES, we
recommend the use of s t a n d - o f f
a n n o t a t i o n ? i .e., annotation that is
maintained in a document separate from the
primary (annotated) data.7 The xlink attribute
uses the XML Path Language (XPath) (Clark
& DeRose, 1999) to specify the location of
the relevant data in the primary document.
? ref : refers to a node defined elsewhere, used
instead of xlink.
? rel?: specifies a type of relation (e.g., ?subj?)
?  head : specifies the node corresponding to
the head of the relation
? dependent : specifies the node corresponding
to the dependent of the relation
? introducer : specifies the node corresponding
to an introducing word or phrase
? initial : gives a thematic or semantic role of a
component, e.g., ?subj? for the object of a
by-phrase in a passive sentence.
The hierarchy of <struct> elements
corresponds to the nodes in a phrase structure
analysis; each <struct> element is typed
accordingly. The grammar underlying the
annotation  therefore specifies constraints on
embedding that can be instantiated in an XML
schema, which can then be used to prevent or
detect tree structures that do not conform to the
grammar. Conversely, the grammar rules
implicit in annotated treebanks, which are
typically not annotated according to a formal
grammar, can be easily extracted from the
abstract structural encoding.
The skeleton also includes a <feat> (feature)
element, which can be used to provide
additional information (e.g., gender, number)
that is attached to the node in the tree
represented by the enclosing <struct> element.
Like <struct>, this element can be recursively
nested or can point to a description in another
                                                           
7
 The stand-off scheme also provides means to represent
ambiguities, since there can be multiple links between data
and alternative annotations.
document, thereby providing means to associate
information at any level of detail or complexity
to the annotated structure.
Figure 4 shows the annotation from the PTB
(Figure 1) rendered in the abstract XML format.
Note that in this example, relations are encoded
only when they appear explicitly in the original
annotation (therefore, heads of relations default
to ?unknown?.)  An XSLT script could be used
to create a second XML document that includes
the relations implicit in the embedding (e.g., the
first embedded <struct> with category NP has
relation ?subject?, the first VP is the head, etc.).
A strict dependency annotation encoded in the
abstract format uses a flat hierarchy and
specifies all relations explicitly with the rel
attribute, as shown in Figure 5.8
4 Using the XCES Scheme
The Virtual AML provides a pivot format that
enables comparison of annotations in different
formats ? including not only different
constituency-based annotations, but also
constituency-based and dependency annotations.
For example, the PTB annotation corresponding
to the dependency annotation in Figure 2 is
shown in Figure 6. Figure 7 gives the
corresponding encoding in the XCES abstract
scheme. It is relatively trivial with an XSLT
script to extract the information in the
dependency annotation  (Figure 5) from the PTB
encoding (Figure 7) to produce a nearly identical
dependency encoding. The script would use
rules to make relations that are implicit in the
structure of the P T B encoding explicit (for
example, the ?xcomp? relation  that is implicit in
the embedding of the ?S? phrase).
The ability to generate a common
representation for different annotations
overcomes several obstacles that have hindered
evaluation exercises in the past. For instance, the
evaluation technique used in the PARSEVAL
exercise is applicable to phrase structure
analyses only, and cannot be applied to
dependency-style analyses or ?lexical? parsing
frameworks such as finite-state constraint
parsers. As the example above shows, this
                                                           
8
 For the sake of readability, this encoding assumes that the
sentence ?Paul intends to leave IBM? is marked up as
<s1><w1>Paul</w1><w2>intends</w2><w3>to</w3><w
4>leave</w4><w5>IBM</w5></s1>.
problem can be addressed using the XCES
framework.
It has also been noted that that the PARSEVAL
bracket-precision measure penalizes parsers that
return more structure than exists in the relatively
?flat? treebank structures, even if they are
correct (Srinivas, et al, 1995). XSLT scripts can
extract the appropriate information for
comparison purposes while retaining links to
additional parts of the annotation in the original
document, thus eliminating the need to ?dumb
down? parser output in order to participate in the
evaluation exercise. Similarly, information lost
in the transduction from phrase structure to a
dependency-based analysis (as in the example
above), which, as Atwell (1996) points out, may
eliminate grammatical information potentially
required for later processing, can also be
retained.
((S (NP-SBJ-1 Paul)
(VP intends)
(S (NP-SBJ *-1)
(VP  to
                (VP  leave
        (NP IBM))))
.))
Figure 6. PTB annotation of "Paul intends to
leave IBM.
<struct id="s0" type="S">
 <struct id="s1" type="NP"
          xlink:href="xptr(substring(/p/s[1]/text(),1,5))"
          rel ="SBJ"/>
 <struct id="s2" type="VP"
          xlink:href="xptr(substring(/p/s[1]/text(),7,8))"/>
 <struct id="s3" type="NP"
          xlink:href="xptr(substring(/p/s[1]/text(),16,3))"/>
 <struct id="s4" type="PP"
          xlink:href="xptr(substring(/p/s[1]/text(),20,4))"
          rel="DIR">
  <struct id="s5" type="NP"
           xlink:href="xptr(substring(/p/s[1]/text(),25,14))"/>
 </struct>
 <struct id="s6" type="S" rel="ADV">
     <struct id="s7" ref="s1" type="NP" rel="SBJ"/>
     <struct id="s8" type="VP"
             xlink:href="xptr(substring(/p/s[1]/text(),41,7))">
         <struct id="s9" type="NP"
                  xlink:href="xptr(substring(/p/s[1]/text(),49,8))"/>
         <struct id="s10" type="PP" rel="DIR"
                  xlink:href="xptr(substring(/p/s[1]/text(),57,6))">
             <struct id="s11" type="NP"
                      xlink:href="xptr(substring(/p/s[1]/text(),64,3))"/>
            </struct>
     </struct>
   </struct>
</struct>
Figure 4. The PTB example encoded according to the structural skeleton
<struct rel="subj"  head="w2" dependent="w1"/>
<struct rel="xcomp" head="w2" dependent="w4"  introducer="w3"/>
<struct rel="subj"  head="w4" dependent="w1"/>
<struct rel="dobj"  head="w4" dependent="w5"/>
Figure 5. Abstract XML encoding for the  dependency annotation in Figure 2.
<struct id="s0" type="S?>
        <struct id="s1" type="NP? target="w1?
                rel="SBJ" head="s2"/>
        <struct id="s2" type="VP? target="w2"/>
        <struct id="s3" type="S?>
                <struct id="s4" ref="s1"
                        rel="SBJ" head="s6"/>
                <struct id="s5" type="VP? target="w3">
                        <struct id="s6" type="VP? target="w4">
                                <struct id=?s7? type="NP? target="w5"/>
                        </struct>
                </struct>
        </struct>
</struct>
Figure 4 : PTB encoding of "Paul intends to leave IBM."
5 Discussion
Despite its seeming complexity, the XCES
framework is designed to reduce overhead for
annotators and users. Part of the work of the
XCES is to provide XML support (e.g.,
development of XSLT scripts, XML schemas,
etc.) for use by the research community, thus
eliminating the need for XML expertise at
each development site. Because XML-
encoded annotated corpora are increasingly
used for interchange between processing and
analytic tools, we are developing XSLT
scripts for mapping, and extraction of
annotated data, import/export of (partially)
annotated material, and integration of results
of external tools into existing annotated data
in XML. Tools for editing annotations in the
abstract format, which automatically generate
virtual AML from Data Category and Dialect
Specifications, are already under development
in the context of work on the Terminological
Markup Language, and a tool for
automatically generating RDF specifications
for user-specified data categories has already
been developed in the SALT project.9 Several
freely distributed interpreters for XSLT have
also been developed (e.g., xt10, Xalan11). In
practice, annotators and users of annotated
corpora will rarely see XML and RDF
instantiations of annotated data; rather, they
will access the data via interfaces that
automatically generate, interpret, and display
the data in easy-to-read formats.
                                                           
9
  http://www.loria.fr/projets/SALT
10
 Clark, J., 1999. XT Version 1991105.
http://www.jclark.com/xml/xt.html
11
 http://www.apache.org
The abstract model that captures the
fundamental properties of syntactic annotation
schemes provides a conceptual tool for
assessing the coherence and consistency of
existing schemes and those being developed.
The model enforces clear distinctions between
implicit and explicit information (e.g.,
functional relations implied by structural
relations in constituent analyses), and phrasal
and functional relations. It is alarmingly
common for annotation schemes to represent
these different kinds of information in the
same way, rendering their distinction
computationally intractable (even if they are
perfectly understandable by the informed
human reader).  Hand-developed annotation
schemes used in treebanks are often described
informally in guidebooks for annotators,
leaving considerable room for variation; for
example, Charniak (1996) notes that the PTB
implicitly contains more than 10,000 context-
free rules, most of which are used only once.
Comparison and transduction of schemes
becomes virtually impossible under such
circumstances. While requiring that annotators
make relations explicit and consider the
mapping to the XCES abstract format
increases overhead, we feel that the exercise
will help avoid such problems and can only
lead to greater coherence, consistency, and
inter-operability among annotation schemes.
The most important contribution to inter-
operability of annotation schemes is the Data
Category Registry. By mapping site-specific
categories onto definitions in the Registry,
equivalences (and non-equivalences) are made
explicit. Again, the provision of a ?standard?
set of categories, together with the
requirement that scheme-specific categories
are mapped to them where possible, will
contribute to greater consistency and
commonality among annotation schemes.
6 Conclusion
The XCES framework for linguistic
annotation is built around some relatively
straightforward ideas: separation of
information conveyed by means of structure
and information conveyed directly by
specification of content categories;
development of an abstract format that puts a
layer of abstraction between site-specific
annotat ion schemes and standard
specifications; and creation of a Data
Category Registry to provide a reference set
of annotation categories. The emergence of
XML and related standards such as RDF
provides the enabling technology. We are,
therefore, at a point where the creation and
use of annotated data and concerns about the
way it is represented can be treated
separately?that is, researchers can focus on
the question of what to encode, independent of
the question of how  to encode it. The end
result should be greater coherence,
consistency, and ease of use and access for
annotated data.
References
Anne Abeill? (ed.), forthcoming.  Treebanks:
Building and Using Syntactically Annotated
Corpora, Kluwer Academic Publishers.
Eric Atwell, 1996. Comparative evaluation of
grammatical annotation models. In R. Sutcliffe,
H. Koch, A. McElligott (eds.), Industrial
Parsing of Software Manuals, 25-46. Rodopi.
Paul Biron and Ashok Malhotra, 2000. XML
Schema Part 2: Datatypes. W3C Candidate
Recommendation.
http://www.w3.org/TR/xmlschema-2/.
Tim Bray, Jean Paoli and C. Michael Sperberg-
McQueen (eds.), 1998. Extensible Markup
Language (XML).
Dan Brickley and R.V. Guha, 2000. Resource
Description Framework (RDF) Schema
Specification 1.0. http://www.w3.org/TR/rdf-
schema/.
John Carroll, Guido Minnen, and Ted Briscoe,
forthcoming. Parser Evaluation Using a
Grammatical Relation Annotation Scheme. In
Anne Abeill? (ed.)  Treebanks: Building and
Using Syntactically Annotated Corpora, Kluwer
Academic Publishers.
Eugene Charniak, 1996. Tree-bank grammars.
Proceedings of the 13th National Conference on
Artificial Intelligence, AAAI?96, 1031-36.
James Clark (ed.), 1999. XSL Transformations
(XSLT). http://www.w3.org/TR/xslt.
James Clark and Steven DeRose, 1999. XML Path
Language. http://www.w3.org/TR/xpath.
Philip Harrison, Steven Abney, Ezra Black, Dan
Flickinger, Claudia Gdaniec, Ralph Grishman,
Don Hindle, Bob Ingria, Mitch Marcus,
Beatrice Santorini, and Tomek Strzalkowski,
1991. Evaluating syntax performance of
parser/grammars of English. Proceedings of the
Workshop on Evaluating Natural Language
Processing Systems, 71-77.
Nancy Ide,  Patrice Bonhomme, and Laurent
Romary, 2000. XCES: An XML-based Standard
for Linguistic Corpora. Proceedings of the
Second Language Resources and Evaluation
Conference (LREC), 825-30.
Nancy Ide, Adam Kilgarriff, and Laurent Romary,
2000. A Formal Model of Dictionary Structure
and Content. In Proceedings of EURALEX?00,
113-126.
Ora Lassila and Ralph Swick, 1999. Resource
Description framework (RDF) Model and
Syntax. http://www.w3.org/TR/REC-rdf-syntax.
Geoffrey Leech, R. Barnett, and P. Kahrel, 1996.
EAGLES Recommendations for the Syntactic
Annotation of Corpora.
Daniel Sleator and Davy Temperley, 1993. Parsing
English with a link grammar. T h i r d
International Workshop on Parsing
Technologies.
Bangalore Srinivas, Christy Doran, Beth-Ann
Hockey and Avarind Joshi,  1996. An approach
to robust partial parsing and evaluation metrics.
Proceedings of the ESSLI?96 Workshop on
Robust Parsing,  70-82.
Pasi Tapanainen and Timo J?rvinen. 1997.  A non-
projective dependency parser. Proceedings of
the 5th Conference on Applied Natural
Language Processing (ANLP?97), 64-71.
Henry Thompson, David Beech, Murray Maloney,
and Noah Mendelsohn, 2000. XML Schema
Part 1: Structures.
 http://www.w3.org/TR/xmlschema-1/.
Sense Discrimination with Parallel Corpora
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Tomaz Erjavec
Dept. of Intelligent Systems
Institute "Jozef Stefan"
Jamova 39,
SI-1000 Ljubljana
SLOVENIA
tomaz.erjavec@ijs.si
Dan Tufis
RACAI
Romanian Academy
Casa Academiei,
Calea 13 Septembrie 13,
Bucharest 74311, ROMANIA
tufis@racai.ro
Abstract
This paper describes an experiment that
uses translation equivalents derived from
parallel corpora to determine sense
distinctions that can be used for automatic
sense-tagging and other disambiguation
tasks. Our results show that sense
distinctions derived from cross-lingual
information are at least as reliable as those
made by human annotators. Because our
approach is fully automated through all its
steps, it could provide means to obtain
large samples of ?sense-tagged? data
without the high cost of human
annotation.
1 Introduction
It is well known that the most nagging issue for
word sense disambiguation (WSD) is the definition
of just what a word sense is. At its base, the
problem is a philosophical and linguistic one that is
far from being resolved. However, work in
automated language processing has led to efforts to
find practical means to distinguish word senses, at
least to the degree that they are useful for natural
language processing tasks such as summarization,
document retrieval, and machine translation.
Resnik and Yarowsky (1997) suggest that for the
purposes of WSD, the different senses of a word
could be determined by considering only sense
distinctions that are lexicalized cross-linguistically.
In particular, they propose that some set of target
languages be identified, and that the sense
distinctions to be considered for language
processing applications and evaluation be restricted
to those that are realized lexically in some
minimum subset of those languages. This idea
would seem to provide an answer, at least in part,
to the problem of determining different senses of a
word: intuitively, one assumes that if another
language lexicalizes a word in two or more ways,
there must be a conceptual motivation. If we look
at enough languages, we would be likely to find the
significant lexical differences that delimit different
senses of a word.
Several studies have used parallel texts for WSD
(e.g., Gale et al, 1993; Dagan et al, 1991; Dagan
and Itai, 1994) as well as to define semantic
properties of and relations among lexemes (Dyvik,
1998). More recently, two studies have examined
the use of cross-lingual lexicalization as a criterion
for validating sense distinctions: Ide (1999) used
translation equivalents derived from aligned
versions of Orwell?s Nineteen Eighty-Four among
five languages from four different languages
families, while Resnik and Yarowsky (2000) used
translations generated by native speakers presented
with isolated sentences in English. In both of these
studies, translation information was used to
validate sense distinctions provided in lexicons
such as WordNet (Miller et al, 1990). Although
the results are promising, especially for coarse-
grained sense distinctions, they rest on the
acceptance of a previously established set of
senses. Given the substantial divergences among
sense distinctions in dictionaries and lexicons,
together with the ongoing debate within the WSD
community concerning which sense distinctions, if
any, are appropriate for language processing
applications, fitting cross-linguistic information to
pre-established sense inventories may not be the
optimal approach.
                     July 2002, pp. 54-60.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
This paper builds on previously reported work (Ide
et al, 2001) that uses translation equivalents
derived from a parallel corpus to determine sense
distinctions that can be used to automatically
sense-tag the data. Our results show that sense
distinctions derived from cross-lingual information
are at least as reliable as those made by human
annotators. Our approach therefore provides a
promising means to automatically identify sense
distinctions.
2 Methodology
We conducted a study using parallel, aligned
versions of George Orwell's Nineteen Eighty-Four
(Erjavec and Ide, 1998) in seven languages:
English, Romanian, Slovene, Czech, Bulgarian,
Estonian, and Hungarian. The study involves
languages from four language families (Germanic,
Romance, Slavic, and Finno-Ugric),  three
languages from the same family (Czech, Slovene
and Bulgarian), as well as two  non-Indo-European
languages (Estonian and Hungarian). Although
Nineteen Eighty-Four, (ca. 100,000 words),  is a
work of fiction, Orwell's prose is not highly
stylized and, as such, it provides a reasonable
sample of modern, ordinary language that is not
tied to a given topic or sub-domain (which is the
case for newspapers, technical reports, etc.).
Furthermore, the translations of the text seem to be
relatively faithful to the original: over 95% of the
sentence alignments in the full parallel corpus of
seven languages are one-to-one (Priest-Dorman, et
al., 1997).
2.1 Preliminary Experiment
We constructed a multilingual lexicon based on the
Orwell corpus, using a method outlined in Tufis
and Barbu (2001, 2002). The complete English
Orwell contains 7,069 different lemmas, while the
computed lexicon comprises 1,233 entries, out of
which 845 have (possibly multiple) translation
equivalents in all languages. We then conducted a
preliminary study using a subset of 33 nouns
covering a range of frequencies and degrees of
ambiguity (Ide, et al, 2001).
For each noun in the sample, we extracted all
sentences from the English Nineteen Eighty-Four
containing the lemma in question, together with the
parallel sentences from each of the six translations.
The aligned sentences were automatically scanned
to extract translation equivalents.
1
 A vector was
then created for each occurrence, representing all
possible lexical translations in the six parallel
versions: if a given word is used to translate that
occurrence, the vector contains a 1 in the
corresponding position, 0 otherwise. The vectors
for each ambiguous word were fed to an
agglomerative clustering algorithm (Stolcke,
1996), where the resulting clusters are taken to
represent different senses and sub-senses of the
word in question.
The clusters produced by the algorithm were
compared with sense assignments made by two
human annotators on the basis of WordNet 1.6.
2
 In
order to compare the algorithm results with the
annotators? sense assignments, we normalized the
data as follows: for each annotator and the
algorithm, each of the 33 words was represented as
a vector of length n(n-1)/2, where n is the number
of occurrences of the word in the corpus. The
positions in the vector represent a ?yes-no?
assignment for each pair of occurrences, indicating
whether or not they were judged to have the same
sense (the same WordNet sense for the annotators,
and the same cluster for the algorithm).
Representing the clustering algorithm results in this
form required some means to ?flatten? the cluster
hierarchies, which typically extend to 5 or 6 levels,
to conform more closely to the completely flat
WordNet-based data. Therefore, clusters with a
minimum distance value (as assigned by the
clustering algorithm) at or below 1.7 were
combined, and each leaf of the resulting collapsed
tree was treated as a different sense. This yielded a
set of sense distinctions for each word roughly
similar in number to those assigned by the
annotators.
3
The cluster output for glass  in Figure 1 is an
example of the results obtained from the clustering
algorithm. For clarity, the occurrences have been
manually labeled with WordNet 1.6 senses (Figure
2). The tree shows that the algorithm correctly
                                                           
1
 Sentences in which more than one translation equivalent
appears were eliminated (cca. 5% of the translations).
2
 Originally, the annotators attempted to group occurrences
without reference to an externally defined sense set, but this
proved to be inordinately difficult and produced highly
variable results and was eventually abandoned.
3
 We used the number of senses annotators assigned rather
than the number of WordNet senses as a guide to determine
the minimum distance cutoff, because many WordNet senses
are not represented in the corpus.
grouped occurrences corresponding to WordNet
sense 1 (a solid material) in one of the two main
branches, and those corresponding to sense 2
(drinking vessel) in the other.  The top group is
further divided into two sub-clusters, the lower of
which refer to a looking glass and a magnifying
glass, respectively. While this is a particularly clear
example of good results from the clustering
algorithm, results for other words are, for the most
part, similarly reasonable.
Figure 1 : Output of the clustering algorithm
1. a brittle transparent solid with
irregular atomic structure
2. a glass container for holding liquids
while drinking
3. the quantity a glass will hold
4. a small refracting telescope
5. a mirror; usually a ladies' dressing
mirror
6. glassware collectively; "She collected
old glass"
Figure 2 : WordNet 1.6 senses for glass (noun)
The results of the first experiment are summarized
in Table 1, which shows the percentage of
agreement between the cluster algorithm and each
annotator, between the two annotators, and for the
algorithm and both annotators taken together.
4
 The
percentages are similar to those reported in earlier
work; for example, Ng et al (1999) achieved a raw
percentage score of 58% agreement among
annotators tagging nouns with WordNet 1.6 senses.
Cluster/Annotator 1 66.7%
Cluster/Annotator 2 63.6%
Annotator 1/Annotator 2 76.3%
Cluster/Annotator 1/ Annotator 2 53.4%
Table 1 : Levels of agreement
2.2 Second experiment
Comparison of sense differentiation achieved using
translation equivalents, as determined by the
clustering algorithm, with those assigned by human
annotators suggests that use of translation
equivalents for word sense tagging and
disambiguation is worth pursuing. Agreement
levels are comparable to (and in some cases higher
than) those obtained in earlier studies tagging with
WordNet senses. Furthermore, the pairwise
difference in agreement between the human
annotators and the annotators and the clustering
algorithm is only 10-13%, which is also similar to
scores obtained in other studies.
In the second phase, the experiment was broadened
to include 76 nouns from the multi-lingual lexicon,
including words with varying ambiguity (the range
in number of WordNet senses is 2 to 29, average
7.09) and semantic characteristics (e.g., abstract vs.
concrete: ?thought?, ?stuff?, ?meaning?, ?feeling?
vs. ?hand?, ?boot?, ?glass?, ?girl?, etc.). We chose
nouns that occur a minimum of 10 times in the
corpus, have no undetermined translations and at
least five different translations in the six non-
English languages, and have the log likelihood
score of at least 18; that is:
LL(T
T
, T
S
)  =
? ?
= =
2
1
ij
2
1i
n*2
j
*
j**i
**ij
n*n
n*n
log  
? 18
where n
ij
 stands for the number of times T
T
 and T
S
have been seen together in aligned sentences, n
i*
and n
*j 
stand for the number occurrences of T
T
 and
T
S,
 respectively, and n
**
 represents the total
                                                           
4
 We computed raw percentages only; common measures of
annotator agreement such as the Kappa statistic (Carletta,
1996) proved to be inappropriate for our two-category (?yes-
no?) classification scheme.
                _____|-> (1)
         |-----|     |-> (1)
         |     |_____|---> (1)
         |           |___|-> (1)
         |               |-> (1)
         |         |---> (1)
    |----|         |            _|-> (1)
    |    |         |         |-| |-> (1)
    |    |     |---|       |-| |-> (1)
    |    |     |   |     |-| |-> (1)
    |    |-----|   |   |-| |-> (1)
 |--|          |   |---| |-> (1)
 |  |          |       |-> (1)
 |  |          |___|---> (6)
 |  |              |___|-----> (1)
 |  |                  |-----> (1)
 |  |     _____|-----> (1)
-|  |----|     |-----> (5)
 |       |-----> (4)
 |      |---> (2)
 |  |---|      _|-> (2)
 |  |   |   |-| |-> (2)
 |  |   |---| |-> (2)
 |--|       |-> (2)
    |   |-----> (2)
    |   |           ___|-----> (2)
    |---|     |----|   |-----> (2)
        |     |    |    _|-> (2)
        |     |    |---| |-> (2)
        |-----|        |-> (2)
              |     ____|-> (3)
              |----|    |-> (2)
                   |     _|-> (2)
                   |----| |-> (2)
                               |-> (2)
number of potential translation equivalents in the
parallel corpus. The LL score is set at a maximum
value to ensure high precision for the extracted
translation equivalents, which minimizes sense
clustering errors due to incorrect word alignment.
Table 2 summarizes the data.
No. of words 76
No. of example sentences 2399
Average examples/word 32
No. of senses (annotator 1) 241
No. of senses (annotator 2) 280
No. of senses (annotator 3) 213
No. of senses (annotator 4) 232
No. of senses (all annotators) 345
Average senses per word 4.53
Percentage of annotator agreement:
Full agreement (4/4) 54.27
75% agreement (3/4) 28.13
50% agreement (2/4) 16.92
No agreement 0.66
Table 2 : Summary of the data
In this second experiment, we increased the
number of annotators to four. The results of the
clustering algorithm and the sense assignments
made by the human annotators were normalized
differently than in the earlier experiment, by
ignoring sense numbers and interpreting the
annotators? sense assignments as clusters only. To
see why this was necessary, consider the following
set of sense assignments for the seven occurrences
of ?youth? in Nineteen Eighty-Four:
OCC 1 2 3 4 5 6 7
Ann1
3 1 6 3 6 3 1
Ann2
2 1 4 2 6 2 1
Agreement is 43%; however, both annotators
classify occurrences 1, 4, and 6  as having the same
sense, although each assigned a different sense
number to the group. If we ignore sense numbers
and consider only the annotators? ?clusters?, the
agreement rate is much higher,
5
 and the data is
more comparable to that obtained from the cluster
algorithm.
We also addressed the issue of the appropriate
point at which to cut off the clustering by the
algorithm. Our use of a pre-defined minimum
                                                           
5
 In fact, the only remaining disagreement is that Annotator 1
assigns occurrences 3 and 5 together, whereas Annotator 2
assigns a different sense to occurrence 3?in effect, Annotator
2 makes a finer distinction than Annotator 1 between
occurrences 3 and 5.
distance value to determine the number of clusters
(senses) in the earlier experiment  yielded varying
results for different words (especially words with
significantly different numbers of translation
equivalents) and we sought a more principled
means to determine the cut-off value. The
clustering algorithm was therefore modified to
compute the correct number of clusters
automatically by halting the clustering process
when the number of clusters reached a value
similar to the average number obtained by the
annotators.
6
 As criteria, we used the minimum
distance between existing clusters at each iteration,
which determines the two clusters to be joined,
where minimum distance is computed between two
vectors v
1
, v
2
 length n as:
? 
(v
1
(i) - v
2
(i))
2
i=1
n
?
Best results were obtained when the clustering was
stopped at the point where:
(dist(k)-dist(k+1))/dist(k+1) < 0.12
where dist(k) is the minimal distance between two
clusters at the kth iteration step.
We defined a ?gold standard? annotation by taking
the majority vote of the four annotators (in case of
ties, the annotator closest to the majority vote in
the greatest number of cases was considered to be
right). Using this heuristic, the clustering algorithm
assigned the same number of senses as the gold
standard for 41 words. However, overall agreement
was much worse (67.9%) than when the number of
clusters was pre-specified. The vast majority of
clustering errors occurred when sense distributions
are skewed; we therefore added a post-processing
phase in which the smallest clusters are eliminated
and their members included in the largest cluster
when the number of occurrences in the largest
cluster is at least ten times that of any other
cluster.
7
With this new heuristic, the algorithm produced the
same number of clusters as the gold standard for
only 15 words, but overall agreement reached
74.6%. Mismatching clusters typically included
                                                           
6
 In principle, the upper limit for the number of senses for a
word is the number of senses in WordNet 1.6; however, there
was no case in which all WordNet senses appeared in the text.
7
 The factor of 10 is a conservative threshold; additional
experiments might yield evidence for a lower value.
only one element. There were only five words for
which a difference in the  number of clusters
assigned by the gold standard vs. the algorithm
significantly contributed to the 2.7% depreciation
in agreement.
We also experimented with eliminating the data for
?non-contributing? languages  (i.e., languages for
which there is only one translation for the target
word); this was ultimately abandoned because it
worsened results by amplifying the effect of
synonymous translations in other languages.
Finally, we compared the use of weighted vs.
unweighted clustering algorithms (see, e.g.,
Yarowsky and Florian, 1999) and determined that
results were improved using weighted clustering.
The clusters produced by each pair of classifiers
(human or machine) were mapped for maximum
overlap; differences were considered as
divergences. The agreement between two different
classifications was computed as the number of
common occurrences in the corresponding clusters
of the two classifications divided by the total
number of the occurrences of the target word. For
example, the word movement occurs 40 times in
the corpus; both the ?gold standard? and the
algorithm identified four clusters, but the
distribution of the 40 occurrences was substantially
different, as summarized in Table 3.  Thirty-four of
the 40 occurrences appear in the clusters common
to the two classifications; therefore, the agreement
rate is 85%.
CLUSTER 1 2 3 4
Gold standard 28 6 3 3
Algorithm 25 7 6 2
Intersection 24 6 3 1
Table 3 : Gold standard vs. algorithm clustering for
movement
2.3 Results
The results of our second experiment are
summarized in Table 4, which gives the agreement
rate between baseline clustering (B), in which it is
assumed all occurrences are labeled with the same
sense; each pair of human annotators (1-4); the
gold standard (G); and the clustering algorithm
(A). The table shows that agreement rates among
the human annotators, as compared to those
between the algorithm and all but one annotator,
are not significantly different, and that the
algorithm?s highest level of agreement is with the
baseline. This is not surprising because of the
second heuristic used. However, the second best
agreement rate for the algorithm is with the gold
standard, which suggests that sense distinctions
determined using the algorithm are almost as
reliable as sense distinctions determined manually.
The agreement of the algorithm with the gold
standard falls slightly below that of the human
annotators, but is still well within the range of
acceptability. Also, given that the gold standard
was computed on the basis of the human
annotations, it is understandable that these
annotations do better than the algorithm.
1 2 3 4 G A
B
71.1 65.1 76.3 74.1 75.5 81.5
1
78.1 75.6 83.1 88.6 74.4
2
71.3 75.9 82.5 66.9
3
77.3 82.1 77.1
4
90.4 75.9
G
77.3
Table 4 Agreement rates among baseline, the four
annotators, gold standard, and the algorithm
3 Discussion and Further Work
Our results show that sense distinctions based on
translation variants from parallel corpora are
similar to those obtained from human annotators,
which suggests several potential applications.
Because our approach is fully automated through
all its steps, it could be used to automatically
obtain large samples of ?sense-differentiated? data
without the high cost of human annotation.
Although our method does not choose sense
assignments from a pre-defined list, most language
processing applications (e.g. information retrieval)
do not require this knowledge; they need only the
information that different occurrences of a given
word are used in the same or a different sense.
A by-product of applying our method is that once
words in a text in one language are tagged using
this method, different senses of the corresponding
translations in the parallel texts are also identified,
potentially providing a source of information for
use in other language processing tasks and for
building resources in the parallel languages (e.g.,
WordNets for the Eastern European languages in
our study).  In addition, if different senses of target
words are identified in parallel texts, contextual
information for different senses of a word can be
gathered for use in disambiguating other, unrelated
texts. The greatest obstacle to application of this
approach is, obviously, the lack of parallel corpora:
existing freely available parallel corpora including
several languages are typically small (e.g., the
Orwell), domain dependent (e.g. the MULTEXT
Journal of the Commission (JOC) corpus; Ide and
V?ronis, 1994), and/or represent highly stylized
language (e.g. the Bible; Resnik et al, 1999).
Appropriate parallel data including Asian
languages  is virtually non-existent. Given that our
method applies only to words for which different
senses are lexicalized differently in at least one
other language, its broad application depends on
the future availability of large-scale parallel
corpora including a variety of language types.
Many studies have pointed out that coarser-grained
sense distinctions can be assigned more reliably by
human annotators than finer distinctions such as
those in WordNet. In our study, the granularity of
the sense distinctions was largely ignored, except
insofar as we attempted to cut off the number of
clusters produced by the algorithm at a value
similar to the number identified by the annotators.
The sense distinctions derived from the clustering
algorithm are hierarchical, often identifying four or
five levels of refinement, whereas the WordNet
sense distinctions are organized as a flat list with
no indication of their degree of relatedness. Our
attempt to flatten the cluster data in fact loses much
information about the relatedness of senses.
8
 As a
result, both annotators and the clustering algorithm
are penalized as much for failing to distinguish
fine-grained as coarse-grained distinctions. We are
currently exploring two possible sources of
information about sense relatedness: the output of
the clustering algorithm itself, and WordNet
hypernyms, which may not only improve but also
broaden the applicability of our method.
                                                           
8
 Interestingly, the clustering for ?glass? in Figure 1 reveals
additional sub-groupings that are not distinguished in
WordNet:  the top sub-group of the top cluster includes
occurrences that deal with some physical aspect of the material
(?texture of?, ?surface of?, ?rainwatery?, ?soft?, etc.). In the
lower cluster, the two main sub-groups distinguish a (drinking)
glass as a manipulatable object (by washing, holding, on a
shelf, etc.) from its sense as a vessel (mainly used as the object
of ?pour into?, ?fill?, ?take/pick up?, etc. or modified by
?empty?, ?of gin?, etc.).
We note in our data that although it is not
statistically significant, there is some correlation (-
.51) between the number of WordNet senses for a
word and overall agreement levels. The lowest
overall agreement levels were for ?line? (29
senses), ?step? (10), position (15), ?place? (17),
and ?corner? (11). Perfect agreement was achieved
for several words with under 5 senses, e.g., ?hair?
(5), ?morning? (4), ?sister? (4), ?tree? (2), and
?waist? (2)?all of which were judged by both the
annotators and the algorithm to occur in only one
sense in the text. On the other hand, agreement
levels for some words with under five WordNet
senses had low agreement: e.g., ?rubbish? (2),
?rhyme? (2), ?destruction? (3), and ?belief? (3).
Because both the algorithm (which based
distinctions on translations) and the human
annotators (who used WordNet senses) had low
agreement in these cases, the WordNet sense
distinctions may be overly fine-grained and,
possibly, irrelevant to many language processing
tasks.
We continue to explore the viability of our method
to automatically determine sense distinctions
comparable to those achieved by human
annotators. We are currently exploring methods to
refine the clustering results as well as their
comparison to results obtained from human
annotators (e.g., the Gini Index  [Boley, et al,
1999]).
4 Conclusion
The results reported here represent a first step in
determining the degree to which automated
clustering based on translation equivalents can be
used to differentiate word senses.  Our work so far
indicates that the method is promising and could
provide a significant means to automatically
acquire sense-differentiated data in multiple
languages. Our current results suggest that coarse-
grained agreement is the best that can be expected
from humans, and that our method is capable of
duplicating sense differentiation at this level.
5 Acknowledgements
Our thanks go to Arianna Schlegel, Christine
Perpetua, and Lindsay Schulz who annotated the
data, and to Ion Radu who modified the clustering
algorithm. We would also like to thank the
anonymous reviewers for their comments and
suggestions. All errors, of course, remain our own.
6 References
Boley D., Gini, M, Gross, R., Han, S.,.
Hastings, K and Karypis, G., Kumar, V.,
Mobasher, B, Moore, J. (1999) Partitioning-Based
Clustering for Web Document Categorization.
Decision Support Systems, 27:3, 329-341.
Carletta, J. (1996). Assessing Agreement on
Classification Tasks: The Kappa Statistic.
Computational Linguistics, 22:2, 249-254.
Dagan, I. and Itai, A. (1994). Word sense
disambiguation using a second language
monolingual corpus. Computational Linguistics,
20:4, 563-596.
Dagan, I., Itai, A., and Schwall, U. (1991). Two
languages are more informative than one.
Proceedings of the 29th Annual Meeting of the
ACL, 18-21 Berkeley, California, 130-137.
Dyvik, H. (1998). Translations as Semantic
Mirrors. Proceedings of Workshop Multilinguality
in the Lexicon II, ECAI 98, Brighton, UK, 24-44.
Erjavec, T. and Ide, N. (1998). The
MULTEXT-EAST Corpus. Proceedings of the
First International Conference on Language
Resources and Evaluation, Granada, 971-74.
Gale, W. A., Church, K. W. and Yarowsky, D.
(1993). A method for disambiguating word senses
in a large corpus. Computers and the Humanities,
26, 415-439.
Ide, N. (1999). Cross-lingual sense
determination: Can it work? Computers and the
Humanities, 34:1-2,  223-34.
Ide, N., Erjavec, T., and Tufis, D. (2001).
Automatic sense tagging using parallel corpora.
Proceedings of the Sixth Natural Language
Processing Pacific Rim Symposium, Tokyo,  83-89.
Ide, N., V?ronis, J. (1994). Multext
(Multilingual Tools and Corpora). Proceedings of
the 14th International Conference on
Computational Linguistics, COLING?94, Kyoto,
90-96.
Miller, G. A., Beckwith, R. T. Fellbaum, C. D.,
Gross, D. and Miller, K. J. (1990). WordNet: An
on-line lexical database. International Journal of
Lexicography, 3:4, 235-244.
Ng, H. T., Lim, C. Y., Foo, S. K. (1999). A
Case Study on Inter-Annotator Agreement for
Word Sense Disambiguation. Proceedings of the
ACL SIGLEX Workshop: Standardizing Lexical
Resources, College Park, MD, USA, 9-13.
Priest-Dorman, G.; Erjavec, T.; Ide, N. and
Petkevic, V. (1997). Corpus Markup. COP Project
106 MULTEXT-East D2.3 F.
Resnik, P. and Yarowsky, D. (2000).
Distinguishing systems and distinguishing senses:
New evaluation methods for word sense
disambiguation. Journal of Natural Language
Engineering, 5(2): 113-133.
Resnik, P., Broman Olsen, M., Diab, M. (1999).
Creating a Parallel Corpus from the Book of 2000
Tongues. Computers and the Humanities, 33:1-2.
129-153.
Resnik, Philip and Yarowsky, David (1997). A
perspective on word sense disambiguation methods
and their evaluation. ACL-SIGLEX Workshop
Tagging Text with Lexical Semantics: Why, What,
and How? Washington, D.C., 79-86.
Stolcke, Andreas (1996) Cluster 2.9.
http://www.icsi.berkeley.edu/ftp/global/pub/ai/
stolcke/software/cluster-2.9.tar.Z.
Tufis, D., Barbu, A.-M. (2001) Automatic
Construction of Translation Lexicons. In V.Kluew,
C. D'Attellis N. Mastorakis (eds.) Advances in
Automation, Multimedia and Modern Computer
Science, WSES Press, 156-172
Tufis, D., Barbu, A.-M. (2002), Revealing
translators knowledge: statistical methods in
constructing practical multilingual lexicons for
language and speech processing. International
Journal of Speech Technology (to appear).
Yarowsky, D., Florian. R. (1999). Taking the
load off the conference chairs: towards a digital
paper-routing assistant. Proceedings of the Joint
SIGDAT Conference on Empirical Methods in NLP
and Very Large Corpora, 220-230.
International Standard for a Linguistic Annotation Framework
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Laurent Romary
Equipe Langue et Dialogue
LORIA/INRIA
Vandoeuvre-l?s-Nancy
FRANCE
romary@loria.fr
Eric de la Clergerie
INRIA Rocquencourt, BP 105
78153 Le Chesnay cedex
FRANCE
Eric.De_La_Clergerie@
inria.fr
Abstract
This paper describes the outline of a linguistic
annotation framework under development by
ISO TC37 SC WG1-1. This international
standard will provide an architecture for the
creation, annotation, and manipulation of lin-
guistic resources and processing software. The
outline described here results from a meeting
of approximately 20 experts in the field, who
determined the principles and fundamental
structure of the framework. The goal is to
provide maximum flexibility for encoders and
annotators, while at the same time enabling
interchange and re-use of annotated linguistic
resources.
1 Introduction
Language resources are bodies of electronic language
data used to support research and applications in the
area of natural language processing. Typically, such
data are enhanced (annotated) with linguistic informa-
tion such as morpho-syntactic categories, syntactic or
discourse structure, co-reference information, etc.; or
two or more bodies may be aligned for correspondences
(e.g., parallel translations, speech signal and transcrip-
tion).
Over the past 15-20 years, increasingly large bodies
of language resources have been created and annotated
by the language engineering community. Certain fun-
damental representation principles have been widely
adopted, such as the use of stand-off annotation, use of
XML, etc., and several attempts to provide generalized
annotation mechanisms and formats have been devel-
oped (e.g., XCES, annotation graphs). However, it re-
mains the case that annotation formats often vary
considerably from resource to resource, often to satisfy
constraints imposed by particular processing software.
The language processing community has recognized
that commonality and interoperability are increasingly
imperative to enable sharing, merging, and comparison
of language resources.
To provide an infra-structure and framework for
language resource development and use, the Interna-
tional Organization for Standardization (ISO) has
formed a sub-committee (SC4) under Technical Com-
mittee 37 (TC37, Terminology and Other Language
Resources) devoted to Language Resource Manage-
ment. The objective of ISO/TC 37/SC 4 is to prepare
international standards and guidelines for effective lan-
guage resource management in applications in the mul-
tilingual information society. To this end, the committee
is developing principles and methods for creating, cod-
ing, processing and managing language resources, such
as written corpora, lexical corpora, speech corpora, dic-
tionary compiling and classification schemes. The focus
of the work is on data modeling, markup, data exchange
and the evaluation of language resources other than ter-
minologies (which have already been treated in ISO/TC
37). The worldwide use of ISO/TC 37/SC 4 standards
should improve information management within indus-
trial, technical and scientific environments, and increase
efficiency in computer-supported language communica-
tion.
At present, language professionals and standardiza-
tion experts are not sufficiently aware of the standardi-
zation efforts being undertaken by ISO/TC 37/SC 4.
Promoting awareness of future activities and rising
problems, therefore, is crucial for the success of the
committee, and will be required to ensure widespread
adoption of the standards it develops. An even more
critical factor for the success of the committee's work is
to involve, from the outset, as many and as broad a
range of potential users of the standards as possible.
Within ISO/TC 37/SC 4, a working group (WG1-1)
has been established to develop a Linguistic Annotation
Framework (LAF) that can serve as a basis for harmo-
nizing existing language resources as well as developing
new ones. In order to ensure that the framework is de-
veloped based on the input and consensus of the re-
search community, a group of experts
1
 was convened on
November 21-22, 2002, at Pont-?-Mousson, France, to
lay out the overall structure of the framework. .In this
paper, we outline the conclusions from this meeting, and
solicit the input of other members of the community to
inform its further development.
2 Background and rationale
The standardization of principles and methods for the
collection, processing and presentation of language re-
sources requires a distinct type of activity. Basic stan-
dards must be produced with wide-ranging applications
in view. In the area of language resources, these stan-
dards should provide various technical committees of
ISO, IEC and other standardizing bodies with the
groundwork for building more precise standards for
language resource management.
The need for harmonization of representation for-
mats for different kinds of linguistic information is criti-
cal, as resources and information are more and more
frequently merged, compared, or otherwise utilized in
common systems. This is perhaps most obvious for
processing multi-modal information, which must sup-
port the fusion of multimodal inputs and represent the
combined and integrated contributions of different types
of input (e.g., a spoken utterance combined with gesture
and facial expression), and enable multimodal output
(see, for example, Bunt and Romary, 2002). However,
language processing applications of any kind require the
integration of varieties of linguistic information, which,
in today?s environment, come from potentially diverse
sources. We can therefore expect use and integration of,
for example, syntactic, morphological, discourse, etc.
information for multiple languages, as well as informa-
tion structures like domain models and ontologies.
We are aware that standardization is a difficult busi-
ness, and that many members of the targeted communi-
ties are skeptical about imposing any sort of standards at
all. There are two major arguments against the idea of
standardization for language resources. First, the diver-
sity of theoretical approaches to, in particular, the an-
                                                           
1
 Participants: Nuria Bel (Universitat de Barcelona), David
Durand (Brown University), Henry Thompson (University of
Edinburgh), Koiti Hasida (AIST Tokyo), Eric De La Clergerie
(INRIA), Lionel Clement (INRIA), Laurent Romary (LORIA),
Nancy Ide (Vassar College), Kiyong Lee (Korea University),
Keith Suderman (Vassar College), Aswani Kumar (LORIA),
Chris Laprun (NIST), Thierry Declerck (DFKI), Jean Carletta
(University of Edinburgh), Michael Strube (European Media
Laboratory), Hamish Cunningham (University of Sheffield),
Tomaz Erjavec (Institute Jozef Stefan), Hennie Brugman
(Max-Planck-Institut f?r Psycholinguistik), Fabio Vitali (Uni-
versite di Bologna), Key-Sun Choi (Korterm), Jean-Michel
Borde (Digital Visual), Eric Kow (LORIA).
notation of various linguistic phenomena suggests that
standardization is at least impractical, if not impossible.
Second, it is feared that vast amounts of existing data
and processing software, which may have taken years of
effort and considerable funding to develop, will be ren-
dered obsolete by the acceptance of new standards by
the community. Recognizing the validity of both of
these concerns, WG1-1 does not seek to establish a sin-
gle, definitive annotation scheme or format. Rather, the
goal is to provide a framework for linguistic annotation
of language resources that can serve as a reference or
pivot for different annotation schemes, and which will
enable their merging and/or comparison. To this end,
the work of WG1-1 includes the following:
o analysis of the full range of annotation types and
existing schemes, to identify the fundamental
structural principles and content categories;
o instantiation of an abstract format capable of cap-
turing the structure and content of  linguistic anno-
tations, based on the analysis in (1);
o establishment of a mechanism for formal definition
of a set of reference content categories which can
be used ?off the shelf? or serve as a point of depar-
ture for precise definition of new or modified cate-
gories.
o provision of both a set of guidelines and principles
for developing new annotation schemes and con-
crete mechanisms for their implementation, for
those who wish to use them.
By situating all of the standards development
squarely in the framework of XML and related stan-
dards such as RDF, DAML+OIL, etc., we hope to en-
sure not only that the standards developed by the
committee provide for compatibility with established
and widely accepted web-based technologies, but also
that transduction from legacy formats into XML formats
conformant to the new standards is feasible.
3  General requirements for a linguistic
annotation framework
The following general requirements for a linguistic an-
notation framework were identified by the group of ex-
perts at Pont-?-Mousson:
Expressive adequacy
The framework must provide means to represent all
varieties of linguistic information (and possibly also
other types of information). This includes representing
the full range of information from the very general to
information at the finest level of granularity.
Media independence
The framework must handle all potential media types,
including text, audio, video, image, etc. and should, in
principle, provide common mechanisms for handling all
of them. The framework will rely on existing or devel-
oping standards for representing multi-media.
Semantic adequacy
o Representation structures must have a formal se-
mantics, including definitions of logical operations
o There must exist a centralized way of sharing de-
scriptors and information categories
Incrementality
o The framework must provide support for various
stages of input interpretation and output generation.
o The framework must provide for the representation
of partial/under-specified results and ambiguities,
alternatives, etc. and their merging and comparison.
Uniformity
Representations must utilize same ?building blocks? and
the same methods for combining them.
Openness
The framework must not dictate representations de-
pendent on a single linguistic theory.
Extensibility
The framework must provide ways to declare and inter-
change extensions to the centralized data category reg-
istry.
Human readability
Representations must be human readable, at least for
creation and editing.
Processability (explicitness)
Information in an annotation scheme must be ex-
plicit?that is, the burden of interpretation should not be
left to the processing software.
Consistency
Different mechanisms should not be used to indicate the
same type of information.
To fulfill these requirements, it is necessary to iden-
tify a consistent underlying data model for data and its
annotations. A data model is a formalized description of
the data objects (in terms of composition, attributes,
class membership, applicable procedures, etc.) and rela-
tions among them, independent of their instantiation in
any particular form. A data model capable of capturing
the structure and relations in diverse types of data and
annotations is a pre-requisite for developing a common
corpus-handling environment: it impacts the design of
annotation schema, encoding formats and data archi-
tectures, and tool architectures.
As a starting assumption, we can conceive of an an-
notation as a one- or two-way link between an annota-
tion object and a point (or a list/set of points) or span (or
a list/set of spans) within a base data set. Links may or
may not have a semantics--i.e., a type--associated with
them. Points and spans in the base data may themselves
be objects, or sets or lists of objects. We make several
observations concerning this assumption:
o the model assumes a fundamental linearity of ob-
jects in the base,
2
  e.g., as a time line (speech); a s e-
quence of characters, words, sentences, etc.; or
pixel data representing images;
o the granularity of the data representation and en-
coding is critical: it must be possible to uniquely
point to the smallest possible component (e.g.,
character, phonetic component, pitch signal, mor-
pheme, word, etc.);
o an annotation scheme must be mappable to the
structures defined for annotation objects in the
model;
o an encoding scheme must be able to capture the
object structure and relations expressed in the
model, including class membership and inheritance,
therefore requiring a sophisticated means to specify
linkage within and between documents;
o it is necessary to consider the logistics of identify-
ing spans by enclosing them in start and end tags
(thus enabling hierarchical grouping of objects in
the data itself), vs. explicit addressing of start and
end points, which is required for read-only data;
o it must be possible to represent objects and rela-
tions in some (fairly straightforward) form that pre-
vents information loss;
o ideally, it should be possible to represent the ob-
jects and relations in a variety of formats suitable to
different tools and applications.
ISO TC37/SC 4?s goal is to develop a framework for
the design and implementation of linguistic resource
formats and processes in order to facilitate the exchange
of information between language processing modules. A
well-defined representational framework for linguistic
information will also provide for the specification and
comparison of existing application-specific representa-
tions and the definition of new ones, while ensuring a
level of interoperability between them. The framework
should allow for variation in annotation schemes while
                                                           
2
 Note that this observation applies to the fundamental struc-
ture of stored data. Because the targets of a relation may be
either individual objects, or sets or lists of objects, information
with more than one dimension is accommodated.
at the same time enabling comparison and evaluation,
merging of different annotations, and development of
common tools for creating and using annotated data. For
this purpose we envisage a common ?pivot? format
based on a data model capable of capturing all types of
information in linguistic annotations, into and out of
which site-specific representation formats can be trans-
duced. This strategy is similar to that adopted in the
design of languages intended to be reusable across plat-
forms, such as Java. The pivot format must support the
communication among all modules in the system, and
be adequate for representing not only the end result of
interpretation, but also intermediate results.
4 Terms and definitions
The following terms and definitions are used in the dis-
cussion that follows:
Annotation: The process of adding linguistic informa-
tion to language data (?annotation of a corpus?) or the
linguistic information itself (?an annotation?), inde-
pendent of its representation. For example, one may
annotate a document for syntax using a LISP-like repre-
sentation, an XML representation, etc.
Representation: The format in which the annotation is
rendered, e.g. XML, LISP, etc. independent of its con-
tent. For example, a phrase structure syntactic annota-
tion and a dependency-based annotation may both be
represented using XML, even though the annotation
information itself is very different.
Types of Annotation: We distinguish two fundamental
types of annotation activity:
1. segmentation : delimits linguistic elements that
appear in the primary data. Including
o continuous segments (appear contiguously in
the primary data)
o super- and sub-segments, where groups of
segments will comprise the parts of a larger
segment (e.g., a contiguous word segments
typically comprise a sentence segment)
o discontinuous segments (linking continuous
segments)
o landmarks (e.g time stamps) that note a point in
the primary data
In current practice, segmental information may
or may not appear in the document containing the
primary data itself. Documents considered to be
read-only, for example, might be segmented by
specifying byte offsets into the primary document
where a given segment begins and ends.
2. linguistic annotation: provides linguistic informa-
tion about the segments in the primary data, e.g., a
morpho-syntactic annotation in which a part of
speech and lemma are associated with each seg-
ment in the data. Note that the identification of a
segment as a word, sentence, noun phrase, etc. also
constitutes linguistic annotation. In current practice,
when it is possible to do so, segmentation and
identification of the linguistic role or properties of
that segment are often combined (e.g., syntactic
bracketing, or delimiting each word in the docu-
ment with an XML tag that identifies the segment
as a word, sentence, etc.).
Stand-off annotation: Annotations layered over a
given primary document and instantiated in a document
separate from that containing the primary data. Stand-
off annotations refer to specific locations in the primary
data, by addressing byte offsets, elements, etc. to which
the annotation applies. Multiple stand-off annotation
documents for a given type of annotation can refer to
the same primary document (e.g., two different part of
speech annotations for a given text). There is no re-
quirement that a single XML-compliant document may
be created by merging stand-off annotation documents
with the primary data; that is, two annotation documents
may specify trees over the primary data that contain
overlapping hierarchies.
5 Design principles
The following general principles will guide the LAF
development:
o The data model and document form are distinct but
mappable to one another
o The data model is parsimonious, general, and for-
mally precise
o The data model is built around a clear separation of
structure and content
o There is an inventory of logical operations sup-
ported by the data model, which define its abstract
semantics
o The document form is largely under user control
o The mapping between the flexible document form
and data model is via a rigid dump-format
o The mapping from document form to the dump
format is documented in an XML Schema (or the
functional equivalent thereof) associated with the
document
o Mapping is operationalized either via schema-based
data-binding process o r via schema-derived
stylesheet mapping between the user document and
the dump-format document.
o It must be possible to isolate specific layers of an-
notation from other annotation layers or the primary
(base) data; i.e., it must be possible to create a for-
mat using stand-off annotation
o The dump format must be designed to enable
stream marshalling and unmarshalling
The overall architecture of LAF as dictated by these
principles is given in Figure 1. The left side of the dia-
gram represents the user-defined document form, and is
labeled ?human? to indicate that creation and editing, of
the resource is accomplished via human interaction with
this format. This format should, to the extent possible,
be human readable. We will support XML for these
formats (e.g., by providing style sheets, examples, etc.)
but not disallow other formats. The right side represents
the dump format, which is machine processable, and
may not be human readable as it is intended for use only
in processing. This format will be instantiated in XML.
Figure 1. Overall LAF architecture
6 Practice
The following set of practices will guide the implementa-
tion of the LAF:
o The data model is essentially a feature structure
graph with a moderate admixture of algebra (e.g.
disjunction, sets), grounded in n-dimensional regions
of primary data and literals.
o The dump format is isomorphic to the data model.
o Semantic coherence is provided by a registry of fea-
tures in an XML-compatible format (e.g., RDF),
which can be used directly in the user-defined for-
mats and is always used with the dump format.
o Resources will be available to support the design and
specification of document forms, for example:
- XML Schemas in several normal forms based on
type definitions and abstract elements that can be
exploited via type derivation and/or substitution
group;
- XPointer design-patterns with standoff seman-
tics;
- Schema annotations specifying mapping between
document form and data model;
- Meta-stylesheet for mapping from annotated
XML Schema to mapping stylesheets;
- Data-binding stylesheets with language-specific
bindings (e.g. Java).
o Users may define their own data categories or estab-
lish variants of categories in the registry. In such
cases, the newly defined data categories will be for-
malized using the same format as definitions avail-
able in the registry, and will be associated with the
dump format.
o The responsibility of converting to the dump format
is on the producer of the resource.
o The producer is responsible for documenting the
mapping from the user format to the data model.
Dump format
(some instantiation of
data model)
MAPPING SPEC
(e.g. W3C
XML schema)
User annotation
form
Meta-stylesheet
stylesheet
Intended for archive
Accompanied by
mapping to data
model
Pivot format
HUMAN MACHINE
o The ISO working group will provide test suites and
examples following these guidelines:
- The example format should illustrate use of data
model/mapping
- The examples will show both the left (human-
readable) and right (machine processable) side
formats
- Examples will be provided that use existing
schemes
7 Discussion
The framework outlined in the previous section provides
for the use of any annotation format consistent with the
feature structure-based data model that will be used to
define the pivot format. This suggests a future scenario in
which annotators may create and edit annotations in a
proprietary format, transduce the annotations using avail-
able tools to the pivot format for interchange and/or proc-
essing, and if desired, transduce the pivot form of the
annotations (and/or additional annotation introduced by
processing) back into the proprietary format. We antici-
pate the future development of annotation tools that pro-
vide a user-oriented interface for specifying annotation
information, and which then generate annotations in the
pivot format directly. Thus the pivot format is intended to
function in the same way as, for example, Java byte code
functions for programmers, as a universal ?machine lan-
guage? that is interpreted by processing software into an
internal representation suited to its particular require-
ments. As with Java byte code, users need never see or
manipulate the pivot format; it is solely for machine con-
sumption.
Information units or data categories provide the se-
mantics of an annotation. Data categories are the most
theory and application-specific part of an annotation
scheme. Therefore, LAF includes a Data Category Regis-
try to provide a means to formally define data categories
for reference and use in annotation. To make them maxi-
mally interoperable and consistent with existing stan-
dards, RDF schemas can be used to formalize the
properties and relations associated with each data cate-
gory. The RDF schema ensures that each instantiation of
the described objects is recognized as a sub-class of more
general classes and inherits the appropriate properties.
Annotations will reference the data categories via a URL
identifying their instantiations in the Data Category Reg-
istry itself. The class and sub-class mechanisms provided
in RDFS and its extensions in OWL will also enable
creation of an ontology of annotation classes and types.
A formally defined set of categories will have several
functions: (1) it will provide a precise semantics for an-
notation categories that can be either used ?off the shelf?
by annotators or modified to serve specific needs; (2) it
will provide a set of reference categories onto which
scheme-specific names can be mapped; and (3) it will
provide a point of departure for definition of variant or
more precise categories. Thus the overall goal of the Data
Category Registry is not to impose a specific set of cate-
gories, but rather to ensure that the semantics of data
categories included in annotations (whether they exist in
the Registry or not) are well-defined and understood.
The data model that will define the pivot format must
be capable of representing all of the information con-
tained in diverse annotation types. The model we assume
is a feature structure graph for annotation information,
capable of referencing n-dimensional regions of primary
data as well as other annotations. The choice of this
model is indicated by its almost universal use in defining
general-purpose annotation formats, including the Ge-
neric Modeling Tool (GMT) (Ide & Romary, 2001, 2002)
and Annotation Graphs (Bird & Liberman, 2001). The
XML-based GMT could serve as a starting point for de-
fining the pivot format; its applicability to diverse anno-
tation types, including terminology, dictionaries and other
lexical data (Ide, et al, 2000), morphological annotation
(Ide & Romary, 2001a; 2003) and syntactic annotation
(Ide & Romary, 2001b) demonstrates its generality. As
specified by the LAF architecture, the GMT implements a
feature structure graph, and exploits the hierarchical
structure of XML elements and XML?s powerful inter-
and intra-document pointing and linkage mechanisms for
referencing both ?raw? and XML-tagged primary data
and its annotations.
The provision of development resources, including
schemas, design patterns, and stylesheets, will enable
annotators and software developers to immediately adapt
to LAF. Example mappings, e.g., for XCES-encoded an-
notations, will also be provided.
8 Conclusion
In this paper we describe the Linguistic Annotation
Framework under development by ISO TC37/SC 4 WG1-
1, as defined by a group of experts convened at a work-
shop in Pont-?-Mousson, France, in late 2002. Its design
is intended to allow for, on the one hand, maximum flexi-
bility for annotators, and. on the other, processing effi-
ciency and reusability. This is accomplished by
separating user annotation formats from the ex-
change/processing format. This separation also ensures
that pre-existing annotations are compatible with LAF.
ISO TC37/SC4 is just beginning its work, and will use
the general framework discussed in the preceding sections
as its starting point. However, the work of the committee
will not be successful unless it is accepted by the lan-
guage processing community. To ensure widespread ac-
ceptance, it is critical to involve as many representatives
of the community in the development of the standards as
possible, in order to ensure that all needs are addressed.
This paper serves as a call for participation to the lan-
guage processing community; those interested should
contact the TC 37/SC 4 chairman (Laurent Romary:
romary@loria.fr). For general information, consult the
ISO TC37/SC4 website (http://www.tc37sc4.org).
References
Bird, S. & Liberman, M. (2001). A formal framework for
linguistic annotation. Speech Communication, 33:1-2,
23-60.
Bunt. H. & Romary, L. (2002). Towards Multimodal
Content Representation.  Proceedings of the Workshop
on International Standards for Terminology and Lan-
guage Resource Management, Las Palmas.
Ide, N. & Romary, L. (2001a). Standards for Language
Resources, IRCS Workshop on Linguistic Databases,
Philadelphia, 141-49.
Ide, N. & Romary, L. (2001b). A Common Framework
for Syntactic Annotation. Proceedings of ACL'2001,
Toulouse, 298-305.
Ide, N., Kilgarriff, A., & Romary, L. (2000). A Formal
Model of Dictionary Structure and Content. Proceed-
ings of Euralex 2000, Stuttgart, 113-126.
Ide, N. & Romary, L. (2003). Encoding Syntactic Anno-
tation. In Abeill?, A. (ed.). Treebanks: Building and
Using Syntactically Annotated Corpora. Dordrecht:
Kluwer Academic Publishers (in press).
Outline of the International Standard Linguistic Annotation
Framework
Nancy Ide
Dept. of Computer Science
Vassar College
Poughkeepsie,
New York 12604-0520
 USA
ide@cs.vassar.edu
Laurent Romary
Equipe Langue et Dialogue
LORIA/INRIA
Vandoeuvre-l?s-Nancy
FRANCE
romary@loria.fr
Abstract
This paper describes the outline of a lin-
guistic annotation framework under de-
velopment by ISO TC37 SC WG1-1. This
international standard provides an archi-
tecture for the creation, annotation, and
manipulation of linguistic resources and
processing software. The goal is to pro-
vide maximum flexibility for encoders
and annotators, while at the same time
enabling interchange and re-use of anno-
tated linguistic resources. We describe
here the outline of the standard for the
purposes of enabling annotators to begin
to explore how their schemes may map
into the framework.
1 Introduction
Over the past 15-20 years, increasingly large bod-
ies of language resources have been created and
annotated by the language engineering community.
Certain fundamental representation principles have
been widely adopted, such as the use of stand-off
annotation, use of XML, etc., and several attempts
to provide generalized annotation mechanisms and
formats have been developed (e.g., XCES, annota-
tion graphs). However, it remains the case that an-
notation formats often vary considerably from
resource to resource, often to satisfy constraints
imposed by particular processing software. The
language processing community has recognized
that commonality and interoperability are increas-
ingly imperative to enable sharing, merging, and
comparison of language resources.
To provide an infra-structure and framework for
language resource development and use, the Inter-
national Organization for Standardization (ISO)
has formed a sub-committee (SC4) under Techni-
cal Committee 37 (TC37, Terminology and Other
Language Resources) devoted to Language Re-
source Management. The objective of ISO/TC
37/SC 4 is to prepare international standards and
guidelines for effective language resource man-
agement in applications in the multilingual infor-
mation society. To this end, the committee is
developing principles and methods for creating,
coding, processing and managing language re-
sources, such as written corpora, lexical corpora,
speech corpora, and dictionary compiling and clas-
sification schemes. The focus of the work is on
data modeling, markup, data exchange and the
evaluation of language resources other than termi-
nologies (which have already been treated in
ISO/TC 37). The worldwide use of ISO/TC 37/SC
4 standards should improve information manage-
ment within industrial, technical and scientific en-
vironments, and increase efficiency in computer-
supported language communication.
Within ISO/TC 37/SC 4, a working group (WG1-
1) has been established to develop a Linguistic An-
notation Framework (LAF) that can serve as a ba-
sis for harmonizing existing language resources as
well as developing new ones. The overall design of
the architecture and the data model that it will in-
stantiate have been described in Ide et al, 2003. In
this paper we provide a description of the data
model and its instantiations in LAF, in order to
enable annotators to begin to explore how their
schemes will map into the framework.
2 Terms and definitions
The following terms and definitions are used in the
discussion that follows:
Annotation: The process of adding linguistic in-
formation to language data (?annotation of a cor-
pus?) or the linguistic information itself (?an
annotation?), independent of its representation. For
example, one may annotate a document for syntax
using a LISP-like representation, an XML repre-
sentation, etc.
Representation: The format in which the annota-
tion is rendered, e.g. XML, LISP, etc. independent
of its content. For example, a phrase structure syn-
tactic annotation and a dependency-based annota-
tion may both be represented using XML, even
though the annotation information itself is very
different.
Types of Annotation: We distinguish two funda-
mental types of annotation activity:
1. Segmentation: delimits linguistic elements that
appear in the primary data. Including
o  continuous segments (appear contiguously
in the primary data)
o  super- and sub-segments, where groups of
segments will comprise the parts of a
larger segment (e.g., a contiguous word
segments typically comprise a sentence
segment)
o  discontinuous segments (linked continuous
segments)
o  landmarks (e.g. time stamps) that note a
point in the primary data
In current practice, segmental information may
or may not appear in the document containing
the primary data itself. Documents considered
to be read-only, for example, might be seg-
mented by specifying byte offsets into the
primary document where a given segment be-
gins and ends.
2. Linguistic annotation: provides linguistic in-
formation about the segments in the primary
data, e.g., a morpho-syntactic annotation in
which a part of speech and lemma are associ-
ated with each segment in the data. Note that
the identification of a segment as a word, sen-
tence, noun phrase, etc. also constitutes lin-
guistic annotation. In current practice, when it
is possible to do so, segmentation and identifi-
cation of the linguistic role or properties of that
segment are often combined (e.g., syntactic
bracketing, or delimiting each word in the
document with an XML tag that identifies the
segment as a word, sentence, etc.).
Stand-off annotation: Annotations layered over a
given primary document and instantiated in a
document separate from that containing the pri-
mary data. Stand-off annotations refer to specific
locations in the primary data, by addressing byte
offsets, elements, etc. to which the annotation ap-
plies. Multiple stand-off annotation documents for
a given type of annotation can refer to the same
primary document (e.g., two different part of
speech annotations for a given text). There is no
requirement that a single XML-compliant docu-
ment may be created by merging stand-off annota-
tion documents with the primary data; that is, two
annotation documents may specify trees over the
primary data that contain overlapping hierarchies.
3 LAF overview
LAF development has proceeded by first identify-
ing an abstract data model that can formally de-
scribe linguistic annotations, distinct from any
particular representation (as defined in the previous
section). Development of this model has been dis-
cussed extensively within the language engineering
community and tested on a variety of annotation
types (see Ide and Romary, 2001a, 2001b, 2002).
The data model forms the core of the framework
by serving as the reference point for all annotation
representation schemes.
The overall design of LAF is illustrated in Figure
1. The fundamental principle is that the user con-
trols the representation format for linguistic anno-
tations, which is mappable to the data model. This
mapping is accomplished via a rigid ?dump? for-
mat, isomorphic to the data model and intended
primarily for machine rather than human use.
Figure 1. Overall LAF architecture
4 Dump format specification
The data model is built around a clear separation of
the structure of annotations and their content, that
is, the linguistic information the annotation pro-
vides. The model therefore combines a structural
meta-model, that is, an abstract structure shared by
all documents of a given type (e.g. syntactic anno-
tation), and a set of data categories associated with
the various components of the structural meta-
model.
The structural component of the data model is a
feature structure graph capable of referencing n-
dimensional regions of primary data as well as
other annotations. The choice of this model is indi-
cated by its almost universal use in defining gen-
eral-purpose annotation formats, including the
Generic Modeling Tool (GMT) (Ide and Romary,
2001, 2002) and Annotation Graphs (Bird and
Liberman, 2001). A small inventory of logical op-
erations over annotation structures is specified,
which define the model?s abstract semantics. These
operations allow for expressing the following rela-
tions among annotation fragments:
? Parallelism: two or more annotations refer to
the same data object;
? Alternatives: two or more annotations com-
prise a set of mutually exclusive alternatives
(e.g., two possible part-of-speech assignments,
before disambiguation);
? Aggregation: two or more annotations com-
prise a list (ordered) or set (unordered) that
should be taken as a unit.
The feature structure graph is a graph of elemen-
tary structural nodes to which one or more data
category/value pairs are attached, providing the
semantics of the annotation. LAF does not provide
definitions for data categories. Rather, to ensure
semantic coherence we specify a mechanism for
the formal definition of categories and relations,
and provide a Data Category Registry of pre-
defined categories that can be used directly in an-
notations. Alternatively, users may define their
own data categories or establish variants of catego-
ries in the registry; in such cases, the newly defined
data categories will be formalized using the same
format as definitions available in the registry.
5 Implementation
5.1 Dump format
The dump format is instantiated in XML. Struc-
tural nodes are represented as XML elements. The
XML-based GMT will serve as a starting point for
defining the dump format. Its applicability to di-
verse annotation types, including terminology, dic-
tionaries and other lexical data (Ide, et al, 2000),
morphological annotation (Ide and Romary, 2002)
and syntactic annotation (Ide and Romary, 2001b,
2003) demonstrates its generality.
As specified by the LAF architecture, the GMT
implements a feature structure graph. Structural
nodes in the graph are represented with the XML
element <struct>. <brack> and <alt> elements
User-defined
representation
format
User-defined
representation
format
User-definedrepresentationformat DUMPFORMAT DATAMODELMappingspecifica-
tion
are provided as grouping tags to handle aggrega-
tion (grouping) and alternatives (disjunction), as
described above. A <feat> element is used to ex-
press category/value pairs. All of these elements
are recursively nestable. Therefore, hierarchical
relations among annotations and annotation com-
ponents can be expressed via XML syntax via ele-
ment nesting. Other relations, including those
among discontiguous elements, rely on XML?s
powerful inter- and intra-document pointing and
linkage mechanisms. Because all annotations are
stand-off (i.e., in documents separate from the pri-
mary data and other annotations), the same mecha-
nisms are used to associate annotations with both
?raw? and XML-tagged primary data and with
other annotations.
The final XML implementation of the dump format
may differ slightly from the GMT, in particular
where processing concerns (e.g. ease of processing
elements vs. attributes vs. content) and conciseness
are applied. However, in its general form the above
are sufficient to express the information required in
LAF. For examples of morphological and syntactic
annotation in GMT format, see Ide and Romary,
2001a; 2003; and Ide and Romary, 2001b.
5.2 Data Categories
To make them maximally interoperable and con-
sistent with existing standards, RDF schemas can
be used to formalize the properties and relations
associated with data categories. Instances of the
categories themselves will be represented in RDF.
The RDF schema ensures that each instantiation of
the described objects is recognized as a sub-class
of more general classes and inherits the appropriate
properties. Annotations will reference the data
categories via a URL identifying their instantia-
tions in the Data Category Registry itself. The class
and sub-class mechanisms provided in RDFS and
its extensions in OWL will also enable creation of
an ontology of annotation classes and types.
For example, the syntactic feature defined in the
ISLE/MILE format for lexical entries (Calzolari, et
al. 2003) can be represented in RDF as follows
1
:
                                                       
1
 For brevity, this representation does not include the full i n-
formation necessary for the RDF representation.
<rdf:RDF>
<Phrase rdf:ID="Vauxhave">
   <hasSynFeature>
     <SynFeature>
        <hasSynFeatureName rdf:value="aux"/>
        <hasSynFeatureValue rdf:value="have"/>
   </SynFeature>
</hasSynFeature></Phrase>
</rdf:RDF>
Once declared in the Data Category registry, an-
notations or lexicons can reference this object di-
rectly, for example:
<Self rdf:ID="eat1Self">
  <headedBy
   rdf:resource="http://www.DCR /Vauxhave"/>
</Self>
For a full example of the use of RDF-instantiated
data categories, see Ide, et al, in this volume.
Note that RDF descriptions function much like
class definitions in an object-oriented program-
ming language: they provide, effectively, templates
that describe how objects may be instantiated, but
do not constitute the objects themselves. Thus, in a
document containing an actual annotation, several
objects with the same type may be instantiated,
each with a different value. The RDF schema en-
sures that each instantiation is recognized as a sub-
class of more general classes and inherits the ap-
propriate properties.
A formally defined set of categories will have sev-
eral functions: (1) it will provide a precise seman-
tics for annotation categories that can be either
used ?off the shelf? by annotators or modified to
serve specific needs; (2) it will provide a set of ref-
erence categories onto which scheme-specific
names can be mapped; and (3) it will provide a
point of departure for definition of variant or more
precise categories. Thus the overall goal of the
Data Category Registry is not to impose a specific
set of categories, but rather to ensure that the se-
mantics of data categories included in annotations
(whether they exist in the Registry or not) are well-
defined and understood.
6 Conclusion
In this paper we describe the Linguistic Annotation
Framework under development by ISO TC37/SC 4
WG1-1. Its design is intended to allow for, on the
one hand, maximum flexibility for annotators, and.
on the other, processing efficiency and reusability.
This is accomplished by separating user annotation
formats from the exchange/processing format. This
separation ensures that pre-existing annotations are
compatible with LAF, and that users have the free-
dom to design specific schemes to meet their
needs, while still conforming to LAF requirements.
LAF provides for the use of any annotation format
consistent with the feature structure-based data
model that will be used to define the pivot format.
This suggests a future scenario in which annotators
may create and edit annotations in a proprietary
format, transduce the annotations using available
tools to the pivot format for interchange and/or
processing, and if desired, transduce the pivot form
of the annotations (and/or additional annotation
introduced by processing) back into the proprietary
format. We anticipate the future development of
annotation tools that provide a user-oriented inter-
face for specifying annotation information, and
which then generate annotations in the pivot format
directly. Thus the pivot format is intended to func-
tion in the same way as, for example, Java byte
code functions for programmers, as a universal
?machine language? that is interpreted by process-
ing software into an internal representation suited
to its particular requirements. As with Java byte
code, users need never see or manipulate the pivot
format; it is solely for machine consumption.
Part of the work of SC4 WG1-1 is to provide de-
velopment resources, including schemas, design
patterns, and stylesheets, which will enable anno-
tators and software developers to immediately
adapt to LAF. Example mappings, e.g., for XCES-
encoded annotations, will also be provided. In this
way, we hope to realize the goal of harmonized and
reusable resources in the near future.
References
Bird, S. and Liberman, M. (2001). A formal
framework for linguistic annotation. Speech Com-
munication, 33:1-2, 23-60.
Bunt. H. and Romary, L. (2002). Towards Multi-
modal Content Representation.  Proceedings of the
Workshop on International Standards for Termi-
nology and Language Resource Management, Las
Palmas.
Calzolari, N., Bertagna, F., Lenci, A., Monachini,
M., 2003. Standards and best Practice for Multi-
lingual Computational Lexicons and MILE (Multi-
lingual ISLE Lexical Entry), ISLE Computational
Lexicon Working Group deliverables D2.2 ? D3.2,
Pisa.
Ide, N. and Romary, L. (2001a). Standards for
Language Resources, IRCS Workshop on Linguis-
tic Databases, Philadelphia, 141-49.
Ide, N. and Romary, L. (2001b). A Common
Framework for Syntactic Annotation. Proceedings
of ACL'2001, Toulouse, 298-305.
Ide, N. and Romary, L. (2002). Standards for Lan-
guage Resources. Proceedings of the Third Lan-
guage Resources and Evaluation Conference
(LREC), Las Palmas, Canary Islands, Spain, 839-
44.
Ide, N. and Romary, L. (2003). Encoding Syntactic
Annotation. In Abeill?, A. (ed.). Treebanks:
Building and Using Syntactically Annotated Cor-
pora. Dordrecht: Kluwer Academic Publishers (in
press).
Ide, N., Kilgarriff, A., and Romary, L. (2000). A
Formal Model of Dictionary Structure and Content.
Proceedings of Euralex 2000, Stuttgart, 113-126.
Ide, N., Lenci, A., And Calzolari, N. (2003). RDF
Instantiation of ISLE/MILE Lexical Entries. This
volume.
Ide, N., Romary, L, and De la Clergerie, E. (2003).
International Standard for a Linguistic Annotation
Framework. Proceedings of NAACL?03 Workshop
on Software Engineering and Architecture of Lan-
guage Technology Systems (to appear).
RDF Instantiation of ISLE/MILE Lexical Entries
Nancy Ide
Department of Computer
Science
Vassar College
Poughkeepsie, New York
USA 12604-0520
ide@cs.vassar.edu
Alessandro Lenci
Universit? di Pisa
Dipartimento di Linguistica
Via Santa Maria 36
56100 PISA
Italy
lenci@ilc.cnr.it
Nicoletta Calzolari
Istituto di Linguistica
Computazionale, CNR
Area della Ricerca
Via Moruzzi 1 ? 56100 PISA
Italy
glottolo@ilc.cnr.it
Abstract
In this paper we describe the overall
model for MILE lexical entries and
provide an instantiation of the model in
RDF/OWL. This work has been done
with an eye toward the goal of creating a
web-based registry of lexical data
categories and enabling the description of
lexical information by establishing
relations among them, and/or using pre-
defined objects that may reside at various
locations on the web. It is also assumed
that using OWL specifications to enhance
specifications of the ontology of lexical
objects will eventually enable the
exploitation of inferencing engines to
retrieve and possibly create lexical
information on the fly, as suited to
particular contexts. As such, the model
and RDF instantiation provided here are
in line with the goals of ISO TC37 SC4,
and should be fully mappable to the
proposed pivot.
1 Introduction
The eventual vision for computational lexicons is
to enable universal access to sophisticated
linguistic information, which in turn will serve as a
central component for content-based information
management on the web. This demands, first of all,
some standardized means to represent complex
lexical information while retaining the flexibility
required to accommodate diverse approaches to
lexicon organization and use. To this end, the
ISLE
1
 ( International Standards for Language
Engineering) Computational Lexicons Working
Group (CLWG) has designed MILE (Multilingual
ISLE Lexical Entry), a general schema for the
encoding of multilingual lexical information
intended as a meta-entry that can serve as a
standardized representational layer for multilingual
lexical resources. MILE consists of an incremental
definition of object-oriented layers for lexical
description that will enable open and distributed
lexicons, with elements possibly residing in
different sites of the web. The defined lexical
objects are intended for use by lexicon and
application developers to build and target lexical
data at high level of abstraction.
The Resource Definition Framework (RDF) and
the Ontology Web Language (OWL) recently
developed by the World Wide Web Consortium
(W3C) build upon the XML web infrastructure to
enable the creation of a Semantic Web, wherein
web objects can be classified according to their
properties, and the semantics of their relations
(links) to other web objects can be precisely
defined. This in turn will enable powerful
inferencing capabilities that can adapt language
processing applications to particular contexts.
The MILE lexical entry is an ideal structure for
rendering via RDF/OWL. It consists of a hierarchy
of lexical objects that are built up in a layered
fashion by combining atomic data categories via
clearly defined relations. The overall architecture is
modular and layered, as described in Atkins et al
                                                       
1
 ISLE Web Site URL:
lingue.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Page.htm
(2002) and Calzolari et al (2003). On the
horizontal dimension, independent, linked modules
target different dimensions of lexical entries. On
the vertical dimension, the layered organization
allows for varying degrees of granularity in lexical
descriptions, allowing both ?shallow? and ?deep?
lexical representations. RDF?s class hierarchy
mechanism, together with its capacity to specify
named relations among objects in the various
classes, provide a web-based means to represent
this architecture.
2
 Furthermore, because RDF
allows for instantiating objects in any defined class
and subsequently referring to them as the target of
appropriate relations, lexical objects at any level of
specificity can be pre-defined. This provides an
important mechanism for standardization of lexical
elements, since these elements may be pre-defined,
organized in class hierarchies with inherited
properties, and used ?off-the-shelf? as needed.
In this paper we describe the overall model for
MILE lexical entries and provide an instantiation
of the model in RDF/OWL. This work has been
done with an eye toward the goal of creating a
web-based registry of lexical data categories and
enabling the description of lexical information by
establishing relations among them, and/or using
pre-defined objects that may reside at various
locations on the web. It is also assumed that using
OWL specifications to enhance specifications of
the ontology of lexical objects will eventually
enable the exploitation of inferencing engines to
retrieve and possibly create lexical information on
the fly, as suited to particular contexts. As such, the
model and RDF instantiation provided here are in
line with the goals of ISO TC37 SC4, and should
be fully mappable to the proposed pivot.
3
2 The MILE Lexical Model
The MILE Lexical Model (MLM) consists of two
primary components: a mono-lingual component
and a multi-lingual component. The mono-lingual
                                                       
2
 It should be noted that this architecture is analogous to
other data models, including ER diagrams and various
knowledge representation schemes.
3
 We have in fact produced a version of the prototype
ISLE lexical entry in an XML format instantiating the
proposed ISO pivot format (Ide and Romary,
Vassar/LORIA internal document).
component comprises three layers: morphological,
syntactic, and semantic. The overall architecture is
shown in Figure 1.
Within each of the MLM layers, two types of
objects are defined:
1. MILE Lexical Classes (MLC): the main
building blocks of lexical entries. They
formalize the basic lexical notions for each
layer defined in the ISLE project (Calzolari et
al. 2003). The MLM defines each class by
specifying its attributes and the relations
among them. Classes represent notions like
syntactic feature, syntactic phrase, predicate,
semantic relation, synset, etc. Instances of
MLCs are the MILE Data Categories (MDC).
So for instance, NP and VP are data category
instances of the class <Phrase>, and SUBJ and
OBJ are data category instances of the class
<Function>. Each MDC is identified by a URI.
MDC can be either user- defined or reside in a
shared repository.
2. lexical operations: special lexical entities
which allow users to state conditions and
perform complex operations over lexical
entries. They will for instance allow
lexicographers to establish multilingual
conditions, link the slots within two different
syntactic frames, link semantic arguments with
syntactic slots, etc.
The MLM is described with Entity-Relationship
(E-R) diagrams defining the entities of the lexical
model and the way they can be combined to design
an actual lexical entry. As such, the MLM does not
correspond to a specific lexical entry, but is rather
an entry schema corresponding to a lexical meta-
entry. This means that different possible lexical
entries can be designed as instances of the schema
provided by the MLM. Instance entries might
therefore differ for the type of information they
include (e.g. morphological, syntactic, semantic,
monolingual or multilingual, etc.), and for the
depth of lexical description.
Figure 2 depicts the MLM classes and relations for
the syntactic layer (SynU for ?syntactic unit?). Full
definitions for the MLM can be found in the ISLE
document (Calzolari et al 2003).
Figure 1. Overall MILE architecture
MLC:SynU 
 
 
id: xs:anyURY 
comment: xs:string 
example: xs:string 
MLC:SyntacticFrame  
hasSyntacticFrame  
1..* 
MLC:FrameSet  
Composition 
hasFrameSet  
* 
* 
composedBy  
MLC:SemU 
correspondsTo  
* 
CorrespSynUSemU  
Figure 2. Lexical classes and their relations for the syntactic layer (SynU)
morphological 
layer 
syntactic layer 
semantic layer 
linking conditions 
mono-Mile 
multi-MILE 
 
 
 
 
 
multilingual 
correspondence 
conditions 
mono-Mile 
3 RDF instantiation
We have created an RDF schema for the
syntactic layer of the ISLE/MILE lexical entry
and instantiated one entry in several alternative
forms to explore its potential as a representation
for lexical data that can be integrated into the
Semantic Web. The following describes the
various components.
3.1.1 RDF schema for ISLE lexical entries
An RDF schema defines classes of objects and
their relations to other objects. It does not in
itself comprise an instance of these objects, but
simply specifies the properties and constraints
applicable to objects that conform to it.
The RDF schema for the syntactic layer of ISLE
lexical entries can be accessed at
http://www.cs.vassar.edu/~ide/rdf/isle-schema-
v.6. The classes and relations (properties)
defined in the schema correspond to the ER
diagrams in Calzolari et al (2003). The schema
indicates that there is class of objects called
Entry; a property declaration indicates that the
relation hasSynU holds between Entry objects
and SynU  objects. Note that classes can be
defined to be subclasses of other classes, in
which case properties associated with the parent
class are inherited. In the ISLE schema, for
example, the objects Self and SlotRealization
are defined to be sub-classes of PhraseElement,
and the hasPhrase property holds between any
object of type PhraseElement (including its
sub-classes) and objects of type Phrase.
The ISLE RDF schema and entries have been
validated using the ICS-FORTH Validating RDF
Parser (VRP v2.1), which analyzes the syntax of
a given RDF/ XML file according to the RDF
Model and Syntax Specification
4
 and checks
whether the statements contained in both RDF
schemas and resource descriptions satisfy the
semantic constraints derived by the RDF Schema
Specification.
5
                                                       
4
 http://www.w3.org/TR/rdf-syntax-grammar/
5
 http://www.w3.org/TR/rdf-schema/
4  ISLE Lexical Entries and the Data
Category Registry
Appendix A contains three versions of the SynU
description for ?eat?, instantiated as RDF
objects. The first is a ?full? version in which all
of the information is specified, including atomic
values (strings) at the leaves of the tree structure.
The second two versions, rather than specifying
all information explicitly, rely on the existence
of a Data Category Registry (DCR) in which
pre-defined lexical objects are instantiated and
may be included in the entry by a direct
reference.
The potential to develop a Data Category
Registry in which lexical objects are instantiated
in RDF is one of the most important for the
creation of multi-lingual, reusable lexicons. It
allows for the following:
1. specification of a universally accessible,
standard set of morphological, syntactic, and
semantic information that can serve as a
reference for lexicons creators;
2. a fully modular specification of lexical
entities that enables use of all or parts of the
lexical information in the repository as
desired or appropriate, to build more
complex lexical information modules;
3. a template for data category description that
lexicon creators can use to create their own
data categories at any level of granularity;
4. means to reuse lexical specifications in
entries sharing common properties, thereby
eliminating redundancy as well as providing
direct means to identify lexical entries or
sub-entries with shared properties;
5. a universally accessible set of lexical
information categories that may be used in
applications or resources other than lexicons.
Note that the existence of a repository of lexical
objects, instantiated and specified at different
levels of complexity, does not imply that these
objects must be used by lexicon creators. Rather,
it provides a set of ?off the shelf? lexical objects
which either may be used as is, or which provide
a departure point for the definition of new or
modified categories.
The examples in Appendix A provide a general
idea of how a repository of RDF-instantiated
lexical objects can be used. Sample repositories
at three different levels of granularity,
corresponding to the examples in Appendix A,
are given in Appendix B:
1. a repository of enumerated classes for
lexical objects at the lowest level of
granularity; this comprises a definition of
sets of possible values for various lexical
objects. Any object of this type must be
instantiated with one of the listed values.
2. a repository of phrase classes which
instantiate common phrase types, e.g., NP,
VP, etc.
3. a repository of constructions containing
instantiations of common syntactic
constructions (e.g., for verbs which are both
transitive and intransitive, as shown in the
example).
The example entries demonstrate three different
possibilities for the use of information in the
repositories:
1. Entry 1 uses only the enumerated classes in
the LDCR for SynFeatureName and
SynFeatureValue. Note that in this case, the
LDCR only provides a closed list of possible
values, from which the assigned value in the
entry must be chosen.
2. Entry 2 refers to instances of phrase objects
in the LDCR rather than including them in
the entry; this enables referring to a complex
phrase (Vauxhave in the example) rather
than including it directly in the entry, and
provides the potential to reuse the same
instance by reference in the same or other
entries (this is done with N P  in the
example).
3. Entry 3 takes advantage of construction
instances in the LDCR, thus eliminating the
full specification in the entry and, again,
allowing for reuse in other entries.
5 Summary
This exercise is intended to exemplify how RDF
may be used to instantiate lexical objects at
various levels of granularity, which can be used
and reused to create lexical entries within a
single lexicon as well as across lexicons. By
relying on the developing standardized
technologies underlying the Semantic Web, we
ensure universal accessibility and commonality.
Ultimately, lexical objects defined in this way
can be used not only for lexicons, but also in
language processing and other applications.
This example serves primarily as a proof of
concept that may be refined and modified as we
consider in more depth the exact RDF
representation that would best serve the needs of
lexicon creation. However, the potential of
exploiting the developments in the Semantic
Web world for lexicon development should be
clear. More importantly, by situating our work in
the context of W3 standards, we are in step with
ISO TC37/SC4 vision of a Linguistic Annotation
Framework that includes a Data Category
Registry of the type we describe here.
References
Atkins, S., Bel, N., Bertagna, F., Bouillon, P.,
Calzolari, N., Fellbaum, C., Grishman, R., Lenci,
A., MacLeod, C., Palmer, M., Thurmair, G.,
Villegas, M., Zampolli A., 2002. ?From
Resources to Applications. Designing the
Multilingual ISLE Lexical Entry?, Proceedings
of LREC 2002, Las Palmas, Canary Islands,
Spain: 687-693.
Calzolari, N., Bertagna, F., Lenci, A.,
Monachini, M., 2003. Standards and best
Practice for Multilingual Computational
Lexicons and MILE (Multilingual ISLE Lexical
Entry), ISLE Computational Lexicon Working
Group deliverables D2.2 ? D3.2, Pisa.
Appendix A: Sample Entries
ENTRY 1 : Full entry
Highlighted lines refer to objects whose values are constrained in DCR definitions (Appendix B).
<?xml version="1.0"?>
<!-- Sample ISLE lexical Entry for EAT (transitive), SynU only
     Abbreviated syntax version using no pre-defined objects
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                     <headedBy>
                        <Phrase rdf:ID="Vauxhave">
                           <hasSynFeature>
                              <SynFeature>
                                 <hasSynFeatureName rdf:value="aux"/>
                                 <hasSynFeatureValue rdf:value="have"/>
              </SynFeature></hasSynFeature></Phrase></headedBy></Self></hasSelf>
               <hasConstruction>
                  <Construction rdf:ID="eat1Const">
                     <slot>
                        <SlotRealization rdf:ID="NPsubj">
                            <hasFunction rdf:value="Subj"/>
                            <filledBy rdf:value="NP"/>
                     </SlotRealization></slot>
                     <slot>
                        <SlotRealization rdf:ID="NPobj">
                             <hasFunction rdf:value="Obj"/>
                             <filledBy rdf:value="NP"/>
              </SlotRealization></slot></Construction></hasConstruction>
                <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
</SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
ENTRY 2 : Using DCR categories for PHRASE
The highlighted lines refer to pre-instantiated lexical objects. A portion of the LDCR for Phrases is given
in Appendix C. The URL reference is to the actual web address where the object is instantiated.
<?xml version="1.0"?>
<!--
     Sample ISLE lexical Entry for EAT (transitive), SynU only
     Abbreviated syntax version using no pre-defined objects
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                    <headedBy rdf:resource=
                    "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#Vauxhave"/>
               </Self></hasSelf>
               <hasConstruction>
                  <Construction rdf:ID="eat1Const">
                     <slot>
                       <SlotRealization rdf:ID="NPsubj">
                         <hasFunction rdf:value="Subj"/>
                         <filledBy rdf:resource=
                         "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
                     </SlotRealization></slot>
                     <slot>
                       <SlotRealization rdf:ID="NPobj">
                         <hasFunction rdf:value="Obj"/>
                         <filledBy rdf:resource=
                         "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
                </SlotRealization></slot></Construction></hasConstruction>
                <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
</SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
ENTRY 3 : Using DCR categories for CONSTRUCTION
The highlighted lines refer to a pre-instantiated Construction object. A portion of the DCR for
Constructions is given in Appendix B. The URL reference is to the actual web address where the object is
instantiated.
<?xml version="1.0"?>
<!-- Sample ISLE lexical Entry for EAT (transitive)
     Abbreviated syntax version using pre-defined construction
     2002/10/23 Author: Nancy Ide -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#"
         xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Entry rdf:ID="eat1">
   <!-- The SynU for eat1 -->
   <hasSynu rdf:parseType="Resource">
      <SynU rdf:ID="eat1-SynU">
         <example>John ate the cake</example>
         <hasSyntacticFrame>
            <SyntacticFrame rdf:ID="eat1SynFrame">
               <hasSelf>
                  <Self rdf:ID="eat1Self">
                     <headedBy rdf:resource=
                    "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#Vauxhave"/>
                  </Self></hasSelf>
               <hasConstruction rdf:resource=
          "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Constructions#TransIntrans"/>
               <hasFrequency rdf:value="8788" mlc:corpus="PAROLE"/>
            </SyntacticFrame></hasSyntacticFrame></SynU></hasSynu></Entry></rdf:RDF>
Appendix B: DCR definitions
Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification uses the
Ontology Web Language (OWL) to list valid values for objects of the defined class.
<!-- Enumerated classes for ISLE lexical entries v0.1 2002/10/23 Author: Nancy Ide  -->
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:owl ="http://www.w3.org/2002/07/owl#
         xmlns:isle ="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<rdfs:Class rdf:about=
            "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#FunctionType">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>Subj</rdf:li>
      <rdf:li>Obj</rdf:li>
      <rdf:li>Comp</rdf:li>
      <rdf:li>Arg</rdf:li>
      <rdf:li>Iobj</rdf:li>
</rdf:Seq></owl:oneOf></rdfs:Class>
<rdfs:Class rdf:about=
     "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#SynFeatureName">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>tense</rdf:li>
      <rdf:li>gender</rdf:li>
      <rdf:li>control</rdf:li>
      <rdf:li>person</rdf:li>
      <rdf:li>aux</rdf:li>
</rdf:Seq></owl:oneOf> </rdfs:Class>
<rdfs:Class rdf:about=
     "http://www.cs.vassar.edu/~ide/rdf/isle-enumerated-classes#SynFeatureValue">
<owl:oneOf>
   <rdf:Seq>
      <rdf:li>have</rdf:li>
      <rdf:li>be</rdf:li>
      <rdf:li>subject_control</rdf:li>
      <rdf:li>object_control</rdf:li>
      <rdf:li>masculine</rdf:li>
      <rdf:li>feminine</rdf:li>
</rdf:Seq></owl:oneOf></rdfs:Class></rdf:RDF>
Sample LDCR entry for two Phrase objects
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
         xmlns:mlc="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Phrase rdf:ID="NP" rdfs:label="NP"/>
<Phrase rdf:ID="Vauxhave">
   <hasSynFeature>
     <SynFeature>
        <hasSynFeatureName rdf:value="aux"/>
        <hasSynFeatureValue rdf:value="have"/>
      </SynFeature></hasSynFeature></Phrase></rdf:RDF>
Sample LDCR entry for a Construction object
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
          xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
          xmlns="http://www.cs.vassar.edu/~ide/rdf/isle-schema-v.6#">
<Construction rdf:ID="TransIntrans">
    <slot>
       <SlotRealization rdf:ID="NPsubj">
          <hasFunction rdf:value="Subj"/>
          <filledBy rdf:resource=
                   "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
    </SlotRealization></slot>
    <slot>
       <SlotRealization rdf:ID="NPobj">
          <hasFunction rdf:value="Obj"/>
          <filledBy rdf:resource=
                  "http://www.cs.vassar.edu/~ide/rdf/isle-datcats/Phrases#NP"/>
</SlotRealization></slot></Construction></rdf:RDF>
Layering and Merging Linguistic Annotations  Keith Suderman Department of Computer Science Vassar College Poughkeepsie, NY USA suderman@cs.vassar.edu 
Nancy Ide Department of Computer Science Vassar College Poughkeepsie, NY USA ide@cs.vassar.edu   Abstract The American National Corpus and its annotations are represented in a stand-off XML format compliant with the specifi-cations of ISO TC37 SC4 WG1?s Lin-guistic Annotation Framework. Because few systems that enable search and ac-cess of the corpus currently support stand-off markup, the project has devel-oped a SAX like parser that generates ANC data with annotations in-line, in a variety of output formats.  1 Introduction The American National Corpus (ANC) project1 recently released its 2nd release consisting of ap-proximately 22 million words of data, represent-ing a variety of genres of both written and spo-ken data. The corpus is annotated with several layers of automatically produced linguistic in-formation, including sentence and token bounda-ries, part of speech using two different POS tag-sets (a version of the Penn tagset2 and the Biber tagset3), and noun chunks and verb chunks.  ANC primary documents are plain text (UTF-16) documents and are treated as ?read only? resources. All annotations are represented in stand-off XML documents referencing spans in the primary data or other annotation documents, using the XCES4 implementation of the specifi-cations of ISO TC37 SC4?s Linguistic Annota-tion Framework (LAF) (Ide and Romary, 2004). Because few systems that enable search and ac-cess of the corpus currently support stand-off markup, the project has developed a parser that generates ANC data with annotations in-line, in a variety of output formats.                                                 1http:// americannationalcorpus.org 2http://americannationalcorpus.org/FirstRelease/gatetags.txt 3http://americannationalcorpus.org/FirstRelease/Biber-tags.txt 4http://www.xces.org 
This demonstration will show the ?life-cycle? of an ANC document, from acquisition of a document in any of a variety of formats (MS Word, PDF, HTML, etc.) through annotation and final representation in the stand-off format. The ANC tool for merging annotations of the user?s choice with the primary data to produce a single document with in-line annotations will also be demonstrated. 2 ANC Document Life-Cycle Documents to be included in the ANC are ac-quired in many different formats, including MS Word, PDF, HTML, Quark Express, etc. Proc-essing involves a series of steps, which are out-lined below.  2.1 Conversion from original format to ?rudimentary? XML The ANC receives documents in a variety of dif-ferent formats. The first step in processing is to convert the input documents into XCES XML with basic structural annotations included. The most common types of file formats encountered are: ? Microsoft Word. The release of OpenOf-fice 2 has greatly simplified the processing of MS Word documents.  OpenOffice uses XSL and XSLT stylesheets to export files to XML and ships with stylesheets to gen-erate DocBook and TEI-compliant for-mats. We modified the TEI stylesheet to create XCES XML. OpenOffice?s Java API enables us to automate and integrate OpenOffice with later processing steps. ? XML/SGML/HTML. processing of XML files typically involves using XSLT to map element names to XCES. SGML and HTML files typically require pre-processing to render them into valid XML, followed by the application of an XSLT stylesheet to convert them to XCES. 
89
? Quark Express. Several publishers pro-vided documents prepared for publication using Quark Express. Quark documents can be exported in XML, but doing so is worthwhile only if the creator of the document takes advantage of Quark?s style-definition facilities (which was not the case for any of the contributed Quark documents). We therefore exported the documents in RTF; however, many fonts and special characters are improperly ren-dered, and fairly extensive manual editing was therefore required to render the files into a format that could be used. Once ed-ited, the same procedures for MS Word documents are used to generate XCES.  ? PDF.  Bitmap PDF files are unusable for our purposes. Adobe Acrobat can generate plain text from PDF, although this process loses much of the formatting information that would be desirable to retain to facili-tate later processing. In some cases, liga-tures and other special characters are im-properly represented in the text version, and it is not always possible to automati-cally detect and convert them to conform to the original. PDF documents with two or more columns cannot, to our knowl-edge, be extracted without some mis-ordering of the text in the results. ? Other formats. Other formats in which the ANC has acquired documents include plain text and plain text that employed a variety of proprietary markup languages. These documents are processed on a case by case basis, using specialized scripts. 2.2 GATE processing and annotation We use the University of Sheffield?s GATE sys-tem5 for the bulk of ANC document processing and annotation, currently including tokenization, sentence splitting, part of speech tagging, noun chunking, and verb chunking.  Most annotations are produced using GATE?s built-in ANNIE components; we have, however, modified the ANNIE sentence splitter and created several Java plug-ins for use in GATE that perform basic bookkeeping, renaming of annotations/features, moving of annotations between annotation sets etc. We have also developed a scripting language (XORO6) for use with GATE to enable easy bulk                                                 5http://gate.ac.uk 6 http:// americannationalcorpus.org/xoro.html 
processing and re-processing of the entire cor-pus, or to apply selected annotation steps without having to load the files into a GATE corpus or data store. This eases iterative development as documents are added and tools are refined. 2.3 Creation of standoff annotation docu-ments  We have developed several custom processing resources that plug into GATE to generate stand-off annotations in the XCES implementation of the LAF format. The last step in our GATE pipe-line is to create the primary text document and generate all the required standoff annotation files. 3 Standoff Format  The ANC standoff format for annotations is a simple graph representation, consisting of one node set and one, or more, edge sets. The node set is represented by the text itself, with an im-plied node between each character. Each edge set is represented by an XML document and may contain one or more annotation types: logical structure, sentence boundaries, tokens, etc.  An ANC header file for each document is used to associate the source text with the standoff an-notation documents; for example: <cesHeader>   ...   <annotations>  <annotation type="content"    ann.loc="en_4065.txt">   Text content</annotation>  <annotation type="logical"    ann.loc="en_4065-logical.xml">   Logical structure</annotation>      <annotation type="s"    ann.loc="en_4065-s.xml">   Sentence boundaries</annotation>      <annotation type="hepple"    ann.loc="en_4065-hepple.xml">   Hepple POS tags</annotation>      <annotation type="biber"    ann.loc="en_4065-biber.xml">   Biber POS tags</annotation>      <annotation type="vp"    ann.loc="en_4065-vp.xml">   Verb chunks</annotation>      <annotation type="np"    ann.loc="en_4065-np.xml">   Noun chunks</annotation>   </annotations>       ... </cesHeader> ANC annotation documents are marked up with the XCES representation of the nodes and edge sets of the annotation graph. The following shows a segment of the document containing part of speech annotation: 
90
<cesAna xmlns="http://www.xces.org/schema/2003" version="1.0.4"> <struct type="tok" from="4" to="6">    <feat name="base" value="in"/>    <feat name="msd" value="IN"/> </struct> <struct type="tok" from="7" to="11">    <feat name="msd" value="DT"/>    <feat name="base" value="this"/> </struct> <struct type="tok" from="12" to="19">    <feat name="base" value="chapter"/>    <feat name="msd" value="NN"/> </struct> ... </cesAna> Each <struct> element represents an edge in the graph; values of the from and to attributes denote the nodes (between characters in the primary text document) over which the edge spans.  3.1 Annotating discontiguous spans Presently, the ANC includes standoff annotations that reference contiguous spans of data in the original (primary) document. However, we plan to add a wide variety of automatically-produced annotations for various linguistic phenomena to the ANC data, some of which will reference dis-contiguous regions of the primary data, or may reference annotations contained in other standoff documents. This is handled as follows: given an annotation graph, G, we create an edge graph G? whose nodes can themselves be annotated, thereby allowing for edges between the edges of the original annotation graph G. For example, consider the sentence ?My dog has fleas.? The standoff annotations for the to-kens would be:                     1 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 |M|y| |d|o|g| |h|a|s| |f|l|e|a|s|  <struct ? id="t1" from="0" to="2"/> <struct ? id="t2" from="3" to="6"/> <struct ? id="t3" from="7" to="10"/> <struct ? id="t4" from="11" to="16"/> Now consider the dependency tree generated by Minipar7 given in Figure 2. The tree can be represented by annotating the token elements in the standoff annotation document as follows:  <!-- Define some pseudo nodes --> <node type="root" id"E0" ref="t3"/> <node type="clone" id="E2" ref="t2"/>   <!-- Define edges in dependency tree --> <struct type="subj" id="s1"  from="t3" to="E2"/> <struct type="s" id="s2"  from="t3" to="t2"/>                                                 7http://www.cs.ualberta.ca/~lindek/minipar.htm 
<struct type="gen" id="gen"  from="t2" to="t1"/> <struct type="obj" id="obj"  from="t3" to="t4"/> 
 Figure 2. Dependency tree generated by Minipar.8 4 Creating In-line Annotation Docu-ments We have developed an ?XCES Parser?9 that im-plements the org.xml.sax.XMLReader interface to create ANC documents containing in-line an-notations in XML (or any other format).  The XCES parser works as follows: annota-tions to be loaded are selected with the org.xml.sax.XMLReader.setProperty() method. The selected annotation sets are then loaded into a single list in memory and sorted, first by offset and, if the offsets are the same, secondly by annotation type. At present, the or-dering of annotation types are hard coded into the parser; work is underway to make the XCES parser "schema aware" so that embedding speci-fications can be provided by the user. Once the text is loaded and sorted, the appropriate SAX2 events are generated and dispatched to the org.xml.sax.ContentHandler (if one has been reg-istered with the parser) in sequence to simulate the parsing of an XML document. While the parser will allow the programmer to specify an ErrorHandler, DTDHandler, or EntityResolver, at this time no methods from those interfaces will be invoked during parsing. In the current version of the XCES parser, when overlapping annotations are encountered, they are "truncated". For example: <s>Sentence <em>one.</s><s>Sentence</em> two.</s>                                                 8 Image generated by http://ai.stanford.edu/~rion/parsing/minipar_viz.html 9 http://americannationalcorpus.org/tools/index.html#xces-parser 
91
becomes <s>Sentence <em>one.</em></s><s>Sentence two.</s> Work is underway to provide the option to gen-erate milestones in CLIX/HORSE (DeRose, 2004) format to represent overlapping hierar-chies. 4.1 Using the XCES parser The XCES parser can be used in three ways: ? from the command line.  The xces-parser.jar file can be run as a command line program to print XML with inline an-notation to standard output. ? as the XML parser used by other applica-tions.  For example, Saxon10 can take the name of the parser to use to parse the source document as a command line pa-rameter.  This allows us to apply XSLT stylesheets to ANC documents without having to translate them into XML first. ? as a library for use in other Java applica-tions. For example, The ANC Tool11 is a graphical front end to the XCES parser. 4.2 The ANC tool The ANC Tool provides a graphical user inter-face for the XCES parser and is used to convert ANC documents to other formats.  Users specify their choice of annotations to be included. Cur-rently, the ANC Tool can be used to generate the following output formats: ? XML XCES format, suitable for use with the BNC?s XAIRA12 search and access in-terface; ? Text with part of speech tags appended to each word and separated by an under-score; ? WordSmith/MonoConc Pro format.  The ANC Tool uses multiple implementations of the org.xml.sax.DocumentHandler interface, one for each output format, which the XCES parser uses to generate the desired output. Addi-tional output formats can be easily generated by implementing additional interfaces. Of course, if the target application understands annotation graphs, there is no need to bother with the XCES parser or conversion to XML. For ex-ample, we provide several resources for GATE                                                 10 http://saxon.sourceforge.net/ 11 http:// americannationalcorpus.org/tools/anctool.html 12 http://sourceforge.net/projects/xaira 
that permit GATE to open and read ANC docu-ments with standoff annotations, or to load standoff annotations into an already loaded document. 5 Future Work  Currently the XCES parser is a proof of concept rather than a production grade tool. The parser is being augmented to invoke all the appropriate methods from the org.xml.sax.*Handler interfaces and throw the proper SAXExceptions at the appropriate times.  We are also providing for some level of SAX conformance, rather than simply ?doing what Xerces does?. 6 Conclusion The ANC has implemented an efficient pipeline for the processing of text into a corpus of ma-chine usable documents. For some document types this process is almost completely auto-mated and can be regarded as a Corpus-Builder-in-a Box: raw data goes in one end, and a fully annotated corpus with standoff annotations comes out the other.  The use of standoff annotations allows for an accurate representation of the ANC data as pro-vided by the contributors and allows us to easily provide several modular annotation sets that can be included or excluded by the end user as de-sired. By providing a SAX like parser for ANC documents, we are able to leverage a number of available XML tools without the restrictions im-posed by an XML representation of the docu-ments. For users who are not interested in XML or standoff annotations, the plain text version is preserved. 
References  DeRose, Steven J. (2004). Markup Overlap: A Re-view and a Horse. http://www.mulberrytech.com/ Extreme/Proceedings/html/2004/DeRose01/ EML2004DeRose01.html Ide, N., Romary, L. (2004). International standard for a linguistic annotation framework. Journal of Natural Language Engineering, 10:3-4, 211-225. 
92
Proceedings of the Linguistic Annotation Workshop, pages 1?8,
Prague, June 2007. c?2007 Association for Computational Linguistics
GrAF: A Graph-based Format for Linguistic Annotations 
Nancy Ide Department of Computer Science Vassar College Poughkeepsie, New York USA ide@cs.vassar.edu 
Keith Suderman Department of Computer Science Vassar College Poughkeepsie, New York USA suderman@cs.vassar.edu 
  Abstract In this paper we describe the Graph Anno-tation Format (GrAF) and show how it is used represent not only independent lin-guistic annotations, but also sets of merged annotations as a single graph. To demon-strate this, we have automatically trans-duced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph rep-resentation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations. 1 Introduction Although linguistic annotation of corpora has a long history, over the past several years the need for corpora annotated for a wide variety of phe-nomena has come to be recognized as critical for the future development of language processing ap-plications. Considerable attention has been devoted to the development of means to represent annota-tions so that phenomena at different levels can be merged and/or analyzed in combination. A particu-
lar focus has been on the development of standards and best practices for representing annotations that can facilitate ?annotation interoperability?, that is, the use and re-use of annotations produced in dif-ferent formats and by different groups and to en-able easy adaptation to the input requirements of existing annotation tools. In this paper we describe the Graph Annotation Format (GrAF) and show how it is used represent not only independent linguistic annotations, but also sets of merged annotations as a single graph. We also discuss how, as a standard graph represen-tation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC41 and as such, im-plements state-of-the-art best practice guidelines for representing linguistic annotations. This paper has several aims: (1) to show the generality of the graph model for representing lin-guistic annotations; (2) to demonstrate how the graph-based model enables merging and analysis of multi-layered annotations; and (3) to propose as the underlying model for linguistic annotations, due to its generality and the ease with which it is mapped to other formats. To accomplish this, we have automatically transduced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. Discussion of the                                                 1 International Standards Organization Technical Committee 37 Sub-Committee 4 for Language Resource Management. 
1
transduction process brings to light several prob-lems and concerns with current annotation formats and leads to some recommendations for the design of annotation schemes.  2 Overview Graph theory provides a well-understood model for representing objects that can be viewed as a con-nected set of more elementary sub-objects, to-gether with a wealth of graph-analytic algorithms for information extraction and analysis.  As a result, graphs and graph-analytic algorithms are playing an increasingly important role in language data analysis, including finding related web pages (Kleinberg, 1999; Dean and Henzinger, 1999; Brin, 1998; Grangier and Bengio, 2005), patterns of web access (McEneaney, 2001; Zaki, 2002), and the extraction of semantic information from text (Widdows and Dorow, 2002; Krizhanovsky, 2005; Nastase and Szpakowicz, 2006). Recently, there has been work that treats linguistic annotations as graphs (Cui et al, 2005; Bunescu and Mooney, 2006; Nguyen et al, 2007; Gabrilovich and Mark-ovitch, 2007) in order to identify, for example, measures of semantic similarity based on common subgraphs. As the need to merge and study linguistic anno-tations for multiple phenomena becomes increas-ingly important for language analysis, it is essential to identify a general model that can capture the relevant information and enable efficient and effec-tive analysis. Graphs have long been used to de-scribe linguistic annotations, most familiarly in the form of trees (a graph in which each node has a single parent) for syntactic annotation. Annotation Graphs (Bird and Liberman, 2001) have been widely used to represent layers of annotation, each associated with primary data, although the concept was not extended to allow for annotations linked to other annotations and thus to consider multiple annotations as a single graph. More recently, the Penn Discourse TreeBank released its annotations of the Penn TreeBank as a graph, accompanied by an API that provides a set of standard graph-handling functions for query and access 2 . The graph model therefore seems to be gaining ground as a natural and flexible model for linguistic anno-tations which, as we demonstrate below, can repre-                                                2 http://www.seas.upenn.edu/~nikhild/PDTBAPI/ 
sent all annotation varieties, even those that were not originally designed with the graph model as a basis. 2.1 LAF LAF provides a general framework for represent-ing annotations that has been described elsewhere in detail (Ide and Romary, 2004, 2006). Its devel-opment has built on common practice and conver-gence of approach in linguistic annotation over the past 15-20 years. The core of the framework is specification of an abstract model for annotations instantiated by a pivot format, into and out of which annotations are mapped for the purposes of exchange.          
 
Figure 1: Use of the LAF pivot format Figure 1 shows the overall idea for six different user annotation formats (labeled A ? F), which re-quires two mappings for each scheme?one into and one out of the pivot format, provided by the scheme designer. The maximum number of map-pings among schemes is therefore 2n, vs. n2-n mu-tual mappings without the pivot.  To map to the pivot, an annotation scheme must be (or be rendered via the mapping) isomorphic to the abstract model, which consists of (1) a referen-tial structure for associating stand-off annotations with primary data, instantiated as a directed graph; and (2) a feature structure representation for anno-tation content. An annotation thus forms a directed graph referencing n-dimensional regions of pri-mary data as well as other annotations, in which nodes are labeled with feature structures providing the annotation content. Formally, LAF consists of: ? A data model for annotations based on directed graphs defined as follows:  A graph of annota-tions G is a set of vertices V(G) and a set of edges E(G). Vertices and edges may be labeled 
Pivot   
A 
B 
C F 
E 
D 
2
with one or more features. A feature consists of a quadruple (G?, VE, K, V) where, G? is a graph, VE is a vertex or edge in G?, K is the name of the feature and V is the feature value. ?  A base segmentation of primary data that de-fines edges between virtual nodes located be-tween each ?character? in the primary data.3 The resulting graph G is treated as an edge graph G? whose nodes are the edges of G, and which serve as the leaf (?sink?) nodes. These nodes provide the base for an annotation or several layers of annotation. Multiple segmen-tations can be defined over the primary data, and multiple annotations may refer to the same segmentation. ? Serializations of the data model, one of which is designated as the pivot.  ? Methods for manipulating the data model. Note that LAF does not provide specifications for annotation content categories (i.e., the labels describing the associated linguistic phenomena), for which standardization is a much trickier matter. The LAF architecture includes a Data Category Registry (DCR) containing pre-defined data ele-ments and schemas that may be used directly in annotations, together with means to specify new categories and modify existing ones (see Ide and Romary, 2004).  2.2 GrAF  GrAF is an XML serialization of the generic graph structure of linguistic annotations described by LAF. A GrAF document represents the referential structure of an annotation with two XML elements: <node> and <edge>. Both <node> and <edge> elements may be labeled with associated annota-tion information. Typically, annotations describing a given object are associated with <node> ele-ments. Although some annotations, such as de-pendency analyses, are traditionally depicted with labeled edges, GrAF converts these to nodes in order to analyze both the annotated objects and the relations of a graph uniformly. Associating annota-tions with nodes also simplifies the association of an annotation (node) with multiple objects. 
                                                3 A character is defined to be a contiguous byte sequence of a specified length .For text, the default is UTF-16. 
 According to the LAF specification, an annota-tion is itself a graph representing a feature structure. In GrAF, feature structures are encoded in XML according to the specifications of ISO TC37 SC4 document 1884. The feature structure graph associ-ated with a given node is the corresponding  <node> element?s content. Note that the ISO specifications implement the full power of feature structures and define inheritance, unification, and subsumption mechanisms over the structures, thus enabling the representation of linguistic informa-tion at any level of complexity. The specifications also provide a concise format for representing sim-ple feature-value pairs that suffices to represent many annotations, and which, because it is suffi-cient to represent the vast majority of annotation information, we use in our examples. <edge> elements may also be labeled (i.e., as-sociated with a feature structure), but this informa-tion is typically not an annotation per se, but rather information concerning the meaning, or role, of the link itself. For example, in PropBank, when there is more than one target of an annotation (i.e., a node containing an annotation has two or more outgoing edges), the targets may be either co-referents or a ?split argument? whose constituents are not contiguous, in which case the edges collect an ordered list of constituents. In other case, the outgoing edges may point to a set of alternatives. To differentiate the role of edges in such cases, the edge may be annotated. Unlabeled edges default to pointing to an unordered list of constituents.  A base segmentation contains only <sink> elements (i.e., nodes with no outgoing edges), which are a sub-class of <node> elements. As noted above, the segmentation is an edge graph created from edges (spans) defined over primary data. The from and to attributes on <sink> ele-ments in the base segmentation identify the start and end points of these edges in the primary data. Each annotation document declares and associ-ates the elements in its content with a unique namespace. Figure 2 shows several XML frag-ments in GrAF format.  
                                                4 See ISO TC37 SC4 document N188, Feature Structures-Part 1: Feature Structure Representation (2005-10-01), available at http://www.tc37sc4.org/ 
3
Figure 2: GrAF annotations in XML 3 Transduction To test the utility of GrAF for representing annotations of different types produced by different groups, we transduced the Penn TreeBank (PTB), PropBank (PB), NomBank (NB), Penn Discourse TreeBank (PDTB), and TimeBank (TB) annotations of the Wall Street Journal (WSJ) corpus to conform to the specifications of LAF and GrAF. These annotations are represented in several different formats, including both stand-off and embedded formats. The details of the transduction process, although relatively mundane, show that the process is not always trivial. Furthermore, they reveal several seemingly harmless practices that can cause difficulties for transduction to any other format and, therefore, use by others. Consideration of these details is therefore informative for the development of best practice annotation guidelines. The Penn TreeBank annotations of the WSJ are embedded in the data itself, by bracketing compo-nents of syntactic trees. Leaf nodes of the tree are comprised of POS-word pairs; thus, the PTB in-cludes annotations for both morpho-syntax and syntax. To coerce the annotations into LAF/GrAF, it was necessary to  ? extract the text in order to create a primary data document; ? provide a primary segmentation reflecting the tokenization implicit in the PTB; ? separate the morpho-syntactic annotation from the syntactic annotation and render 
each as a stand-off document in GrAF for-mat, with links to the primary segmentation. NB, PB, and PDTB do not annotate primary data, but rather annotate the PTB syntax trees by providing stand-off documents with references to PTB Tree nodes. The format of the NB and PB stand-off annotations is nearly identical; consider for example the following PB annotation:  wsj/00/wsj_0003.mrg 18 18 gold include.01 p---a 14:1,16:1-ARG2 18:0-rel 19:1-ARG1 In GrAF, this becomes           Each line in the PB and NB stand-off files pro-vides a single annotation and therefore interpreted as an annotation node with a unique id. Each anno-tation is associated with a node with an edge to the annotated entity. The PB/NB comma notation (e.g., 14:1,16:1) denotes reference to more than one node in the PTB tree; in GrAF, a dummy node is created to group them so that if, for example, a NB annotation refers to the same node set, in a merged representation a graph minimization algorithm can collapse the dummy nodes while retaining the an-notations from each of PB and NB as separate nodes. Some interpretation was required for the trans-duction, for example, we assume that the sense number and morpho-syntactic descriptor are asso-ciated with the element annotated as ?rel? (vs. the ?gold? status that is associated with the entire proposition), an association that is automatically discernible from the structure. Also, because the POS/word pairs in the PTB leaf nodes have been split into separate nodes, we assume the PB/NB annotations should refer to the POS annotation rather than the string in the primary data, but either option is possible. Given the similarities of the underlying data models for the PDTB and LAF, creating GrAF-compliant structures from the PDTB data is rela-
role: ARG1 
role: ARG2 
cat: NP 
role: rel sns: 01 msd: p---a 
cat: NP cat: VBG cat: PP 
id: pb0003.18 status: gold 
Base segmentation: <seg:sink seg:id="42" seg:start="24"       seg:end="35"/> Annotation over the base segmentation: <msd:node msd:id=?16?>    <msd:f name=?cat? value=?NN?/> </msd:node>  <msd:edge from="msd:16" to="seg:42"/> Annotation over another annotation: <ptb:node ptb:id="23">    <ptb:f name="type" value="NP"/>    <ptb:f name="role" value="-SBJ"/> </ptb:node>  <ptb:edge from="ptb:23"to="msd:16"/>  
4
tively trivial.  This task is simplified even further because the PDTB API allows PDTB files to be loaded in a few simple steps, and allows the pro-grammer to set and query features of the node as well as iterate over the children of the node. So, given a node P that represents the root node of a PDTB tree, an equivalent graph G in GrAF format can be created by traversing the PDTB tree and creating matching nodes and edges in the graph G. Like the PTB, TimeBank annotation is embed-ded in the primary data by surrounding annotated elements with XML tags. TB also includes sets of ?link? tags at the end of each document, specifying relations among annotated elements. The same steps for rendering the PTB into GrAF could be followed for TB; however, this would result in a separate (and possibly different) primary data document. Therefore, it is necessary to first align the text extracted from TB with the primary data derived from PTB, after which the TB XML anno-tations are rendered in GrAF format and associated with the corresponding nodes in the base segmen-tation.  Note that in the current GrAF representation, TB?s tlink, slink, and alink annotations are applied to edges, since they designate relations among nodes. However, further consideration of the na-ture and use of the information associated with these links may dictate that associating it with a node is more appropriate and/or useful. Variations in tokenization exist among the dif-ferent annotations, most commonly for splitting contractions or compounds (?cannot? split into ?can? and ?not?, ?New York-based? split into ?New York?, ?-?, and ?based?, etc.). This can be handled by adding edges to the base segmentation (not necessarily in the same segmentation docu-ment) that cover the relevant sub-spans, and point-ing to the new edge nodes as necessary. Annota-tions may now reference the original span, the en-tire annotation, or any sub-part of the annotation, by pointing to the appropriate node. Alternative segmentations of the same span can be joined by a ?dummy? parent node so that when different anno-tations of the same data are later merged, nodes labeling a sub-graph covering the same span can be combined. For example, in Figure 3, if the PTB segmentation (in gray) is the base segmentation, an alternative segmentation of the same span (in black) is created and associated to the PTB seg-mentation via a dummy node. When annotations 
using each of the different segmentations are merged into a single graph, features associated with any node covering the same sub-tree (in bold) are applied to the dummy node (as a result of graph minimization), thus preserving the commonality in the merged graph.             Figure 3: Alternative segmentations 4 Merging Annotations Once they are in in GrAF format, merging annota-tions of the same primary data, or annotations ref-erencing annotations of the same primary data, in-volves simply combining the graphs for each anno-tation, starting with graph G describing the base segmentation and using the algorithm in Figure 4. Once merged, graph minimization, for which effi-cient algorithms exist (see, e.g., Cardon and Cro-chemore, 1982; Habib et al, 1999), can be applied to collapse identically-labeled nodes with edges to common subgraphs and eliminate dummy nodes such as the one in Figure 3. 
 Figure 4: Graph-merging algorithm 
cat: NP 
cat: PUNC type: hyphen cat: VBG N e w  Y o r k  -  b a s e d 
cat: JJ cat: NNP 
cat: ADJP 
role: alt role: alt 
Given a graph G : for each graph of annotations Gp do   for each vertex vp in Gp do   if vp is not a leaf in Gp then     add vp to G   for each edge (vi, vj) in Gp do   if vj is a leaf in Gp then     find corresponding vertex vg ? G    add a new edge (vi, vg) to G     else  add edge (vi, vj) to G 
5
5 Using the Graphs Because the GrAF format is isomorphic to input to many graph-analytic tools, existing software can be exploited; for example, we have generated graph diagrams directly from a merged graph in-cluding PTB, NB, and PB annotations using GraphViz5, which takes as its input a simple text file representation of a graph. Generating the input files to GraphViz involves simply iterating over the nodes and edges in the graph and printing out a suitable string representation. Figure 5 shows a segment of the GraphViz output generated from the PTB/NB/PB merged annotations (modified slightly for readability).  
 Figure 5: Fragment of GraphViz output Graph-traversal and graph-coloring algorithms can be used to identify and generate statistics concern-ing commonly annotated components in the merged graph. For example, we modified the merging algorithm to "color" the annotated nodes as the graphs are constructed to reflect the source of the annotation (e.g., PTB, NB, PB, etc.) and the annotation content itself. Colors are propagated via outgoing edges down to the base segmentation, so that each node in the graph can be identified by the source and type of annotation applied. The colored graph can then be used to identify common sub-graphs. So, for example, a graph traversal can identify higher-level nodes in PTB that cover the same spans as TB annotations, which in the merged graph are connected to sink nodes (tokens) only, thus effectively ?collapsing? the two annota-tions.  
                                                5 www.graphviz.org 
Traversal of the colored graph can also be used to generate statistics reflecting the interactions among annotations. As a simple example, we gen-erated a list of all nodes annotated as ARG0 by both PB and NB6, the ?related? element (a verb for PB, a nominalization for NB), the PTB annotation, and the set of sink nodes covered by the node, which reveals clusters of verb/nominalization pairs and can be used, for example, to augment semantic lexicons. Similar information generated via graph traversal can obviously provide a wealth of statis-tics that can in turn be used to study interactions among linguistic phenomena. Other graph-analytic algorithms?including common sub-graph analy-sis, shortest paths, minimum spanning trees, con-nectedness, identification of articulation vertices, topological sort, graph partitioning, etc.?may prove to be useful for mining information from a graph of annotations at multiple linguistic levels, possibly revealing relationships and interactions that were previously difficult to observe. We have, for example, generated frequent subgraphs of the PB and NB annotations using the IBM Frequent Subgraph Miner7 (Inokuchi et al, 2005). We are currently exploring several additional applications of graph algorithms to annotation analysis.  The graph format also enables manipulations that may be desirable in order to add information, modify the graph to reflect additional analysis, cor-rect errors, etc. For example, it may be desirable to delete or move constituents such as punctuation and parenthetical phrases under certain circum-stances, conjoin sub-graphs whose sink nodes are joined by a conjunction such as ?and?, or correct PP attachments based on information in the tree.  6 Discussion GrAF provides a serialization of annotations that follows the specifications of LAF and is therefore a candidate to serve as the LAF pivot format. The advantages of a pivot format, and, in general, the use of the graph model for linguistic annotations, are numerous. First, transduction of the various formats into GrAF, as described in section 4, de-manded substantial programming effort; similar effort would be required to transduce to any other                                                 6 The gray nodes in Figure 5 are those that have been ?col-ored? by both PB and NB. 7 http://www.alphaworks.ibm.com/tech/fsm 
6
format, graph-based or not. The role of the LAF pivot format is to reduce this effort across the com-munity by an order of magnitude, as shown in Figure 1. Whether or not GrAF is the pivot, the adoption of the graph model, at least for the pur-poses of exchange, would result in a similar reduc-tion of effort, since graph representations are in general trivially mappable. In addition to enabling the generation of input to a wide range of graph-handling software, the graph model for annotations is isomorphic to representa-tion formats used by emerging annotation frame-works, in particular, UIMA?s Common Analysis System8. It is also compatible with tools such as the PDTBAPI, which is easily generalized to han-dle graphs as well as trees. In addition, the graph model underlies Semantic Web formats such as RDF and OWL, so that any annotation graph is trivially transducable to their serializations (which include not only XML but several others as well), and which, as noted above, has spawned a flurry of research using graph algorithms to extract and ana-lyze semantic information from the web. A final advantage of the graph model is that it provides a sound basis for devising linguistic anno-tation schemes. For example, the PB and NB for-mat, although ultimately mappable to a graph rep-resentation, was not developed with the graph model as a basis. The format is ambiguous as to the relations among the parts of the annotation, in particular, the relation between the information at the beginning of the line providing the status (?gold?), sense number, and morpho-syntactic de-scription, and the rest of the annotation. Human interpretation can determine that the status (proba-bly) applies to the whole annotation, and the sense number and msd apply to the PTB lexical item be-ing annotated, as reflected in the graph-based rep-resentation given in section 3. This somewhat in-nocuous example demonstrates an all-too-pervasive feature of many annotation schemes: reliance on human interpretation to determine structural relations that are implicit in the content of the annotation. Blind automatic transduction of the format to any other format is therefore impos-sible, and the interpretation, although more or less clear in this example, is prone to human error. If the designers of the PB/NB format had begun with a graph-based model?i.e., had been forced to                                                 8 http://www.alphaworks.ibm.com/tech/uima 
?draw the circles and lines??this ambiguity would likely have been avoided. 7 Conclusion We have argued that a graph model for linguistic annotations provides the generality and flexibility required for representing linguistic annotations of different types, and provides powerful and well-established means to analyze these annotations in ways that have been previously unexploited. We introduce GrAF, an XML serialization of the graph model, and demonstrate how it can be used to rep-resent annotations originally made available in widely varying formats. GrAF is designed to be used in conjunction with the Linguistic Annotation Framework, which defines an overall architecture for representing layers of linguistic annotation. We show how LAF stand-off annotations in GrAF format can be easily merged and analyzed, and discuss the application of graph-analytic algo-rithms and tools. Linguistic annotation has a long history, and over the past 15-20 years we have seen increasing attention to the need for standardization as well as continuing development and convergence of best practices to enable annotation interoperability. Dramatic changes in technology, an in particular the development of the World Wide Web, have impacted both the ways in which we represent lin-guistic annotations and the urgency of the need to develop sophisticated language processing applica-tions that rely on them. LAF and GrAF are not based on brand new ideas, but rather reflect and make explicit what appears to be evolving as common best practice methodology.  References A. Cardon and Maxime Crochemore, 1982. Partitioning a graph in O(|A| log2 |V| ).Theoretical Computer Sci-ence, 19(1):85?98.  Akihiro Inokuchi, Takashi Washio, and Hiroshi Mo-toda, 2005. A General Framework for Mining Fre-quent Subgraphs from Labeled Graphs. Fundamenta Informaticae, 66:1-2, 53-82. Andrew A. Krizhanovsky, 2005. Synonym search in Wikipedia: Synarcher. http://www.citebase.org/abstract?id=oai:arXiv.org:cs/0606097 
7
Dat P.T Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka, 2007. Exploiting Syntactic and Semantic Information for Relation Extraction from Wikipedia. IJCAI Workshop on Text-Mining & Link-Analysis (TextLink 2007). Dominic Widdows and Beate Dorow, 2002. A graph model for unsupervised lexical acquisition. Proceed-ings of the 19th International Conference on Compu-tational Linguistics, 1093-1099. Evgeniy Gabrilovich and Shaul Markovitch, 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. Proceedings of the 20th International Joint Conference on Artificial In-telligence, Hyderabad, India. Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan and Tat-Seng Chua, 2005. Question answering passage re-trieval using dependency relations. SIGIR '05: Pro-ceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 400-407. Jeffrey Dean, Monika R. Henzinger, 1999. Finding re-lated pages in the World Wide Web. Computer Net-works, 31(11-16):1467?1479. John E. McEneaney, 2001. Graphic and numerical methods to assess navigation in hypertext. Interna-tional Journal of Human-Computer Studies, 55, 761-786.  Jon M. Kleinberg, 1999. Authoritative sources in a hy-per-linked environment. Journal of the ACM 46(5):604-632.  Michel Habib, Christophe Paul, Laurent Viennot, 1999. Partition refinement techniques: An interesting algo-rithmic tool kit. International Journal of Foundations of Computer Science, 10(2):147?170. Mohammed J. Zaki, 2002. Efficiently mining trees in a forest. Proceedings of SIGKDD?02. Nancy Ide and Laurent Romary, 2004. A Registry of Standard Data Categories for Linguistic Annotation. Proceedings of the Fourth Language Resources and Evaluation Conference (LREC), Lisbon, 135-39. Nancy Ide and Laurent Romary, 2004. International Standard for a Linguistic Annotation Framework. Journal of Natural Language Engineering, 10:3-4, 211-225. Nancy Ide and Laurent Romary, 2006.  Representing Linguistic Corpora and Their Annotations. Proceed-ings of the Fifth Language Resources and Evaluation Conference (LREC), Genoa, Italy. 
Razvan C. Bunescu and Raymond J. Mooney, 2007. Extracting relations from text: From word sequences to dependency paths. In Anne Kao and Steve Poteet (eds.), Text Mining and Natural Language Process-ing, Springer, 29-44. Sergey Brin, 1998. Extracting patterns and relations from the world wide web. Proceedings of the 1998 International Workshop on the Web and Databases, 172-183. Sisay Fissaha Adafre and Maar ten de Rijke, 2005. Dis-covering missing links in Wikipedia. Workshop on Link Discovery: Issues, Approaches and Applica-tions. Stephen Bird and Mark Liberman, 2001. A formal framework for linguistic annotation. Speech Commu-nication, 33:1-2, 23-60. Vivi Nastase and Stan Szpakowicz, 2006. Matching syntactic-semantic graphs for semantic relation as-signment. Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing, 81-88. 
8
Proceedings of the Linguistic Annotation Workshop, pages 184?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Shared Corpora Working Group Report
Adam Meyers
New York
University
New York, NY
meyers
at cs.nyu.edu
Nancy Ide
Vassar College
Poughkeepsie, NY
ide at cs.vassar.edu
Ludovic Denoyer
University of Paris
Paris, France
ludovic.denoyer
at lip6.fr
Yusuke Shinyama
New York
University
New York, NY
yusuke
at cs.nyu.edu
Abstract
We seek to identify a limited amount of rep-
resentative corpora, suitable for annotation
by the computational linguistics annotation
community. Our hope is that a wide vari-
ety of annotation will be undertaken on the
same corpora, which would facilitate: (1)
the comparison of annotation schemes; (2)
the merging of information represented by
various annotation schemes; (3) the emer-
gence of NLP systems that use informa-
tion in multiple annotation schemes; and (4)
the adoption of various types of best prac-
tice in corpus annotation. Such best prac-
tices would include: (a) clearer demarca-
tion of phenomena being annotated; (b) the
use of particular test corpora to determine
whether a particular annotation task can fea-
sibly achieve good agreement scores; (c)
The use of underlying models for represent-
ing annotation content that facilitate merg-
ing, comparison, and analysis; and (d) To
the extent possible, the use of common an-
notation categories or a mapping among cat-
egories for the same phenomenon used by
different annotation groups.
This study will focus on the problem of
identifying such corpora as well as the suit-
ability of two candidate corpora: the Open
portion of the American National Corpus
(Ide and Macleod, 2001; Ide and Suder-
man, 2004) and the ?Controversial? portions
of the WikipediaXML corpus (Denoyer and
Gallinari, 2006).
1 Introduction
This working group seeks to identify a limited
amount of representative corpora, suitable for an-
notation by the computational linguistics annotation
community. Our hope is that a wide variety of anno-
tation will be undertaken on the same corpora, which
would facilitate:
1. The comparison of annotation schemes
2. The merging of information represented by var-
ious annotation schemes
3. The emergence of NLP systems that use infor-
mation in multiple annotation schemes; and
4. The adoption of various types of best practice
in corpus annotation, including:
(a) Clearer demarcation of the phenomena be-
ing annotated. Thus if predicate argu-
ment structure annotation adequately han-
dles relative pronouns, a new project that
is annotating coreference is less likely to
include relative pronouns in their annota-
tion; and
(b) The use of particular test corpora to de-
termine whether a particular annotation
task can feasibly achieve good agreement
scores.
(c) The use of underlying models for repre-
senting annotation content that facilitate
merging, comparison, and analysis.
184
(d) To the extent possible, the use of common
annotation categories or a mapping among
categories for the same phenomenon used
by different annotation groups.
In selecting shared corpora, we believe that the
following issues must be taken into consideration:
1. The diversity of genres, lexical items and lin-
guistic phenomena ? this will ensure that the
corpora will be useful to many different types
of annotation efforts. Furthermore, systems us-
ing these corpora and annotation as data will
be capable of handling larger and more varied
corpora.
2. The availability of the same or similar corpora
in a wide variety of languages;
3. The availability of corpora in a standard format
that can be easily processed ? there should be
mechanisms in place to maintain the availabil-
ity of corpora in this format in the future;
4. The ease in which the corpora can be obtained
by anyone who wants to process or annotate
them ? corpora with free licenses or that are in
the public domain are preferred
5. The degree with which the corpora is represen-
tative of text to be processed ? this criterion can
be met if the corpora is diverse (1 above) and/or
if more corpora of the same kind is available for
processing.
We have selected the following corpora for con-
sideration:1
1. The OANC: the Open sections of the ANC cor-
pus. These are the sections of the American
National Corpus subject to the opened license,
allowing them to be freely distributed. The full
Open ANC (Version 2.0) contains about 14.5
megawords of American English and covers a
variety of genres as indicated by the full path-
names taken from the ANC distribution (where
a final 1 or 2 indicates which DVD the directory
originates from):
1These corpora can be downloaded from:
http://nlp.cs.nyu.edu/wiki/corpuswg/SharedCorpora
? spoken/telephone/switchboard
? written 1/fiction/eggan
? written 1/journal/slate
? written 1/letters/icic
? written 2/non-fiction/OUP
? written 2/technical/biomed
? written 2/travel guides/berlitz1
? written 2/travel guides/berlitz2
? written 1/journal/verbatim
? spoken/face-to-face/charlotte
? written 2/technical/911report
? written 2/technical/plos
? written 2/technical/government
2. The Controversial-Wikipedia-Corpus, a section
of the Wikipedia XML corpus. WikipediaXML
is a corpus derived from Wikipedia, convert-
ing Wikipedia into an XML corpus suitable
for NLP processing. This corpus was selected
from:
? Those articles cited as controversial
according to the November 28, 2006
version of the following Wikipedia page:
http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues
? The talk pages corresponding to these ar-
ticles where Wikipedia users and the com-
munity debate aspects of articles. These
debates may be about content or editorial
considerations.
? Articles in Japanese that are linked to
the English pages (and the associated talk
pages) are also part of our corpus.
2 American National Corpus
The American National Corpus (ANC) project (Ide
and Macleod, 2001; Ide and Suderman, 2004) has
released over 20 million words of spoken and writ-
ten American English, available from the Linguis-
tic Data Consortium. The ANC 2nd release con-
sists of fiction, non-fiction, newspapers, technical
reports, magazine and journal articles, a substan-
tial amount of spoken data, data from blogs and
other unedited web sources, travel guides, techni-
cal manuals, and other genres. All texts are an-
notated for sentence boundaries; token boundaries,
185
lemma, and part of speech produced by two differ-
ent taggers ; and noun and verb chunks. A sub-
corpus of 10 million words reflecting the genre dis-
tribution of the full ANC is currently being hand-
validated for word and sentence boundaries, POS,
and noun and verb chunks. For a complete descrip-
tion of the ANC 2nd release and its contents, see
http://AmericanNationalCorpus.org.
Approximately 65 percent of the ANC data is dis-
tributed under an open license, which allows use and
re-distribution of the data without restriction. The
remainder of the corpus is distributed under a re-
stricted license that disallows re-distribution or use
of the data for commercial purposes for five years
after its release date, unless the user is a member of
the ANC Consortium. After five years, the data in
the restricted portions of the corpus are covered by
the open license.
ANC annotations are distributed as stand-off doc-
uments representing a set of graphs over the primary
data, thus allowing for layering of annotations and
inclusion of multiple annotations of the same type.
Because most existing tools for corpus access and
manipulation do not handle stand-off annotations,
we have developed an easy-to-use tool and user in-
terface to merge the user?s choice of stand-off anno-
tations with the primary data to form a single docu-
ment in any of several XML and non-XML formats,
which is distributed with the corpus. The ANC ar-
chitecture and format is described fully in (Ide and
Suderman, 2006).
2.1 The ULA Subcorpus
The Unified Linguistic Annotation (ULA) project
has selected a 40,000 word subcorpus of the Open
ANC for annotation with several different annota-
tion schemes including: the Penn Treebank, Prop-
Bank, NomBank, the Penn Discourse Treebank,
TimeML and Opinion Annotation.2 This initial sub-
corpus can be broken down as follows:
? Spoken Language
? charlotte: 5K words
? switchboard: 5K words
? letters: 10K words
2Other corpora being annotated by the ULA project include
sections of the Brown corpus and LDC parallel corpora.
? Slate (Journal): 5K words
? Travel guides: 5K words
? 911report: 5K words
? OUP books (Kaufman): 5K words
As the ULA project progresses, the participants
intend to expand the corpora annotated to include a
larger subsection of the OANC. They believe that the
diversity of this corpus make it a reasonable testbed
for tuning annotation schemes for diverse modali-
ties. The Travel guides and some of the slate arti-
cles have already been annotated by the FrameNet
project. Thus the inclusion of these documents fur-
thered the goal of producing a multiply annotated
corpus by one additional project.
It is the recommendation of this working group
that: (1) other groups annotate these same subcor-
pora; and (2) other groups choose additional corpora
from the OANC to annotate and publicly announce
which subsections they choose. We would be happy
to put all such subsections on our website for down-
load. The basic idea is to build up a consensus of
what should be mutually annotated, in part, based
on what groups choose to annotate and to try to get
annotation projects to gravitate toward multiply an-
notated, freely available corpora.
3 The WikipediaXML Corpus
3.1 Why Wikipedia?
The Wikipedia corpus consists of articles in a wide
range of topics written in different genres and
mainly (a) main pages are encyclopedia style arti-
cles; and (b) talk pages are discussions about main
pages they are linked to. The topics of these discus-
sions range from editing contents to disagreements
about content. Although Wikipedia texts are mostly
limited to these two genres, we believe that it is well
suited as training data for natural language process-
ing because:
1. they are lexically diverse (e.g., providing a lot
of lexical information for statistical systems);
2. the textual information is well structured
3. Wikipedia is a large and growing corpus
186
4. the articles are multilingual (cf. section 3.4)
5. and the corpus has various other properties that
many researchers feel would be interesting to
exploit.
To date research in Computational Linguistics us-
ing Wikipedia includes: Automatic derivation of
taxonomy information (Strube and Ponzetto, 2006;
Suchanek et al, 2007; Zesch and Gurevych, 2007;
Ponzetto, 2007); automatic recognition of pairs of
similar sentences in two languages (Adafre and de
Rijke, 2006); corpus mining (Ru?diger Gleim and
Alexander Mehler and Matthias Dehmer, 2007),
Named Entity Recognition (Toral and noz, 2007;
Bunescu and Pasc?a, 2007) and relation extraction
(Nguyen et al, 2007). In addition several shared
tasks have been set up using Wikipedia as the tar-
get corpus including question answering (cf. (D.
Ahn and V. Jijkoun and G. Mishne and K. Mu?ller
and M. de Rijke and S. Schlobach, 2004) and
http://ilps.science.uva.nl/WiQA/); and information
retrieval (Fuhr et al, 2006). Some other interest-
ing properties of Wikipedia that have yet to be ex-
plored to our knowledge include: (1) Most main ar-
ticles have talk pages which discuss them ? perhaps
this relation can be exploited by systems which try
to detect discussions about topics, e.g., searches for
discussions about current events topics; (2) There
are various meta tags, many of which are not in-
cluded in the WikipediaXML (see below), but nev-
ertheless are retrievable from the original HTML
files. Some of these may be useful for various ap-
plications. For example, the levels of disputabil-
ity of the content of the main articles is annotated
(cf. http://en.wikipedia.org/wiki/Wikipedia: Tem-
plate messages/Disputes ).
3.2 Why WikipediaXML?
WikipediaXML (Denoyer and Gallinari, 2006) is an
XML version of Wikipedia data, originally designed
for Information Retrieval tasks such as INEX (Fuhr
et al, 2006) and the XML Document Mining Chal-
lenge (Denoyer and P. Gallinari, 2006). Wikipedi-
aXML has become a standard machine readable
form for Wikipedia, suitable for most Computa-
tional Linguistics purposes. It makes it easy to
identify and read in the text portions of the doc-
ument, removing or altering html and wiki code
that is difficult to process in a standard way. The
WikipediaXML standard has (so far) been used to
process Wikipedia documents written in English,
German, French, Dutch, Spanish, Chinese, Arabic
and Japanese.
3.3 The Controversial Wikipedia Corpus
The English Wikipedia corpus is quite large (about
800K articles and growing). Frozen versions of
the corpus are periodically available for download.
We selected a 5 million word subcorpus which
we believed would be good for a wide variety
of annotation schemes. In particular, we chose
articles listed as being controversial (in the En-
glish speaking world) according to the November
28, 2006 version of the following Wikipedia
page: http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues. We believed that
controversial articles would be more likely than
randomly selected articles to: (1) include interesting
discourse phenomena and emotive language; and
(2) have interesting ?talk? pages (indeed, some of
Wikipedia pages have no associated talk pages).
3.4 The Multi-linguality of Wikipedia
One of the main good points of Wikipedia is the fact
that it is a very large multilingual resource. This
provides several advantages over single-language
corpora, perhaps the clearest such advantage being
the availability of same-genre/same-format text for
many languages. Although, Wikipedia in languages
other than English do not approach 800K articles in
size, there are currently at least 14 languages with
over 100K entries.
It should be clear however, that it is definitely not
a parallel corpus. Although pages are sometimes
translated in their entirety, this is the exception, not
the rule. Pages can be partially translated or summa-
rized into the target language. Individually written
pages can be linked after they are created if it is be-
lieved that they are about the same topic. Also, ini-
tially parallel pages can be edited in both languages,
causing them to diverge. We therefore decided to
do a small small pilot study to attempt to charac-
terize the degree of similarity between English arti-
cles in Wikipedia and articles written in other lan-
guages that have been linked. There are 476 En-
glish Wikipedia articles in the Controversial corpus
187
Classification Frequency
Totally Different 2
Same General Topic 3
Overlapping Topics 11
Same Topics 33
Parallel 1
and 384 associated ?talk? pages. There are approxi-
mately 10,000 articles of various languages that are
linked to the English articles. We asked some En-
glish/Japanese bilingual speakers to evaluate the de-
gree of similarity of as many of the the 305 Japanese
articles that were linked to English controversial ar-
ticles. As of this date, 50 articles were evaluated
with the results summarized as table 3.4.3 These
preliminary results suggest the following:
? Languate-linked Wikipedia would usually be
classified as ?comparable? corpora as 34 (68%)
of the articles were classified as covering the
same topics or being parallel.
? It may be possible to extract a parallel corpus
for a given pair of languages from Wikipedia.
If the above sample is representative, approxi-
mately 2% of the articles are parallel. (While
the existance of one parallel article does not
provide statistically significant evidence that
2% of Wikipedia is parallel, the article?s ex-
istance is still significant.) Furthermore, addi-
tional parallel sentences may be extracted from
some of the other comparable articles using
techniques along the lines of (Adafre and de Ri-
jke, 2006).
Obviously, a more detailed study would be neces-
sary to gain a more complete understanding of how
language-linked articles are related in Wikipedia.4
Such a study would include characterizations of all
linked articles for several languages. This study
could lead to some practical applications, e.g., (1)
the creation of parallel subcorpora for a number of
languages; (2) the selection of an English monolin-
gual subcorpus consisting of articles, each of which
3According to www.wikipedia.org there are currently over
350K Japanese articles.
4Long Wikipedia articles may be split into multiple articles.
This can result in N to 1, or even N to N, matches between
language-linked articles if a topic is split in one language, but
not in another.
is parallel to some article in some other language;
etc.; (3) A compilation of parallel sentences ex-
tracted from comparable articles. While parallel
subcorpora are of maximal utility, finding parallel
sentences could still be extremely useful. (Adafre
and de Rijke, 2006) reports one attempt to automat-
ically select parallel Dutch/English sentences from
language-linked Wikipedia articles with an accuracy
of approximately 45%. Even if higher accuracy can-
not be achieved, this still suggests that it is possible
to create a parallel corpus (of isolated sentences) us-
ing a combination of automatic and manual means.
A human translator would have to go through pro-
posed parallel sentences and eliminate about one
half of them, but would not have to do any man-
ual translation. Selection of corpora for annotation
purposes depends on a number of factors including:
the type of annotation (e.g., a corpus of isolated sen-
tences would not be appropriate for discourse anno-
tation); and possibly an application the annotation
is tuned for (e.g., Machine Translation, Information
Extraction, etc.)
It should be noted that the corpus was chosen for
the controversialness of its articles in the English-
speaking community. It should, however, not be ex-
pected that the same articles will be controversial
in other languages. More generally, the language-
linked Wikipedia articles may have different cultural
contexts depending on the language they are written
in. This is an additional feature that we could test
in a wider study. Furthermore, English pages are
somewhat special because they?re considered as the
common platform and expected to be neutral to any
country. But other lanauages somewhat reflects the
view of each country where the language is spoken.
Indeed, some EN articles are labeled as USA-centric
(cf. http://en.wikipedia.org/wiki/Category:USA-
centric).
Finally, our choice of a corpus based on contro-
versy may have not been the most efficient choice
if our goal had been specifically to find parallel cor-
pora. Just as choosing corpora of articles that are
controversial (in the English-speaking world) may
have helped finding articles interesting to annotate
it is possible that some other choice, e.g., techni-
cal articles, may have helped select articles likely
188
to be translated in full5 Thus further study may be
required to choose the right Wikipedia balance for a
set of priorities agreed upon by the annotation com-
munity.
4 Legal Issues
The American National Corpus has taken great pains
to establish that the open subset of the corpus is
freely usable by the community. The open license6
makes it clear that these corpora can be used for any
reason and are freely distributable.
In contrast, some aspects of the licensing agree-
ment of corpora derived from Wikipedia are unclear.
Wikipedia is governed by the GNU Free Document
License which includes a provision that ?derived
works? are subject to this license as well. While
most academic researchers would be uneffected by
this provision, the effect of this provision is unclear
with respect to commercial products.
Under one view, a machine translation system that
uses a statistical model trained on Wikipedia corpora
is not derived from these corpora. However, on an-
other view it is derived. We contacted Wikipedia
staff by letter asking for clarification on this issue
and received the following response from Michelle
Kinney on behalf of Wikipedia information team:
Wikipedia does not offer legal advice,
and therefore cannot help you decide how
the GNU Free Documentation License
(GFDL) or any other free license applies
to your particular situation. Please con-
tact a local bar association, law society or
similar association of jurists in your legal
jurisdiction to obtain a referral to a com-
petent legal professional.
You may also wish to review the full text
of the GFDL yourself:
http://en.wikipedia.org/wiki/Wikipedia:
Text of the GNU Free Documentation License
5Informally, we observe that linked Japanese/English pairs
of articles about abstract topics (e.g., Adultery, Agnosticsism,
Antisemitism, Capitalism, Censorship, Catholicism) are less
likely to contain parallel sentences than articles about specific
events or people (e.g., Adolf Hitler, Barbara Streisand, The Los
Angeles Riots, etc.)
6http://projects.ldc.upenn.edu/ANC/ANC SecondRelease
EndUserLicense Open.htm
While some candidate corpora are completely in
the public domain, e.g., political speeches and very
old documents, many candidate corpora are under
the GFDL or similar ?copyleft? licenses. These in-
clude other licenses by the GNU organization and
several Creative Commons licenses. It is simply un-
clear how copyleft licenses should be applied to cor-
pora used as data in computational linguistics and
we believe that this is an important legal question
for the Computational Linguistics community. In
addition to Wikipedia, this issue effects a wide vari-
ety of corpora (e.g., other wiki corpora, some of the
corpora being developed by the American National
Corpus, etc.).
However, getting such legal opinions is expensive
and has to be done carefully. Hypothetically, sup-
pose NYU?s legal department wrote an opinion let-
ter stating that products that were not corpora them-
selves were not to be considered derived works for
purposes of some list of copyleft licensing agree-
ments. Furthermore, let?s suppose that several anno-
tation projects relied on this opinion and produced
millions of dollars worth of annotation for one such
corpus. Large corporations still might not use these
corpora unless their own legal departments agreed
with NYU?s opinion. For the annotation community,
this could mean that certain annotation would only
be used by academics and not by industry, and most
annotation researchers would not be happy with this
outcome. It therefore may be worth some effort
on the part of whole NLP community to seek some
clear determinations on this issue.
5 Concluding Remarks
The working group selected two freely distributable
corpora for purposes of annotation. Our goal was to
choose texts for annotation by multiple annotation
research groups and describe the process and the pit-
falls involved in selecting those texts. We, further-
more, aimed to establish a protocol for sharing texts,
so that the same texts are annotated with multiple
annotation schemes. This protocol cannot be setup
carte blanche by this group of researchers. Rather,
we believe that our report in combination with the
discussion at the upcoming meeting of the Lingus-
tic Annotation Workshop will provide the jumpstart
necessary for such a protocol to be put in place.
189
References
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-ing Similar Sentences across Multiple Languages in
Wikipedia. In EACL 2006 Workshop: Wikis and blogsand other dynamic text source, Trento, Italy.
Razvan Bunescu and Marius Pasc?a. 2007. Using En-
cyclopedic Knowledge for Named Entity Disambigua-tion. In Proc. of NAACL/HLT 2007.
D. Ahn and V. Jijkoun and G. Mishne and K. Mu?ller and
M. de Rijke and S. Schlobach. 2004. Using Wikipediaat the TREC QA Track. In Proc. TREC 2004.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
L. Denoyer and A. Vercoustre P. Gallinari. 2006. Reporton the XML Mining Track at INEX 2005 and INEX
2006 : Categorization and Clustering of XML Docu-ments. In Advances in XML Information Retrieval andEvaluation: Fifthth Workshop of the INitiative for theEvaluation of XML Retrieval (INEX?06).
N. Fuhr, M. Lalmas, and S. Malik. 2006. Advances inXML Information Retrieval and Evaluation. In 5th In-ternational Workshop of the Initiative for the Evalua-tion of XML Retrieval, INEX 2006.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american english.In Proceedings of Corpus Linguistics 2001, Lancaster,UK.
N. Ide and K. Suderman. 2004. The american nationalcorpus first release. In Proceedings of LREC 2004,pages 1681?1684, Lisbon, Portugal.
N. Ide and K. Suderman. 2006. Integrating linguistic re-sources: The american national corpus model. In Pro-ceedings of the 6th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.
D. P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Sub-tree Mining for Relation Extraction from Wikipedia.
In Proc. of NAACL/HLT 2007.
Simone Paolo Ponzetto. 2007. Creating a KnowledgeBase From a Collaboratively Generated Encyclopedia.
In Proc. of NAACL/HLT 2007.
Ru?diger Gleim and Alexander Mehler and MatthiasDehmer. 2007. Web Corpus Mining by instance of
Wikipedia. In Proc. 2nd Web as Corpus Workshop atEACL 2006.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia. In Proc.of AAAI-06, pages 1419?1424.
F. M. Suchanek, G. Kasneci, and G.Weikum. 2007.
YAGO: A core of semantic knowledge. In Proc. ofWWW-07.
Antonio Toral and Rafael Mu noz. 2007. A proposal toautomatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. In Proc. ofNAACL/HLT 2007.
Torsten Zesch and Iryna Gurevych. 2007. Analysis ofthe Wikipedia Category Graph for NLP Applications.
In Proc of NAACL-HLT 2007 Workshop: TextGraphs-2.
190
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Making Sense of Word Sense Variation
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
Center for Computational Learning Systems
Columbia University
New York, NY, USA
(becky@cs|ansaf@ccls).columbia.edu
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Abstract
We present a pilot study of word-sense an-
notation using multiple annotators, relatively
polysemous words, and a heterogenous cor-
pus. Annotators selected senses for words in
context, using an annotation interface that pre-
sented WordNet senses. Interannotator agree-
ment (IA) results show that annotators agree
well or not, depending primarily on the indi-
vidual words and their general usage proper-
ties. Our focus is on identifying systematic
differences across words and annotators that
can account for IA variation. We identify three
lexical use factors: semantic specificity of the
context, sense concreteness, and similarity of
senses. We discuss systematic differences in
sense selection across annotators, and present
the use of association rules to mine the data
for systematic differences across annotators.
1 Introduction
Our goal is to grapple seriously with the natural
sense variation arising from individual differences in
word usage. It has been widely observed that usage
features such as vocabulary and syntax vary across
corpora of different genres and registers (Biber,
1995), and that serve different functions (Kittredge
et al, 1991). Still, we are far from able to pre-
dict specific morphosyntactic and lexical variations
across corpora (Kilgarriff, 2001), much less quan-
tify them in a way that makes it possible to apply
the same analysis tools (taggers, parsers) without re-
training. In comparison to morphosyntactic proper-
ties of language, word and phrasal meaning is fluid,
and to some degree, generative (Pustejovsky, 1991;
Nunberg, 1979). Based on our initial observations
from a word sense annotation task for relatively pol-
ysemous words, carried out by multiple annotators
on a heterogeneous corpus, we hypothesize that dif-
ferent words lead to greater or lesser interannota-
tor agreement (IA) for reasons that in the long run
should be explicitly modelled in order for Natural
Language Processing (NLP) applications to handle
usage differences more robustly. This pilot study is
a step in that direction.
We present related work in the next section, then
describe the annotation task in the following one. In
Section 4, we present examples of variation in agree-
ment on a matched subset of words. In Section 5
we discuss why we believe the observed variation
depends on the words and present three lexical use
factors we hypothesize to lead to greater or lesser
IA. In Section 6, we use association rules to mine
our data for systematic differences among annota-
tors, thus to explain the variations in IA. We con-
clude with a summary of our findings goals.
2 Related Work
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al, 2005)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al, 2005). Differences in IA
and system performance across part-of-speech have
been examined, as in (Ng et al, 1999; Palmer et al,
2
Word POS No. senses No. occurrences
fair Adj 10 463
long Adj 9 2706
quiet Adj 6 244
land Noun 11 1288
time Noun 10 21790
work Noun 7 5780
know Verb 11 10334
say Verb 11 20372
show Verb 12 11877
tell Verb 8 4799
Table 1: Ten Words
2005). Pedersen (Pedersen, 2002a) examines varia-
tion across individual words in evaluating WSD sys-
tems, but does not attempt to explain it.
Factors that have been proposed as affecting
human or system sense disambiguation include
whether annotators are allowed to assign multilabels
(Veronis, 1998; Ide et al, 2002; Passonneau et al,
2006), the number or granularity of senses (Ng et al,
1999), merging of related senses (Snow et al, 2007),
sense similarity (Chugur et al, 2002), sense perplex-
ity (Diab, 2004), entropy (Diab, 2004; Palmer et
al., 2005), and in psycholinguistic experiments, re-
actions times required to distinguish senses (Klein
and Murphy, 2002; Ide and Wilks, 2006).
With respect to using multiple annotators, Snow
et al included disambiguation of the word
president?a relatively non-polysemous word with
three senses?in a set of tasks given to Amazon Me-
chanical Turkers, aimed at determining how to com-
bine data from multiple non-experts for machine
learning tasks. The word sense task comprised 177
sentences taken from the SemEval Word Sense Dis-
ambiguation Lexical Sample task. Majority voting
among three annotators achieve 99% accuracy.
3 The Annotation Task
The Manually Annotated Sub-Corpus (MASC)
project is creating a small, representative corpus
of American English written and spoken texts
drawn from the Open American National Cor-
pus (OANC).1 The MASC corpus includes hand-
validated or manually produced annotations for a va-
riety of linguistic phenomena. One of the goals of
1http://www.anc.org
Figure 1: MASC word sense annotation tool
the project is to support efforts to harmonize Word-
Net (Miller et al, 1993) and FrameNet (Ruppen-
hofer et al, 2006), in order to bring the sense distinc-
tions each makes into better alignment. As a start-
ing sample, we chose ten fairly frequent, moderately
polysemous words for sense tagging, targeting in
particular words that do not yet exist in FrameNet, as
well as words with different numbers of senses in the
two resources. The ten words with part of speech,
number of senses, and occurrences in the OANC
are shown in Table 1. One thousand occurrences
of each word , including all occurrences appear-
ing in the MASC subset and others semi-randomly2
chosen from the remainder of the 15 million word
OANC, were annotated by at least one annotator of
six undergraduate annotators at Vassar College and
Columbia University.
Fifty occurrences of each word in context were
sense-tagged by all six annotators for the in-depth
study of inter-annotator agreement (IA) reported
here. We have just finished collecting annotations
of fifty new occurrences. All annotations are pro-
2The occurrences were drawn equally from each of the
genre-specific portions of the OANC.
3
duced using the custom-built interface to WordNet
shown in Figure 1: the sentence context is at the top
with the word in boldface (fair), a comment region
below that allows the annotator to keep notes, and
a scrollable area below that shows three of the ten
WordNet senses for ?fair.?
4 Observation: Varying Agreement,
depending on Lexical Items
We expected to find varying levels of interannotator
agreement (IA) among all six annotators, depend-
ing on obvious grouping factors such as the part of
speech, or the number of senses per word. We do
find widely varying levels of agreement, but as de-
scribed here, most of the variation does not depend
on these a priori factors. Inherent usage properties
of the words themselves, and systematic patterns of
variation across annotators, seem to be the primary
factors, with a secondary effect of part of speech.
In previous work (Passonneau, 2004), we have
discussed why we use Krippendorff?s ? (Krippen-
dorff, 1980), and for purposes of comparison we
also report Cohen?s ?; note the similarity in values3.
As with the various agreement coefficients that fac-
tor out the agreement that would occur by chance,
values range from 1 for perfect agreement and -1
for perfect opposition, to 0 for chance agreement.
While there are no hard and fast criteria for what
constitutes good IA, Landis and Koch (Landis and
Koch, 1977) consider values between 0.40 and 0.60
to represent moderately good agreement, and values
above 0.60 as quite good; Krippendorff (Krippen-
dorff, 1980) considers values above 0.67 moderately
good, and values above 0.80 as quite good. (cf. (Art-
stein and Poesio, 2008) for discussion of agreement
measurement for computational linguistic tasks.)
Table 2 shows IA for a pair of adjectives, nouns
and verbs from our sample for which the IA scores
are at the extremes (high and low) in each pair: the
average delta is 0.24. Note that the agreement de-
creases as part-of-speech varies from adjectives to
nouns to verbs, but for all three parts-of-speech,
there is a wide spread of values. It is striking, given
that the same annotators did all words, that one in
each pair has relatively better agreement.
3? handles multiple annotators; Arstein and Poesio (Artstein
and Poesio, 2008) propose an extension of ? (?3) we use here.
POS Word ? ? No. senses Used
adj long 0.6664 0.6665 9 8
fair 0.3546 0.3593 10 5
noun work 0.5359 0.5358 7 7
land 0.2627 0.2671 11 8
verb tell 0.4152 0.4165 8 8
show 0.2636 0.2696 12 11
Table 2: Varying interannotator agreement across words
The average of the agreement values shown in
Table 2 (?=0.4164; ?=0.4191) is somewhat higher
than the average 0.317 found for 191 words anno-
tated for WordNet senses in (Ng et al, 1999), but
lower than their recomputed ? of 0.85 for verbs, af-
ter they reanalyzed the data to merge senses for 42
of the verbs. It is widely recognized that achieving
high ? scores (or percent agreement between anno-
tators, cf. (Palmer et al, 2005)) is difficult for word
sense annotation.
Given that the same annotators have higher IA on
some words, and lower on others, we hypothesize
that it is the word usages themselves that lead to the
high deltas in IA for each part-of-speech pair. We
discuss the impact of three factors on the observed
variations in agreement:
1. Greater specificity in the contexts of use leads to
higher agreement
2. More concrete senses give rise to higher agreement
3. A sense inventory with closely related senses
(e.g., relatively lower average inter-sense similarity
scores) gives rise to lower agreement
5 Explanatory Factors
First we list factors that can not explain the variation
in Table 2. Then we turn to examples illustrating
factors that can, based on a manual search for exam-
ples of two types: examples where most annotators
agreed on a single sense, and examples where two
or three senses were agreed upon by multiple anno-
tators. Later we how how we use association rules
to detect these two types of cases automatically. For
these examples, the WordNet sense number is shown
(e.g., WN S1) with an abbreviated gloss, followed
by the number of annotators who chose it.
4
5.1 Ruled Out Factors
It appears that neither annotator expertise, a word?s
part of speech, the number of senses in WordNet,
the number of senses annotators find in the corpus,
nor the nature of the distribution across senses, can
account for the variation in IA in Table 2. All six
annotators used the same annotation tool, the same
guidelines, and had already become experienced in
the word sense annotation task.
The six annotators all exhibit roughly the same
performance. We measure an individual annotator?s
performance by computing the average pairwise IA
(IA2). For every annotator Ai, we first compute the
pairwise agreement of Ai with every other annota-
tor, then average. This gives us a measure for com-
paring individual annotators with each other: an-
notators that have a higher IA2 have more agree-
ment, on average, with other annotators. Note that
we get the same ranking of individuals when for
each annotator, we calculate how much the agree-
ment among the five remaining annotators improves
over the agreement among all six annotators. If
agreement improves relatively more when annota-
tor Ai is dropped, then Ai agrees less well with the
other five annotators. While both approaches give
the same ranking among annotators, IA2 also pro-
vides a number that has an interpretable value.
On a word-by-word basis, some annotators do
better than others. For example, for long, the best
annotator (A) has IA2=0.79, and the worst (F) has
0.44. However, across ten words annotated by all
six, the average of their IA2 is 0.39 with a standard
deviation of 0.037. F at 0.32 is an outlier; apart from
F, annotators have similar IA across words.
Table 2 lists the distribution of available senses
in WordNet for the four words (column 4), and the
number of senses used (column 5). The words work
and tell have relatively fewer senses (seven and eig-
ith) compared with nine through twelve for the other
words. However, neither the number (or proportion)
of senses used by annotators, nor the distribution
across senses, has a significant correlation with IA,
as given by Pearson?s correlation test.
5.2 Lexical Use Factors
Underspecified contexts lead to ambiguous word
meanings, a factor that has been recognized as be-
ing associated with polysemous contexts (Palmer et
al., 2005). We find that the converse is also true:
relatively specific contexts reduce ambiguity.
The word long seems to engender the greatest IA
primarily because the contexts are concrete and spe-
cific, with a secondary effect that adjectives have
higher IA overall than the other parts of speech. Sen-
tences such as (1.), where a specific unit of temporal
or spatial measurement is mentioned (months), re-
strict the sense to extent in space or time.
1. For 18 long months Michael could not find a job.
WN S1. temporal extent [N=6 of 6]
In the few cases where annotators disagree on
long, the context is less specific or less concrete. In
example (2.), long is predicated of the word chap-
ter, which has non-concrete senses that exemplify
a certain type of productive polysemy (Pustejovsky,
1991). It can be taken to refer to a physical object
(a specific set of pages in an actual book), or a con-
ceptual object (the abstract literary work). The ad-
jective inherits this polysemy. The three annotators
who agree on sense two (spatial extent) might have
the physical object sense in mind; the two who select
sense one (temporal extent) possibly took the point
of view of the reader who requires a long time to
read the chapter.
2. After I had submitted the manuscript my editor at
Simon Schuster had suggested a number of cuts to
streamline what was already a long and involved
chapter on Brians ideas.
WN S2.spatial extent [N=3 of 6],
WN S1.temporal extent [N=2 of 6],
WN S9.more than normal or necessary [N=1 of 6]
Several of the senses of work are concrete, and
quite distinct: sense seven, ?an artist?s or writer?s
output?; sense three, ?the occupation you are paid
for?; sense five, ?unit of force in physics?; sense
six, ?the place where one works.? These are the
senses most often selected by a majority of annota-
tors. Senses one and two, which are closely related,
are the two senses most often selected by different
annotators for the same instance. They also repre-
sent examples of productive polysemy, here between
an activity sense (sense one) and a product-of-the-
activity sense (sense two). Example (3) shows a sen-
5
tence where the verb perform restricts the meaning
to the activity sense, which all annotators selected.
3. The work performed by Rustom and colleagues
suggests that cell protrusions are a general mech-
anism for cell-to-cell communication and that in-
formation exchange is occurring through the direct
membrane continuity of connected cells indepen-
dently of exo- and endocytosis.
WN S1.activity of making something [N=6 of 6]
In sentence (4.), four annotators selected sense
one (activity) and two selected sense two (result):
4. A close friend is a plastic surgeon who did some
minor OK semi-major facial work on me in the past.
WN S1.activity directed toward making something
[N=4 of 6],
WN S2.product of the effort of a person or thing
[N=2 of 6]
For the word fair, if five or six annotators agree,
often they have selected sense one??free of fa-
voritism or bias??as in example (5). However, this
sense is often selected along with sense two??not ex-
cessive or extreme?as in example (6). Both senses
are relatively abstract.
5. By insisting that everything Microsoft has done is
fair competition they risk the possibility that the
public if it accepts the judges finding to the con-
trary will conclude that Microsoft doesn?t know the
difference.
WN S1.free of favoritism/bias [N=6 of 6]
6. I I think that?s true I can remember times my parents
would say well what do you think would be a fair
punishment.
WN S1.free of favoritism/bias [N=3 of 6],
WN S2.not excessive or extreme [N=3 of 6]
Example (7) illustrates a case where all annota-
tors agreed on a sense for land. The named entity
India restricts the meaning to sense five, ?territory
occupied by a nation.? Apart from a few such cases
of high consensus, land seems to have low agree-
ment due to senses being so closely related they can
be merged. Senses one and seven both have to do
with property (cf. example (8))., senses three and
five with geopolitical senses, and senses two and
four with the earth?s surface or soil. If these three
pairs of senses are merged into three senses, the IA
goes up from 0.2627 to 0.3677.
7. India is exhilarating exhausting and infuriating a
land where you?ll find the practicalities of daily life
overlay the mysteries that popular myth attaches to
India.
WN S5.territory occupied by a nation [N=6 of 6]
8. uh the Seattle area we lived outside outside of the
city in the country and uh we have five acres of land
up against a hillside where i grew up and so we did
have a garden about a one a half acre garden
WN S4.solid part of the earth?s surface [N=1 of 6],
WN S1.location of real estate [N=2 of 6],
WN S7.extensive landed property [N=3 of 6]
Examples for tell and show exhibit the same trend
in which agreement is greater when the sense is
more specific or concrete, which we illustrate briefly
with show. Example (9) describes a specific work of
art, an El Greco painting, and agreement is universal
among the six annotators on sense 5. In contrast, ex-
ample (10) shows a fifty-fifty split among annotators
for a sentence with a very specific context, an ex-
periment regarding delivery of a DNA solution, but
where the sense is abstract rather than concrete: the
argument of show is an abstract proposition, namely
a conclusion is drawn regarding what the experiment
demonstrates, rather than a concrete result such as a
specific measurement, or statistical outcome. Sense
two in fact contains the word ?experiment? that oc-
curs in (9), which presumably biases the choice of
sense two. Impressionistically, senses two and three
appear to be quite similar.
9. El Greco shows St. Augustine and St. Stephen,
in splendid ecclesiastical garb, lifting the count?s
body.
WN S5.show in, or as in, a picture, N=6 of 6
10. These experiments show that low-volume jet
injection specifically targeted delivery of a DNA
solution to the skin and that the injection paths did
not reach into the underlying tissue.
WN S2.establish the validity of something, as by
an example, explanation or experiment, N=3 of 6
WN S3.provide evidence for, N=3 of 6
6
5.3 Quantifying Sense Similarity
Application of an inter-sense similarity measure
(ISM) proposed in (Ide, 2006) to the sense invento-
ries for each of the six words supports the observa-
tion that words with very similar senses have lower
IA scores. ISM is computed for each pair in a given
word?s sense inventory, using a variant of the lesk
measure (Banerjee and Pedersen, 2002). Agglom-
erative clustering may then be applied to the result-
ing similarity matrix to reveal the overall pattern of
inter-sense relations.
ISMs for senses pairs of long, fair, work, land,
tell, and show range from 0 to 1.44.4 We compute
a confusion threshhold CT based on the ISMs for all
250 sense pairs as
CT = ?A + 2?A
where A is the sum of the ISMs for the six words? 250
sense pairs.
Table 3 shows the ISM statistics for the six words. The
values show that the ISMs for work and long are signifi-
cantly lower than for land and fair. The ISMs for the two
verbs in the study, show and tell, are distributed across
nearly the same range (0 - 1.38 and 0 - 1.22, respec-
tively), despite substantially lower IA scores for show.
However, the ISMs for three of show?s sense pairs are
well above CT , vs. one for tell, suggesting that in addi-
tion to the range of ISMs for a given word?s senses, the
number of sense pairs with high similarity contributes to
low IA. Overall, the correlation between the percentage
of ISMs above CT for the words in this study and their
IA scores is .8, which supports this claim.
POS Word Max Mean Std. Dev > CT
adj long .71 .28 .18 0
fair 1.25 .28 .34 5
noun work .63 .22 .16 0
land 1.44 .17 .29 3
verb tell 1.22 .15 .25 1
show 1.38 .18 .27 3
Table 3: ISM statistics
6 Association Rules
Association rules express relations among instances
based on their attributes. Here the attributes of interest are
4Note that because the scores are based on overlaps among
WordNet relations, glosses, examples, etc., there is no pre-
defined ceiling value for the ISMs. For the words in this study,
we compute a ceiling value by taking the maximum of the ISMs
for each of the 57 senses with itself, 4.85 in this case.
the annotators who choose one sense versus those who
choose another. Mining association rules to find strong
relations has been studied in many domains (see for in-
stance (Agrawal et al, 1993; Zaki et al, 1997; Salleb-
Aouissi et al, 2007)). Here we illustrate how association
rules can be used to mine relations such as systematic dif-
ferences in word sense choices across annotators.
An association rule is an expression C1 ? C2, where
C1 and C2 express conditions on features describing the
instances in a dataset. The strength of the rules is usually
evaluated by means of measures such as Support (Supp)
and Confidence (Conf). Where C, C1 and C2 express con-
ditions on attributes:
? Supp(C) is the fraction of instances satisfying C
? Supp(C1 ? C2) = Supp(C1 ? C2)
? Conf(C1 ? C2) = Supp(C1 ? C2)/Supp(C1)
Given two thresholds MinSupp (for minimum support)
and MinConf (for minimum confidence), a rule is strong
when its support is greater than MinSupp and its confi-
dence greater than MinConf. Discovering strong rules is
usually a two-step process of retrieving instances above
MinSupp, then from these retrieving instances above
MinConf.
The types of association rules to mine can include
any attributes in either the left hand side or the right
hand side of rules. In our data, the attributes consist
of the word sense assigned by annotators, the annota-
tors, and the instances (words). In order to find rules
that relate annotators to each other, the dataset must be
pre-processed to produce flat (two-dimensional) tables.
Here we focus on annotators to get a flat table in which
each line corresponds to an annotator/sense combination:
Annotator Sense. We denote the six annotators as A1
through A6, and word senses by WordNet sense number.
Here are 15 unique pairs of annotators, so one way
to look at where agreements occur is to determine how
many of these pairs choose the same sense with non-
negligible support and confidence. Tell has much bet-
ter IA than show, but less than long and work. We
would expect association rules among many pairs of
annotators for some but not all of its senses. We
find 11 pairs of rules of the form Ai Tell:Sense1 ?
Aj Tell:Sense1, Aj Tell:Sense1 ? Ai Tell:Sense1,
indicating a bi-directional relationship between pairs of
annotators choosing the same sense, with support rang-
ing from 14% to 44% and confidence ranging from 37%
to 96%. This indicates good support and confidence for
many possible pairs
Our interest here is primarily in mining for systematic
disagreements thus we now turn to pairs of rules where
in one rule, an attribute Annotator Sensei occurs in the
left hand side, and a distinct attributeAnnotator Sensej
occurs in the right. Again, we are especially interested in
7
i j Supp(%) Confi(%) Confj(%)
Ai fair.S1 ? Aj fair.S2
A3 A6 20 100 32.3
A5 A6 20 100 31.2
A1 A2 16 80 40
Ai show.S2 ? Aj show.S3
A1 A3 32 84.2 69.6
A5 A3 24 63.2 80.0
A4 A3 22 91.7 57.9
A4 A6 14 58.3 46.7
A4 A2 12 60.0 50.0
A5 A2 12 60.0 40.0
Ai show.S5 ? Aj show.S10
A1 A6 12 85.7 40.0
A5 A2 10 83.3 50.0
A4 A2 10 83.3 30.5
A4 A6 10 71.4 38.5
A3 A2 8 66.7 40.0
A3 A6 8 57.1 40.0
A5 A6 8 57.1 40.0
Table 4: Association Rules for Systematic Disagreements
bi-directional cases where there is a corresponding rule
with the left and right hand clauses reversed. Table 4
shows some general classes of disagreement rules using a
compact representation with a bidirectional arrow, along
with a table of variables for the different pairs of annota-
tors associated with different levels of support and confi-
dence.
For fair, Table 4 summarizes three pairs of rules with
good support (16-20% of all instances) in which one an-
notator chooses sense 1 of fair and another chooses sense
2: A3 and A5 choose sense 1 where A6 chooses sense 2,
and A1 chooses sense 1 where A2 chooses sense 2. The
confidence varies for each rule, thus in 100% of cases
where A6 selects sense 2 of fair, A3 selects sense 1, but
in only 32.3% of cases is the converse true. Example (6)
where half the annotators picked sense 1 of fair and half
picked sense 2 falls into the set of instances covered by
these rules. The rules indicate this is not isolated, but
rather part of a systematic pattern of usage.
The word land had the lowest interannotator agree-
ment among the six annotators, with eight of eleven
senses were used overall (cf. Table 2). Here we did not
find pairs of rules in which distinct Annotator Sense
attributes that occur in the left and right sides of one rule
occur in the right and left sides of another rule. For show,
Table 4 illustrates two systematic divisions among
groups of annotators. With rather good support rang-
ing from 12% to 32%, senses 2 and 3 exhibit a system-
atic difference: annotators A1, A4 and A5 select sense
2 where annotators A3, A3 and A6 select sense 3. Sim-
ilarly, senses 5 and 10 exhibit a systematic difference:
with a more modest support of 8% to 12%, annotators
A1, A3, A4 and A5 select sense 5 where annotators A2
and A6 select sense 10.
7 Conclusion
We have performed a sense assignment experiment
among multiple annotators for word occurrences drawn
from a broad range of genres, rather than the domain-
specific data utilized in many studies. The selected words
were all moderately polysemous. Based on the results,
we identify several factors that distinguish words with
high vs. low interannotator agreement scores. We also
show the use of association rules to mine the data for
systematic annotator differences. Where relevant, the re-
sults can be used to merge senses, as done in much pre-
vious work, or to identify internal structure within a set
of senses, such as a word-based sense-hierarchy. In our
future work, we want to develop the use of association
rules in several ways. First, we hope to fully automated
the process of finding systematic patterns of difference
across annotators. Second, we hope to extend their use
to mining associations among the representations of in-
stances in order to further investigate the lexical use fac-
tors discussed here.
Acknowledgments
This work was supported in part by National Science
Foundation grant CRI-0708952.
References
Rakesh Agrawal, Tomasz Imielinski, and Arun N.
Swami. 1993. Mining association rules between sets
of items in large databases. In Peter Buneman and
Sushil Jajodia, editors, Proceedings of the 1993 ACM
SIGMOD International Conference on Management of
Data, Washington, D.C., May 26-28, 1993, pages 207?
216. ACM Press.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 136?45,
Mexico City, Mexico.
Douglas Biber. 1995. Dimensions of register variation :
a cross-linguistic comparison. Cambridge University
Press, Cambridge.
8
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32?39,
Philadelphia.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, pages 303?311.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors, Word
Sense Disambiguation: Algorithms and Applications,
pages 47?74, Dordrecht, The Netherlands. Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of ACL?02 Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions, pages 54?60,
Philadelphia.
Nancy Ide. 2006. Making senses: Bootstrapping sense-
tagged lists of semantically-related words. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text, pages 13?27, Dordrecht, The Nether-
lands. Springer.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference on
Language Resources and Evaluation (LREC), pages
581?588, Granada.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6:1?37.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication knowl-
edge. Computational Intelligence, 7(4):305?314.
Devra Klein and Gregory Murphy. 2002. Paper has been
my ruin: Conceptual relations of polysemous words.
Journal of Memory and Language, 47:548?70.
Klaus Krippendorff. 1980. Content analysis: An intro-
duction to its methodology. Sage Publications, Bev-
erly Hills, CA.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science Labo-
ratory (CSL) Report 43, Princeton University, Prince-
ton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop On
Standardizing Lexical Resources.
Geoffrey Nunberg. 1979. The non-uniqueness of seman-
tic solutions: Polysemy. Linguistics and Philosophy,
3:143?184.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005. Making fin-egrained and coarse-grained
sense distinctions. Journal of Natural Language Engi-
neering, 13.2:137?163.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a multilin-
gual semantic annotation task. In Proceedings of the
International Conference on Language Resources and
Evaluation (LREC), pages 1951?1956, Genoa, Italy.
Rebecca J. Passonneau. 2004. Computing reliability for
coreference annotation. In Proceedings of the Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), Portugal.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02 Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 40?46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Recent
Successes and Future Directions, pages 81?87.
James Pustejovsky. 1991. The generative lexicon. Com-
putational Linguitics, 17(4):409?441.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Ansaf Salleb-Aouissi, Christel Vrain, and Cyril Nortet.
2007. Quantminer: A genetic algorithm for mining
quantitative association rules. In IJCAI, pages 1035?
1040.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 1005?1014, Prague.
Jean Veronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mit-
sunori Ogihara, and Wei Li. 1997. New algorithms
for fast discovery of association rules. In KDD, pages
283?286.
9
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 27?34,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bridging the Gaps:
Interoperability for GrAF, GATE, and UIMA
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
Keith Suderman
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
suderman@anc.org
Abstract
This paper explores interoperability for
data represented using the Graph Anno-
tation Framework (GrAF) (Ide and Sud-
erman, 2007) and the data formats uti-
lized by two general-purpose annotation
systems: the General Architecture for Text
Engineering (GATE) (Cunningham, 2002)
and the Unstructured Information Man-
agement Architecture (UIMA). GrAF is
intended to serve as a ?pivot? to enable
interoperability among different formats,
and both GATE and UIMA are at least im-
plicitly designed with an eye toward inter-
operability with other formats and tools.
We describe the steps required to per-
form a round-trip rendering from GrAF to
GATE and GrAF to UIMA CAS and back
again, and outline the commonalities as
well as the differences and gaps that came
to light in the process.
1 Introduction
The creation of language data and linguistic anno-
tations remains a fundamental activity in the field
of language technology, in order to develop in-
creasingly sophisticated understanding and gener-
ation capabilities for the world?s languages. Sub-
stantial effort has been devoted to the creation of
resources for major languages, and new projects
are developing similar resources for less widely-
used languages; the cost and effort of resource cre-
ation, as well as the possibilities for linking multi-
lingual and multi-modal language data, demands
that resources and tools are reusable as well as
compatible in terms of their representation. Var-
ious representation standards and annotation tools
have emerged over the past decade and have con-
tributed to some convergence in practice, but at the
same time, there has been growing recognition that
interoperability among formats and tools, rather
than universal use of a single representation for-
mat, is more suited to the needs of the community
and language technology research in general.
This paper explores interoperability for data
represented using the Graph Annotation Frame-
work (GrAF) (Ide and Suderman, 2007) and the
data formats utilized by two general-purpose an-
notation systems: the General Architecture for
Text Engineering (GATE) (Cunningham, 2002)
and the Unstructured Information Management
Architecture (UIMA)1. UIMA and GATE are sim-
ilar in design and purpose: both represent docu-
ments as text plus annotations and allow users to
define pipelines of processes that manipulate the
document. However, there are some differences
in implementation and representation format that
prohibit direct exchange of data and annotations
between the two.
The Graph Annotation Framework (GrAF) (Ide
and Suderman, 2007) is intended to serve as a
?pivot? to enable interoperability among different
formats for data and linguistics annotations and
the systems that create and exploit them. In this
paper, we describe the steps required to perform
a round-trip rendering from GrAF to GATE and
GrAF to UIMA CAS and back again, and outline
the commonalities as well as the differences and
gaps that came to light in the process. In doing
so, we hope to shed some light on the design and
implementation choices that either contribute to
or impede progress toward interoperability, which
can feed future development.
2 Background
A handful of formats for linguistic data and
annotations have been proposed as standards
over the past ten years, including Annotation
Graphs (AG) (Bird and Liberman, 2001), and,
1http://www.oasis-open.org/committees/uima/
27
most recently, the Graph Annotation Framework
(GrAF) (Ide and Suderman, 2007). UIMA?s
Common Analysis System (CAS) also provides a
?common? way to represent annotations so that
they can be shared and reused among UIMA an-
notator components.
Annotation Graphs were introduced primarily
as a means to handle time-stamped speech data, in
large part to overcome the problem of overlapping
annotations that violate the strict tree structure of
XML-based schemes. However, AGs are limited
by the inability to represent hierarchical relations
among annotations (as, for instance, in a syntax
tree). AGs are used in GATE to represent standoff
annotations.
GrAF has been developed by the International
Standards Organization (ISO)?s TC37 SC4, as a
part of the Linguistic Annotation Framework (In-
ternational Standards Organization, 2008). GrAF
provides an XML serialization of an abstract data
model for annotations that is intended to serve as
a ?pivot? for transducing among user-defined and
tool input annotation formats. GrAF is intended to
function in much the same way as an interlingua
in machine translation: a common, abstract con-
ceptual representation into and out of which user-
and tool-specific formats are transduced, so that
a transduction of any specific format into and out
of GrAF accomplishes the transduction between
it and any number of other GrAF-conformant for-
mats. GrAF is currently an ISO Candidate Draft.
The UIMA framework is a data management
system that supports pipelined applications over
unstructured data. UIMA was originally de-
veloped by IBM and is currently under further
development by an OASIS technical commit-
tee2. Apache UIMA3 is an Apache-licensed open
source implementation of the UIMA specification
being developed as an Apache incubator project.
UIMA?s Common Analysis System (CAS) is used
to describe typed objects (annotations) associated
with a given text or other media, upon which pro-
cessing modules (?annotators?) operate.
2.1 Annotation models
Each of the formats described above is based on
some model of annotations and their relation to
the data they describe. The AG model consists of
sets of arcs defined over nodes corresponding to
2http://www.oasis-open.org/committees/uima/
3http://incubator.apache.org/uima/index.html
timestamps in primary data, each of which is la-
beled with an arbitrary linguistic description that
applies to that region. Multiple annotations over
the data produce multiple arcs; there is no provi-
sion for arcs associating annotations.
GrAF defines the regions to be annotated in pri-
mary data as the area bounded by two or more an-
chors. The definition of anchor and the number
of anchors needed to define a region depends on
the medium being annotated. The only assumption
that GrAF makes is that anchors have a natural or-
dering. For textual data GrAF uses character off-
sets for anchors, and two anchors bound each re-
gion. Regions serve as the leaf nodes of a directed
acyclic graph. Annotations in the form of feature
structures are associated with nodes in the graph,
including nodes associated with both regions and
other annotations, via edges in the graph. GrAF
can represent common annotation types such as
hierarchical syntax trees by allowing, for exam-
ple, a sentence annotation to have edges to con-
stituent annotations such as NP, VP, etc. As op-
posed to AGs, annotations typically label nodes
rather than edges in GrAF, although labeled edges
are allowed, and the information comprising the
annotations is represented using feature structures
rather than simple labels.
The underlying model of UIMA CAS is simi-
lar to GrAF?s, due to its hierarchical type system
and the use of feature structures to represent anno-
tation information. In fact, the GrAF model, con-
sisting of a directed acyclic graph whose nodes are
labeled with feature structures, provides the rele-
vant abstraction underlying UIMA CAS. In prin-
ciple, then, annotations represented in GrAF and
UIMA CAS are trivially mappable to one another.
The same is not true for AGs: in GrAF, annota-
tions can be directly linked to other annotations,
but in the AG model annotations are effectively in-
dependent layers linked to the primary data. As a
result, while it is possible to ?flatten? a GrAF rep-
resentation so that it can be represented as an AG,
it is not possible to take the round trip back into
GrAF without losing information about relations
among annotations. An AG can, of course, always
be represented in GrAF, since independent graphs
layered over data (possibly with shared anchors in
the data) are valid GrAF structures.
28
3 GrAF? UIMA? GrAF
Conversion of a GrAF data structure into UIMA
involves generating (1) a UIMA data structure (a
CAS), (2) a UIMA type system, and a specification
of type priorities.
The CAS consists of a subject of analysis (sofa),
which is the data (in our examples here, a text) it-
self, together with its annotations. The CAS XML
representation of the annotations is very similar to
the GrAF XML representation: each annotation is
identified by its start and end location in the data
expressed in terms of virtual nodes between each
character in the data, where the position before the
first character is node 0. The conversion of GrAF
anchors to UIMA indexes is therefore trivial.
3.1 UIMA Type Systems
A UIMA type system specifies the type of data
that can be manipulated by annotator components.
A type system defines two kinds of objects; types
and features. The type defines the kinds of data
that can be manipulated in a CAS, arranged in an
inheritance hierarchy. A feature defines a field,
or slot, within a type. Each CAS type specifies
a single supertype and a list of features that may
be associated with that type. A type inherits all
of the features from its supertype, so the features
that can be associated with a type is the union of
all features defined by all supertypes in the inher-
itance tree. A feature is a name/value pair where
the value can be one of UIMA?s built in primitive
types (boolean, char, int, etc.) or a reference to
another UIMA object. UIMA also allows feature
values to be arrays of either primitive types or ar-
rays of references to other objects.
UIMA defines a top level type uima.cas.TOP
which contains no features and serves as the
root of the UIMA type system inheritance tree.
The root type uima.cas.TOP is the supertype
of uima.cas.AnnotationBase, which is the super-
type of uima.tcas.Annotation, which in turn is
the supertype for org.xces.graf.uima.Annotation.
All UIMA annotations generated by GrAF use
org.xces.graf.uima.Annotation as their supertype.
Note that the UIMA type hierarchy is strictly an is-
a hierarchy; for example, there may be an annota-
tion type pos with subtypes penn pos, claws pos,
etc., indicating that each of these annotations are
a kind of part of speech annotation. The hierar-
chy does not reflect other kinds of relations such
as the relation between a ?lemma? annotation and
a ?pos? annotation (i.e., a lemma and a pos are
typically companion parts of a morpho-syntactic
description, but neither one is a morpho-syntactic
description), or constituency relations in syntactic
annotation schemes.
The GrAF Java API provides a Java class that
generates a valid UIMA type system given one or
more GrAF objects. The type system is generated
by iterating over all the nodes in the graph and cre-
ating a new type for each kind of annotation en-
countered (e.g., token, sentence, POS, etc.). Fea-
ture descriptions are generated for each type at the
same time.
One drawback of deriving a type system auto-
matically is that some of the power of UIMA type
systems is lost in the conversion. For example,
in the process of conversion, all feature values are
assumed to be strings, even though UIMA allows
specification of the type of a feature value. Since
in GrAF, feature values have been serialized from
the contents of an XML attribute, all feature values
are represented internally as strings; to convert a
feature value to any other representation would re-
quire that GrAF have some external knowledge of
the annotation format being deserialized. There-
fore, any type checking capability for feature value
types in UIMA is lost after automatic generation
of the type system. Similarly, it is not possible
to determine a supertype for an annotation if it is
more specific than org.xces.graf.uima.Annotation
from the information in the GrAF representation
alone, so in effect, it is not possible to derive
any meaningful type hierarchy without additional
knowledge. For example, it is not possible to in-
clude the information in the type system descrip-
tion that penn pos and claws pos are subtypes of
pos since this information is not represented in the
graph. Even in cases where this kind of informa-
tion is represented in the graph, it is not retriev-
able; for example, FrameNet annotation includes
a grammaticalFunction annotation whose children
are elements such as subject, object, etc.
However, there is no way to determine what the
parent-child relation is between nodes without a
priori knowledge of the annotation scheme.
Without a source of external knowledge, GrAF
does not attempt to make any assumptions about
the annotations and features in the graph. How-
ever, all of these problems are avoided by pro-
viding an XML Schema or other source of infor-
mation about the GrAF annotations that can be
29
used when generating the type system. The XML
schema can specify the type hierarchy, data types
and restricted ranges for feature values, etc. (see,
for example, the XCES (Ide et al, 2000) schema is
used for the data and annotations in the American
National Corpus (ANC)4.)
3.2 UIMA Views and Indexes
A UIMA CAS object may contain more than one
view of the artifact being annotated; for example, a
CAS may contain an audio stream as one view and
the transcribed text as another. Each view contains
a copy of the artifact, referred to as the subject of
analysis (sofa), and a set of indexes that UIMA an-
notators (processing modules) use to access data in
the CAS. Each index is associated with one CAS
type and indexes that type by its features?that is,
the features are the keys for the index.
The indexes are the only way for UIMA annota-
tors to access annotations in the CAS. It is neces-
sary to generate these indexes, which are not pro-
vided automatically within UIMA. The GrAF Java
API provides a module that generates the indexes
at the same time the it generates the type system
description. Since we do not know, and make no
assumptions about, which annotations might be
required by other annotators, all annotations are
indexed by all of their features.
3.3 Type Priorities
Type priorities in UIMA are used to determine
nesting relations when iterating over collections of
annotations. That is, if two annotations have the
same start and end offsets, then the order in which
they will be presented by an iterator is determined
by their type priority; the annotation with the high-
est priority will be presented first. Type priorities
are specified by an ordered listing of annotation
types, where order determines priority. In GrAF,
annotation nesting is implicit in the graph itself.
To generate an explicit type priority specifica-
tion for UIMA we must first obtain a list of all
annotation types that appear in the graph and then
sort the list based on the order they are encoun-
tered during a a depth first traversal of the graph.
During the depth first traversal a N x N precedence
matrix is constructed where N is the number of an-
notation types in the graph. If precedes[A,B] ==
true then A was encountered as an ancestor of B
in the depth first traversal. If precedes[A,B] ==
4http://www.anc.org
precedes[B,A] == true then it is assumed that the
annotation types have the same priority. Once the
list of annotation types has been collected and the
precedence matrix constructed, the matrix can be
used to to sort the annotation types:
int compare(Annotation A,
Annotation B,
PrecedenceMatrix m)
{
boolean AB = m.precedes(A,B);
boolean BA = m.precedes(B,A);
if (AB && BA)
{
return 0; // equal
}
else if (AB)
{
return -1; // A first.
}
else if (BA)
{
return 1; // B first.
}
// Neither AB or BA means A and
// B are not in connected
// components.
return 0;
}
Not all nodes in the graph may be reachable
in a depth first traversal, particularly if multiple
annotations formats have been merged together.
Therefore, after the initial traversal has been com-
pleted each node is checked to determine if it
has been visited. If not, then another traversal is
started from that node. This is repeated until all
nodes/annotations in the graph have been visited
at least once.
We have found that UIMA type priorities im-
pose some limitations because they cannot repre-
sent context sensitive annotation orderings. For
example, given
<!ELEMENT E1 (A,B)>
<!ELEMENT E2 (B,A)>
The order of A and B differs depending on whether
the parent annotation is E1 or E2. This type of re-
lationship cannot be expressed by a simple order-
ing of annotations.
3.4 Naming Conflicts
The annotation type names used when generat-
ing the UIMA type system are derived automat-
ically based on the annotation names used in
the graph. Annotations in GrAF may also be
grouped into named annotation sets and the gen-
30
<as type="POS">
<a label="token">
<fsr:fs type="PENN">
<fsr:f name="msd" fVal="NN"/>
</fsr:fs>
<fsr:fs type="CLAWS5">
<fsr:f name="msd" fVal="NN"/>
</fsr:fs>
</a>
</as>
Figure 1: GrAF representation of alternative POS
annotations
erated UIMA type name consists of a concatena-
tion of the nested annotation set names with the
annotation label appended. For example, multiple
part of speech annotations may be represented in
different annotation sets, as shown in Figure 1.5
For the above example, two types will
be generated: POS token PENN and
POS token CLAWS5. However, GrAF places
no restrictions on the names used for annotation
set names, annotation labels, or feature structure
types. Therefore, it is possible that the derived
type name is not a valid UIMA identifier, which
are required to follow Java naming conventions.
For example, Part-Of-Speech is a valid name
for an annotation label in GrAF, but because of
the hyphen it is not a valid Java identifier and
therefore not valid in UIMA.
To avoid the naming problem, a derived name
is converted into a valid UIMA identifier before
creating the UIMA type description. To permit
round trip engineering, that is, ensuring a GrAF?
UIMA?GrAF transformation results in the same
GrAF representation as the original, a NameMap
file is produced that maps a generated name to
the compatible UIMA name. NameMaps can be
used in a UIMA? GrAF conversion to ensure the
GrAF annotations and annotation sets created are
given the same names as they had in the original
GrAF representation.
3.5 Preserving the Graph Structure
While UIMA does not have any graph-specific
functionality, the value of a UIMA feature can
be an array of annotations, or more specifically,
an array of references to other annotations. In
5The use of the fVal attribute in this example is sub-
ject to change according to revisions of ISO/DIS 24610-1
Language Resource Management - Feature Structures - Part
1: Feature Structure Representation (International Standards
Organization, 2005), to which the representation of feature
structures in GrAF adheres.
this way, annotations can effectively ?point? to
other annotations in UIMA. We exploit this ca-
pability to preserve the structure of the original
graph in the UIMA representation, by adding two
features to each annotation: graf children
and graf ancestors. This information can be
used to recreate the GrAF representation, should
that ever be desired. It can also be used by UIMA
annotators that have been designed to use and/or
manipulate this information.
Although rarely used, GrAF permits edges in
the graph to be annotated in the same way that
nodes are. For UIMA conversion, if a graph con-
tains labeled edges it must be converted into an
equivalent graph without labeled edges. A graph
with labeled edges can be converted into an equiv-
alent graph without labeled edges, where a node
replaces the original edge. To preserve the origi-
nal graph structure, an attribute indicating that the
node is represented as a a labeled edge in GrAF is
included.
4 GrAF? GATE? GrAF
The conversion to/from GATE is much simpler
than conversion to UIMA, since GATE is type-
less and does not require the overhead of gener-
ating a type system or type priorities list. While
GATE does support annotation schemas, they are
optional, and annotations and features can be cre-
ated at will. GATE is also much more lenient
on annotation and feature names; names automat-
ically generated by GrAF are typically valid in
GATE.
Representing the graph structure in GATE is not
as straightforward as it is in UIMA. We have de-
veloped a plugin to GATE that loads GrAF stand-
off annotations into GATE, and a parallel plugin
that generates GrAF from GATE?s internal format.
As noted above, GATE uses annotation graphs to
represent annotations, However, because annota-
tion graphs do not provide for annotations of an-
notations, to transduce from GrAF to the GATE in-
ternal format it is necessary to ?flatten? the graph
so that nodes with edges to other nodes are mod-
ified to contain edges directly into the primary
data. GATE assigns a unique id value to every an-
notation, so it is possible to link annotations by
creating a special feature and referencing the par-
ent/child annotations by their GATE id values.
The greatest difficulty in a GrAF? GATE con-
version arises from the fact that in GATE, every
31
Figure 2: UIMA rendering of GrAF annotations
annotation is expected to have a start and end off-
set. In GrAF, a node may have multiple edges
to other nodes that cover disjoint regions of text.
For example, the FrameNet6 annotation for a given
verb typically includes edges to the associated role
fillers (e.g., agent, theme, instrument, etc.), which
are rarely contiguous in the text itself. Our current
solution to this problem is to give a start and end
offset that covers the smallest region of the text
covering the regions associated with all descen-
dants of the annotation, and recording the infor-
mation concerning the original graph structure in
attributes to enable reconversion into the original
GrAF representation.
5 Exploiting Interoperability
GrAF is intended to serve as the lingua franca for
data and annotations used in processing systems
such as GATE and UIMA. As such, it provides
a way for users to take advantage of each frame-
work?s strengths, e.g., UIMAs capabilities for de-
ploying analysis engines as services that can be
run remotely, and GATE?s wide array of process-
ing resources and capabilities for defining regu-
6http://framenet.icsi.berkeley.edu/
lar expressions over annotations (JAPE). It should
be noted that GATE provides wrappers to allow a
UIMA analysis engine to be used within GATE,
and to allow a GATE processing pipeline to be
used within UIMA. To share data and annota-
tions between the two systems, it is necessary to
construct a mapping descriptor to define how to
map annotations between the UIMA CAS and the
GATE Document, which operate similarly to the
converters from and to GrAF from data and an-
notations described above. However, one advan-
tage of using a GrAF representation as a pivot be-
tween the two systems is that when an annotation
schema is used with GrAF data, the conversion
from GATE to UIMA is more robust, reflecting the
true type description and type priority hierarchies.
Using GrAF as a pivot has more general ad-
vantages, for example, by allowing annotations
to be imported from and exported to a wide va-
riety of formats, and also enabling merging an-
notations from disparate sources into a single an-
notation graph. Figure 2 shows a rendering of
a Penn Treebank annotation (bracketed format)
and a FrameNet annotation (XML) that have been
transduced to GrAF, merged, and the transduced
32
Figure 3: GATE rendering of GrAF annotations
for use in UIMA. The same data is shown ren-
dered in GATE in Figure 3. The two ?views?
of the data consisting of overlaid annotations for
each annotation type are visible in each render-
ing. There are multiple possibilities for exploiting
and exploring merged annotations representing a
range of annotation types within these two frame-
works. For example, a UIMA analysis engine
could be developed to identify regions annotated
by both schemes, or all FrameNet elements that
are annotated as agent and also annotated with
Penn Treebank NP-OBJ, etc. In GATE, JAPE
rules could locate patterns in annotations obtained
from different sources, or named entity recogni-
tion rules could be enhanced with annotation in-
formation from data annotated in other formats.
It would also be possible to compare multiple an-
notations of the same type, such as different tok-
enizations, different POS taggings , etc.
As a final note, we point out that in addi-
tion to conversion to UIMA and GATE, annota-
tions from different sources (singly or merged in
any combination) can also be converted to sev-
eral other formats by using the GrAF Java API.
The API allows the user to select from among ex-
isting annotations and specify an output format
for their merged representation. Currently, in ad-
dition to GrAF, the following output formats are
supported: XML documents with inline annota-
tions; formats compatible with Monoconc Pro7
and Wordsmith Tools8; NLTK9; CONLL (B-I-E)
format; and UIMA CAS.10 So, for example, it is
possible to load a collection of standoff annota-
tion files and convert to XML, and then present
them to XML-aware applications as XML files
with inline annotations. As a result, we are be-
ginning to see possibilities for true interoperabil-
ity among not only major frameworks like UIMA
and GATE, but also applications with more limited
functionalities as well as in-house formats. This,
in turn, opens up the potential to mix and match
among tools for various kinds of processing as ap-
propriate to a given task. In general, the trans-
duction of ?legacy schemes? such as Penn Tree-
bank into GrAF greatly facilitates their use in ma-
jor systems such as UIMA and GATE, as well as
7http://www.athel.com/mono.html
8http://www.lexically.net/wordsmith/
9http://www.nltk.org/
10Note that to render GrAF into GATE, a plugin within the
GATE environment is used to perform the conversion.
33
Figure 4: Conversion capabilities
other applications and systems. Figure 4 shows
the conversion capabilities among a few annota-
tions schemes, GrAF, and UIMA and GATE.
All of our conversion tools and GATE plugins
are freely available for download with no restric-
tions at http://www.anc.org. The UIMA project
has received support to develop a UIMA? GrAF
conversion module, which should be available in
the near future.
6 Conclusion
Consideration of the transduction from a generic,
relatively abstract representation scheme such as
GrAF into the formats required for widely adopted
frameworks for creating and analyzing linguisti-
cally annotated data has several ramifications for
interoperability. First, it brings to light the kinds
of implementation choices that either contribute to
or impede progress toward interoperability, which
can feed future development. Second, our work
on converting GrAF to the formats supported by
UIMA and GATE shows that while minor differ-
ences exist, the underlying data models used by
the two frameworks are essentially the same, as
well as being very similar to the data model under-
lying GrAF. This is good news for interoperability,
since it means that there is at least implicit conver-
gence on the data model best suited for data and
annotations; the differences lie primarily in the
ways in which the model is serialized internally
and as output by different tools. It also means that
transduction among the various formats is possible
without loss of information.
We have shown that a UIMA?GrAF or GATE
? GrAF conversion is fairly straightforward; the
expressive power of GrAF can easily represent the
data models used by UIMA and GATE. On the
other hand, GrAF ? UIMA or GrAF ? GATE
transformations are less straightforward. Both
frameworks can represent graphs, but neither pro-
vides a standard representation that other compo-
nents are guaranteed to understand. Given that
powerful analysis algorithms for data in graphs are
well-established, there may be considerable ad-
vantage to using the graph as a general-purpose
format for use within various modules and ana-
lytic engines. In any case, the generality and flexi-
bility of the GrAF representation has already been
shown to be an effective means to exchange lin-
guistic data and annotations that exist in different
formats, as well as a model for development of an-
notation schemes in the future.
Acknowledgments
This work was supported by an IBM UIMA In-
novation Award and National Science Foundation
grant INT-0753069.
References
Steven Bird and Mark Liberman. 2001. A Formal
Framework for Linguistic Annotation. Speech Com-
munication, 33:1-2, 23-60.
Nancy Ide and Keith Suderman. 2007. GrAF:
A Graph-based Format for Linguistic Annotations.
Proceedings of the First Linguistic Annotation
Workshop, Prague, Czech Republic, June 28-29, 1-8.
International Standards Organization. 2008. Lan-
guage Resource Management - Linguistic Annota-
tion Framework. ISO Document WD 24611.
International Standards Organization. 2005. Language
Resource Management - Feature Structures - Part 1:
Feature Structure Representation. ISO Document
ISO/DIS 24610-1.
Nancy Ide, Patrice Bonhomme, and Laurent Ro-
mary. 2000. XCES: An XML-based Standard
for Linguistic Corpora. Proceedings of the Sec-
ond Language Resources and Evaluation Confer-
ence (LREC), Athens, Greece, 825-30.
Hamish Cunningham. 2002. GATE, a General Ar-
chitecture for Text Engineering. Computers and the
Humanities, 36:223-254
34
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 178?181,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
The SILT and FlaReNet International Collaboration for Interoperability
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
James Pustejovsky
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
jamesp@cs.brandeis.edu
Nicoletta Calzolari
CNR-ILC
Pisa, Italy
glottolo@ilc.cnr.it
Claudia Soria
CNR-ILC
Pisa, Italy
claudia.soria@ilc.cnr.it
Abstract
Two major projects in the U.S. and Eu-
rope have joined in a collaboration to work
toward achieving interoperability among
language resources. In the U.S., a project
entitled ?Sustainable Interoperability for
Language Technology? (SILT) has been
funded by the National Science Founda-
tion under the INTEROP program, and in
Europe, FLaReNet Fostering Language
Resources Network has been funded
by the European Commission under the
eContentPlus framework. This interna-
tional collaborative effort involves mem-
bers of the language processing commu-
nity and others working in related ar-
eas to build consensus regarding the shar-
ing of data and technologies for language
resources and applications, to work to-
wards interoperability of existing data,
and, where possible, to promote standards
for annotation and resource building. In
addition to broad-based US and European
participation, we are seeking the partici-
pation of colleagues in Asia. This pre-
sentation describing the projects and their
goals will, we hope, serve to involve mem-
bers of the community who may not have
been aware of the effort before, in particu-
lar colleagues in Asia.
1 Overview
One of today?s greatest challenges is the develop-
ment of language processing capabilities that will
enable easy and natural access to computing facil-
ities and information. Because natural language
processing (NLP) research relies heavily on such
resources to provide training data to develop lan-
guage models and optimize statistical algorithms,
language resources?including (usually large) col-
lections of language data and linguistic descrip-
tions in machine readable form, together with
tools and systems (lemmatizers, parsers, summa-
rizers, information extractors, speech recognizers,
annotation development software, etc.)? are criti-
cal to this development.
Over the past two decades, the NLP commu-
nity has invested substantial effort in the creation
of computational lexicons and compendia of se-
mantic information (e.g., framenets, ontologies,
knowledge bases) together with language corpora
annotated for all varieties of linguistic features,
which comprise the central resource for current
NLP research. However, the lack of a thorough,
well-articulated longer-term vision for language
processing research has engendered the creation of
a disjointed set of language resources and tools,
which exist in a wide variety of (often incom-
patible) formats, are often unusable with systems
other than those for which they were developed,
and utilize linguistic categories derived from dif-
ferent theoretical frameworks. Furthermore, these
expensive investments are often produced only for
one of several relatively isolated subfields (e.g.,
NLP, information retrieval, machine translation,
speech processing), or even worse, for one appli-
cation in one subfield. In addition, the high cost of
resource development has prevented the creation
of reliable, large-scale language data and anno-
tations for many phenomena, and for languages
other than English.
Interoperability of resources, tools, and frame-
works has recently come to be recognized as per-
haps the most pressing current need for language
processing research. Interoperability is especially
178
critical at this time because of the widely recog-
nized need to create and merge annotations and
information at different linguistic levels in order
to study interactions and interleave processing at
these different levels. It has also become criti-
cal because new data and tools for emerging and
strategic languages such as Chinese and Arabic as
well as minor languages are in the early stages of
development.
Two major projects in the U.S. and Europe have
joined in a collaboration to work toward achiev-
ing interoperability among language resources. In
the U.S., a project entitled ?Sustainable Interoper-
ability for Language Technology? (SILT) has been
funded by the National Science Foundation under
the INTEROP program, and in Europe, FLaReNet
Fostering Language Resources Network has been
funded by the European Commission under the
eContentPlus framework. This international col-
laborative effort involves members of the lan-
guage processing community and others working
in related areas to build consensus regarding the
sharing of data and technologies for language re-
sources and applications, to work towards inter-
operability of existing data, and, where possible,
to promote standards for annotation and resource
building. In addition to broad-based US and Eu-
ropean participation, we are seeking the participa-
tion of colleagues in Asia.
To ensure full community involvement and con-
solidation of effort, SILT and FLaReNet are estab-
lishing ties with major ongoing projects and con-
sortia, including the International Standards Orga-
nization TC37 SC4 (Language Resource Manage-
ment)1, The World Wide Web Consortium (W3C),
the Text Encoding Initiative, the ACL Special In-
terest Group on Annotation (SIGANN)2, and oth-
ers. The ultimate goal is to create an Open Lan-
guage Infrastructure (OLI) that will provide free
and open access to resources, tools, and other in-
formation that support work in the field, in order to
facilitate collaboration, accessibility for all mem-
bers of the community, and convergence toward
interoperability.
The following sections outline the goals of SILT
and FLaReNet.
1http://www.tc37sc4.org
2http://www.cs.vassar.edu/sigann
2 SILT
The creation and use of language resources spans
several related but relatively isolated disciplines,
including NLP, information retrieval, machine
translation, speech, and the semantic web. SILT?s
goal is to turn existing, fragmented technology and
resources developed within these groups in rela-
tive isolation into accessible, stable, and interop-
erable resources that can be readily reused across
several fields.
The major activities of the effort are:
? carefully surveying the field to identify the
resources, tools, and frameworks in order to
examine what exists and what needs to be
developed, and to identify those areas for
which interoperability would have the broad-
est impact in advancing research and devel-
opment and significant applications depen-
dent on them;
? identifying the major efforts on standards de-
velopment and interoperable system design
together with existing and developing tech-
nologies, and examining ways to leverage
their results to define an interoperablity in-
frastructure for both tools and data;
? analyzing innovative methods and techniques
for the creation and maintenance of language
resources in order to reduce the high costs,
increase productivity, and enable rapid devel-
opment of resources for languages that cur-
rently lack them;
? implementing proposed annotation standards
and best practices in corpora currently under
development (e.g., American National Cor-
pus3, TimeBank4) to evaluate their viability
and feed into the process of further standards
development, testing, and use of interoper-
ability frameworks (e.g., GATE5, UIMA6)
and implementation of processing modules,
and distributing all software, data, and anno-
tations.
? ensuring the broadest possible community
engagement in the development of consensus
and agreement on strategies, priorities, and
3http://www.anc.org
4http://www.timeml.org/site/timebank/timebank.html
5http://gate.ac.uk
6http://www.oasis-open.org/committees/uima/
179
best approaches for achieving broad interop-
erability by means of sessions, open meet-
ings, and special workshops at major confer-
ences in the field, together with active main-
tenance of and involvement in open web fo-
rums and Wikis;
? providing the technical expertise necessary to
turn consensus and agreement into robust in-
teroperability frameworks along with the ap-
propriate tools and resources for their broad
use and implementation by means of tutorials
and training workshops, especially for under-
graduate and graduate students in the field.
3 FLaReNet
The multilingual Europe urgently needs language
technologies in order to bridge its language bar-
riers. In order to achieve better quality and fast
development of language technologies that seam-
lessly work on all devices, for spoken and written
language alike, the European scenario now needs a
coherent and unified effort. The demand for cross-
lingual technologies is pressing, the expectations
are high, and at the same time, the field is suf-
fering from fragmentation, lack of vision and di-
rection. The main objective of FLaReNet is to
steer the process that in the near future will de-
fine the actors, the overall direction and the prac-
tical forms of collaboration in language technolo-
gies and their ?raw material?, language resources.
Under this respect, the goals of FLaReNet lie at
a higher level than those of SILT, as they are ori-
ented towards consolidating a community around
a number of key topics that, in the end, will allow
networking of language technology professionals
and their clients, as well as easy sharing of data,
corpora, language resources and tools.
From this perspective, FLaReNet has three
main lines of action:
The creation and mobilization of a unified
and committed community in the field of Lan-
guage Resources and Technologies. To this end,
FLaReNet is bringing together leading experts of
research institutions, academies, companies, fund-
ing agencies, public and private bodies, both at
European and international level, with the spe-
cific purpose of creating consensus around short,
medium and long-term strategic objectives. The
Network is currently composed of around 200 in-
dividuals belonging to academia, research insti-
tutes, industries and government.
The identification of a set of priority themes
on which to stimulate action, under the form of a
roadmap for Language Resources and Technolo-
gies. In order to avoid scattered or conflicting ef-
forts, the major players in the field of Language
Resources and Technologies need to consensually
work together and indicate a clear direction of ac-
tion and a shared policy for the next years. This
will take the form of identification of priorities of
intervention as well as short, medium, and long-
term strategic objectives at all levels, from re-
search directions to implementation choices, from
distribution and access policies to the landscape
of languages, domain and modalities covered by
Language Resources and Technologies.
The elaboration of a blueprint of priority ar-
eas for actions in the field and a coherent set of
recommendations for the policy-makers (funding
agencies especially), the business community and
the public at large. Whatever action cannot be im-
plemented on a long term without the help of the
necessary financial and political framework to sus-
tain them. This is even most true for actions re-
garding Language Resources that typically imply
a sustained effort at national level. To this end,
the FLaReNet Network will propose the priority
themes under the form of consensual recommen-
dations and a plan of action for EC Member States,
other European-wide decision makers, companies,
as well as non-EU and International organizations.
The following Thematic Areas are currently
covered by FLaReNet:
? The Chart for the area of LRs and LT in its
different dimensions
? Methods and models for LR building, reuse,
interlinking, and maintenance
? Harmonisation of formats and standards
? Definition of evaluation and validation proto-
cols and procedures
? Methods for the automatic construction and
processing of Language Resources
FLaReNet builds upon years of research and de-
velopment in the field of standards and language
resources, as well as on the achievements (both
in terms of results and community awareness),
of past EU projects such as EAGLES7, ISLE8,
7http://www.ilc.cnr.it/EAGLES/home.html
8http://www.ilc.cnr.it/EAGLES/isle/ISLE Home Page.htm
180
INTERA9, and LIRICS10. Close collaboration is
also established with many relevant ongoing EU
projects, such as CLARIN11.
9http://www.elda.org/intera
10http://lirics.loria.fr/
11http://www.clarin.eu
181
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 565?576, Dublin, Ireland, August 23-29 2014.
Biber Redux: Reconsidering Dimensions of Variation in American English
Rebecca J. Passonneau
Center for Computational Learning Systems
Columbia University
New York, New York USA
becky@ccls.columbia.edu
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
Songqiao Su
Department of Computer Science
Columbia University
New York, New York USA
ss4555@columbia.edu
Jesse Stuart
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
jestuart@cs.vassar.edu
Abstract
Genre classification has been found to improve performance in many applications of statistical
NLP, including language modeling for spoken language, domain adaptation of statistical parsers,
and machine translation. It has also been found to benefit retrieval of spoken or written docu-
ments. At its base, however, classification assumes separability. This paper revisits an assump-
tion that genre variation is continuous along multiple dimensions, and an early use of principal
component analysis to find these dimensions. Results on a very heterogeneous corpus of post-
1990s American English reveal four major dimensions, three of which echo those found in prior
work and the fourth depending on features not used in the earlier study. The resulting model
can provide a basis for more detailed analysis of sub-genres and the relation between genre and
situations of language use, as well as a means to predict distributional properties of new genres.
1 Introduction
Although a precise definition of the term ?genre? has traditionally proven to be elusive, it cannot be dis-
puted that a genre represents a set of shared regularities among written or spoken documents that enables
readers, writers, listeners and speakers to signal discourse function, and that conditions their expectations
of linguistic form. Genre distinctions are therefore an important aspect of language use and understand-
ing. They clearly have a role to play in statistical language processing, which relies on regularities of
form as well as content. Indeed, with the advent of the Web, statistical methods for genre differenti-
ation have been applied to information retrieval to limit search criteria and organize results (Karlgren
and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of
genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More
recently, the development of genre-dependent models for a variety of natural language processing (NLP)
tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition
(Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine trans-
lation (Wang et al., 2012) has been found to significantly improve performance. The ability to match
documents by genre has also become important for collecting data to train language models for spoken
language understanding, given the difficulty of creating large repositories of transcribed spoken language
corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005).
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
565
While the utility of document characterization by genre for empirical language analysis is widely ac-
knowledged, there is relatively little agreement on methodology. In part, this stems from the difficulty of
providing a comprehensive list of genres or even an operational definition of what constitutes a distinct
genre, much less a definitive set of features to characterize genre differences. The earliest large-scale
statistical study of genre is that of Biber (Biber, 1988), who applied principal component analysis (PCA)
to a one-million word corpus consisting of heterogeneous varieties of spoken and written discourse in
order to identify multiple dimensions of variation in language. Biber argued that linguistic variation was
continuous along six dimensions: involved vs. informational, narrative vs. non-narrative, explicit vs.
situation-dependent reference, overt expression of persuasion, abstract vs. non-abstract information, and
on-line information elaboration; he identified features associated with each dimension, and characterized
kinds of discourse by joint assessment of similarities and differences across these dimensions. Inter-
estingly, since Biber?s study, there has been comparatively little investigation of how genres vary using
multivariate distributional methods (see, for example, the discussion in (Kilgarriff, 2001)).
Biber?s work, which was completed in the mid-1980?s, relied on a large number of features extracted
using somewhat ad hoc methods and reported no reliability measures. Given the renewed interest in genre
classification and the increasing interest in automatic techniques to adapt NLP tools across different kinds
of corpora, we feel it is worth subjecting Biber?s thesis to a new test, utilizing state-of-the-art methods
for extracting features from a high quality, very heterogeneous corpus. In addition to replicating Biber?s
basic approach with more reliable features, we include newer genres (e.g., email, blogs, tweets) in an
attempt to verify that these methods can generalize over different kinds of data. We use a smaller feature
set that overlaps with Biber?s for the most part, but which also includes features unavailable in the
earlier work. In our set, each feature was identified using freely available NLP tools and was manually
validated. In our use of different features, our experiment constitutes a strong test of Biber?s claim
that the dimensions of variation he identified arise from underlying constraints on usage. We find three
components similar to his, and a new one he did not find, based on our use of Named Entity features. We
find that genres that are separable on one component are often co-extensive on another. To quantify the
distinctiveness of each of the genres relative to the others, we use a metric that has previously been used
to measure separability of classes.
2 Related work and motivation
Our work builds on Biber?s 1988 study, but differs in the corpus and features used. Biber?s corpus and
MASC (Ide et al., 2010), the corpus used in our study, differ in source language (British English versus
American English), time coverage (skewed towards a single year versus three decades), and the situations
of use. Biber?s corpus was drawn from the Lancaster-Oslo-Bergen (LOB) Corpus of British English,
consisting of works published in 1961, the London-Lund corpus of spoken English, consisting of 87
texts of British English from private conversation, public interviews and panel discussions, telephone
conversations, radio broadcasts, spontaneous speeches and prepared speeches produced in the 1970s. To
these Biber added a collection of his own professional and personal letters. MASC represents a larger time
slice (1990s to present) and is more heterogeneous, including a wider range of traditional genres as well
as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage
of MASC?s rich set of validated annotations to include features that would not have been (easily) available
at the time of Biber?s study, and reconsider the use of some features used in his work.
Some work on genre classification contrasts with Biber?s approach, which assumes that documents
fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task
(Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini,
2004), or combined with topic classification (Rauber and M?uller-K?ogler, 2001; Lee and Myaeng, 2002).
All of these studies assume that documents fall discretely into distinct classes or clusters. These studies
vary in their approach to determining the genre of text, either by using corpora with pre-defined classes
(Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre
classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets
in genre studies have remained rather stable over the past three decades, mostly utilizing word-based
566
features similar to many of Biber?s such as individual lexical items and/or their orthographic charac-
teristics (e.g., contractions), part-of-speech (POS), punctuation (Kessler et al., 1997; Stamatatos et al.,
2000b), derivative statistics (e.g., average word/sentence length, ratios among lexical or POS classes),
and POS-ngrams (Santini, 2004; Feldman et al., 2009).
Karlgren and Cutting (1994) apply discriminant analysis to pre-defined classes from the Brown corpus
using easily identifiable information such as POS counts, type/token ratios, and sentence length. They
achieve relatively low accuracy of 52%. Kessler et al. (1997) also use the Brown corpus and classify
documents into three facets: brow, narrative, and genre. They extract 55 features, avoiding features
at the syntactic level that are computationally expensive to identify, and characterize them as lexical,
character-level, and derivative (log ratios and their sums). They achieve nearly 80% accuracy on their
six genre classes (reportage, editorial, scitech, legal, non-fiction, fiction). Feldman et al. (2009) create
a corpus of eight genres of speech and web text and test an approach to factor documents by genre,
formality and number of speakers. They achieve accuracy of 55% using quadratic discriminant analysis
on a representation consisting of features based on POS tags, words, and punctuation, reduced using
PCA. Santini (2004) applies high-dimensional POS trigram vectors to ten BBC genres (four spoken, six
written) with Na??ve Bayes classification. A document representation using a length-835 vector achieves
82.6% accuracy for 10-fold cross-validation on all 10 genres, and a Kappa agreement of 0.80.
Rauber and M?uller-K?ogler (2001) apply self-organizing maps (Kohonen, 1995) for both topic and
genre clustering, using features typical of readability measures (e.g., sentence and word lengths, punc-
tuation frequency). Lee and Myaeng (2002) address classification of web text and also do simultaneous
genre and subject (topic) classification, using a Naive Bayes learner. Tests on seven genres for both
English and Korean achieve 0.80 micro-averaged f-measure or 0.87 cosine similarity.
More recent work finds good performance from the use of ngram features for words, characters and
part-of-speech (Gries et al., 2009; Kanaris and Stamatatos, 2009; Sharoff et al., 2010). Gries et al. (2009)
relies only on word ngrams of various lengths to produce clusters with high maximum average silhouette
width, where higher widths represent more homogeneous clusters that are more distinct from one another.
They find that trigrams do best. Kanaris and Stamatatos (2009) uses frequently occurring character
ngrams without regard to their discriminatory power, and Sharoff et al. (2010) find that character ngrams
outperform word and pos ngrams. On benchmark corpora with from 4 to 8 genres, the latter two works
achieve accuracies of up to 96-97% on some corpora. They assume that genres can be taken as a given,
although Sharoff et al. (2010) note that chance-corrected human agreement on the gold standard is only
moderate.
Another strand of investigation addresses genre variation as a requirement for achieving better perfor-
mance in new domains, as in language modeling for speech applications (Bulyko and Ostendorf, 2003;
Sarikaya et al., 2005) or statistical parsers applied to text (Ravi et al., 2008; McClosky et al., 2010; Roux
et al., 2012), where downstream applications can include assignment of semantic argument structure.
Bulyko and Ostendorf (2003) select web text for class-based n-gram language modeling. They locate
relevant documents using queries representative of conversational speech, rather than characterizing the
documents as a whole in terms of statistical features, but demonstrate a significant reduction in Word
Error Rate (WER) for their enhanced language models. Sarikaya et al. (2005) achieve even higher im-
provements using a similar query methodology, then use BLEU scores, a machine translation similarity
method (Papineni et al., 2002), to find sentences that are closest to a domain sample. Ravi et al. (2008)
propose a method to predict parser accuracy based on properties of the new domain of interest and prop-
erties of the domain on which the parser was trained. Lexical features for words other than the 500
most frequent were found to generalize less well than features such as POS and sentence length. Subse-
quent work models corpus differences using regression models to predict parser accuracy McClosky et
al. (2010), or incorporates explicit genre classifiers Roux et al. (2012).
In our initial exploration of genre variation in MASC, we exploited a set of features that subsume most
of those discussed in the works reviewed above. We applied a variety of methods, including k-means
clustering, discriminative classifiers such as Na??ve Bayes, and PCA. Through comparison of results, we
discovered that classification had variable performance, and that PCA provided an explanation: docu-
567
Genre Code No. words Pct corpus
Court transcript CT 30052 6%
Debate transcript DT 32325 6%
Email EM 27642 6%
Essay ES 25590 5%
Fiction FT 31518 6%
Gov?t documents GV 24578 5%
Journal JO 25635 5%
Letters LT 23325 5%
Newspaper NP 23545 5%
Non-fiction NF 25182 5%
Spoken SP 25783 5%
Technical TC 27895 6%
Travel guides TG 26708 5%
Twitter TW 24180 5%
Blog BG 28199 6%
Ficlets FC 26299 5%
Movie script MS 28240 6%
Spam SM 23490 5%
Jokes JK 26582 5%
TOTAL 506768
(a) Genre distribution in MASC
Annotation type No. words
Logical 506659
Token 506659
Sentence 506659
POS/lemma (GATE) 506659
POS (Penn) 506659
Noun chunks 506659
Verb chunks 506659
Named Entities 506659
FrameNet 39160
Penn Treebank 506659
Coreference 506659
Discourse structure* 506659
Opinion 51243
TimeBank *55599
PropBank 88530
Committed Belief 4614
Event 4614
Dependency treebank 5434
(b) Summary of MASC annotations
Figure 1: Composition of the Manually Annotated Sub-Corpus
ments from distinct classes often fell within an identifiable region on one or more dimensions discovered
by PCA, but these regions overlapped one another along other dimensions. We concluded that whether
or not a set of documents can be categorized into relatively distinct classes by their linguistic forms rather
than content depends on how the documents are selected, how the classes are defined, and what features
are used. Our goal here is to refine a method to learn key dimensions of variation relevant for the same
types of applications referenced in work on genre identification, as discussed in Section 7.
3 Corpus and data preparation
MASC is a 500,000 word corpus of post 1990s American English comprised of texts from nineteen genres
of spoken and written language data in roughly equal amounts, shown in Figure 1a). Roughly 15% of
the corpus consists of spoken transcripts, both formal (court and debate) and informal (face-to-face,
telephone conversation, etc.); the remaining 85% covers a wide range of written genres, including social
media (tweets, blogs). The annotation types and coverage in MASC are given in Figure 1b); all MASC
annotations are hand-validated or manually produced. The corpus is fully open and freely available.
1
To prepare the data, we developed a framework in Groovy
2
(a dialect of Java) to extract linguistic
features, using version 1.2.0 of the GrAF API
3
to access the MASC data and annotations. Most texts
in MASC comprise complete discourse units, e.g. full conversations, letters, chapters from a book, etc.,
with the exception of tweets, jokes, and (to some extent) ficlets.
4
As shown in Figure 1a), although
each MASC genre contains roughly 25,000 tokens, the number of texts in any given genre varies widely,
from as few as two to over 100. To standardize the number of data points per genre, the texts in each
genre were concatenated and then divided into samples of even length, rounded to the nearest sentence
boundary. Portions of the texts containing email headers, bibliographic references, and computer code,
which contain an excess of certain punctuation and other special characters, were eliminated prior to
creating the samples.
Initially, we created sample sets consisting of 1,000 tokens per sample,
5
motivated by Biber?s observa-
tion that even rare linguistic features are relatively stable across samples of this size (Biber, 1993). Our
1
MASC is downloadable from http://www.anc.org/data/masc and available from the Linguistic Data Consortium (LDC).
2
http://groovy.codehaus.org
3
http://sourceforge.net/projects/iso-graf/
4
Ficlets are story fragments to which ?prequels? or ?sequels? are added by online participants.
5
We use tokens as the unit of analysis rather than blank-separated words (strings), which, given the MASC tokenization
strategy, means that hyphenated words such as ?able-bodied? and possessive markers (?s) are treated as individual tokens.
568
1 1st/2nd person pro.
2 3rd person pro.
3 Pronoun it
4 Copula verbs
5 All NEs
6 NEs w/o date
7 Verbs, base
8 Verbs, past
9 Gerunds/Pres. ptp.
10 Past ptp.
11 1st/2nd pres. sg. V
12 3rd pres. sg. V
13 Common nouns
14 All verbs
15 Proper nouns
16 Adjectives
17 Adverbs
18 Superlatives
19 All pers. pro.
20 Prepositions
21 Foreign words
22 Exist. there
23 Interjec.
24 NEs, person
25 NEs, date
26 NEs, location
27 NEs, org.
28 Suasive verbs
29 Stative verbs
30 Noun chunk length
31 Verb chunk length
32 Tokens/sentence
33 Characters/token
34 Periods
35 Questions
36 Exclamations
37 Commas
(a) Thirty-seven features
(b) Boxplots of the 37 features: the box shows the range of the 25th to 75th percentiles with the
median value identified by the vertical red bar. The black whiskers show the extreme values not
considered outliers, and the red are the outliers. The most extreme outliers of feature 21 were
dropped to save space.
Figure 2: Feature names and boxplots
experiments showed, however, that for the features used here, results were comparable using 500-token
chunks, which enabled us to work with a set of data points of the same size as Biber?s. Our process
generated 965 500-token chunks, with roughly 50 chunks per genre.
4 Features and feature analysis
Biber used sixty-seven features consisting primarily of lexical items and groups, parts of speech, and
quasi-syntactic features such as coordination, negation, relative pronoun deletion, that-clauses, and so
on. Many of the features in our set overlap with Biber?s, but we also exploit annotations in MASC to
provide additional features. All the MASC annotations have been manually validated, including those
produced by automated tools such as POS-taggers, NE recognizers, and shallow parsers.
PCA is appropriate for data with normally distributed values and can be used to reduce the number of
features to include only those that are the least correlated. It highlights features with the greatest varia-
tion. Figure 2b) shows boxplots of thirty-seven features we began with. These are mainly frequencies
normalized by the total token count in the document samples we created. They also include the average
characters per word, and average tokens per sentence, noun chunk, and verb chunk. Figure 2a) lists the
features by number. Features 21, 23, 28 and 36, which are foreign words, interjections, suasive verbs
and exclamations, have median values (red line within the box) near the 25th percentile, so are highly
skewed. We therefore dropped these and carried out the PCA with the remaining thirty-three.
6
Hierarchical clustering of the dataset by MASC genre yields the dendogram in Figure 3. We used
the city block metric (also known as taxicab distance), which is similar to Euclidean distance but less
sensitive to outliers. The legend identifies six major clusters for the 19 genres, with two singletons (Travel
guides and Technical documents), a cluster with three spoken genres (Court and Debate transcripts,
and transcripts of face-to-face and telephone conversations), two four-genre clusters, and one six-genre
6
To insure comparability of feature influence, all our features were re-scaled in [-1,1] with mean 0.
569
(a) Six groups from hierarchical clustering
(b) Hierarchical clustering
Figure 3: Hierarchical clustering of 19 MASC genres
cluster. These larger clusters consist of ?story-telling? genres (ficlets, fiction, jokes and movie scripts),
offline-interactive genres (letters, spam, email and tweets), and discursive text (blog, essay, journal, non-
fiction, government documents, and news). Thus the distribution of our features across the data predict
groupings that correspond well with our intuitions about the genres defined in MASC, providing some
justification for both our feature selection and the genre assignments in the corpus. The groupings also
reflect several of Biber?s dimensions of variation, as discussed in Section 7.
Here, we describe PCA in general terms to present four principal components identified in our analysis.
We focus on features associated with the components, and on the six MASC document clusters.
PCA starts with a covariance matrix of all features: a square matrix where each cell value is the
covariance of feature x
i
with feature x
j
for i, j ? M . Covariance of x
i
, x
j
is analogous to variance: for
all datapoints n ? [1 : N ], you subtract x
i
n
from x
i
, x
j
n
from x
j
, sum the products of these differences,
and normalize by n-1.
7
A common explanatory visualization will show a scatterplot of hypothetical
data values in a sausage shape at a diagonal to the x-axis. A line along the maximum width of the
sausage represents the dimension of greatest variation. A second axis can be placed orthogonal to this
first component; it will account for less of the variance in the data, and in a different direction. PCA
consists of computation of these axes (eigenvectors) from a covariance matrix.
5 PCA results
Figure 4a) shows a plot of our first principal component by the second component and the features
that contribute most to each, based on the features? loadings (weights) on the new components. The
components are rotated to become the new x,y axes and centered at zero. Projection of the individual
features onto the rotated axes shows which features contribute most directly to each dimension. Figure
5a) shows a similar plot for the third and fourth components. Twenty-seven features have loadings of at
least 0.2 on any component. Many have similar loadings (e.g., commas and prepositions on the fourth
component), indicating the data could be represented with fewer, uncorrelated features.
Past tense verbs, copula verbs, personal pronouns, and adverbs load heavily on one pole of the first
principal component, while characters per word, noun chunk length and nouns load higher on the op-
posite pole. This component corresponds rather well to Biber?s first component, which had similar
loadings for personal pronouns, adverbs, nouns and word length, and which he interpreted as involved
versus informational?i.e., interactive, unplanned, primarily spoken data vs. polished written documents
conveying (sometimes dense) information about a given topic.
7
See any text on covariance for an explanation of why n-1 is a better normalization term than n.
570
(a) First and second principal components
(b) Document regions for components one and two
Figure 4: First and Second Principal Components
(a) Third and fourth principal components
(b) Document regions for components three and four
Figure 5: Third and Fourth Principal Components
Our second principal component is defined almost entirely by the contrast between NEs and common
nouns. It corresponds to none of Biber?s components; he had no NE features. Our third component has
loadings from 3rd person present tense verbs (and other verb forms) at one end, and past tense verbs,
third person pronouns, and person NEs at the other. It corresponds to Biber?s second component, which
had similar loadings for past tense verbs and third person pronouns, and somewhat less for present tense
verbs. He interpreted this dimension as representing the variation from non-narrative to narrative.
Our fourth component corresponds to Biber?s fifth, which he characterized as abstract versus non-
abstract. At one extreme we have commas, prepositions, sentence length (in tokens) and past participles,
with base verbs loading to some degree on the other extreme. The features loaded on Biber?s fifth
component were conjuncts, which might correlate with longer sentence length, past participles, and
agentless passives. In the corresponding scatterplots (Figures 4b and 5b), each datapoint (document
chunk) has been color-coded according to the six clusters found in the preceding section. There are
clearly distinct regions along the first component for spoken interactions (black), story telling (red),
offline interaction (pink) and discursive (blue), but with a great deal of overlap. Travel guides (green) and
technical (gold) are at the blue extreme, but at different locations along the second dimension. Moving
from left to right in Figure 4b), each next color has greater dispersion along the second component,
apart from green and gold, which have clearly separate locations from each other, at the top and bottom,
571
Story telling Discursive Offline Interaction Spoken Interaction Travel Guide Technical
Story Telling 0.00 0.23 0.13 0.06 0.63 0.91
Discursive 0.23 0.00 0.21 0.24 0.15 0.35
Offline Interaction 0.13 0.21 0.00 0.07 0.57 1.07
Spoken Interaction 0.06 0.24 0.07 0.00 0.68 0.88
Travel Guide 0.63 0.15 0.57 0.68 0.00 0.78
Technical 0.91 0.35 1.07 0.88 0.78 0.00
Table 1: Mean Bhattacharyya Distance of all Genre Pairs using PCA Scores
respectively. In Figure 5b), the overall dispersion is more even across both dimensions, with separate
centers for each of the four major colors (black, pink, red and blue), but again without sharp separation.
6 Genre Distance Measurement
A metric that summarizes how separable a pair of genres are in the defined PCA space would be more
convenient than the visualizations in Figures 4b and 5b. Bhattacharyya distance, which measures the
similarity of two discrete or continuous probability distributions, has been used in image segmentation
and signal selection, to minimize the probability of misclustering for segmentations (Coleman and An-
drews, 1979), or the probability of misclassifying different signals (Kailath, 1967). Here we illustrate its
use in summarizing the separability of a pair of genres across the four principal components.
In statistics, the Bhattacharyya distance measures the similarity of two discrete or continuous proba-
bility distributions. It is closely related to the Bhattacharyya coefficient, which measures the amount of
overlap between two statistical samples or populations.
The Bhattacharyya coefficient for two continuous probability distributions p(x) and q(x) is:
Bhattacharyya coefficient = ? =
?
C
?
q(x)p(x)dx
Where C is the domain of probability density p(x) and q(x). The Bhattacharyya coefficient takes on
values in [0,1]. Bhattacharyya distance maps the Bhattacharyya coefficient to [0,?]:
Bhattacharyya distance = B = ? ln ?
We take the mean Bhattacharyya Distance of a pair of genres across all four components as a summary
measure of seprability. As an illustration, consider the two clusters of offline interaction (pink) and
discursive text (blue) from Figures 4b) and 5b). Their Bhattacharyya Distances on the first through
fourth components, using the PCA scores, are: 0.05, 0.01, 0.14, 0.63. They have the largest distance on
the fourth component, the axis of abstract vs non-abstract, which is consistent with the visualizations.
The summary statistic is then the mean of the four individual distances: 0.21.
Table 1 gives the mean Bhattacharyya Distance of each pair of genres for the four components. The
pair of genres that is the closest on all four components is story telling and spoken interaction (0.06;
underlined). The pair that is the most distant on all four components is technical and offline interaction
(1.07; in bold). Bhattacharyya Distance can also be computed for each pair of genres using the original
normalized feature values. In three cases the Bhattacharyya Distance in the PCA space is the same as in
the original feature space, but in all other cases the Bhattacharyya Distance is much greater.
7 Discussion
Strong patterns of similarity in dimensions of variation across many genres of English emerge from our
comparison with Biber?s study, despite differences in the features used, the contrast between American
and British English, and the use of new media types. The results support the view that relatively stable
dimensions of variation arise from properties of the situations of use across varieties of English. This
applies as well to genres that did not exist in Biber?s time (email, twitter, spam), which group with the
interactive genre included in Biber?s corpus (letters) and are similar to other offline discourse despite
representing an interactive form?albeit an ?offline interactive? form?of discourse.
572
A significant departure from Biber?s results concerns the component defined primarily by Named Enti-
ties (NEs), which emerges as the second strongest dimension of variation in our study. This demonstrates
that additional features?in particular, features beyond those based on orthographic and morpho-syntactic
properties that have figured in most genre studies to date?can dramatically impact Biber?s original model
and extend the range of properties that can characterize particular text types. It also suggests that higher-
level linguistic properties and other more complex features can contribute substantially to genre charac-
terization and discrimination, a topic we plan to pursue in the future.
In what follows, we discuss similarities and differences in the two PCA analyses, the conclusions
this leads to regarding the feasibility of genre classification, and ways in which the analysis can support
retrieval, language modeling, and domain adaptation.
Our first principal component is very similar to Biber?s first factor, which he interpreted as differen-
tiating situations of use with more of an informational focus from those with an interactive or affective
function. In addition, he noted a contrast between online and offline production?i.e., spoken vs. written
production modes. The heavily loaded features the two analyses have in common are consistent with
the interpretation: 1st/2nd person pronouns, many verb features, and adverbs are at one pole, with word
length and nouns at the other. He claimed that this distinction is obviously a very powerful factor . . . not
an artifact of the factor extraction technique, meaning that it arises from differences between the de-
mands of face-to-face, online interaction and those of offline, expository discourse. Having found a very
similar dimension using different (correlated) features, we agree with this claim. Figure 4b) shows that
the spoken interaction documents in MASC fall on the ?involved? side of this dimension, while expository
texts fall on the ?informational? side.
Interestingly, the genres that did not exist in Biber?s time (email, twitter, spam) group with the inter-
active genre included in Biber?s corpus (letters), and they are similar to other offline discourse despite
representing an interactive form?albeit an ?offline interactive? form?of discourse. This provides a strong
argument for the validity of the first component and its link to underlying situational factors of language
use. In Figure 4b), the hypothetical centroid of the pink (offline interactive) region seems somewhat less
to the right on the x-axis than a corresponding centroid for the blue (expository) set, but the pink and
blue are relatively co-extensive, and in particular, are clearly separated from both the black (face-to-face
online interaction) and red (storytelling) genres. This makes intuitive sense, as storytelling genres often
depict face-to-face interaction (?so the elephant says to the camel?), and therefore mimic its immediacy.
Our second principal component is defined primarily by Named Entities (NEs), which has no correlate
in Biber?s study; his features included proper nouns but not NEs. Person NEs load with past tense
verbs and third person pronouns on our third component, which resembles Biber?s narrative dimension.
Most of the MASC genres seem to be dispersed all along our second dimension, suggesting that NE
frequency varies across texts in these genres; the exception is travel guides, which consistently include
larger numbers of NEs. The explanation here is less on production constraints than on function, as travel
guides survey geographical points of interest, historical monuments and persons, hotels and restaurants,
and so on.
As noted in Section 5, our third component is very similar to Biber?s second (narrative versus non-
narrative), and our fourth is somewhat similar to Biber?s fifth (abstract versus non-abstract). Note that
the fourth dimension shows a greater separation of expository (blue) and offline-interactive (pink) gen-
res, which substantially overlap on the first dimension. This provides a good example of how the 4-
dimensional visualization provided by the scatterplots reveals potentially very different relations among
genres across the components, which in turn explains why fixed definitions of genre are difficult, if not
impossible, and why genre classification can be hard to achieve. We observe that the genre classes can be
more or less separable on one dimension but not another. As another example, travel guides and technical
documents are at distinct locations on the second component, but span the same locations on the first.
This lack of separability on one or more dimensions is true for nearly all pairs of our six genre classes,
as well as for any pair of dimensions. This suggests that an application that requires genre classification
could use PCA to find dimensions of variation that lead to the best separation, and summarize the sepa-
rability using the mean Bhattacharyya distance. As the number of genres one needs to classify increases,
573
it could be that the number of orthogonal dimensions required to lead to the best separation might also
increase. In Table 1, for example, with the exception of the row for Discursive Text, all rows have at least
one cell with a value close to or above 0.80, indicating that each of the six genres can be clearly sepa-
rated from at least one other genre. We would predict that Discursive Text would be the most difficult to
classify using genre features alone.
The strong similarities among the major components in Biber?s study and ours support the view that
genre variation is continuous along multiple dimensions due to contextual properties such as cognitive
constraints, interactivity, and function. As such, we view the dimensions as arising from observable
properties of discourse situations. Given a new genre, it should be possible to predict where it would be
located in the PCA space defined here. We would predict that chats, for example, would pattern more
closely with face-to-face interaction than with offline interactive genres. The same methodology could be
applied to a sub-genre, such as the discursive texts, to discover more specific dimensions to differentiate
among them.
Because language use changes over time, and new genres arise, we do not view the 4-dimensions
as a definitive representation of genre space. We do, however, envision a concrete application of this
particular representation, namely to measure corpus similarity in a multivariate fashion. Because our
PCA analysis makes it possible to locate new documents in the defined space, it would be possible
to identify which MASC documents a new set of documents is most similar to. PCA scores could be
computed on the four dimensions for corresponding features in the new documents. This approach could
be used in any application where it is desirable to find similar documents, such as retrieval, language
modeling, or domain adaptation. For example, in recent work on domain adaptation of parsers, McClosky
et al. (2010) present a confusion matrix with six corpora to demonstrate how performance of a Charniak
parser (Charniak, 2000) varies depending on which corpus it is trained on. They assume that a new
target domain will be a mixture of their six source domains and build a simple regression (three features)
to predict which of the six parsers will perform best on a new corpus. They subsequently state that
an alternative approach could use a high-dimensional vector space to compare corpora. Inspired by
this suggestion, we are currently developing a web service that will allow researchers to locate their
corpora in the 4-dimensional space identified in this study, and to compute the values of their PCA
scores. This would make it possible to use Bhattacharrya distance as described in Section 6 to measure
the similarity of corpora in genre space, which could be quite relevant for adapting parsers or other
NLP tools. This contrasts with the similarity measures used in Ravi and Knight (Ravi et al., 2008) and
McClosky (McClosky et al., 2010), which are based on lexical features.
8 Conclusion
Using a relatively small set of under three dozen features to represent the linguistic forms in discourse,
PCA reveals four principal components of variation in a very heterogeneous corpus of post 1990s Amer-
ican English that are comparable to those identified in Biber?s work, as well as additional dimensions
based on features not included in that earlier study. Six genres derived from the MASC corpus using
hierarchical clustering are separable on some but not all components. These differences in separabil-
ity potentially explain the variations in performance across different works that do genre classification.
The resulting 4-dimensional genre space provides a basis for more detailed analysis of sub-genres, for
a better understanding of the relation between genre and situations of language use, and for predicting
the distributional properties of new genres. In future work, we plan to build on this basis to develop an
increasingly detailed and, at the same time, generalizable characterization of genre.
Our results depict a big picture for how discourse in English varies with respect to style or form,
and how different genres are conditioned by aspects of the situations of language use. We believe that
exploration of genre in these terms can provide a more viable approach to measuring distinctions among
texts than the approach used in most recent work, and can provide a more informed basis to incorporate
genre distinctions in information retrieval, language modeling, and domain adaptation for statistical NLP.
574
Acknowledgements
This work was supported in part by NSF CRI-1059312.
References
Douglas Biber. 1988. Variation across Speech and Writing. Cambridge University Press, Cambridge, UK.
Douglas Biber. 1993. The multi-dimensional approach to linguistic analyses of genre variation: An overview of
methodology and findings. Computers and the Humanities, 26:331?345.
Ivan Bulyko and Mari Ostendorf. 2003. Getting more mileage from web text sources for conversational speech
language modeling using class-dependent mixtures. In Proc. HLT-NAACL 2003, pages 7?9.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American Chapter
of the Association for Computational Linguistics Conference, NAACL 2000, pages 132?139, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guy Barrett Coleman and Harry C Andrews. 1979. Image segmentation by clustering. Proceedings of the IEEE,
67(5):773?785.
Sergey Feldman, Marius Marin, Julie Medero, and Mari Ostendorf. 2009. Classifying factored genres with part-
of-speech histograms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,
pages 173?176, Boulder, Colorado, June. Association for Computational Linguistics.
Stefan Th. Gries, John Newman, Cyrus Shaoul, and Philip Dilts. 2009. N-grams and the clustering of genres.
Paper presented at the workshop on Corpus, Colligation, Register Variation at the 31st Annual Meeting of the
Deutsche Gesellschaft fr Sprachwissenschaft.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Rebecca Passonneau. 2010. The Manually Annotated Sub-
Corpus: A Community Resource for and by the People. In Proceedings of the ACL 2010 Conference Short
Papers, pages 68?73, Uppsala, Sweden, July. Association for Computational Linguistics.
Rukmini Iyer and Mari Ostendorf. 1999. Relevance weighting for combining multi-domain data for n-gram
language modeling. Computer Speech & Language, 13(3):267?282.
Thomas Kailath. 1967. The divergence and Bhattacharyya distance measures in signal selection. Communication
Technology, IEEE Transactions on, 15(1):52?60.
Ioannis Kanaris and Efstathios Stamatatos. 2009. Learning to recognize webpage genres. Information Processing
and Management, 45(5):499?512, September.
Jussi Karlgren and Douglass Cutting. 1994. Recognizing text genres with simple metrics using discriminant
analysis. In Proceedings of the 15th Conference on Computational Linguistics - Volume 2, COLING ?94, pages
1071?1075, Stroudsburg, PA, USA. Association for Computational Linguistics.
Brett Kessler, Geoffrey Nunberg, and Hinrich Sch?utze. 1997. Automatic detection of text genre. In Proceedings
of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the
European Chapter of the Association for Computational Linguistics, ACL ?98, pages 32?38, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Adam Kilgarriff. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):1?37.
Teuvo Kohonen. 1995. Self-organizing Maps. Springer-Verlag, Berlin.
Yong-Bae Lee and Sung Hyon Myaeng. 2002. Text genre classification with genre-revealing and subject-revealing
features. In SIGIR ?02: Proceedings of the 25th annual international ACM SIGIR conference on Research and
development in information retrieval, pages 145?150, New York, NY, USA. ACM Press.
David Martinez and Eneko Agirre. 2000. One sense per collocation and genre/topic variations. In Proceedings
of the 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large
Corpora: Held in Conjunction with the 38th Annual Meeting of the Association for Computational Linguistics
- Volume 13, EMNLP ?00, pages 207?215, Stroudsburg, PA, USA. Association for Computational Linguistics.
575
David McClosky, Eugene Charniak, and Mark Johnson. 2010. Automatic domain adaptation for parsing. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 28?36, Stroudsburg, PA, USA. Association for Computational
Linguistics.
A. Mehler, S. Sharoff, and M. Santini. 2010. Genres on the Web: Computational Models and Empirical Studies.
Text, Speech and Language Technology. Springer.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.
Andreas Rauber and Alexander M?uller-K?ogler. 2001. Integrating automatic genre analysis into digital libraries.
In First ACM-IEEE Joint Conference on Digital Libraries, pages 1?10.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Automatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887?896, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad, Zadeh Kaljahi, and Anton Bryl. 2012. DUC-
Paris13 systems for the SANCL 2012 shared task.
Marina Santini. 2004. A shallow approach to syntactic feature extraction for genre classification. Technical Report
ITRI-04-02, Information Technology Research Institute, University of Brighton. Also published in Proceedings
of the 7th Annual Colloquium for the UK Special Interest Group for Computational Linguistics, Birmingham,
UK.
Ruhi Sarikaya, Agust??n Gravano, and Yuqing Gao. 2005. Rapid language model development using external re-
sources for new spoken dialog domains. In International Congress of Acoustics, Speech, and Signal Processing
(ICASSP), pages 573?576, Philadelphia, PA, USA. IEEE, Signal Processing Society.
Serge Sharoff, Zhili Wu, and Katja Markert. 2010. The web library of Babel: evaluating genre collections. In
Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on
Language Resources and Evaluation (LREC?10), Valletta, Malta, may. European Language Resources Associa-
tion (ELRA).
Efstathios Stamatatos, Nikos Fakotakis, and George Kokkinakis. 2000a. Text genre detection using common word
frequencies. In Proceedings of the 18th Conference on Computational Linguistics - Volume 2, COLING ?00,
pages 808?814, Stroudsburg, PA, USA. Association for Computational Linguistics.
Efstathios Stamatatos, George Kokkinakis, and Nikos Fakotakis. 2000b. Automatic text categorization in terms of
genre and author. Computational Linguistics, 26(4):471?495, December.
Wei Wang, Klaus Macherey, Wolfgang Macherey, Franz Och, and Peng Xu. 2012. Improved domain adaptation
for statistical machine translation. In AMTA-2012.
Nigel G. Ward and Steven D. Werner. 2013. Using dialog-activity similarity for spoken information retrieval. In
Fr?ed?eric Bimbot, Christophe Cerisara, C?ecile Fougeron, Guillaume Gravier, Lori Lamel, Franc?ois Pellegrino,
and Pascal Perrier, editors, 14th Annual Conference of the International Speech Communication Association,
Interspeech, pages 1569?1573. ISCA.
576
Proceedings of the ACL 2010 Conference Short Papers, pages 68?73,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Manually Annotated Sub-Corpus:
A Community Resource For and By the People
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Collin Baker
International Computer Science Institute
Berkeley, California USA
collinb@icsi.berkeley.edu
Christiane Fellbaum
Princeton University
Princeton, New Jersey USA
fellbaum@princeton.edu
Rebecca Passonneau
Columbia University
New York, New York USA
becky@cs.columbia.edu
Abstract
The Manually Annotated Sub-Corpus
(MASC) project provides data and annota-
tions to serve as the base for a community-
wide annotation effort of a subset of the
American National Corpus. The MASC
infrastructure enables the incorporation of
contributed annotations into a single, us-
able format that can then be analyzed as
it is or ported to any of a variety of other
formats. MASC includes data from a
much wider variety of genres than exist-
ing multiply-annotated corpora of English,
and the project is committed to a fully
open model of distribution, without re-
striction, for all data and annotations pro-
duced or contributed. As such, MASC
is the first large-scale, open, community-
based effort to create much needed lan-
guage resources for NLP. This paper de-
scribes the MASC project, its corpus and
annotations, and serves as a call for con-
tributions of data and annotations from the
language processing community.
1 Introduction
The need for corpora annotated for multiple phe-
nomena across a variety of linguistic layers is
keenly recognized in the computational linguistics
community. Several multiply-annotated corpora
exist, especially for Western European languages
and for spoken data, but, interestingly, broad-
based English language corpora with robust anno-
tation for diverse linguistic phenomena are rela-
tively rare. The most widely-used corpus of En-
glish, the British National Corpus, contains only
part-of-speech annotation; and although it con-
tains a wider range of annotation types, the fif-
teen million word Open American National Cor-
pus annotations are largely unvalidated. The most
well-known multiply-annotated and validated cor-
pus of English is the one million word Wall Street
Journal corpus known as the Penn Treebank (Mar-
cus et al, 1993), which over the years has been
fully or partially annotated for several phenomena
over and above the original part-of-speech tagging
and phrase structure annotation. The usability of
these annotations is limited, however, by the fact
that many of them were produced by independent
projects using their own tools and formats, mak-
ing it difficult to combine them in order to study
their inter-relations. More recently, the OntoNotes
project (Pradhan et al, 2007) released a one mil-
lion word English corpus of newswire, broadcast
news, and broadcast conversation that is annotated
for Penn Treebank syntax, PropBank predicate ar-
gument structures, coreference, and named enti-
ties. OntoNotes comes closest to providing a cor-
pus with multiple layers of annotation that can be
analyzed as a unit via its representation of the an-
notations in a ?normal form?. However, like the
Wall Street Journal corpus, OntoNotes is limited
in the range of genres it includes. It is also limited
to only those annotations that may be produced by
members of the OntoNotes project. In addition,
use of the data and annotations with software other
than the OntoNotes database API is not necessar-
ily straightforward.
The sparseness of reliable multiply-annotated
corpora can be attributed to several factors. The
greatest obstacle is the high cost of manual pro-
duction and validation of linguistic annotations.
Furthermore, the production and annotation of
corpora, even when they involve significant scien-
tific research, often do not, per se, lead to publish-
able research results. It is therefore understand-
68
able that many research groups are unwilling to
get involved in such a massive undertaking for rel-
atively little reward.
The Manually Annotated Sub-Corpus
(MASC) (Ide et al, 2008) project has been
established to address many of these obstacles
to the creation of large-scale, robust, multiply-
annotated corpora. The project is providing
appropriate data and annotations to serve as the
base for a community-wide annotation effort,
together with an infrastructure that enables the
representation of internally-produced and con-
tributed annotations in a single, usable format
that can then be analyzed as it is or ported to any
of a variety of other formats, thus enabling its
immediate use with many common annotation
platforms as well as off-the-shelf concordance
and analysis software. The MASC project?s aim is
to offset some of the high costs of producing high
quality linguistic annotations via a distribution of
effort, and to solve some of the usability problems
for annotations produced at different sites by
harmonizing their representation formats.
The MASC project provides a resource that is
significantly different from OntoNotes and simi-
lar corpora. It provides data from a much wider
variety of genres than existing multiply-annotated
corpora of English, and all of the data in the cor-
pus are drawn from current American English so
as to be most useful for NLP applications. Per-
haps most importantly, the MASC project is com-
mitted to a fully open model of distribution, with-
out restriction, for all data and annotations. It is
also committed to incorporating diverse annota-
tions contributed by the community, regardless of
format, into the corpus. As such, MASC is the
first large-scale, open, community-based effort to
create a much-needed language resource for NLP.
This paper describes the MASC project, its corpus
and annotations, and serves as a call for contribu-
tions of data and annotations from the language
processing community.
2 MASC: The Corpus
MASC is a balanced subset of 500K words of
written texts and transcribed speech drawn pri-
marily from the Open American National Corpus
(OANC)1. The OANC is a 15 million word (and
growing) corpus of American English produced
since 1990, all of which is in the public domain
1http://www.anc.org
Genre No. texts Total words
Email 2 468
Essay 4 17516
Fiction 4 20413
Gov?t documents 1 6064
Journal 10 25635
Letters 31 10518
Newspaper/newswire 41 17951
Non-fiction 4 17118
Spoken 11 25783
Debate transcript 2 32325
Court transcript 1 20817
Technical 3 15417
Travel guides 4 12463
Total 118 222488
Table 1: MASC Composition (first 220K)
or otherwise free of usage and redistribution re-
strictions.
Where licensing permits, data for inclusion in
MASC is drawn from sources that have already
been heavily annotated by others. So far, the
first 80K increment of MASC data includes a
40K subset consisting of OANC data that has
been previously annotated for PropBank predi-
cate argument structures, Pittsburgh Opinion an-
notation (opinions, evaluations, sentiments, etc.),
TimeML time and events2, and several other lin-
guistic phenomena. It also includes a handful of
small texts from the so-called Language Under-
standing (LU) Corpus3 that has been annotated by
multiple groups for a wide variety of phenomena,
including events and committed belief. All of the
first 80K increment is annotated for Penn Tree-
bank syntax. The second 120K increment includes
5.5K words of Wall Street Journal texts that have
been annotated by several projects, including Penn
Treebank, PropBank, Penn Discourse Treebank,
TimeML, and the Pittsburgh Opinion project. The
composition of the 220K portion of the corpus an-
notated so far is shown in Table 1. The remain-
ing 280K of the corpus fills out the genres that are
under-represented in the first portion and includes
a few additional genres such as blogs and tweets.
3 MASC Annotations
Annotations for a variety of linguistic phenomena,
either manually produced or corrected from output
of automatic annotation systems, are being added
2The TimeML annotations of the data are not yet com-
pleted.
3MASC contains about 2K words of the 10K LU corpus,
eliminating non-English and translated LU texts as well as
texts that are not free of usage and redistribution restrictions.
69
Annotation type Method No. texts No. words
Token Validated 118 222472
Sentence Validated 118 222472
POS/lemma Validated 118 222472
Noun chunks Validated 118 222472
Verb chunks Validated 118 222472
Named entities Validated 118 222472
FrameNet frames Manual 21 17829
HSPG Validated 40* 30106
Discourse Manual 40* 30106
Penn Treebank Validated 97 87383
PropBank Validated 92 50165
Opinion Manual 97 47583
TimeBank Validated 34 5434
Committed belief Manual 13 4614
Event Manual 13 4614
Coreference Manual 2 1877
Table 2: Current MASC Annotations (* projected)
to MASC data in increments of roughly 100K
words. To date, validated or manually produced
annotations for 222K words have been made avail-
able.
The MASC project is itself producing annota-
tions for portions of the corpus forWordNet senses
and FrameNet frames and frame elements. To de-
rive maximal benefit from the semantic informa-
tion provided by these resources, the entire cor-
pus is also annotated and manually validated for
shallow parses (noun and verb chunks) and named
entities (person, location, organization, date and
time). Several additional types of annotation have
either been contracted by the MASC project or
contributed from other sources. The 220K words
ofMASC I and II include seventeen different types
of linguistic annotation4, shown in Table 2.
All MASC annotations, whether contributed or
produced in-house, are transduced to the Graph
Annotation Framework (GrAF) (Ide and Suder-
man, 2007) defined by ISO TC37 SC4?s Linguistic
Annotation Framework (LAF) (Ide and Romary,
2004). GrAF is an XML serialization of the LAF
abstract model of annotations, which consists of
a directed graph decorated with feature structures
providing the annotation content. GrAF?s primary
role is to serve as a ?pivot? format for transducing
among annotations represented in different for-
mats. However, because the underlying data struc-
ture is a graph, the GrAF representation itself can
serve as the basis for analysis via application of
4This includes WordNet sense annotations, which are not
listed in Table 2 because they are not applied to full texts; see
Section 3.1 for a description of the WordNet sense annota-
tions in MASC.
graph-analytic algorithms such as common sub-
tree detection.
The layering of annotations over MASC texts
dictates the use of a stand-off annotation repre-
sentation format, in which each annotation is con-
tained in a separate document linked to the pri-
mary data. Each text in the corpus is provided in
UTF-8 character encoding in a separate file, which
includes no annotation or markup of any kind.
Each file is associated with a set of GrAF standoff
files, one for each annotation type, containing the
annotations for that text. In addition to the anno-
tation types listed in Table 2, a document contain-
ing annotation for logical structure (titles, head-
ings, sections, etc. down to the level of paragraph)
is included. Each text is also associated with
(1) a header document that provides appropriate
metadata together with machine-processable in-
formation about associated annotations and inter-
relations among the annotation layers; and (2) a
segmentation of the primary data into minimal re-
gions, which enables the definition of different to-
kenizations over the text. Contributed annotations
are also included in their original format, where
available.
3.1 WordNet Sense Annotations
A focus of the MASC project is to provide corpus
evidence to support an effort to harmonize sense
distinctions in WordNet and FrameNet (Baker and
Fellbaum, 2009), (Fellbaum and Baker, to appear).
The WordNet and FrameNet teams have selected
for this purpose 100 common polysemous words
whose senses they will study in detail, and the
MASC team is annotating occurrences of these
words in the MASC. As a first step, fifty oc-
currences of each word are annotated using the
WordNet 3.0 inventory and analyzed for prob-
lems in sense assignment, after which the Word-
Net team may make modifications to the inven-
tory if needed. The revised inventory (which will
be released as part of WordNet 3.1) is then used to
annotate 1000 occurrences. Because of its small
size, MASC typically contains less than 1000 oc-
currences of a given word; the remaining occur-
rences are therefore drawn from the 15 million
words of the OANC. Furthermore, the FrameNet
team is also annotating one hundred of the 1000
sentences for each word with FrameNet frames
and frame elements, providing direct comparisons
of WordNet and FrameNet sense assignments in
70
attested sentences.5
For convenience, the annotated sentences are
provided as a stand-alone corpus, with the Word-
Net and FrameNet annotations represented in
standoff files. Each sentence in this corpus is
linked to its occurrence in the original text, so that
the context and other annotations associated with
the sentence may be retrieved.
3.2 Validation
Automatically-produced annotations for sentence,
token, part of speech, shallow parses (noun and
verb chunks), and named entities (person, lo-
cation, organization, date and time) are hand-
validated by a team of students. Each annotation
set is first corrected by one student, after which it
is checked (and corrected where necessary) by a
second student, and finally checked by both auto-
matic extraction of the annotated data and a third
pass over the annotations by a graduate student
or senior researcher. We have performed inter-
annotator agreement studies for shallow parses in
order to establish the number of passes required to
achieve near-100% accuracy.
Annotations produced by other projects and
the FrameNet and Penn Treebank annotations
produced specifically for MASC are semi-
automatically and/or manually produced by those
projects and subjected to their internal quality con-
trols. No additional validation is performed by the
ANC project.
The WordNet sense annotations are being used
as a base for an extensive inter-annotator agree-
ment study, which is described in detail in (Pas-
sonneau et al, 2009), (Passonneau et al, 2010).
All inter-annotator agreement data and statistics
are published along with the sense tags. The re-
lease also includes documentation on the words
annotated in each round, the sense labels for each
word, the sentences for each word, and the anno-
tator or annotators for each sense assignment to
each word in context. For the multiply annotated
data in rounds 2-4, we include raw tables for each
word in the form expected by Ron Artstein?s cal-
culate alpha.pl perl script6, so that the agreement
numbers can be regenerated.
5Note that several MASC texts have been fully annotated
for FrameNet frames and frame elements, in addition to the
WordNet-tagged sentences.
6http://ron.artstein.org/resources/calculate-alpha.perl
4 MASC Availability and Distribution
Like the OANC, MASC is distributed without
license or other restrictions from the American
National Corpus website7. It is also available
from the Linguistic Data Consortium (LDC)8 for
a nominal processing fee.
In addition to enabling download of the entire
MASC, we provide a web application that allows
users to select some or all parts of the corpus and
choose among the available annotations via a web
interface (Ide et al, 2010). Once generated, the
corpus and annotation bundle is made available to
the user for download. Thus, the MASC user need
never deal directly with or see the underlying rep-
resentation of the stand-off annotations, but gains
all the advantages that representation offers. The
following output formats are currently available:
1. in-line XML (XCES9), suitable for use with
the BNCs XAIRA search and access inter-
face and other XML-aware software;
2. token / part of speech, a common input for-
mat for general-purpose concordance soft-
ware such as MonoConc10, as well as the
Natural Language Toolkit (NLTK) (Bird et
al., 2009);
3. CONLL IOB format, used in the Confer-
ence on Natural Language Learning shared
tasks.11
5 Tools
The ANC project provides an API for GrAF an-
notations that can be used to access and manip-
ulate GrAF annotations directly from Java pro-
grams and render GrAF annotations in a format
suitable for input to the open source GraphViz12
graph visualization application.13 Beyond this, the
ANC project does not provide specific tools for
use of the corpus, but rather provides the data in
formats suitable for use with a variety of available
applications, as described in section 4, together
with means to import GrAF annotations into ma-
jor annotation software platforms. In particular,
the ANC project provides plugins for the General
7http://www.anc.org
8http://www.ldc.upenn.edu
9XML Corpus Encoding Standard, http://www.xces.org
10http://www.athel.com/mono.html
11http://ifarm.nl/signll/conll
12http://www.graphviz.org/
13http://www.anc.org/graf-api
71
Architecture for Text Engineering (GATE) (Cun-
ningham et al, 2002) to input and/or output an-
notations in GrAF format; a ?CAS Consumer?
to enable using GrAF annotations in the Un-
structured Information Management Architecture
(UIMA) (Ferrucci and Lally, 2004); and a corpus
reader for importing MASC data and annotations
into NLTK14.
Because the GrAF format is isomorphic to in-
put to many graph-analytic tools, existing graph-
analytic software can also be exploited to search
and manipulate MASC annotations. Trivial merg-
ing of GrAF-based annotations involves simply
combining the graphs for each annotation, after
which graph minimization algorithms15 can be ap-
plied to collapse nodes with edges to common
subgraphs to identify commonly annotated com-
ponents. Graph-traversal and graph-coloring al-
gorithms can also be applied in order to iden-
tify and generate statistics that could reveal in-
teractions among linguistic phenomena that may
have previously been difficult to observe. Other
graph-analytic algorithms ? including common
sub-graph analysis, shortest paths, minimum span-
ning trees, connectedness, identification of artic-
ulation vertices, topological sort, graph partition-
ing, etc. ? may also prove to be useful for mining
information from a graph of annotations at multi-
ple linguistic levels.
6 Community Contributions
The ANC project solicits contributions of anno-
tations of any kind, applied to any part or all of
the MASC data. Annotations may be contributed
in any format, either inline or standoff. All con-
tributed annotations are ported to GrAF standoff
format so that they may be used with other MASC
annotations and rendered in the various formats
the ANC tools generate. To accomplish this, the
ANC project has developed a suite of internal tools
and methods for automatically transducing other
annotation formats to GrAF and for rapid adapta-
tion of previously unseen formats.
Contributions may be emailed to
anc@cs.vassar.edu or uploaded via the
ANC website16. The validity of annotations
and supplemental documentation (if appropriate)
are the responsibility of the contributor. MASC
14Available in September, 2010.
15Efficient algorithms for graph merging exist; see,
e.g., (Habib et al, 2000).
16http://www.anc.org/contributions.html
users may contribute evaluations and error reports
for the various annotations on the ANC/MASC
wiki17.
Contributions of unvalidated annotations for
MASC and OANC data are also welcomed and are
distributed separately. Contributions of unencum-
bered texts in any genre, including stories, papers,
student essays, poetry, blogs, and email, are also
solicited via the ANC web site and the ANC Face-
Book page18, and may be uploaded at the contri-
bution page cited above.
7 Conclusion
MASC is already the most richly annotated corpus
of English available for widespread use. Because
the MASC is an open resource that the commu-
nity can continually enhance with additional an-
notations and modifications, the project serves as a
model for community-wide resource development
in the future. Past experience with corpora such
as the Wall Street Journal shows that the commu-
nity is eager to annotate available language data,
and we anticipate even greater interest in MASC,
which includes language data covering a range of
genres that no existing resource provides. There-
fore, we expect that as MASC evolves, more and
more annotations will be contributed, thus creat-
ing a massive, inter-linked linguistic infrastructure
for the study and processing of current American
English in its many genres and varieties. In addi-
tion, by virtue of its WordNet and FrameNet anno-
tations, MASC will be linked to parallel WordNets
and FrameNets in languages other than English,
thus creating a global resource for multi-lingual
technologies, including machine translation.
Acknowledgments
The MASC project is supported by National
Science Foundation grant CRI-0708952. The
WordNet-FrameNet algnment work is supported
by NSF grant IIS 0705155.
References
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as complementary resources for
annotation. In Proceedings of the Third Linguistic
17http://www.anc.org/masc-wiki
18http://www.facebook.com/pages/American-National-
Corpus/42474226671
72
Annotation Workshop, pages 125?129, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media, 1st edition.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of ACL?02.
Christiane Fellbaum and Collin Baker. to appear.
Aligning verbs in WordNet and FrameNet. Linguis-
tics.
David Ferrucci and Adam Lally. 2004. UIMA: An
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10(3-4):327?348.
Michel Habib, Christophe Paul, and Laurent Viennot.
2000. Partition refinement techniques: an interest-
ing algorithmic tool kit. International Journal of
Foundations of Computer Science, 175.
Nancy Ide and Laurent Romary. 2004. International
standard for a linguistic annotation framework. Nat-
ural Language Engineering, 10(3-4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the Linguistic Annotation Workshop, pages
1?8, Prague, Czech Republic, June. Association for
Computational Linguistics.
Nancy Ide, Collin Baker, Christiane Fellbaum, Charles
Fillmore, and Rebecca Passonneau. 2008. MASC:
The Manually Annotated Sub-Corpus of American
English. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
Nancy Ide, Keith Suderman, and Brian Simms. 2010.
ANC2Go: A web application for customized cor-
pus creation. In Proceedings of the Seventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC), Valletta, Malta, May. European Lan-
guage Resources Association.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and
Nancy Ide. 2009. Making sense of word sense
variation. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 2?9, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Rebecca Passonneau, Ansaf Salleb-Aouissi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense an-
notation of polysemous words by multiple annota-
tors. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC), Valletta, Malta.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
73
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 47?55,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Anveshan: A Framework for Analysis of Multiple Annotators? Labeling
Behavior
Vikas Bhardwaj,
Rebecca J. Passonneau and Ansaf Salleb-Aouissi
Columbia University
New York, NY, USA
vsb2108@columbia.edu
(becky@cs|ansaf@ccls).columbia.edu
Nancy Ide
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Abstract
Manual annotation of natural language to
capture linguistic information is essen-
tial for NLP tasks involving supervised
machine learning of semantic knowledge.
Judgements of meaning can be more or
less subjective, in which case instead of
a single correct label, the labels assigned
might vary among annotators based on the
annotators? knowledge, age, gender, intu-
itions, background, and so on. We intro-
duce a framework ?Anveshan,? where we
investigate annotator behavior to find out-
liers, cluster annotators by behavior, and
identify confusable labels. We also in-
vestigate the effectiveness of using trained
annotators versus a larger number of un-
trained annotators on a word sense annota-
tion task. The annotation data comes from
a word sense disambiguation task for pol-
ysemous words, annotated by both trained
annotators and untrained annotators from
Amazon?s Mechanical turk. Our results
show that Anveshan is effective in uncov-
ering patterns in annotator behavior, and
we also show that trained annotators are
superior to a larger number of untrained
annotators for this task.
1 Credits
This work was supported by a research supple-
ment to the National Science Foundation CRI
award 0708952.
2 Introduction
Manual annotation of language data in order to
capture linguistic knowledge has become increas-
ingly important for semantic and pragmatic an-
notation tasks. A very short list of a few such
tasks illustrates the range of types of annotation,
in varying stages of development: predicate ar-
gument structure (Palmer et al, 2005b), dialogue
acts (Hu et al, 2009), discourse structure (Carbone
et al, 2004), opinion (Wiebe and Cardie, 2005),
emotion (Alm et al, 2005). The number of ef-
forts to create corpus resources that include man-
ual annotations has also been growing. A common
approach in assessing the resulting manual anno-
tations is to report a single quantitative measure
reflecting the quality of the annotations, either a
summary statistic such as percent agreement, or
an agreement coefficient from the family of met-
rics that include Krippendorff?s alpha (Krippen-
dorff, 1980) and Cohen?s kappa (Cohen, 1960).
We present some new assessment methods to use
in combination with an agreement coefficient for
understanding annotator behavior when there are
multiple annotators and many annotation values.
Anveshan (Annotation Variance Estimation)1 is
a suite of procedures for analyzing patterns of
agreement and disagreement among annotators,
as well as the distributions of annotation values
across annotators. Anveshan thus makes it pos-
sible to explore annotator behavior in more detail.
Currently, it includes three types of analysis: inter-
annotator agreement (IA) among all subsets of an-
notators, leverage of annotation values for outlier
detection, and metrics for comparing annotators?
distributions of annotation values (e.g., Kullbach-
Liebler divergence).
As an illustration of the utility of Anveshan, we
compare two groups of annotators on the same an-
notation word sense annotation tasks: a half dozen
trained annotators and fourteen Mechanical Turk-
ers. Previous work has argued that it can be cost
effective to collect multiple labels from untrained
labelers at a low cost per label, and to combine
the multiple labels through a voting method, rather
than to collect single labels from highly trained la-
1Anveshan is a Sanskrit word which literally means
search or exploration.
47
belers (Snow et al, 2008; Sheng et al, 2008; Lam
and Stork, 2003). The tasks included in (Snow et
al., 2008), for example, include word sense an-
notation; in contrast to our case, where the av-
erage number of senses per word is 9.5, the one
word sense annotation task had three senses. We
find that the same half dozen trained annotators
can agree well or not on sense labels for poly-
semous words. When they agree less well, we
find that it is possible to distinguish between prob-
lems in the labels (e.g., confusable senses) and
systematic differences of interpretation among an-
notators. When we use twice the number of Me-
chanical Turkers as trained annotators for three of
our ten polysemous words, we find inconsistent re-
sults.
The next section of the paper presents the moti-
vation for Anveshan and its relevance to the word
sense annotation task, followed by a section on
related work. The word sense annotation data is
given in section 5. Anveshan is described in the
subsequent section, followed by the results of its
application to the two data sets. We discuss the
comparison of trained annotators and Mechanical
Turkers, as well as differences among words, in
section 7. Section 7 concludes with a short recap
of Anveshan in general, and its application to word
sense annotations in particular.
3 Beyond Interannotator Agreement (IA)
Assessing the reliability of an annotation typically
addresses the question of whether different anno-
tators (effectively) assign the same annotation la-
bels. Various measures can be used to compare
different annotators, including agreement coeffi-
cients such as Krippendorff?s alpha (Krippendorff,
1980). Extensive reviews of the properties of such
coefficients have been presented elsewhere, e.g.,
(Artstein and Poesio, 2008). Briefly, an agree-
ment produce values in the interval [-1,1] indicat-
ing how much of the observed agreement is above
(or below) agreement that would be predicted by
chance (value of 0). To measure reliability in this
way is to assume that for most of the instances in
the data, there is a single correct response. Here
we present the use of reliability metrics and other
measures for word sense annotation, and we as-
sume that in some cases there may not be a single
correct response. When annotators have less than
excellent agreement, we aim to examine possible
causes.
We take word sense to be a problematic anno-
tation to perform, thus requiring a deeper under-
standing of the conditions under which annotators
might disagree. The many reasons can only be
touched on here. For example, word senses are
not discrete, atomic units that can be delimited and
enumerated. While dictionaries and other lexical
resoures, such as WordNet (Miller et al, 1993) or
the Hector lexicon (cf. SENSEVAL-1 (Kilgarriff
and Palmer, 2000)), do provide enumerations of
the senses for a given word, and their interrela-
tions (e.g., a list of senses, a tree of senses), it is
widely agreed that this is a convenient abstraction,
if for no other reason than the fact that words shift
meanings along with the communicative needs of
the groups of individuals who use them. The con-
text in which a word is used plays a significant role
in restricting the current sense. As a result, it is
often argued that the best representation for word
meaning would consist in clustering the contexts
in which words are used (Kilgarriff, 1997). Yet
even this would be insufficient because new com-
munities arise, new behaviors and artifacts emerge
along with them, hence new contexts of use and
new clusters. At the same time, contexts of use
and the senses that go along with them can fade
away (cf. the use of handbag discussed in (Kilgar-
riff, 1997) pertaining to disco dancing). Because
an enumeration of word senses is somewhat arti-
ficial, annotators might disagree on word senses
because they disagree on the boundaries between
one sense and another, just as professional lexi-
cographers do.
Apart from the artificiality of creating flat or
hierarchical sense inventories, the meanings of
words can vary in their subjectivity, due to differ-
ences in the perception or experience of individu-
als. This can be true for word senses that are inher-
ently relative, such as cold (as in, turn up the ther-
mostat, it?s too cold in here); or that derive their
meaning from cultural norms that may differ from
community to community, such as justice; or that
change as one grows older, e.g., whether a long
time to wait pertains to hours versus days.
Despite the arguments against using word sense
inventories, until they are replaced with an equally
convenient and more representative abstraction,
they are an extremely convenient computational
representation. We rely on WordNet senses, which
are presented to annotators with a gloss (defini-
tion) and with example uses. In order to better un-
48
derstand reasons for disagreement on senses, we
collect labels from multiple annotators. When an-
notators agree, having multiple annotators is re-
dundant. But when annotators disagree, having
multiple annotators is necessary in order to de-
termine whether the disagreement is due to noise
based on insufficiently clear sense definitions ver-
sus a systematic difference between individuals,
e.g., those who see a glass as half empty where
others see it as half full. To insure the opportu-
nity to observe how varied the labeling of a single
word can be, we collect word sense annotations
from multiple annotators. One potential benefit of
such investigation might be a better understanding
of how to model word meaning.
In sum, we hypothesize the following cases:
? Outliers: A small proportion of annotators
may assign senses in a manner that differs
markedly from the remaining annotators.
? Confusability of senses: If multiple annota-
tors assign multiple senses in an apparently
random fashion, it may be that the senses are
not sufficiently distinct.
? Systematic differences among subsets of an-
notators: If the same 50% of annotators al-
ways pick sense X where the remaining an-
notators always pick sense Y, it may be that
properties of the annotators, such as their age
cohort, account for the disagreement.
4 Related Work
There has been a decade-long community-wide ef-
fort to evaluate word sense disambiguation (WSD)
systems across languages in the four Senseval ef-
forts (1998, 2001, 2004, and 2007, cf. (Kilgarriff,
1998; Pedersen, 2002a; Pedersen, 2002b; Palmer
et al, 2005a)), with a corollary effort to investi-
gate the issues pertaining to preparation of man-
ually annotated gold standard corpora tagged for
word senses (Palmer et al, 2005a).
Differences in IA and system performance
across part-of-speech have been examined, as
in (Ng et al, 1999; Palmer et al, 2005a). Fac-
tors that have been proposed as affecting agree-
ment include whether annotators are allowed to as-
sign multilabels (Ve?ronis, 1998; Ide et al, 2002;
Passonneau et al, 2006), the number or granu-
larity of senses (Ng et al, 1999), merging of re-
lated senses (Snow et al, 2007), sense similar-
ity (Chugur et al, 2002), entropy (Diab, 2004;
Palmer et al, 2005a), and reactions times required
to distinguish senses (Klein and Murphy, 2002;
Ide and Wilks, 2006).
We anticipate that one of the ways in which the
data will be used will be to train machine learning
approaches to WSD. Noise in labeling and the im-
pact on machine learning has been discussed from
various perspectives. In (Reidsma and Carletta,
2008), it is argued that machine learning perfor-
mance does not vary consistently with interannota-
tor agreement. Through a simulation study, the au-
thors find that machine learning performance can
degrade or not with lower agreement, depending
on whether the disagreement is due to noise or sys-
tematic behavior. Noise has relatively little impact
compared with systematic disagreements. In (Pas-
sonneau et al, 2008), a similar lack of correla-
tion between interannotator agreement and ma-
chine learning performance is found in an empiri-
cal investigation.
5 Word Sense Annotation Data
5.1 Trained Annotator data
The Manually Annotated Sub-Corpus (MASC)
project (Ide et al, 2010) is creating a small,
representative corpus of American English written
and spoken texts drawn from the Open American
National Corpus (OANC).2 The MASC corpus
includes hand-validated or manual annotations
for a variety of linguistic phenomena. The first
MASC release, available as of May 2010, consists
of 82K words.3 One of the goals of MASC is
to support efforts to harmonize WordNet (Miller
et al, 1993) and FrameNet (Ruppenhofer et al,
2006), in order to bring the sense distinctions each
makes into better alignment.
We chose ten fairly frequent, moderately poly-
semous words for sense tagging. One hundred oc-
currences of each word were sense annotated by
five or six trained annotators. The ten words are
shown in Table 1, the words are grouped by part of
speech, with the number of WordNet senses, the
number of senses used by the trained annotators
(TAs), the number of annotators, and Alpha. We
call this the Trained annotator (TA) data.
We find that interannotator agreement (IA)
among half a dozen annotators varies depending
on the word. For ten words nearly balanced with
2http://www.anc.org
3http://www.anc.org/MASC/Home.html
49
Senses
Word-pos Avail. Used Ann Alpha
long-j 9 4 6 0.67
fair-j 10 6 5 0.54
quiet-j 6 5 6 0.49
time-n 10 8 5 0.68
work-n 7 7 5 0.62
land-n 11 9 6 0.49
show-v 12 10 5 0.46
tell-v 8 8 6 0.46
know-v 11 10 5 0.37
say-v 11 10 6 0.37
Table 1: Interannotator agreement on ten poly-
semous words: three adjectives, three nouns and
four verbs among trained annotators
respect to part of speech, we find a range of about
0.50 to 0.70 for nouns and adjectives, and about
0.37 to 0.46 for verbs. Table 1 shows the ten words
and the alpha scores for the same five or six an-
notators. The layout of the table illustrates both
that verbs have lower agreement than adjectives
or nouns, and that within each part of speech, an-
notators achieve varying levels of agreement, de-
pending on the word. The annotators, their level
of training, the number of sense choices, the anno-
tation tool, and other factors remain constant from
word to word. Thus we hypothesize that the differ-
ences in IA reflect differences in the degree of sub-
jectivity of the sense choices, the sense similarity,
or both. Anveshan is a data exploration framework
to help understand the differences in the ability of
the same annotators to agree well on sense anno-
tation for some words and not others.
As shown, annotators achieve respectable
agreement on long, time and work, and lower
agreement on the remaining words. Verbs have
lower agreement overall.
Figure 1 shows WordNet senses for long in the
form displayed to annotators, who used an annota-
tion GUI developed in Java. The sense number ap-
pears in the first column, followed by the glosses,
then sample phrases; only three senses are shown,
to conserve space. Note that annotators did not see
the WordNet synsets (sets of synonymous words)
for a given sense.
5.2 Mechanical Turk data
Amazon?s Mechanical Turk is a crowd-sourcing
marketplace where Human Intelligence Tasks
Senses
Word-pos Avail. Used Ann Alpha
long-j 9 9 14 0.15
fair-j 10 10 14 0.25
quiet-j 6 6 15 0.08
Table 2: Interannotator agreement on adjectives
among Mechanical Turk annotators
(HITs) such as sense annotation for words in a
sentence, can be set up and results from a large
number of annotators (or turkers) can be obtained
quickly. We used Mechanical Turk to obtain anno-
tations from 14 annotators on the set of adjectives
to analyze IA for a larger set of untrained annota-
tors.
The task was set up to get 150 occurrences an-
notated for each of the three adjectives: fair, long
and quiet, by 14 mechanical turk annotators each.
100 of these occurrences were the same as those
done by the trained annotators. For each word,
the 150 instances were divided into 15 HITs of 10
instances each. The average submit time of a HIT
was 200 seconds. We report the IA among the Me-
chanical Turk annotators using Krippendorff?s Al-
pha in Table 2. As shown, the turkers have poor
agreement, particularly on long and quiet, which
is at the chance level.
6 Anveshan
Anveshan: Annotation Variance Estimation, is
our approach to perform a more subtle analysis
of inter-annotator agreement. Anveshan uses sim-
ple statistical methods to achieve the three goals
identified in section 3: outlier detection, confus-
able senses, and distinct subsets of annotators that
agree with each other.
6.1 Method
This section uses the following notation to explain
Anveshan?s methodology:
We assume that we have n annotators annotat-
ingm senses. The probability of annotator a using
sense si is given by
Pa(S = si) =
count(si, a)
?m
j=1 count(sj , a)
where, count(si, a) is number of times si was
used by a.
50
1 primarily temporal sense; being or indicating a relatively great or greater than average duration or passage of time
or a duration as specified: ?a long life?; ?a long boring speech?; ?a long time?; ?a long friendship?;
?a long game?; ?long ago?; ?an hour long?
2 primarily spatial sense; of relatively great or greater than average spatial extension or extension as specified:
?a long road?; ?a long distance?; ?contained many long words?; ?ten miles long?
3 of relatively great height: ?a race of long gaunt men? (Sherwood Anderson); ?looked out the long French windows?
Figure 1: Three of the WordNet senses for ?Long?
Anveshan uses the Kullbach-Liebler divergence
(KLD), Jensen-Shannon divergence (JSD) and
Leverage to compare probability distributions.
The KLD of two probability distributions P and
Q is given by:
KLD(P,Q) =
?
i
P (i) log
P (i)
Q(i)
JSD is a modified version of KLD, it is also
known as total divergence to the average, and is
given by:
JSD(P,Q) =
1
2
KLD(P,M) +
1
2
KLD(Q,M)
where
M = (P + Q)/2
We define Leverage Lev of probability distribu-
tion P over Q as:
Lev(P,Q) =
?
k
|P (k) ?Q(k)|
We now compute the following statistics:
? For each annotator ai, we compute Pai .
? We compute Pavg, which is (
?
i Pai)/n.
? We compute Lev(Pai , Pavg),?i
? Then we compute JSD(Pai , Paj ) ?(i, j),
where i, j ? n and i 6= j
? Lastly, we compute a distance measure for
each annotator, by computing the KLD be-
tween each annotator and the average of
the remaining annotators, i.e. we get
?i,Dai = KLD(Pai , Q), where Q =
(
?
j 6=i Paj )/(n? 1)
These statistics give us a deeper understanding
of annotator behavior. Looking at the sense us-
age probabilities, we can identify how frequently
senses are used by an annotator. We can see how
much an annotator deviates from the average sense
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
A107	 ? A101	 ? A103	 ? A102	 ? A105	 ? A108	 ?
Figure 2: Distance measure (KLD) for Annotators
of long in TA Data
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
A101	 ? A102	 ? A103	 ? A105	 ? A107	 ? A108	 ?
101	 ?
102	 ?
999	 ?
103	 ?
108	 ?
Figure 3: Sense Usage distribution for long by an-
notators in TA Data
usage distribution by looking at Leverage. JSD be-
tween two annotators gives us a measure of how
close they are to each other. KLD of an annota-
tor with the remaining annotators shows us how
different the annotator is from the rest. In the fol-
lowing section we show results, which illustrate
the effectiveness of Anveshan in identifying use-
ful patterns in the data from the trained annotators
(TAs) and Mechanical Turkers (MTs).
6.2 Results
We used Anveshan on all data from TAs and MTs.
We were successful in correctly identifying out-
liers on many words. Also, analyzing the sense
usage patterns and observing the JSD and KLD
scores gave us useful insights on annotator differ-
ences. In the figures for this section, the six TAs
are represented by their unique identifiers (A101,
A102, A103, A105, A107, A108). Word senses
are identified by adding 100 to the WordNet sense
51
Word Old Alpha Ann Dropped New Alpha
long 0.67 1 0.80
land 0.49 1 0.54
know 0.377 1 0.48
tell 0.45 2 0.52
say 0.37 2 0.44
fair 0.54 2 0.63
Table 3: Increase in IA score by dropping annota-
tors (TA Data)
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
A102	 ?
A105	 ?
Figure 4: Sense usage patterns of annotators ?102?
and ?105? for show in TA Data
number. An additional ?None of the Above? label
is represented as 999; annotators select this when
no sense applies, when the word occurs as part of
a large lexical unit (collocation) with a clearly dis-
tinct meaning, or when the sentence is not a cor-
rect example for other reasons (e.g., wrong part of
speech).
Figure 2 shows the distance measure (KLD) for
each annotator from the rest of the annotators for
the word long with respect to the probability for
each of the four senses used (cf. Table 1). It can
be clearly seen that annotator A108 is an outlier.
A108 differs in her excessive use of label 999, as
shown in Figure 3. Indeed, by dropping A108,
we see that the IA score (Alpha) jumps from 0.67
to 0.8 for long. Similar results were obtained
for annotations for other words as well. Table 3
shows the jump in IA score after outlier(s) were
dropped.
Anveshan helps us differentiate between noisy
disagreement versus systematic disagreement.
The word show with 5 annotators has a low
agreement score of 0.45. By looking at the
sense distributions for the various annotators,
and observing annotation preferences for each
annotator, we can see that annotators A102 and
A105 have similar behavior (Figure 4, with a
pairwise alpha of 0.52 versus 0.46 for all five
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
A107	 ?
A108	 ?
Figure 5: Sense usage patterns of annotators ?107?
and ?108? for show in TA Data
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
105	 ? 102	 ? 104	 ? 103	 ? 101	 ? 999	 ? 108	 ? 106	 ? 107	 ? 110	 ? 109	 ?
Overall	 ?
A101	 ?
Figure 6: Sense usage distribution of annotator
?101? vs. the average of all annotators for show
in TA Data
annotators), and annotators A107 and A108 have
similar behavior (Figure 5, with a pairwise alpha
of 0.53). In contrast, Annotator A101 has very
distinct preferences (Figure 6). This behavior
is captured by computing JSD scores among all
pairs of annotators. As can be seen in Figure 7,
the pairs A102-A105 and A107-A108 have very
low JSD values, indicating similarity in annotator
behavior. At the same time we also see the pairs
having A101 in them have a much higher JSD
score, which is attributed to the fact that A101
is different from everyone else. If we look at
corresponding Alpha scores, we see that pairs
having low JSD values have higher agreement
scores and vice versa.
Observing the sense usage distributions also
helps us identify confusable senses. For example,
Figure 8 shows us the differences in sense usage
patterns of A101, A103 and the average of all
annotators for the word say. We can see that
A101 and A103 deviate in distinct ways from the
average. A101 prefers sense 101 whereas A103
prefers sense 102. This indicates that sense 101
and 102 might be confusable. Sense 1 is given
as ?expressing words?; sense 2 as ?report or
maintain?.
52
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
A105	 ? A108	 ? A105	 ? A102	 ? A107	 ? A108	 ?
A102	 ? A107	 ? A101	 ? A101	 ? A101	 ? A101	 ?
JSD	 ?
Alpha	 ?
Figure 7: JSD and Alpha scores for pairs of anno-
tators for show in TA Data
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
Overall	 ? A101	 ? A103	 ?
102	 ?
101	 ?
108	 ?
103	 ?
Figure 8: Sense usage distribution for say in TA
Data for annotators ?101? and ?103?
0	 ?
0.05	 ?
0.1	 ?
0.15	 ?
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
A102	 ? A105	 ? A108	 ? A101	 ? A107	 ?
Figure 9: Distance measure (KLD) for annotators
of work in TA Data
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
1.2	 ?
Overall	 ? 	 ?	 ?A101	 ? 	 ?	 ?A102	 ? 	 ?	 ?A104	 ? 	 ?	 ?A107	 ? 	 ?	 ?A108	 ? 	 ?	 ?A111	 ? 	 ?	 ?A112	 ? 	 ?	 ?A115	 ? 	 ?	 ?A116	 ? 	 ?	 ?A117	 ? 	 ?	 ?A118	 ? 	 ?	 ?A119	 ? 	 ?	 ?A120	 ? 	 ?	 ?A121	 ?
106	 ?109	 ?999	 ?108	 ?103	 ?102	 ?101	 ?
Figure 10: Sense usage distribution among MTs
for long
0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1	 ?
1.2	 ?
A10
1-??TA
	 ?
A10
2-??TA
	 ?
A10
3-??TA
	 ?
A10
4-??TA
	 ?
A10
5-??TA
	 ?
A10
2-??M
T	 ?
A10
6-??M
T	 ?
A10
7-??M
T	 ?
A10
8-??M
T	 ?
A11
4-??M
T	 ?
105	 ?
999	 ?
102	 ?
101	 ?
Figure 11: Sense usage distribution among TAs
and MTs for fair
Anveshan not only helps us understand under-
lying patterns in annotator behavior and remove
noise from IA scores, but also helps identify
cases where there is no noise and no systematic
subsets of annotators that agree with each other.
An example can be seen in for the noun work. We
observed that the annotators do not have largely
different behavior, which is reflected in Figure 9.
As none of the annotators are significantly differ-
ent from the others, the KLD scores are low and
the plotted line does not have any steep rises, as
seen in Figure 2.
Similar to the results for TA data, Anveshan
was successful in identifying outliers in Mechan-
ical Turk data as well. In order to compare the
agreement among TAs and MTs, we looked at IA
scores of all subsets of annotators for the three ad-
jectives in the Mechanical Turk data. We observed
that MTs used much more senses than TAs for all
words and that there was a lot of noise in sense us-
age distribution. Figure 10 illustrates the sense us-
age statistics for long among MTs, for frequently
used senses.
We also looked at agreement scores among all
subsets of MTs to see if there are any subsets of
annotators who agree as much as TAs, and we ob-
served that for both long and quiet, there were no
53
subsets of MT annotators whose agreement was
comparable or greater than the same number of the
TAs, however for fair, we found one set of 5 an-
notators whose IA score (0.61) was greater than
the IA score (0.54) of trained annotators. We also
observed that among both these pairs of annota-
tors, the frequently used senses were the same, as
illustrated in Figure 11. Still, the two groups of an-
notators have sufficiently distinct sense usage that
the overall IA for the combined set drops to 0.43.
7 Conclusion and Future Work
For annotations on a subjective task, there are
cases where there is no single correct label. In
this paper, we presented Anveshan, an approach to
study annotator behavior and to explore datasets
with multiple annotators, and with a large set of
annotation values. Here we looked at data from
half a dozen trained annotators and fourteen un-
trained Mechanical Turkers on word sense anno-
tation for polysemous words. The analysis using
Anveshan provided many insights into sources of
disagreement among the annotators.
We learn that IA Scores do not give us a com-
plete picture and it is necessary to delve deeper
and study annotator behavior in order to identify
noise possibly due to sense confusability, to elim-
inate noise due to outliers, and to identify system-
atic differences where subsets of annotators have
much higher IA than the full set.
The results from Anveshan are encouraging and
the methodology can be readily extended to study
patterns in human behavior. We plan to extend
our work by looking at JSD scores of all subsets
of annotators instead of pairs, to identify larger
subsets of annotators who have similar behavior.
We also plan to investigate other statistical meth-
ods of outlier detection such as the orthogonalized
Gnanadesikan-Kettenring estimator.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard
Sproat. 2005. Emotions from text: machine learn-
ing for text-based emotion prediction. In HLT ?05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 579?586, Morristown, NJ,
USA. Association for Computational Linguistics.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Marco Carbone, Yaakov Gal, Stuart Shieber, and Bar-
bara Grosz. 2004. Unifying annotated discourse hi-
erarchies to create a gold standard. In Proceedings
of the 5th Sigdial Workshop on Discourse and Dia-
logue.
Irina Chugur, Julio Gonzalo, and Felisa Verdejo. 2002.
Polysemy and sense proximity in the senseval-2 test
suite. In Proceedings of the SIGLEX/SENSEVAL
Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 32?39,
Philadelphia.
Jacob Cohen. 1960. A coeffiecient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37?46.
Mona Diab. 2004. Relieving the data acquisition bot-
tleneck in word sense disambiguation. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, pages 303?311.
Jun Hu, Rebecca J. Passonneau, and Owen Rambow.
2009. Contrasting the interaction structure of an
email and a telephone corpus: A machine learning
approach to annotation of dialogue function units.
In Proceedings of the 10th SIGDIAL on Dialogue
and Discourse.
Nancy Ide and Yorick Wilks. 2006. Making sense
about sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation: Algorithms and Appli-
cations, pages 47?74, Dordrecht, The Netherlands.
Springer.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002.
Sense discrimination with parallel corpora. In Pro-
ceedings of ACL?02 Workshop on Word Sense Dis-
ambiguation: Recent Successes and Future Direc-
tions, pages 54?60, Philadelphia.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: A community resource for and by the
people. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden.
Adam Kilgarriff and Martha Palmer. 2000. Introduc-
tion to the special issue on senseval. Computers and
the Humanities, 34:1?2.
Adam Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Adam Kilgarriff. 1998. SENSEVAL: An exercise in
evaluating word sense disambiguation programs. In
Proceedings of the First International Conference
on Language Resources and Evaluation (LREC),
pages 581?588, Granada.
Devra Klein and Gregory Murphy. 2002. Paper has
been my ruin: Conceptual relations of polysemous
words. Journal of Memory and Language, 47:548?
70.
54
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Chuck P. Lam and David G. Stork. 2003. Evaluating
classifiers by means of test data with noisy labels.
In Proceedings of the 18th International Joint Con-
ference on Artificial Intelligence (IJCAI-03), pages
513?518, Acapulco.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993. In-
troduction to WordNet: An on-line lexical database
(revised). Technical Report Cognitive Science
Laboratory (CSL) Report 43, Princeton University,
Princeton. Revised March 1993.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. A case study on inter-annotator agreement for
word sense disambiguation. In SIGLEX Workshop
On Standardizing Lexical Resources.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2005a. Making fine-grained and coarse-
grained sense distinctions. Journal of Natural Lan-
guage Engineering, 13.2:137?163.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005b. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71?106.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a mul-
tilingual semantic annotation task. In Proceedings
of the International Conference on Language Re-
sources and Evaluation (LREC), pages 1951?1956,
Genoa, Italy.
Rebecca Passonneau, Tom Lippincott, Tae Yano, and
Judith Klavans. 2008. Relation between agreement
measures on human labeling and machine learning
performance: results from an art history domain. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC),
pages 2841?2848.
Ted Pedersen. 2002a. Assessing system agreement
and instance difficulty in the lexical sample tasks of
Senseval-2. In Proceedings of the ACL-02Workshop
on Word Sense Disambiguation: Recent Successes
and Future Directions, pages 40?46.
Ted Pedersen. 2002b. Evaluating the effectiveness of
ensembles of decision trees in disambiguating SEN-
SEVAL lexical samples. In Proceedings of the ACL-
02 Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 81?87.
Dennis Reidsma and Jean Carletta. 2008. Reliabil-
ity measurement without limits. Comput. Linguist.,
34(3):319?326.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. Available from
http://framenet.icsi.berkeley.edu/index.php.
Victor S. Sheng, Foster Provost, and Panagiotis G.
Ipeirotis. 2008. Get another label? improving data
quality and data mining using multiple, noisy label-
ers. In Proceeding of the 14th ACM SIG KDD Inter-
national Conference on Knowledge Discovery and
Data Mining, pages 614?622, Las Vegas.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2007.
Learning to merge word senses. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 1005?
1014, Prague.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but
is it good? evaluating non-expert annotations for
natural language tasks. In Proceedings of Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 254?263, Honolulu.
Jean Ve?ronis. 1998. A study of polysemy judgements
and inter-annotator agreement. In SENSEVAL Work-
shop, pages Sussex, England.
Janyce Wiebe and Claire Cardie. 2005. Annotating
expressions of opinions and emotions in language.
language resources and evaluation. In Language
Resources and Evaluation (formerly Computers and
the Humanities, page 2005.
55
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 247?255,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Anatomy of Annotation Schemes:
Mapping to GrAF
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, NY, USA
ide@cs.vassar.edu
Harry Bunt
Tilburg Center for Creative Computing
Tilburg University, The Netherlands
harry.bunt@uvt.nl
Abstract
In this paper, we apply the annota-
tion scheme design methodology defined
in (Bunt, 2010) and demonstrate its use
for generating a mapping from an exist-
ing annotation scheme to a representa-
tion in GrAF format. The most impor-
tant features of this methodology are (1)
the distinction of the abstract and con-
crete syntax of an annotation language;
(2) the specification of a formal seman-
tics for the abstract syntax; and (3) the
formalization of the relation between ab-
stract and concrete syntax, which guar-
antees that any concrete syntax inherits
the semantics of the abstract syntax, and
thus guarantees meaning-preserving map-
pings between representation formats. By
way of illustration, we apply this map-
ping strategy to annotations from ISO-
TimeML, PropBank, and FrameNet.
1 Introduction
The Linguistic Annotation Framework (LAF, (Ide
and Romary, 2004); ISO 24612, 2009) defines
an abstract model for annotations together with
an XML serialization of the model, the Graph
Annotation Format (GrAF, (Ide and Suderman,
2007)). GrAF is intended to be a pivot format ca-
pable of representing diverse annotation types of
varying complexity, guaranteeing syntactic con-
sistency among the different annotations. GrAF
does not address the issue of semantic consis-
tency among annotation labels and categories; this
is assumed to be handled by other standardiza-
tion efforts such as ISOCat (Kemps-Snijders et al,
2009). ISOCat provides a set of data categories at
various levels of granularity, each accompanied by
a precise definition of its linguistic meaning. La-
bels applied in a user-defined annotation scheme
should be mapped to these categories in order to
ensure semantic consistency among annotations of
the same phenomenon.
While the mapping of annotation labels to a
common definition, coupled with the syntactic
consistency guaranteed by GrAF, takes a giant
step towards the harmonization of linguistic an-
notations, this is still not enough to ensure that
these annotations are sufficiently compatible to en-
able merging, comparison, and manipulation with
common software. For this, the conceptual struc-
ture of the annotation, in terms of the structural
relations among the defined annotation categories,
must also be consistent. It is therefore necessary to
consider this aspect of annotation scheme design
in order to achieve a comprehensive treatment of
the requirements for full harmonization of linguis-
tic annotations.
In (Bunt, 2010), a design methodology for se-
mantic annotation schemes is proposed, devel-
oped during the ISO project ?Semantic annota-
tion framework, Part 1: Time and events? (?Se-
mAF/Time?, for short), which is currently near-
ing completion (see ISO DIS 24617-1, 2009). The
methodology includes a syntax that specifies both
a class of representation structures and a class
of more abstract annotation structures. These
two components of the language specification are
called its concrete and abstract syntax, respec-
tively. A distinguishing feature of the proposed
methodology is that the semantics is defined for
the structures of the abstract syntax, rather than
for the expressions that represent these structures.
In this paper, we generalize the design method-
ology defined in (Bunt, 2010) and demonstrate
its use for generating a mapping from an ex-
isting annotation scheme to a representation in
GrAF format. By way of illustration, we apply
the mapping strategy to annotations from ISO-
TimeML (ISO, 2009), PropBank (Palmer et al,
2005), and FrameNet (Baker et al, 1998).
247
2 Background
The process of corpus annotation may consist of
attaching simple labels to textual elements, such
as part of speech and syntactic designations and
named entity tags. For more complex types of
annotation, annotations include a variety of ad-
ditional information about linguistic features and
relations. This is especially true for the kinds
of semantic annotation that have recently begun
to be undertaken in earnest, including semantic
role labeling (e.g., FrameNet and PropBank) and
time and event annotation (e.g., TimeML). How-
ever, these annotation schemes are not always de-
signed based on formal principles, and as a result,
comparing or merging information?even from two
schemes annotating the same phenomenon?can be
difficult or impossible without substantial human
effort.
A major source of difficulties in interpreting an-
notation scheme content is that information in the
annotation is implicit rather than explicit, making
(especially) structural relations among parts of the
linguistic information ambiguous. This often re-
sults from the use of an impoverished representa-
tion scheme, which provides only minimal mech-
anisms for bracketing and association. Consider,
for example, the two annotation fragments below,
expressed with parenthetic bracketing, taken from
a computational lexicon:
(1) (SUBC ((NP-TO-INF-LOC) (NP-PP)))
(2) (FEATURES ((NHUMAN) (COUNTABLE)))
In (1), the bracketed information is a list of alter-
natives, whereas in (2), it is a set of properties, but
there is no way to automatically distinguish the
two in order to process them differently. Another
example comes from PropBank:
wsj/00/wsj_0003.mrg 13 6 gold have.03
vn--a 0:2-ARG0 6:0-rel 7:1-ARG1
10:1-ARGM-ADV
Because of the ?flat? representation1, it is im-
possible to automatically determine if the mor-
phosyntactic descriptor ?vn?a? is associated with
the element annotated as ?rel?, vs. the ?gold?
descriptor that is (assumedly) associated with the
entire proposition. In both of these examples,
linguistically-informed humans have little diffi-
culty determining the structure because of the
knowledge they bring to the interpretation. This
knowledge is then embedded in the processing
1In PropBank annotation, this information appears on a
single line.
software so that the data are processed properly;
however, because it is not a part of the represen-
tation itself, it is not available to others who may
develop software for other kinds of processing.
To avoid these problems, annotation scheme de-
sign in ISO projects is split into two phases: the
specification of (1) an abstract model consisting
of annotation categories and structures and (2)
specification of (possibly multiple) representation
structures. An abstract model of annotation struc-
tures is typically implemented via development of
a ?metamodel?, i.e. a listing of the categories
of entities and relations to be considered, often
visualized by a UML-like diagram?i.e., a graph.
Schemes described via this method are trivially
mappable to GrAF, ensuring that syntactic con-
sistency among the different schemes, whatever
their original representation structures may be, is
achievable. It also ensures that these schemes are
trivially mappable to different representation for-
mats that are used in various software systems,
e.g., GATE, UIMA, NLTK, GraphViz, etc.
3 Anatomy of an annotation scheme
As specified in (Bunt, 2010), an annotation
scheme consists of a syntax that specifies a class of
more abstract annotation structures (the abstract
syntax) and a class of representation structures (the
concrete syntax), plus a semantics associated with
the abstract syntax.
3.1 Abstract syntax
The abstract syntax of an annotation scheme de-
fines the set-theoretical structures which constitute
the information that may be contained in annota-
tions. It consists of (a) a specification of the el-
ements from which these structures are built up,
called a conceptual inventory; and (b) annota-
tion construction rules, which describe the possi-
ble combinations of these elements into annota-
tion structures. The semantics of the annotation
scheme components is defined for the annotation
structures of the abstract syntax; Bunt (2010) pro-
vides a formal specification of the semantics of
ISO-TimeML in terms of Discourse Representa-
tion Structures (Kamp and Reyle, 1993), and de-
fines the class of concrete representations of the
structures defined by the abstract syntax.
For example, a fragment of the ISO-TimeML2
2All references to ISO-TimeML are based on the state
of the project as documented in ISO 264617-1:2009(E) from
248
conceptual inventory includes:3
? finite sets of elements called event types,
tenses, aspects, signatures, cardinalities, and
veracities.
? finite sets of elements called temporal rela-
tions, duration relations, event subordination
relations, aspectual relations, etc.
The annotation construction rules for ISO-
TimeML specify how to construct two types
of annotation structures: entity structures and
link structures. One type of entity structure,
called an event structure, is defined as a 6-tuple
?e, t, a, s, k, v? where e is a member of the set of
event types; t and a are a tense and an aspect,
respectively; s is a signature (a set-theoretical
type that is used for handling quantification over
events); k is a cardinality, used for expressing in-
formation about the size of a set of events in-
volved in a quantified relation; and v is a verac-
ity, which is used to represent whether an event is
claimed to have occurred, or claimed not to have
occurred (for dealing with positive and negative
polarity, respectively), or to have yet another sta-
tus such as ?possibly? or ?requested?, for handling
such cases as Please come back later today. A
time-amount structure is a pair ?n, u? or a triple
?R,n, u?, where n is a real number, R a numerical
relation, and u a temporal unit. The rules also de-
fine a link structure called an event duration struc-
ture as a triple ?event structure, time-amount
structure, duration relation?.
3.2 Concrete syntax
The concrete syntax provides the representation of
annotation structures defined in the abstract syn-
tax. A concrete syntax is said to be ideal for
a given abstract syntax if there is a one-to-one
correspondence between the structures defined by
the abstract syntax and those defined by the con-
crete syntax. An ideal concrete syntax RF1 de-
fines a function F1 from annotation structures to
RFi-representations, and an inverse function F
?1
i
from RF1-representations to annotation structures.
In other words, the abstract and the concrete syn-
tax are isomorphic. Since this holds for any ideal
concrete syntax, it follows that any two ideal rep-
resentation formats are isomorphic. Given two
September 2009.
3See (Bunt, 2010) for the full specification for ISO-
TimeML.
<isoTimeML-ICS1rep xml:id="a1">
<EVENT xml:id="e1" anchor="t2"
type ="FAST" tense=PAST
signature="INDIVIDUAL"/>
<TIME-AMOUNT xml:id="ta1"
anchor="t4" numeral="2" unit="day"/>
<MLINK event="e1"
duration="ta1" relType="FOR"/>
</isoTimeML-ICS1rep>
Tokens: [It1][fastedt2][fort3][twot4][dayst5].
Figure 1: ISO-TimeML ICS1 annotation
ideal representation formats RFi and RFj we can
define a homomorphic mapping Cij from RFi-
representations to RFj-representations by
(1) Cij =D Fj ? F?1i , i.e. Cij(r) = Fj(F
?1
i (r))
for any RFi-representation r
and conversely, we can define a homomorphic
mapping Cji from RFj-representations to RFi-
representations by
(2) Cji =D Fi ? F?1j , i.e. Cji(r) = Fi(F
?1
j (r))
for any RFj-representation r
These two mappings constitute conversions from
one format to the other, that is, they constitute
one-to-one meaning-preserving mappings: if ?(r)
denotes the meaning of representation r, then
?(Cij(r)) = ?(r) for any Fi-representation r,
and conversely, ?(Cji(r?)) = ?(r?) for any Fj-
representation r?.
Figure 1 shows a rendering of the sentence I
fasted for two days using a concrete XML-based
syntax for the annotation structures defined by
the ISO-TimeML abstract syntax, called the ICS-1
format, as described in (Bunt, 2010).
4 GrAF overview
GrAF is an exchange or pivot format intended to
simplify the processes of merging of annotations
from different sources and using annotations with
different software systems. The underlying data
model is a directed acyclic graph, which is iso-
morphic to UML-like structures that may be used
to define an abstract syntax for a given annotation
scheme, as described in section 3.
GrAF is an XML serialization of a formal graph
consisting of nodes and edges, either or both
of which are decorated with feature structures.
Nodes may have edges to one or more other nodes
249
<node xml:id="fn-n1"/>
<a label="FE" ref="fn-n1" as="FrameNet">
<fs>
<f name="FE" value="Recipient"/>
<f name="GF" value="Obj"/>
<f name="PT" value="NP"/>
</fs>
</a>
<edge id="e1" from="fn-n1"
to="fntok-n5"/>
Figure 2: FrameNet frame element annotation in
GrAF
in the graph, or they may be linked directly to re-
gions within the primary data that is being anno-
tated. The feature structure attached to a node or
edge provides the content of the annotation?that
is, the associated linguistic information expressed
as a set of attribute-value pairs. The feature struc-
tures in GrAF conform to formal feature struc-
ture specifications and may be subjected to op-
erations defined over feature structures, including
subsumption and unification. As a result, any rep-
resentation of an annotation in GrAF must consist
of a feature structure that provides all of the rele-
vant linguistic information.
Figure 2 shows a fragment of a FrameNet frame
element annotation, serialized in GrAF XML. It
consists of a graph node with id ?fn-n1? and an an-
notation with the label ?FE?4. The ref attribute on
the <a> (annotation) element associates the anno-
tation with node ?fn-n1?. The annotation contains
a feature structure with three features: FE (Frame
element), GF (Grammatical Function), and PT
(Phrase Type). An edge connects the node to an-
other node in the graph with the id ?fntok-n5? (not
shown here), which is associated with annotation
information for a token that in turn references the
span of text in primary data being annotated.
5 Mapping to GrAF
LAF specifies that an annotation representation R
is valid if it is mappable to a meaning-preserving
representation in GrAF, and that its GrAF repre-
sentation is in turn mappable to R. In terms of
the definitions in section 3, a LAF-valid repre-
sentation R is one where ?(R) = ?(CRG(R))
and ?(G) = ?(CGR(G)), where G is a GrAF
4Note that the value of the label attribute is, for practical
purposes, a convenience; it is used primarily when generating
alternative representation formats.
representation. We can also define a valid anno-
tation scheme in terms of conversion transitivity
through GrAF; that is, for two arbitrary annotation
schemes R and S, the following holds:
?(R) = ?(CRG(R)) = ?(CGS(S))
Our goal here is to provide a formal speci-
fication for the mapping function CRG, assum-
ing the existence of a formal specification of
an annotation scheme as outlined in section 3.
To accomplish this, it is necessary to identify
the two components of an abstract syntax for
annotation scheme R: the conceptual inventory
and the annotation construction rules that indi-
cate how elements of the conceptual inventory are
combined into annotation structures?specifically,
entity structures, which describe annotation ob-
jects, and link structures, which describe relations
among entity structures. Once these are available,
a general procedure for establishing a GrAF repre-
sentation of the annotation structures is as follows:
For each type of entity structure e:
? introduce a label Le, where Le is the entity
structure type;
? define a set of features f corresponding one-
to-one with the components of the n-tuple
of elements from the conceptual inventory
defining entity structure e.
A link structure is a triple ?E1, E2, r? consisting
of two sets of entity structures and a relational el-
ement defining a relation between them. For each
type of link structure:
1. introduce a label Lr, where Lr is the type
name of relation r.
2. If r is associated with a set of elements from
the conceptual inventory, then features are
created as in (2), above.
In GrAF, an annotation A consists of a label L
and a feature structure containing a set of features
f . Annotations may be associated with nodes or
edges in the graph. Typically, entity structures are
associated with nodes that have links into a region
of primary data or one or more edges connecting it
to other nodes in the graph. Link structures are as-
sociated with edges, identifying a relation among
two or more entity structures. In the simplest case,
a link structure consists of a relation between two
250
entity structures, each of a given type; in the cor-
responding GrAF representation, the link structure
label is associated with an edge d that connects
nodes n1, n2, each of which is decorated with an-
notations labeled L1, L2, respectively.
For example, for the ISO-TimeML abstract
syntax fragment provided in section 3, we de-
fine the labels EVENT and INSTANT cor-
responding to the two entity structures with
names event structure and time amount struc-
ture, and a link structure TIME-ANCHORING.
Because an event structure is defined as a 6-
tuple ?e, t, a, s, k, v?, we define six features event,
tense, aspect signature, cardinality, and verac-
ity.5 A time-amount structure may be a pair
?n, u? or a triple ?R,n, u?, where n is a real
number, R a numerical relation, and u a tem-
poral unit, so we introduce features numeral,
unit, and relType. Finally, the time anchoring
link structure is a triple ?event structure, time-
amountstructure, duration relation?. In this
case, the first two elements of the triple are the
entity structures being linked; these will be repre-
sented as nodes in the GrAF implementation. The
label and features associated with each entity and
link structure provide the template for an annota-
tion corresponding to that structure with appropri-
ate values filled in, which may then be associated
with a node or edge in the graph.
5.1 ISO-TimeML example
The GrAF representation of the ISO-TimeML an-
notation for the sentence I fasted for two days is
shown in Figure 3, based on the abstract syntax
given in section 3.1.
To create an annotation corresponding to an
ISO-TimeML entity structure, a node <node> el-
ement) is created and assigned a unique identi-
fier as the value of the XML attribute xml:id. An
annotation (<a>) element is also created, with a
label attribute whose value is the entity structure
name, and which contains a feature structure pro-
viding the appropriate feature/value pairs for that
entity structure. The annotation is associated with
the node by using the node?s unique identifier as
the value of the ref attribute on the <a> element.
An edge is then created from the node to another
node in the graph (r2) that references the data to be
annotated?in this case, one or more tokens defined
5The latter three attributes have the default values INDI-
VIDUAL, 1, and POSITIVE, respectively, and will be omit-
ted in the examples to follow if they have these values.
over regions of the primary data.
ISO-TimeML link structures define a relation
between two entity structures, and are rendered in
GrAF as a labeled edge between the nodes anno-
tated with the entity structure information. In the
ISO-TimeML example, an annotation with label
MLINK (?measure link?) is created with a single
feature relType. The from and to attributes on the
<edge> element link the node with the EVENT
entity structure annotation (node tml-n1 in the
example) to the node with the TIME-AMOUNT
annotation (tml-n2). This edge is then associ-
ated with the MLINK annotation (cf. Bunt and
Pustejovsky, 2009; Pustejovsky et al, 2010).
Figure 1 shows the rendering of the ISO-
TimeML abstract syntax in the ICS-1 concrete
syntax. Following Section 3.2, these two realiza-
tions of the abstract syntax for ISO-TimeML are
isomorphic.
<node xml:id="tml-n1"/>
<a label="EVENT" ref="tml-n1"
as="TimeML">
<fs>
<f name="event" value="fast"/>
<f name="tense" value="Past"/>
<f name="signature"
value="individual"/>
</fs>
</a>
<edge xml:id="tml-e1" from="tml-n1"
to="t2"/>
<node xml:id="tml-n2"/>
<a label="TIME-AMOUNT" ref="tml-n2"
as="TimeML">
<fs>
<f name="numeral" value="2"/>
<f name="unit" value="day"/>
</fs>
</a>
<edge xml:id="tml-e2" from="tml-n2"
to="t4"/>
<edge xml:id="tml-e3" from="tml-n2"
to="t5"/>
<edge xml:id="tml-e4" from="tml-n1"
to="tml-n2"/>
<a label="MLINK" ref="tml-e4"
as="TimeML">
<fs>
<f name="relType" value="FOR"/>
</fs>
</a>
Tokens: [It1][fastedt2][fort3][twot4][dayst5].
Figure 3: ISO-TimeML annotation in GrAF
251
5.2 Reverse engineering the abstract syntax
The previous two sections show how schemes for
which an abstract syntax is specified can be ren-
dered in GrAF as well as other concrete syn-
tax representations. However, as noted in sec-
tion 2, many annotation formats?especially legacy
formats?were not designed based on an underly-
ing data model. Therefore, in order to achieve a
mapping to GrAF, it is necessary to ?reverse en-
gineer? the annotation format to define its abstract
syntax. Because of problems such as those out-
lined in Section 2, this exercise may require some
extrapolation of information that is implicit, or not
specified, in the original annotation format. We
provide two examples below, one for PropBank
and one for FrameNet.
5.2.1 An abstract syntax for PropBank
The PropBank format specifies an annotation for
a sentence consisting of several columns, specify-
ing the file path; the sentence number within the
file; the number of the terminal in the sentence
that is the location of the verb; a status indica-
tion; a frameset identifier (frame and sense num-
ber); an inflection field providing person, tense,
aspect, voice, and form of the verb; and one or
more ?proplabels? representing an annotation as-
sociated with a particular argument or adjunct of
the proposition. Proplabels are associated with
primary data via reference to the Penn Treebank
(PTB) node in the syntax tree of the sentence.
Based on this we can specify a portion of a
PropBank conceptual Inventory:
? a special proposition type verb, designating
the verb (replaces PropBank ?rel?);
? a finite set PROP = {ARGA,ARGM,
ARG0, ARG1, ARG2} of proposition la-
bels;
? a finite set FEAT = {EXT,DIR,LOC,
TMP,REC,PRD,NEG,MOD,ADV,
MNR,CAU,PNC,DIS}, plus the set of
prepositions and ?null?, comprising the set of
features;
? a finite set of sets INF =
{form, tense, aspect, person, voice},
where form = {infinitive, gerund,
participle, finite}, tense = {future,
past, present}, aspect = {perfect,
progressive, both}, person =
{default, 3rd},
and voice = {active, passive}.
? a finite set FrameSets = {fs1, fs2, ...fsn}
where each fsi is a frame set defined in Prop-
Bank.
An abstract syntax for PropBank could specify
the following annotation construction rules:
? a proposition entity structure is a pair ?f,A?
where f is a frameset and A is a set of argu-
ment entity structures.6
? an argument entity structure is an argument
a ? PROP ? FEAT .
? a verb entity structure is a 5-tuple
?f, t, a, p, v? where f ? form, t ? tense,
a ? aspect, p ? person, and v ? voice.
Based on this, the PropBank annotation in Sec-
tion 2 can be rendered into a concrete syntax; in
this case, in GrAF as shown in Figure 4. Note that
the to attribute on <edge> elements have as val-
ues the reference to PTB nodes from the original
PropBank encoding; in GrAF, these values would
be identifers on the appropriate nodes in a GrAF
representation of PTB. We have also included role
names (e.g., ?owner?) in the annotation, which are
not present in the original; this was done for con-
venience and readability, and the values for the
?role? feature could have been given as arg-0, arg-
1, etc. instead.
The original PropBank encoding is close to an
ideal concrete syntax, as it can be generated from
the abstract syntax. However, the round trip back
to the abstract syntax is not possible, because it is
necessary to do some interpretation of associations
among bits of annotation information in order to
construct the abstract syntax and, subsequently,
map the PropBank format to GrAF. Specifically,
in the GrAF encoding the inflection information is
associated with the node referencing the verb, but
this association is not explicit in the original (and
in fact may not be what the annotation scheme de-
signers intended).
5.2.2 An abstract syntax for FrameNet
The FrameNet XML format is shown in Fig-
ure 5.7 The structure and content of this encod-
ing is highly oriented toward a presentation view,
6We do not include the bookkeeping information associ-
ated with a PropBank annotation in the abstract syntax.
7Some detail concerning the html display has been omit-
ted for brevity.
252
<node xml:id="pb-n1"/>
<a label="Proposition" ref="pb-n1"
as="PropBank">
<fs>
<f name="file"
value="wsj/00/wsj_0003.mrg"/>
<f name="sentenceNo" value="13"/>
<f name="verbOffset" value="6"/>
<f name="status" value="gold"/>
<f name="frameSet"
value="have.03"/>
</fs>
</a>
<node xml:id="pb-n2"/>
<a label="VERB" ref="pb-n2"
as="PropBank">
<fs>
<f name="role" value="rel"/>
<f name="form" value="finite"/>
<f name="tense" value="present"/>
<f name="voice" value="active"/>
</fs>
</a>
<edge xml:id="pb-e1" from="pb-n1"
to="pb-n2"/>
<edge xml:id="pb-e2" from="pb-n2"
to="ptb-6-0"/>
<node xml:id="pb-n3"/>
<a label="ARG0" ref="pb-n3"
as="PropBank">
<fs>
<f name="role" value="owner"/>
</fs>
</a>
<edge xml:id="pb-e3" from="pb-n1"
to="pb-n3"/>
<edge xml:id="pb-e4" from="pb-n3"
to="ptb-0-2"/>
<node xml:id="pb-n4"/>
<a label="ARG1" ref="pb-n4"
as="PropBank">
<fs>
<f name="role" value="possession"/>
</fs>
</a>
<edge xml:id="e5" from="pb-n1"
to="pb-n4"/>
<edge xml:id="e6" from="pb-n4"
to="ptb-7-1"/>
<node xml:id="pb-n5"/>
<a label="ARGM" ref="pb-n5"
as="PropBank">
<fs>
<f name="role" value="adjunct"/>
<f name="feature" value="adverbial"/>
</fs>
</a>
<edge xml:id="e7" from="pb-n1"
to="pb-n5"/>
<edge xml:id="e8" from="pb-n5"
to="ptb-10-1"/>
Figure 4: PropBank annotation in GrAF
intended to support display of the sentence and
frame elements in a browser.
A partial abstract syntax for FrameNet derived
from this format includes the following conceptual
inventory:
? a Target, designating the frame-evoking lex-
ical unit;
? a finite set FE = {Recipient, Supplier,
Means, ...} of frame element labels;
? a finite set GF = {Obj,Ext,Dep, ...} of
grammatical functions.
? a finite set PT = {NP,PP, ...} of phrase
types.
? a finite set LU = {u1, u2, ...un} where each
ui is a lexical unit.
? a finite set POS = {n, v, a, r} denoting
parts of speech;
? a finite set FrameNames = {f1, f2,...fn}
where each fi is a frame defined in
FrameNet.
An abstract syntax for this partial inventory
could specify the following annotation construc-
tion rules:
? a frame entity structure is a pair ?f,A? where
f is a frame name, u is a lexical unit, and F is
a set of frame element (FE) entity structures.
? an FE entity structure is a triple {f, g, p}, f ?
FE, g ? GF, p ? PT .
The GrAF rendering of the abstract syntax is
given in Figure 6, which was generated from the
FrameNet abstract syntax using the rules outlined
in section 5. Both the FrameNet XML and the
GrAF rendering provide an ideal concrete syntax
because they are isomorphic8 to the abstract syn-
tax and, by the definition in section 3.2, are con-
versions of one another.
6 Conclusion
In this paper we outlined a methodology for an-
notation scheme design and development; demon-
strated how schemes designed using this method-
ology may be easily mapped to GrAF; and demon-
strated how ?reverse engineering? an annotation
8Obviously, in the FrameNet XML additional elements
are introduced for display and bookkeeping purposes.
253
format whose abstract syntax is unspecified can
provide the information required to map that for-
mat to GrAF. This work was undertaken with two
goals in mind: (1) to provide a formal method for
mapping to GrAF; and (2) to demonstrate the ad-
vantages of a methodology for annotation scheme
design that is based on an abstract model, as
adopted in ISO TC37 SC4 projects and formalized
in (Bunt, 2010). The ultimate goal is, of course, to
achieve harmonization of annotation formats, so
that they can be merged, enabling the study of in-
teractions among information at different linguis-
tic levels; compared, in order to both evaluate and
improve automatic annotation accuracy; and to en-
able seamless transition from one software envi-
ronment to another when creating and using lin-
guistic annotations.
<annotationSet lexUnitRef="11673"
luName="provide.v" frameRef="1346"
frameName="Supply"
status="MANUAL" ID="2022935">
<layer rank="1" name="Target">
<label end="109" start="103"
name="Target"/>
</layer>
<layer rank="1" name="FE">
<label bgColor="0000FF" ... end="138"
start="111" name="Recipient"/>
<label bgColor="FF0000"... end="84"
start="83" name="Supplier"/>
<label bgColor="FF00FF"... end="79"
start="0" name="Means"/>
</layer>
<layer rank="1" name="GF">
<label end="138" start="111"
name="Obj"/>
<label end="84" start="83"
name="Ext"/>
<label end="79" start="0"
name="Dep"/>
</layer>
<layer rank="1" name="PT">
<label end="138" start="111"
name="NP"/>
<label end="84" start="83"
name="NP"/>
<label end="79" start="0" name="PP"/>
</layer>
...
</annotationSet>
Figure 5: FrameNet XML format
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
<node xml:id="fn-as1"/>
<a label="annotationSet" ref="fn-as1"
as="FrameNet">
<fs>
<f name="lexUnitRef" value="11673"/>
<f name="luName" value="provide.v"/>
<f name="frameRef" value="1346"/>
<f name="frameName" value="Supply"/>
<f name="status" value="MANUAL"/>
<f name="ID" value="2022935"/>
</fs>
</a>
<node xml:id="fn-n1"/>
<a label="Target" ref="fn-n1"
as="FrameNet">
<fs>
<f name="name" value="Target"/>
</fs>
</a>
<edge xml:id="e69" from="fn-as1"
to="fn-n1"/>
<edge xml:id="e90" from="fn-n1"
to="fn-t1"/>
<node xml:id="fn-n2"/>
<a label="FE" ref="fn-n2"
as="FrameNet">
<fs>
<f name="FE" value="Recipient"/>
<f name="GF" value="Obj"/>
<f name="PT" value="NP"/>
</fs>
</a>
<edge xml:id="e67" from="fn-as1"
to="fn-n2"/>
<edge xml:id="e91" from="fn-n2"
to="fn-t2"/>
<node xml:id="fn-n3"/>
<a label="FE" ref="fn-n3"
as="FrameNet">
<fs>
<f name="FE" value="Supplier"/>
<f name="GF" value="Ext"/>
<f name="PT" value="NP"/>
</fs>
</a>
<edge xml:id="e46" from="fn-as1"
to="fn-n3"/>
<edge xml:id="e92" from="fn-n3"
to="fn-t3"/>
<node xml:id="fn-n4"/>
<a label="FE" ref="fn-n4"
as="FrameNet">
<fs>
<f name="FE" value="Means"/>
<f name="GF" value="Dep"/>
<f name="PT" value="PP"/>
</fs>
</a>
<edge xml:id="e10" from="fn-as1"
to="fn-n4"/>
<edge xml:id="e93" from="fn-n4"
to="fn-t4"/>
Figure 6: FrameNet in GrAF format
254
ings of the 17th international conference on Compu-
tational linguistics, pages 86?90, Morristown, NJ,
USA. Association for Computational Linguistics.
Harry Bunt and James Pustejovsky. 2010. Annotation
of temporal and event quantification. In Proceed-
ings of the Fifth International Workshop on Interop-
erable Semantic Annotation (ISA-5), pages 15?22,
Hong Kong SAR. City University of Hong Kong.
Harry Bunt. 2010. A methodology for designing
semantic annotation languages exploiting semantic-
syntactic isomorphisms. In Proceedings of the Sec-
ond International Conference on Global Interoper-
ability for Language Resources (ICGL2010), pages
29?46, Hong Kong SAR. City University of Hong
Kong.
Nancy Ide and Laurent Romary. 2004. Interna-
tional standard for a linguistic annotation frame-
work. Journal of Natural Language Engineering,
10(3?4):211?225.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the First Linguistic Annotation Workshop,
pages 1?8, Prague.
ISO. 2009. Language Resource Management - Seman-
tic Annotation Framework (SemAF) - Part 1: Time
and Events. Secretariat KATS, October. ISO In-
ternational Standard 24617-1:2009(E)), 11 October
2009.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic. Kluwer Academic Publishers, Dordrecht.
Marc Kemps-Snijders, Menzo Windhouwer, Peter Wit-
tenburg, and Sue Ellen Wright. 2009. ISOcat : Re-
modelling metadata for language resources. Inter-
national Journal of Metadata and Semantic Ontolo-
gies, 4(4):261?276.
Inderjeet Mani, James Pustejovsky, and Beth Sund-
heim. 2004. Introduction to the special issue on
temporal information processing. ACM Transac-
tions on Asian Language Information Processing
(TALIP), 3(1):1?10.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106, March.
James Pustejovsky, Harry Bunt, Kiyong Lee, and Lau-
rent Romary. 2010. ISO-TimeML: An International
Standard for Semantic Annotation. In Proceedings
of the Fifth International Workshop on Interoperable
Semantic Annotation (ISA-5), Paris. ELDA.
255
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 98?102,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Importing MASC into the ANNIS linguistic database:
A case study of mapping GrAF
Arne Neumann
EB Cognitive Science and SFB 632
University of Potsdam
neumana@uni-potsdam.de
Nancy Ide
Department of Computer Science
Vassar College
ide@cs.vassar.edu
Manfred Stede
EB Cognitive Science and SFB 632
University of Potsdam
stede@uni-potsdam.de
Abstract
This paper describes the importation of
Manually Annotated Sub-Corpus (MASC)
data and annotations into the linguistic
database ANNIS, which allows users to vi-
sualize and query linguistically-annotated
corpora. We outline the process of
mapping MASC?s GrAF representation to
ANNIS?s internal format relANNIS and
demonstrate how the system provides ac-
cess to multiple annotation layers in the
corpus. This access provides information
about inter-layer relations and dependen-
cies that have been previously difficult to
explore, and which are highly valuable for
continued development of language pro-
cessing applications.
1 Introduction
Over the past decade, corpora with multiple lay-
ers of linguistic annotation have been developed
in order to extend the range of empirically-based
linguistic research and enable study of inter-layer
interactions. Recently created corpora include
OntoNotes (Pradhan et al, 2007), the Groningen
Meaning Bank (Basile et al, 2012), and the Man-
ually Annotated Sub-Corpus (MASC)1 (Ide et al,
2010). Typically, such corpora are represented
in idiosyncratic in-house formats, and developers
provide special software to access and query the
annotations (for example, the OntoNotes ?db tool?
and Groningen?s GMB Explorer). Access without
the use of developer-supplied software often re-
quires significant programming expertise, and as
a result, it is not easy?or even possible?for others
to add to or modify data and annotations in the re-
source.
This paper describes the importation of MASC
data and annotations into the linguistic database
1www.anc.org/MASC
ANNIS2 (Chiarcos et al, 2008; Zeldes et al,
2009), which was designed to visualize and query
linguistically-annotated corpora. Unlike most
other corpora with multi-layer annotations, no
special software has been developed for access
to MASC. Instead, all MASC data and annota-
tions are represented in GrAF (Ide and Suder-
man, 2007), the XML serialization of the abstract
model for annotations defined by ISO TC37 SC4?s
Linguistic Annotation Framework (ISO/LAF) (Ide
and Suderman, In press). GrAF is intended to
serve as a generic ?pivot? format that is isomor-
phic to annotation schemes conforming to the ab-
stract model and therefore readily mappable to
schemes used in available systems. We outline
the process of mapping GrAF to ANNIS?s internal
format relANNIS and demonstrate how the sys-
tem provides access to multiple annotation layers
in MASC.
2 The ANNIS Infrastructure
The ANNIS system is a linguistic database geared
toward the requirements of querying multi-layer
annotated corpora, and providing various visual-
ization means for layers with different structural
properties. In particular, the annotation types
supported are spans, DAGs with labelled edges,
and pointing relations between terminals or non-
terminals. For illustration, Figure 1 shows a
screenshot where various parallel annotations of
the same data are provided: dependency trees,
constituent trees (here with ?secondary edges? in
dotted lines), and a grid view for annotations that
assign labels to token spans. In addition, ANNIS
offers a ?discourse view? giving the complete text
with coreference relations indicated by color and
underlining. In the top of the screenshot, it can be
noted that the system also stored video (and au-
2http://www.sfb632.uni-potsdam.de/
annis/
98
Figure 1: Screenshot of ANNIS2
Figure 2: Querying MASC in ANNIS2 for an NP that includes both
a food frame element and a location named entity
99
dio) data, but that aspect shall not concern us in
this paper.
The system is web-based; the user interface is
written in Java and ExtJS. The backend is Post-
greSQL3. In general, all components are open
source under the Apache License 2.0, and you
can download ANNIS from the above-mentioned
URL. We offer two versions: A server version, and
the more lightweight ?ANNIS kickstarter?, which
can be installed locally, e.g., on laptops.
ANNIS is complemented by SaltNPepper, a
framework for converting annotations stemming
from various popular annotation tools (MMAX,
EXMARaLDA, annotate/Synpathy, RSTTool) ?
see Section 4.
3 MASC and GrAF
MASC is a fully open, half-million word corpus
covering nineteen diverse genres of American En-
glish drawn from the Open American National
Corpus (OANC)4. The corpus includes manually
produced or hand-validated annotations for mul-
tiple linguistic layers, including morphosyntax
(two different annotations), shallow parse (noun
and verb chunks), Penn Treebank syntax, and
named entities. Portions of the corpus are also
annotated for FrameNet frames, opinion, Prop-
Bank predicate-arguments, and WordNet 3.1 word
senses. Discourse-level annotation, including co-
reference, clauses, and discourse markers, will be
available in fall, 2013.
Like the OANC, all MASC annotations
are rendered in standoff form using GrAF,
the graph-based format developed as a part
of the ISO Linguistic Annotation Framework
(ISO/LAF)(ISO 24612, 2012). GrAF is an XML
serialization of the LAF abstract model for annota-
tions, a formalization of models used across mul-
tiple applications for associating (linking) infor-
mation, including not only directed-acyclic graphs
(DAGs) but also ER diagrams, the Universal Mod-
eling Language (UML), semantic and neural net-
works, RDF/OWL, and, more generally, hyper-
linked data on the World Wide Web. The model
is sufficiently general to represent any type of lin-
guistic annotation; any serialization of the model
can therefore serve as a pivot or intermediary
among diverse annotation formats that conform to
the abstract model. Thus, any sufficiently well-
3http://www.postgresql.org/
4www.anc.org/OANC
formed annotation scheme should be isomorphic
to a GrAF representation of the same information.
Problems arise only when a scheme does not spec-
ify information explicitly but rather embeds the in-
terpretation in processing software rather than in
the representation itself; for transduction to GrAF,
this information must be made explicit in the rep-
resentation.
Funding for MASC did not allow for extensive
software development; the expectation is that by
rendering the corpus in the ISO standard GrAF
format, access could rely on GrAF-aware software
developed by others, or transduction from GrAF
to appropriate alternative formats would be trivial.
We have already developed and deployed means
to import linguistic data represented in GrAF into
UIMA, GATE, and NLTK, and we provide trans-
ducers from GrAF to inline XML and the CoNLL
IOB format.5 Additionally, a GrAF-to-RDF trans-
ducer is near completion, which will enable inclu-
sion of MASC in the Linguistic Linked Open Data
(LLOD) cloud6. The incorporation of a GRAF
transducer for ANNIS provides another example
of the flexibility afforded via the GrAF represen-
tation.
4 Mapping GrAF to ANNIS via
SaltNPepper
Pepper is a software framework that converts lin-
guistic data among various formats, e.g. CoNLL,
EXMARaLDA, PAULA, TigerXML, RSTTool
and TreeTagger (Zipser et al, 2011). It is built
upon the graph-based Salt meta model (Zipser and
Romary, 2010), which is in turn based on the LAF
abstract model for linguistic annotation. Map-
ping GrAF to Salt extends the range of formats
into which annotations represented in GrAF can
be automatically transduced to those to which Salt
has been mapped, including ANNIS?s relational
database format relANNIS.
The following steps were taken to import the
MASC corpus into ANNIS: first, the MASC cor-
pus data was extracted with the GrAF API7. Sec-
ond, a mapping between GrAF and Salt data
structures was created. Most of the conversion
is straightforward, since both models are graph-
based. The only added processing is to provide
5Available from http://www.anc.org/MASC.
6http://linguistics.okfn.org/
resources/llod/
7http://sourceforge.net/projects/
iso-graf/
100
explicit edge labels in the Salt representation for
ordered constiuents: in GrAF, directed edges from
one to several other nodes by default represent sets
of ordered constituents and need not be explicitly
labeled as such, whereas in Salt, the role of all
edges must be specified explicitly. Explicit labels
in ANNIS are required in order to generate the ap-
propriate visualizations automatically (e.g. trees
for syntactic hierarchies and arc diagrams for syn-
tactic dependencies).
Finally, the code was structured as a plug-in
for Pepper and parameterized to make it usable
for GrAF-formatted corpora other than MASC. It
will be included in the next SaltNPepper release.
The code is currently available from our software
repository8.
5 MASC in ANNIS: Examples
The ANNIS Query Language (AQL) allows users
to search for specific token values and annotations
as well as relationships between them, even across
annotation level boundaries.9 Token values are
represented as text between quotes (e.g. "men"),
while annotations are specified as attribute-value
pairs (e.g. pos="NN", a part-of-speech attribute
with the value NN). A query for an annotation will
return all elements with that annotation. Where
necessary, namespaces10 can be added to any ele-
ment to disambiguate, e.g., ptb:cat="NP" sig-
nifies all annotation attribute-value pairs (attribute:
cat, value: NP) that are in the ptb (Penn Tree-
bank) namespace.
Relations among elements are specified by
back-referencing incremental variable numbers,
e.g. #1, #2 etc. Linguistically motivated opera-
tors bind the elements together; e.g. #1 > #2
means that the first element dominates the second
in a tree. Operators can express overlap and adja-
cency between annotation spans, as well as recur-
sive hierarchical relations that hold between nodes
(such as elements in a syntactic tree).
The following examples show AQL queries that
combine annotations from different layers:
8https://korpling.german.hu-berlin.
de/svn/saltnpepper/PepperModules/
GrAFModules/
9Note that ANNIS does not allow searching for arbitrary
strings from the primary data, but only for pre-identified seg-
ments such as tokens, named entities, etc.
10A namespace groups one or more types of annotation
into a logical unit, e.g all annotations produced by a specific
tool or project.
1. A VP that dominates a PP which contains a
named person at its right border:
cat="VP" & cat="PP" & NER="person" &
#1>#2 & #2 r #3
2. a VP of passive form in past tense that in-
cludes a mention of a FrameNet frame ele-
ment:
cat="VP" & voice="passive" &
tense="SimPas" & FE="Event" & #1 i #2
& #1 i #3 & #1 i #4
Figure 2 shows the results of a search for an
NP that includes both a named entity of the type
country and a FrameNet frame element of the type
Food:
cat="NP" & anc:type="country" &
FE="Food" & #1 i #2 & #1 i #3
6 Summary and Outlook
We explained the mapping of the MASC multi-
layer corpus to the ANNIS database by interpret-
ing the GrAF format via the Pepper framework.
Both MASC and ANNIS are freely available; a
portion of MASC will also be added to the online
demo version of ANNIS. We are also making the
Pepper converter module for GrAF available.
Version 3 of ANNIS is currently under devel-
opment11. Besides a new front-end and a REST-
based API, it offers improved tokenization support
(annotation on the level of subtokens; conflicting
tokenizations) and handles dialogue corpora with
simultaneous speakers as well as time-aligned au-
dio/video data.
The ability to query across multiple annota-
tion levels opens up significant new possibilities
for exploring linguistically annotated data. Most
commonly, language models are developed us-
ing information from at most one or two linguis-
tic layers; ANNIS enables user to explore inter-
dependencies that have been previously difficult
to detect. By providing tools and data that are
entirely free for use by the community, the AN-
NIS and MASC efforts contribute to the growing
trend toward transparent sharing and openness of
linguistic data and tools.
11Early development releases can be found at
http://www.sfb632.uni-potsdam.de/annis/
annis3.html
101
Acknowledgments
MASC and GrAF development was supported by
US NSF award CRI-0708952. The work of A.N.
and M.S. was supported by Deutsche Forschungs-
gemeinschaft as part of the Collaborative Research
Center ?Information Structure? (SFB 632) at Univ.
Potsdam and HU Berlin.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Christian Chiarcos, Stefanie Dipper, Michael Go?tze,
Ulf Leser, Anke Lu?deling, Julia Ritz, and Manfred
Stede. 2008. A flexible framework for integrating
annotations from different tools and tag sets. Traite-
ment Automatique des Langues (TAL), 49(2).
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-
based format for linguistic annotations. In Proceed-
ings of the First Linguistic Annotation Workshop,
pages 1?8, Prague.
Nancy Ide and Keith Suderman. In press. The Linguis-
tic Annotation Framework: A Standard for Annota-
tion Interchange and Merging. Language Resources
and Evaluation.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The Manually Anno-
tated Sub-Corpus : A community resource for and
by the people. In Proceedings of the The 48th An-
nual Meeting of the Association for Computational
Linguistics, Uppsala, Sweden.
ISO 24612. 2012. Language Resource Management
? Linguistic Annotation Framework. International
Standards Organization, Geneva, Switzerland.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A unified relational
semantic representation. In ICSC ?07: Proceed-
ings of the International Conference on Semantic
Computing, pages 517?526, Washington, DC, USA.
IEEE Computer Society.
Amir Zeldes, Julia Ritz, Anke Lu?deling, and Christian
Chiarcos. 2009. ANNIS: A search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics 2009.
Florian Zipser and Laurent Romary. 2010. A model
oriented approach to the mapping of annotation for-
mats using standards. In Proceedings of the Work-
shop on Language Resource and Language Technol-
ogy Standards, LREC 2010, pages 7?18, Malta.
Florian Zipser, Amir Zeldes, Julia Ritz, Laurent Ro-
mary, and Ulf Leser. 2011. Pepper: Handling
a multiverse of formats. In 33. Jahrestagung
der Deutschen Gesellschaft fu?r Sprachwissenschaft,
Go?ttingen.
102
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 18?21,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
FrameNet and Linked Data
Nancy Ide
Department of Computer Science, Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
Abstract
FrameNet is the ideal resource for repre-
sentation as linked data, and several ren-
derings of the resource in RDF/OWL have
been created. FrameNet has also been
and continues to be linked to other major
resources, including WordNet, BabelNet,
and MASC, in the Linguistic Linked Open
Data cloud. Although so far the supporting
technologies have not enabled easy and
widespread access to the envisioned mas-
sive network of language resources, a con-
flation of recent efforts suggests this may
be a reality in the not-too-distant future.
FrameNet (Fillmore et al., 2002; Ruppenhofer
et al., 2006) is the ideal resource for representation
in the Semantic Web (SW) as what is now widely
known as ?linked data?. The Semantic Web con-
sists of objects whose properties are represented
by named links to other objects that constitute
their values and supports representing and reason-
ing over ontologies defined the the SW frame-
work. FrameNet is also a complex semantic net-
work linking lexical units to semantic frames, and
semantic frames to one another in a shallow hier-
archy, over which inheritance and sub-frame re-
lations are defined. In sentences annotated for
FrameNet frame elements, the role is a property
of a frame object that is linked to the entity (ob-
ject) that fills it; FrameNet also includes a hierar-
chy of semantic types that constrain the possible
fillers for a given role. FrameNet thus defines a
dense network of objects and properties supported
by ontological relations?exactly what the Seman-
tic Web is intended to be.
1
The suitability of FrameNet for representation
in the Semantic Web was recognized fairly early
on in the development of the family of Semantic
1
For a fuller description of the structure of FrameNet data,
see (Scheffczyk et al., 2008).
Web formats, which include the Resource Defi-
nition Framework (RDF) and the Web Ontology
Language (OWL), which first became available as
W3C standards in the late 90s and early 2000s. In
one of the earliest projects to adapt linguistic re-
sources to the Semantic Web, FrameNet was ren-
dered in RDF and DAML+OIL (the precursor to
OWL) in 2003, soon after these formats first be-
came standardized, for the stated goal of providing
?a potential resource to aid in the automatic iden-
tification and disambiguation of word meanings
on the semantic web? (Narayanan et al., 2003a).
Later, the DAML+OIL portion was converted to
OWL (Scheffczyk et al., 2008; Scheffczyk et al.,
2010). Other conversions include (Coppola et al.,
2009) and (Narayanan et al., 2003b); most re-
cently, FrameNet was ported to RDF/OWL for in-
clusion in the Linked Open Data (LOD) cloud
2
(Nuzzolese et al., 2011). The possibility of link-
ing WordNet and FrameNet in the Semantic Web
has also spawned efforts such as (Bryl et al., 2012)
that build on numerous efforts over the past several
years to align and/or extend these two resources
(Burchardt et al., 2005; Ide, 2006; De Cao et al.,
2008; de Melo et al., 2012; Bryl et al., 2012). Oth-
ers have analyzed FrameNet in order to formalize
its semantics so as to be appropriate for use with
Description Logic (DL) reasoners compatible with
OWL-DL (Ovchinnikova et al., 2010).
Given all of the activity surrounding FrameNet
as a resource for the Semantic Web, one would ex-
pect to see multiple examples of the use of Seman-
tic Web implementations of FrameNet for NLP de-
velopment and research. However, these exam-
ples do not exist, for two reasons. The first is
a reality of the Semantic Web: simply put, the
Semantic Web has not yet come to fruition, de-
spite its having been around as a concept for well
over a decade, and despite the development of a
suite of W3C standard technologies to support it.
2
http://linkeddata.org
18
One of the most important of these technologies is
SPARQL (Prud?hommeaux and Seaborne, 2008),
a query language for data in RDF format, which
is the crucial tool for exploiting the inter-linkages
among linguistic resources to support NLP. Un-
fortunately, SPARQL is new enough that it is not
yet widely deployed and has not had the bene-
fit of decades of optimization to improve its per-
formance, which so far often suffers from slug-
gishness. The good news is that new research
and implementations are rapidly contributing to
the improvement of SPARQL and other Semantic
Web technologies, and as a result, we are seeing
signs that the requisite base infrastructure may be
(or may soon be) in place to support accelerated
growth and deployment.
At the same time, over the past four or five years
several efforts in Semantic Web development?in
particular, the deployment of knowledge bases,
lexicons, ontologies, and similar resources as
linked data?have made notable progress, includ-
ing the LOD cloud and, of special interest for
the NLP community, its companion Linguistic
Linked Open Data (LLOD) cloud (Chiarcos et
al., 2012a). Efforts to link, especially, lexical-
semantic databases like FrameNet, WordNet, and
BabelNet (Navigli and Ponzetto, 2010) are under-
way, although full, operational linkage may not
be immediate. At the same time, however, there
is virtually no language data in the form of cor-
pora in the LLOD, and none that contains annota-
tions that can be linked to lexicons and knowledge
bases.
This suggests a second reason why FrameNet as
linked data has not yet been used in NLP research:
a more useful configuration for a FrameNet-based
resource in the Semantic Web would include link-
age from frame governors and frame elements to
(many) examples in corpora, rather than a sim-
ple rendering of linkages among lexical units,
frames, and frame elements. Coupled with linkage
to WordNet and multilingual semantic resources
such as BabelNet (which has also been recently
ported to RDF?see (Navigli, 2012)), a Semantic
Web resource of this type and magnitude would
enable SPARQL queries that collect information
across several linguistic phenomena and levels, for
example, ?find all tokens in English and Russian
that refer to land as a political unit (synonyms
from the WordNet synset land%1:15:02::) in the
VICTIM role of the ATTACK frame?. This is a
trivial example; the full range of possibilities is left
to the reader?s imagination, and awaits SPARQL?s
transition to full adulthood.
FrameNet has always hand-annotated sample
sentences as input to frame construction, due to the
insistence by FrameNet?s founder on grounding
the theory in real language data. FrameNet?s early
annotation efforts used examples from the British
National Corpus (BNC); however, as time went
on, FrameNet and similar annotation projects
3
found that usage examples extracted from the
BNC were often unusable or misrepresentative
for developing templates to describe semantic ar-
guments and the like, due to significant syntac-
tic differences between British and American En-
glish. This motivated a proposal for an American
National Corpus (ANC)
4
(Fillmore et al., 1998),
comparable to the BNC but including genres non-
existent at the time of BNC development (blogs,
email, chat rooms, tweets, etc.) as well as annota-
tions beyond part-of-speech, to serve as basis for
the development of lexical-semantic resources and
NLP research in general.
5
In 2006, the ANC, FrameNet, and WordNet
projects received a substantial grant from the U.S.
National Science Foundation
6
to produce a half-
million word Manually Annotated Sub-Corpus
(MASC)
7
(Ide et al., 2010), consisting of data
drawn from the ANC and annotated for multiple
types of linguistic phenomena. The project in-
cluded a component to annotate portions of the
corpus for WordNet senses and FrameNet frame
elements, in order to provide input to an effort to
harmonize these two resources (Baker and Fell-
baum, 2009). The first full version of the cor-
pus, released in 2012, included over 16 different
annotation types and was coupled with a separate
sentence corpus (Passonneau et al., 2012) that in-
cludes WordNet 3.1 sense-tags for approximately
1000 occurrences of each of 114 words chosen by
the WordNet and FrameNet teams (ca. 114,000
annotated occurrences). Of these, 100 occurrences
of each word (over 1000 sentences) are also anno-
3
E.g., Comlex (http://nlp.cs.nyu.edu/comlex/) and Nom-
Lex (http://nlp.cs.nyu.edu/nomlex/)
4
http://www.anc.org/
5
The ANC never received the substantial funding and text
contributions enjoyed by the BNC, and as a result has so far
released only 22 million words of data, including a 15 million
word subset that is unrestricted for any use called the Open
ANC? (OANC), available at http://www.anc.org/data/oanc/.
6
NSF CRI 0708952
7
http://www.anc.org/data/masc/
19
tated for FrameNet frame elements. These annota-
tions were subsequently used in a major WordNet-
FrameNet alignment effort (de Melo et al., 2012).
MASC provides a missing link in the Semantic
Web scenario for FrameNet and related resources.
The corpus contains not only FrameNet and Word-
Net annotations, but also annotations over parts
or all the corpus at several other linguistic layers
including morphosyntax, syntax (shallow parse,
Penn Treebank annotation), semantics (named en-
tities, opinion, PropBank), and discourse (corefer-
ence, clause boundaries and nucleus/satellite rela-
tions). All of MASC is currently being incorpo-
rated into the LLOD cloud, and its FrameNet and
WordNet annotations will be linked to the linked
data versions of those resources.
8
The resulting
resource, connecting multiple major semantic re-
sources and a broad-genre corpus, will be unpar-
alleled as a foundation for NLP research and de-
velopment.
When the annotations for other phenomena in
MASC are added into the mix, the potential to
study and process language data across multiple
linguistic levels becomes even greater. It is in-
creasingly recognized that to perform human-like
language understanding, NLP systems will ulti-
mately have to dynamically integrate information
from all linguistic levels as they process input,
but despite this recognition most work in the field
continues to focus on isolated phenomena or uti-
lizes only selected phenomena from a few lin-
guistic levels. Some corpora with multiple anno-
tation layers, including MASC and a (very few)
others such as OntoNotes (Pradhan et al., 2007),
have recently been created, but due to the high
costs of their development they are limited in size
and do not include annotations across the gamut
of linguistic phenomena. Similarly, standardized
formats for annotated data (e.g., (ISO, 2012)),
lexical-semantic resources (ISO, 2008), and ref-
erence categories for linguistic annotations (Marc
Kemps-Snijders and Wright, 2008) have been de-
veloped to enable merging of annotations of differ-
ent types and formats, but there still remains con-
siderable disparity among and/or lack of support
for processing merged resources.
8
See (Chiarcos et al., 2012b) for a discussion of the pro-
cess and benefits. BabelNet annotations of MASC, which are
in turn linked to wordnets in multiple languages, have also
been recently contributed (Moro et al., 2014), thus opening
up the possibility for linkage from MASC to that resource
as well?and, by extension, linkage between BabelNet and
MASC?s existing FrameNet and WordNet annotations.
Is the Semantic Web the answer? Will it be the
vehicle for a paradigm shift in NLP research and
development? Likely, it or something it evolves
into will ultimately provide the required common
representation and processing framework which,
coupled with potentially enormous advances in
computer and network speed, data capacity, neu-
rotechnology, network-on-chip technologies, and
the like, will fundamentally change our approach
to NLP in the decades to come. In the meantime,
it remains to be seen how quickly Semantic Web
technology will progress, and how soon most or
all language resources will reside in places like the
LLOD cloud, so that they can begin to be fully
and readily exploited. But whether the Semantic
Web as we know it now is the ultimate solution or
simply a developmental step, the direction seems
clear; and fittingly, FrameNet is one of the first re-
sources on board.
References
Collin F. Baker and Christiane Fellbaum. 2009. Word-
Net and FrameNet as Complementary Resources for
Annotation. In Proceedings of the Third Linguistic
Annotation Workshop, pages 125?129.
Volha Bryl, Sara Tonelli, Claudio Giuliano, and Lu-
ciano Serafini. 2012. A novel Framenet-based re-
source for the semantic web. In SAC, pages 360?
365.
Aljoscha Burchardt, Katrin Erk, and Anette Frank.
2005. A WordNet detour to FrameNet. In Proceed-
ings of the GLDV 2005 workshop GermaNet II.
Christian Chiarcos, Sebastian Hellmann, and Sebas-
tian Nordhoff. 2012a. Linking Linguistic Re-
sources: Examples from the Open Linguistics Work-
ing Group. In Linked Data in Linguistics, pages
201?216. Springer.
Christian Chiarcos, John McCrae, Philipp Cimiano,
and Christiane Fellbaum. 2012b. Towards open
data for linguistics: Linguistic linked data. In New
Trends of Research in Ontologies and Lexical Re-
sources. Springer.
Bonaventura Coppola, Aldo Gangemi, Alfio Massimil-
iano Gliozzo, Davide Picca, and Valentina Presutti.
2009. Frame Detection over the Semantic Web. In
Proceedings of the 6th European Semantic Web Con-
ference.
Diego De Cao, Danilo Croce, Marco Pennacchiotti,
and Roberto Basili. 2008. Combining Word Sense
and Usage for Modeling Frame Semantics. In Pro-
ceedings of the 2008 Conference on Semantics in
Text Processing, pages 85?101.
20
Gerard de Melo, Collin F. Baker, Nancy Ide, Rebecca
Passonneau, and Christiane Fellbaum. 2012. Em-
pirical Comparisons of MASC Word Sense Annota-
tions. In Eighth International Conference on Lan-
guage Resources and Evaluation (LREC 2012).
Charles J. Fillmore, Nancy Ide, Daniel Jurafsky, and
Catherine Macleod. 1998. An American National
Corpus: A Proposal. In Proceedings of the First An-
nual Conference on Language Resources and Eval-
uation, pages 965?969, Granada, Spain.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. The FrameNet Database and Software Tools.
In Proceedings of the Third International Confer-
ence on Language Resources and Evaluation, vol-
ume IV.
Nancy Ide, Collin Baker, Christiane Fellbaum, and Re-
becca Passonneau. 2010. The Manually Annotated
Sub-Corpus: A Community Resource for and by the
People. In Proceedings of ACL 2010, pages 68?73.
Nancy Ide. 2006. Making Senses: Bootstrapping
Sense-Tagged Lists of Semantically-Related Words.
In Computational Linguistics and Intelligent Text,
pages 13?27.
2008. Language Resource Management ? Lexical
Markup Framework. International Standard ISO
24613.
2012. Language Resource Management ? Linguistic
Annotation Framework. International Standard ISO
24612.
Peter Wittenburg Marc Kemps-Snijders, Menzo Wind-
houwer and Sue Ellen Wright. 2008. ISOCat: Cor-
ralling Data Categories in the Wild. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08).
Andrea Moro, Roberto Navigli, Francesco Maria
Tucci, and Rebecca J. Passonneau. 2014. Annotat-
ing the MASC corpus with babelnet. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC? 2014).
Srini Narayanan, Collin F. Baker, Charles J. Fillmore,
and Miriam R.L. Petruck. 2003a. FrameNet Meets
the Semantic Web: Lexical Semantics for the Web.
In The Semantic Web - ISWC 2003, pages 771?787.
Springer.
Srinivas Narayanan, Miriam R.L. Petruck, Collin F.
Baker, and Charles J. Fillmore. 2003b. Putting
FrameNet Data into the ISO Linguistic Annotation
Framework. In Proceedings of the ACL 2003 Work-
shop on Linguistic Annotation: Getting the Model
Right, page 22?29.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a Very Large Multilingual Se-
mantic Network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 216?225.
Roberto Navigli. 2012. BabelNet goes to the (Multi-
lingual) Semantic Web. In ISWC 2012 Workshop on
Multilingual Semantic Web.
Andrea Giovanni Nuzzolese, Aldo Gangemi, and
Valentina Presutti. 2011. Gathering lexical linked
data and knowledge patterns from FrameNet. In K-
CAP, pages 41?48.
Ekaterina Ovchinnikova, Laure Vieu, Alessandro
Oltramari, Stefano Borgo, and Theodore Alexan-
drov. 2010. Data-Driven and Ontological Anal-
ysis of FrameNet for Natural Language Reason-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10).
Rebecca J. Passonneau, Collin F. Baker, Christiane
Fellbaum, and Nancy Ide. 2012. The MASC Word
Sense Corpus. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2012).
Sameer S. Pradhan, Eduard H. Hovy, Mitchell P.
Marcus, Martha Palmer, Lance A. Ramshaw, and
Ralph M. Weischedel. 2007. Ontonotes: A unified
relational semantic representation. In Proceedings
of the First IEEE International Conference on Se-
mantic Computing (ICSC 2007), pages 517?526.
Eric Prud?hommeaux and Andy Seaborne. 2008.
SPARQL Query Language for RDF.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Jan Scheffczyk, Collin F. Baker, and Srini Narayanan.
2008. Ontology-Based reasoning about lexical re-
sources. In Ontologies and Lexical Resources for
Natural Language Processing. Cambridge Univer-
sity Press.
Jan Scheffczyk, Collin Baker, and Srrini Narayanan,
2010. Reasoning over Natural Language Text by
Means of FrameNet and Ontologies, pages 53?71.
Cambridge University Press.
21
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 34?43,
Dublin, Ireland, August 23rd 2014.
The Language Application Grid Web Service Exchange Vocabulary
Nancy Ide
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
ide@cs.vassar.edu
James Pustejovsky
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
jamesp@cs.brandeis.edu
Keith Suderman
Department of Computer Science
Vassar College
Poughkeepsie, New York USA
suderman@anc.org
Marc Verhagen
Department of Computer Science
Brandeis University
Waltham, Massachusetts USA
marc@cs.brandeis.edu
Abstract
In the context of the Linguistic Applications (LAPPS) Grid project, we have undertaken the def-
inition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce lin-
guistically annotated data. The goal is not to define a new set of terms, but rather to provide a
single web location where terms relevant for exchange among NLP tools are defined and pro-
vide a ?sameAs? link to all known web-based definitions that correspond to them. The WS-EV
is intended to be used by a federation of six grids currently being formed but is usable by any
web service platform. Ultimately, the WS-EV could be used for data exchange among tools in
general, in addition to web services.
1 Introduction
There is clearly a demand within the community for some sort of standard for exchanging annotated lan-
guage data among tools.
1
This has become particularly urgent with the emergence of web services, which
has enabled the availability of language processing tools that can and should interact with one another,
in particular, by forming pipelines that can branch off in multiple directions to accomplish application-
specific processing. While some progress has been made toward enabling syntactic interoperability via
the development of standard representation formats (e.g., ISO LAF/GrAF (Ide and Suderman, 2014;
ISO-24612, 2012), NLP Interchange Format (NIF) (Hellmann et al., 2013), UIMA
2
Common Analysis
System (CAS)) which, if not identical, can be trivially mapped to one another, semantic interoperability
among NLP tools remains problematic (Ide and Pustejovsky, 2010). A few efforts to create repositories,
type systems, and ontologies of linguistic terms (e.g., ISOCat
3
, OLiA
4
, various repositories for UIMA
type systems
5
, GOLD
6
, NIF Core Ontology
7
) have been undertaken to enable (or provide) a mapping
among linguistic terms, but none has yet proven to include all requisite terms and relations or be easy
to use and reference. General repositories such as Dublin Core
8
, schema.org, and the Friend of a Friend
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
See, for example, proceedings of the recent LREC workshop on ?Language Technology Service Platforms: Synergies,
Standards, Sharing? (http://www.ilc.cnr.it/ltsp2014/).
2
https://uima.apache.org/
3
http://www.isocat.org
4
http://nachhalt.sfb632.uni-potsdam.de/owl/
5
E.g., http://www.julielab.de/Resources/Software/UIMA+type+system-p-91.html
6
http://linguistics-ontology.org
7
http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/nif-core
8
http://dublincore.org
34
project
9
include some relevant terms, but they are obviously not designed to fully cover the kinds of
information found in linguistically annotated data.
In the context of the Linguistic Applications (LAPPS) Grid project (Ide et al., 2014), we have under-
taken the definition of a Web Service Exchange Vocabulary (WS-EV) specifying a terminology for a core
of linguistic objects and features exchanged among NLP tools that consume and produce linguistically
annotated data. The work is being done in collaboration with ISO TC37 SC4 WG1 in order to ensure
full community engagement and input. The goal is not to define a new set of terms, but rather to provide
a single web location where terms relevant for exchange among NLP tools are defined and provide a
?sameAs? link to all known web-based definitions that correspond to them. A second goal is to define
relations among the terms that can be used when linguistic data are exchanged. The WS-EV is intended
to be used by a federation of grids currently being formed, including the Kyoto Language Grid
10
, the
Language Grid Jakarta Operation Center
11
, the Xinjiang Language Grid, the Language Grid Bangkok
Operation Center
12
, LinguaGrid
13
, MetaNET/Panacea
14
, and LAPPS, but is usable by any web service
platform. Ultimately, the WS-EV could be used for data exchange among tools in general, in addition to
web services.
This paper describes the LAPPS WS-EV, which is currently under construction. We first describe the
LAPPS project and then overview the motivations and principles for developing the WS-EV. Because
our goal is to coordinate with as many similar projects and efforts as possible to avoid duplication, we
also describe existing collaborations and invite other interested groups to provide input.
2 The Language Application Grid Project
The Language Application (LAPPS) Grid project is in the process of establishing a framework that
enables language service discovery, composition, and reuse, in order to promote sustainability, manage-
ability, usability, and interoperability of natural language Processing (NLP) components. It is based on
the service-oriented architecture (SOA), a more recent, web- oriented version of the pipeline architecture
that has long been used in NLP for sequencing loosely-coupled linguistic analyses. The LAPPS Grid
provides a critical missing layer of functionality for NLP: although existing frameworks such as UIMA
and GATE provide the capability to wrap, integrate, and deploy language services, they do not provide
general support for service discovery, composition, and reuse.
The LAPPS Grid is a collaborative effort among US partners Brandeis University, Vassar College,
Carnegie-Mellon University, and the Linguistic Data Consortium at the University of Pennsylvania, and
is funded by the US National Science Foundation (NSF). The project builds on the foundation laid in
the NSF-funded project SILT (Ide et al., 2009), which established a set of needs for interoperability
and developed standards and best practice guidelines to implement them. LAPPS is similar in its scope
and goals to ongoing projects such as The Language Grid
15
, PANACEA/MetaNET
16
, LinguaGrid
17
, and
CLARIN
18
, which also provide web service access to basic NLP processing tools and resources and
enable pipelining these tools to create custom NLP applications and composite services such as question
answering and machine translation, as well as access to language resources such as mono- and multi-
lingual corpora and lexicons that support NLP. The transformative aspect of the LAPPS Grid is therefore
not the provision of a suite of web services, but rather that it orchestrates access to and deployment of
language resources and processing functions available from servers around the globe, and enables users
to easily add their own language resources, services, and even service grids to satisfy their particular
needs.
9
http://www.foaf-project.org
10
http://langrid.nict
11
http://langrid.portal.cs.ui.ac.id/langrid/
12
http://langrid.servicegrid-bangkok.org
13
http://www.linguagrid.org/
14
http://www.panacea-lr.eu
15
http://langrid.nict
16
http://panacea-lr.eu/
17
http://www.linguagrid.org/
18
http://www.clarin.eu/
35
The most distinctive innovation in the LAPPS Grid that is not included in other projects is the provision
of an open advancement (OA) framework (Ferrucci et al., 2009a) for component- and application-based
evaluation of NLP tools and pipelines. The availability of this type of evaluation service will provide an
unprecedented tool for NLP development that could, in itself, take the field to a new level of productivity.
OA involves evaluating multiple possible solutions to a problem, consisting of different configurations
of component tools, resources, and evaluation data, to find the optimal solution among them, and en-
abling rapid identification of frequent error categories, together with an indication of which module(s)
and error type(s) have the greatest impact on overall performance. On this basis, enhancements and/or
modifications can be introduced with an eye toward achieving the largest possible reduction in error rate
(Ferrucci et al., 2009; Yang et al., 2013). OA was used in the development of IBM?s Watson to achieve
steady performance gains over the four years of its development (Ferrucci et al., 2010); more recently,
the open-source OAQA project has released software frameworks which provide general support for
open advancement (Garduno et al., 2013; Yang et al., 2013), which has been used to rapidly develop
information retrieval and question answering systems for bioinformatics (Yang et al., 2013; Patel et al.,
2013).
The fundamental system architecture of the LAPPS Grid is based on the Open Service Grid Initiative?s
Service Grid Server Software
19
developed by the National Institute of Information and Communications
Technology (NICT) in Japan and used to implement Kyoto University?s Language Grid, a service grid
that supports multilingual communication and collaboration. Like the Language Grid, the LAPPS Grid
provides three main functions: language service registration and deployment, language service search,
and language service composition and execution. As noted above, the LAPPS Grid is instrumented
to provide relevant component-level measures for standard metrics, given gold-standard test data; new
applications automatically include instrumentation for component-level and end-to-end measurement,
and intermediate (component-level) I/O is logged to support effective error analysis.
20
The LAPPS
Grid also implements a dynamic licensing system for handling license agreements on the fly
21
, provides
the option to run services locally with high-security technology to protect sensitive information where
required, and enables access to grids other than those based on the Service Grid technology.
We have adopted the JSON-based serialization for Linked Data (JSON-LD) to represent linguistically
annotated data for the purposes of web service exchange. The JavaScript Object Notation (JSON) is a
lightweight, text-based, language-independent data interchange format that defines a small set of format-
ting rules for the portable representation of structured data. Because it is based on the W3C Resource
Definition Framework (RDF), JSON-LD is trivially mappable to and from other graph-based formats
such as ISO LAF/GrAF and UIMA CAS, as well as a growing number of formats implementing the
same data model. Most importantly, JSON- LD enables services to reference categories and definitions
in web-based repositories and ontologies or any suitably defined concept at a given URI.
The LAPPS Grid currently supports SOAP services, with plans to support REST services in the
near future. We provide two APIs: org.lappsgrid.api.DataSource, which provides data
to other services, and org.lappsgrid.api.WebService, for tools that annotate, transform, or
otherwise manipulate data from a datasource or another web service. All LAPPS services exchange
org.lappsgrid.api.Data objects consisting of a discriminator (type) that indicates how to inter-
pret the payload, and a payload (typically a utf-8 string) that consists of the JSON-LD representation.
Data converters included in the LAPPS Grid Service Engines map from commonly used formats to the
JSON-LD interchange format; converters are automatically invoked as needed to meet the I/O require-
ments of pipelined services. Some LAPPS services are pre-wrapped to produce and consume JSON-LD.
Thus, JSON-LD provides syntactic interoperability among services in the LAPPS Grid; semantic inter-
19
http://servicegrid.net
20
Our current user interface provides easy (re-)configuration of single pipelines; we are currently extending the interface
to allow the user to specify an entire range of pipeline configurations using configuration descriptors (ECD; (Yang et al.,
2013) to define a space of possible pipelines, where each step might be achieved by multiple components or services and each
component or service may have configuration parameters with more than one possible value to be tested. The system will then
automatically generate metrics measurements plus variance and statistical significance calculations for each possible pipeline,
using a service-oriented version of the Configuration Space Exploration (CSE) algorithm (Yang et al., 2013).
21
See (Cieri et al., 2014) for a description of how licensing issues are handled in the LAPPS Grid.
36
operability is provided by the LAPPS Web Service Exchange Vocabulary, described in the next section.
3 LAPPS Web Service Exchange Vocabulary
3.1 Motivation
The WS-EV addresses a relatively small but critical piece of the overall LAPPS architecture: it allows
web services to communicate about the content they deliver, such that the meaning?i.e., exactly what
to do with and/or how to process the data?is understood by the receiver. As such it performs the same
function as a UIMA type system performs for tools in a UIMA pipeline that utilize that type system,
or the common annotation labels (e.g., ?Token?, ?Sentence?, etc.) required for communication among
pipelined tools in GATE: these mechanisms provide semantic interoperability among tools as long as one
remains in either the UIMA or GATE world. To pipeline a tool whose output follows GATE conventions
with a tool that expects input that complies with a given UIMA type system, some mapping of terms and
structures is likely to be required.
22
This is what the WS-EV is intended to enable; effectively, it is a
meta-type-system for mapping labels assigned to linguistically annotated data so that they are understood
and treated consistently by tools that exchange them in the course of executing a pipeline or workflow.
Since web services included in LAPPS and federated grids may use any i/o semantic conventions, the
WS-EV allows for communication among any of them?including, for example, between GATE and
UIMA services
23
The ability to pipeline components from diverse sources is critical to the implementation of the OA
development approach described in the previous section, it must be possible for the developer to ?plug
and play? individual tools, modules, and resources in order to rapidly re-configure and evaluate new
pipelines. These components may exist on any server across the globe, consist of modules developed
within frameworks such as UIMA and GATE, and or be user-defined services existing on a local machine.
3.2 WS-EV Design
The WS-EV was built around the following design principles, which were compiled based on input from
the community:
1. The WS-EV will not reinvent the wheel. Objects and features defined in the WS-EV will be linked
to definitions in existing repositories and ontologies wherever possible.
2. The WS-EV will be designed so as to allow for easy, one-to-one mapping from terms designating
linguistic objects and features commonly produced and consumed by NLP tools that are wrapped
as web services. It is not necessary for the mapping to be object-to-object or feature-to-feature.
3. The WS-EV will provide a core set of objects and features, on the principle that ?simpler is better?,
and provide for (principled) definition of additional objects and features beyond the core to represent
more specialized tool input and output.
4. The WS-EV is not LAPPS-specific; it will not be governed by the processing requirements or
preferences of particular tools, systems, or frameworks.
5. The WS-EV is intended to be used only for interchange among web services performing NLP tasks.
As such it can serve as a ?pivot? format to which user and tool-specific formats can be mapped.
6. The web service provider is responsible for providing wrappers that perform the mapping from
internally-used formats to and/or from the WS-EV.
7. The WS-EV format should be compact to facilitate the transfer of large datasets.
22
Within UIMA, the output of tools conforming to different type systems may themselves require conversion in order to be
used together.
23
Figure 5 shows a pipeline in which both GATE and UIMA services are called; GATE-to-GATE and UIMA-to-UIMA
communication does not use the WS-EV, but it is used for communication between GATE and UIMA services, as well as other
services.
37
8. The WS-EV format will be chosen to take advantage, to the extent possible, of existing technologi-
cal infrastructures and standards.
As noted in the first principle, where possible the objects and features in the WS-EV are drawn from
existing repositories such as ISOCat and the NIF Core Ontology and linked to them via the owl:sameAs
property
24
or, where appropriate, rdfs:subClassOf
25
. However, many repositories do not include some
categories and objects relevant for web service exchange (e.g., ?token? and other segment descriptors),
do include multiple (often very similar) definitions for the same concept, and/or do not specify relations
among terms. We therefore attempted to identify a set of (more or less) ?universal? concepts by surveying
existing type systems and schemas ? for example, the Julie Lab and DARPA GALE UIMA type systems
and the GATE schemas for linguistic phenomena ? together with the I/O requirements of commonly
used NLP software (e.g., the Stanford NLP tools, OpenNLP, etc.). Results of the survey for token and
sentence identification and part-of-speech labeling
26
showed that even for these basic categories, no
existing repository provides a suitable set of categories and relations.
Perhaps more problematically, sources that do specify relations among concepts, such as the various
UIMA type systems and GATE?s schemas, vary widely in their choices of what is an object and what
is a feature; for example, some treat ?token? as an object (label) and ?lemma? and ?POStag? as asso-
ciated features, while others regard ?lemma? and/or ?POStag? as objects in their own right. Decisions
concerning what is an object and what is a feature are for the most part arbitrary; no one scheme is right
or wrong, but a consistent organization is required for effective web service interchange. The WS-EV
therefore defines an organization of objects and features for the purposes of interchange only. Where
possible, the choices are principled, but they are otherwise arbitrary. The WS-EV includes sameAs and
similarTo mappings that link to like concepts in other repositories where possible, thus serving primar-
ily to group the terms and impose a structure of relations required for web service exchange in one
web-based location.
In addition to the principles above, the WS-EV is built on the principle of orthogonal design, such that
there is one and only one definition for each concept. It is also designed to be very lightweight and easy
to find and reference on the web. To that end we have established a straightforward web site (the Web
Service Exchange Vocabulary Repository
27
), similar to schema.org, in order to provide web-addressable
terms and definitions for reference from annotations exchanged among web services. Our approach is
bottom-up: we have adopted a minimalist strategy of adding objects and features to the repository only
as they are needed as services are added to the LAPPS Grid. Terms are organized in a shallow ontology,
with inheritance of properties, as shown in Figure 1.
4 WS-EV and JSON-LD
References in the JSON-LD representation used for interchange among LAPPS Grid web services point
to URIs providing definitions for specific linguistic categories in the WS-EV. They also reference doc-
umentation for processing software and rules for processes such as tokenization, entity recognition, etc.
used to produce a set of annotations, which are often left unspecified in annotated resources (see for
example (Fokkens et al., 2013)). While not required for web service exchange in the LAPPS Grid, the
inclusion of such references can contribute to the better replication and evaluation of results in the field.
Figure 3 shows the information for Token, which defines the concept, identifies application types that
produce objects of this type, cross-references a similar concept in ISOCat, and provides the URI for use
in the JSON-LD representation. It also specifies the common properties that can be specified for a set
of Token objects, and the individual properties that can be associated with a Token object. There is no
requirement to use any or all of the properties in the JSON-LD representation, and we foresee that many
web services will require definition of objects and properties not included in the WS-EVR or elsewhere.
24
http://www.w3.org/TR/2004/REC-owl-semantics-20040210/#owl sameAs
25
http://www.w3.org/TR/owl-ref/#subClassOf-def
26
Available at http://www.anc.org/LAPPS/EP/Meeting-2013-09-26-Pisa/ep-draft.pdf
27
http://vocab.lappsgrid.org
38
Figure 1: Fragment of the WS-EV ontology (associated properties in gray)
We therefore provide mechanisms for (principled) definition of objects and features beyond the WS-
EVR. Two options exist: users can provide a URI where a new term or other documentation is defined,
or users may add a definition to the WS-EVR. In the latter case, service providers use the name space
automatically assigned to them at the time of registration, thereby avoiding name clashes and providing
a distinction between general categories used across services and more idiosyncratic categories.
Figure 2 shows a fragment of the JSON-LD representation that references terms in the WS-
EV. The context statement at the top identifies the URI that is to be prefixed to any unknown
name in order to identify the location of its definition. For the purposes of the example, the
text to be processed is given inline. Our current implementation includes results from each step
in a pipeline, where applicable, together with metadata describing the service applied in each step
(here, org.anc.lapps.stanford.SATokenizer:1.4.0) and identified by an internally-defined type (stan-
ford). The annotations include references to the objects defined in the WS-EV, in this example, To-
ken (defined at http://vocab.lappsgrid.org/Token) with (inherited) features id, start, end and specific
feature string, defined at http://vocab.lappsgrid.org/Token#id, http://vocab.lappsgrid.org/Token#start,
http://vocab.lappsgrid.org/Token#end, and http://vocab.lappsgrid.orgToken/#string, respectively. The
web page defining these terms is shown in Figure 3.
"@context" : "http://vocab.lappsgrid.org/",
"metadata" : { },
"text" : {
"@value" : "Some of the strongest critics of our welfare system..." }
"steps" : [ {
"metadata" : {
"contains" : {
"Token" : {
"producer" : "org.anc.lapps.stanford.SATokenizer:1.4.0",
"type" : "stanford"
}
}
},
"annotations" : [ {
"@type" : "Token",
"id" : "tok0",
"start" : 18,
"end" : 22,
"features" : {
string" : "Some" }
},
Figure 2: JSON-LD fragment referencing the LAPPS Grid WS-EV
39
Figure 3: Token definition in the LAPPS WS-EVR
4.1 Mapping to JSON-LD
As noted above in Section 1, existing schemes and systems for organizing linguistic information ex-
changed by NLP tools vary considerably. Figure 4 shows some variants for a few commonly used NLP
tools, which differ in terminology, structure, and physical format. To be used in the LAPPS Grid, tools
such as those in the list are wrapped so that their output is in JSON-LD format, which provides syntactic
interoperability, terms are mapped to corresponding objects in the WS-EV, and the object-feature rela-
tions reflect those defined in the WS-EV. Correspondingly, wrappers transduce the JSON-LD/WS-EV
representation to the format used internally by the tool on input. This way, the tools use their internal
format as usual and map to JSON-LD/WS-EV for exchange only.
40
Name Input Form Output Form Example
Stanford tagger pt n/a word pos opl box NN1
XML n/a XML inline <word id=?0? pos=?VB?>Let</word>
NaCTeM tagger pt n/a word/pos inline box/NN1
CLAWS (1) pt n/a word pos inline box NN1
CLAWS (2) pt n/a XML inline <w id=?2? pos=?NN1?>Type</w>
CST Copenhagen pt n/a word/pos inline box/NN1
TreeTagger pt? n/a word pos lem opl The DT the
TnT token opl word pos opl der ART
word (pos pr)+ opl Falkenstein NE 8.00 NN 1.99
Twitter NLP pt opl word pos conf opl smh G 0.9406
NLTK pt s, bls [(?word?, ?pos?)] inline [(?At?, ?IN?), (?eight?, ?CD?),]
OpenNLP splitter pt n/a sentences ospl I can?t tell you if he?s here.
OpenNLP tokenizer sent ospl tokens wss, ospl I can ?t tell you if he ?s here .
OpenNLP tagger token wss, ospl word pos ospl At IN eight CD o?clock JJ on IN
pt = plain text opl = one per line wss = white space separated
ospl = one sentence per line bps = blank line separated
Figure 4: I/O variants for common splitters, tokenizers, and POS taggers
For example, the Stanford POS tagger XML output format produces output like this:
<word id="0" pos="VB">Let</word>
This maps to the following JSON-LD/WS-EV representation:
{
"@type" : "Token",
"id" : 0",
"start" : 18,
"end" : 21,
"features" : {
"string" : "Let",
"pos" : "VB"
}
}
The Stanford representation uses the term ?word? as an XML element name, gives an id and pos
as attribute-value pairs, and includes the string being annotated as element content. For conversion to
JSON-LD/WS-EV, ?word? is mapped to ?Token?, the attributes id and pos map to features of the Token
object with the same names, and the element content becomes the value of the string feature. Because
the JSON-LD representation uses standoff annotation, the attributes start and end are added in order to
provide the offset location of the string in the original data.
Services that share a format other than JSON-LD need not map into and out of JSON-LD/WS-EV
when pipelined in the LAPPS Grid. For example, two GATE services would exchange GATE XML
documents, and two UIMA services would exchange UIMA CAS, as usual. This avoids unnecessary
conversion and at the same time allows including services (consisting of individual tools or composite
workflows) from other frameworks. Figure 5 gives an example of the logical flow in the LAPPS Grid,
showing conversions into and out of JSON-LD/WS-EV where needed.
Each service in the LAPPS Grid is required to provide metadata that specifies what kind of input is
required and what kind of output is produced. For example, any service as depicted in the flow diagram
in Figure 5 can require input of a particular format (gate, uima, json-ld) with specific content (tokens,
sentences, etc.). The LAPPS Grid uses the notion of discriminators to encode these requirements, and
the pipeline composer can use these discriminators to determine if conversions are needed and/or input
requirements are met. The discriminators refer to elements of the vocabulary.
5 Collaborations
The LAPPS Grid project is collaborating with several other projects in an attempt to harmonize the
development of web service platforms, and ultimately to participate in a federation of grids and ser-
vice platforms throughout the world. Existing and potential projects across the globe are beginning to
41
Figure 5: Logical flow through the LAPPS Grid (client-server communication not represented)
converge on common data models, best practices, and standards, and the vision of a comprehensive in-
frastructure supporting discovery and deployment of web services that deliver language resources and
processing components is an increasingly achievable goal. Our vision is therefore not for a monolithic
grid, but rather a heterogeneous configuration of federated grids that implement common strategies for
managing and inter-changing linguistic information, so that services on all of these grids are mutually
accessible.
To this end, the LAPPS Grid project has established a multi-way international collaboration among the
US partners and institutions in Asia, Australia, and Europe. The basis is a formal federation among the
LAPPS Grid, the Language Grid (Kyoto University, Japan), NECTEC (Thailand), grids operated by the
University of Indonesia and Xinjiang University (China), and LinguaGrid
28
, scheduled for implementa-
tion in January 2015. The connection of these six grids into a single federated entity will enable access
to all services and resources on any of these grids by users of any one of them and, perhaps most impor-
tantly, facilitate adding additional grids and service platforms to the federation. Currently, the European
META-NET initiative is committed to joining the federation in the near future.
In addition to the projects listed above, we are also collaborating with several groups on technical
solutions to achieve interoperability and in particular, on development of the WS-EV, the JSON-LD
format, and a corollary development of an ontology of web service types. These collaborators include
the Alveo Project (Macquarie University, Australia) (Cassidy et al., 2014), the Language Grid project,
and the Lider project
29
. We actively seek collaboration with others in order to move closer to achieving
a ?global laboratory? for language applications.
6 Conclusion
In this paper, we have given a brief overview of the LAPPS Web Service Exchange Vocabulary (WS-
EV), which provides a terminology for a core of linguistic objects and features exchanged among NLP
tools that consume and produce linguistically annotated data. The goal is to bring the field closer to
achieving semantic interoperability among NLP data, tools, and services. We are actively working to both
engage with existing projects and teams and leverage available resources to move toward convergence
of terminology in the field for the purposes of exchange, as well as promote an environment (the LAPPS
Grid) within which the WS-EV can help achieve these goals.
28
http://www.linguagrid.org/
29
http://www.lider-project.eu
42
Acknowledgements
This work was supported by National Science Foundation grants NSF-ACI 1147944 and NSF-ACI
1147912.
References
Steve Cassidy, Dominique Estival, Timothy Jones, Denis Burnham, and Jared Burghold. 2014. The Alveo Virtual
Laboratory: A Web based Repository API. In Proceedings of the Ninth International Conference on Language
Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association
(ELRA).
Christopher Cieri, Denise DiPersio, , and Jonathan Wright. 2014. Intellectual property rights management with
web services. In Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT,
Dublin, Ireland, August.
David Ferrucci, Eric Nyberg, James Allan, Ken Barker, Eric Brown, Jennifer Chu-Carroll, Arthur Ciccolo, Pablo
Duboue, James Fan, David Gondek, Eduard Hovy, Boris Katz, Adam Lally, Michael McCord, Paul Morarescu,
Bill Murdock, Bruce Porter, John Prager, Tomek Strzalkowski, Chris Welty, and Wlodek Zadrozny. 2009.
Towards the Open Advancement of Question Answering Systems. Technical report, IBM Research, Armonk,
New York.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010.
Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59?79.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring
from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691?1701, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Elmer Garduno, Zi Yang, Avner Maiberg, Collin McCormack, Yan Fang, and Eric Nyberg. 2013. CSE Frame-
work: A UIMA-based Distributed System for Configuration Space Exploration Unstructured Information Man-
agement Architecture. In Peter Klgl, Richard Eckart de Castilho, and Katrin Tomanek, editors, UIMA@GSCL,
CEUR Workshop Proceedings, pages 14?17. CEUR-WS.org.
Sebastian Hellmann, Jens Lehmann, S?oren Auer, and Martin Br?ummer. 2013. Integrating nlp using linked data.
In 12th International Semantic Web Conference, 21-25 October 2013, Sydney, Australia.
Nancy Ide and James Pustejovsky. 2010. What Does Interoperability Mean, Anyway? Toward an Operational
Definition of Interoperability. In Proceedings of the Second International Conference on Global Interoperability
for Language Resources. ICGL.
Nancy Ide and Keith Suderman. 2014. The Linguistic Annotation Framework: A Standard for Annotation Inter-
change and Merging. Language Resources and Evaluation.
Nancy Ide, James Pustejovsky, Nicoletta Calzolari, and Claudia Soria. 2009. The SILT and FlaReNet international
collaboration for interoperability. In Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP,
August.
Nancy Ide, James Pustejovsky, Christopher Cieri, Eric Nyberg, Di Wang, Keith Suderman, Marc Verhagen, and
Jonathan Wright. 2014. The language application grid. In Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources
Association (ELRA).
ISO-24612. 2012. Language Resource Management - Linguistic Annotation Framework. ISO 24612.
Alkesh Patel, Zi Yang, Eric Nyberg, and Teruko Mitamura. 2013. Building an optimal QA system automatically
using configuration space exploration for QA4MRE?13 tasks. In Proceedings of CLEF 2013.
Zi Yang, Elmer Garduno, Yan Fang, Avner Maiberg, Collin McCormack, and Eric Nyberg. 2013. Building optimal
information systems automatically: Configuration space exploration for biomedical information systems. In
Proceedings of the CIKM?13.
43
