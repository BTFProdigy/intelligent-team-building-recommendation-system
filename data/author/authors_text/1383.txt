Explaining away ambiguity: Learning verb selectional preference 
with Bayesian networks* 
Mass imi l iano  C iarami ta  and Mark  Johnson  
Cognit ive and l , inguist ic  Sci('.n(:es 
Box \]978, Brown Univers i ty 
Pr()vi(h;n(:e, l{,I 02912, USA 
mas s ?m?:l J_ano_ciaramita@broun. edu mj @cs. broun, edu 
Abst ract  
This t)at)er presents a Bayesian lnodel for unsu- 
t)(;rvised learning of v(;rb s(;le(-t;ional t)refer(;nc('s. 
For each vert) the model creates a 13~Wo.sian 
n(~twork whose archii;e(:lan'(~ is (l(fl;(wmin(;d t).5' 
the h',xical hicr~m:hy of W()r(hmt mtd whos('~ 
1)ar~mmt;(;rs are ('~sl;im;~l:(~d from a list; of v('d'l)- 
ol)je('t pairs \]'cram from a tort)us. "lgxl)laining 
away", t~ well-known t)rop('xi;y of Baycsi~m net- 
works, helps the moth;1 (teal in a natural  fash- 
ion with word sense aml)iguity in tlw, training 
(tat~L. ()n a, word sense disamt)igu;~tion Lest our 
model t)erformed l)ctl;c,r than ot;h(',r stal;(~ of tim 
art systems for unSUl)crvis(~d learning ()t7 seh'x:- 
tionM t)r(d'er(mces. Coml)utational (:Oml)lcxity 
l)rol)lems, wn.ys of improving tiffs ;tl)l)roa(:h mM 
methods for iml)h'menting "('xt)laining away" in 
oth(;r graphical frameworks are dis('ussed. 
1 Se lec t iona l  p re ference  and sense 
ambigu i ty  
R('.gularil;i('~s of avcrt) with rcsl)e(:t o t;lw. seman- 
tic class of its m:guments (sul)j('.cl:, ol)j('.(:l; mM 
indirect o\])je(:l;) arc called selectional prefer- 
enees (S1)) (Katz and Fodor, 1964; Chomsky, 
1965; Johnson-Laird, 1983). The verb pilot car- 
ries the information thal; its ol)jecl; will likely 1)e 
some kind of veh, icle; sut)jects of tim vert) th, int,: 
t(md to 1)e h,'uman; ;rod sul)jects of the verb bark 
l;end l;o l)c, dogs. For the sake of simt)licity we 
will locus on the verl)-ot)je(:t relation all;hough 
the techniques we will describe can be at)t)li(;d 
to other verb-argument pairs. 
* We wouhl like to ttmnk the Brown Lal)orat;ory for Lin- 
guistic Inibrmation Processing; Thomas IIoflnann; Elie 
Bienenstoek; 1)hilip Resnik, who provided us with train- 
ing and test data; and Daniel Oarcia for his hel ) with the 
SMILE lil)rary of (:lasses tbr Bayesian etworks that we 
used for our exl)eriments. This research was SUl)l)orted 
1)y NSF awards 9720368, 9870676 and 9812169. 
. EN77TY._ 
,vomet\]tiug - " ~ , " - . .  " ~ ,~ . 
. _ " " CtHHy 
FOOl )  I , IQUl l )  PIISI:h'ICAI. O l t J I iCT 
..... "? / '  i " \  , I , \  \ \  . . . .  ; -  " \ \ ,  ICK .lbod 
/ I I . IM l iNT III'VI;'RA(;I" WAT/ : 'R  liqltid lAND olejeet ": 
? / 
71<4 bel'i'rat~e ('OI"H'.'I'; dr ink carlh land ISLAND ~ro l l ( 'APE  
<'q~lFe I;'S1't?1:'S.';0 .IA VA- I ,I/'~ VA-2 IIAI,I i.~hmd 
t . . . . . . . . . . . . . . .   i 
e.v~rcs.~o .\]av~ bali 
Figm'e 1: A 1)ortion of Wordnet. 
Models of the acquisition of SP arc impor- 
|;ant in their own right and h;w(', at)plic~tt.ions in 
N~tural ,anguage l~ro(:essing (NIA'). The selcc- 
tional l)rclhr(;nc(;s of ~L verb can b(; used t;o inti;r 
I;he l)ossil)\]c meanings of an mlknown re'gum(mr 
of a known verb; e.g., it; might be possibh; to 
infer that  xzzz  is ~ kind ot! dog front the tbllow- 
ing sentence: "The :rzJ:z barked all night". In 
p~rsing ;~ sentence seh,ctional l)refe, rcn(:es can 1)(; 
used to rank competing parses, providing a par- 
tim nlt',asur(; of scmmlt;ic well-forlnedness, in- 
v('stigating SI ) might hel l) us to understand the 
structure of the mental lexicon. 
Systems for mlsupervised learning of SP usu- 
ally combine statistical aim knowledge-1)ased 
approaches. The knowledge-base component 
is typicMly a database that  groups words into 
classes. In the models w(' will see. the knowl- 
edge base is Wordnet (Miller, 1990). Word- 
net groups nouns into c, lasses of synonyms 
ret)resenting concel)ts , called synsets ,  e.g., 
{car,,,'ld, o,a, 'utomobilc,. . .}. A noun that lm- 
hmgs to sew:ral synsets is ambiguous .  A t ran-  
187 
sitive and asymmetrical relation, hyponymy,  
is defined between synsets. A synset is a hy- 
ponym of another synset if the tbrmer has the 
latter as a broader concept; for example, BEV- 
ERAGE is a hyponym of LIQ UID. Figure 1 de- 
picts a portion of the hierarchy. 
The statistical component consists of 
predicate-argument pairs extracted from a 
corpus in which the semantic lass of the words 
is not indicated. A trivial algorithm might 
get a list of words that occurred as objects 
of the verb and output the semantic classes 
the words belong to according to Wordnet. 
For example, if the verb drink occurred with 
'water and water E L IQUID,  the nlodel would 
learn that drink selects tbr LIQUID. As Resnik 
(1997) and abney and Light (1999) have found, 
the main problem these systems face is the 
presence of ambiguous words in the training 
data. If the word java also occurred as an 
object of drink, since java C BEVERAGE 
and java C ISLAND,  this model would learn 
that drink selects tbr both BEVERAGE and 
ISLAND.  
More complex models have been t)roposed. 
These models, though, deal with word sense 
ambiguity by applying an unselective strategy 
similar to the one above; i.e., they assmne that 
anfl)iguous words provide equal evidence tbr all 
their senses. These models choose as the con- 
eepts the verb selects tbr those that are in com- 
mon among several words (e.g., BEVERAGE 
above). This strategy works to the extent that 
these overlapping senses are also the concepts 
the verb selects tbr. 
2 Prev ious  approaches to learn ing  
se lect iona l  preference 
2.1 P~esnik's model 
Ours system is closely related to those proposed 
in (Resnik, 1997) and (Atmey and Light, 1999). 
The fact; that a predicate p selects for a class 
c, given a syntactic relation r, can be repre- 
sented as a relation, selects(p, r, c); e.g., that 
eat selects for FOOD in object position can 
be represented as selects(eat, object, FOOD). 
In (Resnik, 1997) selectional preference is quan- 
tiffed by comparing the prior distribution of 
a given class c appearing as an argument, 
P(c)~ and the conditional probability of |;lie 
same class given a predicate and a syntac- 
COGNI770N 1/4 FOOD 7/4 
ESSENCE 1/4 FLESH 1/4 FRUH" 1/2 BREAD I/2 DAIRY I/2 
l' J l l l 
idea(O) meal(l) apple(I) bagel(l) cheese(l) 
Figure 2: Simplified Wordnet. The numbers 
next to the synsets represent he values of 
freq(p, r, c) estimated using (3), the nmnbers in 
l)arentheses represent the values of frcq(p, r, w). 
tic relation P(c\[p,r),  e.g., P(FOOD) and 
J ( OODleat, object), The relative ntropy be- 
tween P(c) and P(clp, r) measures how nmch 
the predicate constrains its arguments: 
S(p,r) = D(P(clp, r) II P(c)) (1) 
Resnik defines the se lect ional  assoc iat ion of 
a predicate for a particular class c to be the por- 
tion of the selectional preference strength due to 
that class: 
P(clP, ~') 1 P(clp , r) log (2) A(p,, . ,  c) - S(v, , ' )  P(c)  
Here the main problem is the estimation of 
P(clp, r). Resnik suggests  as  a plausil)le esti- 
mator /5(clp, r) de=r freq(p, r, c)/freq(p, r). 13ut 
since the model is trained on data that are not 
sense-tagged, there is no obvious way to esti- 
mate freq(p, r, c). Resnik suggests considering 
each observation of a word as evidence tbr each 
of the classes |;lie word belongs to, 
count(p, r, w) 
c) aasses(w) (3) 
~uc:e 
where count(p, r, w) is the nmnber of times 
the word w occurred as an argument of p in 
relation r, and classes(w) is the number of 
classes w belongs to. For exmnple, suppose 
the system is trained on (eat, object) pairs and 
the verb occurred once each with meat, ap- 
ple, bagel, and cheese, and Wordnet is simpli- 
fied as in Figure 2. An ambiguous word like 
meat provides evidence also tbr classes that ap- 
pear unrelated to those selected by the verb. 
Resnik's assumption is that only the classes e- 
lected by the verb will be associated with each 
188 
( )  ,-<., 
/ / - "  1\8 \ \ - .~S  
COGNIllON ( ) FOOl) 
1\7 " - .  " - - -  
( - - .  1 1 ' I ' I ' 
idea meat apph' bagel cheeae 
Figure 3: The HMM version of the siml)le ex- 
ample. 
of the observed words, and hence will re(:eive 
the highest values for l'(clp, r). Using (3) we 
fill(t that the highesl; fre(luen(;y is in t'~t(;t as- 
social;ed with FOOD: .f'req(ea, I, objecl,, food) 
I + ~ + 7 _1_ 2 1  ~ ~ -- 717 an(t I'(leOODie.,l, ) = 0.44. 
H()wever, some eviden('e is tomM also for COG- 
NIT ION:  .fr(:q(cat,, obj,cl,, co9'~,,it, io'~,,) ~ ? and 
I ' (COGNIT ' IONieat)  = 0.06. 
2.2 Abney  and  L ight ' s  approach  
Atmey and Light (1999) pointed ouI; l;h~tt l;he 
distri l)ution of se.llSeS of all anfl)iguous word is 
no|; unifornl. '\['tiey not;iced also l:hai; it is nol; 
clear how the 1)rol)ability l'((:\[p, 7") is t;o 1)e ini;er- 
t)reted sin(:e there is no exl)lieit sto(:llasl, i(: geil- 
(',ration nlodel involved. 
They \])rol)ose(t a syst;enl l;hat; ass()(',ial;es 
a Hidden Markov Model (\[IMM) wii;h 0,a(:h 
l)re(lic~te-re, lal;ion 1)air (p,r). 'l~'ansil;ions be- 
tween synsel; states rel)resent he hyt)onynly re- 
lation, and c, the empty word, is emitted with 
probal)ility 1; transit ions to ~t tinal sta|;e enli|; a 
word w with 1)rot)al)ility 0 < P('m) < I. 35an- 
sit|on and emission t)rol)al)ilities are est imated 
using the \]'DM ;dgorittnn on i;raining data I;hat 
consist of the lieu|is that o(;clirrext wi th  the w;rb. 
Abney and Ligh|,'s mo(lel e, stintates i '(clp, r ) 
ti'om the model trained tbr (p,r); the disl, ri- 
l int|on P(c) (:ml 1)e calculatxxt from a model 
trained for all n(mns in the (:orpus. 
This model did not perti)rm as well as ex- 
pected. All amt)iguous word ill the nlodel call 
be generated 1)y more than one sl;;~te sequence. 
Atmey and Light dis(:overed that the EM al- 
goril;hm tinds t)arameter wflues that associate 
some t)rol)ability mass with all the trmlsitions 
in the lnultil)le paths that lead to an ambigu- 
ous word. In other words, when there are sev- 
eral state sequences fi)r the same word, \]DM does 
not select one of l;hen: over the others. I Figure 3 
shows the I)arameters esl;imated by EM for the 
same examt)le as above. The transit ion to the 
COGNITION sl;ate has \])een assigne(t a t)rol)a- 
1)|lily of 1/8 because it is part of ~t possible l)ath 
to meat. The I IMM nlodel does not solve t, he 
l)roblent of the unselective distr ibution of the 
frequen(:y of oceurren(:e of an aml)iguous word 
to all its senses. A1)ney and Light claimed that; 
this is a serious l)roblem, par|;i(:iilarly when l;he 
aml)iguous word is a ti'equent one, and cruised 
the model to learn the wrong seleel;ional pref- 
eren('es. To (:orre(:i; this undesiral)le outcome 
they introduced some smoothing and t)alam:ing 
te, chniques. Howe, ver, even with these modiliea- 
tions their sysl;em's l)erfornlance, was t)elow fllal; 
a(:hieved l)y Resnik's. 
3 Bayes ian  networks  
A Bayes ian  network  (Pearl, 1988), or 
Bayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol; 
of var iab les  and a sel; of d i rec ted  edges  (:on- 
neel;ing the w~riat)les. The variables and tile 
edges detine, a dire,(:te, d acyclic graph (DAG) 
where each wtrial)le is rei)resented 1)y ~ node. 
Ea(:h vmi~d)le is asso(:iated with a finite number 
of (nmi;u;dly ex('lusive) sl;ates. '1)) each wu:ial)le 
A with \]);n'eni;s \]31,..., I7~ is ;t|;l;;mll(',(l ;t condi- 
tio'n, al probability tabh', (CPT) l'(A\[131, ...,Hn). 
Given a BBN, Bayesiml int~rence (:~m 1)e used 
1;o esi;ilm~l;e marg ina l  and poster io r  p roba-  
b i l i t ies  given the evidence at hand ~md (;It(', in- 
fornlation six)red in the CPTs,  the pr io r  prob-  
ab i l i t ies ,  by means of B~yes' rule, P(H IE  ) = 
l'(H)P(s~ln) P(E) , where It stands fi)r hyt)othesis and 
E tbr eviden(:e. 
Baye, sian nel;works display ml exl;remely inter- 
eslfing t)roi)ert;y called exp la in ing  away.  Word 
sense mnbiguity in the 1)recess of learning SP de,- 
tines a 1)rot)lem that nlight, l)e solved by a model 
that imt)lements an explaining away strategy. 
Sul)t)ose we ;~re learning the, selectional 1)refer- 
en(:e of drink, and the network ill Figure 4 is the 
'As a nmtt;er of fi*cl;, for this HMM there are (in- 
finitely) many i )a ramel ;e r  vahles that nmxinfize the like- 
lihood of t;he training data; i.e., l;he i)arame, l;ers are, not; 
idenl;ifiable. The intuil;ively correct solution is one of 
l;helll, \])ILL SO are infinitely lilalty ()|;her, intuitively incor- 
re(:t; ones. Thus il, is no surprise l;hat the EM algorithm 
emmet; lind the intuitively correct sohlt;ion. 
189 
I ISIAND ?> BEVERAGE 
.\](11'(1 0 WllICF 
Figure 4: A Bayesian network for word ainbigu- 
ity. 
knowledge base. The verb occurred with java 
and water. This situation can be ret)resented 
as a Bayesian network. The variables ISLAND 
and BEVERAGE represent concepts in a se- 
mantic hierarchy. The wtriables java and water 
stand for I)ossible instantiations of the concet)ts. 
All the w,riables are Boolean; i.e., they are as- 
sociated with two states, true or false. Suppose 
the tbllowing CPTs define the priors associated 
with each node. 2 The unconditional probabili- 
ties are P ( I  = t,'~,,e) = P (B  = t,'~,,~0 = 0.01 and 
P( I  = false) = I"(13 = false) = 0.99, and the 
CPTs for the child nodes are 
I =>)  I 
1,13 1, ~13 ~I,13 -~I, ~f3 
j = true 0.99 (I.99 0.99 0.01 
j = false 0.01 0.01 0.01 0.99 
w = true 0.99 0.99 0.01 (I.01 
w false. 0.01 0.01 0.99 0.99 
These vahms mean that the occu,'rence of either 
concept is a priori unlikely. If either concept is 
true the word java is likely to occur. Similarly, 
if BE VERA GE occurs it; is likely to observe also 
the word water. As the posterior probabilities 
show, if java occurs, the belief~ in both concepts 
increase: P(II.j) = P(B I j  ) = 0.3355. However, 
'water provides evidence for BEVERAGE only. 
Overall there is more evidence for the hypoth- 
esis that the concept being expressed is BEV- 
ERAGE and not ISLAND. Bayesian networks 
implement his inference scheme; if we compute 
the conditional probabilities given that both 
words occurred, we obtain P(BI j  , w) = 0.98 and 
P(I\[ j ,  w) = 0.02. The new evidence caused the 
"island" hyt)othesis to be explained away! 
3.1 The relevance of priors 
Explaining away seems to depend on the spec- 
ification of the prior prolmt)ilities. The priors 
2I, 13, j and w abbrev ia te  ISLAND, 1lEVERAGE, 
java and water, respect ively.  
(-)coaNmo,v ) roe, \[0 ii~ 
\] L_ m 
Figm'e 5: A Bayesian network for the simple 
example. 
define the background knowledge awdlable to 
the model relative to the conditional probabili- 
ties of the events represented by the variables, 
but also about the joint distributions of several 
events. In the simple network above, we de- 
fined the probat)ility that either concept is se- 
lected (i.e., that the correst)onding variable is 
true) to be extremely small. Intuitively, there 
are many concepts and the probability of ob- 
serving any particular one is small. This means 
that the joint probability of the two events is 
much higher in the case in which only one of 
them is true (0.0099) than in the case in which 
they are both true (0.0001). Therefore, via the 
priors, we introduced a bias according to which 
the hypothesis that one concept is selected will 
be t/wored over two co-occurring ones. This is a 
general pattern of Bayesian networks; the prior 
causes impler explanations to be preferred over 
more complex ones, and thereby the explaining 
away effect. 
4 A Bayesian network approach to 
learning select ional  preference 
4.1 Structure and parameters of the 
model 
The hierarchy of nouns in Wordnet defines a 
DAG. Its mapping into a BBN is straightibr- 
ward. Each word or synset in Wordnet is a 
node in the network. If A is a hyponym of B 
there is an are in the network from B to A. All 
the variables are Boolean. A synset node is true 
if the verb selects for that class. A word node is 
true if the word can appear as an argument of 
the verb. The priors are defined tbllowing two 
intuitive principles. First, it is unlikely that a 
verb a priori selects for troy particular synset. 
Second, if a verb does select for a synset, say 
FOOD, then it; is likely that it also selects tbr 
190 
i(;s hyl)onyms, say FR.UFI'. The sam(', l>rin(:il)les 
;H)t)Iy (;o words: it is likely l;h~t a wor(t ;q)I)ears 
as an m:gmnent of t,h(; verl) if the vert) seh;(:l;s for 
any of il;s possible senses. On l;he other h~m(t, 
if (,he verb does nol; selecl; for a synsel;, it; is 
'u, nlikely that  the words insl;anl;b~l;ing (;he synse(; 
occur ~s its ;~rgumen~s. "Likely" and "unlikely" 
are given mml(;rical values l;h~l; Sllill 1l t) 1;o 1. 
The following l;at)le defines l;\]te scheme for the 
CPTs  associated wil;h each node in the nel;work; 
p i (X)  (lcnot;cs 1;12(: il.h, t2ar(:n(, of (;h(: no(t(: X. 
F__ \ ] .  P(X = xlI, I(X)V,... , VI,,(X) = t,",,(~) \] 
z .lalsc, unl ihc ly 
\[_ T l " (X = : , : lp , (X )a , . . . , /> , , (X )= .l',,.Z.~(:) \] 
:t; .fitls(: like.l?/ 
For (;h(; rool; nod(:s, the l;;d)le r(:(hl(:es 1;o (;h(: 
uncondil;ion~d t)rol)al/iliI;y of (;h(: node. Now 
WO, (;}UI I;(:Si; (;he mo(M on (;h(: siml)l(: ex~mq)\](: 
seen (:~rli(:r. W + is th(: set of wor(ls l;ha( o(:- 
(:m'rc(t with I;he v(:rl). 'l.'\]l(: \]2o(l(:s (:orr(:st)on(l- 
ing (;o |;he wor(ls in l/l/q ;w(: s(:l, 1;o lr,,,c ;rod 
(;h(: o(;hers l(:f(; mls(:l;. For 1;12(: l)revious ex- 
ample W -I = {mca/,~ apph'., bagel, ch, c~c'.sc'.}, mt(\] 
l;h(; (:orrest)on(ting no(t(:s m:c sol; (;o I/rue, as (t(:- 
t)i(:l;e(t in Figm'(; 5. Wil;h lil,:(dy ;m(t ',.nhT~:c'.ly 
r(:sl)(:(:l;iv(:ly equal Ix) 0.9.0 mM 0.01, tit(: i)osl;(:- 
rior l)rol);d)ilil;i(:s are a /)(/i'\[m, a., b, c.) = 0.9899 
mM .P(Cl,m,, a, b, c) = 0.0101. Expla.ining away 
works. Th(; t)osl;(;rior 1)rol);fl)ilil;y of COGNI -  
T ION g(;l;s as low as i(;s prior, whcr(',as l;h(; 
1)rol)al)ilil;y of FOOD goes u t) to almost; 1. A 
13~y(;sim~ n(;l;work ~q)t/roa(:\]~ seems 1;o ;l(:l;u~dl.y 
imt)leancn(; he. conse.rva/,'ive, stral;egy w(: l;hough(; 
(;o 1)e (;he corr(;(:I; one. for unsupervisc(t l(;~mfing 
of sehx-t, ional resi;ri(:tions. 
4.2 Computat iona l  i ssues  in bu i ld ing  
BBNs  based  on  Wordnet  
Th(: imt)l(:m(:ni;~d;ion f a BBN for (;h(; whole of 
W()r(lnel; fax:as (:oml)ul;al;ional (:oml)lexi|;y pro|)- 
l(:ms (;ypi(:al of graphi(:al too(Ms. A (l(:ns(:ly 
(:ommcI;ed \]IBN presents (;wo kinds of l)rol)l(:ms. 
The tirst is (;he storage of the CPTs.  The size 
of a CPT  grows extIonenl;ially with the nmnber 
of parents of the node. 4 This prol)lem can lie 
aF, C, m, a, b and c respec(;ively stand for FOOD,  
COGNITION, meat, apph'., bagd and chces(: 
4Some words in \Vordnet have mor(: than 20 senses. 
For (,Xaml)h: , line in \'Vordne(: is asso(:ia(,ed with 25 
EN77TY 
? j 
? "~.  
- / 
I"0()1) I,IQUID 
\- ..// 
B E VERA GE 
('OFI"EI? drink 
I 
,IA VA - I 
I>IISYSIC'AL OB,II'2CT 
1AND 
t 
ISIANI) 
>/ 
. /  
/ -  
JAVA -2 
X. / : /  
j<,ra 
Figure. 6: Th(; sulmel;work for d'riuJ<. 
solved 1)y ot)i;infizing the r(:l)r(:s(;nl;~ti;ion f thes(: 
(;;d)l(:s. In our case most of l,h(: (:ntri(:s h~w(: l;he 
s~tln('~ v;~luos~ ~ttl(l ~t COlll\])a,(:(; re l )resenl ;a l ; ion for 
l ; lmln ( ;&n l)(; fOllll(\] (ltlll(;h l ike l;h(', ( )he  llS(Xl in 
(;h(', no isy -OR too(tel (Pearl, 1{)88)). 
A h;~r(l(:r lirol)lem is lmrforming inf(;rc\]me. 
The gr;q)hi(:al sl;rlt('i;llr( ~,of & BBN r(;t)resenl;s 
l;h(: (l(:t)(:n(len(:y r(:lal;ions among th(: rml(lom 
wtrial)lcs of the, nel;work, r\]?he ~dgoril;hlns use(t 
wil;lt B\]INs usmdly l)(M'orm inference t)y (ty- 
mmfi(" t)rogrmmning on the tri~mgul~d;e(t lnor~d 
gr~q)h. A low(n" 1)(mn(t on l, he mmfl)er of (:om- 
l)ul;al;ions l;h;~(; are n(:(:(:ssa.ry I;() mo(t(:I l;h(; joint 
(lisl;ritmi;ion ov(:r l;h(: wn'bd)h:s using su(:h ~dgo- 
ril;hms is 21"1t I wh(:r(: 'r~, is t;\]m size of (:h(; ma.x- 
imal l)(mn(tary s(:l; a(:(:or(ling (;o t;hc visil;a,tion 
s(:h(:(lul(:. 
4.3 Subnetworks  and  ba lanc ing  
B(,(:mls(; of l;h(:s(, 1)rol)h:ms w(: (:(told not t)uihl a 
singl(: BBN for Wor(hmI;. Insl;e~M w(: simt/litie(t 
(;he sl;rll('l;ur(: of 1;12(: model by building a smaller 
sutmei;work for each 1)re(ticate-argumenl; pair. A 
sulm(:twork consis(;s of (;he mlion of the s(:ts of 
ml(:(;sl;ors of the words in W +. Figure. 6 pro- 
vid(:s ml example of the union of these :%nces- 
tral sul)grat)hs" of Wordne(; for (;he words java 
~m(l drink (COml)~We i(; wil;h Figure 1). 
This siml)liti(:ation (toes not atfe(:t the (:om- 
pul;~tion of the (tistril)ui;ions we are inl;(;resl;ed 
in; l;h;fl; is, the marginals of the synset nodes. 
A BBN provi(tes a coral)act representation tbr 
the joinI; disl;rit)ution over the set of variables 
senses. The size of its OPT is therefor('. 2 2(~. Six)ring a (:a- 
1)Ie of tloa(; numbers tbr l;his node alone requires around 
(2'-)~)8 = 537 MBytes of memory. 
191 
in the network. If N = X1, ..., Xn  i8 a Bayesian 
network with variables X1,..., Xn, its joint dis- 
tribution P(N) is the product of all the condi- 
tional probabilities pecified in the network, 
P (N)  = I I  P(XJl p,,(Xj)) (4) 
J 
where pa(X) is the set of parents of X. A BBN 
generates a factorization of the joint distribu- 
tion over its variables. Consider a network of 
three nodes A, B~ C with arcs fl'om A to \]5 and 
C. Its joint distribution can be characterized as 
P(A, B, C) = P(A)P(BIA)P(CIA ). If there is 
no evidence for C the joint distribution is 
P(A ,B ,C)  = P(A)P(BIA ) ~ P(CIA ) 
c 
= P(A)P(B IA  ) 
= P(A, B) 
The node C gets marginalized out. Marginaliz- 
ing over a childless node is equivalent o remov- 
ing it with its connections from the network. 
Therefore the subnetworks are equivalent to the 
whole network; i.e., they have the same joint 
distribution. 
Our model comtmtes the value of P(c\[p,r), 
lint we did not compute the prior P(c) for all 
n(mns in the cortms. We assumed this to be 
a constant, equal to the 'u'nlihcly wflue, for all 
classes. In a BBN the wdues of the marginals 
increase with their distance fl'om the root nodes. 
To avoid undesired bias (see table of results) we 
defined a balancing formula that adjusted the 
conditional probabilities of the CPTs in such a 
way that we got; all tim marginals to have ap- 
proximately the same wdue)  
5 Exper iments  and  resu l t s  a 
5.1 Learn ing of  se leet iona l  pre ferences  
When trained on t)redicate-argument pairs ex- 
tracted from a large corpus, the San .Jose Mer- 
cury Corpus, the model gave very good results. 
The corpus contains about 1.a million verb- 
object tokens. The obtained rankings of classes 
according to their posterior marginal probabili- 
ties were good. Table 1 shows the top and the 
'~More details can be found in an extended version of 
the paper: www.cog.brown.edu/~massi/. 
6For these experimc'nts we used values for the likely 
and unlikely 1)arameters of 0.9 and 0.1~ respectively. 
Ranking Synset P(clp, r) 
1 VEIIICLE 0.9995 
2 VESSEL 0.9893 
3 AIRCRAFT 0.9937 
4 AIRPLANE 0.9500 
5 SHIP 0.9114 
255 CONCEPT 0.1002 
256 LAW 0.1001 
257 PIIILOSOPIIY 0.1000 
258 ,IUI?,ISPRUDENCE 0.1000 
Table 1: Results tbr (maneuver, object). 
bottom of the list of synsets for the verb ma- 
neuver. Tile model learned that maneuver "se- 
lects" for melnbers of the class VEttlCLE and 
of other plausible classes, hyponynls of I/EHI - 
CLE. It also learned that the verb does not 
select for direct; objects that are inembers of 
(:lasses, like CONCEPT or PItILOSOPltY. 
5.2 Word  sense d i sambiguat ion  test  
A direct ewfluation measure for unsupervised 
learning of SP models does not exist. These 
lnodels are instead evaluated on a word-sense 
disambiguation test (WSD). The idea is that 
systems that learn SP produce word sense dis- 
amt)iguation as a side-effect. Java might be in- 
terl)reted as the island or the beverage, but in a 
context like "the tourists flew to Java" the for- 
mer seems more correct, because fly could select 
for geographic locations but not for beverages. 
A system trained on a predicate p should he 
able to disambiguate arguments of p if it has 
learned its selectional restrictions. 
We tested our model using the test and 
training data developed by Resnik (see Resnik, 
1997). The same test was used in (Almey 
and Light, 1999). The training data consists 
of predicate-object ounts extracted fl'oln 4/5 
of the Brown corpus (at)out 1M words). The 
test set consists of predicate-object pairs from 
the remaining 1/5 of the corpus, which has 
been manually sense-annotated by Wordnet re- 
searchers. The results are shown in Table 2. 
The baseline algorithm chooses at random one 
of the multiple senses of an ambiguous word. 
The "first sense" method always chooses the 
most frequent sense (such a system should be 
trained on sense-tagged data). Our model per- 
192 
Method Result 
Baseline 28.5% 
Abney mM Light (HMM smootlm(l) 35.6% 
Abney and Light (ItMM 1)alan(:ed) 42.3% 
Resnik 44.3% 
BBN (without bM~mcing) 45.6% 
BBN (with bMancing) 51.4(/(/ 
First Sense 82.5(/0 
Table 2: R,esults 
formed 1)etter th;m the state of tlm art mo(tels 
for mlSUl)ervised le~n'ning of SP. It seems to de- 
fine i~ l)ett(;r esi;ima, l;or for \])(ell), 'r). 
It is remnrkabh: fliat the model ~mhi(:ved this 
result making only a limi(;(:(t use of distribu- 
tional informal;ion. A n(mn is in 14 f+ if it oe- 
('urred at h;ast once in th(: tra,ining set, 1)ut the 
sysi;em does not know if i(; o(:(:urre(l once or sev- 
eral times; either it oc(:urred or it didn't. The 
nl()(te, l did not; suffer too mu(:h froxn this limi- 
(;ntion (htril~g (;his task. This is 1)rol)~d)ly (tuc 
to the Sl)arsencss of the (;rltining (t;tl;a for (;tie 
test. For each verb (;he average mmfl)er ()f el)- 
\]eel; tyl)es is 3.3, for each of them tim :werng(: 
lxumber of (;ok(ms is 1.3; i.e., most of the, words 
in the training data. only ()(:(:urred once. Pin" 
this training set we ~dso t(:sted a version of (;he 
m()del that |rail(; it word node ti)r each el)served 
ol)je(:l; token ;~n(| (;here, fore inl,(~gral;e(t the (listri- 
lmtional informntion. ()n (;\]m WSI) test it; per- 
ti)rmed exactly the stone its the simph~r version. 
When trained on the, San .lose Mercury Cort)us 
the model l)erfornle(t worse on the WSI) t:esl; 
(35.8%). This is not too surprising considering 
(;he diftbxelmes beA;ween tim SJM and the \]~rown 
(:ort)or~: the former, i~ re(:ent newswire cortms; 
the llg;ter, &Xl older, |)alml(:ed ('orl)uS. Allot;her 
ilnportmlt factor is the different relevance of dis- 
tributional informntion. The training (tatn fl'om 
the SJM Corlms are nnlch ri(:her and noisier 
than the Brown data. Here the, fl'equen(:y in- 
tbrm~tion is probably crucbfl; however, in this 
case we could not imt)lement the silnl)le s('hem(~ 
;tb ove. 
5.3 Conc lus ion  
Explaining away imt)lements ;~ (:ognitively ~tt- 
tractive and successful strategy. A straighttbr- 
ward lint)rove, men( would lm tbr the me(tel to 
make flfll use of the distrilmtional ildbrmtLtion 
present in the training data; we only partially 
achieved this. B~yesian networks are usually 
confronted with a single present~Ltion of evi- 
den('e. Their exi;ension to multil)le evidence is 
not triviM. We believe the model can be ex- 
tended in this direction. Possibly there m'e sev- 
erM ways to do so (muir(hernial Saml)ling , ded- 
i(:ated implementations, etc.). However, we be- 
lieve that the most relevmit finding of this re- 
search mighl; t)e (;h~t(; %xplnining itww" is not 
only a 1)roperl;y of Bayesian networks but of 
\]3ayesimx infe, rence in general and that it might 
tm imt)lemental)le in other kinds of graphical 
models. We, observed that (;he prol)erty seems to 
del)end on the specification of the prior proba- 
bilities. We t'omld (;lm(; (;he HMM model of (Ab- 
hey mid Ligh(;, 1999) was 'unidentifiable; that is, 
(;here are several sohli;iolxs tbr the \])~tra, xnel;els of 
the lno(te,1, including the desired Olle. OHr intu- 
ition is (;hat it shouht l)e possible to imt)lemelxt 
"exl)laining awlty" in a HMM with 1)tiers, so 
(;hit(; il; wouh\[ 1)rethr only ()lie or ~t ti~w solu(;ions 
()ver lmuiy. This model would have also the a(t- 
wmtttgc of t)eing (:Onll)U|;~d;ionally silnt)ler. 
References  
S. Almey told M. Light. 1999. Hiding a serum> 
tic hierarchy in ~ Mm'kov model. In P'm(:e,d- 
ings q/' the Workshop o'n Uns'~q)ervi,scd Le,,'r'n,- 
ing in Nat'wra,1 Lang,uagc Proeessin9, A CL. 
N. Chomsky. 1965. Aspcct.s of th, e Theory of 
Sy'nta,:r. MIT Press, Cambridge, MA. 
P. N. Johnson-Laird. 1983. Mental Models: 2b- 
'wards a Cognitive Sciev, ce of Lang'uage , Pn:fi',r- 
c'nce, a'n,d Con.sciousncss. ilm'w~rd University 
Press. 
J . . \] .  Kntz and .\]. A. Fodor. 1964. The. struc- 
ture of ;~ semmiti(: theory. In ,\]. ,J. KaI;z mM 
J. A. l/odor, editors, The St'ructure of Lan- 
g'uage. Prentice-Ilall. 
G. Miller. 299(I. Wordnet: An on-line lexical 
database, btternational Journal of Lexicog- 
raphy, 3(4). 
J. Pearl. \] 988. Probabilistic R, easo'n, ing in Intel- 
ligent, Systems: Net'worl?s of Plausible l~l:fer- 
e'n, ee. Morgml Kimthmn. 
P. Resnik. 2997. Sele(:tiona\] prefi:ren(:e and 
sens(; disamt)iguation. In Proceedings of th.e 
ANLP-97 Workshop: Taggin 9 Text with Lex- 
ical Semantics: Wll.y, What, and Itow? 
193 
Boosting automatic lexical acquisition with morphological information  
Massimiliano Ciaramita
Department of Cognitive and Linguistic Sciences
Brown University
Providence, RI, USA 02912
massimiliano ciaramita@brown.edu
Abstract
In this paper we investigate the impact of
morphological features on the task of au-
tomatically extending a dictionary. We
approach the problem as a pattern clas-
sification task and compare the perfor-
mance of several models in classifying
nouns that are unknown to a broad cov-
erage dictionary. We used a boosting clas-
sifier to compare the performance of mod-
els that use different sets of features. We
show how adding simple morphological
features to a model greatly improves the
classification performance.
1 Introduction
The incompleteness of the available lexical re-
sources is a major bottleneck in natural language
processing (NLP). The development of methods for
the automatic extension of these resources might af-
fect many NLP tasks. Further, from a more general
computational perspective, modeling lexical mean-
ing is a necessary step toward semantic modeling of
larger linguistic units.
We approach the problem of lexical acquisition
as a classification task. The goal of the classifier is
to insert new words into an existing dictionary. A
dictionary1 in this context simply associates lexical

I would like to thank for their input everybody in the Brown
Laboratory for Linguistic Information Processing (BLLIP) and
Information Retrieval and Machine Learning Group at Brown
(IRML), and particularly Mark Johnson and Thomas Hofmann.
I also thank Brian Roark and Jesse Hochstadt.
1Or lexicon, we use the two terms interchangeably.
forms with class labels; e.g.,
 	
	
,
where the arrow can be interpreted as the ISA rela-
tion. In this study we use a simplified version of
Wordnet as our base lexicon and we ignore other
relevant semantic relations (like hyponymy) and the
problem of word sense ambiguity. We focus on
finding features that are useful for associating un-
known words with class labels from the dictionary.
In this paper we report the following preliminary
findings. First of all we found that the task is dif-
ficult. We developed several models, based on near-
est neighbor (NN), naive Bayes (NB) and boosting
classifiers. Unfortunately, the error rate of these
models is much higher than what is found in text
categorization tasks2 with comparable numbers of
classes. Secondly, it seems obvious that informa-
tion that is potentially useful for word classifica-
tion can be of very diverse types, e.g., semantic
and syntactic, morphological and topical. There-
fore methods that allow flexible feature combination
and selection are desirable. We experimented with a
multiclass boosting algorithm (Schapire and Singer,
2000), which proved successful in this respect. In
this context boosting combines two sources of in-
formation: words co-occurring near the new word,
which we refer to as collocations, and morpholog-
ical properties of the new word. This classifier
shows improved performance over models that use
only collocations. In particular, we found that even
rudimentary morphological information greatly im-
2Text categorization is the task of associating documents
with topic labels (POLITICS, SPORT, ...) and it bears simi-
larities with semantic classification tasks such as word sense
disambiguation, information extraction and acquisition.
                     July 2002, pp. 17-25.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
SHAPE TRAIT QUALITY PROPERTY OTHER ATTR SOCIAL REL SPATIAL REL OTHER REL TIME OTHER ABS
ATTRIBUTE RELATION
MEASURE
ABSTRACTION
Figure 1: A few classes under the root class ABSTRACTION in MiniWordnet.
proves classification performance and should there-
fore be part of any word classification model.
The outline of the paper is as follows. In section
2 we introduce the dictionary we used for our tests,
a simplified version of Wordnet. In section 3 we de-
scribe more formally the task, a few simple mod-
els, and the test methods. In section 4 we describe
the boosting model and the set of morphological fea-
tures. In section 5 we summarize the results of our
experiments. In section 6 we describe related work,
and then in section 7 we present our conclusions.
2 MiniWordnet
Ideally the lexicon we would like to extend is a
broad coverage machine readable dictionary like
Wordnet (Miller et al, 1990; Fellbaum, 1998). The
problem with trying to directly use Wordnet is that it
contains too many classes (synsets), around 70 thou-
sand. Learning in such a huge class space can be
extremely problematic, and intuitively it is not the
best way to start on a task that hasn?t been much ex-
plored3. Instead, we manually developed a smaller
lexicon dubbed MiniWordnet, which is derived from
Wordnet version 1.6. The reduced lexicon has the
same coverage (about 95 thousand noun types) but
only a fraction of the classes. In this paper we con-
sidered only nouns and the noun database. The goal
was to reduce the number of classes to about one
hundred4 of roughly comparable taxonomical gen-
erality and consistency, while maintaining a little bit
of hierarchical structure.
3Preliminary experiments confirmed this; classification is
computationally expensive, performance is low, and it is very
hard to obtain even very small improvements when the full
database is used.
4A magnitude comparable to the class space of well stud-
ied text categorization data sets like the Reuters-21578 (Yang,
1999).
The output of the manual coding is a set of 106
classes that are the result of merging hundreds of
synsets. A few random examples of these classes
are PERSON, PLANT, FLUID, LOCATION, AC-
TION, and BUSINESS. One way to look at this set
of classes is from the perspective of named-entity
recognition tasks, where there are a few classes of
a similar level of generality, e.g, PERSON, LOCA-
TION, ORGANIZATION, OTHER. The difference
here is that the classes are intended to capture all
possible taxonomic distinctions collapsed into the
OTHER class above. In addition to the 106 leaves
we also kept a set of superordinate levels. We
maintained the 9 root classes in Wordnet plus 18
intermediate ones. Examples of these intermedi-
ate classes are ANIMAL, NATURAL OBJECT, AR-
TIFACT, PROCESS, and ORGANIZATION. The rea-
son for keeping some of the superordinate structure
is that hierarchical information might be important
in word classification; this is something we will in-
vestigate in the future. For example, there might not
be enough information to classify the noun ostrich
in the BIRD class but enough to label it as ANIMAL.
The superordinates are the original Wordnet synsets.
The database has a maximum depth of 5.
We acknowledge that the methodology and results
of reducing Wordnet in this way are highly subjec-
tive and noisy. However, we also think that go-
ing through an intermediary step with the reduced
database has been useful for our purposes and it
might also be so for other researchers5. Figure 1 de-
picts the hierarchy below the root class ABSTRAC-
TION. The classes that are lined up at the bottom
of the figure are leaves. As in Wordnet, some sub-
5More information about MiniWordnet and the
database itself are available at www.cog.brown.edu/ 
massi/research.
hierarchies are more densely populated than others.
For example, the ABSTRACTION sub-hierarchy is
more populated (11 leaves) than that of EVENT (3
leaves). The most populated and structured class is
ENTITY, with almost half of the leaves (45) and sev-
eral superordinate classes (10).
3 Automatic lexical acquisition
3.1 Word classification
We frame the task of inserting new words into the
dictionary as a classification problem:  is the set
of classes defined by the dictionary. Given a vector
of features Supersense Tagging of Unknown Nouns in WordNet  
Massimiliano Ciaramita
Brown University
massi@brown.edu
Mark Johnson
Brown University
mark johnson@brown.edu
Abstract
We present a new framework for classify-
ing common nouns that extends named-
entity classification. We used a fixed set
of 26 semantic labels, which we called su-
persenses. These are the labels used by
lexicographers developing WordNet. This
framework has a number of practical ad-
vantages. We show how information con-
tained in the dictionary can be used as ad-
ditional training data that improves accu-
racy in learning new nouns. We also de-
fine a more realistic evaluation procedure
than cross-validation.
1 Introduction
Lexical semantic information is useful in many nat-
ural language processing and information retrieval
applications, particularly tasks that require com-
plex inferences involving world knowledge, such
as question answering or the identification of co-
referential entities (Pasca and Harabagiu, 2001;
Pustejovsky et al, 2002).
However, even large lexical databases such as
WordNet (Fellbaum, 1998) do not include all of
the words encountered in broad-coverage NLP ap-
plications. Ideally, we would like a system that
automatically extends existing lexical resources by

We would like to thank Thomas Hofmann, Brian Roark,
and our colleagues in the Brown Laboratory for Linguistic In-
formation Processing (BLLIP), as well as Jesse Hochstadt for
his editing advice. This material is based upon work supported
by the National Science Foundation under Grant No. 0085940.
identifying the syntactic and semantic properties of
unknown words. In terms of the WordNet lexical
database, one would like to automatically assign un-
known words a position in the synset hierarchy, in-
troducing new synsets and extending the synset hier-
archy where appropriate. Doing this accurately is a
difficult problem, and in this paper we address a sim-
pler problem: automatically determining the broad
semantic class, or supersense, to which unknown
words belong.
Systems for thesaurus extension (Hearst, 1992;
Roark and Charniak, 1998), information extrac-
tion (Riloff and Jones, 1999) or named-entity recog-
nition (Collins and Singer, 1999) each partially ad-
dress this problem in different ways. The goal
in these tasks is automatically tagging words with
semantic labels such as ?vehicle?, ?organization?,
?person?, etc.
In this paper we extend the named-entity recogni-
tion approach to the classification of common nouns
into 26 different supersenses. Rather than define
these ourselves, we adopted the 26 ?lexicographer
class? labels used in WordNet, which include labels
such as person, location, event, quantity, etc. We be-
lieve our general approach should generalize to other
definitions of supersenses.
Using the WordNet lexicographer classes as su-
persenses has a number of practical advantages.
First, we show how information contained in the dic-
tionary can be used as additional training data that
improves the system?s accuracy. Secondly, it is pos-
sible to use a very natural evaluation procedure. A
system can be trained on an earlier release of Word-
Net and tested on the words added in a later release,
1 person 7 cognition 13 attribute 19 quantity 25 plant
2 communication 8 possession 14 object 20 motive 26 relation
3 artifact 9 location 15 process 21 animal
4 act 10 substance 16 Tops 22 body
5 group 11 state 17 phenomenon 23 feeling
6 food 12 time 18 event 24 shape
Table 1. Lexicographer class labels, or supersenses.
since these labels are constant across different re-
leases. This new evaluation defines a realistic lexi-
cal acquisition task which is well defined, well mo-
tivated and easily standardizable.
The heart of our system is a multiclass perceptron
classifier (Crammer and Singer, 2002). The features
used are the standard ones used in word-sense classi-
fication and named-entity extraction tasks, i.e., col-
location, spelling and syntactic context features.
The experiments presented below show that when
the classifier also uses the data contained in the dic-
tionary its accuracy improves over that of a tradition-
ally trained classifier. Finally, we show that there are
both similarities and differences in the results ob-
tained with the new evaluation and standard cross-
validation. This might suggest that in fact that the
new evaluation defines a more realistic task.
The paper is organized as follows. In Section 2
we discuss the problem of unknown words and the
task of semantic classification. In Section 3 we de-
scribe the WordNet lexicographer classes, how to
extract training data from WordNet, the new evalu-
ation method and the relation of this task to named-
entity classification. In Section 4 we describe the
experimental setup, and in Section 5 we explain the
averaged perceptron classifier used. In Section 6 and
7 we discuss the results and the two evaluations.
2 Unknown Words and Semantic
Classification
Language processing systems make use of ?dictio-
naries?, i.e., lists that associate words with useful
information such as the word?s frequency or syn-
tactic category. In tasks that also involve inferences
about world knowledge, it is useful to know some-
thing about the meaning of the word. This lexical
semantic information is often modeled on what is
found in normal dictionaries, e.g., that ?irises? are
flowers or that ?exane? is a solvent.
This information can be crucial in tasks such
as question answering - e.g., to answer a ques-
tion such as ?What kind of flowers did Van Gogh
paint?? (Pasca and Harabagiu, 2001) - or the indi-
viduation of co-referential expressions, as in the pas-
sage ?... the prerun can be performed with 	

... this 

 
 can be considered ...? (Pustejovsky
et al, 2002).
Lexical semantic information can be extracted
from existing dictionaries such as WordNet. How-
ever, these resources are incomplete and systems
that rely on them often encounter unknown words,
even if the dictionary is large. As an example, in the
Bllip corpus (a very large corpus of Wall Street Jour-
nal text) the relative frequency of common nouns
that are unknown to WordNet 1.6 is approximately
0.0054; an unknown noun occurs, on average, ev-
ery eight sentences. WordNet 1.6 lists 95,000 noun
types. For this reason the importance of issues such
as automatically building, extending or customizing
lexical resources has been recognized for some time
in computational linguistics (Zernik, 1991).
Solutions to this problem were first proposed
in AI in the context of story understanding, cf.
(Granger, 1977). The goal is to label words using
a set of semantic labels specified by the dictionary.
Several studies have addressed the problem of ex-
panding one semantic category at a time, such as
?vehicle? or ?organization?, that are relevant to a
particular task (Hearst, 1992; Roark and Charniak,
1998; Riloff and Jones, 1999). In named-entity clas-
sification a large set of named entities (proper nouns)
are classified using a comprehensive set of semantic
labels such as ?organization?, ?person?, ?location?
or ?other? (Collins and Singer, 1999). This latter
approach assigns all named entities in the data set a
semantic label. We extend this approach to the clas-
sification of common nouns using a suitable set of
semantic classes.
3 Lexicographer Classes for Noun
Classification
3.1 WordNet Lexicographer Labels
WordNet (Fellbaum, 1998) is a broad-coverage
machine-readable dictionary. Release 1.71 of the
English version lists about 150,000 entries for all
open-class words, mostly nouns (109,000 types), but
also verbs, adjectives, and adverbs. WordNet is or-
ganized as a network of lexicalized concepts, sets of
synonyms called synsets; e.g., the nouns  chairman,
chairwoman, chair, chairperson  form a synset. A
word that belongs to several synsets is ambiguous.
To facilitate the development of WordNet, lexi-
cographers organize synsets into several domains,
based on syntactic category and semantic coherence.
Each noun synset is assigned one out of 26 broad
categories1. Since these broad categories group to-
gether very many synsets, i.e., word senses, we call
them supersenses. The supersense labels that Word-
Net lexicographers use to organize nouns are listed
in Table 12. Notice that since the lexicographer la-
bels are assigned to synsets, often ambiguity is pre-
served even at this level. For example, chair has
three supersenses: ?person?, ?artifact?, and ?act?.
This set of labels has a number of attractive fea-
tures for the purposes of lexical acquisition. It is
fairly general and therefore small. The reasonable
size of the label set makes it possible to apply state-
of-the-art machine learning methods. Otherwise,
classifying new words at the synset level defines a
multiclass problem with a huge class space - more
than 66,000 noun synsets in WordNet 1.6, more than
75,000 in the newest release, 1.71 (cf. also (Cia-
ramita, 2002) on this problem). At the same time
the labels are not too abstract or vague. Most of the
classes seem natural and easily recognizable. That
is probably why they were chosen by the lexicog-
raphers to facilitate their task. But there are more
important practical and methodological advantages.
3.2 Extra Training Data from WordNet
WordNet contains a great deal of information about
words and word senses.The information contained
1There are also 15 lexicographer classes for verbs, 3 for ad-
jectives and 1 for adverbs.
2The label ?Tops? refers to about 40 very general synsets,
such as ?phenomenon? ?entity? ?object? etc.
in the dictionary?s glosses is very similar to what
is typically listed in normal dictionaries: synonyms,
definitions and example sentences. This suggests a
very simple way in which it can be put into use: it
can be compiled into training data for supersense la-
bels. This data can then be added to the data ex-
tracted from the training corpus.
For several thousand concepts WordNet?s glosses
are very informative. The synset ?chair? for example
looks as follows:
Multi-Component Word Sense Disambiguation  
Massimiliano Ciaramita Mark Johnson
Brown University
Department of Cognitive and Linguistic Sciences
Providence, RI 02912

massi@brown.edu,mark johnson@brown.edu 
Abstract
This paper describes the system MC-WSD pre-
sented for the English Lexical Sample task. The
system is based on a multicomponent architecture.
It consists of one classifier with two components.
One is trained on the data provided for the task. The
second is trained on this data and, additionally, on
an external training set extracted from the Wordnet
glosses. The goal of the additional component is to
lessen sparse data problems by exploiting the infor-
mation encoded in the ontology.
1 Introduction
One of the main difficulties in word sense classifi-
cation tasks stems from the fact that word senses,
such as Wordnet?s synsets (Fellbaum, 1998), de-
fine very specific classes1 . As a consequence train-
ing instances are often too few in number to cap-
ture extremely fine-grained semantic distinctions.
Word senses, however, are not just independent enti-
ties but are connected by several semantic relations;
e.g., the is-a, which specifies a relation of inclusion
among classes such as ?car is-a vehicle?. Based on
the is-a relation Wordnet defines large and complex
hierarchies for nouns and verbs.
These hierarchical structures encode potentially
useful world-knowledge that can be exploited for
word sense classification purposes, by providing
means for generalizing beyond the narrowest synset
level. To disambiguate an instance of a noun like
?bat? a system might be more successful if, in-
stead of limiting itself to applying what it knows
about the concepts ?bat-mammal? and ?bat-sport-
implement?, it could use additional knowledge
about other ?animals? and ?artifacts?.
Our system implements this intuition in two
steps. First, for each sense of an ambiguous word
we generate an additional set of training instances

We would like to thank Thomas Hofmann and our colleagues
in the Brown Laboratory for Linguistic Information Processing
(BLLIP).
151% of the noun synsets in Wordnet contain only 1 word.
from the Wordnet glosses. This data is not limited to
the specific synset that represents one of the senses
of the word, but concerns also other synsets that are
semantically similar, i.e., close in the hierarchy, to
that synset. Then, we integrate the task-specific and
the external training data with a multicomponent
classifier that simplifies the system for hierarchical
word sense disambiguation presented in (Ciaramita
et al, 2003). The classifier consists of two com-
ponents based on the averaged multiclass percep-
tron (Collins, 2002; Crammer and Singer, 2003).
The first component is trained on the task-specific
data while the second is trained on the former and
on the external training data. When predicting a la-
bel for an instance the classifier combines the pre-
dictions of the two components. Cross-validation
experiments on the training data show the advan-
tages of the multicomponent architecture.
In the following section we describe the features
used by our system. In Section 3 we explain how we
generated the additional training set. In Section 4
we describe the architecture of the classifier and in
Section 5 we discuss the specifics of the final system
and some experimental results.
2 Features
We used a set of features similar to that which
was extensively described and evaluated in (Yoong
and Hwee, 2002). The sentence with POS annota-
tion ?A-DT newspaper-NN and-CC now-RB a-DT
bank-NN have-AUX since-RB taken-VBN over-
RB? serves as an example to illustrate them. The
word to disambiguate is bank (or activate for (7)).
1. part of speech of neighboring words  ,
	
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple Languages
Jan Hajic?? Massimiliano Ciaramita? Richard Johansson? Daisuke Kawahara?
Maria Anto`nia Mart???? Llu??s Ma`rquez?? Adam Meyers?? Joakim Nivre?? Sebastian Pado???
Jan ?Ste?pa?nek? Pavel Stran?a?k? Mihai Surdeanu?? Nianwen Xue?? Yi Zhang??
?: Charles University in Prague, {hajic,stepanek,stranak}@ufal.mff.cuni.cz
?: Google Inc., massi@google.com
?: University of Trento, johansson@disi.unitn.it
?: National Institute of Information and Communications Technology, dk@nict.go.jp
??: University of Barcelona, amarti@ub.edu
??: Technical University of Catalonia, Barcelona, lluism@lsi.upc.edu
??: New York University, meyers@cs.nyu.edu
??: Uppsala University and Va?xjo? University, joakim.nivre@lingfil.uu.se
??: Stuttgart University, pado@ims.uni-stuttgart.de
??: Stanford University, mihais@stanford.edu
??: Brandeis University, xuen@brandeis.edu
??: Saarland University, yzhang@coli.uni-sb.de
Abstract
For the 11th straight year, the Conference
on Computational Natural Language Learn-
ing has been accompanied by a shared task
whose purpose is to promote natural language
processing applications and evaluate them in
a standard setting. In 2009, the shared task
was dedicated to the joint parsing of syntac-
tic and semantic dependencies in multiple lan-
guages. This shared task combines the shared
tasks of the previous five years under a unique
dependency-based formalism similar to the
2008 task. In this paper, we define the shared
task, describe how the data sets were created
and show their quantitative properties, report
the results and summarize the approaches of
the participating systems.
1 Introduction
Every year since 1999, the Conference on Com-
putational Natural Language Learning (CoNLL)
launches a competitive, open ?Shared Task?. A
common (?shared?) task is defined and datasets are
provided for its participants. In 2004 and 2005, the
shared tasks were dedicated to semantic role label-
ing (SRL) in a monolingual setting (English). In
2006 and 2007 the shared tasks were devoted to
the parsing of syntactic dependencies, using corpora
from up to 13 languages. In 2008, the shared task
(Surdeanu et al, 2008) used a unified dependency-
based formalism, which modeled both syntactic de-
pendencies and semantic roles for English. The
CoNLL-2009 Shared Task has built on the 2008 re-
sults by providing data for six more languages (Cata-
lan, Chinese, Czech, German, Japanese and Span-
ish) in addition to the original English1. It has thus
naturally extended the path taken by the five most
recent CoNLL shared tasks.
As in 2008, the CoNLL-2009 shared task com-
bined dependency parsing and the task of identify-
ing and labeling semantic arguments of verbs (and
other parts of speech whenever available). Partici-
pants had to choose from two tasks:
? Joint task (syntactic dependency parsing and
semantic role labeling), or
? SRL-only task (syntactic dependency parses
have been provided by the organizers, using
state-of-the art parsers for the individual lan-
guages).
1There are some format changes and deviations from the
2008 task data specification; see Sect. 2.3
1
In contrast to the previous year, the evaluation data
indicated which words were to be dealt with (for the
SRL task). In other words, (predicate) disambigua-
tion was still part of the task, whereas the identi-
fication of argument-bearing words was not. This
decision was made to compensate for the significant
differences between languages and between the an-
notation schemes used.
The ?closed? and ?open? challenges have been
kept from last year as well; participants could have
chosen one or both. In the closed challenge, systems
had to be trained strictly with information contained
in the given training corpus; in the open challenge,
systems could have been developed making use of
any kind of external tools and resources.
This paper is organized as follows. Section 2 de-
fines the task, including the format of the data, the
evaluation metrics, and the two challenges. A sub-
stantial portion of the paper (Section 3) is devoted
to the description of the conversion and develop-
ment of the data sets in the additional languages.
Section 4 shows the main results of the submitted
systems in the Joint and SRL-only tasks. Section 5
summarizes the approaches implemented by partic-
ipants. Section 6 concludes the paper. In all sec-
tions, we will mention some of the differences be-
tween last year?s and this year?s tasks while keeping
the text self-contained whenever possible; for details
and observations on the English data, please refer to
the overview paper of the CoNLL-2008 Shared Task
(Surdeanu et al, 2008) and to the references men-
tioned in the sections describing the other languages.
2 Task Definition
In this section we provide the definition of the shared
task; after introducing the two challenges and the
two tasks the participants were to choose, we con-
tinue with the format of the shared task data, fol-
lowed by a description of the evaluation metrics
used.
For three of the languages (Czech, English and
German), out-of-domain data (OOD) have also been
prepared for the final evaluation, following the same
guidelines and formats.
2.1 Closed and Open Challenges
Similarly to the CoNLL-2005 and CoNLL-2008
shared tasks, this shared task evaluation is separated
into two challenges:
Closed Challenge The aim of this challenge was to
compare performance of the participating systems in
a fair environment. Systems had to be built strictly
with information contained in the given training cor-
pus, and tuned with the development section. In
addition, the lexical frame files (such as the Prop-
Bank and NomBank for English, the valency dictio-
nary PDT-Vallex for Czech etc.) were provided and
may have been used. These restrictions mean that
outside parsers (not trained by the participants? sys-
tems) could not be used. However, we did provide
the output of a single, state-of-the-art dependency
parser for each language so that participants could
build a SRL-only system (using the provided parses
as inputs) within the closed challenge (as opposed to
the 2008 shared task).
Open Challenge Systems could have been devel-
oped making use of any kind of external tools and
resources. The only condition was that such tools or
resources must not have been developed with the an-
notations of the test set, both for the input and output
annotations of the data. In this challenge, we were
interested in learning methods which make use of
any tools or resources that might improve the per-
formance. The comparison of different systems in
this setting may not be fair, and thus ranking of sys-
tems is not necessarily important.
2.2 Joint and SRL-only tasks
In 2008, systems participating in the open challenge
could have used state-of-the-art parsers for the syn-
tactic dependency part of the task. This year, we
have provided the output of these parsers for all the
languages in an uniform way, thus allowing an or-
thogonal combination of the two tasks and the two
challenges. For the SRL-only task, participants in
the closed challenge simply had to use the provided
parses only.
Despite the provisions for the SRL-only task, we
are more interested in the approaches and results of
the Joint task. Therefore, primary system ranking is
provided for the Joint task while additional measures
2
are computed for various combinations of parsers
and SRL methods across the tasks and challenges.
2.3 Data Format
The data format used in this shared task has been
based on the CoNLL-2008 shared task, with some
differences. The data follows these general rules:
? The files contain sentences separated by a blank
line.
? A sentence consists of one or more tokens and
the information for each token is represented on
a separate line.
? A token consists of at least 14 fields. The fields
are separated by one or more whitespace char-
acters (spaces or tabs). Whitespace characters
are not allowed within fields.
The data is thus a large table with whitespace-
separated fields (columns). The fields provided in
the data are described in Table 1. They are identical
for all languages, but they may differ in contents;
for example, some fields might not be filled for all
the languages provided (such as the FEAT or PFEAT
fields).
For the SRL-only task, participants have been
provided will all the data but the PRED and
APREDs, which they were supposed to fill in with
their correct values. However, they did not have
to determine which tokens are predicates (or more
precisely, which are the argument-bearing tokens),
since they were marked by ?Y? in the FILLPRED
field.
For the Joint task, participants could not (in ad-
dition to the PRED and APREDs) see the gold-
standard nor the predicted syntactic dependencies
(HEAD, PHEAD) and their labels (DEPREL, PDE-
PREL). These syntactic dependencies were also to
be filled by participants? systems.
In both tasks, participants have been free to
use any other data (columns) provided, except the
LEMMA, POS and FEAT columns (to get more ?re-
alistic? results using only their automatically pre-
dicted variants PLEMMA, PPOS and PFEAT).
Besides the corpus proper, predicate dictionaries
have been provided to participants in order to be able
to properly match the predicates to the tokens in the
corpus; their contents could have been used e.g. as
features for the PRED/APREDs predictions (or even
for the syntactic dependencies, i.e., for filling in the
PHEAD and PDEPREL fields).
The system of filling-in the APREDs follows
the 2008 pattern; for each argument-bearing token
(predicate), a new APREDn column is created in the
order in which the predicate token is encountered
within the sentence (i.e., based on its ID seen as a
numerical value). Then, for each token in the sen-
tence, the value in the intersection of the APREDn
column and the token row is either left unfilled
(if the token is not an argument), or a predicate-
argument label(s) is(are) filled in.
The differences between the English-only 2008
task and this year?s multilingual task can be briefly
summarized as follows:
? only ?split?2 lemmas and forms have been pro-
vided in the English datasets (for the other lan-
guages, original tokenization from the respec-
tive treebanks has been used);
? rich morphological features have been added
wherever available;
? syntactic dependencies by state-of-the-art
parsers have been provided (for the SRL-only
task);
? multiple semantic labels for a single token have
been allowed (and properly evaluated) in the
APREDs columns;
? predicates have been pre-identified and marked
in both the training and test data;
? some of the fields (e.g. the APREDx) and val-
ues (ARG0? A0 etc.) have been renamed.
2.4 Evaluation Measures
It was required that participants submit results in all
seven languages in the chosen task and in any of (or
both) the challenges. Submission of out-of-domain
data files has been optional.
The main evaluation measure, according to which
systems are primarily compared, is the Joint task,
2Splitting of forms and lemmas in English has been intro-
duced in the 2008 shared task to match the tokenization con-
vention for the arguments in NomBank.
3
Field # Name Description
1 ID Token counter, starting at 1 for each new sentence
2 FORM Form or punctuation symbol (the token; ?split? for English)
3 LEMMA Gold-standard lemma of FORM
4 PLEMMA Automatically predicted lemma of FORM
5 POS Gold-standard POS (major POS only)
6 PPOS Automatically predicted major POS by a language-specific tagger
7 FEAT Gold-standard morphological features (if applicable)
8 PFEAT Automatically predicted morphological features (if applicable)
9 HEAD Gold-standard syntactic head of the current token (ID or 0 if root)
10 PHEAD Automatically predicted syntactic head
11 DEPREL Gold-standard syntactic dependency relation (to HEAD)
12 PDEPREL Automatically predicted dependency relation to PHEAD
13 FILLPRED Contains ?Y? for argument-bearing tokens
14 PRED (sense) identifier of a semantic ?predicate? coming from a current token
15... APREDn Columns with argument labels for each semantic predicate (in the ID order)
Table 1: Description of the fields (columns) in the data provided. The values of columns 9, 11 and 14 and above are
not provided in the evaluation data; for the Joint task, columns 9?12 are also empty in the evaluation data.
closed challenge, Macro F1 score. However, scores
can also be computed for a number of other condi-
tions:
? Task: Joint or SRL-only
? Challenge: open or closed
? Domain: in-domain data (IDD, separated from
training corpus) or out-of-domain data (OOD)
Joint task participants are also evaluated separately
on the syntactic dependency task (labeled attach-
ment score, LAS). Finally, systems competing in
both tasks are compared on semantic role labeling
alone, to assess the impact of the the joint pars-
ing/SRL task compared to an SRL-only task on pre-
parsed data.
Finally, as an explanatory measure, precision and
recall of the semantic labeling task have been com-
puted and tabulated.
We have decided to omit several evaluation fig-
ures that were reported in previous years, such as the
percentage of completely correct sentences (?Exact
Match?), unlabeled scores, etc. With seven lan-
guages, two tasks (plus two challenges, and the
IDD/OOD distinction), there are enough results to
get lost even as it is.
2.4.1 Syntactic Dependency Measures
The LAS score is defined similarly as in the pre-
vious shared tasks, as the percentage of tokens for
which a system has predicted the correct HEAD and
DEPREL columns. The unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
HEAD regardless if the DEPREL is correct, has not
been officially computed this year. No precision and
recall measures are applicable, since all systems are
supposed to output a single dependency with a single
label (see also below the footnote to the description
of the combined score).
2.4.2 Semantic Labeling Measures
The semantic propositions are evaluated by con-
verting them to semantic dependencies, i.e., we cre-
ate n semantic dependencies from every predicate
to its n arguments. These dependencies are labeled
with the labels of the corresponding arguments. Ad-
ditionally, we create a semantic dependency from
each predicate to a virtual ROOT node. The latter
dependencies are labeled with the predicate senses.
This approach guarantees that the semantic depen-
dency structure conceptually forms a single-rooted,
connected (but not necessarily acyclic) graph. More
importantly, this scoring strategy implies that if a
system assigns the incorrect predicate sense, it still
receives some points for the arguments correctly as-
signed. For example, for the correct proposition:
verb.01: A0, A1, AM-TMP
the system that generates the following output for
the same argument tokens:
4
verb.02: A0, A1, AM-LOC
receives a labeled precision score of 2/4 because two
out of four semantic dependencies are incorrect: the
dependency to ROOT is labeled 02 instead of 01
and the dependency to the AM-TMP is incorrectly la-
beled AM-LOC. Using this strategy we compute pre-
cision, recall, and F1 scores for semantic dependen-
cies (labeled only).
For some languages (Czech, Japanese) there may
be more than one label in a given argument position;
for example, this happens in Czech in special cases
of reciprocity when the same token serves as two or
more arguments to the same predicate. The scorer
takes this into account and considers such cases to
be (as if) multiple predicate-argument relations for
the computation of the evaluation measures.
For example, for the correct proposition:
v1f1: ACT|EFF, ADDR
the system that generates the following output for
the same argument tokens:
v1f1: ACT, ADDR|PAT
receives a labeled precision score of 3/4 because
the PAT is incorrect and labeled recall 3/4 be-
cause the EFF is missing (should the ACT|EFF and
ADDR|PAT be taken as atomic values, the scores
would then be zero).
2.4.3 Combined Syntactic and Semantic Score
We combine the syntactic and semantic measures
into one global measure using macro averaging. We
compute macro precision and recall scores by aver-
aging the labeled precision and recall for semantic
dependencies with the LAS for syntactic dependen-
cies:3
LMP = Wsem ? LPsem + (1?Wsem) ? LAS (1)
LMR = Wsem ? LRsem + (1 ?Wsem) ? LAS (2)
where LMP is the labeled macro precision and
LPsem is the labeled precision for semantic depen-
dencies. Similarly, LMR is the labeled macro re-
call and LRsem is the labeled recall for semantic
dependencies. Wsem is the weight assigned to the
3We can do this because the LAS for syntactic dependen-
cies is a special case of precision and recall, where the predicted
number of dependencies is equal to the number of gold depen-
dencies.
semantic task.4 The macro labeled F1 score, which
was used for the ranking of the participating sys-
tems, is computed as the harmonic mean of LMP
and LMR.
3 Data
The unification of the data formats for the various
languages appeared to be a challenge in itself. We
will briefly describe the processes of the conversion
of the existing treebanks in the seven languages of
the CoNLL-2009 shared task. In many instances,
the original treebanks had to be not only converted
format-wise, but also merged with other resources in
order to generate useful training and testing data that
fit the task description.
3.1 The Input Corpora
The data used as the input for the transformations
aimed at arriving at the data contents and format de-
scribed in Sect. 2.3 are described in (Taule? et al,
2008), (Xue and Palmer, 2009), (Hajic? et al, 2006),
(Surdeanu et al, 2008), (Burchardt et al, 2006) and
(Kawahara et al, 2002).
In the subsequent sections, the procedures for the
data conversion for the individual languages are de-
scribed. The data has been collected by the main
organization site and checked for format errors, and
repackaged for distribution.
There were three packages of the data distributed
to the participants: Trial, Training plus Develop-
ment, and Evaluation. The Trial data were rather
small, just to give the feeling of the format and
languages involved. A visual representation of the
Trial data was also created to make understanding
of the data easier. Any data in the same format
can be transformed and displayed in the Tree Editor
TrEd5 (Pajas and ?Ste?pa?nek, 2008) with the CoNLL
2009 Shared Task extension that can be installed
from within the editor. A sample visualization of an
English sentence after its conversion to the shared
task format (Sect. 2.3) is in Fig. 1.
Due to licensing requirements, every package of
the data had to be split into two portions. One
portion (Catalan, German, Japanese, and Spanish
data) was published on the task?s webpage for down-
4We assign equal weight to the two tasks, i.e., Wsem = 0.5.
5http://ufal.mff.cuni.cz/?pajas/tred
5
$QG
'(3 &&
VRPHWLPHV
703 5%
D
102' '7
UHSXWDEOH
102' --
FKDULW\
6%- 11
ZLWK
102' ,1
D
102' '7
KRXVHKROG
102' 11
QDPH QDPH
302' 11
JHWV JHW
5227 9%=
XVHG XVH
9& 9%1
DQG
&225' &&
GRHV
&21- 9%=
Q
W
$'9 5%
HYHQ
$'9 5%
NQRZ NQRZ
9& 9%
LW
2%- 353

3  
$0703$0703$0703
 

$$$$

 
 

$
 

 

$



$01(*

$0$'9
 


$

Figure 1: Visualisation of the English sentence ?And sometimes a reputable charity with a houshold name gets used
and doesn?t even know it.? (Penn Treebank, wsj 0559) showing jointly the labeled syntactic and semantic depen-
dencies. The basic tree shape comes from the syntactic dependencies; syntactic labels and POS tags are on the 2nd
line at each node. Semantic dependencies which do not follow the syntactic ones use dotted lines. Predicate senses
in parentheses (use:01, ...) follow the word label. SRLs (A0, AM-TMP, ...) are on the last line. Please note that
multiple semantic dependencies (e.g., there are four for charity: A0? know, A1? gets, A1? used, A1? name)
and self-dependencies (name) appear in this sentence.
load, the other portion (Czech, English, and Chinese
data) was invoiced and distributed by the Linguistic
Data Consortium under a special agreement free of
charge.
Distribution of the Evaluation package was a bit
more complicated, because there were two types of
the packages - one for the Joint task and one for the
SRL-only task. Every participant had to subscribe
to one of the two tasks; subsequently, they obtained
the appropriate data (again, from the webpage and
LDC).
Prior to release, each data file was checked to
eliminate errors. The following test were carried
out:
? For every sentence, number of PREDs rows
matches the number of APREDs columns.
? The first line of each file is never empty, while
the last line always is.
? The first character on a non-empty line is al-
ways a digit, the last one is never a whitespace.
? The number of empty lines (i.e. the number
of sentences) equals the number of lines begin-
ning with ?1?.
? The data contain no spaces nor double tabs.
Some statistics on the data can be seen in Ta-
bles 2, 3 and 4. Whereas the training sizes of the
data have not been that different as they were e.g.
for the 2007 shared task on multilingual dependency
parsing (Nivre et al, 2007)6, substantial differences
existed in the distribution of the predicates and ar-
guments, the input features, the out-of-vocabulary
rates, and other statistical characteristics of the data.
Data sizes have been relatively uniform in all the
datasets, with Japanese having the smallest dataset
6http://nextens.uvt.nl/depparse-wiki/
DataOverview
6
containing data for SRL annotation training. To
compensate at least for the dependency parsing part,
an additional, large Japanese corpus with syntactic
dependency annotation has been provided.
The average sentence length, the vocabulary sizes
for FORM and LEMMA fields and the OOV rates
characterize quite naturally the properties of the re-
spective languages (in the domain of the training and
evaluation data). It is no surprise that the FORM
OOV rate is the highest for Czech, a highly inflec-
tional language, and that the LEMMA OOV rate is
the highest for German (as a consequence of keeping
compounds as a single lemma). The other statistics
also reflect (to a large extent) the annotation speci-
fication and conventions used for the original tree-
banks and/or the result of the conversion process to
the unified CoNLL-2009 Shared Task format.
Starting with the POS and FEAT fields, it can be
seen that Catalan, Czech and Spanish use only the
12 major part-of-speech categories as values of the
POS field (with richly populated FEAT field); En-
glish and Chinese are the opposite extreme, disre-
garding the use of the FEAT field completely and
coding everything as a POS value. While for Chi-
nese this is quite understandable, English follows the
PTB tradition in this respect. German and Japanese
use relatively rich set of values in both the POS and
FEAT fields.
For the dependency relations (DEPREL), all
the languages use a similarly-sized set except for
Japanese, which only encodes the distinction be-
tween a root and a dependent node (and some in-
frequent special ones).
Evaluation data are over 10% of the size of the
training data for Catalan, Chinese, Czech, Japanese
and Spanish and roughly 5% for English and Ger-
man.
Table 3 shows the distribution of the five most fre-
quent dependency relations (determined as part of
the subtask of syntactic parsing). With the exception
of Japanese, which essentially does not label depen-
dency relations at this level, all the other languages
show little difference in this distribution. For exam-
ple, the unconditioned probability of ?subjects? is
almost the same for all the six other languages (be-
tween 6 and 8 percent). The probability mass cov-
ered by the first five most frequent DEPRELs is also
almost the same (again, except for Japanese), sug-
gesting that the labeling task might have similar dif-
ficulty7. The most skewed one is for Czech (after
Japanese).
Table 4 shows similar statistics for the argument
labels (PRED/APREDs); it also adds the average
number of arguments per ?predicate? token, since
this is part of the SRL task8. It is apparent from the
comparison of the ?Total? rows in this table and Ta-
ble 3 that the first five argument labels cover more
that their syntactic counterparts. For example, the
arguments A0-A4 account for all but 3% of all ar-
guments labels, whereas Spanish and Catalan have
much more rich set of argument labels, with a high
entropy of the most-frequent-label distribution.
3.2 Catalan and Spanish
The Catalan and Spanish datasets (Taule? et al, 2008)
were generated from the AnCora corpora9 through
an automatic conversion process from a constituent-
based formalism to dependencies (Civit et al, 2006).
AnCora corpora contain about half million words
for Catalan and Spanish annotated with syntactic
and semantic information. Text sources for the Cata-
lan corpus are EFE news agency (?75Kw), ACN
Catalan news agency (?225Kw), and ?El Perio?dico?
newspaper (?200Kw). The Spanish corpus comes
from the Lexesp Spanish balanced corpus (?75Kw),
the EFE Spanish news agency (?225Kw), and the
Spanish version of ?El Perio?dico? (?200Kw). The
subset from ?El Perio?dico? corresponds to the same
news in Catalan and Spanish, spanning from January
to December 2000.
Linguistic annotation is the same in both lan-
guages and includes: PoS tags with morphologi-
cal features (gender, number, person, etc.), lemma-
tization, syntactic dependencies (syntactic func-
tions), semantic dependencies (arguments and the-
matic roles), named entities and predicate semantic
classes (Lexical Semantic Structure, LSS). Tag sets
are shared by the two languages.
If we take into account the complete PoS tags,
7Yes, this is overgeneralization since this distribution does
not condition on the features, dependencies etc. But as a rough
measure, it often correlates well with the results.
8A number below 1 means there are some argument-bearing
words (often nouns) which have no arguments in the particular
sentence in which they appear.
9http://clic.ub.edu/ancora
7
Characteristic Catalan Chinese Czech English German Japanese Spanish
Training data size (sentences) 13200 22277 38727 39279 36020 4393a 14329
Training data size (tokens) 390302 609060 652544 958167 648677 112555a 427442
Avg. sentence length (tokens) 29.6 27.3 16.8 24.4 18.0 25.6 29.8
Tokens with argumentsb (%) 9.6 16.9 63.5 18.7 2.7 22.8 10.3
DEPREL types 50 41 49 69 46 5 49
POS types 12 41 12 48 56 40 12
FEAT types 237 1 1811 1 267 302 264
FORM vocabulary size 33890 40878 86332 39782 72084 36043 40964
LEMMA vocabulary size 24143 40878 37580 28376 51993 30402 26926
Evaluation data size (sent.) 1862 2556 4213 2399 2000 500 1725
Evaluation data size (tokens) 53355 73153 70348 57676 31622 13615 50630
Evaluation FORM OOVc 5.40 3.92 7.98/8.62d 1.58/3.76d 7.93/7.57d 6.07 5.63
Evaluation LEMMA OOVc 4.14 3.92 3.03/4.29d 1.08/2.30d 5.83/7.36d 5.21 3.69
Table 2: Elementary data statistics for the CoNLL-2009 Shared Task languages. The data themselves, the original
treebanks they were derived from and the conversion process are described in more detail in sections 3.2-3.7. All
evaluation data statistics are derived from the in-domain evaluation data.
aThere were additional 33257 sentences (839947 tokens) available for syntactic dependency parsing of Japanese; the type and
vocabulary statistics are computed using this larger dataset.
bPercentage of tokens with FILLPRED=?Y?.
cPercentage of FORM/LEMMA tokens not found in the respective vocabularies derived solely from the training data.
dOOV percentage for in-domain/out-of-domain data.
DEPREL Catalan Chinese Czech English German Japanese Spanish
sn 0.16 COMP 0.21 Atr 0.26 NMOD 0.27 NK 0.31 D 0.93 sn 0.16
spec 0.15 NMOD 0.14 AuxP 0.10 P 0.11 PUNC 0.14 ROOT 0.04 spec 0.15
Labels f 0.11 ADV 0.10 Adv 0.10 PMOD 0.10 MO 0.12 P 0.03 f 0.12
sp 0.09 UNK 0.09 Obj 0.07 SBJ 0.07 SB 0.07 A 0.00 sp 0.08
suj 0.07 SBJ 0.08 Sb 0.06 OBJ 0.06 ROOT 0.06 I 0.00 suj 0.08
Total 0.58 0.62 0.59 0.61 0.70 1.00 0.59
Table 3: Unigram probability for the five most frequent DEPREL labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five dependency labels shown.
APRED Catalan Chinese Czech English German Japanese Spanish
arg1-pat 0.22 A1 0.30 RSTR 0.30 A1 0.37 A0 0.40 GA 0.33 arg1-pat 0.20
arg0-agt 0.18 A0 0.27 PAT 0.18 A0 0.25 A1 0.39 WO 0.15 arg0-agt 0.19
Labels arg1-tem 0.15 ADV 0.20 ACT 0.17 A2 0.12 A2 0.12 NO 0.15 arg1-tem 0.15
argM-tmp 0.08 TMP 0.07 APP 0.06 AM-TMP 0.06 A3 0.06 NI 0.09 arg2-atr 0.08
arg2-atr 0.08 DIS 0.04 LOC 0.04 AM-MNR 0.03 A4 0.01 DE 0.06 argM-tmp 0.08
Total 0.71 0.91 0.75 0.83 0.97 0.78 0.70
Avg. 2.25 2.26 0.88 2.20 1.97 1.71 2.26
Table 4: Unigram probability for the five most frequent APRED labels in the training data of the CoNLL-2009
Shared Task is shown. Total is the probability mass covered by the five argument labels shown. The ?Avg.? line
shows the average number of arguments per predicate or other argument-bearing token (i.e. for those marked by
FILLPRED=?Y?).
8
AnCora has 280 different labels. Considering only
the main syntactic categories, the tag set is reduced
to 47 tags. The syntactic tag set consists of 50 dif-
ferent syntactic functions. Regarding semantic ar-
guments, we distinguish Arg0, Arg1, Arg2, Arg3,
Arg4, ArgM, and ArgL. The first five tags are num-
bered from less to more obliqueness with respect
to the verb, ArgM corresponds to adjuncts. The
list of thematic roles consists of 20 different labels:
AGT (Agent), AGI (Induced Agent), CAU (Cause),
EXP (Experiencer), SCR (Source), PAT (Patient),
TEM (Theme), ATR (Attribute), BEN (Beneficiary),
EXT (Extension), INS (Instrument), LOC (Loca-
tive), TMP (Time), MNR (Manner), ORI (Origin),
DES (Goal), FIN (Purpose), EIN (Initial State), EFI
(Final State), and ADV (Adverbial). Each argument
position can map onto specific thematic roles. By
way of example, Arg1 can be PAT, TEM or EXT. For
Named Entities, we distinguish six types: Organiza-
tion, Person, Location, Date, Number, and Others.
An incremental process guided the annotation of
AnCora, since semantics depends on morphosyntax,
and syntax relies on morphology. This procedure
made it possible to check, correct, and complete
the previous annotations, thus guaranteeing the final
quality of the corpora and minimizing the error rate.
The annotation process was carried out sequentially
from lower to upper layers of linguistic description.
All resulting layers are independent of each other,
thus making easier the data management. The ini-
tial annotation was performed manually for syntax,
semiautomatically in the case of arguments and the-
matic roles, and fully automatically for PoS (Mart??
et al, 2007; Ma`rquez et al, 2007).
The Catalan and Spanish AnCora corpora were
straightforwardly translated into the CoNLL-2009
shared task formatting (information about named
entities was skipped in this process). The resulting
Catalan corpus (including training, development and
test partitions) contains 16,786 sentences with an av-
erage length of 29.59 lexical tokens per sentence.
Long sentences abound in this corpus. For instance,
10.73% of the sentences are longer than 50 tokens,
and 4.42% are longer than 60. The corpus con-
tains 47,537 annotated predicates (2.83 predicates
per sentence, on average) with 107,171 arguments
(2.25 arguments per predicate, on average). From
the latter, 73.89% correspond to core arguments and
26.11% to adjuncts. Numbers for the Spanish cor-
pus are comparable in all aspects: 17,709 sentences
with 29.84 lexical tokens on average (11.58% of the
sentences longer than 50 tokens, 4.07% longer than
60); 54,075 predicates (3.05 per sentence, on aver-
age) and 122,478 arguments (2.26 per predicate, on
average); 73.34% core arguments and 26.66% ad-
juncts.
The following are important features of the Cata-
lan and Spanish corpora in the CoNLL-2009 shared
task setting: (1) all dependency trees are projective;
(2) no word can be the argument of more than one
predicate in a sentence; (3) semantic dependencies
completely match syntactic dependency structures
(i.e., no new edges are introduced by the semantic
structure); (4) only verbal predicates are annotated
(with exceptional cases referring to words that can
be adjectives and past participles); (5) the corpus is
segmented so multi-words, named entities, temporal
expressions, compounds, etc. are grouped together;
and (6) segmentation also accounts for elliptical pro-
nouns (there are marked as empty lexical tokens ?_?
with a pronoun POS tag).
Finally, the predicted columns (PLEMMA,
PPOS, and PFEAT) have been generated with the
FreeLing Open source suite of Language Analyz-
ers10. Accuracy in PLEMMA and PPOS columns
is above 95% for the two languages. PHEAD
and PDEPREL columns have been generated using
MaltParser11. Parsing accuracy (LAS) is above 86%
for the the two languages.
3.3 Chinese
The Chinese Corpus for the 2009 CoNLL Shared
Task was generated by merging the Chinese Tree-
bank (Xue et al, 2005) and the Chinese Proposition
Bank (Xue and Palmer, 2009) and then converting
the constituent structure to a dependency formalism
as specified in the CoNLL Shared Task. The Chi-
nese data used in the shared task is based on Chinese
Treebank 6.0 and the Chinese Proposition Bank 2.0,
both of which are publicly available via the Linguis-
tic Data Consortium.
The Chinese Treebank Project originated at Penn
and was later moved to University of Colorado at
10http://www.lsi.upc.es/?nlp/freeling
11http://w3.msi.vxu.se/?jha/maltparser
9
Boulder. Now it is the process of being to moved
to Brandeis University. The data sources of the Chi-
nese Treebank range from Xinhua newswire (main-
land China), Hong Kong news, and Sinorama Maga-
zine (Taiwan). More recently under DARPA GALE
funding it has been expanded to include broadcast
news, broadcast conversation, news groups and web
log data. It currently has over one million words
and is fully segmented, POS-tagged and annotated
with phrase structure. The version of the Chinese
Treebank used in this shared task, CTB 6.0, includes
newswire, magazine articles, and transcribed broad-
cast news 12. The training set has 609,060 tokens,
the development set has 49,620 tokens, and the test
set has 73,153 tokens.
The Chinese Proposition Bank adds a layer of se-
mantic annotation to the syntactic parses in the Chi-
nese Treebank. This layer of semantic annotation
mainly deals with the predicate-argument structure
of Chinese verbs and their nominalizations. Each
major sense (called frameset) of a predicate takes a
number of core arguments annotated with numeri-
cal labels Arg0 through Arg5 which are defined in
a predicate-specific manner. The Chinese Proposi-
tion Bank also annotates adjunctive arguments such
as locative, temporal and manner modifiers of the
predicate. The version of the Chinese Propbank used
in this CoNLL Shared Task is CPB 2.0, but nominal
predicates are excluded because the annotation is in-
complete.
Since the Chinese Treebank is annotated with
constituent structures, the conversion and merging
procedure converts the constituent structures to de-
pendencies by identifying the head for each con-
stituent in a parse tree and making its sisters its de-
pendents. The Chinese Propbank pointers are then
shifted from the entire constituent to the head of that
constituent. The conversion procedure identifies the
head by first exploiting the structural information
in the syntactic parse and detecting six broad cate-
gories of syntactic relations that hold between the
head and its dependents (predication, modification,
complementation, coordination, auxiliary, and flat)
and then designating the head based on these rela-
tions. In particular, the first conjunct of a coordina-
12A small number of files were taken out of the CoNLL
shared task data due to conversion problems and time con-
straints to fix them.
tion structure is designated as the head and the heads
of the other conjuncts are the conjunctions preced-
ing them. The conjunctions all ?modify? the first
conjunct.
3.4 Czech
For the training, development and evaluation data,
Prague Dependency Treebank 2.0 was used (Hajic?
et al, 2006). For the out-of-domain evaluation data,
part of the Czech side of the Prague Czech-English
Dependency Treebank (version 2, under construc-
tion) was used13, see also ( ?Cmejrek et al, 2004). For
the OOD data, no manual annotation of LEMMA,
POS, and FEAT existed, so the predicted values
were used. The same conversion procedure has been
applied to both sources.
The FORM column was created from the form
element of the morphological layer, not from the
?token? from the word-form layer. Therefore, most
typos, errors in word segmentation and tokenization
are corrected and numerals are normalized.
The LEMMA column was created from the
lemma element of the morphological layer. Only
the initial string of the element was used, so there is
no distinction between homonyms. However, some
components of the detailed lemma explanation were
incorporated into the FEAT column (see below).
The POS column was created form the morpho-
logical tag element, its first character more pre-
cisely.
The FEAT column was created from the remain-
ing characters of the tag element. In addition, the
special feature ?Sem? corresponds to a semantic fea-
ture of the lemma.
For the HEAD and DEPREL columns, the PDT
analytical layer was used. The DEPREL was taken
from the analytic function (the afun node at-
tribtue). There are 27 possible values for afun el-
ement: Pred, Pnom, AuxV, Sb, Obj, Atr, Adv,
Atv, AtvV, Coord, Apos, ExD, and a number
of auxiliary and ?double-function? labels. The first
nine of these are the ?most interesting? from the
point of view of the shared task, since they relate to
semantics more closely than the rest (at least from
the linguistic point of view). The HEAD is a pointer
to its parent, which means the PDT?s ord attribute
13http://ufal.mff.cuni.cz/pedt
10
(within-sentence ID / word position number) of the
parent. If a node is a member of a coordination
or apposition (is_member element), its DEPREL
obtains the _M suffix. The parenthesis annotation
(is_parenthesis_root element) was ignored.
The PRED and APREDs columns were created
from the tectogrammatical layer of PDT 2.0 and the
valency lexicon PDT-Vallex according to the follow-
ing rules:
? Every line corresponding to an analytical node
referenced by a lexical reference (a/lex.rf)
from the tectogrammatical layer has a PRED
value filled. If the referring non-generated
tectogrammatical node (is_generated not
equal to 1) has a valency frame assigned
(val_frame.rf), the value of PRED is the
identifier of the frame. Otherwise, it is set to
the same value as the LEMMA column.
? For every tectogrammatical node, a corre-
sponding analytical node is searched for:
1. If the tectogrammatical node is not
generated and has a lexical reference
(a/lex.rf), the referenced node is
taken.
2. Otherwise, if the tectogrammatical node
has a coreference (coref_text.rf or
coref_gram.rf) or complement refer-
ence (compl.rf) to a node that has an
analytical node assigned (by 1. or 2.), the
assigned node is taken.
APRED columns are filled with respect to the
following correspondence: for a tectogrammatical
node P and its effective child C with functor F, the
column for P?s corresponding analytical node at the
row for C?s corresponding analytical node is filled
with F. Some nodes can thus have several functors
in one APRED column, separated by a vertical bar
(see Sect. 2.4.2).
PLEMMA, PPOS and PFEAT were gener-
ated by the (cross-trained) morphological tagger
MORCE (Spoustova? et al, 2009), which gives full
combined accuracy (PLEMMA+PPOS+PFEAT)
slightly under 96%.
PHEAD and PDEPREL were generated by
the (cross-trained) MST parser for Czech (Chu?
Liu/Edmonds algorithm, (McDonald et al, 2005)),
which has typical dependency accuracy around
85%.
The valency lexicon, converted from (Hajic? et al,
2003), has four columns:
1. lemma (can occur several times in the lexicon,
with different frames)
2. frame identifier (as found in the PRED column)
3. list of space-separated actants and obligatory
members of the frame
4. example(s)
The source of the out-of-domain data uses an
extended valency lexicon (because of out-of-
vocabulary entries). For simplicity, the extended
lexicon was not provided; instead, such words were
not marked as predicates in the OOD data (their
FILLPRED was set to ?_?) and thus not evaluated.
3.5 English
The English corpus is almost identical to the cor-
pus used in the closed challenge in the CoNLL-2008
shared task evaluation (Surdeanu et al, 2008). This
corpus was generated through a process that merges
several input corpora and converts them from the
constituent-based formalism to dependencies. The
following corpora were used as input to the merging
procedure:
? Penn Treebank 3 ? The Penn Treebank 3 cor-
pus (Marcus et al, 1994) consists of hand-
coded parses of the Wall Street Journal (test,
development and training) and a small subset
of the Brown corpus (W. N. Francis and H.
Kucera, 1964) (test only).
? BBN Pronoun Coreference and Entity Type
Corpus ? BBN?s NE annotation of the Wall
Street Journal corpus (Weischedel and Brun-
stein, 2005) takes the form of SGML inline
markup of text, tokenized to be completely
compatible with the Penn Treebank annotation.
For the CoNLL-2008 shared task evaluation,
this corpus was extended by the task organizers
to cover the subset of the Brown corpus used as
a secondary testing dataset. From this corpus
we only used NE boundaries to derive NAME
11
dependencies between NE tokens, e.g., we cre-
ate a NAME dependency from Mary to Smith
given the NE mention Mary Smith.
? Proposition Bank I (PropBank) ? The Prop-
Bank annotation (Palmer et al, 2005) classifies
the arguments of all the main verbs in the Penn
Treebank corpus, other than be. Arguments are
numbered (Arg0, Arg1, . . .) based on lexical
entries or frame files. Different sets of argu-
ments are assumed for different rolesets. De-
pendent constituents that fall into categories in-
dependent of the lexical entries are classified as
various types of adjuncts (ArgM-TMP, -ADV,
etc.).
? NomBank ? NomBank annotation (Meyers et
al., 2004) uses essentially the same framework
as PropBank to annotate arguments of nouns.
Differences between PropBank and NomBank
stem from differences between noun and verb
argument structure; differences in treatment of
nouns and verbs in the Penn Treebank; and dif-
ferences in the sophistication of previous re-
search about noun and verb argument structure.
Only the subset of nouns that take arguments
are annotated in NomBank and only a subset of
the non-argument siblings of nouns are marked
as ArgM.
The complete merging process and the conversion
from the constituent representation to dependencies
is detailed in (Surdeanu et al, 2008).
The main difference between the 2008 and 2009
version of the corpora is the generation of word lem-
mas. In the 2008 version the only lemmas pro-
vided were predicted using the built-in lemmatizer
in WordNet (Fellbaum, 1998) based on the most fre-
quent sense for the form and the predicted part-of-
speech tag. These lemmas are listed in the 2009
corpus under the PLEMMA column. The LEMMA
column in the 2009 version of the corpus contains
lemmas generated using the same algorithm but us-
ing the correct Treebank part-of-speech tags. Addi-
tionally, the PHEAD and PDEPREL columns were
generated using MaltParser14, similarly to the open
challenge corpus in the CoNLL 2008 shared task.
14http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
3.6 German
The German in-domain dataset is based on the an-
notated verb instances of the SALSA corpus (Bur-
chardt et al, 2006), a total of around 40k sen-
tences15. SALSA provides manual semantic role
annotation on top of the syntactically annotated
TIGER newspaper corpus, one of the standard Ger-
man treebanks. The original SALSA corpus uses se-
mantic roles in the FrameNet paradigm. We con-
structed mappings between FrameNet frame ele-
ments and PropBank argument positions at the level
of frame-predicate pairs semi-automatically. For the
frame elements of each frame-predicate pair, we first
identified the semantically defined PropBank Arg-
0 and Arg-1 positions. To do so, we annotated a
small number of very abstract frame elements with
these labels (Agent, Actor, Communicator as Arg-
0, and Theme, Effect, Message as Arg-1) and per-
colated these labels through the FrameNet hierar-
chy, adding further manual labels where necessary.
Then, we used frequency and grammatical realiza-
tion information to map the remaining roles onto
higher-numbered Arg roles. We considerably sim-
plified the annotations provided by SALSA, which
use a rather complex annotation scheme. In partic-
ular, we removed annotation for multi-word expres-
sions (which may be non-contiguous), annotations
involving multiple frames for the same predicate
(metaphors, underspecification), and inter-sentence
roles.
The out-of-domain dataset was taken from a study
on the multi-lingual projection of FrameNet annota-
tion (Pado and Lapata, 2005). It is sampled from
the EUROPARL corpus and was chosen to maxi-
mize the lexical coverage, i.e., it contains of a large
number of infrequent predicates. Both syntactic and
semantic structure were annotated manually, in the
TIGER and SALSA format, respectively. Since it
uses a simplified annotation schemes, we did not
have to discard any annotation.
For both datasets, we converted the syntactic
TIGER (Brants et al, 2002) representations into de-
pendencies with a similar set of head-finding rules
used for the preparation of the CoNLL-X shared task
German dataset. Minor modifications (for the con-
15Note, however, that typically not all predicates in each sen-
tence are annotated (cf. Table 2).
12
version of person names and coordinations) were
made to achieve better consistency with datasets
of other languages. Since the TIGER annotation
allows non-contiguous constituents, the resulting
dependencies can be non-projective. Secondary
edges were discarded in the conversion. As for the
automatically constructed features, we used Tree-
Tagger (Schmid, 1994) to produce the PLEMMA
and PPOS columns, and the Morphisto morphol-
ogy (Zielinski and Simon, 2008) for PFEAT.
3.7 Japanese
For Japanese, we used the Kyoto University Text
Corpus (Kawahara et al, 2002), which consists of
approximately 40k sentences taken from Mainichi
Newspapers. Out of them, approximately 5k sen-
tences are annotated with syntactic and semantic de-
pendencies, and are used the training, development
and test data of this year?s shared task. The remain-
ing sentences, which are annotated with only syntac-
tic dependencies, are provided for the training cor-
pus of syntactic dependency parsers.
This corpus adopts a dependency structure repre-
sentation, and thus the conversion to the CoNLL-
2009 format was relatively straightforward. How-
ever, since the original dependencies are annotated
on the basis of phrases (Japanese bunsetsu), we
needed to automatically convert the original annota-
tions to word-based ones using several criteria. We
used the following basic criteria: the words except
the last word in a phrase depend on the next (right)
word, and the last word in a phrase basically depends
on the head word of the governing phrase.
Semantic dependencies are annotated for both
verbal predicates and nominal predicates. The se-
mantic roles (APRED columns) consist of 41 sur-
face cases, many of which are case-marking post-
positions such as ga (nominative), wo (accusative)
and ni (dative). Semantic frame discrimination is not
annotated, and so the PRED column is the same as
the LEMMA column. The original corpus contains
coreference annotations and inter-sentential seman-
tic dependencies, such as inter-sentential zero pro-
nouns and bridging references, but we did not use
these annotations, which are not the target of this
year?s shared task.
To produce the PLEMMA, PPOS and PFEAT
columns, we used the morphological analyzer JU-
MAN 16 and the dependency and case structure an-
alyzer KNP 17. To produce the PHEAD and PDE-
PREL columns, we used the MSTParser 18.
4 Submissions and Results
Participants uploaded the results through the shared
task website, and the official evaluation was per-
formed centrally. Feedback was provided if any for-
mal problems were encountered (for a list of checks,
see the previous section). One submission had to
be rejected because only English results were pro-
vided. After the evaluation period had passed, the
results were anonymized and published on the web.
A total of 20 systems participated in the closed
challenge; 13 of them in the Joint task and seven in
the SRL-only task. Two systems participated in the
open challenge (Joint task). Moreover, 17 systems
provided output in the out-of-domain part of the task
(11 in the OOD Joint task and six in the OOD SRL-
only task).
The main results for the core task - the Joint task
(dependency syntax and semantic relations) in the
context of the closed challenge - are summarized and
ranked in Table 5.
The largest number of systems can be compared
in the SRL results table (Table 6), where all the sys-
tems have been evaluated solely on the SRL perfor-
mance regardless whether they participated in the
Joint or SRL-only task. However, since the results
might have been influenced by the supplied parser,
separate ranking is provided for both types of the
systems.
Additional breakdown of the results (open chal-
lenge, precision and recall tables for the semantic
labeling task, etc.) are available from the CoNLL-
2009 Shared Task website19.
5 Approaches
Table 7 summarizes the properties of the systems
that participated in the closed the open challenges.
16http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
17http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
18http://sourceforge.net/projects/
mstparser
19http://ufal.mff.cuni.cz/conll2009-st
13
Rank System Average Catalan Chinese Czech English German Japanese Spanish
1 Che 82.64 81.84 76.38 83.27 87.00 82.44 85.65 81.90
2 Chen 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
3 Merlo 82.14 82.66 76.15 83.21 86.03 79.59 84.91 82.43
4 Bohnet 80.85 80.44 75.91 79.57 85.14 81.60 82.51 80.75
5 Asahara 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
6 Brown 77.27 77.40 72.12 75.66 83.98 77.86 76.65 77.21
7 Zhang 76.49 75.00 73.42 76.93 82.88 73.76 78.17 75.25
8 Dai 73.98 72.09 72.72 67.14 81.89 75.00 80.89 68.14
9 Lu Li 73.97 71.32 65.53 75.85 81.92 70.93 80.49 71.72
10 Llu??s 71.49 56.64 66.18 75.95 81.69 72.31 81.76 65.91
11 Vallejo 70.81 73.75 67.16 60.50 78.19 67.51 77.75 70.78
12 Ren 67.81 59.42 75.90 60.18 77.83 65.77 77.63 57.96
13 Zeman 51.07 49.61 43.50 57.95 50.27 49.57 57.69 48.90
Table 5: Official results of the Joint task, closed challenge. Teams are denoted by the last name (first name added
only where needed) of the author who registered for the evaluation data. Results are sorted in descending order of the
language-averaged macro F1 score on the closed challenge Joint task. Bold numbers denote the best result for a given
language.
Rank Rank in task System Average Catalan Chinese Czech English German Japanese Spanish
1 1 (SRLonly) Zhao 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
2 2 (SRLonly) Nugues 80.31 80.01 78.60 85.41 85.63 79.71 76.30 76.52
3 1 (Joint) Chen 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
4 2 (Joint) Che 79.94 77.10 77.15 86.51 85.51 78.61 78.26 76.47
5 3 (Joint) Merlo 78.42 77.44 76.05 86.02 83.24 71.78 77.23 77.19
6 3 (SRLonly) Meza-Ruiz 77.46 78.00 77.73 75.75 83.34 73.52 76.00 77.91
7 4 (Joint) Bohnet 76.00 74.53 75.29 79.02 80.39 75.72 72.76 74.31
8 5 (Joint) Asahara 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
9 6 (Joint) Brown 72.85 72.18 72.43 78.02 80.43 73.40 61.57 71.95
10 7 (Joint) Dai 70.78 66.34 71.57 75.50 78.93 67.43 71.02 64.64
11 8 (Joint) Zhang 70.31 67.34 73.20 78.28 77.85 62.95 64.71 67.81
12 9 (Joint) Lu Li 69.72 66.95 67.06 79.08 77.17 61.98 69.58 66.23
13 4 (SRLonly) Baoli Li 69.26 74.06 70.37 57.46 69.63 67.76 72.03 73.54
14 10 (Joint) Vallejo 68.95 70.14 66.71 71.49 75.97 61.01 68.82 68.48
15 5 (SRLonly) Moreau 66.49 65.60 67.37 71.74 72.14 66.50 57.75 64.33
16 11 (Joint) Llu??s 63.06 46.79 59.72 76.90 75.86 62.66 71.60 47.88
17 6 (SRLonly) Ta?ckstro?m 61.27 57.11 63.41 71.05 67.64 53.42 54.74 61.51
18 7 (SRLonly) Lin 57.18 61.70 70.33 60.43 65.66 59.51 23.78 58.87
19 12 (Joint) Ren 56.69 41.00 72.58 62.82 67.56 54.31 58.73 39.80
20 13 (Joint) Zeman 32.14 24.19 34.71 58.13 36.05 16.44 30.13 25.36
Table 6: Official results of the semantic labeling, closed challenge, all systems. Teams are denoted by the last name
(first name added only where needed) of the author who registered for the evaluation data. Results are sorted in
descending order of the semantic labeled F1 score (closed challenge). Bold numbers denote the best result for a given
language. Separate ranking is provided for SRL-only systems.
The second column of the table highlights the over-
all architectures. We used + to indicate that the
components are sequentially connected. The lack of
a + sign indicates that the corresponding tasks are
performed jointly.
It is perhaps not surprising that most of the obser-
vations from the 2008 shared task still hold; namely,
the best systems overall do not use joint learning or
optimization (the best such system was placed third
in the Joint task, and there were only four systems
where the learning methodology can be considered
?joint?).
Therefore, most of the observations and conclu-
sions from 2008 shared task hold as well for the
current results. For details, we will leave it to the
reader to interpret the architectures and methods
14
O
v
er
a
ll
D
D
D
PA
PA
PA
Jo
in
t
M
L
Sy
st
em
a
A
rc
h.
b
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
c
A
rc
h.
C
o
m
b.
In
fe
re
n
ce
Le
a
rn
in
g/
O
pt
.
M
et
ho
ds
Zh
ao
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
/g
lo
ba
l
se
ar
ch
(S
R
L-
o
n
ly
)
M
E
N
u
gu
es
(P
C+
A
I+
A
C)
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
be
am
se
ar
ch
+
re
ra
n
ki
n
g
(S
R
L-
o
n
ly
)
L2
-
re
gu
la
riz
ed
lin
.
re
gr
es
sio
n
Ch
en
P
+
PC
+
A
I+
A
C
gr
ap
h
pa
rt
ia
lly
M
ST
C
L
/E
cl
as
s
n
o
gr
ee
dy
(?)
n
o
M
E
Ch
e
D
+
PC
+
A
IC
gr
ap
h
n
o
M
ST
H
O
E
cl
as
s
n
o
IL
P
n
o
SV
M
,
M
E
M
er
lo
D
PA
IC
+
D
ge
n
er
at
iv
e,
tr
an
s
n
o
be
am
se
ar
ch
tr
an
s
n
o
be
am
se
ar
ch
sy
n
ch
ro
n
iz
ed
de
riv
at
io
n
IS
B
N
M
ez
a-
R
u
iz
PA
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
M
ar
ko
v
LN
n
o
Cu
tti
n
g
Pl
an
e
(S
R
L-
o
n
ly
)
M
IR
A
B
o
hn
et
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
C
+
re
ar
ra
n
ge
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
IR
A
)
A
sa
ha
ra
D
+
PI
C
+
A
IC
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
n
-
be
st
re
la
x
.
n
o
pe
rc
ep
tr
o
n
D
ai
D
+
PC
+
A
C
gr
ap
h
n
o
M
ST
C
cl
as
s
n
o
pr
o
b
ite
ra
tiv
e
M
E
Zh
an
g
D
+
A
I+
A
C
+
PC
gr
ap
h
n
o
M
ST
E
cl
as
s
n
o
cl
as
sifi
ca
tio
n
n
o
M
IR
A
,
M
E
Lu
Li
D
+
(P
C
||
A
IC
)
gr
ap
h
fo
r
ea
ch
la
n
g.
M
ST
C
L
/E
,
M
ST
E
cl
as
s
n
o
gr
ee
dy
n
o
M
E
B
ao
li
Li
PC
+
A
IC
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
,
kN
N
,
M
E
Va
lle
jod
[D
+
P+
A
]C
+
D
I
cl
as
s
n
o
re
ra
n
ki
n
g
cl
as
s
n
o
re
ra
n
ki
n
g
u
n
ifi
ed
la
be
ls
M
B
L
M
o
re
au
D
+
PI
+
Cl
u
st
er
in
g
+
A
I+
A
C
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
CR
F
(S
R
L-
o
n
ly
)
CR
F
Ll
u
??s
D
+
D
A
IC
+
PC
gr
ap
h
n
o
M
ST
E
gr
ap
h
n
o
M
ST
E
ye
s,
M
ST
E
Av
g.
Pe
rc
ep
tr
o
n
Ta?
ck
st
ro?
m
D
+
PI
+
A
I
+
A
C
+
Co
n
st
ra
in
tS
at
isf
ac
tio
n
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
(S
R
L-
o
n
ly
)
cl
as
s
n
o
gr
ee
dy
(S
R
L-
o
n
ly
)
SV
M
R
en
D
+
PC
+
A
IC
tr
an
s
n
o
gr
ee
dy
cl
as
s
n
o
gr
ee
dy
n
o
SV
M
(M
al
t),
M
E
Ze
m
an
D
I+
D
C+
PC
+
A
I+
A
C
tr
an
s
n
o
gr
ee
dy
w
ith
he
u
ris
tic
s
cl
as
s
n
o
gr
ee
dy
n
o
co
o
cc
u
rr
en
ce
Ta
bl
e
7:
Su
m
m
ar
y
o
fs
ys
te
m
ar
ch
ite
ct
u
re
s
fo
r
th
e
Co
N
LL
-
20
09
sh
ar
ed
ta
sk
;
al
ls
ys
te
m
s
ar
e
in
cl
u
de
d.
SR
L-
o
n
ly
sy
st
em
s
do
n
o
t
ha
v
e
th
e
D
co
lu
m
n
s
an
d
th
e
Jo
in
t
Le
ar
in
g/
O
pt
.
co
lu
m
n
s
fil
le
d
in
.
Th
e
sy
st
em
s
ar
e
so
rt
ed
by
th
e
se
m
an
tic
la
be
le
d
F 1
sc
o
re
av
er
ag
ed
o
v
er
al
lt
he
la
n
gu
ag
es
(sa
m
e
as
in
Ta
bl
e
6).
O
n
ly
th
e
sy
st
em
s
th
at
ha
v
e
a
co
rr
es
po
n
di
n
g
pa
pe
r
in
th
e
pr
o
ce
ed
in
gs
ar
e
in
cl
u
de
d.
A
cr
o
n
ym
s
u
se
d:
D
-
sy
n
ta
ct
ic
de
pe
n
de
n
ci
es
,
P
-
pr
ed
ic
at
e,
A
-
ar
gu
m
en
t,
I-
id
en
tifi
ca
tio
n
,
C
-
cl
as
sifi
ca
tio
n
.
O
v
er
a
ll
a
rc
h.
st
an
ds
fo
r
th
e
co
m
pl
et
e
sy
st
em
ar
ch
ite
ct
u
re
;D
A
rc
h.
st
an
ds
fo
r
th
e
ar
ch
ite
ct
u
re
o
ft
he
sy
n
ta
ct
ic
pa
rs
er
;D
C
o
m
b.
in
di
ca
te
s
if
th
e
fin
al
pa
rs
er
o
u
tp
u
tw
as
ge
n
er
at
ed
u
sin
g
pa
rs
er
co
m
bi
n
at
io
n
;D
In
fe
re
n
ce
st
an
ds
fo
r
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
sy
n
ta
ct
ic
pa
rs
in
g;
PA
A
rc
h.
st
an
ds
th
e
ty
pe
o
fa
rc
hi
te
ct
u
re
u
se
d
fo
r
PA
IC
;P
A
C
o
m
b.
in
di
ca
te
s
if
th
e
PA
o
u
tp
u
t
w
as
ge
n
er
at
ed
th
ro
u
gh
sy
st
em
co
m
bi
n
at
io
n
;P
A
In
fe
re
n
ce
st
an
ds
fo
r
th
e
th
e
ty
pe
o
fi
n
fe
re
n
ce
u
se
d
fo
r
PA
IC
;J
o
in
tL
ea
rn
in
g/
O
pt
.
in
di
ca
te
s
if
so
m
e
fo
rm
o
fjo
in
tl
ea
rn
in
g
o
r
o
pt
im
iz
at
io
n
w
as
im
pl
em
en
te
d
fo
r
th
e
sy
n
ta
ct
ic
+
se
m
an
tic
gl
o
ba
lt
as
k;
M
L
M
et
ho
ds
lis
ts
th
e
M
L
m
et
ho
ds
u
se
d
th
ro
u
gh
o
u
tt
he
co
m
pl
et
e
sy
st
em
.
a
A
u
th
o
rs
o
ft
w
o
sy
st
em
s:
?
B
ro
w
n
?
an
d
?
Li
n
?
di
dn
?
ts
u
bm
it
a
pa
pe
r,
so
th
ei
r
sy
st
em
s?
ar
ch
ite
ct
u
re
s
ar
e
u
n
kn
ow
n
.
b T
he
sy
m
bo
l+
in
di
ca
te
s
se
qu
en
tia
lp
ro
ce
ss
in
g
(ot
he
rw
ise
,
pa
ra
lle
l/jo
in
t).
Th
e
||
m
ea
n
s
th
at
se
v
er
al
di
ffe
re
n
ta
rc
hi
te
ct
u
re
s
sp
an
n
in
g
m
u
lti
pl
e
su
bt
as
ks
ra
n
in
pa
ra
lle
l.
c
M
ST
C
L
/E
as
u
se
d
by
M
cD
o
n
al
d
(20
05
),
M
ST
C
by
Ca
rr
er
as
(20
07
),M
ST
E
by
Ei
sn
er
(20
00
),
M
ST
H
O
E
=
M
ST
E
w
ith
hi
gh
er
-
o
rd
er
fe
at
u
re
s
(si
bl
in
gs
+
al
lg
ra
n
dc
hi
ld
re
n
).
d T
he
sy
st
em
u
n
ifi
es
th
e
sy
n
ta
ct
ic
an
d
se
m
an
tic
la
be
ls
in
to
o
n
e
la
be
l,
an
d
tr
ai
n
s
cl
as
sifi
er
s
o
v
er
th
em
.
It
is
th
u
s
di
ffi
cu
lt
to
sp
lit
th
e
sy
st
em
ch
ar
ac
te
ris
tic
in
to
a
?
D
?
/?
PA
?
pa
rt
.
15
when comparing Table 7 with the Tables 5 and 6).
6 Conclusion
This year?s task has been demanding in several re-
spects, but certainly the most difficulty came from
the fact that participants had to tackle all seven lan-
guages. It is encouraging that despite this added af-
fort the number of participating systems has been
almost the same as last year (20 vs. 22 in 2008).
There are several positive outcomes from this
year?s enterprise:
? we have prepared a unified format and data for
several very different lanaguages, as a basis
for possible extensions towards other languages
and unified treatment of syntactic depenndecies
and semantic role labeling across natural lan-
guages;
? 20 participants have produced SRL results for
all seven languages, using several different
methods, giving hope for a combined system
with even substantially better performance;
? initial results have been provided for three lan-
guages on out-of-domain data (being in fact
quite close to the in-domain results).
Only four systems tried to apply what can be de-
scribed as joint learning for the syntactic and seman-
tic parts of the task. (Morante et al, 2009) use a true
joint learning formulation that phrases syntactico-
semantic parsing as a series of classification where
the class labels are concatenations of syntactic and
semantic edge labels. They predict (a), the set of
syntactico-semantic edge labels for each pair of to-
kens; (b), the set of incoming syntactico-semantic
edge labels for each individual token; and (c), the
existence of an edge between each pair of tokens.
Subsequently, they combine the (possibly conflict-
ing) output of the three classifiers by a ranking ap-
proach to determine the most likely structure that
meets all well-formedness constraints. (Llu??s et al,
2009) present a joint approach based on an exten-
sion of Eisner?s parser to accommodate also seman-
tic dependency labels. This architecture is similar
to the one presented by the same authors in the past
edition, with the extension to a second-order syn-
tactic parsing and a particular setting for Catalan
and Spanish. (Gesmundo et al, 2009) use an in-
cremental parsing model with synchronous syntac-
tic and semantic derivations and a joint probability
model for syntactic and semantic dependency struc-
tures. The system uses a single input queue but two
separate stacks and synchronizes syntactic and se-
mantic derivations at every word. The synchronous
derivations are modeled with an Incremental Sig-
moid Belief Network that has latent variables for
both syntactic and semantic states and connections
from syntax to semantics and vice versa. (Dai et
al., 2009) designed an iterative system to exploit
the inter-connections between the different subtasks
of the CoNLL shared task. The idea is to decom-
pose the joint learning problem into four subtasks
? syntactic dependency identification, syntactic de-
pendency labeling, semantic dependency identifica-
tion and semantic dependency labeling. The initial
step is to use a pipeline approach to use the input of
one subtask as input to the next, in the order speci-
fied. The iterative steps then use additional features
that are not available in the initial step to improve the
accuracy of the overall system. For example, in the
iterative steps, semantic information becomes avail-
able as features to syntactic parsing, so on and so
forth.
Despite these results, it is still not clear whether
joint learning has a significant advantage over other
approaches (and if yes, then for what languages). It
is thus necessary to carefully plan the next shared
tasks; it might be advantageous to bring up a sim-
ilar task in the future once again, and/or couple it
with selected application(s). There, (we hope) the
benefits of the dependency representation combined
with semantic roles the way we have formulated it
in 2008 and 2009 will really show up.
Acknowledgments
We would like to thank the Linguistic Data Consor-
tium, mainly to Denise DiPersio, Tony Casteletto
and Christopher Cieri for their help and handling
of invoicing and distribution of the data for which
LDC has a license. For all of the trial, training and
evaluation data they had to act a very short notice.
All the data has been at the participants? disposal
(again) free of charge. We are grateful to all of them
for LDC?s continuing support of the CoNLL Shared
16
Tasks.
We would also like to thank organizers of the pre-
vious four shared tasks: Sabine Buchholz, Xavier
Carreras, Ryan McDonald, Amit Dubey, Johan Hall,
Yuval Krymolowski, Sandra Ku?bler, Erwin Marsi,
Jens Nilsson, Sebastian Riedel and Deniz Yuret.
This shared task would not have been possible with-
out their previous effort.
We also acknowledge the support of the M?SMT
of the Czech Republic, projects MSM0021620838
and LC536; the Grant Agency of the Academy of
sciences of the Czech Republic 1ET201120505 (for
Jan Hajic?, Jan ?Ste?pa?nek and Pavel Stran?a?k).
Llu??s Ma`rquez and M. Anto`nia Mart?? partici-
pation was supported by the Spanish Ministry of
Education and Science, through the OpenMT and
TextMess research projects (TIN2006-15307-C03-
02, TIN2006-15265-C06-06).
The following individuals directly contributed to
the Chinese Treebank (in alphabetic order): Meiyu
Chang, Fu-Dong Chiou, Shizhe Huang, Zixin Jiang,
Tony Kroch, Martha Palmer, Mitch Marcus, Fei
Xia, Nianwen Xue. The contributors to the Chi-
nese Proposition Bank include (in alphabetic order):
Meiyu Chang, Gang Chen, Helen Chen, Zixin Jiang,
Martha Palmer, Zhiyi Song, Nianwen Xue, Ping Yu,
Hua Zhong. The Chinese Treebank and the Chinese
Proposition Bank were funded by DOD, NSF and
DARPA.
Adam Meyers? work on the shared task has been
supported by the NSF Grant IIS-0534700 ?Structure
Alignment-based MT.?
We thank the Mainichi Newspapers for the per-
mission of distributing the sentences of the Kyoto
University Text Corpus for this shared task.
References
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER tree-
bank. In Proceedings of the Workshop on Treebanks
and Linguistic Theories, Sozopol.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings of
EMNLP-CoNLL 2007, pages 957?961, June. Prague,
Czech Republic.
Montserrat Civit, M. Anto`nia Mart??, and Nu?ria Buf??.
2006. Cat3LB and Cast3LB: from constituents to
dependencies. In Proceedings of the 5th Interna-
tional Conference on Natural Language Processing,
FinTAL, pages 141?153, Turku, Finland. Springer Ver-
lag, LNAI 4139.
Qifeng Dai, Enhong Chen, and Liu Shi. 2009. An it-
erative approach for joint dependency parsing and se-
mantic role labeling. In Proceedings of the 13th Con-
ference on Computational Natural Language Learning
(CoNLL-2009), June 4-5, Boulder, Colorado, USA.
June 4-5.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Tehcnologies, pages 29?62. Kluwer Academic
Publishers.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press, Cambridge.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5, Boulder, Colorado, USA. June 4-5.
Jan Hajic?, Jarmila Panevova?, Zden?ka Ures?ova?, Alevtina
Be?mova?, Veronika Kola?r?ova?- ?Rezn??c?kova??, and Petr
Pajas. 2003. PDT-VALLEX: Creating a Large-
coverage Valency Lexicon for Treebank Annotation.
In J. Nivre and E. Hinrichs, editors, Proceedings of The
Second Workshop on Treebanks and Linguistic Theo-
ries, pages 57?68, Vaxjo, Sweden. Vaxjo University
Press.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA. June 4-5.
17
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
Llu??s Ma`rquez, Luis Villarejo, M. Anto`nia Mart??, and
Mariona Taule?. 2007. SemEval-2007 Task 09: Mul-
tilevel semantic annotation of catalan and spanish.
In Proceedings of the 4th International Workshop on
Semantic Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
M. Anto`nia Mart??, Mariona Taule?, Llu??s Ma`rquez, and
Manu Bertran. 2007. Anotacio?n semiautoma?tica
con papeles tema?ticos de los corpus CESS-ECE.
Procesamiento del Lenguaje Natural, SEPLN Journal,
38:67?76.
Ryan McDonald, Fernando Pereira, Jan Hajic?, and Kiril
Ribarov. 2005. Non-projective dependency parsing
using spanning tree algortihms. In Proceedings of
NAACL-HLT?05, Vancouver, Canada, pages 523?530.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-
ska, B. Young, and R. Grishman. 2004. The Nom-
Bank Project: An Interim Report. In NAACL/HLT
2004 Workshop Frontiers in Corpus Annotation,
Boston.
Roser Morante, Vincent Van Asch, and Antal van den
Bosch. 2009. A simple generative pipeline approach
to dependency parsing and semantic role labeling. In
Proceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), Boulder,
Colorado, USA. June 4-5.
Joakim Nivre, Johann Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on depen-
dency parsing. In Proceedings of the EMNLP-CoNLL
2007 Conference, pages 915?932, Prague, Czech Re-
public.
Sebastian Pado and Mirella Lapata. 2005. Cross-lingual
projection of role-semantic information. In Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP-2005), pages 859?
866, Vancouver, BC.
Petr Pajas and Jan ?Ste?pa?nek. 2008. Recent advances in
a feature-rich framework for treebank annotation. In
The 22nd International Conference on Computational
Linguistics - Proceedings of the Conference (COL-
ING?08), pages 673?680, Manchester.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Drahom??ra ?Johanka? Spoustova?, Jan Hajic?, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the European ACL Cenference EACL?09,
Athens, Greece.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), pages 159?177.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Martin ?Cmejrek, Jan Cur???n, Jan Hajic?, Jir??? Havelka,
and Vladislav Kubon?. 2004. Prague Czech-English
Dependency Treebank: Syntactically Anntoated Re-
sources for Machine Translation. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC-2004), pages 1597?
1600, Lisbon, Portugal.
W. N. Francis and H. Kucera. 1964. Brown Corpus Man-
ual of Information to accompany A Standard Corpus
of Present-Day Edited American English, for use with
Digital Computers. Revised 1971, Revised and Am-
plified 1979, available at www.clarinet/brown.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Technical report,
Lin- guistic Data Consortium.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Nianwen Xue, Fei Xia, Fu Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
Structure Annotation of a Large Corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Andrea Zielinski and Christian Simon. 2008. Morphisto:
An open-source morphological analyzer for german.
In Proceedings of the Conference on Finite State Meth-
ods in Natural Language Processing.
18
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1046?1055,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Gazpacho and summer rash:
lexical relationships from temporal patterns of web search queries
Enrique Alfonseca Massimiliano Ciaramita Keith Hall
Google
Z?urich, Switzerland
ealfonseca@google.com, massi@google.com, kbhall@google.com
Abstract
In this paper we investigate temporal pat-
terns of web search queries. We carry out
several evaluations to analyze the proper-
ties of temporal profiles of queries, reveal-
ing promising semantic and pragmatic re-
lationships between words. We focus on
two applications: query suggestion and
query categorization. The former shows
a potential for time-series similarity mea-
sures to identify specific semantic relat-
edness between words, which results in
state-of-the-art performance in query sug-
gestion while providing complementary
information to more traditional distribu-
tional similarity measures. The query cat-
egorization evaluation suggests that the
temporal profile alone is not a strong in-
dicator of broad topical categories.
1 Introduction
The temporal patterns of word occurrences in hu-
man communication carry an implicit measure of
their relationship to real-world events and behav-
ioral patterns. For example, when there is an event
affecting a given entity (such as a natural disaster
in a country), the entity name will turn up more
frequently in human conversation, newswire arti-
cles and web documents; and people will search
for it more often. Two entities that are closely
related in the real world, such as the name of a
country and a prominent region inside the coun-
try are likely to share common events and there-
fore be closely associated in human communica-
tion. Finally, two instances of the same class
are also likely to share common usage patterns.
For example, names of airlines or retail stores are
more likely to be used by day rather than by night
(Chien, 2005).
In this paper we explore the linguistic relation-
ship between phrases that are judged to be sim-
ilar based on their frequency time series correla-
tion in search query logs. For every phrase
1
avail-
able in WordNet 3.0
2
(Miller, 1995), we have ob-
tained its temporal signature from query logs, and
calculated all their pairwise correlations. Next,
we study the relationship in the top-ranked pairs
with respect to their distribution in WordNet and a
human-annotated labelling.
We also discuss possible applications of this
data to solve open problems and present the results
of two experiments: one where time series corre-
lations turned out to be highly discriminative; and
another where they were not particularly informa-
tive but shed some light on the nature of temporal
semantics and topical categorization:
? Query suggestion, i.e. given a query, generate
a ranked list of alternative queries in which
the user may be interested.
? Query categorization, i.e. given a predefined
set of categories, find the top categories to
which the query can be assigned.
Finally, we illustrate with an example another ap-
plication of time series in solving information ex-
traction problems.
Although query logs are typically proprietary
data, there are ongoing initiatives, like the Lemur
toolbar
3
, which make this kind of information
available for research purposes. Other work
(Bansal and Koudas, 2007b; Bansal and Koudas,
2007a) shows that temporal information can also
be extracted from public data, such as blogs. More
traditional types of text, such as news, are also typ-
ically associated with temporal labels; e.g., dates
and timestamps.
This paper is structured in the following way:
1
We use the term phrase to refer to any single word or
multi-word expression that belongs to a synset in WordNet.
Examples of phrases are person, causal entity or william
shakespeare. We focused on the nouns hierarchy only.
2
http://wordnet.princeton.edu
3
http://www.lemurproject.org/
querylogtoolbar/
1046
Section 2 summarizes the related work. Section 3
describes the correlation analysis between all pairs
of phrases from WordNet. Next, Section 4 de-
scribes the application to query suggestion, and
Section 5 the application to labelling queries in
topical categories. Section 7 summarizes the con-
clusions and outlines ideas for future research.
2 Related work
The study of query time series explores a particu-
lar instance of the so-called wisdom of the crowds
effect. Within this area, we can distinguish two
kinds of phenomena. Knowledge and resources
assembled by people explicitly, either individu-
ally, such as the case of blogs, or in a collabora-
tive way, as in forums or wikis. These resources
are valuable for human-consumption and can also
be exploited in order to learn computational re-
sources (Medelyan et al, 2008; Weld et al, 2008;
Zesch et al, 2008b; Zesch et al, 2008a). On
the other hand, it is possible to acquire useful re-
sources and knowledge from aggregating behav-
ioral patterns of large groups of people, even in
the absence of a conscious effort. There is exten-
sive ongoing research on the use of web search
usage patterns to develop knowledge resources.
Some examples are clustering co-click patterns to
learn semantically related queries (Beeferman and
Berger, 2000), combining co-click patterns with
hitting times (Mei et al, 2008), analyzing query
revisions made by users when querying search en-
gines (Jones et al, 2006), replacing query words
with other words that have the highest pointwise
mutual information (Terra and Clarke, 2004), or
using the temporal distribution of words in docu-
ments to improve ranking of search results (Jones
and Diaz, 2007).
Within this second category, an important area
is dedicated to the study of time-related features
of search queries. News aggregators use real-time
frequencies of user queries to detect spikes and
identify news shortly after the spikes occur (Mu-
rata, 2008). Web users? query patterns have also
proved useful for building a real-time surveillance
system that accurately estimates region-by-region
influenza activity with a lag of one day (Ginsberg
et al, 2009). Search engines specifically devel-
oped for real-time searches, like Twitter search,
will most likely provide new use cases and sce-
narios for quickly detecting trends in user search
query patterns.
Figure 1: Time series obtained for the queries
[gazpacho] and [summertime] (normalized
scales).
Our study builds upon the work of Chien
(2005), who observed that queries with highly-
correlated temporal usage patterns are typically
semantically related, and described a procedure
for calculating the correlations efficiently. We
have extended the analysis described in this work,
by performing a more extensive evaluation of the
kinds of semantic relationships that we can find
among temporally-similar queries. We also pro-
pose, to our knowledge for the first time, areas
of applications in solving well-established prob-
lems which shed some light on the nature of time-
based semantic similarity. This work is also re-
lated to the analysis of temporal properties of
information streams in data mining (Kleinberg,
2006) and information retrieval from time series
databases (Agrawal et al, 1993).
3 Time-based similarities between
phrases
Similarly to the method described in Chien (2005),
we take a time interval, divide it into equally
spaced subintervals, and represent each phrase of
interest as the sequence of frequencies with which
the phrase was observed in the subintervals. In
our experiments, we have used as source data
the set of fully anonymized query logs from the
Google search engine between January 1st, 2004
and March 1st, 2009.
4
.
These data have been aggregated on a daily ba-
sis so that we have the daily frequency of the
4
Part of this data is publicly available from http://
www.google.com/trends
1047
queries of interest for over five years. The frequen-
cies are then normalized with the total number of
queries that happened on that day. The normaliza-
tion is necessary to avoid daily and seasonal varia-
tions as there are typically more queries on week-
days than on weekends and fewer queries during
holiday seasons than in the rest of the year. It
also helps reducing the effect deriving from the
fact that the population with Internet access is still
monotonically growing, so we can expect that the
number of queries will become higher and higher
over time.
Given two phrases and their associated time se-
ries, the similarity metric used is the correlation
coefficient between the two series (Chien, 2005).
For illustration, Figure 1 shows the time series ob-
tained for two sample queries, gazpacho and sum-
mertime, whose time series yield a correlation of
0.92. Similar high correlations can be observed
with other queries related to phenomena that oc-
cur mainly in summer in the countries from which
most queries come, like summer rash.
3.1 WordNet-based evaluation
In this section, we describe a study carried out
with the purpose of discovering the traditional
lexico-semantic relationships which hold between
the queries that are most strongly related accord-
ing to their temporal profiles.
For this evaluation, we have taken the nomi-
nal phrases appearing in WordNet 3.0. Given that
users, when writing queries, typically do not pay
attention to punctuation and case, we have normal-
ized all phrases by lowercasing them and remov-
ing all punctuation. Next, we collected the time se-
ries for each phrase by computing the normalized
daily frequency of each of them as exact queries
in the query logs. The computation of the pair-
wise correlations was performed in parallel using
the MapReduce infrastructure running over 2048
cores with 500 MB of RAM each. The total ex-
ecution (including data shuffling and networking
time) took approximately three hours.
Next, we represented the data as a complete
graph where phrases are nodes and the edge be-
tween each pair of nodes is weighted by their time
series correlation. Using a simple graph-cut we
obtained clusters of related terms. A minimum
weight threshold equal to 0.9 was applied;
5
thus,
5
This threshold is the same used by Chien (2005), and was
confirmed after a manual inspection of a sample of the data
two phrases belong to the same cluster if there is
a path between them only via edges with weight
over 0.9.
The previous procedure produced a set of 604
clusters, with highly different sizes. The first ob-
servation is that 70% of the phrases in WordNet
do not have a correlation over 0.9 with any other
phrase, so they are placed alone in singleton clus-
ters. There are several reasons for this. The clus-
ters obtained are very specific: only phrases that
have a very strong temporal association have tem-
poral correlations exceeding the threshold. This is
combined with the fact that we are using a very
restricted vocabulary, namely the terms included
in WordNet, which is many orders of magnitude
smaller than the vocabulary of all possible queries
from the users. Few phrase pairs in WordNet
have a temporal association and popularity strong
enough to be clustered together. Finally, many of
the phrases in WordNet are rare, including scien-
tific names of animals and plants, genuses or fami-
lies, which are not commonly used. Therefore, the
clusters extracted here correspond to very salient
sets of phrases. If, instead of WordNet, we choose
a vocabulary from known user queries (cf. Sec-
tion 4), there would be many fewer singleton clus-
ters, as the options of similar phrases to choose
from would be much larger.
From the phrases that belong to clusters, 25%
of the WordNet phrases do not have strong daily
temporal profiles. The typical pattern for these
terms is an almost flat time series, usually with
small drops at summertime and Christmas (when
seasonal leisure-related queries dominate). There-
fore, these phrases were collected in just one clus-
ter containing them all. Typical examples of the
elements of this set are names of famous scientists
and mathematicians (Gauss, Isaac Newton, Al-
bert Einstein, Thomas Alva Edison, Hipprocrates,
Gregor Mendel, ...), common terms (fertilization,
famine, macroeconomics, genus, nationalism, ...),
numbers and common first names, among other
things. It is possible that using sub-day intervals
might help to discriminate within this cluster.
The items in this big cluster contrast with pe-
riodical events, which display recurring patterns
(e.g., queries related to elections or tax-returns),
and names of famous people and other entities
which appeared in the news in the past few years.
All of these are associated with irregular, spiky
time series. These constitute the final 5% of the
1048
Type Pairs Examples
Synonyms 283 (angel cake, angel food cake), (thames, river thames), (armistice day, Nov 11)
Hyponym/hyperonyms 86 (howard hughes, aviator), (muhammad, prophet), (olga korbut, gymnast)
Siblings in hyponym taxonomy 611 (hiroshima, nagasaki), (junior school, primary school), (aids, welt)
Meronym/holonyms 53 (tutsi, rwanda), (july 4, july), (pyongyang, north korea)
Siblings in meronymy taxonomy 7 (everglades, everglades national park), (mississipi, orleans)
Other paths 471 (maundy thursday, maundy money), (tap water, water tap), (gren party, liberal)
Not structurally related 1009 (poppy, veterans day), (olympic games, gimnast), (belmont park, horse racing)
Table 1: Relationships between pairs of WordNet phrases belonging to the same cluster.
phrases belonging to small, highly focused, clus-
ters.
Table 1 shows the relationships that hold be-
tween all pairs of phrases belonging to any of the
smaller clusters. Out of 2520 pairs, 283 belong
to the same synset, 697 are related via hyponymy
links, 60 via meronymy links, and 471 by alternat-
ing hyponymy and meronymy links in the path.
When the phrases were polysemous, the short-
est path between any of their meaning was used.
About 40% of the relations do not have a clear
structural interpretation in WordNet.
The majority of pairs are related via more or
less complex paths in the WordNet graph. Inter-
estingly, even the structurally unrelated terms are
characterized by transparent relations in terms of
world knowledge, as it is the case between poppy
and veteran day. Note as well that sometimes a
WordNet term is used with a meaning not present
in WordNet or in a different language, which may
explain why aids has a very high correlation with
welt (AIDS and welt are both hyponyms of health
problem, but the correlation may be explained bet-
ter by the AIDS World Day, Welt Aids Tag in Ger-
man), and it also has a very high correlation with
sida, defined in WordNet as a genus of tropical
herbs, but which is in fact the translation of AIDS
into Spanish. These observations motivated an ad-
ditional manual labelling of the extracted pairs.
3.2 Hand labelled evaluation
As can be seen in Table 2, most of the terms that
constitute a cluster are related to each other, al-
though the kinds of semantic relationships that
hold between them can vary significantly. Exam-
ples of the following kinds can be observed:
? True synonyms, as in the case of november
and nov, or architeuthis and giant squid.
? Variations of people names, especially if a
person?s first name or surname is typically
used to refer to that person, as in the case of
john lennon and lennon, or janis joplin and
joplin. Sometimes the variations include per-
sonal titles, as it is the case of president carter
and president nixon, which are highly corre-
lated with jimmy carter and richard nixon.
? Geographically-related terms, referring to
locations which are located close to each
other, as in the clusters {korea, north ko-
rean, south korea, pyongyang, north korea}
and {strasbourg, grenoble, toulouse, poitiers,
lyon, lille, nantes, reims}.
? Synonyms of location names, like bahrain
and bahrein.
? Derived words, like north korea and north
korean, or lebanese and lebanon.
? Generic word optionalizations, which hap-
pen when one word in a multi-word phrase
is very correlated to the phrase, as in the
case of spanish inquisition and inquisition,
or red bone marrow and red marrow, where
the most common interpretation for the short-
ened version of the phrase is the same as for
the long version.
? Word reordering, where the two related
phrases have the same words in a different or-
der, as in the case of maple sugar and sugar
maple, or oil palm and palm oil.
? Morphological variants: WordNet does not
contain many morphological variants in the
main dataset, but there are a few, like station
of the cross and stations of the cross.
? Acronyms, like federal emergency manage-
ment agency and fema.
? Hyperonym-hyponym, like fern and plant.
? Sibling terms in a taxonomy, as in the clus-
ter {lutheran, methodist, presbyterian, united
methodist church, lutheran church,methodist
church, presbyterian church,baptist, baptist
church}, which contains mostly names of
Christian denominations.
? Co-occurring events in time, as is the case
of hitch and pacifier, both titles of movies
which were launched at almost the same
1049
hydrant,fire hydrant
inauguration day,inauguration,swearing,investiture,inaugural address,inaugural,benediction,oath
indulgence,self indulgence
insulation,heating
interstate highway,interstate, intestine,small intestine
iq,iq test
irish people,irish,irish potato,irish gaelic,gaelic,irish soda bread,irish stew,st patrick,saint patrick,leprechaun,
march 17,irish whiskey,shillelagh
ironsides,old ironsides
james,joyce,james joyce
janis joplin,joplin
jesus christ,pilate,pontius pilate,passion of christ,passion,aramaic
jewish new year,rosh hashana,rosh hashanah,shofar
john lennon,lennon
julep,mint julep,kentucky derby,kentucky
keynote,keynote address
kickoff,time off
korea,north korean,south korea,pyongyang,north korea
l ron hubbard,scientology
leap,leap year,leap day,february 29
left brain,right brain
leftover,leftovers,turkey stew
linseed oil,linseed
listeria,listeriosis,maple leaf
lobster tail,lobster,tails
lohan,lindsay
loire,rhone,rhone alpes
looking,looking for
lutheran,methodist,presbyterian,united methodist church,lutheran church,methodist church,presbyterian church,
baptist,baptist church
mahatma gandhi,mahatma
malignant hyperthermia,hyperthermia
maple sugar,sugar maple
martin luther,martin luther king,luther,martin,martin luther king day
matzo,matzah,matzoh,passover,seder,matzo meal,pesach,haggadah,gefilte fish
mestizo,half blood,half and half
meteorology,weather bureau
moslem,muslim,prophet,mohammed,mohammad,muhammad,mahomet
movie star,star,revenge,film star,menace,george lucas
mt st helens,mount saint helens,mount st helens
myeloma,multiple myeloma
ness,loch ness,loch ness monster,loch,nessie
new guinea,papua new guinea,papua
november,nov
pacifier,hitch
papa,pope,vatican,vatican city,karol wojtyla,john paul ii,holy see,pius xii,papacy,paul vi,john xxiii,the holy see,
vatican ii,pontiff,gulp,pater,nostradamus,ii,pontifex
parietal lobe,glioma,malignant tumor
particle accelerator,atom smasher,hadron,large,tallulah bankhead,bankhead,tanner
pledge,allegiance
president carter,jimmy carter
president nixon,richard nixon,richard m nixon
sept 11,september 11,sep 11,twin towers,wtc,ground zero,world trade center
slum,millionaire,pinto
strasbourg,grenoble,toulouse,poitiers,lyon,lille,nantes,reims
valentine,valentine day,february 14,romantic
aeon,flux
alien,predator
anne hathaway,hathaway
architeuthis,giant squid
basal temperature,basal body temperature
execution,saddam hussein,hussein,saddam,hanging,husain
flood,flooding
george herbert walker bush,george walker bush
intifada,palestine
may 1,may day,maypole
Table 2: Sample of clusters obtained from the temporal correlations.
1050
Type Clusters
True synonyms 19
Variations of people names 42
People names with and without titles 4
First name and surname from the same person 4
Geographically-related terms 18
Synonyms of location names 4
Derived words 4
Word optionalizations 87
Word reordering 7
Morphological variants 1
Acronyms 1
Cross-language synonyms 3
Hyperonym/hyponym 10
Sibling terms 10
Co-ocurring events in time 8
Topically related 38
Unrelated 72
Table 3: Results of the manual annotation of 2-
item clusters.
time. A particular example of this is when
the two terms are part of a named entity, as in
the case of quantum and solace, which have
a similar correlation because they appear to-
gether in a movie title.
? Topically-related terms, as the cluster
{jesus christ, pilate, pontius pilate, passion of
christ, passion, aramaic}, or the cluster con-
taining popes and the Vatican. A similar ex-
ample, execution is highly correlated to sad-
dam hussein, because his execution attracted
more interest worldwide during this time pe-
riod than any other execution. Interestingly,
topical correlation emerges at very specific
granularity.
For the manual analysis of the results, we ran-
domly selected 332 clusters containing only two
items (so that 664 phrases were considered in to-
tal). Each of these pairs has been classified in one
of the previous categories. The results of this anal-
ysis are shown in Table 3.
4 Application to query suggestion
Query suggestion is a feature of search engines
that helps users reformulate queries in order to bet-
ter describe their information need with the pur-
pose of reducing the time needed to find the de-
sired information (Beeferman and Berger, 2000;
Kraft and Zien, 2004; Sahami and Heilman, 2006;
Cucerzan and White, 2007; Yih and Meek, 2008).
In this section, we explore the application of a sim-
ilarity metric based on time series correlations for
finding related queries to suggest to the users.
As a test set, we have used the query sugges-
Method P@1 P@3 P@5 mAP
Random 0.37 0.37 0.37 0.43
Web Kernel 0.51 0.47 0.42 0.51
Dist. simil. 0.72 0.63 0.60 0.64
Time series 0.74 0.63 0.53 0.67
Combination 0.79 0.68 0.60 0.69
Table 4: Results for the query suggestion task.
tion dataset from (Alfonseca et al, 2009). It con-
tains a set of 57 queries and an average of 22 can-
didate query suggestions for each of them. Each
suggestion was rated by two human raters using
the 5-point Likert scale defined in (Sahami and
Heilman, 2006), from irrelevant to highly relevant.
The task involves providing a ranking of the sug-
gestions that most closely resembles the human
scores. The evaluation is based on standard IR
metrics: precision at 1, 3 and 5, and mean average
precision. In order to compute the precision- and
recall-based metrics, we infer a binary distinction
from the ratings: related or not related. The inter-
annotator agreement for this dataset given the bi-
nary classification as computed by Cohen?s Kappa
is 0.6171.
We used three baselines: the average values that
would be produced by a random scorer of the can-
didate suggestions, Sahami and Heilman (2006)?s
system (based on calculating similarities between
the retrieved snippets), and a recent competitive
ranker based on calculating standard distributional
similarities (Alfonseca et al, 2009) between the
original query and the suggestion. Please refer to
the referenced work for details.
In order to produce the ranked lists of candi-
date suggestions for each query, due to the lack of
training data, we have opted for the unsupervised
procedure described in the previous section:
1. Collect the daily time series of each of the
queries and the candidate suggestions.
2. Calculate the correlation between the original
query and each of the candidate suggestions
provided for it, and use it as the candidate?s
score.
3. For each query, rank its candidate sugges-
tions in decreasing order of correlation.
Finally, taking into account that the source of
similarity is very different to the one used for dis-
tributional similarity, we tested the hypothesis that
1051
a combination of the two techniques would be ben-
eficial to capture different features of the queries
and suggestions. We have trained a linear mixture
model combining both scores (time series and dis-
tributional similarities), using 10-fold cross vali-
dation.
The results are displayed in Table 4. For eval-
uating the results, whenever a system produced a
tie between several suggestions, we generated 100
random orderings of the elements in the tie, and
report the average scores.
Using distributional similarities and the tempo-
ral series turned out to be indistinguishable for the
precision scores at 0.95 confidence, and both are
significantly better than the similarity metric based
on the web kernel. The combination produced an
improvement across all metrics, although not sta-
tistically significant at p=0.05.
This is quite a positive finding as the time series
method relies on stored information requiring only
simple and highly optimized lookups.
5 Application to query categorization
The results from the manual evaluation in Sec-
tion 3.2 support the conclusion that time series
from query logs provide powerful signals for clus-
tering at a fine-grained level, in some cases un-
covering synonyms (may 1st, may day) and even
causal relations (insulation, heating). A natural
question is if temporal information is correlated
with other types of categorizations. In this sec-
tion we carry out a preliminary exploration of the
relation between query time series and query cat-
egorization. To this extent we adapt the data from
the KDD 2005 CUP (Li et al, 2005), which pro-
vides a set of queries classified into 67 broad topi-
cal categories. Since the data is rather sparse (678
queries) we applied Fourier analysis to ?smooth?
the time series.
5.1 The KDD CUP data
The KDD Cup 2005
6
introduced a query catego-
rization task and dataset consisting of 800,000 un-
labeled queries for unsupervised training, and an
evaluation set of 911 queries, 111 for development
and 800 for the final evaluation. The systems sub-
mitted for this task can be quite complex and made
full use of the large unlabeled set. Our goal here is
not to provide a comparative evaluation, but only
6
http://www.sigkdd.org/kdd2005/kddcup.
html
?
101234567
TIME
STANDARDIZED FREQUENCY  
 
Figure 2: RDFT reconstruction for the query
?brush cutters? using the first 25 Fourier coeffi-
cients. The squares represent the original time
series datapoints, while the continuous line repre-
sents the reconstructed signal.
to use the labelled data
7
in a simplified manner to
better understand the semantic properties of query
time series. Each query in the dataset is assessed
by three editors who can assign multiple topic la-
bels from a set of 67 categories belonging to seven
broad topics: Computers, Entertainment, Informa-
tion, Living, Online Community, Shopping and
Sports. We merged the KDD Cup development
and test set, out of the 911 queries we were able to
retrieve significant temporal information for 678
queries. We joined the sets of labels from each as-
sessor for each query. On average, each query is
assigned five labels.
5.2 DFT analysis
Assessing the similarity of data represented as
time series has been addressed mostly my means
of Fourier analysis; e.g., Agrawal et al (1993) in-
troduce a method for efficiently retrieving time
series from databases based on Discrete Fourier
Transform (DFT). Several other methods have
been proposed, e.g., Discrete Wavelet Trans-
form (DWT), however DFT provide a competitive
benchmark approach (Wu et al, 2000).
We use DFT to generate the Fourier coefficients
of the time series and Reverse DFT (RDFT) to re-
construct the original signal using only a subset
of the coefficients. This analysis effectively com-
presses the time series producing a smoother ap-
proximate representation. DFT can be computed
efficiently via Fast Fourier Transform (FFT), with
7
The KDD Cup dataset is probably the only public query
log providing topical categorization information.
1052
Method Accuracy ? std-err
Random 0.107 0.03
MostFrequent 0.490 0.07
DFT-c10 0.425 0.06
DFT-c50 0.456 0.05
DFT-c100 0.502 0.05
DFT-c200 0.456 0.04
DFT-c400 0.506 0.05
DFT-c600 0.481 0.06
DFT-c800 0.478 0.04
DFT-c1000 0.466 0.05
Table 5: Results of the KDD dataset exploration.
complexityO(n log n) where n is the length of the
sequence. The approximate representation is use-
ful not only to address sparsity but can also be used
to efficiently estimate the similarity of two time
series using only a small subset of coefficients as
in (Agrawal et al, 1993). As an example, Fig-
ure 2 shows the original time series for the query
?brush cutters? and its reconstructed signal using
only the first 25 Fourier coefficients. The recon-
structed signal captures the essence of the period-
icity of the query and highlights the yearly peaks
registered for the query in spring and summer.
5.3 Experiment and discussion
To explore the correlation between the structured
temporal representation of queries provided by the
time series and topical categorization we run the
following experiment. Each KDD Cup query was
reconstructed via RDFT using a variable number
of coefficients. The set of 679 queries was parti-
tioned in 10 sets and a 10-fold evaluation was per-
formed. For each fold we trained a classifier on the
remaining 9 folds. We used an average multi-class
perceptron (Freund and Schapire, 1999) adapted to
multi-label learning (Crammer and Singer, 2003).
Each model was trained on a fixed number of 10
iterations. The accuracy of each model was eval-
uated as the fraction of test items for which the
selected highest scoring class was in the gold stan-
dard set provided by the editors. As a lower bound
we estimated the accuracy of randomly choosing
a label for each test instance, and as a baseline we
used the most frequent label. The latter is a pow-
erful predictor: baselines based on class frequency
outperformmost of the systems that participated in
the KDD Cup (Lin and Wu, 2009).
Table 5 reports the average accuracy over the
10 runs with relative standard errors. Each DFT-
based model is characterized by the number of co-
efficients used for the reconstruction. Two main
patterns are noticeable. First, none of the differ-
ences between the frequency-based baseline and
the DFT models is significant, this seems to indi-
cate that temporal structure alone is not a good dis-
criminator of topic, at least of broad categories. In
retrospect, this is somewhat predictable. The tem-
poral dimension is a basic semantic component of
lexical meaning and world knowledge which is not
necessarily associated with any broad, and to some
extent subjective, categorization. An inspection of
the patterns found in each category shows in fact
that similar patterns often emerge in different cat-
egories; e.g., ?Halloween costume? and ?cheese-
cake recipe? have a similar yearly periodical pat-
tern with spikes in early winter, while monotoni-
cally decaying patterns are shared across all cate-
gories; e.g., between computer hardware and kids
toys.
The second interesting finding is the trend of
the DFT system results, higher at low-intermediate
values, providing some initial promising evidence
that DFT analysis generates useful compressed
representations which could be indexed and ap-
plied efficiently. Notice that the sequences recon-
structed using 1,000 coefficients reproduce almost
identically the original signals.
6 Applications in information extraction
Time series from query logs are particularly rel-
evant for phrases that refer to entities which are
involved in recent events. Therefore, we expect
them to be useful for solving other applications
that require handling entities, such as named en-
tity recognition and classification, relation extrac-
tion or disambiguation.
To illustrate this point, we mention an example
of relation extraction between actors and movies:
movies usually have spikes when they are re-
leased, and then the frequency again drops sharply.
At the same times, when a movie is released, the
search engine users have a renewed interest in
their actors. Figure 3 displays the time series for
the five most recent movies by Jim Carrey (as of
march 2009), and the time series for Jim Carrey.
As can be seen, the spikes are at exactly the same
points in time. If we add up the series (a) through
(e) into a single series and calculate the correlation
with (f), it turns out to be very high (0.88).
1053
(a) (b) (c)
(d) (e) (f)
Figure 3: Time series obtained for the five most recent movies with Jim Carrey, and (f) time serie for the
query [jim carrey] (normalized scales).
System Precision Recall F-measure
Random 0.24 0.14 0.17
Time series 0.53 0.66 0.57
Table 6: Results for the query suggestion task.
To validate the hypothesis that this data should
be useful for identifying related entities, we have
performed a small experiment in the following
way: by choosing five popular actors
8
and the cin-
ema movies in which they appear since the year
2004, obtained from IMDB
9
. Using the time se-
ries, for each actor we choose the combination of
movies such that, by adding up the time series of
those movies, we maximise the correlation with
the actor?s time series. It has been implemented
with a greedy beam search, with a beam size of
100. The results are shown in Table 6. The random
baseline randomly associates the movies from the
dataset with the five actors.
We do not believe this to be a perfect feature as,
for example, actors may have a peak in the time se-
ries related to their personal lives, not necessarily
to movies. However, the high correlations that can
be obtained when the pairing between actors and
movies is correct, and the improvement with re-
spect a random baseline, indicates this is a feature
which can probably be integrated with other re-
lation extraction systems when handling relation-
ships between entities that have big temporal de-
pendencies.
8
Ben Stiller, Edward Norton, Jim Carrey, Leonardo Di-
caprio, and Tom Hanks.
9
www.imdb.com.
7 Conclusions and future work
This paper explores the relationships between
queries whose associated time series obtained
from query logs are highly correlated. The use
of time series in semantic similarity has been dis-
cussed by Chien (2005), but only a very prelimi-
nary evaluation was described, and, to our knowl-
edge, they had never been applied and evaluated
in solving existing problems. Our results indicate
that, for a substantial percentage of phrases in a
thesaurus, it is possible to find other highly-related
phrases; and we have categorized the kind of se-
mantic relationships that hold between them.
We have found that in a query suggestion
task, somewhat surprisingly, results are compara-
ble with other state-of-the-art techniques based on
distributional similarities. Furthermore, informa-
tion obtained from time series seems to be com-
plementary with them, as a simple combination of
similarity metrics produces an important increase
in performance..
From an analysis on a query categorization task
the initial evidence suggests that there is no strong
correlation between broad topics and temporal
profiles. This agrees with the intuition that time
provides a fundamental semantic dimension possi-
bly orthogonal to broad topical classification. This
issue however deserves further investigation. An-
other issue which is worth a deeper investigation
is the application of Fourier transform methods
which offer tools for studying the periodic struc-
ture of the temporal sequences.
1054
References
R. Agrawal, C. Faloutsos, and A.N. Swami. 1993. Ef-
ficient similarity search in sequence databases. In
Proceedings of the 4th International Conference on
Foundations of Data Organization and Algorithms,
pages 69?84.
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics
- Human Language Technologies conference.
N. Bansal and N. Koudas. 2007a. BlogScope: a sys-
tem for online analysis of high volume text streams.
In Proceedings of the 33rd international conference
on Very large data bases, pages 1410?1413.
N. Bansal and N. Koudas. 2007b. BlogScope: Spatio-
temporal analysis of the blogosphere. In Proceed-
ings of the 16th international conference on World
Wide Web, pages 1269?1270.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In Proceed-
ings of the sixth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 407?416.
S. Chien. 2005. Semantic similarity between search
engine queries using temporal correlation. In Pro-
ceedings of the 14th international conference on
World Wide Web, pages 2?11.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
S. Cucerzan and R.W. White. 2007. Query sugges-
tion based on user landing pages. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 875?876.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer,
M.S. Smolinski, and L. Brilliant. 2009. Detecting
influenza epidemics using search engine query data.
Nature, 457, February.
R. Jones and F. Diaz. 2007. Temporal profiles of
queries. ACM Transactions on Information Systems,
25(3):14.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proceedings of
the 15th international conference on World Wide
Web, pages 387?396.
J. Kleinberg. 2006. Temporal dynamics of on-line in-
formation streams. In Data Stream Management:
Processing High-Speed Data. Springer.
R. Kraft and J. Zien. 2004. Mining anchor text for
query refinement. In Proceedings of the 13th inter-
national conference on World Wide Web, pages 666?
674.
Y. Li, Z. Zheng, and H. Dai. 2005. KDD Cup-2005
report: Facing a grat challenge. SIGKDD Explor.
Newsl., 7(2):91?99.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing.
O. Medelyan, C. Legg, D. Milne, and I.H. Witten.
2008. Mining meaning from Wikipedia. Dept. of
Computer Science, University of Waikato.
Q. Mei, D. Zhou, and K. Church. 2008. Query sug-
gestion using hitting time. In Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 469?478.
G.A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?
41.
T. Murata. 2008. Detection of breaking news from
online web search queries. New Generation Com-
puting, 26(1):63?73.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international
conference on World Wide Web, pages 377?386.
E. Terra and C.L.A. Clarke. 2004. Scoring missing
terms in information retrieval tasks. In Proceedings
of the thirteenth ACM international conference on
Information and knowledge management, pages 50?
58.
D.S. Weld, F. Wu, E. Adar, S. Amershi, J. Fogarty,
R. Hoffmann, K. Patel, and M. Skinner. 2008. In-
telligence in Wikipedia. In Proceedings of the 23rd
Conference on Artificial Intelligence.
Y. Wu, D. Agrawal, and A. El Abbadi. 2000. A com-
parison of DFT and DWT based similarity search
in time-series databases. In Proceedings of the 9th
International ACM Conference on Information and
Knowledge Management, pages 488?495.
W. Yih and C. Meek. 2008. Consistent Phrase Rel-
evance Measures. Workshop on Data Mining and
Audience Intelligence for Advertising, page 37.
T. Zesch, C. Muller, and I. Gurevych. 2008a. Extract-
ing lexical semantic knowledge from Wikipedia and
Wiktionary. In Proceedings of the Conference on
Language Resources and Evaluation.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
Wiktionary for computing semantic relatedness. In
Proceedings of the Conference on Artificial Intelli-
gence, pages 861?867.
1055
Learning to Rank Answers to Non-Factoid
Questions fromWeb Collections
Mihai Surdeanu?
Stanford University
Massimiliano Ciaramita??
Google Inc.
Hugo Zaragoza?
Yahoo! Research
This work investigates the use of linguistically motivated features to improve search, in par-
ticular for ranking answers to non-factoid questions. We show that it is possible to exploit
existing large collections of question?answer pairs (from online social Question Answering sites)
to extract such features and train ranking models which combine them effectively. We investigate
a wide range of feature types, some exploiting natural language processing such as coarse word
sense disambiguation, named-entity identification, syntactic parsing, and semantic role label-
ing. Our experiments demonstrate that linguistic features, in combination, yield considerable
improvements in accuracy. Depending on the system settings we measure relative improvements
of 14% to 21% in Mean Reciprocal Rank and Precision@1, providing one of the most compelling
evidence to date that complex linguistic features such as word senses and semantic roles can have
a significant impact on large-scale information retrieval tasks.
1. Introduction
The problem of Question Answering (QA) has received considerable attention in the
past few years. Nevertheless, most of the work has focused on the task of factoid
QA, where questions match short answers, usually in the form of named or numerical
entities. Thanks to international evaluations organized by conferences such as the Text
REtrieval Conference (TREC) and the Cross Language Evaluation Forum (CLEF) Work-
shop, annotated corpora of questions and answers have become available for several
languages, which has facilitated the development of robust machine learning models
for the task.1
? Stanford University, 353 Serra Mall, Stanford, CA 94305?9010. E-mail: mihais@stanford.edu.
?? Google Inc., Brandschenkestrasse 110, CH?8002 Zu?rich, Switzerland. E-mail: massi@google.com.
? Yahoo! Research, Avinguda Diagonal 177, 8th Floor, 08018 Barcelona, Spain.
E-mail: hugoz@yahoo-inc.com.
1 TREC: http://trec.nist.gov; CLEF: http://www.clef-campaign.org.
The primary part of this work was carried out while all authors were working at Yahoo! Research.
Submission received: 1 April 2010; revised submission received: 11 September 2010; accepted for publication:
23 November 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 2
Table 1
Sample content from Yahoo! Answers.
High Q: How do you quiet a squeaky door?
Quality A: Spray WD-40 directly onto the hinges of the door. Open and close the door
several times. Remove hinges if the door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to removed hinges. Put the hinges back,
open and close door several times again.
High Q: How does a helicopter fly?
Quality A: A helicopter gets its power from rotors or blades. So as the rotors turn,
air flows more quickly over the tops of the blades than it does below.
This creates enough lift for flight.
Low Q: How to extract html tags from an html documents with c++?
Quality A: very carefully
The situation is different once one moves beyond the task of factoid QA. Com-
paratively little research has focused on QA models for non-factoid questions such
as causation, manner, or reason questions. Because virtually no training data is avail-
able for this problem, most automated systems train either on small hand-annotated
corpora built in-house (Higashinaka and Isozaki 2008) or on question?answer pairs
harvested from Frequently Asked Questions (FAQ) lists or similar resources (Soricut
and Brill 2006; Riezler et al 2007; Agichtein et al 2008). None of these situations is
ideal: The cost of building the training corpus in the former setup is high; in the latter
scenario the data tend to be domain-specific, hence unsuitable for the learning of open-
domain models, and for drawing general conclusions about the underlying scientific
problems.
On the other hand, recent years have seen an explosion of user-generated content
(or social media). Of particular interest in our context are community-driven question-
answering sites, such as Yahoo! Answers, where users answer questions posed by other
users and best answers are selected manually either by the asker or by all the partici-
pants in the thread.2 The data generated by these sites have significant advantages over
other Web resources: (a) they have a high growth rate and they are already abundant;
(b) they cover a large number of topics, hence they offer a better approximation of open-
domain content; and (c) they are available for many languages. Community QA sites,
similar to FAQs, provide a large number of question?answer pairs. Nevertheless, these
data have a significant drawback: they have high variance of quality (i.e., questions
and answers range from very informative to completely irrelevant or even abusive).
Table 1 shows some examples of both high and low quality content from the Yahoo!
Answers site.
In this article we investigate two important aspects of non-factoid QA:
1. Is it possible to learn an answer-ranking model for non-factoid questions, in a
completely automated manner, using data available in on-line social QA sites?
This is an interesting question because a positive answer indicates that a
plethora of training data are readily available to researchers and system
2 http://answers.yahoo.com.
352
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
developers working on natural language processing, information retrieval,
and machine learning.
2. Which features and models are more useful in this context, that is, ample but
noisy data? For example: Are similarity models as effective as models that
learn question-to-answer transformations? Does syntactic and semantic
information help?
Social QA sites are the ideal vehicle to investigate such questions. Questions posted
on these sites typically have a correct answer that is selected manually by users. As-
suming that all the other candidate answers are incorrect (we discuss this assumption
in Section 5), it is trivial to automatically organize these data into a format ready for
discriminative learning, namely, the pair question?correct answer generates one posi-
tive example and all other answers for the same question are used to generate negative
examples. This allows one to use the collection in a completely automated manner to
learn answer ranking models.
The contributions of our investigation are the following:
1. We introduce and evaluate many linguistic features for answer re-ranking.
Although several of these features have been introduced in previous work,
some are novel in the QA context, for example, syntactic dependencies
and semantic role dependencies with words generalized to semantic tags.
Most importantly, to the best of our knowledge this is the first work that
combines all these features into a single framework. This allows us to
investigate their comparative performance in a formal setting.
2. We propose a simple yet powerful representation for complex linguistic
features, that is, we model syntactic and semantic information as bags of
syntactic dependencies or semantic role dependencies and build similarity
and translation models over these representations. To address sparsity,
we incorporate a back-off approach by adding additional models where
lexical elements in these structures are generalized to semantic tags.
These models are not only simple to build, but, as our experiments
indicate, they perform at least as well as complex, dedicated models such
as tree kernels.
3. We are the first to evaluate the impact of such linguistic features in a
large-scale setting that uses real-world noisy data. The impact on QA of
some of the features we propose has been evaluated before, but these
experiments were either on editorialized data enhanced with gold
semantic structures (e.g., the Wall Street Journal corpus with semantic
roles from PropBank [Bilotti et al 2007]), or on very few questions
(e.g., 413 questions from TREC 12 [Cui et al 2005]). On the other hand,
we evaluate on over 25,000 questions, and each question has up to
100 candidate answers from Yahoo! Answers. All our data are processed
with off-the-shelf natural language (NL) processors.
The article is organized as follows. We describe our approach, including all the
features explored for answer modeling, in Section 2. We introduce the corpus used in
our empirical analysis in Section 3. We detail our experiments and analyze the results
353
Computational Linguistics Volume 37, Number 2
in Section 4. Section 5 discusses current shortcomings of our system and proposes
solutions. We overview related work in Section 6 and conclude the article in Section 7.
2. Approach
Figure 1 illustrates our QA architecture. The processing flow is the following. First, the
answer retrieval component extracts a set of candidate answers A for a question Q
from a large collection of answers, C, provided by a community-generated question-
answering site. The retrieval component uses a state-of-the-art information retrieval
(IR) model to extract A given Q. The second component, answer ranking, assigns to
each answer Ai ? A a score that represents the likelihood that Ai is a correct answer
for Q, and ranks all answers in descending order of these scores. In our experiments,
the collection C contains all answers previously selected by users of a social QA site
as best answers for non-factoid questions of a certain type (e.g., ?How to? questions).
The entire collection of questions, Q, is split into a training set and two held-out sets:
a development one used for parameter tuning, and a testing one used for the formal
evaluation.
Our architecture follows closely the architectures proposed in the TREC QA track
(see, e.g., Voorhees 2001). For efficiency reasons, most participating systems split the
answer extraction phase into a retrieval phase that selected likely answer snippets
using shallow techniques, followed by a (usually expensive) answer ranking phase
that processes only the candidates proposed by the retrieval component. Due to this
separation, such architectures can scale to collections of any size. We discuss in Section 6
how related work has improved this architecture further?for example, by adding
query expansion terms from the translation models back to answer retrieval (Riezler
et al 2007).
The focus of this work, however, is on the re-ranking model implemented in the
answer ranking component. We call this model FMIX?from feature mix?because the
proposed scoring function is a linear combination of four different classes of features
Figure 1
Architecture of our QA framework.
354
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
(detailed in Section 2.2). To accommodate and combine all these feature classes, our
QA approach combines three types of machine learning methodologies (as highlighted
in Figure 1): the answer retrieval component uses unsupervised IR models, the an-
swer ranking is implemented using discriminative learning, and finally, some of the
ranking features are produced by question-to-answer translation models, which use
class-conditional generative learning. To our knowledge, this combined approach is
novel in the context of QA. In the remainder of the article, we will use the FMIX
function to answer the research objectives outlined in the Introduction. To answer the
first research objective we will compare the quality of the rankings provided by this
component against the rankings generated by the IRmodel used for answer retrieval. To
answer the second research objective we will analyze the contribution of the proposed
feature set to this function.
We make some simplifying assumptions in this study. First, we will consider only
manner questions, and in particular only ?How to? questions. This makes the corpus
more homogeneous and more focused on truly informational questions (as opposed
to social questions such as ?Why don?t girls like me??, or opinion questions such as
?Who will win the next election??, both of which are very frequent in Yahoo! Answers).
Second, we concentrate on the task of answer-re-ranking, and ignore all other modules
needed in a complete on-line social QA system. For example, we ignore the problem
of matching questions to questions, very useful when retrieving answers in a FAQ or
a QA collection (Jeon, Croft, and Lee 2005), and we ignore all ?social? features such as
the authority of users (Jeon et al 2006; Agichtein et al 2008). Instead, we concentrate on
matching answers and on the different textual features. Hence, the document collection
used in our experiments contains only answers, without the corresponding questions
answered. Furthermore, we concentrate on the re-ranking phase and we do not explore
techniques to improve the recall of the initial retrieval phase (by methods of query
expansion, for example). Such aspects are complementary to our work, and can be
investigated separately.
2.1 Representations of Content
One of our main interests in using very large data sets was to show that complex lin-
guistic features can improve rankingmodels if they are correctly combinedwith simpler
features, in particular using discriminative learning methods on a particular task. For
this reason we explore several forms of textual representation going beyond the bag
of words. In particular, we generate our features over four different representations
of text:
Words (W): This is the traditional IR view where the text is seen as a bag of words.
n-grams (N): The text is represented as a bag of word n-grams, where n ranges from two
up to a given length (we discuss structure parameters in the following).
Dependencies (D): The text is converted to a bag of syntactic dependency chains. We
extract syntactic dependencies in the style of the CoNLL-2007 shared task using the
syntactic processor described in Section 3.3 From the tree of syntactic dependencies we
extract all the paths up to a given length following modifier-to-head links. The top part
3 http://depparse.uvt.nl/depparse-wiki/SharedTaskWebsite.
355
Computational Linguistics Volume 37, Number 2
Figure 2
Sample syntactic dependencies and semantic tags.
Figure 3
Sample semantic proposition.
of Figure 2 shows a sample corpus sentence with the actual syntactic dependencies ex-
tracted by our syntactic processor. The figure indicates that this representation captures
important syntactic relations, such as subject?verb (e.g., helicopter
SBJ??? gets) or object-
verb (e.g., power
OBJ??? gets).
Semantic Roles (R): The text is represented as a bag of predicate?argument rela-
tions extracted using the semantic parser described in Section 3. The parser follows the
PropBank notations (Palmer, Gildea, and Kingsbury 2005), that is, it assigns semantic
argument labels to nodes in a constituent-based syntactic tree. Figure 3 shows an exam-
ple. The figure shows that the semantic proposition corresponding to the predicate gets
includes A helicopter as the Arg0 argument (Arg0 stands for agent), its power as the Arg1
argument (or patient), and from rotors or blades as Arg2 (or instrument). Semantic roles
have the advantage that they extract meaning beyond syntactic representations (e.g., a
syntactic subject may be either an agent or a patient in the actual proposition). We con-
vert the semantic propositions detected by our parser into semantic dependencies using
the same approach as Surdeanu et al (2008), that is, we create a semantic dependency
between each predicate and the syntactic head of every one of its arguments. These
dependencies are labeled with the label of the corresponding argument. For example,
the semantic dependency that includes the Arg0 argument in Figure 3 is represented as
gets
Arg0
??? helicopter. If the syntactic constituent corresponding to a semantic argument is
356
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
a prepositional phrase (PP), we convert it to a bigram that includes the preposition and
the head word of the attached phrase. For example, the tuple for Arg2 in the example is
represented as gets
Arg2
??? from-rotors.
In all representations we remove structures where either one of the elements is a
stop word and convert the remaining words to their WordNet lemmas.4
The structures we propose are highly configurable. In this research, we investigate
this issue along three dimensions:
Degree of lexicalization:We reduce the sparsity of the proposed structures by replacing
the lexical elements with semantic tags which might provide better generalization. In
this article we use two sets of tags, the first consisting of coarse WordNet senses, or
supersenses (WNSS) (Ciaramita and Johnson 2003), and the second of named-entity
labels extracted from the Wall Street Journal corpus. We present in detail the tag sets
and the processors used to extract them in Section 3. For an overview, we show a sample
annotated sentence in the bottom part of Figure 2.
Labels of relations: Both dependency and predicate?argument relations can be labeled
or unlabeled (e.g., gets
Arg0
??? helicopter versus gets? helicopter). We make this distinction
in our experiments for two reasons: (a) removing relation labels reduces the model
sparsity because fewer elements are created, and (b) performing relation recognition
without classification is simpler than performing the two tasks, so the corresponding
NL processors might be more robust in the unlabeled-relation setup.
Structure size: This parameter controls the size of the generated structures, namely,
number of words in n-grams or dependency chains, or number of elements in the
predicate?argument tuples. Nevertheless, in our experiments we did not see any im-
provements from structure sizes larger than two. In the experiments reported in this
article, all the structures considered are of size two, that is, we use bigrams, dependency
chains of two elements, and tuples of one predicate and one semantic argument.
2.2 Features
We explore a rich set of features inspired by several state-of-the-art QA systems
(Harabagiu et al 2000; Magnini et al 2002; Cui et al 2005; Soricut and Brill 2006; Bilotti
et al 2007; Ko, Mitamura, and Nyberg 2007). To the best of our knowledge this is the
first work that: (a) adapts all these features for non-factoid answer ranking, (b) combines
them in a single scoring model, and (c) performs an empirical evaluation of the different
feature families and their combinations.
For clarity, we group the features into four sets: features that model the similarity
between questions and answers (FG1), features that encode question-to-answer trans-
formations using a translation model (FG2), features that measure keyword density and
frequency (FG3), and features that measure the correlation between question?answer
pairs and other collections (FG4). Wherever applicable, we explore different syntactic
and semantic representations of the textual content, as introduced previously. We next
explain in detail each of these feature groups.
4 http://wordnet.princeton.edu.
357
Computational Linguistics Volume 37, Number 2
FG1: Similarity Features. We measure the similarity between a question Q and an
answer A using the length-normalized BM25 formula (Robertson and Walker 1997),
which computes the score of the answer A as follows:
BM25(A) =
|Q|
?
i=0
(k1 + 1)tf
A
i (k3 + 1)tf
Q
i
(K+ tf Ai )(k3 + tf
Q
i )
log(idfi) (1)
where tf Ai and tf
Q
i are the frequencies of the question term i in A and Q, and idfi is
the inverse document frequency of term i in the answer collection. K is the length-
normalization factor:
K = k1((1? b)+ b|A|/avg len)
where avg len is the average answer length in the collection. For all the constants in the
formula (b, k1, and k3) we use values reported optimal for other IR collections (b = 0.75,
k1 = 1.2, and k3 = 1, 000).
We chose this similarity formula because, of all the IR models we tried, it provided
the best ranking at the output of the answer retrieval component. For completeness
we also include in the feature set the value of the tf ? idf similarity measure. For both
formulas we use the implementations available in the Terrier IR platform with the
default parameters.5
To understand the contribution of our syntactic and semantic processors we com-
pute the similarity features for different representations of the question and answer
content, ranging from bag of words to semantic roles. We detail these representations in
Section 2.1.
FG2: Translation Features. Berger et al (2000) showed that similarity-based models
are doomed to perform poorly for QA because they fail to ?bridge the lexical chasm?
between questions and answers. One way to address this problem is to learn question-
to-answer transformations using a translation model (Berger et al 2000; Echihabi and
Marcu 2003; Soricut and Brill 2006; Riezler et al 2007). In our model, we incorporate this
approach by adding the probability that the question Q is a translation of the answer
A, P(Q|A), as a feature. This probability is computed using IBM?s Model 1 (Brown et al
1993):
P(Q|A) =
?
q?Q
P(q|A) (2)
P(q|A) = (1? ?)Pml(q|A)+ ?Pml(q|C) (3)
Pml(q|A) =
?
a?A
(T(q|a)Pml(a|A)) (4)
where the probability that the question term q is generated from answer A, P(q|A),
is smoothed using the prior probability that the term q is generated from the entire
collection of answers C, Pml(q|C). ? is the smoothing parameter. Pml(q|C) is computed
5 http://ir.dcs.gla.ac.uk/terrier.
358
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
using the maximum likelihood estimator. To mitigate sparsity, we set Pml(q|C) to a small
value for out-of-vocabulary words.6 Pml(q|A) is computed as the sum of the probabilities
that the question term q is a translation of an answer term a, T(q|a), weighted by the
probability that a is generated fromA. The translation table for T(q|a) is computed using
the EM algorithm implemented in the GIZA++ toolkit.7
Translation models have one important limitation when used for retrieval tasks:
They do not guarantee that the probability of translating a word to itself, that is, T(w|w),
is high (Murdock and Croft 2005). This is a problem for QA, where word overlap
between question and answer is a good indicator of relevance (Moldovan et al 1999).
We address this limitation with a simple algorithm: we set T(w|w) = 0.5 and re-scale
the other T(w?|w) probabilities for all other words w? in the vocabulary to sum to 0.5, to
guarantee that
?
w? T(w
?|w) = 1. This has the desired effect that T(w|w) becomes larger
than any other T(w?|w). Our initial experiments proved empirically that this is essential
for good performance.
As prior work indicates, tuning the smoothing parameter ? is also crucial for the
performance of translation models, especially in the context of QA (Xue, Jeon, and
Croft 2008). We tuned the ? parameter independently for each of the translation models
introduced as follows: (a) for a smaller subset of the development corpus introduced
in Section 3 (1,500 questions) we retrieved candidate answers using our best retrieval
model (BM25); (b) we implemented a simple re-ranking model using as the only feature
the translation model probability; and (c) we explored a large range of values for ?
and selected the one that maximizes the mean reciprocal rank (MRR) of the re-ranking
model. This process selected a wide range of values for the ? parameter for the different
translation models (e.g., 0.09 for the translation model over labeled syntactic depen-
dencies, and 0.43 for the translation model over labeled semantic role dependencies).
Similarly to the previous feature group, we add translation-based features for the
different text representations detailed in Section 2.1. By moving beyond the bag-of-
words representation we hope to learn relevant transformations of structures, for ex-
ample, from the squeaky? door dependency to spray?WD-40 in the Table 1 example.
FG3: Density and Frequency Features. These features measure the density and fre-
quency of question terms in the answer text. Variants of these features were used
previously for either answer or passage ranking in factoid QA (Moldovan et al 1999;
Harabagiu et al 2000). Tao and Zhai (2007) evaluate a series of proximity-based mea-
sures in the context of information retrieval.
Same word sequence: Computes the number of non-stop question words that are
recognized in the same order in the answer.
Answer span: The largest distance (in words) between two non-stop question words in
the answer. We compute multiple variants of this feature, where we count: (a) the total
number of non-stop words in the span, or (b) the number of non-stop nouns.
Informativeness: Number of non-stop nouns, verbs, and adjectives in the answer text
that do not appear in the question.
6 We used 1E-9 for the experiments in this article.
7 http://www.fjoch.com/GIZA++.html.
359
Computational Linguistics Volume 37, Number 2
Same sentencematch:Number of non-stop question termsmatched in a single sentence
in the answer. This feature is added both unnormalized and normalized by the question
length.
Overall match: Number of non-stop question terms matched in the complete answer.
All these features are computed as raw counts and as normalized counts (dividing
the count by the question length, or by the answer length in the case of Answer span).
The last two features (Same sentence match and Overall match) are computed for all
text representations introduced, including syntactic and semantic dependencies (see
Section 2.1).
Note that counting the number of matched syntactic dependencies is essentially
a simplified tree kernel for QA (e.g., see Moschitti et al 2007) matching only trees of
depth 2. We also include in this feature group the following tree-kernel features.
Tree kernels: Tomodel larger syntactic structures that are shared between questions and
answers we compute the tree kernel values between all question and answer sentences.
We implemented a dependency-tree kernel based on the convolution kernels proposed
by Collins and Duffy (2001). We add as features the largest value measured between
any two individual sentences, as well as the average of all computed kernel values for
a given question and answer. We compute tree kernels for both labeled and unlabeled
dependencies, and for both lexicalized trees and for trees where words are generalized
to their predicted WNSS or named-entity tags (when available).
FG4: Web Correlation Features. Previous work has shown that the redundancy of a
large collection (e.g., the Web) can be used for answer validation (Brill et al 2001;
Magnini et al 2002). In the same spirit, we add features that measure the correlation
between question?answer pairs and large external collections:
Web correlation:Wemeasure the correlation between the question?answer pair and the
Web using the Corrected Conditional Probability (CCP) formula of Magnini et al (2002):
CCP(Q,A) = hits(Q+ A)/(hits(Q) hits(A)2/3) (5)
where hits returns the number of page hits from a search engine. The hits procedure
constructs a Boolean query from the given set of terms, represented as a conjunction of
all the corresponding keywords. For example, for the second question in Table 1, hits(Q)
uses the Boolean query: helicopter AND fly.
It is notable that this formula is designed for Web-based QA, that is, the conditional
probability is adjusted with 1/hits(A)2/3 to reduce the number of cases when snippets
containing high-frequency words are marked as relevant answers. This formula was
shown to perform best for the task of QA (Magnini et al 2002). Nevertheless, this
formula was designed for factoid QA, where both the question and the exact answer
have a small number of terms. This is no longer true for non-factoid QA. In this context
it is likely that the number of hits returned forQ, A, orQ+ A is zero given the large size
of the typical question and answer. To address this issue, wemodified the hits procedure
to include a simple iterative query relaxation algorithm:
1. Assign keyword priorities using a set of heuristics inspired by
Moldovan et al (1999). The complete priority detection algorithm
is listed in Table 2.
360
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 2
Keyword priority heuristics.
Step Keyword type Priority
(a) Non-stop keywords within quotes 8
(b) Non-stop keywords tagged as proper nouns 7
(c) Contiguous sequences of 2+ adjectives as nouns 6
(d) Contiguous sequences of 2+ nouns 5
(e) Adjectives not assigned in step (c) 4
(f) Nouns not assigned in steps (c) or (d) 3
(g) Verbs and adverbs 2
(h) Non-stop keywords not assigned in the previous steps 1
2. Fetch the number of page hits using the current query.
3. If the number of hits is larger than zero, stop; otherwise discard the set of
keywords with the smallest priority in the current query and repeat from
step 2.
Query-log correlation: As in Ciaramita, Murdock, and Plachouras (2008), we also com-
pute the correlation between question?answer pairs from a search-engine query-log
corpus of more than 7.5 million queries, which shares roughly the same time stamp
with the community-generated question?answer corpus. Using the query-log correla-
tion between two snippets of text was shown to improve performance for contextual
advertising, that is, linking a user?s query to the description of an ad (Ciaramita,
Murdock, and Plachouras 2008). In this work, we adapt this idea to the task of QA.
However, because it is not clear which correlation metric performs best in this context,
we compute both the Pointwise Mutual Information (PMI) and chi square (?2) associ-
ation measures between each question?answer word pair in the query-log corpus. The
largest and the average values are included as features, as well as the number of QA
word pairs which appear in the top 10, 5, and 1 percentile of the PMI and ?2 word pair
rankings.
We replicate all features that can be computed for different content representations
using every independent representation and parameter combination introduced in
Section 2.1. For example, we compute similarity scores (FG1) for 16 different repre-
sentations of question/answer content, produced by different parametrizations of the
four different generic representations (W, N, D, R). One important exception to this
strategy are the translation-model features (FG2). Because our translation models aim
to learn both lexical and structural transformations between questions and answers,
it is important to allow structural variations in the question/answer representations.
In this article, we implement a simple and robust approximation for this purpose: For
translation models we concatenate all instances of structured representations (N, D, R)
with the corresponding bag-of-words representation (W). This allows the translation
models to learn some combined lexical and structural transformation (e.g., from the
dependency squeaky? door dependency to the tokenWD-40). All in all, replicating our
features for all the different content representations yields 137 actual features to be used
for learning.
361
Computational Linguistics Volume 37, Number 2
2.3 Ranking Models
Our approach is agnostic with respect to the actual learning model. To emphasize this,
we experimented with two learning algorithms. First, we implemented a variant of the
ranking Perceptron proposed by Shen and Joshi (2005). In this framework the ranking
problem is reduced to a binary classification problem. The general idea is to exploit the
pairwise preferences induced from the data by training on pairs of patterns, rather than
independently on each pattern. Given a weight vector ?, the score for a pattern x (a
candidate answer) is given by the inner product between the pattern and the weight
vector:
f?(x) = ?x,?? (6)
However, the error function depends on pairwise scores. In training, for each pair
(xi, xj) ? A, the score f?(xi ? xj) is computed; note that if f is an inner product f?(xi ?
xj) = f?(xi)? f?(xj). In this framework one can define suitable margin functions that
take into account different levels of relevance; for example, Shen and Joshi (2005)
propose g(i, j) = ( 1i ?
1
j ), where i and j are the rank positions of xi and xj. Because in
our case there are only two relevance levels we use a simpler sign function yi,j, which
is negative if i > j and positive otherwise; yi,j is then scaled by a positive rate ? found
empirically on the development data. In the presence of numbers of possible rank levels
appropriate margin functions can be defined. During training, if f?(xi ? xj) ? yi,j?, an
update is performed as follows:
?t+1 = ?t + (xi ? xj)yi,j? (7)
We notice, in passing, that variants of the perceptron including margins have been
investigated before; for example, in the context of uneven class distributions (see Li et al
2002). It is interesting to notice that such variants have been found to be competitive
with SVMs in terms of performance, while being more efficient (Li et al 2002; Surdeanu
and Ciaramita 2007). The comparative evaluation from our experiments are consistent
with these findings. For regularization purposes, we use as a final model the average of
all Perceptron models posited during training (Freund and Schapire 1999).
We also experimented with SVM-rank (Joachims 2006), which is an instance of
structural SVM?a family of Support Vector Machine algorithms that model structured
outputs (Tsochantaridis et al 2004)?specifically tailored for ranking problems.8 SVM-
rank optimizes the area under a ROC curve. The ROC curve is determined by the true
positive rate vs. the false positive rate for varying values of the prediction threshold,
thus providing a metric closely related to Mean Average Precision (MAP).
3. The Corpus
The corpus is extracted from a sample of the U.S. Yahoo! Answers questions and
answers. We focus on the subset of advice or ?how to? questions due to their fre-
quency, quality, and importance in social communities. Nevertheless, our approach
8 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html.
362
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
is independent of the question type. To construct our corpus, we implemented the
following successive filtering steps:
Step 1: From the full corpus we keep only questions that match the regular
expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by the asker or by the
participants in the thread. The outcome of this step is a set of
364,419 question?answer pairs.
Step 2: From this corpus we remove the questions and answers of dubious
quality. We implement this filter with a simple heuristic by keeping
only questions and answers that have at least four words each, out
of which at least one is a noun and at least one is a verb. The
rationale for this step is that genuine answers to ?how to? questions
should have a minimal amount of structure, approximated by the
heuristic. This step filters out questions like How to be excellent? and
answers such as I don?t know. The outcome of this step forms our
answer collection C. C contains 142,627 question?answer pairs.
This corpus is freely available through the Yahoo! Webscope
program.9
Arguably, all these filters could be improved. For example, the first step can be
replaced by a question classifier (Li and Roth 2006). Similarly, the second step can be
implemented with a statistical classifier that ranks the quality of the content using
both the textual and non-textual information available in the database (Jeon et al 2006;
Agichtein et al 2008). We plan to further investigate these issues, which are not the
main object of this work.
The data was processed as follows. The text was split at the sentence level, token-
ized and POS tagged, in the style of the Wall Street Journal Penn TreeBank (Marcus,
Santorini, and Marcinkiewicz 1993). Each word was morphologically simplified using
the morphological functions of the WordNet library. Sentences were annotated with
WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates
text with a 46-label tagset.10 These tags, defined by WordNet lexicographers, provide
a broad semantic categorization for nouns and verbs and include labels for nouns such
as food, animal, body, and feeling, and for verbs labels such as communication, contact,
and possession. We chose to annotate the data with this tagset because it is less biased
towards a specific domain or set of semantic categories than, for example, a named-
entity tagger. Using the same tagger as before we also annotated the text with a named-
entity tagger trained on the BBNWall Street Journal (WSJ) Entity Corpus which defines
105 categories for entities, nominal concepts, and numerical types.11 See Figure 2 for a
sample sentence annotated with these tags.
Next, we parsed all sentences with the dependency parser of Attardi et al (2007).12
We chose this parser because it is fast and it performed very well in the domain adap-
tation shared task of CoNLL 2007. Finally, we extracted semantic propositions using
9 You can request the corpus by email at research-data-requests@yahoo-inc.com. More information
about this corpus can be found at: http://www.yr-bcn.es/MannerYahooAnswers.
10 http://sourceforge.net/projects/supersensetag.
11 LDC catalog number LDC2005T33.
12 http://sourceforge.net/projects/desr.
363
Computational Linguistics Volume 37, Number 2
the SwiRL semantic parser of Surdeanu et al (2007).13 SwiRL starts by syntactically
analyzing the text using a constituent-based full parser (Charniak 2000) followed by a
semantic layer, which extracts PropBank-style semantic roles for all verbal predicates in
each sentence.
It is important to realize that the output of all mentioned processing steps is noisy
and contains plenty of mistakes, because the data have huge variability in terms of
quality, style, genres, domains, and so forth. In terms of processing speed, both the
semantic tagger of Ciaramita and Altun and the Attardi et al parser process 100+
sentences/second. The SwiRL system is significantly slower: On average, it parses less
than two sentences per second. However, recent research showed that this latter task
can be significantly sped up without loss of accuracy (Ciaramita et al 2008).
We used 60% of the questions for training, 20% for development, and 20% for test-
ing. Our ranking model was tuned strictly on the development set for feature selection
(described later) and the ? parameter of the translation models. The candidate answer
set for a given question is composed of one positive example, that is, its corresponding
best answer, and as negative examples all the other answers retrieved in the top N by
the retrieval component.
4. Experiments
We used several measures to evaluate our models. Recall that we are using an initial
retrieval engine to select a pool of N answer candidates (Figure 1), which are then re-
ranked. This couples the performance of the initial retrieval engine and the re-rankers.
We tried to de-couple them in our performance measures, as follows. We note that if
the initial retrieval engine does not rank the correct answer in the pool of top N results,
it is impossible for any re-ranker to do well. We therefore follow the approach of Ko
et al (2007) and define performance measures only with respect to the subset of pools
which contain the correct answer for a given N.
This complicates slightly the typical notions of recall and precision. Let us callQ the
set of all queries in the collection and QN the subset of queries for which the retrieved
answer pool of size N contains the correct answer. We will then use the following
performance measure definitions:
Retrieval Recall@N: The usual recall definition:
|QN|
|Q| . This is equal for all re-rankers.
Re-ranking Precision@1: Average Precision@1 over the QN set, where the Precision@1
of a query is defined as 1 if the correct answer is re-ranked into the first position,
0 otherwise.
Re-ranking MRR: MRR over theQN set, where the reciprocal rank is the inverse of the
rank of the correct answer.
Note that as N gets larger, QN grows in size, increasing the Retrieval Recall@N but
also increasing the difficulty of the task for the re-ranker, and therefore decreasing Re-
ranking Precision@1 and Re-ranking MRR.
During training of the FMIX re-ranker, the presentation of the training instances is
randomized, which defines a randomized training protocol producing different models
with each permutation of the data. We exploit this property to estimate the variance on
the experimental results by reporting the average performance of 10 different models,
together with an estimate of the standard deviation.
13 http://swirl-parser.sourceforge.net.
364
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 3
Re-ranking evaluation for Perceptron and SVM-rank. Improvement indicates relative
improvement over the baseline.
N = 15 N = 25 N = 50 N = 100
Retrieval Recall@N 29.04% 32.81% 38.09% 43.42%
Re-ranking Precision@1
Baseline 41.48 36.74 31.66 27.75
FMIX (Perceptron) 49.87?0.03 44.48?0.03 38.53?0.11 33.72?0.05
FMIX (SVM-rank) 49.48 44.10 38.18 33.52
Improvement (Perceptron) +20.22% +21.06% +21.69% +21.51%
Improvement (SVM-rank) +19.28% +20.03% +20.59% +20.79%
Re-ranking MRR
Baseline 56.12 50.31 43.74 38.53
FMIX (Perceptron) 64.16?0.01 58.20?0.01 51.19?0.07 45.29?0.05
FMIX (SVM-rank) 63.81 57.89 50.93 45.12
Improvement (Perceptron) +14.32% +15.68% +17.03% +17.54%
Improvement (SVM-rank) +13.70% +15.06% +16.43% +17.10%
The initial retrieval engine used to select the pool of candidate answers is the BM25
score as described earlier. This is also our baseline re-ranker. We will compare this to the
FMIX re-ranker using all features or using subsets of features.
4.1 Overall Results
Table 3 and Figure 4 show the results obtained using FMIX and the baseline for in-
creasing values of N. We report results for Perceptron and SVM-rank using the optimal
feature set for each (we discuss feature selection in the next sub-section).
Looking at the first column in Table 3 we see that a good bag-of-words representa-
tion alone (BM25 in this case) can achieve 41.5% Precision@1 (for the 29.0% of queries for
Figure 4
Re-ranking evaluation; precision-recall curve.
365
Computational Linguistics Volume 37, Number 2
which the retrieval engine can find an answer in the top N = 15 results). These baseline
results are interesting because they indicate that the problem is not hopelessly hard, but
it is far from trivial. In principle, we see much room for improvement over bag-of-words
methods. Indeed, the FMIX re-ranker greatly improves over the baseline. For example,
the FMIX approach using Perceptron yields a Precision@1 of 49.9%, a 20.2% relative
increase.
SettingN to a higher valuewe see recall increase at the expense of precision. Because
recall depends only on the retrieval engine and not on the re-ranker, what we are
interested in is the relative performance of our re-rankers for increasing numbers of
N. For example, setting N = 100 we observe that the BM25 re-ranker baseline obtains
27.7% Precision@1 (for the 43.4% of queries for which the best answer is found in the
top N = 100). For this same subset, the FMIX re-ranker using Perceptron obtains 33.7%
Precision@1, a 21.5% relative improvement over the baseline model.
The FMIX system yields a consistent and significant improvement for all values
of N, regardless of the type of learning algorithm used. As expected, as N grows the
precision of both re-rankers decreases, but the relative improvement holds or increases.
This can be seen most clearly in Figure 4 where re-ranking Precision and MRR are
plotted against retrieval Recall. Recalling that the FMIX model was trained only once,
using pools of N = 15, we can note that the training framework is stable at increasing
sizes of N.
Table 3 and Figure 4 show that the two FMIX variants (Perceptron and SVM-rank)
yield scores that are close (e.g., Precision@1 scores are within 0.5% of each other). We
hypothesize that the small difference between the two different learning models is
caused by our greedy tuning procedures (described in the next section), which converge
to slightly different solutions due to the different learning algorithms. Most importantly,
the fact that we obtain analogous results with two different learningmodels underscores
the robustness of our approach and of our feature set.
These overall results provide strong evidence that: (a) readily available and scalable
NLP technology can be used to improve lexical matching and translation models for
retrieval and QA tasks, (b) we can use publicly available online QA collections to
investigate features for answer ranking without the need for costly human evaluation,
and (c) we can exploit large and noisy on-line QA collections to improve the accuracy of
answer ranking systems. In the remainder of this section we analyze the performance
of the different features.
4.2 Contribution of Feature Groups
In order to gain some insights about the effectiveness of the different features groups,
we carried out a greedy feature selection procedure. We implemented similar processes
for Perceptron and SVM-rank, to guarantee that our conclusions are not biased by a
particular learning model.
4.2.1 Perceptron. We initialized the feature selection process with a single feature that
replicates the baseline model (BM25 applied to the bag-of-words [W] representation).
Then the algorithm incrementally adds to the feature set the single feature that provides
the highest MRR improvement in the development partition. The process stops when
no features yield any improvement. Note that this is only a heuristic process, and needs
to be interpreted with care. For example, if two features were extremely correlated, the
algorithm would choose one at random and discard the other. Therefore, if a feature is
366
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
missing from the selection process it means that it is either useless, or strongly correlated
with other features in the list.
Table 4 summarizes the outcome of this feature selection process. Where applicable,
we show within parentheses the text representation for the corresponding feature: W
for words, N for n-grams, D for syntactic dependencies, and R for semantic roles. We
use subscripts to indicate if the corresponding representation is fully lexicalized (no
subscript), or its elements are replaced by WordNet supersenses (WNSS) or named-
entity tags (WSJ). Where applicable, we use the l superscript to indicate if the cor-
responding structures are labeled. No superscript indicates unlabeled structures. For
example, DWNSS stands for unlabeled syntactic dependencies where the participating
tokens are replaced by their WordNet supersense; RlWSJ stands for semantic tuples of
predicates and labeled arguments with the words replaced with the corresponding WSJ
named-entity tags.
The table shows that, although the features selected span all the four feature groups
introduced, the lion?s share is taken by the translation features (FG2): 75% of the MRR
improvement is achieved by these features. The frequency/density features (FG3) are
responsible for approximately 16% of the improvement. The rest is due to the query-log
correlation features (FG4). This indicates that, even though translation models are the
most useful, it is worth exploring approaches that combine several strategies for answer
ranking.
As we noted before, many features may be missing from this list simply because
they are strongly correlated with others. For example most similarity features (FG1) are
correlated with BM25(W); for this reason the selection process does not choose a FG1
feature until iteration 9. On the other hand, some features do not provide a useful signal
Table 4
Summary of the model selection process using Perceptron.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.14
1 + translation(R) FG2 61.18 46.33
2 + translation(N) FG2 62.49 47.97
3 + overall match(DWNSS) FG3 63.07 48.93
4 + translation(W) FG2 63.27 49.12
5 + query-log avg(PMI) FG4 63.57 49.56
6 + overall match(W) FG3 63.72 49.74
7 + overall match(W), normalized by Q size FG3 63.82 49.89
8 + same word sequence, normalized by Q size FG3 63.90 49.94
9 + BM25(N) FG1 63.98 50.00
10 + informativeness: verb count FG3 64.16 49.97
11 + query-log max(PMI) FG4 64.37 50.28
12 + same sentence match(W) FG3 64.42 50.40
13 + overall match(NWSJ) FG3 64.49 50.51
14 + query-log max(?2) FG4 64.56 50.59
15 + same word sequence FG3 64.66 50.72
16 + BM25(RWSJ) FG1 64.68 50.78
17 + translation(RlWSJ) FG2 64.71 50.75
18 + answer span, normalized by A size FG3 64.76 50.80
19 + query-log top10(?2) FG4 64.89 51.06
20 + tree kernel(DWSJ) FG3 64.93 51.07
21 + translation(RWNSS) FG2 64.95 51.16
367
Computational Linguistics Volume 37, Number 2
at all. A notable example in this class is theWeb-based CCP feature, which was designed
originally for factoid answer validation and does not adapt well to our problem. To test
this, we learned a model with BM25 and the Web-based CCP feature only, and this
model did not improve over the baseline model at all. We hypothesize that because the
length of non-factoid answers is typically significantly larger than in the factoid QA
task, we have to discard a large part of the query when computing hits(Q+ A) to reach
non-zero counts. This means that the final hit counts, hence the CCP value, are generally
uncorrelated with the original (Q,A) tuple.
One interesting observation is that two out of the first three features chosen by
our model selection process use information from the NLP processors. The first feature
selected is the translation probability computed between the R representation (unla-
beled semantic roles) of the question and the answer. This feature alone accounts for
57% of the measured MRR improvement. This is noteworthy: Semantic roles have been
shown to improve factoid QA, but to the best of our knowledge this is the first result
demonstrating that semantic roles can improve ad hoc retrieval (on a large set of non-
factoid open-domain questions). We also find noteworthy that the third feature chosen
measures the number of unlabeled syntactic dependencies with words replaced by their
WNSS labels that are matched in the answer. Overall, the features that use the output of
NL processors account for 68% of the improvement produced by our model over the IR
baseline. These results provide empirical evidence that natural language analysis (e.g.,
coarse word sense disambiguation, syntactic parsing, and semantic role labeling) has a
positive contribution to non-factoid QA, even in broad-coverage noisy settings based
on Web data. To our knowledge, this had not been shown before.
Finally, we note that tree kernels provide minimal improvement: A tree kernel
feature is selected only in iteration 20 and the MRR improvement is only 0.04 points.
One conjecture is that, due to the sparsity and the noise of the data, matching trees of
depth higher than 2 is highly uncommon. Hence matching immediate dependencies
is a valid approximation of kernels in this setup. Another possible explanation is that
because the syntactic trees produced by the parser contain several mistakes, the tree
kernel, which considers matches between an exponential number of candidate sub-
trees, might be particularly unreliable on noisy data.
4.2.2 SVM-rank. For SVM-rank we employed a tuning procedure similar to the one used
for the Perceptron that implements both feature selection and tuning of the regularizer
parameter C. We started with the baseline feature alone and greedily added one feature
at a time. In each iteration we added the feature that provided the best improvement.
The procedure continues to evaluate all available features, until no improvement is
observed. For this step we set the regularizer parameter to 1.0, a value which provided
a good tradeoff between accuracy and speed as evaluated in an initial experiment.
The selection procedure generated 12 additional features. At this point, using only the
selected features, we fine-tuned the regularization parameter C across a wide spectrum
of possible values. This can be useful because in SVM-rank the interpretation of C is
slightly different than in standard SVM, specifically Csvm = Crank/m, where m is the
number of queries, or questions in our case. Therefore, an optimal value can depend
crucially on the target data. The final value selected by this search procedure was equal
to 290, although performance is relatively stable with values between 1 and 100,000. As
a final optimization step, we continued the feature selection routine, starting from the
13 features already chosen and C = 290. This last step selected six additional features.
A further attempt at fine-tuning the C parameter did not provide any improvements.
368
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
This process is summarized in Table 5, using the same notations as Table 4. Al-
though the features selected by SVM-rank are slightly different than the ones chosen
by the Perceptron, the conclusions drawn are the same as before: Features generated
by NL processors provide a significant boost on top of the IR model. Similarly to the
Perceptron, the first feature chosen by the selection procedure is a translation probabil-
ity computed over semantic role dependencies (labeled, unlike the Perceptron, which
prefers unlabeled dependencies). This feature alone accounts for 33.3% of the measured
MRR improvement. This further enforces our observation that semantic roles improve
retrieval performance for complex tasks such as our non-factoid QA exercise. All in all,
13 out of the 18 selected features, responsible for 70% of the total MRR improvement,
use information from the NL processors.
4.3 Contribution of Natural Language Structures
One of the conclusions of the previous analysis is that features based on natural lan-
guage processing are important for the problem of QA. This observation deserves a
more detailed analysis. Table 6 shows the performance of our first three feature groups
when they are applied to each of the content representations and incremental combina-
tions of representations. In this table, for simplicity we merge features from labeled
and unlabeled representations. For example, R indicates that features are extracted
from both labeled (Rl) and unlabeled (R) semantic role representations. The g subscript
indicates that the lexical terms in the corresponding representation are separately gener-
alized toWNSS andWSJ labels. For example,Dgmerges features generated fromDWNSS,
Table 5
Summary of the model selection process using SVM-rank.
Iteration Feature Set Group MRR P@1 (%)
0 BM25(W) FG1 56.09 41.12
1 + translation(Rl) FG2 59.02 43.99
2 + answer span FG3 60.31 45.05
3 + translation(W) FG2 61.16 46.13
4 + translation(R) FG2 61.65 46.77
5 + overall match(D) FG3 62.85 48.57
6 + translation(RlWSJ) FG2 63.05 48.78
7 + translation(NWSJ) FG2 63.23 48.88
8 + translation(DlWSJ) FG2 63.47 49.21
9 + query-log max(?2) FG4 63.64 49.35
10 + translation(D) FG2 63.77 49.53
11 + translation(N) FG2 63.85 49.66
12 + overall match(NWSJ) FG3 64.03 49.93
+ C fine tuning 64.49 50.43
13 + BM25(DWSJ) FG1 64.49 50.43
14 + BM25(N) FG1 64.74 50.71
15 + tf ? idf(DWNSS) FG1 64.74 50.60
16 + answer span in nouns FG3 64.74 50.60
17 + tf ? idf(Rl) FG1 64.84 50.89
18 + translation(Dl) FG2 64.88 50.91
369
Computational Linguistics Volume 37, Number 2
Table 6
Contribution of natural language structures in each feature group. Scores are MRR changes of
the Perceptron on the development set over the baseline model (FG1 with W), for N = 15. The
best scores for each feature group (i.e., column in the table), are marked in bold.
FG1 FG2 FG3
W ? +4.18 ?6.80
N ?13.97 +2.49 ?13.63
Ng ?18.65 +3.63 ?15.57
D ?15.15 +1.48 ?15.39
Dg ?19.31 +3.41 ?18.18
R ?27.61 +0.33 ?27.82
Rg ?28.29 +3.46 ?26.74
W +N +1.46 +5.20 ?4.36
W +N +Ng +1.51 +5.33 ?4.31
W +N +Ng +D +1.56 +5.78 ?4.31
W +N +Ng +D +Dg +1.56 +5.85 ?4.21
W +N +Ng +D +Dg + R +1.58 +6.12 ?4.28
W +N +Ng +D +Dg + R + Rg +1.65 +6.29 ?4.28
DWSJ, D
l
WNSS, and D
l
WSJ. For each cell in the table, we use only the features from the
corresponding feature group and representation to avoid the correlation with features
from other groups. We generate each best model using the same feature selection
process described above.
The top part of the table indicates that all individual representations perform worse
than the bag-of-words representation (W) in every feature group. The differences range
from less than one MRR point (e.g., FG2[Rg] versus FG2[W]), to over 28 MRR points
(e.g., FG1[Rg] versus FG1[W]). Such a large difference is justified by the fact that for
feature groups FG1 and FG3 we compute feature values using only the corresponding
structures (e.g., only semantic roles), which could be very sparse. For example, there
are questions in our corpus where our SRL system does not detect any semantic propo-
sition. Because translation models merge all structured representations with the bag-
of-word representation, they do not suffer from this sparsity problem. Furthermore, on
their own, FG3 features are significantly less powerful than FG1 or FG2 features. This
explains why models using FG3 features fail to improve over the baseline. Regardless
of these differences, the analysis indicates that in our noisy setting the bag-of-words
representation outperforms any individual structured representation.
However, the bottom part of the table tells a more interesting story: The second
part of our analysis indicates that structured representations provide complementary
information to the bag-of-words representation. Even the combination of bag of words
with the simplest n-gram structures (W + N) always outperforms the bag-of-words
representation alone. But the best results are always obtained when the combination
includes more natural language structures. The improvements are relatively small, but
remarkable (e.g., see FG2) if we take into account the significant scale and settings of the
evaluation. The improvements yielded by natural language structures are statistically
significant for all feature groups. This observation correlates well with the analysis
shown in Tables 4 and 5, which shows that features using semantic (R) and syntactic
(D) representations contribute the most on top of the IR model (BM25(W)).
370
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
5. Error Analysis and Discussion
Similar to most re-ranking systems, our system improves the answer quality for some
questions while decreasing it for others. Table 7 lists the percentage of questions from
our test set that are improved (i.e., the correct answer is ranked higher after re-ranking),
worsened (i.e., the correct answer is ranked lower), and unchanged (i.e., the position of
the correct answer does not change after re-ranking). The table indicates that, regard-
less of the number of candidate answers for re-ranking (N), the number of improved
questions is approximately twice the number of worsened questions. This explains the
consistent improvements in P@1 and MRR measured for various values of N. As N
increases, the number of questions that are improved also grows, which is an expected
consequence of having more candidate answers to re-rank. However, the percentage
of improved questions grows at a slightly lower rate than the percentage of worsened
questions. This indicates that choosing the ideal number of candidate answers to re-
rank requires a trade-off: On the one hand, having more candidate answers increases
the probability of capturing the correct answer in the set; on the other hand, it also
increases the probability of choosing an incorrect answer due to the larger number
of additional candidates. For our problem, it seems that re-ranking using values of N
much larger than 100 would not yield significant benefits over smaller values of N.
This analysis is consistent with the experiments reported in Table 3 where we did not
measure significant growth in P@1 or MRR for N larger than 50.
Although Table 7 gives the big picture of the behavior of our system, it is important
to look at actual questions that are improved or worsened by the re-ranking model in
order to understand the strengths and weaknesses of our system. Table 8 lists some
representative questions where the re-ranking model brings the correct answer to the
top position. For every question we list: (a) the correct answer and its position as given
by the baseline IR Model (?Baseline?) and the re-ranking model (?Re-ranking?); and (b)
the answer that was ranked by the baseline model in the first position and its position
after re-ranking.
Generally, Table 8 indicates that our model performs considerably better than the
bag-of-words IRmodel. For example, we boost the rank of answers that share structures
with the question: for example, the cook ? grouse syntactico-semantic dependency for
the second sample question or make ? call and see ? number for the third example.
Modeling structures is important especially for questions with minimal context, that is,
short length and common terms, like the third sample question. Due to the structure-
based translation models and/or the generalizations to supersenses or named-entity
labels, our model can match structures even when they are not identical. For example,
Table 7
Percentage of questions in the test set that are improved/worsened/unchanged after re-ranking.
This experiment used the Perceptron model.
Better (%) Worse (%) Unchanged (%)
N = 10 33.98 16.81 49.21
N = 15 36.76 18.56 44.68
N = 25 39.64 20.68 39.68
N = 50 42.95 23.30 33.75
N = 100 45.18 25.28 29.54
371
Computational Linguistics Volume 37, Number 2
Table 8
Examples of questions improved by our re-ranking model. URLs were replaced with <URL> in
answer texts. Some non-relevant text was replaced with <...> to save space. The remaining text
maintains the original capitalization and spelling. Non-stop question terms are emphasized in
the answers.
How would you rank the top 5 NFL teams? Do your rankings depend on the outcome
of the Colts Vs. Pats Game?
Baseline Re-ranking Correct? Answer Text
2 1 yes Ok. How can you think the Chargers are better than the Colts, Ravens, and
the Broncos? As for the Rankings, this is what I got: 1. Colts (they actually beat
a good team) 2. Bears (for now, they are going to lose to the Giants) 3. Patriots
(the winner of Cotls vs Patriots should be the next #1) 4. Broncos (They lost
on a field goal) 5. Ravens (all about the D)
1 2 no Basically in sport rankings you will look at the how they perform their
wins against their loss or their stats. Ranking is the process of positioning
individuals, groups or businesses on a ordinal scale in relation to others.
A list arranged in this way is said to be in rank order. Some examples: In
many sports, individuals or teams are given rankings, generally by the sport?s
governing body. In football (soccer), national teams are ranked in the FIFA
World Rankings. In snooker, players are ranked using the Snooker world
rankings. In ice hockey, national teams are ranked in the IIHF World Ranking.
In golf, the top male golfers are ranked using the Official World Golf Rankings
how can i cook grouse quick with normal household spices w/o going out to buy stuff?
Baseline Re-ranking Correct? Answer Text
10 1 yes I?ve never cooked a grouse, but poultry is poultry... You could salt it and pepper
it, put some flour on a plate and roll it in the flour to coat it lightly, then heat
a few tablespoons of olive oil in a skillet and pan-fry it. (If you have no olive
oil, use a little vegetable oil plus a pat of butter ? the oil is to keep the butter
from burning.) Squeeze a few drops of lemon juice over it if you want. Or:
Skip the flour. Salt and pepper the grouse. Pan-fry it in a little olive oil. When
it?s looking close to ready, pour in a little white wine which will sizzle and
reduce down to a sauce. If you?ve got some fresh or dried herbs (rosemary,
thyme, parsley) you could sprinkle a pinch of that in.
1 2 no Well, a grouse is a prey animal. If there was a decline in the population
of grouse, then the animals that usually prey on the grouse - coyotes, owls,
etc - would probably start eating other prey animals, like the pheasants and
squirrels.
how did I do for make a call and that the other dont see my number?
Baseline Re-ranking Correct? Answer Text
2 1 yes to make a call so that the other person cant see the number... dial *67 and wait
for the three beeps.. then dial the number
1 2 no Oneday out of the blue call her. If u dont have her number, when u see her ask
her if she wanted to go out oneday then get her number. When u talk on the
phone get to know her. But dont ask her out too soon because she may not
feel the same way. After a couple of days or weeks taking to her let her know
how u felt about her since the first time u met her.
372
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 8
(continued)
how can i find a veterinary college with dorms?
Baseline Re-ranking Correct? Answer Text
14 1 yes <...> I would say not to look for a specific school of veterinarianmedicine but
rather find a creditable University that offers a degree such as Pre-Vet. Then
from there you can attend graduate school to finish up to become a doctor in
that field. Most major universities will have this degree along with dorms. In
my sources you can see that this is just one of many major universities that
offer Pre-vet medicine.
1 7 no Hi there... here?s an instructional video by Cornell University Feline Health
Center - College of VeterinaryMedicine on how to pill cats: <URL>
how to handle commission splits with partners in Real estate?
Baseline Re-ranking Correct? Answer Text
5 1 yes My company splits the commissions all evenly. However many various
agents/brokers are involved (or even think they are involved), it gets split
further. Keeps everyone happy. No one complains that someone gets ?more?.
1 3 no You will find information regarding obtaining a real estate license in Okla-
homa at the Oklahoma Real Estate Commission?s website (<URL>) Good luck!
for the fourth question, find? college can bematched to look? school if the structures are
generalized toWordNet supersenses. Translation models are crucial to fetching answers
rich in terms related to question concepts. For example, for the first question, our model
boosts the position of the correct answer due to the large numbers of concepts that
are related to NFL, Colts, and Pats: Ravens, Broncos, Patriots, and so forth. In the second
example, our model ranks on the first position the answer containing many concepts
related to cook: salt, pepper, flour, tablespoons, oil, skillet, and so on. In the last example,
our model is capable of associating the bigram real estate to agent and broker. Without
these associationsmany answers are lost to false positives provided by the bag-of-words
similarity models. For example, in the first and last examples in the table, the answers
selected by the baseline model contain more matches of the questions terms than the
correct answers extracted by our model.
All in all, this analysis proves that non-factoid QA is a complex problem where
many phenomena must be addressed. The key for success does not seem to be a unique
model, but rather a combination of approaches each capable of addressing different
facets of the problem. Our model makes a step forward towards this goal, mainly
through concept expansion and the exploration of syntactico-semantic structures. Nev-
ertheless, our model is not perfect. To understand where FMIX fails we performed
a manual error analysis on 50 questions where FMIX performs worse than the IR
baseline and we identified seven error classes. Table 9 lists the distribution of these error
classes and Table 10 lists sample questions and answers from each class. Note that the
percentage values listed in Table 9 sum up to more than 100% because the error classes
are not exclusive. We now detail each of these error classes.
373
Computational Linguistics Volume 37, Number 2
Table 9
Distribution of error classes in questions where FMIX (Perceptron) performs worse.
COMPLEX INFERENCE 38%
ELLIPSIS 36%
ALSO GOOD 18%
REDIRECTION 10%
ANSWER QUALITY 4%
SPELLING 2%
CLARIFICATION 2%
COMPLEX INFERENCE: This is the most common class of errors (38%). Questions in this
class could theoretically be answered by an automated system but such a system would
require complex reasoning mechanisms, large amounts of world knowledge, and dis-
course understanding. For example, to answer the first question in Table 10, a system
would have to understand that confronting or being supportive are forms of dealing with
a person. To answer the second question, the system would have to know that creating
a CD at what resolution you need supersedes making a low resolution CD. Our approach
captures some simple inference rules through translationmodels but fails to understand
complex implications such as these.
ELLIPSIS: This class of errors is not necessarily a fault of our approach but is rather
caused by the problem setting. Because in a social QA site each answer responds to a
specific question, discourse ellipsis (i.e., omitting the context set by the question in the
answer text) is common. This makes some answers (e.g., the third answer in Table 10)
ambiguous, hence hard to retrieve automatically. This affects 36% of the questions
analyzed.
ALSO GOOD: It is a common phenomenon in Yahoo! Answers that a question is asked
several times by different users, possibly in a slightly different formulation. To enable
our large scale automatic evaluation, we considered an answer as correct only if it was
chosen as the ?best answer? for the corresponding question. So in our setting, ?best
answers? from equivalent questions are marked as incorrect. This causes 18% of the
?errors? of the re-ranking model. One example is the fourth question in Table 10, where
the answer selected by our re-ranking model is obviously also correct. It is important
to note that at testing time we do not have access to the questions that generated the
candidate answers for the current test question, that is, the system does not know
which questions are answered by the answers in the ALSO GOOD section of Table 10.
So the answers in the ALSO GOOD category are not selected based on the similarity of
the corresponding queries, but rather, based on better semantic matching between test
question and candidate answer.
REDIRECTION: Some answers (10% of the questions analyzed) do not directly answer a
question but rather redirect the user to relevant URLs (see the fifth question in Table 10).
Because we do not extract the text behind URLs in the answer content, such questions
are virtually impossible to answer using our approach.
ANSWER QUALITY: For a small number of the questions analyzed (4%) the choice of ?best
answer? is dubious (see the sixth example in Table 10). This is to be expected in a
social QA site, where the selection of best answers is not guaranteed to be optimal.
Nevertheless, the relatively small number of such cases is unlikely to influence the
quality of the evaluation.
374
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Table 10
Examples of questions in each error class. The corresponding error class is listed on the left side
of the question text. We list the answer ranked at the top position by FMIX only where relevant
(e.g., the ALSO GOOD category). URLs were replaced with <URL> in answer texts. Some
non-relevant text was replaced with <...> to save space. The remaining text maintains the
original capitalization and spelling.
COMPLEX INFERENCE how to deal with a person in denial with M.P.D.?
Baseline Re-ranking Correct? Answer Text
1 6 yes First, i would find out if MPD has been diagnosed by a pro-
fessional. In current terminology, MPD is considered a part
of Dissociative Personality Disorder. In any case, it would be
up to the professionals to help this person because you could
cause further problems by confronting this person with what
you think the problem is. If this person is a family member,
you could ask for a consultation with the psychiatric profes-
sional who is treating him/her. Please, please, just do you
best to be supportive without being confrontational since that
might make things even worse for that person.
COMPLEX INFERENCE How do I make a low resolution CD of 100 phtos that were shot at
8 megapixels?
Baseline Re-ranking Correct? Answer Text
1 4 yes you can use picasa2 from google: <URL> is free. In picasa
you can do ?GiftCD? create a CD with you picture at what
resolution you need (including original size)
ELLIPSIS How do mineral ions affect the biology and survival of a pond organism?
Baseline Re-ranking Correct? Answer Text
1 3 yes Some mineral ions are fertilizer and will increase vegetative
growth while others are poisons.
ALSO GOOD How to learn the British accent?
Baseline Re-ranking Correct? Answer Text
4 5 yes Get a dictionary where there is a pronunciation guide which
gives the pronunciation in British English. Watch british
movies and imitate what you can. Then just practice, practice
practice. But before you go about learning accents, slangs or
dialects, make sure you brush up on your basic grammar.
<...>
3 1 no You can do one of two things: first, go to a local bookstore, like
Barnes and Noble. They sell cd?s with different accents from
around the world, accompanied by a book that phonetically
spells the words. This is designed for actors/actresses who
need to learn different accents. Also, go rent a bunch of british
movies, or watch british television. Continually pause and
repeat common phrases and words.
375
Computational Linguistics Volume 37, Number 2
Table 10
(continued)
REDIRECTION How can I build an easy lean-to shed out of scrap wood and skids?
Baseline Re-ranking Correct? Answer Text
6 15 yes the pallet shed... <URL> building a wood shed from pallets...
<URL> good ideas from those who?ve been there...<URL> pics. of
the shed... <URL> nice pics. <URL> taking pallets apart... and other
tips... <URL> <...>
ANSWER QUALITY How do make a Naruto AMV? Can you show me how? I need the
website or program and the exact directions.?
Baseline Re-ranking Correct? Answer Text
2 94 yes i?m not an expert. but i sure do like Naruto. i?ll wait for answers
too
SPELLING how does aliquid expansion boiler thrrmosstat work?
Baseline Re-ranking Correct? Answer Text
2 4 yes the liquid expands inside the thermostat when the liquid reaches
the shutoff temp or pressure it will shut off the boiler preventing
boiler explosions
CLARIFICATION how could you combine your styles and personalities effectively to
produce the best paper?
Baseline Re-ranking Correct? Answer Text
29 1 yes Your question is not clear. Are you asking about writing styles?
it also depends on what kind of paper you are writing? Your
question cannot be answered without more info.
SPELLING: Two percent (2%) of the error cases analyzed are caused by spelling errors
(e.g., the seventh example in Table 10). Because these errors are relatively infrequent,
they are not captured by our translation models, and our current system does not
include any other form of spelling correction.
CLARIFICATION: Another 2% of the questions inspected manually had answers that
pointed to errors or ambiguities in the question text rather than responding to the given
question (see the last example in Table 10). These answers are essentially correct but
they require different techniques to be extracted: Our assumption is that questions are
always correct and sufficient for answer extraction.
6. Related Work
There is a considerable amount of previous work in several related areas. First, we will
discuss related work with respect to the features and models used in this research; most
of this work is to be found in the factoid QA community, where the most sophisticated
376
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
QA selection and re-ranking algorithms have been developed. We then review existing
work in non-factoid QA; we will see that in this area there is much less work, and the
emphasis has been so far in query re-writing and scalability using relatively simple
features andmodels. Finally wewill discuss relatedwork in the area of community-built
(social) QA sites. Although we do not exploit the social aspect of our QA collection, this
is complementary to our work and would be a natural extension. Table 11 summarizes
aspects of the different approaches discussed in this section, highlighting the differences
and similarities with our current work.
Our work borrows ideas from many of the papers mentioned in this section, es-
pecially for feature development; indeed our work includes matching features as well
as translation and retrieval models, and operates at the lexical level, the parse tree
Table 11
Comparison of some of the characteristics of the related work cited. Task: Document Retrieval
(DRet), Answer Extraction (Ex) or Answer Re-ranking or Selection (Sel).Queries: factoid (Fact)
or non-factoid (NonFact). Features: lexical (L), n-grams (Ngr), collocations (Coll), paraphrases
(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE),
WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal
relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words
scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning
(kernel) (SL), probabilistic grammar (PG), statistical machine translation (SMT), query likelihood
language model (QLLM).Development and Evaluation: data sizes used, expressed as number
of queries/number of query?answer pairs (i.e., sum of all candidate answers per question).
Data: type of source used for feature construction, training and/or evaluation. Question marks
are place holders for information not available or not applicable in the corresponding work.
Publication Task Queries Features Models Devel Eval Data
Agichtein et al DRet NonFact L, Ngr, Coll BOW, LM ?/10K 50/100 WWW
(2001)
Echihabi and Sel Fact, L, Ngr, Coll, SMT 4.6K/100K 1K/300K? TREC, KM,
Marcu (2003) NonFact DT, NE, WWW
WN
Higashinaka and Sel NonFact L, WN, SRL, SL 1K/500K 1K/500K WHYQA
Isozaki (2008) (Why) CR
Punyakanok et al Sel Fact L, POS, DT, TreeMatch ?/400 TREC13
(2004) NE, QC
Riezler et al DRet NonFact L, Ngr, Para SMT 10M/10M 60/1.2K WWW, FAQ
(2007)
Soricut and Brill DRet, NonFact L, Ngr, Coll BOW, SMT 1M/? 100/? WWW, FAQ
(2006) Sel,
Ex
Verberne et al Sel NonFact CT, WN, BOW, LLM same as eval 186/28K Webclopedia,
(2010) (Why) Para Wikipedia
Wang et al (2007) Sel Fact L, POS, DT, LLM, PG 100/1.7K 200/1.7K TREC13
NE, WNR,
Hyb
Xue et al (2008) DRet, NonFact, L, Coll SMT, QLLM 1M/1M 50/? SocQA,
Sel Fact TREC9
This work Sel NonFact L, Ngr, POS, TreeMatch, 112K/1.6M 28K/up to SocQA,
(How) DT, SRL, BOW, SMT, 2.8M QLog
NE, WN, SL
WNSS,
Hyb,
QLCoOcc
377
Computational Linguistics Volume 37, Number 2
level, as well as the level of semantic roles, named entities, and lexical semantic classes.
However, to the best of our knowledge no previous work in QA has evaluated the use
of so many types of features concurrently, nor has it built so many combinations of these
features at different levels. Furthermore, we employ unsupervised methods, generative
methods, and supervised learning methods. This is made possible by the choice of the
task and the data collection, another novelty of our work which should enable future
research in complex linguistic features for QA and ranking.
Factoid QA. Within the statistical machine translation community there has been
much research on the issue of automatically learning transformations (at the lexical,
syntactical, and semantical level). Some of this work has been applied to automated
QA systems, mostly for factoid questions. For example, Echihabi and Marcu (2003)
presented a noisy-channel approach (IBM model 4) adapted for the task of QA. The
features used included lexical and parse-tree elements as well as some named entities
(such as dates). They use a dozen heuristic rules to heavily reduce the feature space and
choose a single representation mode for each of the tokens in the queries (for example:
?terms overlapping with the question are preserved as surface text?) and learn language
models on the resulting representation. We extend Echihabi and Marcu by considering
deeper semantic representations (such as SRL andWNSS), but instead of using selection
heuristics we learn models from each of the full representations (as well as from some
hybrid representations) and then combine them using discriminant learning techniques.
Punyakanok, Roth, and Yih (2004) attempted a more comprehensive use of the
parse tree information, computing a similarity score between question and answer
parse trees (using a distance function based on approximate tree matching algorithms).
This is an unsupervised approach, which is interesting especially when coupled with
appropriate distances. Shen and Joshi (2005) extend this idea with a supervised learning
approach, training dependency tree kernels to compute the similarity. In our work we
also used this type of feature, although we show that, in our context, features based on
dependency tree kernels are subsumed by simpler features that measure the overlap
of binary dependencies. Another alternative is proposed by Cui et al (2005), where
significant words are aligned and similarity measures (based on mutual information of
correlations) are then computed on the resulting dependency paths. Shen and Klakow
(2006) extend this using a dynamic time warping algorithm to improve the alignment
for approximate question phrase mapping, and learn a Maximum Entropy model to
combine the obtained scores for re-ranking. Wang, Smith, andMitamura (2007) propose
to use a probabilistic quasi-synchronous grammar to learn the syntactic transformations
between questions and answers. We extend the work of Cui et al by considering paths
within and across different representations beyond dependency trees, although we do
not investigate the issue of alignment specifically?instead we use standard statistical
translation models for this.
Non-factoid QA. The previous works dealt with the problem of selection, that is,
finding the single sentence that correctly answers the question out of a set of candidate
documents. A related problem in QA is that of retrieval: selecting potentially relevant
documents or sentences prior to the selection phase. This problem is closer to gene-
ral document retrieval and it is therefore easier to generalize to the non-factoid domain.
Retrieval algorithms tend to be much simpler than selection algorithms, however, in
part due to the need for speed, but also because there has been little previous evidence
that complex algorithms or deeper linguistic analysis helps at this stage, especially in
the context of non-factoid questions.
378
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Previous work addressed the task by learning transformations between questions
and answers and using them to improve retrieval. All these works use only lexical
features. For example, Agichtein et al (2001) learned lexical transformations (from the
original question to a set of Web search queries, from ?what is a? to ?the term?, ?stands
for?, etc.) which are likely to retrieve good candidate documents in commercial Web
search engines; they applied this successfully to large-scale factoid and non-factoid QA
tasks. Murdock and Croft (2005) study the problem of candidate sentence retrieval for
QA and show that a lexical translation model can be exploited to improve factoid QA.
Xue, Jeon, and Croft (2008) show that a linear interpolation of translation models and
a query likelihood language model outperforms each individual model for a QA task
that is independent of the question type. In the same space, Riezler et al (2007) develop
SMT-based query expansionmethods and use them for retrieval from FAQpages. In our
work we did not address the issue of query expansion and re-writing directly: While
our re-ranking approach is limited to the recall of the retrieval model, these methods of
query transformation could be used in a complementary manner to improve the recall.
Even more interesting would be to couple the two approaches in an efficient manner;
this remains as future work.
There has also been some work in the problem of selection for non-factoid ques-
tions. Girju (2003) extracts non-factoid answers by searching for certain semantic struc-
tures (e.g., causation relations as answers to causation questions). We generalized this
methodology (in the form of semantic roles) and evaluated it systematically. Soricut
and Brill (2006) develop a statistical model by extracting (in an unsupervised manner)
QA pairs from one million FAQs obtained from the Web. They show how different
statistical models may be used for the problems of ranking, selection, and extraction
of non-factoid QAs on the Web; due to the scale of their problem they only consider lex-
ical n-grams and collocations, however. More recent work has showed that structured
retrieval improves answer ranking for factoid questions: Bilotti et al (2007) showed that
matching predicate?argument frames constructed from the question and the expected
answer types improves answer ranking. Cui et al (2005) learned transformations of
dependency paths from questions to answers to improve passage ranking. All these
approaches use similarity models at their core because they require the matching of
the lexical elements in the search structures, however. On the other hand, our approach
allows the learning of full transformations from question structures to answer structures
using translation models applied to different text representations.
The closest work to ours is that of Higashinaka and Isozaki (2008) and Verberne
et al (2010), both on Why questions. Higashinaka et al consider a wide range of
semantic features by exploiting WordNet and gazetteers, semantic role labeling, and
extracted causal relations. Verberne et al exploit syntactic information from constituent
trees, WordNet synonymy sets and relatedness measures, and paraphrases. As in our
models, both these works combine these features using discriminative learning tech-
niques and apply the learned models to re-rank answers to non-factoid questions (Why
type questions). Their features, however, are based on counting matches or events
defined heuristically. We have extended this approach in several ways. First, we use a
much larger feature set that includes correlation and transformation-based features and
five different content representations. Second, we use generative (translation) models
to learn transformation functions before they are combined by the discriminant learner.
Finally, we carry out training and evaluation at a much larger scale.
Content from community-built question?answer sites can be retrieved by searching
for similar questions already answered (Jeon, Croft, and Lee 2005) and ranked using
meta-data information like answerer authority (Jeon et al 2006; Agichtein et al 2008).
379
Computational Linguistics Volume 37, Number 2
Here we show that the answer text can be successfully used to improve answer ranking
quality. Our method is complementary to the earlier approaches. It is likely that an
optimal retrieval engine from social media would combine all three methodologies.
Moreover, our approach might have applications outside of social media (e.g., for open-
domainWeb-based QA), because the rankingmodel built is based only on open-domain
knowledge and the analysis of textual content.
7. Conclusions
In this work we describe an answer ranking system for non-factoid questions built
using a large community-generated question?answer collection. We show that the best
ranking performance is obtained when several strategies are combined into a single
model. We obtain the best results when similarity models are aggregated with features
that model question-to-answer transformations, frequency and density of content, and
correlation of QA pairs with external collections. Although the features that model
question-to-answer transformations provide the most benefits, we show that the com-
bination is crucial for improvement. Further, we show that complex linguistic features,
most notably semantic role dependencies and semantic labels derived from WordNet
senses, yield a statistically significant performance increase on top of the traditional
bag-of-words and n-gram representations. We obtain these results using only off-the-
shelf NL processors that were not adapted in any way for our task. As a side effect, our
experiments prove that we can effectively exploit large amounts of availableWeb data to
do research on NLP for non-factoid QA systems, without any annotation or evaluation
cost. This provides an excellent framework for large-scale experimentation with various
models that otherwise might be hard to understand or evaluate.
As implications of our work, we expect the outcome of our investigation to help
several applications, such as retrieval from social media and open-domain QA on the
Web. On social media, for example, our system should be combined with a component
that searches for similar questions already answered; the output of this ensemble can
possibly be filtered further by a content-quality module that explores ?social? features
such as the authority of users, and so on. Although we do not experiment on Wikipedia
or news sites in this work, one can view our data as a ?worse-case scenario,? given its
ungrammaticality and annotation quality. It seems reasonable to expect that training our
model on cleaner data (e.g., fromWikipedia or news), would yield even better results.
This work can be extended in several directions. First, answers that were not se-
lected as best, but were marked as good by a minority of voters, could be incorporated
in the training data, possibly introducing a graded notion of relevance. This wouldmake
the learning problemmore interesting andwould also provide valuable insights into the
possible pitfalls of user-annotated data. It is not clear if more data, but of questionable
quality, is beneficial. Another interesting problem concerns the adaptation of the re-
ranking model trained on social media to collections from other genres and/or domains
(news, blogs, etc.). To our knowledge, this domain adaptation problem for QA has not
been investigated yet.
References
Agichtein, Eugene, Carlos Castillo, Debora
Donato, Aristides Gionis, and Gilad Mishne.
2008. Finding high-quality content in social
media, with an application to community-
based question answering. In Proceedings of
the Web Search and Data Mining Conference
(WSDM), pages 183?194, Stanford, CA.
Agichtein, Eugene, Steve Lawrence, and Luis
Gravano. 2001. Learning search engine
specific query transformations for question
answering. In Proceedings of the World
380
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
Wide Web Conference, pages 169?178,
Hong Kong.
Attardi, Giuseppe, Felice Dell?Orletta, Maria
Simi, Atanas Chanev, and Massimiliano
Ciaramita. 2007. Multilingual dependency
parsing and domain adaptation using
DeSR. In Proceedings of the Shared Task
of the Conference on Computational
Natural Language Learning (CoNLL),
pages 1112?1118, Prague.
Berger, Adam, Rich Caruana, David Cohn,
Dayne Freytag, and Vibhu Mittal. 2000.
Bridging the lexical chasm: Statistical
approaches to answer finding. In
Proceedings of the 23rd Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 192?199, Athens, Greece.
Bilotti, Matthew W., Paul Ogilvie, Jamie
Callan, and Eric Nyberg. 2007. Structured
retrieval for question answering. In
Proceedings of the 30th Annual International
ACM SIGIR Conference on Research &
Development on Information Retrieval,
pages 351?358, Amsterdam.
Brill, Eric, Jimmy Lin, Michele Banko,
Susan Dumais, and Andrew Ng. 2001.
Data-intensive question answering.
In Proceedings of the Text REtrieval
Conference (TREC), pages 393?400,
Gaithersburg, MD, USA.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the North American Chapter of the Association
for Computational Linguistics (NAACL),
pages 132?139, Seattle, WA.
Ciaramita, Massimiliano and Yasemin
Altun. 2006. Broad coverage sense
disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 594?602,
Sidney.
Ciaramita, Massimiliano, Giuseppe Attardi,
Felice Dell?Orletta, and Mihai Surdeanu.
2008. Desrl: A linear-time semantic role
labeling system. In Proceedings of the Shared
Task of the 12th Conference on Computational
Natural Language Learning (CoNLL-2008),
pages 258?262, Manchester.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
2003 Conference on Empirical Methods in
Natural Language Processing,
pages 168?175, Sapporo.
Ciaramita, Massimiliano, Vanessa Murdock,
and Vassilis Plachouras. 2008. Semantic
associations for contextual advertising.
Journal of Electronic Commerce
Research?Special Issue on Online
Advertising and Sponsored Search, 9(1):1?15.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Proceedings of the Neural Information
Processing Systems Conference (NIPS),
pages 625?632, Vancouver, Canada.
Cui, Hang, Renxu Sun, Keya Li, Min-Yen
Kan, and Tat-Seng Chua. 2005. Question
answering passage retrieval using
dependency relations. In Proceedings of the
28th Annual International ACM SIGIR
Conference on Research & Development in
Information Retrieval, pages 400?407,
Salvador.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 16?23, Sapporo.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm.Machine Learning,
37:277?296.
Girju, Roxana. 2003. Automatic detection of
causal relations for question answering. In
Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics
(ACL), Workshop on Multilingual
Summarization and Question Answering,
pages 76?83, Sapporo.
Harabagiu, Sanda, Dan Moldovan, Marius
Pasca, Rada Mihalcea, Mihai Surdeanu,
Razvan Bunescu, Roxana Girju, Vasile Rus,
and Paul Morarescu. 2000. Falcon:
Boosting knowledge for answer engines.
In Proceedings of the Text REtrieval
Conference (TREC), pages 479?487,
Gaithersburg, MD.
Higashinaka, Ryuichiro and Hideki Isozaki.
2008. Corpus-based question answering
for why-questions. In Proceedings of the
Third International Joint Conference on
Natural Language Processing (IJCNLP),
pages 418?425, Hyderabad.
Jeon, Jiwoon, W. Bruce Croft, and
Joon Hoo Lee. 2005. Finding similar
questions in large question and answer
archives. In Proceedings of the ACM
Conference on Information and Knowledge
381
Computational Linguistics Volume 37, Number 2
Management (CIKM), pages 84?90,
Bremen.
Jeon, Jiwoon, W. Bruce Croft, Joon Hoo Lee,
and Soyeon Park. 2006. A framework to
predict the quality of answers with
non-textual features. In Proceedings of the
29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 228?235,
Seattle, WA.
Joachims, Thorsten. 2006. Training linear
svms in linear time. In KDD ?06:
Proceedings of the 12th ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 217?226,
New York, NY.
Ko, Jeongwoo, Teruko Mitamura, and Eric
Nyberg. 2007. Language-independent
probabilistic answer ranking for question
answering. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics, pages 784?791,
Prague.
Li, Xin and Dan Roth. 2006. Learning
question classifiers: The role of semantic
information. Natural Language Engineering,
12:229?249.
Li, Yaoyong, Hugo Zaragoza, Ralf Herbrich,
John Shawe-Taylor, and Jaz S. Kandola.
2002. The perceptron algorithm with
uneven margins. In Proceedings of the
Nineteenth International Conference on
Machine Learning, pages 379?386,
Sidney.
Magnini, Bernardo, Matteo Negri, Roberto
Prevete, and Hristo Tanev. 2002.
Comparing statistical and content-based
techniques for answer validation on the
web. In Proceedings of the VIII Convegno
AI*IA, Siena, Italy.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Moldovan, Dan, Sanda Harabagiu, Marius
Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus. 1999.
Lasso?a tool for surfing the answer net. In
Proceedings of the Text REtrieval Conference
(TREC), pages 175?183, Gaithersburg, MD.
Moschitti, Alessandro, Silvia Quarteroni,
Roberto Basili, and Suresh Manandhar.
2007. Exploiting syntactic and shallow
semantic kernels for question/answer
classification. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 776?783, Prague.
Murdock, Vanessa and W. Bruce Croft. 2005.
A translation model for sentence retrieval.
In Proceedings of the Conference on Human
Language Technology and Empirical Methods
in Natural Language Processing,
pages 684?691, Vancouver.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106
Punyakanok, Vasin, Dan Roth, and Wen-tau
Yih. 2004. Mapping dependencies trees:
An application to question answering.
Proceedings of AI&Math 2004, pages 1?10,
Fort Lauderdale, FL.
Riezler, Stefan, Alexander Vasserman,
Ioannis Tsochantaridis, Vibhu Mittal,
and Yi Liu. 2007. Statistical machine
translation for query expansion in
answer retrieval. In Proceedings of the
45th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 464?471, Prague.
Robertson, Stephen and Stephen G. Walker.
1997. On relevance weights with little
relevance information. In Proceedings of
the Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 16?24,
New York, NY.
Shen, Dan and Dietrich Klakow. 2006.
Exploring correlation of dependency
relation paths for answer extraction. In
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 889?896,
Sydney.
Shen, Libin and Aravind K. Joshi. 2005.
Ranking and reranking with perceptron.
Machine Learning. Special Issue on Learning
in Speech and Language Technologies,
60(1):73?96.
Soricut, Radu and Eric Brill. 2006. Automatic
question answering using the Web:
Beyond the factoid. Journal of Information
Retrieval?Special Issue on Web Information
Retrieval, 9(2):191?206.
Surdeanu, Mihai and Massimiliano
Ciaramita. 2007. Robust information
extraction with perceptrons. In Proceedings
of the NIST 2007 Automatic Content
Extraction Workshop (ACE07), College Park,
MD. Available at: http://www.surdeanu.
name/mihai/papers/ace07a.pdf.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Lluis Marquez, and Joakim Nivre.
2008. The CoNLL-2008 shared task on joint
parsing of syntactic and semantic
382
Surdeanu et al Learning to Rank Answers to Non-Factoid Questions fromWeb Collections
dependencies. In Proceedings of the
Conference on Computational Natural
Language Learning (CoNLL), pages 159?177,
Manchester.
Surdeanu, Mihai, Lluis Marquez, Xavier
Carreras, and Pere R. Comas. 2007.
Combination strategies for semantic role
labeling. Journal of Artificial Intelligence
Research, 29:105?151.
Tao, Tao and ChengXiang Zhai. 2007. An
exploration of proximity measures in
information retrieval. In Proceedings of the
30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 259?302,
Amsterdam.
Tsochantaridis, Ioannis, Thomas Hofmann,
Thorsten Joachims, and Yasemin Altun.
2004. Support vector machine learning for
interdependent and structured output
spaces. In ICML ?04: Proceedings of the
Twenty-First International Conference on
Machine Learning, pages 104?111,
New York, NY.
Verberne, Suzan, Lou Boves, Nelleke
Oostdijk, and Peter-Arno Coppen. 2010.
What is not in the bag of words for
why-qa? Computational Linguistics,
36(2):229?245.
Voorhees, Ellen M. 2001. Overview of the
TREC-9 question answering track. In
Proceedings of the Text REtrieval Conference
(TREC) TREC-9 Proceedings, pages 1?15,
Gaithersburg, MD.
Wang, Mengqiu, Noah A. Smith, and Teruko
Mitamura. 2007. What is the Jeopardy
model? A quasi-synchronous grammar for
QA. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning, pages 22?32,
Prague.
Xue, Xiaobing, Jiwoon Jeon, and W. Bruce
Croft. 2008. Retrieval models for question
and answer archives. In Proceedings of the
Annual ACM SIGIR Conference on Research
and Development in Information Retrieval,
pages 475?482, Singapore.
383

A Figure of Merit for the Evaluation of Web-Corpus Randomness
Massimiliano Ciaramita
Institute of Cognitive Science and Technology
National Research Council
Roma, Italy
m.ciaramita@istc.cnr.it
Marco Baroni
SSLMIT
Universita` di Bologna
Forl?`, Italy
baroni@sslmit.unibo.it
Abstract
In this paper, we present an automated,
quantitative, knowledge-poor method to
evaluate the randomness of a collection
of documents (corpus), with respect to a
number of biased partitions. The method
is based on the comparison of the word
frequency distribution of the target corpus
to word frequency distributions from cor-
pora built in deliberately biased ways. We
apply the method to the task of building a
corpus via queries to Google. Our results
indicate that this approach can be used,
reliably, to discriminate biased and unbi-
ased document collections and to choose
the most appropriate query terms.
1 Introduction
The Web is a very rich source of linguistic data,
and in the last few years it has been used in-
tensively by linguists and language technologists
for many tasks (Kilgarriff and Grefenstette, 2003).
Among other uses, the Web allows fast and in-
expensive construction of ?general purpose? cor-
pora, i.e., corpora that are not meant to repre-
sent a specific sub-language, but a language as a
whole. There are several recent studies on the
extent to which Web-derived corpora are com-
parable, in terms of variety of topics and styles,
to traditional ?balanced? corpora (Fletcher, 2004;
Sharoff, 2006). Our contribution, in this paper, is
to present an automated, quantitative method to
evaluate the ?variety? or ?randomness? (with re-
spect to a number of non-random partitions) of
a Web corpus. The more random/less-biased to-
wards specific partitions a corpus is, the more it
should be suitable as a general purpose corpus.
We are not proposing a method to evaluate
whether a sample of Web pages is a random sam-
ple of the Web, although this is a related issue
(Bharat and Broder, 1998; Henzinger et al, 2000).
Instead, we propose a method, based on simple
distributional properties, to evaluate if a sample
of Web pages in a certain language is reasonably
varied in terms of the topics (and, perhaps, tex-
tual types) it contains. This is independent from
whether they are actually proportionally represent-
ing what is out there on the Web or not. For exam-
ple, although computer-related technical language
is probably much more common on the Web than,
say, the language of literary criticism, one might
prefer a biased retrieval method that fetches docu-
ments representing these and other sub-languages
in comparable amounts, to an unbiased method
that leads to a corpus composed mostly of com-
puter jargon. This is a new area of investigation ?
with traditional corpora, one knows a priori their
composition. As the Web plays an increasingly
central role as data source in NLP, we believe that
methods to efficiently characterize the nature of
automatically retrieved data are becoming of cen-
tral importance to the discipline.
In the empirical evaluation of the method, we
focus on general purpose corpora built issuing au-
tomated queries to a search engine and retrieving
the corresponding pages, which has been shown to
be an easy and effective way to build Web-based
corpora (Ghani et al, 2001; Ueyama and Baroni,
2005; Sharoff, 2006). It is natural to ask which
kinds of query terms, henceforth seeds, are more
appropriate to build a corpus comparable, in terms
of variety, to traditional balanced corpora such as
the British National Corpus, henceforth BNC (As-
ton and Burnard, 1998). We test our procedure
to assess Web-corpus randomness on corpora built
217
using seeds chosen following different strategies.
However, the method per se can also be used to as-
sess the randomness of corpora built in other ways;
e.g., by crawling the Web.
Our method is based on the comparison of the
word frequency distribution of the target corpus
to word frequency distributions constructed using
queries to a search engine for deliberately biased
seeds. As such, it is nearly resource-free, as it
only requires lists of words belonging to specific
domains that can be used as biased seeds. In our
experiments we used Google as the search engine
of choice, but different search engines could be
used as well, or other ways to obtain collections
of biased documents, e.g., via a directory of pre-
categorized Web-pages.
2 Relevant work
Our work is related to the recent literature on
building linguistic corpora from the Web using au-
tomated queries to search engines (Ghani et al,
2001; Fletcher, 2004; Ueyama and Baroni, 2005;
Sharoff, 2006). Different criteria are used to se-
lect the seeds. Ghani and colleagues iteratively
bootstrapped queries to AltaVista from retrieved
documents in the target language and in other lan-
guages. They seeded the bootstrap procedure with
manually selected documents, or with small sets
of words provided by native speakers of the lan-
guage. They showed that the procedure produces
a corpus that contains, mostly, pages in the rele-
vant language, but they did not evaluate the results
in terms of quality or variety. Fletcher (2004) con-
structed a corpus of English by querying AltaVista
for the 10 top frequency words from the BNC.
He then conducted a qualitative analysis of fre-
quent n-grams in the Web corpus and in the BNC,
highlighting the differences between the two cor-
pora. Sharoff (2006) built corpora of English, Rus-
sian and German via queries to Google seeded
with manually cleaned lists of words that are fre-
quent in a reference corpus in the relevant lan-
guage, excluding function words, while Ueyama
and Baroni (2005) built corpora of Japanese using
seed words from a basic Japanese vocabulary list.
Both Sharoff and Ueyama and Baroni evaluated
the results through a manual classification of the
retrieved pages and by qualitative analysis of the
words that are most typical of the Web corpora.
We are also interested in evaluating the effect
that different seed selection (or, more in general,
corpus building) strategies have on the nature of
the resulting Web corpus. However, rather than
performing a qualitative investigation, we develop
a quantitative measure that could be used to evalu-
ate and compare a large number of different corpus
building methods, as it does not require manual in-
tervention. Moreover, our emphasis is not on the
corpus building methodology, nor on classifying
the retrieved pages, but on assessing whether they
appear to be reasonably unbiased with respect to a
range of topics or other criteria.
3 Measuring distributional properties of
biased and unbiased collections
Our goal is to create a ?balanced? corpus of Web
pages in a given language; e.g., the portion com-
posed of all Spanish Web pages. As we observed
in the introduction, obtaining a sample of unbi-
ased documents is not the same as obtaining an
unbiased sample of documents. Thus, we will not
motivate our method in terms of whether it favors
unbiased samples from the Web, but in terms of
whether the documents that are sampled appear to
be balanced with respect to a set of deliberately
biased samples. We leave it to further research to
investigate how the choice of the biased sampling
method affects the performance of our procedure
and its relations to uniform sampling.
3.1 Corpora as unigram distributions
A compact way of representing a collection of
documents is by means of frequency lists, where
each word is associated with the number of times
it occurs in the collection. This representation de-
fines a simple ?language model?, a stochastic ap-
proximation to the language of the collection; i.e.,
a ?0th order? word model or a ?unigram? model.
Language models of varying complexity can be
defined. As the model?s complexity increases, its
approximation to the target language improves ?
cf. the classic example of Shannon (1948) on the
entropy of English. In this paper we focus on un-
igram models, as a natural starting point, however
the approach extends naturally to more complex
language models.
3.2 Corpus similarity measure
We start by making the assumption that similar
collections will determine similar language mod-
els, hence that the similarity of collections of doc-
uments is closely related to the similarity of the
218
derived unigram distributions. The similarity of
two unigram distributions P and Q is estimated as
the relative entropy, or Kullback Leibler distance,
or KL (Cover and Thomas, 1991) D(P ||Q):
D(P ||Q) =
?
x?W
P (x) log P (x)Q(x) (1)
KL is a measure of the cost, in terms of aver-
age number of additional bits needed to describe
the random variable, of assuming that the distribu-
tion is Q when instead the true distribution is P .
Since D(P ||Q) ? 0, with equality only if P = Q,
unigram distributions generated by similar collec-
tions should have low relative entropy. To guaran-
tee that KL is always finite we make the assump-
tion that the random variables are defined over the
same finite alphabet W , the set of all word types
occurring in the observed data. To avoid further
infinite cases a smoothing value ? is added when
estimating probabilities; i.e.,
P (x) = cP (x) + ?|W |? +
?
x?W cP (x)
(2)
where cP (x) is the frequency of x in distribution
P, and |W | is the number of word types in W .
3.3 A scoring function for sampled unigram
distributions
What properties distinguish unigram distributions
drawn from the whole of a document collection
such as the BNC or the Web (or, rather, from the
space of the Web we are interested in sampling
from) from distributions drawn from biased sub-
sets of it? This is an important question because,
if identified, such properties might help discrimi-
nating between sampling methods which produce
more random collections of documents from more
biased ones. We suggest the following hypothesis.
Unigrams sampled from the full set of documents
have distances from biased samples which tend
to be lower than the distances of biased samples
to other samples based on different biases. Sam-
ples from the whole corpus, or Web, should pro-
duce lower KL distances because they draw words
across the whole vocabulary, while biased samples
have mostly access to a single specialized vocab-
ulary. If this hypothesis is true then, on average,
the distance between the unbiased sample and all
other samples should be lower than the distance
between a biased sample and all other samples.
2
1
m
b
a2
1
b2
a
b
m
m
l
c
c
c
1a
h
g
A
C
B
Figure 1. Distances (continuous lines with arrows) be-
tween points representing unigram distributions, sam-
pled from biased partitions A and B and from the full
collection of documents C = A ?B.
Figure 1 depicts a geometric interpretation of
the intuition behind this hypothesis. Suppose that
the two squares A and B represent two parti-
tions of the space of documents C. Additionally,
m pairs of unigram distributions, represented as
points, are produced by sampling documents uni-
formly at random from these partitions; e.g. a1
and b1. The mean Euclidean distance between
(ai, bi) pairs is a value between 0 and h, the length
of the diagonal of the rectangle which is the union
of A and B. Instead of drawing pairs we can draw
triples of points, one point from A, one from B,
and another point from C = A ? B. Approxi-
mately half of the points drawn from C will lie in
the A square, while the other half will lie in the B
square. The distance of the points drawn from C
from the points drawn from B will be between 0
and g, for approximately half of the points (those
laying in the B region), while the distance is be-
tween 0 and h for the other half of the points (those
inA). Therefore, ifm is large enough, the average
distance between C and B (or A) must be smaller
than the average distance between A and B, be-
cause h > g.
To summarize, then, we suggest the hypothe-
sis that samples from the full distribution have
a smaller mean distance than all other samples.
More precisely, let Ui,k be the kth of N unigram
distributions sampled with method yi, yi ? Y ,
where Y is the set of sampling categories. Ad-
ditionally, for clarity, we will always denote with
y1 the predicted unbiased sample, while yj , j =
2..|Y |, denote the biased samples. Let M be a
matrix of measurements, M ? IR|Y |?|Y |, such
that Mi.j =
PN
k=1 D(Ui,k,Uj,k)
N , where D(., .) is the
relative entropy. In other words, the matrix con-
tains the average distances between pairs of sam-
219
Mode Domain Genre
1 BNC BNC BNC
2 W S education W miscellaneous
3 S W leisure W pop lore
4 W arts W nonacad soc sci
5 W belief thought W nonacad hum art
.. .. ..
C-4 S spont conv C1 S sportslive
C-3 S spont conv C2 S consultation
C-2 S spont conv DE W fict drama
C-1 S spont conv UN S lect commerce
C no cat no cat
Table 1. Rankings based on ?, as the mean distance
between samples from the BNC partitions plus samples
from the whole corpus (BNC). C is the total number of
categories. W stands for Written, S for Spoken. C1, C2,
DE, UN are demographic classes for the spontaneous
conversations, no cat is the BNC undefined category.
ples (biased or unbiased). Each row Mi ? IR|Y |
contains the average distances between yi and all
other ys, including yi. A score ?i is assigned to
each yi which is equal to the mean of the vector
Mi (excluding Mi,j , j = i, which is always equal
to 0):
?i =
1
|Y | ? 1
|Y |
?
j=1,j 6=i
Mi,j (3)
We propose this function as a figure of merit1
for assigning a score to sampling methods. The
smaller the ? value the closer the sampling method
is to a uniform sampling method, with respect to
the pre-defined set of biased sampling categories.
3.4 Randomness of BNC samples
Later we will show how this hypothesis is consis-
tent with empirical evidence gathered from Web
data. Here we illustrate a proof-of-concept exper-
iment conducted on the BNC. In the BNC docu-
ments come classified along different dimensions
thus providing a controlled environment to test our
hypothesis. We adopt here David Lee?s revised
classification (Lee, 2001) and we partition the doc-
uments in terms of ?mode? (spoken/written), ?do-
main? (19 labels; e.g., imaginative, leisure, etc.)
and ?genre? (71 labels; e.g., interview, advertise-
ment, email, etc.). For each of the three main
partitions we sampled with replacement (from a
distribution determined by relative frequency in
the relevant set) 1,000 words from the BNC and
from each of the labels belonging to the specific
1A function which measures the quality of the sampling
method with the convention that smaller values are better as
with merit functions in statistics.
partitions.2 Then we measured the distance be-
tween each label in a partition, plus the sample
from the whole BNC. We repeated this experiment
100 times, built a matrix of average distances, and
ranked each label yi, within each partition type,
using ?i. Table 1 summarizes the results (only par-
tial results are shown for domain and genre). In all
three experiments the unbiased sample ?BNC? is
ranked higher than all other categories. At the top
of the rankings we also find other less narrowly
topic/genre-dependent categories such as ?W? for
mode, or ?W miscellaneous? and ?W pop lore?
for genre. Thus the hypothesis seems supported by
these experiments. Unbiased sampled unigrams
tend to be closer, on average, to biased samples.
4 Evaluating the randomness of
Google-derived corpora
When downloading documents from the Web via a
search engine (or sample them in other ways), one
cannot choose to sample randomly, nor select doc-
uments belonging to a certain category. One can
try to control the typology of documents returned
by using specific query terms. At this point a mea-
sure such as the one we proposed can be used to
choose the least biased retrieved collection among
a set of retrieved collections.
4.1 Biased and unbiased query categories
To construct a ?balanced? corpus via a search
engine one reasonable strategy is to use appro-
priately balanced query terms, e.g., using ran-
dom terms extracted from an available balanced
corpus (Sharoff, 2006). We will evaluate sev-
eral such strategies by comparing the derived
collections with those obtained with openly bi-
ased/specialized Web corpora. In order to build
specialized domain corpora, we use biased query
terms from the appropriate domain following the
approach of Baroni and Bernardini (2004). We
compiled several lists of words that define likely
biased and unbiased categories. We extracted the
less biased terms from the balanced 1M-words
Brown corpus of American English (Kuc?era and
Francis, 1967), from the 100M-words BNC, and
from a list of English ?basic? terms. From these
resources we defined the following categories of
query terms:
2We filtered out words in a stop list containing 1,430
types, which were either labeled with one of the BNC func-
tion word tags (such as ?article? or ?coordinating conjunc-
tion?), or occurred more than 50,000 times.
220
1. Brown.hf: the top 200 most frequent words
from the Brown corpus;
2. Brown.mf: 200 random terms with fre-
quency between 100 and 50 inclusive from
Brown;
3. Brown.af: 200 random terms with minimum
frequency 10 from Brown;
4. BNC.mf: 200 random terms with frequency
between 506 and 104 inclusive from BNC;
5. BNC.af: 200 random terms from BNC;
6. BNC.demog: 200 random terms with fre-
quency between 1000 and 50 inclusive from
the BNC spontaneous conversation sections;
7. 3esl: 200 random terms from an ESL ?core
vocabulary? list.3
Some of these lists implement plausible strate-
gies to get an unbiased sample from the search
engine: high frequency words and basic vocab-
ulary words should not be linked to any specific
domain; while medium frequency words, such as
the words in the Brown.mf/af and BNC.mf lists,
should be spread across a variety of domains and
styles. The BNC.af list is sampled randomly from
the whole BNC and, because of the Zipfian prop-
erties of word types, coupled with the large size
of the BNC, it is mostly characterized by very low
frequency words. In this case, we might expect
data sparseness problems. Finally, we expect the
spoken demographic sample to be a ?mildly bi-
ased? set, as it samples only words used in spoken
conversational English.
In order to build biased queries, hopefully lead-
ing to the retrieval of topically related documents,
we defined a set of specialized categories us-
ing the WordNet (Fellbaum, 1998) ?domain? lists
(Magnini and Cavaglia, 2000). We selected 200
words at random from each of the following do-
mains: administration, commerce, computer sci-
ence, fashion, gastronomy, geography, law, mili-
tary, music, sociology. These domains were cho-
sen since they look ?general? enough that they
should be very well-represented on the Web, but
not so general as to be virtually unbiased (cf. the
WordNet domain person). We selected words only
among those that did not belong to more than
3http://wordlist.sourceforge.net/
12dicts-readme.html
one WordNet domain, and we avoided multi-word
terms.
It is important to realize that a balanced corpus
is not necessary to produce unbiased seeds, nor a
topic-annotated lexical resource for biased seeds.
Here we focus on these sources to test plausible
candidate seeds. However, biased seeds can be ob-
tained following the method of Baroni and Bernar-
dini (2004) for building specialized corpora, while
unbiased seeds could be selected, for example,
from word lists extracted from all corpora ob-
tained using the biased seeds.
4.2 Experimental setting
From each source list we randomly select 20 pairs
of words without replacement. Each pair is used
as a query to Google, asking for pages in En-
glish only. Pairs are used instead of single words
to maximize our chances to find documents that
contain running text (Sharoff, 2006). For each
query, we retrieve a maximum of 20 documents.
The whole procedure is repeated 20 times with all
lists, so that we can compute the mean distances
to fill the distance matrices. Our unit of analysis
is the corpus of all the non-duplicated documents
retrieved with a set of 20 paired word queries.
The documents retrieved from the Web undergo
post-processing, including filtering by minimum
and maximum size, removal of HTML code and
?boilerplate? (navigational information and simi-
lar) and heuristic filtering of documents that do
not contain connected text. A corpus can con-
tain maximally 400 documents (20 queries times
20 documents retrieved per query), although typi-
cally the documents retrieved are less, because of
duplicates, or because some query pairs are found
in less than 20 documents. Table 2 summarizes
the average size in terms of word types, tokens
and number of documents of the resulting cor-
pora. Queries for the unbiased seeds tend to re-
trieve more documents except for the BNC.af set,
which, as expected, found considerably less data
than the other unbiased sets. Most of the differ-
ences are not statistically significant and, as the ta-
ble shows, the difference in number of documents
is often counterbalanced by the fact that special-
ized queries tend to retrieve longer documents.
4.3 Distance matrices and bootstrap error
estimation
After collecting the data each sample was repre-
sented as a frequency list as we did before with
221
Search category Types Tokens Docs
Brown.hf 39.3 477.2 277.2
Brown.mf 32.8 385.3 261.1
Brown.af 35.9 441.5 262.5
BNC.mf 45.6 614.7 253.6
BNC.af 23.0 241.7 59.7
BNC.demog 32.6 367.1 232.2
3esl 47.1 653.2 261.9
Admin 39.8 545.1 220.5
Commerce 38.9 464.5 184.7
Comp sci 25.8 311.5 185.3
Fashion 44.5 533.7 166.2
Gastronomy 36.5 421.7 159.0
Geography 42.7 498.0 167;6
Law 49.2 745.4 211.4
Military 47.1 667.8 223.0
Music 45.5 558.7 201.3
Sociology 56.0 959.5 258.8
Table 2. Average number of types, tokens and docu-
ments of corpora constructed with Google queries (type
and token sizes in thousands).
the BNC partitions (cf. section 3.4). Unigram dis-
tributions resulting from different search strate-
gies were compared by building a matrix of mean
distances between pairs of unigram distributions.
Rows and columns of the matrices are indexed by
the query category, the first category corresponds
to one unbiased query, while the remaining in-
dexes correspond to the biased query categories;
i.e., M ? IR11?11, Mi,j =
P20
k=1 D(Ui,k,Uj,k)
20 ,
where Us,k is the kth unigram distribution pro-
duced with query category ys.
These Web-corpora can be seen as a dataset D
of n = 20 data-points each consisting of a series
of unigram word distributions, one for each search
category. If all n data-points are used once to build
the distance matrix we obtain one such matrix for
each unbiased category and rank each search strat-
egy yi using ?i, as before (cf. section 3.3). Instead
of using all n data-points once, we createB ?boot-
strap? datasets (Duda et al, 2001) by randomly se-
lecting n data-points fromD with replacement (we
used a value of B=10). The B bootstrap datasets
are treated as independent sets and used to produce
B individual matricesMb from which we compute
the score ?i,b, i.e., the mean distance of a category
yi with respect to all other query categories in that
specific bootstrap dataset. The bootstrap estimate
of ?i, called ??i is the mean of the B estimates on
the individual datasets:
??i =
1
B
B
?
b=1
?i,b (4)
Bootstrap estimation can be used to compute the
standard error of ?i:
?boot[?i] =
?
?
?
?
1
B
B
?
b=1
[??i ? ?i,b]2 (5)
Instead of building one matrix of average dis-
tances over N trials, we could build N matri-
ces and compute the variance from there rather
than with bootstrap methods. However this sec-
ond methodology produces noisier results. The
reason for this is that our hypothesis rests on the
assumption that the estimated average distance is
reliable. Otherwise, the distance of two arbitrary
biased distributions can very well be smaller than
the distance of one unbiased and a biased one, pro-
ducing noisier measurements.
As we did before for the BNC data, we
smoothed the word counts by adding a count of 1
to all words in the overall dictionary. This dictio-
nary is approximated with the set of all words oc-
curring in the unigrams involved in a given exper-
iment, overall on average approximately 1.8 mil-
lion types (notice that numbers and other special
tokens are boosting up this total). Words with an
overall frequency greater than 50,000 are treated
as stop words and excluded from consideration
(188 types).
5 Results
Table 3 summarizes the results of the experiments
with Google. Each column represents one experi-
ment involving a specific ? supposedly ? unbiased
category. The category with the best (lowest) ?
score is highlighted in bold. The unbiased sample
is always ranked higher than all biased samples.
The results show that the best results are achieved
with Brown corpus seeds. The bootstrapped er-
ror estimate shows that the unbiased Brown sam-
ples are significantly more random than the biased
samples and, orthogonally, of the BNC and 3esl
samples. In particular medium frequency terms
seem to produce the best results, although the dif-
ference among the three Brown categories are not
significant. Thus, while more testing is needed,
our data provide some support for the choice of
medium frequency words as best seeds.
Terms extracted from the BNC are less effec-
tive than terms from the Brown corpus. One pos-
sible explanation is that the Web is likely to con-
tain much larger portions of American than British
English, and thus the BNC queries are overall
222
? scores with bootstrap error estimates
Category Brown.mf Brown.af Brown.hf BNC.mf BNC.demog BNC.all 3esl
Unbiased .1248/.0015 .1307/.0019 .1314/.0010 .1569/.0025 .1616/.0026 .1635/.0026 .1668/.0030
Commerce .1500/.0074 .1500/.0074 .1500/.0073 .1708/.0088 .1756/.0090 .1771/.0091 .1829/.0093
Geography .1702/.0084 .1702/.0084 .1707/.0083 .1925/.0089 .1977/.0091 .1994/.0092 .2059/.0094
Fashion .1732/.0060 .1732/.0060 .1733/.0059 .1949/.0069 .2002/.0070 .2019/.0071 .2087/.0073
Admin .1738/.0034 .1738/.0034 .1738/.0033 .2023/.0037 .2079/.0038 .2096/.0038 .2163/.0039
Comp sci .1749/.0037 .1749/.0037 .1746/.0038 .1858/.0041 .1912/.0042 .1929/.0042 .1995/.0043
Military .1899/.0070 .1899/.0070 .1901/.0067 .2233/.0079 .2291/.0081 .2311/.0082 .2384/.0084
Music .1959/.0067 .1959/.0067 .1962/.0067 .2196/.0077 .2255/.0078 .2274/.0079 .2347/.0081
Gastronomy .1973/.0122 .1973/.0122 .1981/.0120 .2116/.0133 .2116/.0133 .2193/.0138 .2266/.0142
Law .1997/.0060 .1997/.0060 .1990/.0061 .2373/.0067 .2435/.0068 .2193/.0138 .2533/.0070
Sociology .2393/.0063 .2393/.0063 .2389/.0062 .2885/.0069 .2956/.0070 .2980/.0071 .3071/.0073
Table 3. Mean scores based on ? with bootstrap standard error (B=10). In bold the lowest (best) score in each
column, always the unbiased category.
more biased than the Brown queries. Alterna-
tively, this might be due to the smaller, more con-
trolled nature of the Brown corpus, where even
medium- and low-frequency words tend to be rel-
atively common terms. The internal ranking of the
BNC categories, although not statistically signifi-
cant, seems also to suggest that medium frequency
words (BNC.mf) are better than low frequency
words. In this case, the all/low frequency set
(BNC.af) tends to contain very infrequent words;
thus, the poor performance is likely due to data
sparseness issues, as also indicated by the rela-
tively smaller quantity of data retrieved (Table 2
above). We take the comparatively lower rank
of BNC.demog to constitute further support for
the validity of our method, given that the corre-
sponding set, being entirely composed of words
from spoken English, should be more biased than
other unbiased sets. This latter finding is partic-
ularly encouraging because the way in which this
set is biased, i.e., in terms of mode of communica-
tion, is completely different from the topic-based
bias of the WordNet sets. Finally, the queries
extracted from the 3esl set are the most biased.
This unexpected result might relate to the fact
that, on a quick inspection, many words in this
set, far from being what we would intuitively con-
sider ?core? vocabulary, are rather cultivated, of-
ten technical terms (aesthetics, octopi, misjudg-
ment, hydroplane), and thus they might show a
register-based bias that we do not find in lists
extracted from balanced corpora. We randomly
selected 100 documents from the corpora con-
structed with the ?best? unbiased set (Brown.mf)
and 100 documents from this set, and we classi-
fied them in terms of genre, topic and other cat-
egories (in random order, so that the source of
the rated documents was not known). This pre-
liminary analysis did not highlight dramatic dif-
ferences between the two corpora, except for the
fact that 6 over 100 documents in the 3esl sub-
corpus pertained to the rather narrow domain of
aviation and space travel, while no comparably
narrow topic had such a large share of the distri-
bution in the Brown.mf sub-corpus. More research
is needed into the qualitative differences that cor-
relate with our figure of merit. Finally, although
different query sets retrieve different amounts of
documents, and lead to the construction of corpora
of different lengths, there is no sign that these dif-
ferences are affecting our figure of merit in a sys-
tematic way; e.g., some of the larger collections,
in terms of number of documents and token size,
are both at the top (most unbiased samples) and at
the bottom of the ranks (law, sociology).
On Web data we observed the same effect we
saw with the BNC data, where we could directly
sample from the whole collection and from its bi-
ased partitions. This provides support for the hy-
pothesis that our measure can be used to evaluate
how unbiased a corpus is, and that issuing unbi-
ased/biased queries to a search engine is a viable,
nearly knowledge-free way to create unbiased cor-
pora, and biased corpora to compare them against.
6 Conclusion
As research based on the Web as corpus becomes
more prominent within computational and corpus-
based linguistics, many fundamental issues have
to be tackled in a systematic way. Among these,
the problem of assessing the quality and nature
of automatically created corpora, where we do
not know a priori the composition of the cor-
pus. In this paper, we considered an approach to
automated corpus construction, via search engine
queries for combinations of a set of seed words.
223
We proposed an automated, quantitative, nearly
knowledge-free way to evaluate how biased a cor-
pus constructed in this way is. Our method is
based on the idea that the more a collection is un-
biased the closer its distribution of words will be,
on average, to reference distributions derived from
biased partitions (we showed that this is indeed the
case using a fully available balanced collection;
i.e., the BNC), and on the idea that biased collec-
tions of Web documents can be created by issu-
ing biased queries to a search engine. The results
of our experiments with Google support our hy-
pothesis, and suggest that seeds to build unbiased
corpora should be selected among mid-frequency
words rather than high or low frequency words.
We realize that our study opens many ques-
tions. The most crucial issue is probably what it
means for a corpus to be unbiased. As we already
stressed, we do not necessarily want our corpus
to be an unbiased sample of what is out there on
the Net ? we want it to be composed of content-
rich pages, and reasonably balanced in terms of
topics and genres, despite the fact that the Web
itself is unlikely to be ?balanced?. For our pur-
poses, we implicitly define balance in terms of the
set of biased corpora that we compare the target
corpus against. Assuming that our measure is ap-
propriate, what it tells us is that a certain corpus is
more/less biased than another corpus with respect
to the biased corpora they are compared against. It
remains to be seen how well the results generalize
across different typologies of biased corpora.
The method is not limited to the evaluation of
corpora built via search engine queries; e.g., it
would be interesting to compare the latter to cor-
pora built by Web crawling. The method could
be also applied to the analysis of corpora in gen-
eral (Web-derived or not), both for the purpose of
evaluating biased-ness, and as a general purpose
corpus comparison technique (Kilgarriff, 2001).
Acknowledgments
We would like to thank Ioannis Kontoyiannis,
Adam Kilgarriff and Silvia Bernardini for useful
comments on this work.
References
G. Aston and L. Burnard. 1998. The BNC Handbook:
Exploring the British National Corpus with SARA.
Edinburgh University Press, Edinburgh.
M. Baroni and S. Bernardini. 2004. BootCaT: Boot-
strapping Corpora and Terms from the Web. In Pro-
ceedings of LREC 2004, pages 1313?1316.
K. Bharat and A. Broder. 1998. A Technique for Mea-
suring the Relative Size and Overlap of the Public
Web Search Engines. In Proceedings of WWW7,
pages 379?388.
T.M. Cover and J.A. Thomas. 1991. Elements of In-
formation Theory. Wiley, New York.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification 2nd ed. Wiley Interscience, Wiley In-
terscience.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
B. Fletcher. 2004. Making the Web more Useful as
a Source for Linguistic Corpora. In U. Conor and
T. Upton, editors, Corpus Linguistics in North Amer-
ica 2002. Rodopi, Amsterdam.
R. Ghani, R. Jones, and D. Mladenic. 2001. Using
the Web to Create Minority Language Corpora. In
Proceedings of the 10th International Conference on
Information and Knowledge Management.
M. Henzinger, A. Heydon, and M. Najork. 2000. On
Near-Uniform URL Sampling. In Proceedings of
WWW9.
A. Kilgarriff and G. Grefenstette. 2003. Introduction
to the Special Issue on the Web as Corpus. Compu-
tational Linguistics, 29:333?347.
A. Kilgarriff. 2001. Comparing Corpora. Interna-
tional Journal of Corpus Linguistics, 6:1?37.
H. Kuc?era and W. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown Uni-
versity Press, Providence, RI.
D. Lee. 2001. Genres, Registers, Text, Types, Do-
mains and Styles: Clarifying the Concepts and Nav-
igating a Path through the BNC Jungle. Language
Learning & Technology, 5(3):37?72.
B. Magnini and G. Cavaglia. 2000. Integrating Subject
Field Codes into WordNet. In Proceedings of LREC
2000, Athens, pages 1413?1418.
C.E. Shannon. 1948. A Mathematical Theory of Com-
munication. Bell System Technical Journal, 27:379?
423 and 623?656.
S. Sharoff. 2006. Creating General-Purpose Corpora
Using Automated Search Engine Queries. In M. Ba-
roni and S. Bernardini, editors, WaCky! Working pa-
pers on the Web as Corpus. Gedit, Bologna.
M. Ueyama and M. Baroni. 2005. Automated Con-
struction and Evaluation of a Japanese Web-Based
Reference Corpus. In Proceedings of Corpus Lin-
guistics 2005.
224
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 594?602,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Broad-Coverage Sense Disambiguation and Information Extraction with
a Supersense Sequence Tagger?
Massimiliano Ciaramita
Inst. of Cognitive Science and Technology
Italian National Research Council
m.ciaramita@istc.cnr.it
Yasemin Altun
Toyota Technological Institute
at Chicago
altun@tti-c.org
Abstract
In this paper we approach word sense
disambiguation and information extrac-
tion as a unified tagging problem. The
task consists of annotating text with the
tagset defined by the 41 Wordnet super-
sense classes for nouns and verbs. Since
the tagset is directly related to Wordnet
synsets, the tagger returns partial word
sense disambiguation. Furthermore, since
the noun tags include the standard named
entity detection classes ? person, location,
organization, time, etc. ? the tagger, as
a by-product, returns extended named en-
tity information. We cast the problem of
supersense tagging as a sequential label-
ing task and investigate it empirically with
a discriminatively-trained Hidden Markov
Model. Experimental evaluation on the
main sense-annotated datasets available,
i.e., Semcor and Senseval, shows consid-
erable improvements over the best known
?first-sense? baseline.
1 Introduction
Named entity recognition (NER) is the most stud-
ied information extraction (IE) task. NER typi-
cally focuses on detecting instances of ?person?,
?location?, ?organization? names and optionally
instances of ?miscellaneous? or ?time? categories.
The scalability of statistical NER allowed re-
searchers to apply it successfully on large col-
lections of newswire text, in several languages,
and biomedical literature. Newswire NER per-
formance, in terms of F-score, is in the upper
?The first author is now at Yahoo! Research. The tag-
ger described in this paper is free software and can be down-
loaded from http://www.loa-cnr.it/ciaramita.html.
80s (Carreras et al, 2002; Florian et al, 2003),
while Bio-NER accuracy ranges between the low
70s and 80s, depending on the data-set used for
training/evaluation (Dingare et al, 2005). One
shortcoming of NER is its over-simplified onto-
logical model, leaving instances of other poten-
tially informative categories unidentified. Hence,
the utility of named entity information is limited.
In addition, instances to be detected are mainly re-
stricted to (sequences of) proper nouns.
Word sense disambiguation (WSD) is the task
of deciding the intended sense for ambiguous
words in context. With respect to NER, WSD
lies at the other end of the semantic tagging spec-
trum, since the dictionary defines tens of thou-
sand of very specific word senses, including NER
categories. Wordnet (Fellbaum, 1998)1, possibly
the most used resource for WSD, defines word
senses for verbs, common and proper nouns. Word
sense disambiguation, at this level of granularity,
is a complex task which resisted all attempts of
robust broad-coverage solutions. Many distinc-
tions are too subtle to be captured automatically,
and the magnitude of the class space ? several
orders larger than NER?s ? makes it hard to ap-
proach the problem with sophisticated, but scal-
able, machine learning methods. Lastly, even if
the methods would scale up, there are not enough
manually tagged data, at the word sense level, for
training a model. The performance of state of
the art WSD systems on realistic evaluations is
only comparable to the ?first sense? baseline (cf.
Section 5.3). Notwithstanding much research, the
benefits of disambiguated lexical information for
language processing are still mostly speculative.
This paper presents a novel approach to broad-
1When referring to Wordnet, throughout the paper, we
mean Wordnet version 2.0.
594
NOUNS
SUPERSENSE NOUNS DENOTING SUPERSENSE NOUNS DENOTING
act acts or actions object natural objects (not man-made)
animal animals quantity quantities and units of measure
artifact man-made objects phenomenon natural phenomena
attribute attributes of people and objects plant plants
body body parts possession possession and transfer of possession
cognition cognitive processes and contents process natural processes
communication communicative processes and contents person people
event natural events relation relations between people or things or ideas
feeling feelings and emotions shape two and three dimensional shapes
food foods and drinks state stable states of affairs
group groupings of people or objects substance substances
location spatial position time time and temporal relations
motive goals Tops abstract terms for unique beginners
VERBS
SUPERSENSE VERBS OF SUPERSENSE VERBS OF
body grooming, dressing and bodily care emotion feeling
change size, temperature change, intensifying motion walking, flying, swimming
cognition thinking, judging, analyzing, doubting perception seeing, hearing, feeling
communication telling, asking, ordering, singing possession buying, selling, owning
competition fighting, athletic activities social political and social activities and events
consumption eating and drinking stative being, having, spatial relations
contact touching, hitting, tying, digging weather raining, snowing, thawing, thundering
creation sewing, baking, painting, performing
Table 1. Nouns and verbs supersense labels, and short description (from the Wordnet documentation).
coverage information extraction and word sense
disambiguation. Our goal is to simplify the disam-
biguation task, for both nouns and verbs, to a level
at which it can be approached as any other tagging
problem, and can be solved with state of the art
methods. As a by-product, this task includes and
extends NER. We define a tagset based on Word-
net?s lexicographers classes, or supersenses (Cia-
ramita and Johnson, 2003), cf. Table 1. The size
of the supersense tagset alows us to adopt a struc-
tured learning approach, which takes local depen-
dencies between labels into account. To this ex-
tent, we cast the supersense tagging problem as a
sequence labeling task and train a discriminative
Hidden Markov Model (HMM), based on that of
Collins (2002), on the manually annotated Semcor
corpus (Miller et al, 1993). In two experiments
we evaluate the accuracy of the tagger on the Sem-
cor corpus itself, and on the English ?all words?
Senseval 3 shared task data (Snyder and Palmer,
2004). The model outperforms remarkably the
best known baseline, the first sense heuristic ? to
the best of our knowledge, for the first time on the
most realistic ?all words? evaluation setting.
The paper is organized as follows. Section 2
introduces the tagset, Section 3 discusses related
work and Section 4 the learning model. Section 5
reports on experimental settings and results. In
Section 6 we summarize our contribution and con-
sider directions for further research.
2 Supersense tagset
Wordnet (Fellbaum, 1998) is a broad-coverage
machine-readable dictionary which includes
11,306 verbs mapped to 13,508 word senses,
called synsets, and 114,648 common and proper
nouns mapped to 79,689 synsets. Each noun or
verb synset is associated with one of 41 broad
semantic categories, in order to organize the
lexicographer?s work of updating and managing
the lexicon (see Table 1). Since each lexicog-
rapher category groups together many synsets
they have been also called supersenses (Ciaramita
and Johnson, 2003). There are 26 supersenses
for nouns, 15 for verbs. This coarse-grained
ontology has a number of attractive features, for
the purpose of natural language processing. First,
the small size of the set makes it possible to build
a single tagger which has positive consequences
on robustness. Second, classes, although fairly
general, are easily recognizable and not too
abstract or vague. More importantly, similar word
senses tend to be merged together.
As an example, Table 2 summarizes all senses
of the noun ?box?. The 10 synsets are mapped
to 6 supersenses: ?artifact?, ?quantity?, ?shape?,
?state?, ?plant?, and ?act?. Three similar senses
(2), (7) and (9), and the probably related (8), are
merged in the ?artifact? supersense. This process
can help disambiguation because it removes sub-
595
1. {box} (container) ?he rummaged through a box of
spare parts? - n.artifact
2. {box, loge} (private area in a theater or grandstand
where a small group can watch the performance) ?the
royal box was empty? - n.artifact
3. {box, boxful} (the quantity contained in a box) ?he
gave her a box of chocolates? - n.quantity
4. {corner, box} (a predicament from which a skillful or
graceful escape is impossible) ?his lying got him into a
tight corner? - n.state
5. {box} (a rectangular drawing) ?the flowchart contained
many boxes? - n.shape
6. {box, boxwood} (evergreen shrubs or small trees) -
n.plant
7. {box} (any one of several designated areas on a ball
field where the batter or catcher or coaches are posi-
tioned) ?the umpire warned the batter to stay in the bat-
ter?s box? - n.artifact
8. {box, box seat} (the driver?s seat on a coach) ?an armed
guard sat in the box with the driver? - n.artifact
9. {box} (separate partitioned area in a public place for a
few people) ?the sentry stayed in his box to avoid the
cold? - n.artifact
10. {box} (a blow with the hand (usually on the ear)) ?I
gave him a good box on the ear? - n.act
Table 2. The noun ?box? in Wordnet: each line lists one
synset, the set of synonyms, a definition, an optional
example sentence, and the supersense label.
tle distinctions, which are hard to discriminate and
increase the size of the class space. One possi-
ble drawback is that senses which one might want
to keep separate, e.g., the most common sense
box/container (1), can be collapsed with others.
One might argue that all ?artifact? senses share
semantic properties which differentiate them from
the other senses and can support useful semantic
inferences. Unfortunately, there are no general so-
lutions to the problem of sense granularity. How-
ever, major senses identified by Wordnet are main-
tained at the supersense level. Hence, supersense-
disambiguated words are also, at least partially,
synset-disambiguated.
Since Wordnet includes both proper and com-
mon nouns, the new tagset suggests an extended
notion of named entity. As well as the usual
NER categories, ?person?, ?group?, ?location?,
and ?time?2, supersenses include categories such
as artifacts, which can be fairly frequent, but usu-
ally neglected. To a greater extent than in stan-
dard NER, research in Bio-NER has focused on
the adoption of richer ontologies for information
extraction. Genia (Ohta et al, 2002), for exam-
ple, is an ontology of 46 classes ? with annotated
2The supersense category ?group? is rather a superordi-
nate of ?organization? and has wider scope.
corpus ? designed for supporting information ex-
traction in the molecular biology domain. In addi-
tion, there is growing interest for extracting rela-
tions between entities, as a more useful type of IE
(cf. (Rosario and Hearst, 2004)).
Supersense tagging is inspired by similar con-
siderations, but in a domain-independent setting;
e.g., verb supersenses can label semantic interac-
tions between nominal concepts. The following
sentence (Example 1), extracted from the data ?
further described in Section 5.1 ? shows the infor-
mation captured by the supersense tagset:
(1) Clara Harrisn.person, one of the
guestsn.person in the boxn.artifact, stood
upv.motion and demandedv.communication
watern.substance.
As Example 1 shows there is more information
that can be extracted from a sentence than just
the names; e.g. the fact that ?Clara Harris? and
the following ?guests? are both tagged as ?person?
might suggest some sort of co-referentiality, while
the coordination of verbs of motion and commu-
nication, as in ?stood up and demanded?, might be
useful for language modeling purposes. In such a
setting, structured learning methods, e.g., sequen-
tial, can help tagging by taking the senses of the
neighboring words into account.
3 Related Work
Sequential models are common in NER, POS tag-
ging, shallow parsing, etc.. Most of the work in
WSD, instead, has focused on labeling each word
individually, possibly revising the assignments of
senses at the document level; e.g., following the
?one sense per discourse? hypothesis (Gale et al,
1992). Although it seems reasonable to assume
that occurrences of word senses in a sentence can
be correlated, hence that structured learning meth-
ods could be successful, there has not been much
work on sequential WSD. Segond et al (1997) are
possibly the first to have applied an HMM tag-
ger to semantic disambiguation. Interestingly, to
make the method more tractable, they also used
the supersense tagset and estimated the model on
Semcor. By cross-validation they show a marked
improvement over the first sense baseline. How-
ever, in (Segond et al, 1997) the tagset is used dif-
ferently, by defining equivalence classes of words
with the same set of senses. From a similar per-
spective, de Loupy et al (de Loupy et al, 1998)
596
also investigated the potential advantages of using
HMMs for disambiguation. More recently, vari-
ants of the generative HMM have been applied to
WSD (Molina et al, 2002; Molina et al, 2004)
and evaluated also on Senseval data, showing per-
formance comparable to the first sense baseline.
Previous work on prediction at the supersense
level (Ciaramita and Johnson, 2003; Curran, 2005)
has focused on lexical acquisition (nouns exclu-
sively), thus aiming at word type classification
rather than tagging. As far as applications are con-
cerned, it has been shown that supersense infor-
mation can support supervised WSD, by provid-
ing a partial disambiguation step (Ciaramita et al,
2003). In syntactic parse re-ranking supersenses
have been used to build useful latent semantic fea-
tures (Koo and Collins, 2005). We believe that
supersense tagging has the potential to be useful,
in combination with other sources of information
such as part of speech, domain-specific NER mod-
els, chunking or shallow parsing, in tasks such
as question answering and information extraction
and retrieval, where large amounts of text need
to be processed. It is also possible that this kind
of shallow semantic information can help build-
ing more sophisticated linguistic analysis as in full
syntactic parsing and semantic role labeling.
4 Sequence Tagging
We take a sequence labeling approach to learn-
ing a model for supersense tagging. Our goal is
to learn a function from input vectors, the obser-
vations from labeled data, to response variables,
the supersense labels. POS tagging, shallow pars-
ing, NP-chunking and NER are all examples of
sequence labeling tasks in which performance can
be significantly improved by optimizing the choice
of labeling over whole sequences of words, rather
than individual words. The limitations of the gen-
erative approach to sequence tagging, i. e. Hidden
Markov Models, have been overcome by discrim-
inative approaches proposed in recent years (Mc-
Callum et al, 2000; Lafferty et al, 2001; Collins,
2002; Altun et al, 2003). In this paper we apply
perceptron trained HMMs originally proposed in
(Collins, 2002).
4.1 Perceptron-trained HMM
HMMs define a probabilistic model for observa-
tion/label sequences. The joint model of an obser-
vation/label sequence (x,y), is defined as:
P (y,x) =
?
i
P (yi|yi?1)P (xi|yi)), (2)
where yi is the ith label in the sequence and xi is
the ith word. In the NLP literature, a common ap-
proach is to model the conditional distribution of
label sequences given the label sequences. These
models have several advantages over generative
models, such as not requiring questionable inde-
pendence assumptions, optimizing the conditional
likelihood directly and employing richer feature
representations. This task can be represented as
learning a discriminant function F : X ?Y ? IR,
on a training data of observation/label sequences,
where F is linear in a feature representation ? de-
fined over the joint input/output space
F (x,y;w) = ?w,?(x,y)?. (3)
? is a global feature representation, mapping each
(x,y) pair to a vector of feature counts ?(x,y) ?
IRd, where d is the total number of features. This
vector is given by
?(x,y) =
d?
i=1
|y|?
j=1
?i(yj?1, yj ,x). (4)
Each individual feature ?i typically represents a
morphological, contextual, or syntactic property,
or also the inter-dependence of consecutive la-
bels. These features are described in detail in Sec-
tion 4.2. Given an observation sequence x, we
make a prediction by maximizing F over the re-
sponse variables:
fw(x) = argmax
y?Y
F (x,y;w). (5)
This involves computing the Viterbi decoding with
respect to the parameter vector w ? IRd. The
complexity of the Viterbi algorithm scales linearly
with the length of the sequence.
There are different ways of estimatingw for the
described model. We use the perceptron algorithm
for sequence tagging (Collins, 2002). The per-
ceptron algorithm focuses on minimizing the error
rate, without involving any normalization factors.
This property makes it very efficient which is a de-
sirable feature in a task dealing with a large tagset
such as ours. Additionally, the performance of
perceptron-trained HMMs is very competitive on
a number of tasks; e.g., in shallow parsing, where
597
Algorithm 1 Hidden Markov average perceptron
algorithm.
1: Initialize w0 = ~0
2: for t = 1...., T do
3: Choose xi
4: Compute y? = argmaxy?Y F (xi,y;w)
5: if yi 6= y? then
6: wt+1 ? wt + ?(xi,yi)? ?(xi, y?)
7: end if
8: w = 1T
?
twt
9: end for
10: return w
the perceptron performance is comparable to that
of Conditional Random Field models (Sha and
Pereira, 2003), The tendency to overfit of the per-
ceptron can be mitigated in a number of ways in-
cluding regularization and voting. Here we apply
averaging and straightforwardly extended Collins
algorithm, summarized in Algorithm 1.
4.2 Features
We used the following combination of
spelling/morphological and contextual fea-
tures. For each observed word xi in the data ?
extracts the following features:
1. Words: xi, xi?1, xi?2, xi+1, xi+2;
2. First sense: supersense baseline prediction
for xi, fs(xi), cf. Section 5.3;
3. Combined (1) and (2): xi + fs(xi);
4. Pos: posi (the POS of xi), posi?1, posi?2,
posi+1, posi+2, posi[0], posi?1[0], posi?2[0],
posi+1[0], posi+2[0], pos commi if xi?s POS
tags is ?NN? or ?NNS? (common nouns), and
pos propi if xi?s POS is ?NNP? or ?NNPS?
(proper nouns);
5. Word shape: sh(xi), sh(xi?1), sh(xi?2),
sh(xi+1), sh(xi+2), where sh(xi) is as
described below. In addition shi = low
if the first character of xi is lowercase,
shi = cap brk if the first character of xi is up-
percase and xi?1 is a full stop, question or
exclamation mark, or xi is the first word of
the sentence, shi = cap nobrk otherwise;
6. Previous label: supersense label yi?1.
Word features (1) are morphologically simplified
using the morphological functions of the Word-
net library. The first sense feature (2) is the label
predicted for xi by the baseline model, cf. Sec-
tion 5.3. POS labels (4) were generated using
Brants? TnT tagger (Brants, 2002). POS features
of the form posi[0] extract the first character from
the POS label, thus providing a simplified repre-
sentation of the POS tag. Finally, word shape fea-
tures (5) are regular expression-like transforma-
tion in which each character c of a string s is sub-
stituted with X if c is uppercase, if lowercase, c
is substituted with x, if c is a digit it is substituted
with d and left as it is otherwise. In addition each
sequence of two or more identical characters c is
substituted with c?. For example, for s = ?Merrill
Lynch& Co.?, sh(s) = Xx ? Xx ?&Xx..
Exploratory experiments with richer feature
sets, including syntactic information, affixes, and
topic labels associated with words, did not result
in improvements in terms of performance. While
more experiments are needed to investigate the
usefulness of other sources of information, the fea-
ture set described above, while basic, offers good
generalization properties.
5 Experiments
5.1 Data
We experimented with the following data-sets3.
The Semcor corpus (Miller et al, 1993), a frac-
tion of the Brown corpus (Kuc?era and Francis,
1967) which has been manually annotated with
Wordnet synset labels. Named entities of the cat-
egories ?person?, ?location? and ?group? are also
annotated. The original annotation with Wordnet
1.6 synset IDs has been converted to the most re-
cent version 2.0 of Wordnet. Semcor is divided
in three parts: ?brown1? and ?brown2?, here re-
ferred to as ?SEM?, in which nouns, verbs, adjec-
tives and adverbs are annotated. In addition, the
section ?brownv?, ?SEMv? here, contains annota-
tions only for verbs. We also experimented with
the Senseval-3 English all-words tasks data (Sny-
der and Palmer, 2004), here called ?SE3?. The
Senseval all-words task evaluates the performance
of WSD systems on all open class words in com-
plete documents. The Senseval-3 data consists of
two Wall Street Journal Articles, ?wsj 1778? and
3These datasets are available in a con-
sistent format and can be downloaded from
http://www.cs.unt.edu/ rada/downloads.html
598
Dataset
Counts SE3 SEM SEMv
Sentences 300 20,138 17,038
Tokens 5,630 434,774 385,546
Supersenses 1,617 135,135 40,911
Verbs 725 47,710 40,911
Nouns 892 87,425 0
Avg-poly-N-WS 4.66 4.41 4.33
Avg-poly-N-SS 2.86 2.75 2.66
Avg-poly-V-WS 11.17 10.87 11.05
Avg-poly-V-SS 4.20 4.11 4.16
Table 3. Statistics of the datasets. The row ?Super-
senses? lists the number of instances of supersense
labels, partitioned, in the following two rows, between
verb and noun supersense labels. The lowest four rows
summarize average polysemy figures at the synset and
supersense level for both nouns and verbs.
?wsj 1695?, and a fiction excerpt, ?cl 23?, from
the unannotated portion of the Brown corpus. Ta-
ble 3 summarizes a few statistics about the compo-
sition of the datasets. The four lower rows report
the average polysemy of nouns (?N?) and verbs
(?V?), in each dataset, both at the synset level
(?WS?) and supersense (?SS?) level. The average
number of senses decreases significantly when the
more general sense inventory is considered.
We substituted the corresponding supersense to
each noun and verb synset in all three data-sets:
SEM, SEMv and SE3. All other tokens were
labeled ?0?. The supersense label ?noun.Tops?
refers to 45 synsets which lie at the very top
of the Wordnet noun hierarchy. Some of these
synsets are expressed by very general nouns such
as ?biont?, ?benthos?, ?whole?, and ?nothing?.
However, others undoubtedly refer to other super-
senses, for which they provide the label, such as
?food?, ?person?, ?plant? or ?animal?. Since these
nouns tend to be fairly frequent, it is confusing
and inconsistent to label them ?noun.Tops?; e.g.,
nouns such as ?chowder? and ?Swedish meatball?
would be tagged as ?noun.food?, but the noun
?food? would be tagged as ?noun.Tops?. For this
reason, in all obvious cases, we substituted the
?noun.Tops? label with the more specific super-
sense label for the noun4.
The SEMv dataset only includes supersense la-
bels for verbs. In order to avoid unwanted false
negatives, that is, thousands of nouns labeled ?0?,
4The nouns which are left with the ?noun.Top? label are:
entity, thing, anything, something, nothing, object, living
thing, organism, benthos, heterotroph, life, and biont.
we applied the following procedure. Rather than
using the full sentences from the SEMv dataset,
from each sentence we generated the fragments in-
cluding a verb but no common or proper nouns;
e.g., from a sentence such as ?Karns? ruling per-
tainedverb.stative to eight of the 10 cases.? only the
fragment ?pertainedverb.stative to eight of the 10?
is extracted and used for training.
Sometimes more than one label is assigned to
a word, in all data-sets. In these cases we adopted
the heuristic of only using the first label in the data
as the correct synset/supersense. We leave the ex-
tension of the tagger to the multilabel case for fu-
ture research. As for now, we can expect that this
solution will simply lower, somewhat, both the
baseline and the tagger performance. Finally, we
adopted a beginning (B) and continuation of entity
(I) plus no label (0), encoding; i.e., the actual class
space defines 83 labels.
5.2 Setup
The supersense tagger was trained on the Semcor
datasets SEM and SEMv. The only free parame-
ter to set in evaluation is the number of iterations
to perform T (cf. Algorithm 1). We evaluated the
model?s accuracy on Semcor by splitting the SEM
data randomly in training, development and evalu-
ation. In a 5-fold cross-validation setup the tagger
was trained on 4/5 of the SEM data, the remain-
ing data was split in two halves, one used to fix T
the other for evaluating performance on test. The
full SEMv data was always added to the training
portion of SEM. We also evaluated the model on
the Senseval-3 data, using the same value for T set
by cross-validation on the SEM data5. The order-
ing of the training instances is randomized across
different runs, therefore the algorithm outputs dif-
ferent results after each run, even if the evaluation
set is fixed, as is the case for the Senseval evalu-
ation. The variance in the results on the SE3 data
was measured in this way.
5.3 Baseline tagger
The first sense baseline is the supersense of the
most frequent synset for a word, according to
Wordnet?s sense ranking. This baseline is very
competitive inWSD tasks, and it is extremely hard
to improve upon even slightly. In fact, the baseline
has been proposed as a good alternative to WSD
5On average T is equal to 12 times the size of the training
data.
599
Semcor Senseval-3
Method Recall Precision F-score [?] Recall Precision F-score [?]
Rand 42.99 38.17 40.44 42.09 35.84 38.70
Baseline 69.25 63.90 66.47 68.65 60.10 64.09
Supersense-Tagger 77.71 76.65 77.18 0.45 73.74 67.60 70.54 0.21
Table 4. Summary of results for random and first sense baselines and supersense tagger, ? is the standard error
computed on the five trials results.
altogether (cf. (McCarthy et al, 2004)). For this
reason we include the first sense prediction as one
of the features of our tagging model.
We apply the heuristic as follows. First, in each
sentence, we identify the longest sequence which
has an entry in Wordnet as either noun or verb.
We carry out this step using the Wordnet?s library
functions, which perform also morphological sim-
plification. Hence, in Example 1 the entry ?stand
up? is detected, although also ?stand? has an en-
try in Wordnet. Then, each word identified in
this way is assigned its most frequent sense ? the
only one available if the word is unambiguous. To
reduce the number of candidate supersenses we
distinguish between common and proper nouns;
e.g. ?Savannah? (city/river) is distinguished from
?savannah? (grassland). This method improves
slightly the accuracy of the baseline which does
not distinguish between different types of nouns.
5.4 Results
Table 4 summarizes overall performance6. The
first line shows the accuracy of a baseline which
assigns possible supersenses of identified words at
random. The second line shows the performance
of the first sense baseline (cf. Section 5.3), the
marked difference between the two is a measure of
the robustness of the first sense heuristic. On the
Semcor data the tagger improves over the base-
line by 10.71%, 31.19% error reduction, while
on Senseval-3 the tagger improves over the base-
line by 6.45%, 17.96% error reduction. We can
put these results in context, although indirectly,
by comparison with the results of the Senseval-
3 all words task systems. There, with a base-
line of 62.40%, only 4 out of 26 systems per-
formed above the baseline, with the two best sys-
tems (Mihalcea and Faruque, 2004; Decadt et al,
2004) achieving an F-score of 65.2% (2.8% im-
provement, 7.45% error reduction). The system
based on the HMM tagger (Molina et al, 2004),
6Scoring was performed with a re-implementation of the
?conlleval? script .
achieved an F-score of 60.9%. The supersense
tagger improves mostly on precision, while also
improving on recall. Overall the tagger achieves
F-scores between 70.5 and 77.2%. If we compare
these figures with the accuracy of NER taggers
the results are very encouraging. Given the con-
siderably larger ? one order of magnitude ? class
space some loss has to be expected. Experiments
with augmented tagsets in the biomedical domain
also show performance loss with respect to smaller
tagsets; e.g., Kazama et al (2002) report an F-
score of 56.2% on a tagset of 25 Genia classes,
compared to the 75.9% achieved on the simplest
binary case. The sequence fragments from SEMv
contribute about 1% F-score improvement.
Table 5 focuses on subsets of the evaluation.
The upper part summarizes the results on Sem-
cor for the classes comparable to standard NER?s:
?person?, ?group?, ?location? and ?time?. How-
ever, these categories here are composed of com-
mon nouns as well as proper names/named enti-
ties. On this four tags the tagger achieves an aver-
age 82.46% F-score, not too far from NER results.
The lower portion of Table 5 summarizes the re-
sults on the five most frequent noun and verb su-
persense labels on the Senseval-3 data, providing
more specific evidence for the supersense tagger?s
disambiguation accuracy. The tagger outperforms
the first sense baseline on all categories, with the
exception of ?verb.cognition? and ?noun.person?.
The latter case has a straightforward explanation,
named entities (e.g., ?Phil Haney?, ?Chevron? or
?Marina District?) are not annotated in the Sense-
val data, while they are in Semcor. Hence the tag-
ger learns a different model for nouns than the one
used to annotate the Senseval data. Because of this
discrepancy the tagger tends to return false posi-
tives for some categories. In fact, the other noun
categories on which the tagger performs poorly in
SE3 are ?group? and ?location? (baseline 52.10
tagger 44.72 and baseline 47.62% tagger 47.54%
F-score). Naturally, the lower performance on
Senseval is also explained by the fact that the eval-
600
NER supersenses in Semcor
Supersense-Tagger Baseline
Supersense # Supersenses R P F R P F
n.person 1526 92.04 87.94 89.94 56.29 77.35 65.16
n.group 665 75.38 79.56 77.40 62.42 66.81 64.54
n.location 459 77.21 75.37 76.25 67.88 63.33 65.53
n.time 412 88.36 84.30 86.27 78.26 83.88 80.98
5 most frequent verb supersenses in Senseval-3
Supersense # Supersenses R P F R P F
v.stative 184 80.33 81.30 80.81 72,83 63.81 68.02
v.communication 88 77.53 83.36 80.33 71.91 74.42 73.14
v.motion 81 69.63 64.54 66.98 58.02 60.26 59.12
v.cognition 61 73.44 67.91 70.56 75.41 71.87 73.60
v.change 60 68.33 67.47 67.89 56.67 57.63 57.14
5 most frequent noun supersenses in Senseval-3
Supersense # Supersenses R P F R P F
n.person 148 92.24 60.49 73.06 89.12 79.39 83.97
n.artifact 131 80.91 77.73 79.29 74.24 75.97 75.10
n.act 96 61.46 72.37 66.45 58.33 65.12 61.54
n.cognition 67 45.80 52.87 49.06 49.28 46.58 47.89
n.event 60 70.33 89.83 78.87 71.67 75.44 73.50
Table 5. Summary of results of baseline and tagger on selected subsets of labels: NER categories evaluated on
Semcor (upper section), and 5 most frequent verb (middle) and noun (bottom) categories evaluated on Senseval.
uation comes from different sources than training.
6 Conclusions
In this paper we presented a novel approach to
broad-coverage word sense disambiguation and
information extraction. We defined a tagset based
on Wordnet supersenses, a much simpler and gen-
eral semantic model than Wordnet which, how-
ever, preserves significant polysemy information
and includes standard named entity recognition
categories. We showed that in this framework it is
possible to perform accurate broad-coverage tag-
ging with state of the art sequence learning meth-
ods. The tagger considerably outperformed the
most competitive baseline on both Semcor and
Senseval data. To the best of our knowledge the re-
sults on Senseval data provide the first convincing
evidence of the possibility of improving by con-
siderable amounts over the first sense baseline.
We believe both the tagset and the structured
learning approach contribute to these results. The
simplified representation obviously helps by re-
ducing the number of possible senses for each
word (cf. Table 3). Interestingly, the relative im-
provement in performance is not as large as the
relative reduction in polysemy. This indicates that
sense granularity is only one of the problems in
WSD. More needs to be understood concerning
sources of information, and processes, that affect
word sense selection in context. As far as the tag-
ger is concerned, we applied the simplest feature
representation, more sophisticated features can be
used, e.g., based on kernels, which might con-
tribute significantly by allowing complex feature
combinations. These results also suggest new di-
rections of research within this model. In partic-
ular, the labels occurring in each sequence tend
to coincide with predicates (verbs) and arguments
(nouns and named entities). A sequential depen-
dency model might not be the most accurate at
capturing the grammatical dependencies between
these elements. Other conditional models, e.g.,
designed on head to head, or similar, dependen-
cies could prove more appropriate.
Another interesting issue is the granularity of
the tagset. Supersenses seem more practical then
synsets for investigating the impact of broad-
coverage semantic tagging, but they define a very
simplistic ontological model. A natural evolution
of this kind of approach might be one which starts
by defining a semantic model at an intermediate
level of abstraction (cf. (Ciaramita et al, 2005)).
601
References
Y. Altun, T. Hofmann, and M. Johnson. 2003.
Discriminative Learning for Label Sequences via
Boosting. In Proceedings of NIPS 2003.
T. Brants. 2002. TnT - A Statistical Part-of-Speech
Tagger. In Proceedings of ANLP 2000.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
Entity Extraction Using AdaBoost. In Proceedings
of CONLL 2002.
M. Ciaramita and M. Johnson. 2003. Supersense Tag-
ging of Unknown Nouns in WordNet. In Proceed-
ings of EMNLP 2003.
M. Ciaramita, T. Hofmann, and M. Johnson. 2003. Hi-
erarchical Semantic Classification: Word Sense Dis-
ambiguation with World Knowledge. In Proceed-
ings of IJCAI 2003.
M. Ciaramita, S. Sloman, M. Johnson, and E. Upfal.
2005. Hierarchical Preferences in a Broad-Coverage
Lexical Taxonomy. In Proceedings of CogSci 2005.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings
of EMNLP 2002, pages 1?8.
J. Curran. 2005. Supersense Tagging of Unknown
Nouns Using Semantic Similarity. In Proceedings
of ACL 2005, pages 26?33.
C. de Loupy, M. El-Beze, and P.F. Marteau. 1998.
Word Sense Disambiguation Using HMM Tagger.
In Proceedings of LREC 1998, pages 1255?1258.
B. Decadt, V. Hoste, W. Daelemans, and A. van der
Bosch. 2004. GAMBL, Genetic Algorithm Opti-
mization of Memory-Based WSD. In Proceedings
of SENSEVAL-3/ACL 2004.
S. Dingare, M. Nissim, J. Finkel, C. Manning, and
C. Grover. 2005. A System for Identifying Named
Entities in Biomedical Text: How Results from
Two Evaluations Reflect on Both the System and
the Evaluations. Comparative and Functional Ge-
nomics, 6:77?85.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named Entity Extraction through Classifier Combi-
nation. In Proceedings of CONLL 2003.
W. Gale, K. Church, and D. Yarowsky. 1992. One
Sense per Discourse. In Proceedings of the DARPA
Workshop on Speech and Natural Language.
J. Kazama, T. Makino, Y. Ohta, and J. Tsujii. 2002.
Tuning Support Vector Machines for Biomedical
Named Entity Recognition. In Proceedings of the
Workshop on Natural Language Processing in the
Biomedical Domain (ACL 2002).
T. Koo and M. Collins. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In Proceedings of
EMNLP 2005.
H. Kuc?era and W. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown Uni-
versity Press, Providence, RI.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML 2001, pages 282?289.
A. McCallum, D. Freitag, and F. Pereira. 2000. Max-
imum Entropy Markov Models for Information Ex-
traction and Segmentation. In Proceedings of ICML
2000, pages 591?598.
D. McCarthy, R. Koeling, and J. Carroll. 2004. Find-
ing Predominant Senses in Untagged Text. In Pro-
ceedings of ACL 2004.
R. Mihalcea and E. Faruque. 2004. SenseLearner:
Minimally Supervised Word Sense Disambiguation
for All Words in Open Text. In Proceedings of
SENSEVAL-3/ACL 2004.
G.A. Miller, C. Leacock, T. Randee, and R. Bunker.
1993. A Semantic Concordance. In Proceedings of
the 3 DARPA Workshop on Human Language Tech-
nology, pages 303?308.
A. Molina, F. Pla, and E. Segarra. 2002. A Hid-
den Markov Model Approach to Word Sense Dsi-
ambiguation. In Proceedings of IBERAMIA 2002.
A. Molina, F. Pla, and E. Segarra. 2004. WSD System
Based on Specialized Hidden Markov Model (upv-
shmm-eaw). In Proceedings of SENSEVAL-3/ACL
2004.
Y. Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.
2002. The GENIA Corpus: An Annotated Research
Abstract Corpus in the Molecular Biology Domain.
In Proceedings of HLT 2002.
B. Rosario and M. Hearst. 2004. Classifying Seman-
tic Relations in Bioscience Text. In Proceedings of
ACL 2004).
F. Segond, A. Schiller, G. Grefenstette, and J.P.
Chanod. 1997. An Experiment in Semantic Tagging
Using Hidden Markov Model. In Proceedings of the
Workshop on Automatic Information Extraction and
Building of Lexical Semantic Resources (ACL/EACL
1997), pages 78?81.
F. Sha and F. Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of HLT-
NAACL 2003, pages 213?220.
B. Snyder and M. Palmer. 2004. The english All-
Words Tasks. In Proceedings of SENSEVAL-3/ACL
2004.
602
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1112?1118,
Prague, June 2007. c?2007 Association for Computational Linguistics
Multilingual Dependency Parsing and Domain Adaptation using DeSR 
Giuseppe Attardi 
Felice Dell?Orletta 
Maria Simi 
Dipartimento di Informatica 
largo B. Pontecorvo 3 
I-56127 Pisa, Italy 
attardi@di.unipi.it 
felice.dellorletta@
ilc.cnr.it 
simi@di.unipi.it 
Atanas Chanev 
Universit? di Trento 
via Matteo del Ben 5 
I-38068 Rovereto, Italy 
Fondazione Bruno Kessler-irst 
via Sommarive 18 
I-38050 Povo, Italy 
chanev@form.unitn.it 
 
Massimiliano Ciaramita 
Yahoo! Research Barcelona 
Ocata 1 
S-08003 Barcelona, Spain 
massi@yahoo-inc.com 
 
Abstract 
We describe our experiments using the 
DeSR parser in the multilingual and do-
main adaptation tracks of the CoNLL 2007 
shared task. DeSR implements an incre-
mental deterministic Shift/Reduce parsing 
algorithm, using specific rules to handle 
non-projective dependencies. For the multi-
lingual track we adopted a second order 
averaged perceptron and performed feature 
selection to tune a feature model for each 
language. For the domain adaptation track 
we applied a tree revision method which 
learns how to correct the mistakes made by 
the base parser on the adaptation domain. 
1 Introduction 
Classifier-based dependency parsers (Yamada and 
Matsumoto, 2003; Nivre and Scholz, 2004) learn 
from an annotated corpus how to select an 
appropriate sequence of Shift/Reduce actions to 
construct the dependency tree for a sentence. 
Learning is based on techniques such as SVM 
(Vapnik 1998) or Memory Based Learning 
(Daelemans 2003), which provide high accuracy 
but are often computationally expensive. For the 
multilingual track in the CoNLL 2007 Shared 
Task, we employed a Shift/Reduce parser which 
uses a perceptron algorithm with second-order 
feature maps, in order to verify whether a simpler 
and faster algorithm can still achieve comparable 
accuracy. 
For the domain adaptation track we wished to 
explore the use of tree revisions in order to 
incorporate language knowledge from a new 
domain. 
2 Multilingual Track 
The overall parsing algorithm is a deterministic 
classifier-based statistical parser, which extends 
the approach by Yamada and Matsumoto (2003), 
by using different reduction rules that ensure 
deterministic incremental processing of the input 
sentence and by adding specific rules for handling 
non-projective dependencies. The parser also 
performs dependency labeling within a single 
processing step. 
The parser is modular and can use several 
learning algorithms. The submitted runs used a 
second order Average Perceptron, derived from the 
multiclass perceptron of Crammer and Singer 
(2003). 
No additional resources were used. No pre-
processing or post-processing was used, except 
stemming for English, by means of the Snowball 
stemmer (Porter 2001). 
3 Deterministic Classifier-based Parsing 
DeSR (Attardi, 2006) is an incremental determinis-
tic classifier-based parser. The parser constructs 
dependency trees employing a deterministic bot-
tom-up algorithm which performs Shift/Reduce 
actions while analyzing input sentences in left-to-
right order. 
Using a notation similar to (Nivre and Scholz, 
2003), the state of the parser is represented by a 
1112
quadruple ?S, I, T, A?, where S is the stack of past 
tokens, I is the list of (remaining) input tokens, T is 
a stack of temporary tokens and A is the arc rela-
tion for the dependency graph. 
Given an input string W, the parser is initialized 
to ?(), W, (), ()?, and terminates when it reaches a 
configuration ?S, (), (), A?. 
The three basic parsing rule schemas are as fol-
lows: ?S, n|I, T, A? Shift ?n|S, I, T, A? ?s|S, n|I, T, A? Rightd ?S, n|I, T, A?{(s, d, n)}? ?s|S, n|I, T, A? Leftd ?S, s|I, T, A?{(n, d, s)}? 
The schemas for the Left and Right rules are in-
stantiated for each dependency type d ? D, for a 
total of 2|D| + 1 rules. These rules perform both 
attachment and labeling. 
At each step the parser uses classifiers trained 
on a treebank corpus in order to predict which ac-
tion to perform and which dependency label to as-
sign given the current configuration. 
4 Non-Projective Relations 
For handling non-projective relations, Nivre and 
Nilsson (2005) suggested applying a pre-
processing step to a dependency parser, which con-
sists in lifting non-projective arcs to their head re-
peatedly, until the tree becomes pseudo-projective. 
A post-processing step is then required to restore 
the arcs to the proper heads. 
In DeSR non-projective dependencies are han-
dled in a single step by means of the following ad-
ditional parsing rules, slightly different from those 
in (Attardi, 2006): 
 ?s1|s2|S, n|I, T, A? Right2d ? S, s1|n|I, T, A?{(s2, d, n)}? ?s1|s2|S, n|I, T, A? Left2d ?s2|S, s1|I, T, A?{(n, d, s2)}? ?s1|s2|s3|S, n|I, T, A? Right3d ? S, s1|s2|n|I, T, A?{(s3, d, n)}? ?s1|s2|s3|S, n|I, T, A? Left3d ?s2|s3|S, s1|I, T, A?{(n, d, s3)}? 
 ?s1|s2|S, n|I, T, A? Extract ?n|s1|S, I, s2|T, A? ?S, I, s1|T, A? Insert ?s1|S, I, T, A? 
Left2, Right2 are similar to Left and Right, except 
that they create links crossing one intermediate 
node, while Left3 and Right3 cross two intermedi-
ate nodes. Notice that the RightX actions put back 
on the input the intervening tokens, allowing the 
parser to complete the linking of tokens whose 
processing had been delayed. Extract/Insert gener-
alize the previous rules by moving one token to the 
stack T and reinserting the top of T into S. 
5 Perceptron Learning and 2nd-Order 
Feature Maps 
The software architecture of the DeSR parser is 
modular. Several learning algorithms are available, 
including SVM, Maximum Entropy, Memory-
Based Learning, Logistic Regression and a few 
variants of the perceptron algorithm. 
We obtained the best accuracy with a multiclass 
averaged perceptron classifier based on the 
ultraconservative formulation of Crammer and 
Singer (2003) with uniform negative updates. The 
classifier function is: { }xxF k
k
?= ?maxarg)(  
where each parsing action k is associated with a 
weight vector ?k. To regularize the model the final 
weight vectors are computed as the average of all 
weight vectors posited during training. The number 
of learning iterations over the training data, which 
is the only adjustable parameter of the algorithm, 
was determined by cross-validation.  
In order to overcome the limitations of a linear 
perceptron, we introduce a feature map ?: IRd ? 
IRd(d+1)/2 that maps a feature vector x into a higher 
dimensional feature space consisting of all un-
ordered feature pairs: ?(x) = ?xixj | i = 1, ?, d, j = i, ?, d? 
In other words we expand the original 
representation in the input space with a feature 
map that generates all second-order feature 
combinations from each observation. We call this 
the 2nd-order model, where the inner products are 
computed as ?k ? ?(x), with ?k a vector of dimen-
sion d(d+1)/2. Applying a linear perceptron to this 
feature space corresponds to simulating a polyno-
mial kernel of degree two.  
A polynomial kernel of degree two for SVM 
was also used by Yamada and Matsumoto (2003). 
However, training SVMs on large data sets like 
those arising from a big training corpus was too 
1113
computationally expensive, forcing them to resort 
to partitioning the training data (by POS) and to 
learn several models. 
Our implementation of the perceptron algorithm 
uses sparse data structures (hash maps) so that it 
can handle efficiently even large feature spaces in 
a single model. For example the feature space for 
the 2nd-order model for English contains over 21 
million. Parsing unseen data can be performed at 
tens of sentences per second. More details on such 
aspects of the DeSR parser can be found in (Ci-
aramita and Attardi 2007). 
6 Tuning 
The base parser was tuned on several parameters to 
optimize its accuracy as follows. 
6.1 Feature Selection 
Given the different characteristics of languages and 
corpus annotations, it is worth while to select a 
different set of features for each language. For ex-
ample, certain corpora do not contain lemmas or 
morphological information so lexical information 
will be useful. Vice versa, when lemmas are pre-
sent, lexical information might be avoided, reduc-
ing the size of the feature set. 
We performed a series of feature selection ex-
periments on each language, starting from a fairly 
comprehensive set of 43 features and trying all 
variants obtained by dropping a single feature. The 
best of these alternatives feature models was cho-
sen and the process iterated until no further gains 
were achieved. The score for the alternatives was 
computed on a development set of approximately 
5000 tokens, extracted from a split of the original 
training corpus. 
Despite the process is not guaranteed to produce 
a global optimum, we noticed LAS improvements 
of up to 4 percentage points on some languages. 
The set of features to be used by DeSR is con-
trolled by a number of parameters supplied through 
a parameter file. Each parameter describes a fea-
ture and from which tokens to extract it. Tokens 
are referred through positive numbers for input 
tokens and negative numbers for tokens on the 
stack. For example 
PosFeatures -2 -1 0 1 2 3 
means to use the POS tag of the first two tokens on 
the stack and of the first four tokens on the input. 
The parameter PosPrev refers to the POS of the 
preceding token in the original sentence, PosLeftChild refers to the POS of the left chil-
dren of a token, PastActions tells how many 
previous actions to include as features. 
The selection process was started from the fol-
lowing base feature model: 
LexFeatures -1 0 1 LemmaFeatures -2 -1 0 1 2 3 LemmaPrev  -1 0 LemmaSucc  -1 0 LemmaLeftChild -1 0 LemmaRightChild -1 MorphoFeatures -1 0 1 2 PosFeatures -2 -1 0 1 2 3 PosNext  -1 0 PosPrev  -1 0 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 DepFeatures -1 0 DepLeftChild -1 0 DepRightChild -1 PastActions 1 
The selection process produced different variants 
for each language, sometimes suggesting dropping 
certain intermediate features, like the lemma of the 
third next input token in the case of Catalan: 
LemmaFeatures -2 -1 0 1 3 LemmaPrev  0 LemmaSucc  -1 LemmaLeftChild 0 LemmaRightChild -1 PosFeatures -2 -1 0 1 2 3 PosPrev  0 PosSucc  -1 PosLeftChild -1 0 PosRightChild -1 0 CPosFeatures -1 0 1 MorphoFeatures 0 1 DepLeftChild -1 0 DepRightChild -1 
For Italian, instead, we ran a series of tests in par-
allel using a set of manually prepared feature mod-
els. The best of these models achieved a LAS of 
80.95%. The final run used this model with the 
addition of the morphological agreement feature 
discussed below. 
 
English was the only language for which no feature 
selection was done and for which lexical features 
1114
were used. English is also the language where the 
official score is significantly lower than what we 
had been getting on our development set (90.01% 
UAS). 
6.2 Prepositional Attachment 
Certain languages, such as Catalan, use detailed 
dependency labeling, that for instance distinguish 
between adverbials of location and time. We ex-
ploited this information by introducing a feature 
that captures the entity type of a child of the top 
word on the stack or in the input. During training a 
list of nouns occurring in the corpus as dependent 
on prepositions with label CCL (meaning ?com-
plement of location? for Catalan) was created and 
similarly for CCT (complement of time). The en-
tity type TIME is extracted as a feature depending 
on whether the noun occurs in the time list more 
than ? times than in the location list, and similarly 
for the feature LOCATION. ? was set to 1.5 in our 
experiments. 
6.3 Morphological Agreement 
Certain languages require gender and number 
agreement between head and dependent. The fea-
ture MorphoAgreement is computed for such lan-
guages and provided noticeable accuracy 
improvements. 
For example, for Italian, the improvement was 
from: 
  LAS: 80.95%,  UAS: 85.03% 
to: 
  LAS: 81.34%,  UAS: 85.54% 
For Catalan, adding this feature we obtained an 
unofficial score of: 
  LAS: 87.64%,  UAS: 92.20% 
with respect to the official run: 
  LAS: 86.86%,  UAS: 91.41% 
7 Accuracy 
Table 1 reports the accuracy scores in the multilin-
gual track. They are all considerably above the 
average and within 2% from the best for Catalan, 
3% for Chinese, Greek, Italian and Turkish. 
8 Performance 
The experiments were performed on a 2.4 Ghz 
AMD Opteron machine with 32 GB RAM. Train-
ing the parser using the 2nd-order perceptron on the 
English corpus required less than 3 GB of memory 
and about one hour for each iteration over the 
whole dataset. Parsing the English test set required 
39.97 sec. For comparison, we tested the MST 
parser version 0.4.3 (Mstparser, 2007), configured 
for second-order, on the same data: training took 
73.9 minutes to perform 10 iterations and parsing 
took 97.5 sec. MST parser achieved: 
LAS: 89.01%, UAS: 90.17% 
9 Error Analysis on Catalan 
The parser achieved its best score on Catalan, so 
we performed an analysis on its output for this lan-
guage. 
Among the 42 dependency relations that the 
parser had to assign to a sentence, the largest num-
ber of errors occurred assigning CC (124), SP (33), CD (27), SUJ (26), CONJUNCT (22), SN (23). 
The submitted run for Catalan did not use the 
entity feature discussed earlier and indeed 67 er-
rors were due to assigning CCT or CCL instead of 
CC (generic complement of circumstance). How-
ever over half of these appear as underspecified 
annotation errors in the corpus rather than parser 
errors. 
By adding the ChildEntityType feature, 
which distinguishes better between CCT and CCL, 
the UAS improved, while the LAS dropped 
slightly, due to the effect of underspecified annota-
tions in the corpus: 
   LAS: 87.22%,    UAS: 91.71% 
Table 1. Multilingual track official scores. 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
Arabic  76.52  72.66 68.34  86.09  82.53 78.84  
Basque  76.92  69.48 68.06  82.80  76.86 75.15  
Catalan  88.70  86.86 79.85  93.40  91.41 87.98  
Chinese  84.69  81.50 76.59  88.94  86.73 81.98  
Czech  80.19  77.37 70.12  86.28  83.40 77.56  
English  89.61  85.85 80.95  90.63  86.99 82.67  
Greek  76.31  73.92 70.22  84.08  80.75 77.78  
Hungarian  80.27  76.81 71.49  83.55  81.81 76.34  
Italian  84.40  81.34 78.06  87.91  85.54 82.45  
Turkish  79.81  76.87 73.19  86.22  83.56 80.33  
1115
A peculiar aspect of the original Catalan corpus 
was the use of a large number (195) of dependency 
labels. These labels were reduced to 42 in the ver-
sion used for CoNNL 2007, in order to make it 
comparable to other corpora. However, performing 
some preliminary experiments using the original 
Catalan collection with all 195 dependency labels, 
the DeSR parser achieved a significantly better 
score: 
LAS: 88.80%, UAS: 91.43% 
while with the modified one, the score dropped to: 
LAS: 84.55%, UAS: 89.38% 
This suggests that accuracy might improve for 
other languages as well if the training corpus was 
labeled with more precise dependencies. 
10 Adaptation Track 
The adaptation track originally covered two do-
mains, the CHILDES and the Chemistry domain.  
The CHILDES (Brown, 1973; MacWhinney, 
2000) consists of transcriptions of dialogues with 
children, typically short sentences of the kind: 
Would you like more grape juice ? 
That 's a nice box of books . 
Phrases are short, half of them are questions. The 
only difficulty that appeared from looking at the 
unlabeled collection supplied for training in the 
domain was the presence of truncated terms like goin (for going), d (for did), etc. However none 
of these unusually spelled words appeared in the 
test set, so a normal English parser performed rea-
sonably well on this task. Because of certain in-
consistencies in the annotation guidelines, the 
organizers decided to make this task optional and 
hence we submitted just the parse produced by the 
parser trained for English. 
For the second adaptation task we were given a 
large collection of unlabeled data in the chemistry 
domain (Kulick et al 2004) as well as a test set of 
5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll). 
There were three sets of unlabeled documents: 
we chose the smallest (unlab1) consisting of over 
300,000 tokens (11663 sentences). unlab1 was 
tokenized, POS and lemmas were added using our 
version of TreeTagger (Schmid, 1994), and lem-
mas replaced with stems, which had turned out to 
be more effective than lemmas. We call this set pchemtb_unlab1.conll. 
We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB col-
lection provided for CoNLL 2007. This consists of 
WSJ sections 02-11, half of the usual set 02-23, for 
a total of 460,000 tokens with dependencies gener-
ated with the converter by Johansson and Nugues 
(2007). 
We added stems and produced a parser called DeSRwsj. By parsing eng-lish_pchem_test.conll with DeSRwsj we 
obtained pchemtb_test_base.desr, our base-
line for the task. 
By visual inspection using DgAnnotator 
(DgAnnotator, 2006), the parses looked generally 
correct. Most of the errors seemed due to improper 
handling of conjunctions and disjunctions. The 
collection in fact contains several phrases like: 
Specific antibodies raised against 
P450IIB1 , P450 IA1 or IA2 , 
P450IIE1 , and P450IIIA2 inhibited 
the activation in liver microsomes 
from rats pretreated with PB , BNF , 
INH and DEX respectively 
The parser did not seem to have much of a problem 
with terminology, possibly because the supplied 
gold POS were adequate. 
For the adaptation we proceeded as follows. We 
parsed pchemtb_unlab1.conll using DeSRwsj 
obtaining pchemtb_unlab1.desr. 
We then extracted a set of 12,500 sentences 
from ptb_train.conll and 7,500 sentences 
from pchemtb_unlab1.desr, creating a corpus 
of 20,000 sentences called combined.conll. In 
both cases the selection criteria was to choose sen-
tences shorter than 30 tokens. 
We then trained a low accuracy parser (called DesrCombined) on combined.conll, by using 
a 1st-order averaged perceptron. DesrCombined 
was used to parse english_ptb_train.conll, 
the original training corpus for English. By com-
paring this parse with the original, one can detect 
where such parser makes mistakes. The rationale 
for using an inaccurate parser is to obtain parses 
with many errors so that they form a suitably large 
training set for the next step: parser revision. 
We then used a parsing revision technique (At-
tardi and Ciaramita, 2007) to learn how to correct 
these errors, producing a parse reviser called DesrReviser. The revision technique consists of 
comparing the parse trees produced by the parser 
with the gold standard parse trees, from the 
annotated corpus. Where a difference is noted, a 
1116
revision rule is determined to correct the mistake. 
Such rules consist in movements of a single link to 
a different head. Learning how to revise a parse 
tree consists in training a classifier on a set of 
training examples consisting of pairs ?(wi, d, wj), 
ti?, i.e. the link to be modified and the 
transformation rule to apply. Attardi and Ciaramita 
(2007) showed that 80% of the corrections can be 
typically dealt with just 20 tree revision rules. For 
the adaptation track we limited the training to 
errors recurring at least 20 times and to 30 rules. DesrReviser was then applied to pchemtb_test_base.desr producing pchemtb_test_rev.desr, our final submission. 
Many conjunction errors were corrected, in par-
ticular by moving the head of the sentence from a 
coordinate verb to the conjunction ?and? linking 
two coordinate phrases. 
The revision step produced an improvement of 
0.42% LAS over the score achieved by using just 
the base DeSRwsj parser. 
Table 2 reports the official accuracy scores on 
the closed adaptation track. DeSR achieved a close 
second best UAS on the ptchemtb test set and 
third best on CHILDES. The results are quite en-
couraging, particularly considering that the revi-
sion step does not yet correct the dependency 
labels and that our base English parser had a lower 
rank in the multilingual track. 
 
LAS UAS 
Task 
1st DeSR Avg 1st DeSR Avg 
CHILDES     61.37 58.67 57.89 
Pchemtb  81.06 80.40 73.03 83.42  83.08 76.42 
Table 2. Closed adaptation track scores. 
Notice that the adaptation process could be iter-
ated. Since the combination DeSRwsj+DesrReviser is a more accurate parser 
than DeSRwsj, we could use it again to parse pchemtb_unlab1.conll and so on. 
11 Conclusions 
For performing multilingual parsing in the CoNLL 
2007 shared task we employed DeSR, a classifier-
based Shift/Reduce parser. We used a second order 
averaged perceptron as classifier and achieved ac-
curacy scores quite above the average in all lan-
guages. For proper comparison with other 
approaches, one should take into account that the 
parser is incremental and deterministic; hence it is 
typically faster than other non linear algorithms. 
For the adaptation track we used a novel ap-
proach, based on the technique of tree revision, 
applied to a parser trained on a corpus combining 
sentences from both the training and the adaptation 
domain. The technique achieved quite promising 
results and it also offers the interesting possibility 
of being iterated, allowing the parser to incorporate 
language knowledge from additional domains. 
Since the technique is applicable to any parser, 
we plan to test it also with more accurate English 
parsers. 
Acknowledgments.  The following treebanks 
were used for training the parser: (Aduriz et al, 
2003; B?hmov? et al, 2003; Chen et al, 2003; Ha-
ji? et al, 2004; Marcus et al, 1993; Mart? et al, 
2002; Montemagni et al 2003; Oflazer et al, 2003; 
Prokopidis et al, 2005; Csendes et al, 2005). 
Ryan McDonald and Jason Baldridge made avail-
able mstparser and helped us using it. We grate-
fully acknowledge Hugo Zaragoza and Ricardo 
Baeza-Yates for supporting the first author during 
a sabbatical at Yahoo! Research Barcelona. 
References 
A. Abeill?, editor. 2003. Treebanks: Building and Using 
Parsed Corpora. Kluwer. 
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), 201?204. 
G. Attardi. 2006. Experiments with a Multilanguage 
non-projective dependency parser. In Proc. of the 
Tenth CoNLL, 2006. 
G. Attardi, M. Ciaramita. 2007. Tree Revision Learning 
for Dependency Parsing. In Proc. of NAACL/HLTC 
2007. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 2003. 
The PDT: a 3-level annotation scenario. In Abeill? 
(2003), chapter 7, 103?127. 
R. Brown. 1973. A First Language: The Early Stages. 
Harvard University Press. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementation. 
In Abeill? (2003), chapter 13, 231?248. 
1117
M. Ciaramita, G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information. Proc. of the 12th International 
Workshop on Parsing Technologies (IWPT), 2007. 
K. Crammer, Y. Singer. 2003. Ultraconservative Online 
Algorithms for Multiclass Problems. Journ. of Ma-
chine Learning Research. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
DgAnnotator. 2006. 
http://medialab.di.unipi.it/Project/Parser/DgAnnotato
r/. 
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. In 
Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc- 
Donald, M. Palmer, A. Schein, and L. Ungar. 2004. 
Integrated annotation for biomedical information ex- 
traction. In Proc. of the Human Language 
Technology Conference and the Annual Meeting of 
the North American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
R. McDonald, et al 2005. Non-projective Dependency 
Parsing using Spanning Tree Algorithms. In Proc. of 
HLT-EMNLP. 
B. MacWhinney. 2000. The CHILDES Project: Tools 
for Analyzing Talk. Lawrence Erlbaum. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
189?210.  
Mstparser 0.4.3. 2007.  
http://sourceforge.net/projects/mstparser/ 
J. Nivre, et al 2004. Memory-based Dependency Pars-
ing. In Proc.s of the Eighth CoNLL, ed. H. T. Ng and 
E. Riloff, Boston, Massachusetts, 49?56. 
J. Nivre and J. Nilsson. 2005. Pseudo-Projective De-
pendency Parsing. In Proc. of the 43rd Annual Meet-
ing of the ACL, 99?106. 
J. Nivre and M. Scholz. 2004. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING 
2004, Geneva, Switzerland, 64?70. 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 shared 
task on dependency parsing. In Proc. of the CoNLL 
2007 Shared Task. Joint Conf. on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. T?r. 
2003. Building a Turkish treebank. In Abeill? (2003), 
chapter 15, 261?277.  
M.F. Porter. 2001. Snowball Stemmer.  
http://www.snowball.tartarus.org/ 
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th Workshop 
on Treebanks and Linguistic Theories (TLT), pages 
149?160. 
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In Proc. of International Con-
ference on New Methods in Language Processing. 
V. N. Vapnik. 1998. The Statistical Learning Theory. 
Springer. 
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In 
Proc. of the 8th International Workshop on Parsing 
Technologies (IWPT), 195?206. 
 
1118
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 246?254,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Company-Oriented Extractive Summarization of Financial News?
Katja Filippova?, Mihai Surdeanu?, Massimiliano Ciaramita?, Hugo Zaragoza?
?EML Research gGmbH ?Yahoo! Research
Schloss-Wolfsbrunnenweg 33 Avinguda Diagonal 177
69118 Heidelberg, Germany 08018 Barcelona, Spain
filippova@eml-research.de,{mihais,massi,hugoz}@yahoo-inc.com
Abstract
The paper presents a multi-document sum-
marization system which builds company-
specific summaries from a collection of fi-
nancial news such that the extracted sen-
tences contain novel and relevant infor-
mation about the corresponding organiza-
tion. The user?s familiarity with the com-
pany?s profile is assumed. The goal of
such summaries is to provide information
useful for the short-term trading of the cor-
responding company, i.e., to facilitate the
inference from news to stock price move-
ment in the next day. We introduce a
novel query (i.e., company name) expan-
sion method and a simple unsupervized al-
gorithm for sentence ranking. The sys-
tem shows promising results in compari-
son with a competitive baseline.
1 Introduction
Automatic text summarization has been a field of
active research in recent years. While most meth-
ods are extractive, the implementation details dif-
fer considerably depending on the goals of a sum-
marization system. Indeed, the intended use of the
summaries may help significantly to adapt a par-
ticular summarization approach to a specific task
whereas the broadly defined goal of preserving rel-
evant, although generic, information may turn out
to be of little use.
In this paper we present a system whose goal is
to extract sentences from a collection of financial
?This work was done during the first author?s internship
at Yahoo! Research. Mihai Surdeanu is currently affiliated
with Stanford University (mihais@stanford.edu).
Massimiliano Ciaramita is currently at Google
(massi@google.com).
news to inform about important events concern-
ing companies, e.g., to support trading (i.e., buy or
sell) the corresponding symbol on the next day, or
managing a portfolio. For example, a company?s
announcement of surpassing its earnings? estimate
is likely to have a positive short-term effect on its
stock price, whereas an announcement of job cuts
is likely to have the reverse effect. We demonstrate
how existing methods can be extended to achieve
precisely this goal.
In a way, the described task can be classified
as query-oriented multi-document summarization
because we are mainly interested in information
related to the company and its sector. However,
there are also important differences between the
two tasks.
? The name of the company is not a query,
e.g., as it is specified in the context of the
DUC competitions1, and requires an exten-
sion. Initially, a query consists exclusively
of the ?symbol?, i.e., the abbreviation of the
name of a company as it is listed on the stock
market. For example, WPO is the abbrevia-
tion used on the stock market to refer to The
Washington Post?a large media and educa-
tion company. Such symbols are rarely en-
countered in the news and cannot be used to
find all the related information.
? The summary has to provide novel informa-
tion related to the company and should avoid
general facts about it which the user is sup-
posed to know. This point makes the task
related to update summarization where one
has to provide the user with new information
1http://duc.nist.gov; since 2008 TAC: http:
//www.nist.gov/tac.
246
given some background knowledge2. In our
case, general facts about the company are as-
sumed to be known by the user. Given WPO,
we want to distinguish between The Wash-
ington Post is owned by The Washington Post
Company, a diversified education and media
company and The Post recently went through
its third round of job cuts and reported an
11% decline in print advertising revenues for
its first quarter, the former being an example
of background information whereas the lat-
ter is what we would like to appear in the
summary. Thus, the similarity to the query
alone is not the decisive parameter in com-
puting sentence relevance.
? While the summaries must be specific for a
given organization, important but general fi-
nancial events that drive the overall market
must be included in the summary. For exam-
ple, the recent subprime mortgage crisis af-
fected the entire economy regardless of the
sector.
Our system proceeds in the three steps illus-
trated in Figure 1. First, the company symbol is
expanded with terms relevant for the company, ei-
ther directly ? e.g., iPod is directly related to Apple
Inc. ? or indirectly ? i.e., using information about
the industry or sector the company operates in. We
detail our symbol expansion algorithm in Section
3. Second, this information is used to rank sen-
tences based on their relatedness to the expanded
query and their overall importance (Section 4). Fi-
nally, the most relevant sentences are re-ranked
based on the degree of novelty they carry (Section
5).
The paper makes the following contributions.
First, we present a new query expansion tech-
nique which is useful in the context of company-
dependent news summarization as it helps identify
sentences important to the company. Second, we
introduce a simple and efficient method for sen-
tence ranking which foregrounds novel informa-
tion of interest. Our system performs well in terms
of the ROUGE score (Lin & Hovy, 2003) com-
pared with a competitive baseline (Section 6).
2 Data
The data we work with is a collection of financial
news consolidated and distributed by Yahoo! Fi-
2See the DUC 2007 and 2008 update tracks.
nance3 from various sources4. Each story is la-
beled as being relevant for a company ? i.e., it
appears in the company?s RSS feed ? if the story
mentions either the company itself or the sector the
company belongs to. Altogether the corpus con-
tains 88,974 news articles from a period of about
5 months (148 days). Some articles are labeled
as being relevant for several companies. The total
number of (company name, news collection) pairs
is 46,444.
The corpus is cleaned of HTML tags, embed-
ded graphics and unrelated information (e.g., ads,
frames) with a set of manually devised rules. The
filtering is not perfect but removes most of the
noise. Each article is passed through a language
processing pipeline (described in (Atserias et al,
2008)). Sentence boundaries are identified by
means of simple heuristics. The text is tokenized
according to Penn TreeBank style and each to-
ken lemmatized using Wordnet?s morphological
functions. Part of speech tags and named entities
(LOC, PER, ORG, MISC) are identified by means
of a publicly available named-entity tagger5 (Cia-
ramita & Altun, 2006, SuperSense). Apart from
that, all sentences which are shorter than 5 tokens
and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in
textual information only. Numeric information
contained, e.g., in tables can be easily and more
reliably obtained from the indices tables available
online.
3 Query Expansion
In company-oriented summarization query expan-
sion is crucial because, by default, our query con-
tains only the symbol, that is the abbreviation of
the name of the company. Unfortunately, exist-
ing query expansion techniques which utilize such
knowledge sources as WordNet or Wikipedia are
not useful for symbol expansion. WordNet does
not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear
how it can be used for expansion.
3http://finance.yahoo.com
4http://biz.yahoo.com, http://www.
seekingalpha.com, http://www.marketwatch.
com, http://www.reuters.com, http://www.
fool.com, http://www.thestreet.com, http:
//online.wsj.com, http://www.forbes.com,
http://www.cnbc.com, http://us.ft.com,
http://www.minyanville.com
5http://sourceforge.net/projects/
supersensetag
247
Expansion
Query
Expanded
Query
Relatedness
to Query
Filtering
Relevant
Sentences
Ranking
Novelty
Company
Profile
Yahoo! Finance
Symbol
Summary
News
Figure 1: System architecture
Intuitively, a good expansion method should
provide us with a list of products, or properties,
of the company, the field it operates in, the typi-
cal customers, etc. Such information is normally
found on the profile page of a company at Yahoo!
Finance6. There, so called ?business summaries?
provide succinct and financially relevant informa-
tion about the company. Thus, we use business
summaries as follows. For every company sym-
bol in our collection, we download its business
summary, split it into tokens, remove all words
but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-
tive in the context of our task, we do not want to
include them in the expanded query. To filter out
such words, we compute the company-dependent
TF*IDF score for every word on the collection of
all business summaries:
score(w) = tfw,c ? log
?
N
cfw
?
(1)
where c is the business summary of a company,
tfw,c is the frequency of w in c, N is the total
number of business summaries we have, cfw is
the number of summaries that contain w. This
formula penalizes words occurring in most sum-
maries (e.g., company, produce, offer, operate,
found, headquarter, management). At the mo-
ment of running the experiments, N was about
3,000, slightly less than the total number of sym-
6http://finance.yahoo.com/q/pr?s=AAPL
where the trading symbol of any company can be used
instead of AAPL.
bols because some companies do not have a busi-
ness summary on Yahoo! Finance. It is impor-
tant to point out that companies without a business
summary are usually small and are seldom men-
tioned in news articles: for example, these compa-
nies had relevant news articles in only 5% of the
days monitored in this work.
Table 1 gives the ten high scoring words for
three companies (Apple Inc. ? the computer and
software manufacture, Delta Air Lines ? the air-
line, and DaVita ? dyalisis services). Table 1
shows that this approach succeeds in expanding
the symbol with terms directly related to the com-
pany, e.g., ipod for Apple, but also with more gen-
eral information like the industry or the company
operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain
threshold ? are included in the expanded query (?
was tuned to a value of 5.0 on the development
set).
4 Relatedness to Query
Once the expanded query is generated, it can be
used for sentence ranking. We chose the system of
Otterbacher et al (2005) as a a starting point for
our approach and also as a competitive baseline
because it has been successfully tested in a simi-
lar setting?it has been applied to multi-document
query-focused summarization of news documents.
Given a graph G = (S,E), where S is the set
of all sentences from all input documents, and E is
the set of edges representing normalized sentence
similarities, Otterbacher et al (2005) rank all sen-
248
AAPL DAL DVA
apple air dialysis
music flight davita
mac delta esrd
software lines kidney
ipod schedule inpatient
computer destination outpatient
peripheral passenger patient
movie cargo hospital
player atlanta disease
desktop fleet service
Table 1: Top 10 scoring words for three companies
tence nodes based on the inter-sentence relations
as well as the relevance to the query q. Sentence
ranks are found iteratively over the set of graph
nodes with the following formula:
r(s, q) = ?
rel(s|q)
P
t?S rel(t|q)
+(1??)
X
t?S
sim(s, t)
P
v?S sim(v, t)
r(t, q) (2)
The first term represents the importance of a sen-
tence defined in respect to the query, whereas the
second term infers the importance of the sentence
from its relation to other sentences in the collec-
tion. ? ? (0, 1) determines the relative importance
of the two terms and is found empirically. Another
parameter whose value is determined experimen-
tally is the sentence similarity threshold ? , which
determines the inclusion of a sentence in G. Ot-
terbacher et al (2005) report 0.2 and 0.95 to be
the optimal values for ? and ? respectively. These
values turned out to produce the best results also
on our development set and were used in all our
experiments. Similarity between sentences is de-
fined as the cosine of their vector representations:
sim(s, t) =
P
w?s?t weight(w)
2
q
P
w?s weight(w)2 ?
q
P
w?t weight(w)2
(3)
weight(w) = tfw,sidfw,S (4)
idfw,S = log
( |S| + 1
0.5 + sfw
)
(5)
where tfw,s is the frequency of w in sentence s,
|S| is the total number of sentences in the docu-
ments from which sentences are to be extracted,
and sfw is the number of sentences which contain
the word w (all words in the documents as well
as in the query are stemmed and stopwords are re-
moved from them). Relevance to the query is de-
fined in Equation (6) which has been previously
used for sentence retrieval (Allan et al, 2003):
rel(s|q) =
X
w?q
log(tfw,s + 1) ? log(tfw,q + 1) ? idfw,S (6)
where tfw,x stands for the number of times w ap-
pears in x, be it a sentence (s) or the query (q). If
a sentence shares no words other than stopwords
with the query, the relevance becomes zero. Note
that without the relevance to the query part Equa-
tion 2 takes only inter-sentence similarity into ac-
count and computes the weighted PageRank (Brin
& Page, 1998).
In defining the relevance to the query, in Equa-
tion (6), words which do not appear in too many
sentences in the document collection weigh more.
Indeed, if a word from the query is contained in
many sentences, it should not count much. But it
is also true that not all words from the query are
equally important. As it has been mentioned in
Section 3, words like product or offer appear in
many business summaries and are equally related
to any company. To penalize such words, when
computing the relevance to the query, we multiply
the relevance score of a given word w with the in-
verted document frequency of w on the corpus of
business summaries Q ? idfw,Q:
idfw,Q = log
( |Q|
qfw
)
(7)
We also replace tfw,s with the indicator function
s(w) since it has been reported to be more ad-
equate for sentences, in particular for sentence
alignment (Nelken & Shieber, 2006):
s(w) =
{
1 if s contains w
0 otherwise
(8)
Thus, the modified formula we use to compute
sentence ranks is as follows:
rel(s|q) =
X
w?q
s(w) ? log(tfw,q + 1) ? idfw,S ? idfw,Q (9)
We call these two ranking algorithms that use
the formula in (2) OTTERBACHER and QUERY
WEIGHTS, the difference being the way the rel-
evance to the query is computed: (6) or (9). We
use the OTTERBACHER algorithm as a baseline in
the experiments reported in Section 6.
249
5 Novelty Bias
Apart from being related to the query, a good sum-
mary should provide the user with novel infor-
mation. According to Equation (2), if there are,
say, two sentences which are highly similar to the
query and which share some words, they are likely
to get a very high score. Experimenting with the
development set, we observed that sentences about
the company, such as e.g., DaVita, Inc. is a lead-
ing provider of kidney care in the United States,
providing dialysis services and education for pa-
tients with chronic kidney failure and end stage re-
nal disease, are ranked high although they do not
contribute new information. However, a non-zero
similarity to the query is indeed a good filter of the
information related to the company and to its sec-
tor and can be used as a prerequisite of a sentence
to be included in the summary. These observations
motivate our proposal for a ranking method which
aims at providing relevant and novel information
at the same time.
Here, we explore two alternative approaches to
add the novelty bias to the system:
? The first approach bypasses the relatedness
to query step introduced in Section 4 com-
pletely. Instead, this method merges the dis-
covery of query relatedness and novelty into
a single algorithm, which uses a sentence
graph that contains edges only between sen-
tences related to the query, (i.e., sentences for
which rel(s|q) > 0). All edges connecting
sentences which are unrelated to the query
are skipped in this graph. In this way we limit
the novelty ranking process to a subset of sen-
tences related to the query.
? The second approach models the problem
in a re-ranking architecture: we take the
top ranked sentences after the relatedness-to-
query filtering component (Section 4) and re-
rank them using the novelty formula intro-
duced below.
The main difference between the two approaches
is that the former uses relatedness-to-query and
novelty information but ignores the overall impor-
tance of a sentence as given by the PageRank al-
gorithm in Section 4, while the latter combines all
these aspects ?i.e., importance of sentences, relat-
edness to query, and novelty? using the re-ranking
architecture.
To amend the problem of general information
ranked inappropriately high, we modify the word-
weighting formula (4) so that it implements a nov-
elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight
of a word would be to draw a line between the
?known? words, i.e., words appearing in the busi-
ness summary, and the rest. In this approach all
the words from the business summary are equally
related to the company and get the weight of 0:
weight(w) =
{
0 if Q contains w
tfw,sidfw,S otherwise
(10)
We call this weighting scheme SIMPLE. As
an alternative, we also introduce a more elab-
orate weighting procedure which incorporates
the relatedness-to-query (or rather distance from
query) in the word weight formula. Intuitively, the
more related to the query a word is (e.g., DaVita,
the name of the company), the more familiar to the
user it is and the smaller its novelty contribution
is. If a word does not appear in the query at all, its
weight becomes equal to the usual tfw,sidfw,S :
weight(w) =
 
1 ? tfw,q ? idfw,QP
wi?q
tfwi,q ? idfwi,Q
!
? tfw,sidfw,S (11)
The overall novelty ranking formula is based
on the query-dependent PageRank introduced in
Equation (2). However, since we already incorpo-
rate the relatedness to the query in these two set-
tings, we focus only on related sentences and thus
may drop the relatedness to the query part from
(2):
r?(s, q) = ? + (1 ? ?)
?
t?S
sim(s, t, q)
?
u?S sim(t, u, q)
(12)
We set ? to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh-
old ? to a very low value (0.05) to prevent the
graph from becoming exceedingly bushy. Note
that this novelty-ranking formula can be equally
applied in both scenarios introduced at the begin-
ning of this section. In the first scenario, S stands
for the set of nodes in the graph that contains only
sentences related to the query. In the second sce-
nario, S contains the highest ranking sentences
detected by the relatedness-to-query component
(Section 4).
250
5.1 Redundancy Filter
Some sentences are repeated several times in the
collection. Such repetitions, which should be
avoided in the summary, can be filtered out ei-
ther before or after the sentence ranking. We ap-
ply a simple repetition check when incrementally
adding ranked sentences to the summary. If a sen-
tence to be added is almost identical to the one
already included in the summary, we skip it. Iden-
tity check is done by counting the percentage of
non-stop word lemmas in common between two
sentences. 95% is taken as the threshold.
We do not filter repetitions before the rank-
ing has taken place because often such repetitions
carry important and relevant information. The re-
dundancy filter is applied to all the systems de-
scribed as they are equally prone to include repe-
titions.
6 Evaluation
We randomly selected 23 company stock names,
and constructed a document collection for each
containing all the news provided in the Yahoo! Fi-
nance news feed for that company in a period of
two days (the time period was chosen randomly).
The average length of a news collection is about
600 tokens. When selecting the company names,
we took care of not picking those which have only
a few news articles for that period of time. This
resulted into 9.4 news articles per collection on av-
erage. From each of these, three human annotators
independently selected up to ten sentences. All an-
notators had average to good understanding of the
financial domain. The annotators were asked to
choose the sentences which could best help them
decide whether to buy, sell or retain stock for the
company the following day and present them in
the order of decreasing importance. The anno-
tators compared their summaries of the first four
collections and clarified the procedure before pro-
ceeding with the other ones. These four collec-
tions were then later used as a development set.
All summaries ? manually as well as automat-
ically generated ? were cut to the first 250 words
which made the summaries 10 words shorter on
average. We evaluated the performance automat-
ically in terms of ROUGE-2 (Lin & Hovy, 2003)
using the parameters and following the methodol-
ogy from the DUC events. The results are pre-
sented in Table 2. We also report the 95% confi-
dence intervals in brackets. As in DUC, we used
METHOD ROUGE-2
Otterbacher 0.255 (0.226 - 0.285)
Query Weights 0.289 (0.254 - 0.324)
Novelty Bias (simple) 0.315 (0.287 - 0.342)
Novelty Bias 0.302 (0.277 - 0.329)
Manual 0.472 (0.415 - 0.531)
Table 2: Results of the four extraction methods
and human annotators
jackknife for each (query, summary) pair and com-
puted a macro-average to make human and au-
tomatic results comparable (Dang, 2005). The
scores computed on summaries produced by hu-
mans are given in the bottom line (MANUAL) and
serve as upper bound and also as an indicator for
the inter-annotator agreement.
6.1 Discussion
From Table 2 follows that the modifications we
applied to the baseline are sensible and indeed
bring an improvement. QUERY WEIGHTS per-
forms better than OTTERBACHER and is in turn
outperformed by the algorithms biased to novel in-
formation (the two NOVELTY systems). The over-
lap between the confidence intervals of the base-
line and the simple version of the novelty algo-
rithm is minimal (0.002).
It is remarkable that the achieved improvement
is due to a more balanced relatedness to the query
ranking (9), as well as to the novelty bias re-
ranking. The fact that the simpler novelty weight-
ing formula (10) produced better results than the
more elaborated one (11) requires a deeper anal-
ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach
allows for a better combination of both novelty
and relatedness to query. Since the more complex
novelty ranking formula penalizes terms related
to the query (Equation (11)), it favors a scenario
where novelty is boosted in detriment of related-
ness to query, which is not always realistic.
It is important to note that, compared with the
baseline, we did not do any parameter tuning for
? and the inter-sentence similarity threshold. The
improvement between the system of Otterbacher
et al (2005) and our best model is statistically
significant.
251
6.2 System Combination
Recall from Section 5 that the motivation for pro-
moting novel information came from the fact that
sentences with background information about the
company obtained very high scores: they were re-
lated but not novel. The sentences ranked by OT-
TERBACHER or QUERY WEIGHTS required a re-
ranking to include related and novel sentences in
the summary. We checked whether novelty re-
ranking brings an improvement if added on top
of a system which does not have a novelty bias
(baseline or QUERY WEIGHTS) and compared it
with the setting where we simply limit the novelty
ranking to all the sentences related to the query
(NOVELTY SIMPLE and NOVELTY). In the simi-
larity graph, we left only edges between the first
30 sentences from the ranked list produced by
one of the two algorithms described in Section 4
(OTTERBACHER or QUERY WEIGHTS). Then we
ranked the sentences biased to novel information
the same way as described in Section 5. The re-
sults are presented in Table 3. What we evalu-
ate here is whether a combination of two methods
performs better than the simple heuristics of dis-
carding edges between sentences unrelated to the
query.
METHOD ROUGE-2
Otterbacher + Novelty simple 0.280 (0.254 - 0.306)
Otterbacher + Novelty 0.273 (0.245 - 0.301)
Query Weights + Novelty simple 0.275 (0.247 - 0.302)
Query Weights + Novelty 0.265 (0.242 - 0.289)
Table 3: Results of the combinations of the four
methods
From the four possible combinations, there is
an improvement over the baseline only (0.255 vs.
0.280 resp. 0.273). None of the combinations per-
forms better than the simple novelty bias algo-
rithm on a subset of edges. This experiment sug-
gests that, at least in the scenario investigated here
(short-term monitoring of publicly-traded compa-
nies), novelty is more important than relatedness
to query. Hence, the simple novelty bias algo-
rithm, which emphasizes novelty and incorporates
relatedness to query only through a loose con-
straint (rel(s|q) > 0) performs better than com-
plex models, which are more constrained by the
relatedness to query.
7 Related Work
Summarization has been extensively investigated
in recent years and to date there exists a multi-
tude of very different systems. Here, we review
those that come closest to ours in respect to the
task and that concern extractive multi-document
query-oriented summarization. We also mention
some work on using textual news data for stock
indices prediction which we are aware of.
Stock market prediction: Wu?thrich et al
(1998) were among the first who introduced an au-
tomatic stock indices prediction system which re-
lies on textual information only. The system gen-
erates weighted rules each of which returns the
probability of the stock going up, down or remain-
ing steady. The only information used in the rules
is the presence or absence of certain keyphrases
provided by a human expert who ?judged them
to be influential factors potentially moving stock
markets?. In this approach, training data is re-
quired to measure the usefulness of the keyphrases
for each of the three classes. More recently, Ler-
man et al (2008) introduced a forecasting system
for prediction markets that combines news anal-
ysis with a price trend analysis model. This ap-
proach was shown to be successful for the fore-
casting of public opinion about political candi-
dates in such prediction markets. Our approach
can be seen as a complement to both these ap-
proaches, necessary especially for financial mar-
kets where the news typically cover many events,
only some related to the company of interest.
Unsupervized summarization systems extract
sentences whose relevance can be inferred from
the inter-sentence relations in the document col-
lection. In (Radev et al, 2000), the centroid of
the collection, i.e., the words with the highest
TF*IDF, is considered and the sentences which
contain more words from the centroid are ex-
tracted. Mihalcea & Tarau (2004) explore sev-
eral methods developed for ranking documents
in information retrieval for the single-document
summarization task. Similarly, Erkan & Radev
(2004) apply in-degree and PageRank to build a
summary from a collection of related documents.
They show that their method, called LexRank,
achieves good results. In (Otterbacher et al, 2005;
Erkan, 2006) the ranking function of LexRank is
extended to become applicable to query-focused
summarization. The rank of a sentence is deter-
mined not just by its relation to other sentences in
252
the document collection but also by its relevance
to the query. Relevance to the query is defined as
the word-based similarity between query and sen-
tence.
Query expansion has been used for improv-
ing information retrieval (IR) or question answer-
ing (QA) systems with mixed results. One of the
problems is that the queries are expanded word
by word, ignoring the context and as a result the
extensions often become inadequate7. However,
Riezler et al (2007) take the entire query into ac-
count when adding new words by utilizing tech-
niques used in statistical machine translation.
Query expansion for summarization has not yet
been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for
query expansion and proposes that a concept can
be expanded by adding the text of all hyper-
links from the first paragraph of the Wikipedia
article about this concept. The automatic eval-
uation demonstrates that extracting relevant con-
cepts from Wikipedia leads to better performance
compared with WordNet: both expansion systems
outperform the no-expansion version in terms of
the ROUGE score. Although this method proved
helpful on the DUC data, it seems less appropriate
for expanding company names. For small compa-
nies there are short articles with only a few links;
the first paragraphs of the articles about larger
companies often include interesting rather than
relevant information. For example, the text pre-
ceding the contents box in the article about Apple
Inc. (AAPL) states that ?Fortune magazine named
Apple the most admired company in the United
States?8. The link to the article about the For-
tune magazine can be hardly considered relevant
for the expansion of AAPL. Wikipedia category
information, which has been successfully used in
some NLP tasks (Ponzetto & Strube, 2006, inter
alia), is too general and does not help discriminate
between two companies from the same sector.
Our work suggests that query expansion is
needed for summarization in the financial domain.
In addition to previous work, we also show that an-
other key factor for success in this task is detecting
and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:
//trec.nist.gov.
8Checked on September 17, 2008.
8 Conclusions
In this paper we presented a multi-document
company-oriented summarization algorithm
which extracts sentences that are both relevant for
the given organization and novel to the user. The
system is expected to be useful in the context of
stock market monitoring and forecasting, that is,
to help the trader predict the move of the stock
price for the given company. We presented a
novel query expansion method which works par-
ticularly well in the context of company-oriented
summarization. Our sentence ranking method is
unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive
baseline showed supportive results, indicating that
the ranking algorithm is able to select relevant
sentences and promote novel information at the
same time.
In the future, we plan to experiment with po-
sitional features which have proven useful for
generic summarization. We also plan to test the
system extrinsically. For example, it would be of
interest to see if a classifier may predict the move
of stock prices based on a set of features extracted
from company-oriented summaries.
Acknowledgments: We would like to thank the
anonymous reviewers for their helpful feedback.
References
Allan, James, Courtney Wade & Alvaro Bolivar
(2003). Retrieval and novelty detection at the
sentence level. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval Toronto, On., Canada, 28 July ? 1 Au-
gust 2003, pp. 314?321.
Atserias, Jordi, Hugo Zaragoza, Massimiliano
Ciaramita & Giuseppe Attardi (2008). Se-
mantically annotated snapshot of the English
Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008.
Brin, Sergey & Lawrence Page (1998). The
anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN
Systems, 30(1?7):107?117.
Ciaramita, Massimiliano & Yasemin Altun
(2006). Broad-coverage sense disambiguation
253
and information extraction with a supersense
sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing, Sydney, Australia,
22?23 July 2006, pp. 594?602.
Dang, Hoa Trang (2005). Overview of DUC
2005. In Proceedings of the 2005 Document
Understanding Conference held at the Human
Language Technology Conference and Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 9?
10 October 2005.
Erkan, Gu?nes? (2006). Using biased random walks
for focused summarization. In Proceedings
of the 2006 Document Understanding Confer-
ence held at the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,,
New York, N.Y., 8?9 June 2006.
Erkan, Gu?nes? & Dragomir R. Radev (2004).
LexRank: Graph-based lexical centrality as
salience in text summarization. Journal of Arti-
ficial Intelligence Research, 22:457?479.
Lerman, Kevin, Ari Gilder, Mark Dredze & Fer-
nando Pereira (2008). Reading the markets:
Forecasting public opinion of political candi-
dates by news analysis. In Proceedings of
the 22st International Conference on Computa-
tional Linguistics, Manchester, UK, 18?22 Au-
gust 2008, pp. 473?480.
Lin, Chin-Yew & Eduard H. Hovy (2003). Au-
tomatic evaluation of summaries using N-gram
co-occurrence statistics. In Proceedings of the
Human Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics, Edmonton, Alberta,
Canada, 27 May ?1 June 2003, pp. 150?157.
Mihalcea, Rada & Paul Tarau (2004). Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Nat-
ural Language Processing, Barcelona, Spain,
25?26 July 2004, pp. 404?411.
Nastase, Vivi (2008). Topic-driven multi-
document summarization with encyclopedic
knowledge and activation spreading. In Pro-
ceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Hon-
olulu, Hawaii, 25?27 October 2008. To appear.
Nelken, Rani & Stuart M. Shieber (2006). To-
wards robust context-sensitive sentence align-
ment for monolingual corpora. In Proceedings
of the 11th Conference of the European Chapter
of the Association for Computational Linguis-
tics, Trento, Italy, 3?7 April 2006, pp. 161?168.
Otterbacher, Jahna, Gu?nes? Erkan & Dragomir
Radev (2005). Using random walks for
question-focused sentence retrieval. In Pro-
ceedings of the Human Language Technology
Conference and the 2005 Conference on Empir-
ical Methods in Natural Language Processing,
Vancouver, B.C., Canada, 6?8 October 2005,
pp. 915?922.
Ponzetto, Simone Paolo & Michael Strube (2006).
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Pro-
ceedings of the Human Language Technology
Conference of the North American Chapter of
the Association for Computational Linguistics,
New York, N.Y., 4?9 June 2006, pp. 192?199.
Radev, Dragomir R., Hongyan Jing & Malgorzata
Budzikowska (2000). Centroid-based summa-
rization of mutliple documents: Sentence ex-
traction, utility-based evaluation, and user stud-
ies. In Proceedings of the Workshop on Au-
tomatic Summarization at ANLP/NAACL 2000,
Seattle, Wash., 30 April 2000, pp. 21?30.
Riezler, Stefan, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal & Yi Liu (2007).
Statistical machine translation for query expan-
sion in answer retrieval. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Re-
public, 23?30 June 2007, pp. 464?471.
Wu?thrich, B, D. Permunetilleke, S. Leung, V. Cho,
J. Zhang & W. Lam (1998). Daily prediction of
major stock indices from textual WWW data. In
In Proceedings of the 4th International Confer-
ence on Knowledge Discovery and Data Mining
- KDD-98, pp. 364?368.
254
Proceedings of NAACL HLT 2007, pages 388?395,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Tree Revision Learning for Dependency Parsing
Giuseppe Attardi
Dipartimento di Informatica
Universita` di Pisa
Pisa, Italy
attardi@di.unipi.it
Massimiliano Ciaramita
Yahoo! Research Barcelona
Barcelona, Spain
massi@yahoo-inc.com
Abstract
We present a revision learning model for
improving the accuracy of a dependency
parser. The revision stage corrects the out-
put of the base parser by means of revi-
sion rules learned from the mistakes of
the base parser itself. Revision learning
is performed with a discriminative classi-
fier. The revision stage has linear com-
plexity and preserves the efficiency of the
base parser. We present empirical evalu-
ations on the treebanks of two languages,
which show effectiveness in relative error
reduction and state of the art accuracy.
1 Introduction
A dependency parse tree encodes useful semantic in-
formation for several language processing tasks. De-
pendency parsing is a simpler task than constituent
parsing, since dependency trees do not have ex-
tra non-terminal nodes and there is no need for a
grammar to generate them. Approaches to depen-
dency parsing either generate such trees by consid-
ering all possible spanning trees (McDonald et al,
2005), or build a single tree on the fly by means of
shift-reduce parsing actions (Yamada & Matsumoto,
2003). In particular, Nivre and Scholz (2004) and
Attardi (2006) have developed deterministic depen-
dency parsers with linear complexity, suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
We investigate a novel revision approach to
dependency parsing related to re-ranking and
transformation-based methods (Brill, 1993; Brill,
1995; Collins, 2000; Charniak & Johnson, 2005;
Collins & Koo, 2006). Similarly to re-ranking, the
second stage attempts to improve the output of a
base parser. Instead of re-ranking n-best candi-
date parses, our method works by revising a sin-
gle parse tree, either the first-best or the one con-
structed by a deterministic shift-reduce parser, as in
transformation-based learning. Parse trees are re-
vised by applying rules which replace incorrect with
correct dependencies. These rules are learned by
comparing correct parse trees with incorrect trees
produced by the base parser on a training corpus.
We use the same training corpus on which the base
parser was trained, but this need not be the case.
Hence, we define a new learning task whose output
space is a set of revision rules and whose input is
a set of features extracted at each node in the parse
trees produced by the parser on the training corpus.
A statistical classifier is trained to solve this task.
The approach is more suitable for dependency
parsing since trees do not have non-terminal nodes,
therefore revisions do not require adding/removing
nodes. However, the method applies to any parser
since it only analyzes output trees. An intuitive mo-
tivation for this method is the observation that a
dependency parser correctly identifies most of the
dependencies in a tree, and only local corrections
might be necessary to produce a correct tree. Per-
forming several parses in order to generate multiple
trees would often just repeat the same steps. This
could be avoided by focusing on the points where at-
tachments are incorrect. In the experiments reported
below, on average, the revision stage performs 4.28
388
corrections per sentence, or one every 6.25 tokens.
In our implementation we adopt a shift-reduce
parser which minimizes computational costs. The
resulting two-stage parser has complexity O(n), lin-
ear in the length of the sentence. We evaluated our
model on the treebanks of English and Swedish. The
experimental results show a relative error reduction
of, respectively, 16% and 11% with respect to the
base parser, achieving state of accuracy on Swedish.
2 Dependency parsing
Detection of dependency relations can be useful
in tasks such as information extraction (Culotta &
Sorensen, 2004), lexical acquisition (Snow et al,
2005), ontology learning (Ciaramita et al, 2005),
and machine translation (Ding & Palmer, 2005).
A dependency parser is trained on a corpus an-
notated with lexical dependencies, which are eas-
ier to produce by annotators without deep linguis-
tic knowledge and are becoming available in many
languages (Buchholz & Marsi, 2006). Recent de-
velopments in dependency parsing show that deter-
ministic parsers can achieve good accuracy (Nivre &
Scholz, 2004), and high performance, in the range of
hundreds of sentences per second (Attardi, 2006).
A dependency parser takes as input a sentence
s and returns a dependency graph G. Let D =
{d1, d2, ..., dm} be the set of permissible depen-
dency types. A dependency graph for a sentence
s = ?s1, s2, ..., sn? is a labeled directed graph G =
(s,A), such that:
(a) s is the set of nodes, corresponding to the to-
kens in the input string;
(b) A is a set of labeled arcs (wi, d, wj), wi,j ? s,
d ? D; wj is called the head, wi the modifier
and d the dependency label;
(c) ?wi ? s there is at most one arc a ? A, such
that a = (wi, d, wj);
(d) there are no cycles;
In statistical parsing a generator (e.g. a
PCFG) is used to produce a number of candidate
trees (Collins, 2000) with associated scores. This
approach has been used also for dependency parsing,
generating spanning trees as candidates and comput-
ing the maximum spanning tree using discriminative
learning algorithms (McDonald et al, 2005).
Shift ?S,n|I,T,A??n|S,I,T,A? (1)
Right ?s|S,n|I,T,A??S,n|I,T,A?{(s,r,n)}? (2)
Left ?s|S,n|I,T,A??S,s|I,T,A?{(n,r,s)}? (3)
Right2
?s1|s2|S,n|I,T,A?
?s1|S,n|I,T,A?{(s2,r,n)}?
(4)
Left2
?s1|s2|S,n|I,T,A?
?s2|S,s1|I,T,A?{(n,r,s2)}?
(5)
Right3
?s1|s2|s3|S,n|I,T,A?
?s1|s2|S,n|I,T,A?{(s3,r,n)}?
(6)
Left3
?s1|s2|s3|S,n|I,T,A?
?s2|s3|S,s1|I,T,A?{(n,r,s3)}?
(7)
Extract ?s1|s2|S,n|I,T,A??n|s1|S,I,s2|T,A? (8)
Insert ?S,I,s1|T,A??s1|S,I,T,A? (9)
Table 1. The set of parsing rules of the base parser.
Yamada and Matsumoto (2003) have proposed an
alternative approach, based on deterministic bottom-
up parsing. Instead of learning directly which tree
to assign to a sentence, the parser learns which
Shift/Reduce actions to use for building the tree.
Parsing is cast as a classification problem: at each
step the parser applies a classifier to the features rep-
resenting its current state to predict the next action to
perform. Nivre and Scholz (2004) proposed a vari-
ant of the model of Yamada and Matsumoto that re-
duces the complexity from the worst case quadratic
to linear. Attardi (2006) proposed a variant of the
rules that allows deterministic single-pass parsing
and as well as handling non-projective relations.
Several approaches to dependency parsing on multi-
ple languages have been evaluated in the CoNLL-X
Shared Task (Buchholz & Marsi, 2006).
3 A shift-reduce dependency parser
As a base parser we use DeSR, a shift-reduce
parser described in (Attardi, 2006). The parser
constructs dependency trees by scanning input sen-
tences in a single left-to-right pass and performing
Shift/Reduce parsing actions. The parsing algorithm
is fully deterministic and has linear complexity. Its
behavior can be described as repeatedly selecting
and applying some parsing rules to transform its
state.
The state of the parser is represented by a quadru-
389
ple ?S, I, T,A?: S is the stack, I is the list of (re-
maining) input tokens, T is a stack of saved to-
kens and A is the arc relation for the dependency
graph, consisting of a set of labeled arcs (wi, r, wj),
wi, wj ? W (the set of tokens), and d ? D (the
set of dependencies). Given an input sentence s,
the parser is initialized to ??, s, ?, ??, and terminates
when it reaches the configuration ?s, ?, ?, A?.
Table 1 lists all parsing rules. The Shift rule
advances on the input, while the various Left,
Right variants create links between the next in-
put token and some previous token on the stack.
Extract/Insert generalize the previous rules by
respectively moving one token to the stack T and
reinserting the top of T into S. An essential differ-
ence with respect to the rules of Yamada and Mat-
sumoto (2003) is that the Right rules move back to
the input the top of the stack, allowing some further
processing on it, which would otherwise require a
second pass. The extra Left and Right rules (4-
7, Table 1), and the ExtractInsert rules (8 and
9, Table 1), are new rules added for handling non-
projective trees. The algorithm works as follows:
Algorithm 1: DeSR
input: s = w1, w2, ..., wn
begin
S ? ??
I ? ?w1, w2, ..., wn?
T ? ??
A? ??
while I 6= ?? do
x? getContext(S, I, T,A)
y ? estimateAction(w,x)
performAction(y, S, I, T,A)
end
The function getContext() extracts a vector x
of contextual features around the current token, i.e.,
from a subset of I and S. estimateAction() pre-
dicts a parsing action y given a trained modelw and
x. In the experiments presented below, we used as
features the lemma, Part-of-Speech, and dependency
type of the following items:
? 2 top items from S;
? 4 items from I;
Step Description
r Up to root node
u Up one parent
?n Left to the n-th token
+n Right to the n-th token
[ Head of previous constituent
] Head of following constituent
> First token of previous constituent
< First token of following constituent
d?? Down to the leftmost child
d + + Down to the rightmost child
d? 1 Down to the first left child
d + 1 Down to the first right child
dP Down to token with POS P
Table 2. Description of the atomic movements allowed on
the graph relatively to a token w.
? 2 leftmost and 2 rightmost children from the
top of S and I .
4 Revising parse trees
The base parser is fairly accurate and even when
there are mistakes most sentence chunks are correct.
The full correct parse tree can often be recovered by
performing just a small number of revisions on the
base parse. We propose to learn these revisions and
to apply them to the single best tree output by the
base parser. Such an approach preserves the deter-
ministic nature of the parser, since revising the tree
requires a second sequential step over the whole sen-
tence. The second step may also improve accuracy
by incorporating additional evidence, gathered from
the analysis of the tree which is not available during
the first stage of parsing.
Our approach introduces a second learning task
in which a model is trained to revise parse trees.
Several questions needs to be addressed: which tree
transformations to use in revising the parse tree,
how to determine which transformation to apply, in
which order, and which features to use for learning.
4.1 Basic graph movements
We define a revision as a combination of atomic
moves on a graph; e.g., moving a link to the follow-
ing or preceding token in the sentence, up or down
the graph following the directed edges. Table 2 sum-
marizes the set of atomic steps we used.
390
Figure 1. An incorrect dependency tree: the dashed arrow from ?sale? to ?by? should be replaced with the one from
?offered? to ?by?.
4.2 Revision rules
A revision rule is a sequence of atomic steps on the
graph which identifies the head of a modifier. As an
example, Figure 1 depicts a tree in which the mod-
ifier ?by? is incorrectly attached to the head ?sale?
(dashed arrow), rather than to the correct head ?of-
fered? (continuous arrow)1. There are several possi-
ble revision rules for this case: ?uu?, move up two
nodes; ?3, three tokens to the left, etc. To bound
the complexity of feature extraction the maximum
length of a sequence is bound to 4. A revision for
a dependency relation is a link re-direction, which
moves a single link in a tree to a different head. This
is an elementary transformation which preserves the
number of nodes in the tree.
A possible problem with these rules is that they
are not tree-preserving, i.e. a tree may become a
cyclic graph. For instance, rules that create a link
to a descendant introduce cycles, unless the appli-
cation of another rule will link one of the nodes in
the path to the descendant to a node outside the cy-
cle. To address these issues we apply the following
heuristics in selecting the proper combination: rules
that redirect to child nodes are chosen only when
no other rule is applicable (upwards rule are safe),
and shorter rules are preferred over longer ones. In
our experiments we never observed the production
of any cycles.
On Wall Street Journal Penn Treebank section 22
we found that the 20 most frequent rules are suffi-
cient to correct 80% of the errors, see Table 3. This
confirms that the atomic movements produce simple
and effective revision rules.
1Arrows go from head to modifier as agreed among the par-
ticipants to the CoNLL-X shared task.
COUNTS RULE TARGET LOCATION
983 uu Up twice
685 -1 Token to the left
469 +1 Token to the right
265 [ Head of previous constituent
215 uuu Up 3 times
197 +1u Right, up
194 r To root
174 -1u Left, up
116 >u Token after constituent, up
103 ud?? Up down to leftmost child
90 V To 1st child with POS verb
83 d+1 Down to first right child
82 uuuu Up 4 times
74 < Token before constituent
73 ud+1 Up down to 1st right child
71 uV Up, down to 1st verb
61 ud-1 Up, down to last left child
56 ud+1d+1 Up, down to 1st right child twice
55 d+1d+1 Down to 1st right child twice
48 d?? Down to leftmost child
Table 3. 20 most frequent revision rules in wsj22.
4.3 Tree revision problem
The tree revision problem can be formalized as fol-
lows. Let G = (s,A) be a dependency tree for
sentence s = ?w1, w2, ..., wn?. A revision rule is
a mapping r : A ? A which, when applied to an
arc a = (wi, d, wj), returns an arc a? = (wi, d, ws).
A revised parse tree is defined as r(G) = (s,A?)
such that A? = {r(a) : a ? A}.
This definition corresponds to applying the revi-
sions to the original tree in a batch, as in (Brill,
1993). Alternatively, one could choose to apply the
transformations incrementally, applying each one to
the tree resulting from previous applications. We
chose the first alternative, since the intermediate
trees created during the transformation process may
not be well-formed dependency graphs, and analyz-
ing them in order to determine features for classifi-
391
cation might incur problems. For instance, the graph
might have abnormal properties that differ from
those of any other graph produced by the parser.
Moreover, there might not be enough cases of such
graphs to form a sufficiently large training set.
5 Learning a revision model
We frame the problem of revising a tree as a super-
vised classification task. Given a training set S =
(xi, yi)Ni=1, such that xi ? IR
d and yi ? Y , our goal
is to learn a classifier, i.e., a function F : X ? Y .
The output space represents the revision rules, in
particular we denote with y1 the identity revision
rule. Features represents syntactic and morphologi-
cal properties of the dependency being examined in
its context on the graph.
5.1 Multiclass perceptron
The classifier used in revision is based on the per-
ceptron algorithm (Rosemblatt, 1958), implemented
as a multiclass classifier (Crammer & Singer, 2003).
One introduces a weight vector ?i ? IRd for each
yi ? Y , in which ?i,j represents the weight associ-
ated with feature j in class i, and learn ? with the
perceptron from the training data using a winner-
take-all discriminant function:
F (x) = argmax
y?Y
?x, ?y? (10)
The only adjustable parameter in this model is the
number of instances T to use for training. We chose
T by means of validation on the development data,
typically with a value around 10 times the size of the
training data. For regularization purposes we adopt
an average perceptron (Collins, 2002) which returns
for each y, ?y = 1T
?T
t=1 ?
t
y, the average of all
weight vectors ?ty posited during training. The per-
ceptron was chosen because outperformed other al-
gorithms we experimented with (MaxEnt, MBL and
SVM), particularly when including feature pairs, as
discussed later.
5.2 Features
We used as features for the revision phase the same
type of features used for training the parser (de-
scribed in Section 3). This does not have to be the
case in general. In fact, one might want to introduce
features that are specific for this task. For example,
global features of the full tree which might be not
possible to represent or extract while parsing, as in
statistical parse re-ranking (Collins & Koo, 2006).
The features used are lemma, Part-of-Speech, and
dependency type of the following items: the current
node, its parent, grandparent, great-grandparent, of
the children thereof and, in addition, the previous
and next tokens of the node. We also add as features
all feature pairs that occurred more than 10 times,
to reduce the size of the feature space. In alternative
one could use a polynomial kernel. We preferred this
option because, given the large size of the training
data, a dual model is often impractical.
5.3 Revision model
Given a dependency graph G = (s,A), for a sen-
tence s = ?w1, ..., wn?, the revised tree is R(G) =
(s,A?), where each dependency a?i is equal to F (ai).
In other words, the head in ai has been changed, or
not, according to the rule predicted by the classifier.
In particular, we assume that revisions are indepen-
dent of each other and perform a revision of a tree
from left to right. As Table 3 suggests, there are
many revision rules with low frequency. Rather than
learning a huge classifier, for rules with little train-
ing data, we limit the number of classes to a value
k. We experimented with values between 30 and
50, accounting for 98-99% of all rules, and even-
tually used 50, by experimenting with the develop-
ment portion of the data. All rules that fall outside
the threshold are collected in a single class y0 of ?un-
resolved? cases. If predicted, y0, similarly to y1, has
no effect on the dependency.
Occasionally, in 59 sentences out of 2416 on
section 23 of the Wall Street Journal Penn Tree-
bank (Marcus et al, 1993), the shift-reduce parser
fails to attach a node to a head, producing a dis-
connected graph. The disconnected node will ap-
pear as a root, having no head. The problem occurs
most often on punctuations (66/84 on WSJ section
23), so it affects only marginally the accuracy scores
(UAS, LAS) as computed in the CoNLL-X evalua-
tion (Buchholz & Marsi, 2006). A final step of the
revision deals with multiple roots, using a heuristic
rule it selects one of the disconnected sub-trees as
root, a verb, and attaches all sub-trees to it.
392
Figure 2. Frequency of the 30 most frequent rules ob-
tained with different parsers on wsj22 and wsj2-21.
5.4 Algorithm complexity
The base dependency parser is deterministic and per-
forms a single scan over the sentence. For each word
it performs feature extraction and invokes the classi-
fier to predict the parsing action. If prediction time is
bound by a constant, as in linear classifiers, parsing
has linear complexity. The revision pass is deter-
ministic and performs similar feature extraction and
prediction on each token. Hence, the complexity of
the overall parser is O(n). In comparison, the com-
plexity of McDonald?s parser (2006) is cubic, while
the parser of Yamada and Matsumoto (2003) has a
worst case quadratic complexity.
6 Experiments
6.1 Data and setup
We evaluated our method on English using the stan-
dard partitions of the Wall Street Journal Penn Tree-
bank: sections 2-21 for training, section 22 for
development, and section 23 for evaluation. The
constituent trees were transformed into dependency
trees by means of a script implementing rules pro-
posed by Collins and Yamada2. In a second eval-
uation we used the Swedish Treebank (Nilsson et
al., 2005) from CoNLL-X, approximately 11,000
sentences; for development purposes we performed
cross-validation on the training data.
We trained two base parsers on the Penn Tree-
bank: one with our own implementation of Maxi-
2http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
Parser UAS LAS
DeSR-ME 84.96 83.53
DeSR-MBL 88.41 86.85
Revision-MBL 89.11 86.39
Revision-ME 90.27 86.44
N&S 87.3 -
Y&M 90.3 -
MST-2 91.5 -
Table 4. Results on the Wall Street Journal Penn Tree-
bank.
mum Entropy, one with the TiMBL library for Mem-
ory Based Learning (MBL, (Timbl, 2003)). We
parsed sections 2 to 21 with each parser and pro-
duced two datasets for training the revision model:
?wsj2-21.mbl? and ?wsj2-21.me?. Each depen-
dency is represented as a feature vector (cf. Sec-
tion 5.2), the prediction is a revision rule (cf. Sec-
tion 4.2). For the smaller Swedish data we trained
one base parser with MaxEnt and one with the SVM
implementation in libSVM (Chang & Lin, 2001) us-
ing a polynomial kernel with degree 2.
6.2 Results
On the Penn Treebank, the base parser trained with
MBL (DeSR-MBL) achieves higher accuracy, 88.41
unlabeled accuracy score (UAS), than the same
parser trained with MaxEnt (DeSR-ME), 84.96
UAS. The revision model trained on ?wsj2-21.me?
(Revision-ME) increases the accuracy of DeSR-ME
to 88.01 UAS (+3%). The revision model trained
on ?wsj2-21.mbl? (DeSR-MBL) improves the accu-
racy of DeSR-MBL from 88.42 to 89.11 (+0.7%).
The difference is mainly due to the fact that DeSR-
MBL is quite accurate on the training data, almost
99%, hence ?wsj2-21.mbl? contains less errors on
which to train the revision parser. This is typi-
cal of the memory-based learning algorithm used
in DeSR-MBL. Conversely, DeSR-ME achieves a
score of of 85% on the training data, which is
closer to the actual accuracy of the parser on unseen
data. As an illustration, Figure 2 plots the distri-
butions of revision rules in ?wsj2-21.mbl? (DeSR-
MBL), ?wsj2-21.me? (DeSR-ME), and ?wsj22.mbl?
(DeSR-MBL) which represents the distribution of
correct revision rules on the output of DeSR-MBL
on the development set. The distributions of ?wsj2-
393
Parser UAS LAS
DeSR-SVM 88.41 83.31
Revision-ME 89.76 83.13
Corston-Oliver& Aue 89.54 82.33
Nivre 89.50 84.58
Table 5. Results on the Swedish Treebank.
21.me? and ?wsj22.mbl? are visibly similar, while
?wsj2-21.mbl? is significantly more skewed towards
not revising. Hence, the less accurate parser DeSR-
ME might be more suitable for producing revision
training data. Applying the revision model trained
on ?wsj2-21.me? (Revision-ME) to the output of
DeSR-MBL the result is 90.27% UAS. A relative
error reduction of 16.05% from the previous 88.41
UAS of DeSR-MBL. This finding suggests that it
may be worth while experimenting with all possi-
ble revision-model/base-parser pairs as well as ex-
ploring alternative ways for generating data for the
revision model; e.g., by cross-validation.
Table 4 summarizes the results on the Penn Tree-
bank. Revision models are evaluated on the output
of DeSR-MBL. The table also reports the scores ob-
tained on the same data set by by the shift reduce
parsers of Nivre and Scholz?s (2004) and Yamada
andMatsumoto (2003), andMcDonald and Pereira?s
second-order maximum spanning tree parser (Mc-
Donald & Pereira, 2006). However the scores are
not directly comparable, since in our experiments
we used the settings of the CoNLL-X Shared Task,
which provide correct POS tags to the parser.
On the Swedish Treebank collection we trained
a revision model (Revision-ME) on the output of
the MaxEnt base parser. We parsed the evalua-
tion data with the SVM base parser (DeSR-SVM)
which achieves 88.41 UAS. The revision model
achieves 89.76 UAS, with a relative error reduc-
tion of 11.64%. Here we can compare directly with
the best systems for this dataset in CoNLL-X. The
best system (Corston-Oliver & Aue, 2006), a vari-
ant of the MST algorithm, obtained 89.54 UAS,
while the second system (Nivre, 2006) obtained
89.50; cf. Table 5. Parsing the Swedish evalua-
tion set (about 6,000 words) DeSR-SVM processes
1.7 words per second on a Xeon 2.8Ghz machine,
DeSR-ME parses more than one thousand w/sec. In
the revision step Revision-ME processes 61 w/sec.
7 Related work
Several authors have proposed to improve parsing
via re-ranking (Collins, 2000; Charniak & Johnson,
2005; Collins & Koo, 2006). The base parser pro-
duces a list of n-best parse trees for a sentence. The
re-ranker is trained on the output trees, using addi-
tional global features, with a discriminative model.
These approaches achieve error reductions up to
13% (Collins & Koo, 2006). In transformation-
based learning (Brill, 1993; Brill, 1995; Satta &
Brill, 1995) the learning algorithm starts with a
baseline assignment, e.g., the most frequent Part-of-
Speech for a word, then repeatedly applies rewriting
rules. Similarly to re-ranking our method aims at
improving the accuracy of the base parser with an
additional learner. However, as in transformation-
based learning, it avoids generating multiple parses
and applies revisions to arcs in the tree which it con-
siders incorrect. This is consistent with the architec-
ture of our base parser, which is deterministic and
builds a single tree, rather than evaluating the best
outcome of a generator.
With respect to transformation-based methods,
our method does not attempt to build a tree but only
to revise it. That is, it defines a different output space
from the base parser?s: the possible revisions on the
graph. The revision model of Nakagawa et al (2002)
applies a second classifier for deciding whether the
predictions of a base learner are accurate. However,
the model only makes a binary decision, which is
suitable for the simpler problem of POS tagging.
The work of Hall and Novak (Hall & Novak, 2005)
is the closest to ours. Hall and Novak develop a cor-
rective model for constituency parsing in order to
recover non-projective dependencies, which a stan-
dard constituent parser does not handle. The tech-
nique is applied to parsing Czech.
8 Conclusion
We presented a novel approach for improving the
accuracy of a dependency parser by applying re-
vision transformations to its parse trees. Experi-
mental results prove that the approach is viable and
promising. The proposed method achieves good ac-
curacy and excellent performance using a determin-
istic shift-reduce base parser. As an issue for further
investigation, we mention that in this framework, as
394
in re-ranking, it is possible to exploit global features
in the revision phase; e.g., semantic features such as
those produced by named-entity detection systems.
Acknowledgments
We would like to thank Jordi Atserias and Brian
Roark for useful discussions and comments.
References
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
E. Brill. 1993. Automatic Grammar Induction and Pars-
ing free Text: A Transformation-Based Approach. In
Proceedings of ACL 1993.
E. Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing. Compu-
tational Linguistics 21(4): pp.543-565.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
C. Chang and C. Lin. 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
M. Ciaramita, A. Gangemi, E. Ratsch, J. Saric? and I. Ro-
jas. 2005. Unsupervised Learning of Semantic Rela-
tions between Concepts of a Molecular Biology Ontol-
ogy. In Proceedings of IJCAI 2005.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
M. Collins and T. Koo. 2006. Discriminative Reranking
for Natural Language Parsing. Computational Lin-
guistics 31(1): pp.25-69.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
S. Corston-Oliver and A. Aue. 2006. Dependency Pars-
ing with Reference to Slovene, Spanish and Swedish.
In Proceedings of CoNLL-X.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2003. Timbl: Tilburg memory
based learner, version 5.0, reference guide. Technical
Report ILK 03-10, Tilburg University, ILK.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
K. Hall and V. Novak. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of the 9th International Workshop on Parsing Tech-
nologies.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic?. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
T. Nakagawa, T. Kudo and Y. Matsumoto. 2002. Revi-
sion Learning and its Applications to Part-of-Speech
Tagging. In Proceedings of ACL 2002.
J. Nilsson, J. Hall and J. Nivre. 2005. MAMBA Meets
TIGER: Reconstructing a Swedish Treebank from An-
tiquity. In Proceedings of the NODALIDA.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
J. Nivre. 2006. Labeled Pseudo-Projective Dependency
Parsing with Support Vector Machines. In Proceed-
ings of CoNLL-X.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
G. Satta and E. Brill. 1995, Efficient Transformation-
Based Parsing. In Proceedings of ACL 1996.
R. Snow, D. Jurafsky and Y. Ng 2005. Learning Syn-
tactic Patterns for Automatic Hypernym Discovery. In
Proceedings of NIPS 17.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proceedings of the 9th International Workshop on
Parsing Technologies.
395
Proceedings of ACL-08: HLT, pages 719?727,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning to Rank Answers on Large Online QA Collections
Mihai Surdeanu, Massimiliano Ciaramita, Hugo Zaragoza
Barcelona Media Innovation Center, Yahoo! Research Barcelona
mihai.surdeanu@barcelonamedia.org, {massi,hugo}@yahoo-inc.com
Abstract
This work describes an answer ranking engine
for non-factoid questions built using a large
online community-generated question-answer
collection (Yahoo! Answers). We show how
such collections may be used to effectively
set up large supervised learning experiments.
Furthermore we investigate a wide range of
feature types, some exploiting NLP proces-
sors, and demonstrate that using them in com-
bination leads to considerable improvements
in accuracy.
1 Introduction
The problem of Question Answering (QA) has re-
ceived considerable attention in the past few years.
Nevertheless, most of the work has focused on the
task of factoid QA, where questions match short an-
swers, usually in the form of named or numerical en-
tities. Thanks to international evaluations organized
by conferences such as the Text REtrieval Confer-
ence (TREC)1 or the Cross Language Evaluation Fo-
rum (CLEF) Workshop2, annotated corpora of ques-
tions and answers have become available for several
languages, which has facilitated the development of
robust machine learning models for the task.
The situation is different once one moves beyond
the task of factoid QA. Comparatively little research
has focused on QA models for non-factoid ques-
tions such as causation, manner, or reason questions.
Because virtually no training data is available for
this problem, most automated systems train either
1http://trec.nist.gov
2http://www.clef-campaign.org
Q: How do you quiet a squeaky door?
A: Spray WD-40 directly onto the hinges
of the door. Open and close the door
several times. Remove hinges if the
door still squeaks. Remove any rust,
dirt or loose paint. Apply WD-40 to
High removed hinges. Put the hinges back,
Quality open and close door several times again.
Q: How to extract html tags from an html
Low documents with c++?
Quality A: very carefully
Table 1: Sample content from Yahoo! Answers.
on small hand-annotated corpora built in house (Hi-
gashinaka and Isozaki, 2008) or on question-answer
pairs harvested from Frequently Asked Questions
(FAQ) lists or similar resources (Soricut and Brill,
2006). None of these situations is ideal: the cost
of building the training corpus in the former setup
is high; in the latter scenario the data tends to be
domain-specific, hence unsuitable for the learning of
open-domain models.
On the other hand, recent years have seen an ex-
plosion of user-generated content (or social media).
Of particular interest in our context are community-
driven question-answering sites, such as Yahoo! An-
swers3, where users answer questions posed by other
users and best answers are selected manually either
by the asker or by all the participants in the thread.
The data generated by these sites has significant ad-
vantages over other web resources: (a) it has a high
growth rate and it is already abundant; (b) it cov-
ers a large number of topics, hence it offers a better
3http://answers.yahoo.com
719
approximation of open-domain content; and (c) it is
available for many languages. Community QA sites,
similar to FAQs, provide large number of question-
answer pairs. Nevertheless, this data has a signifi-
cant drawback: it has high variance of quality, i.e.,
answers range from very informative to completely
irrelevant or even abusive. Table 1 shows some ex-
amples of both high and low quality content.
In this paper we address the problem of answer
ranking for non-factoid questions from social media
content. Our research objectives focus on answering
the following two questions:
1. Is it possible to learn an answer ranking model
for complex questions from such noisy data?
This is an interesting question because a posi-
tive answer indicates that a plethora of training
data is readily available to QA researchers and
system developers.
2. Which features are most useful in this sce-
nario? Are similarity models as effective as
models that learn question-to-answer transfor-
mations? Does syntactic and semantic infor-
mation help? For generality, we focus only on
textual features extracted from the answer text
and we ignore all meta data information that is
not generally available.
Notice that we concentrate on one component of a
possible social-media QA system. In addition to
answer ranking, a complete system would have to
search for similar questions already answered (Jeon
et al, 2005), and rank content quality using ?social?
features such as the authority of users (Jeon et al,
2006; Agichtein et al, 2008). This is not the focus of
our work: here we investigate the problem of learn-
ing an answer ranking model capable of dealing with
complex questions, using a large number of, possi-
ble noisy, question-answer pairs. By focusing exclu-
sively on textual content we increase the portability
of our approach to other collections where ?social?
features might not available, e.g., Web search.
The paper is organized as follows. We describe
our approach, including all the features explored for
answer modeling, in Section 2. We introduce the
corpus used in our empirical analysis in Section 3.
We detail our experiments and analyze the results in
Section 4. We overview related work in Section 5
and conclude the paper in Section 6.
AnswerCollection
Answers
Translation
Features
Web Correlation
FeaturesFeatures
Similarity
Answer
RankingQ AnswerRetrieval
(unsupervised)
(discriminative learning)
(class?conditional learning)
Features
Density/Frequency
Figure 1: System architecture.
2 Approach
The architecture of the QA system analyzed in the
paper, summarized in Figure 1, follows that of the
most successful TREC systems. The first com-
ponent, answer retrieval, extracts a set of candi-
date answers A for question Q from a large col-
lection of answers, C, provided by a community-
generated question-answering site. The retrieval
component uses a state-of-the-art information re-
trieval (IR) model to extract A given Q. Since
our focus is on exploring the usability of the an-
swer content, we do not perform retrieval by find-
ing similar questions already answered (Jeon et al,
2005), i.e., our answer collection C contains only
the site?s answers without the corresponding ques-
tions answered.
The second component, answer ranking, assigns
to each answer Ai ? A a score that represents
the likelihood that Ai is a correct answer for Q,
and ranks all answers in descending order of these
scores. The scoring function is a linear combina-
tion of four different classes of features (detailed in
Section 2.2). This function is the focus of the pa-
per. To answer our first research objective we will
compare the quality of the rankings provided by this
component against the rankings generated by the IR
model used for answer retrieval. To answer the sec-
ond research objective we will analyze the contri-
bution of the proposed feature set to this function.
Again, since our interest is in investigating the util-
ity of the answer textual content, we use only infor-
mation extracted from the answer text when learn-
ing the scoring function. We do not use any meta
information (e.g., answerer credibility, click counts,
etc.) (Agichtein et al, 2008; Jeon et al, 2006).
Our QA approach combines three types of ma-
chine learning methodologies (as highlighted in Fig-
ure 1): the answer retrieval component uses un-
720
supervised IR models, the answer ranking is im-
plemented using discriminative learning, and fi-
nally, some of the ranking features are produced
by question-to-answer translation models, which use
class-conditional learning.
2.1 Ranking Model
Learning with user-generated content can involve
arbitrarily large amounts of data. For this reason
we choose as a ranking algorithm the Perceptron
which is both accurate and efficient and can be
trained with online protocols. Specifically, we im-
plement the ranking Perceptron proposed by Shen
and Joshi (2005), which reduces the ranking prob-
lem to a binary classification problem. The general
intuition is to exploit the pairwise preferences in-
duced from the data by training on pairs of patterns,
rather than independently on each pattern. Given a
weight vector ?, the score for a pattern x (a candi-
date answer) is simply the inner product between the
pattern and the weight vector:
f?(x) = ?x, ?? (1)
However, the error function depends on pairwise
scores. In training, for each pair (xi,xj) ? A,
the score f?(xi ? xj) is computed; note that if f
is an inner product f?(xi?xj) = f?(xi)? f?(xj).
Given a margin function g(i, j) and a positive rate ? ,
if f?(xi ? xj) ? g(i, j)? , an update is performed:
?t+1 = ?t + (xi ? xj)?g(i, j) (2)
By default we use g(i, j) = (1i ? 1j ), as a mar-
gin function, as suggested in (Shen and Joshi, 2005),
and find ? empirically on development data. Given
that there are only two possible ranks in our set-
ting, this function only generates two possible val-
ues. For regularization purposes, we use as a final
model the average of all Perceptron models posited
during training (Freund and Schapire, 1999).
2.2 Features
In the scoring model we explore a rich set of features
inspired by several state-of-the-art QA systems. We
investigate how such features can be adapted and
combined for non-factoid answer ranking, and per-
form a comparative feature analysis using a signif-
icant amount of real-world data. For clarity, we
group the features into four sets: features that model
the similarity between questions and answers (FG1),
features that encode question-to-answer transfor-
mations using a translation model (FG2), features
that measure keyword density and frequency (FG3),
and features that measure the correlation between
question-answer pairs and other collections (FG4).
Wherever applicable, we explore different syntactic
and semantic representations of the textual content,
e.g., extracting the dependency-based representation
of the text or generalizing words to their WordNet
supersenses (WNSS) (Ciaramita and Altun, 2006).
We detail each of these feature groups next.
FG1: Similarity Features
We measure the similarity between a question
Q and an answer A using the length-normalized
BM25 formula (Robertson and Walker, 1997). We
chose this similarity formula because, out of all the
IR models we tried, it provided the best ranking at
the output of the answer retrieval component. For
completeness we also include in the feature set the
value of the tf ?idf similarity measure. For both for-
mulas we use the implementations available in the
Terrier IR platform4 with the default parameters.
To understand the contribution of our syntactic
and semantic processors we compute the above sim-
ilarity features for five different representations of
the question and answer content:
Words (W) - this is the traditional IR view where the
text is seen as a bag of words.
Dependencies (D) - the text is represented as a bag
of binary syntactic dependencies. The relative syn-
tactic processor is detailed in Section 3. Dependen-
cies are fully lexicalized but unlabeled and we cur-
rently extract dependency paths of length 1, i.e., di-
rect head-modifier relations (this setup achieved the
best performance).
Generalized dependencies (Dg) - same as above, but
the words in dependencies are generalized to their
WNSS, if detected.
Bigrams (B) - the text is represented as a bag of bi-
grams (larger n-grams did not help). We added this
view for a fair analysis of the above syntactic views.
Generalized bigrams (Bg) - same as above, but the
words are generalized to their WNSS.
4http://ir.dcs.gla.ac.uk/terrier
721
In all these representations we skip stop words
and normalize all words to their WordNet lemmas.
FG2: Translation Features
Berger et al (2000) showed that similarity-based
models are doomed to perform poorly for QA be-
cause they fail to ?bridge the lexical chasm? be-
tween questions and answers. One way to address
this problem is to learn question-to-answer trans-
formations using a translation model (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007). In our model, we in-
corporate this approach by adding the probability
that the question Q is a translation of the answer A,
P (Q|A), as a feature. This probability is computed
using IBM?s Model 1 (Brown et al, 1993):
P (Q|A) =
?
q?Q
P (q|A) (3)
P (q|A) = (1? ?)Pml(q|A) + ?Pml(q|C) (4)
Pml(q|A) =
?
a?A
(T (q|a)Pml(a|A)) (5)
where the probability that the question term q is
generated from answer A, P (q|A), is smoothed us-
ing the prior probability that the term q is gen-
erated from the entire collection of answers C,
Pml(q|C). ? is the smoothing parameter. Pml(q|C)
is computed using the maximum likelihood estima-
tor. Pml(q|A) is computed as the sum of the proba-
bilities that the question term q is a translation of an
answer term a, T (q|a), weighted by the probability
that a is generated from A. The translation table for
T (q|a) is computed using the EM-based algorithm
implemented in the GIZA++ toolkit5.
Similarly with the previous feature group, we
add translation-based features for the five differ-
ent text representations introduced above. By
moving beyond the bag-of-word representation we
hope to learn relevant transformations of structures,
e.g., from the ?squeaky? ? ?door? dependency to
?spray? ? ?WD-40? in the Table 1 example.
FG3: Density and Frequency Features
These features measure the density and frequency
of question terms in the answer text. Variants of
these features were used previously for either an-
swer or passage ranking in factoid QA (Moldovan
et al, 1999; Harabagiu et al, 2000).
5http://www.fjoch.com/GIZA++.html
Same word sequence - computes the number of non-
stop question words that are recognized in the same
order in the answer.
Answer span - the largest distance (in words) be-
tween two non-stop question words in the answer.
Same sentence match - number of non-stop question
terms matched in a single sentence in the answer.
Overall match - number of non-stop question terms
matched in the complete answer.
These last two features are computed also for the
other four text representations previously introduced
(B, Bg, D, and Dg). Counting the number of
matched dependencies is essentially a simplified
tree kernel for QA (e.g., see (Moschitti et al,
2007)) matching only trees of depth 2. Experiments
with full dependency tree kernels based on several
variants of the convolution kernels of Collins and
Duffy (2001) did not yield improvements. We con-
jecture that the mistakes of the syntactic parser may
be amplified in tree kernels, which consider an ex-
ponential number of sub-trees.
Informativeness - we model the amount of informa-
tion contained in the answer by counting the num-
ber of non-stop nouns, verbs, and adjectives in the
answer text that do not appear in the question.
FG4: Web Correlation Features
Previous work has shown that the redundancy of
a large collection (e.g., the web) can be used for an-
swer validation (Brill et al, 2001; Magnini et al,
2002). In the same spirit, we add features that mea-
sure the correlation between question-answer pairs
and large external collections:
Web correlation - we measure the correlation be-
tween the question-answer pair and the web using
the Corrected Conditional Probability (CCP) for-
mula of Magnini et al (2002): CCP (Q,A) =
hits(Q + A)/(hits(Q) hits(A)2/3) where hits re-
turns the number of page hits from a search engine.
When a query returns zero hits we iteratively relax it
by dropping the keyword with the smallest priority.
Keyword priorities are assigned using the heuristics
of Moldovan et al (1999).
Query-log correlation - as in (Ciaramita et al, 2008)
we also compute the correlation between question-
answer pairs and a search-engine query-log cor-
pus of more than 7.5 million queries, which shares
722
roughly the same time stamp with the community-
generated question-answer corpus. We compute the
Pointwise Mutual Information (PMI) and Chi square
(?2) association measures between each question-
answer word pair in the query-log corpus. The
largest and the average values are included as fea-
tures, as well as the number of QA word pairs which
appear in the top 10, 5, and 1 percentile of the PMI
and ?2 word pair rankings.
3 The Corpus
The corpus is extracted from a sample of the U.S.
Yahoo! Answers logs. In this paper we focus on
the subset of advice or ?how to? questions due to
their frequency and importance in social communi-
ties.6 To construct our corpus, we implemented the
following successive filtering steps:
Step 1: from the full corpus we keep only questions
that match the regular expression:
how (to|do|did|does|can|would|could|should)
and have an answer selected as best either by
the asker or by the participants in the thread.
The outcome of this step is a set of 364,419
question-answer pairs.
Step 2: from the above corpus we remove the questions
and answers of obvious low quality. We im-
plement this filter with a simple heuristic by
keeping only questions and answers that have
at least 4 words each, out of which at least 1 is
a noun and at least 1 is a verb. This step filters
out questions like ?How to be excellent?? and
answers such as ?I don?t know?. The outcome
of this step forms our answer collection C. C
contains 142,627 question-answer pairs.7.
Arguably, all these filters could be improved. For
example, the first step can be replaced by a question
classifier (Li and Roth, 2005). Similarly, the second
step can be implemented with a statistical classifier
that ranks the quality of the content using both the
textual and non-textual information available in the
database (Jeon et al, 2006; Agichtein et al, 2008).
We plan to further investigate these issues which are
not the main object of this work.
6Nevertheless, the approach proposed here is independent
of the question type. We will explore answer ranking for other
non-factoid question types in future work.
7The data will be available through the Yahoo! Webscope
program (research-data-requests@yahoo-inc.com).
The data was processed as follows. The text was
split at the sentence level, tokenized and PoS tagged,
in the style of the Wall Street Journal Penn Tree-
Bank (Marcus et al, 1993). Each word was morpho-
logically simplified using the morphological func-
tions of the WordNet library8. Sentences were an-
notated with WNSS categories, using the tagger of
Ciaramita and Altun (2006)9, which annotates text
with a 46-label tagset. These tags, defined by Word-
Net lexicographers, provide a broad semantic cat-
egorization for nouns and verbs and include labels
for nouns such as food, animal, body and feeling,
and for verbs labels such as communication, con-
tact, and possession. Next, we parsed all sentences
with the dependency parser of Attardi et al (2007)10.
It is important to realize that the output of all men-
tioned processing steps is noisy and contains plenty
of mistakes, since the data has huge variability in
terms of quality, style, genres, domains etc., and do-
main adaptation for the NLP tasks involved is still
an open problem (Dredze et al, 2007).
We used 60% of the questions for training, 20%
for development, and 20% for test. The candidate
answer set for a given question is composed by one
positive example, i.e., its corresponding best answer,
and as negative examples all the other answers re-
trieved in the top N by the retrieval component.
4 Experiments
We evaluate our results using two measures: mean
Precision at rank=1 (P@1) ? i.e., the percentage of
questions with the correct answer on the first posi-
tion ? and Mean Reciprocal Rank (MRR) ? i.e., the
score of a question is 1/k, where k is the position
of the correct answer. We use as baseline the output
of our answer retrieval component (Figure 1). This
component uses the BM25 criterion, the highest per-
forming IR model in our experiments.
Table 2 lists the results obtained using this base-
line and our best model (?Ranking? in the table) on
the testing partition. Since we are interested in the
performance of the ranking model, we evaluate on
the subset of questions where the correct answer is
retrieved by answer retrieval in the top N answers
(similar to Ko et al (2007)). In the table we report
8http://wordnet.princeton.edu
9sourceforge.net/projects/supersensetag
10http://sourceforge.net/projects/desr
723
MRR P@1
N = 10 N = 15 N = 25 N = 50 N = 10 N = 15 N = 25 N = 50
recall@N 26.25% 29.04% 32.81% 38.09% 26.25% 29.04% 32.81% 38.09%
Baseline 61.33 56.12 50.31 43.74 45.94 41.48 36.74 31.66
Ranking 68.72?0.01 63.84?0.01 57.76?0.07 50.72?0.01 54.22?0.01 49.59?0.03 43.98?0.09 37.99?0.01
Improvement +12.04% +13.75% +14.80% +15.95% +18.02% +19.55% +19.70% +19.99%
Table 2: Overall results for the test partition.
results for several N values. For completeness, we
show the percentage of questions that match this cri-
terion in the ?recall@N? row.
Our ranking model was tuned strictly on the de-
velopment set (i.e., feature selection and parame-
ters of the translation models). During training, the
presentation of the training instances is randomized,
which generates a randomized ranking algorithm.
We exploit this property to estimate the variance in
the results produced by each model and report the
average result over 10 trials together with an esti-
mate of the standard deviation.
The baseline result shows that, for N = 15,
BM25 alone can retrieve in first rank 41% of the
correct answers, and MRR tells us that the correct
answer is often found within the first three answers
(this is not so surprising if we remember that in this
configuration only questions with the correct answer
in the first 15 were kept for the experiment). The
baseline results are interesting because they indicate
that the problem is not hopelessly hard, but it is far
from trivial. In principle, we see much room for im-
provement over bag-of-word methods.
Next we see that learning a weighted combina-
tion of features yields consistently marked improve-
ments: for example, for N = 15, the best model
yields a 19% relative improvement in P@1 and 14%
in MRR. More importantly, the results indicate that
the model learned is stable: even though for the
model analyzed in Table 2 we used N = 15 in train-
ing, we measure approximately the same relative im-
provement as N increases during evaluation.
These results provide robust evidence that: (a) we
can use publicly available online QA collections to
investigate features for answer ranking without the
need for costly human evaluation, (b) we can exploit
large and noisy online QA collections to improve the
accuracy of answer ranking systems and (c) readily
available and scalable NLP technology can be used
Iter. Feature Set MRR P@1
0 BM25(W) 56.06 41.12%
1 + translation(Bg) 61.13 46.24%
2 + overall match(D) 62.50 48.34%
3 + translation(W) 63.00 49.08%
4 + query-log avg(?2) 63.50 49.63%
5 + answer span
normalized by A size 63.71 49.84%
6 + query-log max(PMI) 63.87 50.09%
7 + same word sequence 63.99 50.23%
8 + translation(B) 64.03 50.30%
9 + tfidf(W) 64.08 50.42%
10 + same sentence match(W) 64.10 50.42%
11 + informativeness:
verb count 64.18 50.36%
12 + tfidf(B) 64.22 50.36%
13 + same word sequence
normalized by Q size 64.33 50.54%
14 + query-log max(?2) 64.46 50.66%
15 + same sentence match(W)
normalized by Q size 64.55 50.78%
16 + query-log avg(PMI) 64.60 50.88%
17 + overall match(W) 64.65 50.91%
Table 3: Summary of the model selection process.
to improve lexical matching and translation models.
In the remaining of this section we analyze the per-
formance of the different features.
Table 3 summarizes the outcome of our automatic
greedy feature selection process on the development
set. Where applicable, we show within parentheses
the text representation for the corresponding feature.
The process is initialized with a single feature that
replicates the baseline model (BM25 applied to the
bag-of-words (W) representation). The algorithm
incrementally adds to the feature set the feature that
provides the highest MRR improvement in the de-
velopment partition. The process stops when no fea-
tures yield any improvement. The table shows that,
while the features selected span all the four feature
groups introduced, the lion?s share is taken by the
translation features: approximately 60% of the MRR
724
W B Bg D Dg W + W + W + B + W + B + Bg
B B + Bg Bg + D D + Dg
FG1 (Similarity) 0 +1.06 -2.01 +0.84 -1.75 +1.06 +1.06 +1.06 +1.06
FG2 (Translation) +4.95 +4.73 +5.06 +4.63 +4.66 +5.80 +6.01 +6.36 +6.36
FG3 (Frequency) +2.24 +2.33 +2.39 +2.27 +2.41 +3.56 +3.56 +3.62 +3.62
Table 4: Contribution of NLP processors. Scores are MRR improvements on the development set.
improvement is achieved by these features. The fre-
quency/density features are responsible for approx-
imately 23% of the improvement. The rest is due
to the query-log correlation features. This indicates
that, even though translation models are the most
useful, it is worth exploring approaches that com-
bine several strategies for answer ranking.
Note that if some features do not appear in Table 3
it does not necessarily mean that they are useless.
In some cases such features are highly correlated
with features previously selected, which already ex-
ploited their signal. For example, most similarity
features (FG1) are correlated. Because BM25(W)
is part of the baseline model, the selection process
chooses another FG1 feature only much later (iter-
ation 9) when the model is significantly changed.
On the other hand, some features do not provide a
useful signal at all. A notable example in this class
is the web-based CCP feature, which was designed
originally for factoid answer validation and does not
adapt well to our problem. Because the length of
non-factoid answers is typically significantly larger
than in the factoid QA task, we have to discard a
large part of the query when computing hits(Q+A)
to reach non-zero counts. This means that the final
hit counts, hence the CCP value, are generally un-
correlated with the original (Q,A) tuple.
One interesting observation is that the first two
features chosen by our model selection process use
information from the NLP processors. The first cho-
sen feature is the translation probability computed
between the Bg question and answer representations
(bigrams with words generalized to their WNSS
tags). The second feature selected measures the
number of syntactic dependencies from the question
that are matched in the answer. These results pro-
vide empirical evidence that coarse semantic disam-
biguation and syntactic parsing have a positive con-
tribution to non-factoid QA, even in broad-coverage
noisy settings based on Web data.
The above observation deserves a more detailed
analysis. Table 4 shows the performance of our first
three feature groups when they are applied to each
of the five text representations or incremental com-
binations of representations. For each model cor-
responding to a table cell we use only the features
from the corresponding feature group and represen-
tation to avoid the correlation with features from
other groups. We generate each best model using
the same feature selection process described above.
The left part of Table 4 shows that, generally, the
models using representations that include the output
of our NLP processors (Bg, D and Dg) improve over
the baseline (FG1 and W).11 However, comparable
improvements can be obtained with the simpler bi-
gram representation (B). This indicates that, in terms
of individual contributions, our NLP processors can
be approximated with simpler n-gram models in this
task. Hence, is it fair to say that syntactic and se-
mantic analysis is useful for such Web QA tasks?
While the above analysis seems to suggest a neg-
ative answer, the right-hand side of Table 4 tells a
more interesting story. It shows that the NLP anal-
ysis provides complementary information to the n-
gram-based models. The best models for the FG2
and FG3 feature groups are obtained when combin-
ing the n-gram representations with the representa-
tions that use the output of the NLP processors (W +
B + Bg + D). The improvements are relatively small,
but remarkable (e.g., see FG2) if we take into ac-
count the significant scale of the evaluation. This
observation correlates well with the analysis shown
in Table 3, which shows that features using semantic
(Bg) and syntactic (D) representations contribute the
most on top of the IR model (BM25(W)).
11The exception to this rule are the models FG1(Bg) and
FG1(Dg). This is caused by the fact that the BM25 formula
is less forgiving with errors of the NLP processors (due to the
high idf scores assigned to bigrams and dependencies), and the
WNSS tagger is the least robust component in our pipeline.
725
5 Related Work
Content from community-built question-answer
sites can be retrieved by searching for similar ques-
tions already answered (Jeon et al, 2005) and
ranked using meta-data information like answerer
authority (Jeon et al, 2006; Agichtein et al, 2008).
Here we show that the answer text can be success-
fully used to improve answer ranking quality. Our
method is complementary to the above approaches.
In fact, it is likely that an optimal retrieval engine
from social media should combine all these three
methodologies. Moreover, our approach might have
applications outside of social media (e.g., for open-
domain web-based QA), because the ranking model
built is based only on open-domain knowledge and
the analysis of textual content.
In the QA literature, answer ranking for non-
factoid questions has typically been performed by
learning question-to-answer transformations, either
using translation models (Berger et al, 2000; Sori-
cut and Brill, 2006) or by exploiting the redundancy
of the Web (Agichtein et al, 2001). Girju (2003) ex-
tracts non-factoid answers by searching for certain
semantic structures, e.g., causation relations as an-
swers to causation questions. In this paper we com-
bine several methodologies, including the above,
into a single model. This approach allowed us to per-
form a systematic feature analysis on a large-scale
real-world corpus and a comprehensive feature set.
Recent work has showed that structured retrieval
improves answer ranking for factoid questions:
Bilotti et al (2007) showed that matching predicate-
argument frames constructed from the question and
the expected answer types improves answer ranking.
Cui et al (2005) learned transformations of depen-
dency paths from questions to answers to improve
passage ranking. However, both approaches use
similarity models at their core because they require
the matching of the lexical elements in the search
structures. On the other hand, our approach al-
lows the learning of full transformations from ques-
tion structures to answer structures using translation
models applied to different text representations.
Our answer ranking framework is closest in spirit
to the system of Ko et al (2007) or Higashinaka et
al. (2008). However, the former was applied only
to factoid QA and both are limited to similarity, re-
dundancy and gazetteer-based features. Our model
uses a larger feature set that includes correlation and
transformation-based features and five different con-
tent representations. Our evaluation is also carried
out on a larger scale. Our work is also related to that
of Riezler et al (2007) where SMT-based query ex-
pansion methods are used on data from FAQ pages.
6 Conclusions
In this work we described an answer ranking en-
gine for non-factoid questions built using a large
community-generated question-answer collection.
On one hand, this study shows that we can effec-
tively exploit large amounts of available Web data
to do research on NLP for non-factoid QA systems,
without any annotation or evaluation cost. This pro-
vides an excellent framework for large-scale experi-
mentation with various models that otherwise might
be hard to understand or evaluate. On the other hand,
we expect the outcome of this process to help sev-
eral applications, such as open-domain QA on the
Web and retrieval from social media. For example,
on the Web our ranking system could be combined
with a passage retrieval system to form a QA system
for complex questions. On social media, our system
should be combined with a component that searches
for similar questions already answered; this output
can possibly be filtered further by a content-quality
module that explores ?social? features such as the
authority of users, etc.
We show that the best ranking performance
is obtained when several strategies are combined
into a single model. We obtain the best results
when similarity models are aggregated with features
that model question-to-answer transformations, fre-
quency and density of content, and correlation of
QA pairs with external collections. While the fea-
tures that model question-to-answer transformations
provide most benefits, we show that the combination
is crucial for improvement.
Lastly, we show that syntactic dependency pars-
ing and coarse semantic disambiguation yield a
small, yet statistically significant performance in-
crease on top of the traditional bag-of-words and
n-gram representation. We obtain these results us-
ing only off-the-shelf NLP processors that were not
adapted in any way for our task.
726
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev and
M. Ciaramita. 2007. Multilingual Dependency Pars-
ing and Domain Adaptation using DeSR. Proc. of
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G.
Mishne. 2008. Finding High-Quality Content in So-
cial Media, with an Application to Community-based
Question Answering. Proc. of WSDM.
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning Search Engine Specific Query Transforma-
tions for Question Answering. Proc. of WWW.
A. Berger, R. Caruana, D. Cohn, D. Freytag, and V. Mit-
tal. 2000. Bridging the Lexical Chasm: Statistical
Approaches to Answer Finding. Proc. of SIGIR.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured Retrieval for Question Answering. Proc. of
SIGIR.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001.
Data-Intensive Question Answering. Proc. of TREC.
P. Brown, S. Della Pietra, V. Della Pietra, R. Mercer.
1993. The Mathematics of Statistical Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 19(2).
M. Ciaramita and Y. Altun. 2006. Broad Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. Proc. of EMNLP.
M. Ciaramita, V. Murdock and V. Plachouras. 2008. Se-
mantic Associations for Contextual Advertising. 2008.
Journal of Electronic Commerce Research - Special Is-
sue on Online Advertising and Sponsored Search, 9(1),
pp.1-15.
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proc. of NIPS 2001.
H. Cui, R. Sun, K. Li, M. Kan, and T. Chua. 2005. Ques-
tion Answering Passage Retrieval Using Dependency
Relations. Proc. of SIGIR.
M. Dredze, J. Blitzer, P. Pratim Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly Hard
Domain Adaptation for Parsing. In Proc. of EMNLP-
CoNLL 2007 Shared Task.
A. Echihabi and D. Marcu. 2003. A Noisy-Channel Ap-
proach to Question Answering. Proc. of ACL.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37, pp. 277-296.
R. Girju. 2003. Automatic Detection of Causal Relations
for Question Answering. Proc. of ACL, Workshop on
Multilingual Summarization and Question Answering.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P.
Morarescu. 2000. Falcon: Boosting Knowledge for
Answer Engines. Proc. of TREC.
R. Higashinaka and H. Isozaki. 2008. Corpus-based
Question Answering for why-Questions. Proc. of IJC-
NLP.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding Simi-
lar Questions in Large Question and Answer Archives.
Proc. of CIKM.
J. Jeon, W. B. Croft, J. H. Lee, and S. Park. 2006. A
Framework to Predict the Quality of Answers with
Non-Textual Features. Proc. of SIGIR.
J. Ko, T. Mitamura, and E. Nyberg. 2007. Language-
independent Probabilistic Answer Ranking. for Ques-
tion Answering. Proc. of ACL.
X. Li and D. Roth. 2005. Learning Question Classifiers:
The Role of Semantic Information. Natural Language
Engineering.
B. Magnini, M. Negri, R. Prevete, and H. Tanev. 2002.
Comparing Statistical and Content-Based Techniques
for Answer Validation on the Web. Proc. of the VIII
Convegno AI*IA.
M.P. Marcus, B. Santorini and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn TreeBank. Computational Linguis-
tics, 19(2), pp. 313-330.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea, R.
Goodrum, R. Girju, and V. Rus. 1999. LASSO - A
Tool for Surfing the Answer Net. Proc. of TREC.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Semantic
Kernels for Question/Answer Classification. Proc. of
ACL.
S. Robertson and S. Walker. 1997. On relevance Weights
with Little Relevance Information. Proc. of SIGIR.
R. Soricut and E. Brill. 2006. Automatic Question An-
swering Using the Web: Beyond the Factoid. Journal
of Information Retrieval - Special Issue on Web Infor-
mation Retrieval, 9(2).
L. Shen and A. Joshi. 2005. Ranking and Reranking
with Perceptron, Machine Learning. Special Issue on
Learning in Speech and Language Technologies, 60(1-
3), pp. 73-96.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal
and Y. Liu. 2007. Statistical Machine Translation
for Query Expansion in Answer Retrieval. In Proc.
of ACL.
727
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 406?409,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT-Yahoo: SuperSenseLearner: Combining SenseLearner with
SuperSense and other Coarse Semantic Features
Rada Mihalcea and Andras Csomai
University of North Texas
rada@cs.unt.edu,csomaia@unt.edu
Massimiliano Ciaramita
Yahoo! Research Barcelona
massi@yahoo-inc.com
Abstract
We describe the SUPERSENSELEARNER
system that participated in the English all-
words disambiguation task. The system re-
lies on automatically-learned semantic mod-
els using collocational features coupled with
features extracted from the annotations of
coarse-grained semantic categories gener-
ated by an HMM tagger.
1 Introduction
The task of word sense disambiguation consists of
assigning the most appropriate meaning to a poly-
semous word within a given context. Applications
such as machine translation, knowledge acquisition,
common sense reasoning, and others, require knowl-
edge about word meanings, and word sense disam-
biguation is considered essential for all these tasks.
Most of the efforts in solving this problem
were concentrated so far toward targeted supervised
learning, where each sense tagged occurrence of a
particular word is transformed into a feature vector,
which is then used in an automatic learning process.
The applicability of such supervised algorithms is
however limited only to those few words for which
sense tagged data is available, and their accuracy
is strongly connected to the amount of labeled data
available at hand.
Instead, methods that address all words in unre-
stricted text have received significantly less atten-
tion. While the performance of such methods is usu-
ally exceeded by their supervised lexical-sample al-
ternatives, they have however the advantage of pro-
viding larger coverage.
In this paper, we describe SUPERSENSE-
LEARNER ? a system for solving the semantic am-
biguity of all words in unrestricted text. SUPER-
SENSELEARNER brings together under one system
the features previously used in the SENSELEARNER
(Mihalcea and Csomai, 2005) and the SUPERSENSE
(Ciaramita and Altun, 2006) all-words word sense
disambiguation systems. The system is using a rel-
atively small pre-existing sense-annotated data set
for training purposes, and it learns global semantic
models for general word categories.
2 Learning for All-Words Word Sense
Disambiguation
Our goal is to use as little annotated data as possi-
ble, and at the same time make the algorithm gen-
eral enough to be able to disambiguate as many
content words as possible in a text, and efficient
enough so that large amounts of text can be anno-
tated in real time. SUPERSENSELEARNER is at-
tempting to learn general semantic models for var-
ious word categories, starting with a relatively small
sense-annotated corpus. We base our experiments
on SemCor (Miller et al, 1993), a balanced, se-
mantically annotated dataset, with all content words
manually tagged by trained lexicographers.
The input to the disambiguation algorithm con-
sists of raw text. The output is a text with word
meaning annotations for all open-class words.
The algorithm starts with a preprocessing stage,
where the text is tokenized and annotated with part-
406
of-speech tags; collocations are identified using a
sliding window approach, where a collocation is de-
fined as a sequence of words that forms a compound
concept defined in WordNet (Miller, 1995).
Next, a semantic model is learned for all pre-
defined word categories, where a word category is
defined as a group of words that share some com-
mon syntactic or semantic properties. Word cate-
gories can be of various granularities. For instance,
a model can be defined and trained to handle all the
nouns in the test corpus. Similarly, using the same
mechanism, a finer-grained model can be defined to
handle all the verbs for which at least one of the
meanings is of type e.g., ?<move>?. Finally, small
coverage models that address one word at a time, for
example a model for the adjective ?small,? can be
also defined within the same framework. Once de-
fined and trained, the models are used to annotate the
ambiguous words in the test corpus with their corre-
sponding meaning. Sections 3 and 4 below provide
details on the features implemented by the various
models.
Note that the semantic models are applicable only
to: (1) words that are covered by the word category
defined in the models; and (2) words that appeared
at least once in the training corpus. The words that
are not covered by these models (typically about 10-
15% of the words in the test corpus) are assigned the
most frequent sense in WordNet.
3 SenseLearner Semantic Models
Different semantic models can be defined and
trained for the disambiguation of different word cat-
egories. Although more general than models that
are built individually for each word in a test corpus
(Decadt et al, 2004), the applicability of the seman-
tic models built as part of SENSELEARNER is still
limited to those words previously seen in the train-
ing corpus, and therefore their overall coverage is
not 100%.
Starting with an annotated corpus consisting of
all the annotated files in SemCor, augmented with
the SENSEVAL-2 and SENSEVAL-3 all-words data
sets, a separate training data set is built for each
model. There are seven models provided with the
current SENSELEARNER distribution, implementing
the following features:
3.1 Noun Models
modelNN1: A contextual model that relies on the
first noun, verb, or adjective before the target noun,
and their corresponding part-of-speech tags.
modelNNColl: A collocation model that imple-
ments collocation-like features based on the first
word to the left and the first word to the right of the
target noun.
3.2 Verb Models
modelVB1 A contextual model that relies on the
first word before and the first word after the target
verb, and their part-of-speech tags.
modelVBColl A collocation model that implements
collocation-like features based on the first word to
the left and the first word to the right of the target
verb.
3.3 Adjective Models
modelJJ1 A contextual model that relies on the first
noun after the target adjective.
modelJJ2 A contextual model that relies on the first
word before and the first word after the target adjec-
tive, and their part-of-speech tags.
modelJJColl A collocation model that implements
collocation-like features using the first word to the
left and the first word to the right of the target adjec-
tive.
Based on previous performance in the
SENSEVAL-2 and SENSEVAL-3 evaluations,
we selected the noun and verb collocational models
for inclusion in the SUPERSENSELEARNER system
participating in the SEMEVAL all-words task.
4 SuperSenses and other Coarse-Grained
Semantic Features
A great deal of work has focused in recent years
on shallow semantic annotation tasks such as named
entity recognition and semantic role labeling. In the
former task, systems analyze text to detect mentions
of instances of coarse-grained semantic categories
such as ?person?, ?organization? and ?location?. It
seems natural to ask if this type of shallow seman-
tic information can be leveraged to improve lexical
disambiguation. Particularly, since the best perform-
ing taggers typically implement sequential decoding
schemes, e.g., Viterbi decoding, which have linear
407
complexity and can be performed quite efficiently.
In practice thus, this type of pre-processing resem-
bles POS-tagging and could provide the WSD sys-
tem with useful additional evidence.
4.1 Tagsets
We use three different tagsets. The first is the set of
WordNet supersenses (Ciaramita and Altun, 2006):
a mapping of WordNet?s synsets to 45 broad lexi-
cographers categories, 26 for nouns, 15 for verbs,
3 for adjectives and 1 for adverbs. The second
tagset is based on the ACE 2007 English data for
entity mention detection (EMD) (ACE, 2007). This
tagset defines seven entity types: Facility, Geo-
Political Entity, Location, Organization, Person, Ve-
hicle, Weapon; further subdivided in 44 subtypes.
The third tagset is derived from the BBN Entity
Corpus (BBN, 2005) which complements the Wall
Street Journal Penn Treebank with annotations of a
large set of entities: 12 named entity types (Person,
Facility, Organization, GPE, Location, Nationality,
Product, Event, Work of Art, Law, Language, and
Contact-Info), nine nominal entity types (Person,
Facility, Organization, GPE, Product, Plant, Animal,
Substance, Disease and Game), and seven numeric
types (Date, Time, Percent, Money, Quantity, Ordi-
nal and Cardinal). Several of these types are further
divided into subtypes, for a total of 105 classes.1
4.2 Taggers
We annotate the training and evaluation data using
three sequential taggers, one for each tagset. The
tagger is a Hidden Markov Model trained with the
perceptron algorithm introduced in (Collins, 2002),
which applies Viterbi decoding and is regularized
using averaging. Label to label dependencies are
limited to the previous tag (first order HMM). We
use a generic feature set for NER based on words,
lemmas, POS tags, and word shape features, in addi-
tion we use as a feature of each token the supersense
of a first (super)sense baseline. A detailed descrip-
tion of the features used and the tagger can be found
in (Ciaramita and Altun, 2006). The supersense tag-
ger is trained on the Brown sections one and two of
SemCor. The BBN tagger is trained on sections 2-
21 of the BBN corpus. The ACE tagger is trained
1BBN Corpus documentation.
on the 599 ACE 2007 training files. The accuracy
of the tagger is, approximately, 78% F-score for su-
persenses and ACE, and 87% F-score for the BBN
corpus.
4.3 Features
The taggers disregard the lemmatization of the eval-
uation data. In practice, this means that multiword
lemmas such as ?take off?, are split into their ba-
sic components. In fact, the goal of the tagger is
to guess the elements of the instances of semantic
categories by means of the usual BIO encoding. In
other words, the tagger predicts a labeled bracket-
ing of the tokens in each sentence. As an exam-
ple, the supersense tagger annotates the tokens in the
phrase ?substance abuse? as ?substanceB?noun.act?
and ?abuseI?noun.act?, although the gold standard
segmentation of the data does not identify the phrase
as one lemma. We use the labels generated in this
way as features of each token to disambiguate.
5 Feature Combination
For the final system we create a combined feature set
for each target word, consisting of the lemma, the
part of speech, the collocational SENSELEARNER
features, and the three coarse grained semantic tags
of the target word. Note that the semantic fea-
tures are represented as lemma TAG to avoid over-
generalization.
In the training stage, a feature vector is con-
structed for each sense-annotated word covered by
a semantic model. The features are model-specific,
and feature vectors are added to the training set
pertaining to the corresponding model. The label
of each such feature vector consists of the target
word and the corresponding sense, represented as
word#sense. Table 1 shows the number of feature
vectors constructed in this learning stage for each
semantic model. To annotate new text, similar vec-
tors are created for all the content-words in the raw
text. Similar to the training stage, feature vectors
are created and stored separately for each semantic
model.
Next, word sense predictions are made for all the
test examples, with a separate learning process run
for each semantic model. For learning, we are using
the Timbl memory based learning algorithm (Daele-
408
Training RESULTS
mode size Precision Recall
noun 89052 0.658 0.228
verb 48936 0.539 0.353
all 137988 0.583 0.583
Table 1: Precision and recall for the SUPERSENSE-
LEARNER semantic models.
Training RESULTS
mode size Precision Recall
noun 89052 0.666 0.233
verb 48936 0.554 0.360
all 137988 0.593 0.593
Table 2: Precision and recall for the SUPERSENSE-
LEARNER semantic models - without U labels.
mans et al, 2001), which was previously found use-
ful for the task of word sense disambiguation (Hoste
et al, 2002; Mihalcea, 2002).
Following the learning stage, each vector in the
test data set is labeled with a predicted word and
sense. If the word predicted by the learning algo-
rithm coincides with the target word in the test fea-
ture vector, then the predicted sense is used to an-
notate the test instance. Otherwise, if the predicted
word is different from the target word, no annota-
tion is produced, and the word is left for annotation
in a later stage (e.g., using the most frequent sense
back-off method).
6 Results
The SUPERSENSELEARNER system participated in
the SEMEVAL all-words word sense disambigua-
tion task. Table 1 shows the results obtained for
each part-of-speech (nouns and verbs), as well as
the overall results. We have also ran a separate
evaluation excluding the U (unknown) tag, which
is shown in Table 2. SUPERSENSELEARNER was
ranked the third among the fourteen participating
systems, proving the validity of the approach.
Acknowledgments
We would like to thank Mihai Surdeanu for provid-
ing a pre-processed version of the ACE data.
References
2007. Automatic content extraction workshop.
http://www.nist.gov/speech/tests/ace/ace07/index.htm.
2005. BBN pronoun coreference and entity type cor-
pus. Linguistic Data Consortium (LDC) catalog num-
ber LDC2005T33.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Philadelphia, July. Association for
Computational Linguistics.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner,
version 4.0, reference guide. Technical report, Univer-
sity of Antwerp.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. Gambl, genetic algorithm optimization
of memory-based wsd. In Senseval-3: Third Interna-
tional Workshop on the Evaluation of Systems for the
Semantic Analysis of Text, Barcelona, Spain, July.
V. Hoste, W. Daelemans, I. Hendrickx, and A. van den
Bosch. 2002. Evaluating the results of a memory-
based word-expert approach to unrestricted word sense
disambiguation. In Proceedings of the ACL Workshop
on ?Word Sense Disambiguatuion: Recent Successes
and Future Directions?, Philadelphia, July.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
R. Mihalcea. 2002. Instance based learning with auto-
matic feature selection applied to Word Sense Disam-
biguation. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), Taipei, Taiwan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993.
A semantic concordance. In Proceedings of the 3rd
DARPA Workshop on Human Language Technology,
Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Commu-
nication of the ACM, 38(11):39?41.
409
Proceedings of the 10th Conference on Parsing Technologies, pages 133?143,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Dependency Parsing with Second-Order Feature Maps and Annotated
Semantic Information
Massimiliano Ciaramita
Yahoo! Research
Ocata 1, S-08003
Barcelona, Spain
massi@yahoo-inc.com
Giuseppe Attardi
Dipartimento di Informatica
Universita` di Pisa
L. B. Pontecorvo 3, I-56127
Pisa, Italy
attardi@di.unipi.it
Abstract
This paper investigates new design options
for the feature space of a dependency parser.
We focus on one of the simplest and most
efficient architectures, based on a determin-
istic shift-reduce algorithm, trained with the
perceptron. By adopting second-order fea-
ture maps, the primal form of the perceptron
produces models with comparable accuracy
to more complex architectures, with no need
for approximations. Further gains in accu-
racy are obtained by designing features for
parsing extracted from semantic annotations
generated by a tagger. We provide experi-
mental evaluations on the Penn Treebank.
1 Introduction
A dependency tree represents a sentence as a labeled
directed graph encoding syntactic and semantic in-
formation. The labels on the arcs can represent ba-
sic grammatical relations such as ?subject? and ?ob-
ject?. Dependency trees capture grammatical struc-
tures that can be useful in several language process-
ing tasks such as information extraction (Culotta &
Sorensen, 2004) and machine translation (Ding &
Palmer, 2005). Dependency treebanks are becoming
available in many languages, and several approaches
to dependency parsing on multiple languages have
been evaluated in the CoNLL 2006 and 2007 shared
tasks (Buchholz & Marsi, 2006; Nivre et al, 2007).
Dependency parsing is simpler than constituency
parsing, since dependency trees do not have extra
non-terminal nodes and there is no need for a gram-
mar to generate them. Approaches to dependency
parsing either generate such trees by considering all
possible spanning trees (McDonald et al, 2005), or
build a single tree by means of shift-reduce parsing
actions (Yamada & Matsumoto, 2003). Determinis-
tic dependency parsers which run in linear time have
also been developed (Nivre & Scholz, 2004; Attardi,
2006). These parsers process the sentence sequen-
tially, hence their efficiency makes them suitable for
processing large amounts of text, as required, for ex-
ample, in information retrieval applications.
Recent work on dependency parsing has high-
lighted the benefits of using rich feature sets
and high-order modeling. Yamada and Mat-
sumoto (2003) showed that learning an SVM model
in the dual space with higher-degree polynomial ker-
nel functions improves significantly the parser?s ac-
curacy. McDonald and Pereira (2006) have shown
that incorporating second order features relating to
adjacent edge pairs improves the accuracy of max-
imum spanning tree parsers (MST). In the SVM-
based approach, if the training data is large, it is not
feasible to train a single model. Rather, Yamada and
Matsumoto (see also (Hall et al, 2006)) partition the
training data in different sets, on the basis of Part-
of-Speech, then train one dual SVM model per set.
While this approach simplifies the learning task it
makes the parser more sensitive to the error rate of
the POS tagger. The second-order MST algorithm
has cubic time complexity. For non-projective lan-
guages the algorithm is NP-hard and McDonald and
Pereira (2006) introduce an approximate algorithm
to handle such cases.
In this paper we extend shift reduce parsing with
second-order feature maps which explicitly repre-
133
sent all feature pairs. Also the augmented fea-
ture sets impose additional computational costs.
However, excellent efficiency/accuracy trade-off is
achieved by using the perceptron algorithm, with-
out the need to resort to approximations, producing
high-accuracy classifiers based on a single model.
We also evaluate a novel set of features for pars-
ing. Recently various forms of shallow semantic
processing have been investigated such as named-
entity recognition (NER), semantic role labeling
(SRL) and relation extraction. Syntactic parsing can
provide useful features for these tasks; e.g., Pun-
yakanok et al (2005) show that full parsing is effec-
tive for semantic role labeling (see also related ap-
proaches evaluated within the CoNNL 2005 shared
task (Carreras et al, 2005)). However, no evidence
has been provided so far that annotated semantic
information can be leveraged for improving parser
performance. We report experiments showing that
adding features extracted by an entity tagger im-
proves the accuracy of a dependency parser.
2 Dependency parsing
A dependency parser takes as input a sentence s and
returns a dependency graph d. Figure 1 shows a de-
pendency tree for the sentence ?Last week CBS Inc.
canceled ?The People Next Door?.?1. Dependencies
are represented as labeled arrows from the head of
the relation to the modifier word; thus, in the exam-
ple, ?Inc.? is the modifier of a dependency labeled
?SUB? (subject) to the main verb, the head, ?can-
celed?.
In statistical syntactic parsing a generator (e.g.,
a PCFG) is used to produce a number of candi-
date trees (Collins, 2000) with associated proba-
bility scores. This approach has been used also
for dependency parsing, generating spanning trees
as candidates and computing the maximum span-
ning tree (MST) using discriminative learning algo-
rithms (McDonald et al, 2005). Second-order MST
dependency parsers currently represent the state of
the art in terms of accuracy. Yamada and Mat-
sumoto (2003) proposed a deterministic classifier-
based parser. Instead of learning directly which
tree to assign to a sentence, the parser learns which
1The figure also contains entity annotations which will be
explained below in Section 4.1.
Shift/Reduce actions to use in building the tree. Pars-
ing is cast as a classification problem: at each step
the parser applies a classifier to the features rep-
resenting its current state to predict which action
to perform on the tree. Similar deterministic ap-
proaches to parsing have been investigated also in
the context of constituent parsing (Wong & Wu,
1999; Kalt, 2004).
Nivre and Scholz (2004) proposed a variant of the
model of Yamada and Matsumoto that reduces the
complexity, from the worst case quadratic to linear.
Attardi (2006) proposed a variant of the rules that
handle non-projective relations while parsing deter-
ministically in a single pass. Shift-reduce algorithms
are simple and efficient, yet competitive in terms
of accuracy: in the CoNLL-X shared task, for sev-
eral languages, there was no statistically significant
difference between second-order MST parsers and
shift-reduce parsers.
3 A shift-reduce parser
We build upon DeSR, the shift-reduce parser de-
scribed in (Attardi, 2006). This and Nivre and
Scholz?s (2004) provide among the simplest and
most efficient methods. This parser constructs de-
pendency trees by scanning input sentences in a
single left-to-right pass and performing shift/reduce
parsing actions. The parsing algorithm is fully de-
terministic and has linear complexity. The parser?s
behavior can be described as repeatedly selecting
and applying a parsing rule to transform its state,
while advancing through the sentence. Each to-
ken is analyzed once and a decision is made lo-
cally concerning the action to take, that is, without
considering global properties of the tree being built.
Nivre (2004) investigated the issue of (strict) incre-
mentality for this type of parsers; i.e., if at any point
of the analysis the processed input forms one con-
nected structure. Nivre found that strict incremen-
tality is not guaranteed within this parsing frame-
work, although for correctly parsed trees the prop-
erty holds in almost 90% of the cases.
3.1 Parsing algorithm
The state of the parser is represented by a triple
?S, I,A?, where S is the stack, I is the list of input
tokens that remain to be processed and A is the arc
134
Figure 1. A dependency tree from the Penn Treebank, with additional entity annotation from the BBN corpus.
relation for the dependency graph, which consists of
a set of labeled arcs (wi, r, wj), where wi, wj ? W
(the set of tokens), d ? D (the set of dependencies).
Given an input sentence s, the parser is initialized
to ??, s, ??, and terminates at configuration ?s, ?, A?.
There are three parsing schemata:
Shift ?S,n|I,A??n|S,I,A?(1)
Rightr
?s|S,n|I,A?
?S,n|I,A?{(s,r,n)}?(2)
Leftr
?s|S,n|I,A?
?S,s|I,A?{(n,r,s)}?(3)
The Shift rule advances on the input; each Leftr and
Rightr rule creates a link r between the next input
token n and the top token on the stack s. For produc-
ing labeled dependencies the rules Leftr and Rightr
are instantiated several times once for each depen-
dency label.
Additional parsing actions (cf. (Attardi, 2006))
have been introduced for handling non-projective
dependency trees: i.e., trees that cannot be drawn
in the plane without crossing edges. However, they
are not needed in the experiments reported here,
because in the Penn Treebank used in our experi-
ments dependencies are extracted without consider-
ing empty nodes and the resulting trees are all pro-
jective2.
The pseudo code in Algorithm 1 reproduces
schematically the parsing process.
The function getContext() extracts a vector of
features x relative to the structure built up to that
point from the context of the current token, i.e., from
a subset of I , S and A. The step estimateAction()
predicts a parsing action y, given a trained model ?
2Instead, the version of the Penn Treebank used for the
CoNLL 2007 shared task includes also non-projective represen-
tations.
Algorithm 1: DeSR: Dependency Shift Reduce
parser.
input: s = w1, w2, ..., wn
begin
S ? ??
I ? ?w1, w2, ..., wn?
A? ??
while I 6= ?? do
x? getContext(S, I,A)
y ? estimateAction(x, ?)
performAction(y, S, I, A)
end
and x. The final step performAction() updates the
state according to the predicted parsing rule.
3.2 Features
The set of features used in this paper were chosen
with a few simple experiments on the development
data as a variant of a generic model. The only fea-
tures of the tokens used are ?Lemma?, ?Pos? and
?Dep?: ?Lemma? refers to the morphologically sim-
plified form of the token, ?Pos? is the Part-of-Speech
and ?Dep? is the label on a dependency. ?Child?
refers to the child of a node (right or left): up to
two furthest children of a node are considered. Ta-
ble 1 lists which feature is extracted for which to-
ken: negative numbers refer to tokens on the stack,
positive numbers refer to input tokens. As an exam-
ple, POS(-1) is the Part-of-Speech of the token on
the top of the stack, while Lemma(0) is the lemma
of the next token in the input, PosLeftChild(-1) ex-
tracts the Part-of-Speech of the leftmost child of the
token on the top of the stack, etc.
135
TOKEN
FEATURES Stack Input
Lemma -2 -1 0 1 2 3
Pos -2 -1 0 1 2 3
LemmaLeftChild -1 0
PosLeftChild -1 0
DepLeftChild -1 0
LemmaRightChild -1 0
PosRightChild -1 0
DepRightChild -1
LemmaPrev 0
PosSucc -1
Table 1. Configuration of the feature parameters used in
the experiments.
3.3 Learning a parsing model with the
perceptron
The problem of learning a parsing model can be
framed as a classification task where each class
yi ? Y represents one of k possible parsing actions.
Each of such actions is associated with a weight vec-
tor ?k ? IR
d. Given a datapoint x ? X , a d-
dimensional vector of binary features in the input
space X , a parsing action is chosen with a winner-
take-all discriminant function:
estimateAction(x, ?) = argmax
k
f(x, ?k) (4)
when using a linear classifier, such as the perceptron
or SVM, f(u,v) = ?u,v? is the inner product be-
tween vectors u and v.
We learn the parameters ? from the training data
with the perceptron (Rosemblatt, 1958), in the on-
line multiclass formulation of the algorithm (Cram-
mer & Singer, 2003) with uniform negative updates.
The perceptron has been used in previous work on
dependency parsing by Carreras et al (2006), with
a parser based on Eisner?s algorithm (Eisner, 2000),
and also on incremental constituent parsing (Collins
& Roark, 2006). Also the MST parser of McDonald
uses a variant of the perceptron algorithm (McDon-
ald, 2006). The choice is motivated by the simplicity
and performance of perceptrons, which have proved
competitive on a number of tasks; e.g., in shallow
parsing, where perceptron?s performance is com-
parable to that of Conditional Random Field mod-
els (Sha & Pereira, 2003).
The only adjustable parameter of the model is the
number of instances T to use for training. We fixed
T using the development portion of the data. In
our experiments, the best value is between 20 and
30 times the size of the training data. To regularize
the model we take as the final model the average of
all weight vectors posited during training (Collins,
2002). Algorithm 2 illustrates the perceptron learn-
ing procedure. The final average model can be com-
puted efficiently during training without storing the
individual ? vectors (e.g., see (Ciaramita & Johnson,
2003)).
Algorithm 2: Average multiclass perceptron
input : S = (xi, yi)N ;?0k = ~0, ?k ? Y
for t = 1 to T do
choose j
Et = {r ? Y : ?xj , ?tr? ? ?xj , ?
t
yj ?}
if |Et| > 0 then
?t+1r = ?
t
r ?
xj
|Et| , ?r ? E
t
?t+1yj = ?
t
yj + xj
output: ?k = 1T
?
t ?
t
k, ?k ? Y
3.4 Higher-order feature spaces
Yamada and Matsumoto (2003) and McDonald and
Pereira (2006) have shown that higher-order fea-
ture representations and modeling can improve pars-
ing accuracy, although at significant computational
costs. To make SVM training feasible in the dual
model with polynomial kernels, Yamada and Mat-
sumoto split the training data into several sets, based
on POS tags, and train a parsing model for each
set. McDonald and Pereira?s second-order MST
parser has O(n3) complexity, while for handling
non-projective trees, otherwise an NP-hard problem,
the parser resorts to an approximate algorithm. Here
we discuss how the feature representation can be
enriched to improve parsing while maintaining the
simplicity of the shift-reduce architecture, and per-
forming discriminative learning without partitioning
the training data.
The linear classifier (see Equation 4) learned with
the perceptron is inherently limited in the types of
solutions it can learn. As originally pointed out by
Minsky and Papert (1969), there are problems which
require non-linear solutions that cannot be learned
by such models. A simple workaround this limi-
tation relies on feature maps ? : IRd ? IRh that
136
map the input vectors x ? X into some higher h-
dimensional representation ?(X ) ? IRh, the fea-
ture space. The feature space can represent, for ex-
ample, all combinations of individual features in the
input space. We define a feature map which ex-
tracts all second order features of the form xixj ;
i.e., ?(x) = (xi, xj |i = 1, ..., d, j = i, ..., d). The
linear perceptron working in ?(X ) effectively im-
plements a non-linear classifier in the original in-
put space X . One shortcoming of this approach is
that it inflates considerably the feature representa-
tion and might not scale. In general, the number of
features of degree g over an input space of dimen-
sion d is
(d+g?1
g
)
. In practice, a second-order fea-
ture map can be handled with reasonable efficiency
by the perceptron. We call this the 2nd-order model,
which uses a modified scoring function:
g(x, ?k) = f(?(x), ?k) (5)
where also ?k is h-dimensional. The proposed fea-
ture map is equivalent to a polynomial kernel func-
tion of degree two. Yamada and Matsumoto (2003)
have shown that the degree two polynomial ker-
nel has superior accuracy than the linear model and
polynomial kernels of higher degrees. However, us-
ing the dual model is not always practical for depen-
dency parsing. The discriminant function of the dual
model is defined as:
f ?(x, ?) = argmax
k
N?
i=1
?k,i?x,xi?
g (6)
where the weights ? are associated with class-
instance pairs rather than class-feature pairs. With
respect to the discriminant function of equation (4)
there is an additional summation. In principle, the
inner products can be cached in a Kernel matrix to
speed up training.
There are two shortcomings to using such a model
in dependency parsing. First, if the amount of train-
ing data is large it might not be feasible to store the
Kernel matrix; which for a dataset of sizeN requires
O(N3) computations and O(N2) space. As an ex-
ample, the number of training instances N in the
Penn Treebank is over 1.8 million, caching the Ker-
nel matrix would require several Terabytes of space.
The second shortcoming is independent of training.
In predicting a tree for unseen sentences the model
will have to recompute the inner products between
the observation and all the support vectors; i.e., all
class-instance pairs with ?k,i > 0. The second-order
feature map with the perceptron is more efficient and
allows faster training and prediction. Training a sin-
gle parsing model avoids a potential loss of accuracy
that occurs when using the technique of partitioning
the training data according to the POS. Inaccurate
predictions of the POS can affect significantly the
accuracy of the actions predicted, while the single
model is more robust, since the POS is just one of
the many features used in prediction.
4 Semantic features
Semantic information is used implicitly in parsing.
For example, conditioning on lexical heads pro-
vides a source of semantic information. There have
been a few attempts at using semantic information
more explicitly. Charniak?s 1997 parser (1997), de-
fined probability estimates backed off to word clus-
ters. Collins and Koo (Collins & Koo, 2005) in-
troduced an improved reranking model for parsing
which includes a hidden layer of semantic features.
Yi and Palmer (2005) retrained a constituent parser
in which phrases were annotated with argument in-
formation to improve SRL, however this didn?t im-
prove over the output of the basic parser.
In recent years there has been a significant
amount of work on semantic annotation tasks such
as named-entity recognition, semantic role labeling
and relation extraction. There is evidence that de-
pendency and constituent parsing can be helpful in
these and other tasks; e.g., by means of tree ker-
nels in question classification and semantic role la-
beling (Zhang & Lee, 2003; Moschitti, 2006).
It is natural to ask if also the opposite holds:
whether semantic annotations can be used to im-
prove parsing. In particular, it would be interesting
to know if entity-like tags can be used for this pur-
pose. One reason for this is that entity tagging is ef-
ficient and does not seem to need parsing for achiev-
ing top performance. Beyond improving traditional
parsing, independently learned semantic tags might
be helpful in adapting a parser to a new domain. To
the best of our knowledge, no evidence has been pro-
duced yet that annotated semantic information can
improve parsing. In the following we investigate
137
adding entity tags as features of our parser.
4.1 BBN Entity corpus
The BBN corpus (BBN, 2005) supplements the Wall
Street Journal Penn Treebank with annotation of a
large set of entity types. The corpus includes an-
notation of 12 named entity types (Person, Facility,
Organization, GPE, Location, Nationality, Product,
Event, Work of Art, Law, Language, and Contact-
Info), nine nominal entity types (Person, Facility,
Organization, GPE, Product, Plant, Animal, Sub-
stance, Disease and Game), and seven numeric types
(Date, Time, Percent, Money, Quantity, Ordinal and
Cardinal). Several of these types are further divided
into subtypes3. This corpus provides adequate sup-
port for experimenting semantic features for parsing.
Figure 1 illustrates the annotation layer provided
by the BBN corpus4. It is interesting to notice one
apparent property of the combination of semantic
tags and dependencies. When we consider segments
composed of several words there is exactly one de-
pendency connecting a token outside the segment
with a token inside the segment; e.g., ?CBS Inc.? is
connected outside only through the token ?Inc.?, the
subject of the main verb. With respect to the rest of
the tree, segments tend to form units, with their own
internal structure. Intuitively, this information seems
relevant for parsing. This locally-structured patterns
could help particularly simple algorithms like ours,
which have limited knowledge of the global struc-
ture being built.
Table 2 lists the 40 most frequent categories in
sections 2 to 21 of the BBN corpus, and the per-
centage of all entities they represent ? together more
than 97%. Sections 2-21 are comprised of 949,853
tokens, 23.5% of the tokens have a non-null BBN
entity tag, on average there is one tagged token every
four. The total number of entities is 139,029, 70.5%
of which are named entities and nominal concepts,
17% are numerical types and the remaining 12.5%
describe time entities.
We designed three new features which extract
simple properties of entities from the semantic an-
notation information:
3BBN Corpus documentation.
4The full label for ?ORG? is ?ORG:Corporation?, and
?WOA? stands for ?WorkOfArt:Other?.
TOKEN
FEATURES Stack Input
AS-0 = EOS+BIO+TAG 0
AS-1 = EOS+BIO+TAG -1 0 1
AS-2 = EOS+BIO+TAG -2 -1 0 1 2
EOS -2 -1 0 1 2
BIO -2 -1 0 1 2
TAG -2 -1 0 1 2
Table 3. Additional configurations for the models with
BBN entity features.
? EOS: Distance to the end of the segment; e.g.,
EOS(?Last?) = 1, EOS(?canceled?) = 0;
? BIO: The first character of the BBN label
for a token; e.g., BIO(?CBS?) = ?B?, and
BIO(?canceled?) = 0;
? TAG: Full BBN tag for the token; e.g.,
TAG(?CBS?) = ?B-ORG:Corporation?,
TAG(?week?) = ?I-DATE?.
The feature EOS provides information about the rel-
ative position of the token within a segment with re-
spect to the end of the segment. The feature BIO dis-
criminates tokens with no semantic annotation as-
sociated, from tokens within a segment and token
which start a segment. Finally the feature TAG iden-
tifies the full semantic tag associated with the token.
With respect to the former two features this bears
the most fine-grained semantics. Table 3 summa-
rizes six additional models we implemented. The
first three use all additional features together, ap-
plied to different sets of tokens, while the last three
apply only one feature, on top of the base model,
relative to the next token in the input, the following
two tokens in the input, and the previous two tokens
on the stack.
4.2 Corpus pre-processing
The original BBN corpus has its own tokeniza-
tion which often does not reflect the Penn Tree-
bank tokenization; e.g., when an entity intersects
an hyphenated compound, thus ?third-highest? be-
comes ?thirdORDINAL - highest?. This is problem-
atic for combining entity annotation and dependency
trees. Since our main focus is parsing we re-aligned
the BBN Corpus with the Treebank tokenization.
Thus, for example, when an entity splits a Tree-
bank token we extend the entity boundary to contain
138
WSJ-BBN Corpus Categories
Tag % Tag % Tag % Tag %
PER DESC 15.5 ORG:CORP 13.7 DATE:DATE 9.2 ORG DESC:CORP 8.9
PERSON 8.13 MONEY 6.5 CARDINAL 6.0 PERCENT 3.5
GPE:CITY 3.12 GPE:COUNTRY 2.9 ORG:GOV 2.6 NORP:NATION-TY 1.9
DATE:DURATION 1.8 GPE:PROVINCE 1.5 ORG DESC:GOV 1.4 FAC DESC:BLDG 1.1
ORG:OTHER 0.7 PROD DESC:VEHICLE 0.7 ORG DESC:OTHER 0.6 ORDINAL 0.6
TIME 0.5 GPE DESC:COUNTRY 0.5 SUBST:OTHER 0.5 SUBST:FOOD 0.5
DATE:OTHER 0.4 NORP:POLITICAL 0.4 DATE:AGE 0.4 LOC:REGION 0.3
SUBST:CHEM 0.3 WOA:OTHER 0.3 FAC DESC:OTHER 0.3 SUBST:DRUG 0.3
ANIMAL 0.3 GPE DESC:PROVINCE 0.2 PROD:VEHICLE 0.2 GPE DESC:CITY 0.2
PRODUCT:OTHER 0.2 LAW 0.2 ORG:POLITICAL 0.2 ORG:EDU 0.2
Table 2. The 40 most frequent labels in sections 2 to 21 of the Wall Street Journal BBN Corpus and the percentage of
tags occurrences.
the whole original Treebank token, thus obtaining
?third-highestORDINAL? in the example above.
4.3 Semantic tagger
We treated semantic tags as POS tags. A tagger
was trained on the BBN gold standard annotation
and used it to annotate development and evaluation
data. We briefly describe the tagger (see (Ciaramita
& Altun, 2006) for more details), a Hidden Markov
Model trained with the perceptron algorithm intro-
duced in (Collins, 2002). The tagger uses Viterbi
decoding. Label to label dependencies are limited to
the previous tag (first order HMM). A generic fea-
ture set for NER based on words, lemmas, POS tags,
and word shape features was used.
The tagger is trained on sections 2-21 of the BBN
corpus. As before, section 22 of the BBN corpus
is used for choosing the perceptron?s parameter T .
The tagger?s model is regularized as described for
Algorithm 2. The full BBN tagset is comprised
of 105 classes organized hierarchically, we ignored
the hierarchical organization and treated each tag as
an independent class in the standard BIO encoding.
The tagger evaluated on section 23 achieves an F-
score of 86.8%. The part of speech for the evalua-
tion/development sections was produced with Tree-
Tagger. As a final remark we notice that the tagger?s
complexity, linear in the length of the sentence, pre-
serves the parser?s complexity.
5 Parsing experiments
5.1 Data and setup
We used the standard partitions of the Wall Street
Journal Penn Treebank (Marcus et al, 1993); i.e.,
sections 2-21 for training, section 22 for develop-
ment and section 23 for evaluation. The constituent
trees were transformed into dependency trees by
means of a program created by Joakim Nivre that
implements the rules proposed by Yamada and Mat-
sumoto, which in turn are based on the head rules
of Collins? parser (Collins, 1999)5. The lemma for
each token was produced using the ?morph? func-
tion of the WordNet (Fellbaum, 1998) library6. The
data in the WSJ sections 22 and 23, both for the
parser and for the semantic tagger, was POS-tagged
using TreeTagger7, which has an accuracy of 97.0%
on section 23.
Training a parsing model on the Wall Street Jour-
nal requires a set of 22 classes: 10 of the 11 labels
in the dependency corpus generated from the Penn
Treebank (e.g., subj, obj, sbar, vmod, nmod, root,
etc.) are paired with both a Left and Right actions.
In addition, there is in one rule for the ?root? label
and one for the Shift action. The total number of
features found in training ranges from two hundred
thousand for the 1st-order model to approximately
20 million of the 2nd-order models.
We evaluated several models, each trained with
1st-order and 2nd-order features. The base model
(BASE) only uses the traditional set of features (cf.
Table 1). Models EOS, BIO and TAG each use only
one type of semantic feature with the configuration
described in Table 3. Models AS-0, AS-1, and AS-2
use all three semantic features for the token on the
stack in AS-0, plus the previous token on the stack
and the new token in the input in AS-1, plus an addi-
5The script is available from
http://w3.msi.vxu.se/%7enivre/research/Penn2Malt.html
6http://wordnet.princeton.edu
7TreeTagger is available from http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/
139
1st-order scores 2nd-order scores
DeSR MODEL LAS UAS Imp LAC LAS UAS Imp LAC
BASE 84.01 85.56 - 88.24 89.20 90.55 - 92.22
EOS 84.89 86.37 +5.6 88.94 89.36 90.64 +1.0 92.37
BIO 84.95 86.37 +6.6 89.06 89.63 90.89 +3.6 92.55
TAG 84.76 86.26 +4.8 88.80 89.54 90.81 +2.8 92.55
AS-0 84.40 85.95 +2.7 88.38 89.41 90.72 +1.8 92.38
AS-1 85.13 86.52 +6.6 89.11 89.57 90.77 +2.3 92.49
AS-2 85.32 86.71 +8.0 89.25 89.87 91.10 +5.8 92.68
Table 4. Results of the different models on WSJ section 23 using the CoNLL scores Labeled attachment score (LAS),
Unlabeled attachment score (UAS), and Label accuracy score (LAC). The column labeled ?Imp? reports the improve-
ment in terms of relative error reduction with respect to the BASE model for the UAS score. In bold the best results.
tional token from the stack and an additional token
from the input for AS-2 (cf. Table 3).
5.2 Results of 2nd-order models
Table 4 summarizes the results of all experiments.
We report the following scores, obtained with the
CoNLL-X scoring script: labeled attachment score
(LAS), unlabeled attachment score (UAS) and label
accuracy score (LAC). For the UAS score, the most
frequently reported, we include the improvement in
relative error reduction.
The 2nd-order base model improves on all mea-
sures over the 1st-order model by approximately
5%. The UAS score is 90.55%, with an improve-
ment of 4.9%. The magnitude of the improve-
ment is remarkable and reflects the 4.6% improve-
ment that Yamada and Matsumoto (Yamada & Mat-
sumoto, 2003) report going from the linear SVM to
the polynomial of degree two. Our base model?s ac-
curacy (90.55% UAS) compares well with the ac-
curacy of the parsers based on the polynomial ker-
nel trained with SVM of Yamada and Matsumoto
(UAS 90.3%), and Hall et al (2006) (UAS 89.4%).
We notice in particular that, given the lack of non-
projective cases/rules, the parser of Hall et al (2006)
is almost identical to our parser, hence the differ-
ence in accuracy (+1.1%) might effectively be due
to a better classifier. Yamada & Matsumoto?s parser
is slightly more complex than our parser, and has
quadratic worst-case complexity. Overall, the accu-
racy of the 2nd-order parser is comparable to that of
the 1st-order MST parser (90.7%).
There is no direct evidence that our perceptron
produces better classifiers than SVM. Rather, the
pattern of results produced by the perceptron seems
comparable to that of SVM (Yamada & Matsumoto,
2003). This is a useful finding in itself, given that
the former is more efficient: perceptron?s update is
linear while SVM solves a quadratic problem at each
update. However, one major difference between the
two approaches lies in the fact that learning with the
primal model does not require splitting the model
by Part-of-Speech, or other means. As a conse-
quence, beyond the greater simplicity, our method
might benefit from not depending so strongly on the
quality of POS tagging. POS information is encoded
as a feature and contributes its weight to the selec-
tion of the parsing action, together with all addi-
tionally available information. In the SVM-trained
methods the model that makes the prediction for the
parsing rule is essentially chosen by an oracle, the
prediction of the POS tagger. Furthermore, it might
be argued that learning a single model makes a bet-
ter use of the training data by exploiting the cor-
relations between all datapoints, while in the dual
split-training case the interaction is limited to dat-
apoints in the same partition. In any case, second-
order feature maps could be used also with SVM or
other classifiers. The advantage of using the per-
ceptron lies in the unchallenged accuracy/efficiency
trade-off. Finally, we recall that training in the pri-
mal model can be performed fully on-line without
affecting the resulting model nor the complexity of
the algorithm.
5.3 Results of models with semantic features
All models based on semantic features improve over
the base model on all measures. The best configura-
140
Parser UAS
Hall et al ?06 89.4
Yamada & Matsumoto ?03 90.3
DeSR 90.55
McDonald & Pereira 1st-order MST 90.7
DeSR AS-2 91.1
McDonald & Pereira 2nd-order MST 91.5
Sagae & Lavie ?06 92.7
Table 5. Comparison of main results on the Penn Tree-
bank dataset.
tion is that of model AS-2 which extracts all seman-
tic features from the widest context. In the 1st-order
AS-2 model the improvement, 86.71% UAS (+8%
relative error reduction) is more marked than in the
2nd-order AS-2 model, 91.1% UAS (+5.8% error
reduction). A possible simple exaplanation is that
some information captured by the semantic features
is correlated with other higher-order features which
do not occur in the 1st-order encoding. Overall the
accuracy of the DeSR parser with semantic informa-
tion is slightly inferior to that of the second-order
MST parser (McDonald & Pereira, 2006) (91.5%
UAS). The best result on this dataset to date (92.7%
UAS) is that of Sagae and Lavie (Sagae & Lavie,
2006) who use a parser which combines the predic-
tions of several pre-existing parsers, including Mc-
Donald?s and Nivre?s parsers. Table 5 lists the main
results to date on the version of the Penn Treebank
for dependency parsing task used in this paper.
In Table 4 we also evaluate the gain obtained by
adding one semantic feature type at a time (cf. rows
EOS/BIO/TAG). These results show that all seman-
tic features provide some improvement (with the du-
bious case of EOS in the 2nd-order model). The
BIO encoding seems to produce the most accurate
features. This could be promising because it sug-
gests that the benefit does not depend only on the
specific tags, but that the segmentation in itself is
important. Hence tagging could improve the adapta-
tion of parsers to new domains even if only generic
tagging methods are available.
5.4 Remarks on efficiency
All experiments were performed on a 2.4GHz AMD
Opteron CPU machine with 32GB RAM. The 2nd-
order parser uses almost 3GB of memory. While
Parsing time/sec
Parser English Chinese
MST 2n-order 97.52 59.05
MST 1st-order 76.62 49.13
DeSR 36.90 21.22
Table 6. Parsing times for the CoNNL 2007 English and
Chinese datasets for MST and DeSR.
it is several times slower and larger than the 1st-
order model8 the 2nd-order model performance is
still competitive. It takes 3 minutes (user time) to
parse section 23, POS tagging included. In train-
ing, the model takes about 1 hour to process the full
dataset once. As a comparison, Hall et al (2006)
reports 1.5 hours for training the partitioned SVM
model and 10 minutes for parsing the evaluation set
on the same Penn Treebank data. We also compared
directly the parsing time of our parser with that of
the MST parser using the version 0.4.3 of MST-
Parser9. For these experiments we used two datasets
from the CoNLL 2007 shared task for English and
Chinese. Table 6 reports the times, in seconds, to
parse the test sets for these languages on a 3.3GHz
Xeon machine with 4 GB Ram, of the MST 1st and
2nd-order parser and DeSR parser (without semantic
features).
The architecture of the model presented here of-
fers several options for optimization. For exam-
ple, implementing the ? models with full vectors
rather than hash tables speeds up parsing by a factor
of three, at the expense of memory. Alternatively,
memory load in training can be reduced, at the ex-
pense of time, by using on-line training. However,
the most valuable option for space need reduction
might be to filter out low-frequency second-order
features. Since the frequency of such features seems
to follow a power law distribution, this reduces sig-
nificantly the feature space size even for low thresh-
olds at small accuracy expense. In this paper how-
ever we focused on the full model, no approxima-
tions were required to run the experiments.
8The 1st-order parser takes 7 seconds (user time) to process
Section 23.
9Available from sourceforge.net.
141
6 Conclusion
We explored the design space of a dependency
parser by modeling and extending the feature repre-
sentation, while adopting one of the simplest parsing
architecture: a single-pass deterministic shift-reduce
algorithm trained with a regularized multiclass per-
ceptron. We showed that with the perceptron it is
possible to adopt higher-order feature maps equiva-
lent to polynomial kernels without need of approx-
imating the model (although this remains an option
for optimization). The resulting models achieve ac-
curacies comparable (or better) to more complex ar-
chitectures based on dual SVM training, and faster
parsing on unseen data. With respect to learning, it is
possible that more sophisticated formulations of the
perceptron (e.g. MIRA (Crammer & Singer, 2003))
could provide further gains in accuracy, as shown
with the MST parser (McDonald et al, 2005).
We also experimented with novel types of se-
mantic features, extracted from the annotations pro-
duced by an entity tagger trained on the BBN cor-
pus. This model further improves over the standard
model yielding an additional 5.8% relative error re-
duction. Although the magnitude of the improve-
ment is not striking, to the best of our knowledge
this is the first encouraging evidence that annotated
semantic information can improve parsing and sug-
gests several options for further research. For exam-
ple, this finding might indicate that this type of ap-
proach, which combines semantic tagging and pars-
ing, is viable for the adaptation of parsing to new
domains for which semantic taggers exist. Seman-
tic features could be also easily included in other
types of dependency parsing algorithms, e.g., MST,
and in current methods for constituent parse rerank-
ing (Collins, 2000; Charniak & Johnson, 2005).
For future research several issues concerning the
semantic features could be tackled. We notice that
more complex semantic features can be designed
and evaluated. For example, it might be useful to
guess the ?head? of segments with simple heuris-
tics, i.e., the guess the node which is more likely to
connect the segment with the rest of the tree, which
all internal components of the entity depend upon.
It would be also interesting to extract semantic fea-
tures from taggers trained on different datasets and
based on different tagsets.
Acknowledgments
The first author would like to thank Thomas Hof-
mann for useful inputs concerning the presentation
of the issue of higher-order feature representations
of Section 3.4. We would also like to thank Brian
Roark and the anonymous reviewers for useful com-
ments and pointers to related work.
References
G. Attardi. 2006. Experiments with a Multilanguage
Non-Projective Dependency Parser. In Proceedings of
CoNNL-X 2006.
BBN. 2005. BBN Pronoun Coreference and Entity Type
Corpus. Linguistic Data Consortium (LDC) catalog
number LDC2005T33.
S. Buchholz and E. Marsi. 2006. Introduction to
CoNNL-X Shared Task on Multilingual Dependency
Parsing. In Proceedings of CoNNL-X 2006.
X. Carreras and L. Ma`rquez. 2005. Introduction to the
CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedings of CoNLL 2005.
X. Carreras, M. Surdeanu, and L. Ma`rquez. 2006 Pro-
jective Dependency Parsing with Perceptron. In Pro-
ceedings of CoNLL-X.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intel-
ligence AAAI.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine n-
Best Parsing and MaxEnt Discriminative Reranking.
In Proceedings of ACL 2005.
M. Ciaramita and Y. Altun. 2006. Broad-Coverage Sense
Disambiguation and Information Extraction with a Su-
persense Sequence Tagger. In Proceedings of EMNLP
2006.
M. Ciaramita and M. Johnson. 2003. Supersense Tag-
ging of Unknown Nouns in WordNet. In Proceedings
of EMNLP 2003.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. Thesis, University
of Pennsylvania.
M. Collins. 2000. Discriminative Reranking for Natural
Language Parsing. In Proceedings of ICML 2000.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP 2002.
142
M. Collins and T. Koo. 2005. Hidden-Variable Mod-
els for Discriminative Reranking. In Proceedings of
EMNLP 2005.
M. Collins and B. Roark. 2004. Incremental Parsing
with the Perceptron Algorithm. In Proceedings of ACL
2004.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research 3: pp.951-991.
A. Culotta and J. Sorensen. 2004. Dependency Tree Ker-
nels for Relation Extraction. In Proceedings of ACL
2004.
Y. Ding and M. Palmer. 2005. Machine Translation us-
ing Probabilistic Synchronous Dependency Insertion
Grammars. In Proceedings of ACL 2005.
J. Eisner. 2000. Bilexical Grammars and their Cubic-
Time Parsing Algorithms. In H.C. Bunt and A. Ni-
jholt, eds. New Developments in Natural Language
Parsing, pp. 29-62. Kluwer Academic Publishers.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database MIT Press, Cambridge, MA. 1969.
J. Hall, J. Nivre and J. Nilsson. 2006. Discriminative
Classifiers for Deterministic Dependency Parsing. In
Proceedings of the COLING/ACL 2006.
T. Kalt. 2004. Induction of Greedy Controllers for Deter-
ministic Treebank Parsers. In Proceedings of EMNLP
2004.
M. Marcus, B. Santorini and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The
Penn Treebank. Computational Linguistics, 19(2): pp.
313-330.
R. McDonald. 2006. Discriminative Training and Span-
ning Tree Algorithms for Dependency Parsing. Ph.D.
Thesis, University of Pennsylvania.
R. McDonald, F. Pereira, K. Ribarov and J. Hajic?. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP
2005.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL 2006.
M.L. Minsky and S.A. Papert. 1969. Perceptrons: An
Introduction to Computational Geometry. MIT Press,
Cambridge, MA. 1969.
A. Moschitti. 2006. Efficient Convolution Kernels for
Dependency and Constituent Syntactic Trees. In Pro-
ceedings of ECML 2006.
J. Nivre. 2004. Incrementality in Deterministic Depen-
dency Parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together. Workshop at ACL-
2004.Spain, 50-57.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing, In Proceedings
of EMNLP-CoNLL 2007.
J. Nivre and M. Scholz. 2004. Deterministic Depen-
dency Parsing of English Text. In Proceedings of
COLING 2004.
V. Punyakanok, D. Roth, and W. Yih. 2005. The Neces-
sity of Syntactic Parsing for Semantic Role Labeling.
In Proceedings of IJCAI 2005.
F. Rosemblatt. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psych. Rev., 68: pp. 386-407.
K. Sagae and A. Lavie. 2005. Parser Combination by
Reparsing. In Proceedings of HLT-NAACL 2006.
F. Sha and F. Pereira. 2003. Shallow Parsing with Condi-
tional Random Fields. In Proceedings of HLT-NAACL
2003.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In
Proceedings of the Eighth International Workshop on
Parsing Technologies. Nancy, France.
S. Yi and M. Palmer. 2005. The Integration of Syntactic
Parsing and Semantic Role Labeling. In Proceedings
of CoNLL 2005.
A. Wong and D. Wu. 1999. Learning a Lightweight De-
terministic Parser. In Proceedings of EUROSPEECH
1999.
D. Zhang and W.S. Less. 2003. Question Classification
using Support Vector Machines. In Proceedings of SI-
GIR 2003.
143
Coling 2010: Poster Volume, pages 819?827,
Beijing, August 2010
Instance Sense Induction from Attribute Sets
Ricardo Martin-Brualla
Google Inc
rmbrualla@gmail.com
Enrique Alfonseca
Google Inc
ealfonseca@google.com
Marius Pasca
Google Inc
mars@google.com
Keith Hall
Google Inc
kbhall@google.com
Enrique Robledo-Arnuncio
Google Inc
era@google.com
Massimiliano Ciaramita
Google Inc
massi@google.com
Abstract
This paper investigates the new problem
of automatic sense induction for instance
names using automatically extracted at-
tribute sets. Several clustering strategies
and data sources are described and eval-
uated. We also discuss the drawbacks of
the evaluation metrics commonly used in
similar clustering tasks. The results show
improvements in most metrics with re-
spect to the baselines, especially for pol-
ysemous instances.
1 Introduction
Recent work on information extraction increas-
ingly turns its attention to the automatic acqui-
sition of open-domain information from large
text collections (Etzioni et al, 2008). The ac-
quired information typically includes instances
(e.g. barack obama or hillary clinton), class la-
bels (e.g. politician or presidential candidate)
and relations and attributes of the instances (e.g.
president-country or date-of-birth) (Sekine, 2006;
Banko et al, 2007).
Within the larger area of relation extraction,
the acquisition of instance attributes (e.g. pres-
ident for instances of countries, or side effects
for instances of drugs) plays an important role,
since attributes may serve as building blocks in
any knowledge base constructed around open-
domain classes of instances. Thus, a variety
of attribute extraction methods mine textual data
sources ranging from unstructured (Tokunaga et
al., 2005) or structured (Cafarella et al, 2008) text
within Web documents, to human-compiled ency-
clopedia (Wu et al, 2008; Cui et al, 2009) and
Web search query logs (Pas?ca and Van Durme,
2007), attempting to extract, for a given class, a
ranked list of attributes that is as comprehensive
and accurate as possible.
Previous work on attribute extraction, however,
does not capture or address attributes of polyse-
mous instances. An instance may have differ-
ent meanings, and the extracted attributes may
not apply to all of them. For example, the
most salient meanings of darwin are the scientist
Charles Darwin, an Australian city, and an op-
erating system, plus many less-known meanings.
For these ambiguous instances, it is common for
the existing procedures to extract mixed lists of
attributes that belong to incompatible meanings,
e.g. {biography, population, hotels, books}.
This paper explores the problem of automati-
cally inducing instance senses from the learned
attribute lists, and describes several clustering so-
lutions based on a variety of data sources. For
that, it brings together research on attribute acqui-
sition and on word sense induction. Results show
that we can generate meaninful groupings of at-
tributes for polysemous instance names, while not
harming much the monosemous instance names
by generating unwanted clusters for them. The
results are much better than for a random base-
line, and are superior to the one-in-all and the all-
singleton baselines.
2 Previous Work
Previous work on attribute extraction uses a va-
riety of types of textual data as sources for mining
attributes. Some methods take advantage of struc-
tured and semi-structured text available within
Web documents. Examples of this are the use of
markup information in HTML documents to ex-
819
tract patterns and clues around attributes (Yoshi-
naga and Torisawa, 2007; Wong and Lam, 2009;
Ravi and Pas?ca, 2008), or the use of articles
within online encyclopedia as sources of struc-
tured text for attribute extraction (Suchanek et al,
2007; Nastase and Strube, 2008; Wu and Weld,
2008). Regarding unstructured text in Web docu-
ments, the method described in (Tokunaga et al,
2005) takes various class labels as input, and ap-
plies manually-created lexico-syntactic patterns to
document sentences to extract candidate attributes
ranked using several frequency statistics. In (Bel-
lare et al, 2007), the extraction is guided by a set
of seed instances and attributes rather than hand-
crafted patterns, with the purpose of generating
training data and extract new instance-attribute
pairs from text.
Web search queries have also been used as a
data source for attribute extraction, using lexico-
syntactic patterns (Pas?ca and Van Durme, 2007) or
seed attributes (Pas?ca, 2007) to guide the extrac-
tion, and leading to attributes of higher accuracy
than those extracted with equivalent techniques
from Web documents (Pas?ca et al, 2007).
Another related area to this work is the field of
word sense induction: the task of identifying the
possible senses of a word in a corpus using unsu-
pervised methods (Yarowsky, 1995), as opposed
to traditional disambiguation methods which rely
on the availability of a finite and static list of pos-
sible meanings. In (Agirre and Soroa, 2007) a
framework is proposed for evaluating such sys-
tems. Word sense induction can be naturally for-
mulated as a clustering task. This introduces
the complication of choosing the right number
of possible senses, hence a Bayesian approach to
WSI was proposed which deals with this problem
within a principled generative framework (Brody
and Lapata, 2009). Another related line of work
Turkey Attributes Darwin Attributes
maps1 capital1 maps1 definition1,3
recipes2 culture1 awards2 jobs1
pictures1,2 history1 shoes1 tourism1
calories2 tourism1 evolution3 biography3
facts1,2 nutrition facts2 theory3 attractions1
nutrition2 beaches1 weather1 hotels1
cooking time2 brands2 pictures1,3 ports4
religion1 language1 quotes3 population1
Table 1: Attributes extracted for the instances
Turkey and Darwin.
is the disambiguation of people names (Mann and
Yarowsky, 2003). In SEMEVAL-1, a shared task
was introduced dedicated to this problem, the Web
People Search task (Artiles et al, 2007; Artiles et
al., 2009). Disambiguating names is also often ap-
proached as a clustering problem. One challenge
shared by word sense induction and name disam-
biguation (and most unsupervised settings), is the
evaluation. In both tasks, simple baselines such as
predicting one single cluster tend to outperform
more sophisticated approaches (Agirre and Soroa,
2007; Artiles et al, 2007).
3 Instance Sense Induction
3.1 Problem description
This paper assumes the existence of an attribute
extraction procedure. Using those attributes, our
aim is to identify the coarse-grained meanings
with which each attribute is associated. As an
example, Table 1 shows the top 16 attributes ex-
tracted using the procedure described in (Pas?ca
and Van Durme, 2007). Salient meanings for
turkey are the country name (labeled as 1 in the
table), and the bird name (labeled as 2). Some at-
tributes are applicable to both meanings (pictures
and facts). The second example, darwin, can re-
fer to a city (sense 1), the Darwin Awards (sense
2), the person (sense 3), and an operating system
(sense 4).
Examples of applications that need to dis-
criminate between the several meanings of in-
stances are user-facing applications requiring the
attributes to be organized logically and informa-
tion extraction pipelines that depend on the ex-
tracted attributes to find values in documents.
The problem we are addressing is the automatic
induction of instance senses from the attribute
sets, by grouping together the attributes that can
be applied to a particular sense. As in related work
on sense induction (Agirre and Soroa, 2007; Ar-
tiles et al, 2007), we approach this as a clustering
problem: finding the right similarity metrics and
clustering procedures to identify sets of related at-
tributes in an instance. We propose a clustering
based on the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), exploring differ-
ent parameters, similarity sources, and prior dis-
tributions.
820
3.2 Instance and attributes input data
The input data of instances and attributes has been
obtained, in a fully automated way, following
the method described in (Pas?ca and Van Durme,
2007). The input dataset is a set of fully anony-
mized set of English queries submitted to a popu-
lar (anonymized) search engine. The set contains
millions of unique isolated, individual queries that
are independent from one another. Each query
is accompanied by its frequency of occurrence
in the query logs. The sum of frequencies of
all queries in the dataset is hundreds of millions.
Other sources of similar data are available pub-
licly for research purposes (Gao et al, 2007). This
extraction method applies a few patterns (e.g., the
A of I, or I?s A, or A of I) to queries within
query logs, where an instance I is one of the most
frequent 5 million queries from the repository of
isolated queries, and A is a candidate attribute.
For each instance, the method extracts ranked lists
containing zero, one or more attributes, along with
frequency-based scores. For this work, only the
top 32 attributes of each instance were used, in or-
der to have an input set for the clustering with a
reasonable size, but to keep precision at high lev-
els.
3.3 Per-attribute clustering information
For each (instance, attribute) pair, the following
information is collected:
Search results: The top 20 search results (in-
cluding titles and snippets) returned by a popular
search engine for a query created by concatenat-
ing the instance and the attribute. The motivation
for this data source is that the attributes that re-
fer to the same meaning of the instance should
help the search engine in selecting web pages that
refer to that meaning. The titles and snippets of
these search results are expected to contain other
terms related to that meaning. For example, for
the queries [turkey maps] and [turkey culture] the
search results will contain information related to
the country, whereas [turkey recipes] and [turkey
nutritional value] should share many terms about
the poultry.
Query sessions: A query session is a series of
queries submitted by a single user within a small
range of time (Silverstein et al, 1999). Informa-
tion stored in the session logs may include the text
For each (instance, attribute) pair:
? Retrieve all the sessions that contained the query [in-
stance attribute].
? Collect the set of all the queries that appeared in the
same session and which are a superstring of instance.
? Remove instance from each of those queries, and out-
put the resulting set of query words.
Figure 1: Algorithm to collect session phrases as-
sociated to attributes.
of the queries and metadata, such as the time, the
type of query (e.g., using the normal or the ad-
vance form), and user settings such as the Web
browser used (Silverstein et al, 1999).
Users often search for related queries within
a session: queries on the culture of the coun-
try Turkey will tend to be surrounded by queries
about topics related to the country; similarly,
queries about turkey recipes will tend to be sur-
rounded by other queries on recipes. Therefore,
if two attributes refer to the same meaning of the
instance, the distributions of terms that co-occur
with them in the same search sessions is expected
to be similar. To ensure that the user did not
change intent during the session, we also require
the queries from which we extract phrases to con-
tain the instance of interest. The pseudocode of
the procedure is shown in Figure 1.
Class labels: As described in (Pas?ca and Van
Durme, 2008), we collect for each instance (e.g.,
turkey), a ranked list of class labels (e.g., country,
location, poultry, food). The procedure uses a col-
lection of Web documents and applies some IsA
extraction patterns selected from (Hearst, 1992).
Using the (instance, ranked-attributes) and the (in-
stance, ranked-class labels) lists, it is possible to
aggregate the two datasets to obtain, for each at-
tribute, the class labels that are most strongly as-
sociated to it (Figure 2).
3.4 EM clustering
We run a set of EM clusterings separately for the
attributes of each instance. The model imple-
mented is the following: given an instance, let
A = {a1, a2, ..., an} be the set of attributes as-
sociated with that instance. Let T be the vocabu-
lary for the terms found in the search results, S the
vocabulary of session log terms co-occurring with
821
For each attribute:
? Collect all the instances that contain that attribute.
? For each class label, average its ranks for those in-
stances. If an instance does not contain a particular
class label, use as rank the size of the longest list of
class labels plus one.
? Rank the class labels from smaller to larger average
rank.
Figure 2: Algorithm to collect class labels associ-
ated to attributes.
the attribute, and C be the set of all the possible
class labels. Let K be the cluster function which
assigns cluster indexes to the attributes.
We assume that the distributions for snippet
terms, session terms and class labels are condi-
tionally independent given the clustering. Further-
more, we assume that the distribution of terms for
queries in a cluster are also conditionally indepen-
dent given the cluster assignments:
p?(T |K,A) ?
Y
j
p?(tj |K,A)
p?(S|K,A) ?
Y
k
p?(sk|K,A)
p?(C|K,A) ?
Y
l
p?(cl|K,A)
The clustering model for each instance (the ex-
pectation step) is, therefore:
p?(KT SC|A,?) =
N?
i
p?(K|A)p?(T |K,A)p?(S|K,A)p?(C|K,A)
To estimate the parameters of the model, we must
be able to estimate the following distributions dur-
ing the maximization step:
? p?(tj |K,A) = E?(tj ,K|A)E?(K|A)
? p?(sk|K,A) = E?(sk,K|A)E?(K|A)
? p?(cl|K,A) = E?(cl,K|A)E?(K|A)One advantage of this approach is that it allows
using a subset of the available data sources to eval-
uate their relative influence on the clustering qual-
ity. In the experiments we have tried all possible
combinations of the three data sources to find the
settings that give the best results.
3.5 Initialization strategies
The initial assignment of attributes to clusters is
important, since a bad seed clustering can lead
EM to local optima. We have tried the following
two strategies:
Random assignment: the attributes are assigned
to clusters randomly. To make the results repeat-
able, for each instance we use the instance name
as the seed for the random number generator.
K-means: the initial assignments of attributes
to clusters is performed using K-means. In this
model, we use a simple vector-space-model in the
following way:
1. Each attribute is represented with a bag-of-
words of the snippets of the search results for
a concatenation of the instance name and the
attribute. This is the same data already col-
lected for EM.
2. Each of the snippet terms in these bag-of-
words is weighted using the tf ? idf score,
with inverse document frequencies estimated
from an English web corpus with hundreds
of millions of documents.
3. The cosine of the angle of the vectors is used
as the similarity metric between each pair of
attributes.
Several values of K have been tried in our exper-
iments, as mentioned in Section 4.
3.6 Post-processing
EM works with a fixed set of clusters. In order
to decide which is the optimal number of clusters,
we have run all the experiments with a number of
clusters K that is large enough to accommodate
most of the queries in our dataset, and we run a
post-processing step that merges clusters for in-
stances that have less than K meanings.
Since we have, for each attribute, a distribution
of the most likely class labels (Section 3.3), the
post-processing performs as follows:
1. Generate a list of class labels per cluster, by
combining the ranked lists of per-attribute
class labels as was done in Section 3.3.
2. Merge together all the clusters such that their
sets of top k class labels are the same.
The values ofK and k are chosen by doing several
runs with different values on the development set,
as described in Section 4.
822
4 Evaluation and Results
4.1 Evaluation metrics
There does not exist a fully agreed evaluation
metric for clustering tasks in NLP (Geiss, 2009;
Amigo? et al, 2009). Each metric has its own
idiosyncrasies, so we have chosen to compute
six different evaluation metrics as described in
(Amigo? et al, 2009). Empirical results show they
are highly correlated, i.e., tuning a parameter by
hill-climbing on F-score typically also improves
the B3 F-score.
Purity (Zhao and Karypis, 2002): Let C be
the clusters to evaluate, L the set of cate-
gories (the clusters in the gold-standard), and
N the number of clustered items. Purity is
the average of the precision values: Purity =?
i
|Ci|
N maxj Prec(Ci, Lj), where the precisionfor cluster Ci with respect to category Lj is
Prec(Ci, Lj) = |Ci?Lj ||Ci| . Purity is a precision met-ric. Inverting the roles of the categories L and the
clusters C gives a recall metric, inverse purity,
which rewards grouping items together. The two
metrics can be combined in an F-score.
B3 Precision (Bagga and Baldwin, 1998): Let
L(e) and C(e) denote the gold-standard-category
and the cluster of an item e. The correctness of the
relation between e and other element e? is defined
as
Correctness(e, e?) =
?
1 iffL(e) = L(e?)? C(e) = C(e?)
0 otherwise
The B3 Precision of an item is the proportion
of items in its cluster which belong to its cat-
egory, including itself. The total precision is
the average of the item precisions: B3 Prec =
avge[avge?:C(e)=C(e?)Correctness(e, e?)]
B3 Recall: is calculated in a similar way, inverting
the roles of clusters and categories. The B3 F-
score is obtained by combining B3 precision and
B3 recall.
4.2 Gold standards
We have built two annotated sets, one to be used
as a development set for adjusting the parame-
ters, and a second one as a test set. The evalu-
ation settings were chosen without knowledge of
Purity Inv. F-score B3 B3 B3
Purity Precision Recall F-score
0.94 0.95 0.92 0.90 0.92 0.91
Table 2: Inter-judge agreement scores.
Polysemous Main meanings
airplane machine, movie
apple fruit, company
armstrong unit, company, person
chain reaction company, film, band, chemistry
chf airport, currency, heart attack
darwin person, city
david copperfield book, performer, movie
delta letter, airways
Table 3: Examples of polysemous instances.
the test set. Each of the two sets contains 75 in-
stances chosen randomly from the complete set of
instances with ranked attributes (Section 3.2 de-
scribed the input data). For the random sampling,
the instances were weighted with their frequency
in the query logs as full queries, so that more
frequent instances have higher chance to be cho-
sen. This ensures that uncommon instances are
not overrepresented in the gold-standard.
The annotators contributed 50 additional in-
stances (25 for development and 25 for testing)
that they considered interesting to study, e.g., be-
cause of having several salient meanings.
Five human annotators were shown the top 32
attributes for each instance, and they were asked
to cluster them. We decided to start with a sim-
plified version of the problem by considering it a
hard clustering task.
Table 2 shows that the average agreement
scores between judge pairs, measured with the
same evaluation metrics used for the system out-
put, are quite high. In the first three metrics, the
F-score is not an average of precision and recall,
but a weighted average calculated separately for
each cluster, so it may have a value that is not be-
tween the values of precision and recall.
The annotated instances were classified as
monosemous/polysemous, depending on wether
or not they had more than one cluster with enough
(five) attributes. This classification allows to re-
port separate results for the whole set (where in-
stances with just one major sense dominate) and
for the subset of polysemous instances. Table 3
shows examples of polysemous instances. Exam-
823
All instances polysemous instances
Weights Purity Inv. F B3 B3 B3 F Purity Inv. F B3 B3 B3 F
Purity score Prec. Recall score Purity score Prec. Recall score
All-in-one 0.797 1.000 0.766 0.700 1.000 0.797 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242 1.000 0.205 0.266 1.000 0.205 0.333
Random 0.888 0.322 0.451 0.851 0.246 0.373 0.685 0.362 0.447 0.595 0.276 0.373
Random Only snippets 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
Init. Only sessions 0.797 0.948 0.728 0.700 0.944 0.753 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.798 0.983 0.760 0.701 0.969 0.785 0.561 0.990 0.541 0.415 0.981 0.574
No snippets 0.798 0.934 0.723 0.702 0.918 0.744 0.561 0.990 0.541 0.415 0.981 0.574
No sessions 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
No class labels 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
All 0.809 0.380 0.420 0.736 0.316 0.414 0.596 0.430 0.400 0.483 0.361 0.399
K-Means Only snippets 0.844 0.765 0.700 0.771 0.654 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Init. Only sessions 0.798 0.957 0.736 0.702 0.949 0.759 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.824 0.656 0.622 0.747 0.568 0.604 0.641 0.768 0.565 0.519 0.699 0.575
No snippets 0.824 0.655 0.622 0.748 0.562 0.598 0.640 0.768 0.565 0.518 0.698 0.574
No sessions 0.843 0.770 0.701 0.769 0.661 0.677 0.671 0.806 0.587 0.556 0.719 0.611
No class labels 0.844 0.762 0.698 0.771 0.651 0.673 0.671 0.806 0.587 0.556 0.719 0.611
All 0.843 0.767 0.699 0.770 0.657 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Table 4: Scores over all instances and over polysemous instances.
ples of monosemous instances are activision, am-
ctheaters, american airlines, ask.com, bebo, dis-
ney or einstein. 22% of the instances in the devel-
opment set and 13% of the instances in the test set
are polysemous.
4.3 Parameter tuning
We tuned the different parameters of the algorithm
using the development set. We performed several
EM runs including all three data sources, modi-
fying the following parameters: the smoothing 
added to the cluster soft-assignment in the Maxi-
mization step (Manning et al, 2008), the number
K of clusters for K-Means and EM, and the num-
ber k of top ranked class labels that two clusters
need to have in common in order to be merged
at the post-processing step. The best results were
obtained with  = 0.4, K = 5 and k = 1. These
are the values used in the experiments mentioned
from now on.
4.4 EM initialization and data sources
Table 4 shows the results after running EM over
the development set, using every possible combi-
nation of data sources, and the two initialization
strategies (random and K-Means). Several obser-
vations can be drawn from this table:
First, as mentioned in Section 2, the evalua-
tion metrics are biased towards the all-in-one solu-
tion. This is worsened by the fact that the majority
of the instances in our dataset are monosemous.
Therefore, the highest F-scores and B3 F-scores
are obtained by the all-in-one baseline, although
it is not the most useful clustering.
When using only class labels, EM tends to pro-
duce results similar to the all-in-one baseline This
can be explained by the limited class vocabulary
which makes most of the attributes share class la-
bels. The bad results when using only sessions are
caused by the presence of attributes with no ses-
sion terms, due to insufficient data.
The random clustering baseline (third line in
Table 4) tends to give smaller clusters than EM,
because it distributes instances uniformly across
the clusters. This leads to better precision scores,
and much worse recall and F-score metrics.
From these results, we conclude that snippet
terms are the most useful resource for clustering.
The other data sources do not provide a signifi-
cant improvement over it. The best results overall
for the polysemous instances, and the highest re-
sults for the whole dataset (excluding the outliers
that are too similar to the all-in-one baseline) are
obtained using snippet terms. For these configura-
tions, as we expected, the K-Means initialization
does a better job in avoiding local optima during
EM than the random one.
4.5 Post-processing
Table 5 includes the results on the development
set after post-processing, using the best configu-
ration for EM (K-Means initialization and snippet
terms for EM). Post-processing slightly hurts the
B3 F-score for polysemous terms, but it improves
results for the whole dataset, as it merges many
clusters for the monosemous instances.
824
Data Method Purity Inv. Purity F-score B3 Prec. B3 Recall B3 F-score
All instances All-in-one 0.797 1.000 0.766 0.700 1.000 0.797
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242
K-Means + EM (snippets) 0.844 0.765 0.700 0.771 0.654 0.675
K-Means + EM (snippets) + postprocessing 0.825 0.837 0.728 0.743 0.761 0.722
Polysemous All-in-one 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.205 0.266 1.000 0.205 0.333
K-Means + EM (snippets) 0.671 0.806 0.587 0.556 0.719 0.611
K-Means + EM (snippets) + postprocessing 0.644 0.846 0.592 0.518 0.777 0.607
Table 5: Scores only over all and polysemous instances, without and with postprocessing.
K-Means output EM output Post-processing
pictures, family, logo, biography pictures, biography, inauguration pictures, biography, inauguration
inauguration, song, lyrics, foods, song, lyrics, foods, timeline, song, lyrics, goods, timeline,
quotes, timeline, shoes, health care camping, shoes, maps, art, history, camping, shoes, maps, art, history
maps, art, kids, history, speeches official website, facts, speeches official website, facts, speeches
official website, facts, scandal scandal, blog, music scandal, blog, music, family, kids
economy, blog, music, flag, camping approval rating, health care, daughters
approval rating economy approval rating, health care,
daughters family, kids, daughters economy
symbol logo, quotes, symbol, flag logo, quotes, symbol, definition
definition, religion, definition, religion, slogan, books religion, slogan, books, flag
slogan, books
Table 6: Attributes extracted for the monosemous instance obama, using snippet terms for EM.
4.6 Clustering examples
Tables 6 and 7 show examples of clustering results
for three instances chosen as representatives of the
monosemous and the polysemous subsets. These
show that the output of the K-Means initialization
can uncover some meaningful clusters, but tends
to generate a dominant cluster and a few small or
singleton clusters. EM distributes the attributes
more evenly across clusters, combining attributes
that are closely related.
For monosemous instances like obama, EM
generates small clusters of highly related at-
tributes (e.g, family, kids and daughters). Post-
processing merges some of the clusters together,
but it fails to merge all into a single cluster.
For darwin, two of the small clusters given by
K-Means are actually good, as ports is the only at-
tribute of the operating system, and lyrics is one of
the two attributes referring to a song titled Darwin.
EM again redistributes the attributes, creating two
large and mostly correct clusters.
For david copperfield, EM creates two clusters
for the performer, one for the book, one for the
movie, and one for tattoo (off-topic for this in-
stance). The two clusters referring to the per-
former are merged in the post-processing, with
some errors remaining, e.g, trailer and second
wife are in the wrong cluster.
4.7 Results on the test set
Table 8 show the results of the EM clustering and
the postprocessing step when executed on the test
set. The settings are those that produced the best
results on the development set: using EM initial-
ized with K-Means, and using only snippet terms
for the generative model.
As mentioned above, the test set has a higher
proportion of monosemous queries than the de-
velopment set, so the all-in-one baseline pro-
duces better results than before. Still, we can see
the same trend happening: for the whole dataset
the F-score metrics are somewhat worse than the
best baseline, given that the evaluation metrics all
overvalue the all-in-one baseline, but this can be
considered an artifact of the metrics. As with the
development set, using EM produces the best pre-
cision scores (except for the all-singletons base-
line), and the postprocessing improves precision
and F-score over the all-in-one baseline. The
whole system improves considerably the F-score
for the polysemous terms.
5 Conclusions
This paper investigates the new task of inducing
instance senses using ranked lists of attributes as
input. It describes a clustering procedure based
on the EM model, capable of integrating differ-
825
Instance K-Means output EM output Post-processing
Darwin maps, shoes, logo, awards, maps, shoes, logo, maps, shoes, logo,
weather pictures, quotes, weather jobs, tourism weather jobs, tourism
definition, jobs, tourism, hotels, attractions, hotels, attractions,
biography, hotels, beaches, accommodation, beaches, accommodation,
attractions, beaches, tv show, clothing, tv show, clothing,
accommodation, tv show, postcode, music, review postcode, music, review
clothing, postcode, music side effects, airlines, side effects, airlines,
facts, review, history prices, lighting prices, lighting
side effects, airlines, awards, ports definition, population
prices, lighting evolution, theory, quotes awards, ports
ports pictures, biography, evolution, theory, quotes
evolution, theory, books facts, history, books pictures, biography,
lyrics lyrics facts, history, books
population definition, population lyrics
David Copperfield summary, biography, pictures, biography, pictures, quotes, biography, pictures, girlfriend
quotes, strokes, book review, strokes, tricks, tour dates, quotes, strokes, tricks, tattoo
tricks, tour dates, characters, lyrics, dating, logo, tour dates, secrets, lyrics,
lyrics, plot, synopsis, dating, filmography, cast members, wives, music, dating, logo,
logo, themes, author, official website, trailer, filmography, blog, cast members,
filmography, cast members, setting, religion official website, trailer,
official website, trailer, book review, review, house, setting, religion
setting, religion reviews book review, review, house,
house, reviews tattoo reviews
tattoo summary, second wife, summary, second wife,
second wife characters, plot, synopsis, characters, plot, synopsis,
girlfriend, secrets, wives, themes, author themes, author
review, music, blog girlfriend, secrets, wives,
music, blog
Table 7: Attributes extracted for three polysemous instances, using snippet terms for EM.
Set Solution Purity Inverse Purity F-score B3 Precision B3 Recall B3 F-score
All All-in-one 0.907 1.000 0.892 0.858 1.000 0.908
All-singletons 1.000 0.076 0.114 1.000 0.076 0.136
Random 0.936 0.325 0.463 0.914 0.243 0.377
EM 0.927 0.577 0.664 0.896 0.426 0.561
EM+postprocessing 0.919 0.806 0.804 0.878 0.717 0.764
Polysemous All-in-one 0.588 1.000 0.586 0.457 1.000 0.613
All-singletons 1.000 0.141 0.210 1.000 0.141 0.239
Random 0.643 0.382 0.441 0.549 0.288 0.369
EM 0.706 0.631 0.556 0.626 0.515 0.547
EM+postprocessing 0.675 0.894 0.650 0.564 0.842 0.661
Table 8: Scores in the test set.
ent data sources, and explores cluster initializa-
tion and post-processing strategies. The evalu-
ation shows that the most important of the con-
sidered data sources is the snippet terms obtained
from search engine results to queries made by
concatenating the instance and the attribute. A
simple post-processing that merges attribute clus-
ters that have common class labels can improve
recall for monosemous queries. The results show
improvements across most metrics with respect to
a random baseline, and F-score improvements for
polysemous instances.
Future work includes extending the generative
model to be applied across the board, linking the
clustering models of different instances with each
other. We also intend to explore applications of
the clustered attributes in order to perform extrin-
sic evaluations on these data.
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007, pages 7?12.
Association for Computational Linguistics.
Amigo?, E., J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Retrieval,
12(4):461?486.
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proceed-
ings of SemEval-2007, pages 64?69.
Artiles, J., J. Gonzalo, and S. Sekine. 2009. Weps 2 evalua-
tion campaign: overview of the web people search cluster-
ing task. In 2nd Web People Search Evaluation Workshop
(WePS 2009), 18th WWW Conference.
Bagga, A. and B. Baldwin. 1998. Entity-based cross-
document co-referencing using the vector space model,
Proceedings of the 17th international conference on Com-
putational linguistics. In Proceedings of ACL-98.
826
Banko, M., Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information extraction
from the Web. In Proceedings of IJCAI-07, pages 2670?
2676, Hyderabad, India.
Bellare, K., P.P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. In NIPS 2007 Workshop
on Machine Learning for Web Search.
Brody, Samuel and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL ?09, pages 103?
111.
Cafarella, M.J., A. Halevy, D.Z. Wang, and Y. Zhang. 2008.
Webtables: Exploring the Power of Tables on the Eeb.
Proceedings of the VLDB Endowment archive, 1(1):538?
549.
Cui, G., Q. Lu, W. Li, and Y. Chen. 2009. Automatic Acqui-
sition of Attributes for Ontology Construction. In Pro-
ceedings of the 22nd International Conference on Com-
puter Processing of Oriental Languages, pages 248?259.
Springer.
Dempster, A.P., N.M. Laird, D.B. Rubin, et al 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society. Series B
(Methodological), 39(1):1?38.
Etzioni, O., M. Banko, S. Soderland, and S. Weld. 2008.
Open Information Extraction from the Web. Communica-
tions of the ACM, 51(12), December.
Gao, W., C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
SIGIR-07, pages 463?470, Amsterdam, The Netherlands.
Geiss, J. 2009. Creating a Gold Standard for Sentence Clus-
tering in Multi-Document Summarization. ACL-IJCNLP
2009.
Hearst, M. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of COLING-92, pages
539?545, Nantes, France.
Mann, Gideon S. and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings of
HLT-NAACL 2003, pages 33?40. Association for Compu-
tational Linguistics.
Manning, C.D., P. Raghavan, and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge University
Press New York, NY, USA.
Nastase, V. and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI-08, pages 1219?1224, Chicago, Illinois.
Pas?ca, M. and B. Van Durme. 2007. What you seek is what
you get: Extraction of class attributes from query logs. In
Proceedings of IJCAI-07, pages 2832?2837, Hyderabad,
India.
Pas?ca, M. and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of ACL-
08, pages 19?27, Columbus, Ohio.
Pas?ca, M., B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of CIKM-07, pages 485?494, Lis-
bon, Portugal.
Pas?ca, M. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of WWW-07, pages 101?110,
Banff, Canada.
Ravi, S. and M. Pas?ca. 2008. Using Structured Text for
Large-Scale Attribute Extraction. In CIKM. ACM New
York, NY, USA.
Sekine, S. 2006. On-Demand Information Extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Compu-
tational Linguistics Morristown, NJ, USA.
Silverstein, C., H. Marais, M. Henzinger, and M. Moricz.
1999. Analysis of a very large web search engine query
log. In ACM SIGIR Forum, pages 6?12. ACM New York,
NY, USA.
Suchanek, F., G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of WWW-07, pages 697?706,
Banff, Canada.
Tokunaga, K., J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web docu-
ments. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP-05),
pages 106?118, Jeju Island, Korea.
Wong, T.L. and W. Lam. 2009. An Unsupervised Method
for Joint Information Extraction and Feature Mining
Across Different Web Sites. Data & Knowledge Engi-
neering, 68(1):107?125.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of WWW-
08, pages 635?644, Beijing, China.
Wu, F., R. Hoffmann, and D. Weld. 2008. Information
extraction from Wikipedia: Moving down the long tail.
In Proceedings of KDD-08, pages 731?739, Las Vegas,
Nevada.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
ACL-95, pages 189?196. Association for Computational
Linguistics.
Yoshinaga, N. and K. Torisawa. 2007. Open-Domain
Attribute-Value Acquisition from Semi-Structured Texts.
In Proceedings of the Workshop on Ontolex, pages 55?66.
Zhao, Y. and G. Karypis. 2002. Criterion functions for docu-
ment clustering. Technical report, Experiments and Anal-
ysis University of Minnesota, Department of Computer
Science/Army HPC Research Center.
827
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Dense Models of Query Similarity from User Click Logs
Fabio De Bona?
Friedrich Miescher Laboratory
of the Max Planck Society
Tu?bingen, Germany
fabio@tuebingen.mpg.de
Stefan Riezler
Google Research
Zu?rich, Switzerland
riezler@google.com
Keith Hall
Google Research
Zu?rich, Switzerland
kbhall@google.com
Massimiliano Ciaramita
Google Research
Zu?rich, Switzerland
massi@google.com
Amac? Herdag?delen?
University of Trento
Rovereto, Italy
amac@herdagdelen.com
Maria Holmqvist?
Linkopings University
Linkopings, Sweden
marho@ida.liu.se
Abstract
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
1 Introduction
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
?The work presented in this paper was done while the au-
thors were visiting Google Research, Zu?rich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al, 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
474
ference of our approach to Jones et al (2006) lies
in our particular choice of string similarity metrics.
While Jones et al (2006) deploy ?syntactic? fea-
tures such as Levenshtein distance, and ?semantic?
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
2 Query Similarity Measures
2.1 Semantic measures
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
. (1)
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
2.2 Taxonomic normalizations
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., ?camping? to ?outdoor activities? is
a generalization) and character level (?animal pic-
tures? to ?cat pictures? is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
PMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
PMI(S)(wi, wj) = PMI(wi, wj)/? log(p(wi)).
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
475
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
PMI(G)(wi, wj) = PMI(wi, wj)/? log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition ?apple? to ?mac os? PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
?ferrari models? to ?ferrari? we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a ?perfect? gener-
alization of the source1.
2.3 Syntactic measures
Let V be a finite vocabulary and ? be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ? {V ? {?} ? V ?
{?}} \ {(?, ?)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ? = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(?) =
?n
i=1 c(?i). The Levenshtein distance, or
edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al, 2009; Jones et al, 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
1The values are computed from Web counts.
a finite vocabulary T containing all terms occurring
in queries:
?a, b ? T, cE1(a, b) = 1 if(a 6= b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
?a, b ? T, cE2(a, b) = dA(a, b) ifa ? b 6= ?, 1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that ?brooklyn pizza? and ?pizza brooklyn?
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between ?brooklyn pizza? and the less
relevant ?brooklyn college? is only one (substitute).
The sorted variant relaxes the ordering constraint.
2.4 Generalized measures
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:
s(wi, wj) = 2? 2f(wi, wj) + 
476
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:
?a, b ? T, cSE(a, b) = s(a, b) ifa ? b 6= ?, 1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is ?cheaper? than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The 
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
2.5 Cost matrix estimation
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q?s = qs \ qt and
q?t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
ni,j(qs, qt) =
?
?
?
1 if wi = wj ? wi ? qs ? wj ? qt
1/(|q?s| |q
?
t|) if wi ? q
?
s ? wj ? q
?
t
0 else.
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ? qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =
?
qs,qt ni,j(qs, qt). Let N =
?
wi,wj
Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =
Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ?) =
?
wj
Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(?, j) =
?
wi
Ni,j/N . Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al (2010).
3 Learning to Rank from Co-Click Data
3.1 Extracting Weak Labels from Co-Clicks
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
477
Table 1: Statistics of co-click data sets.
train dev test
number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30
percentage of rewrites
with ? 10 coclicks 0.2 0.2 43
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of ?goodness?, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ?
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ? {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ? {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation piq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let ?q denote the set of all possible permuta-
tions on xq, and let piqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function ?(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
?(xq, piq) =
1
|Pq|
?
(i,j)?Pq
?(xqi)??(xqj)sgn(
1
piqi
?
1
piqj
).
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = ?w, ?(xqi)?, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, piq) = ?w, ?(xq, piq)?.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
min
w
?
xq ,yq
`(w)
478
by stochastic updating
wt+1 = wt ? ?tgt
where ?t is a learning rate, and gt is the gradient
gt = ?`(w)
where
?`(w) =
?
?
?w1
`(w),
?
?w2
`(w), . . . ,
?
?wn
`(w)
?
.
3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al, 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al (2007), or Chakrabarti et al (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
`lh(w) = (L(yq, pi
?
q )?
?
w, ?(xq, yq)? ?(xq, pi
?
q )
?
)+
where pi?q is the maximizer of the
maxpiq??q\yq L(yq, pi
?
q ) +
?
w, ?(xq, pi?q )
?
ex-
pression, (z)+ = max{0, z} and L(yq, piq) ? [0, 1]
denotes a prediction loss of a predicted ranking piq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
LAP (yq, piq) = 1?AP (yq, piq)
where AP is defined as follows:
AP (yq, piq) =
?n(q)
j=1 Prec(j) ? (|yqj ? 2|)
?n(q)
j=1 (|yqj ? 2|)
,
P rec(j) =
?
k:piqk?piqj
(|yqk ? 2|)
piqj
.
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation piq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let piq denote
a permutation that is consistent with ranking yq .
Note that the ranking scenario is in this case bipartite
with yqi ? {1, 2}.
The derivatives for `lh are as follows:
?
?wk
`lh =
?
?
?
0 if
(?
w, ?(xq, yq)? ?(xq, pi?q )
?)
> L(yq, pi?q ),
?(?k(xq, yq)? ?k(xq, pi?q )) else.
SGD optimization involves computing pi?q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al (2007). We will refer to this method as the
SVM-MAP model.
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
L(yqi, yqj) = |
1
yqi
?
1
yqj
|
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
`ph(w) =
?
(i,j)?Pq
(L(yqi, yqj)?
?w, ?(xqi)? ?(xqj)? sgn(
1
yqi
?
1
yqj
))+
479
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
?
?wk
`lp =
?
????
????
0 if (?w, ?(xqi)? ?(xqj)?
sgn( 1yqi ?
1
yqj
)) > L(yqi, yqj),
?(?k(xqi)? ?k(xqj))sgn( 1yqi ?
1
yqj
)
else.
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =
(n(q)
2
)
pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =
?r?1
i=1
?r
j=i+1 |li||lj |. For bipartite ranking of p
positive and n negative instances, |Pq| = p ? n com-
parisons are necessary.
3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al (2002):
`llm(w) = ? log
?
xqi?xq |yqi=1
e?w,?(xqi)?
?
xqi?xq
e?w,?(xqi)?
.
The gradient of `llm is calculated as a difference be-
tween two expectations:
?
?wk
`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
4 Experimental Results
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
480
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates ? ?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ? 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
3For a definition of these metrics see Manning et al (2008)
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
5 Discussion
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
References
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese cakes?: Query reformula-
481
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Massimiliano Ciaramita, Amac? Herdag?delen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML?07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8?12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML?05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW?06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119?128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647?656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751?
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS?09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627?633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW?06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML?04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
482
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 965?975,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Piggyback: Using Search Engines for Robust Cross-Domain
Named Entity Recognition
Stefan Ru?d
Institute for NLP
University of Stuttgart
Germany
Massimiliano Ciaramita
Google Research
Zu?rich
Switzerland
Jens Mu?ller and Hinrich Schu?tze
Institute for NLP
University of Stuttgart
Germany
Abstract
We use search engine results to address a par-
ticularly difficult cross-domain language pro-
cessing task, the adaptation of named entity
recognition (NER) from news text to web
queries. The key novelty of the method is that
we submit a token with context to a search
engine and use similar contexts in the search
results as additional information for correctly
classifying the token. We achieve strong gains
in NER performance on news, in-domain and
out-of-domain, and on web queries.
1 Introduction
As statistical Natural Language Processing (NLP)
matures, NLP components are increasingly used in
real-world applications. In many cases, this means
that some form of cross-domain adaptation is neces-
sary because there are distributional differences be-
tween the labeled training set that is available and
the real-world data in the application. To address
this problem, we propose a new type of features
for NLP data, features extracted from search en-
gine results. Our motivation is that search engine
results can be viewed as a substitute for the world
knowledge that is required in NLP tasks, but that can
only be extracted from a standard training set or pre-
compiled resources to a limited extent. For example,
a named entity (NE) recognizer trained on news text
may tag the NE London in an out-of-domain web
query like London Klondike gold rush as a location.
But if we train the recognizer on features derived
from search results for the sentence to be tagged,
correct classification as person is possible. This is
because the search results for London Klondike gold
rush contain snippets in which the first name Jack
precedes London; this is a sure indicator of a last
name and hence an NE of type person.
We call our approach piggyback and search result-
derived features piggyback features because we pig-
gyback on a search engine like Google for solving a
difficult NLP task.
In this paper, we use piggyback features to ad-
dress a particularly hard cross-domain problem, the
application of an NER system trained on news to
web queries. This problem is hard for two reasons.
First, the most reliable cue for NEs in English, as
in many languages, is capitalization. But queries
are generally lowercase and even if uppercase char-
acters are used, they are not consistent enough to
be reliable features. Thus, applying NER systems
trained on news to web queries requires a robust
cross-domain approach.
News to queries adaptation is also hard because
queries provide limited context for NEs. In news
text, the first mention of a word like Ford is often
a fully qualified, unambiguous name like Ford Mo-
tor Corporation or Gerald Ford. In a short query
like buy ford or ford pardon, there is much less con-
text than in news. The lack of context and capitaliza-
tion, and the noisiness of real-world web queries (to-
kenization irregularities and misspellings) all make
NER hard. The low annotator agreement we found
for queries (Section 5) also confirms this point.
The correct identification of NEs in web queries
can be crucial for providing relevant pages and ads
to users. Other domains have characteristics sim-
ilar to web queries, e.g., automatically transcribed
speech, social communities like Twitter, and SMS.
Thus, NER for short, noisy text fragments, in the
absence of capitalization, is of general importance.
965
NER performance is to a large extent determined
by the quality of the feature representation. Lexical,
part-of-speech (PoS), shape and gazetteer features
are standard. While the impact of different types of
features is well understood for standard NER, fun-
damentally different types of features can be used
when leveraging search engine results. Returning to
the NE London in the query London Klondike gold
rush, the feature ?proportion of search engine results
in which a first name precedes the token of interest?
is likely to be useful in NER. Since using search en-
gine results for cross-domain robustness is a new ap-
proach in NLP, the design of appropriate features is
crucial to its success. A significant part of this paper
is devoted to feature design and evaluation.
This paper is organized as follows. Section 2 dis-
cusses related work. We describe standard NER fea-
tures in Section 3. One main contribution of this
paper is the large array of piggyback features that
we propose in Section 4. We describe the data sets
we use and our experimental setup in Sections 5?6.
The results in Section 7 show that piggyback fea-
tures significantly increase NER performance. This
is the second main contribution of the paper. We dis-
cuss challenges of using piggyback features ? due to
the cost of querying search engines ? and present our
conclusions and future work in Section 8.
2 Related work
Barr et al (2008) found that capitalization of NEs in
web queries is inconsistent and not a reliable cue for
NER. Guo et al (2009) exploit query logs for NER
in queries. This is also promising, but the context
in search results is richer and potentially more infor-
mative than that of other queries in logs.
The insight that search results provide useful ad-
ditional context for natural language expressions is
not new. Perhaps the oldest and best known applica-
tion is pseudo-relevance feedback which uses words
and phrases from search results for query expansion
(Rocchio, 1971; Xu and Croft, 1996). Search counts
or search results have also been used for sentiment
analysis (Turney, 2002), for transliteration (Grefen-
stette et al, 2004), candidate selection in machine
translation (Lapata and Keller, 2005), text similar-
ity measurements (Sahami and Heilman, 2006), in-
correct parse tree filtering (Yates et al, 2006), and
paraphrase evaluation (Fujita and Sato, 2008). The
specific NER application we address is most similar
to the work of Farkas et al (2007), but they mainly
used frequency statistics as opposed to what we view
as the main strength of search results: the ability to
get additional contextually similar uses of the token
that is to be classified.
Lawson et al (2010), Finin et al (2010), and
Yetisgen-Yildiz et al (2010) investigate how to best
use Amazon Mechanical Turk (AMT) for NER. We
use AMT as a tool, but it is not our focus.
NLP settings where training and test sets are from
different domains have received considerable atten-
tion in recent years. These settings are difficult be-
cause many machine learning approaches assume
that source and target are drawn from the same dis-
tribution; this is not the case if they are from differ-
ent domains. Systems applied out of domain typi-
cally incur severe losses in accuracy; e.g., Poibeau
and Kosseim (2000) showed that newswire-trained
NER systems perform poorly when applied to email
data (a drop of F1 from .9 to .5). Recent work in ma-
chine learning has made substantial progress in un-
derstanding how cross-domain features can be used
in effective ways (Ben-David et al, 2010). The de-
velopment of such features however is to a large ex-
tent an empirical problem. From this perspective,
one of the most successful approaches to adaptation
for NER is based on generating shared feature rep-
resentations between source and target domains, via
unsupervised methods (Ando, 2004; Turian et al,
2010). Turian et al (2010) show that adapting from
CoNLL to MUC-7 (Chinchor, 1998) data (thus be-
tween different newswire sources), the best unsuper-
vised feature (Brown clusters) improves F1 from .68
to .79. Our approach fits within this line of work
in that it empirically investigates features with good
cross-domain generalization properties. The main
contribution of this paper is the design and evalu-
ation of a novel family of features extracted from
the largest and most up-to-date repository of world
knowledge, the web.
Another source of world knowledge for NER is
Wikipedia: Kazama and Torisawa (2007) show that
pseudocategories extracted from Wikipedia help for
in-domain NER. Cucerzan (2007) uses Wikipedia
and web search frequencies to improve NE disam-
biguation, including simple web search frequencies
966
BASE: lexical and input-text part-of-speech features
1 WORD(k,i) binary: wk = wi
2 POS(k,t) binary: wk has part-of-speech t
3 SHAPE(k,i) binary: wk has (regular expression) shape regexpi
4 PREFIX(j) binary: w0 has prefix j (analogously for suffixes)
GAZ: gazetteer features
5 GAZ-Bl(k,i) binary: wk is the initial word of a phrase, consisting of l words, whose gaz. category is i
6 GAZ-Il(k,i) binary: wk is a non-initial word in a phrase, consisting of l words, whose gaz. category is i
URL: URL features
7 URL-SUBPART N(w0 is substring of a URL)/N(URL)
8 URL-MI(PER) 1/N(URL-parts)?[[p?URL-parts]] 3MIu(p, PER)?MIu(p,O)?MIu(p,ORG)?MIu(p,LOC)
LEX: local lexical features
9 NEIGHBOR(k) 1/N(k-neighbors)?[[v?k-neighbors]] log[NE-BNC(v, k)/OTHER-BNC(v, k)]
10 LEX-MI(PER,d) 1/N(d-words)?[[v?d-words]] 3MId(v, PER)?MId(v,O)?MId(v,ORG)?MId(v,LOC)
BOW: bag-of-word features
11 BOW-MI(PER) 1/N(bow-words)?[[v?bow-words]] 3MIb(v, PER)?MIb(v,O)?MIb(v,ORG)?MIb(v,LOC)
MISC: shape, search part-of-speech, and title features
12 UPPERCASE N(s0 is uppercase)/N(s0)
13 ALLCAPS N(s0 is all-caps)/N(s0)
14 SPECIAL binary: w0 contains special character
15 SPECIAL-TITLE N(s?1 or s1 in title contains special character)/(N(s?1)+N(s1))
16 TITLE-WORD N(s0 occurs in title)/N(title)
17 NOMINAL-POS N(s0 is tagged with nominal PoS)/N(s0)
18 CONTEXT(k) N(sk is typical neighbor at position k of named entity)/N(s0)
19 PHRASE-HIT(k) N(wk = sk, i.e., word at position k occurs in snippet)/N(s0)
20 ACRONYM N(w?1w0 or w0w1 or w?1w0w1 occur as acronym)/N(s0)
21 EMPTY binary: search result is empty
Table 1: NER features used in this paper. BASE and GAZ are standard features. URL, LEX, BOW and MISC are
piggyback (search engine-based) features. See text for explanation of notation. The definitions of URL-MI, LEX-MI,
and BOW-MI for LOC, ORG and O are analogous to those for PER. For better readability, we write
?
[[x]] for
?
x.
for compound entities.
3 Standard NER features
As is standard in supervised NER, we train an NE
tagger on a dataset where each token is represented
as a feature vector. In this and the following section
we present the features used in our study divided in
groups. We will refer to the target token ? the to-
ken we define the feature vector for ? as w0. Its left
neighbor is w?1 and its right neighbor w1. Table 1
provides a summary of all features.
Feature group BASE. The first class of fea-
tures, BASE, is standard in NER. The binary fea-
ture WORD(k,i) (line 1) is 1 iff wi, the ith word in
the dictionary, occurs at position k with respect to
w0. The dictionary consists of all words in the train-
ing set. The analogous feature for part of speech,
POS(k,t) (line 2), is 1 iff wk has been tagged with
PoS t, as determined by TnT tagger (Brants, 2000).
We also encode surface properties of the word with
simple regular expressions, e.g., x-ray is encoded as
x-x and 9/11 as d/dd (SHAPE, line 3). For these fea-
tures, k ? {?1, 0, 1}. Finally, we encode prefixes
and suffixes, up to three characters long, for w0 (line
4).
Feature group GAZ. Gazetteer features (lines 5
& 6) are an efficient and effective way of building
world knowledge into an NER model. A gazetteer
is simply a list of phrases that belong to a par-
ticular semantic category. We use gazetteers from
(i) GATE (Cunningham et al, 2002): countries,
first/last names, trigger words; (ii) WordNet: the
46 lexicographical labels (food, location, person
etc.); and (iii) Fortune 500: company names. The
two gazetteer features are the binary features GAZ-
Bl(k,i) and GAZ-Il(k,i). GAZ-Bl (resp. GAZ-Il) is 1
967
iff wk occurs as the first (resp. non-initial or internal)
word in a phrase of length l that the gazetteer lists as
belonging to category i where k ? {?1, 0, 1}.
4 Piggyback features
Feature groups URL, LEX, BOW, and MISC are
piggyback features. We produce these by segment-
ing the input text into overlapping trigrams w1w2w3,
w2w3w4, w3w4w5 etc. Each trigram wi?1wiwi+1
is submitted as a query to the search engine. For
all experiments we used the publicly accessible
Google Web Search API.1 The search engine returns
a search result for the query consisting of, in most
cases, 10 snippets,2 each of which contains 0, 1 or
more hits of the search term wi. We then compute
features for the vector representation of wi based on
the snippets. We again refer to the target token and
its neighbors (i.e., the search string) as w?1w0w1.
w0 is the token that is to be classified (PER, LOC,
ORG, or O) and the previous word and the next word
serve as context that the search engine can exploit to
provide snippets in which w0 is used in the same NE
category as in the input text. O is the tag of a token
that is neither LOC, ORG nor PER.
In the definition of the features, we refer to the
word in the snippet that matches w0 as s0, where
the match is determined based on edit distance. The
word immediately to the left (resp. right) of s0 in a
snippet is called s?1 (resp. s1).
For non-binary features, we first calculate real
values and then binarize them into 10 quantile bins.
Feature group URL. This group exploits NE
information in URLs. The feature URL-SUBPART
(line 7) is the fraction of URLs in the search re-
sult containing w0 as a substring. To avoid spurious
matches, we set the feature to 0 if length(w0) ? 2.
For URL-MI (line 8), each URL in the search re-
sult is split on special characters into parts (e.g., do-
main and subdomains). We refer to the set of all
parts in the search result as URL-parts. The value
of MIu(p,PER) is computed on the search results of
the training set as the mutual information (MI) be-
tween (i) w0 being PER and (ii) p occurring as part
of a URL in the search result. MI is defined as fol-
1Now deprecated in favor of the new Custom Search API.
2Less than 0.5% of the queries return fewer than 10 snippets.
lows:
MI(p,PER) =
?
i?{p?,p}
?
j?{ ?PER,PER}
P (i, j) log P (i, j)P (i)P (j)
For example, for the URL-part p = ?staff? (e.g.,
in bigcorp.com/staff.htm), P (staff) is the
proportion of search results that contain a URL
with the part ?staff?, P (PER) is the proportion of
search results where the search token w0 is PER
and P (staff,PER) is the proportion of search results
where w0 is PER and one of the URLs returned by
the search engine has part ?staff?.
The value of the feature URL-MI is the average
difference between the MI of PER and the other
named entities. The feature is calculated in the same
way for LOC, ORG, and O.
Our initial experiments that used binary features
for URL parts were not successful. We then de-
signed URL-MI to integrate all URL information
specific to an NE class into one measurement in
a way that gives higher weight to strong features
and lower weight to weak features. The inner
sum on line 8 is the sum of the three differences
MI(PER) ? MI(O), MI(PER) ? MI(ORG), and
MI(PER)?MI(LOC). Each of the three summands
indicates the relative advantage a URL part p gives
to PER vs O (or ORG and LOC). By averaging over
all URL parts, one then obtains an assessment of the
overall strength of evidence (in terms of MI) for the
NE class in question.
Feature group LEX. These features assess how
appropriate the words occurring in w0?s local con-
texts in the search result are for an NE class.
For NEIGHBOR (line 9), we calculate for each
word v in the British National Corpus (BNC) the
count NE-BNC(v, k), the number of times it oc-
curs at position k with respect to an NE; and
OTHER-BNC(v, k), the number of times it occurs
at position k with respect to a non-NE. We instan-
tiate the feature for k = ?1 (left neighbor) and
k = 1 (right neighbor). The value of NEIGHBOR(k)
is defined as the average log ratio of NE-BNC(v, k)
and OTHER-BNC(v, k), averaged over the set k-
neighbors, the set of words that occur at position k
with respect to s0 in the search result.
In the experiments reported in this paper, we use
a PoS-tagged version of the BNC, a balanced cor-
pus of 100M words of British English, as a model
968
of word distribution in general contexts and in NE
contexts that is not specific to either target or source
domain. In the BNC, NEs are tagged with just one
PoS-tag, but there is no differentiation into subcat-
egories. Note that the search engine could be used
again for this purpose; for practical reasons we pre-
ferred a static resource for this first study where
many design variants were explored.
The feature LEX-MI interprets words occurring
before or after s0 as indicators of named entitihood.
The parameter d indicates the ?direction? of the fea-
ture: before or after. MId(v,PER) is computed on
the search results of the training set as the MI be-
tween (i) w0 being PER and (ii) v occurring close to
s0 in the search result either to the left (d = ?1) or
to the right (d = 1) of s0. Close refers to a window
of 2 words. The value of LEX-MI(PER,d) is then
the average difference between the MI of PER and
the other NEs. The definition for LEX-MI(PER,d)
is given on line 10. The feature is calculated in the
same way for LOC, ORG, and O.
Feature group BOW. The features LEX-MI con-
sider a small window for cooccurrence information
and distinguish left and right context. For BOW fea-
tures, we use a larger window and ignore direction.
Our aim is to build a bag-of-words representation of
the contexts of w0 in the result snippets.
MIb(v,PER) is computed on the search results
of the training set as the MI between (i) w0 being
PER and (ii) v occurring anywhere in the search re-
sult. The value of BOW-MI(PER) is the average dif-
ference between the MI of PER and the other NEs
(line 11). The average is computed over all words
v ? bow-words that occur in a particular search re-
sult. The feature is calculated in the same way for
LOC, ORG, and O.
Feature group MISC. We collect the remaining
piggyback features in the group MISC.
The UPPERCASE and ALLCAPS features (lines
12&13) compute the fraction of occurrences of w0
in the search result with capitalization of only the
first letter and all letters, respectively. We exclude
titles: capitalization in titles is not a consistent clue
for NE status.
The SPECIAL feature (line 14) returns 1 iff any
character of w0 is a number or a special character.
NEs are often surrounded by special characters in
web pages, e.g., Janis Joplin - Summertime. The
SPECIAL-TITLE feature (line 15) captures this by
counting the occurrences of numbers and special
characters in s?1 and s1 in titles of the search result.
The TITLE-WORD feature (line 16) computes the
fraction of occurrences of w0 in the titles of the
search result.
The NOMINAL-POS feature (line 17) calculates
the proportion of nominal PoS tags (NN, NNS, NP,
NPS) of s0 in the search result, as determined by
a PoS tagging of the snippets using TreeTagger
(Schmid, 1994).
The basic idea behind the CONTEXT(k) feature
(line 18) is that the occurrence of words of certain
shapes and with certain parts of speech makes it ei-
ther more or less likely that w0 is an NE. For k = ?1
(the word preceding s0 in the search result), we test
for words that are adjectives, indefinites, posses-
sive pronouns or numerals (partly based on tagging,
partly based on a manually compiled list of words).
For k = 1 (the word following s0), we test for words
that contain numbers and special characters. This
feature is complementary to the feature group LEX
in that it is based on shape and PoS and does not
estimate different parameters for each word.
The feature PHRASE-HIT(?1) (line 19) calculates
the proportion of occurrences of w0 in the search re-
sult where the left neighbor in the snippet is equal
to the word preceding w0 in the search string, i.e.,
k = ?1: s?1 = w?1. PHRASE-HIT(1) is the
equivalent for the right neighbor. This feature helps
identify phrases ? search strings containing NEs are
more likely to occur as a phrase in search results.
The ACRONYM feature (line 20) computes the
proportion of the initials of w?1w0 or w0w1 or
w?1w0w1 occurring in the search result. For ex-
ample, the abbreviation GM is likely to occur when
searching for general motors dealers.
The binary feature EMPTY (line 21) returns 1 iff
the search result is empty. This feature enables the
classifier to distinguish true zero values (e.g., for the
feature ALLCAPS) from values that are zero because
the search engine found no hits.
5 Experimental data
In our experiments, we train an NER classifier on an
in-domain data set and test it on two different out-
of-domain data sets. We describe these data sets in
969
CoNLL trn CoNLL tst IEER KDD-D KDD-T
LOC 4.1 4.1 1.9 11.9 10.6
ORG 4.9 3.7 3.2 8.2 8.3
PER 5.4 6.4 3.8 5.3 5.4
O 85.6 85.8 91.1 74.6 75.7
Table 2: Percentages of NEs in CoNLL, IEER, and KDD.
this section and the NER classifier and the details of
the training regime in the next section, Section 6.
As training data for all models evaluated we used
the CoNLL 2003 English NER dataset, a corpus
of approximately 300,000 tokens of Reuters news
from 1992 annotated with person, location, organi-
zation and miscellaneous NE labels (Sang and Meul-
der, 2003). As out-of-domain newswire evaluation
data3 we use the development test data from the
NIST 1999 IEER named entity corpus, a dataset of
50,000 tokens of New York Times (NYT) and Asso-
ciated Press Weekly news.4 This corpus is annotated
with person, location, organization, cardinal, dura-
tion, measure, and date labels. CoNLL and IEER
are professionally edited and, in particular, properly
capitalized news corpora. As capitalization is ab-
sent from queries we lowercased both CoNLL and
IEER. We also reannotated the lowercased datasets
with PoS categories using the retrained TnT PoS tag-
ger (Brants, 2000) to avoid using non-plausible PoS
information. Notice that this step is necessary as
otherwise virtually no NNP/NNPS categories would
be predicted on the query data because the lower-
case NEs of web queries never occur in properly
capitalized news; this causes an NER tagger trained
on standard PoS to underpredict NEs (1?3% positive
rate).
The 2005 KDD Cup is a query topic categoriza-
tion task based on 800,000 queries (Li et al, 2005).5
We use a random subset of 2000 queries as a source
of web queries. By means of simple regular ex-
pressions we excluded from sampling queries that
looked like urls or emails (? 15%) as they are easy
to identify and do not provide a significant chal-
3A reviewer points out that we use the terms in-domain
and out-of-domain somewhat liberally. We simply use ?differ-
ent domain? as a short-hand for ?different distribution? without
making any claim about the exact nature of the difference.
4nltk.googlecode.com/svn/trunk/nltk data
5www.sigkdd.org/kdd2005/kddcup.html
lenge. We also excluded queries shorter than 10
characters (4%) and longer than 50 characters (2%)
to provide annotators with enough context, but not
an overly complex task. The annotation procedure
was carried out using Amazon Mechanical Turk. We
instructed workers to follow the CoNLL 2003 NER
guidelines (augmented with several examples from
queries that we annotated) and identify up to three
NEs in a short text and copy and paste them into a
box with associated multiple choice menu with the
4 CoNLL NE labels: LOC, MISC, ORG, and PER.
Five workers annotated each query. In a first round
we produced 1000 queries later used for develop-
ment. We call this set KDD-D. We then expanded
the guidelines with a few uncertain cases. In a sec-
ond round, we generated another 1000 queries. This
set will be referred to as KDD-T. Because annota-
tor agreement is low on a per-token basis (? = .30
for KDD-D, ? = .34 for KDD-T (Cohen, 1960)),
we remove queries with less than 50% agreement,
averaged over the tokens in the query. After this
filtering, KDD-D and KDD-T contain 777 and 819
queries, respectively. Most of the rater disagreement
involves the MISC NE class. This is not surprising
as MISC is a sort of place-holder category that is
difficult to define and identify in queries, especially
by untrained AMT workers. We thus replaced MISC
with the null label O. With these two changes, ? was
.54 on KDD-D and .64 on KDD-T. This is sufficient
for repeatable experiments.6
Table 2 shows the distribution of NE types in the
5 datasets. IEER has fewer NEs than CoNLL, KDD
has more. PER is about as prevalent in KDD as
in CoNLL, but LOC and ORG have higher percent-
ages, reflecting the fact that people search frequently
for locations and commercial organizations. These
differences between source domain (CoNLL) and
target domains (IEER, KDD) add to the difficulty
of cross-domain generalization in this case.
6 Experimental setup
Recall that the input features for a token w0 con-
sist of standard NER features (BASE and GAZ) and
features derived from the search result we obtain by
6The two KDD sets, along with additional statistics on an-
notator agreement requested by a reviewer, are available at
ifnlp.org/?schuetze/piggyback11.
970
running a search for w?1w0w1 (URL, LEX, BOW,
and MISC). Since the MISC NE class is not anno-
tated in IEER and has low agreement on KDD in
the experimental evaluation we focus on the four-
class (PER, LOC, ORG, O) NER problem on all
datasets. We use BIO encoding as in the original
CoNLL task (Sang and Meulder, 2003).
ALL LOC ORG PER
CoNLL
c1 l BASE GAZ 88.8? 91.9 77.9 93.0
c2 l GAZ URL BOW MISC 86.4? 90.7 74.0 90.9
c3 l BASE URL BOW MISC 92.3? 93.7 84.8 96.0
c4 l BASE GAZ BOW MISC 91.1? 93.3 82.2 94.9
c5 l BASE GAZ URL MISC 92.7? 94.9 84.5 95.9
c6 l BASE GAZ URL BOW 92.3? 94.2 84.4 95.8
c7 l BASE GAZ URL BOW MISC 93.0 94.9 85.1 96.4
c8 l BASE GAZ URL LEX BOW MISC 92.9 94.7 84.9 96.5
c9 c BASE GAZ 92.9 95.3 87.7 94.6
IEER
i1 l BASE GAZ 57.9? 71.0 37.7 59.9
i2 l GAZ URL LEX BOW MISC 63.8? 76.2 26.0 75.9
i3 l BASE URL LEX BOW MISC 64.9? 71.8 38.3 73.8
i4 l BASE GAZ LEX BOW MISC 67.3 76.7 41.2 74.6
i5 l BASE GAZ URL BOW MISC 67.8 76.7 40.4 75.8
i6 l BASE GAZ URL LEX MISC 68.1 77.2 36.9 77.8
i7 l BASE GAZ URL LEX BOW 66.6? 77.4 38.3 73.9
i8 l BASE GAZ URL LEX BOW MISC 68.1 77.4 36.2 78.0
i9 c BASE GAZ 68.6? 77.3 52.3 73.1
KDD-T
k1 l BASE GAZ 34.6? 48.9 19.2 34.7
k2 l GAZ URL LEX MISC 40.4? 52.1 15.4 50.4
k3 l BASE URL LEX MISC 40.9? 50.0 20.1 48.0
k4 l BASE GAZ LEX MISC 41.6? 55.0 25.2 45.2
k5 l BASE GAZ URL MISC 43.0 57.0 15.8 50.9
k6 l BASE GAZ URL LEX 40.7? 55.5 15.8 42.9
k7 l BASE GAZ URL LEX MISC 43.8 56.4 17.0 52.0
k8 l BASE GAZ URL LEX BOW MISC 43.8 56.5 17.4 52.3
Table 3: Evaluation results. l = text lowercased, c = orig-
inal capitalization preserved. ALL scores significantly
different from the best results for the three datasets (lines
c7, i8, k7) are marked ? (see text).
We use SuperSenseTagger (Ciaramita and Altun,
2006)7 as our NER tagger. It is a first-order con-
ditional HMM trained with the perceptron algo-
7sourceforge.net/projects/supersensetag
rithm (Collins, 2002), a discriminative model with
excellent efficiency-performance trade-off (Sha and
Pereira, 2003). The model is regularized by aver-
aging (Freund and Schapire, 1999). For all models
we used an appropriate development set for choos-
ing the only hyperparameter, T , the number of train-
ing iterations on the source data. T must be tuned
separately for each evaluation because different tar-
get domains have different overfitting patterns.
We train our NER system on an 80% sample of
the CoNLL data. For our in-domain evaluation, we
tune T on a 10% development sample of the CoNLL
data and test on the remaining 10%. For our out-of-
domain evaluation, we use the IEER and KDD test
sets. Here T is tuned on the corresponding develop-
ment sets. Since we do not train on IEER and KDD,
these two data sets do not have training set portions.
For each data set, we perform 63 runs, correspond-
ing to the 26?1 = 63 different non-empty combina-
tions of the 6 feature groups. We report average F1,
generated by five-trial training and evaluation, with
random permutations of the training data. We com-
pute the scores using the original CoNLL phrase-
based metric (Sang and Meulder, 2003). As a bench-
mark we use the baseline model with gazetteer fea-
tures (BASE and GAZ). The robustness of this sim-
ple approach is well documented; e.g., Turian et al
(2010) show that the baseline model (gazetteer fea-
tures without unsupervised features) produces an F1
of .778 against .788 of the best unsupervised word
representation feature.
7 Results and discussion
Table 3 summarizes the experimental results. In
each column, the best numbers within a dataset for
the ?lowercased? runs are bolded (see below for dis-
cussion of the ?capitalization? runs on lines c9 and
i9). For all experiments, we selected a subset of the
combinations of the feature groups. This subset al
ways includes the best results and a number of other
combinations where feature groups are added to or
removed from the optimal combination.
Results for the CoNLL test set show that the 5
feature groups without LEX achieve optimal per-
formance (line c7). Adding LEX improves perfor-
mance on PER, but decreases overall performance
(line c8). Removing GAZ, URL, BOW and MISC
971
from line c7, causes small comparable decreases in
performance (lines c3?c6). These feature groups
seem to have about the same importance in this ex-
perimental setting, but leaving out BASE decreases
F1 by a larger 6.6% (lines c7 vs c2).
The main result for CoNLL is that using piggy-
back features (line c7) improves F1 of a standard
NER system that uses only BASE and GAZ (line
c1) by 4.2%. Even though the emphasis of this pa-
per is on cross-domain robustness, we can see that
our approach also has clear in-domain benefits.
The baseline in line c1 is the ?lowercase? base-
line as indicated by ?l?. We also ran a ?capitalized?
baseline (?c?) on text with the original capitalization
preserved and PoS-tagged in this unchanged form.
Comparing lines c7 and c9, we see that piggyback
features are able to recover all the performance that
is lost when proper capitalization is unavailable. Lin
and Wu (2009) report an F1 score of 90.90 on the
original split of the CoNLL data. Our F1 scores
> 92% can be explained by a combination of ran-
domly partitioning the data and the fact that the four-
class problem is easier than the five-class problem
LOC-ORG-PER-MISC-O.
We use the t-test to compute significance on the
two sets of five F1 scores from the two experiments
that are being compared (two-tailed, p < .01 for t >
3.36).8 CoNLL scores that are significantly different
from line c7 are marked with ?.
For IEER, the system performs best for all six
feature groups (line i8). Runs significantly different
from i8 are marked ?. When URL, LEX and BOW
are removed from the set, performance does not de-
crease, or only slightly (lines i4, i5, i6), indicating
that these three feature groups are least important.
In contrast, there is significant evidence for the im-
portance of BASE, GAZ, and MISC: removing them
decreases performance by at least 1% (lines i2, i3,
i7). The large increase of ORG F1 when URL is
not used is surprising (41.2% on line i4, best per-
formance). The reason seems to be that URL fea-
tures (and LEX to a lesser extent) do not generalize
for ORG. Locations like Madrid in CoNLL are fre-
quently tagged ORG when they refer to sports clubs
like Real Madrid. This is rare in IEER and KDD.
8We make the assumption that the distribution of F1 scores
is approximately normal. See Cohen (1995), Noreen (1989) for
a discussion of how this affects the validity of the t-test.
Compared to standard NER (using feature groups
BASE and GAZ), our combined feature set achieves
a performance that is by more than 10% higher (lines
i8 vs i1). This demonstrates that piggyback features
have robust cross-domain generalization properties.
The comparison of lines i8 and i9 confirms that the
features effectively compensate for the lack of cap-
italization, and perform almost as well as (although
statistically worse than) a model trained on capital-
ized data.
The best run on KDD-D was the run with feature
groups BASE, GAZ, URL, LEX and MISC. On line
k7, we show results for this run for KDD-T and for
runs that differ by one feature group (lines k2?k6,
k8).9 The overall best result (43.8%) is achieved
when using all feature groups (line k8). Omitting
BOW results in the same score for ALL (line k7).
Apparently, the local LEX features already capture
most useful cooccurrence information and looking
at a wider window (as implemented by BOW) is of
limited utility. On lines k2?k6, performance gen-
erally decreases on ALL and the three NE classes
when dropping one of the five feature groups on line
k7. One notable exception is an increase for ORG
when feature group URL is dropped (line k4, 25.2%,
the best performance for ORG of all runs). This is in
line with our discussion of the same effect on IEER.
The key take-away from our results on KDD-T is
that piggyback features are again (as for IEER) sig-
nificantly better than standard feature groups BASE
and GAZ. Search engine based adaptation has an ad-
vantage of 9.2% compared to standard NER (lines
k7 vs k1). An F1 below 45% may not yet be good
enough for practical purposes. But even if additional
work is necessary to boost the scores further, our
model is an important step in this direction.
The low scores for KDD-T are also partially due
to our processing of the AMT data. Our selection
procedure is biased towards short entities whereas
CoNLL guidelines favor long NEs. We can address
this by forcing AMT raters to be more consistent
with the CoNLL guidelines in the future.
We summarize the experimental results as fol-
lows. Piggyback features consistently improve NER
for non-well-edited text when used together with
standard NER features. While relative improve-
9KDD-D F1 values were about 1% higher than for KDD-T.
972
ment due to piggyback features increases as out-
of-domain data become more different from the in-
domain training set, performance declines in abso-
lute terms from .930 (CoNLL) to .681 (IEER) and
.438 (KDD-T).
8 Conclusion
Robust cross-domain generalization is key in many
NLP applications. In addition to surface and linguis-
tic differences, differences in world knowledge pose
a key challenge, e.g., the fact that Java refers to a
location in one domain and to coffee in another. We
have proposed a new way of addressing this chal-
lenge. Because search engines attempt to make op-
timal use of the context a word occurs in, hits shown
to the user usually include other uses of the word in
semantically similar snippets. These snippets can be
used as a more robust and domain-independent rep-
resentation of the context of the word/phrase than
what is available in the input text.
Our first contribution is that we have shown that
this basic idea of using search engines for robust
domain-independent feature representations yields
solid results for one specific NLP problem, NER.
Piggyback features achieved an improvement of F1
of about 10% compared to a baseline that uses BASE
and GAZ features. Even in-domain, we were able
to get a smaller, but still noticeable improvement of
4.2% due to piggyback features. These results are
also important because there are many application
domains with noisy text without reliable capitaliza-
tion, e.g., automatically transcribed speech, tweets,
SMS, social communities and blogs.
Our second contribution is that we address a type
of NER that is of particular importance: NER for
web queries. The query is the main source of in-
formation about the user?s information need. Query
analysis is important on the web because under-
standing the query, including the subtask of NER, is
key for identifying the most relevant documents and
the most relevant ads. NER for domains like Twitter
and SMS has properties similar to web queries.
A third contribution of this paper is the release of
an annotated dataset for web query NER. We hope
that this dataset will foster more research on cross-
domain generalization and domain adaptation ? in
particular for NER ? and the difficult problem of
web query understanding.
This paper is about cross-domain generalization.
However, the general idea of using search to provide
rich context information to NLP systems is applica-
ble to a broad array of tasks. One of the main hurdles
that NLP faces is that the single context a token oc-
curs in is often not sufficient for reliable decisions,
be they about attachment, disambiguation or higher-
order semantic interpretation. Search makes dozens
of additional relevant contexts available and can thus
overcome this bottleneck. In the future, we hope to
be able to show that other NLP tasks can also benefit
from such an enriched context representation.
Future work. We used a web search engine in the
experiments presented in this paper. Latencies when
using one of the three main commercial search en-
gines Bing, Google and Yahoo! in our scenario range
from 0.2 to 0.5 seconds per token. These execution
times are prohibitive for many applications. Search
engines also tend to limit the number of queries per
user and IP address. To gain widespread acceptance
of the piggyback idea of using search results for ro-
bust NLP, we therefore must explore alternatives to
search engines.
In future work, we plan to develop more efficient
methods of using search results for cross-domain
generalization to avoid the cost of issuing a large
number of queries to search engines. Caching will
be of obvious importance in this regard. Another av-
enue we are pursuing is to build a specialized search
system for our application in a way similar to Ca-
farella and Etzioni (2005). While we need good
coverage of a large variety of domains for our ap-
proach to work, it is not clear how big the index
of the search engine must be for good performance.
Conceivably, collections much smaller than those in-
dexed by major search engines (e.g., the Google 1T
5-gram corpus or ClueWeb09) might give rise to fea-
tures with similar robustness properties. It is impor-
tant to keep in mind, however, that one of the key
factors a search engine allows us to leverage is the
notion of relevance which might not be always pos-
sible to model as accurately with other data.
Acknowledgments. This research was funded by
a Google Research Award. We would like to thank
Amir Najmi, John Blitzer, Richa?rd Farkas, Florian
Laws, Slav Petrov and the anonymous reviewers for
their comments.
973
References
Rie Kubota Ando. 2004. Exploiting unannotated cor-
pora for tagging and chunking. In ACL, Companion
Volume, pages 142?145.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of English web-search queries. In
EMNLP, pages 1021?1030.
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2010. A theory of learning from different
domains. Machine Learning, 79:151?175.
Thorsten Brants. 2000. TnT ? A statistical part-of-
speech tagger. In ANLP, pages 224?231.
Michael J. Cafarella and Oren Etzioni. 2005. A search
engine for natural language applications. In WWW,
pages 442?452.
Nancy A. Chinchor, editor. 1998. Proceedings of the
Seventh Message Understanding Conference. NIST.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Paul R. Cohen. 1995. Empirical methods for artificial
intelligence. MIT Press, Cambridge, MA, USA.
Michael Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In EMNLP-
CoNLL, pages 708?716.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In ACL, pages
168?175.
Richa?rd Farkas, Gyo?rgy Szarvas, and Ro?bert Orma?ndi.
2007. Improving a state-of-the-art named entity recog-
nition system using the world wide web. In Industrial
Conference on Data Mining, pages 163?172.
Tim Finin, Will Murnane, Anand Karandikar, Nicholas
Keller, Justin Martineau, and Mark Dredze. 2010.
Annotating named entities in twitter data with crowd-
sourcing. In NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 80?88.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Atsushi Fujita and Satoshi Sato. 2008. A probabilis-
tic model for measuring grammaticality and similar-
ity of automatically generated paraphrases of predicate
phrases. In COLING, pages 225?232.
Gregory Grefenstette, Yan Qu, and David A. Evans.
2004. Mining the web to create a language model
for mapping between English names and phrases and
Japanese. In Web Intelligence, pages 110?116.
Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009.
Named entity recognition in query. In SIGIR, pages
267?274.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Exploit-
ing Wikipedia as external knowledge for named entity
recognition. In EMNLP-CoNLL, pages 698?707.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1):1?31.
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechani-
cal turk. In NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechani-
cal Turk, pages 71?79.
Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005.
KDD CUP 2005 report: Facing a great challenge.
SIGKDD Explorations Newsletter, 7:91?99.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 1030?1038.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience.
Thierry Poibeau and Leila Kosseim. 2000. Proper name
extraction from non-journalistic texts. In CLIN, pages
144?157.
J. J. Rocchio. 1971. Relevance feedback in informa-
tion retrieval. In Gerard Salton, editor, The Smart Re-
trieval System ? Experiments in Automatic Document
Processing, pages 313?323. Prentice-Hall.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In WWW, pages 377?386.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL 2003 Shared Task, pages 142?147.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
974
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology - Volume 1, pages 134?141.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL, pages
384?394.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL, pages 417?424.
Jinxi Xu and W. Bruce Croft. 1996. Query expansion
using local and global document analysis. In SIGIR,
pages 4?11.
Alexander Yates, Stefan Schoenmackers, and Oren Et-
zioni. 2006. Detecting parser errors using web-based
semantic filters. In EMNLP, pages 27?34.
Meliha Yetisgen-Yildiz, Imre Solti, Fei Xia, and
Scott Russell Halgrim. 2010. Preliminary experience
with Amazon?s Mechanical Turk for annotating medi-
cal named entities. In NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, pages 180?183.
975
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 258?262
Manchester, August 2008
DeSRL: A Linear-Time Semantic Role Labeling System
Massimiliano Ciaramita
??
massi@yahoo-inc.com
Felice Dell?Orletta
?
dellorle@di.unipi.it
?: Yahoo! Research Barcelona, Ocata 1, 08003, Barcelona, Catalunya, Spain
?: Dipartimento di Informatica, Universit`a di Pisa, L. B. Pontecorvo 3, I-56127, Pisa, Italy
?: Barcelona Media Innovation Center, Ocata 1, 08003, Barcelona, Catalunya, Spain
Giuseppe Attardi
?
attardi@di.unipi.it
Mihai Surdeanu
?,?
mihai.surdeanu@barcelonamedia.org
Abstract
This paper describes the DeSRL sys-
tem, a joined effort of Yahoo! Research
Barcelona and Universit`a di Pisa for the
CoNLL-2008 Shared Task (Surdeanu et
al., 2008). The system is characterized by
an efficient pipeline of linear complexity
components, each carrying out a different
sub-task. Classifier errors and ambigui-
ties are addressed with several strategies:
revision models, voting, and reranking.
The system participated in the closed chal-
lenge ranking third in the complete prob-
lem evaluation with the following scores:
82.06 labeled macro F1 for the overall task,
86.6 labeled attachment for syntactic de-
pendencies, and 77.5 labeled F1 for se-
mantic dependencies.
1 System description
DeSRL is implemented as a sequence of compo-
nents of linear complexity relative to the sentence
length. We decompose the problem into three sub-
tasks: parsing, predicate identification and clas-
sification (PIC), and argument identification and
classification (AIC). We address each of these sub-
tasks with separate components without backward
feedback between sub-tasks. However, the use of
multiple parsers at the beginning of the process,
and re-ranking at the end, contribute beneficial
stochastic aspects to the system. Figure 1 summa-
rizes the system architecture. We detail the parsing
?
All authors contributed equally to this work.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
sub-task in Section 2 and the semantic sub-tasks
(PIC and AIC) in Section 3.
2 Parsing
In the parsing sub-task we use a combination strat-
egy on top of three individual parsing models,
two developed in-house ?DeSR
left?to?right
and
DeSR
revision
right?to?left
? and a third using an off-the-
shelf parser, Malt 1.0.0
1
.
2.1 DeSR
left?to?right
This model is a version of DeSR (Attardi, 2006),
a deterministic classifier-based Shift/Reduce
parser. The parser processes input tokens advanc-
ing on the input from left to right with Shift ac-
tions and accumulates processed tokens on a stack
with Reduce actions. The parser has been adapted
for this year?s shared task and extended with addi-
tional classifiers, e.g., Multi Layer Perceptron and
multiple SVMs.
2
The parser uses the following features:
1. SPLIT LEMMA: from tokens ?1, 0, 1, prev(0),
leftChild(0), rightChild(0)
2. PPOSS: from ?2, ?1, 0, 1, 2, 3, prev(0), next(?1),
leftChild(?1), leftChild(0), rightChild(?1),
rightChild(0)
3. DEPREL: from leftChild(?1), leftChild(0),
rightChild(?1)
4. HDIST: from ?1, 0
In the above list negative numbers refer to tokens
on the stack, positive numbers to tokens in the in-
put queue. We use the following path operators:
leftChild(x) refers to the leftmost child of token
x, rightChild(x) to the rightmost child of token
x, prev(x) and next(x) respectively to the token
preceding or following x in the sentence.
1
http://w3.msi.vxu.se/
?
nivre/research/
MaltParser.html
2
This parser is available for download at: http://
sourceforge.net/projects/desr/.
258
Voting PIC AIC Reranking
Argument
Frame
DeSR left?to?right
Malt
DeSR right?to?leftrevision OutputInput
Figure 1: DeSRL system architecture.
The first three types of features are directly ex-
tracted from the attributes of tokens present in the
training corpus. The fourth feature represents the
distance of the token to the head of the noun phrase
to which it belongs, or ?O? if it does not belong to
a noun phrase. This distance is computed with a
simple heuristic, based on a pattern of POS tags.
Attardi and Dell?Orletta (2008) have shown that
this feature improves the accuracy of a shift/reduce
dependency parser by providing approximate in-
formation about NP chunks in the sentence. In fact
no token besides the head of a noun phrase can
have a head referring to a token outside the noun
phrase. Hence the parser can learn to avoid creat-
ing such links. The addition of this feature yields
an increase of 0.80% in Labeled Accuracy on the
development set.
2.2 Revision Parser: DeSR
revision
right?to?left
Our second individual parsing model implements
an alternative to the method of revising parse trees
of Attardi and Ciaramita (2007) (see also (Hall &
Novak, 2005)). The original approach consisted in
training a classifier to revise the errors of a base-
line parser. The approach assumed that only lo-
cal revisions to the parse tree would be needed,
since the dependency parser mostly gets individual
phrases correctly. The experiments showed that in-
deed most of the corrections can be expressed by
a small set of (about 20) complex movement rules.
Furthermore, there was evidence that one could get
higher improvements from the tree revision classi-
fier if this was trained on the output of a lower ac-
curacy parser. The reason for this is that the num-
ber of errors is higher and this provides a larger
amount of training data.
For the CoNLL 2008 shared task, we refined this
idea, but instead of using an independent classi-
fier for the revision, we use the parser itself. The
second parser is trained on the original corpus ex-
tended with dependency information predicted by
a lower accuracy parser. To obtain the base parser
we use DeSR trained on half the training corpus
using a Maximum Entropy (ME) classifier. The
ME classifier is considerably faster to train but has
a lower accuracy: this model achieved an LAS of
76.49% on the development set. Using the out-
put of the ME-based parser we extend the original
corpus with four additional columns: the lemma
of the predicted head (PHLEMMA), the PPOSS of
the predicted head (PHPPOSS), the dependency of
the predicted head (PHDEPREL), and the indica-
tion of whether a token appears before or after its
predicted head. A second parser is trained on this
corpus, scanning sentences from right to left and
using the following additional features:
1. PHPPOSS: from ?1, 0
2. PHLEMMA: from ?1, 0
3. PHDEPREL: from ?1, 0
4. PHHDIST: from 0
Performing parsing in reverse order helps reduce
several of the errors that a deterministic parser
makes when dependency links span a long distance
in the input sequence. Experiments on the CoNLL
2007 corpora (Dell?Orletta, 2008) have shown that
this indeed occurs, especially for distances in the
range from 6 to 23. In particular, the most signifi-
cant improvements are for dependencies with label
COORD (+ 6%) and P (+ 8%).
The revision parser achieves an LAS of 85.81%
on the development set. Note that the extra fea-
tures from the forward parser are indeed use-
ful, since a simple backward parser only achieves
82.56% LAS on the development set.
2.3 Parser Combination
The final step consists in combining the out-
puts of the three individual models a simple
voting scheme: for each token we use major-
ity voting to select its head and dependency la-
bel. In case of ties, we chose the dependency
predicted by our overall best individual model
(DeSR
revision
right?to?left
).
3
Note that typical approaches to parser
combination combine the outputs of inde-
pendent parsers, while in our case one base
model (DeSR
revision
right?to?left
) is trained with
3
We tried several voting strategies but none performed bet-
ter.
259
information predicted by another individual
model(DeSR
left?to?right
). To the best of our
knowledge, combining individual parsing models
that are inter-dependent is novel.
3 Semantic Role Labeling
We implement the Semantic Role Labeling (SRL)
problem using three components: PIC, AIC, and
reranking of predicted argument frames.
3.1 Predicate Identification and Classification
The PIC component carries out the identification
of predicates, as well as their partial disambigua-
tion, and it is implemented as a multiclass average
Perceptron classifier (Crammer & Singer, 2003).
For each token i we extract the following features
(?, ? stands for token combination):
1. SPLIT LEMMA: from ?i?1, i?, i?1, i, i+1, ?i, i+1?
2. SPLIT FORM: from i? 2, i? 1, i, i+ 1.i+ 2
3. PPOSS: from ?i?2, i?1?, ?i?1, i?, i?1, i, i+1, ?i, i+
1?, ?i+ 1, i+ 2?
4. WORD SHAPE: e.g., ?Xx*? for ?Brazil?, from ?i?2, i?
1, i?, ?i? 1, i?, i? 1, i, i+1, ?i, i+1?, ?i, i+1, i+2?
5. Number of children of node i
6. For each children j of i: split lemma
j
, pposs
j
,
deprel
i,j
, ?split lemma
i
, split lemma
j
?, ?pposs
i
,
pposs
j
?
7. Difference of positions: j ? i, for each child j of i.
The PIC component uses one single classifier map-
ping tokens to one of 8 classes corresponding to
the rolesets suffixes 1 to 6, the 6 most frequent
types, plus a class grouping all other rolesets, and
a class for non predicates; i.e., Y = {0, 1, 2, .., 7}.
Each token classified as y
7
is mapped by default to
the first sense y
1
. This approach is capable of dis-
tinguishing between different predicates based on
features 1 and 2, but it can also exploit information
that is shared between predicates due to similar
frame structures. The latter property is intuitively
useful especially for low-frequency predicates.
The classifier has an accuracy in the multiclass
problem, considering also the mistakes due to the
non-predicted classes, of 96.2%, and an F-score of
92.7% with respect to the binary predicate iden-
tification problem. To extract features from trees
(5-7) we use our parser?s output on training, devel-
opment and evaluation data.
3.2 Argument Identification and
Classification
Algorithm 1 describes our AIC framework. The al-
gorithm receives as input a sentence S where pred-
icates have been identified and classified using the
Algorithm 1: AIC
input : sentence S; inference strategy I; model w
foreach predicate p in S do
set frame F
in
= {}
foreach token i in S do
if validCandidate(i) then
?
y = arg max
y?Y
score(?(p, i),w, y)
if
?
y 6= nil then
add argument (i,
?
y) to F
in
F
out
= inference(F
in
, I)
output: set of all frames F
out
PIC component, an inference strategy I is used
to guarantee that the generated best frames satisfy
the domain constraints, plus an AIC classification
model w. We learn w using a multiclass Percep-
tron, using as output label setY all argument labels
that appear more than 10 times in training plus a nil
label assigned to all other tokens.
During both training and evaluation we se-
lect only the candidate tokens that pass the
validCandidate filter. This function requires that
the length of the dependency path between pred-
icate and candidate argument be less than 6, the
length of the dependency path between argument
and the first common ancestor be less than 3, and
the length of the dependency path between the
predicate and the first common ancestor be less
than 5. This heuristic covers over 98% of the ar-
guments in training.
In the worst case, Algorithm 1 has quadratic
complexity in the sentence size. But, on average,
the algorithm has linear time complexity because
the number of predicates per sentence is small (av-
eraging less than five for sentences of 25 words).
The function ? generates the feature vector for
a given predicate-argument tuple. ? extracts the
following features from a given tuple of a predicate
p and argument a:
1. token(a)
4
, token(modifier of a) if a is the
head of a prepositional phrase, and token(p).
2. Patterns of PPOSS tags and DEPREL labels
for: (a) the predicate children, (b) the children
of the predicate ancestor across VC and IM
dependencies, and (c) the siblings of the same
ancestor. In all paths we mark the position of
p, a and any of their ancestors.
3. The dependency path between p and a. We
add three versions of this feature: just the
4
token extracts the split lemma, split form, and PPOSS
tag of a given token.
260
path, and the path prefixed with p and a?s
PPOSS tags or split lemmas.
4. Length of the dependency path.
5. Distance in tokens between p and a.
6. Position of a relative to p: before or after.
We implemented two inference strategies:
greedy and reranking. The greedy strategy sorts
all arguments in a frame F
in
in descending order
of their scores and iteratively adds each argument
to the output frame F
out
only if it respects the do-
main constraints with the other arguments already
selected. The only domain constraint we use is that
core arguments cannot repeat.
3.3 Reranking of Argument Frames
The reranking inference strategy adapts the ap-
proach of Toutanova et al (2005) to the depen-
dency representation with notable changes in can-
didate selection, feature set, and learning model.
For candidate selection we modify Algorithm 1:
instead of storing only y? for each argument in F
in
we store the top k best labels. Then, from the ar-
guments in F
in
, we generate the top k frames with
the highest score, where the score of a frame is the
product of all its argument probabilities, computed
as the softmax function on the output of the Per-
ceptron. In this set of candidate frames we mark
the frame with the highest F
1
score as the positive
example and all others as negative examples.
From each frame we extract these features:
1. Position of the frame in the set ordered by
frame scores. Hence, smaller positions in-
dicate candidate frames that the local model
considered better (Marquez et al, 2007).
2. The complete sequence of arguments and
predicate for this frame (Toutanova, 2005).
We add four variants of this feature: just the
sequence and sequence expanded with: (a)
predicate voice, (b) predicate split lemma,
and (c) combination of voice and split lemma.
3. The complete sequence of arguments and
predicate for this frame combined with their
PPOSS tags. Same as above, we add four
variants of this feature.
4. Overlap with the PropBank or NomBank
frame for the same predicate lemma and
sense. We add the precision, recall, and F
1
score of the overlap as features (Marquez et
al., 2007).
5. For each frame argument, we add the features
from the local AIC model prefixed with the
WSJ + Brown WSJ Brown
Labeled macro F
1
82.69 83.83 73.51
LAS 87.37 88.21 80.60
Labeled F
1
78.00 79.43 66.41
Table 1: DeSRL results in the closed challenge,
for the overall task, syntactic dependencies, and
semantic dependencies.
Devel WSJ Brown
DeSR
left?to?right
85.61 86.54 79.74
DeSR
revision
right?to?left
85.81 86.19 78.91
MaltParser 84.10 85.50 77.06
Voting 87.37 88.21 80.60
Table 2: LAS of individual and combined parsers.
corresponding argument label in the current
frame (Toutanova, 2005).
The reranking classifier is implemented as multi-
layer perceptron with one hidden layer of 5 units,
trained to solve a regression problem with a least
square criterion function. Previously we experi-
mented, unsuccessfully, with a multiclass Percep-
tron and a ranking Perceptron. The limited number
of hidden units guarantees a small computational
overhead with respect to a linear model.
4 Results and Analysis
Table 1 shows the overall results of our system
in the closed challenge. Note that these scores
are higher than those of our submitted run mainly
due to improved parsing models (discussed be-
low) whose training ended after the deadline. The
score of the submitted system is the third best
for the complete task. The system throughput in
our best configuration is 28 words/second, or 30
words/second without reranking. In exploratory
experiments on feature selection for the re-ranking
model we found that several features classes do
not contribute anything and could be filtered out
speeding up significantly this last SRL step. Note
however that currently over 90% of the runtime is
occupied by the syntactic parsers? SVM classifiers.
We estimate that we can increase throughput one
order of magnitude simply by switching to a faster,
multiclass classifier in parsing.
4.1 Analysis of Parsing
Table 2 lists the labeled attachment scores (LAS)
achieved by each parser and by their combination
on the development set, the WSJ and Brown test
sets. The results are improved with respect to the
official run, by using a revision parser trained on
the output of the lower accuracy ME parser, as
261
Labeled F
1
Unlabeled F
1
Syntax PIC Inference Devel WSJ Brown Devel WSJ Brown
gold gold greedy 88.95 90.21 84.95 93.71 94.34 93.29
predicted gold greedy 85.96 86.70 78.68 90.60 90.98 88.02
predicted predicted greedy 79.88 79.27 66.41 86.07 85.33 80.14
predicted predicted reranking 80.13 79.43 66.41 86.33 85.62 80.41
Table 3: Scores of the SRL component under various configurations.
Devel WSJ Brown
Unlabeled F
1
92.69 90.88 86.96
Labeled F
1
(PIC) 87.29 84.87 71.99
Labeled F
1
(Sense 1) 79.62 78.94 70.11
Table 4: Scores of the PIC component.
mentioned earlier. These results show that vot-
ing helps significantly (+1.56% over the best single
parser) even though inter-dependent models were
used. However, our simple voting scheme does
not guarantee that a well-formed tree is generated,
leaving room for further improvements; e.g., as
in (Sagae & Lavie, 2006).
4.2 Analysis of SRL
Table 3 shows the labeled and unlabeled F
1
scores
of our SRL component as we move from gold to
predicted information for syntax and PIC. For the
shared task setting ?predicted syntax and predicted
PIC? we show results for the two inference strate-
gies implemented: greedy and reranking. The first
line in the table indicates that the performance of
the SRL component when using gold syntax and
gold PIC is good: the labeled F
1
is 90 points for the
in-domain corpus and approximately 85 points for
the out-of-domain corpus. Argument classification
suffers the most on out-of-domain input: there is
a difference of 5 points between the labeled scores
on WSJ and Brown, even though the correspond-
ing unlabeled scores are comparable.
The second line in the table replicates the setup
of the 2005 CoNLL shared task: predicted syntax
but gold PIC. This yields a moderate drop of 3 la-
beled F
1
points on in-domain data and a larger drop
of 6 points for out-of-domain data.
We see larger drops when switching to predicted
PIC (line 3): 5-6 labeled F
1
points in domain and
12 points out of domain. This drop is caused by the
PIC component, e.g., if a predicate is missed the
whole frame is lost. Table 4 lists the scores of our
PIC component, which we compare with a base-
line system that assigns sense 1 to all identified
predicates. The table indicates that, even though
our disambiguation component improves signifi-
cantly over the baseline, it performs poorly, espe-
cially on out-of-domain data. Same as SRL, the
classification sub-task suffers the most out of do-
main (there is a difference of 15 points between
unlabeled and labeled F
1
scores on Brown).
Finally, the reranking inference strategy yields
only modest improvements (last line in Table 3).
We attribute these results to the fact that, unlike
Toutanova et al (2005), we use only one tree to
generate frame candidates, hence the variation in
the candidate frames is small. Considering that the
processing overhead of reranking is already large
(it quadruples the runtime of our AIC component),
we do not consider reranking a practical extension
to a SRL system when processing speed is a dom-
inant requirement.
References
G. Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc.
of CoNNL-X 2006.
G. Attardi and M. Ciaramita. 2007. Tree Revi-
sion Learning for Dependency Parsing. In Proc. of
NAACL/HLTC 2007.
G. Attardi, F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc. of Workshop on Partial
Parsing.
K. Crammer and Y. Singer. 2003. Ultraconservative
Online Algorithms for Multiclass Problems. Journal
of Machine Learning Research 3: pp.951-991.
F. Dell?Orletta. 2008. Improving the Accuracy of De-
pendency Parsing. PhD Thesis. Dipartimento di In-
formatica, Universit`a di Pisa, forthcoming.
K. Hall and V. Novak. 2005. Corrective Modeling
for Non-Projective Dependency Parsing. In Proc. of
IWPT.
L. Marquez, L. Padro, M. Surdeanu, and L. Villarejo.
2007. UPC: Experiments with Joint Learning within
SemEval Task 9. In Proc. of SemEval 2007.
K. Sagae and A. Lavie. 2006. Parser Combination by
reparsing. In Proc. of HLT/NAACL.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez
and J. Nivre. 2008. The CoNLL-2008 Shared Task
on Joint Parsing of Syntactic and Semantic Depen-
dencies. In Proc. of CoNLL-2008.
K. Toutanova, A. Haghighi, and C. Manning. 2005.
Joint Learning Improves Semantic Role Labeling. In
Proc. of ACL.
262
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 1?7,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Adaptive Parameters for Entity Recognition with Perceptron HMMs
Massimiliano Ciaramita?
Google
Zu?rich, Switzerland
massi@google.com
Olivier Chapelle
Yahoo! Research
Sunnyvale, CA, USA
chap@yahoo-inc.com
Abstract
We discuss the problem of model adapta-
tion for the task of named entity recog-
nition with respect to the variation of la-
bel distributions in data from different do-
mains. We investigate an adaptive exten-
sion of the sequence perceptron, where the
adaptive component includes parameters
estimated from unlabelled data in combi-
nation with background knowledge in the
form of gazetteers. We apply this idea
empirically on adaptation experiments in-
volving two newswire datasets from dif-
ferent domains and compare with other
popular methods such as self training and
structural correspondence learning.
1 Introduction
Model adaptation is a central problem in learning-
based natural language processing. In the typical
setting a model is trained on annotated in domain,
or source, data, and is used on out of domain, or
target, data. The main difference with respect to
similar problems such as semi-supervised learning
is that source and target data are not assumed to
be drawn from the same distribution, which might
actually differ in relevant distributional properties:
topic, domain, genre, style, etc. In some formu-
lations of the problem a few target labeled data is
assumed to be available (Daume? III, 2007). How-
ever, we are interested in the case in which no la-
beled data is available from the target domain ?
except for evaluation purposes and fine tuning of
hyperparameters.
Most of the work in adaptation has focused
so far on the input side; e.g, proposing solutions
based on generating shared source-target represen-
tations (Blitzer et al, 2006). Here we focus in-
stead on the output aspect. We hypothesize that
?This work was carried out while the first author was
working at Yahoo! Research Barcelona.
part of the loss incurred in using a model out of
domain is due to its built-in class priors which
do not match the class distribution in the target
data. Thus we attempt to explicitly correct the
prediction of a pre-trained model for a given la-
bel by taking into account a noisy estimate of the
label frequency in the target data. The correc-
tion is carried out by means of adaptive param-
eters, estimated from unlabelled target data and
background ?world knowledge? in the form of
gazetteers, and taken in consideration in the de-
coding phase. We built a suitable dataset for exper-
imenting with different adaptation approaches for
named entity recognition (NER). The main find-
ings from our experiments are as follows. First,
the problem is challenging and only marginal im-
provements are possible under all evaluated frame-
works. Second, we found that our method com-
pares well with current state-of-the-art approaches
such as self training and structural correspondence
learning (McClosky et al, 2006; Blitzer et al,
2006) and taps on an interesting aspect which
seems worth of further research. Although we
concentrate on a segmentation task within a spe-
cific framework, the perceptron HMM introduced
by Collins (2002), we speculate that the same in-
tuition could be straightforwardly applied in other
learning frameworks (e.g., Support Vector Ma-
chines) and different tasks (e.g., standard classi-
fication).
2 Related work
Recent work in domain adaptation has focused
on approaches such as self-training and struc-
tural correspondence learning (SCL). The former
approach involves adding self-labeled data from
the target domain produced by a model trained
in-domain (McClosky et al, 2006). The latter
approach focuses on ways of generating shared
source-target representations based on good cross-
domain (pivot) features (Blitzer et al, 2006) (see
1
also (Ando, 2004)). Self training has proved ef-
fective in syntactic parsing, particularly in tan-
dem with discriminative re-ranking (Charniak and
Johnson, 2005), while the SCL has been applied
successfully to tasks such PoS tagging and opin-
ion analysis (Blitzer et al, 2006; Blitzer et al,
2007). We address a different aspect of the adapta-
tion problem, namely the difference in label distri-
butions between source and target domains. Chan
and Ng (2006) proposed correcting the class priors
for domain adaptation purposes in a word sense
disambiguation task. They adopt a generative
framework where the base model is a naive Bayes
classifier and priors are re-estimated with EM. The
approach proposed by Chelba and Acero (2004) is
also related as they propose a MAP adaptation via
Gaussian priors of a MaxEnt model for recovering
the correct capitalization of text.
Domain adaptation naturally invokes the exis-
tence of a specific task and data. As such it is
natural to consider the modeling aspects within
the context of a specific application. Here we
focus on the problem of named entity recogni-
tion (NER). There is still little work on adapta-
tion for NER. Ando (2004) reports successful ex-
periments on adapting with an SCL-like approach,
while Ciaramita and Altun (2005) effectively used
external knowledge in the form of gazetteers in
a semi-Markov model. Mika et al (2008) used
Wikipedia to generate additional training data for
domain adaptation purposes.
3 Problem statement
Named entity taggers detect mentions of instances
of pre-defined categories such as person (Per),
location (Loc), organization (Org) and miscel-
laneous (Misc). The problem can be naturally
framed as a segmentation and labeling task. State
of the art systems, e.g., based on sequential op-
timization, achieve excellent accuracy in domain.
However, accuracy degrades if the target data di-
verges in relevant distributional aspects from the
source. As an example, the following is the out-
put of a perceptron HMM1 trained on the CoNLL
2003 English data (news) (Sang and Muelder,
2003) when applied to a molecular biology text:2
1We used the implementation available from http:
//sourceforge.net/projects/supersensetag,
more details on this tagger can be found in (Ciaramita and
Altun, 2006).
2The same model achieves F-scores well in excess of 90%
evaluated in domain.
(1) Cdc2-cyclin Org B-activated Polo-like Misc
kinase specifically phosphorylates at least three
components of APC Org .
The tagger predicts several CoNLL entities which
are unlikely to occur in that context. One source
of confusion is probably the shape of words, in-
cluding case, numbers, and non alphabetical char-
acters, which are also typical, and thus mislead-
ing, of unrelated CoNLL entities. However, we
argue that the problem is partially due to the pa-
rameters learned which reflect the distribution of
classes in the source data. The parameter, acting
as biased priors, lead the tagger to generate inap-
propriate distributions of labels. We propose that
this aspect of the problem might be alleviated by
correcting the score for each class with an estimate
of the class frequency in the target data. Thus,
with respect to the example, we would like to de-
crease the score of ?Org? labels according to their
expected frequency in a molecular biology corpus.
4 A perceptron with adjustable priors
As generic taggers we adopt perceptron-trained
HMMs (Collins, 2002) which have excellent ef-
ficiency/performance trade-off (Nguyen and Guo,
2007). The objective of learning is a discrimi-
nant F : X ? Y ? IR, where Y denotes se-
quences of labels from a pre-defined set of cate-
gories Y . F (x,y;?) = ??,?(x,y)? is linear in
a feature representation ? defined over a joint in-
put/output space,3 a global feature representation
mapping each (x,y) pair to a vector of feature
counts ?(x,y) ? IRd:
[?(x,y)]i =
|y|?
j=1
?i(yj?1, yj ,x), (2)
where ?i is a (binary) predicate. Given an input
sequence x, we find the optimal label sequence,
f(x;?) = arg maxy?Y F (x,y;?), with Viterbi
decoding. The model ? is learned with the per-
ceptron algorithm.
Each feature represents a spelling or contex-
tual property, or the previous label. The sim-
plest baseline (model B) uses the features listed
in the upper half of Table 1. In previous work
on NER adaptation, Ciaramita and Altun (2005)
found that gazetteers, in combination with semi-
Markov models, significantly improved adapta-
tion. Similarly, we define additional features using
3?u,v? denoting the inner product between u and v.
2
Model B features
Feature example token feature value(s) Position
Lowercase word Pierre pierre i-1, i, i+1
Part of Speech Pierre NNP i-1, i, i+1
Word Shape Pierre Xx i-1, i, i+1
Suffix2/3 Pierre {re, rre} i
Prefix2/3 Pierre {pi, pie} i
Previous label Vinken (in ?Pierre Vinken?) B-PER (label on ?Pierre?) i
Additional features of model BG
Feature example token feature value(s) Position
InGazetteer Islands (in ?Cayman Islands?) I-Country2 (inside a 2-word country name) i-1, i, i+1
Most frequent supersense Eve B-Per1 (1 token Person label) i
2 most frequent supersenses Eve B-Per-Time1 (1 token Person/Time label) i
Number of supersenses Eve B-NSS41 i
Table 1. Feature list and examples. The upper half lists the features for the baseline tagger (B), the lower half
lists the additional features extracted from the gazetteers included to the second non-adapted tagger (BG). The
last number on the feature indicates the length of the entry in the list; e.g., ?Islands? in the example is the end of
a two-word item, in the country gazetteer, because of ?Cayman Islands?. The remaining features capture the most
frequent Wordnet supersense of the word, the first and second most frequent supersenses, and the total number of
supersenses.
the gazetteers from GATE,4 (Cunningham et al,
2002) namely, countries, person first/last names,
trigger words; and also from Wordnet: using the
lexicographers or supersense labels; and a list of
company names from Fortune 500. For this sec-
ond baseline (model BG) we also extract the fea-
tures in the bottom half of Table 1.
4.1 Decoding with external priors
In our method training is performed on the source
data using the perceptron algorithm. Adaptation
takes place at decoding time, when the score of
the entity labels is adjusted according to a k-
dimensional parameter vector ?, k = |Y |, esti-
mated by comparing the source and the unlabeled
target data. The score of a sequence y? for input x
in the target domain is computed with a variant of
the original discriminant:
F ?(x,y;?) =
|y|?
j=1
(
d?
i=1
?i(yj?1, yj ,x)?i
)
+ ??yj (3)
where ?yj is the adaptive parameter associated
with yj , and ? is a scaling factor. The new predic-
tion for x is f ?(x;?) = arg maxy?Y F ?(x,y;?).
5 Adaptive parameters
5.1 Theta
The vector ? encodes information about the ex-
pected difference in frequency of each cate-
gory between source and target. Let gQ(c) =
4http://www.gate.ac.uk/.
count(c,Q)P
c? count(c
?,Q) be an estimate of the relative fre-
quency of class c in corpus Q. We propose to for-
mulate ?c as:
?c =
gT (c)? gS(c)
gS(c)
(4)
where T and S are, respectively, the source and
target data. This is the ratio of the difference be-
tween in and out domain relative frequencies for
class c, with respect to the in domain frequency.
Intuitively, gS(c) represents an estimate of the fre-
quency of c in the source S, and ?c an estimate
of the expected decrease/increase as a fraction of
the initial guess; ?c is negative if class c is less
frequent in the target data than in the source data,
and positive otherwise. From this, it is clear that
equation (3) will offset the scores in the desired
direction.
A crucial issue is the estimation of count(c,Q),
a guess of the frequency of c in Q. A simple
solution could be to count directly the class fre-
quencies from the labeled source data, and to ob-
tain a noisy estimate on the target data by count-
ing the occurrence of entities that have known la-
bels in the source data. This approach unfortu-
nately works very badly for at least two reasons.
First, the number of entities in each class reflects
the frequency of the class in the source. There-
fore using lists of entities from the source as prox-
ies for the class in the target data can transfer the
source bias to the target. Second, entities can have
different senses in different domains; e.g., several
English city names occur in the Wall Street Jour-
nal as locations (Liverpool, Manchester, etc.) and
3
Attribute CoNLL BBN-4
# tokens 300K 1.046M
Source Reuters Wall Street Journal
Domain General news Financial
Years 1992 1987
# entities 34,841 58,637
Loc 30.48% 22.51%
Per 28.58% 20.08%
Org 26.55% 46.27%
Misc 14.38% 10.41%
Table 2. BBN and CoNLL datasets.
in Reuters news as both locations and organiza-
tions (football clubs). We propose to use lists of
words which are strongly associated with entities
of specific classes but are extracted from an inde-
pendent third source. In this way, we hope the bias
they carry will be transferred in similar ways to
both source and target. Similarly, potential am-
biguities should be randomly distributed between
source and target. Thus, as a first approximation,
we propose that given a list of words Lc, suppos-
edly related to c and generated independently from
source and target, count(c,Q) can be defined as:
count(c,Q) ?
?
w?Lc
count(w,Q) (5)
5.2 Tau
The scalar ? needs to be large enough to revise
the decision of the base model, if necessary. How-
ever, ? should not be too large, otherwise the best
prediction of the base model would be ignored.
In order for ? to have an effective, but balanced,
magnitude we introduce a simple notion of mar-
gin. Let the score of a given label ys on token s
be: G(x, ys;?) =
?d
i=1 ?i(ys?1, ys,x)?i, and let
y?s = arg maxy?Y G(x, y;?), we define the mar-
gin on s as:
Ms ? min
ys 6=y?s
(G(x, y?s;?)?G(x, ys;?)). (6)
The mean of M provides a rough quantification
of the necessary amount by which we need to
offset the scores G(x, ys;?) in order to change
the predictions. As a first guess, we take ? =
?(MS) = 1|S|
?|S|
s Ms, which we interpret as an
upper bound on the desired value of ? . While
experimenting on the development data we found
that ? /2 yields good results.
6 Experimental setup
6.1 Data
We used two datasets for evaluation. The first
is the English CoNLL 2003 dataset (Sang and
Muelder, 2003), a corpus of Reuters news an-
notated with person, location, organization and
miscellaneous entity tags. The second is the
BBN corpus (BBN, 2005), which supplements the
WSJ Penn TreeBank with annotation for 105 cat-
egories: named entities, nominal entities and nu-
meric types. We made the two datasets ?seman-
tically? compatible as follows. We tagged a large
collection of text from the English Wikipedia with
CoNLL and BBN taggers. We counted the fre-
quencies of BBN/CoNLL tag pairs for the same
strings, and assigned each BBN tag the most fre-
quent CoNLL tag;5 e.g.,
BBN tag CoNLL tag
Work of art:Book ? Misc
Organization:Educational ? Org
Location:Continent ? Loc
Person ? Per
48 BBN-to-CoNLL pairs were labelled in this
way. Remaining categories, e.g., descriptive and
numerical types, were mapped to the Outside tag
as they are not marked in CoNLL. Finally, we sub-
stituted all tags in the BBN corpus with the corre-
sponding CoNLL tag, we call this corpus BBN-
4. The data is summarized in Table 2. Notice
the different label distributions: the BBN-4 data
is characterized by a skewed distribution of labels
with organization by far the most frequent class,
while the CoNLL data has a more uniform dis-
tribution with location as the most frequent class.
The CoNLL data was randomly split in three dis-
joint sets of sentences for training (16,540 sen-
tences), development (2.068) and test (2,136). For
BBN-4 we used WSJ sections 2-21 for training
(39,823), section 22 for development (1,700) and
section 23 for test (2,416). We evaluated models in
both directions; i.e., swapping CoNLL and BBN-4
as source/target.
6.2 Model tuning
We regularize the perceptrons by averaging (Fre-
und and Schapire, 1999). The perceptron HMM
5A simpler approach might that of manually mapping the
two tagsets, however a number of cases that are not trivial to
resolve emerges in this way. For this reason we decided to
adopt the described data-driven heuristic approach.
4
has only one hyper-parameter, the number of train-
ing iterations (or epochs). Models trained for ap-
plication out of domain can benefit from early
stopping which provides an additional mean of
regularization. For all models compared we used
the development sets for choosing the number of
epochs for training the perceptron on the source
data. This is an important step as different adapta-
tion approaches yield different overfitting pattern
and it is important to control for this factor for a
fair comparison. As an example, we found that
the self-training models consistently overfit after
just a few iterations after which performance has a
steep drop. The order of presentation of instances
in the training algorithm is randomized; for each
method we repeat the process 10 times and report
average F-score and standard error.
The vector ? was estimated using one of the
same gazetteers used in the base tagger (BG), a
list of 1,438 trigger words from GATE.6 These
are words associated with certain categories; e.g.,
?abbess/Per?, ?academy/Org?, ?caves/Loc?, and
?manifesto/Misc?. The lists for different classes
contain varying numbers of items and might con-
tain misleading words. To obtain more reliable es-
timates of comparable magnitude between classes
we computed equation (4) several times by sam-
pling an equal number of words from each list
and taking the mean. On the development set this
proved better than computing the counts from the
entire list.
Other sources could be evaluated, for exam-
ple lists of entities of each class extracted from
Wikipedia. We used all single-word triggers: 191
for Loc, 171 for Misc, 89 for Org and 592 for Per.
With each list we estimated ? as in Section 5.1 for
each of the four labels starting with ?B?, i.e., en-
tity beginnings, ? = 0 for the other five labels. To
find ? we use as source S, the in-domain data, and
as target T the out-domain data. The lists contain
different number of items and might contain mis-
leading words.
To set ? we compute the mean margin (6)
on CoNLL, using the tagger trained on CoNLL
(mean(Ms) ? 50), similarly for BBN-4
(mean(Ms) ? 38). We used the development
set to fine tune the adaptive rate setting it equal
to ? = 12mean(Ms).
6This list corresponds to the list of words Lc of Sec-
tion 5.1.
6.3 Self training
To compare with self-training we trained a tagger
(BG) on the training set of CoNLL. With the tag-
ger we annotated the training set of BBN-4, and
added the self-labeled data, 39,823 BBN-4 sen-
tences, to the gold standard CoNLL training. Sim-
ilarly, in the reverse direction we trained a tagger
(BG) on the training set of BBN-4, annotated the
training set of CoNLL, and added the self-labeled
16,540 CoNLL sentences to the BBN-4 training.
We denote these models BGSELF , and the aug-
mented sources as CoNLL+ and BBN-4+.
6.4 Structural correspondence learning
We first implemented a simple baseline following
the idea presented in (Ando, 2004). The basic idea
consists in performing an SVD decomposition of
the feature-token matrix, where the matrix con-
tains all the sentences from the source and target
domains. The goal is to capture co-occurrences of
features and derive new features which are more
stable. More specifically, we extracted the 50 prin-
cipal directions of the feature-token matrix and
projected all the data onto these directions. This
results in 50 new additional features for each to-
ken that we append to the original (sparse binary)
feature vector ?i, 1 ? i ? d. In order to give
equal importance to the original and new features,
we multiplied the new features by a constant fac-
tor such that the average L1 norms of the new and
old features are the same. Note that this weight-
ing might not be optimal but should be sufficient
to detect if these new features are helpful or not.
We then implemented several versions of struc-
tural correspondence learning. First, following the
original formulation (we refer to this model as
SCL1), 100 pivot features are selected, these are
frequent features in both source and target data.
For a given pivot feature k, a vector wk ? Rd
is computed by performing a regularized linear
regression between all the other features and the
given pivot feature. The matrixW whose columns
are the wk is formed and the original feature vec-
tors are projected onto the 50 top left singular vec-
tors of W , yielding 50 new features. We also tried
the following variants. In the version we refer to
as SCL2 we rescale the left singular vectors of W
by their corresponding singular values. In the last
variant (SCL3) we select the pivot features which
are frequent in the source and target domains and
which are also predictive for the task (as measured
5
Model Source Target Test
B BBN-4 CoNLL 60.4 ?.28
BG BBN-4 CoNLL 66.1 ?.32
BGSVD BBN-4 CoNLL 66.5 ?.26
BGSCL1 BBN-4 CoNLL 66.8 ?.18
BGSCL2 BBN-4 CoNLL 64.7 ?.24
BGSCL3 BBN-4 CoNLL 66.8 ?.27
BGSELF BBN-4+ CoNLL 65.5 ?.26
BG? BBN-4 CoNLL 66.8 ?.53
Model Source Target Test
B CoNLL BBN-4 65.0 ?.77
BG CoNLL BBN-4 67.6 ?.69
BGSVD CoNLL BBN-4 67.9 ?.54
BGSCL1 CoNLL BBN-4 67.9 ?.45
BGSCL2 CoNLL BBN-4 68.1 ?.53
BGSCL3 CoNLL BBN-4 67.8 ?.34
BGSELF CoNLL+ BBN-4 68.3 ?.36
BG? CoNLL BBN-4 70.3 ?.61
Table 3. Results of baselines and adaptive models.
by the mutual information between the feature and
the class label). The 50 additional features are ap-
pended to the original (sparse binary) feature vec-
tor ?i, 1 ? i ? d, and again, they are first rescaled
in order to have the same average L1 norm as the
old features over the entire dataset.
7 Results and discussion
Table 3 summarizes the experimental results on
both datasets. We refer to our adaptive model as
BG?. Adapting a model from BBN-4 to CoNLL,
self training (BGSELF, 65.5%) performs slightly
worse than the base model (BG, 66.1%). The
best SCL model, the original formulation, pro-
duces a small, but likely significant, improvement
(BGSCL1, 66.8%). Our model (BG?, 66.8%),
achieves the same result but with larger variance.
The improvement of the best models over the first
baseline (B, 60.4%) is considerable, +6.4%, but
mostly due to gazetteers.
In the adaptation experiments from CoNLL to
BBN-4 both self training (BGSELF, 68.3%) and
the best SCL model (BGSCL1, 68.1%) are com-
parable to the baseline (BG, 67.6%). The adap-
tive perceptron HMM (BG?, 70.3%) improves by
2.7%, as much as model BG over B, again with a
slightly larger variance. It is not clear why other
methods do not improve as much. Speculatively,
although we implemented several variants, SCL
might benefit from further tuning as it involves
several pre-processing steps. As for self training,
the base tagger might be too inaccurate to support
this technique. It is fair to assume that the ad-
ditional hyperparameters available to our model,
e.g., ? , provided some additional flexibility. We
also experimented with a few variants of estimat-
ing ? on the development set; i.e., different splits
of the unlabeled source/target data and different
sampling modes: with and without replacement,
number of trials. All of these aspects can have
a significant impact on the quality of the model.
This point brings up a more general issue with
the type of approach explored here: while adapt-
ing the class priors seems easier than adapting the
full model it is not trivial to encode noisy world
knowledge into meaningful priors. Alternatively,
in the presence of some labeled data one could op-
timize ? directly. This information could be also
elicited from domain experts. Another interesting
alternative is the unsupervised estimation via EM
as in (Chan and Ng, 2006).
Overall, adaptation from BBN-4 to CoNLL is
harder than from CoNLL to BBN-4. A possi-
ble explanation is that adapting from specific to
general is harder then in the opposite direction:
the specific corpus is more heavily biased towards
a domain (finance). This intuition is compatible
with the baselines performing better in the CoNLL
to BBN-4 direction. However, the opposite argu-
ment, that adapting from specific to general should
be easier, has some appeal as well; e.g., if more
general means higher entropy it seems easier to
make a distribution more uniform than finding the
right peak.
In general, all adaptive techniques we evalu-
ated provided only marginal improvements over
the baseline (BG) model. To put things in con-
text, it is useful to recall that when evaluated in
domain the CoNLL and BBN-4 taggers (model
BG) achieve, respectively, 92.7% and 91.6% aver-
age F-scores on the test data. As the results illus-
trate there is a considerable drop in out domain ac-
curacy, significantly alleviated by adding features
from gazetteers and to some extent by other meth-
ods. Following Dredze et al (2007) we hypoth-
esize that a significant fraction of the loss is due
to labeling inconsistencies between datasets. Al-
though we did our best to optimize the benchmark
methods it is possible that even better results could
be achieved with self-training and SCL. However
we stress that different methods get at different
aspects of the problem: self-training targets data
sparseness, SCL methods aims at generating better
shared input representations, while our approach
focuses on generating output distribution more
compatible with the target data. It seems reason-
6
able to expect that better adaptation performance
would result from composite approaches, aiming
at both better machine learning and task-specific
aspects for the named entity recognition problem.
8 Conclusion
We investigated the model adaptation problem for
named entity recognition where the base model is
a discriminatively trained HMM (Collins, 2002).
We hypothesized that part of the loss incurred in
using a pre-trained model out of domain is due
to its built-in class priors which do not match the
class distribution of the out of domain data. To
test this hypothesis, and attempt a solution, we
propose to explicitly correct the prediction of the
model for a given label by taking into account a
noisy estimate of the label frequency in the tar-
get data. We found encouraging results from pre-
liminary experiments. It might thus be worth in-
vestigating more principled formulations of this
type of method, in particular to eliminate some
heuristic aspects, improve unsupervised estima-
tions, and generalize to other classification tasks
beyond NER.
Acknowledgments
We would like to thank the anonymous reviewers
for useful comments and pointers to related work.
References
Rie Kubota Ando. 2004. Exploiting unannotated cor-
pora for tagging and chunking. In Proceedings of
ACL 2004. Association for Computational Linguis-
tics.
BBN. 2005. Pronoun coreference and entity type
corpus. Linguistic Data Consortium (LDC) catalog
number LDC2005T33.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP 2006.
Association for Computational Linguistics.
John Blitzer, Mark Dredzde, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes, and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proceedings of Coling-ACL, pages
89?96. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of ACL 2005, pages 173?
180. Association for Computational Linguistics.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help
a lot. In Proceedings of EMNLP, pages 285?292.
Association for Computational Linguistics.
Massimiliano Ciaramita and Yasemin Altun. 2005.
Named-entity recognition in novel domains with ex-
ternal lexical knowledge. In Advances in Structured
Learning for Text and Speech Processing (NIPS
2005).
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of EMNLP, pages 594?602. Associ-
ation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002, pages 1?8.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: A
framework and graphical development environment
for robust NLP tools and applications. In Proceed-
ings of ACL 2002. Association for Computational
Linguistics.
Hal Daume? III. 2007. Frustratingly easy domain
adaptation. In Proceedings of ACL. Association for
Computational Linguistics.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adaptation
for parsing. In Proceedings of CoNLL Shared Task
2007. Association for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337?344. Association for Computational Lin-
guistics.
Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,
and Jordi Atserias. 2008. Learning to tag and tag-
ging to learn: A case study on Wikipedia. IEEE
Intelligent Systems, 23(5):26?33.
Nam Nguyen and Yunsong Guo. 2007. Comparison
of sequence labeling algorithms and extensions. In
Proceedings of ICML 2007, pages 681?688.
Erik F. Tjong Kim Sang and Fien De Muelder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL 2003 Shared Task, pages
142?147. Association for Computational Linguis-
tics.
7
