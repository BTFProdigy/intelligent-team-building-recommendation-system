Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1378?1387,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Convolution Kernels on Constituent, Dependency and Sequential
Structures for Relation Extraction
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
nguyenthi,moschitti,riccardi@disi.unitn.it
Department of Information Engineering and Computer Science
University of Trento
38050 Povo (TN), Italy
Abstract
This paper explores the use of innovative
kernels based on syntactic and semantic
structures for a target relation extraction
task. Syntax is derived from constituent
and dependency parse trees whereas se-
mantics concerns to entity types and lex-
ical sequences. We investigate the effec-
tiveness of such representations in the au-
tomated relation extraction from texts. We
process the above data by means of Sup-
port Vector Machines along with the syn-
tactic tree, the partial tree and the word
sequence kernels. Our study on the ACE
2004 corpus illustrates that the combina-
tion of the above kernels achieves high ef-
fectiveness and significantly improves the
current state-of-the-art.
1 Introduction
Relation Extraction (RE) is defined in ACE as the
task of finding relevant semantic relations between
pairs of entities in texts. Figure 1 shows part
of a document from ACE 2004 corpus, a collec-
tion of news articles. In the text, the relation be-
tween president and NBC?s entertainment division
describes the relationship between the first entity
(person) and the second (organization) where the
person holds a managerial position.
Several approaches have been proposed for au-
tomatically learning semantic relations from texts.
Among others, there has been increased interest in
the application of kernel methods (Zelenko et al,
2002; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2005; Wang, 2008). Their main prop-
erty is the ability of exploiting a huge amount of
This work has been partially funded by the LiveMemo-
ries project (http://www.livememories.org/) and Expert Sys-
tem (http://www.expertsystem.net/) research grant.
Jeff Zucker, the longtime executive producer of
NBC?s ?Today? program, will be named Friday
as the new president of NBC?s entertainment
division, replacing Garth Ancier, NBC execu-
tives said.
Figure 1: A document from ACE 2004 with all
entity mentions in bold.
features without an explicit feature representation.
This can be done by computing a kernel function
between a pair of linguistic objects, where such
function is a kind of similarity measure satisfy-
ing certain properties. An example is the sequence
kernel (Lodhi et al, 2002), where the objects are
strings of characters and the kernel function com-
putes the number of common subsequences of
characters in the two strings. Such substrings are
then weighted according to a decaying factor pe-
nalizing longer ones. In the same line, Tree Ker-
nels count the number of subtree shared by two in-
put trees. An example is that of syntactic (or sub-
set) tree kernel (SST) (Collins and Duffy, 2001),
where trees encode grammatical derivations.
Previous work on the use of kernels for RE
has exploited some similarity measures over di-
verse features (Zelenko et al, 2002; Culotta and
Sorensen, 2004; Zhang et al, 2005) or subse-
quence kernels over dependency graphs (Bunescu
and Mooney, 2005a; Wang, 2008). More specif-
ically, (Bunescu and Mooney, 2005a; Culotta
and Sorensen, 2004) use kernels over depen-
dency trees, which showed much lower accuracy
than feature-based methods (Zhao and Grishman,
2005). One problem of the dependency kernels
above is that they do not exploit the overall struc-
tural aspects of dependency trees. A more effec-
tive solution is the application of convolution ker-
nels to constituent parse trees (Zhang et al, 2006)
but this is not satisfactory from a general per-
1378
spective since dependency structures offer some
unique advantages, which should be exploited by
an appropriate kernel.
Therefore, studying convolution tree kernels for
dependency trees is worthwhile also considering
that, to the best of our knowledge, these models
have not been previously used for relation extrac-
tion
1
task. Additionally, sequence kernels should
be included in such global study since some of
their forms have not been applied to RE.
In this paper, we study and evaluate diverse con-
volution and sequence kernels for the RE problem
by providing several kernel combinations on con-
stituent and dependency trees and sequential struc-
tures. To fully exploit the potential of dependency
trees, in addition to the SST kernel, we applied
the partial tree (PT) kernel proposed in (Moschitti,
2006), which is a general convolution tree kernel
adaptable for dependency structures. We also in-
vestigate various sequence kernels (e.g. the word
sequence kernel (WSK) (Cancedda et al, 2003))
by incorporating dependency structures into word
sequences. These are also enriched by including
information from constituent parse trees.
We conduct experiments on the standard ACE
2004 newswire and broadcast news domain. The
results show that although some kernels are less
effective than others, they exhibit properties that
are complementary to each other. In particu-
lar, we found that relation extraction can benefit
from increasing the feature space by combining
kernels (with a simple summation) exploiting the
two different parsing paradigms. Our experiments
on RE show that the current composite kernel,
which is constituent-based is more effective than
those based on dependency trees and individual
sequence kernel but at the same time their com-
binations, i.e. dependency plus constituent trees,
improve the state-of-the-art in RE. More interest-
ingly, also the combinations of various sequence
kernels gain significant better performance than
the current state-of-the-art (Zhang et al, 2005).
Overall, these results are interesting for the
computational linguistics research since they show
that the above two parsing paradigms provide dif-
ferent and important information for a semantic
task such as RE. Regarding sequence-based ker-
nels, the WSK gains better performance than pre-
vious sequence and dependency models for RE.
1
The function defined on (Culotta and Sorensen, 2004),
although on dependency trees, is not a convolution tree ker-
nel.
A review of previous work on RE is described
in Section 2. Section 3 introduces support vec-
tor machines and kernel methods whereas our spe-
cific kernels for RE are described is Section 4. The
experiments and conclusions are presented in sec-
tions 5 and 6, respectively.
2 Related Work
To identify semantic relations using machine
learning, three learning settings have mainly been
applied, namely supervised methods (Miller et
al., 2000; Zelenko et al, 2002; Culotta and
Sorensen, 2004; Kambhatla, 2004; Zhou et al,
2005), semi supervised methods (Brin, 1998;
Agichtein and Gravano, 2000), and unsupervised
method (Hasegawa et al, 2004). In a supervised
learning setting, representative related work can
be classified into generative models (Miller et al,
2000), feature-based (Roth and tau Yih, 2002;
Kambhatla, 2004; Zhao and Grishman, 2005;
Zhou et al, 2005) or kernel-based methods (Ze-
lenko et al, 2002; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005a; Zhang et al, 2005;
Wang, 2008; Zhang et al, 2006).
The learning model employed in (Miller et al,
2000) used statistical parsing techniques to learn
syntactic parse trees. It demonstrated that a lexi-
calized, probabilistic context-free parser with head
rules can be used effectively for information ex-
traction. Meanwhile, feature-based approaches
often employ various kinds of linguistic, syntac-
tic or contextual information and integrate into
the feature space. (Roth and tau Yih, 2002) ap-
plied a probabilistic approach to solve the prob-
lems of named entity and relation extraction with
the incorporation of various features such as word,
part-of-speech, and semantic information from
WordNet. (Kambhatla, 2004) employed maximum
entropy models with diverse features including
words, entity and mention types and the number
of words (if any) separating the two entities.
Recent work on Relation Extraction has mostly
employed kernel-based approaches over syntac-
tic parse trees. Kernels on parse trees were pi-
oneered by (Collins and Duffy, 2001). This
kernel function counts the number of common
subtrees, weighted appropriately, as the measure
of similarity between two parse trees. (Culotta
and Sorensen, 2004) extended this work to cal-
culate kernels between augmented dependency
trees. (Zelenko et al, 2002) proposed extracting
1379
relations by computing kernel functions between
parse trees. (Bunescu and Mooney, 2005a) pro-
posed a shortest path dependency kernel by stipu-
lating that the information to model a relationship
between two entities can be captured by the short-
est path between them in the dependency graph.
Although approaches in RE have been domi-
nated by kernel-based methods, until now, most
of research in this line has used the kernel as some
similarity measures over diverse features (Zelenko
et al, 2002; Culotta and Sorensen, 2004; Bunescu
and Mooney, 2005a; Zhang et al, 2005; Wang,
2008). These are not convolution kernels and pro-
duce a much lower number of substructures than
the PT kernel. A recent approach successfully em-
ploys a convolution tree kernel (of type SST) over
constituent syntactic parse tree (Zhang et al, 2006;
Zhou et al, 2007), but it does not capture gram-
matical relations in dependency structure. We be-
lieve that an efficient and appropriate kernel can
be used to solve the RE problem, exploiting the
advantages of dependency structures, convolution
tree kernels and sequence kernels.
3 Support Vector Machines and Kernel
Methods
In this section we give a brief introduction to sup-
port vector machines, kernel methods, diverse tree
and sequence kernel spaces, which can be applied
to the RE task.
3.1 Support Vector Machines (SVMs)
Support Vector Machines refer to a supervised ma-
chine learning technique based on the latest results
of the statistical learning theory (Vapnik, 1998).
Given a vector space and a set of training points,
i.e. positive and negative examples, SVMs find a
separating hyperplane H(~x) = ~? ? ~x + b = 0
where ? ? R
n
and b ? R are learned by applying
the Structural Risk Minimization principle (Vap-
nik, 1995). SVMs is a binary classifier, but it can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods (robert Mller et al, 2001)
to implicitly map data in a new space where the
examples are more easily separable as described
in the next section.
3.2 Kernel Methods
Kernel methods (Schlkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
~x with a feature function ? : O ? <
n
, whereO is
the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
H(~x) =
(
?
i=1..l
y
i
?
i
~x
i
)
? ~x+ b =
?
i=1..l
y
i
?
i
~x
i
? ~x+ b =
?
i=1..l
y
i
?
i
?(o
i
) ? ?(o) + b,
where y
i
is equal to 1 for positive and -1 for neg-
ative examples, ?
i
? < with ?
i
? 0, o
i
?i ?
{1, .., l} are the training instances and the product
K(o
i
, o) = ??(o
i
) ? ?(o)? is the kernel function
associated with the mapping ?.
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to their ability
of implicitly exploring huge amounts of structural
features automatically extracted from the origi-
nal object representation. The kernels for struc-
tured natural language data, such as parse tree
kernel (Collins and Duffy, 2001) and string ker-
nel (Lodhi et al, 2002) are examples of the well-
known convolution kernels used in many NLP ap-
plications.
Tree kernels represent trees in terms of their
substructures (called tree fragments). Such frag-
ments form a feature space which, in turn, is
mapped into a vector space. Tree kernels mea-
sure the similarity between pair of trees by count-
ing the number of fragments in common. There
are three important characterizations of fragment
type (Moschitti, 2006): the SubTrees (ST), the
SubSet Trees (SST) and the Partial Trees (PT). For
sake of space, we do not report the mathematical
description of them, which is available in (Vish-
wanathan and Smola, 2002), (Collins and Duffy,
1380
2001) and (Moschitti, 2006), respectively. In con-
trast, we report some descriptions in terms of fea-
ture space that may be useful to understand the
new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A Sub-
Set Tree (SST) is a more general structure which
does not necessarily include all the descendants. It
must be generated by applying the same grammat-
ical rule set, which generated the original tree. A
Partial Tree (PT) is a more general form of sub-
structures obtained by relaxing constraints over
the SST.
4 Kernels for Relation Extraction
In this section we describe the previous kernels
based on constituent trees as well as new kernels
based on diverse types of trees and sequences for
relation extraction. As mentioned in the previ-
ous section, we can engineer kernels by combin-
ing tree and sequence kernels. Thus we focus on
the problem to define structure embedding the de-
sired syntactic relational information between two
named entities (NEs).
4.1 Constituent and Dependency Structures
Syntactic parsing (or syntactic analysis) aims at
identifying grammatical structures in a text. A
parser thus captures the hidden hierarchy of the
input text and processes it into a form suitable for
further processing. There are two main paradigms
for representing syntactic information: constituent
and dependency parsing, which produces two dif-
ferent tree structures.
Constituent tree encodes structural properties
of a sentence. The parse tree contains constituents,
such as noun phrases (NP) and verb phrases (VP),
as well as terminals/part-of-speech tags, such as
determiners (DT) or nouns (NN). Figure 2.a shows
the constituent tree of the sentence: In Washing-
ton, U.S. officials are working overtime.
Dependency tree encodes grammatical rela-
tions between words in a sentence with the words
as nodes and dependency types as edges. An edge
from a word to another represents a grammatical
relation between these two. Every word in a de-
pendency tree has exactly one parent except the
root. Figure 2.b shows and example of the depen-
dency tree of the previous sentence.
Given two NEs, such as Washington and offi-
cials, both the above trees can encode the syntactic
dependencies between them. However, since each
parse tree corresponds to a sentence, there may be
more than two NEs and many relations expressed
in a sentence. Thus, the use of the entire parse
tree of the whole sentence holds two major draw-
backs: first, it may be too computationally expen-
sive for kernel calculation since the size of a com-
plete parse tree may be very large (up to 300 nodes
in the Penn Treebank (Marcus et al, 1993)); sec-
ond, there is ambiguity on the target pairs of NEs,
i.e. different NEs associated with different rela-
tions are described by the same parse tree. There-
fore, it is necessary to identify the portion of the
parse tree that best represent the useful syntactic
information.
Let e
1
and e
2
be two entity mentions in the same
sentence such that they are in a relationship R.
For the constituent parse tree, we used the path-
enclosed tree (PET), which was firstly proposed
in (Moschitti, 2004) for Semantic Role Labeling
and then adapted by (Zhang et al, 2005) for re-
lation extraction. It is the smallest common sub-
tree including the two entities of a relation. The
dashed frame in Figure 2.a surrounds PET associ-
ated with the two mentions, officials and Washing-
ton. Moreover, to improve the representation, two
extra nodes T1-PER, denoting the type PERSON,
and T2-LOC, denoting the type LOCATION, are
added to the parse tree, above the two target NEs,
respectively. In this example, the above PET is de-
signed to capture the relation Located-in between
the entities ?officials? and ?Washington? from the
ACE corpus. Note that, a third NE, U.S., is char-
acterized by the node GPE (GeoPolitical Entity),
where the absence of the prefix T1 or T2 before
the NE type (i.e. GPE), denotes that the NE does
not take part in the target relation.
In previous work, some dependency trees have
been used (Bunescu and Mooney, 2005a; Wang,
2008) but the employed kernel just exploited the
syntactic information concentrated in the path be-
tween e
1
and e
2
. In contrast, we defined and stud-
ied three different dependency structures whose
potential can be fully exploited by our convolution
partial tree kernel:
- Dependency Words (DW) tree is similar to
PET adapted for dependency tree constituted
by simple words. We select the minimal sub-
tree which includes e
1
and e
2
, and we insert
an extra node as father of the NEs, labeled
with the NE category. For example, given
1381
Figure 2: The constituent and dependency parse trees integrated with entity information
the tree in Figure 2.b, we design the tree in
Figure 2.c surrounded by the dashed frames,
where T1-PER, T2-LOC and GPE are the ex-
tra nodes inserted as fathers of Washington,
soldier and U.S..
- Grammatical Relation (GR) tree, i.e. the DW
tree in which words are replaced by their
grammatical functions, e.g. prep, pobj and
nsubj. For example, Figure 2.d, shows the
GR tree for the previous relation: In is re-
placed by prep , U.S. by nsubj and so on.
- Grammatical Relation and Words (GRW)
tree, words and grammatical functions are
both used in the tree, where the latter are in-
serted as a father node of the former. For
example, Figure 2.e, shows such tree for the
previous relation.
4.2 Sequential Structures
Some sequence kernels have been used on depen-
dency structures (Bunescu and Mooney, 2005b;
Wang, 2008). These kernels just used lexical
words with some syntactic information. To fully
exploit syntactic and semantic information, we de-
fined and studied six different sequences (in a style
similar to what proposed in (Moschitti, 2008)),
which include features from constituent and de-
pendency parse trees and NEs:
1. Sequence of terminals (lexical words) in the
PET (SK
1
), e.g.:
T2-LOC Washington , U.S. T1-PER officials.
2. Sequence of part-of-speech (POS) tags in the
PET (SK
2
), i.e. the SK
1
in which words are
replaced by their POS tags, e.g.:
T2-LOC NN , NNP T1-PER NNS.
3. Sequence of grammatical relations in the
1382
PET (SK
3
), i.e. the SK
1
in which words are
replaced by their grammatical functions, e.g.:
T2-LOC pobj , nn T1-PER nsubj.
4. Sequence of words in the DW (SK
4
), e.g.:
Washington T2-LOC In working T1-PER of-
ficials GPE U.S..
5. Sequence of grammatical relations in the GR
(SK
5
), i.e. the SK
4
in which words are re-
placed by their grammatical functions, e.g.:
pobj T2-LOC prep ROOT T1-PER nsubj GPE
nn.
6. Sequence of POS tags in the DW (SK
6
), i.e.
the SK
4
in which words are replaced by their
POS tags, e.g.:
NN T2-LOC IN VBP T1-PER NNS GPE
NNP.
It is worth noting that the potential information
contained in such sequences can be fully exploited
by the word sequence kernel.
4.3 Combining Kernels
Given that syntactic information from different
parse trees may have different impact on relation
extraction (RE), the viable approach to study the
role of dependency and constituent parsing is to
experiment with different syntactic models and
measuring the impact in terms of RE accuracy.
For this purpose we compared the composite ker-
nel described in (Zhang et al, 2006) with the par-
tial tree kernels applied to DW , GR, and GRW
and sequence kernels based on six sequences de-
scribed above. The composite kernels include
polynomial kernel applied to entity-related feature
vector. The word sequence kernel (WSK) is al-
ways applied to sequential structures. The used
kernels are described in more detail below.
4.3.1 Polynomial Kernel
The basic kernel between two named entities of
the ACE documents is defined as:
K
P
(R
1
, R
2
) =
?
i=1,2
K
E
(R
1
.E
i
, R
2
.E
i
),
where R
1
and R
2
are two relation instances, E
i
is
the i
th
entity of a relation instance. K
E
(?, ?) is a
kernel over entity features, i.e.:
K
E
(E
1
, E
2
) = (1 + ~x
1
? ~x
2
)
2
,
where ~x
1
and ~x
2
are two feature vectors extracted
from the two NEs.
For the ACE 2004, the features used include:
entity headword, entity type, entity subtype, men-
tion type, and LDC
2
mention type. The last four
attributes are taken from the ACE corpus 2004. In
ACE, each mention has a head annotation and an
extent annotation.
4.3.2 Kernel Combinations
1. Polynomial kernel plus a tree kernel:
CK
1
= ? ?K
P
+ (1? ?) ?K
x
,
where ? is a coefficient to give more impact
to K
P
and K
x
is either the partial tree ker-
nel applied to one the possible dependency
structures, DW, GR or GRW or the SST ker-
nel applied to PET, described in the previous
section.
2. Polynomial kernel plus constituent plus de-
pendency tree kernels:
CK
2
= ? ?K
P
+ (1? ?) ? (K
SST
+K
PT
)
where K
SST
is the SST kernel and K
PT
is
the partial tree kernel (applied to the related
structures as in point 1).
3. Constituent tree plus square of polynomial
kernel and dependency tree kernel:
CK
3
= ? ?K
SST
+ (1??) ? (K
P
+K
PT
)
2
4. Dependency word tree plus grammatical re-
lation tree kernels:
CK
4
= K
PT?DW
+K
PT?GR
where K
PT?DW
and K
PT?GR
are the par-
tial tree kernels applied to dependency struc-
tures DW and GR.
5. Polynomial kernel plus dependency word
plus grammatical relation tree kernels:
CK
5
= ??K
P
+(1??)?(K
PT?DW
+K
PT?GR
)
Some preliminary experiments on a validation set
showed that the second, the fourth and the fifth
combinations yield the best performance with ? =
0.4 while the first and the third combinations yield
the best performance with ? = 0.23.
Regarding WSK, the following combinations
are applied:
2
Linguistic Data Consortium (LDC):
http://www.ldc.upenn.edu/Projects/ACE/
1383
1. SK
3
+ SK
4
2. SK
3
+ SK
6
3. SSK =
?
i=1,..,6
SK
i
4. K
SST
+ SSK
5. CSK = ? ?K
P
+ (1??) ? (K
SST
+SSK)
Preliminary experiments showed that the last com-
bination yields the best performance with ? =
0.23.
We used a polynomial expansion to explore the
bi-gram features of i) the first and the second en-
tity participating in the relation, ii) grammatical
relations which replace words in the dependency
tree. Since the kernel function set is closed un-
der normalization, polynomial expansion and lin-
ear combination (Schlkopf and Smola, 2001), all
the illustrated composite kernels are also proper
kernels.
5 Experiments
Our experiments aim at investigating the effec-
tiveness of convolution kernels adapted to syntac-
tic parse trees and various sequence kernels for
the RE task. For this purpose, we use the sub-
set and partial tree kernel over different kinds of
trees, namely constituent and dependency syntac-
tic parse trees. Diverse sequences are applied indi-
vidually and in combination together. We consider
our task of relation extraction as a classification
problem where categories are relation types. All
pairs of entity mentions in the same sentence are
taken to generate potential relations, which will be
processed as positive and negative examples.
5.1 Experimental setup
We use the newswire and broadcast news domain
in the English portion of the ACE 2004 corpus
provided by LDC. This data portion includes 348
documents and 4400 relation instances. It defines
seven entity types and seven relation types. Every
relation is assigned one of the seven types: Phys-
ical, Person/Social, Employment/Membership/-
Subsidiary, Agent-Artifact, PER/ORG Affiliation,
GPE Affiliation, and Discourse. For sake of space,
we do not explain these relationships here, never-
theless, they are explicitly described in the ACE
document guidelines. There are 4400 positive and
38,696 negative examples when generating pairs
of entity mentions as potential relations.
Documents are parsed using Stanford
Parser (Klein and Manning, 2003) to pro-
duce parse trees. Potential relations are generated
by iterating all pairs of entity mentions in the same
sentence. Entity information, namely entity type,
is integrated into parse trees. To train and test our
binary relation classifier, we used SVMs. Here,
relation detection is formulated as a multiclass
classification problem. The one vs. rest strategy
is employed by selecting the instance with largest
margin as the final answer. For experimentation,
we use 5-fold cross-validation with the Tree
Kernel Tools (Moschitti, 2004) (available at
http://disi.unitn.it/?moschitt/Tree-Kernel.htm).
5.2 Results
In this section, we report the results of different
kernels setup over constituent (CT) and depen-
dency (DP) parse trees and sequences taken from
these parse trees. The tree kernel (TK), compos-
ite kernel (CK
1
, CK
2
, CK
3
, CK
4
, and CK
5
corresponding to five combination types in Sec-
tion 4.3.2) were employed over these two syntactic
trees. For the tree kernel, we apply the SST kernel
for the path-enclosed tree (PET) of the constituent
tree and the PT kernel for three kinds of depen-
dency tree DW, GR, and GRW, described in the
previous section. The two composite kernels CK
2
and CK
3
are applied over both two parse trees.
The word sequence kernels are applied over six
sequences SK
1
, SK
2
, SK
3
, SK
4
, SK
5
, and SK
6
(described in Section 4.3).
The results are shown in Table 1 and Table 2.
In the first table, the first column indicates the
structure used in the combination shown in the
second column, e.g. PET associated with CK
1
means that the SST kernel is applied on PET (a
portion of the constituent tree) and combined with
the CK
1
schema whereas PET and GR associated
with CK
5
means that SST kernel is applied to
PET and PT kernel is applied to GR in CK
5
. The
remaining three columns report Precision, Recall
and F1 measure. The interpretation of the second
table is more immediate since the only tree ker-
nel involved is the SST kernel applied to PET and
combined by means of CK
1
.
We note that: first, the dependency kernels,
i.e. the results on the rows from 3 to 6 are be-
low the composite kernel CK
1
, i.e. 68.9. This
is the state-of-the-art in RE, designed by (Zhang
et al, 2006), where our implementation provides
1384
Parse Tree Kernel P R F
PET CK
1
69.5 68.3 68.9
DW CK
1
53.2 59.7 56.3
GR CK
1
58.8 61.7 60.2
GRW CK
1
56.1 61.2 58.5
DW and GR CK
5
59.7 64.1 61.8
PET and GR
CK
2
70.7 69.0 69.8
CK
3
70.8 70.2 70.5
Table 1: Results on the ACE 2004 evaluation test
set. Six structures were experimented over the
constituent and dependency trees.
Kernel P R F
CK
1
69.5 68.3 68.9
SK
1
72.0 52.8 61.0
SK
2
61.7 60.0 60.8
SK
3
62.6 60.7 61.6
SK
4
73.1 50.3 59.7
SK
5
59.0 60.7 59.8
SK
6
57.7 61.8 59.7
SK
3
+ SK
4
75.0 63.4 68.8
SK
3
+ SK
6
66.8 65.1 65.9
SSK =
?
i
SK
i
73.8 66.2 69.8
CSK 75.6 66.6 70.8
CK
1
+ SSK 76.6 67.0 71.5
(Zhou et al, 2007)
82.2 70.2 75.8
CK
1
with Heuristics
Table 2: Performance comparison on the ACE
2004 data with different kernel setups.
a slightly smaller result than the original version
(i.e. an F1 of about 72 using a different syntactic
parser).
Second, CK
1
improves to 70.5, when the con-
tribution of PT kernel applied to GR (dependency
tree built using grammatical relations) is added.
This suggests that dependency structures are effec-
tively exploited by PT kernel and that such infor-
mation is somewhat complementary to constituent
trees.
Third, in the second table, the model CK
1
+
SSK, which adds to CK
1
the contribution of di-
verse sequence kernels, outperforms the state-of-
the-art by 2.6%. This suggests that the sequential
information encoded by several sequence kernels
can better represents the dependency information.
Finally, we also report in the last row (in italic)
the superior RE result by (Zhou et al, 2007).
However, to achieve this outcome the authors used
the composite kernel CK
1
with several heuristics
to define an effective portion of constituent trees.
Such heuristics expand the tree and remove unnec-
essary information allowing a higher improvement
on RE. They are tuned on the target RE task so al-
though the result is impressive, we cannot use it to
compare with pure automatic learning approaches,
such us our models.
6 Conclusion and Future Work
In this paper, we study the use of several types
of syntactic information: constituent and depen-
dency syntactic parse trees. A relation is repre-
sented by taking the path-enclosed tree (PET) of
the constituent tree or of the path linking two enti-
ties of the dependency tree. For the design of auto-
matic relation classifiers, we have investigated the
impact of dependency structures to the RE task.
Our novel composite kernels, which account for
the two syntactic structures, are experimented with
the appropriate convolution kernels and show sig-
nificant improvement with respect to the state-of-
the-art in RE.
Regarding future work, there are many research
line that may be followed:
i) Capturing more features by employing ex-
ternal knowledge such as ontological, lexical re-
source or WordNet-based features (Basili et al,
2005a; Basili et al, 2005b; Bloehdorn et al, 2006;
Bloehdorn and Moschitti, 2007) or shallow se-
mantic trees, (Giuglea and Moschitti, 2004; Giu-
glea and Moschitti, 2006; Moschitti and Bejan,
2004; Moschitti et al, 2007; Moschitti, 2008;
Moschitti et al, 2008).
ii) Design a new tree-based structures, which
combines the information of both constituent and
dependency parses. From dependency trees we
can extract more precise but also more sparse
relationships (which may cause overfit). From
constituent trees, we can extract subtrees consti-
tuted by non-terminal symbols (grammar sym-
bols), which provide a better generalization (with
a risk of underfitting).
iii) Design a new kernel which can integrate the
advantages of the constituent and dependency tree.
The new tree kernel should inherit the benefits of
the three available tree kernels: ST, SST or PT.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
1385
lections. In Proceedings of the 5th ACM Interna-
tional Conference on Digital Libraries.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005a. Effective use of WordNet semantics
via kernel-based learning. In Proceedings of the
Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 1?8, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2005b. A semantic kernel to classify texts
with very few training examples. In In Proceedings
of the Workshop on Learning in Web Search, at the.
Stephan Bloehdorn and Alessandro Moschitti. 2007.
Structure and semantics for expressive text ker-
nels. In CIKM ?07: Proceedings of the sixteenth
ACM conference on Conference on information and
knowledge management, pages 861?864, New York,
NY, USA. ACM.
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
of feature similarity. In Proceedings of the 6th IEEE
International Conference on Data Mining (ICDM
06), Hong Kong, 18-22 December 2006, DEC.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceeding of WebDB
Workshop at 6th International Conference on Ex-
tending Database Technology, pages 172?183.
Razvan C. Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of EMNLP, pages 724?731.
Razvan C. Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In Pro-
ceedings of EMNLP.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence ker-
nels. Journal of Machine Learning Research, pages
1059?1082.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neu-
ral Information Processing Systems (NIPS?2001).
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting on ACL, Barcelona,
Spain.
Ana-Maria Giuglea and Alessandro Moschitti. 2004.
Knowledge discovery using framenet, verbnet and
propbank. In A. Meyers, editor, Workshop on On-
tology and Knowledge Discovering at ECML 2004,
Pisa, Italy.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic Role Labeling via Framenet, Verbnet and
Propbank. In Proceedings of ACL 2006, Sydney,
Australia.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named en-
tities from large corpora. In Proceedings of the 42nd
Annual Meeting on ACL, Barcelona, Spain.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, Barcelona, Spain.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the ACL, pages 423?430.
Huma Lodhi, Craig Saunders, John Shawe-Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Mitchell P. Marcus, Beatrice Santorini, , and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: the penn treebank. Computa-
tional Linguistics, 19(2):313?330.
Scott Miller, Heidi Fox, Lance Ramshaw, , and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st conference on North American chapter of the
ACL, pages 226?233, Seattle, USA.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification.
In CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question/answer classification. In Proceedings of
ACL?07, Prague, Czech Republic.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of the 42nd Meeting of the ACL, Barcelona,
Spain.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning, Berlin, Germany.
Alessandro Moschitti. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM ?08: Proceeding of the 17th ACM conference
on Information and knowledge management, pages
253?262, New York, NY, USA. ACM.
Ryan Michael Rifkin and Tomaso Poggio. 2002. Ev-
erything old is new again: a fresh look at historical
approaches in machine learning. PhD thesis, Mas-
sachusetts Institute of Technology.
1386
Klaus robert Mller, Sebastian Mika, Gunnar Rtsch,
Koji Tsuda, , and Bernhard Schlkopf. 2001. An
introduction to kernel-based learning algorithms.
IEEE Transactions on Neural Networks, 12(2):181?
201.
Dan Roth and Wen tau Yih. 2002. Probabilistic rea-
soning for entity and relation recognition. In Pro-
ceedings of the COLING-2002, Taipei, Taiwan.
Bernhard Schlkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer?Verlag, New York.
Vladimir N. Vapnik. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
S.V.N. Vishwanathan and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In Proceedings of
Neural Information Processing Systems.
Mengqiu Wang. 2008. A re-examination of depen-
dency path kernels for relation extraction. In Pro-
ceedings of the 3rd International Joint Conference
on Natural Language Processing-IJCNLP.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations be-
tween named entities from a large raw corpus using
tree similarity-based clustering. In Proceedings of
IJCNLP?2005, Lecture Notes in Computer Science
(LNCS 3651), pages 378?389, Jeju Island, South
Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of COLING-ACL 2006, pages 825?
832.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Meeting of the
ACL, pages 419?426, Ann Arbor, Michigan, USA.
GuoDong Zhou, Jian Su, Jie Zhang, , and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Meeting of the
ACL, pages 427?434, Ann Arbor, USA, June.
GuoDong Zhou, Min Zhang, DongHong Ji, and
QiaoMing Zhu. 2007. Tree kernel-based relation
extraction with context-sensitive structured parse
tree information. In Proceedings of EMNLP-CoNLL
2007, pages 728?736.
1387
Coling 2010: Poster Volume, pages 901?909,
Beijing, August 2010
Kernel-based Reranking for Named-Entity Extraction
Truc-Vien T. Nguyen and Alessandro Moschitti and Giuseppe Riccardi
Department of Information Engineering and Computer Science
University of Trento
nguyenthi,moschitti,riccardi@disi.unitn.it
Abstract
We present novel kernels based on struc-
tured and unstructured features for rerank-
ing the N-best hypotheses of conditional
random fields (CRFs) applied to entity ex-
traction. The former features are gener-
ated by a polynomial kernel encoding en-
tity features whereas tree kernels are used
to model dependencies amongst tagged
candidate examples. The experiments on
two standard corpora in two languages,
i.e. the Italian EVALITA 2009 and the En-
glish CoNLL 2003 datasets, show a large
improvement on CRFs in F-measure, i.e.
from 80.34% to 84.33% and from 84.86%
to 88.16%, respectively. Our analysis re-
veals that both kernels provide a compara-
ble improvement over the CRFs baseline.
Additionally, their combination improves
CRFs much more than the sum of the indi-
vidual contributions, suggesting an inter-
esting kernel synergy.
1 Introduction
Reranking is a promising computational frame-
work, which has drawn special attention in the
Natural Language Processing (NLP) community.
Basically, this method first employs a probabilis-
tic model to generate a list of top-n candidates and
then reranks this n-best list with additional fea-
tures. One appeal of this approach is its flexibility
of incorporating arbitrary features into a model.
These features help in discriminating good from
bad hypotheses and consequently their automatic
learning. Various algorithms have been applied
for reranking in NLP applications (Huang, 2008;
Shen et al, 2004; Collins, 2002b; Collins and
Koo, 2000), including parsing, name tagging and
machine translation. This work has exploited the
disciminative property as one of the key criterion
of the reranking algorithm.
Reranking appears extremely interesting if cou-
pled with kernel methods (Dinarelli et al, 2009;
Moschitti, 2004; Collins and Duffy, 2001), as the
latter allow for extracting from the ranking hy-
potheses a huge amount of features along with
their dependencies. Indeed, while feature-based
learning algorithms involve only the dot-product
between feature vectors, kernel methods allow
for a higher generalization by replacing the dot-
product with a function between pairs of linguis-
tic objects. Such functions are a kind of similarity
measure satisfying certain properties. An exam-
ple is the tree kernel (Collins and Duffy, 2001),
where the objects are syntactic trees that encode
grammatical derivations and the kernel function
computes the number of common subtrees. Simi-
larly, sequence kernels (Lodhi et al, 2002) count
the number of common subsequences shared by
two input strings.
Named-entities (NEs) are essential for defin-
ing the semantics of a document. NEs are ob-
jects that can be referred by names (Chinchor and
Robinson, 1998), such as people, organizations,
and locations. The research on NER has been
promoted by the Message Understanding Con-
ferences (MUCs, 1987-1998), the shared task of
the Conference on Natural Language Learning
(CoNLL, 2002-2003), and the Automatic Content
Extraction program (ACE, 2002-2005). In the lit-
erature, there exist various learning approaches
to extract named-entities from text. A NER sys-
901
tem often builds some generative/discriminative
model, then, either uses only one classifier (Car-
reras et al, 2002) or combines many classifiers us-
ing some heuristics (Florian et al, 2003).
To the best of our knowledge, reranking has
not been applied to NER except for the rerank-
ing algorithms defined in (Collins, 2002b; Collins,
2002a), which only targeted the entity detection
(and not entity classification) task. Besides, since
kernel methods offer a natural way to exploit lin-
guistic properties, applying kernels for NE rerank-
ing is worthwhile.
In this paper, we describe how kernel methods
can be applied for reranking, i.e. detection and
classification of named-entities, in standard cor-
pora for Italian and English. The key aspect of
our reranking approach is how structured and flat
features can be employed in discriminating candi-
date tagged sequences. For this purpose, we apply
tree kernels to a tree structure encoding NE tags of
a sentence and combined them with a polynomial
kernel, which efficiently exploits global features.
Our main contribution is to show that (a) tree
kernels can be used to define general features (not
merely syntactic) and (b) using appropriate al-
gorithms and features, reranking can be very ef-
fective for named-entity recognition. Our study
demonstrates that the composite kernel is very
effective for reranking named-entity sequences.
Without the need of producing and heuristically
combining learning models like previous work on
NER, the composite kernel not only captures most
of the flat features but also efficiently exploits
structured features. More interestingly, this kernel
yields significant improvement when applied to
two corpora of two different languages. The eval-
uation in the Italian corpus shows that our method
outperforms the best reported methods whereas on
the English data it reaches the state-of-the-art.
2 Background
2.1 The data
Different languages exhibit different linguistic
phenomena and challenges. A robust NER sys-
tem is expected to be well-adapted to multiple
domains and languages. Therefore, we experi-
mented with two datasets: the EVALITA 2009
Italian corpus and the well-known CoNLL 2003
English shared task corpus.
The EVALITA 2009 Italian dataset is based
on I-CAB, the Italian Content Annotation
Bank (Magnini et al, 2006), annotated with four
entity types: Person (PER), Organization (ORG),
Geo-Political Entity (GPE) and Location (LOC).
The training data, taken from the local newspa-
per ?L?Adige?, consists of 525 news stories which
belong to five categories: News Stories, Cultural
News, Economic News, Sports News and Local
News. Test data, on the other hand, consist of
completely new data, taken from the same news-
paper and consists of 180 news stories.
The CoNLL 2003 English dataset is created
within the shared task of CoNLL-2003 (Sang
and Meulder, 2003). It is a collection of news
wire articles from the Reuters Corpus, annotated
with four entity types: Person (PER), Location
(LOC), Organization (ORG) and Miscellaneous
name (MISC). The training and the development
datasets are news feeds from August 1996, while
the test set contains news feeds from December
1996. Accordingly, the named entities in the test
dataset are considerably different from those that
appear in the training or the development set.
Italian GPE LOC ORG PER
Train 2813 362 3658 457724.65% 3.17% 32.06% 40.11%
Test 1143 156 1289 237823.02% 3.14% 25.96% 47.89%
English LOC MISC ORG PER
Train 7140 3438 6321 660030.38% 14.63% 26.90% 28.09%
Dev 1837 922 1341 184230.92% 15.52% 22.57% 31.00%
Test 1668 702 1661 161729.53% 12.43% 29.41% 28.63%
Table 1: Statistics on the Italian EVALITA 2009
and English CoNLL 2003 corpora.
2.2 The baseline algorithm
We selected Conditional Random Fields (Lafferty
et al, 2001) as the baseline model. Conditional
902
random fields (CRFs) are a probabilistic frame-
work for labeling and segmenting sequence data.
They present several advantages over other purely
generative models such as Hidden Markov models
(HMMs) by relaxing the independence assump-
tions required by HMMs. Besides, HMMs and
other discriminative Markov models are prone to
the label bias problem, which is effectively solved
by CRFs.
The named-entity recognition (NER) task is
framed as assigning label sequences to a set of
observation sequences. We follow the IOB nota-
tion where the NE tags have the format B-TYPE,
I-TYPE or O, which mean that the word is a be-
ginning, a continuation of an entity, or not part of
an entity at all. For example, consider the sentence
with their corresponding NE tags, each word is la-
beled with a tag indicating its appropriate named-
entity, resulting in annotated text, such as:
Il/O presidente/O della/O Fifa/B-ORG Sepp/B-PER
Blatter/I-PER affermando/O che/O il/O torneo/O era/O
stato/O ottimo/O (FIFA president Sepp Blatter says that the
tournament was excellent)
For our experiments, we used CRF++ 1 to build
our recognizer, which is a model trained discrim-
inatively with the unigram and bigram features.
These are extracted from a window at k words
centered in the target word w (i.e. the one we want
to classify with the B, O, I tags). More in detail
such features are:
? The word itself, its prefixes, suffixes, and
part-of-speech
? Orthographic/Word features. These are
binary and mutually exclusive features that
test whether a word contains all upper-cased,
initial letter upper-cased, all lower-cased,
roman-number, dots, hyphens, acronym,
lonely initial, punctuation mark, single-char,
and functional-word.
? Gazetteer features. Class (geographical,
first name, surname, organization prefix, lo-
cation prefix) of words in the window.
? Left Predictions. The predicted tags on the
left of the word in the current classification.
1http://crfpp.sourceforge.net
The gazetteer lists are built with names im-
ported from different sources. For English, the
geographic features are imported from NIMA?s
GEOnet Names Server (GNS)2, The Alexandria
Digital Library (ADL) gazetteer3. The company
data is included with all the publicly traded com-
panies listed in Google directory4, the European
business directory5. For Italian, the generic proper
nouns are extracted from Wikipedia and various
Italian sites.
2.3 Support Vector Machines (SVMs)
Support Vector Machines refer to a supervised
machine learning technique based on the latest re-
sults of the statistical learning theory. Given a
vector space and a set of training points, i.e. posi-
tive and negative examples, SVMs find a separat-
ing hyperplane H(~x) = ~? ? ~x + b = 0 where
? ? Rn and b ? R are learned by applying the
Structural Risk Minimization principle (Vapnik,
1998). SVMs are a binary classifier, but they can
be easily extended to multi-class classifier, e.g. by
means of the one-vs-all method (Rifkin and Pog-
gio, 2002).
One strong point of SVMs is the possibility to
apply kernel methods to implicitly map data in
a new space where the examples are more easily
separable as described in the next section.
2.4 Kernel methods
Kernel methods (Scho?lkopf and Smola, 2001) are
an attractive alternative to feature-based methods
since the applied learning algorithm only needs
to compute a product between a pair of objects
(by means of kernel functions), avoiding the ex-
plicit feature representation. A kernel function
is a scalar product in a possibly unknown feature
space. More precisely, The object o is mapped in
~x with a feature function ? : O ? <n, where O
is the set of the objects.
The kernel trick allows us to rewrite the deci-
sion hyperplane as:
H(~x) =
( ?
i=1..l
yi?i~xi
)
? ~x+ b =
2http://www.nima.mil/gns/html
3http://www.alexandria.ucsb.edu
4http://directory.google.com/Top/Business
5http://www.europages.net
903
?i=1..l
yi?i~xi ? ~x+ b =
?
i=1..l
yi?i?(oi) ? ?(o) + b,
where yi is equal to 1 for positive and -1 for
negative examples, ?i ? < with ?i ? 0, oi
?i ? {1, .., l} are the training instances and the
product K(oi, o) = ??(oi) ? ?(o)? is the kernel
function associated with the mapping ?.
Kernel engineering can be carried out by com-
bining basic kernels with additive or multiplica-
tive operators or by designing specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
Regarding NLP applications, kernel methods
have attracted much interest due to the ability of
implicitly exploring huge amounts of structural
features. The parse tree kernel (Collins and Duffy,
2001) and string kernel (Lodhi et al, 2002) are
examples of the well-known convolution kernels
used in various NLP tasks.
2.5 Tree Kernels
Tree kernels represent trees in terms of their sub-
structures (called tree fragments). Such fragments
form a feature space which, in turn, is mapped into
a vector space. Tree kernels measure the similar-
ity between pair of trees by counting the number
of fragments in common. There are three impor-
tant characterizations of fragment type: the Sub-
Trees (ST), the SubSet Trees (SST) and the Partial
Trees (PT). For sake of space, we do not report the
mathematical description of them, which is avail-
able in (Vishwanathan and Smola, 2002), (Collins
and Duffy, 2001) and (Moschitti, 2006), respec-
tively. In contrast, we report some descriptions in
terms of feature space that may be useful to un-
derstand the new engineered kernels.
In principle, a SubTree (ST) is defined by tak-
ing any node along with its descendants. A SubSet
Tree (SST) is a more general structure which does
not necessarily include all the descendants. The
distinction is that an SST must be generated by ap-
plying the same grammatical rule set which gen-
erated the original tree, as pointed out in (Collins
and Duffy, 2001). A Partial Tree (PT) is a more
general form of sub-structures obtained by relax-
ing constraints over the SSTs. Figure 1 shows the
overall fragment set of the ST, SST and PT kernels
for the syntactic parse tree of the sentence frag-
Figure 1: Three kinds of tree kernels.
ment: gives a talk .
In the next section, we will define new struc-
tures for tagged sequences of NEs which along
with the application of the PT kernel produce in-
novative tagging kernels for reranking.
3 Reranking Method
3.1 Reranking Strategy
As a baseline we trained the CRFs model to gen-
erate 10-best candidates per sentence, along with
their probabilities. Each candidate was then rep-
resented by a semantic tree together with a feature
vector. We consider our reranking task as a binary
classification problem where examples are pairs
of hypotheses < Hi, Hj >.
Given a sentence ?South African Breweries Ltd
bought stakes in the Lech and Tychy brewers? and three
of its candidate tagged sequences:
H1 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
B-ORG O (the correct sequence)
H2 B-MISC I-MISC B-ORG I-ORG O O O O B-ORG
I-ORG I-ORG O
H3 B-ORG I-ORG I-ORG I-ORG O O O O B-ORG O
B-LOC O
where B-ORG, I-ORG, B-LOC, O are the gen-
erated NE tags according to IOB notation as de-
scribed in Section 3.2.
With the above data (an original sentence to-
gether with a list of candidate tagged sequences),
the following pairs of hypotheses will be gener-
904
ated < H1, H2 >, < H1, H3 >,< H2, H1 > and
< H3, H1 >, where the first two pairs are positive
and the latter pairs are negative instances. Then a
binary classifier based on SVMs and kernel meth-
ods can be trained to discriminate between the
best hypothesis, i.e. < H1 > and the others. At
testing time the hypothesis receiving the highest
score is selected (Collins and Duffy, 2001).
3.2 Representation of Tagged Sequences in
Semantic Trees
We now consider the representation that exploits
the most discriminative aspects of candidate struc-
tures. As in the case of NER, an input can-
didate is a sequence of word/tag pairs x =
{w1/t1...wn/tn} where wi is the i?th word and
ti is the i?th NE tag for that word. The first repre-
sentation we consider is the tree structure. See fig-
ure 2 as an example of candidate tagged sequence
and its semantic tree.
With the sentence ?South African Breweries Ltd
bought stakes in the Lech and Tychy brewers? and three
of its candidate tagged sequences in the previous
section, the training algorithm considers to con-
struct a tree for each sequence, with the named-
entity tags as pre-terminals and the words as
leaves. See figure 2 for an example of the seman-
tic tree for the first tagged sequence.
With this tree representation, for a word wi, the
target NE tag would be set at parent and the fea-
tures for this word are at child nodes. This allows
us to best exploit the inner product between com-
peting candidates. Indeed, in the kernel space,
the inner product counts the number of common
subtrees thus sequences with similar NE tags are
likely to have higher score. For example, the sim-
ilarity between H1 and H3 will be higher than the
similarity of the previous hypotheses withH2; this
is reasonable since these two also have higher F1.
It is worth noting that another useful modifica-
tion is the flexibility of incorporate diverse, ar-
bitrary features into this tree structure by adding
children to the parent node that contains entity tag.
These characteristics can be exploited efficiently
with the PT kernel, which relaxes constraints of
production rules. The inner product can implicitly
include these features and deal better with sparse
data.
3.3 Global features
Mixed n-grams features
In previous works, some global features have
been used (Collins, 2002b; Collins, 2002a) but the
employed algorithm just exploited arbitrary infor-
mation regarding word types and linguistic pat-
terns. In contrast, we define and study diverse
features by also considering n-grams patterns pre-
ceding, and following the target entity.
Complementary context
In supervised learning, NER systems often suf-
fer from low recall, which is caused by lack of
both resource and context. For example, a word
like ?Arkansas? may not appear in the training set
and in the test set, there may not be enough con-
text to infer its NE tag. In such cases, neither
global features (Chieu and Ng, 2002) nor aggre-
gated contexts (Chieu and Ng, 2003) can help.
To overcome this deficiency, we employed the
following unsupervised procedure: first, the base-
line NER is applied to the target un-annotated cor-
pus. Second, we associate each word of the corpus
with the most frequent NE category assigned in
the previous step. Finally, the above tags are used
as features during the training of the improved
NER and also for building the feature represen-
tation for a new classification instance.
This way, for any unknown word w of the test
set, we can rely on the most probable NE category
as feature. The advantage is that we derived it by
using the average over many possible contexts of
w, which are in the different instances of the un-
nanotated corpus.
The unlabeled corpus for Italian was collected
from La Repubblica 6 and it contains over 20 mil-
lions words. Whereas the unlabeled corpus for
English was collected mainly from The New York
Times 7 and BBC news stories 8 with more than
35 millions words.
Head word
As the head word of an entity plays an impor-
tant role in information extraction (Bunescu and
Mooney, 2005a; Surdeanu et al, 2003), it is in-
6http://www.repubblica.it/
7http://www.nytimes.com/
8http://news.bbc.co.uk/
905
Figure 2: Semantic structure of the first sequence
cluded in the global set together with its ortho-
graphic feature. We now describe some primitives
for our global feature framework.
1. wi for i = 1 . . . n is the i?th word
2. ti is the NE tag of wi
3. gi is the gazetteer feature of the word wi
4. fi is the most frequent NE tag seen in a large
corpus of wi
5. hi is the head word of the entity. We nor-
mally set the head word of an entity as its last
word. However, when a preposition exists in
the entity string, its head word is set as the
last word before the preposition. For exam-
ple, the head word of the entity ?University
of Pennsylvania? is ?University?.
6. Mixed n-grams features of the words and
their gazetteers/frequent-tag before/after the
start/end of an entity. In addition to the
normal n-grams solely based on words, we
mixed words with gazetteers/frequent-tag
seen from a large corpus and create mixed
n-grams features.
Table 2 shows the full set of global features in
our reranking framework. Features are anchored
to each entity instance and adapted to entity types.
This helps to discriminate different entities with
the same surface forms. Moreover, they can be
combined with n-grams patterns to learn and ex-
plicitly push the score of the correct sequence
above the score of competing sequences.
3.4 Reranking with Composite Kernel
In this section we describe our novel tagging ker-
nels based on diverse global features as well as
semantic trees for reranking candidate tagged se-
quences. As mentioned in the previous section,
we can engineer kernels by combining tree and
entity kernels. Thus we focus on the problem to
define structure embedding the desired relational
information among tagged sequences.
The Partial Tree Kernel
Let F = f1, f2, . . . , f|F | be a tree fragment
space of type PTs and let the indicator function
Ii(n) be equal to 1 if the target f1 is rooted at node
n and 0 otherwise, we define the PT kernel as:
K(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2)
where NT1 and NT2 are the set of nodes
in T1 and T2 respectively and ?(n1, n2) =?|F |
i=1 Ii(n1)Ii(n2), i.e. the number of common
fragments rooted at the n1 and n2 nodes of the
type shown in Figure 1.c.
The Polynomial Kernel
The polynomial kernel between two candidate
tagged sequences is defined as:
K(x, y) = (1 + ~x1 ? ~x2)2,
where ~x1 and ~x2 are two feature vectors extracted
from the two sequences with the global feature
template.
The Tagging Kernels
In our reranking framework, we incorporate the
probability from the original model with the tree
structure as well as the feature vectors. Let us con-
sider the following notations:
906
Feature Description
ws ws+1 . . . we Entity string
gs gs+1 . . . ge The gazetteer feature within the entity
fs fs+1 . . . fe The most frequent NE tag feature (seen from a
large corpus) within the entity
hw The head word of the entity
lhw Indicates whether the head word is lower-cased
ws?1 ws; ws?1 gs; gs?1 ws; gs?1 gs Mixed bigrams of the words/gazetteer features
before/after the start of the entity
we we+1; we ge+1; ge we+1; ge ge+1 Mixed bigrams of the words/gazetteer features
before/after the end of the entity
ws?1 ws; ws?1 fs; fs?1 ws; fs?1 fs Mixed bigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we we+1; we fe+1; fe we+1; fe fe+1 Mixed bigrams of the words/frequent-tag fea-
tures before/after the end of the entity
ws?2 ws?1 ws; ws?1 ws ws+1; we?1 we we+1; we?2 we?1 we Trigram features of the words before/after the
start/end of the entity
ws?2 ws?1 gs; ws?2 gs?1 ws; ws?2 gs?1 gs;
gs?2 ws?1 ws; gs?2 ws?1 gs; gs?2 gs?1 ws; gs?2 gs?1 gs;
ws?1 ws gs+1; ws?1 gs ws+1; ws?1 gs gs+1;
gs?1 ws ws+1; gs?1 ws gs+1; gs?1 gs ws+1; gs?1 gs gs+1
Mixed trigrams of the words/gazetteer features
before/after the start of the entity
we?1 we ge+1; we?1 ge we+1; we?1 ge ge+1;
ge?1 we we+1; ge?1 we ge+1; ge?1 ge we+1; ge?1 ge ge+1;
we?2 we?1 ge; we?2 ge?1 we; we?2 ge?1 ge;
ge?2 we?1 we; ge?2 we?1 ge; ge?2 ge?1 we; ge?2 ge?1 ge
Mixed trigrams of the words/gazetteer features
before/after the end of the entity
ws?2 ws?1 fs; ws?2 fs?1 ws; ws?2 fs?1 fs;
fs?2 ws?1 ws; fs?2 ws?1 fs; fs?2 fs?1 ws; fs?2 fs?1 fs;
ws?1 ws fs+1; ws?1 fs ws+1; ws?1 fs fs+1;
fs?1 ws ws+1; fs?1 ws fs+1; fs?1 fs ws+1; fs?1 fs fs+1
Mixed trigrams of the words/frequent-tag fea-
tures before/after the start of the entity
we?1 we fe+1; we?1 fe we+1; we?1 fe fe+1;
fe?1 we we+1; fe?1 we fe+1; fe?1 fe we+1; fe?1 fe fe+1;
we?2 we?1 fe; we?2 fe?1 we; we?2 fe?1 fe;
fe?2 we?1 we; fe?2 we?1 fe; fe?2 fe?1 we; fe?2 fe?1 fe
Mixed trigrams of the words/frequent-tag fea-
tures before/after the end of the entity
Table 2: Global features in the entity kernel for reranking. These features are anchored for each entity
instance and adapted to entity categories. For example, the entity string (first feature) of the entity
?United Nations? with entity type ?ORG? is ?ORG United Nations?.
? K(x, y) = L(x) ? L(y) is the basic kernel
where L(x) is the log probability of a can-
didate tagged sequence x under the original
probability model.
? TK(x, y) = t(x) ? t(y) is the partial tree ker-
nel under the structure representation
? FK(x, y) = f(x) ? f(y) is the polynomial
kernel under the global features
The tagging kernels between two tagged se-
quences are defined in the following combina-
tions:
1. CTK = ? ?K + (1? ?) ? TK
2. CFK = ? ?K + (1? ?) ? FK
3. CTFK = ? ?K + (1? ?) ? (TK + FK)
where ?, ?, ? are parameters weighting the two
participating terms. Experiments on the validation
set showed that these combinations yield the best
performance with ? = 0.2 for both languages,
? = 0.4 for English and ? = 0.3 for and Italian,
? = 0.24 for English and ? = 0.2 for Italian.
4 Experimens and Results
4.1 Experimental Setup
As a baseline we trained the CRFs classifier on
the full training portion (11,227 sentences in the
Italian and 14,987 sentences in the English cor-
pus). In developing a reranking strategy for both
English and Italian, the training data was split into
5 sections, and in each case the baseline classifier
was trained on 4/5 of the data, then used to decode
the remaining 1/5.
907
The top 10 hypotheses together with their log
probabilities were recovered for each training sen-
tence. Similarly, a model trained on the whole
training data was used to produce 10 hypotheses
for each sentence in the development set. For the
reranking experiments, we applied different ker-
nel setups to the two corpora described in Section
2.1. The three kernels were trained on the training
portion.
Italian Test P R F
CRFs 83.43 77.48 80.34
CTK 84.97 78.03 81.35
CFK 84.93 79.13 81.93
CTFK 85.99 82.73 84.33
(Zanoli et al, 2009) 84.07 80.02 82.00
English Test P R F
CRFs 85.37 84.35 84.86
CTK 87.19 84.79 85.97
CFK 86.53 86.75 86.64
CTFK 88.07 88.25 88.16
(Ratinov and Roth, ) N/A N/A 90.57
Table 3: Reranking results of the three tagging
kernels on the Italian and English testset.
4.2 Discussion
Table 3 presents the reranking results on the test
data of both corpora. The results show a 20.29%
relative improvement in F-measure for Italian and
21.79% for English.
CFK based on unstructured features achieves
higher accuracy than CTK based on structured
features. However, the huge amount of subtrees
generated by the PT kernel may limit the expres-
sivity of some structural features, e.g. many frag-
ments may only generate noise. This problem is
less important with the polynomial kernel where
global features are tailored for individual entities.
In any case, the experiments demonstrate that
both tagging kernels CTK and CFK give im-
provement over the CRFs baseline in both lan-
guages. This suggests that structured and unstruc-
tured features are effective in discriminating be-
tween competing NE annotations.
Furthermore, the combination of the two tag-
ging kernels on both standard corpora shows a
large improvement in F-measure from 80.34% to
84.33% for Italian and from 84.86% to 88.16%
for English data. This suggests that these two ker-
nels, corresponding to two kinds of feature, com-
plement each other.
To better collocate our results with previous
work, we report the best NER outcome on the
Italian (Zanoli et al, 2009) and the English (Rati-
nov and Roth, ) datasets, in the last row (in italic)
of each table. This shows that our model outper-
forms the best Italian NER system and it is close
to the state-of-art model for English, which ex-
ploits many complex features9. Also note that we
are very close to the F1 achieved by the best sys-
tem of CoNLL 2003, i.e. 88.8.
5 Conclusion
We analyzed the impact of kernel-based ap-
proaches for modeling dependencies between
tagged sequences for NER. Our study illustrates
that each individual kernel, either with structured
or with flat features clearly gives improvement to
the base model. Most interestingly, as we showed,
these contributions are independent and, the ap-
proaches can be used together to yield better re-
sults. The composite kernel, which combines both
kinds of features, can outperform the state-of-the-
art.
In the future, it will be very interesting to
use syntactic/semantic kernels, as for example in
(Basili et al, 2005; Bloehdorn and Moschitti,
2007a; Bloehdorn and Moschitti, 2007b). An-
other promising direction is the use of syntactic
trees, feature sequences and pairs of instances,
e.g. (Nguyen et al, 2009; Moschitti, 2008).
Acknowledgments
We would like to thank Roberto Zanoli and
Marco Dinarelli for helpful explanation
about their work. This work has been par-
tially funded by the LiveMemories project
(http://www.livememories.org/) and Expert
System (http://www.expertsystem.net/) research
grant.
9In the future we will be able to integrate them with the
authors collaboration.
908
References
Basili, Roberto, Marco Cammisa, and Alessandro
Moschitti. 2005. Effective use of WordNet seman-
tics via kernel-based learning. In CoNLL.
Bloehdorn, Stephan and Alessandro Moschitti. 2007a.
Combined syntactic and semantic kernels for text
classification. In ECIR.
Bloehdorn, Stephan and Alessandro Moschitti. 2007b.
Structure and semantics for expressive text kernels.
In CIKM.
Bunescu, Razvan C. and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In EMNLP.
Carreras, Xavier, Llu??s Ma`rques, and Llus Padro?.
2002. Named entity extraction using Adaboost. In
CoNLL.
Chieu, Hai Leong and Hwee Tou Ng. 2002. Named
entity recognition: A maximum entropy approach
using global information. In COLING.
Chieu, Hai Leong and Hwee Tou Ng. 2003. Named
entity recognition with a maximum entropy ap-
proach. In CoNLL.
Chinchor, Nancy and Patricia Robinson. 1998. Muc-7
named entity task definition. In the MUC.
Collins, Michael and Nigel Duffy. 2001. Convolution
kernels for natural language. In NIPS.
Collins, Michael and Terry Koo. 2000. Discriminative
reranking for natural language parsing. In ICML.
Collins, Michael. 2002a. New ranking algorithms for
parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL.
Collins, Michael. 2002b. Ranking algorithms for
named-entity extraction boosting and the voted per-
ceptron. In ACL.
Dinarelli, Marco, Alessandro Moschitti, and Giuseppe
Riccardi. 2009. Re-ranking models based on small
training data for spoken language understanding. In
EMNLP.
Florian, Radu, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In CoNLL.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In ACL-HLT.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Lodhi, Huma, Craig Saunders, John Shawe Taylor,
Nello Cristianini, , and Chris Watkins. 2002. Text
classification using string kernels. Journal of Ma-
chine Learning Research, pages 419?444.
Magnini, Bernardo, Emmanuele Pianta, Christian Gi-
rardi, Matteo Negri, Lorenza Romano, Manuela
Speranza, Valentina Bartalesi Lenzi, and Rachele
Sprugnoli. 2006. I-CAB: the italian content anno-
tation bank. In LREC.
Moschitti, Alessandro. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL.
Moschitti, Alessandro. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In ICML.
Moschitti, Alessandro. 2008. Kernel methods, syntax
and semantics for relational text categorization. In
CIKM.
Nguyen, Truc-Vien T., Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. In EMNLP.
Ratinov, Lev and Dan Roth. Design challenges and
misconceptions in named entity recognition. In
CoNLL.
Rifkin, Ryan Michael and Tomaso Poggio. 2002. Ev-
erything old is new again: a fresh look at historical
approaches in machine learning. PhD thesis, MIT.
Sang, Erik F. Tjong Kim and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition.
In CoNLL.
Scho?lkopf, Bernhard and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine transla-
tion. In HLT-NAACL, Boston, Massachusetts, USA.
Surdeanu, Mihai, Sanda Harabagiu, John Williams,
and Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In ACL.
Vapnik, Vladimir N. 1998. Statistical Learning The-
ory. John Wiley and Sons, New York.
Vishwanathan, S.V.N. and Alexander J. Smola. 2002.
Fast kernels on strings and trees. In NIPS.
Zanoli, Roberto, Emanuele Pianta, and Claudio Giu-
liano. 2009. Named entity recognition through re-
dundancy driven classifiers. In EVALITA.
909
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 277?282,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
End-to-End Relation Extraction Using Distant Supervision
from External Semantic Repositories
Truc-Vien T. Nguyen and Alessandro Moschitti
Department of Information Engineering and Computer Science
University of Trento
38123 Povo (TN), Italy
{nguyenthi,moschitti}@disi.unitn.it
Abstract
In this paper, we extend distant supervision
(DS) based on Wikipedia for Relation Extrac-
tion (RE) by considering (i) relations defined
in external repositories, e.g. YAGO, and (ii)
any subset of Wikipedia documents. We show
that training data constituted by sentences
containing pairs of named entities in target re-
lations is enough to produce reliable supervi-
sion. Our experiments with state-of-the-art re-
lation extraction models, trained on the above
data, show a meaningful F1 of 74.29% on a
manually annotated test set: this highly im-
proves the state-of-art in RE using DS. Addi-
tionally, our end-to-end experiments demon-
strated that our extractors can be applied to
any general text document.
1 Introduction
Relation Extraction (RE) from text as defined in
ACE (Doddington et al, 2004) concerns the extrac-
tion of relationships between two entities. This is
typically carried out by applying supervised learn-
ing, e.g. (Zelenko et al, 2002; Culotta and Sorensen,
2004; Bunescu and Mooney, 2005) by using a hand-
labeled corpus. Although, the resulting models are
far more accurate than unsupervised approaches,
they suffer from the following drawbacks: (i) they
require labeled data, which is usually costly to pro-
duce; (ii) they are typically domain-dependent as
different domains involve different relations; and
(iii), even in case the relations do not change, they
result biased toward the text feature distributions of
the training domain.
The drawbacks above would be alleviated if data
from several different domains and relationships
were available. A form of weakly supervision,
specifically named distant supervision (DS) when
applied to Wikipedia, e.g. (Banko et al, 2007; Mintz
et al, 2009; Hoffmann et al, 2010) has been recently
developed to meet the requirement above. The main
idea is to exploit (i) relation repositories, e.g. the
Infobox, x, of Wikipedia to define a set of relation
types RT (x) and (ii) the text in the page associated
with x to produce the training sentences, which are
supposed to express instances of RT (x).
Previous work has shown that selecting the sen-
tences containing the entities targeted by a given re-
lation is enough accurate (Banko et al, 2007; Mintz
et al, 2009) to provide reliable training data. How-
ever, only (Hoffmann et al, 2010) used DS to de-
fine extractors that are supposed to detect all the re-
lation instances from a given input text. This is a
harder test for the applicability of DS but, at the
same time, the resulting extractor is very valuable:
it can find rare relation instances that might be ex-
pressed in only one document. For example, the re-
lation President(Barrack Obama, United States) can
be extracted from thousands of documents thus there
is a large chance of acquiring it. In contrast, Pres-
ident(Eneko Agirre, SIGLEX) is probably expressed
in very few documents, increasing the complexity
for obtaining it.
In this paper, we extend DS by (i) considering
relations from semantic repositories different from
Wikipedia, i.e. YAGO, and (2) using training in-
stances derived from any Wikipedia document. This
allows for (i) potentially obtaining training data
277
for many more relation types, defined in different
sources; (ii) meaningfully enlarging the size of the
DS data since the relation examples can be extracted
from any Wikipedia document 1.
Additionally, by following previous work, we
define state-of-the-art RE models based on kernel
methods (KM) applied to syntactic/semantic struc-
tures. We use tree and sequence kernels that can
exploit structural information and interdependencies
among labels. Experiments show that our models
are flexible and robust to Web documents as we
achieve the interesting F1 of 74.29% on 52 YAGO
relations. This is even more appreciable if we ap-
proximately compare with the previous result on RE
using DS, i.e. 61% (Hoffmann et al, 2010). Al-
though the experiment setting is different from ours,
the improvement of about 13 absolute percent points
demonstrates the quality of our model.
Finally, we also provide a system for extracting
relations from any text. This required the definition
of a robust Named Entity Recognizer (NER), which
is also trained on weakly supervised Wikipedia data.
Consequently, our end-to-end RE system is appli-
cable to any document. This is another major im-
provement on previous work. The satisfactory RE
F1 of 67% for 52 Wikipedia relations suggests that
our model is also successfully applicable in real sce-
narios.
1.1 Related Work
RE generally relates to the extraction of relational
facts, or world knowledge from the Web (Yates,
2009). To identify semantic relations using ma-
chine learning, three learning settings have been ap-
plied, namely supervised methods, e.g. (Zelenko
et al, 2002; Culotta and Sorensen, 2004; Kamb-
hatla, 2004), semi supervised methods, e.g. (Brin,
1998; Agichtein and Gravano, 2000), and unsuper-
vised method, e.g. (Hasegawa et al, 2004; Banko
et al, 2007). Work on supervised Relation Extrac-
tion has mostly employed kernel-based approaches,
e.g. (Zelenko et al, 2002; Culotta and Sorensen,
2004; Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Zhang et al, 2005; Bunescu, 2007;
Nguyen et al, 2009; Zhang et al, 2006). However,
1Previous work assumes the page related to the Infobox as
the only source for the training data.
Algorithm 2.1: ACQUIRE LABELED DATA()
DS = ?
Y AGO(R) : Instances of Relation R
for each ?Wikipedia article : W ? ? Freebase
do
?
?
?
?
?
??
?
?
?
?
??
S ? set of sentences fromW
for each s ? S
do
?
?
?
??
?
?
??
E ? set of entities from s
for each E1 ? E and E2 ? E and
R ? Y AGO
do
?
?
?
if R(E1, E2) ? YAGO(R)
then DS ? DS ? {s,R+}
else DS ? DS ? {s,R?}
return (DS)
such approaches can be applied to few relation types
thus distant supervised learning (Mintz et al, 2009)
was introduced to tackle such problem. Another so-
lution proposed in (Riedel et al, 2010) was to adapt
models trained in one domain to other text domains.
2 Resources and Dataset Creation
In this section, we describe the resources for the cre-
ation of an annotated dataset based on distant super-
vision. We use YAGO, a large knowledge base of
entities and relations, and Freebase, a collection of
Wikipedia articles. Our procedure uses entities and
facts from YAGO to provide relation instances. For
each pair of entities that appears in some YAGO re-
lation, we retrieve all the sentences of the Freebase
documents that contain such entities.
2.1 YAGO
YAGO (Suchanek et al, 2007) is a huge seman-
tic knowledge base derived from WordNet and
Wikipedia. It comprises more than 2 million entities
(like persons, organizations, cities, etc.) and 20 mil-
lion facts connecting these entities. These include
the taxonomic Is-A hierarchy as well as semantic re-
lations between entities.
We use the YAGO version of 2008-w40-2 with a
manually confirmed accuracy of 95% for 99 rela-
tions. However, some of them are (a) trivial, e.g.
familyNameOf ; (b) numerical attributes that change
over time, e.g. hasPopulation; (c) symmetric, e.g.
hasPredecessor; (d) used only for data management,
e.g. describes or foundIn. Therefore, we removed
those irrelevant relations and obtained 1,489,156 in-
stances of 52 relation types to be used with our DS
approach.
278
2.2 Freebase
To access to Wikipedia documents, we used Free-
base (March 27, 2010 (Metaweb Technologies,
2010)), which is a dump of the full text of all
Wikipedia articles. For our experiments, we used
100,000 articles. Out of them, only 28,074 articles
contain at least one relation for a total of 68,429 of
relation instances. These connect 744,060 entities,
97,828 dates and 203,981 numerical attributes.
Temporal and Numerical Expression
Wikipedia articles are marked with entities like Per-
son or Organization but not with dates or numeri-
cal attributes. This prevents to extract interesting
relations between entities and dates, e.g. John F.
Kennedy was born on May 29, 1917 or between en-
tities and numerical attributes, e.g. The novel Gone
with the wind has 1037 pages. Thus we designed
18 regular expressions to extract dates and other 25
to extract numerical attributes, which range from in-
teger number to ordinal number, percentage, mone-
tary, speed, height, weight, area, time, and ISBN.
2.3 Distant Supervision and generalization
Distant supervision (DS) for RE is based on the
following assumption: (i) a sentence is connected
in some way to a database of relations and (ii)
such sentence contains the pair of entities partic-
ipating in a target relation; (iii) then it is likely
that such sentence expresses the relation. In tra-
ditional DS the point (i) is implemented by the
Infobox, which is connected to the sentences by
a proximity relation (same page of the sentence).
In our extended DS, we relax (i) by allowing
for the use of an external DB of relations such
as YAGO and any document of Freebase (a col-
lection of Wikipedia documents). The alignment
between YAGO and Freebase is implemented by
the Wikipedia page link: for example the link
http://en.wikipedia.org/wiki/James Cameron refers
to the entity James Cameron.
We use an efficient procedure formally described
in Alg. 2.1: for each Wikipedia article in Free-
base, we scan all of its NEs. Then, for each pair
of entities2 seen in the sentence, we query YAGO to
2Our algorithm is robust to the lack of knowledge about the
existence of any relation between two entities. If the relation
retrieve the relation instance connecting these enti-
ties. Note that a simplified version of our approach
is the following: for any YAGO relation instance,
scan all the sentences of all Wikipedia articles to test
point (ii). Unfortunately, this procedure is impossi-
ble in practice due to millions of relation instances
in YAGO and millions of Wikipedia articles in Free-
base, i.e. an order of magnitude of 1014 iterations3.
3 Distant Supervised Learning with
Kernels
We model relation extraction (RE) using state-of-
the-art classifiers based on kernel methods. The
main idea is that syntactic/semantic structures are
used to represent relation instances. We followed the
model in (Nguyen et al, 2009) that has shown sig-
nificant improvement on the state-of-the-art. This
combines a syntactic tree kernel and a polynomial
kernel over feature extracted from the entities:
CK1 = ? ?KP + (1? ?) ? TK (1)
where ? is a coefficient to give more or less impact
to the polynomial kernel,KP , and TK is the syntac-
tic tree kernel (Collins and Duffy, 2001). The best
model combines the advantages of the two parsing
paradigms by adding the kernel above with six se-
quence kernels (described in (Nguyen et al, 2009)).
CSK = ? ?KP +(1??) ?(TK+
?
i=1,..,6
SKi) (2)
Such kernels cannot be applied to Wikipedia doc-
uments as the entity category, e.g. Person or Orga-
nization, is in general missing. Thus, we adapted
them by simply removing the category label in the
nodes of the trees and in the sequences. This data
transformation corresponds to different kernels (see
(Cristianini and Shawe-Taylor, 2000)).
4 Experiments
We carried out test to demonstrate that our DS ap-
proach produces reliable and practically usable re-
lation extractors. For this purpose, we test them on
instance is not in YAGO, it is simply assumed as a negative
instance even if such relation is present in other DBs.
3Assuming 100 sentences for each article.
279
DS data by also carrying out end-to-end RE evalua-
tion. This requires to experiment with a state-of-the-
art Named Entity Recognizer trained on Wikipedia
entities.
Class Precision Recall F-measure
bornOnDate 97.99 95.22 96.58
created 92.00 68.56 78.57
dealsWith 92.30 73.47 81.82
directed 85.19 51.11 63.89
hasCapital 93.69 61.54 74.29
isAffiliatedTo 86.32 71.30 78.10
locatedIn 87.85 78.33 82.82
wrote 82.61 42.22 55.88
Overall 91.42 62.57 74.29
Table 1: Performance of 8 out of 52 individual relations
with overall F1.
4.1 Experimental setting
We used the DS dataset generated from YAGO and
Wikipedia articles, as described in the algorithm
(Alg. 2.1). The candidate relations are generated
by iterating all pairs of entity mentions in the same
sentence. Relation detection is formulated as a mul-
ticlass classification problem. The One vs. Rest
strategy is employed by selecting the instance with
largest margin as the final answer. We carried out
5-fold cross-validation with the tree kernel toolkit4
(Moschitti, 2004; Moschitti, 2008).
4.2 Results on Wikipedia RE
We created a test set by sampling 200 articles from
Freebase (these articles are not used for training).
An expert annotator, for each sentence, labeled all
possible pairs of entities with one of the 52 rela-
tions from YAGO, where the entities were already
marked. This process resulted in 2,601 relation in-
stances.
Table 1 shows the performance of individual clas-
sifiers as well as the overall Micro-average F1 for
our adapted CSK: we note that it reaches an F1-
score of 74.29%. This can be compared with the
Micro-average F1 of CK1, i.e. 71.21%. The lower
result suggests that the combination of dependency
and constituent syntactic structures is very impor-
tant: +3.08 absolute percent points on CK1, which
only uses constituency trees.
4http://disi.unitn.it/ moschitt/Tree-Kernel.htm
Class Precision Recall F-measure
Entity Detection 68.84 64.56 66.63
End-to-End RE 82.16 56.57 67.00
Table 2: Entity Detection and End-to-end Relation Ex-
traction.
4.3 End-to-end Relation Extraction
Previous work in RE uses gold entities available in
the annotated corpus (i.e. ACE) but in real appli-
cations these are not available. Therefore, we per-
form experiments with automatic entities. For their
extraction, we follow the feature design in (Nguyen
et al, 2010), using CRF++ 5 with unigram/features
and Freebase as learning source. Dates and numer-
ical attributes required a different treatment, so we
use the patterns described in Section 2.3. The results
reported in Table 2 are rather lower than in standard
NE recognition. This is due to the high complexity
of predicting the boundaries of thousands of differ-
ent categories in YAGO.
Our end-to-end RE system can be applied to any
text fragment so we could experiment with it and
any Wikipedia document. This allowed us to carry
out an accurate evaluation. The results are shown in
Table 2. We note that, without gold entities, RE from
Wikipedia still achieves a satisfactory performance
of 67.00% F1.
5 Conclusion
This paper proposes two main contributions to Re-
lation Extraction: (i) a new approach to distant su-
pervision (DS) to create training data using relations
defined in different sources, i.e. YAGO, and poten-
tially using any Wikipedia document; and (ii) end-
to-end systems applicable both to Wikipedia pages
as well as to any natural language text.
The results show:
1. A high F1 of 74.29% on extracting 52 YAGO
relations from any Wikipedia document (not
only from Infobox related pages). This re-
sult improves on previous work by 13.29 abso-
lute percent points (approximated comparison).
This is a rough approximation since on one
hand, (Hoffmann et al, 2010) experimented
5http://crfpp.sourceforge.net
280
with 5,025 relations, which indicate that our re-
sults based on 52 relations cannot be compared
with it (i.e. our multi-classifier has two orders
of magnitude less of categories). On the other
hand, the only experiment that can give a re-
alistic measurement is the one on hand-labeled
test set (testing on data automatically labelled
by DS does not provide a realistic outcome).
The size of such test set is comparable with
ours, i.e. 100 documents vs. our set of 200
documents. Although, we do not know how
many types of relations were involved in the
test of (Hoffmann et al, 2010), it is clear that
only a small subset of the 5000 relations could
have been measured. Also, we have to consider
that, in (Hoffmann et al, 2010), only one rela-
tion extractor is supposed to be learnt from one
article (by using Infobox) whereas we can po-
tentially extract several relations even from the
same sentence.
2. The importance of using both dependency and
constituent structures (+3.08% when adding
dependency information to RE based on con-
stituent trees).
3. Our end-to-end system is useful for real appli-
cations as it shows a meaningful accuracy, i.e.
67% on 52 relations.
For this reason, we decided to make available the
DS dataset, the manually annotated test set and the
computational data (tree and sequential structures
with labels).
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the 5th ACM International Confer-
ence on Digital Libraries, pages 85?94.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of IJCAI, pages 2670?2676.
Sergey Brin. 1998. Extracting patterns and relations
from world wide web. In Proceedings of WebDB
Workshop at 6th International Conference on Extend-
ing Database Technology, pages 172?183.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT-EMNLP, pages 724?731, Vancou-
ver, British Columbia, Canada, October.
Razvan C. Bunescu. 2007. Learning to extract relations
from the web using minimal supervision. In Proceed-
ings of ACL.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS?2001), pages
625?632.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press, Cambridge, United Kingdom.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
ACL, pages 423?429, Barcelona, Spain, July.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ace) programtasks, data, and evaluation. In Proceed-
ings of LREC, pages 837?840, Barcelona, Spain.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL, pages
415?422, Barcelona, Spain, July.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of ACL, pages 286?295, Uppsala, Sweden,
July.
Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models
for information extraction. In The Companion Volume
to the Proceedings of ACL, pages 178?181, Barcelona,
Spain, July.
Metaweb Technologies. 2010. Freebase wikipedia ex-
traction (wex), March.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-AFNLP,
pages 1003?1011, Suntec, Singapore, August.
Alessandro Moschitti. 2004. A study on convolution ker-
nels for shallow statistic parsing. In Proceedings of
ACL, pages 335?342, Barcelona, Spain, July.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceedings of CIKM, pages 253?262, New York, NY,
USA. ACM.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. In Proceedings of EMNLP, pages
1378?1387, Singapore, August.
281
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2010. Kernel-based re-ranking for
named-entity extraction. In Proceedings of COLING,
pages 901?909, China, August.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lecture
Notes in Computer Science, pages 148?163. Springer
Berlin / Heidelberg.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago - a core of semantic knowl-
edge. In 16th international World Wide Web confer-
ence, pages 697?706.
Alexander Yates. 2009. Extracting world knowledge
from the web. IEEE Computer, 42(6):94?97, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2002. Kernel methods for relation
extraction. In Proceedings of EMNLP-ACL, pages
181?201.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP?2005, Lecture Notes in Computer Science (LNCS
3651), pages 378?389, Jeju Island, South Korea.
Min Zhang, Jie Zhang, Jian Su, , and Guodong Zhou.
2006. A composite kernel to extract relations between
entities with both flat and structured features. In Pro-
ceedings of COLING-ACL 2006, pages 825?832.
282
