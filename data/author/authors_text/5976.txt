Ext ract ing  the Names  of Genes  and Gene Products  w i th  a 
H idden Markov  Mode l  
Nige l  Co l l i e r ,  Ch ikash i  Nobata  and J un - i ch i  Tsu j i i  
l )el)artm(mt of Information Science 
(h'aduate School of Science 
University of Tokyo, Hongo-7-3-1 
Bunkyo-ku,  Tokyo 113, .Japan 
E-maih {n ige l ,  nova, t su j  ??}@?s. s. u - tokyo ,  ac. jp 
Abst ract  
\~e report the results of a study into the use 
of a linear interpolating hidden Marker model 
(HMM) for the task of extra.('ting lxw\]mi(:al |;er- 
minology fl:om MEDLINE al)stra('ts and texl;s 
in the molecular-bioh)gy domain. Tiffs is the 
first stage isl a. system that will exl;ra('l; evenl; 
information for automatically ut)da.ting 1)ioh)gy 
databases. We trained the HMM entirely with 
1)igrams based (m lexical and character fea- 
tures in a relatively small corpus of 100 MED- 
LINE abstract;s that were ma.rked-ul) l)y (lo- 
main experts wil;h term (:lasses u(:h as t)rol;eins 
and DNA. I.Jsing cross-validation methods we 
a(:\]fieved a,n \].e-score of 0.73 and we (',xmnine the 
('ontrilmtion made by each 1)art of the interl)o- 
lation model to overconfing (la.ta Sl)arsen('.ss. 
1 In t roduct ion  
Ill the last few ye~trs there has t)een a great in- 
vestment in molecula.r-l)iology resear(:h. This 
has yielded many results l;\]la.1;, 1;ogel;her wil;h 
a migration of m:c\]fival mal;erial to the inter- 
net, has resulted in an exl)losion in l;tm nuns- 
\])el7 of research tmbli('ations aa~ailat)le in online 
databases. The results in these 1)al)ers how- 
ever arc not available ill a structured fornmt and 
have to 1)e extracted and synthesized mammlly. 
Updating databases such as SwissProt (Bairoch 
mid Apweiler, 1.997) this way is time (:onsmning 
and nmans l;h~tt he resull;s are not accessible so 
conveniently to he11) researchers in their work. 
Our research is aimed at autonmti(:ally ex- 
tra(:ting facts Kern scientific abstracts and flfll 
papers ill the molecular-biology domain and us- 
ing these to update databases. As the tirst stage 
in achieving this goal we have exl)lored th(; use 
of a generalisable, supervised training method 
based on hidden Markov models (ItMMs) (Ra- 
biner and .\]uang, 1986) fbr tim identification mid 
classitieation of technical expressions ill these 
texts. This task can 1)e considered to be similar 
to the named c.ntity task in the MUC evaluation 
exercises (MUC, 1995). 
In our current work we are using abstracts 
available fl:om PubMed's MEDLINE (MED- 
\],INE, 1999). The MEDLINE (lnta.l)ase is an 
online collection of al)straets for pul)lished jour- 
nal articles in biology mid medicine and con- 
tains more than nine million articles. 
With the rapid growth in the mlmbcr of tmb- 
\]ished l)al)ers in the field of moh;('ular-biolog 3, 
there has been growing interest in the at)pli- 
cation of informa.tion extra(:tion, (Sekimizu et 
al., 1998) (Collier et al, 1999)(Thomas et al, 
1999) (Craven and Kmnlien, 1999), to help solve 
souse (sf the t)robhmss that are associated with 
information overload. 
In the remainder of this i)aper we will first 
of all (ratline the t)ackground to the task and 
then d(~s('ril)e t;hc basics of ItMMs and the fi)r- 
real model wc are using. The following sections 
give an outline of a. lse\v tagged ('orlms (Ohta et 
al., 1999) thnt our team has deveh)i)ed using al)- 
stra('ts taken from a sub-domain of MEDLINF, 
and the results of our experinmnts on this cor- 
lmS. 
2 Background 
Ileeent studies into the use of SUl)ervised 
learning-t)ased models for the n~mled entity task 
in the miero-lsioh)gy domain have. shown that 
lnodels based on HMMs and decision trees such 
as (Nol)al;~t et al, 1999) ~,r(; much more gener- 
alisable and adaptable to slew classes of words 
than systems based on traditional hand-lmilt 
1)attexns a.nd domain specific heuristic rules 
such as (Fukuda et al, 1998), overcoming the 
1)rol)lems associated with data sparseness with 
the help of sophisticated smoothing algorithms 
201 
(Chen and Goodman, 1996). 
HMMs can be considered to be stochastic fi- 
nite state machines and have enjoyed success 
in a number of felds including speech recogni- 
tion and part-of-speech tagging (Kupiec, 1992). 
It has been natural therefore that these mod- 
els have been adapted tbr use in other word- 
class prediction tasks such as the atoned-entity 
task in IE. Such models are often based on n- 
grams. Although the assumption that a word's 
part-of speech or name class can be predicted 
by the previous n-1 words and their classes is 
counter-intuitive to our understanding of lin- 
guistic structures and long distance dependen- 
cies, this simple method does seem to be highly 
effective ill I)ractice. Nymble (Bikel et al, 
1997), a system which uses HMMs is one of the 
most successflfl such systems and trains on a 
corpus of marked-up text, using only character 
features in addition to word bigrams. 
Although it is still early days for the use of 
HMMs for IE, we can see a number of trends 
in the research. Systems can be divided into 
those which use one state per class such as 
Nymble (at the top level of their backoff model) 
and those which automatically earn about the 
model's tructure such as (Seymore t al., 1999). 
Additionally, there is a distinction to be made 
in the source of the knowledge for estimating 
transition t)robabilities between models which 
are built by hand such as (Freitag and McCal- 
lure, 1999) and those which learn fl'om tagged 
corpora in the same domain such as the model 
presented in this paper, word lists and corpora 
in different domains - so-called distantly-labeled 
data (Seymore t al., 1999). 
2.1 Challenges of name finding in 
molecu lar -b io logy  texts  
The names that we are trying to extract fall into 
a number of categories that are often wider than 
the definitions used for the traditional named- 
entity task used in MUC and may be considered 
to share many characteristics of term recogni- 
tion. 
The particular difficulties with identit)dng 
and elassit~qng terms in the molecular-biology 
domain are all open vocabulary and irrgeular 
naming conventions as well as extensive cross- 
over in vocabulary between classes. The irreg- 
ular naming arises in part because of the num- 
ber of researchers from difli;rent fields who are 
TI - Activation of <PROTEIN> JAK kinases 
</PROTEIN> and <PROTEIN>STAT pTvteins 
</PR, OTEIN> by <PROTEIN> interlcukin - 2 
</PROTEIN> and <PROTEIN> intc~fc~vn alph, a
</PROTEIN> , but not the <PROTEIN> T cell 
antigen receptor <~PROTEIN> , in <SOURCE.ct> 
h, uman T lymphoeytes </SOURCE.et> . 
AB The activation of <PROTEIN> Janus 
protein t,.flvsine kinascs </PROTEIN> ( 
<PROTEIN> JAI(s </PROTEIN> ) and 
<PROTEIN> signal transducer and ac- 
tivator of transcription </PROTEIN> ( 
<PROTEIN> STAT </PROTEIN> ) pro- 
reins by <PROTEIN> intcrIcukin ( IL ) 2 
</PROTEIN> , thc  <PROTEIN> T cell antigen 
receptor </PROTEIN> ( <PROTEIN> TCR 
</PROTEIN> ) and <PROTEIN> intc~fcrvn 
( IFN)  alpha </PROTEIN> was czplorcd in 
<SOURCE.ct> human periph, cral blood- derived 
T cclls </SOURCE.et> and the <SOURCE.el> 
leukemic T cell line Kit225 </SOURCE.el> .
Figure 1: Example MEDLINE sentence marked 
up in XML for lfiochemical named-entities. 
working on the same knowledge discovery area 
as well as the large number of substances that 
need to be named. Despite the best, etforts of 
major journals to standardise the terminology, 
there is also a significant problem with syn- 
onymy so that often an entity has more tlm.n 
one name that is widely used. The class cross- 
over of terms arises because nla l ly  prot(:ins are 
named after DNA or RNA with which they re- 
act. 
All of the names which we mark up must be- 
long to only one of the name classes listed in 
Table 1. We determined that all of these name 
classes were of interest o domain experts and 
were essential to our domain model for event 
extraction. Example sentences from a nmrked 
ut) abstract are given in Figure 1. 
We decided not to use separate states ibr 
pre- and post-class words as had been used in 
some other systems, e.g. (Freitag and McCal- 
lure, 1999). Contrary to our expectations, we 
observed that our training data provided very 
poor maximum-likelihood probabilities for these 
words as class predictors. 
We found that protein predictor words had 
the only significant evidence and even this was 
quite weak, except in tlm case of post-class 
words which included a mmfi)er of head nouns 
such as "molecules" or "heterodimers". In our 
202 
Class ~/: Examl)le l)escription 
P1K)TEIN 21.25 .MK ki'n,a.se 
\])NA 358 IL-2 \]rlvmotcr 
\]{NA 30 771I?, 
S()UI{CF,.cl 93 le'ukemic T cell line Kit225 
S()UI\],CE.(:t 417 h,'wm, an T lymphocytes 
SOURCE.too 21 ,%hizosacch, aromyces pombc 
S()URCE.mu 64 mice 
SOURCE.vi 90 ItJV-1 
S()UI{CE.sl 77 membrane 
S()UI{CE.ti 37 central 'ner,vo'us system 
UNK t,y~vsine ph, osphovylal, ion 
t)ro{xfiils~ protein groups, 
families~ cOral)loxes and Slll)Sl;I'llCI;lll'eS. 
I)NAs I)NA groups, regions and genes 
RNAs I~NA groups, regions and genes 
cell line 
(:ell type 
lllOll()-organism 
multiorganism 
viruses 
sublocat;ion 
tissue 
lmckground words 
Table l: Named (mtilsy (:lasses. ~/: indi(:at(ts tsfic ~mmt)cr of XMI, tagged terms in our (:orpus of 100 
abstracts. 
early experiments using I IMMs that in(:orpo- 
rated pro- and 1)ost-c\]ass tates we \[imnd tha.t 
pcrforlnance was signiticantly worse than wil;h- 
Ollt; sll(;h si;at;cs an(l st) w('. formulated the ~uodcl 
as g,~ivcll i l S(;(;\[;iOll :/. 
~,.f(Qi,..~,l < _,Ffi,..~.,, >) + 
(1) 
and for all other words and their name classes 
as tbllows: 
3 Mx.'tzho d 
The lmrl)osc of our mod(;1 is Io lind t;hc n,osl: 
likely so(tilth, liCe of name classes (C) lbr a given 
se(tucncc of wor(ls (W). The set of name ('lasses 
inchutcs the 'Unk' name (:lass whi('h we use li)r 
1)ackgromM words not 1)elonging to ally ()\[ the 
interesting name classes given in Tal)lc 1 and 
t;hc given st;qu(m(:e of words which w(~ ,>('. spans 
a single s(,Jd;cn('c. The task is thcrcfor(~ 1(} max- 
intize Pr((TIH:). \?c iml)lem(mt a I \ ]MM to es- 
t imate this using th('. Markov assuml)tion that 
P r (C I I?  ) can be t'(mnd from t)igrams of ha.me 
classes. 
In th('. following model we (:onsid(u" words to 
1)c ordered pairs consisting of a. surface word, 
W, and a. word tbature, 1", given as < W, F >. 
The word features thcms('Jvcs arc discussed in 
Section 3.1. 
As is common practice, we need to (:alculatc 
the 1)rol)abilities for a word sequence for the 
first; word's name class and every other word 
diflbrently since we have no initial nalnt>class 
to make a transit ion frolll. Accordingly we use 
l;he R)llowing equation to (:alculatc the ilfitial 
name (:lass probability, 
~,,J'(Cz,..~,,I < wi~,..~, 19~,.,~,, >) + 
I',,.( G 
)~o.1' ( G 
A ,./' (G 
;v~.f (G 
5:I./'(G 
), ~./' (G 
),,~.I(G) 
< Wt,l,} >,< l lS,_,, l , i  ~ >,G J) :- 
< 1'15., I ~,, >, < I,V~_~, l )_j >, G.-~) + 
< _, l'i >, < 115_ l, Ft,- ~ >, Ct-., ) + 
< 115, Fi >, < _, P~,_~ >, G ~) + 
< _, l,) >, < ._, 1% ~ >, C~__~) + 
(2) 
whc,:c f(I) is ('alculatcd with nmxinluln- 
likelihood estimates from counts on training 
data, so that tbr example, 
.f(G,I < 1,~5,1,i >,< I,t,~_,, F~_~ >,G-~)  - 
T(< I lS, 1,~ >, G., < 1'15_,, 1~}_~ >, G.-@, 
T(< l'lZj,,l~J >,< \ [ 'Vt- l ,Ft- I  >,Ct- l )  ~3) 
Where T() has been found from counting the 
events in thc training cortms. In our current 
sysl;oln \vc SC\[; t;tlc C()llSt;&lltS ~i }lJld o- i \])y halld 
all(l let ~ ai = 1.0, ~ Ai = 1.0, a0 > al k O-2, 
A0 > A I . . .  _> As. Tile current name-class Ct 
is conditioned oil the current word and fea- 
t;llrc~ thc I)rcviolls name-class, ~*t--l: and t)rc- 
vious word an(t tbaturc. 
Equations 1 and 2 implement a linear- 
interpolating HMM that  incorporates a mmfl)cr 
203 
of sub-models (rethrred to fl'om now by their 
A coefficients) designed to reduce the effects of 
data sparseness. While we hope to have enough 
training data to provide estimates tbr all model 
parameters, in reality we expect to encounter 
highly fl'agmented probability distributions. In 
the worst case, when even a name class pair 
has not been observed beibre in training, the 
model defaults at A5 to an estimate of name 
class unigrams. We note here that the bigram 
language model has a non-zero probability asso- 
ciated with each bigram over the entire vocal)- 
ulary. 
Our model differs to a backoff ormulation be- 
cause we tbund that this model tended to suffer 
fl'om the data sparseness problem on our small 
training set. Bikel et alfor example consid- 
ers each backoff model to be separate models, 
starting at the top level (corresl)onding approx- 
imately to our Ao model) and then falling back 
to a lower level model when there not enough 
evidence. In contrast, we have combined these 
within a single 1)robability calculation tbr state 
(class) transitions. Moreover, we consider that 
where direct bigram counts of 6 or more occur 
in the training set, we can use these directly to 
estimate the state transition probability and we 
nse just the ,~0 model in this case. For counts 
of less than 6 we smooth using Equation 2; this 
can be thought of as a simt)le form of q)nck- 
eting'. The HMM models one state per name 
(:lass as well as two special states tbr the start 
and end o fa  sentence. 
Once the state transition l)rol)abilities have 
been calcnlated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of 1)ossible name class as- 
signments. This is done in linear time, O(MN 2) 
for 54 the nunfl)er of words to be classified and 
N the number of states, to find the highest prob- 
ability path, i.e. to maxinfise Pr(W,  C). In our 
exl)eriments 5/i is the length of a test sentence. 
The final stage of our algorithm that is used 
after name-class tagging is complete is to use 
~ clean-up module called Unity. This creates a 
frequency list of words and name-classes tbr a 
docmnent and then re-tags the document using 
the most frequently nsed name class assigned by 
the HMM. We have generally tbund that this 
improves F-score performance by al)out 2.3%, 
both tbr re-tagging spuriously tagged words and 
Word Feature Exmnl)le 
DigitNmnber 15 
SingleCap M 
GreekLetter alpha 
CapsAndDigits I2 
TwoCaps RalGDS 
LettersAndDigits p52 
hfitCap Interleukin 
LowCaps ka,t)paB 
Lowercase kinases 
IIyphon 
Backslash / 
OpenSquare \[ 
CloseSquare \] 
Colon 
SemiColon 
Percent % 
Oi) enParen ( 
CloseParen ) 
Comma 
FullStop 
Deternliner the 
Conjmmtion and 
Other * + 
Table 2: Word tbatures with examples 
tbr finding untagged words in mlknown contexts 
that had been correctly tagged elsewhere in the 
text. 
3.1 Word  features  
Table 2 shows the character t'eatnres that we 
used which are based on those given for Nymble 
and extended to give high pertbrmance in both 
molecular-biology and newswire domains. The 
intnition is that such features provide evidence 
that helps to distinguish nmne classes of words. 
Moreover we hyt)othesize that such featnres 
will help the model to find sinfilarities between 
known words that were tbnnd in the training 
set and unknown words (of zero frequency in 
the training set) and so overcome the unknown 
word t)rol)lem. To give a simple example: if we 
know that LMP - 1 is a member of PROTEIN  
and we encounter AP - 1 for the first time in 
testing, we can make a fairly good guess about 
the category of the unknown word 'LMP' based 
on its sharing the same feature TwoCaps  with 
the known word 'AP' and 'AP's known relation- 
ship with '- 1'. 
Such unknown word evidence is captured in 
submodels A1 through ),3 in Equation 2. \?e 
204 
consider that character information 1)rovides 
more mealfingflll distinctions between name 
(;\]asses than for examI)le part-of-speech (POS), 
since POS will 1)redominmltly 1)e noun fi)r all 
name-class words. The t'catures were chosen 
to be as domain independent as possit)le, with 
the exception of I lyphon and Greel,:Letter which 
have t)articular signitieance for the terminology 
in this dolnain. 
4 Exper iments  
4.1 Tra in ing  and  tes t ing  set 
The training set we used in our experiments 
('onsisted of 100 MEI)II, INI~ al)stra(:ts, marked 
Ul) ill XS/\[L l)y a (lonmin ext)ert for the name 
('lasses given in Tal)le 1. The mmfl)er of NEs 
that were marked u 1) by class are also given in 
Tfl)le 1 and the total lmmber of words in the 
corlms is 299/\]:0. The al)stracts were chosen from 
a sul)(lomain of moleeular-1)iology that we for- 
mulated by s(',ar(;hing under the terms h/uman, 
blood cell, trav,.scription ,/'actor in the 1)utiMed 
datal)asc, This yiel(l('.(t al)t)roximately 33(10 al/- 
stracts.  
4.2 Resu l ts  
The results are given as F-scores, a (;Ollllll()ll 
measurement for a(:(:ura(:y in tlw, MUC con- 
ferences that eonfl)ines r(;(:all and 1)re(:ision. 
These are eah:ulated using a standard MUC tool 
(Chinchor, 1995). F-score is d('.iin(~d as 
'2 x lS"(eci.sion x l~cc, ll 
F - .~cor.  = (4) 
l)'rccisio~, + \]?,cc(dl 
The tirst set ot7 experiments we did shows the 
effectiveness of the mode.1 for all name (:lasses 
and is smnmarized in Table 3. We see that data 
sparseness does have an etfe('t~ with 1)roteins - 
the most mlmerous (;lass in training - getting 
the best result and I/,NA - the snmllc, st training 
(:lass - getting the worst result. The tal)le also 
shows the ett'eetiveness of the character feature 
set, whi('h in general adds 10.6% to the F-score. 
This is mainly due to a t)ositive effect on words 
in the 1)R,OTEIN and DNA elases, but we also 
see that memt)ers of all SOURCE sul)-('lasses 
sufl'er from featurization. 
We have atteml)ted to incorl)orate generali- 
sation through character t'eatm:es and linear in- 
teri)olation, which has generally \])een quite su(:- 
cessful. Nevertheless we were (:urious to see just 
Class Base llase-l'eatures 
PROTEIN 0.759 0.670 (-11.7%) 
DNA 0.472 0.376 (-20.3%) 
\]~NA 0.025 0.OOO (-leo.o%) 
SOURCE(all) 0.685 0.697 (+1.8%) 
S()UI{CE.cl 0.478 0.503 (+5.2%) 
SOURCE.el 0.708 0.752 (+6.2%) 
SOURCE.me 0.200 0.311 (+55.5%) 
SOURCE.mu 0.396 0.402 (+1.5%) 
SOURCE.vi 0.676 0.713 (+5.5%) 
S()URCI,Lsl 0.540 0.549 (+1.7%) 
SOURCE.ti 0.206 0.216 (+4.9%) 
All classes 0.728 0.651 (-10.6%) 
q)d)le 3: Named entity acquisition results us- 
ing 5-fi)ld cross validation on 100 XML tagged 
MEI)I~INE al/stra(:ts, 80 for training and 20 fin. 
testing, l\]ase-J'(',at'urc.s u es no character feature 
inibrmation. 
)~ Mode\[ No. 
# Texts 0 1 2 3 4 5 
80 
40 
20 
10 
5 
0.06 0.22 0.10 0.67 0.93 1.0 
0.06 0.19 0.10 0.63 0.94 1.0 
().()~l 0.15 0.09 0.59 0.89 1.0 
0.03 0.12 0.08 0.52 0.83 1.0 
0.02 0.09 0.06 0.41 0.68 1.0 
Tal)le 4: M(',an lmml)er of successflll calls to sul)- 
m(i(t(;ls during testing as a fl'aetion of total mnn- 
1)er (If stale transitions in the Viterl)i latti(:e, g/: 
T(!xis indicates the mmfl)er of al)stra(:ts used ill 
training. 
whi(:h t)arts of the model were contributing to 
the bigram s(:ores. Table 4 shows the l)ercent- 
age of bigranls which could be mat('hed against 
training t)igrams. The result indicate tha~ a 
high 1)ereentage of dire(:t bigrams in the test 
eorl)uS never al)t)(;ar in the training (:oft)us and 
shows tha, t our HMM model is highly depel> 
(l(mt on smoothing through models ~kl and )~:~. 
\?e can take another view of the training data 
1)y 'salalni-slieing' the model so that only evi- 
(tenee from 1)art of the model is used. Results 
are shown in Tat)le 5 and support the eonchl- 
sion that models Al, A2 and Aa are. crucial at 
this sir,(; of training data, although we would 
expect their relative ilnportance to fifil as we 
have more (tircct observations of bigrams with 
larger training data sets. 
Tal)le 6 shows the rolmstness of the model 
205 
I Backoff models 
\[ F-score (all classes) 0.728 0.722 0.644 0.572 0.576 \] 
Table 5: F-scores using different nfixtures of models tested on 100 abstracts, 80 training and 20 
testing. 
I # Texts 80 40 20 10 5 \] 
I F-score 0.728 0.705 0.647 0.594 0.534\] 
Table 6: 
training 
stracts). 
F-score for all classes agMnst size of 
corpus (in number of MEDLINE ab- 
for data sparseness, so that even with only 10 
training texts the model can still make sensible 
decisions about term identification and classi- 
fication. As we would expect;, the table ;flso 
clearly shows that more training data is better, 
and we have not yet reached a peak in pertbr- 
i nance .  
5 Conc lus ion  
HMMs are proving their worth for various 
tasks in inibrmation extraction and the results 
here show that this good performance can be 
achieved across domains, i.e. in molecular- 
biology as well as rising news paper reports. The 
task itself', while being similar to named entity 
in MUC, is we believe more challenging due to 
the large nunfl)er of terms which are not proper 
nouns, such as those in the source  sub-classes as 
well as the large lexieal overlap between classes 
such as PROTEIN  and DNA. A usefifl line of 
work in the future would be to find empirical 
methods for comparing difficulties of domains. 
Unlike traditional dictionary-based lnethods, 
the method we have shown has the advantage of 
being portable and no hand-made patterns were 
used. Additiolmlly, since the character tbatures 
are quite powerful, yet very general, there is lit- 
tle need for intervention to create domain spe- 
cific features, although other types of features 
could be added within the interpolation frame- 
work. Indeed the only thing that is required is 
a quite small corpus of text containing entities 
tagged by a domain expert. 
Currently we have optinfized the ,k constants 
by hand but clearly a better way would be to do 
this antomatically. An obvious strategy to use 
would be to use some iterative learning method 
such as Expectation Maximization (Dempster 
et al, 1977). 
The model still has limitations, most obvi- 
ously when it needs to identity, term boundaries 
for phrases containing potentially ambiguous lo- 
cal structures uch as coordination and pa.ren- 
theses. For such cases we will need to add post- 
processing rules. 
There are of course many NF, models that 
are not based on HMMs that have had suc- 
cess in the NE task at the MUC conferences. 
Our main requirement in implementing a model 
for the domain of molecular-biology has been 
ease of development, accuracy and portability 
to other sub-domains since molecular-biology it-
self is a wide field. HMMs seemed to be the 
most favourable option at this time. Alterna- 
tives that have also had considerable success 
are decision trees, e.g. (Nobata et al, 1.999) 
and maximum-entropy. The maximum entropy 
model shown in (Borthwick et al, 1998) in par- 
ticular seems a promising approach because of 
its ability to handle overlapping and large fea- 
ture sets within n well founded nmthenmtical 
ti'amework. However this implementation of the 
method seems to incorporate a number of hand- 
coded domain specitic lexical Datures and dic- 
tionary lists that reduce portability. 
Undoubtedly we could incorporate richer tba- 
tures into our model and based on the evidence 
of others we would like to add head nouns as 
one type of feature in the future. 
Acknowledgements  
We would like to express our gratitude to Yuka 
Tateishi and Tomoko Ohta of the Tsujii labora- 
tory for their efforts to produce the tagged cor- 
tins used in these experiments and to Sang-Zoo 
Lee also of the Tsujii laboratory tbr his com- 
ments regarding HMMs. We would also like to 
thank the anonymous retirees tbr their helpflfl 
comments. 
206 
\]~{,eferences 
A. Bairoch and R. Apweiler. 1997. The SWISS- 
PF\[OT 1)r{)t{~in sequence data bank and its 
new SUl)l)lement 15:EMBL. Nucleic Acids Re- 
search, 25:31-36. 
D. Bikel, S. Miller, I:L Schwartz, and 
R. Wesichedel. 1997. Nymble: a high- 
t)ertbrmanee l arning \]mlne-tin(ler. In Pro- 
ceedings of the Fifth Co~@rcrcncc on Applied 
Natural Langua9 e \])~vcessi'n,g, pages 194 201. 
A. Borthwick, J. Sterling, E. Agichtein, and 
ll,. Grishman. 1998. Ext}l{}iting div(:rse 
knowledge sour(:es via lllaXillllllll (mtrol}y in 
named entity recogniti{}n. In P'mcccdings 
of the Worlcshop on Very Lar.qc Corpora 
(WVLC'98). 
S. Chert and J. Goodman. 1996. An empirical 
study of smoothing te{:hmfiques tbr language 
motleling. 3/tst Annual Meeting of tlt,(: Associ- 
ation of Computational Linguistics, Calffof 
nia, USA, 24-27 .hme. 
N. Chin{:h{}r. 1995. MUC-5 ewduati{m etrics. 
In In Pwcecdings of th, c i"ffl, h, Mc.ss(u.le Un- 
dcrstandin 9 Cou:fe'rencc (MUC-5), Baltimore,, 
Maryland, USA., 1)ages 69 78. 
N. Collier, It.S. Park, N. Ogata, Y. Tateishi, 
C. Nol}ata, 'F. Ohta, T. Sekimizu, H. \]mai, 
and J. Tsujii. 1999. The GENIA 1}r{)je(:t: 
corlms-1)ascd kn(}wlcdge acquisitio\], and in- 
forlnal, ion extra('tion f\]'Olll genome r{',sear(:h 
t)al)ers, in Proccediu, fl.s of the A n',,'aal M(',eting 
of the European ch, aptcr of the Association for 
Computational Lingu'istic,s (EA (/\]3 '99), 3 uuc. 
M. Craven and 3, Kumlien. 1999. Construct- 
ing bioh}gical knowh;{tg{; t}ases t)y extracting 
information from text sour(:es. In \]}~vc(:(,Aings 
of the 7th, hl, tcrnational CoTff(:rence on Intelli- 
gent Systcmps for Molecular Biology (ISMB- 
99), Heidellmrg, Germmly, August 6 10. 
A.P. Dempster, N.M. Laird, and D.B. Rubins. 
1977. Maximmn likelihood from incoml)lete 
data via the EM algorithm. ,\]ou'rnal of the 
Royal Statistical Society (B), 39:1-38. 
l). Freitag and A. McCMlum. 1999. Intbrma- 
tion extraction with HMMs and shrinkage. 
In Proceedings of the AAAl'99 Worl~.~h, op ou, 
Machine Learning for IT~:formation Extrac- 
tion, Orlando, Florida, July 19th. 
K. Fuku(la, T. Tsunoda, A. 2)mmra, and 
T. Takagi. 1998. ~12)ward intbrmation extrac- 
tion: identifying l)rotein names from biologi- 
eal papers. Ill PTvcccdings of thc Pac'lific Sym- 
posium on Biocomp'uting'98 (PSB'98), .Jan- 
1uAYy. 
.1. Kupiec. 1992. l/obust Imrt-ofspeech tag- 
ging using a hidden markov model. Computer 
Speech and Lang'aagc, 6:225-242. 
MEI)LINE. 1999. The PubMed 
datal)ase can be t'(mnd at:. 
httt)://www.ncbi.nhn.nih.gov/Pul}Med/. 
DAIIPA. 1995. l}roceeding.s o.fl th, c Sixth, 
Message Understanding Cou:fcrcnce(MUC-6), 
Cohmdfia, MI), USA, Nove, nfl}er. Morgan 
Nail\['\] l lal l l l .  
C. Nobata, N. Collier, and J. Tsu.iii. 1999. Au- 
tomatic term identification and classification 
in 1}iology texts. In Proceeding.s" of the Nat- 
u'ral Lang,lmgc Pacific Rim Symposium (NL- 
PRS'2000), November. 
Y. Ohta, Y. Tateishi, N. Collie'r, C. No- 
1)ata, K. II}ushi, and J. Tsujii. 1999. A 
senmntieally annotated cort)us from MED- 
L\]\[NE al)sl;ra{:l;s. In l}'rocccd,bu.l s of th.c ~:nth. 
Workshop on Go'home I~fformatics. Universal 
A{:ademy Press, Inc., 14 15 Deccntl)er. 
l~. llabiner and B..\]uang. 1!)86. An intro{tu{:- 
ti(m to hidden Markov too(Ms. H'2EE ASSP 
Magazi',,(',, 1}ages d 16, Jammry. 
T. Sekilnizu, H. Park, and J. 'l'sujii. 1998. 
I{lenti\[ying l;he interaction 1)etween genes an{1 
gOlle i}ro(lucts \]}ase(l on f\]'e(lue\]My seen verbs 
in n\]e{tline al)si;rael;s. Ill ~(:'li,()?ll,('~ \]~ffor'm, al, ics'. 
Univcrsa,1 Academy Press, Inc. 
K. Seymore, A. MeCallum, and l{. I{oscnfeld. 
1999. Learning hidden Markove strucl:ure 
for informati{m (,xtraction. In \])wcccdings of 
the AAAl'99 Workshop on Macfli'n,(: Lcarni'n 9 
for l',fo'rmation E:draction, Orland{}, Flori{ta., 
July 19th. 
.J. Thomas, D. Milward, C. Ouzounis, S. Pul- 
man, and M. Carroll. 1999. Automatic ex- 
traction of 1)rotein interactions fl'om s{'ien- 
tific abstracts. In Proceedings of the I}ac'll/ic 
Symposium on Biocomputing'99 (PSB'99), 
Hawaii, USA, Jmmary 4-9. 
A. 3. Vit(;rbi. 1967. Error l){mnds for {:onvolu- 
tions e{}{les and an asyml)totically optimum 
deco(ling algorithm. IEEE Tran,s'actiou,.s' on
I~formation Theory, IT-13(2):260 269. 
207 
Morphological Analysis of The Spontaneous Speech Corpus
Kiyotaka Uchimoto?, Chikashi Nobata?, Atsushi Yamada?,
Satoshi Sekine?, and Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes a project tagging a sponta-
neous speech corpus with morphological infor-
mation such as word segmentation and parts-of-
speech. We use a morphological analysis system
based on a maximum entropy model, which is
independent of the domain of corpora. In this
paper we show the tagging accuracy achieved by
using the model and discuss problems in tagging
the spontaneous speech corpus. We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
1 Introduction
In recent years, systems developed for analyz-
ing written-language texts have become consid-
erably accurate. This accuracy is largely due
to the large amounts of tagged corpora and the
rapid progress in the study of corpus-based nat-
ural language processing. However, the accu-
racy of the systems developed for written lan-
guage is not always high when these same sys-
tems are used to analyze spoken-language texts.
The reason for this remaining inaccuracy is due
to several differences between the two types of
languages. For example, the expressions used
in written language are often quite different
from those in spoken language, and sentence
boundaries are frequently ambiguous in spoken
language. The ?Spontaneous Speech: Corpus
and Processing Technology? project was imple-
mented in 1999 to overcome this problem. Spo-
ken language includes both monologue and dia-
logue texts; the former (e.g. the text of a talk)
was selected as a target of the project because it
was considered to be appropriate to the current
level of study on spoken language.
Tagging the spontaneous speech corpus with
morphological information such as word seg-
mentation and parts-of-speech is one of the
goals of the project. The tagged corpus is help-
ful for us in making a language model in speech
recognition as well as for linguists investigat-
ing distribution of morphemes in spontaneous
speech. For tagging the corpus with morpholog-
ical information, a morphological analysis sys-
tem is needed. Morphological analysis is one of
the basic techniques used in Japanese sentence
analysis. A morpheme is a minimal grammat-
ical unit, such as a word or a suffix, and mor-
phological analysis is the process of segment-
ing a given sentence into a row of morphemes
and assigning to each morpheme grammatical
attributes such as part-of-speech (POS) and in-
flection type. One of the most important prob-
lems in morphological analysis is that posed by
unknown words, which are words found in nei-
ther a dictionary nor a training corpus. Two
statistical approaches have been applied to this
problem. One is to find unknown words from
corpora and put them into a dictionary (e.g.,
(Mori and Nagao, 1996)), and the other is to
estimate a model that can identify unknown
words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both
approaches. They proposed a morphological
analysis method based on a maximum entropy
(M.E.) model (Uchimoto et al, 2001). We used
their method to tag a spontaneous speech cor-
pus. Their method uses a model that can not
only consult a dictionary but can also identify
unknown words by learning certain characteris-
tics. To learn these characteristics, we focused
on such information as whether or not a string
is found in a dictionary and what types of char-
acters are used in a string. The model esti-
mates how likely a string is to be a morpheme.
This model is independent of the domain of cor-
pora; in this paper we demonstrate that this is
true by applying our model to the spontaneous
speech corpus, Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem of
Japanese morphological analysis can be reduced
to the problem of assigning one of two tags to
each string in a sentence. A string is tagged
with a 1 or a 0 to indicate whether or not it is
a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. The 1
tag is thus divided into the number, n, of gram-
matical attributes assigned to morphemes, and
the problem is to assign an attribute (from 0
to n) to every string in a given sentence. The
(n + 1) tags form the space of ?futures? in the
M.E. formulation of our problem of morpholog-
ical analysis. The M.E. model enables the com-
putation of P (f |h) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
The computation of P (f |h) in any M.E. model
is dependent on a set of ?features? which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h, f) =
?
?
?
?
?
1 : if has(h, x) = true,
x = ?POS(?1)(Major) : verb,??
& f = 1
0 : otherwise.
(1)
Here ?has(h,x)? is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and what part-of-speech
the adjacent morpheme is.
Given a set of features and some training
data, the M.E. estimation process produces a
model, which is represented as follows (Berger
et al, 1996; Ristad, 1997; Ristad, 1998):
P (f |h) =
?
i
?
g
i
(h,f)
i
Z
?
(h)
(2)
Z
?
(h) =
?
f
?
i
?
g
i
(h,f)
i
. (3)
We define a model which estimates the like-
lihood that a given string is a morpheme and
has the grammatical attribute i(1 ? i ? n) as a
morpheme model. This model is represented by
Eq. (2), in which f can be one of (n + 1) tags
from 0 to n.
Given a sentence, it is divided into mor-
phemes, and a grammatical attribute is assigned
to each morpheme so as to maximize the sen-
tence probability estimated by our morpheme
model. Sentence probability is defined as the
product of the probabilities estimated for a par-
ticular division of morphemes in a sentence. We
use the Viterbi algorithm to find the optimal set
of morphemes in a sentence.
3 Experiments and Discussion
3.1 Experimental Conditions
We used the spontaneous speech corpus, CSJ,
which is a tagged corpus of transcriptions of
academic presentations and simulated public
speech. Simulated public speech is short speech
spoken specifically for the corpus by paid non-
professional speakers. For training, we used
805,954 morphemes from the corpus, and for
testing, we used 68,315 morphemes from the
corpus. Since there are no boundaries between
sentences in the corpus, we used two types of
boundaries, utterance boundaries, which are au-
tomatically detected at the place where a pause
of 200 ms or longer emerges in the CSJ, and
sentence boundaries assigned by the sentence
boundary identification system, which is based
on hand-crafted rules which use the pauses as
a clue. In the CSJ, fillers and disfluencies are
marked with tags (F) and (D). In the experi-
ments, we did not use those tags. Thus the in-
put sentences for testing are character strings
without any tags. The output is a sequence
of morphemes with grammatical attributes. As
the grammatical attributes, we define the part-
of-speech categories in the CSJ. There are 12
major categories. Therefore, the number of
grammatical attributes is 12, and f in Eq. (2)
can be one of 13 tags from 0 to 12.
Given a sentence, for every string consist-
ing of five or fewer characters and every string
appearing in a dictionary, whether or not the
string is a morpheme was determined and then
the grammatical attribute of each string deter-
mined to be a morpheme was identified and
assigned to that string. We collected all mor-
phemes from the training corpus except dis-
fluencies and used them as dictionary entries.
We denote the entries with a Corpus dictionary.
The maximum length for a morpheme was set
at five because morphemes consisting of six or
more characters are mostly compound words or
words consisting of katakana characters. We as-
sumed that compound words that do not appear
in the dictionary can be divided into strings con-
sisting of five or fewer characters because com-
pound words tend not to appear in dictionar-
ies. Katakana strings that are not found in the
dictionary were assumed to be included in the
dictionary as an entry having the part-of-speech
?Unknown(Major), Katakana(Minor).? An op-
timal set of morphemes in a sentence is searched
for by employing the Viterbi algorithm. The
assigned part-of-speech in the optimal set is se-
lected from all the categories of the M.E. model
except the one in which the string is not a mor-
pheme.
The features used in our experiments are
listed in Table 1. Each feature consists of a
type and a value, which are given in the rows of
the table. The features are basically some at-
tributes of the morpheme itself or attributes of
the morpheme to the left of it. We used the fea-
tures found three or more times in the training
corpus. The notations ?(0)? and ?(-1)? used in
the feature type column in Table 1 respectively
indicate a target string and the morpheme to
the left of it.
The terms used in the table are as follows:
String: Strings appearing as a morpheme three
or more times in the training corpus
Substring: Characters used in a string.
?(Left1)? and ?(Right1)? respectively rep-
resent the leftmost and rightmost charac-
ters of a string. ?(Left2)? and ?(Right2)?
respectively represent the leftmost and
rightmost character bigrams of a string.
Dic: Entries in the Corpus dictionary. As mi-
nor categories we used inflection types such
as a basic form as well as minor part-of-
speech categories. ?Major&Minor? indi-
cates possible combinations between major
and minor part-of-speech categories. When
the target string is in the dictionary, the
part-of-speech attached to the entry corre-
sponding to the string is used as a feature
value. If an entry has two or more parts-
of-speech, the part-of-speech which leads to
the highest probability in a sentence esti-
mated from our model is selected as a fea-
ture value.
Length: Length of a string
TOC: Types of characters used in a string.
?(Beginning)? and ?(End)?, respectively,
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the ?(Begin-
ning)? and ?(End)? are the same character.
?TOC(0)(Transition)? represents the tran-
sition from the leftmost character to the
rightmost character in a string. ?TOC(-
1)(Transition)? represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
character in the target string. For example,
when the adjacent morpheme on the left
is ??? (sensei, teacher)? and the target
string is ?? (ni, case marker),? the feature
value ?Kanji?Hiragana? is selected.
POS: Part-of-speech.
3.2 Results and Discussion
Results of the morphological analysis obtained
by our method are shown in Table 2. Recall
is the percentage of morphemes in the test cor-
pus whose segmentation and major POS tag are
identified correctly. Precision is the percentage
of all morphemes identified by the system that
are identified correctly. The F-measure is de-
fined by the following equation.
F ? measure =
2 ? Recall ? Precision
Recall + Precision
This result shows that there is no significant
difference between accuracies obtained by us-
ing two types of sentence boundaries. However,
we found that the errors that occurred around
utterance boundaries were reduced in the re-
sult obtained with sentence boundaries assigned
by the sentence boundary identification system.
This shows that there is a high possibility that
we can achieve better accuracy if we use bound-
aries assigned by the sentence boundary identi-
fication system as sentence boundaries and if we
use utterance boundaries as features.
In these experiments, we used only the en-
tries with a Corpus dictionary. Next we show
the experimental results with dictionaries de-
veloped for a corpus on a certain domain. We
added to the Corpus dictionary all the approx-
imately 200,000 entries of the JUMAN dictio-
nary (Kurohashi and Nagao, 1999). We also
added the entries of a dictionary developed by
ATR. We call it the ATR dictionary.
Results obtained with each dictionary or each
combination of dictionaries are shown in Ta-
ble 3. In this table, OOV indicates Out-of-
Vocabulary rates. The accuracy obtained with
the JUMAN dictionary or the ATR dictionary
was worse than the accuracy obtained without
those dictionaries. This is because the segmen-
Table 1: Features.
Feature number Feature type Feature value (Number of value)
1 String(0) (223,457)
2 String(-1) (20,769)
3 Substring(0)(Left1) (2,492)
4 Substring(0)(Right1) (2,489)
5 Substring(0)(Left2) (74,046)
6 Substring(0)(Right2) (73,616)
7 Substring(-1)(Left1) (2,237)
8 Substring(-1)(Right1) (2,489)
9 Substring(-1)(Left2) (12,726)
10 Substring(-1)(Right2) (12,241)
11 Dic(0)(Major) Noun, Verb, Adj, . . . Undefined (13)
12 Dic(0)(Minor) Common noun, Topic marker, Basic form. . . (223)
13 Dic(0)(Major&Minor) Noun&Common noun, Verb&Basic form, . . . (239)
14 Length(0) 1, 2, 3, 4, 5, 6 or more (6)
15 Length(-1) 1, 2, 3, 4, 5, 6 or more (6)
16 TOC(0)(Beginning) Kanji, Hiragana, Number, Katakana, Alphabet (5)
17 TOC(0)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
18 TOC(0)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (25)
19 TOC(-1)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
20 TOC(-1)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (18)
21 POS(-1) Verb, Adj, Noun, . . . (12)
22 Comb(1,21) Combinations Feature 1 and 21 (142,546)
23 Comb(1,2,21) Combinations Feature 1, 2 and 21 (216,431)
24 Comb(1,13,21) Combinations Feature 1, 13 and 21 (29,876)
25 Comb(1,2,13,21) Combinations Feature 1, 2, 13 and 21 (158,211)
26 Comb(11,21) Combinations Feature 11 and 21 (156)
27 Comb(12,21) Combinations Feature 12 and 21 (1,366)
28 Comb(13,21) Combinations Feature 13 and 21 (1,518)
Table 2: Results of Experiments (Segmentation and major POS tagging).
Boundary Recall Precision F-measure
utterance 93.97% (64,198/68,315) 93.25% (64,198/68,847) 93.61
sentence 93.97% (64,195/68,315) 93.18% (64,195/68,895) 93.57
tation of morphemes and the definition of part-
of-speech categories in the JUMAN and ATR
dictionaries are different from those in the CSJ.
Given a sentence, for every string consisting
of five or fewer characters as well as every string
appearing in a dictionary, whether or not the
string is a morpheme was determined by our
morpheme model. However, we speculate that
we can ignore strings consisting of two or more
characters when they are not found in the dic-
tionary when OOV is low. Therefore, we carried
out the additional experiments ignoring those
strings. In the experiments, given a sentence,
for every string consisting of one character and
every string appearing in a dictionary, whether
or not the string is a morpheme is determined
by our morpheme model. Results obtained un-
der this condition are shown in Table 4. We
compared the accuracies obtained with dictio-
naries including the Corpus dictionary, whose
OOVs are relatively low. The accuracies ob-
tained with the additional dictionaries increased
while those obtained only with the Corpus dic-
tionary decreased. These results show that a
dictionary whose OOV in the test corpus is low
contributes to increasing the accuracy when ig-
noring the possibility that strings that consist
of two or more characters and are not found in
the dictionary become a morpheme.
These results show that a dictionary devel-
oped for a corpus on a certain domain can be
used to improve accuracy in analyzing a corpus
on another domain.
The accuracy in segmentation and major
POS tagging obtained for spontaneous speech
was worse than the approximately 95% obtained
for newspaper articles. We think the main rea-
son for this is the errors and the inconsistency
of the corpus, and the difficulty in recognizing
characteristic expressions often used in spoken
language such as fillers, mispronounced words,
and disfluencies. The inconsistency of the cor-
pus is due to the way the corpus was made, i.e.,
completely by human beings, and it is also due
Table 3: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.64% (63,288/68,315) 91.83% (63,288/68,917) 92.24 1.84%
Corpus sentence 92.61% (63,265/68,315) 91.79% (63,265/68,923) 92.20 1.84%
JUMAN utterance 90.28% (61,676/68,315) 90.07% (61,676/68,478) 90.17 6.13%
JUMAN sentence 90.33% (61,710/68,315) 90.22% (61,710/68,403) 90.27 6.13%
ATR utterance 89.80% (61,348/68,315) 90.12% (61,348/68,073) 89.96 8.14%
ATR sentence 89.96% (61,453/68,315) 90.30% (61,453/68,057) 90.13 8.14%
Corpus+JUMAN utterance 92.03% (62,872/68,315) 91.77% (62,872/68,507) 91.90 0.52%
Corpus+JUMAN sentence 92.09% (62,913/68,315) 91.80% (62,913/68,534) 91.95 0.52%
Corpus+ATR utterance 92.35% (63,086/68,315) 92.03% (63,086/68,547) 92.19 0.64%
Corpus+ATR sentence 92.30% (63,057/68,315) 91.94% (63,057/68,585) 92.12 0.64%
JUMAN+ATR utterance 91.60% (62,579/68,315) 91.57% (62,579/68,339) 91.59 4.61%
JUMAN+ATR sentence 91.66% (62,618/68,315) 91.67% (62,618/68,311) 91.66 4.61%
Corpus+JUMAN+ATR utterance 91.72% (62,658/68,315) 91.66% (62,658/68,357) 91.69 0.47%
Corpus+JUMAN+ATR sentence 91.72% (62,657/68,315) 91.62% (62,657/68,391) 91.67 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
Table 4: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.80% (63,395/68,315) 90.47% (63,395/70,075) 91.62 1.84%
Corpus sentence 92.71% (63,333/68,315) 90.48% (63,333/70,000) 91.58 1.84%
Corpus+JUMAN utterance 92.45% (63,154/68,315) 91.60% (63,154/68,942) 92.02 0.52%
Corpus+JUMAN sentence 92.48% (63,179/68,315) 91.71% (63,179/68,893) 92.09 0.52%
Corpus+ATR utterance 92.91% (63,474/68,315) 91.81% (63,474/69,137) 92.36 0.64%
Corpus+ATR sentence 92.75% (63,361/68,315) 91.76% (63,361/69,053) 92.25 0.64%
Corpus+JUMAN+ATR utterance 92.30% (63,055/68,315) 91.57% (63,055/68,858) 91.94 0.47%
Corpus+JUMAN+ATR sentence 92.28% (63,039/68,315) 91.55% (63,039/68,860) 91.91 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
to the definition of morphemes. Several incon-
sistencies in the test corpus existed, such as: ?
?? (tokyo, Noun)(Tokyo), ? (to, Other)(the
Metropolis), ? (ritsu, Other)(founded), ?
? (daigaku, Noun)(university),? and ???
(toritsu, Noun)(metropolitan), ?? (daigaku,
Noun)(university).? Both of these are the
names representing the same university. The
???? is partitioned into two in the first one
while it is not partitioned into two in the second
one according to the definition of morphemes.
When such inconsistencies in the corpus exist, it
is difficult for our model to discriminate among
these inconsistencies because we used only bi-
gram information as features. To achieve bet-
ter accuracy, therefore, we need to use trigram
or longer information. To correctly recognize
characteristic expressions often used in spoken
language, we plan to extract typical patterns
used in the expressions, to generalize the pat-
terns manually, and to generate possible expres-
sions using the generalized patterns, and finally,
to add such patterns to the dictionary. We also
plan to expand our model to skip fillers, mispro-
nounced words, and disfluencies because those
expressions are randomly inserted into text and
it is impossible to learn the connectivity be-
tween those randomly inserted expressions and
others.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis without a Dictionary for
Japanese. In Proceedings of the NLPRS, pages 541?544.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proceedings of the
LREC, pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distribu-
tional Analysis. In Proceedings of the COLING, pages
1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method
for Japanese Unknown Words using a Statistical Model
of Morphology and Context. In Proceedings of the ACL,
pages 277?284.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
Hiroshima City University
nanba@its.hiroshima-cu.ac.jp
Abstract
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
1 Introduction
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al, 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2 has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al, 2003). Multiple document summarization is
now a central issue for text summarization research.
1http://duc.nist.gov
2http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
3It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
fixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al, 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
2.2 Data Preparation for Sentence Extraction
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
4This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
Figure 1: An example of an abstract and its sources.
2. A set of sentences that are suitable as a source
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al, 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
abstract5.
we have two sets of sentences that correspond to
sentence   in the abstract.
(1)  of document  , or
(2) a combination of  and  of document 	
This means that  alone is able to produce   , and
  can also be produced by combining   and   (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
5We use ?set of sentences? since we often find that more
than one sentence corresponds to a sentence in the abstract.
Table 1: Important Sentence Data.
Sentence ID of Abstract Set of Corresponding Sentences
1 
 

2 








Morphological Analysis of a Large Spontaneous Speech Corpus in Japanese
Kiyotaka Uchimoto? Chikashi Nobata? Atsushi Yamada?
Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
3-5, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes two methods for de-
tecting word segments and their morpho-
logical information in a Japanese sponta-
neous speech corpus, and describes how
to tag a large spontaneous speech corpus
accurately by using the two methods. The
first method is used to detect any type of
word segments. The second method is
used when there are several definitions for
word segments and their POS categories,
and when one type of word segments in-
cludes another type of word segments. In
this paper, we show that by using semi-
automatic analysis we achieve a precision
of better than 99% for detecting and tag-
ging short words and 97% for long words;
the two types of words that comprise the
corpus. We also show that better accuracy
is achieved by using both methods than by
using only the first.
1 Introduction
The ?Spontaneous Speech: Corpus and Process-
ing Technology? project is sponsoring the construc-
tion of a large spontaneous Japanese speech corpus,
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al, 2000). The CSJ is a collection of mono-
logues and dialogues, the majority being mono-
logues such as academic presentations and simu-
lated public speeches. Simulated public speeches
are short speeches presented specifically for the cor-
pus by paid non-professional speakers. The CSJ in-
cludes transcriptions of the speeches as well as audio
recordings of them. One of the goals of the project
is to detect two types of word segments and cor-
responding morphological information in the tran-
scriptions. The two types of word segments were
defined by the members of The National Institute for
Japanese Language and are called short word and
long word. The term short word approximates a dic-
tionary item found in an ordinary Japanese dictio-
nary, and long word represents various compounds.
The length and part-of-speech (POS) of each are dif-
ferent, and every short word is included in a long
word, which is shorter than a Japanese phrasal unit,
a bunsetsu. If all of the short words in the CSJ
were detected, the number of the words would be
approximately seven million. That would be the
largest spontaneous speech corpus in the world. So
far, approximately one tenth of the words have been
manually detected, and morphological information
such as POS category and inflection type have been
assigned to them. Human annotators tagged every
morpheme in the one tenth of the CSJ that has been
tagged, and other annotators checked them. The hu-
man annotators discussed their disagreements and
resolved them. The accuracies of the manual tagging
of short and long words in the one tenth of the CSJ
were greater than 99.8% and 97%, respectively. The
accuracies were evaluated by random sampling. As
it took over two years to tag one tenth of the CSJ ac-
curately, tagging the remainder with morphological
information would take about twenty years. There-
fore, the remaining nine tenths of the CSJ must be
tagged automatically or semi-automatically.
In this paper, we describe methods for detecting
the two types of word segments and corresponding
morphological information. We also describe how
to tag a large spontaneous speech corpus accurately.
Henceforth, we call the two types of word segments
short word and long word respectively, or merely
morphemes. We use the term morphological anal-
ysis for the process of segmenting a given sentence
into a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a POS cate-
gory.
2 Problems and Their Solutions
As we mentioned in Section 1, tagging the whole of
the CSJ manually would be difficult. Therefore, we
are taking a semi-automatic approach. This section
describes major problems in tagging a large sponta-
neous speech corpus with high precision in a semi-
automatic way, and our solutions to those problems.
One of the most important problems in morpho-
logical analysis is that posed by unknown words,
which are words found in neither a dictionary nor
a training corpus. Two statistical approaches have
been applied to this problem. One is to find un-
known words from corpora and put them into a
dictionary (e.g., (Mori and Nagao, 1996)), and the
other is to estimate a model that can identify un-
known words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both ap-
proaches. They proposed a morphological analysis
method based on a maximum entropy (ME) model
(Uchimoto et al, 2001). Their method uses a model
that estimates how likely a string is to be a mor-
pheme as its probability, and thus it has a potential
to overcome the unknown word problem. Therefore,
we use their method for morphological analysis of
the CSJ. However, Uchimoto et al reported that the
accuracy of automatic word segmentation and POS
tagging was 94 points in F-measure (Uchimoto et
al., 2002). That is much lower than the accuracy ob-
tained by manual tagging. Several problems led to
this inaccuracy. In the following, we describe these
problems and our solutions to them.
? Fillers and disfluencies
Fillers and disfluencies are characteristic ex-
pressions often used in spoken language, but
they are randomly inserted into text, so detect-
ing their segmentation is difficult. In the CSJ,
they are tagged manually. Therefore, we first
delete fillers and disfluencies and then put them
back in their original place after analyzing a
text.
? Accuracy for unknown words
The morpheme model that will be described
in Section 3.1 can detect word segments and
their POS categories even for unknown words.
However, the accuracy for unknown words is
lower than that for known words. One of the
solutions is to use dictionaries developed for a
corpus on another domain to reduce the num-
ber of unknown words, but the improvement
achieved is slight (Uchimoto et al, 2002). We
believe that the reason for this is that defini-
tions of a word segment and its POS category
depend on a particular corpus, and the defi-
nitions from corpus to corpus differ word by
word. Therefore, we need to put only words
extracted from the same corpus into a dictio-
nary. We are manually examining words that
are detected by the morpheme model but that
are not found in a dictionary. We are also
manually examining those words that the mor-
pheme model estimated as having low proba-
bility. During the process of manual exami-
nation, if we find words that are not found in
a dictionary, those words are then put into a
dictionary. Section 4.2.1 will describe the ac-
curacy of detecting unknown words and show
how much those words contribute to improving
the morphological analysis accuracy when they
are detected and put into a dictionary.
? Insufficiency of features
The model currently used for morphological
analysis considers the information of a target
morpheme and that of an adjacent morpheme
on the left. To improve the model, we need to
consider the information of two or more mor-
phemes on the left of the target morpheme.
However, too much information often leads to
overtraining the model. Using all the informa-
tion makes training the model difficult when
there is too much of it. Therefore, the best
way to improve the accuracy of the morpholog-
ical information in the CSJ within the limited
time available to us is to examine and revise
the errors of automatic morphological analysis
and to improve the model. We assume that the
smaller the probability estimated by a model
for an output morpheme is, then the greater
the likelihood is that the output morpheme is
wrong. Therefore, we examine output mor-
phemes in ascending order of their probabili-
ties. The expected improvement of the accu-
racy of the morphological information in the
whole of the CSJ will be described in Sec-
tion 4.2.1
Another problem concerning unknown words
is that the cost of manual examination is high
when there are several definitions for word seg-
ments and their POS categories. Since there
are two types of word definitions in the CSJ,
the cost would double. Therefore, to reduce the
cost, we propose another method for detecting
word segments and their POS categories. The
method will be described in Section 3.2, and
the advantages of the method will be described
in Section 4.2.2
The next problem described here is one that we
have to solve to make a language model for auto-
matic speech recognition.
? Pronunciation
Pronunciation of each word is indispensable for
making a language model for automatic speech
recognition. In the CSJ, pronunciation is tran-
scribed separately from the basic form writ-
ten by using kanji and hiragana characters as
shown in Fig. 1. Text targeted for morpho-
Basic form Pronunciation
0017 00051.425-00052.869 L:
(F??) (F??)
????? ?????????
0018 00053.073-00054.503 L:
???? ????
0019 00054.707-00056.341 L:
???????? ?????????
?Well, I?m going to talk about morphological analysis.?
Figure 1: Example of transcription.
logical analysis is the basic form of the CSJ
and it does not have information on actual pro-
nunciation. The result of morphological anal-
ysis, therefore, is a row of morphemes that
do not have information on actual pronuncia-
tion. To estimate actual pronunciation by using
only the basic form and a dictionary is impossi-
ble. Therefore, actual pronunciation is assigned
to results of morphological analysis by align-
ing the basic form and pronunciation in the
CSJ. First, the results of morphological anal-
ysis, namely, the morphemes, are transliterated
into katakana characters by using a dictionary,
and then they are aligned with pronunciation
in the CSJ by using a dynamic programming
method.
In this paper, we will mainly discuss methods for
detecting word segments and their POS categories in
the whole of the CSJ.
3 Models and Algorithms
This section describes two methods for detecting
word segments and their POS categories. The first
method uses morpheme models and is used to detect
any type of word segment. The second method uses
a chunking model and is only used to detect long
word segments.
3.1 Morpheme Model
Given a tokenized test corpus, namely a set of
strings, the problem of Japanese morphological
analysis can be reduced to the problem of assign-
ing one of two tags to each string in a sentence. A
string is tagged with a 1 or a 0 to indicate whether
it is a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. A tag desig-
nated as a 1 is thus assigned one of a number, n, of
grammatical attributes assigned to morphemes, and
the problem becomes to assign an attribute (from 0
to n) to every string in a given sentence.
We define a model that estimates the likelihood
that a given string is a morpheme and has a gram-
matical attribute i(1 ? i ? n) as a morpheme
model. We implemented this model within an ME
modeling framework (Jaynes, 1957; Jaynes, 1979;
Berger et al, 1996). The model is represented by
Eq. (1):
p
?
(a|b) =
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
Z
?
(b)
(1)
Short word Long word
Word Pronunciation POS Others Word Pronunciation POS Others
?? (form) ????(keitai) Noun ????? (morphological
analysis)
????????
?
(keitaisokaiseki) Noun
? (element) ? (so) Suffix
?? (analysis)????(kaiseki) Noun
? ? (ni) PPP case marker ???? (about) ???? (nitsuite) PPP case marker,
compound
word
?? (relate) ?? (tsui) Verb KA-GYO, ADF, eu-
phonic change
? ? (te) PPP conjunctive
? ? (o) Prefix ??????(talk) ??????? (ohanashiitasi) Verb SA-GYO,
ADF
?? (talk) ??? (hanashi) Verb SA-GYO, ADF
???(do) ??? (itashi) Verb SA-GYO, ADF
?? ?? (masu) AUX ending form ?? ?? (masu) AUX ending form
PPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial form
Figure 2: Example of morphological analysis results.
Z
?
(b) =
?
a
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
, (2)
where a is one of the categories for classification,
and it can be one of (n+1) tags from 0 to n (This is
called a ?future.?), b is the contextual or condition-
ing information that enables us to make a decision
among the space of futures (This is called a ?his-
tory.?), and Z
?
(b) is a normalizing constant deter-
mined by the requirement that
?
a
p
?
(a|b) = 1 for
all b. The computation of p
?
(a|b) in any ME model
is dependent on a set of ?features? which are binary
functions of the history and future. For instance, one
of our features is
g
i,j
(a, b) =
{
1 : if has(b, f
j
) = 1 & a = a
i
f
j
= ?POS(?1)(Major) : verb,??
0 : otherwise.
(3)
Here ?has(b, f
j
)? is a binary function that returns
1 if the history b has feature f
j
. The features used
in our experiments are described in detail in Sec-
tion 4.1.1.
Given a sentence, probabilities of n tags from 1
to n are estimated for each length of string in that
sentence by using the morpheme model. From all
possible division of morphemes in the sentence, an
optimal one is found by using the Viterbi algorithm.
Each division is represented as a particular division
of morphemes with grammatical attributes in a sen-
tence, and the optimal division is defined as a di-
vision that maximizes the product of the probabil-
ities estimated for each morpheme in the division.
For example, the sentence ???????????
??????? in basic form as shown in Fig. 1 is
analyzed as shown in Fig. 2. ??????? is ana-
lyzed as three morphemes, ??? (noun)?, ?? (suf-
fix)?, and ??? (noun)?, for short words, and as one
morpheme, ?????? (noun)? for long words.
In conventional models (e.g., (Mori and Nagao,
1996; Nagata, 1999)), probabilities were estimated
for candidate morphemes that were found in a dic-
tionary or a corpus and for the remaining strings
obtained by eliminating the candidate morphemes
from a given sentence. Therefore, unknown words
were apt to be either concatenated as one word or di-
vided into both a combination of known words and
a single word that consisted of more than one char-
acter. However, this model has the potential to cor-
rectly detect any length of unknown words.
3.2 Chunking Model
The model described in this section can be applied
when several types of words are defined in a cor-
pus and one type of words consists of compounds of
other types of words. In the CSJ, every long word
consists of one or more short words.
Our method uses two models, a morpheme model
for short words and a chunking model for long
words. After detecting short word segments and
their POS categories by using the former model,
long word segments and their POS categories are de-
tected by using the latter model. We define four la-
bels, as explained below, and extract long word seg-
ments by estimating the appropriate labels for each
short word according to an ME model. The four la-
bels are listed below:
Ba: Beginning of a long word, and the POS cat-
egory of the long word agrees with the short
word.
Ia: Middle or end of a long word, and the POS cat-
egory of the long word agrees with the short
word.
B: Beginning of a long word, and the POS category
of the long word does not agree with the short
word.
I: Middle or end of a long word, and the POS cat-
egory of the long word does not agree with the
short word.
A label assigned to the leftmost constituent of a long
word is ?Ba? or ?B?. Labels assigned to other con-
stituents of a long word are ?Ia?, or ?I?. For exam-
ple, the short words shown in Fig. 2 are labeled as
shown in Fig. 3. The labeling is done deterministi-
cally from the beginning of a given sentence to its
end. The label that has the highest probability as es-
timated by an ME model is assigned to each short
word. The model is represented by Eq. (1). In Eq.
(1), a can be one of four labels. The features used in
our experiments are described in Section 4.1.2.
Short word Long word
Word POS Label Word POS
?? Noun Ba ????? Noun
? Suffix I
?? Noun Ia
? PPP Ba ???? PPP
?? Verb I
? PPP Ia
? Prefix B ?????? Verb
?? Verb Ia
??? Verb Ia
?? AUX Ba ?? AUX
PPP : post-positional particle , AUX : auxiliary verb
Figure 3: Example of labeling.
When a long word that does not include a short
word that has been assigned the label ?Ba? or ?Ia?,
this indicates that the word?s POS category differs
from all of the short words that constitute the long
word. Such a word must be estimated individually.
In this case, we estimate the POS category by us-
ing transformation rules. The transformation rules
are automatically acquired from the training corpus
by extracting long words with constituents, namely
short words, that are labeled only ?B? or ?I?. A rule
is constructed by using the extracted long word and
the adjacent short words on its left and right. For
example, the rule shown in Fig. 4 was acquired in
our experiments. The middle division of the con-
sequent part represents a long word ???? (auxil-
iary verb), and it consists of two short words ???
(post-positional particle) and ??? (verb). If several
different rules have the same antecedent part, only
the rule with the highest frequency is chosen. If no
rules can be applied to a long word segment, rules
are generalized in the following steps.
1. Delete posterior context
2. Delete anterior and posterior contexts
3. Delete anterior and posterior contexts and lexi-
cal entries.
If no rules can be applied to a long word segment in
any step, the POS category noun is assigned to the
long word.
4 Experiments and Discussion
4.1 Experimental Conditions
In our experiments, we used 744,204 short words
and 618,538 long words for training, and 63,037
short words and 51,796 long words for testing.
Those words were extracted from one tenth of the
CSJ that already had been manually tagged. The
training corpus consisted of 319 speeches and the
test corpus consisted of 19 speeches.
Transcription consisted of basic form and pronun-
ciation, as shown in Fig. 1. Speech sounds were
faithfully transcribed as pronunciation, and also rep-
resented as basic forms by using kanji and hiragana
characters. Lines beginning with numerical digits
are time stamps and represent the time it took to
produce the lines between that time stamp and the
next time stamp. Each line other than time stamps
represents a bunsetsu. In our experiments, we used
only the basic forms. Basic forms were tagged with
several types of labels such as fillers, as shown in
Table 1. Strings tagged with those labels were han-
dled according to rules as shown in the rightmost
columns in Table 1.
Since there are no boundaries between sentences
in the corpus, we selected the places in the CSJ that
Anterior context Target words Posterior context
Entry ?? (it, go) ? (te)? (mi, try) ?? (tai, want)
POS Verb PPP Verb AUX
Label Ba B I Ba
Antecedent part
?
Anterior context Long word Posterior context
?? (it, go) ?? (temi, try) ?? (tai, want)
Verb AUX AUX
Consequent part
Figure 4: Example of transformation rules.
Table 1: Type of labels and their handling.
Type of Labels Example Rules
Fillers (F??) delete all
Disfluencies (D?)????? (D2?)? delete all
No confidence in
transcription
(? ?????) leave a candidate
Entirely (?) delete all
Several can- (? ???,????) leave the former
didates exist candidate
Citation on sound or
words
(M?)? (M?)??? leave a candidate
Foreign, archaic, or
dialect words
(O???????) leave a candidate
Personal name, dis-
criminating words,
and slander
???? (R??)??? leave a candidate
Letters and their
pronunciation in
katakana strings
(A????;EU) leave the former
candidate
Strings that cannot
be written in kanji
characters
(K? (F??)??;?) leave the latter can-
didate
are automatically detected as pauses of 500 ms or
longer and then designated them as sentence bound-
aries. In addition to these, we also used utterance
boundaries as sentence boundaries. These are au-
tomatically detected at places where short pauses
(shorter than 200 ms but longer than 50 ms) follow
the typical sentence-ending forms of predicates such
as verbs, adjectives, and copula.
4.1.1 Features Used by Morpheme Models
In the CSJ, bunsetsu boundaries, which are phrase
boundaries in Japanese, were manually detected.
Fillers and disfluencies were marked with the labels
(F) and (D). In the experiments, we eliminated fillers
and disfluencies but we did use their positional infor-
mation as features. We also used as features, bun-
setsu boundaries and the labels (M), (O), (R), and
(A), which were assigned to particular morphemes
such as personal names and foreign words. Thus, the
input sentences for training and testing were charac-
ter strings without fillers and disfluencies, and both
boundary information and various labels were at-
tached to them. Given a sentence, for every string
within a bunsetsu and every string appearing in a
dictionary, the probabilities of a in Eq. (1) were es-
timated by using the morpheme model. The output
was a sequence of morphemes with grammatical at-
tributes, as shown in Fig. 2. We used the POS cate-
gories in the CSJ as grammatical attributes. We ob-
tained 14 major POS categories for short words and
15 major POS categories for long words. Therefore,
a in Eq. (1) can be one of 15 tags from 0 to 14 for
short words, and it can be one of 16 tags from 0 to
15 for long words.
Table 2: Features.
Number Feature Type Feature value
(Number of value) (Short:Long)
1 String(0) (113,474:117,002)
2 String(-1) (17,064:32,037)
3 Substring(0)(Left1) (2,351:2,375)
4 Substring(0)(Right1) (2,148:2,171)
5 Substring(0)(Left2) (30,684:31,456)
6 Substring(0)(Right2) (25,442:25,541)
7 Substring(-1)(Left1) (2,160:2,088)
8 Substring(-1)(Right1) (1,820:1,675)
9 Substring(-1)(Left2) (11,025:12,875)
10 Substring(-1)(Right2) (10,439:13,364)
11 Dic(0)(Major) Noun, Verb, Adjective, . . . Unde-
fined (15:16)
12 Dic(0)(Minor) Common noun, Topic marker, Ba-
sic form. . . (75:71)
13 Dic(0)(Major&Minor) Noun&Common noun,
Verb&Basic form, . . . (246:227)
14 Dic(-1)(Minor) Common noun, Topic marker, Ba-
sic form. . . (16:16)
15 POS(-1) Noun, Verb, Adjective, . . . (14:15)
16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)
17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)
18 TOC(0)(Beginning) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
19 TOC(0)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
20 TOC(0)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (25:25)
21 TOC(-1)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
22 TOC(-1)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (16:15)
23 Boundary Bunsetsu(Beginning), Bun-
setsu(End), Label(Beginning),
Label(End), (4:4)
24 Comb(1,15) (74,602:59,140)
25 Comb(1,2,15) (141,976:136,334)
26 Comb(1,13,15) (78,821:61,813)
27 Comb(1,2,13,15) (156,187:141,442)
28 Comb(11,15) (209:230)
29 Comb(12,15) (733:682)
30 Comb(13,15) (1,549:1,397)
31 Comb(12,14) (730:675)
The features we used with morpheme models in
our experiments are listed in Table 2. Each feature
consists of a type and a value, which are given in the
rows of the table, and it corresponds to j in the func-
tion g
i,j
(a, b) in Eq. (1). The notations ?(0)? and
?(-1)? used in the feature-type column in Table 2 re-
spectively indicate a target string and the morpheme
to the left of it. The terms used in the table are ba-
sically as same as those that Uchimoto et al used
(Uchimoto et al, 2002). The main difference is the
following one:
Boundary: Bunsetsu boundaries and positional in-
formation of labels such as fillers. ?(Begin-
ning)? and ?(End)? in Table 2 respectively indi-
cate whether the left and right side of the target
strings are boundaries.
We used only those features that were found three or
more times in the training corpus.
4.1.2 Features Used by a Chunking Model
We used the following information as features
on the target word: a word and its POS cate-
gory, and the same information for the four clos-
est words, the two on the left and the two on
the right of the target word. Bigram and tri-
gram words that included a target word plus bigram
and trigram POS categories that included the tar-
get word?s POS category were used as features. In
addition, bunsetsu boundaries as described in Sec-
tion 4.1.1 were used. For example, when a target
word was ??? in Fig. 3, ???, ????, ???, ??
??, ???, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,
???&??, ??&???, ?? &?? &??, ??
&??&??, ?Noun&PPP?, ?PPP&Verb?, ?Suf-
fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-
setsu(Beginning)? were used as features.
4.2 Results and Discussion
4.2.1 Experiments Using Morpheme Models
Results of the morphological analysis obtained by
using morpheme models are shown in Table 3 and
4. In these tables, OOV indicates Out-of-Vocabulary
rates. Shown in Table 3, OOV was calculated as the
proportion of words not found in a dictionary to all
words in the test corpus. In Table 4, OOV was cal-
culated as the proportion of word and POS category
pairs that were not found in a dictionary to all pairs
in the test corpus. Recall is the percentage of mor-
phemes in the test corpus for which the segmentation
and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-
fied by the system that were identified correctly. The
F-measure is defined by the following equation.
F ? measure =
2? Recall ? Precision
Recall + Precision
Table 3: Accuracies of word segmentation.
Word Recall Precision F OOV
Short 97.47% (61,444
63,037
) 97.62% (61,444
62,945
) 97.54 1.66%
99.23% (62,553
63,037
) 99.11% (62,553
63,114
) 99.17 0%
Long 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21 5.81%
99.05% (51,306
51,796
) 98.58% (51,306
52,047
) 98.81 0%
Table 4: Accuracies of word segmentation and POS
tagging.
Word Recall Precision F OOV
Short 95.72% (60,341
63,037
) 95.86% (60,341
62,945
) 95.79 2.64%
97.57% (61,505
63,037
) 97.45% (61,505
63,114
) 97.51 0%
Long 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21 6.93%
97.30% (50,396
51,796
) 96.83% (50,396
52,047
) 97.06 0%
Tables 3 and 4 show that accuracies would im-
prove significantly if no words were unknown. This
indicates that all morphemes of the CSJ could be an-
alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting
unknown words and putting them into dictionaries
are about 1.5 in F-measure for detecting word seg-
ments of short words and 2.5 for long words. For de-
tecting the word segments and their POS categories,
for short words we expect an improvement of about
2 in F-measure and for long words 3.
Next, we discuss accuracies obtained when un-
known words existed. The OOV for long words
was 4% higher than that for short words. In gen-
eral, the higher the OOV is, the more difficult de-
tecting word segments and their POS categories
is. However, the difference between accuracies
for short and long words was about 1% in recall
and 2% in precision, which is not significant when
we consider that the difference between OOVs for
short and long words was 4%. This result indi-
cates that our morpheme models could detect both
known and unknown words accurately, especially
long words. Therefore, we investigated the recall
of unknown words in the test corpus, and found
that 55.7% (928/1,667) of short word segments and
74.1% (2,660/3,590) of long word segments were
detected correctly. In addition, regarding unknown
words, we also found that 47.5% (791/1,667) of
short word segments plus their POS categories and
67.3% (2,415/3,590) of long word segments plus
their POS categories were detected correctly. The
recall of unknown words was about 20% higher for
long words than for short words. We believe that
this result mainly depended on the difference be-
tween short words and long words in terms of the
definitions of compound words. A compound word
is defined as one word when it is based on the def-
inition of long words; however it is defined as two
or more words when it is based on the definition of
short words. Furthermore, based on the definition of
short words, a division of compound words depends
on its context. More information is needed to pre-
cisely detect short words than is required for long
words. Next, we extracted words that were detected
by the morpheme model but were not found in a dic-
tionary, and investigated the percentage of unknown
words that were completely or partially matched to
the extracted words by their context. This percent-
age was 77.6% (1,293/1,667) for short words, and
80.6% (2,892/3,590) for long words. Most of the re-
maining unknown words that could not be detected
by this method are compound words. We expect that
these compounds can be detected during the manual
examination of those words for which the morpheme
model estimated a low probability, as will be shown
later.
The recall of unknown words was lower than that
of known words, and the accuracy of automatic mor-
phological analysis was lower than that of manual
morphological analysis. As previously stated, to
improve the accuracy of the whole corpus we take
a semi-automatic approach. We assume that the
smaller the probability is for an output morpheme
estimated by a model, the more likely the output
morpheme is wrong, and we examine output mor-
phemes in ascending order of their probabilities. We
investigated how much the accuracy of the whole
corpus would increase. Fig. 5 shows the relation-
ship between the percentage of output morphemes
whose probabilities exceed a threshold and their
93
94
95
96
97
98
99
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n 
(%
)
Output Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 5: Partial analysis.
precision. In this figure, ?short without UKW?,
?long without UKW??, ?short with UKW?, and
?long with UKW? represent the precision for short
words detected assuming there were no unknown
words, precision for long words detected assuming
there were no unknown words, precision of short
words including unknown words, and precision of
long words including unknown words, respectively.
When the output rate in the horizontal axis in-
creases, the number of low-probability morphemes
increases. In all graphs, precisions monotonously
decrease as output rates increase. This means that
tagging errors can be revised effectively when mor-
phemes are examined in ascending order of their
probabilities.
Next, we investigated the relationship between the
percentage of morphemes examined manually and
the precision obtained after detected errors were re-
vised. The result is shown in Fig. 6. Precision
represents the precision of word segmentation and
POS tagging. If unknown words were detected and
put into a dictionary by the method described in the
fourth paragraph of this section, the graph line for
short words would be drawn between the graph lines
?short without UKW? and ?short with UKW?, and
the graph line for long words would be drawn be-
tween the graph lines ?long without UKW? and
?long with UKW?. Based on test results, we can
expect better than 99% precision for short words
and better than 97% precision for long words in the
whole corpus when we examine 10% of output mor-
93
94
95
96
97
98
99
100
0 20 40 60 80 100 120
Pr
ec
is
io
n 
(%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 6: Relationship between the percentage of
morphemes examined manually and precision ob-
tained after revising detected errors (when mor-
phemes with probabilities under threshold and their
adjacent morphemes are examined).
0
10
20
30
40
50
60
0 5 10 15 20 25 30 35 40 45 50
Er
ro
r R
at
es
 in
 E
xa
m
in
ed
 M
or
ph
em
es
 (%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"short_with_UKW"
"long_without_UKW"
"long_with_UKW"
Figure 7: Relationship between percentage of mor-
phemes examined manually and error rate of exam-
ined morphemes.
phemes in ascending order of their probabilities.
Finally, we investigated the relationship between
percentage of morphemes examined manually and
the error rate for all of the examined morphemes.
The result is shown in Fig. 7. We found that about
50% of examined morphemes would be found as er-
rors at the beginning of the examination and about
20% of examined morphemes would be found as
errors when examination of 10% of the whole cor-
pus was completed. When unknown words were de-
tected and put into a dictionary, the error rate de-
creased; even so, over 10% of examined morphemes
would be found as errors.
4.2.2 Experiments Using Chunking Models
Results of the morphological analysis of long
words obtained by using a chunking model are
shown in Table 5 and 6. The first and second lines
Table 5: Accuracies of long word segmentation.
Model Recall Precision F
Morph 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21
Chunk 97.65% (50,580
51,796
) 97.41% (50,580
51,911
) 97.54
Chunk 98.84% (51,193
51,796
) 98.66% (51,193
51,888
) 98.75
Table 6: Accuracies of long word segmentation and
POS tagging.
Model Recall Precision F
Morph 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21
Chunk 95.59% (49,513
51,796
) 95.38% (49,513
51,911
) 95.49
Chunk 98.56% (51,051
51,796
) 98.39% (51,051
51,888
) 98.47
Chunk w/o TR 92.61% (47,968
51,796
) 92.40% (47,968
51,911
) 92.51
TR : transformation rules
show the respective accuracies obtained when OOVs
were 5.81% and 6.93%. The third lines show the ac-
curacies obtained when we assumed that the OOV
for short words was 0% and there were no errors in
detecting short word segments and their POS cate-
gories. The fourth line in Table 6 shows the accuracy
obtained when a chunking model without transfor-
mation rules was used.
The accuracy obtained by using the chunking
model was one point higher in F-measure than that
obtained by using the morpheme model, and it was
very close to the accuracy achieved for short words.
This result indicates that errors newly produced by
applying a chunking model to the results obtained
for short words were slight, or errors in the results
obtained for short words were amended by apply-
ing the chunking model. This result also shows that
we can achieve good accuracy for long words by ap-
plying a chunking model even if we do not detect
unknown long words and do not put them into a dic-
tionary. If we could improve the accuracy for short
words, the accuracy for long words would be im-
proved also. The third lines in Tables 5 and 6 show
that the accuracy would improve to over 98 points
in F-measure. The fourth line in Tables 6 shows that
transformation rules significantly contributed to im-
proving the accuracy.
Considering the results obtained in this section
and in Section 4.2.1, we are now detecting short and
long word segments and their POS categories in the
whole corpus by using the following steps:
1. Automatically detect and manually examine
unknown words for short words.
2. Improve the accuracy for short words in the
whole corpus by manually examining short
words in ascending order of their probabilities
estimated by a morpheme model.
3. Apply a chunking model to the short words to
detect long word segments and their POS cate-
gories.
As future work, we are planning to use an active
learning method such as that proposed by Argamon-
Engelson and Dagan (Argamon-Engelson and Da-
gan, 1999) to more effectively improve the accuracy
of the whole corpus.
5 Conclusion
This paper described two methods for detecting
word segments and their POS categories in a
Japanese spontaneous speech corpus, and describes
how to tag a large spontaneous speech corpus accu-
rately by using the two methods. The first method is
used to detect any type of word segments. We found
that about 80% of unknown words could be semi-
automatically detected by using this method. The
second method is used when there are several defi-
nitions for word segments and their POS categories,
and when one type of word segments includes an-
other type of word segments. We found that better
accuracy could be achieved by using both methods
than by using only the first method alone.
Two types of word segments, short words and
long words, are found in a large spontaneous speech
corpus, CSJ. We found that the accuracy of auto-
matic morphological analysis for the short words
was 95.79 in F-measure and for long words, 95.49.
Although the OOV for long words was much higher
than that for short words, almost the same accuracy
was achieved for both types of words by using our
proposed methods. We also found that we can ex-
pect more than 99% of precision for short words,
and 97% for long words found in the whole corpus
when we examined 10% of output morphemes in as-
cending order of their probabilities as estimated by
the proposed models.
In our experiments, only the information con-
tained in the corpus was used; however, more appro-
priate linguistic knowledge than that could be used,
such as morphemic and syntactic rules. We would
like to investigate whether such linguistic knowl-
edge contributes to improved accuracy.
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-Based
Sample Selection For Probabilistic Classifiers. Artificial In-
telligence Research, 11:335?360.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39?71.
E. T. Jaynes. 1957. Information Theory and Statistical Me-
chanics. Physical Review, 106:620?630.
E. T. Jaynes. 1979. Where do we Stand on Maximum Entropy?
In R. D. Levine and M. Tribus, editors, The Maximum En-
tropy Formalism, page 15. M. I. T. Press.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis Without a Dictionary for
Japanese. In Proceedings of NLPRS, pages 541?544.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Sponta-
neous Speech Corpus of Japanese. In Proceedings of LREC,
pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING, pages 1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words Using a Statistical Model of Mor-
phology and Context. In Proceedings of ACL, pages 277?
284.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Using
Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP, pages 91?99.
K. Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.
2002. Morphological Analysis of The Spontaneous Speech
Corpus. In Proceedings of COLING, pages 1298?1302.
Comparison between Tagged Corpora for the Named Entity 
Task 
Chi~sh i  NOBATA N ige l  COLL IER  and Jun ' i ch i  TSUJ I I  
Kansa i  Advanced Research Center  Depar tment  of  In format ion  Science 
Communicat ions  Research Laboratory  Graduate  School of  Science 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku University of  Tokyo, Hongo 7-3-1 
Kobe,  Hyogo, 65\].-2492 JAPAN Bunkyo-ku,  Tokyo,  113-0033 JAPAN 
nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp 
Abst rac t  
We present two measures for compar- 
ing corpora based on infbrmation the- 
ory statistics uch as gain ratio as well 
as simple term-class ~equency counts. 
We tested the predictions made by these 
measures about corpus difficulty in two 
domains - -  news and molecular biol- 
ogy - -  using the result of two well-used 
paradigms for NE, decision trees and 
HMMs and found that gain ratio was the 
more reliable predictor. 
made by these measures against actual system 
performance. 
Recently IE systems based on supervised learn- 
ing paradigms uch as hidden Markov models 
(Bikel et al, 1997), maximum entropy (Borth- 
wick et al, 1998) and decision trees (Sekine et 
al., 1998) have emerged that should be easier to 
adapt to new domains than the dictionary-based 
systems of the past. Much of this work has taken 
advantage of smoothing techniques to overcome 
problems associated with data sparseness (Chen 
and Goodman, 1996). 
The two corpora we use in our NE experiments 
represent the following domains: 
1 In t roduct ion  
With the advent of the information society and 
increasing availability of large mounts  of infor- 
mation in electronic form, new technologies such 
as information extraction are emerging to meet 
user's information access needs. Recent evalu- 
ation conferences such as TREC (Voorhees and 
Harman, 2000) showed the feasibility of this task 
and highlighted the need to combine information 
ret r ied  (m) and extraction (IE) to go beyond 
simply offering the user a long ranked list of in- 
teresting documents to providing facts for user's 
questions. 
The problem of domain dependence r mains a 
serious one and in fact there has been very little 
work so far to compare the difllculty of IE tasks for 
different domaln~ and their corpora. Such knowl- 
edge is useful for developing IE systems that are 
portable between domains. This paper begins to 
address this issue, in particular the lowest level of 
IE task, defined in the TIPSTER sponsored MUC- 
6 conference (MUC, 1995) as named entity (NE). 
This is emerging as a key technology in several 
other IF-related tasks such as question answer- 
ing. We seek here to show theoretically motivated 
measures for comparing the ditficulty of corpora 
for the NE task in two domains, newswire and 
molecular-biology. We then test the predictions 
? Newswire: acquisition of names of people, or- 
ganizations and monetary units etc., from the 
MUC-6 data set. 
? Molecular-biology: acquisition of proteins, 
DNAs, RNAs etc. from a subset of the MED- 
LINE database (MEDLINE, 1999). 
Information extraction in the molecular-biology 
domain (Seldmlzu et al, 1998) (Craven and Kum- 
lien, 1999) (Rindflesch et al, 2000) has recently 
become a topic of interest o the NLP community. 
This is a result of the need to formalise the huge 
number of research results that appear in free-text 
form in online collections of journal abstracts and 
papers such as MEDLINE for databases such as 
Swissprot (Ban:och and Apwefler, 1997) and also 
to search such collections for facts in an intelligent 
way. 
The purpose of our study is not to show a high 
level of absolute system performance. In fact since 
we use only the MUC-6 executive succession data 
set of 60 articles and a new MEDLINE data set 
of 100 articles we cannot hope to achieve perfor- 
mance limits. What we aim to do is to compare 
model performance against he predictions of cor- 
pus difficulty made by two different methods. In 
the rest of this paper we firstly introduce the NE 
models used for evaluation, the two corpora we 
20 
examined and then the difficulty comparison met- 
rics. Predictive scores from the metrics are ex- 
amined against he actual performance of the NE 
models. 
2 Mode ls  
Recent studies into the use of supervised learning- 
based modeels for the NE task in the molecular- 
biology domain have shown that models based on 
hidden Markov models (HMMs) (Collier et al, 
2000) and decision trees (Nobata et al, 1999) are 
not only adaptable to this highly technical do- 
main, but are also much more generalizable to new 
classes of words than systems based on traditional 
hand-built heuristic rules such as (Fukuda et al, 
1998). We now describe two models used in our 
experiments based on the decision trees package 
C4.5 (Quiuian, 1993) and HMMs (Rabiner and 
Juang, 1986). 
2.1 Decision tree named entity 
recogniser:NE-DT 
A decision tree is a type of classifier which 
has "leaf nodes" indicating classes and "decision 
nodes" that specify some test to be carried out, 
with one branch or subtree for each possible out- 
come of the test. A decision tree can be used 
to classify an object by starting at the root of 
the tree and moving through it until a leaf is en- 
countered. When we can define suitable features 
for the decision tree, the system can achieve good 
performance with only a small amount of training 
data. 
The system we used is based on one that was 
originally created for Japanese documents (Seine 
et al, 1998). It has two phases, one for creating 
the decision tree from training data and the other 
for generating the class-tagged text based on the 
decision tree. When generating decision trees, tri- 
grams of words were used. For this system, words 
are considered to be quadruple features. The fol- 
lowing features are used to generate conditions in 
the decision tree: 
Par t -o f -speech in format ion:  There are 45 
part-of-speech categories, whose definitions 
are based on Pennsylvania Treebank's cat- 
egories. We use a tagger based on Adwait 
Ratnaparkhi's method (Ratnaparkhi, 1996). 
Character type in format ion:  Orthographic 
information is considered such as upper case, 
lower case, capitalization, numerical expres- 
sions, symbols. These character features 
are the same as those used by NEHMM 
described in the next section and shown in 
Table 1. 
Word  l ists specif ic to  the  domain :  Word 
lists are made from the training corpus. 
Only the 200 highest fxequency words are 
used. 
2.2 H idden Markov  mode l  named ent i ty  
reco~. i ser :  NEHMM 
HMMs are a widely u~d class of learning algo- 
rithms and can be considered to be stochastic fi- 
nite state machines. In the following model, sum- 
marized here from the full description given in 
(Collier et al, 2000), we consider words to be or- 
dered pairs consisting of a surface word, W, and 
a word feature, F ,  given as < W, F >. The word 
features themselves are discussed below. As is 
common practice, we need to calculate the prob- 
abilities for a word sequence for the first word's 
name class and every other word differently since 
we have no initial name-class to make a transition 
from. Accordingly we use the following equation 
to calculate the initial name class probability, 
Pr(NC~\[ < Wf~,t , Flli,,~ >)= 
aof(NC$,,s,\[ < Wf,,,,,Ffi,,t >)+ 
o~f(gcs~,,,I < -,Ff~,,, >) + 
a~f(NCfi,.,,) (i) 
and for all other words and their name classes 
as follows: 
Fr(NCT~ I < Wt,Ft >,< W~-,,Ft-, >,NC~-i) = 
Aof(NGtl < W~,F~ >,< Wt-,,Ft-1 >,NG~-,) + 
Alf(NCtI < .,F~ >,< W~-I,F~-i >,NC~- i )+ 
A2f(NC~I < W,,F~ >, < .. F,-, >,NCt-x) + 
AsI(NG, I < .,Ft >,< _, F~-, >,NG,- ,)+ 
A4f(NC, INC,-,) + 
Asf(NC,) (2) 
where f(I) is calculated with maximum- 
likelihood estimates from counts on training data. 
In our current system we set the constants Ai 
and al by hand and let ~ ai = 1.0, ~ Ai = 1.0, 
ao _> al  > ~,  ~o >_ A , . . .  >_ As. The cur- 
rent name-class NCt is conditioned on the cur- 
rent word and feature, the previous name-class, 
NCt-1, and previous word and feature. 
Equations 1 and 2 implement a linear- 
interpolating HMM that incorporates a number of 
sub-models designed to reduce the effects of data 
sparseness. 
Table 1: Word features v~ith examples 
Word Feature Example 
TwoDig i tN~ 25 
FourDigitNumber 2000 
DigitNumber 15012 
SingleCap M 
GreekLetter alpha 
CapsAndDigits 12 
TwoCaps RalGDS 
LettersAnd.Digits p52 
In i tCap Interleukin 
LowCaps kappaB 
Lowercase kinases 
Hyphon 
Backslash / 
Feature Ex. 
CloseSquare \] 
Colon 
SemiColon ; 
Percent % 
OpenParen ( 
CloseParen ) 
Comma 
FullStop . 
Determiner the 
Conjunction and 
Other *+~ 
Once the state transition probabilities have 
been calculated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of possible name class as- 
signments in linear time to find the highest prob- 
ability path, i.e. to maximise Pr(W, NC). The fi- 
nal stage of our algorithm that is used after narae- 
class tagging is complete is to use a clean-up mod- 
ule called Unity. This creates a frequency list 
of words and name-classes and then re-tags the 
text using the most frequently used name class 
assigned by the HMM. We have generally found 
that this improves F-score performance by be- 
tween 2 and 4%, both for re-tagging spuriously 
tagged words and for finding untagged words in 
unknown contexts that had been correctly tagged 
elsewhere in the text. 
Table 1 shows the char~ter  features that we 
used in both NEHMM and NE-DT. Our intuition 
is that such features will help the model to find 
similarities between known words that were found 
in the training set and unknown words and so 
overcome the unknown word problem. 
3 Corpora  
We used two corpora in our experiments repre- 
senting two popular domains in IE, molecular- 
biology (from MEDLINE) and newswire texts 
(from MUC-6). These are now described. 
3.1 MUC-6  
The corpus for MUC-6 (MUC, 1995) contains 60 
articles, from the test corpus for the dry and for- 
malruns. An example canbe seenin Figure 1. We 
can see several interesting features of the domain 
such as the focus of NF.,s on people and organiza- 
tion profiles. Moreover we see that there are many 
pre-name clue words such as "Ms." or "Rep." indi- 
cating that a Republican politician's name should 
follow. 
3.2 Biology 
In our tests in the domain of molecular-biology 
we are using abstracts available from PubMed's 
MEDLIhrE. The MEDLINE database is an online 
collection of abstracts for published journal arti- 
cles in biology and medicine and contains more 
than nine million articles. Currently we have ex- 
tracted a subset of MEDLINE based on a search 
using the keywords human AND blood cell AND 
transcription .factor yielding about 3650 abstracts. 
Of these 100 docmnents were NE tagged for our 
experiments using a human domain expert. An 
example of the annotated abstracts is shown in 
Figure 2. In contrast o MUC-6 each article is 
quite short and there are few pre-class clue words 
making the task much more like terminology iden- 
tification and classification than pure name find- 
ing. 
4 A f i r s t  a t tempt  a t  corpus  
compar i son  based  on  s imple  
token  f requency  
A simple and intuitive approach to NE task dif- 
ficulty comparison used in some previous tudies 
such as (palmer and Day, 1997) who studied cor- 
pora in six different languages, compares class to 
term-token ratios on the assumption that rarer 
classes are more difficult to acquire. The relative 
frequency counts from these ratios also give an in- 
direct measure of the granularity of a class, i.e. 
how wide it is. While this is appealing, we show 
that this approach does not necessarily give the 
best metric for comparison. 
Tables 2 and 3 show the ratio of the number of 
different words used in NEs to the total number 
of words in the NE  class vocabulary. The num- 
ber of different tokens is influenced by the corpus 
size and is not a suitable index that can uniformly 
show the difficulty for different NE tasks, there- 
fore it should be normalized. Here we use words 
as tokens. A value close to zero indicates little 
variation within the class and should imply that 
the class is easier to acquire. We see that the NEs 
in the biology domain seem overall to be easier 
to acquire than those in the MUC-6 domain given 
hxical variation. 
The figures in the second columns of Tables 2 
and 3 are normalized so that all numerals are re- 
placed by a single token. It still seems though 
that MUC-6 is a considerably more eheJlenging 
domain than biology. This is despite the fact that 
the ratios for ENAMEX expressions such as Date, 
22 
A graduate of <ENAMEX TYPE=" ORGANIZATION" >Harvard Law SChooI</ENAMEX>, Ms. 
<ENAMEX TYPE="PERSON'>Washington</ENAMEX> worked as a laywer for the corporate fi- 
nance division of the <ENAMEX TYPE='ORGANIZATION~>SEC</ENAMEX> in the late <TIMEX 
TYPE='DATE">1970s</TIMEX>. She has been a congressional staffer since <TIMEX TYPE= 
"DATE'>1979</TIMEX>. Separately, <ENAMEX TYPE='PERSON'>Clintou</ENAMEX> transi- 
tion officials said that <ENAMEX TYPE='PERSON">Frank Newman</ENAMEX>, 50, vice chairman 
and chief financial officer of <ENAMEX TYPE=" ORGANIZATION" >BankAmerica Corp.</ENAMEX>, 
is expected to be nominated as assistant <ENAMEX TYPE="ORGANIZATION~>Treasury</ENAMEX> 
secretary for domestic finance. 
Figure 1: Example sentences taken from the annotated MUC-6 NE text 
<PROTEIN>SOX-4</PROTEIN>, an <PROTEIN>Sty-like HMG box protein</PROTEIN>, is 
a transcriptional activator in <SOLrRCE.cell-type>lymphocytes</SOUl:tCE>. Previous studies in 
<SOURCE.cell-type>lymphocytes</SOUB.CE> have described two DNA-binding <PROTEIN>HMG 
bax proteins</PROTEIN>, <PROTEIN>TCF-I</PROTEIN> and <PROTEIN>LEF-I</PROTEIN>, 
with affinity for the <DNA>A/TA/TCAAAG motif</DNA> found in several <SOURCE.cell-type>T 
cell</SOUl~CE>-specific enhancers. Evaluation of cotransfection experiments in <SOURCE.cell-type>non- 
T cells</SOURCE> and the observed inactivity of an <DNA>AACAAAG concatamer</DNA> in the 
<PROTEIN>TCF-1 </PROTEIN> / <PROTEIN>LEF-1 </PROTEIN>-expressing <SOURCE.cell-line>T 
cell line BW5147</SOURCE>, led us to conclude that these two proteins did not mediate the observed 
enhancer effect. 
Figure 2: Example sentences taken from the annotated biology text 
Table 2: Frequency values for words in the MUC-6 
test corpus 
Class 
Org. 
Person 
Loc. 
Date 
Time 
Money 
Percent 
Al l  
Original 
0.28(=507 / 1783) 
0.45(=381 / 838) 
0.38(=148 / 390) 
0.23(=123 / 542) 
1.00(= 3 / 3) 
0.33(=138 / 423) 
0.39(= 42 / 108) 
0.33(=1342/4087) 
Table 3: Frequency values for words in the biology 
corpus 
Norm. numerals Class Original 
0.28(=507 / 1783) DNA 0.21(=245 / 1140) 
0.45(=381 / 838) Protein 0.15(=631 / 4125) 
0.38(=148 / 390) RNA 0.43(= 30 / 70) 
0.11(= 60 / 542) Source 0.16(=248 / 1533) 
1.00(= 3 / 3) All 0.17(=1'154/6868) 
0.05(= 20 / 423) 
0.03(= 3 / 108) 
0.27(=1122/4087) 
Money and Percent all fall significantly. Expres- 
sions in the Time class are so rare however that it 
is di~cult o make any sort of meaningftfl compar- 
ison. In the biology corpus, the ratios are not sig- 
nificantly changed and the NE classes defined for 
biology documents eem to have the same chuj-- 
acteristics as non-numeric ENAMEX classes in 
MUCC-6 documents. 
Comparing between the biology documents and 
the MUC-6 documents, we may say that identify- 
ing entities in biology docmnents is easier than 
identifying ENAMEX entities in MUC-6 docu- 
ments. 
5 Exper iments  
We evaluated the performance ofour two systems 
using a cross validation method. For the MUC- 
6 corpus, 6-fold cross validation was performed 
on the 60 texts and 5-fold cross validation was 
performed for the 100 texts in the biology corpus. 
Norm. numerals 
0.20(=228 / 1140) 
0.13(=540 / 4125) 
0.43(= 30 / 70) 
0.16(=242 / 1833). 
0.15(=I040/6868) 
We use "F-scores ~for evaluation of our experi- 
ments (Van Rijsbergen, 1979). "F-score" is a mea- 
surement combining "Recall" and "Predsion" and 
defined in Equation 3. "Recall" is the percent- 
age of answers proposed by the system that corre- 
spond to those in the human-made key set. "Pre- 
cision" is the percentage of correct answers among 
the answers proposed by the system. The F-scores 
presented here are automatically calculated using 
a scoring program (Chinchor, 1995). 
2 x Precision x Recall 
F-score = Precision + Recall (3) 
In Table 4 we show the actual performance 
of our term recognition systems, NE-DT and 
NEHMM. We can see that corpus comparisons 
based only on class-token ratios are inadequate o 
explain why both systems' performance was about 
the same in both domains or why NEHMM did 
better in both test corpora than NE-DT. The dif- 
ference in performance is despite there being more 
training examples in biology (3301 NEs) than in 
MUC-6 (2182 NEs). Part of the reason for this is 
97 
Table 4: Performance of the NE systems 
NEHMM with Unity 7&4 75.0 
NEHMM w/o Unity 74.2: 73.1 
NE-DT 68:~-" 69.4 
that the class-token ratios ignore individual sys- 
tem knowledge, i.e. the types of features that 
can be captured and useful in the corpus domain. 
Among other considerations they also fail to con- 
sider the overlap of words and features between 
classes in the same corpus domain. 
6 Corpus  compar i son  based  on  
in fo rmat ion  theoret i ca l  measures  
In this section we attempt o present measures 
that overcome some of the limitations of the class- 
token method. We evaluate tbe contribution from 
each feature used in our NE recognition systems 
by calculating its entropy. There are thee  types of 
feature information used by our two systems: lexo 
ical information, character type information, and 
part-of-speech information. 
The entropy for NE classes H(C) is defined by 
= - E p(c) log 2 p(c) H(C) 
cEC 
where: 
n(O 
p(c) = "N 
n(c): the number of words in class c 
N: the total number of words in text 
We can calculate the entropy for features in the 
same way. 
When a feature F is given, the conditional en- 
tropy for NE classes H(CIF) is defined by 
- ~ ~ p(~, f) logs p(cll) H(C\]F) 
cEC fEF  
where: 
p(c, I) = .(c, I) 
N 
n(c, I) p(cll) = n(l) 
n(c, f):  the number of words in class c 
with the feature value f 
n(/): the number of words 
with the feature value f 
Using these entropies, we can calculate infor- 
mation gain (Breiman et al, 1984) and gain ra- 
tio (Quinlan, 1990). Information gain for NE 
classes and a feature I(C; F) is given as follows: 
I(C; F) = H(C) - H(CIF ) 
The information gain I(C; F) shows how the fea- 
ture F is related with NE classes C. When F is 
completely independent ofC, the value of I(C; F) 
becomes the minimum value O. The maximum 
value of I(C;_F) is equivalent to that of H(C), 
when the feature F gives sufficient information to 
recognize named entities. Information gain can 
also be calculated by: 
I(C; F) = H(C) + H(F) - H(C, F) 
We show the values of the above three entropies 
in Table 5,6, and 7. In these tables, F is replaced 
with single letters which represent each of the 
model's features, i.e. character types (T), part- 
of-speech (P), and hxical information (W). 
Gain ratio is the normalized value of in.forma- 
tion gain. The gain ratio GR(C; F) is defined by 
GR(C; F) = I(C; F) 
H(C) 
The range of the gain ratio GR(C; F) is 0 < 
GR(C; F) _~ 1 even when the class entropy is 
different in various corpora, so we can compare 
the values directly in the different NE recognition 
tasks. 
6.1 Character types 
Character type features are used to identify 
named entities in the MUCC-6 and biology corpus. 
However, the distribution of the character types 
are quite different between these two types of doc- 
uments as we can see in Table 5. We see through 
the gain-ratio score that character type informa- 
tion has a greater predictive power for classes in 
MUC~ than biology due to the higher entropy 
of character type and class sequences in the bi- 
ology corpus, i.e. the greater disorder of this in- 
formation. The result partially shows why iden- 
tification and classification is harder in biological 
documents than in newspaper articles such as the 
MUC-6  corpus. 
6.2  Part-of -speech 
Table 6 shows the entropy scores for part-of- 
speech (POS) sequences in the two corpora. We 
see through the gain ratio scores that POS infor- 
mation is not so powerful for acquiring NEs in the 
biology domain compared to the MUC-6 domain. 
24 
Table 5: Values of Entropy for character type 
Entropy MUC-6 Biology 
H(T) \[\[ 1.880 2.013 
H(C) II 0.890 1.264 
H(C,T) II 2.345 2.974 
I(C;T) \[I .0.425 0.302 
GR(C;T) H 0.478 0.239 
Table 6: Values of Entropy for POSs 
Entropy MUC-6 Biology 
"H(P) 4.287 4.037 
H(C) 0.890 1.264 
H(C,P) 4.750 5.029 
I(C;P) 0.426 0.272 
GR(C;P) 0.479 0.216 
In fact POS information for biology is far less use- 
ful than character information when we compare 
the results in Tables 5 and 6, whereas POS has 
about the same predictive power as character in- 
formation in the MUC-6 domain. One likely ex- 
planation for this is that the POS tagger we use in 
NE-DT is trained on a corpus based on newspaper 
articles, therefore the assigned POS tags are often 
incorrect in biology documents. 
6.3 Lexical information 
Table 7 shows the entropy statistics for the two 
domains. Although entropy for words in biology 
is lower than MUC-6, the entropy for classes is 
higher leading to a lower gain ratio in biology. We 
also note that, as we would expect, in comparison 
to the other two types of knowledge, surface word 
forms are by far the most useful type of knowledge 
with a gain ratio in MUC-6 of 0.897 compared to 
0.479 for POS and 0.478 for character types in the 
same domain. However, such knowledge is also 
the least generalizable and runs the risk of data- 
sparseness. It therefore has to be complemented 
by more generalizable knowledge such as character 
features and POS. 
Table 7: Values of Entropy for words 
--Entropy MUC-6 Biology 
H(W) 9.570 8.89O 
H(C) 0.890 1.264 
H(C,W) 9.662 9.232 
I(C;W) 0.798 0.921 
~R(C;W) 0.897 0.729 
Table 8: Values of Entropy for NEHMM features 
in the MUC-6 corpus 
GR 
0.994 
0.898 
0.967 
0.798 
0.340 
0.806 
0.461 
0.558 
0.221 
0.806 
0.563 
0.971 
0.633 
Cross Entropy 
5.38(4.08-9.68) 
7.69(6.97-9.32) 
7.73(7.07-9.30) 
4.38(4.12-.-4.82) 
1.62(1.32-1.90) 
7.65(7.11-8.65) 
2.64(2.41-2.97) 
7.91(7.25--8.99) 
2.94(2.70-3.25) 
7.65(7.11-6.65) 
7.92(7.26-9.03) 
5.42(4.10-9.70) 
4.18(3.91-4.60) 
Coverage 
o.44(o.34-o.78) 
O. 77(0.72-0.90) 
0.79(0.73-0.90) 
0.99(0.98-1.00) 
L00(1.00-L00) 
0.65(0.81-0.93) 
1.00(0.99-1.00) 
0.83(0.79-0.92) 
1.00(1.00-1.00) 
0.85(0.81,-0.93) 
0.83(0.79-0.92) 
0.44 (0.34-O.75) 
0.99(0.99--1.00) 
Features. 
for A0 
for Al 
for A2 
for As 
Ct-1 
Wt 
Ft 
Wt- I  
F~-x 
Wt Fz 
W~-l F=-i 
Wt-l,~ 
F~-Lt 
Table 
in the biology corpus 
GR Cross Entropy 
0.977 5.83(5.66-6.14) 
0.793 7.93(7.77-8.08) 
0.929 7.79(7.65-7.85) 
0.643 5.07(4.95-5.21) 
0.315 2.26(2.24--2.28) 
0.694 7.64(7.52-7.78) 
0.257 3.12(3.06--3.19) 
0.423 7.99(7.62-8.08) 
0.093 3.33(3.27-3,43) 
0.694 7.64(7.52-7.78) 
0.424 7.98(7.82-8.04) 
0.904 5.96(5.78-6.24) 
0.339 4.66(4.53-4,78) 
9: Values of Entropy for NEHMM features 
Coverage 
0.49(0.48--0.52) 
o.6o(o.79-o.61) 
o.so(o.70-o.81) 
0.98(0.98-0.98) 
1.00(1.00-I.00) 
0.89(0.87-0.89) 
1.oo(1.OO-l.OO) 
0.87(0.86-0.88) 
1.00(1.00-1.00) 
0.89(0.87-0,89) 
o.87(0.85-0.86) 
0.50(0.49-0.52) 
0.99(0.98-0.99) 
Features 
for ~to 
for A1 
for ~t2 
for As 
Ct- I 
W= 
Fe 
Wt  Ft 
Wt-1 F,-z 
Wz-l,t 
F~-l,t 
6.4 Compar i son  between the 
comblnutlon of features 
In this section we show a comparison of gain ra- 
tio for the features used by both systems in each 
corpus. Values of gain ratio for each feature set 
are shown on the 'GR' column in Tables 8, 9, 10 
and 111. The values of GR show that surface 
words have the best contribution in both corpora 
for both systems. We can see that gain ratio for 
all features in NE-DT is actually lower than the 
top level model for NEHMM in biology, reflecting 
the actual system performance that we observed. 
We also see that in the biology corpus, the com- 
bination of all features in NE-DT has a lower con- 
tribution than in the MUC-6  corpus. This indi- 
cates the limitation of the current feature set for 
the biology corpus and shows that we need to uti- 
lize other types of features in this domain. 
Values for cross entropy between training and 
test sets are shown in Tables 8, 9, 10 and 11 to- 
IOn the 'Features' col, mn~ "(Features) for A#" 
means the features used in each HMM sub- 
model which corresponds with the A# in Eclua- 
tion 2. And also, 'ALL' in Tables 10 and 11 
means all the features used in decision tree, i.e. 
{P~-l,~,,+l,F~-l,t,t+l,W,-1,~,~+l). 
Table 10: Values of Entropy for NE-DT features 
in the MUC-6 corpus 
0.G91~8 ! Cross Entropy 
1.59(1.38-1.77) 
0.402 5.22(5.09..-5.32) 
0.4681 2.66(2.51-2.87) 
0.844 7.36(7.19-7.57) 
0.670 7.89(7.81-7.97) 
0.6691 3.87(3.67-4.07) 
0.977 4.42(4.10-4.88) 
0.822 9.25(9.10-9.40) 
0.807 4.92(4.72-5.08) 
0.998 1.89(1.67-2.16) 
Coverage 
0.12(0.10-0.13) 
1.00(0.99-:t.00) 
L00(0.99-1.00) 
o.81(o.8o~.83) 
0.98(0.96--0.98) 
0.99(0.98-1.00) 
0.36(0.34--0.40) 
0.89(0.87~0.91) 
0.96(0.95--0.96) 
0.15(0.13-9.17) 
Features 
ALL 
Pt 
Ft 
Wt 
Pt-l,$ 
Ft- l . t  
Wt--l,t 
Pt-l ,t,t+l 
F?-1.:.~+1 
W~-l.t.t+l 
Table 11: Values of Entropy for NE-DT features 
in the biology corpus 
GR Cross Entropy 
0.937 2.31(2.00-2.50) 
0.23"/ 5.31(5.21-5.38) 
0.262 3.27(3.14-3.41) 
0.416 7.63(7.50-7.79) 
0.370 7.78(7.69.-7.86) 
0.363 4.57(4.38-4.67) 
0.586 5.71(5.37-5.93) 
0.541 8.92(8.82-9.02) 
0.502 5.46(5.26-5.64) 
0.764 2.56(2.25-2.76) 
Coverage Features 
0.18(0.15-0.19) ALL 
1.00(0.99-1.00) P, 
1.00(1.00-1.00) Ft 
0.87(0.85--0.68) wt 
0.97(0.96-0.97) P~-a.= 
0.98(0.98-.0.99) F~-I,~ 
0.48(0.45--0.50) Wt-  s,~ 
0.88(0.87--0.89) Pt-x.~t.t +a 
0.96(0.94--0,96) Ft-l.t.~+a 
0.20(0.17--0.21) Wt_L?,t+t 
gether with error bounds in parentheses. These 
values are calculated for pairs of an NE class and 
features, and averaged for the n-fold experiments. 
In the MUC-6 corpus, 60 texts are separated into 
6 subsets, and one of them is used as the test set 
and the others are put together to form a train- 
ing set. Similarly, 100 texts are separated into 5 
subsets in the biology corpus. We also show the 
coverage of the pairs on the 'Coverage' col,,mn. 
Coverage means that how many pairs which ap- 
peared in a test set alo appear in a trainlug set. 
In these columns, the greater the cross entropy 
between features and a class, the more different 
their occurrences between tr~iuing and test sets. 
On the other hand, as the coverage for class- 
features pairs increases, so does the part of the 
test set that is covered with the given feature set. 
The results in both corpora for both systems 
show a drawback of surface words, since their cov- 
erage for a test set is lower than that of features 
like POSs and character types in both corpora 
Also, the coverage of surface words in the biol- 
ogy corpus is higher than in the MUC6 corpus 
as opposed to other features. The result matches 
our intuition that vocabulary inthe biology corpus 
is relatively restricted but has a variety of types 
other than normal English words. 
7 Conc lus ion  
The need for soundly-motivated metrics to com- 
pare the usefulness of corpora for specific tasks 
and systems is dearly necessary for the develop- 
ment of robust and portable information extrac- 
tion systems. 
In this paper we have shown that measures for 
comparing corpora based just on class-token ratios 
have difficulty predicting system performance and 
cannot adequately explain the difficulty of the NE 
task either generally or for specific systems. 
While we should be cautious in ma~ng sweep- 
ing conclusions due to the small size of corpora in 
our study, our results from gain ratio and cross 
entropy indicate that counts from the features of 
both systems will be more useful in the MUC6 cor- 
pus than in the biology corpus. We can also see 
that while the coverage is limited, surface words 
play a leading role for both systems. Gain ra- 
tio statistics for surface words in the two domains 
were far closer than for any other type of feature, 
and given that this is also the dominant knowl- 
edge type this seems to be one likely reason that 
the performance of systems is about the same in 
both domains. 
We have presented the results of applying two 
supervised learning based models to the named 
entity task in two widely different domains and 
explained the performance through class-token ra- 
tios, entropy and gain ratio. Measures such as 
entropy and gain ratio have been found to have 
the best predictive power, although the features 
used to calculate gain ratio are not sufficient o 
describe all the information that is necessary for 
the named entity task. In future work we intend 
to extend our study to new and larger NE corpora 
in various domains and to try to reduce the error 
factor in our calculations that is a result of corpus 
size. 
Re ferences  
A. Bairoch and 1t. Apweiler. 1997. The SWISS- 
PROT protein sequence data bank and its new 
supplement TrEMBL. Nucleic Acids Research, 
25:31-36. 
D. Bikel, S. Miller, R. Schwartz, and 
11. Wesichedel. 1997. Nymble: a high- 
performance learning name-finder. In Pro- 
ceedings of the Fifth Con/ererenee on Applied 
Natural Language Processing, pages 194--201. 
A. Borthwiek, J. Sterling, E. Agichtein, and 
11. Grishman. 1998. Exploiting diverse knowl- 
edge sources via maximum entropy in named 
entity recognition. In Proceedings of the Work- 
shop on Very Large Corpora (WYLC'98). 
L. Breiman, It. Friedman, A. Olshen, and 
C. Stone. 1984. Classification and regressiwa 
26 
trees. Belmont CA: Wadsworth International 
Group. 
S. Chen and J. Goodman. 1996. An empiri- 
cal study of smoothing techniques for language 
modeling. 3gst Annual Meeting of the Associ- 
ation of Computational Linguistics, California, 
USA, 24-27 3tree. 
N. Chinchor. 1995. MUC-5 evaluation metrics. 
In In Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
Maryland, USA., pages 69-78. 
N. Collier, C. Nobata, and J. Tsujii. 2000. Ex- 
tracting the names of genes and gene products 
with a hidden Markov model. In Proceedings 
of the 18th International Conference on Com- 
putational Linguistics (COLING'2000), Saar- 
bruchen, Germany, July 31st-August 4th. 
M. Craven and J. Kumlien. 1999. Constructing 
biological knowledge bases by extracting infor- 
mation from text sources. In Proceedings ofthe 
7th International Conference on Intelligent Sys- 
temps for Molecular Biology (ISMB-99), Hei- 
delburg, Germany, August 6--10. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Tak- 
ag i. 1998. Toward information extraction: 
identifying protein names from biological pa- 
pers. In Proceedings of the Pacific Symposium 
on Biocomputin9'98 (PSB'98), January. 
MEDLINE. 1999. The PubMed 
database can be found at:. 
http://www.ncbi.nlm.nih.gov/PubMed/. 
DARPA. 1995. Proceedings ofthe Sixth Message 
Understanding Conference(MUC-6), Columbia, 
MD, USA, November. Morgan Kaufmann. 
C. Nobata, N. Collier, and I. Tsujii. 1999. Au- 
tomatic term identification and classification 
in biology texts. In Proceedings of the Nat- 
ural Language Pacific Rim Symposium (NL- 
PRS'gO00), November. 
D. Palmer and D. Day. 1997. A statistical 
profile of the named entity task. In Proceed- 
ings of the Fifth Conference on Applied Natural 
Language Processing (ANLP'97), Washington 
D.C., USA., 31 March - 3 April. 
J.R. Quinlan. 1990. Introduction to Decision 
Trees. In J.W. Shavlik and T.G. Dietterich, ed- 
itors, Readings in Machine Learning. Morgan 
Kauf:marm Publishers, Inc., San Mateo, Cali- 
fornia. 
J.R. Quinlan. 1993. cJ.5 Programs for Machine 
Learning. Morgan Kaufmann Publishers, Inc., 
San Mateo, California. 
L. Rabiner and B. Juang. 1986. An introduction 
to bidden Markov models. 1EEE ASSP Maga- 
zine, pages 4-16, January. 
A. Ratnaparkhi. 1996. A maximum entropy 
model for part-of-speech tagging. In Uon\]er- 
ence on Empirical Methods in Natural Language 
Processing, pages 133-142, University of Penn- 
sylvania, May. 
T. Rindflesch, L. Tanabe, N. Weinstein, and L.. 
Hunter. 2000. EDGAR: Extraction of drugs, 
genes and relations from the biomedical litera- 
ture. In Pacific Symposium on Bio-inforraaties 
(PSB '2000), Hawai 'i, USA, January. 
T. Sekimizu, H. Park, and J. Tsujii. 1998. Iden- 
tifying the interaction between genes and gene 
products based on frequently seen verbs in reed- 
line abstracts. In Genome Informatics. Univer- 
sal Academy Press, Inc. 
Satosbi Sekine, Ralph Grishman, and Hiroyuki 
Sbinnou. 1998. A Decision Tree Method for 
Finding and Classifying Names in Japanese 
Texts. In Proceedings o\] the Sixth Workshop 
on Very Large Corpora, Montreal, Canada, Au- 
gust. 
C. Van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
A. J. Viterbi. 1967. Error bounds for convolutions 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, IT-13(2):260-269. 
E.M. Voorhees and D.K. Harman, editors. 
2000. The Eighth Text REtrieval Confer- 
ence (TREC-8), Electronic version available at 
http://trec.nist.gov/pubs.html. 
A Survey for Multi-Document Summarization 
Satoshi Sekine 
New York University 
715 Broadway, 7th floor 
New York, NY, 10003, USA 
sekine@cs.nyu.edu 
 
 
Chikashi Nobata 
Communications Reserach Laboratory 
2-2-2 Hikaridai, Seika-chou, Soraku-gun 
Kyoto, 619-0289, Japan 
nova@crl.go.jp 
 
Abstract 
Automatic Multi-Document summarization is still hard 
to realize. Under such circumstances, we believe, it is 
important to observe how humans are doing the same 
task, and look around for different strategies. 
We prepared 100 document sets similar to the ones 
used in the DUC multi-document summarization task. 
For each document set, several people prepared the 
following data and we conducted a survey. 
A) Free style summarization 
B) Sentence Extraction type summarization 
C) Axis (type of main topic) 
D) Table style summary 
In particular, we will describe the last two in detail, 
as these could lead to a new direction for multi-
summarization research. 
1 Introduction 
Automatic Multi-Document summarization is still hard 
to realize. Like single document summarization for 
newspaper articles, where we don?t have a notably bet-
ter automatic summarization algorithm than a simple 
lead based method, automatic multi-document summa-
rization faces very difficult challenges. Under such 
circumstances, we believe, it is important to observe 
how humans are doing on the same task, and look for 
possible different strategies.  
Assume you are given several documents talking 
about the same topic, and are asked to summarize them, 
what might you do. The authors tried this by them-
selves. First we used a marker to mark the important 
phrases or sentences. Then we tried to connect them, in 
some cases by figuring out the main or common topics 
in the marked sentences, or in some cases, by making a 
list or a table to figure out the overview of the docu-
ments. When we looked at the result at this stage we 
noticed that these are very good summaries, even if 
they are not summaries in the conventional sense (a set 
of sentences to be read). The main topics are good to 
understand the overall issues in the document set and 
the table is a good digest of the issues throughout the 
document set. If we can automatically create such data 
from document sets, we might be able to make a good 
summary. The questions arising here are what kinds of 
?main topic? we can make in general, and what per-
centage of document sets are suitable for table-style 
summarization. 
The main topics we created in our hand summary 
experiment were like lists of keywords, but we found 
that there are more general types like ?these documents 
are talking about a single person?. As keyword extrac-
tion has been one of the techniques in summarization, 
we will focus on the types of the main topics in the 
following experiments. 
We will describe the definition of our types and re-
port on the experiment of manually creating table-style 
summarization, as well as analyses of free style sum-
maries and sentence extraction type summaries. We 
prepared 100 document sets similar to the ones used in 
the DUC multi-document summarization task (DUC 
homepage). For each document set, annotators prepared 
the following data. 
A) Free style summarization 
B) Sentence Extraction type summarization 
C) Axis (type of main topic) 
D) Table style summary 
In particular, we will describe the last two in detail, 
as these could lead to a new direction for multi-
document summarization research. 
2 Document Sets 
First, we describe how we accumulated our 100 multi-
document data. We found that the topics of DUC multi-
document data are a bit biased as it is pre-filtered for 
evaluation purposes, i.e. DUC document sets are care-
fully chosen as described in the guidelines. The pre-
filtering is useful for evaluation purposes, but it does 
not necessarily reflect the distribution of user needs or 
distribution of topics in the news. We would like to 
obtain relatively more balanced document sets. We 
adopted the procedure described in the following, 
where the entire experiment was done using a Japanese 
newspaper corpus (Mainichi 1998 and 1999). 
  Select an article randomly from the corpus 
(seed) 
  Choose keywords from each article. Keywords 
are all nouns of frequency more than 1, except 
for some special types of nouns 
  Use dice coefficient to retrieve articles similar 
to the seed article. Gather all documents that 
have coefficient more than 0.5. 
  Select article sets that have more than 3 articles. 
About 300 such sets are obtained and among 
them, we selected 100 document sets, preferring 
more documents in a set and avoiding overlap-
ping topics. 
The average number of articles in a document set 
was 4.7 and the average number of sentences in a 
document was 12.9. Annotators read the articles in each 
set and detected if there were articles that are different 
from the topic throughout the document set. Such arti-
cles, which turned out to be very few in number, were 
excluded in the following experiments. 
3 Task and annotator 
We have four tasks and three annotators (indicated by a 
number). Annotator 1 and 2 did the same task, but an-
notator 3 did only a part of it. All of them have college 
degrees, in particular annotators 1 and 2 are Japanese 
native speakers and have majors in linguistics at US 
universities. 
Some examples (free summaries for one document 
set, and axes and table data for three sets, all translated 
into English) are shown in the appendix. 
 
Annotator  
Task 1 2 3 
Free style summary 100 40  
Sent. Extraction 100 20 100 
Axis 100 100 100 
Table summary 100 100  
 
Table 1. Task and annotator 
4 Free style summarization 
The first task is a free style summarization. The inter-
annotator agreement based on the word vector metric 
adopted by TSC evaluation (TSC homepage) is calcu-
lated. This is a cosine metric of tf*idf measure of the 
words in the summaries. Most of the pairs (37 sets out 
of 40 sets) had values of 0.5 or more, which is much 
larger than that of automatic systems measured against 
the human made summaries in TSC-1 (ranging around 
0.4 in 10% summary, 0.5 in 40% summary). We can 
reasonably believe the summaries are very reliable. 
5 Sentence Extraction 
Now we will look at the summarization by sentence 
extraction. Annotators 1 and 3 conducted the task for 
the entire data, so we will compare the results of those 
two. We asked the annotators to extract about 20% of 
the sentences as a summary of each document set, but 
the actual numbers of extracted sentences are slightly 
different between the two. Table 2 shows the number of 
sentences selected by the two annotators with inter-
annotator agreement data. The number of sentences 
selected by both annotators (533) looks low, compared 
to the number of sentences selected by only one 
annotator (650 and 746). However, the chi-square test 
is 513.9, which means that these two results are 
strongly correlated (less than 0.0001%  chance). 
 
Annotator 1  
annotator 3 selected not selected 
 
Total 
selected 533 746 1279 
not selected 650 4050 4700 
Total 1183 4796 5979 
 
Table 2. Number of selected sentences 
6 Axis 
Axis is based on the idea of (McKeown et al 2001). 
They defined 4 categories of document sets based on 
the main topic of the document set for the purpose of 
using different summarization strategies (they actually 
used two sub-systems), shown in Table 3.  
 
Category and Description 
Single-Event (2) 
The documents center around one single event at one 
place and at roughly the same time, involving the 
same agents and actions 
Person-centered (10) 
The documents deal with one event concerning one 
person 
Multi-Event (7) 
Several events occurring at different places and times 
and usually with different protagonists, are reported 
together 
Other (11) 
Document sets contain even more loosely related 
documents 
 
Table 3. McKeown?s categories 
The number in brackets for each category indicates 
the number of document sets in the DUC 2001 training 
data. As can be seen, the number of ?person centered? 
sets is quite high. We believe this is due to the pre-
filtering in the DUC data. ?Other? is also high, which 
means more categories may be needed.  
We created new categories based on our study of 
document sets (other than the 100 sets reported here). 
We defined 13 categories, shown in Table 4, for what 
we will call the axis of the document set.  
 
Single-person Multi-Person 
Single-location Multi-location 
Single-organization Multi-organization 
Single-facility Multi-facility 
Single-product Multi-product 
Single-event Multi-event 
Others  
 
Table 4. 13 Axes 
 
The axis is a combination of two types of informa-
tion; single or multi, and 6 kinds of named entities 
(person, location, organization, facility, product and 
event). ?Single? means that all the articles are talking 
about a single event, person or other entity, whereas 
?Multi? articles are talking about multiple entities that 
might participate in similar types of events. We used 6 
categories of entity types, which are the major catego-
ries defined in the MUC (Grishman and Sundheim 
1996) or ACE project (ACE homepage). For example, 
if a document set is talking about Einstein?s biography, 
it should be tagged as ?single-person?, and if a set is 
talking about earthquakes in California last year, it 
should be tagged as ?multi-event?. 
In order to demonstrate the validity of the catego-
ries, we tried to categorize the training data of DUC 
2001?s multi-document sets into our categories. Two 
people assigned one or two categories to each set. We 
allow more than one axis to a document set, as some 
document sets should be inherently categorized into 
more than one axis. If we consider only the first 
choices, the inter annotator agreement ratio is 80% and 
if we include the second choices, the ratio is 93.3%. 
We believe the categorization is practical. Table 5 
shows the distribution of axis categories tagged on our 
100 data sets by three annotators. Note that annotators 
1 and 2 assigned more than one axis to some data sets, 
so the totals exceed 100. 
All the categories except multi-facility are used by 
at least two annotators. Because the axis ?other? is used 
rarely, the set of axes are empirically found to have 
quite good coverage. 
The inter-annotator agreements are 55, 61 and 67 % 
among the three annotators. Although the ratios are 
lower than that on the DUC data (we believe this is 
because of the pre-filtering of document sets), the 
agreement is still high at 55-67% even though there are 
13 kinds of axis. Note that chi-square test is not suit-
able to measure the data because of the data sparseness. 
 
Annotator  
Axis 1 2 3 
s-event 36 32 37 
s-facility 3 1 0 
s-location 7 2 4 
s-organization 6 12 11 
s-person 12 13 12 
s-product 14 14 8 
m-event 15 30 8 
m-facility 0 0 0 
m-location 2 3 2 
m-organization 9 12 13 
m-person 2 7 1 
m-product 2 2 0 
Other 1 0 3 
 
Table 5. Distribution of axis 
 
There are 39 document sets that have the same axis 
assigned by the three annotators, and there are only 7 
document sets that have three different axes by three 
annotators (no overlap at all). Even when different 
categories are tagged, sometimes all of them are under-
standable and we can say that these are all correct. So 
for some document sets, more than one category is in-
stinctively correct. This result indicates that, for some 
large percentage of document sets, it is possible to as-
sign axis(es). We believe for summarizing those docu-
ment sets, knowing the axis before summarization 
could be quite helpful. We are seeking a method to 
automate the process of finding the axis(es). 
7 Table 
A table is a good way to summarize a document set 
talking about multiple events of the same type, a collec-
tion of similar events or chronological events. We 
asked annotators to make a table for each document set. 
Table 6 shows some statistics of the created tables. The 
average number of columns is 3.47 and 5.25 for anno-
tator 1 and annotator 2, respectively. Regarding com-
parison between tables, the percentages of complete 
overlap (relationship of columns is 1 to 1 and the same 
information is collected in the column) are 58% and 
38%. The percentages of overlap (relationship of col-
umns is not 1 to 1, but the information of the columns 
is overlapping between the tables) are 94% and 70%. 
We can see that annotator 1 made fewer columns than 
annotator 2, and most columns made by annotator 1 
overlap columns made by annotator 2. So the differ-
ence is probably due to the fact that annotator 2 made 
more detailed tables. (As this is the first such survey, it 
was not easy to create good instructions) In other words, 
it might be the case that most important information 
(which was turned into columns) is simultaneously 
found by the two annotators. 
 
 
 Annotator 1 Annotator 2 
Ave. num. of column 3.47 5.25 
Complete overlap 58% 38% 
Overlap 94% 70% 
 
Table 6. Statistics of created tables 
 
When we compared the tables created by the two 
annotators one by one, we categorized the results into 5 
categories. 
A) Two tables are completely the same 
B) The information in the tables is the same, but 
the way of segmenting information into col-
umns is different. For example, one of the ta-
bles has a column ?visiting activity (of a 
diplomat)? including information about visiting 
place, person and purpose, whereas the other 
table has columns ?visiting place?, ?the person 
to meet? and ?purpose of the visit?. 
C) Missing one or two columns from either or 
both of tables (in total). This means one of the 
tables has one or two fewer columns and the in-
formation in the columns is not mentioned in the 
other table. As we can guess from Table 6, most 
of the missing columns were found in tables of 
annotator 1. 
D) Missing more than two columns from tables. 
E) The two tables are completely different in 
structure, because of the table creator?s different 
point of view. 
Table 7 shows the result of this survey.  
 
Description Num. of sets 
A) Same table 8 
B) Only segmentation 15 
C) Missing one or two column 34 
D) Missing more than two column 17 
E) Completely different table 26 
Total 100 
 
Table 7. Comparison of tables 
 
There are only a small number of document sets (8) 
from which the annotators made completely the same 
table. However, for more than half the document sets, 
the tables created by the two annotators are quite simi-
lar (including ?same table?, ?only segmentation? and 
?missing one or two columns?). This is complementary 
to the result shown in Table 6; for many document sets, 
the tables by annotator 2 have additional information 
compared to the tables by annotator 1. 
We also asked the annotators to judge if each docu-
ment set is suitable to summarize into a table. We made 
three categories for the survey. 
A) Table is natural for summarizing the document 
set 
B) Information can be summarized in table format 
C) Table is not suitable to summarize the docu-
ment set 
The result for the two annotators is shown in Table 
8. Annotators 1 and 2 judged 40 and 45 sets to be suit-
able for a table, 36 and 38 are OK and 24 and 17 are 
not suitable. This is an interesting result - that for so 
many document sets (40-45%) a table is judged to be 
natural for summarizing. Compared to that, only a 
smaller fraction (17-24%) are judged unsuitable. The 
relationships between the two annotators? judgments 
are also shown in Table 8. The Chi-test is 17.94 and the 
probability is 0.13%; that means that the two judges are 
highly correlated. 
 
Annotator 1  
Annotator 2 A B C 
 
total 
A 28 12 5 45 
B 9 16 13 38 
C 3 8 6 17 
total 40 36 24 100 
 
Table 8. Suitability of table 
 
8 Discussion 
We reported a survey for multi-document summariza-
tion. We believe the results are encouraging for the 
pursuit of some novel strategies of multi-document 
summarization.  
One of them is the notion of axis. As we observed 
that for some percentage of the document sets, the axis 
can be tagged with some certainty, we might be able to 
make an automatic system to find it. Once the axis is 
correctly found, it might be useful for multi document 
summarization. For example, if a set is ?single-person? 
then the summary for the set should be centered on the 
person. This may suggest, for example, generating a 
summary of type ?biography? (Mani 2001). If a docu-
ment set is found to be ?multi-event?, then the sum-
mary should focus on the differences of the events.  
The other result found in the experiment is that a 
quite large percentage of document sets can be summa-
rized in table format. As this is a preliminary experi-
ment, there is incompleteness in the instruction and we 
believe further study on this topic is necessary. In addi-
tion to setting guidelines for the degree of detail, the 
style of cell contents shall be more uniform. Currently, 
cells contain words, phrases and sentences. We believe 
that by making more careful instructions for annotation, 
the comparison between different tables can be more 
systematized. In other words, a systematic evaluation 
may be possible. 
9 Future Work 
Obviously, the future work suggested by these results 
includes automatic methods to find what the human 
found in this experiment. 
We have started finding the axis automatically by 
observing the distribution of named entities, words and 
phrases. 
Once we are able to find the axis and suitability of 
table summary automatically for a given document set, 
the next stage of research will involve using this infor-
mation to select an appropriate way to summarize the 
set, i.e. table summary, sentence extraction summary or 
summary including rewriting. This will be extended if 
the document set is created dynamically from a user?s 
query. The type of query could be a helpful clue in se-
lecting the way to summarize the retrieved document 
set. 
The technology to summarize a document set in ta-
ble format is studied in Information Extraction. How-
ever, it has a hard limitation that the topic of the 
document set has to be known in advance and the 
knowledge to build table has to be created by hand, 
which usually takes a long time. There have been ef-
forts to automate the knowledge creation (Riloff 1996) 
(Yangarber 2000) (Sudo 2001); we hope to make a 
bridge between such automatic IE knowledge discovery 
and automatic summarization efforts. 
 
10 Acknowledgements 
This research is supported by the Defense Advanced 
Research Projects Agency as part of the Translingual 
Information Detection, Extraction and Summarization 
(TIDES) program, under Grant N66001-001-1-8917 
from the Space and Naval Warfare Systems Center, 
San Diego, and by the National Science Foundation 
under Grant IIS-0081962. This paper does not neces-
sarily reflect the position of the U.S. Government. We 
would like to thank our colleagues at New York Uni-
versity, who provided useful suggestions and 
discussions, including, Prof. Ralph Grishman, Mr. 
Kiyoshi Sudo and Mr. Yusuke Shinyama. Also, we 
thank the three annotators to do the tedious job. 
References 
(ACE-homepage)  
http://www.nist.gov/speech/tests/ace/index.htm 
(DUC-homepage)                
http://www-nlpir.nist.gov/projects/duc/ 
(TSC-homepage)  http://lr-www.pi.titech.ac.jp/tsc/ 
 (McKeown et al 2001) K.R. McKeown, R. Barzilay, 
D. Evans, V. Hatzivassiloglou, M. Yen Kan, B. 
Schiffman, S. Teufel, ?Columbia Multi-Document 
Summarization: Approach and Evaluation?, Pro-
ceedings of the Document Understanding Confer-
ence (DUC-2001), 2001  
(Grishman and Sundheim 1996)  R. Grishman, B. 
Sundheim, ?Message Understanding Conference - 6: 
A Brief History?, Proceedings of the 16th Interna-
tional Conference on Computational Linguistics 
(COLING ?96), 1996  
(Mani 2001) I. Mani, ?Automatic Summarization?, 
John Benjamins Publishing Company, 2001 
(Riloff 1996) Ellen Riloff, ?Automatically Generating 
Extraction Patterns from Untagged Text?, Proceed-
ings of the 13th National Conference on Artificial 
Intelligence, 1996 
(Sudo et al 2001) Kiyoshi Sudo, Satoshi Sekine and 
Ralph Grishman, ?Automatic Pattern Acquisition for 
Japanese Information Extraction?, Procedings of 
Human Language Technologies (HLT ?01), 2001 
(Yangarber 2000) Roman Yangarber, Ralph Grishman, 
Pasi Tapanainen and Silja Huttunen: ?Automatic 
Acquisition of Domain Knowledge for Information 
Extraction?, Proceedings of the 18th International 
Conference on Computational Linguistics (COLING 
2000), 2000 
 
 
 
 
 
Appendix 
 
Original data is all in Japanese. The example data here is English translation. 
 
Sample Data (Article Set #mai9899.189565) 
<Axis> Annotator 1: s-event, s-person,     Annotator 2: s-person, s-event,    Annotator 3: s-person 
 
<Summary by annotator 1> 
Mr. Ikuo Kashima, 69, a yacht sailor in Hannnann-shi, Osaka-fu, departed a yacht harbor in Misaki-machi, Osaka, 
on September 15, with his yacht ?Koraasa 70? to achieve a solo sailing around the world without calling at any port 
as a world?s oldest challenger.  The original plan was to sail across the Pacific, the Atlantic, and the Indian Ocean in 
about 330 days, but he called at Honolulu, Hawaii, on October 26 due to the breakdown of fresh-water-generator.  
He repeatedly departed from and returned to Honolulu because of the engine and generator trouble on November 16, 
the leak and the breakdown of the helm in December, and he gave up on his plan after the fourth trial on January 2.  
In July 99, he left Misaki-machi to make another attempt to sail around the world, and the news of passing through 
the Cape Horn, the south most point in South America came on December 30.  He is expected to arrive at the harbor 
in Misaki-machi next July if the voyage goes smoothly. 
 
<Summary by annotator 2> 
To achieve a solo sailing around the world without calling at any port by the world?s oldest challenger, Mr. Ikuo 
Kashima left Tannowa Yacht Harbor in Misaki-machi, Osaka-fu, on September 15, 1998.  Around noon on October 
26, however, he had to call at Honolulu Harbor on Oahu Island, Hawaii, to repair the fresh-water-generator in his 
yacht ?Koraasa 70,? which he was using for this voyage.  Although he changed his plan and made Hawaii his start-
ing point for his around-the-world solo sailing and started sailing again on October 31, he had to return to Honolulu 
again on November 16, due to the breakdown of the engine and the generator.  Another trial started on December 4, 
again, did not succeed.  Mr. Kashima made his fourth trial on January 2, but again, he had to return to Honolulu on 
January 11 for the leak in the yacht and the trouble in the helm this time.  Later, Mr. Kashima, aiming at a solo sail-
ing around the world again, left Tannowa Yacht Harbor on July 20.  On December 30, he succeeded in passing 
through the most difficult navigation point, Cape Horn.  He is planed to be back to Misaki-machi, Osaka, in July 
2000. 
 
<Table summary by annotator 1>  Suitability for table: Suitable 
DATE PLACE EVENT PURPOSE 
9/15/98 Misaki-machi, Osaka Departure A non-stop solo sailing around the world 
10/26 Honolulu, Hawaii Calling To repair fresh-water-generator 
10/31 Honolulu, Hawaii Departure Second trial to sail around the world 
11/16 Honolulu, Hawaii Calling Troubles in the engine and the generator 
12/4 Honolulu, Hawaii Departure Third trial to sail around the world 
? Honolulu, Hawaii Calling Leak and the breakdown of helm 
1/2/99 Honolulu, Hawaii Departure Fourth trial to sail around the world 
7/20 Misaki-machi, Osaka Departure Another trial for the sailing around the world 
12/30 Cape Horn, the south most point in 
South America 
Passing  
 
<Table summary by annotator 2>  Suitability for table: Suitable 
DATE PLACE ACTION CAUSE 
9/15/1998 Tannowa Yacht Harbor, Misaki-
machi, Osaka-fu 
Departure To achieve a non-stop solo sailing around the 
world as the world?s oldest challenger. 
10/26/1998 Honolulu, Oahu Island, Hawaii Calling To repair fresh-water-generator in ?Koraasa70? 
10/31/1998 Honolulu, Oahu Island, Hawaii Departure To challenge a non-stop solo sailing around the 
world starting from Hawaii. 
11/16/1998 Honolulu, Oahu Island, Hawaii Calling To repair ?Koraasa 70?s engine and generator.  
12/4/1998 Honolulu, Oahu Island, Hawaii Departure To aim at a non-stop solo sailing around the 
world starting from Hawaii. 
1/2/1999 Honolulu, Oahu Island, Hawaii Departure To aim at a non-stop solo sailing around the 
world starting from Hawaii. 
1/11/1999 Honolulu, Oahu Island, Hawaii Calling For the leak and the troubles in the helm of 
?Koraasa 70.? 
7/20/1999 Tannowa Yacht Harbor, 
Misaki-machi, Osaka-fu 
Departure To achieve a non-stop solo sailing as a world?s 
oldest challenger 
12/30/1999 Cape Horn Passing  
July 2000 Tannowa Yacht Harbor, Misaki-
machi, Osaka-fu 
Expected to 
arrive 
Arrival from the non-stop solo sailing around 
the world as an oldest challenger. 
 
Sample Data (Article Set #mai9899.106495) 
<Axis> Annotator 1: s-person,     Annotator 2: s-person,    Annotator 3: s-person 
 
<Table summary by annotator 1>   Suitability for table: Not suitable 
DATE COHEN, DEFENSE SECRETARY 
3/27/98 Meeting with Mordechai, Israeli Minister of Defense. Promised to cooperate in Israel?s missile defense 
development. 
11/6/98 Visited Middle Eastern countries. Asking Iraq to withdraw its suspension on cooperating UNISCOM 
1/11/99 Visited Japan. Discussed the matters on US-Japan security guideline-related bills. 
2/22/99 Planned to visit Japan and China in April. (The plan was postponed due to the prolonged air raid in Yugo-
slavia.) 
7/9/99 Planned to visit Japan and South Korea in late-July. 
 
<Table summary by annotator 2>  Suitability for table: Suitable 
DATE PLACE MEETING WITH PURPOSE TIME OF 
VISIT 
3/27/98 Israel Mordechai, Israeli 
Minister of Defense 
To cooperate in Israel?s missile defense devel-
opment 
3/98 
11/6/98 Middle East  To discuss the issues on Iraq 11/98 
1/7/99 Japan Komura, Japanese For-
eign Minister 
Noroda, Japanese 
Chief of Defense 
To discuss the situation in Korean Peninsula and 
an outlook on passing US-Japan security guide-
line-related bill in Japan?s Diet 
1/11-
1/14/98 
1/7/99 South Korea   1/99 
2/2/99 Japan  To explain about the US-China defense coopera-
tion and to discuss the Guideline-related bill and 
its approval in the Diet. 
4/99 
2/2/99 China   4/99 
7/9/99 Japan Obuchi, Japanese 
Prime Minister 
To discuss the issues on North Korea?s ballistic 
missile and on relocation of Futemma Airport in 
Okinawa. 
Late July 
7/9/99 South Korea   Late July 
 
Sample Data (Article Set #mai9899.141141) 
<Axis> Annotator 1: s-location,    Annotator 2: m-event,    Annotator 3: s-product 
 
<Table summary by annotator 1>  Suitability for table: Suitable 
DATE EVENT KOSOVO LIBERATION 
ARMY 
YUGOSLAVIA/ SERBIAN 
GOVERNMENT 
CONTACT 
GROUP 
2/6/99 Peace Talk began Claimed for Kosovo?s in-
dependence 
Disagreed to Kosovo?s inde-
pendence 
Presented a peace 
plan including 
Kosovo?s auton-
omy 
2/10 Conflict in Peace talk Demanded Serbs to sign 
on agreement for immedi-
ate cease-fire 
Requested Kosovo to sign the 
peace plan 
Aimed at reaching 
an agreement 
2/19 A day before the ne-
gotiation deadline 
 Disagreed to NATO?s pres-
ence in Kosovo 
 
2/20 Agreement deadline. 
Talk continued. 
 Rejected NATO peace force?s 
presence in Kosovo 
Foreign Ministers 
arrived 
2/23 Final day for the 
Peace Talk 
Rejected to sign the peace 
plan.  Requested the vote 
for Kosovo independence. 
Rejected to dissolve Kos-
ovo Liberation Army. 
Rejected to sign the peace 
plan, and refused to have 
NATO?s peace force. 
Acknowledged the 
temporary agree-
ment on Kosovo?s 
autonomy. 
3/15 Second Peace Talk 
began 
   
3/16 Second day of Peace 
Talk 
Expressed the willingness 
to sign the peace plan 
Presented the disagreed items 
in peace plan in writing 
Rejected to change 
the peace plan 
3/18  Signed the peace plan  Requested Yugo-
slavia to accept the 
plan by 24th. 
 
<Table summary by annotator 2>  Suitability for table: can-be 
DATE NO. PERSON ACTION 
2/6/99 1 Representative of ?Kosovo Liberation Army? Arrived in Paris 
2/10/99 1 Yugoslavian government representative and 
groups of Albanian residents 
Each group handed in its own requirements and 
caused conflicts 
2/19/99 1 Cook, Foreign Minister, U.K. Arrived in Paris 
2/20/99 1 Albright, Secretary of State, U.S.A 
Fischer, Foreign Minister, Germany 
Arrived in Paris 
3/1/99 2 Albanian residents Expressed their willingness to sign the peace 
plan 
3/16/99 2 Contact Group of US, Europe, and Russia Pursued Serbian Republic representatives to 
accept the peace plan 
3/16/99 2 Representatives of Serbian Republic Presented objection to the peace plan in writing 
3/16/99 2 Hill, Special US envoy Rejected the big change in the peace plan 
3/18/99 2 Representatives of Serbian Republic Announced to sign the peace plan 
3/18/99 2 Contact Group of US, Europe, and Russia Requested Yugoslavia to accept the final peace 
plan by March 24. 
 
Evaluation of Features for Sentence Extraction
on Different Types of Corpora
Chikashi Nobata?, Satoshi Sekine? and Hitoshi Isahara?
? Communications Research Laboratory
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{nova, isahara}@crl.go.jp
? Computer Science Department, New York University
715 Broadway, 7th floor, New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We report evaluation results for our sum-
marization system and analyze the result-
ing summarization data for three differ-
ent types of corpora. To develop a ro-
bust summarization system, we have cre-
ated a system based on sentence extraction
and applied it to summarize Japanese and
English newspaper articles, obtained some
of the top results at two evaluation work-
shops. We have also created sentence ex-
traction data from Japanese lectures and
evaluated our system with these data. In
addition to the evaluation results, we an-
alyze the relationships between key sen-
tences and the features used in sentence
extraction. We find that discrete combi-
nations of features match distributions of
key sentences better than sequential com-
binations.
1 Introduction
Our ultimate goal is to create a robust summariza-
tion system that can handle different types of docu-
ments in a uniform way. To achieve this goal, we
have developed a summarization system based on
sentence extraction. We have participated in eval-
uation workshops on automatic summarization for
both Japanese and English written corpora. We have
also evaluated the performance of the sentence ex-
traction system for Japanese lectures. At both work-
shops we obtained some of the top results, and for
the speech corpus we obtained results comparable
with those for the written corpora. This means that
the features we use are worth analyzing.
Sentence extraction is one of the main methods
required for a summarization system to reduce the
size of a document. Edmundson (1969) proposed a
method of integrating several features, such as the
positions of sentences and the frequencies of words
in an article, in order to extract sentences. He man-
ually assigned parameter values to integrate features
for estimating the significance scores of sentences.
On the other hand, machine learning methods can
also be applied to integrate features. For sentence
extraction from training data, Kupiec et al (1995)
and Aone et al (1998) used Bayes? rule, Lin (1999)
and Nomoto and Matsumoto (1997) generated a de-
cision tree, and Hirao et al (2002) generated an
SVM.
In this paper, we not only show evaluation results
for our sentence extraction system using combina-
tions of features but also analyze the features for dif-
ferent types of corpora. The analysis gives us some
indication about how to use these features and how
to combine them.
2 Summarization data
The summarization data we used for this research
were prepared from Japanese newspaper articles,
Japanese lectures, and English newspaper articles.
By using these three types of data, we could com-
pare two languages and also two different types of
corpora, a written corpus and a speech corpus.
2.1 Summarization data from Japanese
newspaper articles
Text Summarization Challenge (TSC) is an evalua-
tion workshop for automatic summarization, which
is run by the National Institute of Informatics in
Japan (TSC, 2001). Three tasks were presented at
TSC-2001: extracting important sentences, creating
summaries to be compared with summaries prepared
by humans, and creating summaries for informa-
tion retrieval. We focus on the first task here, i.e.,
the sentence extraction task. At TSC-2001, a dry
run and a formal run were performed. The dry run
data consisted of 30 newspaper articles and manu-
ally created summaries of each. The formal run data
consisted of another 30 pairs of articles and sum-
maries. The average number of sentences per article
was 28.5 (1709 sentences / 60 articles). The news-
paper articles included 15 editorials and 15 news re-
ports in both data sets. The summaries were created
from extracted sentences with three compression ra-
tios (10%, 30%, and 50%). In our analysis, we used
the extraction data for the 10% compression ratio.
In the following sections, we call these summa-
rization data the ?TSC data?. We use the TSC data
as an example of a Japanese written corpus to eval-
uate the performance of sentence extraction.
2.2 Summarization data from Japanese
lectures
The speech corpus we used for this experiment
is part of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000), which is being cre-
ated by NIJLA, TITech, and CRL as an ongoing
joint project. The CSJ is a large collection of mono-
logues, such as lectures, and it includes transcrip-
tions of each speech as well as the voice data. We
selected 60 transcriptions from the CSJ for both sen-
tence segmentation and sentence extraction. Since
these transcription data do not have sentence bound-
aries, sentence segmentation is necessary before
sentence extraction. Three annotators manually gen-
erated sentence segmentation and summarization re-
sults. The target compression ratio was set to 10%.
The results of sentence segmentation were unified
to form the key data, and the average number of
sentences was 68.7 (4123 sentences / 60 speeches).
The results of sentence extraction, however, were
not unified, but were used separately for evaluation.
In the following sections, we call these summa-
rization data the ?CSJ data?. We use the CSJ data as
an example of a Japanese speech corpus to evaluate
the performance of sentence extraction.
2.3 Summarization data from English
newspaper articles
Document Understanding Conference (DUC) is an
evaluation workshop in the U.S. for automatic sum-
marization, which is sponsored by TIDES of the
DARPA program and run by NIST (DUC, 2001).
At DUC-2001, there were two types of tasks:
single-document summarization (SDS) and multi-
document summarization (MDS). The organizers of
DUC-2001 provided 30 sets of documents for a dry
run and another 30 sets for a formal run. These data
were shared by both the SDS and MDS tasks, and
the average number of sentences was 42.5 (25779
sentences / 607 articles). Each document set had a
topic, such as ?Hurricane Andrew? or ?Police Mis-
conduct?, and contained around 10 documents rele-
vant to the topic. We focus on the SDS task here, for
which the size of each summary output was set to
100 words. Model summaries for the articles were
also created by hand and provided. Since these sum-
maries were abstracts, we created sentence extrac-
tion data from the abstracts by word-based compar-
ison.
In the following sections, we call these summa-
rization data the ?DUC data?. We use the DUC data
as an example of an English written corpus to eval-
uate the performance of sentence extraction.
3 Overview of our sentence extraction
system
In this section, we give an overview of our sentence
extraction system, which uses multiple components.
For each sentence, each component outputs a score.
The system then combines these independent scores
by interpolation. Some components have more than
one scoring function, using various features. The
weights and function types used are decided by op-
timizing the performance of the system on training
data.
Our system includes parts that are either common
to the TSC, CSJ, and DUC data or specific to one of
these data sets. We stipulate which parts are specific.
3.1 Features for sentence extraction
3.1.1 Sentence position
We implemented three functions for sentence po-
sition. The first function returns 1 if the position of
the sentence is within a given threshold N from the
beginning, and returns 0 otherwise:
P1. Scorepst(Si)(1 ? i ? n) = 1(if i < N)
= 0(otherwise)
The threshold N is determined by the number of
words in the summary.
The second function is the reciprocal of the po-
sition of the sentence, i.e., the score is highest for
the first sentence, gradually decreases, and goes to a
minimum at the final sentence:
P2. Scorepst(Si) =
1
i
These first two functions are based on the hypoth-
esis that the sentences at the beginning of an article
are more important than those in the remaining part.
The third function is the maximum of the recipro-
cal of the position from either the beginning or the
end of the document:
P3. Scorepst(Si) = max(
1
i ,
1
n? i+ 1)
This method is based on the hypothesis that the sen-
tences at both the beginning and the end of an article
are more important than those in the middle.
3.1.2 Sentence length
The second type of scoring function uses sen-
tence length to determine the significance of sen-
tences. We implemented three scoring functions for
sentence length. The first function only returns the
length of each sentence (Li):
L1. Scorelen(Si) = Li
The second function sets the score to a negative
value as a penalty when the sentence is shorter than
a certain length (C):
L2. Scorelen(Si) = 0 (if Li ? C)
Li ? C (otherwise)
The third function combines the above two ap-
proaches, i.e., it returns the length of a sentence that
has at least a certain length, and otherwise returns a
negative value as a penalty:
L3. Scorelen(Si) = Li (if Li ? C)= Li ? C (otherwise)
The length of a sentence means the number of let-
ters, and based on the results of an experiment with
the training data, we set C to 20 for the TSC and
CSJ data. For the DUC data, the length of a sen-
tence means the number of words, and we set C to
10 during the training stage.
3.1.3 Tf*idf
The third type of scoring function is based on term
frequency (tf) and document frequency (df). We ap-
plied three scoring functions for tf*idf, in which the
term frequencies are calculated differently. The first
function uses the raw term frequencies, while the
other two are two different ways of normalizing the
frequencies, as follows, where DN is the number of
documents given:
T1. tf*idf(w) = tf(w) log DNdf(w)
T2. tf*idf(w) = tf(w)-1
tf(w) log
DN
df(w)
T3. tf*idf(w) = tf(w)
tf(w)+1 log
DN
df(w)
For the TSC and CSJ data, we only used the third
method (T3), which was reported to be effective
for the task of information retrieval (Robertson and
Walker, 1994). The target words for these functions
are nouns (excluding temporal or adverbial nouns).
For each of the nouns in a sentence, the system cal-
culates a Tf*idf score. The total score is the sig-
nificance of the sentence. The word segmentation
was generated by Juman3.61 (Kurohashi and Nagao,
1999). We used articles from the Mainichi newspa-
per in 1994 and 1995 to count document frequen-
cies.
For the DUC data, the raw term frequency (T1)
was selected during the training stage from among
the three tf*idf definitions. A list of stop words were
used to exclude functional words, and articles from
the Wall Street Journal in 1994 and 1995 were used
to count document frequencies.
3.1.4 Headline
We used a similarity measure of the sentence to
the headline as another type of scoring function. The
basic idea is that the more words in the sentence
overlap with the words in the headline, the more im-
portant the sentence is. The function estimates the
relevance between a headline (H) and a sentence
(Si) by using the tf*idf values of the words (w) in
the headline:
Scorehl(Si) =
?
w?H?Si
tf(w)
tf(w)+1 log
DN
df(w)
?
w?H
tf(w)
tf(w)+1 log
DN
df(w)
We also evaluated another method based on this
scoring function by using only named entities (NEs)
instead of words for the TSC data and DUC data.
Only the term frequency was used for NEs, because
we judged that the document frequency for an entity
was usually quite small, thereby making the differ-
ences between entities negligible.
3.1.5 Patterns
For the DUC data, we used dependency patterns
as a type of scoring function. These patterns were
extracted by pattern discovery during information
extraction (Sudo et al, 2001). The details of this ap-
proach are not explained here, because this feature
is not among the features we analyze in Section 5.
The definition of the function appears in (Nobata et
al., 2002).
3.2 Optimal weight
Our system set weights for each scoring function in
order to calculate the total score of a sentence. The
total score (Si) is defined from the scoring functions
(Scorej()) and weights (?j) as follows:
TotalScore(Si) =
?
j
?jScorej(Si) (1)
We estimated the optimal values of these weights
from the training data. After the range of each
weight was set manually, the system changed the
values of the weights within a range and summarized
the training data for each set of weights. Each score
was recorded after the weights were changed, and
the weights with the best scores were stored.
A particular scoring method was also selected in
the cases of features with more than one defined
scoring methods. We used the dry run data from
each workshop as TSC and DUC training data. For
the TSC data, since the 30 articles contained 15 ed-
itorials and 15 news reports, we estimated optimal
values separately for editorials and news reports. For
the CSJ data, we used 50 transcriptions for training
and 10 for testing, as mentioned in Section 2.2.
Table 1: Evaluation results for the TSC data.
Ratio 10% 30% 50% Avg.
System 0.363 (1) 0.435 (5) 0.589 (2) 0.463 (2)
Lead 0.284 0.432 0.586 0.434
4 Evaluation results
In this section, we show our evaluation results on the
three sets of data for the sentence extraction system
described in the previous section.
4.1 Evaluation results for the TSC data
Table 1 shows the evaluation results for our sys-
tem and some baseline systems on the task of sen-
tence extraction at TSC-2001. The figures in Ta-
ble 1 are values of the F-measure1. The ?System?
column shows the performance of our system and its
rank among the nine systems that were applied to the
task, and the ?Lead? column shows the performance
of a baseline system which extracts as many sen-
tences as the threshold from the beginning of a doc-
ument. Since all participants could output as many
sentences as the allowed upper limit, the values of
the recall, precision, and F-measure were the same.
Our system obtained better results than the baseline
systems, especially when the compression ratio was
10%. The average performance was second among
the nine systems.
4.2 Evaluation results for the DUC data
Table 2 shows the results of a subjective evalua-
tion in the SDS task at DUC-2001. In this subjec-
tive evaluation, assessors gave a score to each sys-
tem?s outputs, on a zero-to-four scale (where four is
the best), as compared with summaries made by hu-
mans. The figures shown are the average scores over
all documents. The ?System? column shows the per-
formance of our system and its rank among the 12
systems that were applied to this task. The ?Lead?
1The definitions of each measurement are as follows:
Recall (REC) = COR / GLD
Precision (PRE) = COR / SYS
F-measure = 2 * REC * PRE / (REC + PRE),
where COR is the number of correct sentences marked by the
system, GLD is the total number of correct sentences marked
by humans, and SYS is the total number of sentences marked by
the system. After calculating these scores for each transcription,
the average is calculated as the final score.
Table 2: Evaluation results for the DUC data (sub-
jective evaluation).
System Lead Avg.
Grammaticality 3.711 (5) 3.236 3.580
Cohesion 3.054 (1) 2.926 2.676
Organization 3.215 (1) 3.081 2.870
Total 9.980 (1) 9.243 9.126
Table 3: Evaluation results for the CSJ data.
Annotators
A B C Avg.
REC 0.407 0.331 0.354 0.364
PRE 0.416 0.397 0.322 0.378
F 0.411 0.359 0.334 0.368
column shows the performance of a baseline system
that always outputs the first 100 words of a given
document, while the ?Avg.? column shows the aver-
age for all systems. Our system ranked 5th in gram-
maticality and was ranked at the top for the other
measurements, including the total value.
4.3 Evaluation results for the CSJ data
The evaluation results for sentence extraction with
the CSJ data are shown in Table 3. We compared the
system?s results with each annotator?s key data. As
mentioned previously, we used 50 transcriptions for
training and 10 for testing.
These results are comparable with the perfor-
mance on sentence segmentation for written doc-
uments, because the system?s performance for the
TSC data was 0.363 when the compression ratio was
set to 10%. The results of our experiments thus show
that for transcriptions, sentence extraction achieves
results comparable to those for written documents,
if the are well defined.
4.4 Contributions of features
Table 4 shows the contribution vectors for each set
of training data. The contribution here means the
product of the optimized weight and the standard
deviation of the score for the test data. The vec-
tors were normalized so that the sum of the com-
ponents is equal to 1, and the selected function types
for the features are also shown in the table. Our sys-
tem used the NE-based headline function (HL (N))
for the DUC data and the word-based function (HL
Table 4: Contribution (weight? s.d.) of each feature
for each set of summarization data.
TSC
Features Editorial Report DUC CSJ
Pst. P3. 0.446 P1. 0.254 P1. 0.691 P3. 0.055
Len. L3. 0.000 L3. 0.000 L2. 0.020 L2. 0.881
Tf*idf T3. 0.169 T3. 0.185 T1. 0.239 T3. 0.057
HL (W) 0.171 0.292 - 0.007
HL (N) 0.214 0.269 0.045 -
Pattern - - 0.005 -
(W)) for the CSJ data, and both functions for the
TSC data. The columns for the TSC data show the
contributions when the compression ratio was 10%.
We can see that the feature with the biggest con-
tribution varies among the data sets. While the posi-
tion feature was the most effective for the TSC and
DUC data, the length feature was dominant for the
CSJ data. Most of the short sentences in the lectures
were specific expressions, such as ?This is the result
of the experiment.? or ?Let me summarize my pre-
sentation.?. Since these sentences were not extracted
as key sentences by the annotators, it is believed that
the function giving short sentences a penalty score
matched the manual extraction results.
5 Analysis of the summarization data
In Section 4, we showed how our system, which
combines major features, has performed well as
compared with current summarization systems.
However, the evaluation results alone do not suffi-
ciently explain how such a combination of features
is effective. In this section, we investigate the corre-
lations between each pair of features. We also match
feature pairs with distributions of extracted key sen-
tences as answer summaries to find effective combi-
nation of features for sentence extraction.
5.1 Correlation between features
Table 5 shows Spearman?s rank correlation coeffi-
cients among the four features. Significantly corre-
lated feature pairs are indicated by ???(? = 0.001).
Here, the word-based feature is used as the headline
feature. We see the following tendencies for any of
the data sets:
? ?Position? is relatively independent of the other features.
? ?Length? and ?Tf*idf? have high correlation2.
Table 5: Rank correlation coefficients between fea-
tures.
TSC Report
Features Length Tf*idf Headline
Position 0.019 -0.095 -0.139
Length ? 0.546? 0.338?
Tf*idf ? ? 0.696?
TSC Editorial
Features Length Tf*idf Headline
Position -0.047 -0.099 0.046
Length ? 0.532? 0.289?
Tf*idf ? ? 0.658?
DUC Data
Features Length Tf*idf Headline
Position -0.130? -0.108? -0.134?
Length ? 0.471? 0.293?
Tf*idf ? ? 0.526?
CSJ Data
Features Length Tf*idf Headline
Position -0.092? -0.069? -0.106?
Length ? 0.460? 0.224?
Tf*idf ? ? 0.533?
? ?TF*idf? and ?Headline ? also have high correlation.
These results show that while combinations of these
four features enabled us to obtain good evaluation
results, as shown in Section 4, the features are not
necessarily independent of one another.
5.2 Combination of features
Tables 6 and 7 show the distributions of extracted
key sentences as answer summaries with two pairs
of features: sentence position and the tf*idf value,
and sentence position and the headline information.
In these tables, each sentence is ranked by each of
the two feature values, and the rankings are split ev-
ery 10 percent. For example, if a sentence is ranked
in the first 10 percent by sentence position and the
last 10 percent by the tf*idf feature, the sentence be-
longs to the cell with a position rank of 0.1 and a
tf*idf rank of 1.0 in Table 6.
Each cell thus has two letters. The left letter is the
number of key sentences, and the right letter is the
ratio of key sentences to all sentences in the cell. The
left letter shows how the number of sentences differs
from the average when all the key sentences appear
equally, regardless of the feature values. Let T be
2Here we used equation T1 for the tf*idf feature, and the
score of each sentence was normalized with the sentence length.
Hence, the high correlation between ?Length? and ?Tf*idf? is
not trivial.
the total number of key sentences, M(= T100) be the
average number of key sentences in each range, and
S be the standard deviation of the number of key
sentences among all cells. The number of key sen-
tences for cell Ti,j is then categorized according to
one of the following letters:
A: Ti,j ? M + 2S
B: M + S ? Ti,j < M + 2S
C: M ? S ? Ti,j < M + S
D: M ? 2S ? Ti,j < M ? S
E: Ti,j < M ? 2S
O: Ti,j = 0
-: No sentences exist in the cell.
Similarly, the right letter in a cell shows how the ra-
tio of key sentences differs from the average ratio
when all the key sentences appear equally, regard-
less of feature values. Let N be the total number
of sentences, m(= TN ) be the average ratio of key
sentences, and s be the standard deviation of the ra-
tio among all cells. The ratio of key sentences for
cell ti,j is then categorized according to one of the
following letters:
a: ti,j ? m+ 2s
b: m+ s ? ti,j < m+ 2s
c: m? s ? ti,j < m+ s
d: m? 2s ? ti,j < m? s
e: ti,j < m? 2s
o: ti,j = 0
-: No sentences exist in the cell.
When key sentences appear uniformly regardless of
feature values, every cell is defined as ?Cc?. We
show both the range of the number of key sentences
and the ratio of key sentences, because both are nec-
essary to show how effectively a cell has key sen-
tences. If a cell includes many sentences, the num-
ber of key sentences can be large even though the
ratio is not. On the other hand, when the ratio of key
sentences is large and the number is not, the contri-
bution to key sentence extraction is small.
Table 6 shows the distributions of key sentences
when the features of sentence position and tf*idf
were combined. For the DUC data, both the num-
ber and ratio of key sentences were large when the
sentence position was ranked within the first 20 per-
cent and the value of the tf*idf feature was ranked
in the bottom 50 percent (i.e., Pst. ? 0.2, Tf*idf ?
0.5). On the other hand, both the number and ratio
of key sentences were large for the CSJ data when
the sentence position was ranked in the last 10 per-
cent and the value of the tf*idf feature was ranked
Table 6: Distributions of key sentences based on the combination of the sentence position (Pst.) and tf*idf
features.
DUC data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cb Ba Ba Aa Aa Aa Aa
0.2 Cd Cc Cc Cc Cc Cc Bb Bb Bb Bb
0.3 Cd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.5 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.6 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Cd Dd Cc Cc Cc Cc Cc Cc Cc Cc
0.9 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
1.0 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
CSJ data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cc Bc Cc Ab Bb Bb Bb
0.2 Oo Oo Cc Cc Cc Cc Cc Bb Bc Cc
0.3 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Cc Cc Cc Cc Oo Cc Cc Cc Cc Cc
0.5 Oo Cc Oo Oo Cc Oo Cc Cc Cc Cc
0.6 Cc Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Oo Cc Cc Oo Oo Cc Cc Cc Cc Cc
0.9 Cc Cc Cc Cc Cc Cc Cc Cc Cc Bb
1.0 Cc Cc Ba Bb Bb Aa Aa Bb Aa Aa
after the first 30 percent (i.e., Pst. = 1.0, Tf*idf ?
0.3),. When the tf*idf feature was low, the number
and ratio of key sentences were not large, regardless
of the sentence position values. These results show
that the tf*idf feature is effective when the values
are used as a filter after the sentences are ranked by
sentence position.
Table 7 shows the distributions of key sentences
with the combination of the sentence position and
headline features. About half the sentences did not
share words with the headlines and had a value of
0 for the headline feature. As a result, the cells in
the middle of the table do not have corresponding
sentences. The headline feature cannot be used as
a filter, unlike the tf*idf feature, because many key
sentences are found when the value of the headline
feature is 0. A high value of the headline feature is,
however, a good indicator of key sentences when it
is combined with the position feature. The ratio of
key sentences was large when the headline ranking
was high and the sentence was near the beginning
(at Pst. ? 0.2, Headline ? 0.7) for the DUC data.
For the CSJ data, the ratio of key sentences was also
large when the headline ranking was within the top
10 percent (Pst. = 0.1, Headline = 1.0), as well as
for the sentences near the ends of speeches.
These results indicate that the number and ratio
of key sentences sometimes vary discretely accord-
ing to the changes in feature values when features
are combined for sentence extraction. That is, the
performance of a sentence extraction system can be
improved by categorizing feature values into sev-
eral ranges and then combining ranges. While most
sentence extraction systems use sequential combi-
nations of features, as we do in our system based
on Equation 1, the performance of these systems
can possibly be improved by introducing the cat-
egorization of feature values, without adding any
new features. We have shown that discrete combi-
nations match the distributions of key sentences in
two different corpora, the DUC data and the CSJ
data. This indicates that discrete combinations of
corpora are effective across both different languages
and different types of corpora. Hirao et al (2002)
reported the results of a sentence extraction system
using an SVM, which categorized sequential feature
values into ranges in order to make the features bi-
nary. Some effective combinations of the binary fea-
Table 7: Distributions of key sentences based on
the combination of the sentence position (Pst.) and
headline features.
DUC data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Ab -- -- Ca Ba Ba Aa
0.2 Ac -- -- Cb Cc Ca Ca
0.3 Ac -- -- Cc Cc Cb Cb
0.4 Ac -- -- Cc Cc Cc Cb
0.5 Ac -- -- Cc Cc Cc Cc
0.6 Bc -- -- Cc Cc Cc Cc
0.7 Bc -- -- Cc Cc Cc Cc
0.8 Ac -- -- Cd Cc Cc Cc
0.9 Bd -- -- Cd Cc Cc Cc
1.0 Bd -- -- Cd Cc Cc Cc
CSJ data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Bc -- Cc Cc Bb Cc Aa
0.2 Bc -- Cc Cb Cc Cc Bb
0.3 Cc -- Cc Cc Cc Cc Cc
0.4 Cc -- Oo Cc Cc Cc Cc
0.5 Cc -- Oo Cc Oo Cc Cc
0.6 Cc -- Cc Cc Cc Cc Cc
0.7 Cc -- Oo Cc Cc Cc Cc
0.8 Cc -- Cc Cc Cc Cc Cc
0.9 Ac -- Ca Cc Cc Cc Cb
1.0 Ab -- Ca Aa Ba Ba Ba
tures in that report also indicate the effectiveness of
discrete combinations of features.
6 Conclusion
We have shown evaluation results for our sentence
extraction system and analyzed its features for dif-
ferent types of corpora, which included corpora dif-
fering in both language (Japanese and English) and
type (newspaper articles and lectures). The sys-
tem is based on four major features, and it achieved
some of the top results at evaluation workshops in
2001 for summarizing Japanese newspaper articles
(TSC) and English newspaper articles (DUC). For
Japanese lectures, the sentence extraction system
also obtained comparable results when the sentence
boundary was given.
Our analysis of the features used in this sentence
extraction system has shown that they are not neces-
sarily independent of one another, based on the re-
sults of their rank correlation coefficients. The anal-
ysis also indicated that the categorization of feature
values matches the distribution of key sentences bet-
ter than sequential feature values.
There are several features that were not described
here but are also used in sentence extraction sys-
tems, such as some specific lexical expressions and
syntactic information. In our future work, we will
analyze and use these features to improve the per-
formance of our sentence extraction system.
References
C. Aone, M. E. Okurowski, and J. Gorlinsky. 1998. Train-
able, Scalable Summarization Using Robust NLP and Ma-
chine Learning. In Proc. of COLING-ACL?98, pages 62?66.
DUC. 2001. http://duc.nist.gov. Document Understanding
Conference.
H. Edmundson. 1969. New methods in automatic abstracting.
Journal of ACM, 16(2):264?285.
T. Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto. 2002. Ex-
tracting Important Sentences with Support Vector Machines.
In Proc. of COLING-2002.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A Trainable Docu-
ment Summarizaer. In Proc. of SIGIR?95, pages 68?73.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analyzing System: JUMAN version 3.61. Kyoto University.
Chin-Yew Lin. 1999. Training a selection function for extrac-
tion. In Proc. of the CIKM?99.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proc. of LREC2000,
pages 947?952.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman. 2002. Sum-
marization System Integrated with Named Entity Tagging
and IE pattern Discovery. In Proceedings of the LREC-2002
Conference, pages 1742?1745, May.
T. Nomoto and Y. Matsumoto. 1997. The Reliability of Human
Coding and Effects on Automatic Abstracting (in Japanese).
In IPSJ-NL 120-11, pages 71?76, July.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-poisson model for probabilistic
weighted retreival. In Proc. of SIGIR?94.
K. Sudo, S. Sekine, and R. Grishman. 2001. Automatic pattern
acquisition for japanese information extraction. In Proc. of
HLT-2001.
TSC. 2001. Proceedings of the Second NTCIR Workshop
on Research in Chinese & Japanese Text Retrieval and Text
Summarization (NTCIR2). National Institute of Informat-
ics.
