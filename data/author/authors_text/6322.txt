Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 897?905, Prague, June 2007. c?2007 Association for Computational Linguistics
Learning Structured Models for Phone Recognition
Slav Petrov Adam Pauls Dan Klein
Computer Science Department, EECS Divison
University of California at Berkeley
Berkeley, CA, 94720, USA
{petrov,adpauls,klein}@cs.berkeley.edu
Abstract
We present a maximally streamlined approach to
learning HMM-based acoustic models for automatic
speech recognition. In our approach, an initial mono-
phone HMM is iteratively refined using a split-merge
EM procedure which makes no assumptions about
subphone structure or context-dependent structure,
and which uses only a single Gaussian per HMM
state. Despite the much simplified training process,
our acoustic model achieves state-of-the-art results
on phone classification (where it outperforms almost
all other methods) and competitive performance on
phone recognition (where it outperforms standard CD
triphone / subphone / GMM approaches). We also
present an analysis of what is and is not learned by
our system.
1 Introduction
Continuous density hiddenMarkov models (HMMs)
underlie most automatic speech recognition (ASR)
systems in some form. While the basic algorithms
for HMM learning and inference are quite general,
acoustic models of speech standardly employ rich
speech-specific structures to improve performance.
For example, it is well known that a monophone
HMM with one state per phone is too coarse an
approximation to the true articulatory and acoustic
process. The HMM state space is therefore refined
in several ways. To model phone-internal dynam-
ics, phones are split into beginning, middle, and end
subphones (Jelinek, 1976). To model cross-phone
coarticulation, the states of the HMM are refined
by splitting the phones into context-dependent tri-
phones. These states are then re-clustered (Odell,
1995) and the parameters of their observation dis-
tributions are tied back together (Young and Wood-
land, 1994). Finally, to model complex emission
densities, states emit mixtures of multivariate Gaus-
sians. This standard structure is shown schemati-
cally in Figure 1. While this rich structure is pho-
netically well-motivated and empirically success-
ful, so much structural bias may be unnecessary, or
even harmful. For example in the domain of syn-
tactic parsing with probabilistic context-free gram-
mars (PCFGs), a surprising recent result is that au-
tomatically induced grammar refinements can out-
perform sophisticated methods which exploit sub-
stantial manually articulated structure (Petrov et al,
2006).
In this paper, we consider a much more automatic,
data-driven approach to learning HMM structure for
acoustic modeling, analagous to the approach taken
by Petrov et al (2006) for learning PCFGs. We start
with a minimal monophone HMM in which there is
a single state for each (context-independent) phone.
Moreover, the emission model for each state is a sin-
gle multivariate Gaussian (over the standard MFCC
acoustic features). We then iteratively refine this
minimal HMM through state splitting, adding com-
plexity as needed. States in the refined HMMs are
always substates of the original HMM and are there-
fore each identified with a unique base phone. States
are split, estimated, and (perhaps) merged, based on
a likelihood criterion. Our model never allows ex-
plicit Gaussian mixtures, though substates may de-
velop similar distributions and thereby emulate such
mixtures.
In principle, discarding the traditional structure
can either help or hurt the model. Incorrect prior
splits can needlessly fragment training data and in-
correct prior tying can limit the model?s expressiv-
ity. On the other hand, correct assumptions can
increase the efficiency of the learner. Empirically,
897
Start
begin end
End
mid begin endmid
d
7 
= c(#-d-ae)
begin endmid
ae
3 
= c(d-ae-d) d
13 
= c(ae-d-#)
Start
a d
End
a d a d
d ae d
b c b c b c
Figure 1: Comparison of the standard model to our model (here
shown with k = 4 subphones per phone) for the word dad.
The dependence of subphones across phones in our model is
not shown, while the context clustering in the standard model is
shown only schematically.
we show that our automatic approach outperforms
classic systems on the task of phone recognition on
the TIMIT data set. In particular, it outperforms
standard state-tied triphone models like Young and
Woodland (1994), achieving a phone error rate of
26.4% versus 27.7%. In addition, our approach
gives state-of-the-art performance on the task of
phone classification on the TIMIT data set, suggest-
ing that our learned structure is particularly effec-
tive at modeling phone-internal structure. Indeed,
our error rate of 21.4% is outperformed only by the
recent structured margin approach of Sha and Saul
(2006). It remains to be seen whether these posi-
tive results on acoustic modeling will facilitate better
word recognition rates in a large vocabulary speech
recognition system.
We also consider the structures learned by the
model. Subphone structure is learned, similar to,
but richer than, standard begin-middle-end struc-
tures. Cross-phone coarticulation is also learned,
with classic phonological classes often emerging
naturally.
Many aspects of this work are intended to sim-
plify rather than further articulate the acoustic pro-
cess. It should therefore be clear that the basic tech-
niques of splitting, merging, and learning using EM
are not in themselves new for ASR. Nor is the basic
latent induction method new (Matsuzaki et al, 2005;
Petrov et al, 2006). What is novel in this paper is (1)
the construction of an automatic system for acous-
tic modeling, with substantially streamlined struc-
ture, (2) the investigation of variational inference for
such a task, (3) the analysis of the kinds of struc-
tures learned by such a system, and (4) the empirical
demonstration that such a system is not only com-
petitive with the traditional approach, but can indeed
outperform even very recent work on some prelimi-
nary measures.
2 Learning
In the following, we propose a greatly simplified
model that does not impose any manually specified
structural constraints. Instead of specifying struc-
ture a priori, we use the Expectation-Maximization
(EM) algorithm for HMMs (Baum-Welch) to auto-
matically induce the structure in a way that maxi-
mizes data likelihood.
In general, our training data consists of sets
of acoustic observation sequences and phone level
transcriptions r which specify a sequence of phones
from a set of phones Y , but does not label each
time frame with a phone. We refer to an observa-
tion sequence as x = x1, . . . , xT where xi ? R39
are standard MFCC features (Davis and Mermel-
stein, 1980). We wish to induce an HMM over a
set of states S for which we also have a function
pi : S ? Y that maps every state in S to a phone
in Y . Note that in the usual formulation of the EM
algorithm for HMMs, one is interested in learning
HMM parameters ? that maximize the likelihood of
the observations P(x|?); in contrast, we aim to max-
imize the joint probability of our observations and
phone transcriptions P(x, r|?) or observations and
phone sequences P(x,y|?) (see below). We now de-
scribe this relatively straightforward modification of
the EM algorithm.
2.1 The Hand-Aligned Case
For clarity of exposition we first consider a simpli-
fied scenario in which we are given hand-aligned
phone labels y = y1, . . . , yT for each time t, as is
the case for the TIMIT dataset. Our procedure does
not require such extensive annotation of the training
data and in fact gives better performance when the
exact transition point between phones are not pre-
specified but learned.
We define forward and backward probabilities
(Rabiner, 1989) in the following way: the forward
probability is the probability of observing the se-
quence x1, . . . , xt with transcription y1, . . . , yt and
898
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
(a)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0 1
(b)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
0
3
2
1
(c)
next
previous
th
dh
p
t
b
g
dx
w
r
l
s
z
sh
f
cl
vcl
m
n
ng
l
r
er
1
6
0
3
4
7
25
(d)
Figure 2: Iterative refinement of the /ih/ phone with 1, 2, 4, 8 substates.
ending in state s at time t:
?t(s) = P(x1, . . . , xt, y1, . . . yt, st = s|?),
and the backward probability is the probability of
observing the sequence xt+1, . . . , xT with transcrip-
tion yt+1, . . . , yT , given that we start in state s at
time t:
?t(s) = P(xt+1, . . . , xT , yt+1, . . . , yT |st = s, ?),
where ? are the model parameters. As usual, we
parameterize our HMMs with ass? , the probability
of transitioning from state s to s?, and bs(x) ?
N (?s,?s), the probability emitting the observation
x when in state s.
These probabilities can be computed using the
standard forward and backward recursions (Rabiner,
1989), except that at each time t, we only con-
sider states st for which pi(st) = yt, because we
have hand-aligned labels for the observations. These
quantities also allow us to compute the posterior
counts necessary for the E-step of the EM algorithm.
2.2 Splitting
One way of inducing arbitrary structural annota-
tions would be to split each HMM state in into
m substates, and re-estimate the parameters for the
split HMM using EM. This approach has two ma-
jor drawbacks: for larger m it is likely to converge
to poor local optima, and it allocates substates uni-
formly across all states, regardless of how much an-
notation is required for good performance.
To avoid these problems, we apply a hierarchical
parameter estimation strategy similar in spirit to the
work of Sankar (1998) and Ueda et al (2000), but
here applied to HMMs rather than to GMMs. Be-
ginning with the baseline model, where each state
corresponds to one phone, we repeatedly split and
re-train the HMM. This strategy ensures that each
split HMM is initialized ?close? to some reasonable
maximum.
Concretely, each state s in the HMM is split in
two new states s1, s2 with pi(s1) = pi(s2) = pi(s).
We initialize EM with the parameters of the previ-
ous HMM, splitting every previous state s in two
and adding a small amount of randomness  ? 1%
to its transition and emission probabilities to break
symmetry:
as1s? ? ass? + ,
bs1(o) ? N (?s + ,?s),
and similarly for s2. The incoming transitions are
split evenly.
We then apply the EM algorithm described above
to re-estimate these parameters before performing
subsequent split operations.
2.3 Merging
Since adding substates divides HMM statistics into
many bins, the HMM parameters are effectively es-
timated from less data, which can lead to overfitting.
Therefore, it would be to our advantage to split sub-
899
states only where needed, rather than splitting them
all.
We realize this goal by merging back those splits
s ? s1s2 for which, if the split were reversed, the
loss in data likelihood would be smallest. We ap-
proximate the loss in data likelihood for a merge
s1s2 ? swith the following likelihood ratio (Petrov
et al, 2006):
?(s1 s2 ? s) =
?
sequences
?
t
Pt(x,y)
P(x,y)
.
Here P(x,y) is the joint likelihood of an emission
sequence x and associated state sequence y. This
quantity can be recovered from the forward and
backward probabilities using
P(x,y) =
?
s:pi(s)=yt
?t(s) ? ?t(s).
Pt(x,y) is an approximation to the same joint like-
lihood where states s1 and s2 are merged. We ap-
proximate the true loss by only considering merging
states s1 and s2 at time t, a value which can be ef-
ficiently computed from the forward and backward
probabilities. The forward score for the merged state
s at time t is just the sum of the two split scores:
??t(s) = ?t(s1) + ?t(s2),
while the backward score is a weighted sum of the
split scores:
??t(s) = p1?t(s1) + p2?t(s2),
where p1 and p2 are the relative (posterior) frequen-
cies of the states s1 and s2.
Thus, the likelihood after merging s1 and s2 at
time t can be computed from these merged forward
and backward scores as:
P t(x,y) = ??t(s) ? ??t(s) +
?
s?
?t(s
?) ? ?t(s
?)
where the second sum is over the other substates of
xt, i.e. {s? : pi(s?) = xt, s? /? {s1, s2}}. This
expression is an approximation because it neglects
interactions between instances of the same states at
multiple places in the same sequence. In particular,
since phones frequently occur with multiple consec-
utive repetitions, this criterion may vastly overesti-
mate the actual likelihood loss. As such, we also im-
plemented the exact criterion, that is, for each split,
we formed a new HMM with s1 and s2 merged and
calculated the total data likelihood. This method
is much more computationally expensive, requiring
a full forward-backward pass through the data for
each potential merge, and was not found to produce
noticeably better performance. Therefore, all exper-
iments use the approximate criterion.
2.4 The Automatically-Aligned Case
It is straightforward to generalize the hand-aligned
case to the case where the phone transcription is
known, but no frame level labeling is available. The
main difference is that the phone boundaries are not
known in advance, which means that there is now
additional uncertainty over the phone states. The
forward and backward recursions must thus be ex-
panded to consider all state sequences that yield the
given phone transcription. We can accomplish this
with standard Baum-Welch training.
3 Inference
An HMM over refined subphone states s ? S nat-
urally gives posterior distributions P(s|x) over se-
quences of states s. We would ideally like to ex-
tract the transcription r of underlying phones which
is most probable according to this posterior1. The
transcription is two stages removed from s. First,
it collapses the distinctions between states s which
correspond to the same phone y = pi(s). Second,
it collapses the distinctions between where phone
transitions exactly occur. Viterbi state sequences can
easily be extracted using the basic Viterbi algorithm.
On the other hand, finding the best phone sequence
or transcription is intractable.
As a compromise, we extract the phone sequence
(not transcription) which has highest probability in
a variational approximation to the true distribution
(Jordan et al, 1999). Let the true posterior distri-
bution over phone sequences be P(y|x). We form
an approximation Q(y) ? P(y|x), where Q is an
approximation specific to the sequence x and factor-
1Remember that by ?transcription? we mean a sequence of
phones with duplicates removed.
900
izes as:
Q(y) =
?
t
q(t, xt, yt+1).
We would like to fit the values q, one for each time
step and state-state pair, so as to make Q as close to
P as possible:
min
q
KL(P(y|x)||Q(y)).
The solution can be found analytically using La-
grange multipliers:
q(t, y, y?) =
P(Yt = y, Yt+1 = y?|x)
P(Yt = y|x)
.
where we have made the position-specific random
variables Yt explicit for clarity. This approximation
depends only on our ability to calculate posteriors
over phones or phone-phone pairs at individual po-
sitions t, which is easy to obtain from the state pos-
teriors, for example:
P(Yt = y,Yt+1 = y
?|x) =
?
s:pi(s)=y
?
s?:pi(s?)=y?
?t(s)ass?bs?(xt)?t+1(s
?)
P(x)
Finding the Viterbi phone sequence in the approxi-
mate distribution Q, can be done with the Forward-
Backward algorithm over the lattice of q values.
4 Experiments
We tested our model on the TIMIT database, using
the standard setups for phone recognition and phone
classification. We partitioned the TIMIT data into
training, development, and (core) test sets according
to standard practice (Lee and Hon, 1989; Gunawar-
dana et al, 2005; Sha and Saul, 2006). In particu-
lar, we excluded all sa sentences and mapped the 61
phonetic labels in TIMIT down to 48 classes before
training our HMMs. At evaluation, these 48 classes
were further mapped down to 39 classes, again in
the standard way.
MFCC coefficients were extracted from the
TIMIT source as in Sha and Saul (2006), includ-
ing delta and delta-delta components. For all experi-
ments, our system and all baselines we implemented
used full covariance when parameterizing emission
 
0.24
 
0.26
 
0.28 0.3
 
0.32
 
0.34
 
0.36
 
0.38 0.4
 
0.42  0
 
200 
400 
600 
800 1
000 1
200 1
400 1
600 1
800 2
000
Phone Recognition Error
Numb
er of S
tates
split o
nly
split a
nd me
rge
split a
nd me
rge, au
tomati
c align
ment
Figure 3: Phone recognition error for models of increasing size
models.2 All Gaussians were endowed with weak
inverse Wishart priors with zero mean and identity
covariance.3
4.1 Phone Recognition
In the task of phone recognition, we fit an HMM
whose output, with subsequent states collapsed, cor-
responds to the training transcriptions. In the TIMIT
data set, each frame is manually phone-annotated, so
the only uncertainty in the basic setup is the identity
of the (sub)states at each frame.
We therefore began with a single state for each
phone, in a fully connected HMM (except for spe-
cial treatment of dedicated start and end states). We
incrementally trained our model as described in Sec-
tion 2, with up to 6 split-merge rounds. We found
that reversing 25% of the splits yielded good overall
performance while maintaining compactness of the
model.
We decoded using the variational decoder de-
scribed in Section 3. The output was then scored
against the reference phone transcription using the
standard string edit distance.
During both training and decoding, we used ?flat-
tened? emission probabilities by exponentiating to
some 0 < ? < 1. We found the best setting for ?
to be 0.2, as determined by tuning on the develop-
ment set. This flattening compensates for the non-
2Most of our findings also hold for diagonal covariance
Gaussians, albeit the final error rates are 2-3% higher.
3Following previous work with PCFGs (Petrov et al, 2006),
we experimented with smoothing the substates towards each
other to prevent overfitting, but we were unable to achieve any
performance gains.
901
Method Error Rate
State-Tied Triphone HMM
27.7%1
(Young and Woodland, 1994)
Gender Dependent Triphone HMM
27.1%1
(Lamel and Gauvain, 1993)
This Paper 26.4%
Bayesian Triphone HMM
25.6%
(Ming and Smith, 1998)
Heterogeneous classifiers
24.4%
(Halberstadt and Glass, 1998)
Table 1: Phone recognition error rates on the TIMIT core test
from Glass (2003).
1These results are on a slightly easier test set.
independence of the frames, partially due to over-
lapping source samples and partially due to other
unmodeled correlations.
Figure 3 shows the recognition error as the model
grows in size. In addition to the basic setup de-
scribed so far (split and merge), we also show a
model in which merging was not performed (split
only). As can be seen, the merging phase not only
decreases the number of HMM states at each round,
but also improves phone recognition error at each
round.
We also compared our hierarchical split only
model with a model where we directly split all states
into 2k substates, so that these models had the same
number of states as a a hierarchical model after k
split and merge cycles. While for small k, the dif-
ference was negligible, we found that the error in-
creased by 1% absolute for k = 5. This trend is to
be expected, as the possible interactions between the
substates grows with the number of substates.
Also shown in Figure 3, and perhaps unsurprising,
is that the error rate can be further reduced by allow-
ing the phone boundaries to drift from the manual
alignments provided in the TIMIT training data. The
split and merge, automatic alignment line shows the
result of allowing the EM fitting phase to reposition
each phone boundary, giving absolute improvements
of up to 0.6%.
We investigated how much improvement in accu-
racy one can gain by computing the variational ap-
proximation introduced in Section 3 versus extract-
ing the Viterbi state sequence and projecting that se-
quence to its phone transcription. The gap varies,
Method Error Rate
GMM Baseline (Sha and Saul, 2006) 26.0%
HMM Baseline (Gunawardana et al, 2005) 25.1%
SVM (Clarkson and Moreno, 1999) 22.4%
Hidden CRF (Gunawardana et al, 2005) 21.7%
This Paper 21.4%
Large Margin GMM (Sha and Saul, 2006) 21.1%
Table 2: Phone classification error rates on the TIMIT core test.
but on a model with roughly 1000 states (5 split-
merge rounds), the variational decoder decreases er-
ror from 26.5% to 25.6%. The gain in accuracy
comes at a cost in time: we must run a (possibly
pruned) Forward-Backward pass over the full state
space S, then another over the smaller phone space
Y . In our experiments, the cost of variational decod-
ing was a factor of about 3, which may or may not
justify a relative error reduction of around 4%.
The performance of our best model (split and
merge, automatic alignment, and variational decod-
ing) on the test set is 26.4%. A comparison of our
performance with other methods in the literature is
shown in Table 1. Despite our structural simplic-
ity, we outperform state-tied triphone systems like
Young andWoodland (1994), a standard baseline for
this task, by nearly 2% absolute. However, we fall
short of the best current systems.
4.2 Phone Classification
Phone classification is the fairly constrained task of
classifying in isolation a sequence of frames which
is known to span exactly one phone. In order to
quantify how much of our gains over the triphone
baseline stem from modeling context-dependencies
and how much from modeling the inner structure of
the phones, we fit separate HMM models for each
phone, using the same split and merge procedure as
above (though in this case only manual alignments
are reasonable because we test on manual segmen-
tations). For each test frame sequence, we com-
pute the likelihood of the sequence from the forward
probabilities of each individual phone HMM. The
phone giving highest likelihood to the input was se-
lected. The error rate is a simple fraction of test
phones classified correctly.
Table 2 shows a comparison of our performance
with that of some other methods in the literature.
A minimal comparison is to a GMM with the same
number of mixtures per phone as our model?s maxi-
902
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iy ix eh ae ax uw uh aa ey ay oy aw ow er el r w y m n ng dx jh ch z s zh hh v f dh th b p d t g k sil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
iyixehaeaxuwuhaaeyayoyawowerelrwymnngdxjhchzszhhhvfdhthbpdtgksil
Hypothesis
Ref
ere
nce
vowels/semivowels
nasals/flaps
strong fricatives
weak fricatives
stops
Figure 4: Phone confusion matrix. 76% of the substitutions fall
within the shown classes.
mum substates per phone. While these models have
the same number of total Gaussians, in our model
the Gaussians are correlated temporally, while in
the GMM they are independent. Enforcing begin-
middle-end HMM structure (see HMM Baseline) in-
creases accuracy somewhat, but our more general
model clearly makes better use of the available pa-
rameters than those baselines.
Indeed, our best model achieves a surpris-
ing performance of 21.4%, greatly outperform-
ing other generative methods and achieving perfor-
mance competitive with state-of-the-art discrimina-
tive methods. Only the recent structured margin ap-
proach of Sha and Saul (2006) gives a better perfor-
mance than our model. The strength of our system
on the classification task suggests that perhaps it is
modeling phone-internal structure more effectively
than cross-phone context.
5 Analysis
While the overall phone recognition and classifi-
cation numbers suggest that our system is broadly
comparable to and perhaps in certain ways superior
to classical approaches, it is illuminating to investi-
gate what is and is not learned by the model.
Figure 4 gives a confusion matrix over the substi-
tution errors made by our model. The majority of the
next
previous
eh
ow
ao
aa
ey
iy
ix
v
f
k
m
ow
ao
aa
ey
iy
ih
ae
ix
z
f
s
1
4
3
5
62
0
p
Figure 5: Phone contexts and subphone structure. The /l/ phone
after 3 split-merge iterations is shown.
confusions are within natural classes. Some partic-
ularly frequent and reasonable confusions arise be-
tween the consonantal /r/ and the vocalic /er/ (the
same confusion arises between /l/ and /el/, but the
standard evaluation already collapses this distinc-
tion), the reduced vowels /ax/ and /ix/, the voiced
and voiceless alveolar sibilants /z/ and /s/, and the
voiced and voiceless stop pairs. Other vocalic con-
fusions are generally between vowels and their cor-
responding reduced forms. Overall, 76% of the sub-
stitutions are within the broad classes shown in the
figure.
We can also examine the substructure learned for
the various phones. Figure 2 shows the evolution
of the phone /ih/ from a single state to 8 substates
during split/merge (no merges were chosen for this
phone), using hand-alignment of phones to frames.
These figures were simplified from the complete
state transition matrices as follows: (1) adjacent
phones? substates are collapsed, (2) adjacent phones
are selected based on frequency and inbound prob-
ability (and forced to be the same across figures),
(3) infrequent arcs are suppressed. In the first split,
(b), a sonorant / non-sonorant distinction is learned
over adjacent phones, along with a state chain which
captures basic duration (a self-looping state gives
an exponential model of duration; the sum of two
such states is more expressive). Note that the nat-
903
ural classes interact with the chain in a way which
allows duration to depend on context. In further re-
finements, more structure is added, including a two-
track path in (d) where one track captures the distinct
effects on higher formants of r-coloring and nasal-
ization. Figure 5 shows the corresponding diagram
for /l/, where some merging has also occurred. Dif-
ferent natural classes emerge in this case, with, for
example, preceding states partitioned into front/high
vowels vs. rounded vowels vs. other vowels vs. con-
sonants. Following states show a front/back dis-
tinction and a consonant distinction, and the phone
/m/ is treated specially, largely because the /lm/ se-
quence tends to shorten the /l/ substantially. Note
again how context, internal structure, and duration
are simultaneously modeled. Of course, it should
be emphasized that post hoc analysis of such struc-
ture is a simplification and prone to seeing what one
expects; we present these examples to illustrate the
broad kinds of patterns which are detected.
As a final illustration of the nature of the learned
models, Table 3 shows the number of substates allo-
cated to each phone by the split/merge process (the
maximum is 32 for this stage) for the case of hand-
aligned (left) as well as automatically-aligned (right)
phone boundaries. Interestingly, in the hand-aligned
case, the vowels absorb most of the complexity since
many consonantal cues are heavily evidenced on
adjacent vowels. However, in the automatically-
aligned case, many vowel frames with substantial
consontant coloring are re-allocated to those adja-
cent consonants, giving more complex consonants,
but comparatively less complex vowels.
6 Conclusions
We have presented a minimalist, automatic approach
for building an accurate acoustic model for phonetic
classification and recognition. Our model does not
require any a priori phonetic bias or manual spec-
ification of structure, but rather induces the struc-
ture in an automatic and streamlined fashion. Start-
ing from a minimal monophone HMM, we auto-
matically learn models that achieve highly compet-
itive performance. On the TIMIT phone recogni-
tion task our model clearly outperforms standard
state-tied triphone models like Young and Wood-
land (1994). For phone classification, our model
Vowels
aa 31 32
ae 32 17
ah 31 8
ao 32 23
aw 18 6
ax 18 3
ay 32 28
eh 32 16
el 6 4
en 4 3
er 32 31
ey 32 30
ih 32 11
ix 31 16
iy 31 32
ow 26 10
oy 4 4
uh 5 2
uw 21 8
Consonants
b 2 32
ch 13 30
d 2 14
dh 6 31
dx 2 3
f 32 32
g 2 15
hh 3 5
jh 3 16
k 30 32
l 25 32
m 25 25
n 29 32
ng 3 4
p 5 24
r 32 32
s 32 32
sh 30 32
t 24 32
th 8 11
v 23 11
w 10 21
y 3 7
z 31 32
zh 2 2
Other
epi 2 4
sil 32 32
vcl 29 30
cl 31 32
Table 3: Number of substates allocated per phone. The left
column gives the number of substates allocated when training
on manually aligned training sequences, while the right column
gives the number allocated when we automatically determine
phone boundaries.
achieves performance competitive with the state-of-
the-art discriminative methods (Sha and Saul, 2006),
despite being generative in nature. This result to-
gether with our analysis of the context-dependencies
and substructures that are being learned, suggests
that our model is particularly well suited for mod-
eling phone-internal structure. It does, of course
remain to be seen if and how these benefits can be
scaled to larger systems.
References
P. Clarkson and P. Moreno. 1999. On the use of Sup-
port Vector Machines for phonetic classification. In
ICASSP ?99.
S. B. Davis and P. Mermelstein. 1980. Comparison
of parametric representation for monosyllabic word
recognition in continuously spoken sentences. IEEE
Transactions on Acoustics, Speech, and Signal Pro-
cessing, 28(4).
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17(2).
A. Gunawardana, M. Mahajan, A. Acero, and J. Platt.
2005. Hidden Conditional Random Fields for phone
recognition. In Eurospeech ?05.
A. K. Halberstadt and J. R. Glass. 1998. Hetero-
geneous measurements and multiple classifiers for
speech recognition. In ICSLP ?98.
F. Jelinek. 1976. Continuous speech recognition by sta-
tistical methods. Proceedings of the IEEE.
904
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.
Saul. 1999. An introduction to variational methods
for graphical models. Learning in Graphical Models.
L. Lamel and J. Gauvain. 1993. Cross-lingual experi-
ments with phone recognition. In ICASSP ?93.
K. F. Lee and H. W. Hon. 1989. Speaker-independent
phone recognition using Hidden Markov Models.
IEEE Transactions on Acoustics, Speech, and Signal
Processing, 37(11).
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ?05.
J. Ming and F.J. Smith. 1998. Improved phone recogni-
tion using Bayesian triphone models. In ICASSP ?98.
J. J. Odell. 1995. The Use of Context in Large Vocab-
ulary Speech Recognition. Ph.D. thesis, University of
Cambridge.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In COLING-ACL ?06.
L. Rabiner. 1989. A Tutorial on hidden Markov mod-
els and selected applications in speech recognition. In
IEEE.
A. Sankar. 1998. Experiments with a Gaussian merging-
splitting algorithm for HMM training for speech
recognition. In DARPA Speech Recognition Workshop
?98.
F. Sha and L. K. Saul. 2006. Large margin Gaussian mix-
ture modeling for phonetic classification and recogni-
tion. In ICASSP ?06.
N. Ueda, R. Nakano, Z. Ghahramani, and G. E. Hinton.
2000. Split andMerge EM algorithm for mixture mod-
els. Neural Computation, 12(9).
S. J. Young and P. C. Woodland. 1994. State clustering
in HMM-based continuous speech recognition. Com-
puter Speech and Language, 8(4).
905
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418?1427,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Consensus Training for Consensus Decoding in Machine Translation
Adam Pauls, John DeNero and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,denero,klein}@cs.berkeley.edu
Abstract
We propose a novel objective function for dis-
criminatively tuning log-linear machine trans-
lation models. Our objective explicitly op-
timizes the BLEU score of expected n-gram
counts, the same quantities that arise in forest-
based consensus and minimum Bayes risk de-
coding methods. Our continuous objective
can be optimized using simple gradient as-
cent. However, computing critical quantities
in the gradient necessitates a novel dynamic
program, which we also present here. As-
suming BLEU as an evaluation measure, our
objective function has two principle advan-
tages over standard max BLEU tuning. First,
it specifically optimizes model weights for
downstream consensus decoding procedures.
An unexpected second benefit is that it reduces
overfitting, which can improve test set BLEU
scores when using standard Viterbi decoding.
1 Introduction
Increasing evidence suggests that machine trans-
lation decoders should not search for a single
top scoring Viterbi derivation, but should instead
choose a translation that is sensitive to the model?s
entire predictive distribution. Several recent con-
sensus decoding methods leverage compact repre-
sentations of this distribution by choosing transla-
tions according to n-gram posteriors and expected
counts (Tromble et al, 2008; DeNero et al, 2009;
Li et al, 2009; Kumar et al, 2009). This change
in decoding objective suggests a complementary
change in tuning objective, to one that optimizes
expected n-gram counts directly. The ubiquitous
minimum error rate training (MERT) approach op-
timizes Viterbi predictions, but does not explicitly
boost the aggregated posterior probability of de-
sirable n-grams (Och, 2003).
We therefore propose an alternative objective
function for parameter tuning, which we call con-
sensus BLEU or CoBLEU, that is designed to
maximize the expected counts of the n-grams that
appear in reference translations. To maintain con-
sistency across the translation pipeline, we for-
mulate CoBLEU to share the functional form of
BLEU used for evaluation. As a result, CoBLEU
optimizes exactly the quantities that drive efficient
consensus decoding techniques and precisely mir-
rors the objective used for fast consensus decoding
in DeNero et al (2009).
CoBLEU is a continuous and (mostly) differ-
entiable function that we optimize using gradient
ascent. We show that this function and its gradient
are efficiently computable over packed forests of
translations generated by machine translation sys-
tems. The gradient includes expectations of prod-
ucts of features and n-gram counts, a quantity that
has not appeared in previous work. We present a
new dynamic program which allows the efficient
computation of these quantities over translation
forests. The resulting gradient ascent procedure
does not require any k-best approximations. Op-
timizing over translation forests gives similar sta-
bility benefits to recent work on lattice-based min-
imum error rate training (Macherey et al, 2008)
and large-margin training (Chiang et al, 2008).
We developed CoBLEU primarily to comple-
ment consensus decoding, which it does; it pro-
duces higher BLEU scores than coupling MERT
with consensus decoding. However, we found
an additional empirical benefit: CoBLEU is less
prone to overfitting than MERT, even when using
Viterbi decoding. In experiments, models trained
to maximize tuning set BLEU using MERT con-
sistently degraded in performance from tuning to
test set, while CoBLEU-trained models general-
ized more robustly. As a result, we found that op-
timizing CoBLEU improved test set performance
reliably using consensus decoding and occasion-
ally using Viterbi decoding.
1418
Once upon a rhyme
H
1
) Once on a rhyme
H
3
) Once upon a time
H
2
) Once upon a rhyme
Il ?tait une rime
(a) Tuning set sentence and translation
(a) Hypotheses ranked by ?
TM 
= ?
LM 
= 1
(a)  Model score as a function of ?
LM
 
Reference r:
Sentence f:
TM LM
-3 -7 0.67
-5 -6 0.24
-9 -3 0.09
Pr
(b)  Objectives as functions of ?
LM
(b) Computing Consensus Bigram Precision
-18
-12
-6
0
0 2
H
3
H
1
H
2
Parameter: ?
LM
M
o
d
e
l
:
 
T
M
 
+
 
?
L
M
 
?
 
L
M
 
V
i
t
e
r
b
i
 
&
 
C
o
n
s
e
n
s
u
s
 
O
b
j
e
c
t
i
v
e
s
Parameter: ?
LM
E
?
[c(?Once upon?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?upon a?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?a rhyme?, d)|f ] = 0.67 + 0.24 = 0.91
?
g
E
?
[c(g, d)|f ] = 3[0.67 + 0.24 + 0.09]
?
g
min{E
?
[c(g, d)|f ], c(g, r)}
?
g
E
?
[c(g, d)|f ]
=
0.33 + 0.33 + 0.91
3
Figure 1: (a) A simple hypothesis space of translations
for a single sentence containing three alternatives, each
with two features. The hypotheses are scored under a
log-linear model with parameters ? equal to the identity
vector. (b) The expected counts of all bigrams that ap-
pear in the computation of consensus bigram precision.
2 Consensus Objective Functions
Our proposed objective function maximizes n-
gram precision by adapting the BLEU evaluation
metric as a tuning objective (Papineni et al, 2002).
To simplify exposition, we begin by adapting a
simpler metric: bigram precision.
2.1 Bigram Precision Tuning
Let the tuning corpus consist of source sentences
F = f
1
. . . f
m
and human-generated references
R = r
1
. . . r
m
, one reference for each source
sentence. Let e
i
be a translation of f
i
, and let
E = e
1
. . . e
m
be a corpus of translations, one for
each source sentence. A simple evaluation score
for E is its bigram precision BP(R,E):
BP(R,E) =
?
m
i=1
?
g
2
min{c(g
2
, e
i
), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, e
i
)
where g
2
iterates over the set of bigrams in the tar-
get language, and c(g
2
, e) is the count of bigram
g
2
in translation e. As in BLEU, we ?clip? the bi-
gram counts of e in the numerator using counts of
bigrams in the reference sentence.
Modern machine translation systems are typi-
cally tuned to maximize the evaluation score of
Viterbi derivations
1
under a log-linear model with
parameters ?. Let d
?
?
(f
i
) = arg max
d
P
?
(d|f
i
) be
the highest scoring derivation d of f
i
. For a system
employing Viterbi decoding and evaluated by bi-
gram precision, we would want to select ? to max-
imize MaxBP(R,F, ?):
?
m
i=1
?
g
2
min{c(g
2
, d
?
?
(f
i
)), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, d
?
?
(f
i
))
On the other hand, for a system that uses ex-
pected bigram counts for decoding, we would pre-
fer to choose ? such that expected bigram counts
match bigrams in the reference sentence. To this
end, we can evaluate an entire posterior distri-
bution over derivations by computing the same
clipped precision for expected bigram counts us-
ing CoBP(R,F, ?):
?
m
i=1
?
g
2
min{E
?
[c(g
2
, d)|f
i
], c(g
2
, r
i
)}
?
m
i=1
?
g
2
E
?
[c(g
2
, d)|f
i
]
(1)
where
E
?
[c(g
2
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(g
2
, d)
is the expected count of bigram g
2
in all deriva-
tions d of f
i
. We define the precise parametric
form of P
?
(d|f
i
) in Section 3. Figure 1 shows pro-
posed translations for a single sentence along with
the bigram expectations needed to compute CoBP.
Equation 1 constitutes an objective function for
tuning the parameters of a machine translation
model. Figure 2 contrasts the properties of CoBP
and MaxBP as tuning objectives, using the simple
example from Figure 1.
Consensus bigram precision is an instance of a
general recipe for converting n-gram based eval-
uation metrics into consensus objective functions
for model tuning. For the remainder of this pa-
per, we focus on consensus BLEU. However, the
techniques herein, including the optimization ap-
proach of Section 3, are applicable to many differ-
entiable functions of expected n-gram counts.
1
By derivation, we mean a translation of a foreign sen-
tence along with any latent structure assumed by the model.
Each derivation corresponds to a particular English transla-
tion, but many derivations may yield the same translation.
1419
1.0 1.5 2.0 2.5 3.0
-16
-14
-12
-10
?
LM
Log M
odel 
Score
H
1
H
2
H
3
(a)
0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
?
LM
Valu
e of O
bject
ive
CoBP
MaxBP
H
1
H
3
H
1
H
2
H
3
(b)
Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensus
bigram precision (CoBP) on the simple example from Figure 1. (a) MaxBP is only sensitive to the convex hull (the
solid line) of model scores. When varying the single parameter ?
LM
, it entirely disregards the correct translation
H
2
becauseH
2
never attains a maximal model score. (b) A plot of both objectives shows their differing characteris-
tics. The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would
select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP
is only sensitive to the single point of discontinuity between H
1
and H
3
, and disregards H
2
entirely. CoBP peaks
when the distribution most heavily favorsH
2
while suppressingH
1
. ThoughH
2
never has a maximal model score,
if ?
LM
is in the indicated range, consensus decoding would select H
2
, the desired translation.
2.2 CoBLEU
The logarithm of the single-reference
2
BLEU met-
ric (Papineni et al, 2002) has the following form:
ln BLEU(R,E) =
(
1?
|R|
?
m
i=1
?
g
1
c(g
1
, e
i
)
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{c(g
n
, e
i
), c(g
n
, r
i
)}
?
m
i=1
?
g
n
c(g
n
, e
i
)
Above, |R| denotes the number of words in the
reference corpus. The notation (?)
?
is shorthand
for min(?, 0). In the inner sums, g
n
iterates over
all n-grams of order n. In order to adapt BLEU
to be a consensus tuning objective, we follow the
recipe of Section 2.1: we replace n-gram counts
from a candidate translation with expected n-gram
counts under the model.
CoBLEU(R,F, ?)=
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
The brevity penalty term in BLEU is calculated
using the expected length of the corpus, which
2
Throughout this paper, we use only a single reference,
but our objective readily extends to multiple references.
equals the sum of all expected unigram counts.
We call this objective function consensus BLEU,
or CoBLEU for short.
3 Optimizing CoBLEU
Unlike the more common MaxBLEU tuning ob-
jective optimized by MERT, CoBLEU is con-
tinuous. For distributions P
?
(d|f
i
) that factor
over synchronous grammar rules and n-grams, we
show below that it is also analytically differen-
tiable, permitting a straightforward gradient ascent
optimization procedure.
3
In order to perform gra-
dient ascent, we require methods for efficiently
computing the gradient of the objective function
for a given parameter setting ?. Once we have the
gradient, we can perform an update at iteration t
of the form
?
(t+1)
? ?
(t)
+ ?
t
?
?
CoBLEU(R,F, ?
(t)
)
where ?
t
is an adaptive step size.
4
3
Technically, CoBLEU is non-differentiable at some
points because of clipping. At these points, we must com-
pute a sub-gradient, and so our optimization is formally sub-
gradient ascent. See the Appendix for details.
4
After each successful step, we grow the step size by a
constant factor. Whenever the objective does not decrease
after a step, we shrink the step size by a constant factor and
try again until a decrease is attained.
1420
head(h)
tail(h)
u=
Once
S
rhyme
v
1
=
Once
RB
Once
v
2
=
upon
IN
upon
v
3
=
a
NP
rhyme
c(?Once upon?, h)
c(?upon a?, h)
= 1
= 1
!
2
(h) = 2
Figure 3: A hyperedge h represents a ?rule? used in
syntactic machine translation. tail(h) refers to the ?chil-
dren? of the rule, while head(h) refers to the ?head? or
?parent?. A forest of translations is built by combining
the nodes v
i
using h to form a new node u = head(h).
Each forest node consists of a grammar symbol and tar-
get language boundary words used to track n-grams. In
the above, we keep one boundary word for each node,
which allows us to track bigrams.
In this section, we develop an analytical expres-
sion for the gradient of CoBLEU, then discuss
how to efficiently compute the value of the objec-
tive function and gradient.
3.1 Translation Model Form
We first assume the general hypergraph setting of
Huang and Chiang (2007), namely, that deriva-
tions under our translation model form a hyper-
graph. This framework allows us to speak about
both phrase-based and syntax-based translation in
a unified framework.
We define a probability distribution over deriva-
tions d via ? as:
P
?
(d|f
i
) =
w(d)
Z(f
i
)
with
Z(f
i
) =
?
d
?
w(d
?
)
where w(d) = exp(?
>
?(d, f
i
)) is the weight of a
derivation and ?(d, f
i
) is a featurized representa-
tion of the derivation d of f
i
. We further assume
that these features decompose over hyperedges in
the hypergraph, like the one in Figure 3. That is,
?(d, f
i
) =
?
h?d
?(h, f
i
).
In this setting, we can analytically compute the
gradient of CoBLEU. We provide a sketch of the
derivation of this gradient in the Appendix. In
computing this gradient, we must calculate the fol-
lowing expectations:
E
?
[c(?
k
, d)|f
i
] (2)
E
?
[`
n
(d)|f
i
] (3)
E
?
[c(?
k
, d) ? `
n
(d)|f
i
] (4)
where `
n
(d) =
?
g
n
c(g
n
, d) is the sum of all n-
grams on derivation d (its ?length?). The first ex-
pectation is an expected count of the kth feature
?
k
over all derivations of f
i
. The second is an ex-
pected length, the total expected count of all n-
grams in derivations of f
i
. We call the final ex-
pectation an expected product of counts. We now
present the computation of each of these expecta-
tions in turn.
3.2 Computing Feature Expectations
The expected feature counts E
?
[c(?
k
, d)|f
i
] can be
written as
E
?
[c(?
k
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(?
k
, d)
=
?
h
P
?
(h|f
i
)c(?
k
, h)
We can justify the second step since fea-
ture counts are local to hyperedges, i.e.
c(?
k
, d) =
?
h?d
c(?
k
, h). The posterior
probability P
?
(h|f
i
) can be efficiently computed
with inside-outside scores. Let I(u) and O(u) be
the standard inside and outside scores for a node
u in the forest.
5
P
?
(h|f
i
) =
1
Z(f)
w(h) O(head(h))
?
v?tail(h)
I(v)
where w(h) is the weight of hyperedge h, given
by exp(?
>
?(h)), and Z(f) = I(root) is the in-
side score of the root of the forest. Computing
these inside-outside quantities takes time linear in
the number of hyperedges in the forest.
3.3 Computing n-gram Expectations
We can compute the expectations of any specific
n-grams, or of total n-gram counts `, in the same
way as feature expectations, provided that target-
side n-grams are also localized to hyperedges (e.g.
consider ` to be a feature of a hyperedge whose
value is the number of n-grams on h). If the
nodes in our forests are annotated with target-side
5
Appendix Figure 7 gives recursions for I(u) and O(u).
1421
boundary words as in Figure 3, then this will be the
case. Note that this is the same approach used by
decoders which integrate a target language model
(e.g. Chiang (2007)). Other work has computed
n-gram expectations in the same way (DeNero et
al., 2009; Li et al, 2009).
3.4 Computing Expectations of Products of
Counts
While the previous two expectations can be com-
puted using techniques known in the literature, the
expected product of counts E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
is a novel quantity. Fortunately, an efficient dy-
namic program exists for computing this expec-
tation as well. We present this dynamic program
here as one of the contributions of this paper,
though we omit a full derivation due to space re-
strictions.
To see why this expectation cannot be computed
in the same way as the expected feature or n-gram
counts, we expand the definition of the expectation
above to get
?
d
P
?
(d|f
i
) [c(?
k
, d)`
n
(d)]
Unlike feature and n-gram counts, the product of
counts in brackets above does not decompose over
hyperedges, at least not in an obvious way. We
can, however, still decompose the feature counts
c(?
k
, d) over hyperedges. After this decomposi-
tion and a little re-arranging, we get
=
?
h
c(?
k
, h)
?
d:h?d
P
?
(d|f
i
)`
n
(d)
=
1
Z(f
i
)
?
h
c(?
k
, h)
[
?
d:h?d
w(d)`
n
(d)
]
=
1
Z(f
i
)
?
h
c(?
k
, h)
?
D
n
?
(h|f
i
)
The quantity
?
D
n
?
(h|f
i
) =
?
d:h?d
w(d)`
n
(d) is the
sum of the weight-length products of all deriva-
tions d containing hyperedge h. In the same
way that P
?
(h|f
i
) can be efficiently computed
from inside and outside probabilities, this quan-
tity
?
D
n
?
(h|f
i
) can be efficiently computed with two
new inside and outside quantities, which we call
?
I
n
(u) and
?
O
n
(u). We provide recursions for these
quantities in Figure 4. Like the standard inside and
outside computations, these recursions run in time
linear in the number of hyperedges in the forest.
While a full exposition of the algorithm is not
possible in the available space, we give some brief
intuition behind this dynamic program. We first
define
?
I
n
(u):
?
I
n
(u) =
?
d
u
w(d
u
)`
n
(d)
where d
u
is a derivation rooted at node u. This is
a sum of weight-length products similar to
?
D. To
give a recurrence for
?
I, we rewrite it:
?
I
n
(u) =
?
d
u
?
h?d
u
[w(d
u
)`
n
(h)]
Here, we have broken up the total value of `
n
(d)
across hyperedges in d. The bracketed quantity
is a score of a marked derivation pair (d, h) where
the edge h is some specific element of d. The score
of a marked derivation includes the weight of the
derivation and the factor `
n
(h) for the marked hy-
peredge.
This sum over marked derivations gives the in-
side recurrence in Figure 4 by the following de-
composition. For
?
I
n
(u) to sum over all marked
derivation pairs rooted at u, we must consider two
cases. First, the marked hyperedge could be at the
root, in which case we must choose child deriva-
tions from regular inside scores and multiply in the
local `
n
, giving the first summand of
?
I
n
(u). Alter-
natively, the marked hyperedge is in exactly one
of the children; for each possibility we recursively
choose a marked derivation for one child, while
the other children choose regular derivations. The
second summand of
?
I
n
(u) compactly expresses
a sum over instances of this case.
?
O
n
(u) de-
composes similarly: the marked hyperedge could
be local (first summand), under a sibling (second
summand), or higher in the tree (third summand).
Once we have these new inside-outside quanti-
ties, we can compute
?
D as in Figure 5. This com-
bination states that marked derivations containing
h are either marked at h, below h, or above h.
As a final detail, computing the gradient
?C
clip
n
(?) (see the Appendix) involves a clipped
version of the expected product of counts, for
which a clipped
?
D is required. This quantity can
be computed with the same dynamic program with
a slight modification. In Figure 4, we show the dif-
ference as a choice point when computing `
n
(h).
3.5 Implementation Details
As stated, the runtime of computing the required
expectations for the objective and gradient is lin-
ear in the number of hyperedges in the forest. The
1422
?I
n
(u) =
?
h?IN(u)
w(h)
?
?
`
n
(h)
?
v?tail(h)
I(v) +
?
v?tail(h)
?
I
n
(v)
?
w 6=v
I(w)
?
?
?
O
n
(u) =
?
h?OUT(u)
w(h)
?
?
?
?
?
?
`
n
(h) O(head(h))
?
v?tail(h)
v 6=u
I(v) + O(head(h))
?
v?tail(h)
v 6=u
?
I
n
(v)
?
w?tail(h)
w 6=v
w 6=u
I(w) +
?
O
n
(head(h))
?
w?tail(h)
w 6=u
I(w)
?
?
?
?
?
?
`
n
(h) =
{
?
g
n
c(g
n
, h) computing unclipped counts
?
g
n
c(g
n
, h)1 [E
?
[c(g
n
, d)] ? c(g
n
, r
i
)] computing clipped counts
Figure 4: Inside and Outside recursions for
?
I
n
(u) and
?
O
n
(u). IN(u) and OUT(u) refer to the incoming and
outgoing hyperedges of u, respectively. I(?) and O(?) refer to standard inside and outside quantities, defined in
Appendix Figure 7. We initialize with
?
I
n
(u) = 0 for all terminal forest nodes u and
?
O
n
(root) = 0 for the root
node. `
n
(h) computes the sum of all n-grams of order n on a hyperedge h.
?
D
n
?
(h|f
i
) =
w(h)
?
?
?
?
`
n
(h)O(head(h))
?
v?tail(h)
I(v) + O(head(h))
?
v?tail(h)
?
I
n
(v)
?
v?tail(h)
w 6=v
I(w) +
?
O
n
(head(h))
?
w?tail(h)
I(w)
?
?
?
?
Figure 5: Calculation of
?
D
n
?
(h|f
i
) after
?
I
n
(u) and
?
O
n
(u) have been computed.
number of hyperedges is very large, however, be-
cause we must track n-gram contexts in the nodes,
just as we would in an integrated language model
decoder. These contexts are required both to cor-
rectly compute the model score of derivations and
to compute clipped n-gram counts. To speed our
computations, we use the cube pruning method of
Huang and Chiang (2007) with a fixed beam size.
For regularization, we added an L
2
penalty on
the size of ? to the CoBLEU objective, a simple
addition for gradient ascent. We did not find that
our performance varied very much for moderate
levels of regularization.
3.6 Related Work
The calculation of expected counts can be for-
mulated using the expectation semiring frame-
work of Eisner (2002), though that work does
not show how to compute expected products of
counts which are needed for our gradient calcu-
lations. Concurrently with this work, Li and Eis-
ner (2009) have generalized Eisner (2002) to com-
pute expected products of counts on translation
forests. The training algorithm of Kakade et al
(2002) makes use of a dynamic program similar to
ours, though specialized to the case of sequence
models.
4 Consensus Decoding
Once model parameters ? are learned, we must
select an appropriate decoding objective. Sev-
eral new decoding approaches have been proposed
recently that leverage some notion of consensus
over the many weighted derivations in a transla-
tion forest. In this paper, we adopt the fast consen-
sus decoding procedure of DeNero et al (2009),
which directly complements CoBLEU tuning. For
a source sentence f , we first build a translation
forest, then compute the expected count of each
n-gram in the translation of f under the model.
We extract a k-best list from the forest, then select
the translation that yields the highest BLEU score
relative to the forest?s expected n-gram counts.
Specifically, let BLEU(e; r) compute the simi-
larity of a sentence e to a reference r based on
the n-gram counts of each. When training with
CoBLEU, we replace e with expected counts and
maximize ?. In consensus decoding, we replace r
with expected counts and maximize e.
Several other efficient consensus decoding pro-
1423
cedures would similarly benefit from a tuning pro-
cedure that aggregates over derivations. For in-
stance, Blunsom and Osborne (2008) select the
translation sentence with highest posterior proba-
bility under the model, summing over derivations.
Li et al (2009) propose a variational approxima-
tion maximizing sentence probability that decom-
poses over n-grams. Tromble et al (2008) min-
imize risk under a loss function based on the lin-
ear Taylor approximation to BLEU, which decom-
poses over n-gram posterior probabilities.
5 Experiments
We compared CoBLEU training with an imple-
mentation of minimum error rate training on two
language pairs.
5.1 Model
Our optimization procedure is in principle
tractable for any syntactic translation system. For
simplicity, we evaluate the objective using an In-
version Transduction Grammar (ITG) (Wu, 1997)
that emits phrases as terminal productions, as in
(Cherry and Lin, 2007). Phrasal ITG models have
been shown to perform comparably to the state-of-
the art phrase-based system Moses (Koehn et al,
2007) when using the same phrase table (Petrov et
al., 2008).
We extract a phrase table using the Moses
pipeline, based on Model 4 word alignments gen-
erated from GIZA++ (Och and Ney, 2003). Our fi-
nal ITG grammar includes the five standard Moses
features, an n-gram language model, a length fea-
ture that counts the number of target words, a fea-
ture that counts the number of monotonic ITG
rewrites, and a feature that counts the number of
inverted ITG rewrites.
5.2 Data
We extracted phrase tables from the Spanish-
English and French-English sections of the Eu-
roparl corpus, which include approximately 8.5
million words of bitext for each of the language
pairs (Koehn, 2002). We used a trigram lan-
guage model trained on the entire corpus of En-
glish parliamentary proceedings provided with the
Europarl distribution and generated according to
the ACL 2008 SMT shared task specifications.
6
For tuning, we used all sentences from the 2007
SMT shared task up to length 25 (880 sentences
6
See http://www.statmt.org/wmt08 for details.
2 4 6 8 10
0.00
.20.
40.6
0.81
.0
Iterations
Fractio
n of Va
lue at C
onverge
nce
CoBLEU
MERT
Figure 6: Trajectories of MERT and CoBLEU dur-
ing optimization show that MERT is initially unstable,
while CoBLEU training follows a smooth path to con-
vergence. Because these two training procedures op-
timize different functions, we have normalized each
trajectory by the final objective value at convergence.
Therefore, the absolute values of this plot do not re-
flect the performance of either objective, but rather
the smoothness with which the final objective is ap-
proached. The rates of convergence shown in this plot
are not directly comparable. Each iteration for MERT
above includes 10 iterations of coordinate ascent, fol-
lowed by a decoding pass through the training set. Each
iteration of CoBLEU training involves only one gradi-
ent step.
for Spanish and 923 for French), and we tested on
the subset of the first 1000 development set sen-
tences which had length at most 25 words (447
sentences for Spanish and 512 for French).
5.3 Tuning Optimization
We compared two techniques for tuning the nine
log-linear model parameters of our ITG grammar.
We maximized CoBLEU using gradient ascent, as
described above. As a baseline, we maximized
BLEU of the Viterbi translation derivations using
minimum error rate training. To improve opti-
mization stability, MERT used a cumulative k-best
list that included all translations generated during
the tuning process.
One of the benefits of CoBLEU training is that
we compute expectations efficiently over an entire
forest of translations. This has substantial stabil-
ity benefits over methods based on k-best lists. In
Figure 6, we show the progress of CoBLEU as
compared to MERT. Both models are initialized
from 0 and use the same features. This plot ex-
hibits a known issue with MERT training: because
new k-best lists are generated at each iteration,
the objective function can change drastically be-
tween iterations. In contrast, CoBLEU converges
1424
Consensus Decoding
Spanish
Tune Test ? Br.
MERT 32.5 30.2 -2.3 0.992
CoBLEU 31.4 30.4 -1.0 0.992
MERT?CoBLEU 31.7 30.8 -0.9 0.992
French
Tune Test ? Br.
MERT 32.5 31.1* -1.4 0.972
CoBLEU 31.9 30.9 -1.0 0.954
MERT?CoBLEU 32.4 31.2* -0.8 0.953
Table 1: Performance measured by BLEU using a con-
sensus decoding method over translation forests shows
an improvement over MERT when using CoBLEU
training. The first two conditions were initialized by
0 vectors. The third condition was initialized by the
final parameters of MERT training. Br. indicates the
brevity penalty on the test set. The * indicates differ-
ences which are not statistically significant.
smoothly to its final objective because the forests
do not change substantially between iterations, de-
spite the pruning needed to track n-grams. Similar
stability benefits have been observed for lattice-
based MERT (Macherey et al, 2008).
5.4 Results
We performed experiments from both French and
Spanish into English under three conditions. In the
first two, we initialized both MERT and CoBLEU
training uniformly with zero weights and trained
until convergence. In the third condition, we ini-
tialized CoBLEU with the final parameters from
MERT training, denoted MERT?CoBLEU in the
results tables. We evaluated each of these condi-
tions on both the tuning and test sets using the con-
sensus decoding method of DeNero et al (2009).
The results appear in Table 1.
In Spanish-English, CoBLEU slightly outper-
formed MERT under the same initialization, while
the opposite pattern appears for French-English.
The best test set performance in both language
pairs was the third condition, in which CoBLEU
training was initialized with MERT. This con-
dition also gave the highest CoBLEU objective
value. This pattern indicates that CoBLEU is a
useful objective for translation with consensus de-
coding, but that the gradient ascent optimization is
getting stuck in local maxima during tuning. This
issue can likely be addressed with annealing, as
described in (Smith and Eisner, 2006).
Interestingly, the brevity penatly results in
French indicate that, even though CoBLEU did
Viterbi Decoding
Spanish
Tune Test ?
MERT 32.5 30.2 -2.3
MERT?CoBLEU 30.5 30.9 +0.4
French
Tune Test ?
MERT 32.0 31.0 -1.0
MERT?CoBLEU 31.7 30.9 -0.8
Table 2: Performance measured by BLEU using Viterbi
decoding indicates that CoBLEU is less prone to over-
fitting than MERT.
not outperform MERT in a statistically significant
way, CoBLEU tends to find shorter sentences with
higher n-gram precision than MERT.
Table 1 displays a second benefit of CoBLEU
training: compared to MERT training, CoBLEU
performance degrades less from tuning to test
set. In Spanish, initializing with MERT-trained
weights and then training with CoBLEU actually
decreases BLEU on the tuning set by 0.8 points.
However, this drop in tuning performance comes
with a corresponding increase of 0.6 on the test
set, relative to MERT training. We see the same
pattern in French, albeit to a smaller degree.
While CoBLEU ought to outperform MERT us-
ing consensus decoding, we expected that MERT
would give better performance under Viterbi de-
coding. Surprisingly, we found that CoBLEU
training actually outperformed MERT in Spanish-
English and performed equally well in French-
English. Table 2 shows the results. In these ex-
periments, we again see that CoBLEU overfit the
training set to a lesser degree than MERT, as evi-
denced by a smaller drop in performance from tun-
ing to test set. In fact, test set performance actually
improved for Spanish-English CoBLEU training
while dropping by 2.3 BLEU for MERT.
6 Conclusion
CoBLEU takes a fundamental quantity used in
consensus decoding, expected n-grams, and trains
to optimize a function of those expectations.
While CoBLEU can therefore be expected to in-
crease test set BLEU under consensus decoding, it
is more surprising that it seems to better regularize
learning even for the Viterbi decoding condition.
It is also worth emphasizing that the CoBLEU ap-
proach is applicable to functions of expected n-
gram counts other than BLEU.
1425
Appendix: The Gradient of CoBLEU
We would like to compute the gradient of
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
To simplify notation, we introduce the functions
C
n
(?) =
m
?
i=1
?
g
n
E
?
[c(g
n
, e)|f
i
]
C
clip
n
(?) =
m
?
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(r, g
n
)}
C
n
(?) represents the sum of the expected counts
of all n-grams or order n in all translations of
the source corpus F , while C
clip
n
(?) represents the
sum of the same expected counts, but clipped with
reference counts c(g
n
, r
i
).
With this notation, we can write our objective
function CoBLEU(R,F, ?) in three terms:
(
1?
|R|
C
1
(?)
)
?
+
1
4
4
?
n=1
lnC
clip
n
(?)?
1
4
4
?
n=1
lnC
n
(?)
We first state an identity:
?
g
n
?
??
k
E
?
[c(g
n
, d)|f
i
] =
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
?E
?
[`
n
(d)|f
i
] ? E
?
[c(?
k
, d)|f
i
]
which can be derived by expanding the expectation on
the left-hand side
?
g
n
?
d
?
??
k
P
?
(d|f
i
)c(g
n
, d)
and substituting
?
??
k
P
?
(d|f
i
) =
P
?
(d|f
i
)c(?
k
, d)? P
?
(d|f
i
)
?
d
?
P
?
(d
?
|f
i
)c(?
k
, d
?
)
Using this identity and some basic calculus, the
gradient?C
n
(?) is
m
?
i=1
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]? C
n
(?)E
?
[c(?
k
, d)|f
i
]
I(u) =
?
h?IN(u)
w(h)
?
?
?
v?tail(h)
I(v)
?
?
O(u) =
?
h?OUT (u)
w(h)
?
?
?
?
O(head(h))
?
v?tail(h)
v 6=u
I(v)
?
?
?
?
Figure 7: Standard Inside-Outside recursions which
compute I(u) and O(u). IN(u) and OUT(u) refer to the
incoming and outgoing hyperedges of u, respectively.
We initialize with I(u) = 1 for all terminal forest nodes
u and O(root) = 1 for the root node. These quantities
are referenced in Figure 4.
and the gradient?C
clip
n
(?) is given by
m
?
i=1
?
g
n
[
E
?
[c(g
n
, d) ? c(?
k
, d)|f
i
]
?1
[
E
?
[c(g
n
, d)|f
i
] ? c(g
n
, r
i
)
]
]
?C
clip
n
(?)E
?
[c(?
k
, d) + f
i
]
where 1 denotes an indicator function. At the top
level, the gradient of the first term (the brevity
penalty) is
|R|?C
1
(?)
C
1
(?)
2
1
[
C
1
(?) ? |R|
]
The gradient of the second term is
1
4
4
?
n=1
?C
clip
n
(?)
C
clip
n
(?)
and the gradient of the third term is
?
1
4
4
?
n=1
?C
n
(?)
C
n
(?)
Note that, because of the indicator func-
tions, CoBLEU is non-differentiable when
E
?
[c(g
n
, d)|f
i
] = c(g
n
, r
i
) or C
n
(?) = |R|.
Formally, we must compute a sub-gradient at
these points. In practice, we can choose between
the gradients calculated assuming the indicator
function is 0 or 1; we always choose the latter.
1426
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings
of the Conference on Emprical Methods for Natural
Language Processing.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In The Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics Workshop on Syntax and Structure in
Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In The Conference on Em-
pirical Methods in Natural Language Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
The Annual Conference of the Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In The Annual Conference of the Association for
Computational Linguistics.
Sham Kakade, Yee Whye Teh, and Sam T. Roweis.
2002. An alternate objective function for markovian
fields. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
The Annual Conference of the Association for Com-
putational Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In The Annual
Conference of the Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In The Annual Conference of the Association
for Computational Linguistics.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In The Annual
Conference of the Association for Computational
Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 108?116, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
David Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In In Pro-
ceedings of the Association for Computational Lin-
guistics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
1427
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227?235,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing for Transducer Grammars
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, mbansal, adpauls, klein}@cs.berkeley.edu
Abstract
The tree-transducer grammars that arise in
current syntactic machine translation systems
are large, flat, and highly lexicalized. We ad-
dress the problem of parsing efficiently with
such grammars in three ways. First, we
present a pair of grammar transformations
that admit an efficient cubic-time CKY-style
parsing algorithm despite leaving most of the
grammar in n-ary form. Second, we show
how the number of intermediate symbols gen-
erated by this transformation can be substan-
tially reduced through binarization choices.
Finally, we describe a two-pass coarse-to-fine
parsing approach that prunes the search space
using predictions from a subset of the origi-
nal grammar. In all, parsing time reduces by
81%. We also describe a coarse-to-fine prun-
ing scheme for forest-based language model
reranking that allows a 100-fold increase in
beam size while reducing decoding time. The
resulting translations improve by 1.3 BLEU.
1 Introduction
Current approaches to syntactic machine translation
typically include two statistical models: a syntac-
tic transfer model and an n-gram language model.
Recent innovations have greatly improved the effi-
ciency of language model integration through multi-
pass techniques, such as forest reranking (Huang
and Chiang, 2007), local search (Venugopal et al,
2007), and coarse-to-fine pruning (Petrov et al,
2008; Zhang and Gildea, 2008). Meanwhile, trans-
lation grammars have grown in complexity from
simple inversion transduction grammars (Wu, 1997)
to general tree-to-string transducers (Galley et al,
2004) and have increased in size by including more
synchronous tree fragments (Galley et al, 2006;
Marcu et al, 2006; DeNeefe et al, 2007). As a result
of these trends, the syntactic component of machine
translation decoding can now account for a substan-
tial portion of total decoding time. In this paper,
we focus on efficient methods for parsing with very
large tree-to-string grammars, which have flat n-ary
rules with many adjacent non-terminals, as in Fig-
ure 1. These grammars are sufficiently complex that
the purely syntactic pass of our multi-pass decoder is
the compute-time bottleneck under some conditions.
Given that parsing is well-studied in the mono-
lingual case, it is worth asking why MT grammars
are not simply like those used for syntactic analy-
sis. There are several good reasons. The most im-
portant is that MT grammars must do both analysis
and generation. To generate, it is natural to mem-
orize larger lexical chunks, and so rules are highly
lexicalized. Second, syntax diverges between lan-
guages, and each divergence expands the minimal
domain of translation rules, so rules are large and
flat. Finally, we see most rules very few times, so
it is challenging to subcategorize non-terminals to
the degree done in analytic parsing. This paper de-
velops encodings, algorithms, and pruning strategies
for such grammars.
We first investigate the qualitative properties of
MT grammars, then present a sequence of parsing
methods adapted to their broad characteristics. We
give normal forms which are more appropriate than
Chomsky normal form, leaving the rules mostly flat.
We then describe a CKY-like algorithm which ap-
plies such rules efficiently, working directly over the
n-ary forms in cubic time. We show how thoughtful
227
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical normal form (LNF) transformation
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF transformation
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
NP ? DT+NN NNS NP ? DT NN+NNSor
Type-minimizing binarization
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT,NN
DT,NN,NNS 
Minimal binary rules for LNF
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
Figure 1: (a) A synchronous transducer rule has co-
indexed non-terminals on the source and target side. In-
ternal grammatical structure of the target side has been
omitted. (b) The source-side projection of the rule is a
monolingual source-language rule with target-side gram-
mar symbols. (c) A training sentence pair is annotated
with a target-side parse tree and a word alignment, which
license this rule to be extracted.
binarization can further increase parsing speed, and
we present a new coarse-to-fine scheme that uses
rule subsets rather than symbol clustering to build
a coarse grammar projection. These techniques re-
duce parsing time by 81% in aggregate. Finally,
we demonstrate that we can accelerate forest-based
reranking with a language model by pruning with
information from the parsing pass. This approach
enables a 100-fold increase in maximum beam size,
improving translation quality by 1.3 BLEU while
decreasing total decoding time.
2 Tree Transducer Grammars
Tree-to-string transducer grammars consist of
weighted rules like the one depicted in Figure 1.
Each n-ary rule consists of a root symbol, a se-
quence of lexical items and non-terminals on the
source-side, and a fragment of a syntax tree on
the target side. Each non-terminal on the source
side corresponds to a unique one on the target side.
Aligned non-terminals share a grammar symbol de-
rived from a target-side monolingual grammar.
These grammars are learned from word-aligned
sentence pairs annotated with target-side phrase
structure trees. Extraction proceeds by using word
alignments to find correspondences between target-
side constituents and source-side word spans, then
discovering transducer rules that match these con-
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
Figure 2: Transducer grammars are composed of very flat
rules. Above, the histogram shows rule counts for each
rule size among the 332,000 rules that apply to an indi-
vidual 30-word sentence. The size of a rule is the total
number of non-terminals and lexical items in its source-
side yield.
stituent alignments (Galley et al, 2004). Given this
correspondence, an array of extraction procedures
yields rules that are well-suited to machine trans-
lation (Galley et al, 2006; DeNeefe et al, 2007;
Marcu et al, 2006). Rule weights are estimated
by discriminatively combining relative frequency
counts and other rule features.
A transducer grammarG can be projected onto its
source language, inducing a monolingual grammar.
If we weight each rule by the maximumweight of its
projecting synchronous rules, then parsing with this
projected grammar maximizes the translation model
score for a source sentence. We need not even con-
sider the target side of transducer rules until integrat-
ing an n-gram language model or other non-local
features of the target language.
We conduct experiments with a grammar ex-
tracted from 220 million words of Arabic-English
bitext, extracting rules with up to 6 non-terminals. A
histogram of the size of rules applicable to a typical
30-word sentence appears in Figure 2. The grammar
includes 149 grammatical symbols, an augmentation
of the Penn Treebank symbol set. To evaluate, we
decoded 300 sentences of up to 40 words in length
from the NIST05 Arabic-English test set.
3 Efficient Grammar Encodings
Monolingual parsing with a source-projected trans-
ducer grammar is a natural first pass in multi-pass
decoding. These grammars are qualitatively dif-
ferent from syntactic analysis grammars, such as
the lexicalized grammars of Charniak (1997) or the
heavily state-split grammars of Petrov et al (2006).
228
In this section, we develop an appropriate grammar
encoding that enables efficient parsing.
It is problematic to convert these grammars into
Chomsky normal form, which CKY requires. Be-
cause transducer rules are very flat and contain spe-
cific lexical items, binarization introduces a large
number of intermediate grammar symbols. Rule size
and lexicalization affect parsing complexity whether
the grammar is binarized explicitly (Zhang et al,
2006) or implicitly binarized using Early-style inter-
mediate symbols (Zollmann et al, 2006). Moreover,
the resulting binary rules cannot be Markovized to
merge symbols, as in Klein andManning (2003), be-
cause each rule is associated with a target-side tree
that cannot be abstracted.
We also do not restrict the form of rules in the
grammar, a common technique in syntactic machine
translation. For instance, Zollmann et al (2006)
follow Chiang (2005) in disallowing adjacent non-
terminals. Watanabe et al (2006) limit grammars
to Griebach-Normal form. However, general tree
transducer grammars provide excellent translation
performance (Galley et al, 2006), and so we focus
on parsing with all available rules.
3.1 Lexical Normal Form
Sequences of consecutive non-terminals complicate
parsing because they require a search over non-
terminal boundaries when applied to a sentence
span. We transform the grammar to ensure that all
rules containing lexical items (lexical rules) do not
contain sequences of non-terminals. We allow both
unary and binary non-lexical rules.
Let L be the set of lexical items and V the set
of non-terminal symbols in the original grammar.
Then, lexical normal form (LNF) limits productions
to two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? (X1)?(X2)
? = w+(Xiw+)?
Above, all Xi ? V and w+ ? L+. Symbols in
parentheses are optional. The nucleus ? of lexical
rules is a mixed sequence that has lexical items on
each end and no adjacent non-terminals.
Converting a grammar into LNF requires two
steps. In the sequence elimination step, for every
NNP
1
 no d ba una bofetada  DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verd
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
LNF replaces non-terminal sequences in lexical rules
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Non-lexical rules before binarization:
Equivalent binary rules, minimizing symbol count:
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
DT+NN ? DT NN
NP ? DT NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
Figure 3: We transform the original grammar by first
eliminating non-terminal sequences in lexical rules.
Next, we binarize, adding a minimal number of inter-
mediate grammar symbols and binary non-lexical rules.
Finally, anchored LNF further transforms lexical rules
to begin and end with lexical items by introducing ad-
ditional symbols.
lexical rule we replace each sequence of consecutive
non-terminalsX1 . . . Xn with the intermediate sym-
bol X1+. . .+Xn (abbreviated X1:n) and introduce a
non-lexical rule X1+. . .+Xn ? X1 . . . Xn. In the
binarization step, we introduce further intermediate
symbols and rules to binarize all non-lexical rules
in the grammar, including those added by sequence
elimination.
3.2 Non-terminal Binarization
Exactly howwe binarize non-lexical rules affects the
total number of intermediate symbols introduced by
the LNF transformation.
Binarization involves selecting a set of symbols
that will allow us to assemble the right-hand side
X1 . . . Xn of every non-lexical rule using binary
productions. This symbol set must at least include
the left-hand side of every rule in the grammar
(lexical and non-lexical), including the intermediate
229
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+ Figure 4: The number of non-terminal symbols intro-
duced to the grammar through LNF binarization depends
upon the policy for binarizing type sequences. This ex-
periment shows results from transforming a grammar that
has already been filtered for a particular short sentence.
Both the greedy and optimal binarizations use far fewer
symbols than naive binarizations.
symbols X1:n introduced by sequence elimination.
To ensure that a symbol sequence X1 . . . Xn can
be constructed, we select a split point k and add in-
termediate types X1:k and Xk+1:n to the grammar.
We must also ensure that the sequences X1 . . . Xk
and Xk+1 . . . Xn can be constructed. As baselines,
we used left-branching (where k = 1 always) and
right-branching (where k = n? 1) binarizations.
We also tested a greedy binarization approach,
choosing k to minimize the number of grammar
symbols introduced. We first try to select k such that
both X1:k and Xk+1:n are already in the grammar.
If no such k exists, we select k such that one of the
intermediate types generated is already used. If no
such k exists again, we choose k = ?12n
?. This pol-
icy only creates new intermediate types when nec-
essary. Song et al (2008) propose a similar greedy
approach to binarization that uses corpus statistics to
select common types rather than explicitly reusing
types that have already been introduced.
Finally, we computed an optimal binarization that
explicitly minimizes the number of symbols in the
resulting grammar. We cast the minimization as an
integer linear program (ILP). Let V be the set of
all base non-terminal symbols in the grammar. We
introduce an indicator variable TY for each symbol
Y ? V + to indicate that Y is used in the grammar.
Y can be either a base non-terminal symbol Xi or
an intermediate symbol X1:n. We also introduce in-
dicators AY,Z for each pairs of symbols, indicating
that both Y and Z are used in the grammar. Let
L ? V + be the set of left-hand side symbols for
all lexical and non-lexical rules already in the gram-
mar. Let R be the set of symbol sequences on the
right-hand side of all non-lexical rules. Then, the
ILP takes the form:
min ?
Y ?V +
TY (1)
s.t. TY = 1 ? Y ? L (2)
1 ??
k
AX1:k,Xk+1:n ? X1 . . . Xn ? R (3)
TX1:n ?
?
k
AX1:k,Xk+1:n ? X1:n (4)
AY,Z ? TY , AY,Z ? TZ ? Y, Z (5)
The solution to this ILP indicates which symbols
appear in a minimal binarization. Equation 1 explic-
itly minimizes the number of symbols. Equation 2
ensures that all symbols already in the grammar re-
main in the grammar.
Equation 3 does not require that a symbol repre-
sent the entire right-hand side of each non-lexical
rule, but does ensure that each right-hand side se-
quence can be built from two subsequence symbols.
Equation 4 ensures that any included intermediate
type can also be built from two subsequence types.
Finally, Equation 5 ensures that if a pair is used, each
member of the pair is included. This program can be
optimized with an off-the-shelf ILP solver.1
Figure 4 shows the number of intermediate gram-
mar symbols needed for the four binarization poli-
cies described above for a short sentence. Our ILP
solver could only find optimal solutions for very
short sentences (which have small grammars after
relativization). Because greedy requires very little
time to compute and generates symbol counts that
are close to optimal when both can be computed, we
use it for our remaining experiments.
3.3 Anchored Lexical Normal Form
We also consider a further grammar transformation,
anchored lexical normal form (ALNF), in which the
yield of lexical rules must begin and end with a lex-
ical item. As shown in the following section, ALNF
improves parsing performance over LNF by shifting
work from lexical rule applications to non-lexical
1We used lp solve: http://sourceforge.net/projects/lpsolve.
230
rule applications. ALNF consists of rules with the
following two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? w+(Xiw+)?
To convert a grammar into ALNF, we first transform
it into LNF, then introduce additional binary rules
that split off non-terminal symbols from the ends of
lexical rules, as shown in Figure 3.
4 Efficient CKY Parsing
We now describe a CKY-style parsing algorithm for
grammars in LNF. The dynamic program is orga-
nized into spans Sij and computes the Viterbi score
w(i, j,X) for each edge Sij [X], the weight of the
maximum parse over words i+1 to j, rooted at sym-
bol X . For each Sij , computation proceeds in three
phases: binary, lexical, and unary.
4.1 Applying Non-lexical Binary Rules
For a span Sij , we first apply the binary non-lexical
rules just as in standard CKY, computing an interme-
diate Viterbi score wb(i, j,X). Let ?r be the weight
of rule r. Then, wb(i, j,X) =
max
r=X?X1X2
?r
j?1max
k=i+1
w(i, k,X1) ? w(k, j,X2).
The quantitiesw(i, k,X1) andw(k, j,X2) will have
already been computed by the dynamic program.
The work in this phase is cubic in sentence length.
4.2 Applying Lexical Rules
On the other hand, lexical rules in LNF can be ap-
plied without binarization, because they only apply
to particular spans that contain the appropriate lexi-
cal items. For a given Sij , we first compute all the le-
gal mappings of each rule onto the span. A mapping
consists of a correspondence between non-terminals
in the rule and subspans of Sij . In practice, there
is typically only one way that a lexical rule in LNF
can map onto a span, because most lexical items will
appear only once in the span.
Let m be a legal mapping and r its corresponding
rule. Let S(i)k` [X] be the edge mapped to the ith non-terminal of r underm, and ?r the weight of r. Then,
wl(i, j,X) = maxm ?r
?
S(i)k` [X]
w(k, `,X).
Again, w(k, `,X) will have been computed by the
dynamic program. Assuming only a constant num-
ber of mappings per rule per span, the work in this
phase is quadratic. We can then merge wl and wb:
w(i, j,X) = max(wl(i, j,X), wb(i, j,X)).
To efficiently compute mappings, we store lexi-
cal rules in a trie (or suffix array) ? a searchable
graph that indexes rules according to their sequence
of lexical items and non-terminals. This data struc-
ture has been used similarly to index whole training
sentences for efficient retrieval (Lopez, 2007). To
find all rules that map onto a span, we traverse the
trie using depth-first search.
4.3 Applying Unary Rules
Unary non-lexical rules are applied after lexical
rules and non-lexical binary rules.
w(i, j,X) = max
r:r=X?X1
?rw(i, j,X1).
While this definition is recursive, we allow only one
unary rule application per symbol X at each span
to prevent infinite derivations. This choice does not
limit the generality of our algorithm: chains of unar-
ies can always be collapsed via a unary closure.
4.4 Bounding Split Points for Binary Rules
Non-lexical binary rules can in principle apply to
any span Sij where j ? i ? 2, using any split point
k such that i < k < j. In practice, however, many
rules cannot apply to many (i, k, j) triples because
the symbols for their children have not been con-
structed successfully over the subspans Sik and Skj .
Therefore, the precise looping order over rules and
split points can influence computation time.
We found the following nested looping order for
the binary phase of processing an edge Sij [X] gave
the fastest parsing times for these grammars:
1. Loop over symbols X1 for the left child
2. Loop over all rules X ? X1X2 containing X1
3. Loop over split points k : i < k < j
4. Update wb(i, j,X) as necessary
This looping order allows for early stopping via
additional bookkeeping in the algorithm. We track
the following statistics as we parse:
231
Grammar Bound checks Parsing time
LNF no 264
LNF yes 181
ALNF yes 104
Table 1: Adding bound checks to CKY and transforming
the grammar from LNF to anchored LNF reduce parsing
time by 61% for 300 sentences of length 40 or less. No
approximations have been applied, so all three scenarios
produce no search errors. Parsing time is in minutes.
minEND(i,X), maxEND(i,X): The minimum and
maximum position k for which symbol X was
successfully built over Sik.
minSTART(j,X), maxSTART(j,X): The minimum
and maximum position k for which symbol X
was successfully built over Skj .
We then bound k by mink and maxk in the inner
loop using these statistics. If ever mink > maxk,
then the loop is terminated early.
1. set mink = i+ 1,maxk = j ? 1
2. loop over symbols X1 for the left child
mink = max(mink,minEND(i,X1))
maxk = min(maxk,maxEND(i,X1))
3. loop over rules X ? X1X2
mink = max(mink,minSTART(j,X2))
maxk = min(maxk,maxSTART(j,X2))
4. loop over split points k : mink ? k ? maxk
5. update wb(i, j,X) as necessary
In this way, we eliminate unnecessary work by
avoiding split points that we know beforehand can-
not contribute to wb(i, j,X).
4.5 Parsing Time Results
Table 1 shows the decrease in parsing time from in-
cluding these bound checks, as well as switching
from lexical normal form to anchored LNF.
Using ALNF rather than LNF increases the num-
ber of grammar symbols and non-lexical binary
rules, but makes parsing more efficient in three
ways. First, it decreases the number of spans for
which a lexical rule has a legal mapping. In this way,
ALNF effectively shifts work from the lexical phase
to the binary phase. Second, ALNF reduces the time
spent searching the trie for mappings, because the
first transition into the trie must use an edge with a
lexical item. Finally, ALNF improves the frequency
that, when a lexical rule matches a span, we have
successfully built every edge Sk`[X] in the mapping
for that rule. This frequency increases from 45% to
96% with ALNF.
5 Coarse-to-Fine Search
We now consider two coarse-to-fine approximate
search procedures for parsing with these grammars.
Our first approach clusters grammar symbols to-
gether during the coarse parsing pass, following
work in analytic parsing (Charniak and Caraballo,
1998; Petrov and Klein, 2007). We collapse all
intermediate non-terminal grammar symbols (e.g.,
NP) to a single coarse symbol X, while pre-terminal
symbols (e.g., NN) are hand-clustered into 7 classes
(nouns, verbals, adjectives, punctuation, etc.). We
then project the rules of the original grammar into
this simplified symbol set, weighting each rule of
the coarse grammar by the maximum weight of any
rule that mapped onto it.
In our second and more successful approach, we
select a subset of grammar symbols. We then in-
clude only and all rules that can be built using those
symbols. Because the grammar includes many rules
that are compositions of smaller rules, parsing with
a subset of the grammar still provides meaningful
scores that can be used to prune base grammar sym-
bols while parsing under the full grammar.
5.1 Symbol Selection
To compress the grammar, we select a small sub-
set of symbols that allow us to retain as much of
the original grammar as possible. We use a voting
scheme to select the symbol subset. After conver-
sion to LNF (or ALNF), each lexical rule in the orig-
inal grammar votes for the symbols that are required
to build it. A rule votes as many times as it was ob-
served in the training data to promote frequent rules.
We then select the top nl symbols by vote count and
include them in the coarse grammar C.
We would also like to retain as many non-lexical
rules from the original grammar as possible, but the
right-hand side of each rule can be binarized in many
ways. We again use voting, but this time each non-
232
Pruning Minutes Model score BLEU
No pruning 104 60,179 44.84
Clustering 79 60,179 44.84
Subsets 50 60,163 44.82
Table 2: Coarse-to-fine pruning speeds up parsing time
with minimal effect on either model score or translation
quality. The coarse grammar built using symbol subsets
outperforms clustering grammar symbols, reducing pars-
ing time by 52%. These experiments do not include a
language model.
lexical rule votes for its yield, a sequence of sym-
bols. We select the top nu symbol sequences as the
set R of right-hand sides.
Finally, we augment the symbol set of C with in-
termediate symbols that can construct all sequences
in R, using only binary rules. This step again re-
quires choosing a binarization for each sequence,
such that a minimal number of additional symbols is
introduced. We use the greedy approach from Sec-
tion 3.2. We then include in C all rules from the
original grammar that can be built from the symbols
we have chosen. Surprisingly, we are able to re-
tain 76% of the grammar rules while excluding 92%
of the grammar symbols2, which speeds up parsing
substantially.
5.2 Max Marginal Thresholding
We parse first with the coarse grammar to find the
Viterbi derivation score for each edge Sij [X]. We
then perform a Viterbi outside pass over the chart,
like a standard outside pass but replacing ? with
max (Goodman, 1999). The product of an edge?s
Viterbi score and its Viterbi outside score gives a
max marginal, the score of the maximal parse that
uses the edge.
We then prune away regions of the chart that de-
viate in their coarse max marginal from the global
Viterbi score by a fixed margin tuned on a develop-
ment set. Table 2 shows that both methods of con-
structing a coarse grammar are effective in pruning,
but selecting symbol subsets outperformed the more
typical clustering approach, reducing parsing time
by an additional factor of 2.
2We used nl of 500 and nu of 4000 for experiments. These
parameters were tuned on a development set.
6 Language Model Integration
Large n-gram language models (LMs) are critical
to the performance of machine translation systems.
Recent innovations have managed the complexity
of LM integration using multi-pass architectures.
Zhang and Gildea (2008) describes a coarse-to-fine
approach that iteratively increases the order of the
LM. Petrov et al (2008) describes an additional
coarse-to-fine hierarchy over language projections.
Both of these approaches integrate LMs via bottom-
up dynamic programs that employ beam search. As
an alternative, Huang and Chiang (2007) describes a
forest-based reranking algorithm called cube grow-
ing, which also employs beam search, but focuses
computation only where necessary in a top-down
pass through a parse forest.
In this section, we show that the coarse-to-fine
idea of constraining each pass using marginal pre-
dictions of the previous pass also applies effectively
to cube growing. Max marginal predictions from the
parse can substantially reduce LM integration time.
6.1 Language Model Forest Reranking
Parsing produces a forest of derivations, where each
edge in the forest holds its Viterbi (or one-best)
derivation under the transducer grammar. In forest
reranking via cube growing, edges in the forest pro-
duce k-best lists of derivations that are scored by
both the grammar and an n-gram language model.
Using ALNF, each edge must first generate a k-best
list of derivations that are not scored by the language
model. These derivations are then flattened to re-
move the binarization introduced by ALNF, so that
the resulting derivations are each rooted by an n-
ary rule r from the original grammar. The leaves of
r correspond to sub-edges in the chart, which are
recursively queried for their best language-model-
scored derivations. These sub-derivations are com-
bined by r, and new n-grams at the edges of these
derivations are scored by the language model.
The language-model-scored derivations for the
edge are placed on a priority queue. The top of
the priority queue is repeatedly removed, and its
successors added back on to the queue, until k
language-model-scored derivations have been dis-
covered. These k derivations are then sorted and
233
Pruning Max TM LM Total Inside Outside LM Total
strategy beam BLEU score score score time time time time
No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346
CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239
CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241
CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253
Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220
million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.
Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar. Coarse-to-fine reranking used
max marginals to constrain the reranking pass. Coarse-to-fine parse + rerank employed both of these approximations.
supplied to parent edges upon request.3
6.2 Coarse-to-Fine Parsing
Even with this efficient reranking algorithm, inte-
grating a language model substantially increased de-
coding time and memory use. As a baseline, we
reranked using a small fixed-size beam of 20 deriva-
tions at each edge. Larger beams exceeded the mem-
ory of our hardware. Results appear in Table 3.
Coarse-to-fine parsing before LM integration sub-
stantially improved language model reranking time.
By pruning the chart with max marginals from the
coarse symbol subset grammar from Section 5, we
were able to rerank with beams of length 200, lead-
ing to a 0.8 BLEU increase and a 31% reduction in
total decoding time.
6.3 Coarse-to-Fine Forest Reranking
We realized similar performance and speed bene-
fits by instead pruning with max marginals from the
full grammar. We found that LM reranking explored
many edges with low max marginals, but used few
of them in the final decoder output. Following the
coarse-to-fine paradigm, we restricted the reranker
to edges with a max marginal above a fixed thresh-
old. Furthermore, we varied the beam size of each
edge based on the parse. Let ?m be the ratio of
the max marginal for edge m to the global Viterbi
derivation for the sentence. We used a beam of size?
k ? 2ln?m? for each edge.
Computing max marginals under the full gram-
mar required an additional outside pass over the full
parse forest, adding substantially to parsing time.
3Huang and Chiang (2007) describes the cube growing al-
gorithm in further detail, including the precise form of the suc-
cessor function for derivations.
However, soft coarse-to-fine pruning based on these
max marginals also allowed for beams up to length
200, yielding a 1.0 BLEU increase over the baseline
and a 30% reduction in total decoding time.
We also combined the coarse-to-fine parsing ap-
proach with this soft coarse-to-fine reranker. Tiling
these approximate search methods allowed another
10-fold increase in beam size, further improving
BLEU while only slightly increasing decoding time.
7 Conclusion
As translation grammars increase in complexity
while innovations drive down the computational cost
of language model integration, the efficiency of the
parsing phase of machine translation decoding is be-
coming increasingly important. Our grammar nor-
mal form, CKY improvements, and symbol subset
coarse-to-fine procedure reduced parsing time for
large transducer grammars by 81%.
These techniques also improved forest-based lan-
guage model reranking. A full decoding pass with-
out any of our innovations required 511 minutes us-
ing only small beams. Coarse-to-fine pruning in
both the parsing and language model passes allowed
a 100-fold increase in beam size, giving a perfor-
mance improvement of 1.3 BLEU while decreasing
total decoding time by 50%.
Acknowledgements
This work was enabled by the Information Sci-
ences Institute Natural Language Group, primarily
through the invaluable assistance of Jens Voeckler,
and was supported by the National Science Founda-
tion (NSF) under grant IIS-0643742.
234
References
Eugene Charniak and Sharon Caraballo. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
In Computational Linguistics.
Eugene Charniak. 1997. Statistical techniques for natu-
ral language parsing. In National Conference on Arti-
ficial Intelligence.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
The Annual Conference of the Association for Compu-
tational Linguistics.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the Association for
Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In The Conference on Empiri-
cal Methods in Natural Language Processing.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In The Annual Conference of
the Association for Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In The Conference on Empirical
Methods in Natural Language Processing.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In The Con-
ference on Empirical Methods in Natural Language
Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In The Annual Conference
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In The Annual Conference of the Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In North American Chapter of the Associ-
ation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In The Statistical Machine Translation
Workshop at the North American Association for Com-
putational Linguistics Conference.
235
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 557?565,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Hierarchical Search for Parsing
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
Berkeley, CA 94720, USA
{adpauls,klein}@cs.berkeley.edu
Abstract
Both coarse-to-fine and A? parsing use simple
grammars to guide search in complex ones.
We compare the two approaches in a com-
mon, agenda-based framework, demonstrat-
ing the tradeoffs and relative strengths of each
method. Overall, coarse-to-fine is much faster
for moderate levels of search errors, but be-
low a certain threshold A? is superior. In addi-
tion, we present the first experiments on hier-
archical A? parsing, in which computation of
heuristics is itself guided by meta-heuristics.
Multi-level hierarchies are helpful in both ap-
proaches, but are more effective in the coarse-
to-fine case because of accumulated slack in
A? heuristics.
1 Introduction
The grammars used by modern parsers are ex-
tremely large, rendering exhaustive parsing imprac-
tical. For example, the lexicalized grammars of
Collins (1997) and Charniak (1997) and the state-
split grammars of Petrov et al (2006) are all
too large to construct unpruned charts in memory.
One effective approach is coarse-to-fine pruning, in
which a small, coarse grammar is used to prune
edges in a large, refined grammar (Charniak et al,
2006). Indeed, coarse-to-fine is even more effective
when a hierarchy of successive approximations is
used (Charniak et al, 2006; Petrov and Klein, 2007).
In particular, Petrov and Klein (2007) generate a se-
quence of approximations to a highly subcategorized
grammar, parsing with each in turn.
Despite its practical success, coarse-to-fine prun-
ing is approximate, with no theoretical guarantees
on optimality. Another line of work has explored
A? search methods, in which simpler problems are
used not for pruning, but for prioritizing work in
the full search space (Klein and Manning, 2003a;
Haghighi et al, 2007). In particular, Klein and Man-
ning (2003a) investigated A? for lexicalized parsing
in a factored model. In that case, A? vastly im-
proved the search in the lexicalized grammar, with
provable optimality. However, their bottleneck was
clearly shown to be the exhaustive parsing used to
compute the A? heuristic itself. It is not obvious,
however, how A? can be stacked in a hierarchical or
multi-pass way to speed up the computation of such
complex heuristics.
In this paper, we address three open questions
regarding efficient hierarchical search. First, can
a hierarchy of A? bounds be used, analogously to
hierarchical coarse-to-fine pruning? We show that
recent work in hierarchical A? (Felzenszwalb and
McAllester, 2007) can naturally be applied to both
the hierarchically refined grammars of Petrov and
Klein (2007) as well as the lexicalized grammars
of Klein and Manning (2003a). Second, what are
the tradeoffs between coarse-to-fine pruning and A?
methods? We show that coarse-to-fine is generally
much faster, but at the cost of search errors.1 Below
a certain search error rate, A? is faster and, of course,
optimal. Finally, when and how, qualitatively, do
these methods fail? A? search?s work grows quickly
as the slack increases between the heuristic bounds
and the true costs. On the other hand, coarse-to-fine
prunes unreliably when the approximating grammar
1In this paper, we consider only errors made by the search
procedure, not modeling errors.
557
Name Rule Priority
IN r : wr I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? I(At, i, j) : ?A = ?B + ?C + wr ?A + h(A, i, j)
Table 1: Deduction rule for A? parsing. The items on the left of the ? indicate what edges must be present on the
chart and what rule can be used to combine them, and the item on the right is the edge that may be added to the agenda.
The weight of each edge appears after the colon. The rule r is A ? B C.
is very different from the target grammar. We em-
pirically demonstrate both failure modes.
2 Parsing algorithms
Our primary goal in this paper is to compare hi-
erarchical A? (HA?) and hierarchical coarse-to-fine
(CTF) pruning methods. Unfortunately, these two
algorithms are generally deployed in different archi-
tectures: CTF is most naturally implemented using
a dynamic program like CKY, while best-first al-
gorithms like A? are necessarily implemented with
agenda-based parsers. To facilitate comparison, we
would like to implement them in a common architec-
ture. We therefore work entirely in an agenda-based
setting, noting that the crucial property of CTF is
not the CKY order of exploration, but the pruning
of unlikely edges, which can be equally well done
in an agenda-based parser. In fact, it is possible to
closely mimic dynamic programs like CKY using a
best-first algorithm with a particular choice of prior-
ities; we discuss this in Section 2.3.
While a general HA? framework is presented in
Felzenszwalb and McAllester (2007), we present
here a specialization to the parsing problem. We first
review the standard agenda-driven search frame-
work and basic A? parsing before generalizing to
HA?.
2.1 Agenda-Driven Parsing
A non-hierarchical, best-first parser takes as input a
PCFG G (with root symbol R), a priority function
p(?) and a sentence consisting of terminals (words)
T0 . . .Tn?1. The parser?s task is to find the best
scoring (Viterbi) tree structure which is rooted at R
and spans the input sentence. Without loss of gen-
erality, we consider grammars in Chomsky normal
form, so that each non-terminal rule in the grammar
has the form r = A ? B C with weight wr. We
assume that weights are non-negative (e.g. negative
log probabilities) and that we wish to minimize the
sum of the rule weights.
A
CB
B C
?
B
A
?
A
=
?
B
+?
C
+w
r
p=?
A
+h(A,i,j)
i k k j ji
w
r
?
C
Figure 1: Deduction rule for A? depicted graphically.
Items to the left of the arrow indicate edges and rules that
can be combined to produce the edge to the right of the ar-
row. Edges are depicted as complete triangles. The value
inside an edge represents the weight of that edge. Each
new edge is assigned the priority written above the arrow
when added to the agenda.
The objects in an agenda-based parser are edges
e = I(X, i, j), also called items, which represent
parses spanning i to j and rooted at symbol X. We
denote edges as triangles, as in Figure 1. At all
times, edges have scores ?e, which are estimates
of their Viterbi inside probabilities (also called path
costs). These estimates improve over time as new
derivations are considered, and may or may not be
correct at termination, depending on the properties
of p. The parser maintains an agenda (a priority
queue of edges), as well as a chart (or closed list
in search terminology) of edges already processed.
The fundamental operation of the algorithm is to pop
the best (lowest) priority edge e from the agenda,
put it into the chart, and enqueue any edges which
can be built by combining e with other edges in the
chart. The combination of two adjacent edges into
a larger edge is shown graphically in Figure 1 and
as a weighted deduction rule in Table 1 (Shieber et
al., 1995; Nederhof, 2003). When an edge a is built
from adjacent edges b and c and a rule r, its cur-
rent score ?a is compared to ?b + ?c + wr and up-
dated if necessary. To allow reconstruction of best
parses, backpointers are maintained in the standard
way. The agenda is initialized with I(Ti, i, i + 1)
558
for i = 0 . . . n ? 1. The algorithm terminates when
I(R, 0, n) is popped off the queue.
Priorities are in general different than weights.
Whenever an edge e?s score changes, its priority
p(e), which may or may not depend on its score,
may improve. Edges are promoted accordingly in
the agenda if their priorities improve. In the sim-
plest case, the priorities are simply the ?e estimates,
which gives a correct uniform cost search wherein
the root edge is guaranteed to have its correct inside
score estimate at termination (Caraballo and Char-
niak, 1996).
A? parsing (Klein and Manning, 2003b) is a spe-
cial case of such an agenda-driven parser in which
the priority function p takes the form p(e) = ?e +
h(e), where e = I(X, i, j) and h(?) is some approx-
imation of e?s Viterbi outside cost (its completion
cost). If h is consistent, then the A? algorithm guar-
antees that whenever an edge comes off the agenda,
its weight is its true Viterbi inside cost. In particular,
this guarantee implies that the first edge represent-
ing the root I(R, 0, n) will be scored with the true
Viterbi score for the sentence.
2.2 Hierarchical A?
In the standard A? case the heuristics are assumed
to come from a black box. For example, Klein and
Manning (2003b) precomputes most heuristics of-
fline, while Klein and Manning (2003a) solves sim-
pler parsing problems for each sentence. In such
cases, the time spent to compute heuristics is often
non-trivial. Indeed, it is typical that effective heuris-
tics are themselves expensive search problems. We
would therefore like to apply A? methods to the
computation of the heuristics themselves. Hierar-
chical A? allows us to do exactly that.
Formally, HA? takes as input a sentence and a se-
quence (or hierarchy) of m + 1 PCFGs G0 . . .Gm,
where Gm is the target grammar and G0 . . .Gm?1
are auxiliary grammars. Each grammar Gt has an in-
ventory of symbols ?t, hereafter denoted with capi-
tal letters. In particular, each grammar has a distin-
guished terminal symbol Tit for each word Ti in the
input and a root symbol Rt.
The grammars G0 . . .Gm must form a hierarchy in
which Gt is a relaxed projection of Gt+1. A grammar
Gt?1 is a projection of Gt if there exists some onto
function pit : ?t $? ?t?1 defined for all symbols in
Agenda
Chart
I(NP, 3, 5)
O(VP, 4, 8)
I(NN, 2, 3)
.
.
.
.
.
I
I
I
O
O
O
G1
G0
G2
Figure 3: Operation of hierarchical A? parsing. An edge
comes off the agenda and is added to the chart (solid line).
From this edge, multiple new edges can be constructed
and added to the agenda (dashed lines). The chart is com-
posed of two subcharts for each grammar in the hierar-
chy: an inside chart (I) and an outside chart (O).
Gt; hereafter, we will use A?t to represent pit(At). A
projection is a relaxation if, for every rule r = At ?
Bt Ct with weight wr the projection r? = pit(r) =
A?t ? B?tC?t has weight wr? ? wr in Gt?1. Given
a target grammar Gm and a projection function pim,
it is easy to construct a relaxed projection Gm?1 by
minimizing over rules collapsed by pim:
wr? = min
r?Gm:pim(r)=r?
wr
Given a series of projection functions pi1 . . .pim,
we can construct relaxed projections by projecting
Gm to Gm?1, then Gm?1 to Gm?2 and so on. Note
that by construction, parses in a relaxed projection
give lower bounds on parses in the target grammar
(Klein and Manning, 2003b).
HA? differs from standard A? in two ways.
First, it tracks not only standard inside edges
e = I(X, i, j) which represent derivations of
X ? Ti . . .Tj , but also outside edges o =
O(X, i, j) which represent derivations of R ?
T0 . . .Ti?1 X Tj+1 . . .Tn. For example, where
I(VP, 0, 3) denotes trees rooted at VP covering the
span [0, 3], O(VP, 0, 3) denotes the derivation of the
?rest? of the structure to the root. Where inside
edges e have scores ?e which represent (approxima-
tions of) their Viterbi inside scores, outside edges o
have scores ?o which are (approximations of) their
Viterbi outside scores. When we need to denote the
inside version of an outside edge, or the reverse, we
write o = e?, etc.
559
Name Rule Priority
IN-BASE O(T?it , i, i + 1) : ?T ? I(Tit, i, i + 1) : 0 ?TIN r : wr O(A?t, i, j) : ?A? I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? I(At, i, j) : ?A = ?B + ?C + wr ?A + ?A?
OUT-BASE I(Rt, 0, n) : ?R ? O(Rt, 0, n) : 0 ?R
OUT-L r : wr O(At, i, j) : ?A I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? O(Bt, i, k) : ?B = ?A + ?C + wr ?B + ?B
OUT-R r : wr O(At, i, j) : ?A I(Bt, i, k) : ?B I(Ct, k, j) : ?C ? O(Ct, k, j) : ?C = ?A + ?B + wr ?C + ?C
Table 2: Deduction rules for HA?. The rule r is in all cases At ? Bt Ct.
A
CB
A'
B C
?
B
A
?
A
=
?
B
+?
C
+w
r
?
A'
p=?
A
+?
A'
i k k j ji
IN
i j
p
=
?
B
+
?
B
?
B
=
?
A
+?
C
+w
r
B
p
=
?
C
+
?
C
?
C
=
?
A
+?
B
+w
r
C
i k
k j
O
U
T
-
L
O
U
T
-
R
A
CB
A
B C
?
C
?
B
i k k
j
w
r
w
r
i j n0
0
n
?
A
?
C
n
0
0 n
(a)
(b)
Figure 2: Non-base case deduction rules for HA? depicted graphically. (a) shows the rule used to build inside edges
and (b) shows the rules to build outside edges. Inside edges are depicted as complete triangles, while outside edges
are depicted as chevrons. An edge from a previous level in the hierarchy is denoted with dashed lines.
The second difference is that HA? tracks items
from all levels of the hierarchy on a single, shared
agenda, so that all items compete (see Figure 3).
While there is only one agenda, it is useful to imag-
ine several charts, one for each type of edge and each
grammar level. In particular, outside edges from one
level of the hierarchy are the source of completion
costs (heuristics) for inside edges at the next level.
The deduction rules for HA? are given in Table 2
and represented graphically in Figure 2. The IN rule
(a) is the familiar deduction rule from standard A?:
we can combine two adjacent inside edges using a
binary rule to form a new inside edge. The new twist
is that because heuristics (scores of outside edges
from the previous level) are also computed on the
fly, they may not be ready yet. Therefore, we cannot
carry out this deduction until the required outside
edge is present in the previous level?s chart. That
is, fine inside deductions wait for the relevant coarse
outside edges to be popped. While coarse outside
edges contribute to priorities of refined inside scores
(as heuristic values), they do not actually affect the
inside scores of edges (again just like basic A?).
In standard A?, we begin with all terminal edges
on the agenda. However, in HA?, we cannot en-
queue refined terminal edges until their outside
scores are ready. The IN-BASE rule specifies the
base case for a grammar Gt: we cannot begin un-
til the outside score for the terminal symbol T is
ready in the coarser grammar Gt?1. The initial queue
contains only the most abstract level?s terminals,
I(Ti0, i, i + 1). The entire search terminates when
the inside edge I(Rm, 0, n), represting root deriva-
tions in the target grammar, is dequeued.
The deductions which assemble outside edges are
less familiar from the standard A? algorithm. These
deductions take larger outside edges and produce
smaller sub-edges by linking up with inside edges,
as shown in Figure 2(b). The OUT-BASE rule states
that an outside pass for Gt can be started if the in-
side score of the root symbol for that level Rt has
been computed. The OUT-L and OUT-R rules are
560
the deduction rules for building outside edges. OUT-
L states that, given an outside edge over the span
[i, j] and some inside edge over [i, k], we may con-
struct an outside edge over [k, j]. For outside edges,
the score reflects an estimate of the Viterbi outside
score.
As in standard A?, inside edges are placed on the
agenda with a priority equal to their path cost (inside
score) and some estimate of their completion cost
(outside score), now taken from the previous projec-
tion rather than a black box. Specifically, the priority
function takes the form p(e) = ?e + ?e?? , where e??
is the outside version of e one level previous in the
hierarchy.
Outside edges also have priorities which combine
path costs with a completion estimate, except that
the roles of inside and outside scores are reversed:
the path cost for an outside edge o is its (outside)
score ?o, while the completion cost is some estimate
of the inside score, which is the weight ?e of o?s
complementary edge e = o?. Therefore, p(o) = ?o+
?o?.
Note that inside edges combine their inside score
estimates with outside scores from a previous level
(a lower bound), while outside edges combine their
outside score estimates with inside scores from the
same level, which are already available. Felzen-
szwalb and McAllester (2007) show that these
choices of priorities have the same guarantee as stan-
dard A?: whenever an inside or outside edge comes
off the queue, its path cost is optimal.
2.3 Agenda-driven Coarse-to-Fine Parsing
We can always replace the HA? priority function
with an alternate priority function of our choosing.
In doing so, we may lose the optimality guarantees
of HA?, but we may also be able to achieve sig-
nificant increases in performance. We do exactly
this in order to put CTF pruning in an agenda-based
framework. An agenda-based implementation al-
lows us to put CTF on a level playing field with HA?,
highlighting the effectiveness of the various parsing
strategies and normalizing their implementations.
First, we define coarse-to-fine pruning. In stan-
dard CTF, we exhaustively parse in each projection
level, but skip edges whose projections in the previ-
ous level had sufficiently low scores. In particular,
an edge e in the grammar Gt will be skipped entirely
if its projection e? in Gt?1 had a low max marginal:
?e?? + ?e? , that is, the score of the best tree contain-
ing e? was low compared to the score best overall
root derivation ?R? . Formally, we prune all e where
?e?? + ?e? > ?R? + ? for some threshold ? .
The priority function we use to implement CTF in
our agenda-based framework is:
p(e) = ?e
p(o) =
8
><
>:
? ?o + ?o? >
?Rt + ?t
?o + ?o? otherwise
Here, ?t ? 0 is a user-defined threshold for level
t and ?Rt is the inside score of the root for gram-
mar Gt. These priorities lead to uniform-cost explo-
ration for inside edges and completely suppress out-
side edges which would have been pruned in stan-
dard CTF. Note that, by the construction of the IN
rule, pruning an outside edge also prunes all inside
edges in the next level that depend on it; we there-
fore prune slightly earlier than in standard CTF. In
any case, this priority function maintains the set of
states explored in CKY-based CTF, but does not nec-
essarily explore those states in the same order.
3 Experiments
3.1 Evaluation
Our focus is parsing speed. Thus, we would ideally
evaluate our algorithms in terms of CPU time. How-
ever, this measure is problematic: CPU time is influ-
enced by a variety of factors, including the architec-
ture of the hardware, low-level implementation de-
tails, and other running processes, all of which are
hard to normalize.
It is common to evaluate best-first parsers in terms
of edges popped off the agenda. This measure is
used by Charniak et al (1998) and Klein and Man-
ning (2003b). However, when edges from grammars
of varying size are processed on the same agenda,
the number of successor edges per edge popped
changes depending on what grammar the edge was
constructed from. In particular, edges in more re-
fined grammars are more expensive than edges in
coarser grammars. Thus, our basic unit of measure-
ment will be edges pushed onto the agenda. We
found in our experiments that this was well corre-
lated with CPU time.
561
UCS A*
3
HA*
3
HA*
3-5
HA*
0-5
CTF
3
CTF
3-5
CTF
0-5
Edges 
pushed
 (billio
ns)
0
100
200
300
400 424
86.6
78.2
58.8 60.1
8.83 7.12
1.98
Figure 4: Efficiency of several hierarchical parsing algo-
rithms, across the test set. UCS and all A? variants are
optimal and thus make no search errors. The CTF vari-
ants all make search errors on about 2% of sentences.
3.2 State-Split Grammars
We first experimented with the grammars described
in Petrov et al (2006). Starting with an X-Bar gram-
mar, they iteratively refine each symbol in the gram-
mar by adding latent substates via a split-merge pro-
cedure. This training procedure creates a natural hi-
erarchy of grammars, and is thus ideal for our pur-
poses. We used the Berkeley Parser2 to train such
grammars on sections 2-21 of the Penn Treebank
(Marcus et al, 1993). We ran 6 split-merge cycles,
producing a total of 7 grammars. These grammars
range in size from 98 symbols and 8773 rules in the
unsplit X-Bar grammar to 1139 symbols and 973696
rules in the 6-split grammar. We then parsed all sen-
tences of length ? 30 of section 23 of the Treebank
with these grammars. Our ?target grammar? was in
all cases the largest (most split) grammar. Our pars-
ing objective was to find the Viterbi derivation (i.e.
fully refined structure) in this grammar. Note that
this differs from the objective used by Petrov and
Klein (2007), who use a variational approximation
to the most probable parse.
3.2.1 A? versus HA?
We first compare HA? with standard A?. In A? as
presented by Klein and Manning (2003b), an aux-
iliary grammar can be used, but we are restricted
to only one and we must compute inside and out-
side estimates for that grammar exhaustively. For
our single auxiliary grammar, we chose the 3-split
grammar; we found that this grammar provided the
best overall speed.
For HA?, we can include as many or as few
auxiliary grammars from the hierarchy as desired.
Ideally, we would find that each auxiliary gram-
2http://berkeleyparser.googlecode.com
mar increases performance. To check this, we per-
formed experiments with all 6 auxiliary grammars
(0-5 split); the largest 3 grammars (3-5 split); and
only the 3-split grammar.
Figure 4 shows the results of these experiments.
As a baseline, we also compare with uniform cost
search (UCS) (A? with h = 0 ). A? provides a
speed-up of about a factor of 5 over this UCS base-
line. Interestingly, HA? using only the 3-split gram-
mar is faster than A? by about 10% despite using the
same grammars. This is because, unlike A?, HA?
need not exhaustively parse the 3-split grammar be-
fore beginning to search in the target grammar.
When we add the 4- and 5-split grammars to HA?,
it increases performance by another 25%. However,
we can also see an important failure case of HA?:
using all 6 auxiliary grammars actually decreases
performance compared to using only 3-5. This is be-
cause HA? requires that auxiliary grammars are all
relaxed projections of the target grammar. Since the
weights of the rules in the smaller grammars are the
minimum of a large set of rules in the target gram-
mar, these grammars have costs that are so cheap
that all edges in those grammars will be processed
long before much progress is made in the refined,
more expensive levels. The time spent parsing in
the smaller grammars is thus entirely wasted. This
is in sharp contrast to hierarchical CTF (see below)
where adding levels is always beneficial.
To quantify the effect of optimistically cheap
costs in the coarsest projections, we can look at the
degree to which the outside costs in auxiliary gram-
mars underestimate the true outside cost in the target
grammar (the ?slack?). In Figure 5, we plot the aver-
age slack as a function of outside context size (num-
ber of unincorporated words) for each of the auxil-
iary grammars. The slack for large outside contexts
gets very large for the smaller, coarser grammars. In
Figure 6, we plot the number of edges pushed when
bounding with each auxiliary grammar individually,
against the average slack in that grammar. This plot
shows that greater slack leads to more work, reflect-
ing the theoretical property of A? that the work done
can be exponential in the slack of the heuristic.
3.2.2 HA? versus CTF
In this section, we compare HA? to CTF, again
using the grammars of Petrov et al (2006). It is
562
5 10 15 20
020
4060
80100
Number of words in outside context
Average slack
0 split
1 split
2 split
3 split
4 split
5 split
Figure 5: Average slack (difference between estimated
outside cost and true outside cost) at each level of ab-
straction as a function of the size of the outside context.
The average is over edges in the Viterbi tree. The lower
and upper dashed lines represent the slack of the exact
and uniformly zero heuristics.
5 10 15 20 25 30 35
0500
1500
2500
3500
Slack for span length 10
Edges 
pushed
 (million
s)
Figure 6: Edges pushed as a function of the average slack
for spans of length 10 when parsing with each auxiliary
grammar individually.
important to note, however, that we do not use the
same grammars when parsing with these two al-
gorithms. While we use the same projections to
coarsen the target grammar, the scores in the CTF
case need not be lower bounds. Instead, we fol-
low Petrov and Klein (2007) in taking coarse gram-
mar weights which make the induced distribution
over trees as close as possible to the target in KL-
divergence. These grammars represent not a mini-
mum projection, but more of an average.3
The performance of CTF as compared to HA?
is shown in Figure 4. CTF represents a significant
speed up over HA?. The key advantage of CTF, as
shown here, is that, where the work saved by us-
3We tried using these average projections as heuristics in
HA?, but doing so violates consistency, causes many search er-
rors, and does not substantially speed up the search.
5 10 15 20 25 30
020
4060
80
120
Length of sentence
Edges pu
shed per
 sentenc
e (millio
ns)
HA* 3-5
CTF 0-5
Figure 7: Edges pushed as function of sentence length for
HA? 3-5 and CTF 0-5.
ing coarser projections falls off for HA?, the work
saved with CTF increases with the addition of highly
coarse grammars. Adding the 0- through 2-split
grammars to CTF was responsible for a factor of 8
speed-up with no additional search errors.
Another important property of CTF is that it
scales far better with sentence length than does HA?.
Figure 7 shows a plot of edges pushed against sen-
tence length. This is not surprising in light of the in-
crease in slack that comes with parsing longer sen-
tences. The more words in an outside context, the
more slack there will generally be in the outside es-
timate, which triggers the time explosion.
Since we prune based on thresholds ?t in CTF,
we can explore the relationship between the number
of search errors made and the speed of the parser.
While it is possible to tune thresholds for each gram-
mar individually, we use a single threshold for sim-
plicity. In Figure 8, we plot the performance of CTF
using all 6 auxiliary grammars for various values of
? . For a moderate number of search errors (< 5%),
CTF parses more than 10 times faster than HA? and
nearly 100 times faster than UCS. However, below a
certain tolerance for search errors (< 1%) on these
grammars, HA? is the faster option.4
3.3 Lexicalized parsing experiments
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003a).
This lexicalized parsing model is constructed as the
product of a dependency model and the unlexical-
4In Petrov and Klein (2007), fewer search errors are re-
ported; this difference is because their search objective is more
closely aligned to the CTF pruning criterion.
563
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
0.5
2.05.0
20.0
100.0
500.0
Fraction of sentences without search errors
Edges p
ushed (b
illions) HA* 3-5
UCS
Figure 8: Performance of CTF as a function of search er-
rors for state split grammars. The dashed lines represent
the time taken by UCS and HA? which make no search
errors. As search accuracy increases, the time taken by
CTF increases until it eventually becomes slower than
HA?. The y-axis is a log scale.
ized PCFG model in Klein and Manning (2003c).
We constructed these grammars using the Stanford
Parser.5 The PCFG has 19054 symbols 36078 rules.
The combined (sentence-specific) grammar has n
times as many symbols and 2n2 times as many rules,
where n is the length of an input sentence. This
model was trained on sections 2-20 of the Penn Tree-
bank and tested on section 21.
For these lexicalized grammars, we did not per-
form experiments with UCS or more than one level
of HA?. We used only the single PCFG projection
used in Klein and Manning (2003a). This grammar
differs from the state split grammars in that it factors
into two separate projections, a dependency projec-
tion and a PCFG. Klein and Manning (2003a) show
that one can use the sum of outside scores computed
in these two projections as a heuristic in the com-
bined lexicalized grammar. The generalization of
HA? to the factored case is straightforward but not
effective. We therefore treated the dependency pro-
jection as a black box and used only the PCFG pro-
jection inside the HA? framework. When comput-
ing A? outside estimates in the combined space, we
use the sum of the two projections? outside scores as
our completion costs. This is the same procedure as
Klein and Manning (2003a). For CTF, we carry out
a uniform cost search in the combined space where
we have pruned items based on their max-marginals
5http://nlp.stanford.edu/software/
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
3
4
5
67
8
Fraction of sentences without search errors
Edges p
ushed (b
illions)
A*
Figure 9: Performance of CTF for lexicalized parsing as
a function of search errors. The dashed line represents
the time taken by A?, which makes no search errors. The
y-axis is a log scale.
in the PCFG model only.
In Figure 9, we examine the speed/accuracy trade
off for the lexicalized grammar. The trend here is
the reverse of the result for the state split grammars:
HA? is always faster than posterior pruning, even for
thresholds which produce many search errors. This
is because the heuristic used in this model is actu-
ally an extraordinarily tight bound ? on average, the
slack even for spans of length 1 was less than 1% of
the overall model cost.
4 Conclusions
We have a presented an empirical comparison of
hierarchical A? search and coarse-to-fine pruning.
While HA? does provide benefits over flat A?
search, the extra levels of the hierarchy are dramat-
ically more beneficial for CTF. This is because, in
CTF, pruning choices cascade and even very coarse
projections can prune many highly unlikely edges.
However, in HA?, overly coarse projections become
so loose as to not rule out anything of substance. In
addition, we experimentally characterized the fail-
ure cases of A? and CTF in a way which matches
the formal results on A?: A? does vastly more work
as heuristics loosen and only outperforms CTF when
either near-optimality is desired or heuristics are ex-
tremely tight.
Acknowledgements
This work was partially supported by an NSERC Post-Graduate
Scholarship awarded to the first author.
564
References
Sharon Caraballo and Eugene Charniak. 1996. Figures
of Merit for Best-First Probabalistic Parsing. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Eugene Charniak. 1997 Statistical Parsing with a
Context-Free Grammar and Word Statistics. In Pro-
ceedings of the Fourteenth National Conference on Ar-
tificial Intelligence.
Eugene Charniak, Sharon Goldwater and Mark Johnson.
1998. Edge-based Best First Parsing. In Proceedings
of the Sixth Workshop on Very Large Corpora.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine Hill,
R. Shrivaths, Jeremy Moore, Michael Pozar, and
Theresa Vu. 2006. Multilevel Coarse-to-fine PCFG
Parsing. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics.
Michael Collins. 1997. Three Generative, Lexicalised
Models for Statistical Parsing. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
P. Felzenszwalb and D. McAllester. 2007. The General-
ized A? Architecture. In Journal of Artificial Intelli-
gence Research.
Aria Haghighi, John DeNero, and Dan Klein. 2007. Ap-
proximate Factoring for A? Search. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics.
Dan Klein and Chris Manning. 2002. Fast Exact In-
ference with a Factored Model for Natural Language
Processing. In Advances in Neural Information Pro-
cessing Systems.
Dan Klein and Chris Manning. 2003. Factored A?
Search for Models over Sequences and Trees. In Pro-
ceedings of the International Joint Conference on Ar-
tificial Intelligence.
Dan Klein and Chris Manning. 2003. A? Parsing: Fast
Exact Viterbi Parse Selection. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics
Dan Klein and Chris Manning. 2003. Accurate Unlexi-
calized Parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Mark-Jan Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. In Computational Linguistics,
29(1):135?143.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2003. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. In Journal of Logic Programming,
24:3?36.
565
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 958?966,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
K-Best A? Parsing
Adam Pauls and Dan Klein
Computer Science Division
University of California, Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
A? parsing makes 1-best search efficient by
suppressing unlikely 1-best items. Existing k-
best extraction methods can efficiently search
for top derivations, but only after an exhaus-
tive 1-best pass. We present a unified algo-
rithm for k-best A? parsing which preserves
the efficiency of k-best extraction while giv-
ing the speed-ups of A? methods. Our algo-
rithm produces optimal k-best parses under the
same conditions required for optimality in a
1-best A? parser. Empirically, optimal k-best
lists can be extracted significantly faster than
with other approaches, over a range of gram-
mar types.
1 Introduction
Many situations call for a parser to return the k-
best parses rather than only the 1-best. Uses for
k-best lists include minimum Bayes risk decod-
ing (Goodman, 1998; Kumar and Byrne, 2004),
discriminative reranking (Collins, 2000; Char-
niak and Johnson, 2005), and discriminative train-
ing (Och, 2003; McClosky et al, 2006). The
most efficient known algorithm for k-best parsing
(Jime?nez and Marzal, 2000; Huang and Chiang,
2005) performs an initial bottom-up dynamic pro-
gramming pass before extracting the k-best parses.
In that algorithm, the initial pass is, by far, the bot-
tleneck (Huang and Chiang, 2005).
In this paper, we propose an extension of A?
parsing which integrates k-best search with an A?-
based exploration of the 1-best chart. A? pars-
ing can avoid significant amounts of computation
by guiding 1-best search with heuristic estimates
of parse completion costs, and has been applied
successfully in several domains (Klein and Man-
ning, 2002; Klein and Manning, 2003c; Haghighi
et al, 2007). Our algorithm extends the speed-
ups achieved in the 1-best case to the k-best case
and is optimal under the same conditions as a stan-
dard A? algorithm. The amount of work done in
the k-best phase is no more than the amount of
work done by the algorithm of Huang and Chiang
(2005). Our algorithm is also equivalent to stan-
dard A? parsing (up to ties) if it is terminated after
the 1-best derivation is found. Finally, our algo-
rithm can be written down in terms of deduction
rules, and thus falls into the well-understood view
of parsing as weighted deduction (Shieber et al,
1995; Goodman, 1998; Nederhof, 2003).
In addition to presenting the algorithm, we
show experiments in which we extract k-best lists
for three different kinds of grammars: the lexi-
calized grammars of Klein and Manning (2003b),
the state-split grammars of Petrov et al (2006),
and the tree transducer grammars of Galley et al
(2006). We demonstrate that optimal k-best lists
can be extracted significantly faster using our al-
gorithm than with previous methods.
2 A k-Best A? Parsing Algorithm
We build up to our full algorithm in several stages,
beginning with standard 1-best A? parsing and
making incremental modifications.
2.1 Parsing as Weighted Deduction
Our algorithm can be formulated in terms of
prioritized weighted deduction rules (Shieber et
al., 1995; Nederhof, 2003; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
958
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of un-
processed items), as well as a chart of items al-
ready processed. The fundamental operation of
the algorithm is to pop the highest priority item ?
from the agenda, put it into the chart with its cur-
rent weight, and form using deduction rules any
items which can be built by combining ? with
items already in the chart. If new or improved,
resulting items are put on the agenda with priority
given by p(?).
2.2 A? Parsing
The A? parsing algorithm of Klein and Manning
(2003c) can be formulated in terms of weighted
deduction rules (Felzenszwalb and McAllester,
2007). We do so here both to introduce notation
and to build to our final algorithm.
First, we must formalize some notation. As-
sume we have a PCFG1 G and an input sentence
s1 . . . sn of length n. The grammar G has a set of
symbols ?, including a distinguished goal (root)
symbol G. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in G has the form r = A? B C with weight wr
(the negative log-probability of the rule). Edges
are labeled spans e = (A, i, j). Inside derivations
of an edge (A, i, j) are trees rooted at A and span-
ning si+1 . . . sj . The total weight of the best (min-
imum) inside derivation for an edge e is called the
Viterbi inside score ?(e). The goal of the 1-best
A? parsing algorithm is to compute the Viterbi in-
side score of the edge (G, 0, n); backpointers al-
low the reconstruction of a Viterbi parse in the
standard way.
The basic A? algorithm operates on deduc-
tion items I(A, i, j) which represent in a col-
lapsed way the possible inside derivations of edges
(A, i, j). We call these items inside edge items or
simply inside items where clear; a graphical rep-
resentation of an inside item can be seen in Fig-
ure 1(a). The space whose items are inside edges
is called the edge space.
These inside items are combined using the sin-
gle IN deduction schema shown in Table 1. This
schema is instantiated for every grammar rule r
1While we present the algorithm specialized to parsing
with a PCFG, it generalizes to a wide range of hypergraph
search problems as shown in Klein and Manning (2001).
VP
s
3
s
4
s
5
s
1
s
2
... s
6
s
n
...
VP
VBZ NP
DT NN
s
3
s
4
s
5
VP
G
(a) (b)
(c)
VP
VBZ
1
NP
4
DT NN
s
3
s
4
s
5
(e)
VP
6
s
3
s
4
s
5
VBZ NP
DT NN
(d)
Figure 1: Representations of the different types of
items used in parsing. (a) An inside edge item:
I(VP, 2, 5). (b) An outside edge item: O(VP, 2, 5).
(c) An inside derivation item: D(TVP, 2, 5) for a tree
TVP. (d) A ranked derivation item: K(VP, 2, 5, 6).
(e) A modified inside derivation item (with back-
pointers to ranked items): D(VP, 2, 5, 3,VP ?
VBZ NP, 1, 4).
in G. For IN, the function g(?) simply sums the
weights of the antecedent items and the gram-
mar rule r, while the priority function p(?) adds
a heuristic to this sum. The heuristic is a bound
on the Viterbi outside score ?(e) of an edge e;
see Klein and Manning (2003c) for details. A
good heuristic allows A? to reach the goal item
I(G, 0, n) while constructing few inside items.
If the heuristic is consistent, then A? guarantees
that whenever an inside item comes off the agenda,
its weight is its true Viterbi inside score (Klein and
Manning, 2003c). In particular, this guarantee im-
plies that the goal item I(G, 0, n) will be popped
with the score of the 1-best parse of the sentence.
Consistency also implies that items are popped off
the agenda in increasing order of bounded Viterbi
scores:
?(e) + h(e)
We will refer to this monotonicity as the order-
ing property of A? (Felzenszwalb and McAllester,
2007). One final property implied by consistency
is admissibility, which states that the heuristic
never overestimates the true Viterbi outside score
for an edge, i.e. h(e) ? ?(e). For the remain-
der of this paper, we will assume our heuristics
are consistent.
2.3 A Naive k-Best A? Algorithm
Due to the optimal substructure of 1-best PCFG
derivations, a 1-best parser searches over the space
of edges; this is the essence of 1-best dynamic
programming. Although most edges can be built
959
Inside Edge Deductions (Used in A? and KA?)
IN: I(B, i, l) : w1 I(C, l, j) : w2
w1+w2+wr+h(A,i,j)?????????????? I(A, i, j) : w1 + w2 + wr
Table 1: The deduction schema (IN) for building inside edge items, using a supplied heuristic. This schema is
sufficient on its own for 1-best A?, and it is used in KA?. Here, r is the rule A? B C.
Inside Derivation Deductions (Used in NAIVE)
DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2
w1+w2+wr+h(A,i,j)?????????????? D
(
A
TB TC
, i, j
)
: w1 + w2 + wr
Table 2: The deduction schema for building derivations, using a supplied heuristic. TB and TC denote full tree
structures rooted at symbols B and C. This schema is the same as the IN deduction schema, but operates on the
space of fully specified inside derivations rather than dynamic programming edges. This schema forms the NAIVE
k-best algorithm.
Outside Edge Deductions (Used in KA?)
OUT-B: I(G, 0, n) : w1
w1??? O(G, 0, n) : 0
OUT-L: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w3+wr+w2??????????? O(B, i, l) : w1 + w3 + wr
OUT-R: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w2+wr+w3??????????? O(C, l, j) : w1 + w2 + wr
Table 3: The deduction schemata for building ouside edge items. The first schema is a base case that constructs an
outside item for the goal (G, 0, n) from the inside item I(G, 0, n). The second two schemata build outside items
in a top-down fashion. Note that for outside items, the completion cost is the weight of an inside item rather than
a value computed by a heuristic.
Delayed Inside Derivation Deductions (Used in KA?)
DERIV: D(TB , i, l) : w1 D(TC , l, j) : w2 O(A, i, j) : w3
w1+w2+wr+w3??????????? D
(
A
TB TC
, i, j
)
: w1 + w2 + wr
Table 4: The deduction schema for building derivations, using exact outside scores computed using OUT deduc-
tions. The dependency on the outside item O(A, i, j) delays building derivation items until exact Viterbi outside
scores have been computed. This is the final search space for the KA? algorithm.
Ranked Inside Derivation Deductions (Lazy Version of NAIVE)
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2
w1+w2+wr+h(A,i,j)?????????????? D(A, i, j, l, r, u, v) : w1 + w2 + wr
RANK: D1(A, i, j, ?) : w1 . . . Dk(A, i, j, ?) : wk
maxm wm+h(A,i,j)????????????? K(A, i, j, k) : maxm wm
Table 5: The schemata for simultaneously building and ranking derivations, using a supplied heuristic, for the lazier
form of the NAIVE algorithm. BUILD builds larger derivations from smaller ones. RANK numbers derivations
for each edge. Note that RANK requires distinct Di, so a rank k RANK rule will first apply (optimally) as soon as
the kth-best inside derivation item for a given edge is removed from the queue. However, it will also still formally
apply (suboptimally) for all derivation items dequeued after the kth. In practice, the RANK schema need not be
implemented explicitly ? one can simply assign a rank to each inside derivation item when it is removed from the
agenda, and directly add the appropriate ranked inside item to the chart.
Delayed Ranked Inside Derivation Deductions (Lazy Version of KA?)
BUILD: K(B, i, l, u) : w1 K(C, l, j, v) : w2 O(A, i, j) : w3
w1+w2+wr+w3??????????? D(A, i, j, l, r, u, v) : w1 + w2 + wr
RANK: D1(A, i, j, ?) : w1 . . . Dk(A, i, j, ?) : wk O(A, i, j) : wk+1
maxm wm+wk+1???????????? K(A, i, j, k) : maxm wm
Table 6: The deduction schemata for building and ranking derivations, using exact outside scores computed from
OUT deductions, used for the lazier form of the KA? algorithm.
960
using many derivations, each inside edge item
will be popped exactly once during parsing, with
a score and backpointers representing its 1-best
derivation.
However, k-best lists involve suboptimal
derivations. One way to compute k-best deriva-
tions is therefore to abandon optimal substructure
and dynamic programming entirely, and to search
over the derivation space, the much larger space
of fully specified trees. The items in this space are
called inside derivation items, or derivation items
where clear, and are of the form D(TA, i, j), spec-
ifying an entire tree TA rooted at symbol A and
spanning si+1 . . . sj (see Figure 1(c)). Derivation
items are combined using the DERIV schema of
Table 2. The goals in this space, representing root
parses, are any derivation items rooted at symbol
G that span the entire input.
In this expanded search space, each distinct
parse has its own derivation item, derivable only
in one way. If we continue to search long enough,
we will pop multiple goal items. The first k which
come off the agenda will be the k-best derivations.
We refer to this approach as NAIVE. It is very in-
efficient on its own, but it leads to the full algo-
rithm.
The correctness of this k-best algorithm follows
from the correctness of A? parsing. The derivation
space of full trees is simply the edge space of a
much larger grammar (see Section 2.5).
Note that the DERIV schema?s priority includes
a heuristic just like 1-best A?. Because of the
context freedom of the grammar, any consistent
heuristic for inside edge items usable in 1-best A?
is also consistent for inside derivation items (and
vice versa). In particular, the 1-best Viterbi out-
side score for an edge is a ?perfect? heuristic for
any derivation of that edge.
While correct, NAIVE is massively inefficient.
In comparison with A? parsing over G, where there
are O(n2) inside items, the size of the derivation
space is exponential in the sentence length. By
the ordering property, we know that NAIVE will
process all derivation items d with
?(d) + h(d) ? ?(gk)
where gk is the kth-best root parse and ?(?) is the
inside score of a derivation item (analogous to ?
for edges).2 Even for reasonable heuristics, this
2The new symbol emphasizes that ? scores a specific
derivation rather than a minimum over a set of derivations.
number can be very large; see Section 3 for empir-
ical results.
This naive algorithm is, of course, not novel, ei-
ther in general approach or specific computation.
Early k-best parsers functioned by abandoning dy-
namic programming and performing beam search
on derivations (Ratnaparkhi, 1999; Collins, 2000).
Huang (2005) proposes an extension of Knuth?s
algorithm (Knuth, 1977) to produce k-best lists
by searching in the space of derivations, which
is essentially this algorithm. While Huang (2005)
makes no explicit mention of a heuristic, it would
be easy to incorporate one into their formulation.
2.4 A New k-Best A? Parser
While NAIVE suffers severe performance degra-
dation for loose heuristics, it is in fact very effi-
cient if h(?) is ?perfect,? i.e. h(e) = ?(e) ?e. In
this case, the ordering property of A? guarantees
that only inside derivation items d satisfying
?(d) + ?(d) ? ?(gk)
will be placed in the chart. The set of derivation
items d satisfying this inequality is exactly the set
which appear in the k-best derivations of (G, 0, n)
(as always, modulo ties). We could therefore use
NAIVE quite efficiently if we could obtain exact
Viterbi outside scores.
One option is to compute outside scores with
exhaustive dynamic programming over the orig-
inal grammar. In a certain sense, described in
greater detail below, this precomputation of exact
heuristics is equivalent to the k-best extraction al-
gorithm of Huang and Chiang (2005). However,
this exhaustive 1-best work is precisely what we
want to use A? to avoid.
Our algorithm solves this problem by integrat-
ing three searches into a single agenda-driven pro-
cess. First, an A? search in the space of inside
edge items with an (imperfect) external heuristic
h(?) finds exact inside scores. Second, exact out-
side scores are computed from inside and outside
items. Finally, these exact outside scores guide the
search over derivations. It can be useful to imag-
ine these three operations as operating in phases,
but they are all interleaved, progressing in order of
their various priorities.
In order to calculate outside scores, we intro-
duce outside items O(A, i, j), which represent
best derivations of G ? s1 . . . si A sj+1 . . . sn;
see Figure 1(b). Where the weights of inside items
961
compute Viterbi inside scores, the weights of out-
side items compute Viterbi outside scores.
Table 3 shows deduction schemata for building
outside items. These schemata are adapted from
the schemata used in the general hierarchical A?
algorithm of Felzenszwalb and McAllester (2007).
In that work, it is shown that such schemata main-
tain the property that the weight of an outside item
is the true Viterbi outside score when it is removed
from the agenda. They also show that outside
items o follow an ordering property, namely that
they are processed in increasing order of
?(o) + ?(o)
This quantity is the score of the best root deriva-
tion which includes the edge corresponding to o.
Felzenszwalb and McAllester (2007) also show
that both inside and outside items can be processed
on the same queue and the ordering property holds
jointly for both types of items.
If we delay the construction of a derivation
item until its corresponding outside item has been
popped, then we can gain the benefits of using an
exact heuristic h(?) in the naive algorithm. We re-
alize this delay by modifying the DERIV deduc-
tion schema as shown in Table 4 to trigger on and
prioritize with the appropriate outside scores.
We now have our final algorithm, which we call
KA?. It is the union of the IN, OUT, and new ?de-
layed? DERIV deduction schemata. In words, our
algorithm functions as follows: we initialize the
agenda with I(si, i ? 1, i) and D(si, i ? 1, i) for
i = 1 . . . n. We compute inside scores in standard
A? fashion using the IN deduction rule, using any
heuristic we might provide to 1-best A?. Once the
inside item I(G, 0, n) is found, we automatically
begin to compute outside scores via the OUT de-
duction rules. Once O(si, i ? 1, i) is found, we
can begin to also search in the space of deriva-
tion items, using the perfect heuristics given by
the just-computed outside scores. Note, however,
that all computation is done with a single agenda,
so the processing of all three types of items is in-
terleaved, with the k-best search possibly termi-
nating without a full inside computation. As with
NAIVE, the algorithm terminates when a k-th goal
derivation is dequeued.
2.5 Correctness
We prove the correctness of this algorithm by a re-
duction to the hierarchical A? (HA?) algorithm of
Felzenszwalb and McAllester (2007). The input
to HA? is a target grammar Gm and a list of gram-
mars G0 . . .Gm?1 in which Gt?1 is a relaxed pro-
jection of Gt for all t = 1 . . .m. A grammar Gt?1
is a projection of Gt if there exists some onto func-
tion pit : ?t 7? ?t?1 defined for all symbols in Gt.
We use At?1 to represent pit(At). A projection is
relaxed if, for every rule r = At ? BtCt with
weight wr there is a rule r? = At?1 ? Bt?1Ct?1
in Gt?1 with weight wr? ? wr.
We assume that our external heuristic function
h(?) is constructed by parsing our input sentence
with a relaxed projection of our target grammar.
This assumption, though often true anyway, is
to allow proof by reduction to Felzenszwalb and
McAllester (2007).3
We construct an instance of HA? as follows: Let
G0 be the relaxed projection which computes the
heuristic. Let G1 be the input grammar G, and let
G2, the target grammar of our HA? instance, be the
grammar of derivations in G formed by expanding
each symbol A in G to all possible inside deriva-
tions TA rooted atA. The rules in G2 have the form
TA ? TB TC with weight given by the weight of
the rule A ? B C. By construction, G1 is a re-
laxed projection of G2; by assumption G0 is a re-
laxed projection of G1. The deduction rules that
describe KA? build the same items as HA? with
same weights and priorities, and so the guarantees
from HA? carry over to KA?.
We can characterize the amount of work done
using the ordering property. Let gk be the kth-best
derivation item for the goal edge g. Our algorithm
processes all derivation items d, outside items o,
and inside items i satisfying
?(d) + ?(d) ? ?(gk)
?(o) + ?(o) ? ?(gk)
?(i) + h(i) ? ?(gk)
We have already argued that the set of deriva-
tion items satisfying the first inequality is the set of
subtrees that appear in the optimal k-best parses,
modulo ties. Similarly, it can be shown that the
second inequality is satisfied only for edges that
appear in the optimal k-best parses. The last in-
equality characterizes the amount of work done in
the bottom-up pass. We compare this to 1-best A?,
which pops all inside items i satisfying
?(i) + h(i) ? ?(g) = ?(g1)
3KA? is correct for any consistent heuristic but a non-
reductive proof is not possible in the present space.
962
Thus, the ?extra? inside items popped in the
bottom-up pass during k-best parsing as compared
to 1-best parsing are those items i satisfying
?(g1) ? ?(i) + h(i) ? ?(gk)
The question of how many items satisfy these
inequalities is empirical; we show in our experi-
ments that it is small for reasonable heuristics. At
worst, the bottom-up phase pops all inside items
and reduces to exhaustive dynamic programming.
Additionally, it is worth noting that our algo-
rithm is naturally online in that it can be stopped
at any k without advance specification.
2.6 Lazy Successor Functions
The global ordering property guarantees that we
will only dequeue derivation fragments of top
parses. However, we will enqueue all combina-
tions of such items, which is wasteful. By ex-
ploiting a local ordering amongst derivations, we
can be more conservative about combination and
gain the advantages of a lazy successor function
(Huang and Chiang, 2005).
To do so, we represent inside derivations not
by explicitly specifying entire trees, but rather
by using ranked backpointers. In this represen-
tation, inside derivations are represented in two
ways, shown in Figure 1(d) and (e). The first
way (d) simply adds a rank u to an edge, giving
a tuple (A, i, j, u). The corresponding item is the
ranked derivation item K(A, i, j, u), which repre-
sents the uth-best derivation of A over (i, j). The
second representation (e) is a backpointer of the
form (A, i, j, l, r, u, v), specifying the derivation
formed by combining the uth-best derivation of
(B, i, l) and the vth-best derivation of (C, l, j) us-
ing rule r = A? B C. The corresponding items
D(A, i, j, l, r, u, v) are the new form of our inside
derivation items.
The modified deduction schemata for the
NAIVE algorithm over these representations are
shown in Table 5. The BUILD schema pro-
duces new inside derivation items from ranked
derivation items, while the RANK schema as-
signs each derivation item a rank; together they
function like DERIV. We can find the k-best list
by searching until K(G, 0, n, k) is removed from
the agenda. The k-best derivations can then
be extracted by following the backpointers for
K(G, 0, n, 1) . . . K(G, 0, n, k). The KA? algo-
rithm can be modified in the same way, shown in
Table 6.
1
5
50
500
Heuristic
Deriva
tion ite
ms pus
hed (m
illions
)
5-split 4-split 3-split 2-split 1-split 0-split
NAIVE
KA*
Figure 2: Number of derivation items enqueued as a
function of heuristic. Heuristics are shown in decreas-
ing order of tightness. The y-axis is on a log-scale.
The actual laziness is provided by addition-
ally delaying the combination of ranked items.
When an item K(B, i, l, u) is popped off the
queue, a naive implementation would loop over
items K(C, l, j, v) for all v, C, and j (and
similarly for left combinations). Fortunately,
little looping is actually necessary: there is
a partial ordering of derivation items, namely,
that D(A, i, j, l, r, u, v) will have a lower com-
puted priority than D(A, i, j, l, r, u ? 1, v) and
D(A, i, j, l, r, u, v ? 1) (Jime?nez and Marzal,
2000). So, we can wait until one of the latter two
is built before ?triggering? the construction of the
former. This triggering is similar to the ?lazy fron-
tier? used by Huang and Chiang (2005). All of our
experiments use this lazy representation.
3 Experiments
3.1 State-Split Grammars
We performed our first experiments with the gram-
mars of Petrov et al (2006). The training pro-
cedure for these grammars produces a hierarchy
of increasingly refined grammars through state-
splitting. We followed Pauls and Klein (2009) in
computing heuristics for the most refined grammar
from outside scores for less-split grammars.
We used the Berkeley Parser4 to learn such
grammars from Sections 2-21 of the Penn Tree-
bank (Marcus et al, 1993). We trained with 6
split-merge cycles, producing 7 grammars. We
tested these grammars on 100 sentences of length
at most 30 of Section 23 of the Treebank. Our
?target grammar? was in all cases the most split
grammar.
4http://berkeleyparser.googlecode.com
963
0 2000 4000 6000 8000 10000
050
00
15000
25000
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
0 2000 4000 6000 8000 10000
050
00
15000
25000
EXH
k
Items 
pushed
 (milli
ons) K BestBottom-up
Figure 3: The cost of k-best extraction as a function of k for state-split grammars, for both KA? and EXH. The
amount of time spent in the k-best phase is negligible compared to the cost of the bottom-up phase in both cases.
Heuristics computed from projections to suc-
cessively smaller grammars in the hierarchy form
successively looser bounds on the outside scores.
This allows us to examine the performance as a
function of the tightness of the heuristic. We first
compared our algorithm KA? against the NAIVE
algorithm. We extracted 1000-best lists using each
algorithm, with heuristics computed using each of
the 6 smaller grammars.
In Figure 2, we evaluate only the k-best extrac-
tion phase by plotting the number of derivation
items and outside items added to the agenda as
a function of the heuristic used, for increasingly
loose heuristics. We follow earlier work (Pauls
and Klein, 2009) in using number of edges pushed
as the primary, hardware-invariant metric for eval-
uating performance of our algorithms.5 While
KA? scales roughly linearly with the looseness of
the heuristic, NAIVE degrades very quickly as the
heuristics get worse. For heuristics given by gram-
mars weaker than the 4-split grammar, NAIVE ran
out of memory.
Since the bottom-up pass of k-best parsing is
the bottleneck, we also examine the time spent
in the 1-best phase of k-best parsing. As a base-
line, we compared KA? to the approach of Huang
and Chiang (2005), which we will call EXH (see
below for more explanation) since it requires ex-
haustive parsing in the bottom-up pass. We per-
formed the exhaustive parsing needed for EXH
in our agenda-based parser to facilitate compar-
ison. For KA?, we included the cost of com-
puting the heuristic, which was done by running
our agenda-based parser exhaustively on a smaller
grammar to compute outside items; we chose the
5We found that edges pushed was generally well corre-
lated with parsing time.
0 2000 4000 6000 8000 10000
020
0
600
1000
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
Figure 4: The performance of KA? for lexicalized
grammars. The performance is dominated by the com-
putation of the heuristic, so that both the bottom-up
phase and the k-best phase are barely visible.
3-split grammar for the heuristic since it gives the
best overall tradeoff of heuristic and bottom-up
parsing time. We separated the items enqueued
into items enqueued while computing the heuris-
tic (not strictly part of the algorithm), inside items
(?bottom-up?), and derivation and outside items
(together ?k-best?). The results are shown in Fig-
ure 3. The cost of k-best extraction is clearly
dwarfed by the the 1-best computation in both
cases. However, KA? is significantly faster over
the bottom-up computations, even when the cost
of computing the heuristic is included.
3.2 Lexicalized Parsing
We also experimented with the lexicalized parsing
model described in Klein and Manning (2003b).
This model is constructed as the product of a
dependency model and the unlexicalized PCFG
model in Klein and Manning (2003a). We
964
0 2000 4000 6000 8000 10000
05
00
1500
2500
KA*
k
Items 
pushed
 (milli
ons) K BestBottom-up
Heuristic
0 2000 4000 6000 8000 10000
05
00
1500
2500
EXH
k
Items 
pushed
 (milli
ons) K BestBottom-up
Figure 5: k-best extraction as a function of k for tree transducer grammars, for both KA? and EXH.
constructed these grammars using the Stanford
Parser.6 The model was trained on Sections 2-20
of the Penn Treebank and tested on 100 sentences
of Section 21 of length at most 30 words.
For this grammar, Klein and Manning (2003b)
showed that a very accurate heuristic can be con-
structed by taking the sum of outside scores com-
puted with the dependency model and the PCFG
model individually. We report performance as a
function of k for KA? in Figure 4. Both NAIVE
and EXH are impractical on these grammars due
to memory limitations. For KA?, computing the
heuristic is the bottleneck, after which bottom-up
parsing and k-best extraction are very fast.
3.3 Tree Transducer Grammars
Syntactic machine translation (Galley et al, 2004)
uses tree transducer grammars to translate sen-
tences. Transducer rules are synchronous context-
free productions that have both a source and a tar-
get side. We examine the cost of k-best parsing in
the source side of such grammars with KA?, which
can be a first step in translation.
We extracted a grammar from 220 million
words of Arabic-English bitext using the approach
of Galley et al (2006), extracting rules with at
most 3 non-terminals. These rules are highly lex-
icalized. About 300K rules are applicable for a
typical 30-word sentence; we filter the rest. We
tested on 100 sentences of length at most 40 from
the NIST05 Arabic-English test set.
We used a simple but effective heuristic for
these grammars, similar to the FILTER heuristic
suggested in Klein and Manning (2003c). We pro-
jected the source projection to a smaller grammar
by collapsing all non-terminal symbols to X, and
6http://nlp.stanford.edu/software/
also collapsing pre-terminals into related clusters.
For example, we collapsed the tags NN, NNS,
NNP, and NNPS to N. This projection reduced
the number of grammar symbols from 149 to 36.
Using it as a heuristic for the full grammar sup-
pressed ? 60% of the total items (Figure 5).
4 Related Work
While formulated very differently, one limiting
case of our algorithm relates closely to the EXH
algorithm of Huang and Chiang (2005). In par-
ticular, if all inside items are processed before any
derivation items, the subsequent number of deriva-
tion items and outside items popped by KA? is
nearly identical to the number popped by EXH in
our experiments (both algorithms have the same
ordering bounds on which derivation items are
popped). The only real difference between the al-
gorithms in this limited case is that EXH places
k-best items on local priority queues per edge,
while KA? makes use of one global queue. Thus,
in addition to providing a method for speeding
up k-best extraction with A?, our algorithm also
provides an alternate form of Huang and Chiang
(2005)?s k-best extraction that can be phrased in a
weighted deduction system.
5 Conclusions
We have presented KA?, an extension of A? pars-
ing that allows extraction of optimal k-best parses
without the need for an exhaustive 1-best pass. We
have shown in several domains that, with an ap-
propriate heuristic, our algorithm can extract k-
best lists in a fraction of the time required by cur-
rent approaches to k-best extraction, giving the
best of both A? parsing and efficient k-best extrac-
tion, in a unified procedure.
965
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
Seventeenth International Conference on Machine
Learning (ICML).
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Human Language Technologies: The An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
ACL).
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The
Annual Conference of the Association for Compu-
tational Linguistics (ACL).
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D.
thesis, Harvard University.
Aria Haghighi, John DeNero, and Dan Klein. 2007.
Approximate factoring for A* search. In Proceed-
ings of HLT-NAACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53?64.
Liang Huang. 2005. Unpublished manuscript.
http://www.cis.upenn.edu/?lhuang3/
knuth.pdf.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In IWPT, pages 123?134.
Dan Klein and Chris Manning. 2002. Fast exact in-
ference with a factored model for natural language
processing,. In Proceedings of NIPS.
Dan Klein and Chris Manning. 2003a. Accurate unlex-
icalized parsing. In Proceedings of the North Amer-
ican Chapter of the Association for Computational
Linguistics (NAACL).
Dan Klein and Chris Manning. 2003b. Factored A*
search for models over sequences and trees. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence (IJCAI).
Dan Klein and Christopher D. Manning. 2003c. A*
parsing: Fast exact Viterbi parse selection. In
In Proceedings of the Human Language Technol-
ogy Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
Donald Knuth. 1977. A generalization of Dijkstra?s
algorithm. Information Processing Letters, 6(1):1?
5.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of The Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL), pages 152?159.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computationl Linguis-
tics, 29(1):135?143.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. In Ma-
chine Learning, volume 34, pages 151?5175.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
966
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 141?144,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Asynchronous Binarization for Synchronous Grammars
John DeNero, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, adpauls, klein}@cs.berkeley.edu
Abstract
Binarization of n-ary rules is critical for the effi-
ciency of syntactic machine translation decoding.
Because the target side of a rule will generally
reorder the source side, it is complex (and some-
times impossible) to find synchronous rule bina-
rizations. However, we show that synchronous
binarizations are not necessary in a two-stage de-
coder. Instead, the grammar can be binarized one
way for the parsing stage, then rebinarized in a
different way for the reranking stage. Each indi-
vidual binarization considers only one monolin-
gual projection of the grammar, entirely avoid-
ing the constraints of synchronous binarization
and allowing binarizations that are separately op-
timized for each stage. Compared to n-ary for-
est reranking, even simple target-side binariza-
tion schemes improve overall decoding accuracy.
1 Introduction
Syntactic machine translation decoders search
over a space of synchronous derivations, scoring
them according to both a weighted synchronous
grammar and an n-gram language model. The
rewrites of the synchronous translation gram-
mar are typically flat, n-ary rules. Past work
has synchronously binarized such rules for effi-
ciency (Zhang et al, 2006; Huang et al, 2008).
Unfortunately, because source and target orders
differ, synchronous binarizations can be highly
constrained and sometimes impossible to find.
Recent work has explored two-stage decoding,
which explicitly decouples decoding into a source
parsing stage and a target language model inte-
gration stage (Huang and Chiang, 2007). Be-
cause translation grammars continue to increase
in size and complexity, both decoding stages re-
quire efficient approaches (DeNero et al, 2009).
In this paper, we show how two-stage decoding
enables independent binarizations for each stage.
The source-side binarization guarantees cubic-
time construction of a derivation forest, while an
entirely different target-side binarization leads to
efficient forest reranking with a language model.
Binarizing a synchronous grammar twice inde-
pendently has two principal advantages over syn-
chronous binarization. First, each binarization can
be fully tailored to its decoding stage, optimiz-
ing the efficiency of both parsing and language
model reranking. Second, the ITG constraint on
non-terminal reordering patterns is circumvented,
allowing the efficient application of synchronous
rules that do not have a synchronous binarization.
The primary contribution of this paper is to es-
tablish that binarization of synchronous grammars
need not be constrained by cross-lingual reorder-
ing patterns. We also demonstrate that even sim-
ple target-side binarization schemes improve the
search accuracy of forest reranking with a lan-
guage model, relative to n-ary forest reranking.
2 Asynchronous Binarization
Two-stage decoding consists of parsing and lan-
guage model integration. The parsing stage builds
a pruned forest of derivations scored by the trans-
lation grammar only. In the second stage, this for-
est is reranked by an n-gram language model. We
rerank derivations with cube growing, a lazy beam
search algorithm (Huang and Chiang, 2007).
In this paper, we focus on syntactic translation
with tree-transducer rules (Galley et al, 2006).
These synchronous rules allow multiple adjacent
non-terminals and place no restrictions on rule size
or lexicalization. Two example unlexicalized rules
appear in Figure 1, along with aligned and parsed
training sentences that would have licensed them.
2.1 Constructing Translation Forests
The parsing stage builds a forest of derivations by
parsing with the source-side projection of the syn-
chronous grammar. Each forest node P
ij
com-
pactly encodes all parse derivations rooted by
grammar symbol P and spanning the source sen-
tence from positions i to j. Each derivation of P
ij
is rooted by a rule with non-terminals that each
141
?PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
(a)
(b)
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
yo    ayer    com?    en casa
I       ate   yesterday   at home 
PRP  VBD       NN           PP
S
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
  NN
2
   PP
4
S ?
Figure 1: Two unlexicalized transducer rules (top) and
aligned, parsed training sentences from which they could be
extracted (bottom). The internal structure of English parses
has been omitted, as it is irrelevant to our decoding problem.
anchor to some child nodeC
(t)
k`
, where the symbol
C
(t)
is the tth child in the source side of the rule,
and i ? k < ` ? j.
We build this forest with a CKY-style algorithm.
For each span (i, j) from small to large, and each
symbol P , we iterate over all ways of building a
node P
ij
, first considering all grammar rules with
parent symbol P and then, for each rule, consider-
ing all ways of anchoring its non-terminals to ex-
isting forest nodes. Because we do not incorporate
a language model in this stage, we need only oper-
ate over the source-side projection of the grammar.
Of course, the number of possible anchorings
for a rule is exponential in the number of non-
terminals it contains. The purpose of binarization
during the parsing pass is to make this exponential
algorithm polynomial by reducing rule branching
to at most two non-terminals. Binarization reduces
algorithmic complexity by eliminating redundant
work: the shared substructures of n-ary rules are
scored only once, cached, and reused. Caching is
also commonplace in Early-style parsers that im-
plicitly binarize when applying n-ary rules.
While any binarization of the source side will
give a cubic-time algorithm, the particulars of a
grammar transformation can affect parsing speed
substantially. For instance, DeNero et al (2009)
describe normal forms particularly suited to trans-
ducer grammars, demonstrating that well-chosen
binarizations admit cubic-time parsing algorithms
while introducing very few intermediate grammar
symbols. Binarization choice can also improve
monolingual parsing efficiency (Song et al, 2008).
The parsing stage of our decoder proceeds
by first converting the source-side projection of
the translation grammar into lexical normal form
(DeNero et al, 2009), which allows each rule to
be applied to any span in linear time, then build-
ing a binary-branching translation forest, as shown
in Figure 2(a). The intermediate nodes introduced
during this transformation do not have a target-
side projection or interpretation. They only exist
for the sake of source-side parsing efficiency.
2.2 Collapsing Binarization
To facilitate a change in binarization, we transform
the translation forest into n-ary form. In the n-ary
forest, each hyperedge corresponds to an original
grammar rule, and all nodes correspond to original
grammar symbols, rather than those introduced
during binarizaiton. Transforming the entire for-
est to n-ary form is intractable, however, because
the number of hyperedges would be exponential in
n. Instead, we include only the top k n-ary back-
traces for each forest node. These backtraces can
be enumerated efficiently from the binary forest.
Figure 2(b) illustrates the result.
For efficiency, we follow DeNero et al (2009)
in pruning low-scoring nodes in the n-ary for-
est under the weighted translation grammar. We
use a max-marginal threshold to prune unlikely
nodes, which can be computed through a max-
sum semiring variant of inside-outside (Goodman,
1996; Petrov and Klein, 2007).
Forest reranking with a language model can be
performed over this n-ary forest using the cube
growing algorithm of Huang and Chiang (2007).
Cube growing lazily builds k-best lists of deriva-
tions at each node in the forest by filling a node-
specific priority queue upon request from the par-
ent. N -ary forest reranking serves as our baseline.
2.3 Reranking with Target-Side Binarization
Zhang et al (2006) demonstrate that reranking
over binarized derivations improves search accu-
racy by better exploring the space of translations
within the strict confines of beam search. Binariz-
ing the forest during reranking permits pairs of ad-
jacent non-terminals in the target-side projection
of rules to be rescored at intermediate forest nodes.
This target-side binarization can be performed on-
the-fly: when a node P
ij
is queried for its k-best
list, we binarize its n-ary backtraces.
Suppose P
ij
can be constructed from a rule r
with target-side projection
P ? `
0
C
1
`
1
C
2
`
2
. . . C
n
`
n
where C
1
, . . . , C
n
are non-terminal symbols that
are each anchored to a nodeC
(i)
kl
in the forest, and
`
i
are (possibly empty) sequences of lexical items.
142
yo ayer com? en casa
S
PRP+NN+VBD
PRP+NN
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
PRP+VBD+NN
PRP+VBD
?I ate?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    NN
2
]  PP
4
S ?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    PP
4
]  NN
2
S ?
(a) Parsing stage binarization (b) Collapsed n-ary forest (c) Reranking stage binarization
PRP+VBD+PP
Figure 2: A translation forest as it evolves during two-stage decoding, along with two n-ary rules in the forest that are rebi-
narized. (a) A source-binarized forest constructed while parsing the source sentence with the translation grammar. (b) A flat
n-ary forest constructed by collapsing out the source-side binarization. (c) A target-binarized forest containing two derivations
of the root symbol?the second is dashed for clarity. Both derivations share the node PRP+VBD, which will contain a single
k-best list of translations during language model reranking. One such translation of PRP+VBD is shown: ?I ate?.
We apply a simple left-branching binarization to
r, though in principle any binarization is possible.
We construct a new symbol B and two new rules:
r
1
: B ? `
0
C
1
`
1
C
2
`
2
r
2
: P ? B C
3
`
3
. . . C
n
`
n
These rules are also anchored to forest nodes. Any
C
i
remains anchored to the same node as it was in
the n-ary forest. For the new symbol B, we intro-
duce a new forest nodeB that does not correspond
to any particular span of the source sentence. We
likewise transform the resulting r
2
until all rules
have at most two non-terminal items. The original
rule r from the n-ary forest is replaced by binary
rules. Figure 2(c) illustrates the rebinarized forest.
Language model reranking treats the newly in-
troduced forest nodeB as any other node: building
a k-best derivation list by combining derivations
from C
(1)
and C
(2)
using rule r
1
. These deriva-
tions are made available to the parent of B, which
may be another introduced node (if more binariza-
tion were required) or the original root P
ij
.
Crucially, the ordering of non-terminals in the
source-side projection of r does not play a role
in this binarization process. The intermediate
nodes B may comprise translations of discontigu-
ous parts of the source sentence, as long as those
parts are contained within the span (i, j).
2.4 Reusing Intermediate Nodes
The binarization we describe transforms the for-
est on a rule-by-rule basis. We must consider in-
dividual rules because they may contain different
lexical items and non-terminal orderings. How-
ever, two different rules that can build a node often
share some substructures. For instance, the two
rules in Figure 2 both begin with PRP followed by
VBD. In addition, these symbols are anchored to
the same source-side spans. Thus, binarizing both
rules yields the same intermediate forest node B.
In the case where two intermediate nodes share
the same intermediate rule anchored to the same
forest nodes, they can be shared. That is, we need
only generate one k-best list of derivations, then
use it in derivations rooted by both rules. Sharing
derivation lists in this way provides an additional
advantage of binarization over n-ary forest rerank-
ing. Not only do we assess language model penal-
ties over smaller partial derivations, but repeated
language model evaluations are cached and reused
across rules with common substructure.
3 Experiments
The utility of binarization for parsing is well
known, and plays an important role in the effi-
ciency of the parsing stage of decoding (DeNero et
al., 2009). The benefit of binarization for language
143
Forest Reranked BLEU Model Score
N -ary baseline 58.2 41,543
Left-branching binary 58.5 41,556
Table 1: Reranking a binarized forest improves BLEU by 0.3
and model score by 13 relative to an n-ary forest baseline by
reducing search errors during forest rescoring.
model reranking has also been established, both
for synchronous binarization (Zhang et al, 2006)
and for target-only binarization (Huang, 2007). In
our experiment, we evaluate the benefit of target-
side forest re-binarization in the two-stage decoder
of DeNero et al (2009), relative to reranking n-ary
forests directly.
We translated 300 NIST 2005 Arabic sentences
to English with a large grammar learned from a
220 million word bitext, using rules with up to 6
non-terminals. We used a trigram language model
trained on the English side of this bitext. Model
parameters were tuned withMERT. Beam size was
limited to 200 derivations per forest node.
Table 1 shows a modest increase in model
and BLEU score from left-branching binarization
during language model reranking. We used the
same pruned n-ary forest from an identical parsing
stage in both conditions. Binarization did increase
reranking time by 25% because more k-best lists
are constructed. However, reusing intermediate
edges during reranking binarization reduced bina-
rized reranking time by 37%. We found that on
average, intermediate nodes introduced in the for-
est are used in 4.5 different rules, which accounts
for the speed increase.
4 Discussion
Asynchronous binarization in two-stage decoding
allows us to select an appropriate grammar trans-
formation for each language. The source trans-
formation can optimize specifically for the parsing
stage of translation, while the target-side binariza-
tion can optimize for the reranking stage.
Synchronous binarization is of course a way to
get the benefits of binarizing both grammar pro-
jections; it is a special case of asynchronous bi-
narization. However, synchronous binarization is
constrained by the non-terminal reordering, lim-
iting the possible binarization options. For in-
stance, none of the binarization choices used in
Figure 2 on either side would be possible in a
synchronous binarization. There are rules, though
rare, that cannot be binarized synchronously at all
(Wu, 1997), but can be incorporated in two-stage
decoding with asynchronous binarization.
On the source side, these limited binarization
options may, for example, prevent a binarization
that minimizes intermediate symbols (DeNero et
al., 2009). On the target side, the speed of for-
est reranking depends upon the degree of reuse
of intermediate k-best lists, which in turn depends
upon the manner in which the target-side grammar
projection is binarized. Limiting options may pre-
vent a binarization that allows intermediate nodes
to be maximally reused. In future work, we look
forward to evaluating the wide array of forest bi-
narization strategies that are enabled by our asyn-
chronous approach.
References
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of the Annual Conference of the North American
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syn-
tactic translation models. In Proceedings of the Annual
Conference of the Association for Computational Linguis-
tics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In Pro-
ceedings of the Annual Conference of the Association for
Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight.
2008. Binarization of synchronous context-free gram-
mars. Computational Linguistics.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proceedings of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST).
Slav Petrov and Dan Klein. 2007. Improved inference for un-
lexicalized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguistics.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better
binarization for the CKY parsing. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation.
In Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics.
144
Multi-Document Summarization of Evaluative Text
Giuseppe Carenini, Raymond Ng, and Adam Pauls
Deptartment of Computer Science
University of British Columbia Vancouver, Canada
 
carenini,rng,adpauls  @cs.ubc.ca
Abstract
We present and compare two approaches
to the task of summarizing evaluative ar-
guments. The first is a sentence extraction-
based approach while the second is a lan-
guage generation-based approach. We
evaluate these approaches in a user study
and find that they quantitatively perform
equally well. Qualitatively, however, we
find that they perform well for different but
complementary reasons. We conclude that
an effective method for summarizing eval-
uative arguments must effectively synthe-
size the two approaches.
1 Introduction
Many organizations are faced with the challenge
of summarizing large corpora of text data. One im-
portant application is evaluative text, i.e. any doc-
ument expressing an evaluation of an entity as ei-
ther positive or negative. For example, many web-
sites collect large quantities of online customer re-
views of consumer electronics. Summaries of this
literature could be of great strategic value to prod-
uct designers, planners and manufacturers. There
are other equally important commercial applica-
tions, such as the summarization of travel logs, and
non-commercial applications, such as the summa-
rization of candidate reviews.
The general problem we consider in this paper
is how to effectively summarize a large corpora of
evaluative text about a single entity (e.g., a prod-
uct). In contrast, most previous work on multi-
document summarization has focused on factual
text (e.g., news (McKeown et al, 2002), biogra-
phies (Zhou et al, 2004)). For factual documents,
the goal of a summarizer is to select the most im-
portant facts and present them in a sensible or-
dering while avoiding repetition. Previous work
has shown that this can be effectively achieved by
carefully extracting and ordering the most infor-
mative sentences from the original documents in
a domain-independent way. Notice however that
when the source documents are assumed to con-
tain inconsistent information (e.g., conflicting re-
ports of a natural disaster (White et al, 2002)),
a different approach is needed. The summarizer
needs first to extract the information from the doc-
uments, then process such information to identify
overlaps and inconsistencies between the different
sources and finally produce a summary that points
out and explain those inconsistencies.
A corpus of evaluative text typically contains a
large number of possibly inconsistent ?facts? (i.e.
opinions), as opinions on the same entity feature
may be uniform or varied. Thus, summarizing a
corpus of evaluative text is much more similar to
summarizing conflicting reports than a consistent
set of factual documents. When there are diverse
opinions on the same issue, the different perspec-
tives need to be included in the summary.
Based on this observation, we argue that any
strategy to effectively summarize evaluative text
about a single entity should rely on a preliminary
phase of information extraction from the target
corpus. In particular, the summarizer should at
least know for each document: what features of
the entity were evaluated, the polarity of the eval-
uations and their strengths.
In this paper, we explore this hypothesis by con-
sidering two alternative approaches. First, we de-
veloped a sentence-extraction based summarizer
that uses the information extracted from the cor-
pus to select and rank sentences from the corpus.
We implemented this system, called MEAD*, by
305
adapting MEAD (Radev et al, 2003), an open-
source framework for multi-document summariza-
tion. Second, we developed a summarizer that
produces summaries primarily by generating lan-
guage from the information extracted from the
corpus. We implemented this system, called the
Summarizer of Evaluative Arguments (SEA), by
adapting the Generator of Evaluative Arguments
(GEA) (Carenini and Moore, expected 2006) a
framework for generating user tailored evaluative
arguments.
We have performed an empirical formative eval-
uation of MEAD* and SEA in a user study. In
this evaluation, we also tested the effectiveness of
human generated summaries (HGS) as a topline
and of summaries generated by MEAD without
access to the extracted information as a baseline.
The results indicate that SEA and MEAD* quan-
titatively perform equally well above MEAD and
below HGS. Qualitatively, we find that they per-
form well for different but complementary rea-
sons. While SEA appears to provide a more gen-
eral overview of the source text, MEAD* seems to
provide a more varied language and detail about
customer opinions.
2 Information Extraction from
Evaluative Text
2.1 Feature Extraction
Knowledge extraction from evaluative text about
a single entity is typically decomposed into three
distinct phases: the determination of features of
the entity evaluated in the text, the strength of
each evaluation, and the polarity of each evalua-
tion. For instance, the information extracted from
the sentence ?The menus are very easy to navi-
gate but the user preference dialog is somewhat
difficult to locate.? should be that the ?menus?
and the ?user preference dialog? features are eval-
uated, and that the ?menus? receive a very posi-
tive evaluation while the ?user preference dialog?
is evaluated rather negatively.
For these tasks, we adopt the approach de-
scribed in detail in (Carenini et al, 2005). This ap-
proach relies on the work of (Hu and Liu, 2004a)
for the tasks of strength and polarity determina-
tion. For the task of feature extraction, it en-
hances earlier work (Hu and Liu, 2004c) by map-
ping the extracted features into a hierarchy of fea-
tures which describes the entity of interest. The re-
sulting mapping reduces redundancy and provides
conceptual organization of the extracted features.
Camera
Lens
Digital Zoom
Optical Zoom
. . .
Editing/Viewing
Viewfi nder
. . .
Flash
. . .
Image
Image Type
TIFF
JPEG
. . .
Resolution
Effective Pixels
Aspect Ratio
. . .
Figure 1: Partial view of UDF taxonomies for a
digital camera.
Before continuing, we shall describe the ter-
minology we use when discussing the extracted
knowledge. The features evaluated in a corpus of
reviews and extracted by following Hu and Liu?s
approach are called Crude Features.
CF  

c f j  j   1  n
For example, crude features for a digital cam-
era might include ?picture quality?, ?viewfinder?,
and ?lens?. Each sentence sk in the corpus con-
tains a set of evaluations (of crude features) called
eval  sk  . Each evaluation contains both a polar-
ity and a strength represented as an integer in the
range 	 3 
 2 
 1 
 1 
 2 
 3  where  3 is the
most positive possible evaluation and  3 is the
most negative possible evaluation.
There is also a hierarchical set of possibly more
abstract user-defined features 1
UDF  

ud fi  i   1  m
See Figure 1 for a sampleUDF. The process of hi-
erarchically organizing the extracted features pro-
duces a mapping from CF to UDF features (see
(Carenini et al, 2005) for details). We call the set
of crude features mapped to the user-defined fea-
ture ud fi map  ud fi  . For example, the crude fea-
tures ?unresponsiveness?, ?delay?, and ?lag time?
would all be mapped to the ud f ?delay between
shots?.
For each c f j, there is a set of polarity and
strength evaluations ps  c f j  corresponding to
each evaluation of c f j in the corpus. We call the
set of polarity/strength evaluations directly associ-
ated with ud fi
PSi   
c f j?map  ud fi 
ps  c f j 
The total set of polarity/strength evaluations as-
sociated with ud fi, including its descendants, is
1We call them here user-defi ned features for consistency
with (Carenini et al, 2005). In this paper, they are not as-
sumed to be and are not in practice defi ned by the user.
306
TPSi   PSi 
 


ud fk?desc  ud fi 
PSk 

where desc  ud fi  refers to all descendants of ud fi.
3 MEAD*: Sentence Extraction
Most modern summarization systems use sen-
tences extracted from the source text as the ba-
sis for summarization (see (Nat, 2005b) for a rep-
resentative sample). Extraction-based approaches
have the advantage of avoiding the difficult task
of natural language generation, thus maintaining
domain-independence because the system need
not be aware of specialized vocabulary for its tar-
get domain. The main disadvantage of extraction-
based approaches is the poor linguistic coherence
of the extracted summaries.
Because of the widespread and well-developed
use of sentence extractors in summarization, we
chose to develop our own sentence extractor as
a first attempt at summarizing evaluative argu-
ments. To do this, we adapted MEAD (Radev et
al., 2003), an open-source framework for multi-
document summarization, to suit our purposes.
We refer to our adapted version of MEAD as
MEAD*. The MEAD framework decomposes
sentence extraction into three steps: (i) Feature
Calculation: Some numerical feature(s) are cal-
culated for each sentence, for example, a score
based on document position and a score based on
the TF*IDF of a sentence. (ii) Classification: The
features calculated during step (i) are combined
into a single numerical score for each sentence.
(iii) Reranking: The numerical score for each sen-
tence is adjusted relative to other sentences. This
allows the system to avoid redundancy in the final
set of sentences by lowering the score of sentences
which are similar to already selected sentences.
We found from early experimentation that
the most informative sentences could be accu-
rately determined by examining the extractedCFs.
Thus, we created our own sentence-level feature
based on the number, strength, and polarity ofCFs
extracted for each sentence.
CF sum  sk    ?
psi? eval  sk 
psi

During system development, we found this
measure to be effective because it was sensitive
to the number of CFs mentioned in a given sen-
tence as well as to the strength of the evaluation for
each CF . However, many sentences may have the
same CF sum score (especially sentences which
contain an evaluation for only one CF). In such
cases, we used the MEAD 3.072 centroid feature
as a ?tie-breaker?. The centroid is a common fea-
ture in multidocument summarization (cf. (Radev
et al, 2003), (Saggion and Gaizauskas, 2004)).
At the reranking stage, we adopted a different
algorithm than the default in MEAD. We placed
each sentence which contained an evaluation of a
given CF into a ?bucket? for that CF . Because a
sentence could contain more than one CF , a sen-
tence could be placed in multiple buckets. We
then selected the top-ranked sentence from each
bucket, starting with the bucket containing the
most sentences (largest

ps  c f j 

), never selecting
the same sentence twice. Once one sentence had
been selected from each bucket, the process was
repeated3. This selection algorithm accomplishes
two important tasks: firstly, it avoids redundancy
by only selecting one sentence to represent each
CF (unless all other CFs have already been rep-
resented), and secondly, it gives priority to CFs
which are mentioned more frequently in the text.
The sentence selection algorithm permits us to
select an arbitrary number of sentences to fit a de-
sired word length. We then ordered the sentences
according to a primitive discourse planning strat-
egy in which the most general CF (i.e. the CF
mapped to the topmost node in the UDF) is dis-
cussed first. The remaining sentences were then
ordered according to a depth-first traversal of the
UDF hierarchy. In this way, general features are
followed immediately by their more specific chil-
dren in the hierarchy.
4 SEA: Natural Language Generation
The extraction-based approach described in the
previous section has several disadvantages. We al-
ready discussed problems with the linguistic co-
herence of the summary, but more specific prob-
lems arise in our particular task of summarizing
a corpus of evaluative text. Firstly, sentence ex-
traction does not give the reader any explicit infor-
mation about of the distribution of evaluations, for
example, how many users mentioned a given fea-
2The centroid calculation requires an IDF database. We
constructed an IDF database from several corpora of reviews
and a set of stop words.
3In practice the process would only be repeated in sum-
maries long enough to contain sentences for each CF, which
is very rare.
307
ture and whether user opinions were uniform or
varied. It also does not give an aggregate view of
user evaluations because typically it only presents
one evaluation for each CF . It may be that a very
positive evaluation for oneCF was selected for ex-
traction, even though most evaluations were only
somewhat positive and some were even negative.
We thus also developed a system, SEA, that
presents such information in generated natural lan-
guage. This system calculates several important
characteristics of the source corpus by aggregat-
ing the extracted information including the CF to
UDF mapping. We first describe these character-
istics and then discuss their presentation in natural
language.
4.1 Aggregation of Extracted Information
In order to provide an aggregate view of the eval-
uation expressed in a corpus of evaluative text a
summarizer should at least determine: (i) which
features of the evaluated entity were most ?impor-
tant? to the users (ii) some aggregate of the user
opinions for the important features (iii) the distri-
bution of those opinions and (iv) the reasons be-
hind each user opinion. We now discuss each of
these aspects in detail.
4.1.1 Feature Selection
We approach the task of selecting the most ?im-
portant? features by defining a ?measure of impor-
tance? for each feature of the evaluated entity. We
define the ?direct importance? of a feature in the
UDF as
dir moi  ud fi    ?
psk?PSi

psk

2
where by ?direct? we mean the importance de-
rived only from that feature and not from its chil-
dren. This metric produces high scores for fea-
tures which either occur frequently in the corpus
or have strong evaluations (or both). This ?direct?
measure of importance, however, is incomplete, as
each non-leaf node in the UDF effectively serves
a dual purpose. It is both a feature upon which
a user might comment and a category for group-
ing its sub-features. Thus, a non-leaf node should
be important if either its children are important or
the node itself is important (or both). To this end,
we have defined the total measure of importance
moi  ud fi  as
  

 

dir moi  ud fi  ch  ud fi    /0
 ? dir moi  ud fi  
 1  ?

?ud fk?ch  ud fi  moi  ud fk   otherwise
where ch  ud fi  refers to the children of ud fi in
the hierarchy and ? is some real parameter in the
range  0  5 
 1  . In this measure, the importance of a
node is a combination of its direct importance and
of the importance of its children. The parameter
? may be adjusted to vary the relative weight of
the parent and children. We used ?   0  9 for our
experiments. This setting resulted in more infor-
mative summaries during system development.
In order to perform feature selection using this
metric, we must also define a selection procedure.
The most obvious is a simple greedy selection ?
sort the nodes in the UDF by the measure of im-
portance and select the most important node until
a desired number of features is included. How-
ever, because a node derives part of its ?impor-
tance? from its children, it is possible for a node?s
importance to be dominated by one or more of its
children. Including both the child and parent node
would be redundant because most of the informa-
tion is contained in the child. We thus choose a
dynamic greedy selection algorithm in which we
recalculate the importance of each node after each
round of selection, with all previously selected
nodes removed from the tree. In this way, if a
node that dominates its parent?s importance is se-
lected, its parent?s importance will be reduced dur-
ing later rounds of selection. This approach mim-
ics the behaviour of several sentence extraction-
based summarizers (e.g. (Schiffman et al, 2002;
Saggion and Gaizauskas, 2004)) which define a
metric for sentence importance and then greed-
ily select the sentence which minimizes similarity
with already selected sentences and maximizes in-
formativeness.
4.1.2 Opinion Aggregation
We approach the task of aggregating opinions
from the source text in a similar fashion to de-
termining the measure of importance. We cal-
culate an ?orientation? for each UDF by aggre-
gating the polarity/strength evaluations of all re-
lated CFs into a single value. We define the ?di-
rect orientation? of a UDF as the average of the
strength/polarity evaluations of all related CFs
dir ori  ud fi    avg
psk?PSi
psk
308
As with our measure of importance, we must
also include the orientation of a feature?s children
in its orientation. Because a feature in the UDF
conceptually groups its children, the orientation of
a feature should include some information about
the orientation of its children. We thus define the
total orientation ori  ud fi  as
  

 

dir ori  ud fi  ch  ud fi    /0
 ? dir ori  ud fi  
 1  ?

avgud fk?ch  ud fi  ori  ud fk   otherwise
This metric produces a real number between  3
and  3 which serves as an aggregate of user opin-
ions for a feature. We use the same value of ? as
in moi  ud fi  .
4.1.3 Distribution of Opinions
Communicating user opinions to the reader is
not simply a matter of classifying each feature
as being evaluated negatively or positively ? the
reader may also want to know if all users evalu-
ated a feature in a similar way or if evaluations
were varied. We thus also need a method of de-
termining the modality of the distribution of user
opinions. We calculate the sum of positive polar-
ity/strength evaluations (or negative if ori  ud fi  is
negative) for a node and its children as a fraction
of all polarity/strength evaluations
?vi?   psk?TPSi  signum  psk  signum  ori  ud fi  

vi
?vi?TPSi

vi

If this fraction is very close to 0.5, this indicates
an almost perfect split of user opinions on that
features. So we classify the feature as ?bimodal?
and we report this fact to the user. Otherwise, the
feature is classified as ?unimodal?, i.e. we need
only to communicate one aggregate opinion to the
reader.
4.2 Generating Language: Adapting the
Generator of Evaluative Arguments
(GEA)
The first task in generating a natural language
summary from the information extracted from the
corpus is content selection. This task is accom-
plished in SEA by the feature selection strategy
described in Section 4.1.1. After content selection,
the automatic generation of a natural language
summary involves the following additional tasks
(Reiter and Dale, 2000): (i) structuring the content
by ordering and grouping the selected content ele-
ments as well as by specifying discourse relations
(e.g., supporting vs. opposing evidence) between
the resulting groups; (ii) microplanning, which
involves lexical selection and sentence planning;
and (iii) sentence realization, which produces En-
glish text from the output of the microplanner. For
most of these tasks, we have adapted the Genera-
tor of Evaluative Arguments (GEA) (Carenini and
Moore, expected 2006), a framework for generat-
ing user tailored evaluative arguments. For lack of
space we cannot discuss the details here. These
are provided on the online version of this paper,
which is available at the first author?s Web page.
That version also includes a detailed discussion of
related and future work.
5 Evaluation
We evaluated our two summarizers by performing
a user study in which four treatments were consid-
ered: SEA, MEAD*, human-written summaries
as a topline and summaries generated by MEAD
(with all options set to default) as a baseline.
5.1 The Experiment
Twenty-eight undergraduate students participated
in our experiment, seven for each treatment. Each
participant was given a set of 20 customer reviews
randomly selected from a corpus of reviews. In
each treatment three participants received reviews
from a corpus of 46 reviews of the Canon G3 dig-
ital camera and four received them from a cor-
pus of 101 reviews of the Apex 2600 Progressive
Scan DVD player, both obtained from Hu and Liu
(2004b). The reviews from these corpora which
serve as input to our systems have been manually
annotated with crude features, strength, and polar-
ity. We used this ?gold standard? for crude fea-
ture, strength, and polarity extraction because we
wanted our experiments to focus on our summary
and not be confounded by errors in the knowledge
extraction phase.
The participant was told to pretend that they
work for the manufacturer of the product (either
Canon or Apex). They were told that they would
have to provide a 100 word summary of the re-
views to the quality assurance department. The
purpose of these instructions was to prime the user
to the task of looking for information worthy of
summarization. They were then given 20 minutes
to explore the set of reviews.
After 20 minutes, the participant was asked to
stop. The participant was then given a set of in-
309
structions which explained that the company was
testing a computer-based system for automatically
generating a summary of the reviews s/he has
been reading. S/he was then shown a 100 word
summary of the 20 reviews generated either by
MEAD, MEAD*, SEA, or written by a human 4.
Figure 2 shows four summaries of the same 20 re-
views, one of each type.
In order to facilitate their analysis, summaries
were displayed in a web browser. The upper por-
tion of the browser contained the text of the sum-
mary with ?footnotes? linking to reviews on which
the summary was based. For MEAD and MEAD*,
for each sentence the footnote pointed to the re-
view from which the sentence had been extracted.
For SEA and human-generated summaries, for
each aggregate evaluation the footnote pointed to
the review containing a sample sentence on which
that evaluation was based. In all summaries, click-
ing on one of the footnotes caused the correspond-
ing review to be displayed in which the appropri-
ate sentence was highlighted.
Once finished, the participant was asked to fill
out a questionnaire assessing the summary along
several dimensions related to its effectiveness. The
participant could still access the summary while
s/he worked on the questionnaire.
Our questionnaire consisted of nine questions.
The first five questions were the SEE linguistic
well-formedness questions used at the 2005 Doc-
ument Understanding Conference (DUC) (Nat,
2005a). The next three questions were designed to
assess the content of the summary. We based our
questions on the Responsive evaluation at DUC
2005; however, we were interested in a more spe-
cific evaluation of the content that one overall
rank. As such, we split the content into the fol-
lowing three separate questions:
  (Recall) The summary contains all of the information
you would have included from the source text.
  (Precision) The summary contains no information you
would NOT have included from the source text.
  (Accuracy) All information expressed in the summary
accurately reflects the information contained in the
source text.
The final question in the questionnaire asked the
participant to rank the overall quality of the sum-
mary holistically.
4For automatically generated summaries, we generated
the longest possible summary with less than 100 words.
5.2 Quantitative Results
Table 1 consists of two parts. The first top half fo-
cuses on linguistic questions while the second bot-
tom half focuses on content issues. We performed
a two-way ANOVA test with summary type as
rows and the question sets as columns. Overall,
it is easy to conclude that MEAD* and SEA per-
formed at a roughly equal level, while the baseline
MEAD performed significantly lower and the Hu-
man summarizer significantly higher (p   001).
When individual questions/categories are consid-
ered, there are few questions that differentiate be-
tween MEAD* and SEA with a p-value below
0.05. The primary reason is our small sample size.
Nonetheless, if we relax the p-value threshold, we
can make the following observations/hypotheses.
To validate some of these hypotheses, we would
conduct a larger user study in future work.
On the linguistic side, the average
score suggests the ordering of: Human 

MEAD  
 SEA

 MEAD. Both MEAD* and
SEA are also on par with the median DUC score
(Nat, 2005b). On the focus question, in fact,
SEA?s score is tied with the Human?s score, which
may be a beneficial effect of the UDF guiding
content structuring in a top-down fashion. It
is also interesting to see that SEA outperforms
MEAD* on grammaticality, showing that the
generative text approach may be more effective
than simply extracting sentences on this aspect of
grammaticality. On the other hand, MEAD* out-
performs SEA on non-redundancy, and structure
and coherence. SEA?s disappointing performance
on structure and coherence was among the most
surprising finding. One possibility is that our
adaptation of GEA content structuring strategy
was suboptimal or even inappropriate. We plan to
investigate possible causes in the future.
On the content side, the average score sug-
gests the ordering of: Human  SEA  MEAD 
MEAD. As for the three individual content ques-
tions, on the recall one, both SEA and MEAD*
were dominated by the Human summarizer. This
indicates that both SEA and MEAD* omit some
features considered important. We feel that if a
longer summary was allowed, the gap between the
two and the Human summarizer would be nar-
rower. The precision question is somewhat sur-
prising in that SEA actually performs better than
the Human summarizer. In general this indicates
that the feature selection strategy was quite suc-
310
MEAD*: Bottom line , well made camera , easy to use , very flexible and powerful features to include the ability to use external flash and lense / fi lters
choices . 1It has a beautiful design , lots of features , very easy to use , very confi gurable and customizable , and the battery duration is amazing ! Great
colors , pictures and white balance. The camera is a dream to operate in automode , but also gives tremendous flexibility in aperture priority , shutter priority
, and manual modes . I ?d highly recommend this camera for anyone who is looking for excellent quality pictures and a combination of ease of use and the
flexibility to get advanced with many options to adjust if you like.
SEA: Almost all users loved the Canon G3 possibly because some users thought the physical appearance was very good. Furthermore, several users found
the manual features and the special features to be very good. Also, some users liked the convenience because some users thought the battery was excellent.
Finally, some users found the editing/viewing interface to be good despite the fact that several customers really disliked the viewfi nder . However, there
were some negative evaluations. Some customers thought the lens was poor even though some customers found the optical zoom capability to be excellent.
Most customers thought the quality of the images was very good.
MEAD: I am a software engineer and am very keen into technical details of everything i buy , i spend around 3 months before buying the digital camera ;
and i must say , g3 worth every single cent i spent on it . I do n?t write many reviews but i ?m compelled to do so with this camera . I spent a lot of time
comparing different cameras , and i realized that there is not such thing as the best digital camera . I bought my canon g3 about a month ago and i have to
say i am very satisfi ed .
Human: The Canon G3 was received exceedingly well. Consumer reviews from novice photographers to semi-professional all listed an impressive number
of attributes, they claim makes this camera superior in the market. Customers are pleased with the many features the camera offers, and state that the camera
is easy to use and universally accessible. Picture quality, long lasting battery life, size and style were all highlighted in glowing reviews. One flaw in the
camera frequently mentioned was the lens which partially obsructs the view through the view fi nder, however most claimed it was only a minor annoyance
since they used the LCD sceen.
Figure 2: Sample automatically generated summaries.
SEA MEAD* MEAD Human DUC
Question Avg. Dev. Avg. Dev. Avg. Dev. Avg. Dev. Med. Min. Max.
Grammaticality 3.43 1.13 2.71 0.76 3.14 0.90 4.29 0.76 3.86 2.60 4.34
Non-redundancy 3.14 1.57 3.86 0.90 3.57 0.98 4.43 1.13 4.44 3.96 4.74
Referential clarity 3.86 0.69 4.00 1.15 3.00 1.15 4.71 0.49 2.98 2.16 4.14
Focus 4.14 0.69 3.71 1.60 2.29 1.60 4.14 0.69 3.16 2.38 3.94
Structure and Coherence 2.29 0.95 3.00 1.41 1.86 0.90 4.43 0.53 2.10 1.60 3.24
Linguistic Average 3.37 1.19 3.46 1.24 2.77 1.24 4.4 0.74 3.31 2.54 4.08
Recall 2.33 1.03 2.57 0.98 1.57 0.53 3.57 1.27 ? ? ?
Precision 4.17 1.17 3.50 1.38 2.17 1.17 3.86 1.07 ? ? ?
Accuracy 4.00 0.82 3.57 1.13 2.57 1.4 4.29 0.76 ? ? ?
Content Average 3.5 1.26 3.21 1.2 2.1 1.12 3.9 1.04 ? ? ?
Overall 3.14 0.69 3.14 1.21 2.14 1.21 4.43 0.79 ? ? ?
Macro Average 3.39 0.73 3.34 0.51 2.48 0.65 4.24 0.34 ? ? ?
Table 1: Quantative results of user responses to our questionnaire on a scale from 1 (Strongly Disagree)
to 5 (Strongly Agree).
cessful. Finally, for the accuracy question, SEA is
closer to the Human summarizer than MEAD*. In
sum, recall that for evaluative text, it is very pos-
sible that different reviews express different opin-
ions on the same question. Thus, for the summa-
rization of evaluative text, when there is a differ-
ence in opinions, it is desirable that the summary
accurately covers both angles or conveys the dis-
agreement. On this count, according to the scores
on the precision and accuracy questions, SEA ap-
pears to outperform MEAD*.
5.3 Qualitative Results
MEAD*: The most interesting aspect of the
comments made by participants who evaluated
MEAD*-based summaries was that they rarely
criticized the summary for being nothing more
than a set of extracted sentences. For example,
one user claimed that the summary had a ?simple
sentence first, then ideas are fleshed out, and ends
with a fun impact statement?. Other users, while
noticing that the summary was solely quotation,
still felt the summary was adequate (?Shouldn?t
just copy consumers . . . However, it summarized
various aspects of the consumer?s opinions . . . ?).
With regard to content, two main complaints by
participants were: (i) the summary did not reflect
overall opinions (e.g., included positive evalua-
tions of the DVD player even though most eval-
uations were negative), and (ii) the evaluations
of some features were repeated. The first com-
plaint is consistent with the relatively low score of
MEAD* on the accuracy question.
We could address this complaint by only includ-
ing sentences whose CF evaluations have polari-
ties matching the majority polarity for each CF .
The second complaint could be avoided by not
selecting sentences which contain evaluations of
CFs already in the summary.
SEA: Comments about the structure of the sum-
maries generated by SEAmentioned the ?coherent
but robotic? feel of the summaries, the repetition
of ?users/customers? and lack of pronoun use, the
lack of flow between sentences, and the repeated
use of generic terms such as ?good?. These prob-
lems are largely a result of simplistic microplan-
ning and seems to contradict SEA?s disappointing
performance on the structure and coherence ques-
311
tion.
In terms of content, there were two main sets of
complaints. Firstly, participants wanted more ?de-
tails? in the summary, for instance, they wanted
examples of the ?manual features? mentioned by
SEA. Note that this is one complaint absent from
the MEAD* summaries. That is, where the
MEAD* summaries lack structure but contain de-
tail, SEA summaries provide a general, structured
overview while lacking in specifics.
The other set of complaints related to the prob-
lem that participants disagreed with the choice of
features in the summary. We note that this is actu-
ally a problem common to MEAD* and even the
Human summarizer. The best example to illus-
trate this point is on the ?physical appearance? of
the digital camera. One reason participants may
have disagreed with the summarizer?s decision to
include the physical appearance in the summary
is that some evaluations of the physical appear-
ance were quite subtle. For example, the sentence
?This camera has a design flaw? was annotated in
our corpus as evaluating the physical appearance,
although not all readers would agree with that an-
notation.
6 Conclusions
We have presented and compared a sentence
extraction- and language generation based ap-
proach to summarizing evaluative text. A forma-
tive user study of our MEAD* and SEA summa-
rizers found that, quantitatively, they performed
equally well relative to each other, while signifi-
cantly outperforming a baseline standard approach
to multidocument summarization. Trends that we
identified in the results as well as qualitative com-
ments from participants in the user study indicate
that the summarizers have different strengths and
weaknesses. On the one hand, though providing
varied language and detail about customer opin-
ions, MEAD* summaries lack in accuracy and
precision, failing to give and overview of the opin-
ions expressed in the evaluative text. On the other,
SEA summaries provide a general overview of the
source text, while sounding ?robotic?, repetitive,
and surprisingly rather incoherent.
Some of these differences are, fortunately, quite
complimentary. We plan in the future to investi-
gate how SEA and MEAD* can be integrated and
improved.
References
G. Carenini and J. D. Moore. expected 2006. Generat-
ing and evaluating evaluative arguments. AI Journal
(accepted for publication, contact fi rst author for a
draft).
G. Carenini, R.T Ng, and E. Zwart. 2005. Extracting
knowlede from evaluative text. In Proc. Third Inter-
national Conference on Knowledge Capture.
M. Hu and B. Liu. 2004a. Mining and summariz-
ing customer reviews. In Proc. of the 10th ACM
SIGKDD Conf., pages 168?177, New York, NY,
USA. ACM Press.
Minqing Hu and Bing Liu. 2004b. Fea-
ture based summary of customer reviews dataset.
http://www.cs.uic.edu/ liub/FBS/FBS.html.
Minqing Hu and Bing Liu. 2004c. Mining opinion
features in customer reviews. In Proc. AAAI.
K. R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J. L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with Columbia?s
Newsblaster. In Proceedings of the HLT Conf.
2005a. Linguistic quality questions from the
2005 DUC. http://duc.nist.gov/duc2005/quality-
questions.txt.
2005b. Proc. of DUC 2005.
D. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer,
H. Qi, A. elebi, D. Liu, and E. Drabek. 2003. Eval-
uation challenges in large-scale document summa-
rization. In Proc. of the 41st ACL, pages 375?382.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Studies in Natural
Language Processing. Cambridge University Press.
H. Saggion and R. Gaizauskas. 2004. Multi-document
summarization by cluster/profi le relevance and re-
dundancy removal. In Proc. of DUC04.
B. Schiffman, A. Nenkova, and K. McKeown. 2002.
Experiments in multidocument summarization. In
Proc. of HLT02, San Diego, Ca.
M. White, C. Cardie, and V. Ng. 2002. Detecting
discrepancies in numeric estimates using multidoc-
ument hypertext summaries. In Proc of HLT02.
L. Zhou, M. Ticrea, and E. Hovy. 2004. Multi-
document biography summarization. In Proceed-
ings of EMNLP.
312
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1?11, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Syntactic Transfer Using a Bilingual Lexicon
Greg Durrett, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{gdurrett,adpauls,klein}@cs.berkeley.edu
Abstract
We consider the problem of using a bilingual
dictionary to transfer lexico-syntactic infor-
mation from a resource-rich source language
to a resource-poor target language. In con-
trast to past work that used bitexts to trans-
fer analyses of specific sentences at the token
level, we instead use features to transfer the
behavior of words at a type level. In a dis-
criminative dependency parsing framework,
our approach produces gains across a range
of target languages, using two different low-
resource training methodologies (one weakly
supervised and one indirectly supervised) and
two different dictionary sources (one manu-
ally constructed and one automatically con-
structed).
1 Introduction
Building a high-performing parser for a language
with no existing treebank is still an open problem.
Methods that use no supervision at all (Klein and
Manning, 2004) or small amounts of manual su-
pervision (Haghighi and Klein, 2006; Cohen and
Smith, 2009; Naseem et al2010; Berg-Kirkpatrick
and Klein, 2010) have been extensively studied, but
still do not perform well enough to be deployed
in practice. Projection of dependency links across
aligned bitexts (Hwa et al2005; Ganchev et al
2009; Smith and Eisner, 2009) gives better perfor-
mance, but crucially depends on the existence of
large, in-domain bitexts. A more generally appli-
cable class of methods exploits the notion of univer-
sal part of speech tags (Petrov et al2011; Das and
...   the    senators    demand    strict   new    ethics    rules   ...
      DT      NNS          VBP          JJ       JJ       NNS     NNS   
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
          NN                  VVFIN             NN          APPR    ART       NN
       Unions               demand     abandonment     on       the      reform
Figure 1: Sentences in English and German both contain-
ing words that mean ?demand.? The fact that the English
demand takes nouns on its left and right indicates that the
German verlangen should do the same, correctly suggest-
ing attachments to Verzicht and Gewerkschaften.
Petrov, 2011) to train parsers that can run on any lan-
guage with no adaptation (McDonald et al2011)
or unsupervised adaptation (Cohen et al2011).
While these universal parsers currently constitute
the highest-performing methods for languages with-
out treebanks, they are inherently limited by operat-
ing at the coarse POS level, as lexical features are
vital to supervised parsing models.
In this work, we consider augmenting delexical-
ized parsers by transferring syntactic information
through a bilingual lexicon at the word type level.
These parsers are delexicalized in the sense that, al-
though they receive target language words as input,
their feature sets do not include indicators on those
words. This setting is appropriate when there is too
little target language data to learn lexical features di-
rectly. Our main approach is to add features which
are lexical in the sense that they compute a function
of specific target language words, but are still un-
1
lexical in the sense that all lexical knowledge comes
from the bilingual lexicon and training data in the
source language.
Consider the example English and German sen-
tences shown in Figure 1, and suppose that we wish
to parse the German side without access to a Ger-
man treebank. A delexicalized parser operating at
the part of speech level does not have sufficient in-
formation to make the correct decision about, for ex-
ample, the choice of subcategorization frame for the
verb verlangen. However, demand, a possible En-
glish translation of verlangen, takes a noun on its
left and a noun on its right, an observation that in this
case gives us the information we need. We can fire
features in our German parser on the attachments
of Gewerkschaften and Verzicht to verlangen indi-
cating that similar-looking attachments are attested
in English for an English translation of verlangen.
This allows us to exploit fine-grained lexical cues to
make German parsing decisions even when we have
little or no supervised German data; moreover, this
syntactic transfer is possible even in spite of the fact
that demand and verlangen are not observed in par-
allel context.
Using type-level transfer through a dictionary in
this way allows us to decouple the lexico-syntactic
projection from the data conditions under which we
are learning the parser. After computing feature val-
ues using source language resources and a bilingual
lexicon, our model can be trained very simply us-
ing any appropriate training method for a supervised
parser. Furthermore, because the transfer mecha-
nism is just a set of features over word types, we are
free to derive our bilingual lexicon either from bitext
or from a manually-constructed dictionary, making
our method strictly more general than those of Mc-
Donald et al2011) or Ta?ckstro?m et al2012), who
rely centrally on bitext. This flexibility is potentially
useful for resource-poor languages, where a human-
curated bilingual lexicon may be broader in cover-
age or more robust to noise than a small, domain-
limited bitext. Of course, it is an empirical question
whether transferring type level information about
word behavior is effective; we show that, indeed,
this method compares favorably with other transfer
mechanisms used in past work.
The actual syntactic information that we transfer
consists of purely monolingual lexical attachment
statistics computed on an annotated source language
resource.1 While the idea of using large-scale sum-
mary statistics as parser features has been consid-
ered previously (Koo et al2008; Bansal and Klein,
2011; Zhou et al2011), doing so in a projection set-
ting is novel and forces us to design features suitable
for projection through a bilingual lexicon. Our fea-
tures must also be flexible enough to provide benefit
even in the presence of cross-lingual syntactic dif-
ferences and noise introduced by the bilingual dic-
tionary.
Under two different training conditions and with
two different varieties of bilingual lexicons, we
show that our method of lexico-syntactic projection
does indeed improve the performance of parsers that
would otherwise be agnostic to lexical information.
In all settings, we see statistically significant gains
for a range of languages, with our method providing
up to 3% absolute improvement in unlabeled attach-
ment score (UAS) and 11% relative error reduction.
2 Model
The projected lexical features that we propose in this
work are based on lexicalized versions of features
found in MSTParser (McDonald et al2005), an
edge-factored discriminative parser. We take MST-
Parser to be our underlying parsing model and use it
as a testbed on which to evaluate the effectiveness of
our method for various data conditions.2 By instanti-
ating the basic MSTParser features over coarse parts
of speech, we construct a state-of-the-art delexical-
ized parser in the style of McDonald et al2011),
where feature weights can be directly transferred
from a source language or languages to a desired
target language. When we add projected lexical fea-
tures on top of this baseline parser, we do so in a
way that does not sacrifice this generality: while
our new features take on values that are language-
specific, they interact with the model at a language-
independent level. We therefore have the best of
1Throughout this work, we will use English as the source
language, but it is possible to use any language for which the
appropriate bilingual lexicons and treebanks exist. One might
expect to find the best performance from using a source lan-
guage closely related to the target.
2We train MSTParser using the included implementation of
MIRA (Crammer and Singer, 2001) and use projective decoding
for all experiments described in this paper.
2
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, L 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, L [VERB]?CHILD, DIR 0.711
VERB?Gewerkschaften PARENT? [NOUN] 0.822
??? ??? ???
Gewerkschaften     verlangen       Verzicht         auf       die     Reform
        NOUN               VERB           NOUN        ADP    DET     NOUN
        Unions              demand     abandonment     on       the      reform
DELEX
Feature Value
VERB?NOUN 1
VERB?NOUN, R 1
??? ???
PROJ
Query Feature (signature) Value
verlangen?NOUN [VERB]?CHILD 0.723
verlangen?NOUN, R [VERB]?CHILD, DIR 0.521
VERB?Verzicht PARENT?[NOUN] 0.623
??? ??? ???
Figure 2: Computation of features on a dependency arc. DELEX features are indicators over characteristics of depen-
dency links that do not involve the words in the sentence. PROJ features are real-valued analogues of DELEX features
that do contain words. We form a query from each stipulated set of characteristics, compute the values of these queries
heuristically, and then fire a feature based on each query?s signature. Signatures indicate which attachment properties
were considered, which part of the query was lexicalized (shown by brackets here), and the POS of the query word.
This procedure yields a small number of real-valued features that still capture rich lexico-syntactic information.
two worlds in that our features can be learned on
any treebank or treebanks that are available to us,
but still exploit highly specific lexical information
to achieve performance gains over using coarse POS
features alone.
2.1 DELEX Features
Our DELEX feature set consists of all of the unlexi-
calized features in MSTParser, only lightly modified
to improve performance for our setting. McDonald
et al2005) present three basic types of such fea-
tures, ATTACH, INBETWEEN, and SURROUNDING,
which we apply at the coarse POS level. The AT-
TACH features for a given dependency link consist of
indicators of the tags of the head and modifier, sep-
arately as well as together. The INBETWEEN and
SURROUNDING features are indicators on the tags
of the head and modifier in addition to each inter-
vening tag in turn (INBETWEEN) or various com-
binations of tags adjacent to the head or modifier
(SURROUNDING).3
MSTParser by default also includes a copy of
each of these indicator features conjoined with
the direction and distance of the attachment it de-
notes. These extra features are important to getting
3As in Koo et al2008), our feature set contains more
backed-off versions of the SURROUNDING features than are de-
scribed in McDonald et al2005).
good performance out of the baseline model. We
slightly modify the conjunction scheme and expand
it with additional backed-off conjunctions, since
these changes lead to features that empirically trans-
fer better than the MSTParser defaults. Specifically,
we use conjunctions with attachment direction (left
or right), coarsened distance,4 and attachment direc-
tion and coarsened distance combined.
We emphasize again that these baseline features
are entirely standard, and all the DELEX feature set
does is recreate an MSTParser-based analogue of the
direct transfer parser described by McDonald et al
(2011).
2.2 PROJ Features
We will now describe how to compute our projected
lexical features, the PROJ feature set, which con-
stitutes the main contribution of this work. Recall
that we wish our method to be as general as possible
and work under many different training conditions;
in particular, we wish to be able to train our model
on only existing treebanks in other languages when
no target language trees are available (discussed in
Section 3.3), or on only a very small target language
treebank (Section 3.4). It would greatly increase
the power of our model if we were able to include
target-language-lexicalized versions of the ATTACH
4Our five distance buckets are {1, 2, 3?5, 6?10, 11+}.
3
features, but these are not learnable without a large
target language treebank. We instead must augment
our baseline model with a relatively small number of
features that are nonetheless rich enough to transfer
the necessary lexical information.
Our overall approach is sketched in Figure 2,
where we show the features that fire on two pro-
posed edges in a German dependency parse. Fea-
tures on an edge in MSTParser incorporate a sub-
set of observable properties about that edge?s head,
modifier, and context in the sentence. For sets of
properties that do not include a lexical item, such
as VERB?NOUN, we fire an indicator feature from
the DELEX feature set. For those that do include a
lexical item, such as verlangen?NOUN, we form a
query, which resembles a lexicalized indicator fea-
ture. Rather than firing the query as an indicator
feature directly, which would result in a model pa-
rameter for each target word, we fire a broad feature
called an signature whose value reflects the specifics
of the query (computation of these values is dis-
cussed in Section 2.2.2). For example, we abstract
verlangen?NOUN to [VERB]?CHILD, with square
brackets indicating the element that was lexicalized.
Section 2.2.1 discusses this coarsening in more de-
tail. The signatures are agnostic to individual words
and even the language being parsed, so they can be
learned on small amounts of data or data from other
languages.
Our signatures allow us to instantiate features at
different levels of granularity corresponding to the
levels of granularity in the DELEX feature set. When
a small amount of target language data is present,
the variety of signatures available to us means that
we can learn language-specific transfer characteris-
tics: for example, nouns tend to follow prepositions
in both French and English, but the ordering of ad-
jectives with respect to nouns is different. We also
have the capability to train on languages other than
our target language, and while this is expected to be
less effective, it can still teach us to exploit some
syntactic properties, such as similar verb attachment
configurations if we train on a group of SVO lan-
guages distinct from a target SVO language. There-
fore, our feature set manages to provide the training
procedure with choices about how much syntactic
information to transfer at the same time as it prevents
overfitting and provides language independence.
2.2.1 Query and Signature Types
A query is a subset of the following pieces of in-
formation about an edge: parent word, parent POS,
child word, child POS, attachment direction, and
binned attachment distance. It must contain exactly
one word.5 We experimented with properties from
INBETWEEN and SURROUNDING features as well,
but found that these only helped under some circum-
stances and could lead to overfitting.6
A signature contains the following three pieces of
information:
1. The non-empty subset of attachment properties
included in the query
2. Whether we have lexicalized on the parent or
child of the attachment, indicated by brackets
3. The part of speech of the included word
Because either the parent or child POS is included
in the signature, there are three meaningful proper-
ties to potentially condition on, of which we must se-
lect a nonempty subset. Some multiplication shows
that we have 7? 2? 13 = 182 total PROJ features.
As an example, the queries
verlangen? NOUN
verlangen? ADP
sprechen? NOUN
all share the signature [VERB]?CHILD, but
verlangen? NOUN,RIGHT
Verzicht? ADP
VERB ? Verzicht
have [VERB]?CHILD,DIR, [ADP]?CHILD, and
PARENT?[NOUN] as their signatures, respectively.
The level of granularity for signatures is a param-
eter that simply must be engineered. We found some
benefit in actually instantiating two signatures for
every query, one as described above and one that
5Bilexical features are possible in our framework, but we do
not use them here, so for clarity we assume that each query has
one associated word.
6One hypothesis is that features looking at the sentence con-
text are more highly specialized to a given language, since they
examine the parent, the child, and one or more other parts of
speech or words.
4
?demand, DIR
PARENT?demand
demand
Word POS Dir Dist
that
ADP R
3
said
VERB L
7
<root>
ROOT L
6
senators
NOUN L
1
rules
NOUN R
4
We
NOUN L
1
that
ADP R
1
They
NOUN L
1
concessions
NOUN R
1
from
ADP R
2
P
a
r
e
n
t
s
C
h
i
l
d
r
e
n
DIR
Value
L
0.66
R
0.33
PARENT
Value
ADP
0.33
VERB
0.33
ROOT
0.33
  He   reports that   the   senators demand strict new ethics rules [...]
PRON     VERB     ADP     DET       NOUN          VERB        ADJ     ADJ      NOUN  NOUN
   ?      We   demand that these hostilities cease    ,        ?      said [...]
PUNC   PRON       VERB      ADP     DET        NOUN        VERB   PUNC  PUNC   VERB
 They  demand concessions  from   the  Israeli authorities    <root>
  PRON        VERB              NOUN            ADP      DET      ADJ           NOUN               ROOT
???
Figure 3: Computation of query values. For each occurrence of a given source word, we tabulate the attachments it
takes part in (parents and children) and record their properties. We then compute relative frequency counts for each
possible query type to get source language scores, which will later be projected through the dictionary to obtain target
language feature values. Only two query types are shown here, but values are computed for many others as well.
does not condition on the part of speech of the word
in the signature. One can also imagine using more
refined signatures, but we found that this led to over-
fitting in the small training scenarios under consid-
eration.
2.2.2 Query Value Estimation
Each query is given a value according to a gener-
ative heuristic that involves the source training data
and the probabilistic bilingual lexicon.7 For a par-
ticular signature, a query can be written as a tu-
ple (x1, x2, . . . , wt) where wt is the target language
query word and the xi are the values of the included
language-independent attachment properties. The
value this feature takes is given by a simple gener-
ative model: we imagine generating the attachment
properties xi given wt by first generating a source
7Lexicons such as those produced by automatic aligners in-
clude probabilities natively, but obviously human-created lexi-
cons do not. For these dictionaries, we simply assume that each
word translates with uniform probability into each of its pos-
sible translations. Tweaking this method did not substantially
change performance.
word ws from wt based on the bilingual lexicon,
then jointly generating the xi conditioned on ws.
Treating the choice of source translation as a latent
variable to be marginalized out, we have
value = p(x1, x2, . . . |wt)
=
?
ws
p(ws|wt)p(x1, x2, . . . |ws)
The first term of the sum comes directly from our
probabilistic lexicon, and the second we can esti-
mate using the maximum likelihood estimator over
our source language training data:
p(x1, x2, . . . |ws) =
c(x1, x2, . . . , ws)
c(ws)
(1)
where c(?) denotes the count of an event in the
source language data.
The final feature value is actually the logarithm
of this computed value, with a small constant added
before the logarithm is taken to avoid zeroes.
5
3 Experiments
3.1 Data Conditions
Before we describe the details of our experiments,
we sketch the data conditions under which we eval-
uate our method. As described in Section 1, there is
a continuum of lightly supervised parsing methods
from those that make no assumptions (beyond what
is directly encoded in the model), to those that use
a small set of syntactic universals, to those that use
treebanks from resource-rich languages, and finally
to those that use both existing treebanks and bitexts.
Our focus is on parsing when one does not have
access to a full-scale target language treebank, but
one does have access to realistic auxiliary resources.
The first variable we consider is whether we have
access to a small number of target language trees or
only pre-existing treebanks in a number of other lan-
guages; while not our actual target language, these
other treebanks can still serve as a kind of proxy for
learning which features generally transfer useful in-
formation (McDonald et al2011). We notate these
conditions with the following shorthand:
BANKS: Large treebanks in other target languages
SEED: Small treebank in the right target language
Previous work on essentially unsupervised meth-
ods has investigated using a small number of target
language trees (Smith and Eisner, 2009), but the be-
havior of supervised models under these conditions
has not been extensively studied. We will see in
Section 3.4 that with only 100 labeled trees, even
our baseline model can achieve performance equal
to or better than that of the model of McDonald et
al. (2011). A single linguist could plausibly anno-
tate such a number of trees in a short amount of time
for a language of interest, so we believe that this is
an important setting in which to show improvement,
even for a method primarily intended to augment un-
supervised parsing.
In addition, we consider two different sources for
our bilingual lexicon:
AUTOMATIC: Extracted from bitext
MANUAL: Constructed from human annotations
Both bitexts and human-curated bilingual dictionar-
ies are more widely available than complete tree-
banks. Bitexts can provide rich information about
lexical correspondences in terms of how words are
used in practice, but for resource-poor languages,
parallel text may only be available in small quan-
tities, or be domain-limited. We show results of our
method on bilingual dictionaries derived from both
sources, in order to show that it is applicable under a
variety of data conditions and can successfully take
advantage of such resources as are available.
3.2 Datasets
We evaluate our method on a range of languages
taken from the CoNLL shared tasks on multilingual
dependency parsing (Buchholz and Marsi, 2006;
Nivre et al2007). We make use of dependency
treebanks for Danish, German, Greek, Spanish, Ital-
ian, Dutch, Portuguese, and Swedish, all from the
2006 shared task.
For our English resource, we use 500,000 En-
glish newswire sentences from English Gigaword
version 3 (Graff et al2007), parsed with the Berke-
ley Parser (Petrov et al2006) and converted to a
dependency treebank using the head rules of Collins
(1999).8 Our English test set (used in Section 3.4)
consists of the first 300 sentences of section 23 of the
Penn treebank (Marcus et al1993), preprocessed
in the same way. Our model does not use gold fine-
grained POS tags, but we do use coarse POS tags
deterministically generated from the provided gold
fine-grained tags in the style of Berg-Kirkpatrick
and Klein (2010) using the mappings of Petrov et
al. (2011).9 Following McDonald et al2011), we
strip punctuation from all treebanks for the results of
Section 3.3. All results are given in terms of unla-
beled attachment score (UAS), ignoring punctuation
even when it is present.
We use the Europarl parallel corpus (Koehn,
2005) as the bitext from which to extract the AUTO-
MATIC bilingual lexicons. For each target language,
we produce one-to-one alignments on the English-
target bitext by running the Berkeley Aligner (Liang
et al2006) with five iterations of IBM Model 1 and
8Results do not degrade much if one simply uses Sections 2-
21 of the Penn treebank instead. Coverage of rare words in the
treebank is less important when a given word must also appear
in the bilingual lexicon as the translation of an observed German
word in order to be useful.
9Note that even in the absence of gold annotation, such tags
could be produced from bitext using the method of (Das and
Petrov, 2011) or could be read off from a bilingual lexicon.
6
This work Past work
MANUAL AUTOMATIC MPH11* TMU12**
DELEX DELEX+PROJ ? DELEX+PROJ ? Multi-dir Multi-proj ? No clusters X-lingual ?
DA 41.3 43.0 1.67 ? 43.6 2.30 ? 48.9* 0.6* 36.7** 2.0**
DE 58.5 58.7 0.20 59.5 0.94 ? 56.7* -0.1* 48.9** 1.8**
EL 57.9 59.9 1.99 ? 60.5 2.55 ? 60.1* 5.0* 59.5** 3.5**
ES 64.2 65.4 1.20 ? 65.7 1.52 ? 64.2* 0.3* 60.2** 2.7**
IT 65.9 66.5 0.58 67.4 1.54 ? 64.1* 0.9* 64.6** 4.2**
NL 57.0 57.5 0.52 58.8 1.88 ? 55.8* 9.9* 52.8** 1.5**
PT 75.4 77.2 1.83 ? 78.7 3.29 ? 74.0* 1.6* 66.8** 4.2**
SV 64.5 66.1 1.61 ? 66.9 2.34 ? 65.3* 2.7* 55.4** 1.5**
AVG 60.6 61.8 1.20 62.6 2.05 61.1* 2.7* 55.6** 2.7**
Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con-
catenation of non-target-language treebanks (the BANKS setting). Values reported are UAS for sentences of all lengths
in the standard CoNLL test sets, with punctuation removed from training and test sets. Daggers indicate statistical
significance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicates
p < 0.05. We also include the baseline results of McDonald et al2011) and Ta?ckstro?m et al2012) and improve-
ments from their best methods of using bitext and lexical information. These results are not directly comparable to
ours, as indicated by * and **. However, we still see that the performance of our type-level transfer method approaches
that of bitext-based methods, which require complex bilingual training for each new language.
five iterations of the HMM aligner with agreement
training. Our lexicon is then read off based on rel-
ative frequency counts of aligned instances of each
word in the bitext.
We also use our method on bilingual dictionar-
ies constructed in a more conventional way. For
this purpose, we scrape our MANUAL bilingual lex-
icons from English Wiktionary (Wikimedia Founda-
tion, 2012). We mine entries for English words that
explicitly have foreign translations listed as well as
words in each target language that have English def-
initions. We discard all translation entries where
the English side is longer than one word, except
for constructions of the form ?to VERB?, where we
manually remove the ?to? and allow the word to be
defined as the English infinitive. Finally, because
our method requires a dictionary with probability
weights, we assume that each target language word
translates with uniform probability into any of the
candidates that we scrape.
3.3 BANKS
We first evaluate our model under the BANKS data
condition. Following the procedure from McDonald
et al2011), for each language, we train both our
DELEX and DELEX+PROJ features on a concate-
nation of 2000 sentences from each other CoNLL
training set, plus 2000 sentences from the Penn
Treebank. Again, despite the values of our PROJ
queries being sensitive to which language we are
currently parsing, the signatures are language in-
dependent, so discriminative training still makes
sense over such a combined treebank. Training our
PROJ features on the non-English treebanks in this
concatenation can be understood as trying to learn
which lexico-syntactic properties transfer ?univer-
sally,? or at least transfer broadly within the families
of languages we are considering.
Table 1 shows the performance of the DELEX fea-
ture set and the DELEX+PROJ feature set using both
AUTOMATIC and MANUAL bilingual lexicons. Both
methods provide positive gains across the board that
are statistically significant in the vast majority of
cases, though MANUAL is slightly less effective;
we postpone until Section 4.1 the discussion of the
shortcomings of the MANUAL lexicon.
We include for reference the baseline results of
McDonald et al2011) and Ta?ckstro?m et al2012)
(multi-direct transfer and no clusters) and the im-
provements from their best methods using lexi-
cal information (multi-projected transfer and cross-
lingual clusters). We emphasize that these results
are not directly comparable to our own, as we
have different training data (and even different train-
ing languages) and use a different underlying pars-
ing model (MSTParser instead of a transition-based
7
AUTOMATIC
100 train trees 200 train trees 400 train trees
DELEX DELEX+PROJ ? DELEX DELEX+PROJ ? DELEX DELEX+PROJ ?
DA 67.2 69.5 2.32 ? 69.5 72.3 2.77 ? 71.4 74.6 3.16 ?
DE 72.9 73.9 0.97 75.4 76.5 1.09 ? 77.3 78.5 1.25 ?
EL 70.8 72.9 2.07 ? 72.6 74.9 2.30 ? 74.3 76.7 2.41 ?
ES 72.5 73.0 0.46 74.1 75.4 1.29 ? 75.3 77.2 1.81 ?
IT 73.3 75.4 2.13 ? 74.7 77.3 2.54 ? 76.0 78.7 2.74 ?
NL 63.0 65.8 2.82 ? 64.7 67.6 2.86 ? 66.1 69.2 3.06 ?
PT 78.1 79.5 1.45 ? 79.5 81.1 1.66 ? 80.7 82.4 1.63 ?
SV 76.4 78.1 1.69 ? 78.1 80.2 2.02 ? 79.6 81.7 2.07 ?
AVG 71.8 73.5 1.74 73.6 75.7 2.07 75.1 77.4 2.27
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
MANUAL
DA 67.2 68.1 0.88 69.5 70.9 1.44 ? 71.4 73.3 1.92 ?
DE 72.9 73.4 0.44 75.4 76.2 0.77 77.3 78.4 1.12 ?
EL 70.8 71.9 1.06 ? 72.6 74.1 1.48 ? 74.3 75.8 1.56 ?
ES 72.5 71.9 -0.64 74.1 74.3 0.23 75.3 76.4 1.04 ?
IT 73.3 74.3 1.01 ? 74.7 76.4 1.66 ? 76.0 78.0 2.01 ?
NL 63.0 65.4 2.43 ? 64.7 67.5 2.76 ? 66.1 69.0 2.91 ?
PT 78.1 78.2 0.13 79.5 80.1 0.62 80.7 81.5 0.82 ?
SV 76.4 76.6 0.25 78.1 79.1 1.01 ? 79.6 81.0 1.40 ?
AVG 71.8 72.5 0.70 73.6 74.8 1.25 75.1 76.7 1.60
EN 74.4 81.5 7.06 ? 76.6 83.0 6.35 ? 78.3 84.1 5.80 ?
Table 2: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on various
small numbers of target language trees (the SEED setting). Values reported are UAS for sentences of all lengths on
our enlarged CoNLL test sets (see text); each value is based on 50 sampled training sets of the given size. Daggers
indicate statistical significance as described in the text. Statistical significance is not reported for averages.
parser (Nivre, 2008)). However, our baseline is com-
petitive with theirs,10 demonstrating that we have
constructed a state-of-the-art delexicalized parser.
Furthermore, our method appears to approach the
performance of previous bitext-based methods, and
because of its flexibility and the freedom from com-
plex cross-lingual training for each new language, it
can be applied in the MANUAL case as well, a capa-
bility which neither of the other methods has.
3.4 SEED
We now turn our attention to the SEED scenario,
where a small number of target language trees are
available for each language we consider. While it
is imaginable to continue to exploit the other tree-
banks in the presence of target language trees, we
found that training our DELEX features on the seed
treebank alone gave higher performance than any
10The baseline of Ta?ckstro?m et al2012) is lower because it
is trained only on English rather than on many languages.
attempt to also use the concatenation of treebanks
from the previous section. This is not too surpris-
ing because, with this number of sentences, there is
already good monolingual coverage of coarse POS
features, and attempting to train features on other
languages can be expected to introduce noise into
otherwise accurate monolingual feature weights.
We train our DELEX+PROJ model with both AU-
TOMATIC and MANUAL lexicons on target language
training sets of size 100, 200, and 400, and give re-
sults for each language in Table 2. The performance
of parsers trained on small numbers of trees can
be highly variable, so we create multiple treebanks
of each size by repeatedly sampling from each lan-
guage?s train treebank, and report averaged results.
Furthermore, this evaluation is not on the standard
CoNLL test sets, but is instead on those test sets with
a few hundred unused training sentences added, the
reason being that some of the CoNLL test sets are
very small (fewer than 200 sentences) and appeared
8
to give highly variable results. To compute statistical
significance, we draw a large number of bootstrap
samples for each training set used, then aggregate all
of their sufficient statistics in order to compute the fi-
nal p-value. We see that our DELEX+PROJ method
gives statistically significant gains at the 95% level
over DELEX for nearly all language and training set
size pairs, giving on average a 9% relative error re-
duction in the 400-tree case.
Because our features are relatively few in number
and capture heuristic information, one question we
might ask is how well they can perform in a non-
projection context. In the last line of the table, we
report gains that are achieved when PROJ features
computed from parsed Gigaword are used directly
on English, with no intermediate dictionary. These
are not comparable to the other values in the table
because we are using our projection strategy mono-
lingually, which removes the barriers of imperfect
lexical correspondence (from using the lexicon) and
imperfect syntactic correspondence (from project-
ing). As one might expect, the gains on English are
far higher than the gains on other languages. This
indicates that performance is chiefly limited by the
need to do cross-lingual feature adaptation, not in-
herently low feature capacity. We delay further dis-
cussion to Section 4.2.
One surprising thing to note is that the gains given
by our PROJ features are in some cases larger here
than in the BANKS setting. This result is slightly
counterintuitive, as our baseline parsers are much
better in this case and so we would expect dimin-
ished returns from our method. We conclude that ac-
curately learning which signatures transfer between
languages is important, and it is easier to learn good
feature weights when some target language data is
available. Further evidence supporting this hypothe-
sis is the fact that the gains are larger and more sig-
nificant on larger training set sizes.
4 Discussion
4.1 AUTOMATIC versus MANUAL
Overall, we see that gains from using our MANUAL
lexicons are slightly lower than those from our AU-
TOMATIC lexicons. One might expect higher per-
formance because scraped bilingual lexicons are not
prone to some of the same noise that exists in auto-
AUTOMATIC MANUAL
Voc OCC Voc OCC
DA 324K 0.91 22K 0.64
DE 320K 0.89 58K 0.55
EL 196K 0.94 23K 0.43
ES 165K 0.89 206K 0.74
IT 158K 0.91 78K 0.65
NL 251K 0.87 50K 0.72
PT 165K 0.85 46K 0.53
SV 307K 0.93 28K 0.60
Table 3: Lexicon statistics for all languages for both
sources of bilingual lexicons. ?Voc? indicates vocabulary
size and ?OCC? indicates open-class coverage, the frac-
tion of open-class tokens in the test treebanks with entries
in our bilingual lexicon.
matic aligners, but this is empirically not the case.
Rather, as we see in Table 3, the low recall of our
MANUAL lexicons on open-class words appears to
be a possible culprit. The coverage gap between
these and the AUTOMATIC lexicons is partially due
to the inconsistent structure of Wiktionary: inflected
German and Greek words often do not have their
own pages, so we miss even common morphologi-
cal variants of verb forms in those languages. The
inflected forms that we do scrape are also mapped
to the English base form rather than the correspond-
ing inflected form in English, which introduces fur-
ther noise. Coverage is substantially higher if we
translate using stems only, but this did not empir-
ically lead to performance improvements, possibly
due to conflating different parts of speech with the
same base form.
One might hypothesize that our uniform weight-
ing scheme in the MANUAL lexicon is another
source of problems, and that bitext-derived weights
are necessary to get high performance. This is not
the case here. Truncating the AUTOMATIC dictio-
nary to at most 20 translations per word and setting
the weights uniformly causes a slight performance
drop, but is still better than our MANUAL lexicon.
This further demonstrates that these problems are
more a limitation of our dictionary than our method.
English Wiktionary is not designed to be a bilingual
dictionary, and while it conveniently provided an
easy way for us to produce lexicons for a wide array
9
Frauen    wollen    weiter     f?r       die     Quote  k?mpfen
   NN     VMFIN    ADV    APPR   ART      NN    VVINF
Women     want     further     for       the     quota     fight
Women    want    to   continue   to    fight   for   the   quota
   NNP      VBP   TO      VB      TO    VB    IN   DT    NN
Figure 4: Example of a German tree and a parallel En-
glish sentence with high levels of syntactic divergence.
The English verb want takes fundamentally different chil-
dren than wollen does, so properties of the sort we present
in Section 2.2 will not transfer effectively.
of languages, it is not the resource that one would
choose if designing a parser for a specific target lan-
guage. Bitext is not necessary for our approach to
work, and results on the AUTOMATIC lexicon sug-
gest that our type-level transfer method can in fact
do much better given a higher quality resource.
4.2 Limitations
While our method does provide consistent gains
across a range of languages, the injection of lexical
information is clearly not sufficient to bridge the gap
between unsupervised and supervised parsers. We
argued in Section 3.4 that the cross-lingual transfer
step of our method imposes a fundamental limitation
on how useful any such approach can be, which we
now investigate further.
In particular, any syntactic divergence, especially
inconsistent divergences like head switching, will
limit the utility of transferred structure. Consider
the German example in Figure 4, with a parallel En-
glish sentence provided. The English tree suggests
that want should attach to an infinitival to, which has
no correlate in German. Even disregarding this, its
grandchild is the verb continue, which is realized in
the German sentence as the adverb weiter. While
it is still broadly true that want and wollen both
have verbal elements located to their right, it is less
clear how to design features that can still take advan-
tage of this while working around the differences we
have described. Therefore, a gap between the per-
formance of our features on English and the perfor-
mance of our projected features, as is observed in
Table 2, is to be expected in the absence of a more
complete model of syntactic divergence.
5 Conclusion
In this work, we showed that lexical attachment pref-
erences can be projected to a target language at the
type level using only a bilingual lexicon, improving
over a delexicalized baseline parser. This method
is broadly applicable in the presence or absence
of target language training trees and with bilingual
lexicons derived from either manually-annotated re-
sources or bitexts. The greatest improvements arise
when the bilingual lexicon has high coverage and a
number of target language trees are available in or-
der to learn exactly what lexico-syntactic properties
transfer from the source language.
In addition, we showed that a well-tuned discrim-
inative model with the correct features can achieve
good performance even on very small training sets.
While unsupervised and existing projection meth-
ods do feature great versatility and may yet pro-
duce state-of-the-art parsers on resource-poor lan-
guages, spending time constructing small supervised
resources appears to be the fastest method to achieve
high performance in these settings.
Acknowledgments
This work was partially supported by an NSF Grad-
uate Research Fellowship to the first author, by a
Google Fellowship to the second author, and by the
NSF under grant 0643742. Thanks to the anony-
mous reviewers for their insightful comments.
References
Mohit Bansal and Dan Klein. 2011. Web-scale Features
for Full-scale Parsing. In Proceedings of ACL, pages
693?702, Portland, Oregon, USA.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic Grammar Induction. In Proceedings of ACL,
pages 1288?1297, Uppsala, Sweden.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
Proceedings of CoNLL, pages 149?164.
Shay B. Cohen and Noah A. Smith. 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying in
10
Unsupervised Grammar Induction. In Proceedings of
NAACL, pages 74?82, Boulder, Colorado.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised Structure Prediction with Non-Parallel
Multilingual Guidance. In Proceedings of EMNLP,
pages 50?61, Edinburgh, UK.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Koby Crammer and Yoram Singer. 2001. Ultraconserva-
tive Online Algorithms for Multiclass Problems. Jour-
nal of Machine Learning Research, 3:2003.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of ACL, pages 600?609, Port-
land, Oregon, USA.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency Grammar Induction via Bitext Pro-
jection Constraints. In Proceedings of ACL, pages
369?377, Suntec, Singapore.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2007. English Gigaword Third Edition. Linguistic
Data Consortium, Catalog Number LDC2007T07.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Grammar Induction. In Proceedings of CoLING-ACL,
pages 881?888, Sydney, Australia.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
Parsers via Syntactic Projection Across Parallel Texts.
Natural Language Engineering, 11:311?325, Septem-
ber.
Dan Klein and Christopher D. Manning. 2004. Corpus-
Based Induction of Syntactic Structure: Models of De-
pendency and Constituency. In Proceedings of ACL,
pages 479?486.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In MT Summit X,
pages 79?86, Phuket, Thailand. AAMT.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-Supervised Dependency Parsing. In Pro-
ceedings of ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of NAACL, New
York, New York.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated Cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330, June.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. In Proceedings of ACL, pages 91?98, Ann
Arbor, Michigan.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-Source Transfer of Delexicalized Dependency
Parsers. In Proceedings of EMNLP, pages 62?72, Ed-
inburgh, Scotland, UK.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using Universal Linguistic Knowl-
edge to Guide Grammar Induction. In Proceed-
ings of EMNLP, pages 1234?1244, Cambridge, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 Shared Task on Dependency
Parsing. In Proceedings of EMNLP-CoNLL, pages
915?932, Prague, Czech Republic.
Joakim Nivre. 2008. Algorithms for Deterministic Incre-
mental Dependency Parsing. Computational Linguis-
tics, 34:513?553, December.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of ACL,
pages 433?440, Sydney, Australia.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A Universal Part-of-Speech Tagset. In ArXiv, April.
David A. Smith and Jason Eisner. 2009. Parser Adapta-
tion and Projection with Quasi-Synchronous Grammar
Features. In Proceedings of EMNLP, pages 822?831,
Suntec, Singapore.
Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.
2012. Cross-lingual Word Clusters for Direct Trans-
fer of Linguistic Structure. In Proceedings of NAACL,
Montreal, Canada.
Wikimedia Foundation. 2012. Wiktionary. Online at
http://www.wiktionary.org/.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011.
Exploiting Web-Derived Selectional Preference to Im-
prove Statistical Dependency Parsing. In Proceedings
of ACL, pages 1556?1565, Portland, Oregon, USA.
11
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118?126,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Unsupervised Syntactic Alignment with Inversion Transduction Grammars
Adam Pauls Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
David Chiang Kevin Knight
Information Sciences Institute
University of Southern California
{chiang,knight}@isi.edu
Abstract
Syntactic machine translation systems cur-
rently use word alignments to infer syntactic
correspondences between the source and tar-
get languages. Instead, we propose an un-
supervised ITG alignment model that directly
aligns syntactic structures. Our model aligns
spans in a source sentence to nodes in a target
parse tree. We show that our model produces
syntactically consistent analyses where possi-
ble, while being robust in the face of syntactic
divergence. Alignment quality and end-to-end
translation experiments demonstrate that this
consistency yields higher quality alignments
than our baseline.
1 Introduction
Syntactic machine translation has advanced signif-
icantly in recent years, and multiple variants cur-
rently achieve state-of-the-art translation quality.
Many of these systems exploit linguistically-derived
syntactic information either on the target side (Gal-
ley et al, 2006), the source side (Huang et al, 2006),
or both (Liu et al, 2009). Still others induce their
syntax from the data (Chiang, 2005). Despite differ-
ences in detail, the vast majority of syntactic meth-
ods share a critical dependence on word alignments.
In particular, they infer syntactic correspondences
between the source and target languages through
word alignment patterns, sometimes in combination
with constraints from parser outputs.
However, word alignments are not perfect indi-
cators of syntactic alignment, and syntactic systems
are very sensitive to word alignment behavior. Even
a single spurious word alignment can invalidate a
large number of otherwise extractable rules, while
unaligned words can result in an exponentially large
set of extractable rules to choose from. Researchers
have worked to incorporate syntactic information
into word alignments, resulting in improvements to
both alignment quality (Cherry and Lin, 2006; DeN-
ero and Klein, 2007), and translation quality (May
and Knight, 2007; Fossum et al, 2008).
In this paper, we remove the dependence on word
alignments and instead directly model the syntactic
correspondences in the data, in a manner broadly
similar to Yamada and Knight (2001). In particu-
lar, we propose an unsupervised model that aligns
nodes of a parse tree (or forest) in one language to
spans of a sentence in another. Our model is an in-
stance of the inversion transduction grammar (ITG)
formalism (Wu, 1997), constrained in such a way
that one side of the synchronous derivation respects
a syntactic parse. Our model is best suited to sys-
tems which use source- or target-side trees only.
The design of our model is such that, for divergent
structures, a structurally integrated backoff to flatter
word-level (or null) analyses is available. There-
fore, our model is empirically robust to the case
where syntactic divergence between languages pre-
vents syntactically accurate ITG derivations.
We show that, with appropriate pruning, our
model can be efficiently trained on large parallel cor-
pora. When compared to standard word-alignment-
backed baselines, our model produces more con-
sistent analyses of parallel sentences, leading to
high-count, high-quality transfer rules. End-to-
end translation experiments demonstrate that these
higher quality rules improve translation quality by
1.0 BLEU over a word-alignment-backed baseline.
2 Syntactic Rule Extraction
Our model is intended for use in syntactic transla-
tion models which make use of syntactic parses on
either the target (Galley et al, 2006) or source side
(Huang et al, 2006; Liu et al, 2006). Our model?s
118
SNP
DT* NN NN
VP
VBZ
ADVP
RB VBN
the trade surplus has drastically fallen
??
??
???
 ??
 ?
trade
surplus
drastically
fall
(past)
Figure 1: A single incorrect alignment removes an ex-
tractable node, and hence several desirable rules. We
represent correct extractable nodes in bold, spurious ex-
tractable nodes with a *, and incorrectly blocked ex-
tractable nodes in bold strikethrough.
chief purpose is to align nodes in the syntactic parse
in one language to spans in the other ? an alignment
we will refer to as a ?syntactic? alignment. These
alignments are employed by standard syntactic rule
extraction algorithms, for example, the GHKM al-
gorithm of Galley et al (2004). Following that work,
we will assume parses are present in the target lan-
guage, though our model applies in either direction.
Currently, although syntactic systems make use of
syntactic alignments, these alignments must be in-
duced indirectly from word-level alignments. Pre-
vious work has discussed at length the poor interac-
tion of word-alignments with syntactic rule extrac-
tion (DeNero and Klein, 2007; Fossum et al, 2008).
For completeness, we provide a brief example of this
interaction, but for a more detailed discussion we re-
fer the reader to these presentations.
2.1 Interaction with Word Alignments
Syntactic systems begin rule extraction by first iden-
tifying, for each node in the target parse tree, a
span of the foreign sentence which (1) contains ev-
ery source word that aligns to a target word in the
yield of the node and (2) contains no source words
that align outside that yield. Only nodes for which
a non-empty span satisfying (1) and (2) exists may
form the root or leaf of a translation rule; for that
reason, we will refer to these nodes as extractable
nodes.
Since extractable nodes are inferred based on
word alignments, spurious word alignments can rule
out otherwise desirable extraction points. For exam-
ple, consider the alignment in Figure 1. This align-
ment, produced by GIZA++ (Och and Ney, 2003),
contains 4 correct alignments (the filled circles),
but incorrectly aligns the to the Chinese past tense
marker ? (the hollow circle). This mistaken align-
ment produces the incorrect rule (DT ? the ; ?),
and also blocks the extraction of (VBN ? fallen ;
???).
More high-level syntactic transfer rules are also
ruled out, for example, the ?the insertion rule? (NP
? the NN1 NN2 ; NN1 NN2) and the high-level (S
? NP1 VP2 ; NP1 VP2).
3 A Syntactic Alignment Model
The most common approach to avoiding these prob-
lems is to inject knowledge about syntactic con-
straints into a word alignment model (Cherry and
Lin, 2006; DeNero and Klein, 2007; Fossum et al,
2008).1 While syntactically aware, these models re-
main limited by the word alignment models that un-
derly them.
Here, we describe a model which directly infers
alignments of nodes in the target-language parse tree
to spans of the source sentence. Formally, our model
is an instance of a Synchronous Context-Free Gram-
mar (see Chiang (2004) for a review), or SCFG,
which generates an English (target) parse tree T and
foreign (source) sentence f given a target sentence e.
The generative process underlying this model pro-
duces a derivation d of SCFG rules, from which T
and f can be read off; because we condition on e,
the derivations produce e with probability 1. This
model places a distribution over T and f given by
p(T, f | e) =
?
d
p(d | e) =
?
d
?
r?d
p(r | e)
where the sum is over derivations d which yield T
and f . The SCFG rules r come from one of 4 types,
pictured in Table 1. In general, because our model
can generate English trees, it permits inference over
forests. Although we will restrict ourselves to a sin-
gle parse tree for our experiments, in this section, we
discuss the more general case.
1One notable exception is May and Knight (2007), who pro-
duces syntactic alignments using syntactic rules derived from
word-aligned data.
119
Rule Type Root English Foreign Example Instantiation
TERMINAL E e ft FOUR ? four ;?
UNARY A B fl B fr CD ? FOUR ;  FOUR ?
BINARYMONO A B C fl B fm C fr NP ? NN NN ;  NN ? NN 
BINARYINV A B C fl C fm B fr PP ? IN NP ;? NP  IN 
Table 1: Types of rules present in the SCFG describing our model, along with some sample instantiations of each type.
Empty word sequences f have been explicitly marked with an .
The first rule type is the TERMINAL production,
which rewrites a terminal symbol2 E as its En-
glish word e and a (possibly empty) sequence of
foreign words ft. Generally speaking, the majority
of foreign words are generated using this rule. It
is only when a straightforward word-to-word corre-
spondence cannot be found that our model resorts to
generating foreign words elsewhere.
We can also rewrite a non-terminal symbol A us-
ing a UNARY production, which on the English side
produces a single symbol B, and on the foreign side
produces the symbol B, with sequences of words fl
to its left and fr to its right.
Finally, there are two binary productions: BINA-
RYMONO rewrites A with two non-terminals B and
C on the English side, and the same non-terminals
B and C in monotonic order on the foreign side,
with sequences of words fl, fr, and fm to the left,
right, and the middle. BINARYINV inverts the or-
der in which the non-terminals B and C are written
on the source side, allowing our model to capture a
large subset of possible reorderings (Wu, 1997).
Derivations from this model have two key prop-
erties: first, the English side of a derivation is con-
strained to form a valid constituency parse, as is re-
quired in a syntax system with target-side syntax;
and second, for each parse node in the English pro-
jection, there is exactly one (possibly empty) con-
tiguous span of the foreign side which was gener-
ated from that non-terminal or one of its descen-
dants. Identifying extractable nodes from a deriva-
tion is thus trivial: any node aligned to a non-empty
foreign span is extractable.
In Figure 2, we show a sample sentence pair frag-
2For notational convenience, we imagine that for each par-
ticular English word e, there is a special preterminal symbol E
which produces it. These symbols E act like any other non-
terminal in the grammar with respect to the parameterization in
Section 3.1. To denote standard non-terminals, we will use A,
B, and C.
PP[0,4]
IN[3,4]
NP[1,3]
DT[1,1]
NNS[1,3]
the[1,1]
elections[1,3]
? ??
?
??
at parliament election
before
before[3,4]
PP
NP IN
NNSDT
0 1 2 3 4
?
PP ? IN NP ; ? NP IN
NP ? DT NNS ; DT NNS
IN ? before ; before
before ? before ; ??
DT ? the ; the
the ? the ; !
NNS ? elections ; elections
elections ? elections ; ?? ??
Figure 2: Top: A synchronous derivation of a small sen-
tence pair fragment under our model. The English pro-
jection of the derivation represents a valid constituency
parse, while the foreign projection is less constrained.
We connect each foreign terminal with a dashed line to
the node in the English side of the synchronous deriva-
tion at which it is generated. The foreign span assigned
to each English node is indicated with indices. All nodes
with non-empty spans, shown in boldface, are extractable
nodes. Bottom: The SCFG rules used in the derivation.
ment as generated by our model. Our model cor-
rectly identifies that the English the aligns to nothing
on the foreign side. Our model also effectively cap-
tures the one-to-many alignment3 of elections to ?
3While our model does not explicitly produce many-to-one
alignments, many-to-one rules can be discovered via rule com-
position (Galley et al, 2006).
120
? ??. Finally, our model correctly analyzes the
Chinese circumposition ? . . .?? (before . . . ). In
this construction, ?? carries the meaning of ?be-
fore?, and thus correctly aligns to before, while ?
functions as a generic preposition, which our model
handles by attaching it to the PP. This analysis per-
mits the extraction of the general rule (PP ? IN1
NP2 ;? NP2 IN1), and the more lexicalized (PP?
before NP ;? NP??).
3.1 Parameterization
In principle, our model could have one parameter for
each instantiation r of a rule type. This model would
have an unmanageable number of parameters, pro-
ducing both computational and modeling issues ? it
is well known that unsupervised models with large
numbers of parameters are prone to degenerate anal-
yses of the data (DeNero et al, 2006). One solution
might be to apply an informed prior with a compu-
tationally tractable inference procedure (e.g. Cohn
and Blunsom (2009) or Liu and Gildea (2009)). We
opt here for the simpler, statistically more robust so-
lution of making independence assumptions to keep
the number of parameters at a reasonable level.
Concretely, we define the probability of the BI-
NARYMONO rule,4
p(r = A? B C; fl B fm C fr|A, eA)
which conditions on the root of the rule A and the
English yield eA, as
pg(A? B C | A, eA) ? pinv(I | B,C)?
pleft(fl | A, eA)?pmid(fm | A, eA)?pright(fr | A, eA)
In words, we assume that the rule probability de-
composes into a monolingual PCFG grammar prob-
ability pg, an inversion probability pinv, and a proba-
bility of left, middle, and right word sequences pleft,
pmid, and pright.5 Because we condition on e, the
monolingual grammar probability pg must form a
distribution which produces e with probability 1.6
4In the text, we only describe the factorization for the BI-
NARYMONO rule. For a parameterization of all rules, we refer
the reader to Table 2.
5All parameters in our model are multinomial distributions.
6A simple case of such a distribution is one which places all
of its mass on a single tree. More complex distributions can be
obtained by conditioning an arbitrary PCFG on e (Goodman,
1998).
We further assume that the probability of produc-
ing a foreign word sequence fl decomposes as:
pleft(fl | A, eA) = pl(|fl| = m | A)
m?
j=1
p(fj | A, eA)
where m is the length of the sequence fl. The pa-
rameter pl is a left length distribution. The prob-
abilities pmid, pright, decompose in the same way,
except substituting a separate length distribution pm
and pr for pl. For the TERMINAL rule, we emit ft
with a similarly decomposed distribution pterm us-
ing length distribution pw.
We define the probability of generating a foreign
word fj as
p(fj | A, eA) =
?
i?eA
1
| eA |
pt(fj | ei)
with i ? eA denoting an index ranging over the in-
dices of the English words contained in eA. The
reader may recognize the above expressions as the
probability assigned by IBM Model 1 (Brown et al,
1993) of generating the words fl given the words eA,
with one important difference ? the length m of the
foreign sentence is often not modeled, so the term
pl(|fl| = m | A) is set to a constant and ignored.
Parameterizing this length allows our model to ef-
fectively control the number of words produced at
different levels of the derivation.
It is worth noting how each parameter affects the
model?s behavior. The pt distribution is a standard
?translation? table, familiar from the IBM Models.
The pinv distribution is a ?distortion? parameter, and
models the likelihood of inverting non-terminals B
and C. This parameter can capture, for example,
the high likelihood that prepositions IN and noun
phrases NP often invert in Chinese due to its use
of postpositions. The non-terminal length distribu-
tions pl, pm, and pr model the probability of ?back-
ing off? and emitting foreign words at non-terminals
when a more refined analysis cannot be found. If
these parameters place high mass on 0 length word
sequences, this heavily penalizes this backoff be-
haviour. For the TERMINAL rule, the length distri-
bution pw parameterizes the number of words pro-
duced for a particular English word e, functioning
similarly to the ?fertilities? employed by IBM Mod-
els 3 and 4 (Brown et al, 1993). This allows us
121
to model, for example, the tendency of English de-
terminers the and a translate to nothing in the Chi-
nese, and of English names to align to multiple Chi-
nese words. In general, we expect an English word
to usually align to one Chinese word, and so we
place a weak Dirichlet prior on on the pe distribution
which puts extra mass on 1-length word sequences.
This is helpful for avoiding the ?garbage collection?
(Moore, 2004) problem for rare words.
3.2 Exploiting Non-Terminal Labels
There are often foreign words that do not correspond
well to any English word, which our model must
also handle. We elected for a simple augmentation
to our model to account for these words. When gen-
erating foreign word sequences f at a non-terminal
(i.e. via the UNARY or BINARY productions), we
also allow for the production of foreign words from
the non-terminal symbol A. We modify p(fj | eA)
from the previous section to allow production of fj
directly from the non-terminal7 A:
p(fj | eA) = pnt ? p(fj | A)
+ (1? pnt) ?
?
i?eA
1
|eA|
pt(fj | ei)
where pnt is a global binomial parameter which con-
trols how often such alignments are made.
This necessitates the inclusion of parameters like
pt(? | NP) into our translation table. Generally,
these parameters do not contain much information,
but rather function like a traditional NULL rooted
at some position in the tree. However, in some
cases, the particular annotation used by the Penn
Treebank (Marcus et al, 1993) (and hence most
parsers) allows for some interesting parameters to
be learned. For example, we found that our aligner
often matched the Chinese word ?, which marks
the past tense (among other things), to the preter-
minals VBD and VBN, which denote the English
simple past and perfect tense. Additionally, Chinese
measure words like ? and ? often align to the CD
(numeral) preterminal. These generalizations can be
quite useful ? where a particular number might pre-
dict a measure word quite poorly, the generalization
that measure words co-occur with the CD tag is very
robust.
7For terminal symbols E, this production is not possible.
3.3 Membership in ITG
The generative process which describes our model
contains a class of grammars larger than the com-
putationally efficient class of ITG grammars. For-
tunately, the parameterization described above not
only reduces the number of parameters to a man-
ageable level, but also introduces independence as-
sumptions which permit synchronous binarization
(Zhang et al, 2006) of our grammar. Any SCFG that
can be synchronously binarized is an ITG, meaning
that our parameterization permits efficient inference
algorithms which we will make use of in the next
section. Although several binarizations are possi-
ble, we give one such binarization and its associated
probabilities in Table 2.
3.4 Robustness to Syntactic Divergence
Generally speaking, ITG grammars have proven
more useful without the monolingual syntactic con-
straints imposed by a target parse tree. When deriva-
tions are restricted to respect a target-side parse tree,
many desirable alignments are ruled out when the
syntax of the two languages diverges, and align-
ment quality drops precipitously (Zhang and Gildea,
2004), though attempts have been made to address
this issue (Gildea, 2003).
Our model is designed to degrade gracefully in
the case of syntactic divergence. Because it can pro-
duce foreign words at any level of the derivation,
our model can effectively back off to a variant of
Model 1 in the case where an ITG derivation that
both respects the target parse tree and the desired
word-level alignments cannot be found.
For example, consider the sentence pair fragment
in Figure 3. It is not possible to produce an ITG
derivation of this fragment that both respects the
English tree and also aligns all foreign words to
their obvious English counterparts. Our model han-
dles this case by attaching the troublesome ?? at
the uppermost VP. This analysis captures 3 of the
4 word-level correspondences, and also permits ex-
traction of abstract rules like (S? NP VP ; NP VP)
and (NP? the NN ; NN).
Unfortunately, this analysis leaves the English
word tomorrow with an empty foreign span, permit-
ting extraction of the incorrect translation (VP ?
announced tomorrow ; ??), among others. Our
122
Rule Type Root English side Foreign side Probability
TERMINAL E e wt pterm(wt | E)
UNARY A Bu wl Bu pg(A ? B | A)pleft(wl | A, eA)
Bu B B wr pright(wr | A, eA)
BINARY A A1 wl A1 pleft(wl | A, eA)
A1 B C1 B C1 pg(A ? B C | A)pinv(I=false | B,C)
A1 B C1 C1 B pg(A ? B C | A)pinv(I=true | B,C)
C1 C2 fm C2 pmid(fm | A, eA)
C2 C C fr pright(fr | A, eA)
Table 2: A synchronous binarization of the SCFG describing our model.
S[0,4]
NP[3,4]
DT[3,3] NN[3,4]
VP[0,3]
VB[2,2]
VP[2,3]
VBN[2,3]
NN[3,3]
VP[2,3]
MD[1,2]
?? ? ?? ??
listannouncewilltomorrow0 1 2 3 4
the[3,3] list[3,4]
be[2,2]
announced[2,3] tomorrow[3,3]
will[1,2]
(a)
Figure 3: The graceful degradation of our model in the
face of syntactic divergence. It is not possible to align
all foreign words with their obvious English counterparts
with an ITG derivation. Instead, our model analyzes as
much as possible, but must resort to emitting ?? high
in the tree.
point here is not that our model?s analysis is ?cor-
rect?, but ?good enough? without resorting to more
computationally complicated models. In general,
our model follows an ?extract as much as possi-
ble? approach. We hypothesize that this approach
will capture important syntactic generalizations, but
it also risks including low-quality rules. It is an em-
pirical question whether this approach is effective,
and we investigate this issue further in Section 5.3.
There are possibilities for improving our model?s
treatment of syntactic divergence. One option is
to allow the model to select trees which are more
consistent with the alignment (Burkett et al, 2010),
which our model can do since it permits efficient in-
ference over forests. The second is to modify the
generative process slightly, perhaps by including the
?clone? operator of Gildea (2003).
4 Learning and Inference
4.1 Parameter Estimation
The parameters of our model can be efficiently
estimated in an unsupervised fashion using the
Expectation-Maximization (EM) algorithm. The E-
step requires the computation of expected counts un-
der our model for each multinomial parameter. We
omit the details of obtaining expected counts for
each distribution, since they can be obtained using
simple arithmetic from a single quantity, namely, the
expected count of a particular instantiation of a syn-
chronous rule r. This expectation is a standard quan-
tity that can be computed in O(n6) time using the
bitext Inside-Outside dynamic program (Wu, 1997).
4.2 Dynamic Program Pruning
While our model permits O(n6) inference over a
forest of English trees, inference over a full forest
would be very slow, and so we fix a single n-ary En-
glish tree obtained from a monolingual parser. How-
ever, it is worth noting that the English side of the
ITG derivation is not completely fixed. Where our
English trees are more than binary branching, we
permit any binarization in our dynamic program.
For efficiency, we also ruled out span alignments
that are extremely lopsided, for example, a 1-word
English span aligned to a 20-word foreign span.
Specifically, we pruned any span alignment in which
one side is more than 5 times larger than the other.
Finally, we employ pruning based on high-
precision alignments from simpler models (Cherry
and Lin, 2007; Haghighi et al, 2009). We com-
pute word-to-word alignments by finding all word
pairs which have a posterior of at least 0.7 according
to both forward and reverse IBM Model 1 parame-
ters, and prune any span pairs which invalidate more
than 3 of these alignments. In total, this pruning re-
123
Span P R F1
Syntactic Alignment 50.9 83.0 63.1
GIZA++ 56.1 67.3 61.2
Rule P R F1
Syntactic Alignment 39.6 40.3 39.9
GIZA++ 46.2 34.7 39.6
Table 3: Alignment quality results for our syntactic
aligner and our GIZA++ baseline.
duced computation from approximately 1.5 seconds
per sentence to about 0.3 seconds per sentence, a
speed-up of a factor of 5.
4.3 Decoding
Given a trained model, we extract a tree-to-string
alignment as follows: we compute, for each node
in the English tree, the posterior probability of a
particular foreign span assignment using the same
dynamic program needed for EM. We then com-
pute the set of span assignments which maximizes
the sum of these posteriors, constrained such that
the foreign span assignments nest in the obvious
way. This algorithm is a natural synchronous gener-
alization of the monolingual Maximum Constituents
Parse algorithm of Goodman (1996).
5 Experiments
5.1 Alignment Quality
We first evaluated our alignments against gold stan-
dard annotations. Our training data consisted of the
2261 manually aligned and translated sentences of
the Chinese Treebank (Bies et al, 2007) and approx-
imately half a million unlabeled sentences of parallel
Chinese-English newswire. The unlabeled data was
subsampled (Li et al, 2009) from a larger corpus by
selecting sentences which have good tune and test
set coverage, and limited to sentences of length at
most 40. We parsed the English side of the train-
ing data with the Berkeley parser.8 For our baseline
alignments, we used GIZA++, trained in the stan-
dard way.9 We used the grow-diag-final alignment
heuristic, as we found it outperformed union in early
experiments.
We trained our unsupervised syntactic aligner on
the concatenation of the labelled and unlabelled
8http://code.google.com/p/berkeleyparser/
95 iterations of model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4.
data. As is standard in unsupervised alignment mod-
els, we initialized the translation parameters pt by
first training 5 iterations of IBM Model 1 using the
joint training algorithm of Liang et al (2006), and
then trained our model for 5 EM iterations. We
extracted syntactic rules using a re-implementation
of the Galley et al (2006) algorithm from both our
syntactic alignments and the GIZA++ alignments.
We handle null-aligned words by extracting every
consistent derivation, and extracted composed rules
consisting of at most 3 minimal rules.
We evaluate our alignments against the gold stan-
dard in two ways. We calculated Span F-score,
which compares the set of extractable nodes paired
with a foreign span, and Rule F-score (Fossum et al,
2008) over minimal rules. The results are shown in
Table 3. By both measures, our syntactic aligner ef-
fectively trades recall for precision when compared
to our baseline, slightly increasing overall F-score.
5.2 Translation Quality
For our translation system, we used a re-
implementation of the syntactic system of Galley et
al. (2006). For the translation rules extracted from
our data, we computed standard features based on
relative frequency counts, and tuned their weights
using MERT (Och, 2003). We also included a
language model feature, using a 5-gram language
model trained on 220 million words of English text
using the SRILM Toolkit (Stolcke, 2002).
For tuning and test data, we used a subset of the
NIST MT04 and MT05 with sentences of length at
most 40. We used the first 1000 sentences of this set
for tuning and the remaining 642 sentences as test
data. We used the decoder described in DeNero et
al. (2009) during both tuning and testing.
We provide final tune and test set results in Ta-
ble 4. Our alignments produce a 1.0 BLEU improve-
ment over the baseline. Our reported syntactic re-
sults were obtained when rules were thresholded by
count; we discuss this in the next section.
5.3 Analysis
As discussed in Section 3.4, our aligner is designed
to extract many rules, which risks inadvertently ex-
tracting low-quality rules. To quantify this, we
first examined the number of rules extracted by our
aligner as compared with GIZA++. After relativiz-
124
Tune Test
Syntactic Alignment 29.78 29.83
GIZA++ 28.76 28.84
GIZA++ high count 25.51 25.38
Table 4: Final tune and test set results for our grammars
extracted using the baseline GIZA++ alignments and our
syntactic aligner. When we filter the GIZA++ grammars
with the same count thresholds used for our aligner (?high
count?), BLEU score drops substantially.
ing to the tune and test set, we extracted approx-
imately 32 million unique rules using our aligner,
but only 3 million with GIZA++. To check that
we were not just extracting extra low-count, low-
quality rules, we plotted the number of rules with
a particular count in Figure 4. We found that while
our aligner certainly extracts many more low-count
rules, it also extracts many more high-count rules.
Of course, high-count rules are not guaranteed
to be high quality. To verify that frequent rules
were better for translation, we experimented with
various methods of thresholding to remove rules
with low count extracted from using aligner. We
found in early development found that removing
low-count rules improved translation performance
substantially. In particular, we settled on the follow-
ing scheme: we kept all rules with a single foreign
terminal on the right-hand side. For entirely lexical
(gapless) rules, we kept all rules occurring at least
3 times. For unlexicalized rules, we kept all rules
occurring at least 20 times per gap. For rules which
mixed gaps and lexical items, we kept all rules oc-
curring at least 10 times per gap. This left us with
a grammar about 600 000 rules, the same grammar
which gave us our final results reported in Table 4.
In contrast to our syntactic aligner, rules extracted
using GIZA++ could not be so aggressively pruned.
When pruned using the same count thresholds, ac-
curacy dropped by more than 3.0 BLEU on the tune
set, and similarly on the test set (see Table 4). To
obtain the accuracy shown in our final results (our
best results with GIZA++), we had to adjust the
count threshold to include all lexicalized rules, all
unlexicalized rules, and mixed rules occurring at
least twice per gap. With these count thresholds, the
GIZA++ grammar contained about 580 000 rules,
roughly the same number as our syntactic grammar.
We also manually searched the grammars for
rules that had high count in the syntactically-
0 200 400 600 800 1000
1e+00
1e+02
1e+04
1e+06
Count
Numbe
r of rul
es with
 count SyntacticGIZA++
Figure 4: Number of extracted translation rules with a
particular count. Grammars extracted from our syntactic
aligner produce not only more low-count rules, but also
more high-count rules than GIZA++.
extracted grammar and low (or 0) count in the
GIZA++ grammar. Of course, we can always
cherry-pick such examples, but a few rules were il-
luminating. For example, for the ? . . .?? con-
struction discussed earlier, our aligner permits ex-
traction of the general rule (PP? IN1 NP2 ;? NP2
IN1) 3087 times, and the lexicalized rule (PP? be-
fore NP ; ? NP ??) 118 times. In constrast, the
GIZA++ grammar extracts the latter only 23 times
and the former not at all. The more complex rule
(NP? NP2 , who S1 , ; S1 ? NP2), which captures
a common appositive construction, was absent from
the GIZA++ grammar but occurred 63 in ours.
6 Conclusion
We have described a syntactic alignment model
which explicitly aligns nodes of a syntactic parse in
one language to spans in another, making it suitable
for use in many syntactic translation systems. Our
model is unsupervised and can be efficiently trained
with a straightforward application of EM. We have
demonstrated that our model can accurately capture
many syntactic correspondences, and is robust in the
face of syntactic divergence between language pairs.
Our aligner permits the extraction of more reliable,
high-count rules when compared to a standard word-
alignment baseline. These high-count rules also pro-
duce improvements in BLEU score.
Acknowledgements
This project is funded in part by the NSF under grant 0643742;
by BBN under DARPA contract HR0011-06-C-0022; and an
NSERC Postgraduate Fellowship. The authors would like to
thank Michael Auli for his input.
125
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner. 2007.
English chinese translation treebank v 1.0. web download.
In LDC2007T02.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
David Burkett, John Blitzer, and Dan Klein. 2010. Joint pars-
ing and alignment with weakly synchronized grammar. In
Proceedings of the North American Association for Compu-
tational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In Pro-
ceedings of the Association of Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion transduction
grammar for joint phrasal translation modeling. In Workshop
on Syntax and Structure in Statistical Translation.
David Chiang. 2004. Evaluating grammar formalisms for ap-
plications to natural language processing and biological se-
quence analysis. Ph.D. thesis, University of Pennsylvania.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In The Annual Conference of
the Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model of
syntax-directed tree to string grammar induction. In Pro-
ceedings of the Conference on Emprical Methods for Natural
Language Processing.
John DeNero and Dan Klein. 2007. Tailoring word alignments
to syntactic machine translation. In The Annual Conference
of the Association for Computational Linguistics.
John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006.
Why generative phrase models underperform surface heuris-
tics. In Workshop on Statistical Machine Translation at
NAACL.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of NAACL.
Victoria Fossum, Kevin Knight, and Steven Abney. 2008. Us-
ing syntax to improve word alignment precision for syntax-
based machine translation. In Proceedings of the Third
Workshop on Statistical Machine Translation.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proceed-
ings of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In Proceedings of the Association for Compu-
tational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for ma-
chine translation. In Proceedings of the Association for
Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Association for Computational Linguis-
tics.
Joshua Goodman. 1998. Parsing Inside-Out. Ph.D. thesis,
Harvard University.
Aria Haghighi, John Blitzer, John Denero, and Dan Klein.
2009. Better word alignments with supervised itg models.
In Proceedings of the Association for Computational Lin-
guistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. A
syntax-directed translator with extended domain of locality.
In Proceedings of CHSLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch,
Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton,
Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: an
open source toolkit for parsing-based machine translation.
In Proceedings of the Fourth Workshop on Statistical Ma-
chine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2009. Bayesian learning of
phrasal tree-to-string templates. In Proceedings of EMNLP.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation. In
Proceedings of the Association for Computational Linguis-
tics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-
tree translation with packed forests. In Proceedings of ACL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: The Penn Treebank.
In Computational Linguistics.
Jonathan May and Kevin Knight. 2007. Syntactic re-alignment
models for machine translation. In Proceedings of the Con-
ference on Emprical Methods for Natural Language Pro-
cessing.
Robert C. Moore. 2004. Improving ibm word alignment model
1. In The Annual Conference of the Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the Association
for Computational Linguistics.
Andreas Stolcke. 2002. SRILM: An extensible language mod-
eling toolkit. In ICSLP 2002.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the Association of
Computational Linguistics.
Hao Zhang and Daniel Gildea. 2004. Syntax-based alignment:
supervised or unsupervised? In Proceedings of the Confer-
ence on Computational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation. In
Proceedings of the North American Chapter of the Associa-
tion for Computational Linguistics.
126
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447?455,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bayesian Inference for Finite-State Transducers?
David Chiang1 Jonathan Graehl1 Kevin Knight1 Adam Pauls2 Sujith Ravi1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
Abstract
We describe a Bayesian inference algorithm
that can be used to train any cascade of
weighted finite-state transducers on end-to-
end data. We also investigate the problem
of automatically selecting from among mul-
tiple training runs. Our experiments on four
different tasks demonstrate the genericity of
this framework, and, where applicable, large
improvements in performance over EM. We
also show, for unsupervised part-of-speech
tagging, that automatic run selection gives a
large improvement over previous Bayesian ap-
proaches.
1 Introduction
In this paper, we investigate Bayesian infer-
ence for weighted finite-state transducers (WFSTs).
Many natural language models can be captured
by weighted finite-state transducers (Pereira et al,
1994; Sproat et al, 1996; Knight and Al-Onaizan,
1998; Clark, 2002; Kolak et al, 2003; Mathias and
Byrne, 2006), which offer several benefits:
? WFSTs provide a uniform knowledge represen-
tation.
? Complex problems can be broken down into a
cascade of simple WFSTs.
? Input- and output-epsilon transitions allow
compact designs.
? Generic algorithms exist for doing inferences
with WFSTs. These include best-path de-
coding, k-best path extraction, composition,
?The authors are listed in alphabetical order. Please direct
correspondence to Sujith Ravi (sravi@isi.edu). This work
was supported by NSF grant IIS-0904684 and DARPA contract
HR0011-06-C0022.
intersection, minimization, determinization,
forward-backward training, forward-backward
pruning, stochastic generation, and projection.
? Software toolkits implement these generic al-
gorithms, allowing designers to concentrate on
novel models rather than problem-specific in-
ference code. This leads to faster scientific ex-
perimentation with fewer bugs.
Weighted tree transducers play the same role for
problems that involve the creation and transforma-
tion of tree structures (Knight and Graehl, 2005). Of
course, many problems do not fit either the finite-
state string or tree transducer framework, but in this
paper, we concentrate on those that do.
Bayesian inference schemes have become popu-
lar recently in natural language processing for their
ability to manage uncertainty about model param-
eters and to allow designers to incorporate prior
knowledge flexibly. Task-accuracy results have gen-
erally been favorable. However, it can be time-
consuming to apply Bayesian inference methods to
each new problem. Designers typically build cus-
tom, problem-specific sampling operators for ex-
ploring the derivation space. They may factor their
programs to get some code re-use from one problem
to the next, but highly generic tools for string and
tree processing are not available.
In this paper, we marry the world of finite-state
machines with the world of Bayesian inference, and
we test our methods across a range of natural lan-
guage problems. Our contributions are:
? We describe a Bayesian inference algorithm
that can be used to train any cascade of WFSTs
on end-to-end data.
? We propose a method for automatic run selec-
447
tion, i.e., how to automatically select among
multiple training runs in order to achieve the
best possible task accuracy.
The natural language applications we consider
in this paper are: (1) unsupervised part-of-speech
(POS) tagging (Merialdo, 1994; Goldwater and
Griffiths, 2007), (2) letter substitution decipher-
ment (Peleg and Rosenfeld, 1979; Knight et al,
2006; Ravi and Knight, 2008), (3) segmentation of
space-free English (Goldwater et al, 2009), and (4)
Japanese/English phoneme alignment (Knight and
Graehl, 1998; Ravi and Knight, 2009a). Figure 1
shows how each of these problems can be repre-
sented as a cascade of finite-state acceptors (FSAs)
and finite-state transducers (FSTs).
2 Generic EM Training
We first describe forward-backward EM training for
a single FST M. Given a string pair (v,w) from our
training data, we transform v into an FST Mv that
just maps v to itself, and likewise transform w into
an FST Mw. Then we compose Mv with M, and com-
pose the result with Mw. This composition follows
Pereira and Riley (1996), treating epsilon input and
output transitions correctly, especially with regards
to their weighted interleaving. This yields a deriva-
tion lattice D, each of whose paths transforms v into
w.1 Each transition in D corresponds to some tran-
sition in the FST M. We run the forward-backward
algorithm over D to collect fractional counts for the
transitions in M. After we sum fractional counts for
all examples, we normalize with respect to com-
peting transitions in M, assign new probabilities to
M, and iterate. Transitions in M compete with each
other if they leave the same state with the same input
symbol, which may be empty ().
In order to train an FSA on observed string data,
we convert the FSA into an FST by adding an input-
epsilon to every transition. We then convert each
training string v into the string pair (, v). After run-
ning the above FST training algorithm, we can re-
move all input- from the trained machine.
It is straightforward to modify generic training to
support the following controls:
1Throughout this paper, we do not assume that lattices are
acyclic; the algorithms described work on general graphs.
B:E
a:A b:B A:D
A:C
=
a: 
 :D
 :E b: 
a:  :C
Figure 2: Composition of two FSTs maintaining separate
transitions.
Maximum iterations and early stopping. We spec-
ify a maximum number of iterations, and we halt
early if the ratio of log P(data) from one iteration
to the next exceeds a threshold (such as 0.99999).
Initial point. Any probabilities supplied on the pre-
trained FST are interpreted as a starting point for
EM?s search. If no probabilities are supplied, EM
begins with uniform probabilities.
Random restarts. We can request n random restarts,
each from a different, randomly-selected initial
point.
Locking and tying. Transitions on the pre-trained
FST can be marked as locked, in which case EM
will not modify their supplied probabilities. Groups
of transitions can be tied together so that their frac-
tional counts are pooled, and when normalization
occurs, they all receive the same probability.
Derivation lattice caching. If memory is available,
training can cache the derivation lattices computed
in the first EM iteration for all training pairs. Subse-
quent iterations then run much faster. In our experi-
ments, we observe an average 10-fold speedup with
caching.
Next we turn to training a cascade of FSTs on
end-to-end data. The algorithm takes as input: (1) a
sequence of FSTs, and (2) pairs of training strings
(v,w), such that v is accepted by the first FST in
the cascade, and w is produced by the last FST. The
algorithm outputs the same sequence of FSTs, but
with trained probabilities.
To accomplish this, we first compose the supplied
FSTs, taking care to keep the transitions from differ-
ent machines separate. Figure 2 illustrates this with a
small example. It may thus happen that a single tran-
sition in an input FST is represented multiple times
in the composed device, in which case their prob-
448
ABCD:a 
REY:r 
?:c 
1.  Unsupervised part-of-speech tagging with constrained dictionary 
POS Tag 
sequence 
Observed 
word 
sequence 
2.  Decipherment of letter-substitution cipher 
English 
letter 
sequence 
Observed 
enciphered 
text 
3.  Re-Spacing of English text written without spaces 
Word 
sequence 
Observed 
letter 
sequence 
w/o spaces 
4.  Alignment of Japanese/English phoneme sequences 
English 
phoneme 
sequence 
Japanese 
katakana 
phoneme 
sequence 
26 x 26 table 
letter bigram model, 
learned separately 
constrained tag?word 
substitution model tag bigram model 
unigram model over 
words and non-words deterministic spell-out 
mapping from each English  
phoneme to each Japanese  
phoneme sequence of length 1 to 3 
NN 
JJ 
JJ 
JJ 
NN 
VB ? 
? 
? 
NN:fish 
IN:at 
VB:fish 
SYM:a DT:a 
a 
b 
b 
b 
a 
c ? 
? 
? 
a:A 
a:B 
a:C 
b:A b:B b:C 
A AR 
ARE AREY 
AREYO 
?:? 
AREY:a 
?:b 
?:d 
?:r ?:e 
?:y 
AE:? 
?:S 
?:S 
?:U 
Figure 1: Finite-state cascades for five natural language problems.
449
abilities are tied together. Next, we run FST train-
ing on the end-to-end data. This involves creating
derivation lattices and running forward-backward on
them. After FST training, we de-compose the trained
device back into a cascade of trained machines.
When the cascade?s first machine is an FSA,
rather than an FST, then the entire cascade is viewed
as a generator of strings rather than a transformer of
strings. Such a cascade is trained on observed strings
rather than string pairs. By again treating the first
FSA as an FST with empty input, we can train using
the FST-cascade training algorithm described in the
previous paragraph.
Once we have our trained cascade, we can apply it
to new data, obtaining (for example) the k-best out-
put strings for an input string.
3 Generic Bayesian Training
Bayesian learning is a wide-ranging field. We focus
on training using Gibbs sampling (Geman and Ge-
man, 1984), because it has been popularly applied
in the natural language literature, e.g., (Finkel et al,
2005; DeNero et al, 2008; Blunsom et al, 2009).
Our overall plan is to give a generic algorithm
for Bayesian training that is a ?drop-in replacement?
for EM training. That is, we input an FST cas-
cade and data and output the same FST cascade
with trained weights. This is an approximation to a
purely Bayesian setup (where one would always in-
tegrate over all possible weightings), but one which
makes it easy to deploy FSTs to efficiently decode
new data. Likewise, we do not yet support non-
parametric approaches?to create a drop-in replace-
ment for EM, we require that all parameters be spec-
ified in the initial FST cascade. We return to this is-
sue in Section 5.
3.1 Particular Case
We start with a well-known application of Bayesian
inference, unsupervised POS tagging (Goldwater
and Griffiths, 2007). Raw training text is provided,
and each potential corpus tagging corresponds to a
hidden derivation of that data. Derivations are cre-
ated and probabilistically scored as follows:
1. i? 1
2. Choose tag t1 according to P0(t1)
3. Choose word w1 according to P0(w1 | t1)
4. i? i + 1
5. Choose tag ti according to
?P0(ti | ti?1) + ci?11 (ti?1, ti)
? + ci?11 (ti?1)
(1)
6. Choose word wi according to
?P0(wi | ti) + ci?11 (ti,wi)
? + ci?11 (ti)
(2)
7. With probability Pquit, quit; else go to 4.
This defines the probability of any given derivation.
The base distribution P0 represents prior knowl-
edge about the distribution of tags and words, given
the relevant conditioning context. The ci?11 are the
counts of events occurring before word i in the
derivation (the ?cache?).
When ? and ? are large, tags and words are essen-
tially generated according to P0. When ? and ? are
small, tags and words are generated with reference
to previous decisions inside the cache.
We use Gibbs sampling to estimate the distribu-
tion of tags given words. The key to efficient sam-
pling is to define a sampling operator that makes
some small change to the overall corpus derivation.
With such an operator, we derive an incremental
formula for re-scoring the probability of an entire
new derivation based on the probability of the old
derivation. Exchangeability makes this efficient?
we pretend like the area around the small change oc-
curs at the end of the corpus, so that both old and
new derivations share the same cache. Goldwater
and Griffiths (2007) choose the re-sampling operator
?change the tag of a single word,? and they derive
the corresponding incremental scoring formula for
unsupervised tagging. For other problems, design-
ers develop different sampling operators and derive
different incremental scoring formulas.
3.2 Generic Case
In order to develop a generic algorithm, we need
to abstract away from these problem-specific de-
sign choices. In general, hidden derivations corre-
spond to paths through derivation lattices, so we first
450
Figure 3: Changing a decision in the derivation lattice.
All paths generate the observed data. The bold path rep-
resents the current sample, and the dotted path represents
a sidetrack in which one decision is changed.
compute derivation lattices for our observed training
data through our cascade of FSTs. A random path
through these lattices constitutes the initial sample,
and we calculate its derivation probability directly.
One way to think about a generic small change
operator is to consider a single transition in the cur-
rent sample. This transition will generally compete
with other transitions. One possible small change is
to ?sidetrack? the derivation to a competing deriva-
tion. Figure 3 shows how this works. If the sidetrack
path quickly re-joins the old derivation path, then an
incremental score can be computed. However, side-
tracking raises knotty questions. First, what is the
proper path continuation after the sidetracking tran-
sition is selected? Should the path attempt to re-join
the old derivation as soon as possible, and if so, how
is this efficiently done? Then, how can we compute
new derivation scores for all possible sidetracks, so
that we can choose a new sample by an appropriate
weighted coin flip? Finally, would such a sampler be
reversible? In order to satisfy theoretical conditions
for Gibbs sampling, if we move from sample A to
sample B, we must be able to immediately get back
to sample A.
We take a different tack here, moving from point-
wise sampling to blocked sampling. Gao and John-
son (2008) employed blocked sampling for POS tag-
ging, and the approach works nicely for arbitrary
derivation lattices. We again start with a random
derivation for each example in the corpus. We then
choose a training example and exchange its entire
derivation lattice to the end of the corpus. We cre-
ate a weighted version of this lattice, called the pro-
posal lattice, such that we can approximately sample
whole paths by stochastic generation. The probabil-
ities are based on the event counts from the rest of
the sample (the cache), and on the base distribution,
and are computed in this way:
P(r | q) =
?P0(r | q) + c(q, r)
? + c(q)
(3)
where q and r are states of the derivation lattice, and
the c(?) are counts collected from the corpus minus
the entire training example being resampled. This is
an approximation because we are ignoring the fact
that P(r | q) in general depends on choices made
earlier in the lattice. The approximation can be cor-
rected using the Metropolis-Hastings algorithm, in
which the sample drawn from the proposal lattice is
accepted only with a certain probability ?; but Gao
and Johnson (2008) report that ? > 0.99, so we skip
this step.
3.3 Choosing the best derivations
After the sampling run has finished, we can choose
the best derivations using two different methods.
First, if we want to find the MAP derivations of the
training strings, then following Goldwater and Grif-
fiths (2007), we can use annealing: raise the proba-
bilities in the sampling distribution to the 1T power,
where T is a temperature parameter, decrease T to-
wards zero, and take a single sample.
But in practice one often wants to predict the
MAP derivation for a new string w? not contained
in the training data. To approximate the distribution
of derivations of w? given the training data, we aver-
age the transition counts from all the samples (after
burn-in) and plug the averaged counts into (3) to ob-
tain a single proposal lattice.2 The predicted deriva-
tion is the Viterbi path through this lattice. Call this
method averaging. An advantage of this approach is
that the trainer, taking a cascade of FSAs as input,
outputs a weighted version of the same cascade, and
this trained cascade can be used on unseen examples
without having to rerun training.
3.4 Implementation
That concludes the generic Bayesian training algo-
rithm, to which we add the following controls:
2A better approximation might have been to build a proposal
lattice for each sample (after burn-in), and then construct a sin-
gle FSA that computes the average of the probability distribu-
tions computed by all the proposal lattices. But this FSA would
be rather large.
451
Number of Gibbs sampling iterations. We execute
the full number specified.
Base distribution. Any probabilities supplied on the
pre-trained FST are interpreted as base distribution
probabilities. If no probabilities are supplied, then
the base distribution is taken to be uniform.
Hyperparameters. We supply a distinct ? for each
machine in the FST cascade. We do not yet support
different ? values for different states within a single
FST.
Random restarts. We can request multiple runs
from different, randomly-selected initial samples.
EM-based initial point. If random initial samples
are undesirable, we can request that the Gibbs sam-
pler be initialized with the Viterbi path using param-
eter values obtained by n iterations of EM.
Annealing schedule. If annealing is used, it follows
a linear annealing schedule with starting and stop-
ping temperature specified by the user.
EM and Bayesian training for arbitrary FST
cascades are both implemented in the finite-state
toolkit Carmel, which is distributed with source
code.3 All controls are implemented as command-
line switches. We use Carmel to carry out the exper-
iments in the next section.
4 Run Selection
For both EM and Bayesian methods, different train-
ing runs yield different results. EM?s objective func-
tion (probability of observed data) is very bumpy for
the unsupervised problems we work on?different
initial points yield different trained WFST cascades,
with different task accuracies. Averaging task accu-
racies across runs is undesirable, because we want to
deploy a particular trained cascade in the real world,
and we want an estimate of its performance. Select-
ing the run with the best task accuracy is illegal in an
unsupervised setting. With EM, we have a good al-
ternative: select the run that maximizes the objective
function, i.e., the likelihood of the observed training
data. We find a decent correlation between this value
and task accuracy, and we are generally able to im-
prove accuracy using this run selection method. Fig-
ure 4 shows a scatterplot of 1000 runs for POS tag-
ging. A single run with a uniform start yields 81.8%
3http://www.isi.edu/licensed-sw/carmel
 0.75
 0.8
 0.85
 0.9
 211200
 211300
 211400
 211500
 211600
 211700
 211800
 211900
 212000
 212100
 212200
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(data)
EM (random start)EM (uniform start)
Figure 4: Multiple EM restarts for POS tagging. Each
point represents one random restart; the y-axis is tag-
ging accuracy and the x-axis is EM?s objective function,
? log P(data).
accuracy, while automatic selection from 1000 runs
yields 82.4% accuracy.
Gibbs sampling runs also yield WFST cascades
with varying task accuracies, due to random initial
samples and sampling decisions. In fact, the varia-
tion is even larger than what we find with EM. It is
natural to ask whether we can do automatic run se-
lection for Gibbs sampling. If we are using anneal-
ing, it makes sense to use the probability of the fi-
nal sample, which is supposed to approximate the
MAP derivation. When using averaging, however,
choosing the final sample would be quite arbitrary.
Instead, we propose choosing the run that has the
highest average log-probability (that is, the lowest
entropy) after burn-in. The rationale is that the runs
that have found their way to high-probability peaks
are probably more representative of the true distri-
bution, or at least capture a part of the distribution
that is of greater interest to us.
We find that this method works quite well in prac-
tice. Figure 5 illustrates 1000 POS tagging runs
for annealing with automatic run selection, yield-
ing 84.7% accuracy. When using averaging, how-
ever, automatic selection from 1000 runs (Figure 6)
produces a much higher accuracy of 90.7%. This
is better than accuracies reported previously using
452
 0.75
 0.8
 0.85
 0.9
 235100
 235150
 235200
 235250
 235300
 235350
 235400
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) for final sample
Bayesian run (with annealing)
Figure 5: Multiple Bayesian learning runs (using anneal-
ing with temperature decreasing from 2 to 0.08) for POS
tagging. Each point represents one run; the y-axis is tag-
ging accuracy and the x-axis is the ? log P(derivation) of
the final sample.
 0.75
 0.8
 0.85
 0.9
 236800
 236900
 237000
 237100
 237200
 237300
 237400
 237500
 237600
 237700
 237800
 237900
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) averaged over all post-burnin samples
Bayesian run (using averaging)
Figure 6: Multiple Bayesian learning runs (using averag-
ing) for POS tagging. Each point represents one run; the
y-axis is tagging accuracy and the x-axis is the average
? log P(derivation) over all samples after burn-in.
Bayesian methods (85.2% from Goldwater and Grif-
fiths (2007), who use a trigram model) and close to
the best accuracy reported on this task (91.8% from
Ravi and Knight (2009b), who use an integer linear
program to minimize the model directly).
5 Experiments and Results
We run experiments for various natural language ap-
plications and compare the task accuracies achieved
by the EM and Bayesian learning methods. The
tasks we consider are:
Unsupervised POS tagging. We adopt the com-
mon problem formulation for this task described
by Merialdo (1994), in which we are given a raw
24,115-word sequence and a dictionary of legal tags
for each word type. The tagset consists of 45 dis-
tinct grammatical tags. We use the same modeling
approach as as Goldwater and Griffiths (2007), us-
ing a probabilistic tag bigram model in conjunction
with a tag-to-word model.
Letter substitution decipherment. Here, the task
is to decipher a 414-letter substitution cipher and un-
cover the original English letter sequence. The task
accuracy is defined as the percent of ciphertext to-
kens that are deciphered correctly. We work on the
same standard cipher described in previous litera-
ture (Ravi and Knight, 2008). The model consists
of an English letter bigram model, whose probabil-
ities are fixed and an English-to-ciphertext channel
model, which is learnt during training.
Segmentation of space-free English. Given
a space-free English text corpus (e.g.,
iwalkedtothe...), the task is to segment the
text into words (e.g., i walked to the ...).
Our input text corpus consists of 11,378 words,
with spaces removed. As illustrated in Figure 1,
our method uses a unigram FSA that models every
letter sequence seen in the data, which includes
both words and non-words (at most 10 letters long)
composed with a deterministic spell-out model.
In order to evaluate the quality of our segmented
output, we compare it against the gold segmentation
and compute the word token f-measure.
Japanese/English phoneme alignment. We
use the problem formulation of Knight and
Graehl (1998). Given an input English/Japanese
katakana phoneme sequence pair, the task is to
produce an alignment that connects each English
453
MLE Bayesian
EM prior VB-EM Gibbs
POS tagging 82.4 ? = 10?2, ? = 10?1 84.1 90.7
Letter decipherment 83.6 ? = 106, ? = 10?2 83.6 88.9
Re-spacing English 0.9 ? = 10?8, ? = 104 0.8 42.8
Aligning phoneme strings? 100 ? = 10?2 99.9 99.1
Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. ?The output of
EM alignment was used as the gold standard.
phoneme to its corresponding Japanese sounds (a
sequence of one or more Japanese phonemes). For
example, given a phoneme sequence pair ((AH B
AW T) ? (a b a u t o)), we have to produce
the alignments ((AH ? a), (B ? b), (AW ?
a u), (T ? t o)). The input data consists of
2,684 English/Japanese phoneme sequence pairs.
We use a model that consists of mappings from each
English phoneme to Japanese phoneme sequences
(of length up to 3), and the mapping probabilities
are learnt during training. We manually analyzed
the alignments produced by the EM method for
this task and found them to be nearly perfect.
Hence, for the purpose of this task we treat the EM
alignments as our gold standard, since there are no
gold alignments available for this data.
In all the experiments reported here, we run EM
for 200 iterations and Bayesian for 5000 iterations
(the first 2000 for burn-in). We apply automatic run
selection using the objective function value for EM
and the averaging method for Bayesian.
Table 1 shows accuracy results for our four tasks,
using run selection for both EM and Bayesian learn-
ing. For the Bayesian runs, we compared two infer-
ence methods: Gibbs sampling, as described above,
and Variational Bayesian EM (Beal and Ghahra-
mani, 2003), both of which are implemented in
Carmel. We used the hyperparameters (?, ?) as
shown in the table. Setting a high value yields a fi-
nal distribution that is close to the original one (P0).
For example, in letter decipherment we want to keep
the language model probabilities fixed during train-
ing, and hence we set the prior on that model to
be very strong (? = 106). Table 1 shows that the
Bayesian methods consistently outperform EM for
all the tasks (except phoneme alignment, where EM
was taken as the gold standard). Each iteration of
Gibbs sampling was 2.3 times slower than EM for
POS tagging, and in general about twice as slow.
6 Discussion
We have described general training algorithms for
FST cascades and their implementation, and exam-
ined the problem of run selection for both EM and
Bayesian training. This work raises several interest-
ing points for future study.
First, is there an efficient method for perform-
ing pointwise sampling on general FSTs, and would
pointwise sampling deliver better empirical results
than blocked sampling across a range of tasks?
Second, can generic methods similar to the ones
described here be developed for cascades of tree
transducers? It is straightforward to adapt our meth-
ods to train a single tree transducer (Graehl et al,
2008), but as most types of tree transducers are
not closed under composition (Ge?cseg and Steinby,
1984), the compose/de-compose method cannot be
directly applied to train cascades.
Third, what is the best way to extend the FST for-
malism to represent non-parametric Bayesian mod-
els? Consider the English re-spacing application. We
currently take observed (un-spaced) data and build
a giant unigram FSA that models every letter se-
quence seen in the data of up to 10 letters, both
words and non-words. This FSA has 207,253 tran-
sitions. We also define P0 for each individual transi-
tion, which allows a preference for short words. This
set-up works fine, but in a nonparametric approach,
P0 is defined more compactly and without a word-
length limit. An extension of FSTs along the lines
of recursive transition networks may be appropriate,
but we leave details for future work.
454
References
Matthew J. Beal and Zoubin Ghahramani. 2003. The
Variational Bayesian EM algorithm for incomplete
data: with application to scoring graphical model
structures. Bayesian Statistics, 7:453?464.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL-
IJCNLP 2009.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings
of ACL 2002.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of ACL 2005.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In Proceedings of EMNLP 2008.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Proceedings of AMTA
1998.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Knight Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of CICLing-2005.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING-ACL 2006.
Okan Kolak, Willian Byrne, and Philip Resnik. 2003. A
generative probabilistic OCR model for NLP applica-
tions. In Proceedings of HLT-NAACL 2003.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP 2006.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Communications of the ACM, 22(11):598?605.
Fernando C. N. Pereira and Michael D. Riley. 1996.
Speech recognition by composition of weighted finite
automata. Finite-State Language Processing, pages
431?453.
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their appli-
cations to human language processing. In ARPA Hu-
man Language Technology Workshop.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP 2008.
Sujith Ravi and Kevin Knight. 2009a. Learning
phoneme mappings for transliteration without parallel
data. In Proceedings of NAACL HLT 2009.
Sujith Ravi and Kevin Knight. 2009b. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
455
Proceedings of the ACL 2010 Conference Short Papers, pages 200?204,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Top-Down K-Best A? Parsing
Adam Pauls and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
Chris Quirk
Microsoft Research
Redmond, WA, 98052
chrisq@microsoft.com
Abstract
We propose a top-down algorithm for ex-
tracting k-best lists from a parser. Our
algorithm, TKA? is a variant of the k-
best A? (KA?) algorithm of Pauls and
Klein (2009). In contrast to KA?, which
performs an inside and outside pass be-
fore performing k-best extraction bottom
up, TKA? performs only the inside pass
before extracting k-best lists top down.
TKA? maintains the same optimality and
efficiency guarantees of KA?, but is sim-
pler to both specify and implement.
1 Introduction
Many situations call for a parser to return a k-
best list of parses instead of a single best hypothe-
sis.1 Currently, there are two efficient approaches
known in the literature. The k-best algorithm of
Jime?nez and Marzal (2000) and Huang and Chi-
ang (2005), referred to hereafter as LAZY, oper-
ates by first performing an exhaustive Viterbi in-
side pass and then lazily extracting k-best lists in
top-down manner. The k-best A? algorithm of
Pauls and Klein (2009), hereafter KA?, computes
Viterbi inside and outside scores before extracting
k-best lists bottom up.
Because these additional passes are only partial,
KA? can be significantly faster than LAZY, espe-
cially when a heuristic is used (Pauls and Klein,
2009). In this paper, we propose TKA?, a top-
down variant of KA? that, like LAZY, performs
only an inside pass before extracting k-best lists
top-down, but maintains the same optimality and
efficiency guarantees as KA?. This algorithm can
be seen as a generalization of the lattice k-best al-
gorithm of Soong and Huang (1991) to parsing.
Because TKA? eliminates the outside pass from
KA?, TKA? is simpler both in implementation and
specification.
1See Huang and Chiang (2005) for a review.
2 Review
Because our algorithm is very similar to KA?,
which is in turn an extension of the (1-best) A?
parsing algorithm of Klein and Manning (2003),
we first introduce notation and review those two
algorithms before presenting our new algorithm.
2.1 Notation
Assume we have a PCFG2 G and an input sen-
tence s0 . . . sn?1 of length n. The grammar G has
a set of symbols denoted by capital letters, includ-
ing a distinguished goal (root) symbol G. With-
out loss of generality, we assume Chomsky nor-
mal form: each non-terminal rule r in G has the
form r = A ? B C with weight wr. Edges
are labeled spans e = (A, i, j). Inside deriva-
tions of an edge (A, i, j) are trees with root non-
terminalA, spanning si . . . sj?1. The weight (neg-
ative log-probability) of the best (minimum) inside
derivation for an edge e is called the Viterbi in-
side score ?(e), and the weight of the best deriva-
tion of G ? s0 . . . si?1 A sj . . . sn?1 is called
the Viterbi outside score ?(e). The goal of a k-
best parsing algorithm is to compute the k best
(minimum weight) inside derivations of the edge
(G, 0, n).
We formulate the algorithms in this paper
in terms of prioritized weighted deduction rules
(Shieber et al, 1995; Nederhof, 2003). A prior-
itized weighted deduction rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
2While we present the algorithm specialized to parsing
with a PCFG, this algorithm generalizes to a wide range of
200
VP
s
2
s
3
s
4
s
0
s
2
... s
5
s
n-1
...
VP
VBZ NP
DT NN
s
2
s
3
s
4
VP
G
(a) (b)
(c)
VP
VP NP
s
1
s
2
s
n-1
(d) G
s
0
NN
NP
Figure 1: Representations of the different types of items
used in parsing. (a) An inside edge item I(VP, 2, 5). (b)
An outside edge item O(VP, 2, 5). (c) An inside deriva-
tion item: D(TVP, 2, 5). (d) An outside derivation item:
Q(TGVP, 1, 2, {(NP, 2, n)}. The edges in boldface are fron-
tier edges.
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item ? from the
agenda, put it into the chart with its current weight,
and apply deduction rules to form any items which
can be built by combining ? with items already
in the chart. When the resulting items are either
new or have a weight smaller than an item?s best
score so far, they are put on the agenda with pri-
ority given by p(?). Because all antecedents must
be constructed before a deduction rule is executed,
we sometimes refer to particular conclusion item
as ?waiting? on another item before it can be built.
2.2 A?
A? parsing (Klein and Manning, 2003) is an al-
gorithm for computing the 1-best parse of a sen-
tence. A? operates on items called inside edge
items I(A, i, j), which represent the many pos-
sible inside derivations of an edge (A, i, j). In-
side edge items are constructed according to the
IN deduction rule of Table 1. This deduction rule
constructs inside edge items in a bottom-up fash-
ion, combining items representing smaller edges
I(B, i, k) and I(C, k, j) with a grammar rule r =
A ? B C to form a larger item I(A, i, j). The
weight of a newly constructed item is given by the
sum of the weights of the antecedent items and
the grammar rule r, and its priority is given by
hypergraph search problems as shown in Klein and Manning
(2001).
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP
NP
s
1
s
2
s
3
G
s
0
NN
NP
s
4
s
5
VP
VP NN
(a)
(b)
Figure 2: (a) An outside derivation item before expansion at
the edge (VP, 1, 4). (b) A possible expansion of the item in
(a) using the rule VP? VP NN. Frontier edges are marked in
boldface.
its weight plus a heuristic h(A, i, j). For consis-
tent and admissible heuristics h(?), this deduction
rule guarantees that when an inside edge item is
removed from the agenda, its current weight is its
true Viterbi inside score.
The heuristic h controls the speed of the algo-
rithm. It can be shown that an edge e satisfying
?(e) + h(A, i, j) > ?(G, 0, n) will never be re-
moved from the agenda, allowing some edges to
be safely pruned during parsing. The more closely
h(e) approximates the Viterbi outside cost ?(e),
the more items are pruned.
2.3 KA?
The use of inside edge items in A? exploits the op-
timal substructure property of derivations ? since
a best derivation of a larger edge is always com-
posed of best derivations of smaller edges, it is
only necessary to compute the best way of build-
ing a particular inside edge item. When finding
k-best lists, this is no longer possible, since we are
interested in suboptimal derivations.
Thus, KA?, the k-best extension of A?, must
search not in the space of inside edge items,
but rather in the space of inside derivation items
D(TA, i, j), which represent specific derivations
of the edge (A, i, j) using tree TA. However, the
number of inside derivation items is exponential
in the length of the input sentence, and even with
a very accurate heuristic, running A? directly in
this space is not feasible.
Fortunately, Pauls and Klein (2009) show that
with a perfect heuristic, that is, h(e) = ?(e) ?e,
A? search on inside derivation items will only
remove items from the agenda that participate
in the true k-best lists (up to ties). In order
to compute this perfect heuristic, KA? makes
use of outside edge items O(A, i, j) which rep-
resent the many possible derivations of G ?
201
IN??: I(B, i, l) : w1 I(C, l, j) : w2
w1+w2+wr+h(A,i,j)??????????????? I(A, i, j) : w1 + w2 + wr
IN-D?: O(A, i, j) : w1 D(TB , i, l) : w2 D(TC , l, j) : w3
w2+w3+wr+w1??????????? D(TA, i, j) : w2 + w3 + wr
OUT-L?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w3+wr+w2??????????? O(B, i, l) : w1 + w3 + wr
OUT-R?: O(A, i, j) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+w2+wr+w3??????????? O(C, l, j) : w1 + w2 + wr
OUT-D?: Q(TGA , i, j,F) : w1 I(B, i, l) : w2 I(C, l, j) : w3
w1+wr+w2+w3+?(F)???????????????? Q(TGB , i, l,FC) : w1 + wr
Table 1: The deduction rules used in this paper. Here, r is the rule A ? B C. A superscript * indicates that the rule is used
in TKA?, and a superscript ? indicates that the rule is used in KA?. In IN-D, the tree TA is rooted at (A, i, j) and has children
TB and TC . In OUT-D, the tree TGB is the tree T
G
A extended at (A, i, j) with rule r, FC is the list F with (C, l, j) prepended,
and ?(F) is
P
e?F ?(e). Whenever the left child I(B, i, l) of an application of OUT-D represents a terminal, the next edge is
removed from F and is used as the new point of expansion.
s1 . . . si A sj+1 . . . sn (see Figure 1(b)).
Outside items are built using the OUT-L and
OUT-R deduction rules shown in Table 1. OUT-
L and OUT-R combine, in a top-down fashion, an
outside edge over a larger span and inside edge
over a smaller span to form a new outside edge
over a smaller span. Because these rules make ref-
erence to inside edge items I(A, i, j), these items
must also be built using the IN deduction rules
from 1-best A?. Outside edge items must thus wait
until the necessary inside edge items have been
built. The outside pass is initialized with the item
O(G, 0, n) when the inside edge item I(G, 0, n) is
popped from the agenda.
Once we have started populating outside scores
using the outside deductions, we can initiate a
search on inside derivation items.3 These items
are built bottom-up using the IN-D deduction rule.
The crucial element of this rule is that derivation
items for a particular edge wait until the exact out-
side score of that edge has been computed. The al-
gorithm terminates when k derivation items rooted
at (G, 0, n) have been popped from the agenda.
3 TKA?
KA? efficiently explores the space of inside
derivation items because it waits for the exact
Viterbi outside cost before building each deriva-
tion item. However, these outside costs and asso-
ciated deduction items are only auxiliary quanti-
ties used to guide the exploration of inside deriva-
tions: they allow KA? to prioritize currently con-
structed inside derivation items (i.e., constructed
derivations of the goal) by their optimal comple-
tion costs. Outside costs are thus only necessary
because we construct partial derivations bottom-
up; if we constructed partial derivations in a top-
down fashion, all we would need to compute opti-
3We stress that the order of computation is entirely speci-
fied by the deduction rules ? we only speak about e.g. ?initi-
ating a search? as an appeal to intuition.
mal completion costs are Viterbi inside scores, and
we could forget the outside pass.
TKA? does exactly that. Inside edge items are
constructed in the same way as KA?, but once the
inside edge item I(G, 0, n) has been discovered,
TKA? begins building partial derivations from the
goal outwards. We replace the inside derivation
items of KA? with outside derivation items, which
represent trees rooted at the goal and expanding
downwards. These items bottom out in a list of
edges called the frontier edges. See Figure 1(d)
for a graphical representation. When a frontier
edge represents a single word in the input, i.e. is
of the form (si, i, i+ 1), we say that edge is com-
plete. An outside derivation can be expanded by
applying a rule to one of its incomplete frontier
edges; see Figure 2. In the same way that inside
derivation items wait on exact outside scores be-
fore being built, outside derivation items wait on
the inside edge items of all frontier edges before
they can be constructed.
Although building derivations top-down obvi-
ates the need for a 1-best outside pass, it raises a
new issue. When building derivations bottom-up,
the only way to expand a particular partial inside
derivation is to combine it with another partial in-
side derivation to build a bigger tree. In contrast,
an outside derivation item can be expanded any-
where along its frontier. Naively building deriva-
tions top-down would lead to a prohibitively large
number of expansion choices.
We solve this issue by always expanding the
left-most incomplete frontier edge of an outside
derivation item. We show the deduction rule
OUT-D which performs this deduction in Fig-
ure 1(d). We denote an outside derivation item as
Q(TGA , i, j,F), where T
G
A is a tree rooted at the
goal with left-most incomplete edge (A, i, j), and
F is the list of incomplete frontier edges exclud-
ing (A, i, j), ordered from left to right. Whenever
the application of this rule ?completes? the left-
202
most edge, the next edge is removed from F and
is used as the new point of expansion. Once all
frontier edges are complete, the item represents a
correctly scored derivation of the goal, explored in
a pre-order traversal.
3.1 Correctness
It should be clear that expanding the left-most in-
complete frontier edge first eventually explores the
same set of derivations as expanding all frontier
edges simultaneously. The only worry in fixing
this canonical order is that we will somehow ex-
plore the Q items in an incorrect order, possibly
building some complete derivation Q?C before a
more optimal complete derivation QC . However,
note that all items Q along the left-most construc-
tion ofQC have priority equal to or better than any
less optimal complete derivation Q?C . Therefore,
when Q?C is enqueued, it will have lower priority
than all Q; Q?C will therefore not be dequeued un-
til all Q ? and hence QC ? have been built.
Furthermore, it can be shown that the top-down
expansion strategy maintains the same efficiency
and optimality guarantees as KA? for all item
types: for consistent heuristics h, the first k en-
tirely complete outside derivation items are the
true k-best derivations (modulo ties), and that only
derivation items which participate in those k-best
derivations will be removed from the queue (up to
ties).
3.2 Implementation Details
Building derivations bottom-up is convenient from
an indexing point of view: since larger derivations
are built from smaller ones, it is not necessary to
construct the larger derivation from scratch. In-
stead, one can simply construct a new tree whose
children point to the old trees, saving both mem-
ory and CPU time.
In order keep the same efficiency when build-
ing trees top-down, a slightly different data struc-
ture is necessary. We represent top-down deriva-
tions as a lazy list of expansions. The top node
TGG is an empty list, and whenever we expand an
outside derivation item Q(TGA , i, j,F) with a rule
r = A ? B C and split point l, the resulting
derivation TGB is a new list item with (r, l) as the
head data, and TGA as its tail. The tree can be re-
constructed later by recursively reconstructing the
parent, and adding the edges (B, i, l) and (C, l, j)
as children of (A, i, j).
3.3 Advantages
Although our algorithm eliminates the 1-best out-
side pass of KA?, in practice, even for k = 104,
the 1-best inside pass remains the overwhelming
bottleneck (Pauls and Klein, 2009), and our modi-
fications leave that pass unchanged.
However, we argue that our implementation is
simpler to specify and implement. In terms of de-
duction rules, our algorithm eliminates the 2 out-
side deduction rules and replaces the IN-D rule
with the OUT-D rule, bringing the total number
of rules from four to two.
The ease of specification translates directly into
ease of implementation. In particular, if high-
quality heuristics are not available, it is often more
efficient to implement the 1-best inside pass as
an exhaustive dynamic program, as in Huang and
Chiang (2005). In this case, one would only need
to implement a single, agenda-based k-best extrac-
tion phase, instead of the 2 needed for KA?.
3.4 Performance
The contribution of this paper is theoretical, not
empirical. We have argued that TKA? is simpler
than TKA?, but we do not expect it to do any more
or less work than KA?, modulo grammar specific
optimizations. Therefore, we simply verify, like
KA?, that the additional work of extracting k-best
lists with TKA? is negligible compared to the time
spent building 1-best inside edges.
We examined the time spent building 100-best
lists for the same experimental setup as Pauls and
Klein (2009).4 On 100 sentences, our implemen-
tation of TKA? constructed 3.46 billion items, of
which about 2% were outside derivation items.
Our implementation of KA? constructed 3.41 bil-
lion edges, of which about 0.1% were outside edge
items or inside derivation items. In other words,
the cost of k-best extraction is dwarfed by the
the 1-best inside edge computation in both cases.
The reason for the slight performance advantage
of KA? is that our implementation of KA? uses
lazy optimizations discussed in Pauls and Klein
(2009), and while such optimizations could easily
be incorporated in TKA?, we have not yet done so
in our implementation.
4This setup used 3- and 6-round state-split grammars from
Petrov et al (2006), the former used to compute a heuristic
for the latter, tested on sentences of length up to 25.
203
4 Conclusion
We have presented TKA?, a simplification to the
KA? algorithm. Our algorithm collapses the 1-
best outside and bottom-up derivation passes of
KA? into a single, top-down pass without sacri-
ficing efficiency or optimality. This reduces the
number of non base-case deduction rules, making
TKA? easier both to specify and implement.
Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
References
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT), pages 53?64.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, UK. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of the Interna-
tional Workshop on Parsing Technologies (IWPT),
pages 123?134.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computationl Linguis-
tics, 29(1):135?143.
Adam Pauls and Dan Klein. 2009. K-best A* parsing.
In Proccedings of the Association for Computational
Linguistics (ACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
Frank K. Soong and Eng-Fong Huang. 1991. A tree-
trellis based fast search for finding the n best sen-
tence hypotheses in continuous speech recognition.
In Proceedings of the Workshop on Speech and Nat-
ural Language.
204
Proceedings of the ACL 2010 Conference Short Papers, pages 209?214,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Optimization of an MDL-Inspired Objective Function for
Unsupervised Part-of-Speech Tagging
Ashish Vaswani1 Adam Pauls2 David Chiang1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{avaswani,chiang}@isi.edu
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
adpauls@eecs.berkeley.edu
Abstract
The Minimum Description Length (MDL)
principle is a method for model selection
that trades off between the explanation of
the data by the model and the complexity
of the model itself. Inspired by the MDL
principle, we develop an objective func-
tion for generative models that captures
the description of the data by the model
(log-likelihood) and the description of the
model (model size). We also develop a ef-
ficient general search algorithm based on
the MAP-EM framework to optimize this
function. Since recent work has shown that
minimizing the model size in a Hidden
Markov Model for part-of-speech (POS)
tagging leads to higher accuracies, we test
our approach by applying it to this prob-
lem. The search algorithm involves a sim-
ple change to EM and achieves high POS
tagging accuracies on both English and
Italian data sets.
1 Introduction
The Minimum Description Length (MDL) princi-
ple is a method for model selection that provides a
generic solution to the overfitting problem (Barron
et al, 1998). A formalization of Ockham?s Razor,
it says that the parameters are to be chosen that
minimize the description length of the data given
the model plus the description length of the model
itself.
It has been successfully shown that minimizing
the model size in a Hidden Markov Model (HMM)
for part-of-speech (POS) tagging leads to higher
accuracies than simply running the Expectation-
Maximization (EM) algorithm (Dempster et al,
1977). Goldwater and Griffiths (2007) employ a
Bayesian approach to POS tagging and use sparse
Dirichlet priors to minimize model size. More re-
cently, Ravi and Knight (2009) alternately mini-
mize the model using an integer linear program
and maximize likelihood using EM to achieve the
highest accuracies on the task so far. However, in
the latter approach, because there is no single ob-
jective function to optimize, it is not entirely clear
how to generalize this technique to other prob-
lems. In this paper, inspired by the MDL princi-
ple, we develop an objective function for genera-
tive models that captures both the description of
the data by the model (log-likelihood) and the de-
scription of the model (model size). By using a
simple prior that encourages sparsity, we cast our
problem as a search for the maximum a poste-
riori (MAP) hypothesis and present a variant of
EM to approximately search for the minimum-
description-length model. Applying our approach
to the POS tagging problem, we obtain higher ac-
curacies than both EM and Bayesian inference as
reported by Goldwater and Griffiths (2007). On a
Italian POS tagging task, we obtain even larger
improvements. We find that our objective function
correlates well with accuracy, suggesting that this
technique might be useful for other problems.
2 MAP EM with Sparse Priors
2.1 Objective function
In the unsupervised POS tagging task, we are
given a word sequence w = w1, . . . ,wN and want
to find the best tagging t = t1, . . . , tN , where
ti ? T , the tag vocabulary. We adopt the problem
formulation of Merialdo (1994), in which we are
given a dictionary of possible tags for each word
type.
We define a bigram HMM
P(w, t | ?) =
N?
i=1
P(w, t | ?) ? P(ti | ti?1) (1)
In maximum likelihood estimation, the goal is to
209
find parameter estimates
?? = arg max
?
log P(w | ?) (2)
= arg max
?
log
?
t
P(w, t | ?) (3)
The EM algorithm can be used to find a solution.
However, we would like to maximize likelihood
and minimize the size of the model simultane-
ously. We define the size of a model as the number
of non-zero probabilities in its parameter vector.
Let ?1, . . . , ?n be the components of ?. We would
like to find
?? = arg min
?
(
? log P(w | ?) + ????0
)
(4)
where ???0, called the L0 norm of ?, simply counts
the number of non-zero parameters in ?. The
hyperparameter ? controls the tradeoff between
likelihood maximization and model minimization.
Note the similarity of this objective function with
MDL?s, where ? would be the space (measured
in nats) needed to describe one parameter of the
model.
Unfortunately, minimization of the L0 norm
is known to be NP-hard (Hyder and Mahata,
2009). It is not smooth, making it unamenable
to gradient-based optimization algorithms. There-
fore, we use a smoothed approximation,
???0 ?
?
i
(
1 ? e
??i
?
)
(5)
where 0 < ? ? 1 (Mohimani et al, 2007). For
smaller values of ?, this closely approximates the
desired function (Figure 1). Inverting signs and ig-
noring constant terms, our objective function is
now:
?? = arg max
?
?
??????log P(w | ?) + ?
?
i
e
??i
?
?
?????? (6)
We can think of the approximate model size as
a kind of prior:
P(?) =
exp?
?
i e
??i
?
Z
(7)
log P(?) = ? ?
?
i
e
??i
? ? log Z (8)
where Z =
?
d?
exp?
?
i e
??i
? is a normalization
constant. Then our goal is to find the maximum
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1  0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
Function Values
? i
?=0.005 ?=0.05 ?=0.5 1-||? i|| 0
Figure 1: Ideal model-size term and its approxima-
tions.
a posterior parameter estimate, which we find us-
ing MAP-EM (Bishop, 2006):
?? = arg max
?
log P(w, ?) (9)
= arg max
?
(
log P(w | ?) + log P(?)
)
(10)
Substituting (8) into (10) and ignoring the constant
term log Z, we get our objective function (6) again.
We can exercise finer control over the sparsity
of the tag-bigram and channel probability distri-
butions by using a different ? for each:
arg max
?
(
log P(w | ?) +
?c
?
w,t
e
?P(w|t)
? + ?t
?
t,t?
e
?P(t? |t)
?
)
(11)
In our experiments, we set ?c = 0 since previ-
ous work has shown that minimizing the number
of tag n-gram parameters is more important (Ravi
and Knight, 2009; Goldwater and Griffiths, 2007).
A common method for preferring smaller mod-
els is minimizing the L1 norm,
?
i |?i|. However,
for a model which is a product of multinomial dis-
tributions, the L1 norm is a constant.
?
i
|?i| =
?
i
?i
=
?
t
?
??????
?
w
P(w | t) +
?
t?
P(t? | t)
?
??????
= 2|T |
Therefore, we cannot use the L1 norm as part of
the size term as the result will be the same as the
EM algorithm.
210
2.2 Parameter optimization
To optimize (11), we use MAP EM, which is an it-
erative search procedure. The E step is the same as
in standard EM, which is to calculate P(t | w, ?t),
where the ?t are the parameters in the current iter-
ation t. The M step in iteration (t + 1) looks like
?t+1 = arg max
?
(
EP(t|w,?t)
[
log P(w, t | ?)
]
+
?t
?
t,t?
e
?P(t? |t)
?
) (12)
Let C(t,w; t,w) count the number of times the
word w is tagged as t in t, and C(t, t?; t) the number
of times the tag bigram (t, t?) appears in t. We can
rewrite the M step as
?t+1 = arg max
?
(?
t
?
w
E[C(t,w)] log P(w | t)+
?
t
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)??????? (13)
subject to the constraints
?
w P(w | t) = 1 and?
t? P(t
? | t) = 1. Note that we can optimize each
term of both summations over t separately. For
each t, the term
?
w
E[C(t,w)] log P(w | t) (14)
is easily optimized as in EM: just let P(w | t) ?
E[C(t,w)]. But the term
?
t?
(
E[C(t, t?)] log P(t? | t) + ?te
?P(t? |t)
?
)
(15)
is trickier. This is a non-convex optimization prob-
lem for which we invoke a publicly available
constrained optimization tool, ALGENCAN (An-
dreani et al, 2007). To carry out its optimization,
ALGENCAN requires computation of the follow-
ing in every iteration:
? Objective function, defined in equation (15).
This is calculated in polynomial time using
dynamic programming.
? Constraints: gt =
?
t? P(t
? | t) ? 1 = 0 for
each tag t ? T . Also, we constrain P(t? | t) to
the interval [, 1].1
1We must have  > 0 because of the log P(t? | t) term
in equation (15). It seems reasonable to set   1N ; in our
experiments, we set  = 10?7.
? Gradient of objective function:
?F
?P(t? | t)
=
E[C(t, t?)]
P(t? | t)
?
?t
?
e
?P(t? |t)
? (16)
? Gradient of equality constraints:
?gt
?P(t?? | t?)
=
?
???
???
1 if t = t?
0 otherwise
(17)
? Hessian of objective function, which is not
required but greatly speeds up the optimiza-
tion:
?2F
?P(t? | t)?P(t? | t)
= ?
E[C(t, t?)]
P(t? | t)2
+ ?t
e
?P(t? |t)
?
?2
(18)
The other second-order partial derivatives are
all zero, as are those of the equality con-
straints.
We perform this optimization for each instance
of (15). These optimizations could easily be per-
formed in parallel for greater scalability.
3 Experiments
We carried out POS tagging experiments on En-
glish and Italian.
3.1 English POS tagging
To set the hyperparameters ?t and ?, we prepared
three held-out sets H1,H2, and H3 from the Penn
Treebank. Each Hi comprised about 24, 000 words
annotated with POS tags. We ran MAP-EM for
100 iterations, with uniform probability initializa-
tion, for a suite of hyperparameters and averaged
their tagging accuracies over the three held-out
sets. The results are presented in Table 2. We then
picked the hyperparameter setting with the highest
average accuracy. These were ?t = 80, ? = 0.05.
We then ran MAP-EM again on the test data with
these hyperparameters and achieved a tagging ac-
curacy of 87.4% (see Table 1). This is higher than
the 85.2% that Goldwater and Griffiths (2007) ob-
tain using Bayesian methods for inferring both
POS tags and hyperparameters. It is much higher
than the 82.4% that standard EM achieves on the
test set when run for 100 iterations.
Using ?t = 80, ? = 0.05, we ran multiple ran-
dom restarts on the test set (see Figure 2). We find
that the objective function correlates well with ac-
curacy, and picking the point with the highest ob-
jective function value achieves 87.1% accuracy.
211
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 82.81 82.78 83.10 83.50 83.76 83.70 84.07 83.95 83.75
20 82.78 82.82 83.26 83.60 83.89 84.88 83.74 84.12 83.46
30 82.78 83.06 83.26 83.29 84.50 84.82 84.54 83.93 83.47
40 82.81 83.13 83.50 83.98 84.23 85.31 85.05 83.84 83.46
50 82.84 83.24 83.15 84.08 82.53 84.90 84.73 83.69 82.70
60 83.05 83.14 83.26 83.30 82.08 85.23 85.06 83.26 82.96
70 83.09 83.10 82.97 82.37 83.30 86.32 83.98 83.55 82.97
80 83.13 83.15 82.71 83.00 86.47 86.24 83.94 83.26 82.93
90 83.20 83.18 82.53 84.20 86.32 84.87 83.49 83.62 82.03
100 83.19 83.51 82.84 84.60 86.13 85.94 83.26 83.67 82.06
110 83.18 83.53 83.29 84.40 86.19 85.18 80.76 83.32 82.05
120 83.08 83.65 83.71 84.11 86.03 85.39 80.66 82.98 82.20
130 83.10 83.19 83.52 84.02 85.79 85.65 80.08 82.04 81.76
140 83.11 83.17 83.34 85.26 85.86 85.84 79.09 82.51 81.64
150 83.14 83.20 83.40 85.33 85.54 85.18 78.90 81.99 81.88
Table 2: Average accuracies over three held-out sets for English.
system accuracy (%)
Standard EM 82.4
+ random restarts 84.5
(Goldwater and Griffiths, 2007) 85.2
our approach 87.4
+ random restarts 87.1
Table 1: MAP-EM with a L0 norm achieves higher
tagging accuracy on English than (2007) and much
higher than standard EM.
system zero parameters bigram types
maximum possible 1389 ?
EM, 100 iterations 444 924
MAP-EM, 100 iterations 695 648
Table 3: MAP-EM with a smoothed L0 norm
yields much smaller models than standard EM.
We also carried out the same experiment with stan-
dard EM (Figure 3), where picking the point with
the highest corpus probability achieves 84.5% ac-
curacy.
We also measured the minimization effect of the
sparse prior against that of standard EM. Since our
method lower-bounds all the parameters by , we
consider a parameter ?i as a zero if ?i ? . We
also measured the number of unique tag bigram
types in the Viterbi tagging of the word sequence.
Table 3 shows that our method produces much
smaller models than EM, and produces Viterbi
taggings with many fewer tag-bigram types.
3.2 Italian POS tagging
We also carried out POS tagging experiments on
an Italian corpus from the Italian Turin Univer-
 
0.78
 
0.79 0.8
 
0.81
 
0.82
 
0.83
 
0.84
 
0.85
 
0.86
 
0.87
 
0.88
 
0.89 -532
00-53
000-5
2800-
52600
-
52400
-
52200
-
52000
-
51800
-
51600
-
51400
Tagging accuracy
objective
 function
 value
? t=80,
?=0.05,T
est Set
 24115
 Words
Figure 2: Tagging accuracy vs. objective func-
tion for 1152 random restarts of MAP-EM with
smoothed L0 norm.
sity Treebank (Bos et al, 2009). This test set com-
prises 21, 878 words annotated with POS tags and
a dictionary for each word type. Since this is all
the available data, we could not tune the hyperpa-
rameters on a held-out data set. Using the hyper-
parameters tuned on English (?t = 80, ? = 0.05),
we obtained 89.7% tagging accuracy (see Table 4),
which was a large improvement over 81.2% that
standard EM achieved. When we tuned the hyper-
parameters on the test set, the best setting (?t =
120, ? = 0.05 gave an accuracy of 90.28%.
4 Conclusion
A variety of other techniques in the literature have
been applied to this unsupervised POS tagging
task. Smith and Eisner (2005) use conditional ran-
dom fields with contrastive estimation to achieve
212
?t
?
0.75 0.5 0.25 0.075 0.05 0.025 0.0075 0.005 0.0025
10 81.62 81.67 81.63 82.47 82.70 84.64 84.82 84.96 84.90
20 81.67 81.63 81.76 82.75 84.28 84.79 85.85 88.49 85.30
30 81.66 81.63 82.29 83.43 85.08 88.10 86.16 88.70 88.34
40 81.64 81.79 82.30 85.00 86.10 88.86 89.28 88.76 88.80
50 81.71 81.71 78.86 85.93 86.16 88.98 88.98 89.11 88.01
60 81.65 82.22 78.95 86.11 87.16 89.35 88.97 88.59 88.00
70 81.69 82.25 79.55 86.32 89.79 89.37 88.91 85.63 87.89
80 81.74 82.23 80.78 86.34 89.70 89.58 88.87 88.32 88.56
90 81.70 81.85 81.00 86.35 90.08 89.40 89.09 88.09 88.50
100 81.70 82.27 82.24 86.53 90.07 88.93 89.09 88.30 88.72
110 82.19 82.49 82.22 86.77 90.12 89.22 88.87 88.48 87.91
120 82.23 78.60 82.76 86.77 90.28 89.05 88.75 88.83 88.53
130 82.20 78.60 83.33 87.48 90.12 89.15 89.30 87.81 88.66
140 82.24 78.64 83.34 87.48 90.12 89.01 88.87 88.99 88.85
150 82.28 78.69 83.32 87.75 90.25 87.81 88.50 89.07 88.41
Table 4: Accuracies on test set for Italian.
 
0.76
 
0.78 0.8
 
0.82
 
0.84
 
0.86
 
0.88 0.9 -1475
00-14
7400-
147300
-
147200
-
147100
-
147000
-
146900
-
146800
-
146700
-
146600
-
146500
-
146400
Tagging accuracy
objective
 function
 value
EM, T
est Set
 24115
 Words
Figure 3: Tagging accuracy vs. likelihood for 1152
random restarts of standard EM.
88.6% accuracy. Goldberg et al (2008) provide
a linguistically-informed starting point for EM to
achieve 91.4% accuracy. More recently, Chiang et
al. (2010) use GIbbs sampling for Bayesian in-
ference along with automatic run selection and
achieve 90.7%.
In this paper, our goal has been to investi-
gate whether EM can be extended in a generic
way to use an MDL-like objective function that
simultaneously maximizes likelihood and mini-
mizes model size. We have presented an efficient
search procedure that optimizes this function for
generative models and demonstrated that maxi-
mizing this function leads to improvement in tag-
ging accuracy over standard EM. We infer the hy-
perparameters of our model using held out data
and achieve better accuracies than (Goldwater and
Griffiths, 2007). We have also shown that the ob-
jective function correlates well with tagging accu-
racy supporting the MDL principle. Our approach
performs quite well on POS tagging for both En-
glish and Italian. We believe that, like EM, our
method can benefit from more unlabeled data, and
there is reason to hope that the success of these
experiments will carry over to other tasks as well.
Acknowledgements
We would like to thank Sujith Ravi, Kevin Knight
and Steve DeNeefe for their valuable input, and
Jason Baldridge for directing us to the Italian
POS data. This research was supported in part by
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
R. Andreani, E. G. Birgin, J. M. Martnez, and M. L.
Schuverdt. 2007. On Augmented Lagrangian meth-
ods with general lower-level constraints. SIAM
Journal on Optimization, 18:1286?1309.
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
C. Bishop. 2006. Pattern Recognition and Machine
Learning. Springer.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorical grammar tree-
bank for italian. In Eighth International Workshop
on Treebanks and Linguistic Theories (TLT8).
D. Chiang, J. Graehl, K. Knight, A. Pauls, and S. Ravi.
2010. Bayesian inference for Finite-State transduc-
ers. In Proceedings of the North American Associa-
tion of Computational Linguistics.
213
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Computational Linguistics, 39(4):1?
38.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Hyder and K. Mahata. 2009. An approximate L0
norm minimization algorithm for compressed sens-
ing. In Proceedings of the 2009 IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing.
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
H. Mohimani, M. Babaie-Zadeh, and C. Jutten. 2007.
Fast sparse representation based on smoothed L0
norm. In Proceedings of the 7th International Con-
ference on Independent Component Analysis and
Signal Separation (ICA2007).
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceed-
ings of ACL-IJCNLP.
N. Smith. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the ACL.
214
Proceedings of the ACL 2010 Conference Short Papers, pages 348?352,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Hierarchical A? Parsing with Bridge Outside Scores
Adam Pauls and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
Hierarchical A? (HA?) uses of a hierarchy
of coarse grammars to speed up parsing
without sacrificing optimality. HA? pri-
oritizes search in refined grammars using
Viterbi outside costs computed in coarser
grammars. We present Bridge Hierarchi-
cal A? (BHA?), a modified Hierarchial A?
algorithm which computes a novel outside
cost called a bridge outside cost. These
bridge costs mix finer outside scores with
coarser inside scores, and thus consti-
tute tighter heuristics than entirely coarse
scores. We show that BHA? substan-
tially outperforms HA? when the hierar-
chy contains only very coarse grammars,
while achieving comparable performance
on more refined hierarchies.
1 Introduction
The Hierarchical A? (HA?) algorithm of Felzen-
szwalb and McAllester (2007) allows the use of a
hierarchy of coarse grammars to speed up pars-
ing without sacrificing optimality. Pauls and
Klein (2009) showed that a hierarchy of coarse
grammars outperforms standard A? parsing for a
range of grammars. HA? operates by computing
Viterbi inside and outside scores in an agenda-
based way, using outside scores computed under
coarse grammars as heuristics which guide the
search in finer grammars. The outside scores com-
puted by HA? are auxiliary quantities, useful only
because they form admissible heuristics for search
in finer grammars.
We show that a modification of the HA? algo-
rithm can compute modified bridge outside scores
which are tighter bounds on the true outside costs
in finer grammars. These bridge outside scores
mix inside and outside costs from finer grammars
with inside costs from coarser grammars. Because
the bridge costs represent tighter estimates of the
true outside costs, we expect them to reduce the
work of computing inside costs in finer grammars.
At the same time, because bridge costs mix com-
putation from coarser and finer levels of the hier-
archy, they are more expensive to compute than
purely coarse outside costs. Whether the work
saved by using tighter estimates outweighs the ex-
tra computation needed to compute them is an em-
pirical question.
In this paper, we show that the use of bridge out-
side costs substantially outperforms the HA? al-
gorithm when the coarsest levels of the hierarchy
are very loose approximations of the target gram-
mar. For hierarchies with tighter estimates, we
show that BHA? obtains comparable performance
to HA?. In other words, BHA? is more robust to
poorly constructed hierarchies.
2 Previous Work
In this section, we introduce notation and review
HA?. Our presentation closely follows Pauls and
Klein (2009), and we refer the reader to that work
for a more detailed presentation.
2.1 Notation
Assume we have input sentence s0 . . . sn?1 of
length n, and a hierarchy of m weighted context-
free grammars G1 . . .Gm. We call the most refined
grammar Gm the target grammar, and all other
(coarser) grammars auxiliary grammars. Each
grammar Gt has a set of symbols denoted with cap-
ital letters and a subscript indicating the level in
the hierarchy, including a distinguished goal (root)
symbol Gt. Without loss of generality, we assume
Chomsky normal form, so each non-terminal rule
r in Gt has the form r = At ? Bt Ct with weight
wr.
Edges are labeled spans e = (At, i, j). The
weight of a derivation is the sum of rule weights
in the derivation. The weight of the best (mini-
mum) inside derivation for an edge e is called the
Viterbi inside score ?(e), and the weight of the
348
(a) (b)
G
t
s
0
s
2
s
n-1
VP
t
G
t
s
3
s
4
s
5
..
s
0
s
2
s
n-1
s
3
s
4
s
5
..
VP
t
.. ..
Figure 1: Representations of the different types of items
used in parsing and how they depend on each other. (a)
In HA?, the inside item I(VPt, 3, 5) relies on the coarse
outside item O(pit(VPt), 3, 5) for outside estimates. (b) In
BHA?, the same inside item relies on the bridge outside item
O?(VPt, 3, 5), which mixes coarse and refined outside costs.
The coarseness of an item is indicated with dotted lines.
best derivation of G ? s0 . . . si?1 At sj . . . sn?1
is called the Viterbi outside score ?(e). The goal
of a 1-best parsing algorithm is to compute the
Viterbi inside score of the edge (Gm, 0, n); the
actual best parse can be reconstructed from back-
pointers in the standard way.
We assume that each auxiliary grammar Gt?1
forms a relaxed projection of Gt. A grammar Gt?1
is a projection of Gt if there exists some many-
to-one onto function pit which maps each symbol
in Gt to a symbol in Gt?1; hereafter, we will use
A?t to represent pit(At). A projection is relaxed
if, for every rule r = At ? Bt Ct with weight
wr the projection r? = A?t ? B
?
t C
?
t has weight
wr? ? wr in Gt?1. In other words, the weight of r?
is a lower bound on the weight of all rules r in Gt
which project to r?.
2.2 Deduction Rules
HA? and our modification BHA? can be formu-
lated in terms of prioritized weighted deduction
rules (Shieber et al, 1995; Felzenszwalb and
McAllester, 2007). A prioritized weighted deduc-
tion rule has the form
?1 : w1, . . . , ?n : wn
p(w1,...,wn)????????? ?0 : g(w1, . . . , wn)
where ?1, . . . , ?n are the antecedent items of the
deduction rule and ?0 is the conclusion item. A
deduction rule states that, given the antecedents
?1, . . . , ?n with weights w1, . . . , wn, the conclu-
sion ?0 can be formed with weight g(w1, . . . , wn)
and priority p(w1, . . . , wn).
These deduction rules are ?executed? within
a generic agenda-driven algorithm, which con-
structs items in a prioritized fashion. The algo-
rithm maintains an agenda (a priority queue of
items), as well as a chart of items already pro-
cessed. The fundamental operation of the algo-
rithm is to pop the highest priority item ? from
the agenda, put it into the chart with its current
weight, and form using deduction rules any items
which can be built by combining ? with items al-
ready in the chart. If new or improved, resulting
items are put on the agenda with priority given by
p(?). Because all antecedents must be constructed
before a deduction rule is executed, we sometimes
refer to particular conclusion item as ?waiting? on
an other item(s) before it can be built.
2.3 HA?
HA? can be formulated in terms of two types of
items. Inside items I(At, i, j) represent possible
derivations of the edge (At, i, j), while outside
items O(At, i, j) represent derivations of G ?
s1 . . . si?1 At sj . . . sn rooted at (Gt, 0, n). See
Figure 1(a) for a graphical depiction of these
edges. Inside items are used to compute Viterbi in-
side scores under grammar Gt, while outside items
are used to compute Viterbi outside scores.
The deduction rules which construct inside and
outside items are given in Table 1. The IN deduc-
tion rule combines two inside items over smaller
spans with a grammar rule to form an inside item
over larger spans. The weight of the resulting item
is the sum of the weights of the smaller inside
items and the grammar rule. However, the IN rule
also requires that an outside score in the coarse
grammar1 be computed before an inside item is
built. Once constructed, this coarse outside score
is added to the weight of the conclusion item to
form the priority of the resulting item. In other
words, the coarse outside score computed by the
algorithm plays the same role as a heuristic in stan-
dard A? parsing (Klein and Manning, 2003).
Outside scores are computed by the OUT-L and
OUT-R deduction rules. These rules combine an
outside item over a large span and inside items
over smaller spans to form outside items over
smaller spans. Unlike the IN deduction, the OUT
deductions only involve items from the same level
of the hierarchy. That is, whereas inside scores
wait on coarse outside scores to be constructed,
outside scores wait on inside scores at the same
level in the hierarchy.
Conceptually, these deduction rules operate by
1For the coarsest grammar G1, the IN rule builds rules
using 0 as an outside score.
349
HA?
IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O(A?t, i, j) : w3
w1+w2+wr+w3
??????????? I(At, i, j) : w1 + w2 + wr
OUT-L: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3
w1+w3+wr+w2??????????? O(Bt, i, l) : w1 + w3 + wr
OUT-R: O(At, i, j) : w1 I(Bt, i, l) : w2 I(Ct, l, j) : w3
w1+w2+wr+w3??????????? O(Ct, l, j) : w1 + w2 + wr
Table 1: HA? deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
BHA?
B-IN: I(Bt, i, l) : w1 I(Ct, l, j) : w2 O?(At, i, j) : w3
w1+w2+wr+w3??????????? I(At, i, j) : w1 + w2 + wr
B-OUT-L: O?(At, i, j) : w1 I(B?t, i, l) : w2 I(C?t, l, j) : w3
w1+wr+w2+w3
??????????? O?(Bt, i, l) : w1 + wr + w3
B-OUT-R: O?(At, i, j) : w1 I(Bt, i, l) : w2 I(C?t, l, j) : w3
w1+w2+wr+w3
??????????? O?(Ct, l, j) : w1 + w2 + wr
Table 2: BHA? deduction rules. Red underline indicates items constructed under the previous grammar in the hierarchy.
first computing inside scores bottom-up in the
coarsest grammar, then outside scores top-down
in the same grammar, then inside scores in the
next finest grammar, and so on. However, the cru-
cial aspect of HA? is that items from all levels
of the hierarchy compete on the same queue, in-
terleaving the computation of inside and outside
scores at all levels. The HA? deduction rules come
with three important guarantees. The first is a
monotonicity guarantee: each item is popped off
the agenda in order of its intrinsic priority p?(?).
For inside items I(e) over edge e, this priority
p?(I(e)) = ?(e) + ?(e?) where e? is the projec-
tion of e. For outside items O(?) over edge e, this
priority is p?(O(e)) = ?(e) + ?(e).
The second is a correctness guarantee: when
an inside/outside item is popped of the agenda, its
weight is its true Viterbi inside/outside cost. Taken
together, these two imply an efficiency guarantee,
which states that only items x whose intrinsic pri-
ority p?(x) is less than or equal to the Viterbi inside
score of the goal are removed from the agenda.
2.4 HA? with Bridge Costs
The outside scores computed by HA? are use-
ful for prioritizing computation in more refined
grammars. The key property of these scores is
that they form consistent and admissible heuristic
costs for more refined grammars, but coarse out-
side costs are not the only quantity which satisfy
this requirement. As an alternative, we propose
a novel ?bridge? outside cost ??(e). Intuitively,
this cost represents the cost of the best deriva-
tion where rules ?above? and ?left? of an edge e
come from Gt, and rules ?below? and ?right? of
the e come from Gt?1; see Figure 2 for a graph-
ical depiction. More formally, let the spine of
an edge e = (At, i, j) for some derivation d be
VP
t
NP
t
Xt-1
s
1
s
2
s
3
G
t
s
0
NN
t
NP
t
s
4
s
5
VP
t
VP
t
S
t
Xt-1Xt-1 Xt-1
NP
t
Xt-1
NP
t
Xt-1
s
n-1
Figure 2: A concrete example of a possible bridge outside
derivation for the bridge item O?(VPt, 1, 4). This edge is
boxed for emphasis. The spine of the derivation is shown
in bold and colored in blue. Rules from a coarser grammar
are shown with dotted lines, and colored in red. Here we have
the simple projection pit(A) = X , ?A.
the sequence of rules between e and the root edge
(Gt, 0, n). A bridge outside derivation of e is a
derivation d of G ? s1 . . . si At sj+1 . . . sn such
that every rule on or left of the spine comes from
Gt, and all other rules come from Gt?1. The score
of the best such derivation for e is the bridge out-
side cost ??(e).
Like ordinary outside costs, bridge outside costs
form consistent and admissible estimates of the
true Viterbi outside score ?(e) of an edge e. Be-
cause bridge costs mix rules from the finer and
coarser grammar, bridge costs are at least as good
an estimate of the true outside score as entirely
coarse outside costs, and will in general be much
tighter. That is, we have
?(e?) ? ??(e) ? ?(e)
In particular, note that the bridge costs become
better approximations farther right in the sentence,
and the bridge cost of the last word in the sentence
is equal to the Viterbi outside cost of that word.
To compute bridge outside costs, we introduce
350
bridge outside items O?(At, i, j), shown graphi-
cally in Figure 1(b). The deduction rules which
build both inside items and bridge outside items
are shown in Table 2. The rules are very simi-
lar to those which define HA?, but there are two
important differences. First, inside items wait for
bridge outside items at the same level, while out-
side items wait for inside items from the previous
level. Second, the left and right outside deductions
are no longer symmetric ? bridge outside items
can extended to the left given two coarse inside
items, but can only be extended to the right given
an exact inside item on the left and coarse inside
item on the right.
2.5 Guarantees
These deduction rules come with guarantees anal-
ogous to those of HA?. The monotonicity guaran-
tee ensures that inside and (bridge) outside items
are processed in order of:
p?(I(e)) = ?(e) + ??(e)
p?(O?(e)) = ??(e) + ?(e?)
The correctness guarantee ensures that when an
item is removed from the agenda, its weight will
be equal to ?(e) for inside items and ??(e) for
bridge items. The efficiency guarantee remains the
same, though because the intrinsic priorities are
different, the set of items processed will be differ-
ent from those processed by HA?.
A proof of these guarantees is not possible
due to space restrictions. The proof for BHA?
follows the proof for HA? in Felzenszwalb and
McAllester (2007) with minor modifications. The
key property of HA? needed for these proofs is
that coarse outside costs form consistent and ad-
missible heuristics for inside items, and exact in-
side costs form consistent and admissible heuris-
tics for outside items. BHA? also has this prop-
erty, with bridge outside costs forming admissi-
ble and consistent heuristics for inside items, and
coarse inside costs forming admissible and consis-
tent heuristics for outside items.
3 Experiments
The performance of BHA? is determined by the
efficiency guarantee given in the previous sec-
tion. However, we cannot determine in advance
whether BHA? will be faster than HA?. In fact,
BHA? has the potential to be slower ? BHA?
0
10
20
30
40
0-split 1-split 2-split 3-split 4-split 5-split
I
t
e
m
s
 
P
u
s
h
e
d
 
(
B
i
l
l
i
o
n
s
)
BHA*
HA*
Figure 3: Performance of HA? and BHA? as a function of
increasing refinement of the coarse grammar. Lower is faster.
0
2.5
5
7.5
10
3 3-5 0-5
E
d
g
e
s
 
P
u
s
h
e
d
 
(
b
i
l
l
i
o
n
s
)
Figure 4: Performance of BHA? on hierarchies of varying
size. Lower is faster. Along the x-axis, we show which coarse
grammars were used in the hierarchy. For example, 3-5 in-
dicates the 3-,4-, and 5-split grammars were used as coarse
grammars.
builds both inside and bridge outside items under
the target grammar, where HA? only builds inside
items. It is an empirical, grammar- and hierarchy-
dependent question whether the increased tight-
ness of the outside estimates outweighs the addi-
tional cost needed to compute them. We demon-
strate empirically in this section that for hier-
archies with very loosely approximating coarse
grammars, BHA? can outperform HA?, while
for hierarchies with good approximations, perfor-
mance of the two algorithms is comparable.
We performed experiments with the grammars
of Petrov et al (2006). The training procedure for
these grammars produces a hierarchy of increas-
ingly refined grammars through state-splitting, so
a natural projection function pit is given. We used
the Berkeley Parser2 to learn such grammars from
Sections 2-21 of the Penn Treebank (Marcus et al,
1993). We trained with 6 split-merge cycles, pro-
ducing 7 grammars. We tested these grammars on
300 sentences of length ? 25 of Section 23 of the
Treebank. Our ?target grammar? was in all cases
the most split grammar.
2http://berkeleyparser.googlecode.com
351
In our first experiment, we construct 2-level hi-
erarchies consisting of one coarse grammar and
the target grammar. By varying the coarse gram-
mar from the 0-split (X-bar) through 5-split gram-
mars, we can investigate the performance of each
algorithm as a function of the coarseness of the
coarse grammar. We follow Pauls and Klein
(2009) in using the number of items pushed as
a machine- and implementation-independent mea-
sure of speed. In Figure 3, we show the perfor-
mance of HA? and BHA? as a function of the
total number of items pushed onto the agenda.
We see that for very coarse approximating gram-
mars, BHA? substantially outperforms HA?, but
for more refined approximating grammars the per-
formance is comparable, with HA? slightly out-
performing BHA? on the 3-split grammar.
Finally, we verify that BHA? can benefit from
multi-level hierarchies as HA? can. We con-
structed two multi-level hierarchies: a 4-level hier-
archy consisting of the 3-,4-,5-, and 6- split gram-
mars, and 7-level hierarchy consisting of all gram-
mars. In Figure 4, we show the performance of
BHA? on these multi-level hierarchies, as well as
the best 2-level hierarchy from the previous exper-
iment. Our results echo the results of Pauls and
Klein (2009): although the addition of the rea-
sonably refined 4- and 5-split grammars produces
modest performance gains, the addition of coarser
grammars can actually hurt overall performance.
Acknowledgements
This project is funded in part by the NSF under
grant 0643742 and an NSERC Postgraduate Fel-
lowship.
References
P. Felzenszwalb and D. McAllester. 2007. The gener-
alized A* architecture. Journal of Artificial Intelli-
gence Research.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In
Proceedings of the Human Language Technology
Conference and the North American Association
for Computational Linguistics (HLT-NAACL), pages
119?126.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of The Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proccedings of the
Association for Computational Linguistics (ACL).
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Programming,
24:3?36.
352
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 258?267,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Faster and Smaller N -Gram Language Models
Adam Pauls Dan Klein
Computer Science Division
University of California, Berkeley
{adpauls,klein}@cs.berkeley.edu
Abstract
N -gram language models are a major resource
bottleneck in machine translation. In this pa-
per, we present several language model imple-
mentations that are both highly compact and
fast to query. Our fastest implementation is
as fast as the widely used SRILM while re-
quiring only 25% of the storage. Our most
compact representation can store all 4 billion
n-grams and associated counts for the Google
n-gram corpus in 23 bits per n-gram, the most
compact lossless representation to date, and
even more compact than recent lossy compres-
sion techniques. We also discuss techniques
for improving query speed during decoding,
including a simple but novel language model
caching technique that improves the query
speed of our language models (and SRILM)
by up to 300%.
1 Introduction
For modern statistical machine translation systems,
language models must be both fast and compact.
The largest language models (LMs) can contain as
many as several hundred billion n-grams (Brants
et al, 2007), so storage is a challenge. At the
same time, decoding a single sentence can trig-
ger hundreds of thousands of queries to the lan-
guage model, so speed is also critical. As al-
ways, trade-offs exist between time, space, and ac-
curacy, with many recent papers considering small-
but-approximate noisy LMs (Chazelle et al, 2004;
Guthrie and Hepple, 2010) or small-but-slow com-
pressed LMs (Germann et al, 2009).
In this paper, we present several lossless meth-
ods for compactly but efficiently storing large LMs
in memory. As in much previous work (Whittaker
and Raj, 2001; Hsu and Glass, 2008), our meth-
ods are conceptually based on tabular trie encodings
wherein each n-gram key is stored as the concatena-
tion of one word (here, the last) and an offset encod-
ing the remaining words (here, the context). After
presenting a bit-conscious basic system that typifies
such approaches, we improve on it in several ways.
First, we show how the last word of each entry can
be implicitly encoded, almost entirely eliminating
its storage requirements. Second, we show that the
deltas between adjacent entries can be efficiently en-
coded with simple variable-length encodings. Third,
we investigate block-based schemes that minimize
the amount of compressed-stream scanning during
lookup.
To speed up our language models, we present two
approaches. The first is a front-end cache. Caching
itself is certainly not new to language modeling, but
because well-tuned LMs are essentially lookup ta-
bles to begin with, naive cache designs only speed
up slower systems. We present a direct-addressing
cache with a fast key identity check that speeds up
our systems (or existing fast systems like the widely-
used, speed-focused SRILM) by up to 300%.
Our second speed-up comes from a more funda-
mental change to the language modeling interface.
Where classic LMs take word tuples and produce
counts or probabilities, we propose an LM that takes
a word-and-context encoding (so the context need
not be re-looked up) and returns both the probabil-
ity and also the context encoding for the suffix of the
original query. This setup substantially accelerates
the scrolling queries issued by decoders, and also
exploits language model state equivalence (Li and
Khudanpur, 2008).
Overall, we are able to store the 4 billion n-grams
of the Google Web1T (Brants and Franz, 2006) cor-
258
pus, with associated counts, in 10 GB of memory,
which is smaller than state-of-the-art lossy language
model implementations (Guthrie and Hepple, 2010),
and significantly smaller than the best published
lossless implementation (Germann et al, 2009). We
are also able to simultaneously outperform SRILM
in both total size and speed. Our LM toolkit, which
is implemented in Java and compatible with the stan-
dard ARPA file formats, is available on the web.1
2 Preliminaries
Our goal in this paper is to provide data structures
that map n-gram keys to values, i.e. probabilities
or counts. Maps are fundamental data structures
and generic implementations of mapping data struc-
tures are readily available. However, because of the
sheer number of keys and values needed for n-gram
language modeling, generic implementations do not
work efficiently ?out of the box.? In this section,
we will review existing techniques for encoding the
keys and values of an n-gram language model, tak-
ing care to account for every bit of memory required
by each implementation.
To provide absolute numbers for the storage re-
quirements of different implementations, we will
use the Google Web1T corpus as a benchmark. This
corpus, which is on the large end of corpora typically
employed in language modeling, is a collection of
nearly 4 billion n-grams extracted from over a tril-
lion tokens of English text, and has a vocabulary of
about 13.5 million words.
2.1 Encoding Values
In the Web1T corpus, the most frequent n-gram
occurs about 95 billion times. Storing this count
explicitly would require 37 bits, but, as noted by
Guthrie and Hepple (2010), the corpus contains only
about 770 000 unique counts, so we can enumerate
all counts using only 20 bits, and separately store an
array called the value rank array which converts the
rank encoding of a count back to its raw count. The
additional array is small, requiring only about 3MB,
but we save 17 bits per n-gram, reducing value stor-
age from around 16GB to about 9GB for Web1T.
We can rank encode probabilities and back-offs in
the same way, allowing us to be agnostic to whether
1http://code.google.com/p/berkeleylm/
we encode counts, probabilities and/or back-off
weights in our model. In general, the number of bits
per value required to encode all value ranks for a
given language model will vary ? we will refer to
this variable as v .
2.2 Trie-Based Language Models
The data structure of choice for the majority of
modern language model implementations is a trie
(Fredkin, 1960). Tries or variants thereof are
implemented in many LM tool kits, including
SRILM (Stolcke, 2002), IRSTLM (Federico and
Cettolo, 2007), CMU SLM (Whittaker and Raj,
2001), and MIT LM (Hsu and Glass, 2008). Tries
represent collections of n-grams using a tree. Each
node in the tree encodes a word, and paths in the
tree correspond to n-grams in the collection. Tries
ensure that each n-gram prefix is represented only
once, and are very efficient when n-grams share
common prefixes. Values can also be stored in a trie
by placing them in the appropriate nodes.
Conceptually, trie nodes can be implemented as
records that contain two entries: one for the word
in the node, and one for either a pointer to the par-
ent of the node or a list of pointers to children. At
a low level, however, naive implementations of tries
can waste significant amounts of space. For exam-
ple, the implementation used in SRILM represents a
trie node as a C struct containing a 32-bit integer
representing the word, a 64-bit memory2 pointer to
the list of children, and a 32-bit floating point num-
ber representing the value stored at a node. The total
storage for a node alone is 16 bytes, with additional
overhead required to store the list of children. In
total, the most compact implementation in SRILM
uses 33 bytes per n-gram of storage, which would
require around 116 GB of memory to store Web1T.
While it is simple to implement a trie node in this
(already wasteful) way in programming languages
that offer low-level access to memory allocation like
C/C++, the situation is even worse in higher level
programming languages. In Java, for example, C-
style structs are not available, and records are
most naturally implemented as objects that carry an
additional 64 bits of overhead.
2While 32-bit architectures are still in use today, their lim-
ited address space is insufficient for modern language models
and we will assume all machines use a 64-bit architecture.
259
Despite its relatively large storage requirements,
the implementation employed by SRILM is still
widely in use today, largely because of its speed ? to
our knowledge, SRILM is the fastest freely available
language model implementation. We will show that
we can achieve access speeds comparable to SRILM
but using only 25% of the storage.
2.3 Implicit Tries
A more compact implementation of a trie is de-
scribed in Whittaker and Raj (2001). In their imple-
mentation, nodes in a trie are represented implicitly
as entries in an array. Each entry encodes a word
with enough bits to index all words in the language
model (24 bits for Web1T), a quantized value, and
a 32-bit3 offset that encodes the contiguous block
of the array containing the children of the node.
Note that 32 bits is sufficient to index all n-grams in
Web1T; for larger corpora, we can always increase
the size of the offset.
Effectively, this representation replaces system-
level memory pointers with offsets that act as logical
pointers that can reference other entries in the array,
rather than arbitrary bytes in RAM. This represen-
tation saves space because offsets require fewer bits
than memory pointers, but more importantly, it per-
mits straightforward implementation in any higher-
level language that provides access to arrays of inte-
gers.4
2.4 Encoding n-grams
Hsu and Glass (2008) describe a variant of the im-
plicit tries of Whittaker and Raj (2001) in which
each node in the trie stores the prefix (i.e. parent).
This representation has the property that we can re-
fer to each n-gram wn1 by its last word wn and the
offset c(wn?11 ) of its prefix w
n?1
1 , often called the
context. At a low-level, we can efficiently encode
this pair (wn, c(w
n?1
1 )) as a single 64-bit integer,
where the first 24 bits refer to wn and the last 40 bits
3The implementation described in the paper represents each
32-bit integer compactly using only 16 bits, but this represen-
tation is quite inefficient, because determining the full 32-bit
offset requires a binary search in a look up table.
4Typically, programming languages only provide support
for arrays of bytes, not bits, but it is of course possible to simu-
late arrays with arbitrary numbers of bits using byte arrays and
bit manipulation.
encode c(wn?11 ). We will refer to this encoding as a
context encoding.
Note that typically, n-grams are encoded in tries
in the reverse direction (first-rest instead of last-
rest), which enables a more efficient computation of
back-offs. In our implementations, we found that the
speed improvement from switching to a first-rest en-
coding and implementing more efficient queries was
modest. However, as we will see in Section 4.2, the
last-rest encoding allows us to exploit the scrolling
nature of queries issued by decoders, which results
in speedups that far outweigh those achieved by re-
versing the trie.
3 Language Model Implementations
In the previous section, we reviewed well-known
techniques in language model implementation. In
this section, we combine these techniques to build
simple data structures in ways that are to our knowl-
edge novel, producing language models with state-
of-the-art memory requirements and speed. We will
also show that our data structures can be very effec-
tively compressed by implicitly encoding the word
wn, and further compressed by applying a variable-
length encoding on context deltas.
3.1 Sorted Array
A standard way to implement a map is to store an
array of key/value pairs, sorted according to the key.
Lookup is carried out by performing binary search
on a key. For an n-gram language model, we can ap-
ply this implementation with a slight modification:
we need n sorted arrays, one for each n-gram order.
We construct keys (wn, c(w
n?1
1 )) using the context
encoding described in the previous section, where
the context offsets c refer to entries in the sorted ar-
ray of (n ? 1)-grams. This data structure is shown
graphically in Figure 1.
Because our keys are sorted according to their
context-encoded representation, we cannot straight-
forwardly answer queries about an n-gram w with-
out first determining its context encoding. We can
do this efficiently by building up the encoding in-
crementally: we start with the context offset of the
unigram w1, which is simply its integer representa-
tion, and use that to form the context encoding of the
bigram w21 = (w2, c(w1)). We can find the offset of
260
?ran?
w c
15180053
w c
24
bits
40
bits
64 bits
?cat?
15176585
?dog?
6879
6879
6879
6879
6879
6880
6880
6880
6879
00004598
?slept?
00004588
00004568
00004530
00004502
00004668
00004669
00004568
00004577
6879 0000449815176583
15176585
15176593
15176613
15179801
15180051
15176589
15176591
?had?
?the?
?left?
1933
.
.
.
.
.
.
.
.
.
.
.
.
?slept?
1933
1933
1933
1933
1933
1933
1935
1935
1935
.
.
val val val
v
bits
.
.
?dog?
3-grams
2-grams 1-grams
w
Figure 1: Our SORTED implementation of a trie. The dotted paths correspond to ?the cat slept?, ?the cat ran?, and ?the
dog ran?. Each node in the trie is an entry in an array with 3 parts: w represents the word at the node; val represents
the (rank encoded) value; and c is an offset in the array of n ? 1 grams that represents the parent (prefix) of a node.
Words are represented as offsets in the unigram array.
the bigram using binary search, and form the context
encoding of the trigram, and so on. Note, however,
that if our queries arrive in context-encoded form,
queries are faster since they involve only one binary
search in the appropriate array. We will return to this
later in Section 4.2
This implementation, SORTED, uses 64 bits for
the integer-encoded keys and v bits for the values.
Lookup is linear in the length of the key and log-
arithmic in the number of n-grams. For Web1T
(v = 20), the total storage is 10.5 bytes/n-gram or
about 37GB.
3.2 Hash Table
Hash tables are another standard way to implement
associative arrays. To enable the use of our context
encoding, we require an implementation in which
we can refer to entries in the hash table via array
offsets. For this reason, we use an open address hash
map that uses linear probing for collision resolution.
As in the sorted array implementation, in order to
insert an n-gram wn1 into the hash table, we must
form its context encoding incrementally from the
offset of w1. However, unlike the sorted array im-
plementation, at query time, we only need to be
able to check equality between the query key wn1 =
(wn, c(w
n?1
1 )) and a key w
?n
1 = (w
?
n, c(w
?n?1
1 )) in
the table. Equality can easily be checked by first
checking if wn = w?n, then recursively checking
equality between wn?11 and w
?n?1
1 , though again,
equality is even faster if the query is already context-
encoded.
This HASH data structure also uses 64 bits for
integer-encoded keys and v bits for values. How-
ever, to avoid excessive hash collisions, we also al-
locate additional empty space according to a user-
defined parameter that trades off speed and time ?
we used about 40% extra space in our experiments.
For Web1T, the total storage for this implementation
is 15 bytes/n-gram or about 53 GB total.
Look up in a hash map is linear in the length of
an n-gram and constant with respect to the number
261
of n-grams. Unlike the sorted array implementa-
tion, the hash table implementation also permits ef-
ficient insertion and deletion, making it suitable for
stream-based language models (Levenberg and Os-
borne, 2009).
3.3 Implicitly Encoding wn
The context encoding we have used thus far still
wastes space. This is perhaps most evident in the
sorted array representation (see Figure 1): all n-
grams ending with a particular word wi are stored
contiguously. We can exploit this redundancy by
storing only the context offsets in the main array,
using as many bits as needed to encode all context
offsets (32 bits for Web1T). In auxiliary arrays, one
for each n-gram order, we store the beginning and
end of the range of the trie array in which all (wi, c)
keys are stored for each wi. These auxiliary arrays
are negligibly small ? we only need to store 2n off-
sets for each word.
The same trick can be applied in the hash table
implementation. We allocate contiguous blocks of
the main array for n-grams which all share the same
last word wi, and distribute keys within those ranges
using the hashing function.
This representation reduces memory usage for
keys from 64 bits to 32 bits, reducing overall storage
for Web1T to 6.5 bytes/n-gram for the sorted imple-
mentation and 9.1 bytes for the hashed implementa-
tion, or about 23GB and 32GB in total. It also in-
creases query speed in the sorted array case, since to
find (wi, c), we only need to search the range of the
array over which wi applies. Because this implicit
encoding reduces memory usage without a perfor-
mance cost, we will assume its use for the rest of
this paper.
3.4 A Compressed Implementation
3.4.1 Variable-Length Coding
The distribution of value ranks in language mod-
eling is Zipfian, with far more n-grams having low
counts than high counts. If we ensure that the value
rank array sorts raw values by descending order of
frequency, then we expect that small ranks will oc-
cur much more frequently than large ones, which we
can exploit with a variable-length encoding.
To compress n-grams, we can exploit the context
encoding of our keys. In Figure 2, we show a portion
w c val
1933 15176585 3
1933 15176587 2
1933 15176593 1
1933 15176613 8
1933 15179801 1
1935 15176585 298
1935 15176589 1
1933 15176585 563097887 956 3 0 +0 +2 2 +0 +5 1 +0 +40 8
!w !c
val
1933 15176585 3
+0 +2 1
+0 +5 1
+0 +40 8
+0 +188 1
+2 15176585 298
+0 +4 1
|!w| |!c|
|val|
24 40 3
2 3 3
2 3 3
2 9 6
2 12 3
4 36 15
2 6 3
.  .  . 
(a) Context-Encoding (b) Context Deltas (c) Bits Required
(d) Compressed Array
Number 
of bits 
in this 
block
Value rank 
for header 
key
Header 
key
Logical 
offset of 
this block
True if 
all !w in 
block are 
0 
Figure 2: Compression using variable-length encoding.
(a) A snippet of an (uncompressed) context-encoded ar-
ray. (b) The context and word deltas. (c) The number
of bits required to encode the context and word deltas as
well as the value ranks. Word deltas use variable-length
block coding with k = 1, while context deltas and value
ranks use k = 2. (d) A snippet of the compressed encod-
ing array. The header is outlined in bold.
of the key array used in our sorted array implemen-
tation. While we have already exploited the fact that
the 24 word bits repeat in the previous section, we
note here that consecutive context offsets tend to be
quite close together. We found that for 5-grams, the
median difference between consecutive offsets was
about 50, and 90% of offset deltas were smaller than
10000. By using a variable-length encoding to rep-
resent these deltas, we should require far fewer than
32 bits to encode context offsets.
We used a very simple variable-length coding to
encode offset deltas, word deltas, and value ranks.
Our encoding, which is referred to as ?variable-
length block coding? in Boldi and Vigna (2005),
works as follows: we pick a (configurable) radix
r = 2k. To encode a number m, we determine the
number of digits d required to express m in base r.
We write d in unary, i.e. d ? 1 zeroes followed by
a one. We then write the d digits of m in base r,
each of which requires k bits. For example, using
k = 2, we would encode the decimal number 7 as
010111. We can choose k separately for deltas and
value indices, and also tune these parameters to a
given language model.
We found this encoding outperformed other
standard prefix codes, including Golomb
codes (Golomb, 1966; Church et al, 2007)
262
and Elias ? and ? codes. We also experimented
with the ? codes of Boldi and Vigna (2005), which
modify variable-length block codes so that they
are optimal for certain power law distributions.
We found that ? codes performed no better than
variable-length block codes and were slightly more
complex. Finally, we found that Huffman codes
outperformed our encoding slightly, but came at a
much higher computational cost.
3.4.2 Block Compression
We could in principle compress the entire array of
key/value pairs with the encoding described above,
but this would render binary search in the array im-
possible: we cannot jump to the mid-point of the ar-
ray since in order to determine what key lies at a par-
ticular point in the compressed bit stream, we would
need to know the entire history of offset deltas.
Instead, we employ block compression, a tech-
nique also used by Harb et al (2009) for smaller
language models. In particular, we compress the
key/value array in blocks of 128 bytes. At the be-
ginning of the block, we write out a header consist-
ing of: an explicit 64-bit key that begins the block;
a 32-bit integer representing the offset of the header
key in the uncompressed array;5 the number of bits
of compressed data in the block; and the variable-
length encoding of the value rank of the header key.
The remainder of the block is filled with as many
compressed key/value pairs as possible. Once the
block is full, we start a new block. See Figure 2 for
a depiction.
When we encode an offset delta, we store the delta
of the word portion of the key separately from the
delta of the context offset. When an entire block
shares the same word portion of the key, we set a
single bit in the header that indicates that we do not
encode any word deltas.
To find a key in this compressed array, we first
perform binary search over the header blocks (which
are predictably located every 128 bytes), followed
by a linear search within a compressed block.
Using k = 6 for encoding offset deltas and k = 5
for encoding value ranks, this COMPRESSED im-
plementation stores Web1T in less than 3 bytes per
n-gram, or about 10.2GB in total. This is about
5We need this because n-grams refer to their contexts using
array offsets.
6GB less than the storage required by Germann et
al. (2009), which is the best published lossless com-
pression to date.
4 Speeding up Decoding
In the previous section, we provided compact and
efficient implementations of associative arrays that
allow us to query a value for an arbitrary n-gram.
However, decoders do not issue language model re-
quests at random. In this section, we show that lan-
guage model requests issued by a standard decoder
exhibit two patterns we can exploit: they are highly
repetitive, and also exhibit a scrolling effect.
4.1 Exploiting Repetitive Queries
In a simple experiment, we recorded all of the
language model queries issued by the Joshua de-
coder (Li et al, 2009) on a 100 sentence test set.
Of the 31 million queries, only about 1 million were
unique. Therefore, we expect that keeping the re-
sults of language model queries in a cache should be
effective at reducing overall language model latency.
To this end, we added a very simple cache to
our language model. Our cache uses an array of
key/value pairs with size fixed to 2b ? 1 for some
integer b (we used 24). We use a b-bit hash func-
tion to compute the address in an array where we
will always place a given n-gram and its fully com-
puted language model score. Querying the cache is
straightforward: we check the address of a key given
by its b-bit hash. If the key located in the cache ar-
ray matches the query key, then we return the value
stored in the cache. Otherwise, we fetch the lan-
guage model probability from the language model
and place the new key and value in the cache, evict-
ing the old key in the process. This scheme is often
called a direct-mapped cache because each key has
exactly one possible address.
Caching n-grams in this way reduces overall la-
tency for two reasons: first, lookup in the cache is
extremely fast, requiring only a single evaluation of
the hash function, one memory lookup to find the
cache key, and one equality check on the key. In
contrast, even our fastest (HASH) implementation
may have to perform multiple memory lookups and
equality checks in order to resolve collisions. Sec-
ond, when calculating the probability for an n-gram
263
the cat + fell down
the cat fell
cat fell down
18569876 fell
35764106 down
LM
 
0.76
LM
 
0.12
LM
 
LM
 
0.76
0.12
?the cat?
?cat fell?
3576410
E
x
p
l
i
c
i
t
 
R
e
p
r
e
s
e
n
t
a
t
i
o
n
C
o
n
t
e
x
t
 
E
n
c
o
d
i
n
g
Figure 3: Queries issued when scoring trigrams that are
created when a state with LM context ?the cat? combines
with ?fell down?. In the standard explicit representation
of an n-gram as list of words, queries are issued atom-
ically to the language model. When using a context-
encoding, a query from the n-gram ?the cat fell? returns
the context offset of ?cat fell?, which speeds up the query
of ?cat fell down?.
not in the language model, language models with
back-off schemes must in general perform multiple
queries to fetch the necessary back-off information.
Our cache retains the full result of these calculations
and thus saves additional computation.
Federico and Cettolo (2007) also employ a cache
in their language model implementation, though
based on traditional hash table cache with linear
probing. Unlike our cache, which is of fixed size,
their cache must be cleared after decoding a sen-
tence. We would not expect a large performance in-
crease from such a cache for our faster models since
our HASH implementation is already a hash table
with linear probing. We found in our experiments
that a cache using linear probing provided marginal
performance increases of about 40%, largely be-
cause of cached back-off computation, while our
simpler cache increases performance by about 300%
even over our HASH LM implementation. More tim-
ing results are presented in Section 5.
4.2 Exploiting Scrolling Queries
Decoders with integrated language models (Och and
Ney, 2004; Chiang, 2005) score partial translation
hypotheses in an incremental way. Each partial hy-
pothesis maintains a language model context con-
sisting of at most n ? 1 target-side words. When
we combine two language model contexts, we create
several new n-grams of length of n, each of which
generate a query to the language model. These new
WMT2010
Order #n-grams
1gm 4,366,395
2gm 61,865,588
3gm 123,158,761
4gm 217,869,981
5gm 269,614,330
Total 676,875,055
WEB1T
Order #n-grams
1gm 13,588,391
2gm 314,843,401
3gm 977,069,902
4gm 1,313,818,354
5gm 1,176,470,663
Total 3,795,790,711
Table 1: Sizes of the two language models used in our
experiments.
n-grams exhibit a scrolling effect, shown in Fig-
ure 3: the n ? 1 suffix words of one n-gram form
the n? 1 prefix words of the next.
As discussed in Section 3, our LM implementa-
tions can answer queries about context-encoded n-
grams faster than explicitly encoded n-grams. With
this in mind, we augment the values stored in our
language model so that for a key (wn, c(w
n?1
1 )),
we store the offset of the suffix c(wn2 ) as well as
the normal counts/probabilities. Then, rather than
represent the LM context in the decoder as an ex-
plicit list of words, we can simply store context off-
sets. When we query the language model, we get
back both a language model score and context offset
c(w?n?11 ), where w?
n?1
1 is the the longest suffix of
wn?11 contained in the language model. We can then
quickly form the context encoding of the next query
by simply concatenating the new word with the off-
set c(w?n?11 ) returned from the previous query.
In addition to speeding up language model
queries, this approach also automatically supports an
equivalence of LM states (Li and Khudanpur, 2008):
in standard back-off schemes, whenever we compute
the probability for an n-gram (wn, c(wn?11 )) when
wn?11 is not in the language model, the result will be
the same as the result of the query (wn, c(w?
n?1
1 ). It
is therefore only necessary to store as much of the
context as the language model contains instead of
all n ? 1 words in the context. If a decoder main-
tains LM states using the context offsets returned
by our language model, then the decoder will au-
tomatically exploit this equivalence and the size of
the search space will be reduced. This same effect is
exploited explicitly by some decoders (Li and Khu-
danpur, 2008).
264
WMT2010
LM Type bytes/ bytes/ bytes/ Total
key value n-gram Size
SRILM-H ? ? 42.2 26.6G
SRILM-S ? ? 33.5 21.1G
HASH 5.6 6.0 11.6 7.5G
SORTED 4.0 4.5 8.5 5.5G
TPT ? ? 7.5?? 4.7G??
COMPRESSED 2.1 3.8 5.9 3.7G
Table 2: Memory usages of several language model im-
plementations on the WMT2010 language model. A
?? indicates that the storage in bytes per n-gram is re-
ported for a different language model of comparable size,
and the total size is thus a rough projection.
5 Experiments
5.1 Data
To test our LM implementations, we performed
experiments with two different language models.
Our first language model, WMT2010, was a 5-
gram Kneser-Ney language model which stores
probability/back-off pairs as values. We trained this
language model on the English side of all French-
English corpora provided6 for use in the WMT 2010
workshop, about 2 billion tokens in total. This data
was tokenized using the tokenizer.perl script
provided with the data. We trained the language
model using SRILM. We also extracted a count-
based language model, WEB1T, from the Web1T
corpus (Brants and Franz, 2006). Since this data is
provided as a collection of 1- to 5-grams and asso-
ciated counts, we used this data without further pre-
processing. The make up of these language models
is shown in Table 1.
5.2 Compression Experiments
We tested our three implementations (HASH,
SORTED, and COMPRESSED) on the WMT2010
language model. For this language model, there are
about 80 million unique probability/back-off pairs,
so v ? 36. Note that here v includes both the
cost per key of storing the value rank as well as the
(amortized) cost of storing two 32 bit floating point
numbers (probability and back-off) for each unique
value. The results are shown in Table 2.
6www.statmt.org/wmt10/translation-task.html
WEB1T
LM Type bytes/ bytes/ bytes/ Total
key value n-gram Size
Gzip ? ? 7.0 24.7G
T-MPHR? ? ? 3.0 10.5G
COMPRESSED 1.3 1.6 2.9 10.2G
Table 3: Memory usages of several language model im-
plementations on the WEB1T. A ? indicates lossy com-
pression.
We compare against three baselines. The first two,
SRILM-H and SRILM-S, refer to the hash table-
and sorted array-based trie implementations pro-
vided by SRILM. The third baseline is the Tightly-
Packed Trie (TPT) implementation of Germann et
al. (2009). Because this implementation is not freely
available, we use their published memory usage in
bytes per n-gram on a language model of similar
size and project total usage.
The memory usage of all of our models is con-
siderably smaller than SRILM ? our HASH imple-
mentation is about 25% the size of SRILM-H, and
our SORTED implementation is about 25% the size
of SRILM-S. Our COMPRESSED implementation
is also smaller than the state-of-the-art compressed
TPT implementation.
In Table 3, we show the results of our COM-
PRESSED implementation on WEB1T and against
two baselines. The first is compression of the ASCII
text count files using gzip, and the second is the
Tiered Minimal Perfect Hash (T-MPHR) of Guthrie
and Hepple (2010). The latter is a lossy compres-
sion technique based on Bloomier filters (Chazelle
et al, 2004) and additional variable-length encod-
ing that achieves the best published compression of
WEB1T to date. Our COMPRESSED implementa-
tion is even smaller than T-MPHR, despite using a
lossless compression technique. Note that since T-
MPHR uses a lossy encoding, it is possible to re-
duce the storage requirements arbitrarily at the cost
of additional errors in the model. We quote here the
storage required when keys7 are encoded using 12-
bit hash codes, which gives a false positive rate of
about 2?12 =0.02%.
7Guthrie and Hepple (2010) also report additional savings
by quantizing values, though we could perform the same quan-
tization in our storage scheme.
265
LM Type No Cache Cache Size
COMPRESSED 9264?73ns 565?7ns 3.7G
SORTED 1405?50ns 243?4ns 5.5G
HASH 495?10ns 179?6ns 7.5G
SRILM-H 428?5ns 159?4ns 26.6G
HASH+SCROLL 323?5ns 139?6ns 10.5G
Table 4: Raw query speeds of various language model
implementations. Times were averaged over 3 runs on
the same machine. For HASH+SCROLL, all queries were
issued to the decoder in context-encoded form, which
speeds up queries that exhibit scrolling behaviour. Note
that memory usage is higher than for HASH because we
store suffix offsets along with the values for an n-gram.
LM Type No Cache Cache Size
COMPRESSED 9880?82s 1547?7s 3.7G
SRILM-H 1120?26s 938?11s 26.6G
HASH 1146?8s 943?16s 7.5G
Table 5: Full decoding times for various language model
implementations. Our HASH LM is as fast as SRILM
while using 25% of the memory. Our caching also re-
duces total decoding time by about 20% for our fastest
models and speeds up COMPRESSED by a factor of 6.
Times were averaged over 3 runs on the same machine.
5.3 Timing Experiments
We first measured pure query speed by logging all
LM queries issued by a decoder and measuring
the time required to query those n-grams in isola-
tion. We used the the Joshua decoder8 with the
WMT2010 model to generate queries for the first
100 sentences of the French 2008 News test set. This
produced about 30 million queries. We measured the
time9 required to perform each query in order with
and without our direct-mapped caching, not includ-
ing any time spent on file I/O.
The results are shown in Table 4. As expected,
HASH is the fastest of our implementations, and
comparable10 in speed to SRILM-H, but using sig-
8We used a grammar trained on all French-English data
provided for WMT 2010 using the make scripts provided
at http://sourceforge.net/projects/joshua/files
/joshua/1.3/wmt2010-experiment.tgz/download
9All experiments were performed on an Amazon EC2 High-
Memory Quadruple Extra Large instance, with an Intel Xeon
X5550 CPU running at 2.67GHz and 8 MB of cache.
10Because we implemented our LMs in Java, we issued
queries to SRILM via Java Native Interface (JNI) calls, which
introduces a performance overhead. When called natively, we
found that SRILM was about 200 ns/query faster. Unfortu-
nificantly less space. SORTED is slower but of
course more memory efficient, and COMPRESSED
is the slowest but also the most compact repre-
sentation. In HASH+SCROLL, we issued queries
to the language model using the context encoding,
which speeds up queries substantially. Finally, we
note that our direct-mapped cache is very effective.
The query speed of all models is boosted substan-
tially. In particular, our COMPRESSED implementa-
tion with caching is nearly as fast as SRILM-H with-
out caching, and even the already fast HASH imple-
mentation is 300% faster in raw query speed with
caching enabled.
We also measured the effect of LM performance
on overall decoder performance. We modified
Joshua to optionally use our LM implementations
during decoding, and measured the time required
to decode all 2051 sentences of the 2008 News
test set. The results are shown in Table 5. With-
out caching, SRILM-H and HASH were comparable
in speed, while COMPRESSED introduces a perfor-
mance penalty. With caching enabled, overall de-
coder speed is improved for both HASH and SRILM-
H, while the COMPRESSED implementation is only
about 50% slower that the others.
6 Conclusion
We have presented several language model imple-
mentations which are state-of-the-art in both size
and speed. Our experiments have demonstrated im-
provements in query speed over SRILM and com-
pression rates against state-of-the-art lossy compres-
sion. We have also described a simple caching tech-
nique which leads to performance increases in over-
all decoding time.
Acknowledgements
This work was supported by a Google Fellowship for the
first author and by BBN under DARPA contract HR0011-
06-C-0022. We would like to thank David Chiang, Zhifei
Li, and the anonymous reviewers for their helpful com-
ments.
nately, it is not completely fair to compare our LMs against ei-
ther of these numbers: although the JNI overhead slows down
SRILM, implementing our LMs in Java instead of C++ slows
down our LMs. In the tables, we quote times which include
the JNI overhead, since this reflects the true cost to a decoder
written in Java (e.g. Joshua).
266
References
Paolo Boldi and Sebastiano Vigna. 2005. Codes for the
world wide web. Internet Mathematics, 2.
Thorsten Brants and Alex Franz. 2006. Google web1t
5-gram corpus, version 1. In Linguistic Data Consor-
tium, Philadelphia, Catalog Number LDC2006T13.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The Bloomier filter: an efficient
data structure for static support lookup tables. In Pro-
ceedings of the fifteenth annual ACM-SIAM sympo-
sium on Discrete algorithms.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Kenneth Church, Ted Hart, and Jianfeng Gao. 2007.
Compressing trigram language models with golomb
coding. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Marcello Federico and Mauro Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation.
Edward Fredkin. 1960. Trie memory. Communications
of the ACM, 3:490?499, September.
Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009.
Tightly packed tries: how to fit large models into mem-
ory, and make them load fast, too. In Proceedings of
the Workshop on Software Engineering, Testing, and
Quality Assurance for Natural Language Processing.
S. W. Golomb. 1966. Run-length encodings. IEEE
Transactions on Information Theory, 12.
David Guthrie and Mark Hepple. 2010. Storing the web
in memory: space efficient language models with con-
stant time retrieval. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Boulos Harb, Ciprian Chelba, Jeffrey Dean, and Sanjay
Ghemawat. 2009. Back-off language model compres-
sion. In Proceedings of Interspeech.
Bo-June Hsu and James Glass. 2008. Iterative language
model estimation: Efficient data structure and algo-
rithms. In Proceedings of Interspeech.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In Pro-
ceedings of the Second Workshop on Syntax and Struc-
ture in Statistical Translation.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computationl Linguistics, 30:417?449, Decem-
ber.
Andreas Stolcke. 2002. SRILM: An extensible language
modeling toolkit. In Proceedings of Interspeech.
E. W. D. Whittaker and B. Raj. 2001. Quantization-
based language model compression. In Proceedings
of Eurospeech.
267
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 959?968,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Large-Scale Syntactic Language Modeling with Treelets
Adam Pauls Dan Klein
Computer Science Division
University of California, Berkeley
Berkeley, CA 94720, USA
{adpauls,klein}@cs.berkeley.edu
Abstract
We propose a simple generative, syntactic
language model that conditions on overlap-
ping windows of tree context (or treelets) in
the same way that n-gram language models
condition on overlapping windows of linear
context. We estimate the parameters of our
model by collecting counts from automati-
cally parsed text using standard n-gram lan-
guage model estimation techniques, allowing
us to train a model on over one billion tokens
of data using a single machine in a matter of
hours. We evaluate on perplexity and a range
of grammaticality tasks, and find that we per-
form as well or better than n-gram models and
other generative baselines. Our model even
competes with state-of-the-art discriminative
models hand-designed for the grammaticality
tasks, despite training on positive data alone.
We also show fluency improvements in a pre-
liminary machine translation experiment.
1 Introduction
N -gram language models are a central component
of all speech recognition and machine translation
systems, and a great deal of research centers around
refining models (Chen and Goodman, 1998), ef-
ficient storage (Pauls and Klein, 2011; Heafield,
2011), and integration into decoders (Koehn, 2004;
Chiang, 2005). At the same time, because n-gram
language models only condition on a local window
of linear word-level context, they are poor models of
long-range syntactic dependencies. Although sev-
eral lines of work have proposed generative syntac-
tic language models that improve on n-gram mod-
els for moderate amounts of data (Chelba, 1997; Xu
et al, 2002; Charniak, 2001; Hall, 2004; Roark,
2004), these models have only recently been scaled
to the impressive amounts of data routinely used by
n-gram language models (Tan et al, 2011).
In this paper, we describe a generative, syntac-
tic language model that conditions on local con-
text treelets1 in a parse tree, backing off to smaller
treelets as necessary. Our model can be trained sim-
ply by collecting counts and using the same smooth-
ing techniques normally applied to n-gram mod-
els (Kneser and Ney, 1995), enabling us to apply
techniques developed for scaling n-gram models out
of the box (Brants et al, 2007; Pauls and Klein,
2011). The simplicity of our training procedure al-
lows us to train a model on a billion tokens of data in
a matter of hours on a single machine, which com-
pares favorably to the more involved training algo-
rithm of Tan et al (2011), who use a two-pass EM
training algorithm that takes several days on several
hundred CPUs using similar amounts of data.
The simplicity of our approach also contrasts with
recent work on language modeling with tree sub-
stitution grammars (Post and Gildea, 2009), where
larger treelet contexts are incorporated by using so-
phisticated priors to learn a segmentation of parse
trees. Such an approach implicitly assumes that a
?correct? segmentation exists, but it is not clear that
this is true in practice. Instead, we build upon the
success of n-gram language models, which do not
assume a segmentation and instead score all over-
lapping contexts.
We evaluate our model in terms of perplexity, and
show that we achieve the same performance as a
state-of-the-art n-gram model. We also evaluate our
model on several grammaticality tasks proposed in
1We borrow the term treelet from Quirk et al (2005), who
use it to refer to an arbitrary connected subgraph of a tree.
959
(a) The index fell 109.85 Monday . (b) ROOT
S-VBD?ROOT
NP-NN
DT-the
The
NN
index
VP-VBD?S
VBD
fell
CD-DC
109.85
NNTP
Monday
.
.
(c) ROOT
S-VBD?ROOT
NP-NN
DT-the
The
NN
index
VP-VBD?S
VBD
fell
CD-DC
109.85
NNTP
Monday
.
.
5-GRAM
The board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .
They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :
Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :
But 8.32 % stake in and Rep. any money for you got from several months , ? he says .
TREELET
Why a $ 1.2 million investment in various types of the bulk of TVS E. August ?
? One operating price position has a system that has Quartet for the first time , ? he said .
He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations . ?
The centers are losses of meals , and the runs are willing to like them .
Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.
rule context would need its own state in the gram-
mar), and extensive pruning would be in order.
In practice, however, language models are nor-
mally integrated into a decoder, a non-trivial task
that is highly problem-dependent and beyond the
scope of this paper. However, we note that for
machine translation, a model that builds target-side
constituency parses, such as that of Galley et al
(2006), combined with an efficient pruning strategy
like cube pruning (Chiang, 2005), should be able to
integrate our model without much difficulty.
That said, for evaluation purposes, whenever we
need to query our model, we use the simple strategy
of parsing a sentence using a black box parser, and
summing over our model?s probabilities of the 1000-
best parses.4 Note that the bottleneck in this case
is the parser, so our model can essentially score a
sentence at the speed of a parser.
5 Experiments
We evaluate our model along several dimensions.
We first show some sample sentences generated by
our model in Section 5.1. We report perplexity re-
4We found that using the 1-best worked just as well as the
1000-best on our grammaticality tasks, but significantly overes-
timated our model?s perplexities.
sults in Section 5.2. In Section 5.3, we measure
its ability to distinguish between grammatical En-
glish and various types of automatically generated,
or pseudo-negative,5 English. We report machine
translation reranking results in Section 5.4.
5.1 Generating Samples
Because our model is generative, we can qualita-
tively assess it by generating samples and verifying
that they are more syntactically coherent than other
approaches. In Table 1, we show the first four sam-
ples of length between 15 and 20 generated from
both model and a 5-gram model trained on the Penn
Treebank.
5.2 Perplexity
Perplexity is the standard intrinsic evaluation metric
for language models. It measures the inverse of the
per-word probability a model assigns to some held-
out set of grammatical English (so lower is better).
For training data, we constructed a large treebank by
concatenating the Penn Treebank, the Brown Cor-
pus, the 50K BLLIP training sentences from Post
(2011), and the AFP and APW portions of English
5We follow Okanohara and Tsujii (2007) in using the term
pseudo-negative to highlight the fact that automatically gener-
ated negative examples might not actually be ungrammatical.
Figure 1: Conditioning contexts and back-off strategies for Markov models. The bolded symbol indicates the part of the
tree/sentence being generated, and the dotted lines represent the conditioning contexts; back-off proceeds from the largest to the
smallest context. (a) A trigram model. (b) The context used for non-terminal productions in our treelet model. For this context,
P=VP-VBD?S, P ?=S-VBD?ROOT, and r?=S-VBD?ROOT?NP-NN VP-VBD?S . (c) The context used for terminal productions
in our treelet model. Here, P=VBD, R=CD-DC, r?=VP-VBD?S?VBD CD-DC NNTP, w?1=index, and w?2=The. Note that the
tree is a modified version of a standard Penn Treebank parse ? see Section 3 for details.
the literature (Okanohara and Tsujii, 2007; Foster et
al., 2008; Cherry and Quirk, 2008) and show that
it consistently outperforms an n-gram model as well
as other head-driven and tree-driven generative base-
lines. Our model even competes with state-of-the-art
discriminative classifiers specifically designed for
each task, despite being estimated on positive data
alone. We also show fluency improvements in a pre-
liminary machine translation reranking experiment.
2 Treelet Language Modeling
The common denominator of most n-gram language
models is that they assign probabilities roughly ac-
cording to empirical frequencies for observed n-
grams, but fall back to distributions conditioned on
smaller contexts for unobserved n-grams, as shown
in Figure 1(a). This type of smoothing is both highly
robust and easy to implement, requiring only the col-
lection of counts from data.
We would like to apply the same smoothing tech-
niques to distributions over rule yields in a con-
stituency tree, conditioned on contexts consisting
of previously generated treelets (rules, nodes, etc.).
Formally, let T be a constituency tree consisting of
context-free rules of the form r = P ? C1 ? ? ?Cd,
where P is the parent symbol of rule r and Cd1 =
C1 . . . Cd are its children. We wish to assign proba-
bilities to trees2
2A distribution over trees also induces a distribution over
sentences w`1 given by p(w`1) = PT :s(T )=w`1 p(T ), where
p(T ) =
?
r?T
p(Cd1 |h)
where the conditioning context h is some portion of
the already-generated parts of the tree. In this paper,
we assume that the children of a rule are expanded
from left to right, so that when generatin th yi ld
Cd1 , all treelets above and left of th parent P are
available. Note that a raw PCFG would condition
only on P , i.e. h = P .
As in the n-gram case, we would like to pick h
to be large enough to capture relevant dependencies,
but small enough that we can obtain meaningful es-
timates from data. We start with a straightforward
choice of context: we condition on P , as well as the
rule r? that generated P , as shown in in Figure 1(b).
Conditioning on the parent rule r? allows us to
capture several important dependencies. First, it
captures both P and its parent P ?, which predicts
the distribution over child symbols far better than
just P (Johnson, 1998). Second, it captures posi-
tional effects. For example, subject and object noun
phrases (NPs) have different distributions (Klein and
Manning, 2003), and the position of an NP relative
to a verb is a good indicator of this distinction. Fi-
nally, the generation of words at preterminals can
condition on siblings, allowing the model to capture,
for example, verb subcategorization frames.
We should be clear that we are not the first
s(T ) is the terminal yield of T .
960
to use back-off-based smoothing for syntactic lan-
guage modeling ? such techniques have been ap-
plied to models that condition on head-word con-
texts (Charniak, 2001; Roark, 2004; Zhang, 2009).
Parent rule context has also been employed in trans-
lation (Vaswani et al, 2011). However, to our
knowledge, we are the first to apply these techniques
for language modeling on large amounts of data.
2.1 Lexical context
Although it is tempting to think that we can replace
the left-to-right generation of n-gram models with
the purely top-down generation of typical PCFGs,
in practice, words are often highly predictive of the
words that follow them ? indeed, n-gram models
would be terrible language models if this were not
the case. To capture linear effects, we extend the
context for terminal (lexical) productions to include
the previous two wordsw?2 andw?1 in the sentence
in addition to r?; see Figure 1(c) for a depiction. This
allows us to capture collocations and other lexical
correlations.
2.2 Backing off
As with n-gram models, counts for rule yields con-
ditioned on r? are sparse, and we must choose an ap-
propriate back-off strategy. We handle terminal and
non-terminal productions slightly differently.
For non-terminal productions, we back off from
r? to P and its parent P ?, and then to just P .
That is, we back off from a rule-annotated gram-
mar p(Cd1 |P, P ?, r?) to a parent-annotated gram-
mar (Johnson, 1998) p(Cd1 |P, P ?), then to a raw
PCFG p(Cd1 |P ). In order to generalize to unseen
rule yields Cd1 , we further back off from the ba-
sic PCFG probability p(Cd1 |P ) to p(Ci|Ci?1i?3 , P ), a
4-gram model over symbols C conditioned on P ,
interpolated with an unconditional 4-gram model
p(Ci|C
i?1
i?3 ). In other words, we back off from a raw
PCFG to
?
d?
i=1
p(Ci|C
i?1
i?3 , P ) + (1? ?)
d?
i=1
p(Ci|C
i?1
i?3 )
where ? = 0.9 is an interpolation constant.
For terminal (i.e lexical) productions, we
first remove lexical context, backing off from
p(w|P,R, r?, w?1, w?2) to p(w|P,R, r?, w?1) and
then p(w|P,R, r?). From there, we back off to
p(w|P,R) whereR is the sibling immediately to the
right of P , then to a raw PCFG p(w|P ), and finally
to a unigram distribution. We chose this scheme be-
cause p(w|P,R) allows, for example, a verb to be
generated conditioned on the non-terminal category
of the argument it takes (since arguments usually im-
mediately follow verbs). We depict these two back-
off schemes pictorially in Figure 1(b) and (c).
2.3 Estimation
Estimating the probabilities in our model can be
done very simply using the same techniques (in fact,
the same code) used to estimate n-gram language
models. Our model requires estimates of four distri-
butions: p(Cd1 |P, P ?, r?), p(w|P,R, r?, w?1, w?2),
p(Ci|C
i?1
i?n+1, P ), and p(Ci|Ci?1i?n+1). In each case,
we require empirical counts of treelet tuples in the
same way that we require counts of word tuples for
estimating n-gram language models.
There is one additional hurdle in the estimation of
our model: while there exist corpora with human-
annotated constituency parses like the Penn Tree-
bank (Marcus et al, 1993), these corpora are quite
small ? on the order of millions of tokens ? and we
cannot gather nearly as many counts as we can for n-
grams, for which billions or even trillions (Brants et
al., 2007) of tokens are available on the Web. How-
ever, we can use one of several high-quality con-
stituency parsers (Collins, 1997; Charniak, 2000;
Petrov et al, 2006) to automatically generate parses.
These parses may contain errors, but not all parsing
errors are problematic for our model, since we only
care about the sentences generated by our model and
not the parses themselves. We show in our experi-
ments that the addition of data with automatic parses
does improve the performance of our language mod-
els across a range of tasks.
3 Tree Transformations
In the previous section, we described how to condi-
tion on rich parse context to better capture the dis-
tribution of English trees. While such context al-
lows our model to capture many interesting depen-
dencies, several important dependencies require ad-
ditional attention. In this section, we describe a
961
ROOT
S-VB?ROOT
PRP-he
He
VP-VB?S
VB
reset
NP-NNS
JJ
opening
NNS
arguments
PP-for
IN-for
for
NNT
today
.
.
Figure 2: A sample parse from the Penn Treebank after
the tree transformations described in Section 3. Note that
we have not shown head tag annotations on preterminals
because in that case, the head tag is the preterminal itself.
number of transformations of Treebank constituency
parses that allow us to capture such dependencies.
We list the annotations and deletions in the order in
which they are performed. A sample transformed
tree is shown in Figure 2.
Temporal NPs Following Klein and Manning (2003),
we attempt to annotate temporal noun phrases. Although
the Penn Treebank annotates temporal NPs, most off-the-
shelf parsers do not retain these tags, and we do not as-
sume their presence. Instead, we mark any noun that is
the head of a NP-TMP constituent at least once in the
Treebank as a temporal noun, so for example today would
be tagged as NNT and months would be tagged as NNTS.
Head Annotations We annotate every non-terminal or
preterminal with its head word if the head is a closed-
class word3 and with its head tag otherwise. Klein and
Manning (2003) used head tag annotation extensively,
though they applied their splits much more selectively.
NP Flattening We delete NPs dominated by
other NPs, unless the child NPs are in coordi-
nation or apposition. These NPs typically oc-
cur when nouns are modified by PPs, as in
(NP (NP (NN stock) (NNS sales)) (PP (IN by) (NNS traders))). By
removing the dominated NP, we allow the production
NNS?sales to condition on the presence of a modifying
PP (here a PP head-annotated with by).
Number Annotations Numbers are divided into five
classes: CD-YR for numbers that consist of four digits
(which are usually years); CD-NM for entirely numeric
numbers; CD-DC for numbers that have a decimal; CD-
3We define the following to be closed class words: any punc-
tuation; all inflections of the verbs do, be, and have; and any
word tagged with IN, WDT, PDT, WP, WP$, TO, WRB, RP,
DT, SYM, EX, POS, PRP, AUX, or CC.
MX for numbers that mix letters and digits; and CD-AL
for numbers that are entirely alphabetic.
SBAR Flattening We remove any sentential (S) nodes
immediately dominated by an SBAR. S nodes under
SBAR have very distinct distributions from other senten-
tial nodes, mostly due to empty subjects and/or objects.
VP Flattening We remove any VPs immediately domi-
nating a VP, unless it is conjoined with another VP. In the
Treebank, chains of verbs (e.g. will be going) have a sep-
arate VP for each verb. By flattening such structures, we
allow the main verb and its arguments to condition on the
whole chain of verbs. This effect is particularly important
for passive constructions.
Gapped Sentence Annotation Collins (1999) and
Klein and Manning (2003) annotate nodes which have
empty subjects. Because we only assume the presence
of automatically derived parses, which do not produce
the empty elements in the original Treebank, we must
identify such elements on our own. We use a very simple
procedure: we annotate all S or SBAR nodes that have a
VP before any NPs.
Parent Annotation We annotate all VPs with their par-
ent symbol. Because our treelet model already conditions
on the parent, this has the effect of allowing verbs to con-
dition on their grandparents. This was important for VPs
under SBAR nodes, which often have empty objects. We
also parent-annotated any child of the ROOT.
Unary Deletion We remove all unary productions ex-
cept the root and preterminal productions, keeping only
the bottom-most symbol. Because we are not interested
in the internal labels of the trees, unaries are largely a
nuisance, and their removal brings many symbols into the
context of others.
4 Scoring a Sentence
Computing the probability of a sentence w`1 under
our model requires summing over all possible parses
of w`1. Although our model can be formulated as a
straightforward PCFG, allowing O(`3) computation
of this sum, the grammar constant for this PCFG
would be unmanageably large (since every parent
rule context would need its own state in the gram-
mar), and extensive pruning would be in order.
In practice, however, language models are nor-
mally integrated into a decoder, a non-trivial task
that is highly problem-dependent and beyond the
scope of this paper. For machine translation, a model
that builds target-side constituency parses, such as
that of Galley et al (2006), combined with an ef-
ficient pruning strategy like cube pruning (Chiang,
962
5-GRAM
The board ?s will soon be feasible , from everyday which Coke ?s cabinet hotels .
They are all priced became regulatory action by difficulty caused nor Aug. 31 of Helmsley-Spear :
Lakeland , it may take them if the 46-year-old said the loss of the Japanese executives at him :
But 8.32 % stake in and Rep. any money for you got from several months , ? he says .
TREELET
Why a $ 1.2 million investment in various types of the bulk of TVS E. August ?
? One operating price position has a system that has Quartet for the first time , ? he said .
He may enable drops to take , but will hardly revive the rush to develop two-stroke calculations . ?
The centers are losses of meals , and the runs are willing to like them .
Table 1: The first four samples of length between 15 and 20 generated from the 5-GRAM and TREELET models.
2005), should be able to integrate our model without
much difficulty.
That said, for evaluation purposes, whenever we
need to query our model, we use the simple strategy
of parsing a sentence using a black box parser, and
summing over our model?s probabilities of the 1000-
best parses.4 Note that the bottleneck in this case
is the parser, so our model can essentially score a
sentence at the speed of a parser.
5 Experiments
We evaluate our model along several dimensions.
We first show some sample generated sentences in
Section 5.1. We report perplexity results in Sec-
tion 5.2. In Section 5.3, we measure its ability to
distinguish between grammatical English and var-
ious types of automatically generated, or pseudo-
negative,5 English. We report machine translation
reranking results in Section 5.4.
5.1 Generating Samples
Because our model is generative, we can qualita-
tively assess it by generating samples and verifying
that they are more syntactically coherent than other
approaches. In Table 1, we show the first four sam-
ples of length between 15 and 20 generated from our
model and a 5-gram model trained on the Penn Tree-
bank.
4We found that using the 1-best worked just as well as the
1000-best on our grammaticality tasks, but significantly overes-
timated our model?s perplexities.
5We follow Okanohara and Tsujii (2007) in using the term
pseudo-negative to highlight the fact that automatically gener-
ated negative examples might not actually be ungrammatical.
5.2 Perplexity
Perplexity is the standard intrinsic evaluation metric
for language models. It measures the inverse of the
per-word probability a model assigns to some held-
out set of grammatical English (so lower is better).
For training data, we constructed a large treebank by
concatenating the WSJ and Brown portions of the
Penn Treebank, the 50K BLLIP training sentences
from Post (2011), and the AFP and APW portions
of English Gigaword version 3 (Graff, 2003), total-
ing about 1.3 billion tokens. We used the human-
annotated parses for the sentences in the Penn Tree-
bank, but parsed the Gigaword and BLLIP sentences
with the Berkeley Parser. Hereafter, we refer to this
training data as our 1B corpus. We used Section 0
of the WSJ as our test corpus. Results are shown in
Table 2. In addition to our TREELET model, we also
show results for the following baselines:
5-GRAM A 5-gram interpolated Kneser-Ney model.
PCFG-LA The Berkeley Parser in language model mode.
HEADLEX A head-lexicalized model similar to, but
more powerful6 than, Collins Model 1 (Collins, 1999).
PCFG A raw PCFG.
TREELET-TRANS A PCFG estimated on the trees after
the transformations of Section 3.
TREELET-RULE The TREELET-TRANS model with the
parent rule context described in Section 2. This is equiv-
alent to the full TREELET model without the lexical con-
text described in Section 2.1.
6Specifically, like Collins Model 1, we generate a rule yield
conditioned on parent symbol P and head word h by first gen-
erating its head symbol Ch, then generating the head words and
symbols for left and right modifiers outwards from Ch. Unlike
Model 1, which generates each modifier head and symbol con-
ditioned only on Ch, h, and P , we additionally condition on the
previously generated modifier?s head and symbol and back off
to Model 1.
963
Model Perplexity
PCFG 1772
TREELET-TRANS 722
TREELET-RULE 329
TREELET 198?
PCFG-LA 330**
HEADLEX 299
5-GRAM 207?
Table 2: Perplexity of several generative models on Sec-
tion 0 of the WSJ. The differences between scores marked
with ? are not statistically significant. PCFG-LA (marked
with **) was only trained on the WSJ and Brown corpora
because it does not scale to large amounts of data.
We used the Berkeley LM toolkit (Pauls
and Klein, 2011), which implements Kneser-Ney
smoothing, to estimate all back-off models for both
n-gram and treelet models. To deal with unknown
words, we use the following strategy: after the first
10000 sentences, whenever we see a new word in
our training data, we replace it with a signature7
10% of the time.
Our model outperforms all other generative mod-
els, though the improvement over the n-gram model
is not statistically significant. Note that because we
use a k-best approximation for the sum over trees,
all perplexities (except for PCFG-LA and 5-GRAM)
are pessimistic bounds.
5.3 Classification of Pseudo-Negative Sentences
We make use of three kinds of automatically gener-
ated pseudo-negative sentences previously proposed
in the literature: Okanohara and Tsujii (2007) pro-
posed generating pseudo-negative examples from a
trigram language model; Foster et al (2008) create
?noisy? sentences by automatically inserting a sin-
gle error into grammatical sentences with a script
that randomly deletes, inserts, or misspells a word;
and Och et al (2004) and Cherry and Quirk (2008)
both use the 1-best output of a machine translation
system. Examples of these three types of pseudo-
negative data are shown in Table 3. We evaluate our
model?s ability to distinguish positive from pseudo-
negative data, and compare against generative base-
lines and state-of-the-art discriminative methods.
7We use signatures generated by the Berkeley Parser.
These signatures capture surface features such as capitalization,
presents of digits, and common suffixes. For example, the word
vexing would be replaced with the signature UNK-ing.
Noisy There was were many contributors.
Trigram For years in dealer immediately .
MT we must further steps .
Table 3: Sample pseudo-negative sentences.
We would like to use our model to make grammat-
icality judgements, but as a generative model it can
only provide us with probabilities. Simply thresh-
olding generative probabilities, even with a separate
threshold for each length, has been shown to be very
ineffective for grammaticality judgements, both for
n-gram and syntactic language models (Cherry and
Quirk, 2008; Post, 2011). We used a simple measure
for isolating the syntactic likelihood of a sentence:
we take the log-probability under our model and
subtract the log-probability under a unigram model,
then normalize by the length of the sentence.8 This
measure, which we call the syntactic log-odds ratio
(SLR), is a crude way of ?subtracting out? the se-
mantic component of the generative probability, so
that sentences that use rare words are not penalized
for doing so.
5.3.1 Trigram Classification
To facilitate comparison with previous work, we
used the same negative corpora as Post (2011) for
trigram classification. They randomly selected 50K
train, 3K development, and 3K positive test sen-
tences from the BLLIP corpus, then trained a tri-
gram model on 450K BLLIP sentences and gener-
ated 50K train, 3K development, and 3K negative
sentences. We parsed the 50K positive training ex-
amples of Post (2011) with the Berkeley Parser and
used the resulting treebank to train a treelet language
model. We set an SLR threshold for each model on
the 6K positive and negative development sentences.
Results are shown in Table 4. In addition to our
generative baselines, we show results for the dis-
criminative models reported in Cherry and Quirk
(2008) and Post (2011). The former train a latent
PCFG support vector machine for binary classifica-
tion (LSVM). The latter report results for two bi-
nary classifiers: RERANK uses the reranking fea-
tures of Charniak and Johnson (2005), and TSG uses
8Och et al (2004) also report using a parser probability nor-
malized by the unigram probability (but not length), and did not
find it effective. We assume this is either because the length-
normalization is important, or because their choice of syntactic
language model was poor.
964
Generative
BLLIP 1B
PCFG 81.5 81.8
TREELET-TRANS 87.7 90.1
TREELET-RULE 89.8 94.1
TREELET 88.9 93.3
PCFG-LA 87.1* ?
HEADLEX 87.6 92.0
5-GRAM 67.9 87.5
Discriminative
BLLIP 1B
LSVM 81.42** ?
TSG 89.9 ?
RERANK 93.0 ?
Table 4: Classification accuracy for trigram pseudo-negative
sentences on the BLLIP corpus. The number reported for
PCFG-LA is marked with a * to indicate that this model was
trained on the training section of the WSJ, not the BLLIP cor-
pus. The number reported for LSVM (marked with **) was eval-
uated on a different random split of the BLLIP corpus, and so is
not directly comparable.
indicator features extracted from a tree substitution
grammar derivation of each sentence.
Our TREELET model performs nearly as well as
the TSG method, and substantially outperforms the
LSVM method, though the latter was not tested on
the same random split. Interestingly, the TREELET-
RULE baseline, which removes lexical context from
our model, outperforms the full model. This is likely
because the negative data is largely coherent at the
trigram level (because it was generated from a tri-
gram model), and the full model is much more sen-
sitive to trigram coherence than the TREELET-RULE
model. This also explains the poor performance of
the 5-GRAM model.
We emphasize that the discriminative baselines
are specifically trained to separate trigram text from
natural English, while our model is trained on pos-
itive examples alone. Indeed, the methods in Post
(2011) are simple binary classifiers, and it is not
clear that these models would be properly calibrated
for any other task, such as integration in a decoder.
One of the design goals of our system was that
it be scalable. Unlike some of the discriminative
baselines, which require expensive operations9 on
9It is true that in order train our system, one must parse large
amounts of training data, which can be costly, though it only
needs to be done once. In contrast, even with observed train-
ing trees, the discriminative algorithms must still iteratively per-
form expensive operations (like parsing) for each sentence, and
a new model must be trained for new types of negative data.
Model Pairwise Independent
WSJ 1B WSJ 1B
PCFG 79.1 77.0 58.9 58.6
TREELET-RULE 90.3 94.4 63.8 66.2
TREELET 90.7 94.5 63.4 65.5
5-GRAM 86.3 93.5 55.7 60.1
HEADLEX 90.7 94.0 59.5 62.0
PCFG-LA 91.3 ? 59.7 ?
Foster et al (2008) ? ? 65.9 ?
Table 5: Classification accuracies on the noisy WSJ for mod-
els trained on WSJ Sections 2-21 and our 1B token corpus.
?Pairwise? accuracy is the fraction of correct sentences whose
SLR score was higher than its noisy version, and ?independent?
refers to standard binary classification accuracy.
each training sentence, we can very easily scale
our model to much larger amounts of data. In Ta-
ble 4, we also show the performance of the gener-
ative models trained on our 1B corpus. All gener-
ative models improve, but TREELET-RULE remains
the best, now outperforming the RERANK system,
though of course it is likely that RERANK would im-
prove if it could be scaled up to more training data.
5.3.2 ?Noisy? Classification
We also evaluate the performance of our model
on the task of distinguishing the noisy WSJ sen-
tences of Foster et al (2008) from their original
versions. We use the noisy versions of Section 0
and 23 produced by their error-generating proce-
dure. Because they only report classification re-
sults on Section 0, we used Section 23 to tune an
SLR threshold, and tested our model on Section 0.
We show the results of both independent and pair-
wise classification for the WSJ and 1B training sets
in Table 5. Note that independent classification is
much more difficult than for the trigram data, be-
cause sentences contain at most one change, which
may not even result in an ungrammaticality. Again,
our model outperforms the n-gram model for both
types of classification, and achieves the same per-
formance as the discriminative system of Foster et
al. (2008), which is state-of-the-art for this data set.
The TREELET-RULE system again slightly outper-
forms the full TREELET model at independent clas-
sification, though not at pairwise classification. This
probably reflects the fact that semantic coherence
can still influence the SLR score, despite our efforts
to subtract it out. Because the TREELET model in-
cludes lexical context, it is more sensitive to seman-
965
French German Chinese
5-GRAM 44.8 37.8 60.0
TREELET 57.9 66.0 83.8
Table 6: Pairwise comparison accuracy of MT output
against a reference translation for French, German, and
Chinese. The BLEU scores for these outputs are 32.7,
27.8, and 20.8. This task becomes easier, at least for our
TREELET model, as translation quality drops. Cherry and
Quirk (2008) report an accuracy of 71.9% on a similar
experiment with German a source language, though the
translation system and training data were different so the
numbers are not comparable. In particular, their transla-
tions had a lower BLEU score, making their task easier.
tic coherence and thus more likely to misclassify
semantically coherent but ungrammatical sentences.
For pairwise comparisons, where semantic coher-
ence is effectively held constant, such sentences are
not problematic.
5.3.3 Machine Translation Classification
We follow Och et al (2004) and Cherry and Quirk
(2008) in evaluating our language models on their
ability to distinguish the 1-best output of a machine
translation system from a reference translation in a
pairwise fashion. Unfortunately, we do not have
access to the data used in those papers, so a di-
rect comparison is not possible. Instead, we col-
lected the English output of Moses (Hoang et al,
2007), using both French and German as source lan-
guage, trained on the Europarl corpus used by WMT
2009.10 We also collected the output of Joshua (Li
et al, 2009) trained on 500K sentences of GALE
Chinese-English parallel newswire. We trained both
our TREELET model and a 5-GRAM model on the
union of our 1B corpus and the English sides of our
parallel corpora.
In Table 6, we show the pairwise comparison ac-
curacy (using SLR) on these three corpora. We see
that our system prefers the reference much more of-
ten than the 5-GRAM language model.11 However,
we also note that the easiness of the task is corre-
lated with the quality of translations (as measured in
BLEU score). This is not surprising ? high-quality
translations are often grammatical and even a per-
10http://www.statmt.org/wmt09
11We note that the n-gram language model used by the MT
system was much smaller than the 5-GRAM model, as they were
only trained on the English sides of their parallel data.
fect language model might not be able to differenti-
ate such translations from their references.
5.4 Machine Translation Fluency
We also carried out reranking experiments on 1000-
best lists from Moses using our syntactic language
model as a feature. We did not find that the use
of our syntactic language model made any statis-
tically significant increases in BLEU score. How-
ever, we noticed in general that the translations fa-
vored by our model were more fluent, a useful im-
provement to which BLEU is often insensitive. To
confirm this, we carried out an Amazon Mechan-
ical Turk experiment where users from the United
States were asked to compare translations using our
TREELET language model as the language model
feature to those using the 5-GRAM model.12 We had
1000 such translation pairs rated by 4 separate Turk-
ers each. Although these two hypothesis sets had
the same BLEU score (up to statistical significance),
the Turkers preferred the output obtained using our
syntactic language model 59% of the time, indicat-
ing that our model had managed to pick out more
fluent hypotheses that nonetheless were of the same
BLEU score. This result was statistically significant
with p < 0.001 using bootstrap resampling.
6 Conclusion
We have presented a simple syntactic language
model that can be estimated using standard n-gram
smoothing techniques on large amounts of data. Our
model outperforms generative baselines on several
evaluation metrics and achieves the same perfor-
mance as state-of-the-art discriminative classifiers
specifically trained on several types of negative data.
Acknowledgments
We would like to thank David Hall for some modeling
suggestions and the anonymous reviewers for their com-
ments. We thank both Matt Post and Jennifer Foster for
providing us with their corpora. This work was partially
supported by a Google Fellowship to the first author and
by BBN under DARPA contract HR0011-12-C-0014.
12We used translations from the baseline Moses system of
Section 5.3.3 with German as the input language. For each lan-
guage model, we took k-best lists from the baseline system and
replaced the baseline LM score with the new model?s score. We
then retrained all feature weights with MERT on the tune set,
and selected the 1-best output on the test set.
966
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, Jeffrey Dean, and Google Inc. 2007. Large lan-
guage models in machine translation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American chapter
of the Association for Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the Association
for Computational Linguistics.
Ciprian Chelba. 1997. A structured language model. In
Proceedings of the Association for Computational Lin-
guistics.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of the Association for Computa-
tional Linguistics.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proceedings of The Association for Machine Transla-
tion in the Americas.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of As-
sociation for Computational Linguistics.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jennifer Foster, Joachim Wagner, and Josef van Genabith.
2008. Adapting a wsj-trained parser to grammatically
noisy text. In Proceedings of the Association for Com-
putational Linguistics: Short Paper Track.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics (ACL).
David Graff. 2003. English gigaword, version 3. In Lin-
guistic Data Consortium, Philadelphia, Catalog Num-
ber LDC2003T05.
Keith Hall. 2004. Best-first Word-lattice Parsing: Tech-
niques for Integrated Syntactic Language Modeling.
Ph.D. thesis, Brown University.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Hieu Hoang, Alexandra Birch, Chris Callison-burch,
Richard Zens, Rwth Aachen, Alexandra Constantin,
Marcello Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine Moran, and On-
dej Bojar. 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proceedings of the As-
sociation for Computational Linguistics: Demonstra-
tion Session,.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In IEEE
International Conference on Acoustics, Speech and
Signal Processing.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of The Association for Machine
Translation in the Americas.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: an open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Translation.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A Smorgas-
bord of Features for Statistical Machine Translation.
In Proceedings of the North American Association for
Computational Linguistic.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the Association for Com-
putational Linguistics.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of the Asso-
ciation for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL 2006.
Matt Post and Daniel Gildea. 2009. Language model-
ing with tree substitution grammars. In Proceedings
967
of the Conference on Neural Information Processing
Systems.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar. In Proceedings of the Association
for Computational Linguistics: Short Paper Track.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the Association of
Computational Linguistics.
Brian Roark. 2004. Probabilistic top-down parsing and
language modeling. Computational Linguistics.
Ming Tan, Wenli Zhou, Lei Zheng, and Shaojun Wang.
2011. A large scale distributed syntactic, semantic
and lexical language model for machine translation.
In Proceedings of the Association for Computational
Linguistics.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the Association
for Computations Linguistics.
Peng Xu, Ciprian Chelba, and Fred Jelinek. 2002. A
study on richer syntactic dependencies for structured
language modeling. In Proceedings of the Association
for Computational Linguistics. Association for Com-
putational Linguistics.
Ying Zhang. 2009. Structured language models for sta-
tistical machine translation. Ph.D. thesis, Johns Hop-
kins University.
968
