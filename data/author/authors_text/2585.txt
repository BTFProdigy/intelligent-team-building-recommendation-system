Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1017?1024
Manchester, August 2008
Bayesian Semi-Supervised Chinese Word Segmentation
for Statistical Machine Translation
Jia Xu
?
, Jianfeng Gao
?
, Kristina Toutanova
?
, Hermann Ney
?
Computer Science 6
?
Microsoft Corporation
?
RWTH Aachen University One Microsoft Way
D-52056 Aachen, Germany Redmond, WA 98052, USA
{xujia,ney}@cs.rwth-aachen.de {jfgao,kristout}@microsoft.com
Abstract
Words in Chinese text are not naturally
separated by delimiters, which poses a
challenge to standard machine translation
(MT) systems. In MT, the widely used
approach is to apply a Chinese word seg-
menter trained from manually annotated
data, using a fixed lexicon. Such word
segmentation is not necessarily optimal
for translation. We propose a Bayesian
semi-supervised Chinese word segmenta-
tion model which uses both monolingual
and bilingual information to derive a seg-
mentation suitable for MT. Experiments
show that our method improves a state-of-
the-art MT system in a small and a large
data environment.
1 Introduction
Chinese sentences are written in the form of a se-
quence of Chinese characters, and words are not
separated by white spaces. This is different from
most European languages and poses difficulty in
many natural language processing tasks, such as
machine translation.
It is difficult to define ?correct? Chinese word
segmentation (CWS) and various definitions have
been proposed. In this work, we explore the idea
that the best segmentation depends on the task, and
concentrate on developing a CWS method for MT,
which leads to better translation performance.
The common solution in Chinese-to-English
translation has been to segment the Chinese text
using an off-the-shelf CWS method, and to apply
a standard translation model given the fixed seg-
mentation. The most widely applied method for
MT is unigram segmentation, such as segmenta-
tion using the LDC (LDC, 2003) tool, which re-
quires a manual lexicon containing a list of Chi-
nese words and their frequencies. The lexicon and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
frequencies are obtained using manually annotated
data. This method is sub-optimal for MT. For ex-
ample, ?(paper) and ](card) can be two words
or composed into one word ?](cards). Since ?
]does not exist in the manual lexicon, it cannot
be generated by this method.
In addition to unigram segmentation, other
methods have been proposed. For example, (Gao
et al, 2005) described an adaptive CWS system,
and (Andrew, 2006) employed a conditional ran-
dom field model for sequence segmentation. How-
ever, these methods are not specifically devel-
oped for the MT application, and significant im-
provements in translation performance need to be
shown.
In (Xu et al, 2004) and (Xu et al, 2005),
word segmentations are integrated into MT sys-
tems during model training and translation. We re-
fine the method in training using a Bayesian semi-
supervised CWS approach motivated by (Goldwa-
ter et al, 2006). We describe a generative model
which consists of a word model and two alignment
models, representing the monolingual and bilin-
gual information, respectively. In our methods, we
first segment Chinese text using a unigram seg-
menter, and then learn new word types and word
distributions, which are suitable for MT.
Our experiments on both large (NIST) and small
(IWSLT) data tracks of Chinese-to-English trans-
lation show that our method improves the per-
formance of a state-of-the-art machine translation
system.
2 Review of the Baseline System
2.1 Word segmentation
In statistical machine translation, we are given a
Chinese sentence in characters c
K
1
= c
1
. . . c
K
which is to be translated into an English sentence
e
I
1
= e
1
. . . e
I
. In order to obtain a more adequate
mapping between Chinese and English words, c
K
1
is usually segmented into words f
J
1
= f
1
. . . f
J
in
preprocessing.
In our baseline system, we apply the commonly
1017
used unigram model to generate the segmenta-
tion. Given a manually compiled lexicon contain-
ing words and their relative frequencies P
s
(f
?
j
),
the best segmentation f
J
1
is the one that maximizes
the joint probability of all words in the sentence,
with the assumption that words are independent of
each other
1
:
f
J
1
= argmax
f
?
J
?
1
Pr(f
?
J
?
1
|c
K
1
)
? argmax
f
?
J
?
1
J
?
?
j=1
P
s
(f
?
j
),
where the maximization is taken over Chinese
word sequences whose character sequence is c
K
1
.
2.2 Translation system
Once we have segmented the Chinese sentences
into words, we train standard alignment models
in both directions with GIZA++ (Och and Ney,
2002) using models of IBM-1 (Brown et al, 1993),
HMM (Vogel et al, 1996) and IBM-4 (Brown et
al., 1993).
Our MT system uses a phrase-based decoder
and the log-linear model described in (Zens and
Ney, 2004). Features in the log-linear model in-
clude translation models in two directions, a lan-
guage model, a distortion model and a sentence
length penalty. The feature weights are tuned on
the development set using a downhill simplex al-
gorithm (Press et al, 2002). The language model
is a statistical ngram model estimated using modi-
fied Kneser-Ney smoothing.
3 Unigram Dirichlet Process Model for
CWS
The simplest version of our model is based on a
unigram Dirichlet Process (DP) model, using only
monolingual information. Different from a stan-
dard unigram model for CWS, our model can in-
troduce new Chinese word types and learn word
distributions automatically from unlabeled data.
According to this model, a corpus of Chinese
words f
1
, . . . f
m
, . . . , f
M
is generated via:
G|?, P
0
? DP (?, P
0
)
f
m
|G ? G
where G is a distribution over words drawn from a
Dirichlet Process prior with base measure P
0
and
concentration parameter ?.
We never explicitly estimate G but instead
integrate over its possible values and perform
Bayesian inference. It is easy to compute the
1
The notational convention will be as follows: we use the
symbol Pr(?) to denote general probability distributions with
(nearly) no specific assumptions. In contrast, for model-based
probability distributions, we use the generic symbol P (?).
probability of a Chinese word given a set of al-
ready generated words, while integrating over G.
This is done by casting Chinese word generation
as a Chinese restaurant process (CRP) (Aldous,
1985), i.e. a restaurant with an infinite num-
ber of tables (approximately corresponding to Chi-
nese word types), each table with infinite number
of seats (approximately corresponding to Chinese
word frequencies).
The Dirichlet Process model can be viewed in-
tuitively as a cache model (Goldwater et al, 2006).
Each word f
j
in the corpus is either retrieved from
a cache or generated anew given the previously ob-
served words f
?j
:
P (f
j
|f
?j
) =
N(f
j
)
+
?P
0
(f
j
)
N + ?
, (1)
whereN(f
j
) is the number of Chinese words f
j
in
the previous context. N is the total number of Chi-
nese words, P
0
is the base probability over words,
and ? influences the probability of introducing a
new word at each step and controls the size of the
lexicon. The probability of generating a word from
the cache increases as more instances of that word
are seen.
For the base distribution P
0
, which governs the
generation of new words, we use the following dis-
tribution (called the spelling model):
P
0
(f) = P (L)P
0
(f |L)
=
?
L
L!
e
??
u
L
(2)
where
1
u
is the number of characters in the docu-
ment, i.e. character vocabulary size, and L is the
number of Chinese characters of word f . We note
that this is a Poisson distribution on word length
and a unigram distribution on characters given the
length. We used ? = 2 and ? = 0.3 in our experi-
ments.
4 CWS Model for MT
As a solution to the problems with the conventional
approach to CWS mentioned in Section 1, we pro-
pose a generative model for CWS in Section 4.1,
and then extend the model to a more general but
deficient model, similar to a maximum entropy
model in which most features are derived from the
submodels of the generative model.
4.1 Generative Model
The generative model assume that a corpus of par-
allel sentences (c
1
K
,e
1
I
) is generated along with a
hidden sequence of Chinese words f
1
J
and a hid-
den word alignment b
1
I
for every sentence. The
alignment indicates the aligned Chinese word f
b
i
for each English word e
i
, where f
0
indicates a spe-
cial null word as in the IBM models.
1018
Without assuming any special form for the prob-
ability of a sentence pair along with hidden vari-
ables, we can factor it into a monolingual Chi-
nese sentence probability and a bilingual transla-
tion probability as follows:
Pr(c
1
K
, e
1
I
, f
1
J
, b
1
I
)
=Pr(c
K
1
, f
J
1
)Pr(e
I
1
, b
I
1
|f
J
1
)
=Pr(f
J
1
)?(f
1
J
, c
1
K
)Pr(e
I
1
, b
I
1
|f
J
1
),
where ?(f
J
1
, c
K
1
) is 1 if the characters of the se-
quence of words f
1
J
are c
1
K
, and to 0 other-
wise. We can drop the conditioning on c
1
K
in
Pr(e
I
1
, b
I
1
|f
J
1
), because the characters are deter-
ministic given the words.
The joint probability of the observations
(c
1
K
, e
I
1
) can be obtained by summing over all
possible values of the hidden variables f
J
1
and b
I
1
.
In Sections 4.1.1 and 4.1.2, we will describe
the modeling assumptions behind the monolingual
Chinese sentence model and the translation model,
respectively.
4.1.1 Monolingual Chinese sentence model
We use the Dirichlet Process unigram word
model introduced in section 3. In this model, the
parameters of a distribution over words G are first
drawn from the Dirichlet prior DP (?, P
0
). Words
are then independently generated according to G.
The probability of a sequence of Chinese words in
a sentence is thus:
Pr(f
J
1
) ?
J
?
j=1
P (f
j
|G) (3)
4.1.2 Translation model
We employ the Dirichlet Process inverse IBM
model 1 to generate English words and alignment
given the Chinese words. In this model, for every
Chinese word f (including the null word), a distri-
bution over English words G
f
is first drawn from
a Dirichlet Process prior DP (?, P
0
(e)), where
P
0
(e) we used the empirical distribution over En-
glish words in the parallel data. Then, given these
parameters, the probability of an English sentence
and alignment given a Chinese sentence (sequence
of words) is given by:
P (e
I
1
, b
I
1
|f
J
1
, G
f
) =
I
?
i=1
1
J + 1
P (e
i
|G
f
b
i
)
This is the same model form as inverse IBM
model 1, except we have placed Dirichlet Process
priors on the Chinese-word specific distributions
over English words.
2
2
f
b
i
is the Chinese word aligned to e
i
and G
f
b
i
is the
distribution over English words conditioned on the word f
b
i
.
Similarly, e
a
j
is the English word aligned to f
j
in the other di-
rection and G
e
a
j
is the distribution over Chinese words con-
ditioned on e
a
j
.
In practice, we observed that using a word-
alignment model in one direction is not sufficient.
We then added a factor to our model which in-
cludes word alignment in the other direction , i.e. a
Dirichlet Process IBM model 1. We ignore the de-
tailed description here, because the calculation is
the same as that of the inverse IBM model 1. Ac-
cording to this model, for every English word e (in-
cluding the null word), a distribution over Chinese
words G
e
is first drawn from a Dirichlet Process
prior DP (?, P
0
(f)). Here, for the base distribu-
tion P
0
(f) we used the same spelling model as for
the monolingual unigram Dirichlet Process prior.
The probability of a sequence of Chinese words
f
J
1
and a word alignment a
J
1
given a sequence of
English words e
I
1
is then:
P (f
J
1
, a
J
1
|e
I
1
, G
e
) =
J
?
j=1
1
I + 1
P (f
j
|G
e
a
j
)
4.2 Final Model
We put the monolingual model and the transla-
tion models in both directions together into a sin-
gle model, where each of the component models
is weighted by a scaling factor. This is similar to
a maximum entropy model. We fit the weights of
the sub-models on a development set by maximiz-
ing the BLEU score of the final translation.
P (c
K
1
, e
I
1
, f
J
1
, a
J
1
, b
I
1
) (4)
?
1
Z
P (f
J
1
)
?
1
? P (e
I
1
, b
I
1
|f
J
1
)
?
2
?P (f
J
1
, a
J
1
|e
I
1
)
?
3
,
where Z is the normalization factor.
In practice we do not re-normalize the proba-
bilities and our model is thus deficient because it
does not sum to 1 over valid observations. How-
ever, we found the model work very good in our
experiments. Similar deficient models have been
used very successfully before, for example, in the
IBM models 3?6 and in the unsupervised grammar
induction model of (Klein and Manning, 2002).
5 Gibbs Sampling Training
It is generally impossible to find the most likely
segmentation according to our Bayesian model us-
ing exact inference, because the hidden variables
do not allow exact computation of the integrals.
Nonetheless, it is possible to define algorithms us-
ing Markov chain Monte Carlo (MCMC) that pro-
duce a stream of samples from the posterior dis-
tribution of the hidden variables given the obser-
vations. We applied the Gibbs sampler (Geman
and Geman, 1984) ? one of the simplest MCMC
methods, in which transitions between states of the
1019
Figure 1: Case I, transition from a no-boundary to
a boundary state, f to f
?
f
??
.
Figure 2: Case II, transition from a boundary to a
no-boundary state, f
?
f
??
to f .
Markov chain result from sampling each compo-
nent of the state conditioned on the current value
of all other variables.
In our problem, the observations are D =
(d
1
, ..d
n
, .., d
N
), where d
n
=(c
K
1
, e
I
1
) indicates a
bilingual sentence pair, the hidden variables are the
word segmentations f
J
1
and the alignments in two
directions a
J
1
and b
I
1
.
To perform Gibbs sampling, we start with
an initial word segmentation and initial word
alignments, and iteratively re-sample the word-
segmentation and alignments according to our
model of Equation 4.
Note that for efficiency, we only allow limited
modifications to the initial word alignments. Thus
we only use models derived from IBM-1 (instead
of IBM-4) for comparing different word segmenta-
tions. On the other hand, re-sampling the segmen-
tation causes re-linking alignment points to parts
or groups of the original words.
Hence, we organize our sampling process
around possible word boundaries. For each char-
acter c
k
in each sentence, we consider two alterna-
tive segmentations: c
k
+
indicates the segmentation
where there is a boundary after c
k
and c
k
?
indi-
cates the segmentation where there is no boundary
after c
k
, keeping all other boundaries fixed. Let f
denote the single word spanning character c
k
when
there is no boundary after it, and f
?
,f
??
denote the
two adjacent words resulting if there is a bound-
ary: f
?
includes c
k
and f
??
starts just to the right,
with character c
k+1
. The introduction of f
?
and
f
??
leads to M new possible alignments in the E-
to-C direction b
+
k1
, . . . , b
+
kM
, such as in Figure 1.
Together with the boundary vs no-boundary state
at each character position, we re-sample a set of
alignment links between English words and any of
the Chinese words f ,f
?
, and f
??
, keeping all other
word alignments in the sentence pair fixed. (See
Figures 1 and 2.)
Table 1: General Algorithm of GS for CWS.
Input: D with an initial segmentation and alignments
Output: D with sampled segmentation and alignments
for n = 1 to
?
N
for k = 1 to K that c
k
? d
n
Create M+1 candidates, cba
+
k,m
and cba
?
k
, where
cba
+
k,m
: there is a word boundary after c
k
cba
?
k
: there is no word boundary after c
k
Compute probabilities
P (cba
+
k,m
|dh
nk
?
)
P (cba
?
k
|dh
nk
?
)
Sample boundary and relevant alignments
Update counts
Thus at each step in the Gibbs sampler, we con-
sider a set of alternatives for the boundary after
c
k
and relevant alignment links, keeping all other
hidden variables fixed. At each step, we need to
compute the probability of each of the alternatives,
given the fixed values of the other hidden variables.
We introduce some notation to make the presen-
tation easier. For every position k in sentence pair
n, we denote by dh
nk
?
the observations and hid-
den variables for all sentences other than sentence
n, and the observations and hidden variables in-
side sentence n, not involving character position
c
k
. The fixed variables inside the sentence are the
words not neighboring position k, and the align-
ments in both directions to these words.
In the process of sampling, we consider a set
of alternatives: segmentation c
k
+
along with the
product space of relevant alignments in both direc-
tions b
+
k1
, . . . , b
+
kM
, and a
+
k
, and segmentation c
?
k
along with relevant alignments b
k
?
and a
?
k
. For
brevity, we denote these alternatives by cba
k,m
+
and cba
k
?
.
We describe how we derive the set of alterna-
tives in section 5.2 and how we compute their
probabilities in section 5.1.
Table 1 shows schematically one iteration of
Gibbs sampling through the whole training corpus
of parallel sentences, where
?
N is the number of
parallel sentences.
5.1 Computing probabilities of alternatives
For the Gibbs sampling algorithm in Table 1, we
need to compute the probability of each alternative
segmentation/alignments, given the fixed values of
the rest of the data dh
nk
?
. The probability of the
hidden variables in the alternatives is proportional
to the joint probability of the hidden variables and
observations, and thus it is sufficient to compute
the probability of the latter. We compute these
probabilities using the Chinese restaurant process
sampling scheme for the Dirichlet Process, thus in-
1020
tegrating over all of the possible values of the dis-
tributions G, G
f
and G
e
.
Let cba
k
denote an alternative hypothesis in-
cluding boundary or no boundary at position k,
and relevant alignments to English words in both
directions of the one or two Chinese words result-
ing from the segmentation at k. The probability of
this configuration given by our model is:
P (cba
k
|dh
nk
?
) ? P
m
(cba
k
|dh
nk
?
)
?
1
(5)
?P
ef
(cba
k
|dh
nk
?
)
?
2
? P
fe
(cba
k
|dh
nk
?
)
?
3
,
where P
m
(cba
k
|dh
nk
?
) is the monolingual
word probability, and P
fe
(cba
k
|dh
nk
?
) and
P
ef
(cba
k
|dh
nk
?
) are the translation probabilities
in the two directions.
We now describe the computations of each of
the component probabilities.
5.1.1 Word model probability
The word model probability P
m
(cab
k
|dh
nk
?
)
in Equation 5 is derived from Equations 3 and 1:
There are two cases, depending on whether the
hypothesis specifies that there is a boundary after
character c
k
, in which case we need the probabili-
ties of the two resulting words f
?
, and f
??
, or there
is no boundary, in which case we need the proba-
bility of the single word f . (See the initial states in
Figures 1 and 2, respectively.)
Let N denote the total number of word tokens
in the rest of the corpus dh
nk
?
, and N(f) denote
the number of instances of word f in dh
nk
?
. The
probabilities in the two cases are
P
m
(c
+
k
|dh
nk
?
) ?
N(f
?
) + ?P
0
(f
?
)
N + ?
?
N(f
??
) + ?P
0
(f
??
)
N + ?
P
m
(c
?
k
|dh
nk
?
) ?
N(f) + ?P
0
(f)
N + ?
Here P
0
(f) is computed using Equation 2.
5.1.2 Translation model probability
The translation model probabilities depend on
whether or not there is a segmentation boundary
at c
k
and which English words are aligned to the
relevant Chinese words.
In the first case, assume that there is a word
boundary in cab
k
, and that English words {e
?
} are
aligned to f
?
and words {e
??
} are aligned to f
??
in
the E-to-C direction according to the alignment b
k
,
and that f
?
is aligned to e
?
?
and f
??
is aligned to e
?
??
in the C-to-E direction according to the alignment
a
k
(see the initial state in Figure 1). Here we over-
loaded notations and use b
k
and a
k
to indicate the
alignments of the relevant Chinese words at posi-
tion k to any English words. Let I denote the total
number of English words in the sentence, and J+1
denote the number of Chinese words according to
this segmentation. We also denote the total num-
ber of English words aligned to either f
?
or f
??
in
the E-to-C direction by P .
The translation model probability in the E-to-C
direction is thus:
P
ef
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
1
(J + 2
)
P
?
e
?
P (e
?
|f
?
, dh
nk
?
)
?
e
??
P (e
??
|f
??
, dh
nk
?
)
Here we compute P (e|f, dh
nk
?
) as:
P (e|f, dh
nk
?
) =
N(e, f) + ?P
0
(e)
N(f) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
The translation probability in the other direction
is similarly computed as:
P
fe
(c
+
k
, b
k
, a
k
|dh
nk
?
) ?
(
1
I + 1
)
2
P (f
?
|e
?
, dh
nk
?
)P (f
??
|e
?
, dh
nk
?
)
And P (f |e, dh
nk
?
) is computed as:
P (f |e, dh
nk
?
) =
N(f, e) + ?P
0
(f)
N(e) + ?
,
where the counts are computed over the fixed as-
signments dh
nk
?
.
In the second case, if the hypothesis in evalua-
tion does not have a word boundary at position k,
the total number of Chinese words would be one
less, i.e. J instead of J +1 in the equations above,
and there would be a single set of English words
aligned to the word f in the E-to-C direction, and a
single word e
?
aligned to f in the C-to-E direction
(see the initial state in Figure 2. The probability of
this hypothesis is computed analogously.
5.2 Determining the set of alternative
hypotheses
As mentioned earlier, we consider alternative
alignments which deviate minimally from the cur-
rent alignments, and which satisfy the constraints
of the IBM model 1 in both directions. In order
to describe the set of alternatives, we consider two
cases, depending on whether there is a boundary at
the current character before sampling at position k.
Case 1. There was no boundary at c
k
in the previ-
ous state (see Figure 1).
1021
If there is no boundary at c
k
, there is a sin-
gle word f spanning that position. We denote by
{e} the set of English words aligned to f at that
state in the E-to-C direction and by e
?
the En-
glish word aligned to f in the C-to-E direction.
Since every state we consider satisfies the IBM
one-to-many constraints, there is exactly one En-
glish word aligned to f in the C-to-E direction and
the words {e} have no other words aligned to them
in the E-to-C direction.
In this case, we consider as hypothesis cba
k
?
the same segmentation and alignment as in the pre-
vious state. (see Table 1 for an overview of the
alternative hypotheses.)
We consider M different hypotheses which in-
clude a boundary at k in this case, where M de-
pends on the number of words {e} aligned to f
in the previous state. Because we are breaking
the word f into two words f
?
and f
??
by placing
a boundary at c
k
, we need to re-align the words
{e} to either f
?
or f
??
. Additionally we need to
align f
?
and f
??
to English words in the C-to-E
direction. The number of different hypotheses is
equal to 2
P
where P = |{e}|. These alternatives
arise by considering that each of the words in {e}
needs to align to either f
?
or f
??
, and there are 2
P
combinations of these alignments. For example, if
{e} = {e
1
, e
2
}, after splitting the word f there are
four possible alignments, illustrated in Figure 1:
I. (f
?
, e
1
) and (f
??
, e
2
), II. (f
?
, e
2
) and (f
??
, e
1
),
III. (f
?
, e
1
) and (f
?
, e
2
), IV. (f
??
, e
1
) and (f
??
, e
2
).
For the alignment a
k
in the C-to-E direction, we
consider only one option, in which both resulting
words f
?
and f
??
align to e
?
. These alternatives
form cba
k,m
+
in Table 1.
Case 2. There was a boundary at c
k
in the previous
state (see Figure 2).
In this case, for the hypotheses c
+
k
we consider
only one alternative, which is exactly the same as
the assignment of segmentation and alignments in
the previous state. Thus we have M = 1 in Table
1.
Let f
?
and f
??
denote the two words at position
k in the previous state, {e
?
} and {e
??
} denote the
sets of English words aligned to them in the E-to-C
direction, respectively, and e
?
?
and e
?
??
denote the
English words aligned to f
?
and f
??
in the C-to-E
direction.
We consider only one hypothesis cba
k
?
where
there is no boundary at c
k
. In this hypothesis, there
is a single word f = f
?
f
??
spanning position k,
and all words {e
?
} ? {e
??
} align to f in the E-to-
C direction. For the C-to-E direction we consider
the ?better? of the alignments (f, e
?
?
) and (f, e
??
?
)
where the better alignment is defined as the one
having higher probability according to the C-to-E
word translation probabilities.
Table 2: Complete Algorithm of Gibbs Sampler
for CWS including Alignment Models.
Input: D, F
0
Output: A
T
, F
T
for t = 1 to T
Run GIZA++ on (D,F
t?1
) to obtain A
t
Run GS on (D,F
t?1
, A
t
) to obtain F
t
5.3 Complete segmentation algorithm
So far, we have described how we re-sample word
segmentation and alignments according to our
model, starting from an initial segmentation and
alignments from GIZA++. Putting these pieces to-
gether, the algorithm is summarized in Table 1.
We found that we can further improve perfor-
mance by repeatedly aligning the corpus using
GIZA++, after deriving a new segmentation us-
ing our model. The complete algorithm which in-
cludes this step is shown in Table 2, where F
t
in-
dicates the word segmentation at iteration t and A
t
denotes the GIZA++ corpus alignment in both di-
rections. The GS re-segmentation step is done ac-
cording to the algorithm in Table 1.
Using this algorithm, we obtain a new segmen-
tation of the Chinese data and train the translation
models using this segmentation as in the baseline
MT system. To segment the test data for transla-
tion, we use a unigram model, trained with maxi-
mum likelihood estimation off of the final segmen-
tation of the training corpus F
T
.
6 Translation Experiments
We performed experiments using our models for
CWS on a large and a small data track. We evalu-
ated performance by measuring WER (word error
rate), PER (position-independent word error rate),
BLEU (Papineni et al, 2002) and TER (translation
error rate) (Snover et al, 2006) using multiple ref-
erences.
6.1 Translation Task: Large Track NIST
We first report the experiments using our mono-
lingual unigram Dirichlet Process model for word
segmentation on the NIST machine translation task
(NIST, 2005). Because of the computational re-
quirements, we only employed the monolingual
word model for this large data track, i.e. the fea-
ture weights were ?
1
= 1, ?
2
= 0, ?
3
= 0. There-
fore, no alignment information needs to be main-
tained in this case.
The bilingual training corpus is a superset of
corpora in the news domain collected from differ-
ent sources.
We took LDC (LDC, 2003) as a baseline CWS
method (Base). As shown in Table 3, the training
corpus in each language contains more than two
million sentences. There are 56 million Chinese
1022
Table 3: Statistics of corpora in task NIST.
Data Sents. Words[K] Voc.[K]
Cn. En. Cn. En.
Chars 2M 56M 49.5M 65.4 211
Base 39.2M 95.7
GS 40.5M 95.4
02 878 23.1 28.0 2.04 4.34
03 919 24.6 29.2 2.21 4.91
04 1788 49.8 60.7 2.61 6.71
05 1082 30.8 34.2 2.30 5.39
Table 4: Translation performance [% BLEU] with
the baseline(LDC) and GS method on NIST.
MT-eval LDC(Base) GS
2005 32.85 33.26
2002 34.32 34.36
2003 33.41 33.75
2004 33.74 34.06
characters. The LDC and GS word segmentation
methods generated 39.2 and 40.5 million running
words, respectively.
The scaling factors of the translation models de-
scribed in Section 2.2 were optimized on the devel-
opment corpus, MT-eval 05 with 1082 sentences.
The resulting systems were evaluated on the test
corpora MT-eval 02-04. For convenience, we only
list the statistics of the first English reference.
Starting from the baseline LDC output as ini-
tial word segmentation, we performed Gibbs sam-
pling (GS) of word segmentations using 30 itera-
tions over the Chinese training corpus.
Since BLEU is the official NIST measure of
translation performance, we show the translation
results measured in BLEU score only. As shown
in Table 4, on the development data MT-eval 05,
the BLEU score was improved by 0.4% absolute or
more than 1% relative using GS. Similarly, the ab-
solute BLEU scores are also improved on all other
test sets, in the range of 0.04% to 0.4%.
We can see that even a monolingual semi-
supervised word segmentation method can outper-
form a supervised one in MT, probably because the
training/test corpora contain many unknown words
and words have different frequencies in our MT
data from they do in the manually labeled CWS
data.
6.2 Translation Task: Small Track IWSLT
We evaluate our full model, using both monolin-
gual and bilingual information, on the IWSLT data.
As shown in Table 5, the Chinese training
corpus was segmented using the unigram seg-
menter (Base) described in Section 2.1 and our GS
method. Since the unigram segmenter performs
better in our experiments, we took it as the base-
line and the method for initialization in later ex-
periments. We see that the vocabulary size of the
Chinese training corpus was reduced more signif-
icantly by GS than by the baseline method, even
Table 5: Statistics of corpora in task IWSLT.
Test Sents. Words[K] Voc.
Cn. En. Cn. En.
Chars 42.9K 520 420 2780 9930
Base 394 8800
GS 398 6230
Dev2 500 3.74 3.82 1004 821
Dev3 506 4.01 3.90 980 820
Eval 489 3.39 3.72 904 810
Table 6: Translation performance with different
CWS methods on IWSLT[%].
Test Method WER PER BLEU TER
Dev2 Unigram (Base) 38.2 31.2 55.4 37.0
GS 36.8 30.0 56.6 35.5
Dev3 Unigram (Base) 33.5 27.5 60.4 32.1
GS 32.3 26.6 61.0 31.4
Eval Characters 49.3 41.8 35.4 47.5
LDC 46.2 40.0 39.2 45.0
ICT 45.9 40.4 40.1 44.9
Unigram (Base) 46.8 40.2 41.6 45.6
9-gram 46.9 40.4 40.1 45.4
GS 45.9 40.0 41.6 44.8
though they resulted in a similar number of run-
ning words. This shows that the distribution of
Chinese words is more concentrated when using
GS.
The parameter optimizations were performed on
the Dev2 data with 500 sentences, and evaluations
were done both on Dev3 and on Eval data, i.e. the
evaluation corpus of (IWSLT, 2007).
The model weights ? of GS from Section 5.1.2
were optimized using the Powell (Press et al,
2002) algorithm with respect to the BLEU score.
We obtained ?
1
= 1.4, ?
2
= 1 and ?
3
= 0.8 as
optimal values and T = 4 as the optimal number
of iterations of re-alignment with GIZA++.
For a fair comparison, we evaluated on various
CWS methods including translation on characters
, LDC (LDC, 2003), ICT (Zhang et al, 2003), uni-
gram, 9-gram and GS. Improvements using GS can
be seen in Table 6. Under all test sets and evalua-
tion criteria, GS outperforms the baseline method.
The absolute WER decreases with 1.2% on Dev3
and with 1.1% on Eval data over baseline.
We compared the translation outputs using GS
with the baseline method. On the Eval data, 196
sentences are different out of 489 lines, where 64
sentences from GS are better, 33 sentences are
worse, and the rests have similar translation qual-
ities. Table 7 shows two examples from the Eval
corpus. We list segmentations produced by the
baseline and GS methods, as well as the transla-
tions corresponding to these segmentations. The
GS method generates better translation results than
the baseline method in these cases.
1023
Table 7: Segmentation and translation outputs with
baseline and GS methods.
a) Baseline ? ?4 m?
do you have a ?
GS ??4m?
do you have a shorter way ?
REF is there a shorter route ?
b) Baseline >????
please show me the in .
GS >????
please show me the total price .
REF can you tell me the total amount ?
7 Conclusion and future work
We showed that it is possible to learn Chinese word
boundaries such that the translation performance
of Chinese-to-English MT systems is improved.
We presented a Bayesian generative model for
parallel Chinese-English sentences which uses
word segmentation and alignment as hidden vari-
ables, and incorporates both monolingual and
bilingual information to derive a segmentation
suitable for MT.
Starting with an initial word segmentation, our
method learns both new Chinese words and dis-
tributions for these words. In a large and a small
data environment, our method outperformed the
standard Chinese word segmentation approach in
terms of the Chinese to English translation quality.
In future work, we plan to enrich our monolingual
and bilingual models to better represent the true
distribution of the data.
8 Acknowledgments
Jia Xu conducted this research during her intern-
ship at Microsoft Research. This material is also
partly based upon work supported by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-06-C-0023.
References
Aldous, D. 1985. Exchangeability and related topics.
In
?
Ecole d??et?e de probabilit?es de Saint-Flour, XIII-
1983, pages 1?198, Springer, Berlin.
Andrew, G. 2006. A hybrid markov/semi-markov con-
ditional random field for sequence segmentation. In
Proceedings of EMNLP, Sydney, July.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
Gao, J., M. Li, A. Wu, and C. Huang. 2005. Chi-
nese word segmentation and named entity recogni-
tion: A pragmatic approach. Computational Lin-
guistics, 31(4).
Goldwater, S., T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of Coling/ACL, Sydney,
July.
IWSLT. 2007. International workshop on
spoken language translation home page.
http://www.slt.atr.jp/IWSLT2007.
Klein, D. and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proceedings of ACL, pages 128?135.
LDC. 2003. Linguistic data consor-
tium Chinese resource home page.
http://www.ldc.upenn.edu/Projects/Chinese.
NIST. 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
Och, F. J. and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302,
Philadelphia, PA, July.
Papineni, K. A., S. Roukos, T. W., and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, July.
Press, W. H., S. A. Teukolsky, W. T. Vetterling, and
B. P. Flannery. 2002. Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA, pages 223?231, Cambridge, MA, August.
Vogel, S., H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of COLING.
Xu, J., R. Zens, and H. Ney. 2004. Do we need
Chinese word segmentation for statistical machine
translation? In Proceedings of the SIGHAN Work-
shop on Chinese Language Learning, pages 122?
128, Barcelona, Spain, July.
Xu, J., E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proceedings of IWSLT, pages
141?147, Pittsburgh, PA, October.
Zens, R. and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proceedings
of HLT/NAACL, Boston, MA, May.
Zhang, H., H. Yu, D. Xiong, and Q. Liu. 2003.
HHMM-based Chinese lexical analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop on
Chinese Language Learning, pages 184?187, July.
1024
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 199?207, Prague, June 2007. c?2007 Association for Computational Linguistics
Compressing Trigram Language Models With Golomb Coding 
Ken Church 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
Ted Hart 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
Jianfeng Gao 
Microsoft 
One Microsoft Way 
Redmond, WA, USA 
 
{church,tedhar,jfgao}@microsoft.com 
 
 
Abstract 
Trigram language models are compressed 
using a Golomb coding method inspired by 
the original Unix spell program.  
Compression methods trade off space, time 
and accuracy (loss).  The proposed 
HashTBO method optimizes space at the 
expense of time and accuracy.  Trigram 
language models are normally considered 
memory hogs, but with HashTBO, it is 
possible to squeeze a trigram language 
model into a few megabytes or less.  
HashTBO made it possible to ship a 
trigram contextual speller in Microsoft 
Office 2007. 
1 Introduction 
This paper will describe two methods of com-
pressing trigram language models: HashTBO and 
ZipTBO. ZipTBO is a baseline compression me-
thod that is commonly used in many applications 
such as the Microsoft IME (Input Method Editor) 
systems that convert Pinyin to Chinese and Kana to 
Japanese. 
Trigram language models have been so success-
ful that they are beginning to be rolled out to appli-
cations with millions and millions of users: speech 
recognition, handwriting recognition, spelling cor-
rection, IME, machine translation and more.  The 
EMNLP community should be excited to see their 
technology having so much influence and visibility 
with so many people.  Walter Mossberg of the 
Wall Street Journal called out the contextual spel-
ler (the blue squiggles) as one of the most notable 
features in Office 2007: 
There are other nice additions. In Word, Out-
look and PowerPoint, there is now contextual spell 
checking, which points to a wrong word, even if 
the spelling is in the dictionary. For example, if 
you type ?their? instead of ?they're,? Office 
catches the mistake. It really works. 1 
The use of contextual language models in spel-
ling correction has been discussed elsewhere: 
(Church and Gale, 1991), (Mays et al 1991), (Ku-
kich, 1992) and (Golding and Schabes, 1996).  
This paper will focus on how to deploy such me-
thods to millions and millions of users.  Depending 
on the particular application and requirements, we 
need to make different tradeoffs among: 
1. Space (for compressed language model), 
2. Runtime (for n-gram lookup), and 
3. Accuracy (losses for n-gram estimates). 
HashTBO optimizes space at the expense of the 
other two.  We recommend HashTBO when space 
concerns dominate the other concerns; otherwise, 
use ZipTBO. 
 There are many applications where space is ex-
tremely tight, especially on cell phones.  HashTBO 
was developed for contextual spelling in Microsoft 
Office 2007, where space was the key challenge.  
The contextual speller probably would not have 
shipped without HashTBO compression.   
We normally think of trigram language models 
as memory hogs, but with HashTBO, a few mega-
bytes are more than enough to do interesting things 
with trigrams.  Of course, more memory is always 
better, but it is surprising how much can be done 
with so little.   
For English, the Office contextual speller started 
with a predefined vocabulary of 311k word types 
and a corpus of 6 billion word tokens.   (About a 
                                                 
1 
http://online.wsj.com/public/article/SB11678611102296
6326-
T8UUTIl2b10DaW11usf4NasZTYI_20080103.html?m
od=tff_main_tff_top  
199
third of the words in the vocabulary do not appear 
in the corpus.)  The vocabularies for other lan-
guages tend to be larger, and the corpora tend to be 
smaller.  Initially, the trigram language model is 
very large.  We prune out small counts (8 or less) 
to produce a starting point of 51 million trigrams, 
14 million bigrams and 311k unigrams (for Eng-
lish).  With extreme Stolcke, we cut the 51+14+0.3 
million n-grams down to a couple million.  Using a 
Golomb code, each n-gram consumes about 3 
bytes on average. 
With so much Stolcke pruning and lossy com-
pression, there will be losses in precision and re-
call.  Our evaluation finds, not surprisingly, that 
compression matters most when space is tight.  
Although HashTBO outperforms ZipTBO on the 
spelling task over a wide range of memory sizes, 
the difference in recall (at 80% precision) is most 
noticeable at the low end (under 10MBs), and least 
noticeable at the high end (over 100 MBs).  When 
there is plenty of memory (100+ MBs), the differ-
ence vanishes, as both methods asymptote to the 
upper bound (the performance of an uncompressed 
trigram language model with unlimited memory). 
2 Preliminaries 
Both methods start with a TBO (trigrams with 
backoff) LM (language model) in the standard 
ARPA format.  The ARPA format is used by many 
toolkits such as the CMU-Cambridge Statistical 
Language Modeling Toolkit.2 
2.1 Katz Backoff 
No matter how much data we have, we never 
have enough.  Nothing has zero probability.  We 
will see n-grams in the test set that did not appear 
in the training set.  To deal with this reality, Katz 
(1987) proposed backing off from trigrams to bi-
grams (and from bigrams to unigrams) when we 
don?t have enough training data.   
Backoff doesn?t have to do much for trigrams 
that were observed during training.  In that case, 
the backoff estimate of  ?(?? |???2???1)  is simply 
a discounted probability ??(??|???2???1). 
The discounted probabilities steal from the rich 
and give to the poor.  They take some probability 
mass from the rich n-grams that have been seen in 
training and give it to poor unseen n-grams that 
                                                 
2 http://www.speech.cs.cmu.edu/SLM 
might appear in test.  There are many ways to dis-
count probabilities.  Katz used Good-Turing 
smoothing, but other smoothing methods such as 
Kneser-Ney are more popular today. 
Backoff is more interesting for unseen trigrams.  
In that case, the backoff estimate is: 
? ???2???1 ??(??|???1) 
The backoff alphas (?) are a normalization fac-
tor that accounts for the discounted mass.  That is, 
 
? ???2???1 
=
1?  ?(??|???2???1)? ? :?(? ??2? ??1? ?)
1?  ?(??|???1)? ? :?(? ??2? ??1? ?)
 
 
where ? ???2???1?? > 0  simply says that the 
trigram was seen in training data. 
3 Stolcke Pruning 
Both ZipTBO and HashTBO start with Stolcke 
pruning (1998).3   We will refer to the trigram lan-
guage model after backoff and pruning as a pruned 
TBO LM. 
Stolcke pruning looks for n-grams that would 
receive nearly the same estimates via Katz backoff 
if they were removed.  In a practical system, there 
will never be enough memory to explicitly mate-
rialize all n-grams that we encounter during train-
ing.  In this work, we need to compress a large set 
of n-grams (that appear in a large corpus of 6 bil-
lion words) down to a relatively small language 
model of just a couple of megabytes. We prune as 
much as necessary to make the model fit into the 
memory allocation (after subsequent Hash-
TBO/ZipTBO compression).   
Pruning saves space by removing n-grams sub-
ject to a loss consideration: 
1. Select a threshold ?. 
2. Compute the performance loss due to prun-
ing each trigram and bigram individually us-
ing the pruning criterion. 
3. Remove all trigrams with performance loss 
less than ? 
4. Remove all bigrams with no child nodes (tri-
gram nodes) and with performance loss less 
than ?   
5. Re-compute backoff weights. 
                                                 
3 
http://www.nist.gov/speech/publications/darpa98/html/l
m20/lm20.htm  
200
Stolcke pruning uses a loss function based on 
relative entropy.  Formally, let P denote the tri-
gram probabilities assigned by the original un-
pruned model, and let P? denote the probabilities in 
the pruned model.  Then the relative entropy 
D(P||P?) between the two models is 
 
? ? ?,? [log?? ? ? ? log?(?,?)]
? ,?
 
 
where h is the history.  For trigrams, the history is 
the previous two words.  Stolcke showed that this 
reduces to 
?? ? {?(?|?) 
[log? ? ?? + log?? ? ? log?(?|?)] 
+[log??(?) ? log?(?)]  ? ? ? 
? :? ? ,? >0
} 
 
where ??(?)  is the revised backoff weight after 
pruning and h? is the revised history after dropping 
the first word.  The summation is over all the tri-
grams that were seen in training: ? ?,? > 0.  
Stolcke pruning will remove n-grams as neces-
sary, minimizing this loss. 
3.1 Compression on Top of Pruning 
After Stolcke pruning, we apply additional com-
pression (either ZipTBO or HashTBO).  ZipTBO 
uses a fairly straightforward data structure, which 
introduces relatively few additional losses on top 
of the pruned TBO model.  A few small losses are 
introduced by quantizing the log likelihoods and 
the backoff alphas, but those losses probably don?t 
matter much.  More serious losses are introduced 
by restricting the vocabulary size, V, to the 64k 
most-frequent words.  It is convenient to use byte 
aligned pointers.   The actual vocabulary of more 
than 300,000 words for English (and more for oth-
er languages) would require 19-bit pointers (or 
more) without pruning.   Byte operations are faster 
than bit operations.  There are other implementa-
tions of ZipTBO that make different tradeoffs, and 
allow for larger V without pruning losses. 
HashTBO is more heroic.  It uses a method in-
spired by McIlroy (1982) in the original Unix Spell 
Program, which squeezed a word list of N=32,000 
words into a PDP-11 address space (64k bytes).  
That was just 2 bytes per word!   
HashTBO uses similar methods to compress a 
couple million n-grams into half a dozen mega-
bytes, or about 3 bytes per n-gram on average (in-
cluding log likelihoods and alphas for backing off).  
ZipTBO is faster, but takes more space (about 4 
bytes per n-gram on average, as opposed to 3 bytes 
per n-gram).  Given a fixed memory budget, 
ZipTBO has to make up the difference with more 
aggressive Stolcke pruning.  More pruning leads to 
larger losses, as we will see, for the spelling appli-
cation.   
Losses will be reported in terms of performance 
on the spelling task.  It would be nice if losses 
could be reported in terms of cross entropy, but the 
values output by the compressed language models 
cannot be interpreted as probabilities due to quan-
tization losses and other compression losses. 
4 McIlroy?s Spell Program 
McIlroy?s spell program started with a hash ta-
ble.  Normally, we store the clear text in the hash 
table, but he didn?t have space for that, so he 
didn?t.   Hash collisions introduce losses. 
McIlroy then sorted the hash codes and stored 
just the interarrivals of the hash codes instead of 
the hash codes themselves.  If the hash codes, h, 
are distributed by a Poisson process, then the inte-
rarrivals, t, are exponentially distributed: 
 
Pr ? = ????? ,  
 
where ? =
?
?
.  Recall that the dictionary contains 
N=32,000 words.  P is the one free parameter, the 
range of the hash function.   McIlroy hashed words 
into a large integer mod P, where P is a large 
prime that trades off space and accuracy.  Increas-
ing P consumes more space, but also reduces 
losses (hash collisions). 
McIlroy used a Golomb (1966) code to store the 
interarrivals.  A Golomb code is an optimal Huff-
man code for an infinite alphabet of symbols with 
exponential probabilities. 
The space requirement (in bits per lexical entry) 
is close to the entropy of the exponential. 
 
? = ?  Pr ? log2 Pr ? ??
?
?=0
 
? =  
1
log? 2
+  log2
1
?
   
 
201
 The ceiling operator ? ?  is introduced because 
Huffman codes use an integer number of bits to 
encode each symbol. 
We could get rid of the ceiling operation if we 
replaced the Huffman code with an Arithmetic 
code, but it is probably not worth the effort. 
Lookup time is relatively slow.  Technically, 
lookup time is O(N), because one has to start at the 
beginning and add up the interarrivals to recon-
struct the hash codes.  McIlroy actually introduced 
a small table on the side with hash codes and off-
sets so one could seek to these offsets and avoid 
starting at the beginning every time.  Even so, our 
experiments will show that HashTBO is an order 
of magnitude slower than ZipTBO. 
Accuracy is also an issue.  Fortunately, we don?t 
have a problem with dropouts.  If a word is in the 
dictionary, we aren?t going to misplace it.  But two 
words in the dictionary could hash to the same val-
ue.  In addition, a word that is not in the dictionary 
could hash to the same value as a word that is in 
the dictionary.  For McIlroy?s application (detect-
ing spelling errors), the only concern is the last 
possibility.  McIlroy did what he could do to miti-
gate false positive errors by increasing P as much 
as he could, subject to the memory constraint (the 
PDP-11 address space of 64k bytes). 
We recommend these heroics when space domi-
nates other concerns (time and accuracy). 
5 Golomb Coding 
Golomb coding takes advantage of the sparseness 
in the interarrivals between hash codes.  Let?s start 
with a simple recipe.  Let t be an interarrival.   We 
will decompose t into a pair of a quotient (tq) and a 
remainder (tr).  That is, let ? = ??? + ??  where 
?? = ??/ ??  and ?? = ? mod ?.  We choose m to 
be a power of two near ? ?  
? ? 
2
 =  
?
2?
 , where 
E[t] is the expected value of the interarrivals, de-
fined below.  Store tq in unary and tr in binary. 
Binary codes are standard, but unary is not.  To 
encode a number z in unary, simply write out a 
sequence of z-1 zeros followed by a 1.  Thus, it 
takes z bits to encode the number z in unary, as 
opposed to  log2 ? bits in binary. 
This recipe consumes ?? + log2?  bits.  The 
first term is for the unary piece and the second 
term is for the binary piece. 
Why does this recipe make sense?  As men-
tioned above, a Golomb code is a Huffman code 
for an infinite alphabet with exponential probabili-
ties.  We illustrate Huffman codes for infinite al-
phabets by starting with a simple example of a 
small (very finite) alphabet with just three sym-
bols: {a, b, c}. Assume that half of the time, we 
see a, and the rest of the time we see b or c, with 
equal probabilities: 
 
Symbol Code Length Pr 
A 0 1 50% 
B 10 2 25% 
C 11 2 25% 
 
The Huffman code in the table above can be read 
off the binary tree below.   We write out a 0 when-
ever we take a left branch and a 1 whenever we 
take a right branch.  The Huffman tree is con-
structed so that the two branches are equally likely 
(or at least as close as possible to equally likely). 
 
 
 
 
Now, let?s consider an infinite alphabet where 
Pr ? =
1
2
 , Pr ? =
1
4
  and the probability of the 
t+1st symbol is Pr ? = (1? ?)??  where ? =
1
2
.  
In this case, we have the following code, which is 
simply t in unary.  That is, we write out 1?t  zeros 
followed by a 1. 
 
Symbol Code Length Pr 
A 1 1 2?1 
B 01 2 2?2 
C 001 3 2?3 
 
202
The Huffman code reduces to unary when the 
Huffman tree is left branching: 
 
 
 
In general, ? need not be ?.  Without loss of ge-
nerality, assume Pr ? =  1 ? ? ??  where 
1
2
? ? < 1 and ? ? 0.  ? depends on E[t], the ex-
pected value of the interarrivals: 
 
? ? =
?
?
=
?
1? ?
? ? =
? ? 
1 + ? ? 
 
 
Recall that the recipe above calls for expressing 
t as ? ? ?? + ??  where ?? = ?
?
?
?  and ?? =
? mod ?.  We encode tq in unary and tr
 in binary.  
(The binary piece consumes log2?  bits, since tr 
ranges from 0 to m.) 
How do we pick m?   For convenience, let m be 
a power of 2.   The unary encoding makes sense as 
a Huffman code if ?? ?
1
2
.   
Thus, a reasonable choice 4  is ? ?  
? ? 
2
 .   If 
? =
? ? 
1+? ? 
, then ?? =
? ? ?
 1+? ?  ?
? 1?
?
? ? 
.  Set-
ting ?? ?
1
2
, means ? ?
? ? 
2
. 
                                                 
4 This discussion follows slide 29 of 
http://www.stanford.edu/class/ee398a/handouts/lectures/
01-EntropyLosslessCoding.pdf.   See (Witten et al 
6  HashTBO Format 
The HashTBO format is basically the same as McI-
lroy?s format, except that McIlroy was storing 
words and we are storing n-grams.    One could 
store all of the n-grams in a single table, though we 
actually store unigrams in a separate table.  An n-
gram is represented as a key of n integers (offsets 
into the vocabulary) and two values, a log likelih-
ood and, if appropriate, an alpha for backing off.    
We?ll address the keys first. 
6.1 HashTBO Keys 
Trigrams consist of three integers (offsets into 
the Vocabulary): ?1?2?3. These three integers are 
mapped into a single hash between 0 and ? ? 1 in 
the obvious way: 
 
???? =  ?3?
0 +?2?
1 +?1?
2  mod ?  
 
where V is vocabulary size.  Bigrams are hashed 
the same way, except that the vocabulary is padded 
with an extra symbol for NA (not applicable).  In 
the bigram case, ?3 is NA. 
We then follow a simple recipe for bigrams and 
trigrams: 
1. Stolcke prune appropriately 
2. Let N be the number of n-grams 
3. Choose an appropriate P (hash range) 
4. Hash the N n-grams 
5. Sort the hash codes 
6. Take the first differences (which are mod-
eled as interarrivals of a Poisson process) 
7. Golomb code the first differences  
 
We did not use this method for unigrams, since 
we assumed (perhaps incorrectly) that we will have 
explicit likelihoods for most of them and therefore 
there is little opportunity to take advantage of 
sparseness. 
Most of the recipe can be fully automated with a 
turnkey process, but two steps require appropriate 
hand intervention to meet the memory allocation 
for a particular application: 
1. Stolcke prune appropriately, and 
2. Choose an appropriate P  
 
                                                                             
1999) and http://en.wikipedia.org/wiki/Golomb_coding, 
for similar discussion, though with slightly different 
notation.  The primary reference is (Golomb, 1966). 
203
Ideally, we?d like to do as little pruning as poss-
ible and we?d like to use as large a P as possible, 
subject to the memory allocation.  We don?t have a 
principled argument for how to balance Stolcke 
pruning losses with hashing losses; this can be ar-
rived at empirically on an application-specific ba-
sis.  For example, to fix the storage per n-gram at 
around 13 bits: 
13 =  
1
log? 2
+ log2
1
?
  
 
If we solve for ?, we obtain 0000,20/1?? .  In 
other words, set P to a prime near N000,20 and 
then do as much Stolcke pruning as necessary to 
meet the memory constraint.   Then measure your 
application?s accuracy, and adjust accordingly. 
6.2 HashTBO Values and Alphas 
There are N log likelihood values, one for each 
key.  These N values are quantized into a small 
number of distinct bins.  They are written out as a 
sequence of N Huffman codes.  If there are Katz 
backoff alphas, then they are also written out as a 
sequence of N Huffman codes.  (Unigrams and 
bigrams have alphas, but trigrams don?t.) 
6.3 HashTBO Lookup 
The lookup process is given an n-gram, 
???2???1?? , and is asked to estimate a log likelih-
ood, log Pr ??  ???2???1) .  Using the standard 
backoff model, this depends on the likelihoods for 
the unigrams, bigrams and trigrams, as well as the 
alphas. 
The lookup routine not only determines if the n-
gram is in the table, but also determines the offset 
within that table.  Using that offset, we can find the 
appropriate log likelihood and alpha.  Side tables 
are maintained to speed up random access. 
7 ZipTBO Format 
ZipTBO is a well-established representation of 
trigrams.  Detailed descriptions can be found in 
(Clarkson and Rosenfeld 1997; Whittaker and Raj 
2001). 
ZipTBO consumes 8 bytes per unigram, 5 bytes 
per bigram and 2.5 bytes per trigram.  In practice, 
this comes to about 4 bytes per n-gram on average. 
Note that there are some important interactions 
between ZipTBO and Stolcke pruning.  ZipTBO is 
relatively efficient for trigrams, compared to bi-
grams.   Unfortunately, aggressive Stolcke pruning 
generates bigram-heavy models, which don?t com-
press well with ZipTBO. 
 
 
probs 
&
weights
bounds
BIGRAM
ids
probs 
& 
weights
W[i-2]w[i-1]
W[i-2]w[i-1]w[i]
ids probs
bounds
2 1/2
TRIGRAM
UNIGRAM
ids
2 1 2
2 2 4
 
Figure 1.  Tree structure of n-grams in ZipTBO 
format, following Whittaker and Ray (2001) 
 
7.1 ZipTBO Keys 
The tree structure of the trigram model is im-
plemented using three arrays. As shown in Figure 
1, from left to right, the first array (called unigram 
array) stores unigram nodes, each of which 
branches out into bigram nodes in the second array 
(bigram array).  Each bigram node then branches 
out into trigram nodes in the third array (trigram 
array).  
The length of the unigram array is determined 
by the vocabulary size (V).  The lengths of the oth-
er two arrays depend on the number of bigrams 
and the number of trigrams, which depends on how 
aggressively they were pruned.  (We do not prune 
unigrams.) 
We store a 2-byte word id for each unigram, bi-
gram and trigram. 
The unigram nodes point to blocks of bigram 
nodes, and the bigram nodes point to blocks of tri-
gram nodes.  There are boundary symbols between 
blocks (denoted by the pointers in Figure 1).   The 
boundary symbols consume 4 bytes for each uni-
gram and 2 bytes for each bigram. 
In each block, nodes are sorted by their word 
ids. Blocks are consecutive, so the boundary value 
204
of an n?1-gram node together with the boundary 
value of its previous n?1-gram node specifies, in 
the n-gram array, the location of the block contain-
ing all its child nodes. To locate a particular child 
node, a binary search of word ids is performed 
within the block. 
 
Figure 3.  The differences between the methods in 
Figure 2 vanish if we adjust for prune size. 
7.2 ZipTBO Values and Alphas 
Like HashTBO, the log likelihood values and 
backoff alphas are quantized to a small number of 
quantization levels (256 levels for unigrams and 16 
levels for bigrams and trigrams).   Unigrams use a 
full byte for the log likelihoods, plus another full 
byte for the alphas.  Bigrams use a half byte for the 
log likelihood, plus another half byte for the al-
phas.  Trigrams use a half byte for the log likelih-
ood.  (There are no alphas for trigrams.) 
7.3 ZipTBO Bottom Line 
1. 8 bytes for each unigram:  
a. 2 bytes for a word id + 
b. 4 bytes for two boundary symbols +  
c. 1 byte for a log likelihood +  
d. 1 byte for an alpha 
2. 5 bytes for each bigram:  
a. 2 bytes for a word id +  
b. 2 bytes for a boundary symbol +  
c. ? bytes for a log likelihood + 
d. ? bytes for an alpha 
3. 2.5 bytes for each trigram: 
a. 2 bytes for a word id + 
b. ? bytes for a log likelihood 
8 Evaluation 
We normally think of trigram language models 
as memory hogs, but Figure 2 shows that trigrams 
can be squeezed down to a megabyte in a pinch.  
Of course, more memory is always better, but it is 
surprising how much can be done (27% recall at 
80% precision) with so little memory. 
Given a fixed memory budget, HashTBO out-
performs ZipTBO which outperforms StdTBO, a 
baseline system with no compression.  Compres-
sion matters more when memory is tight.  The gap 
between methods is more noticeable at the low end 
(under 10 megabytes) and less noticeable at the 
high end (over 100 megabytes), where both me-
thods asymptote to the performance of the StdTBO 
baseline. 
All methods start with Stolcke pruning.   Figure 
3 shows that the losses are largely due to pruning.  
0.25
0.35
0.45
0.55
1 10 100 1000
R
e
ca
ll 
at
 8
0
%
 P
re
ci
si
o
n
Prune Size (MBs)
HashTBO ZipTBO StdTBO
 
Figure 2. When there is plenty of memory, per-
formance (recall @ 80% precision) asymptotes to 
the performance of baseline system with no com-
pression (StdTBO).   When memory is tight, 
HashTBO >> ZipTBO >> StdTBO. 
 
 
 
Figure 4. On average, HashTBO consumes about 
3 bytes per n-gram, whereas ZipTBO consumes 4. 
0.25
0.35
0.45
0.55
1 10 100
R
e
ca
ll 
at
 8
0
%
 P
re
ci
si
o
n
Memory (MBs)
HashTBO ZipTBO StdTBO
y = 3E-06x - 0.0519
y = 4E-06x + 1.5112
0
2
4
6
8
10
12
14
0 5
0
0
,
0
0
0
1
,
0
0
0
,
0
0
0
1
,
5
0
0
,
0
0
0
2
,
0
0
0
,
0
0
0
2
,
5
0
0
,
0
0
0
3
,
0
0
0
,
0
0
0
3
,
5
0
0
,
0
0
0
4
,
0
0
0
,
0
0
0
4
,
5
0
0
,
0
0
0
Ngrams
M
e
g
a
b
y
t
e
s
HashTBO ZipTBO
205
All three methods perform about equally well, as-
suming the same amount of pruning.   
The difference is that HashTBO can store more 
n-grams in the same memory and therefore it 
doesn?t have to do as much pruning.  Figure 4 
shows that HashTBO consumes 3 bytes per n-gram 
whereas ZipTBO consumes 4. 
Figure 4 combines unigrams, bigrams and tri-
grams into a single n-gram variable.  Figure 5 drills 
down into this variable, distinguishing bigrams 
from trigrams.  The axes here have been reversed 
so we can see that HashTBO can store more of 
both kinds in less space.  Note that both HashTBO 
lines are above both ZipTBO lines.   
 
Figure 5. HashTBO stores more bigrams and tri-
grams than ZipTBO in less space. 
 
In addition, note that both bigram lines are 
above both trigram lines (triangles).  Aggressively 
pruned models have more bigrams than trigrams!   
Linear regression on this data shows that Hash-
TBO is no better than ZipTBO on trigrams (with 
the particular settings that we used), but there is a 
big difference on bigrams.  The regressions below 
model M (memory in bytes) as a function of bi and 
tri, the number of bigrams and trigrams, respec-
tively.  (Unigrams are modeled as part of the inter-
cept since all models have the same number of un-
igrams.) 
 
???????? = 0.8 + 3.4?? + 2.6??? 
??????? = 2.6 + 4.9?? + 2.6??? 
 
As a sanity check, it is reassuring that ZipTBO?s 
coefficients of 4.9 and 2.6 are close to the true val-
ues of 5 bytes per bigram and 2.5 bytes per tri-
gram, as reported in Section 7.3. 
According to the regression, HashTBO is no 
better than ZipTBO for trigrams.  Both models use 
roughly 2.6 bytes per trigram.  When trigram mod-
els have relatively few trigrams, the other coeffi-
cients matter.  HashTBO uses less space for bi-
grams (3.4 bytes/bigram << 4.9 bytes/bigram) and 
it has a better intercept (0.8 << 2.6). 
We recommend HashTBO if space is so tight 
that it dominates other concerns.  However, if there 
is plenty of space, or time is an issue, then the tra-
deoffs work out differently.   Figure 6 shows that 
ZipTBO is an order of magnitude faster than 
HashTBO.  The times are reported in microseconds 
per n-gram lookup on a dual Xeon PC with a 3.6 
ghz clock and plenty of RAM (4GB).  These times 
were averaged over a test set of 4 million lookups.  
The test process uses a cache.  Turning off the 
cache increases the difference in lookup times. 
 
Figure 6. HashTBO is slower than ZipTBO. 
9 Conclusion 
Trigram language models were compressed 
using HashTBO, a Golomb coding method 
inspired by McIlroy?s original spell program for 
Unix.  McIlroy used the method to compress a 
dictionary of 32,000 words into a PDP-11 address 
space of 64k bytes.  That is just 2 bytes per word! 
We started with a large corpus of 6 billion words 
of English.  With HashTBO, we could compress 
the trigram language model into just a couple of 
megabytes using about 3 bytes per n-gram 
(compared to 4 bytes per n-gram for the ZipTBO 
baseline).  The proposed HashTBO method is not 
fast, and it is not accurate (not lossless), but it is 
hard to beat if space is tight, which was the case 
for the contextual speller in Microsoft Office 2007. 
  
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
0 5 10 15
Memory (MB)
N
g
r
a
m
s
 
(
M
i
l
l
i
o
n
s
)
HashTBO Bigrams HashTBO Trigrams
ZipTBO Bigrams ZipTBO Trigrams
0
1
2
3
4
5
6
7
0 5 10 15
Memory (MB)
T
i
m
e
HashTBO ZipTBO
206
Acknowledgments 
We would like to thank Dong-Hui Zhang for his 
contributions to ZipTBO. 
References 
Ashok K. Chandra, Dexter C. Kozen, and Larry 
J.Stockmeyer. 1981 Alternation. Journal of the Asso-
ciation for Computing Machinery, 28(1):114-133.  
Church, K., and Gale, W. 1991 Probability Scoring for 
Spelling Correction, Statistics and Computing. 
Clarkson, P. and Robinson, T. 2001 Improved language 
modeling through better language model evaluation 
measures, Computer Speech and  Language, 15:39-
53, 2001. 
Dan Gusfield. 1997 Algorithms on Strings, Trees and 
Sequences. Cambridge University Press, Cambridge, 
UK 
Gao, J. and Zhang, M., 2002 Improving language model 
size reduction using better pruning criteria. ACL 
2002: 176-182. 
Gao, J., Goodman, J., and Miao, J. 2001 The use of 
clustering techniques for language modeling ? appli-
cation to Asian languages. Computational Linguis-
tics and Chinese Language Processing, 6:1, pp 27-
60. 
Golding, A. R. and Schabes, Y. 1996 Combining Tri-
gram-based and feature-based methods for context-
sensitive spelling correction,  ACL, pp. 71-78. 
Golomb, S.W. 1966 Run-length encodings IEEE Trans-
actions on Information Theory, 12:3, pp. 399-40. 
Goodman, J. and Gao, J. 2000 Language model size 
reduction by pruning and clustering, ICSLP-2000, 
International Conference on Spoken Language 
Processing, Beijing, October 16-20, 2000. 
Mays, E., Damerau, F. J., and Mercer, R. L. 1991 Con-
text based spelling correction. Inf. Process. Manage. 
27, 5 (Sep. 1991), pp. 517-522.  
Katz, Slava, 1987 Estimation of probabilities from 
sparse data for other language component of a 
speech recognizer. IEEE transactions on Acoustics, 
Speech and Signal Processing,  35:3, pp. 400-401. 
Kukich, Karen, 1992 Techniques for automatically cor-
recting words in text, Computing Surveys, 24:4, pp. 
377-439. 
M. D. McIlroy, 1982 Development of a spelling list, 
IEEE Trans. on Communications 30 pp. 91-99.  
Seymore, K., and Rosenfeld, R. 1996 Scalable backoff 
language models. Proc. ICSLP, Vol. 1, pp.232-235. 
Stolcke, A. 1998 Entropy-based Pruning of Backoff Lan-
guage Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270--274, Lans-
downe, VA. 
Whittaker, E. and Ray, B. 2001 Quantization-based lan-
guage model compression. Proc. Eurospeech, pp. 
33-36. 
Witten, I. H., Moffat, A., and Bell, T. C. 1999 Manag-
ing Gigabytes (2nd Ed.): Compressing and Indexing 
Documents and Images. Morgan Kaufmann Publish-
ers Inc. 
207
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 98?107,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Indirect-HMM-based Hypothesis Alignment for Combining Outputs 
from Machine Translation Systems 
 
Xiaodong He?, Mei Yang? *, Jianfeng Gao?, Patrick Nguyen?, and Robert Moore? 
 
?Microsoft Research ?Dept. of Electrical Engineering 
One Microsoft Way University of Washington 
Redmond, WA 98052 USA Seattle, WA 98195, USA 
{xiaohe,jfgao, panguyen, 
bobmoore}@microsoft.com 
yangmei@u.washington.edu 
 
 
Abstract 
This paper presents a new hypothesis alignment method 
for combining outputs of multiple machine translation 
(MT) systems. An indirect hidden Markov model 
(IHMM) is proposed to address the synonym matching 
and word ordering issues in hypothesis alignment.  
Unlike traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly from a 
variety of sources including word semantic similarity, 
word surface similarity, and a distance-based distortion 
penalty. The IHMM-based method significantly 
outperforms the state-of-the-art TER-based alignment 
model in our experiments on NIST benchmark 
datasets.  Our combined SMT system using the 
proposed method achieved the best Chinese-to-English 
translation result in the constrained training track of the 
2008 NIST Open MT Evaluation. 
1 Introduction* 
System combination has been applied successfully 
to various machine translation tasks. Recently, 
confusion-network-based system combination 
algorithms have been developed to combine 
outputs of multiple machine translation (MT) 
systems to form a consensus output (Bangalore, et 
al. 2001, Matusov et al, 2006, Rosti et al, 2007, 
Sim et al, 2007). A confusion network comprises a 
sequence of sets of alternative words, possibly 
including null?s, with associated scores. The 
consensus output is then derived by selecting one 
word from each set of alternatives, to produce the 
sequence with the best overall score, which could 
be assigned in various ways such as by voting, by 
                                                          
* Mei Yang performed this work when she was an intern with 
Microsoft Research. 
using posterior probability estimates, or by using a 
combination of these measures and other features. 
Constructing a confusion network requires 
choosing one of the hypotheses as the backbone 
(also called ?skeleton? in the literature), and other 
hypotheses are aligned to it at the word level. High 
quality hypothesis alignment is crucial to the 
performance of the resulting system combination. 
However, there are two challenging issues that 
make MT hypothesis alignment difficult. First, 
different hypotheses may use different 
synonymous words to express the same meaning, 
and these synonyms need to be aligned to each 
other. Second, correct translations may have 
different word orderings in different hypotheses 
and these words need to be properly reordered in 
hypothesis alignment.  
In this paper, we propose an indirect hidden 
Markov model (IHMM) for MT hypothesis 
alignment. The HMM provides a way to model 
both synonym matching and word ordering. Unlike 
traditional HMMs whose parameters are trained 
via maximum likelihood estimation (MLE), the 
parameters of the IHMM are estimated indirectly 
from a variety of sources including word semantic 
similarity, word surface similarity, and a distance-
based distortion penalty, without using large 
amount of training data. Our combined SMT 
system using the proposed method gave the best 
result on the Chinese-to-English test in the 
constrained training track of the 2008 NIST Open 
MT Evaluation (MT08). 
2 Confusion-network-based MT system 
combination 
The current state-of-the-art is confusion-network-
based MT system combination as described by 
98
 Rosti and colleagues (Rosti et al, 2007a, Rosti et 
al., 2007b). The major steps are illustrated in 
Figure 1. In Fig. 1 (a), hypotheses from different 
MT systems are first collected. Then in Fig. 1 (b), 
one of the hypotheses is selected as the backbone 
for hypothesis alignment. This is usually done by a 
sentence-level minimum Bayes risk (MBR) 
method which selects a hypothesis that has the 
minimum average distance compared to all 
hypotheses. The backbone determines the word 
order of the combined output. Then as illustrated in 
Fig. 1 (c), all other hypotheses are aligned to the 
backbone. Note that in Fig. 1 (c) the symbol ? 
denotes a null word, which is inserted by the 
alignment normalization algorithm described in 
section 3.4. Fig. 1 (c) also illustrates the handling 
of synonym alignment (e.g., aligning ?car? to 
?sedan?), and word re-ordering of the hypothesis. 
Then in Fig. 1 (d), a confusion network is 
constructed based on the aligned hypotheses, 
which consists of a sequence of sets in which each 
word is aligned to a list of alternative words 
(including null) in the same set. Then, a set of 
global and local features are used to decode the 
confusion network.  
  
E1 he have good car 
argmin ( , )B E EE TER E E?? ? ?? ?E E
 
E2 he has nice sedan 
E3 it a nice car        e.g., EB = E1 E4 a sedan he has 
(a)  hypothesis set                    (b) backbone selection 
 
EB he have ? good car      he  have   ?   good   car 
       he   has    ?   nice    sedan 
       it     ?       a   nice    car   
E4 a  ?  sedan  he   has      he   has    a     ?       sedan 
(c)  hypothesis alignment        (d) confusion network 
 
Figure 1: Confusion-network-based MT system 
combination.  
3 Indirect-HMM-based Hypothesis 
Alignment  
In confusion-network-based system combination 
for SMT, a major difficulty is aligning hypotheses 
to the backbone. One possible statistical model for 
word alignment is the HMM, which has been 
widely used for bilingual word alignment (Vogel et 
al., 1996, Och and Ney, 2003). In this paper, we 
propose an indirect-HMM method for monolingual 
hypothesis alignment. 
 
3.1 IHMM for hypothesis alignment  
 
Let 
1 1( ,..., )I Ie e e? denote the backbone, 
1 1( ,..., )J Je e e? ? ??  a hypothesis to be aligned to 1Ie , 
and 
1 1( ,..., )J Ja a a?  the alignment that specifies 
the position of the backbone word aligned to each 
hypothesis word. We treat each word in the 
backbone as an HMM state and the words in the 
hypothesis as the observation sequence. We use a 
first-order HMM, assuming that the emission 
probability 
( | )jj ap e e?
 depends only on the 
backbone word, and the transition probability 
1( | , )j jp a a I?
 depends only on the position of the 
last state and the length of the backbone. Treating 
the alignment as hidden variable, the conditional 
probability that the hypothesis is generated by the 
backbone is given by  
 
 
1
1 1 1
1
( | ) ( | , ) ( | )jJ
JJ I
j j j a
ja
p e e p a a I p e e?
?
? ?? ?? ? ???
 (1) 
  
As in HMM-based bilingual word alignment 
(Och and Ney, 2003), we also associate a null with 
each backbone word to allow generating 
hypothesis words that do not align to any backbone 
word.  
In HMM-based hypothesis alignment, emission 
probabilities model the similarity between a 
backbone word and a hypothesis word, and will be 
referred to as the similarity model. The transition 
probabilities model word reordering, and will be 
called the distortion model. 
 
3.2 Estimation of the similarity model 
 
The similarity model, which specifies the emission 
probabilities of the HMM, models the similarity 
between a backbone word and a hypothesis word. 
Since both words are in the same language, the 
similarity model can be derived based on both 
semantic similarity and surface similarity, and the 
overall similarity model is a linear interpolation of 
the two: 
 
( | ) ( | ) (1 ) ( | )j i sem j i sur j ip e e p e e p e e? ?? ? ?? ? ? ? ?  (2) 
 
99
 where ( | )sem j ip e e?
 and ( | )sur j ip e e?
 reflect the 
semantic and surface similarity between 
je?
 and  
ie , respectively, and ? is the interpolation factor. 
Since the semantic similarity between two 
target words is source-dependent, the semantic 
similarity model is derived by using the source 
word sequence as a hidden layer: 
 
0
( | )
( | ) ( | , )
sem j i
K
k i j k i
k
p e e
p f e p e f e
?
?
???
 
0
( | ) ( | )K k i j k
k
p f e p e f
?
???     (3) 
 
where 
1 1( ,..., )K Kf f f?  is the source sentence. 
Moreover, in order to handle the case that two 
target words are synonyms but neither of them has 
counter-part in the source sentence, a null is 
introduced on the source side, which is represented 
by f0. The last step in (3) assumes that first ei 
generates all source words including null. Then ej? 
is generated by all source words including null.  
In the common SMT scenario where a large 
amount of bilingual parallel data is available, we 
can estimate the translation probabilities from a 
source word to a target word and vice versa via 
conventional bilingual word alignment. Then both 
( | )k ip f e  and ( | )j kp e f?
 in (3) can be derived:  
 
2( | ) ( | )j k s t j kp e f p e f? ??
 
 
where 
2 ( | )s t j kp e f?
 is the translation model from 
the source-to-target word alignment model, and 
( | )k ip f e  , which enforces the sum-to-1 constraint 
over all words in the source sentence, takes the 
following form, 
 
2
2
0
( | )( | )
( | )
t s k i
k i K
t s k i
k
p f ep f e
p f e
?
?
?
 
 
where 
2 ( | )t s k ip f e  is the translation model from 
the  target-to-source word alignment model. In our 
method, 
2 ( | )t s ip null e  for all target words is 
simply a constant pnull, whose value is optimized 
on held-out data 1.  
The surface similarity model can be estimated 
in several ways. A very simple model could be 
based on exact match: the surface similarity model, 
( | )sur j ip e e?
, would take the value 1.0 if e?= e, and 
0 otherwise 2 . However, a smoothed surface 
similarity model is used in our method. If the target 
language uses alphabetic orthography, as English 
does, we treat words as letter sequences and the 
similarity measure can be the length of the longest 
matched prefix (LMP) or the length of the longest 
common subsequence (LCS) between them. Then, 
this raw similarity measure is transformed to a 
surface similarity score between 0 and 1 through 
an exponential mapping,  
 
? ?( | ) exp ( , ) 1sur j i j ip e e s e e?? ?? ?? ? ?? ?    (4) 
 
where ( , )j is e e?
 is computed as 
 
( , )( , ) max(| |,| |)
j i
j i
j i
M e es e e e e
?? ? ?
 
 
and ( , )j iM e e?
 is the raw similarity measure of ej? 
ei, which is the length of the LMP or LCS of ej? 
and ei. and ? is a smoothing factor that 
characterizes the mapping, Thus as ? approaches 
infinity, ( | )sur j ip e e?
 backs off to the exact match 
model. We found the smoothed similarity model of 
(4) yields slightly better results than the exact 
match model. Both LMP- and LCS- based methods 
achieve similar performance but the computation 
of LMP is faster. Therefore, we only report results 
of the LMP-based smoothed similarity model.  
 
3.3 Estimation of the distortion model 
 
The distortion model, which specifies the transition 
probabilities of the HMM, models the first-order 
dependencies of word ordering. In bilingual 
HMM-based word alignment, it is commonly 
assumed that transition probabilities 
                                                          
1  The other direction, 
2 ( | )s t ip e null? , is available from the 
source-to-target translation model. 
2 Usually a small back-off value is assigned instead of 0.  
100
 1( | , )? ?? ?j jp a i a i I
 depend only on the jump 
distance (i - i')  (Vogel et al, 1996):  
 
1
( )( | , )
( )
I
l
c i ip i i I
c l i
?
??? ?
???
             (5) 
 
As suggested by Liang et al (2006), we can 
group the distortion parameters {c(d)}, d= i - i', 
into a few buckets. In our implementation, 11 
buckets are used for c(?-4),  c(-3), ... c(0), ..., c(5), 
c(?6). The probability mass for transitions with 
jump distance larger than 6 and less than -4 is 
uniformly divided. By doing this, only a handful of 
c(d) parameters need to be estimated. Although it 
is possible to estimate them using the EM 
algorithm on a small development set, we found 
that a particularly simple model, described below, 
works surprisingly well in our experiments.  
Since both the backbone and the hypothesis are 
in the same language, It seems intuitive that the 
distortion model should favor monotonic 
alignment and only allow non-monotonic 
alignment with a certain penalty. This leads us to 
use a distortion model of the following form, 
where K is a tuning factor optimized on held-out 
data. 
 
? ? ? ?1 1c d d ??? ? ?, d= ?4, ?, 6   (6) 
 
As shown in Fig. 2, the value of distortion score 
peaks at d=1, i.e., the monotonic alignment, and 
decays for non-monotonic alignments depending 
on how far it diverges from the monotonic 
alignment. 
 
Figure 2, the distance-based distortion parameters 
computed according to (6), where K=2. 
 
Following Och and Ney (2003), we use a fixed 
value p0 for the probability of jumping to a null 
state, which can be optimized on held-out data, and 
the overall distortion model becomes 
 
0
0
              if     state( | , ) (1 ) ( | , )  otherwise
p i nullp i i I p p i i I
??? ? ? ?? ???
 
 
3.4 Alignment normalization 
 
Given an HMM, the Viterbi alignment algorithm 
can be applied to find the best alignment between 
the backbone and the hypothesis, 
 
1
1 1
1
? argmax ( | , ) ( | )jJ
JJ
j j j aa j
a p a a I p e e?
?
? ??? ? ??
  (7) 
 
However, the alignment produced by the 
algorithm cannot be used directly to build a 
confusion network. There are two reasons for this. 
First, the alignment produced may contain 1-N 
mappings between the backbone and the 
hypothesis whereas 1-1 mappings are required in 
order to build a confusion network. Second, if 
hypothesis words are aligned to a null in the 
backbone or vice versa, we need to insert actual 
nulls into the right places in the hypothesis and the 
backbone, respectively. Therefore, we need to 
normalize the alignment produced by Viterbi 
search. 
 
EB ? e2  ?2   ?   
   ?    ?      e2        ?     ?      ? 
           e1'    e2'    e3'   e4'    
Eh e1'    e2'    e3'   e4'  
(a) hypothesis words are aligned to the backbone null  
 
EB e1  ?1  e2  ?2  e3  ?3    
   ?    e1     e2        e3      ? 
           e2'    ?      e1'   
Eh e1'    e2'    ?  
(b) a backbone word is aligned to no hypothesis word 
 
Figure 3: illustration of alignment normalization 
 
First, whenever more than one hypothesis 
words are aligned to one backbone word, we keep 
the link which gives the highest occupation 
probability computed via the forward-backward 
algorithm. The other hypothesis words originally 
 -4                     1                      6  
 1.0 
 0.0 
   c(d) 
  d 
101
 aligned to the backbone word will be aligned to the 
null associated with that backbone word. 
Second, for the hypothesis words that are 
aligned to a particular null on the backbone side, a 
set of nulls are inserted around that backbone word 
associated with the null such that no links cross 
each other. As illustrated in Fig. 3 (a), if a 
hypothesis word e2? is aligned to the backbone 
word e2, a null is inserted in front of the backbone 
word e2 linked to the hypothesis word e1? that 
comes before e2?. Nulls are also inserted for other 
hypothesis words such as e3? and e4? after the 
backbone word e2. If there is no hypothesis word 
aligned to that backbone word, all nulls are 
inserted after that backbone word .3 
For a backbone word that is aligned to no 
hypothesis word, a null is inserted on the 
hypothesis side, right after the hypothesis word 
which is aligned to the immediately preceding 
backbone word. An example is shown in Fig. 3 (b). 
4 Related work 
The two main hypothesis alignment methods for 
system combination in the previous literature are 
GIZA++ and TER-based methods. Matusov et al 
(2006) proposed using GIZA++ to align words 
between different MT hypotheses, where all 
hypotheses of the test corpus are collected to create 
hypothesis pairs for GIZA++ training. This 
approach uses the conventional HMM model 
bootstrapped from IBM Model-1 as implemented 
in GIZA++, and heuristically combines results 
from aligning in both directions. System 
combination based on this approach gives an 
improvement over the best single system. 
However, the number of hypothesis pairs for 
training is limited by the size of the test corpus. 
Also, MT hypotheses from the same source 
sentence are correlated with each other and these 
hypothesis pairs are not i.i.d. data samples. 
Therefore, GIZA++ training on such a data set may 
be unreliable.  
Bangalore et al (2001) used a multiple string-
matching algorithm based on Levenshtein edit 
distance, and later Sim et al (2007) and Rosti et al 
(2007) extended it to a TER-based method for 
hypothesis alignment. TER (Snover et al, 2006) 
                                                          
3  This only happens if no hypothesis word is aligned to a 
backbone word but some hypothesis words are aligned to the 
null associated with that backbone word. 
measures the minimum number of edits, including 
substitution, insertion, deletion, and shift of blocks 
of words, that are needed to modify a hypothesis so 
that it exactly matches the other hypothesis. The 
best alignment is the one that gives the minimum 
number of translation edits. TER-based confusion 
network construction and system combination has 
demonstrated superior performance on various 
large-scale MT tasks (Rosti. et al 2007). However, 
when searching for the optimal alignment, the 
TER-based method uses a strict surface hard match 
for counting edits. Therefore, it is not able to 
handle synonym matching well. Moreover, 
although TER-based alignment allows phrase 
shifts to accommodate the non-monotonic word 
ordering, all non-monotonic shifts are penalized 
equally no matter how short or how long the move 
is, and this penalty is set to be the same as that for 
substitution, deletion, and insertion edits. 
Therefore, its modeling of non-monotonic word 
ordering is very coarse-grained.  
In contrast to the GIZA++-based method, our 
IHMM-based method has a similarity model 
estimated using bilingual word alignment HMMs 
that are trained on a large amount of bi-text data. 
Moreover, the surface similarity information is 
explicitly incorporated in our model, while it is 
only used implicitly via parameter initialization for 
IBM Model-1 training by Matusov et al (2006). 
On the other hand, the TER-based alignment 
model is similar to a coarse-grained, non-
normalized version of our IHMM, in which the 
similarity model assigns no penalty to an exact 
surface match and a fixed penalty to all 
substitutions, insertions, and deletions, and the 
distortion model simply assigns no penalty to a 
monotonic jump, and a fixed penalty to all other 
jumps, equal to the non-exact-match penalty in the 
similarity model. 
There have been other hypothesis alignment 
methods. Karakos, et al (2008) proposed an ITG-
based method for hypothesis alignment, Rosti et al 
(2008) proposed an incremental alignment method, 
and a heuristic-based matching algorithm was 
proposed by Jayaraman and Lavie (2005).  
5 Evaluation 
In this section, we evaluate our IHMM-based 
hypothesis alignment method on the Chinese-to-
English (C2E) test in the constrained training track 
102
 of the 2008 NIST Open MT Evaluation (NIST, 
2008). We compare to the TER-based method used 
by Rosti et al (2007). In the following 
experiments, the NIST BLEU score is used as the 
evaluation metric (Papineni et al, 2002), which is 
reported as a percentage in the following sections.  
 
5.1 Implementation details 
 
In our implementation, the backbone is selected 
with MBR. Only the top hypothesis from each 
single system is considered as a backbone. A 
uniform posteriori probability is assigned to all 
hypotheses. TER is used as loss function in the 
MBR computation.  
Similar to (Rosti et al, 2007), each word in the 
confusion network is associated with a word 
posterior probability. Given a system S, each of its 
hypotheses is assigned with a rank-based score of 
1/(1+r)?, where r is the rank of the hypothesis, and 
? is a rank smoothing parameter. The system 
specific rank-based score of a word w for a given 
system S is the sum of all the rank-based scores of 
the hypotheses in system S that contain the word w 
at the given position (after hypothesis alignment). 
This score is then normalized by the sum of the 
scores of all the alternative words at the same 
position and from the same system S to generate 
the system specific word posterior. Then, the total 
word posterior of w over all systems is a sum of 
these system specific posteriors weighted by 
system weights. 
Beside the word posteriors, we use language 
model scores and a word count as features for 
confusion network decoding. 
Therefore, for an M-way system combination 
that uses N LMs, a total of M+N+1 decoding 
parameters, including M-1 system weights, one 
rank smoothing factor, N language model weights, 
and one weight for the word count feature, are 
optimized using Powell?s method (Brent, 1973) to 
maximize BLEU score on a development set4 . 
Two language models are used in our 
experiments. One is a trigram model estimated 
from the English side of the parallel training data, 
and the other is a 5-gram model trained on the 
English GigaWord corpus from LDC using the 
MSRLM toolkit (Nguyen et al 2007). 
                                                          
4 The parameters of IHMM are not tuned by maximum-BLEU 
training. 
In order to reduce the fluctuation of BLEU 
scores caused by the inconsistent translation output 
length, an unsupervised length adaptation method 
has been devised. We compute an expected length 
ratio between the MT output and the source 
sentences on the development set after maximum- 
BLEU training. Then during test, we adapt the 
length of the translation output by adjusting the 
weight of the word count feature such that the 
expected output/source length ratio is met. In our 
experiments, we apply length adaptation to the 
system combination output at the level of the 
whole test corpus. 
 
5.2  Development and test data  
 
The development (dev) set used for system 
combination parameter training contains 1002 
sentences sampled from the previous NIST MT 
Chinese-to-English test sets: 35% from MT04, 
55% from MT05, and 10% from MT06-newswire. 
The test set is the MT08 Chinese-to-English 
?current? test set, which includes 1357 sentences 
from both newswire and web-data genres. Both 
dev and test sets have four references per sentence. 
As inputs to the system combination, 10-best 
hypotheses for each source sentence in the dev and 
test sets are collected from each of the eight single 
systems. All outputs on the MT08 test set were 
true-cased before scoring using a log-linear 
conditional Markov model proposed by Toutanova 
et al (2008). However, to save computation effort, 
the results on the dev set are reported in case 
insensitive BLEU (ciBLEU) score instead. 
 
5.3  Experimental results 
 
In our main experiments, outputs from a total of 
eight single MT systems were combined. As listed 
in Table 1, Sys-1 is a tree-to-string system 
proposed by Quirk et al, (2005); Sys-2 is a phrase-
based system with fast pruning proposed by Moore 
and Quirk (2008); Sys-3 is a phrase-based system 
with syntactic source reordering proposed by 
Wang et al (2007a); Sys-4 is a syntax-based pre-
ordering system proposed by Li et. al. (2007); Sys-
5 is a hierarchical system proposed by Chiang 
(2007); Sys-6 is a lexicalized re-ordering system 
proposed by Xiong et al (2006); Sys-7 is a two-
pass phrase-based system with adapted LM 
proposed by Foster and Kuhn (2007); and  Sys-8 is 
103
 a hierarchical system with two-pass rescoring 
using a parser-based LM proposed by Wang et al, 
(2007b). All systems were trained within the 
confines of the constrained training condition of 
NIST MT08 evaluation. These single systems are 
optimized with maximum-BLEU training on 
different subsets of the previous NIST MT test 
data. The bilingual translation models used to 
compute the semantic similarity are from the word-
dependent HMMs proposed by He (2007), which 
are trained on two million parallel sentence-pairs 
selected from the training corpus allowed by the 
constrained training condition of MT08.  
 
5.3.1 Comparison with TER alignment 
In the IHMM-based method, the smoothing 
factor for surface similarity model is set to ? = 3, 
the interpolation factor of the overall similarity 
model is set to ? = 0.3, and the controlling factor of 
the distance-based distortion parameters is set to 
K=2. These settings are optimized on the dev set. 
Individual system results and system combination 
results using both IHMM and TER alignment, on 
both the dev and test sets, are presented in Table 1. 
The TER-based hypothesis alignment tool used in 
our experiments is the publicly available TER Java 
program, TERCOM (Snover et al, 2006). Default 
settings of TERCOM are used in the following 
experiments. 
On the dev set, the case insensitive BLEU score 
of the IHMM-based 8-way system combination 
output is about 5.8 points higher than that of the 
best single system. Compared to the TER-based 
method, the IHMM-based method is about 1.5 
BLEU points better. On the MT08 test set, the 
IHMM-based system combination gave a case 
sensitive BLEU score of 30.89%. It outperformed 
the best single system by 4.7 BLEU points and the 
TER-based system combination by 1.0 BLEU 
points. Note that the best single system on the dev 
set and the test set are different. The different 
single systems are optimized on different tuning 
sets, so this discrepancy between dev set and test 
set results is presumably due to differing degrees 
of mismatch between the dev and test sets and the 
various tuning sets. 
 
 
 
 
 
Table 1. Results of single and combined systems 
on the dev set and the MT08 test set  
System Dev 
ciBLEU% 
MT08 
BLEU% 
System 1 34.08 21.75 
System 2 33.78 20.42 
System 3 34.75 21.69 
System 4 37.85 25.52 
System 5 37.80 24.57 
System 6 37.28 24.40 
System 7 32.37 25.51 
System 8 34.98 26.24 
TER 42.11 29.89 
IHMM 43.62 30.89 
 
In order to evaluate how well our method 
performs when we combine more systems, we 
collected MT outputs on MT08 from seven 
additional single systems as summarized in Table 
2. These systems belong to two groups. Sys-9 to 
Sys-12 are in the first group. They are syntax-
augmented hierarchical systems similar to those 
described by Shen et al (2008) using different 
Chinese word segmentation and language models. 
The second group has Sys-13 to Sys-15. Sys-13 is 
a phrasal system proposed by Koehn et al (2003), 
Sys-14 is a hierarchical system proposed by 
Chiang (2007), and Sys-15 is a syntax-based 
system proposed by Galley et al (2006). All seven 
systems were trained within the confines of the 
constrained training condition of NIST MT08 
evaluation.  
We collected 10-best MT outputs only on the 
MT08 test set from these seven extra systems. No 
MT outputs on our dev set are available from them 
at present. Therefore, we directly adopt system 
combination parameters trained for the previous 8-
way system combination, except the system 
weights, which are re-set by the following 
heuristics: First, the total system weight mass 1.0 is 
evenly divided among the three groups of single 
systems: {Sys-1~8}, {Sys-9~12}, and {Sys-
13~15}. Each group receives a total system weight 
mass of 1/3. Then the weight mass is further 
divided in each group: in the first group, the 
original weights of systems 1~8 are multiplied by 
1/3; in the second and third groups, the weight 
mass is evenly distributed within the group, i.e., 
1/12 for each system in group 2, and 1/9 for each 
104
 system in group 35.  Length adaptation is applied to 
control the final output length, where the same 
expected length ratio of the previous 8-way system 
combination is adopted. 
The results of the 15-way system combination 
are presented in Table 3. It shows that the IHMM-
based method is still about 1 BLEU point better 
than the TER-based method. Moreover, combining 
15 single systems gives an output that has a NIST 
BLEU score of 34.82%, which is 3.9 points better 
than the best submission to the NIST MT08 
constrained training track (NIST, 2008). To our 
knowledge, this is the best result reported on this 
task. 
 
Table 2. Results of seven additional single systems 
on the NIST MT08 test set 
System MT08 
BLEU% 
System 9 29.59 
System 10 29.57 
System 11 29.64 
System 12 29.85 
System 13 25.53 
System 14 26.04 
System 15 29.70 
 
Table 3. Results of the 15-way system combination 
on the NIST MT08 C2E test set 
Sys. Comb.  MT08 
BLEU% 
TER 33.81 
IHMM 34.82 
 
5.3.2 Effect of the similarity model  
In this section, we evaluate the effect of the 
semantic similarity model and the surface 
similarity model by varying the interpolation 
weight ? of (2). The results on both the dev and 
test sets are reported in Table 4. In one extreme 
case, ? = 1, the overall similarity model is based 
only on semantic similarity. This gives a case 
insensitive BLEU score of 41.70% and a case 
sensitive BLEU score of 28.92% on the dev and 
test set, respectively. The accuracy is significantly 
improved to 43.62% on the dev set and 30.89% on 
test set when ? = 0.3. In another extreme case, ? = 
                                                          
5 This is just a rough guess because no dev set is available. We 
believe a better set of system weights could be obtained if MT 
outputs on a common dev set were available. 
0, in which only the surface similarity model is 
used for the overall similarity model, the 
performance degrades by about 0.2 point. 
Therefore, the surface similarity information seems 
more important for monolingual hypothesis 
alignment, but both sub-models are useful.  
 
Table 4. Effect of the similarity model 
 Dev 
ciBLEU% 
Test 
BLEU% 
? = 1.0 41.70 28.92 
? = 0.7 42.86 30.50 
? = 0.5 43.11 30.94 
? = 0.3 43.62 30.89 
? = 0.0 43.35 30.73 
 
5.3.3 Effect of the distortion model  
We investigate the effect of the distance-based 
distortion model by varying the controlling factor 
K in (6). For example, setting K=1.0 gives a linear-
decay distortion model, and setting K=2.0 gives a 
quadratic smoothed distance-based distortion 
model. As shown in Table 5, the optimal result can 
be achieved using a properly smoothed distance-
based distortion model. 
 
Table 5. Effect of the distortion model 
 Dev 
ciBLEU% 
Test 
BLEU% 
K=1.0 42.94 30.44 
K=2.0 43.62 30.89 
K=4.0 43.17 30.30 
K=8.0 43.09 30.01 
6 Conclusion 
Synonym matching and word ordering are two 
central issues for hypothesis alignment in 
confusion-network-based MT system combination. 
In this paper, an IHMM-based method is proposed 
for hypothesis alignment. It uses a similarity model 
for synonym matching and a distortion model for 
word ordering. In contrast to previous methods, the 
similarity model explicitly incorporates both 
semantic and surface word similarity, which is 
critical to monolingual word alignment, and a 
smoothed distance-based distortion model is used 
to model the first-order dependency of word 
ordering, which is shown to be better than simpler 
approaches. 
105
 Our experimental results show that the IHMM-
based hypothesis alignment method gave superior 
results on the NIST MT08 C2E test set compared 
to the TER-based method. Moreover, we show that 
our system combination method can scale up to 
combining more systems and produce a better 
output that has a case sensitive BLEU score of 
34.82, which is 3.9 BLEU points better than the 
best official submission of MT08.  
Acknowledgement 
The authors are grateful to Chris Quirk, Arul 
Menezes, Kristina Toutanova, William Dolan, Mu 
Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, 
Ming Zhou, George Foster, Roland Kuhn, Jing 
Zheng, Wen Wang, Necip Fazil Ayan, Dimitra 
Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin 
Knight, Jens-Soenke Voeckler, Spyros Matsoukas, 
and Antti-Veikko Rosti for assistance with the MT 
systems and/or for the valuable suggestions and 
discussions.  
 
References  
Srinivas Bangalore, German Bordel, and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In Proc. 
of IEEE ASRU, pp. 351?354. 
Richard Brent, 1973. Algorithms for Minimization 
without Derivatives. Prentice-Hall, Chapter 7. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201?
228. 
George Foster and Roland Kuhn. 2007. Mixture-Model 
Adaptation for SMT. In Proc. of the Second ACL 
Workshop on Statistical Machine Translation. pp. 
128 ? 136. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve DeNeefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable Inference and Training of 
Context-Rich Syntactic Translation Models. In Proc. 
of COLING-ACL, pp. 961?968. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. In Proc. of the 
Second ACL Workshop on Statistical Machine 
Translation. 
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word 
matching. In Proc. of EAMT. pp. 143 ? 152. 
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, 
and Markus Dreyer. 2008. Machine Translation 
System Combination using ITG-based Alignments. 
In Proc. of ACL-HLT, pp. 81?84. 
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming 
Zhou, Yi Guan. 2007. A Probabilistic Approach to 
Syntax-based Reordering for Statistical Machine 
Translation. In Proc. of ACL. pp. 720 ? 727. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by Agreement. In Proc. of NAACL. pp 
104 ? 111.  
Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 
2006. Computing consensus translation from 
multiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL, pp. 33?40. 
Robert Moore and Chris Quirk. 2007. Faster Beam-
Search Decoding for Phrasal Statistical Machine 
Translation. In Proc. of MT Summit XI. 
Patrick Nguyen, Jianfeng Gao and Milind Mahajan. 
2007. MSRLM: a scalable language modeling 
toolkit. Microsoft Research Technical Report MSR-
TR-2007-144. 
NIST. 2008. The 2008 NIST Open Machine Translation 
Evaluation. www.nist.gov/speech/tests/mt/2008/doc/  
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of ACL, 
pp. 311?318. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase based translation. In Proc. of 
NAACL. pp. 48 ? 54. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proc. of ACL. pp. 271?
279. 
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, 
Richard Schwartz, Necip Fazil Ayan, and Bonnie J. 
Dorr. 2007a. Combining outputs from multiple 
machine translation systems. In Proc. of NAACL-
HLT, pp. 228?235. 
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard 
Schwartz. 2007b. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL, pp. 312?319. 
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental Hypothesis 
Alignment for Building Confusion Networks with 
Application to Machine Translation System 
Combination, In Proc. of the Third ACL Workshop 
on Statistical Machine Translation, pp. 183?186. 
Libin Shen, Jinxi Xu, Ralph Weischedel. 2008. A New 
String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. In Proc. of ACL-HLT, pp. 577?585. 
106
 Khe Chai Sim, William J. Byrne, Mark J.F. Gales, 
Hichem Sahbi, and Phil C. Woodland. 2007. 
Consensus network decoding for statistical machine 
translation system combination. In Proc. of ICASSP, 
vol. 4. pp. 105?108. 
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea 
Micciulla, and John Makhoul. 2006. A study of 
translation edit rate with targeted human annotation. 
In Proc. of AMTA. 
Kristina Toutanova, Hisami Suzuki and Achim Ruopp. 
2008. Applying Morphology Generation Models to 
Machine Translation. In Proc. of ACL. pp. 514 ? 522. 
Stephan Vogel, Hermann Ney, and Christoph Tillmann. 
1996. HMM-based Word Alignment In Statistical 
Translation. In Proc. of COLING. pp. 836-841. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007a. Chinese Syntactic Reordering for Statistical 
Machine Translation.  In Proc. of EMNLP-CoNLL. 
pp. 737-745. 
Wen Wang, Andreas Stolcke, Jing Zheng. 2007b. 
Reranking Machine Translation Hypotheses With 
Structured and Web-based Language Models. In 
Proc. of IEEE ASRU. pp. 159 ? 164. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In Proc. of ACL. 
pp. 521 ? 528. 
107
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344?352,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A comparison of Bayesian estimators for
unsupervised Hidden Markov Model POS taggers
Jianfeng Gao
Microsoft Research
Redmond, WA, USA
jfgao@microsoft.com
Mark Johnson
Brown Univeristy
Providence, RI, USA
Mark?Johnson@Brown.edu
Abstract
There is growing interest in applying Bayesian
techniques to NLP problems. There are a
number of different estimators for Bayesian
models, and it is useful to know what kinds of
tasks each does well on. This paper compares
a variety of different Bayesian estimators for
Hidden Markov Model POS taggers with var-
ious numbers of hidden states on data sets of
different sizes. Recent papers have given con-
tradictory results when comparing Bayesian
estimators to Expectation Maximization (EM)
for unsupervised HMM POS tagging, and we
show that the difference in reported results is
largely due to differences in the size of the
training data and the number of states in the
HMM. We invesigate a variety of samplers for
HMMs, including some that these earlier pa-
pers did not study. We find that all of Gibbs
samplers do well with small data sets and few
states, and that Variational Bayes does well
on large data sets and is competitive with the
Gibbs samplers. In terms of times of conver-
gence, we find that Variational Bayes was the
fastest of all the estimators, especially on large
data sets, and that explicit Gibbs sampler (both
pointwise and sentence-blocked) were gener-
ally faster than their collapsed counterparts on
large data sets.
1 Introduction
Probabilistic models now play a central role in com-
putational linguistics. These models define a prob-
ability distribution P(x) over structures or analyses
x. For example, in the part-of-speech (POS) tag-
ging application described in this paper, which in-
volves predicting the part-of-speech tag ti of each
word wi in the sentence w = (w1, . . . , wn), the
structure x = (w, t) consists of the words w in a
sentence together with their corresponding parts-of-
speech t = (t1, . . . , tn).
In general the probabilistic models used in com-
putational linguistics have adjustable parameters ?
which determine the distribution P(x | ?). In this
paper we focus on bitag Hidden Markov Models
(HMMs). Since our goal here is to compare algo-
rithms rather than achieve the best performance, we
keep the models simple by ignoring morphology and
capitalization (two very strong cues in English) and
treat each word as an atomic entity. This means that
the model parameters ? consist of the HMM state-
to-state transition probabilities and the state-to-word
emission probabilities.
In virtually all statistical approaches the parame-
ters ? are chosen or estimated on the basis of training
data d. This paper studies unsupervised estimation,
so d = w = (w1, . . . , wn) consists of a sequence
of words wi containing all of the words of training
corpus appended into a single string, as explained
below.
Maximum Likelihood (ML) is the most common
estimation method in computational linguistics. A
Maximum Likelihood estimator sets the parameters
to the value ?? that makes the likelihood Ld of the
data d as large as possible:
Ld(?) = P(d | ?)
?? = argmax
?
Ld(?)
In this paper we use the Inside-Outside algo-
rithm, which is a specialized form of Expectation-
344
Maximization, to find HMM parameters which (at
least locally) maximize the likelihood function Ld.
Recently there is increasing interest in Bayesian
methods in computational linguistics, and the pri-
mary goal of this paper is to compare the perfor-
mance of various Bayesian estimators with each
other and with EM.
A Bayesian approach uses Bayes theorem to fac-
torize the posterior distribution P(? | d) into the
likelihood P(d | ?) and the prior P(?).
P(? | d) ? P(d | ?) P(?)
Priors can be useful because they can express pref-
erences for certain types of models. To take an
example from our POS-tagging application, most
words belong to relatively few parts-of-speech (e.g.,
most words belong to a single POS, and while there
are some words which are both nouns and verbs,
very few are prepositions and adjectives as well).
One might express this using a prior which prefers
HMMs in which the state-to-word emissions are
sparse, i.e., each state emits few words. An appro-
priate Dirichlet prior can express this preference.
While it is possible to use Bayesian inference to
find a single model, such as the Maximum A Pos-
teriori or MAP value of ? which maximizes the
posterior P(? | d), this is not necessarily the best
approach (Bishop, 2006; MacKay, 2003). Instead,
rather than commiting to a single value for the pa-
rameters ? many Bayesians often prefer to work
with the full posterior distribution P(? | d), as this
naturally reflects the uncertainty in ??s value.
In all but the simplest models there is no known
closed form for the posterior distribution. However,
the Bayesian literature describes a number of meth-
ods for approximating the posterior P(? | d). Monte
Carlo sampling methods and Variational Bayes are
two kinds of approximate inference methods that
have been applied to Bayesian inference of unsu-
pervised HMM POS taggers (Goldwater and Grif-
fiths, 2007; Johnson, 2007). These methods can also
be used to approximate other distributions that are
important to us, such as the conditional distribution
P(t | w) of POS tags (i.e., HMM hidden states) t
given words w.
This recent literature reports contradictory results
about these Bayesian inference methods. John-
son (2007) compared two Bayesian inference algo-
rithms, Variational Bayes and what we call here a
point-wise collapsed Gibbs sampler, and found that
Variational Bayes produced the best solution, and
that the Gibbs sampler was extremely slow to con-
verge and produced a worse solution than EM. On
the other hand, Goldwater and Griffiths (2007) re-
ported that the same kind of Gibbs sampler produced
much better results than EM on their unsupervised
POS tagging task. One of the primary motivations
for this paper was to understand and resolve the dif-
ference in these results. We replicate the results of
both papers and show that the difference in their re-
sults stems from differences in the sizes of the train-
ing data and numbers of states in their models.
It turns out that the Gibbs sampler used in these
earlier papers is not the only kind of sampler for
HMMs. This paper compares the performance of
four different kinds of Gibbs samplers, Variational
Bayes and Expectation Maximization on unsuper-
vised POS tagging problems of various sizes. Our
goal here is to try to learn how the performance of
these different estimators varies as we change the
number of hidden states in the HMMs and the size
of the training data.
In theory, the Gibbs samplers produce streams
of samples that eventually converge on the true
posterior distribution, while the Variational Bayes
(VB) estimator only produces an approximation to
the posterior. However, as the size of the training
data distribution increases the likelihood function
and therefore the posterior distribution becomes in-
creasingly peaked, so one would expect this varia-
tional approximation to become increasingly accu-
rate. Further the Gibbs samplers used in this paper
should exhibit reduced mobility as the size of train-
ing data increases, so as the size of the training data
increases eventually the Variational Bayes estimator
should prove to be superior.
However the two point-wise Gibbs samplers in-
vestigated here, which resample the label of each
word conditioned on the labels of its neighbours
(amongst other things) only require O(m) steps per
sample (where m is the number of HMM states),
while EM, VB and the sentence-blocked Gibbs sam-
plers require O(m2) steps per sample. Thus for
HMMs with many states it is possible to perform one
or two orders of magnitude more iterations of the
345
point-wise Gibbs samplers in the same run-time as
the other samplers, so it is plausible that they would
yield better results.
2 Inference for HMMs
There are a number of excellent textbook presen-
tations of Hidden Markov Models (Jelinek, 1997;
Manning and Schu?tze, 1999), so we do not present
them in detail here. Conceptually, a Hidden Markov
Model uses a Markov model to generate the se-
quence of states t = (t1, . . . , tn) (which will be in-
terpreted as POS tags), and then generates each word
wi conditioned on the corresponding state ti.
We insert endmarkers at the beginning and end
of the corpus and between sentence boundaries,
and constrain the estimators to associate endmarkers
with a special HMM state that never appears else-
where in the corpus (we ignore these endmarkers
during evaluation). This means that we can formally
treat the training corpus as one long string, yet each
sentence can be processed independently by a first-
order HMM.
In more detail, the HMM is specified by a pair of
multinomials ?t and ?t associated with each state t,
where ?t specifies the distribution over states t? fol-
lowing t and ?t specifies the distribution over words
w given state t.
ti | ti?1 = t ? Multi(?t)
wi | ti = t ? Multi(?t)
(1)
The Bayesian model we consider here puts a fixed
uniform Dirichlet prior on these multinomials. Be-
cause Dirichlets are conjugate to multinomials, this
greatly simplifies inference.
?t | ? ? Dir(?)
?t | ?? ? Dir(??)
A multinomial ? is distributed according to the
Dirichlet distribution Dir(?) iff:
P(? | ?) ?
m
?
j=1
??j?1j
In our experiments we set ? and ?? to the uniform
values (i.e., all components have the same value ? or
??), but it is possible to estimate these as well (Gold-
water and Griffiths, 2007). Informally, ? controls
the sparsity of the state-to-state transition probabil-
ities while ?? controls the sparsity of the state-to-
word emission probabilities. As ?? approaches zero
the prior strongly prefers models in which each state
emits as few words as possible, capturing the intu-
ition that most word types only belong to one POS
mentioned earlier.
2.1 Expectation Maximization
Expectation-Maximization is a procedure that iter-
atively re-estimates the model parameters (?,?),
converging on a local maximum of the likelihood.
Specifically, if the parameter estimate at iteration `
is (?(`),?(`)), then the re-estimated parameters at it-
eration `+ 1 are:
?(`+1)t?|t = E[nt?,t]/E[nt] (2)
?(`+1)w|t = E[n
?
w,t]/E[nt]
where n?w,t is the number of times word w occurs
with state t, nt?,t is the number of times state t? fol-
lows t and nt is the number of occurences of state t;
all expectations are taken with respect to the model
(?(`),?(`)).
The experiments below used the Forward-
Backward algorithm (Jelinek, 1997), which is a dy-
namic programming algorithm for calculating the
likelihood and the expectations in (2) in O(nm2)
time, where n is the number of words in the train-
ing corpus and m is the number of HMM states.
2.2 Variational Bayes
Variational Bayesian inference attempts to find a
function Q(t,?,?) that minimizes an upper bound
(3) to the negative log likelihood.
? log P(w)
= ? log
?
Q(t,?,?)P(w, t,?,?)Q(t, ?, ?) dt d? d?
? ?
?
Q(t,?,?) log P(w, t,?,?)Q(t,?,?) dt d? d? (3)
The upper bound (3) is called the Variational Free
Energy. We make a ?mean-field? assumption that
the posterior can be well approximated by a factor-
ized model Q in which the state sequence t does not
covary with the model parameters ?,?:
P(t,?,? | w) ? Q(t,?,?) = Q1(t)Q2(?,?)
346
P(ti|w, t?i, ?, ??) ?
(n?wi,ti + ??
nti + m???
) (nti,ti?1 + ?
nti?1 + m?
) (nti+1,ti + I(ti?1 = ti = ti+1) + ?
nti + I(ti?1 = ti) + m?
)
Figure 1: The conditional distribution for state ti used in the pointwise collapsed Gibbs sampler, which conditions on
all states t?i except ti (i.e., the counts n do not include ti). Here m? is the size of the vocabulary, m is the number of
HMM states and I(?) is the indicator function (i.e., equal to one if its argument is true and zero otherwise),
The calculus of variations is used to minimize the
KL divergence between the desired posterior distri-
bution and the factorized approximation. It turns
out that if the likelihood and conjugate prior be-
long to exponential families then the optimal Q1 and
Q2 do too, and there is an EM-like iterative pro-
cedure that finds locally-optimal model parameters
(Bishop, 2006).
This procedure is especially attractive for HMM
inference, since it involves only a minor modifica-
tion to the M-step of the Forward-Backward algo-
rithm. MacKay (1997) and Beal (2003) describe
Variational Bayesian (VB) inference for HMMs. In
general, the E-step for VB inference for HMMs is
the same as in EM, while the M-step is as follows:
??(`+1)t?|t = f(E[nt?,t] + ?)/f(E[nt] +m?) (4)
??(`+1)w|t = f(E[n
?
w,t] + ??)/f(E[nt] + m???)
f(v) = exp(?(v))
where m? and m are the number of word types and
states respectively, ? is the digamma function and
the remaining quantities are as in (2). This means
that a single iteration can be performed in O(nm2)
time, just as for the EM algorithm.
2.3 MCMC sampling algorithms
The goal of Markov Chain Monte Carlo (MCMC)
algorithms is to produce a stream of samples from
the posterior distribution P(t | w,?). Besag (2004)
provides a tutorial on MCMC techniques for HMM
inference.
A Gibbs sampler is a simple kind of MCMC
algorithm that is well-suited to sampling high-
dimensional spaces. A Gibbs sampler for P(z)
where z = (z1, . . . , zn) proceeds by sampling and
updating each zi in turn from P(zi | z?i), where
z?i = (z1, . . . , zi?1, zi+1, . . . , zn), i.e., all of the
z except zi (Geman and Geman, 1984; Robert and
Casella, 2004).
We evaluate four different Gibbs samplers in this
paper, which vary along two dimensions. First, the
sampler can either be pointwise or blocked. A point-
wise sampler resamples a single state ti (labeling a
single word wi) at each step, while a blocked sam-
pler resamples the labels for all of the words in a
sentence at a single step using a dynamic program-
ming algorithm based on the Forward-Backward al-
gorithm. (In principle it is possible to use block
sizes other than the sentence, but we did not explore
this here). A pointwise sampler requires O(nm)
time per iteration, while a blocked sampler requires
O(nm2) time per iteration, where m is the number
of HMM states and n is the length of the training
corpus.
Second, the sampler can either be explicit or col-
lapsed. An explicit sampler represents and sam-
ples the HMM parameters ? and ? in addition to
the states t, while in a collapsed sampler the HMM
parameters are integrated out, and only the states t
are sampled. The difference between explicit and
collapsed samplers corresponds exactly to the dif-
ference between the two PCFG sampling algorithms
presented in Johnson et al (2007).
An iteration of the pointwise explicit Gibbs sam-
pler consists of resampling ? and ? given the state-
to-state transition counts n and state-to-word emis-
sion counts n? using (5), and then resampling each
state ti given the corresponding word wi and the
neighboring states ti?1 and ti+1 using (6).
?t | nt,? ? Dir(nt +?)
?t | n?t,?? ? Dir(n?t +??)
(5)
P(ti | wi, t?i,?,?) ? ?ti|ti?1?wi|ti?ti+1|ti (6)
The Dirichlet distributions in (5) are non-uniform;
nt is the vector of state-to-state transition counts in
t leaving state t in the current state vector t, while
347
n?t is the vector of state-to-word emission counts for
state t. See Johnson et al (2007) for a more detailed
explanation, as well as an algorithm for sampling
from the Dirichlet distributions in (5).
The samplers that Goldwater and Griffiths (2007)
and Johnson (2007) describe are pointwise collapsed
Gibbs samplers. Figure 1 gives the sampling distri-
bution for this sampler. As Johnson et al (2007)
explains, samples of the HMM parameters ? and ?
can be obtained using (5) if required.
The blocked Gibbs samplers differ from the point-
wise Gibbs samplers in that they resample the POS
tags for an entire sentence at a time. Besag (2004)
describes the well-known dynamic programming
algorithm (based on the Forward-Backward algo-
rithm) for sampling a state sequence t given the
words w and the transition and emission probabil-
ities ? and ?.
At each iteration the explicit blocked Gibbs sam-
pler resamples ? and ? using (5), just as the explicit
pointwise sampler does. Then it uses the new HMM
parameters to resample the states t for the training
corpus using the algorithm just mentioned. This can
be done in parallel for each sentence in the training
corpus.
The collapsed blocked Gibbs sampler is a
straight-forward application of the Metropolis-
within-Gibbs approach proposed by Johnson et al
(2007) for PCFGs, so we only sketch it here. We
iterate through the sentences of the training data, re-
sampling the states for each sentence conditioned
on the state-to-state transition counts n and state-
to-word emission counts n? for the other sentences
in the corpus. This is done by first computing the
parameters ?? and ?? of a proposal HMM using (7).
??t?|t =
nt?,t + ?
nt + m?
(7)
??w|t =
n?w,t + ??
nt + m??
Then we use the dynamic programming sampler de-
scribed above to produce a proposal state sequence
t? for the words in the sentence. Finally, we use
a Metropolis-Hastings accept-reject step to decide
whether to update the current state sequence for the
sentence with the proposal t?, or whether to keep the
current state sequence. In practice, with all but the
very smallest training corpora the acceptance rate is
very high; the acceptance rate for all of our collapsed
blocked Gibbs samplers was over 99%.
3 Evaluation
The previous section described six different unsu-
pervised estimators for HMMs. In this section
we compare their performance for English part-of-
speech tagging. One of the difficulties in evalu-
ating unsupervised taggers such as these is map-
ping the system?s states to the gold-standard parts-
of-speech. Goldwater and Griffiths (2007) proposed
an information-theoretic measure known as the Vari-
ation of Information (VI) described by Meila? (2003)
as an evaluation of an unsupervised tagging. How-
ever as Goldwater (p.c.) points out, this may not be
an ideal evaluation measure; e.g., a tagger which as-
signs all words the same single part-of-speech tag
does disturbingly well under Variation of Informa-
tion, suggesting that a poor tagger may score well
under VI.
In order to avoid this problem we focus here on
evaluation measures that construct an explicit map-
ping between the gold-standard part-of-speech tags
and the HMM?s states. Perhaps the most straight-
forward approach is to map each HMM state to the
part-of-speech tag it co-occurs with most frequently,
and use this mapping to map each HMM state se-
quence t to a sequence of part-of-speech tags. But as
Clark (2003) observes, this approach has several de-
fects. If a system is permitted to posit an unbounded
number of states (which is not the case here) it can
achieve a perfect score on by assigning each word
token its own unique state.
We can partially address this by cross-validation.
We divide the corpus into two equal parts, and from
the first part we extract a mapping from HMM states
to the parts-of-speech they co-occur with most fre-
quently, and use that mapping to map the states of
the second part of the corpus to parts-of-speech. We
call the accuracy of the resulting tagging the cross-
validation accuracy.
Finally, following Haghighi and Klein (2006) and
Johnson (2007) we can instead insist that at most
one HMM state can be mapped to any part-of-speech
tag. Following these authors, we used a greedy algo-
rithm to associate states with POS tags; the accuracy
of the resulting tagging is called the greedy 1-to-1
348
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165
VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599
GSe,p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811
GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032
GSc,p 0.49910? 0.45028 0.42785 0.43652 0.39182 0.39164
GSc,b 0.49486? 0.46193 0.41162 0.42278 0.38497 0.36793
Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators.
The column heading indicates the size of the corpus and the number of HMM states. In the Gibbs sampler (GS) results
the subscript ?e? indicates that the parameters ? and ? were explicitly sampled while the subscript ?c? indicates that
they were integrated out, and the subscript ?p? indicates pointwise sampling, while ?b? indicates sentence-blocked
sampling. Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but was
still slowly improving.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 0.62115 0.64651 0.44135 0.56215 0.28576 0.46669
VB 0.60484 0.63652 0.48427 0.36458 0.35946 0.36926
GSe,p 0.64190 0.63057 0.53571 0.46986 0.41620 0.37165
GSe,b 0.65953 0.65606 0.57918 0.48975 0.47228 0.37311
GSc,p 0.61391? 0.67414 0.65285 0.65012 0.58153 0.62254
GSc,b 0.60551? 0.65516 0.62167 0.58271 0.55006 0.58728
Figure 3: Average cross-validation accuracy of state sequences produced by HMMs estimated by the various estima-
tors. The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 4.47555 3.86326 6.16499 4.55681 7.72465 5.42815
VB 4.27911 3.44029 5.00509 3.19670 4.80778 3.14557
GSe,p 4.24919 3.53024 4.30457 3.23082 4.24368 3.17076
GSe,b 4.04123 3.46179 4.22590 3.20276 4.29474 3.10609
GSc,p 4.03886? 3.52185 4.21259 3.17586 4.30928 3.18273
GSc,b 4.11272? 3.61516 4.36595 3.23630 4.32096 3.17780
Figure 4: Average Variation of Information between the state sequences produced by HMMs estimated by the various
estimators and the gold tags (smaller is better). The table headings follow those used in Figure 2.
All? 50 All? 17 120K ? 50 120K ? 17 24K ? 50 24K ? 17
EM 558 346 648 351 142 125
VB 473 123 337 24 183 20
GSe,p 2863 382 3709 63 2500 177
GSe,b 3846 286 5169 154 4856 139
GSc,p ? 34325 44864 40088 45285 43208
GSc,b ? 6948 7502 7782 7342 7985
Figure 5: Average number of iterations until the negative logarithm of the posterior probability (or likelihood) changes
by less than 0.5% (smaller is better) per at least 2,000 iterations. No annealing was used.
349
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
?
lo
g
po
st
er
io
r
pr
o
ba
bi
lit
y
50000400003000020000100000
8.1e+06
8.05e+06
8e+06
7.95e+06
7.9e+06
7.85e+06
explicit, pointwise
explicit, blocked
collapsed, pointwise
collapsed,blocked
All data, 50 states, ? = ?? = 0.1
computing time (seconds)
G
re
ed
y
1-
to
-
1
ac
cu
ra
cy
50000400003000020000100000
0.58
0.56
0.54
0.52
0.5
0.48
0.46
0.44
0.42
0.4
Figure 6: Variation in (a) negative log likelihood and (b) 1-to-1 accuracy as a function of running time on a 3GHz
dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states. Each iteration took
approximately 96 sec. for the collapsed blocked sampler, 7.5 sec. for the collapsed pointwise sampler, 25 sec. for the
explicit blocked sampler and 4.4 sec. for the explicit pointwise sampler.
350
accuracy.
The studies presented by Goldwater and Griffiths
(2007) and Johnson (2007) differed in the number of
states that they used. Goldwater and Griffiths (2007)
evaluated against the reduced tag set of 17 tags de-
veloped by Smith and Eisner (2005), while Johnson
(2007) evaluated against the full Penn Treebank tag
set. We ran all our estimators in both conditions here
(thanks to Noah Smith for supplying us with his tag
set).
Also, the studies differed in the size of the corpora
used. The largest corpus that Goldwater and Grif-
fiths (2007) studied contained 96,000 words, while
Johnson (2007) used all of the 1,173,766 words
in the full Penn WSJ treebank. For that reason
we ran all our estimators on corpora containing
24,000 words and 120,000 words as well as the full
treebank.
We ran each estimator with the eight different
combinations of values for the hyperparameters ?
and ?? listed below, which include the optimal
values for the hyperparameters found by Johnson
(2007), and report results for the best combination
for each estimator below 1.
? ??
1 1
1 0.5
0.5 1
0.5 0.5
0.1 0.1
0.1 0.0001
0.0001 0.1
0.0001 0.0001
Further, we ran each setting of each estimator at
least 10 times (from randomly jittered initial start-
ing points) for at least 1,000 iterations, as Johnson
(2007) showed that some estimators require many it-
erations to converge. The results of our experiments
are summarized in Figures 2?5.
1We found that on some data sets the results are sensitive to
the values of the hyperparameters. So, there is a bit uncertainty
in our comparison results because it is possible that the values
we tried were good for one estimator and bad for others. Un-
fortunately, we do not know any efficient way of searching the
optimal hyperparameters in a much wider and more fine-grained
space. We leave it to future work.
4 Conclusion and future work
As might be expected, our evaluation measures dis-
agree somewhat, but the following broad tendancies
seem clear. On small data sets all of the Bayesian
estimators strongly outperform EM (and, to a lesser
extent, VB) with respect to all of our evaluation
measures, confirming the results reported in Gold-
water and Griffiths (2007). This is perhaps not too
surprising, as the Bayesian prior plays a compara-
tively stronger role with a smaller training corpus
(which makes the likelihood term smaller) and the
approximation used by Variational Bayes is likely to
be less accurate on smaller data sets.
But on larger data sets, which Goldwater et aldid
not study, the results are much less clear, and depend
on which evaluation measure is used. Expectation
Maximization does surprisingly well on larger data
sets and is competitive with the Bayesian estimators
at least in terms of cross-validation accuracy, con-
firming the results reported by Johnson (2007).
Variational Bayes converges faster than all of the
other estimators we examined here. We found that
the speed of convergence of our samplers depends
to a large degree upon the values of the hyperparam-
eters ? and ??, with larger values leading to much
faster convergence. This is not surprising, as the ?
and ?? specify how likely the samplers are to con-
sider novel tags, and therefore directly influence the
sampler?s mobility. However, in our experiments the
best results are obtained in most settings with small
values for ? and ??, usually between 0.1 and 0.0001.
In terms of time to convergence, on larger data
sets we found that the blocked samplers were gen-
erally faster than the pointwise samplers, and that
the explicit samplers (which represented and sam-
pled ? and ?) were faster than the collapsed sam-
plers, largely because the time saved in not com-
puting probabilities on the fly overwhelmed the time
spent resampling the parameters.
Of course these experiments only scratch the sur-
face of what is possible. Figure 6 shows that
pointwise-samplers initially converge faster, but are
overtaken later by the blocked samplers. Inspired
by this, one can devise hybrid strategies that inter-
leave blocked and pointwise sampling; these might
perform better than both the blocked and pointwise
samplers described here.
351
References
Matthew J. Beal. 2003. Variational Algorithms for Ap-
proximate Bayesian Inference. Ph.D. thesis, Gatsby
Computational Neuroscience unit, University College
London.
Julian Besag. 2004. An introduction to Markov Chain
Monte Carlo methods. In Mark Johnson, Sanjeev P.
Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-
itors, Mathematical Foundations of Speech and Lan-
guage Processing, pages 247?270. Springer, New
York.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In 10th Conference of the European Chapter of
the Association for Computational Linguistics, pages
59?66. Association for Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
744?751, Prague, Czech Republic, June. Association
for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 320?327, New York
City, USA, June. Association for Computational Lin-
guistics.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
David J.C. MacKay. 1997. Ensemble learning for hidden
Markov models. Technical report, Cavendish Labora-
tory, Cambridge.
David J.C. MacKay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Marina Meila?. 2003. Comparing clusterings by the vari-
ation of information. In Bernhard Scho?lkopf and Man-
fred K. Warmuth, editors, COLT 2003: The Sixteenth
Annual Conference on Learning Theory, volume 2777
of Lecture Notes in Computer Science, pages 173?187.
Springer.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods. Springer.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 354?362, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
352
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 505?513,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Model Adaptation via Model Interpolation and Boosting 
for Web Search Ranking  
Jianfeng Gao*, Qiang Wu*, Chris Burges*, Krysta Svore*, 
Yi Su#, Nazan Khan$, Shalin Shah$, Hongyan Zhou$ 
*Microsoft Research, Redmond, USA  
{jfgao; qiangwu; cburges; ksvore}@microsoft.com 
#Johns Hopkins University, USA 
suy@jhu.edu
  
$Microsoft Bing Search, Redmond, USA 
{nazanka; a-shas; honzhou}@microsoft.com 
Abstract 
This paper explores two classes of model adapta-
tion methods for Web search ranking: Model In-
terpolation and error-driven learning approaches 
based on a boosting algorithm.  The results show 
that model interpolation, though simple, achieves 
the best results on all the open test sets where the 
test data is very different from the training data. 
The tree-based boosting algorithm achieves the 
best performance on most of the closed test sets 
where the test data and the training data are sim-
ilar, but its performance drops significantly on 
the open test sets due to the instability of trees.  
Several methods are explored to improve the 
robustness of the algorithm, with limited success. 
1 Introduction 
We consider the task of ranking Web search 
results, i.e., a set of retrieved Web documents 
(URLs) are ordered by relevance to a query is-
sued by a user.  In this paper we assume that the 
task is performed using a ranking model (also 
called ranker for short) that is learned on labeled 
training data (e.g., human-judged 
query-document pairs).  The ranking model acts 
as a function that maps the feature vector of a 
query-document pair to a real-valued score of 
relevance. 
Recent research shows that such a learned 
ranker is superior to classical retrieval models in 
two aspects (Burges et al, 2005; 2006; Gao et al, 
2005).  First, the ranking model can use arbitrary 
features. Both traditional criteria such as TF-IDF 
and BM25, and non-traditional features such as 
hyperlinks can be incorporated as features in the 
ranker. Second, if large amounts of high-quality 
human-judged query-document pairs were 
available for model training, the ranker could 
achieve significantly better retrieval results than 
the traditional retrieval models that cannot ben-
efit from training data effectively.  However, 
such training data is not always available for 
many search domains, such as non-English 
search markets or person name search. 
One of the most widely used strategies to re-
medy this problem is model adaptation, which 
attempts to adjust the parameters and/or struc-
ture of a model trained on one domain (called the 
background domain), for which large amounts of 
training data are available, to a different domain 
(the adaptation domain), for which only small 
amounts of training data are available.  In Web 
search applications, domains can be defined by 
query types (e.g., person name queries), or lan-
guages, etc. 
In this paper we investigate two classes of 
model adaptation methods for Web search 
ranking: Model Interpolation approaches and 
error-driven learning approaches.  In model 
interpolation approaches, the adaptation data is 
used to derive a domain-specific model (also 
called in-domain model), which is then com-
bined with the background model trained on the 
background data.  This appealingly simple con-
cept provides fertile ground for experimentation, 
depending on the level at which the combination 
is implemented (Bellegarda, 2004).  In er-
ror-driven learning approaches, the background 
model is adjusted so as to minimize the ranking 
errors the model makes on the adaptation data 
(Bacchiani et al, 2004; Gao et al 2006).  This is 
arguably more powerful than model interpola-
tion for two reasons.  First, by defining a proper 
error function, the method can optimize more 
directly the measure used to assess the final 
quality of the Web search system, e.g., Normalized 
Discounted Cumulative Gain (Javelin & Kekalainen, 
2000) in this study.  Second, in this framework, 
the model can be adjusted to be as fine-grained as 
necessary.  In this study we developed a set of 
error-driven learning methods based on a 
boosting algorithm where, in an incremental 
manner, not only each feature weight could be 
505
changed separately, but new features could be 
constructed. 
We focus our experiments on the robustness 
of the adaptation methods. A model is robust if it 
performs reasonably well on unseen test data 
that could be significantly different from training 
data.  Robustness is important in Web search 
applications.  Labeling training data takes time.  
As a result of the dynamic nature of Web, by the 
time the ranker is trained and deployed, the 
training data may be more or less out of date.  
Our results show that the model interpolation is 
much more robust than the boosting-based me-
thods. We then explore several methods to im-
prove the robustness of the methods, including 
regularization, randomization, and using shal-
low trees, with limited success. 
2 Ranking Model and Quality 
Measure in Web Search 
This section reviews briefly a particular example 
of rankers, called LambdaRank (Burges et al, 
2006), which serves as the baseline ranker in our 
study.  
Assume that training data is a set of input/ 
output pairs (x, y). x is a feature vector extracted 
from a query-document pair. We use approx-
imately 400 features, including dynamic ranking 
features such as term frequency and BM25, and 
statistic ranking features such as PageRank.  y is 
a human-judged relevance score, 0 to 4, with 4 as 
the most relevant. 
LambdaRank is a neural net ranker that maps 
a feature vector x to a real value y that indicates 
the relevance of the document given the query 
(relevance score).  For example, a linear Lamb-
daRank simply maps x to y with a learned weight 
vector w such that ? = ? ? ?. (We used nonli-
near LambdaRank in our experiments). Lamb-
daRank is particularly interesting to us due to the 
way w is learned. Typically, w is optimized w.r.t. 
a cost function using numerical methods if the 
cost function is smooth and its gradient w.r.t. w 
can be computed easily.  In order for the ranker 
to achieve the best performance in document 
retrieval, the cost function used in training 
should be the same as, or as close as possible to, 
the measure used to assess the quality of the 
system. In Web search, Normalized Discounted 
Cumulative Gain (NDCG) (Jarvelin and Kekalai-
nen, 2000) is widely used as quality measure. For 
a query,  NDCG is computed  as 
?? = ?? 
2? ?  ? 1
log 1 + ? 
?
?=1
, (1) 
where ?(?) is the relevance level of the j-th doc-
ument, and the normalization constant Ni is 
chosen so that a perfect ordering would result in 
?? = 1.  Here L is the ranking truncation level at 
which NDCG is computed. The ??  are then av-
eraged over a query set. However, NDCG, if it 
were to be used as a cost function, is either flat or 
discontinuous everywhere, and thus presents 
challenges to most optimization approaches that 
require the computation of the gradient of the 
cost function.  
LambdaRank solves the problem by using an 
implicit cost function whose gradients are speci-
fied by rules. These rules are called ?-functions. 
Burges et al (2006) studied several ?-functions 
that were designed with the NDCG cost function 
in mind. They showed that LambdaRank with 
the best ?-function outperforms significantly a 
similar neural net ranker, RankNet (Burges et al, 
2005), whose parameters are optimized using the 
cost function based on cross-entropy. 
The superiority of LambdaRank illustrates the 
key idea based on which we develop the model 
adaptation methods.  We should always adapt 
the ranking models in such a way that the NDCG 
can be optimized as directly as possible. 
3 Model Interpolation 
One of the simplest model interpolation methods 
is to combine an in-domain model with a back-
ground model at the model level via linear in-
terpolation.  In practice we could combine more 
than two in-domain/background models.  Let-
ting Score(q, d) be a ranking model that maps a 
query-document pair to a relevance score, the 
general form of the interpolation model is  
?????(?,?) = ???????? ?,? ,
?
?=1
 (2) 
where the ??s are interpolation weights, opti-
mized on validation data with respect to a pre-
defined objective, which is NDCG in our case.  
As mentioned in Section 2, NDCG is not easy to 
optimize, for which we resort to two solutions, 
both of which achieve similar results in our ex-
periments. 
The first solution is to view the interpolation 
model of Equation (2) as a linear neural net 
ranker where each component  model Scorei(.) is 
defined as a feature function. Then, we can use 
the LambdaRank algorithm described in Section 
2 to find the optimal weights.  
An alternative solution is to view interpola-
tion weight estimation as a multi-dimensional 
optimization problem, with each model as a 
506
dimension. Since NCDG is not differentiable, we 
tried in our experiments the numerical algo-
rithms that do not require the computation of 
gradient. Among the best performers is the 
Powell Search algorithm (Press et al, 1992). It 
first constructs a set of N virtual directions that 
are conjugate (i.e., independent with each other), 
then it uses line search N times, each on one vir-
tual direction, to find the optimum.  Line search 
is a one-dimensional optimization algorithm. 
Our implementation follows the one described in 
Gao et al (2005), which is used to optimize the 
averaged precision.  
The performance of model interpolation de-
pends to a large degree upon the quality and the 
size of adaptation data. First of all, the adaptation 
data has to be ?rich? enough to suitably charac-
terize the new domain.  This can only be 
achieved by collecting more in-domain data.  
Second, once the domain has been characterized, 
the adaptation data has to be ?large? enough to 
have a model reliably trained.  For this, we de-
veloped a method, which attempts to augment 
adaptation data by gathering similar data from 
background data sets. 
The method is based on the k-nearest-neighbor 
(kNN) algorithm, and is inspired by Bishop 
(1995).  We use the small in-domain data set D1 
as a seed, and expand it using the large back-
ground data set D2.  When the relevance labels 
are assigned by humans, it is reasonable to as-
sume that queries with the lowest information 
entropy of labels are the least noisy.  That is, for 
such a query most of the URLs are labeled as 
highly relevant/not relevant documents rather 
than as moderately relevance/not relevant 
documents. 
Due to computational limitations of 
kNN-based algorithms, a small subset of queries 
from D1 which are least noisy are selected. This 
data set is called S1.  For each sample in D2, its 
3-nearest neighbors in S1 are found using a co-
sine-similarity metric.  If the three neighbors are 
within a very small distance from the sample in 
D2, and one of the labels of the nearest neighbors 
matches exactly, the training sample is selected 
and is added to the expanded set E2, in its own 
query.  This way, S1 is used to choose training 
data from D2, which are found to be close in 
some space.  
This process effectively creates several data 
points in close neighborhood of the points in the 
original small data set D1, thus expanding the 
set, by jittering each training sample a little. This 
is equivalent to training with noise (Bishop, 
1995), except that the training samples used are 
actual queries judged by a human. This is found 
to increase the NDCG in our experiments. 
4 Error-Driven Learning 
Our error-drive learning approaches to ranking 
modeling adaptation are based on the Stochastic 
Gradient Boosting algorithm (or the boosting 
algorithm for short) described in Friedman 
(1999). Below, we follow the notations in Fried-
man (2001). 
Let adaptation data (also called training data in 
this section) be a set of input/output pairs {xi, yi}, 
i = 1?N. In error-driven learning approaches, 
model adaptation is performed by adjusting the 
background model into a new in-domain model 
?: ? ? ? that minimizes a loss function L(y, F(x)) 
over all samples in training data  
?? = argmin
?
 ?(?? ,?(??))
?
?=1
. (3) 
We further assume that F(x) takes the form of 
additive expansion as 
? ? =  ??? ?; ??  
?
?=0
, (4) 
where h(x; a) is called basis function, and is 
usually a simple parameterized function of the 
input x, characterized by parameters a. In what 
follows, we drop a, and use h(x) for short.  In 
practice, the form of h has to be restricted to a 
specific function family to allow for a practically 
efficient procedure of model adaptation.  ? is a 
real-valued coefficient. 
Figure 1 is the generic algorithm.  It starts 
with a base model F0, which is a background 
model.  Then for m = 1, 2, ?, M, the algorithm 
takes three steps to adapt the base model so as to 
best fit the adaptation data: (1) compute the re-
sidual of the current base model (line 3), (2) select 
the optimal basis function (line 4) that best fits 
the residual, and (3) update the base model by 
adding the optimal basis function (line 5).  The 
two model adaptation algorithms that will be 
described below follow the same 3-step adapta-
tion procedure. They only differ in the choice of 
h.  In the LambdaBoost algorithm (Section 4.1) h 
1 Set F0(x) be the background ranking model 
2 for m = 1 to M do 
3 ??
? = ?  
?? ? ? ,? ??  
?? ?? 
 
? ? =???1 ? 
, for i = 1? N 
4 (?? ,?? ) = argmin
? ,?
  ??
? ? ??(??) 
2?
?=1
 
5 ??  ? = ???1 ? + ???(?) 
Figure 1. The generic boosting algorithm for model 
adaptation 
507
is defined as a single feature, and in LambdaS-
MART (Section 4.2), h is a regression tree.  
Now, we describe the way residual is com-
puted, the step that is identical in both algo-
rithms. Intuitively, the residual, denoted by y? 
(line 3 in Figure 1), measures the amount of er-
rors (or loss) the base model makes on the train-
ing samples.  If the loss function in Equation (3) is 
differentiable, the residual can be computed 
easily as the negative  
gradient of the loss function.  As discussed in 
Section 2, we want to directly optimize the 
NDCD, whose gradient is approximated via the 
?-function.  Following Burges et al (2006), the 
gradient of a training sample (xi, yi), where xi is a 
feature vector representing the query-document 
pair (qi, di), w.r.t. the current base model is com-
puted by marginalizing the ?-functions of all 
document pairs, (di, dj), of the query, qi, as 
??
? = ?NDCG ?
????
????
,
???
 (5) 
where ?NDCG is the NDCG gained by swapping 
those two documents (after sorting all docu-
ments by their current scores);  ??? ? ?? ? ??  is the 
difference in ranking scores of di and dj given qi; 
and Cij is the cross entropy cost defined as  
??? ? ? ???  = ?? ? ??
+ log(1 + exp(?? ? ?? )). 
(6) 
Thus, we have 
????
????
=
?1
1 + exp ???  
. (7) 
This ?-function essentially uses the cross en-
tropy cost to smooth the change in NDCG ob-
tained by swapping the two documents. A key 
intuition behind the ?-function is the observation 
that NDCG does not treat all pairs equally; for 
example, it costs more to incorrectly order a pair, 
where the irrelevant document is ranked higher 
than a highly relevant document, than it does to 
swap a moderately relevant/not relevant pair. 
4.1 The LambdaBoost Algorithm 
In LambdaBoost, the basis function h is defined 
as a single feature (i.e., an element feature in the 
feature vector x).  The algorithm is summarized 
in Figure 2.  It iteratively adapts a background 
model to training data using the 3-step proce-
dure, as in Figure 1. Step 1 (line 3 in Figure 2) has 
been described.  
Step 2 (line 4 in Figure 2) finds the optimal 
basis function h, as well as its optimal coefficient 
?, that best fits the residual according to the 
least-squares (LS) criterion. Formally, let h and ? 
denote the candidate basis function and its op-
timal coefficient. The LS error on training data 
is ?? ?;? =   ??
? ? ?? ??=0
2
, where ??
?  is com-
puted as Equation (5). The optimal coefficient of 
h is estimated by solving the equation ?   ??
? ???=1
??2/??=0. Then, ? is computed as 
? =
 ??
??(??)
?
?=1
 ?(??)
?
?=1
. (8) 
Finally, given its optimal coefficient ?, the op-
timal LS loss of h is  
?? ?;? = ??
? ? ??
?  
?
?=1
?
  ??
?? ?? 
?
?=1  
2
 ?2(??)
?
?=1
. (9) 
Step 3 (line 5 in Figure 2) updates the base 
model by adding the chosen optimal basis func-
tion with its optimal coefficient.  As shown in 
Step 2, the optimal coefficient of each candidate 
basis function is computed when the basis func-
tion is evaluated.  However, adding the basis 
function using its optimal efficient is prone to 
overfitting. We thus add a shrinkage coefficient 0 
< ? < 1 ? the fraction of the optimal line step 
taken. The update equation is thus rewritten in 
line 5 in Figure 2.   
Notice that if the background model contains 
all the input features in x, then LambdaBoost 
does not add any new features but adjust the 
weights of existing features.  If the background 
model does not contain all of the input features, 
then LambdaBoost can be viewed as a feature 
selection method, similar to Collins (2000), where 
at each iteration the feature that has the largest 
impact on reducing training loss is selected and 
added to the background model. In either case, 
LambdaBoost adapts the background model by 
adding a model whose form is a (weighted) li-
near combination of input features.  The property 
of linearity makes LambdaBoost robust and less 
likely to overfit in Web search applications.  But 
this also limits the adaptation capacity. A simple 
method that allows us to go beyond linear 
adaptation is to define h as nonlinear terms of the 
input features, such as regression trees in 
LambdaSMART. 
4.2 The LambdaSMART Algorithm 
LambdaSMART was originally proposed in Wu 
et al (2008). It is built on MART (Friedman, 2001) 
but uses the ?-function (Burges et a., 2006) to 
1 Set F0(x) to be the background ranking model 
2 for m = 1 to M do 
3 compute residuals according to Equation (5)  
4 select best hm (with its best ?m), according to LS, 
computed by Equations (8) and (9) 
5 ??  ? = ???1 ? + ????(?) 
Figure 2. The LambdaBoost algorithm for model adaptation. 
508
compute gradients. The algorithm is summa-
rized in Figure 3.  Similar to LambdaBoost, it 
takes M rounds, and at each boosting iteration, it 
adapts the background model to training data 
using the 3-step procedure. Step 1 (line 3 in Fig-
ure 3) has been described.  
Step 2 (lines 4 to 6) searches for the optimal 
basis function h to best fit the residual.  Unlike 
LambdaBoost where there are a finite number of 
candidate basis functions, the function space of 
regression trees is infinite. We define h as a re-
gression tree with L terminal nodes.  In line 4, a 
regression tree is built using Mean Square Error 
to determine the best split at any node in the tree.  
The value associated with a leaf (i.e., terminal 
node) of the trained tree is computed first as the 
residual (computed via ?-function) for the train-
ing samples that land at that leaf.  Then, since 
each leaf corresponds to a different mean, a 
one-dimensional Newton-Raphson line step is 
computed for each leaf (lines 5 and 6).  These line 
steps may be simply computed as the derivatives 
of the LambdaRank gradients w.r.t. the model 
scores si.  Formally, the value of the l-th leaf, ?ml, 
is computed as 
??? =
 ??
?
?????
 ???????
, (10) 
where ??
?  is the residual of training sample i, 
computed in Equation (5), and  ??  is the deriva-
tive of ??
? , i.e., ?? = ???
?/??(??). 
In Step 3 (line 7), the regression tree is added 
to the current base model, weighted by the 
shrinkage coefficient 0 < ? < 1.  
Notice that since a regression tree can be 
viewed as a complex feature that combines mul-
tiple input features, LambdaSMART can be used 
as a feature generation method. LambdaSMART 
is arguably more powerful than LambdaBoost in 
that it introduces new complex features and thus 
adjusts not only the parameters but also the 
structure of the background model1. However, 
                                                     
1  Note that in a sense our proposed LambdaBoost 
algorithm is the same as LambdaSMART, but using a 
single feature at each iteration, rather than a tree. In 
particular, they share the trick of using the Lambda 
one problem of trees is their high variance.  
Often a small change in the data can result in a 
very different series of splits.  As a result, 
tree-based ranking models are much less robust 
to noise, as we will show in our experiments.  In 
addition to the use of shrinkage coefficient 0 < ? 
< 1, which is a form of model regularization 
according to Hastie, et al, (2001), we will ex-
plore in Section 5.3 other methods of improving 
the model robustness, including randomization 
and using shallow trees. 
5 Experiments 
5.1 The Data 
We evaluated the ranking model adaptation 
methods on two Web search domains, namely (1) 
a name query domain, which consists of only 
person name queries, and (2) a Korean query 
domain, which consists of queries that users 
submitted to the Korean market.   
For each domain, we used two in-domain 
data sets that contain queries sampled respec-
tively from the query log of a commercial Web 
search engine that were collected in two 
non-overlapping periods of time.  We used the 
more recent one as open test set, and split the 
other into three non-overlapping data sets, 
namely training, validation and closed test sets, 
respectively.  This setting provides a good si-
mulation to the realistic Web search scenario, 
where the rankers in use are usually trained on 
early collected data, and thus helps us investigate 
the robustness of these model adaptation me-
thods. 
The statistics of the data sets used in our per-
son name domain adaptation experiments are 
shown in Table 1. The names query set serves as 
the adaptation domains, and Web-1 as the back-
ground domain. Since Web-1 is used to train a 
background ranker, we did not split it to 
train/valid/test sets. We used 416 input features 
in these experiments.  
For cross-domain adaptation experiments 
from non-Korean to Korean markets, Korean 
data serves as the adaptation domain, and Eng-
lish, Chinese, and Japanese data sets as the 
background domain.  Again, we did not split the 
data sets in the background domain to 
train/valid/test sets.  The statistics of these data 
sets are shown in Table 2. We used 425 input 
features in these experiments. 
                                                                                
gradients to learn NDCG. 
1 Set F0(x) to be the background ranking model 
2 for m = 1 to M do 
3 compute residuals according to Equation (5)  
4 create a  L-terminal node tree, ?? ?  ???  ?=1??  
5 for l = 1 to L do 
6 compute the optimal ?lm according to Equation 
(10), based on approximate Newton step. 
7 ??  ? = ???1 ? + ? ???1(? ? ??? )
?=1??
 
Figure 3. The LambdaSMART algorithm for model adaptation. 
509
In each domain, the in-domain training data is 
used to train in-domain rankers, and the back-
ground data for background rankers. Validation 
data is used to learn the best training parameters 
of the boosting algorithms, i.e., M, the total 
number of boosting iterations, ?, the shrinkage 
coefficient, and L, the number of leaf nodes for 
each regression tree (L=1 in LambdaBoost). 
Model performance is evaluated on the 
closed/open test sets.  
All data sets contain samples labeled on a 
5-level relevance scale, 0 to 4, with 4 as most 
relevant and 0 as irrelevant. The performance of 
rankers is measured through NDCG evaluated 
against closed/open test sets.  We report NDCG 
scores at positions 1, 3 and 10, and the averaged 
NDCG score (Ave-NDCG), the arithmetic mean 
of the NDCG scores at 1 to 10. Significance test 
(i.e., t-test) was also employed. 
5.2 Model Adaptation Results 
This section reports the results on two adapta-
tion experiments.  The first uses a large set of 
Web data, Web-1, as background domain and 
uses the name query data set as adaptation data. 
The results are summarized in Tables 3 and 4.  
We compared the three model adaptation me-
thods against two baselines: (1) the background 
ranker (Row 1 in Tables 3 and 4), a 2-layer 
LambdaRank model with 15 hidden nodes and a 
learning rate of 10-5 trained on Web-1; and (2) the 
In-domain Ranker (Row 2), a 2-layer Lambda-
Rank model with 10 hidden nodes and a learning 
rate of 10-5 trained on Names-1-Train.  We built 
two interpolated rankers.  The 2-way interpo-
lated ranker (Row 3) is a linear combination of 
the two baseline rankers, where the interpolation 
weights were optimized on Names-1-Valid.  To 
build the 3-way interpolated ranker (Row 4), we 
linearly interpolated three rankers.  In addition 
to the two baseline rankers, the third ranker is 
trained on an augmented training data, which 
was created using the kNN method described in 
Section 3.   
In LambdaBoost (Row 5) and LambdaSMART 
(Row 6), we adapted the background ranker to 
name queries by boosting the background ranker 
with Names-1-Train. We trained LambdaBoost 
with the setting M = 500, ? = 0.5, optimized on 
Names-1-Valid. Since the background ranker 
uses all of the 416 input features, in each boosting 
iteration, LambdaBoost in fact selects one exist-
ing feature in the background ranker and adjusts 
its weight. We trained LambdaSMART with M = 
500, L = 20, ? = 0.5, optimized on Names-1-Valid. 
We see that the results on the closed test set 
(Table 3) are quite different from the results on 
the open test set (Table 4).  The in-domain ranker 
outperforms the background ranker on the 
closed test set, but underperforms significantly 
the background ranker on the open test set.  The 
interpretation is that the training set and the 
closed test set are sampled from the same data 
set and are very similar, but the open test set is a 
very different data set, as described in Section 5.1.  
Similarly, on the closed test set, LambdaSMART 
outperforms LambdaBoost with a big margin 
due to its superior adaptation capacity; but on 
the open test set their performance difference is 
much smaller due to the instability of the trees in 
LambdaSMART, as we will investigate in detail 
later.  Interestingly, model interpolation, though 
simple, leads to the two best rankers on the open 
test set. In particular, the 3-way interpolated 
ranker outperforms the two baseline rankers 
Coll. Description  #qry. # url/qry 
Web-1 Background training data 31555 134 
Names-1-Train In-domain training data  
(adaptation data)  
5752 85 
Names-1-Valid In-domain validation data 158 154 
Names-1-Test Closed test data 318 153 
Names-2-Test Open test data 4370 84 
Table 1. Data sets in the names query domain experiments,  
where # qry is number of queries, and # url/qry is number 
of documents per query. 
Coll. Description  # qry. # url/qry 
Web-En Background En training data 6167 198 
Web-Ja Background Ja training data 45012 58 
Web-Cn Background Ch training data 32827 72 
Kokr-1-Train In-domain Ko training data 
(adaptation data)  
3724 64 
Kokr-1-Valid In-domain validation data 334 130 
Kokr-1-Test Korean closed test data 372 126 
Kokr-2-Test Korean open test data 871 171 
Table 2. Data sets in the Korean domain experiments. 
# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. 0.4575 0.4952 0.5446 0.5092 
2 In-domain 0.4921 0.5296 0.5774 0.5433 
3 2W-Interp. 0.4745 0.5254 0.5747 0.5391 
4 3W-Interp. 0.4829 0.5333 0.5814 0.5454 
5 ?-Boost 0.4706 0.5011 0.5569 0.5192 
6 ?-SMART 0.5042 0.5449 0.5951 0.5623 
Table 3. Close test results on Names-1-Test. 
# Models NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. 0.5472 0.5347 0.5731 0.5510 
2 In-domain 0.5216 0.5266 0.5789 0.5472 
3 2W-Interp. 0.5452 0.5414 0.5891 0.5604 
4 3W-Interp. 0.5474 0.5470 0.5951 0.5661 
5 ?-Boost 0.5269 0.5233 0.5716 0.5428 
6 ?-SMART 0.5200 0.5331 0.5875 0.5538 
Table 4. Open test results on Names-2-Test. 
510
significantly (i.e., p-value < 0.05 according to 
t-test) on both the open and closed test sets. 
The second adaptation experiment involves 
data sets from several languages (Table 2).  
2-layer LambdaRank baseline rankers were first 
built from Korean, English, Japanese, and Chi-
nese training data and tested on Korean test sets 
(Tables 5 and 6).  These baseline rankers then 
serve as in-domain ranker and background 
rankers for model adaptation.  For model inter-
polation (Tables 7 and 8), Rows 1 to 4 are three 
2-way interpolated rankers built by linearly in-
terpolating  
each of the three background rankers with the 
in-domain ranker, respectively.  Row 4 is a 4-way 
interpolated ranker built by interpolating the 
in-domain ranker with the three background 
rankers.  For LambdaBoost (Tables 9 and 10) and 
LambdaSMART (Tables 11 and 12), we used the 
same parameter settings as those in the name 
query experiments, and adapted the three back-
ground rankers, to the Korean training data, 
Kokr-1-Train. 
The results in Tables 7 to 12 confirm what we 
learned in the name query experiments. There 
are three main conclusions. (1) Model interpola-
tion is an effective method of ranking model 
adaptation. E.g., the 4-way interpolated ranker 
outperforms other ranker significantly. (2) 
LambdaSMART is the best performer on the 
closed test set, but its performance drops signif-
icantly on the open test set due to the instability 
of trees. (3) LambdaBoost does not use trees. So 
its modeling capacity is weaker than Lamb-
daSMART (e.g., it always underperforms 
LambdaSMART significantly on the closed test 
sets), but it is more robust due to its linearity (e.g., 
it performs similarly to LambdaSMART on the 
open test set). 
5.3 Robustness of Boosting Algorithms 
This section investigates the robustness issue 
of the boosting algorithms in more detail. We 
compared LambdaSMART with different values 
of L (i.e., the number of leaf nodes), and with and 
without randomization. Our assumptions are (1) 
allowing more leaf nodes would lead to deeper 
trees, and as a result, would make the resulting 
ranking models less robust; and (2) injecting 
randomness into the basis function (i.e. regres-
sion tree) estimation procedure would improve 
the robustness of the trained models (Breiman, 
2001; Friedman, 1999).  In LambdaSMART, the 
randomness can be injected at different levels of 
tree construction.  We found that the most effec-
tive method is to introduce the randomness at 
the node level (in Step 4 in Figure 3). Before each 
node split, a subsample of the training data and a 
subsample of the features are drawn randomly. 
(The sample rate is 0.7). Then, the two randomly 
selected subsamples, instead of the full samples, 
are used to determine the best split.  
 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. (En) 0.5371 0.5413 0.5873 0.5616 
2 Back. (Ja) 0.5640 0.5684 0.6027 0.5808 
3 Back. (Cn) 0.4966 0.5105 0.5761 0.5393 
4 In-domain  0.5927 0.5824 0.6291 0.6055 
Table 5. Close test results of baseline rankers, on Kokr-1-Test 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Back. (En) 0.4991 0.5242 0.5397 0.5278 
2 Back. (Ja) 0.5052 0.5092 0.5377 0.5194 
3 Back. (Cn) 0.4779 0.4855 0.5114 0.4942 
4 In-domain  0.5164 0.5295 0.5675 0.5430 
Table 6. Open test results of baseline rankers, on Kokr-2-Test 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Interp. (En) 0.5954 0.5893 0.6335 0.6088 
2 Interp. (Ja) 0.6047 0.5898 0.6339 0.6116 
3 Interp. (Cn) 0.5812 0.5807 0.6268 0.6024 
4 4W-Interp. 0.5878 0.5870 0.6289 0.6054 
Table 7. Close test results of interpolated rankers, on 
Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 Interp. (En) 0.5178 0.5369 0.5768 0.5500 
2 Interp. (Ja) 0.5274 0.5416 0.5788 0.5531 
3 Interp. (Cn) 0.5224 0.5339 0.5766 0.5487 
4 4W-Interp.  0.5278 0.5414 0.5823 0.5549 
Table 8. Open test results of interpolated rankers, on 
Kokr-2-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-Boost (En) 0.5757 0.5716 0.6197 0.5935 
2 ?-Boost (Ja) 0.5801 0.5807 0.6225 0.5982 
3 ?-Boost (Cn)  0.5731 0.5793 0.6226 0.5972 
Table 9. Close test results of ?-Boost rankers, on Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-Boost (En) 0.4960 0.5203 0.5486 0.5281 
2 ?-Boost (Ja) 0.5090 0.5167 0.5374 0.5233 
3 ?-Boost (Cn)  0.5177 0.5324 0.5673 0.5439 
Table 10. Open test results of ?-Boost rankers, on Kokr-2-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?-SMART 
(En) 
0.6096 0.6057 0.6454 0.6238 
2 ?- SMART 
(Ja) 
0.6014 0.5966 0.6385 0.6172 
3 ?- SMART 
(Cn)  
0.5955 0.6095 0.6415 0.6209 
Table 11. Close test results of ?-SMART rankers, on 
Kokr-1-Test. 
# Ranker NDCG@1 NDCG@3 NDCG@10 AveNDCG 
1 ?- SMART 
(En) 
0.5177 0.5297 0.5563 0.5391 
2 ?- SMART 
(Ja) 
0.5205 0.5317 0.5522 0.5368 
3 ?- SMART 
(Cn)  
0.5198 0.5305 0.5644 0.5410 
Table 12. Open test results of ?-SMART rankers, on 
Kokr-2-Test. 
511
We first performed the experiments on name 
queries. The results on the closed and open test sets 
are shown in Figures 4 (a) and 4 (b), respectively. 
The results are consistent with our assumptions. 
There are three main observations.  First, the gray 
bars in Figures 4 (a) and 4 (b) (boosting without 
randomization) show that on the closed test set, as  
expected, NDCG increases with the value of L, but 
the correlation does not hold on the open test set.  
Second, the black bars in these figures (boosting 
with randomization) show that in both closed and 
open test sets, NDCG increases with the value of L.  
Finally, comparing the gray bars with their cor-
responding black bars, we see that randomization 
consistently improves NDCG on the open test set, 
with a larger margin of gain for the boosting algo-
rithms with deeper trees (L > 5). 
These results are very encouraging.  Randomi-
zation seems to work like a charm. Unfortunately, 
it does not work well enough to help the boosting 
algorithm beat model interpolation on the open test 
sets.  Notice that all the LambdaSMART results 
reported in Section 5.2 use randomization with the 
same sampling rate  of 0.7.  We repeated the com-
parison in the cross-domain adaptation experi-
ments.  As shown in Figure 4, results in 4 (c) and 4 
(d) are consistent with those on names queries in 4 
(b). Results in 4 (f) show a visible performance drop 
from LambdaBoost to LambdaSMART with L = 2, 
indicating again the instability of trees. 
6 Conclusions and Future Work 
In this paper, we extend two classes of model 
adaptation methods (i.e., model interpolation and 
error-driven learning), which have been well stu-
died in statistical language modeling for speech 
and natural language applications (e.g., Bacchiani 
et al, 2004; Bellegarda, 2004; Gao et al, 2006), to 
ranking models for Web search applications.  
We have evaluated our methods on two adap-
tation experiments over a wide variety of datasets 
where the in-domain datasets bear different levels 
of similarities to their background datasets.  We 
reach different conclusions from the results of the 
open and close tests, respectively. Our open test 
results show that in the cases where the in-domain 
data is dramatically different from the background 
data, model interpolation is very robust and out-
performs the baseline and the error-driven learning 
methods significantly; whereas our close test re-
sults show that in the cases where the in-domain 
data is similar to the background data, the tree- 
based boosting algorithm (i.e. LambdaSMART) is 
the best performer, and achieves a significant im-
provement over the baselines.  We also show that 
these different conclusions are largely due to the 
instability of the use of trees in the boosting algo-
rithm. We thus explore several methods of im-
proving the robustness of the algorithm, such as 
randomization, regularization, using shallow trees, 
with limited success.  Of course, our experiments, 
 (a)  (b)  
  
(c)  (d)  (e)  
Figure 4. AveNDCG results (y-axis) of LambdaSMART with different values of L (x-axis), where L=1 is LambdaBoost; (a) and (b) are 
the results on closed and open tests using Names-1-Train as adaptation data, respectively;  (d),  (e) and (f) are the results on the 
Korean open test set, using background models trained on Web-En, Web-Ja, and Web-Cn data sets, respectively. 
   
0.49
0.50
0.51
0.52
0.53
0.54
0.55
0.56
0.57
1 2 4 10 20
0.53
0.54
0.54
0.55
0.55
1 2 4 10 20
0.50
0.51
0.52
0.53
0.54
0.55
1 2 4 10 20
0.49
0.50
0.51
0.52
0.53
0.54
1 2 4 10 20
0.51
0.52
0.53
0.54
0.55
1 2 4 10 20
512
described in Section 5.3, only scratch the surface of 
what is possible.  Robustness deserves more inves-
tigation and forms one area of our future work. 
Another family of model adaptation methods 
that we have not studied in this paper is transfer 
learning, which has been well-studied in the ma-
chine learning community (e.g., Caruana, 1997; 
Marx et al, 2008).  We leave it to future work. 
To solve the issue of inadequate training data, in 
addition to model adaptation, researchers have 
also been exploring the use of implicit user feed-
back data (extracted from log files) for ranking 
model training (e.g., Joachims et al, 2005; Radlinski 
et al, 2008).  Although such data is very noisy, it is 
of a much larger amount and is cheaper to obtain 
than human-labeled data.  It will be interesting to 
apply the model adaptation methods described in 
this paper to adapt a ranker which is trained on a 
large amount of automatically extracted data to a 
relatively small amount of human-labeled data. 
Acknowledgments 
This work was done while Yi Su was visiting Mi-
crosoft Research, Redmond. We thank Steven Yao's 
group at Microsoft Bing Search for their help with 
the experiments. 
References 
Bacchiani, M., Roark, B. and Saraclar, M. 2004. 
Language model adaptation with MAP estima-
tion and the Perceptron algorithm. In 
HLT-NAACL, 21-24. 
Bellegarda, J. R. 2004. Statistical language model 
adaptation: review and perspectives. Speech 
Communication, 42: 93-108. 
Breiman, L. 2001. Random forests. Machine Learning, 
45, 5-23. 
Bishop, C.M. 1995. Training with noise is equiva-
lent to Tikhonov regularization. Neural Computa-
tion, 7, 108-116. 
Burges, C. J., Ragno, R., & Le, Q. V. 2006. Learning 
to rank with nonsmooth cost functions. In ICML. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML. 
Caruana, R. 1997. Multitask learning. Machine 
Learning, 28(1): 41-70. 
Collins, M. 2000. Discriminative reranking for nat-
ural language parsing. In ICML. 
Donmea, P., Svore, K. and Burges. 2008. On the 
local optimality for NDCG. Microsoft Technical 
Report, MSR-TR-2008-179. 
Friedman, J. 1999. Stochastic gradient boosting. 
Technical report, Dept. Statistics, Stanford.  
Friedman, J. 2001. Greedy function approximation: 
a gradient boosting machine. Annals of Statistics, 
29(5). 
Gao, J., Qin, H., Xia, X. and Nie, J-Y. 2005. Linear 
discriminative models for information retrieval. 
In SIGIR. 
Gao, J., Suzuki, H. and Yuan, W. 2006. An empirical 
study on language model adaptation. ACM Trans 
on Asian Language Processing, 5(3):207-227. 
Hastie, T., Tibshirani, R. and Friedman, J. 2001. The 
elements of statistical learning. Springer-Verlag, 
New York. 
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation 
methods for retrieving highly relevant docu-
ments. In SIGIR. 
Joachims, T., Granka, L., Pan, B., Hembrooke, H. 
and Gay, G. 2005. Accurately interpreting click-
through data as implicit feedback. In SIGIR. 
Marx, Z., Rosenstein, M.T., Dietterich, T.G. and 
Kaelbling, L.P. 2008. Two algorithms for transfer 
learning. To appear in Inductive Transfer: 10 years 
later. 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and 
B. P. Flannery. 1992. Numerical Recipes In C.  
Cambridge Univ. Press. 
Radlinski, F., Kurup, M. and Joachims, T. 2008. 
How does clickthrough data reflect retrieval 
quality? In CIKM. 
Thrun, S. 1996. Is learning the n-th thing any easier 
than learning the first. In NIPS. 
Wu, Q., Burges, C.J.C., Svore, K.M. and Gao, J. 
2008. Ranking, boosting, and model adaptation. 
Technical Report MSR-TR-2008-109, Microsoft 
Research. 
 
513
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484?1492,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discovery of Term Variation in Japanese Web Search Queries 
 
 Hisami Suzuki, Xiao Li, and Jianfeng Gao 
Microsoft Research, Redmond 
One Microsoft Way, Redmond, WA 98052 USA 
{hisamis,xiaol,jfgao}@microsoft.com 
 
 
 
 
 
  
 
Abstract 
In this paper we address the problem of identi-
fying a broad range of term variations in Japa-
nese web search queries, where these varia-
tions pose a particularly thorny problem due to 
the multiple character types employed in its 
writing system. Our method extends the tech-
niques proposed for English spelling correc-
tion of web queries to handle a wider range of 
term variants including spelling mistakes, va-
lid alternative spellings using multiple charac-
ter types, transliterations and abbreviations. 
The core of our method is a statistical model 
built on the MART algorithm (Friedman, 
2001). We show that both string and semantic 
similarity features contribute to identifying 
term variation in web search queries; specifi-
cally, the semantic similarity features used in 
our system are learned by mining user session 
and click-through logs, and are useful not only 
as model features but also in generating term 
variation candidates efficiently. The proposed 
method achieves 70% precision on the term 
variation identification task with the recall 
slightly higher than 60%, reducing the error 
rate of a na?ve baseline by 38%.  
1 Introduction 
Identification of term variations is fundamental 
to many NLP applications: words (or more gen-
erally, terms) are the building blocks of NLP ap-
plications, and any robust application must be 
able to handle variations in the surface represen-
tation of terms, be it a spelling mistake, valid 
spelling variation, or abbreviation. In search ap-
plications, term variations can be used for query 
expansion, which generates additional query 
terms for better matching with the terms in the 
document set. Identifying term variations is also 
useful in other scenarios where semantic equiva-
lence of terms is sought, as it represents a very 
special case of paraphrase.  
This paper addresses the problem of identify-
ing term variations in Japanese, specifically for 
the purpose of query expansion in web search, 
which appends additional terms to the original 
query string for better retrieval quality. Query 
expansion has been shown to be effective in im-
proving web search results in English, where dif-
ferent methods of generating the expansion terms 
have been attempted, including relevance feed-
back (e.g., Salton and Buckley, 1990), correction 
of spelling errors (e.g., Cucerzan and Brill, 2004), 
stemming or lemmatization (e.g., Frakes, 1992), 
use of manually- (e.g., Aitchison and Gilchrist, 
1987) or automatically- (e.g., Rasmussen 1992) 
constructed thesauri, and Latent Semantic Index-
ing (e.g., Deerwester et al 1990). Though many 
of these methods can be applied to Japanese 
query expansion, there are unique problems 
posed by Japanese search queries, the most chal-
lenging of which is that valid alternative spel-
lings of a word are extremely common due to the 
multiple script types employed in the language. 
For example, the word for 'protein' can be spelled 
as ??????, ?????, ???, ???? 
and so on, all pronounced tanpakushitsu but us-
ing combinations of different script types. We 
give a detailed description of the problem posed 
by the Japanese writing system in Section 2. 
Though there has been previous work on ad-
dressing specific subsets of spelling alterations 
within and across character types in Japanese, 
there has not been any comprehensive solution 
for the purpose of query expansion.  
Our approach to Japanese query expansion is 
unique in that we address the problem compre-
hensively: our method works independently of 
the character types used, and targets a wide range 
of term variations that are both orthographically 
and semantically similar, including spelling er-
rors, valid alternative spellings, transliterations 
and abbreviations. As described in Section 4, we 
define the problem of term variation identifica-
1484
tion as a binary classification task, and build two 
types of classifiers according to the maximum 
entropy model (Berger et al, 1996) and the 
MART algorithm (Friedman, 2001), where all 
term similarity metrics are incorporated as fea-
tures and are jointly optimized. Another impor-
tant contribution of our approach is that we de-
rive our semantic similarity models by mining 
user query logs, which has been explored for the 
purposes of collecting related words (e.g., Jones 
et al, 2006a), improving search results ranking 
(e.g., Craswell and Szummer, 2007) and learning 
query intention (e.g., Li et al, 2008), but not for 
the task of collecting term variations. We show 
that our semantic similarity models are not only 
effective in the term variation identification task, 
but also for generating candidates of term varia-
tions much more efficiently than the standard 
method whose candidate generation is based on 
edit distance metrics.  
2 Term Variations in Japanese 
In this section we give a summary of the Japa-
nese writing system and the problem it poses for 
identifying term variations, and define the prob-
lem we want to solve in this paper.  
2.1 The Japanese Writing System 
There are four different character types that are 
used in Japanese text: hiragana, katakana, kanji 
and Roman alphabet. Hiragana and katakana are 
the two subtypes of kana characters, which are 
syllabic character sets, each with about 50 basic 
characters. There is a one-to-one correspondence 
between hiragana and katakana characters, and, 
as they are phonetic, they can be unambiguously 
converted into a sequence of Roman characters. 
For example, the word for 'mackerel' is spelled in 
hiragana as ?? or in katakana as ??, both of 
which can be transcribed in Roman characters as 
saba, which is how the word is pronounced. 
Kanji characters, on the other hand, are ideo-
graphic and therefore numerous ? more than 
5,000 are in common usage. One difficulty in 
handling Japanese kanji is that each character has 
multiple pronunciations, and the correct pronun-
ciation is determined by the context in which the 
character is used. For instance, the character ? is 
read as kou in the word ?? ginkou 'bank', gyou 
in ?  'column', and i or okona in ???  itta 
'went' or okonatta 'done' depending on the con-
text in which the word is used.1  Proper name 
readings are particularly difficult to disambiguate, 
as their pronunciation cannot be inferred from 
the context (they tend to have the same grammat-
ical function) or from the dictionary (they tend to 
be out-of-vocabulary). Therefore, in Japanese, 
computing a pronunciation-based edit distance 
metric is not straightforward, as it requires esti-
mating the readings of kanji characters.  
2.2 Term Variation by Character Type 
Spelling variations are commonly observed both 
within and across character types in Japanese. 
Within a character type, the most prevalent is the 
variation observed in katakana words. Katakana 
is used to transliterate words from English and 
other foreign languages, and therefore reflects 
the variations in the sound adaptation from the 
source language. For example, the word 
'spaghetti' is transliterated into six different 
forms (?????? supagetti, ???????
supagettii, ?????? supagettei, ?????
supageti, ?????? supagetii, ?????
supagetei) within a newspaper corpus (Masuya-
ma et al, 2004).  
Spelling variants are also prevalent across 
character types: in theory, a word can be spelled 
using any of the character types, as we have seen 
in the example for the word 'protein' in Section 1. 
Though there are certainly preferred character 
types for spelling each word, variations are still 
very common in Japanese text and search queries. 
Alterations are particularly common among hira-
gana, katakana and kanji (e.g. ??~??~ ? sa-
ba 'mackerel'), and between katakana and Roman 
alphabet (e.g. ??????  fedekkusu fedex). 
This latter case constitutes the problem of transli-
teration, which has been extensively studied in 
the context of machine translation (e.g. Knight 
and Graehl, 1998; Bilac and Tanaka, 2004; Brill 
et al, 2001).  
2.3 Term Variation by Re-write Categories 
Table 1 shows the re-write categories of related 
terms observed in web query logs, drawing on 
our own data analysis as well as on previous 
work such as Jones et al (2006a) and Okazaki et 
al. (2008b). Categories 1 though 9 represent 
strictly synonymous relations; in addition, terms 
in Categories 1 through 5 are also similar ortho-
graphically or in pronunciation. Categories 10 
                                                 
1 In a dictionary of 200K entries, we find that on average 
each kanji character has 2.5 readings, with three characters 
(?,?,?) with as many as 11 readings. 
1485
through 12, on the other hand, specify non-
synonymous relations.  
Different sets out of these categories can be 
useful for different purposes. For example, Jones 
et al(2006a; 2006b) target al of these categories, 
as their goal is to collect related terms as broadly 
as possible for the application of sponsored 
search, i.e., mapping search queries to a small 
corpus of advertiser listings. Okazaki et al 
(2008b) define their task narrowly, to focusing 
on spelling variants and inflection, as they aim at 
building lexical resources for the specific domain 
of medical text.  
For web search, a conservative definition of 
the task as dealing only with spelling errors has 
been successful for English; a more general defi-
nition using related words for query expansion 
has been a mixed blessing as it compromises re-
trieval precision. A comprehensive review on 
this topic is provided by Baeza-Yates and Ribei-
ro-Neto (1999). In this paper, therefore, we adopt 
a working definition of the term variation identi-
fication task as including Categories 1 through 5, 
i.e., those that are synonymous and also similar 
in spelling or in pronunciation.2 This definition is 
reasonably narrow so as to make automatic dis-
covery of term variation pairs realistic, while 
covering all common cases of term variation in 
Japanese, including spelling variants and transli-
terations. It is also appropriate for the purpose of 
query expansion: because term variation defined 
in this manner is based on spelling or pronuncia-
tion similarity, their meaning and function tend 
                                                 
2 In reality, Category 3 (Inflection) is extremely rare in Jap-
anese web queries, because nouns do not inflect in Japanese, 
and most queries are nominals.  
to be completely equivalent, as opposed to Cate-
gories 6 through 9, where synonymy is more 
context- or user-dependent. This will ensure that 
the search results by query expansion will avoid 
the problem of compromised precision.  
3 Related Work 
In information retrieval, the problem of vocabu-
lary mismatch between the query and the terms 
in the document has been addressed in many 
ways, as mentioned in Section 1, achieving vary-
ing degrees of success in the retrieval task. In 
particular, our work is closely related to research 
in spelling correction for English web queries 
(e.g., Cucerzan and Brill, 2004; Ahmad and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007). 
Among these, Li et al (2006) and Chen et al 
(2007) incorporate both string and semantic simi-
larity in their discriminative models of spelling 
correction, similarly to our approach. In Li et al 
(2006), semantic similarity was computed as dis-
tributional similarity of the terms using query 
strings in the log as context. Chen et al (2007) 
point out that this method suffers from the data 
sparseness problem in that the statistics for rarer 
terms are unreliable, and propose using web 
search results as extended contextual information. 
Their method, however, is expensive as it re-
quires web search results for each query-
candidate pair, and also because their candidate 
set, generated using an edit distance function and 
phonetic similarity from query log data, is im-
practically large and must be pruned by using a 
language model. Our approach differs from these 
methods in that we exploit user query logs to 
derive semantic knowledge of terms, which is 
Categories Example in English Example in Japanese 
1. Spelling mistake aple ~ apple ???? guuguru ~ ???? gu-guru 'google' 
2. Spelling variant color ~ colour ??~??~?; ?????~??????? (Cf. Sec.2.2) 
3. Inflection matrix ~ matrices ?? tsukuru 'make' ~ ??? tsukutta 'made' 
4. Transliteration  ?????? ~ fedex 'Fedex' 
5. Abbreviation/ 
Acronym 
macintosh ~ mac ???? sekaiginkou ~ ?? segin 'World Bank'; ???
??? makudonarudo ~ ??? makku 'McDonald's' 
6. Alias republican party ~ gop ???? furansu ~ ? futsu 'France' 
7. Translation ???????? pakisutantaishikan ~ Pakistan embassy 
8. Synonym carcinoma ~ cancer ? koyomi ~ ????? karendaa 'calendar' 
9. Abbreviation 
    (user specific) 
mini ~ mini cooper ??????? kuronekoyamato ~ ???? kuroneko 
(name of a delivery service company)  
10. Generalization nike shoes ~ shoes ???? ?? shibikku buhin 'Civic parts' ~ ? ?? ku-
ruma buhin 'car parts' 
11. Specification ipod ~ ipod nano ??? toukyoueki 'Tokyo station' ~ ?????? tou-
kyouekijikokuhyou 'Tokyo station timetable' 
12. Related windows ~ microsoft ??? toyota 'Toyota' ~ ??? honda 'Honda' 
Table 1: Categories of Related Words Found in Web Search Logs 
1486
used both for the purpose of generating a candi-
date set efficiently and as features in the term 
variation identification model.  
Acquiring semantic knowledge from a large 
quantity of web query logs has become popular 
in recent years. Some use only query strings and 
their counts for learning word similarity (e.g., 
Sekine and Suzuki, 2007; Komachi and Suzuki, 
2008), while others use additional information, 
such as the user session information (i.e., a set of 
queries issued by the same user within a time 
frame, e.g., Jones et al, 2006a) or the URLs 
clicked as a result of the query (e.g., Craswell 
and Szummer, 2007; Li et al, 2008). This addi-
tional data serves as an approximation to the 
meaning of the query; we use both user session 
and click-through data for discovering term vari-
ations.  
Our work also draws on some previous work 
on string transformation, including spelling nor-
malization and transliteration. In addition to the 
simple Levenshtein distance, we also use genera-
lized string-to-string edit distance (Brill and 
Moore, 2000), which we trained on aligned kata-
kana-English word pairs in the same manner as 
Brill et al (2001). As mentioned in Section 2.2, 
our work also tries to address the individual 
problems targeted by such component technolo-
gies as Japanese katakana variation, English-to-
katakana transliteration and katakana-to-English 
back-transliteration in a unified framework.  
4 Discriminative Model of Identifying 
Term Variation 
Recent work in spelling correction (Ahmed and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007) 
and normalization (Okazaki et al, 2008b) formu-
lates the task in a discriminative framework:  
??  = argmax??gen  ? ?(?|?) 
This model consists of two components: gen(q) 
generates a list of candidates C(q) for an input 
query q, which are then ranked by the ranking 
function P(c|q). In previous work, gen(q) is typi-
cally generated by using an edit distance function 
or using a discriminative model trained for its 
own purpose (Okazaki et al, 2008b), often in 
combination with a pre-complied lexicon. In the 
current work, we generate the list of candidates 
by learning pairs of queries and their re-write 
candidates automatically from query session and 
click logs, which is far more robust and efficient 
than using edit distance functions. We describe 
our candidate generation method in detail in Sec-
tion 5.1.  
Unlike the spelling correction and normaliza-
tion tasks, our goal is to identify term variations, 
i.e., to determine whether each query-candidate 
pair (q,c) constitutes a term variation or not. We 
formulate this problem as a binary classification 
task. There are various choices of classifiers for 
such a task: we chose to build two types of clas-
sifiers that make a binary decision based on the 
probability distribution P(c|q) over a set of fea-
ture functions fi(q,c). In maximum entropy 
framework, this is defined as:  
? ? ? =
exp ???? ?, ? 
?
?=1
 exp ???? ?, ? 
?
?=1?
 
where ?1,?, ?k are the feature weights. The op-
timal set of feature weights ?* is computed by 
maximizing the log-likelihood of the training 
data. We used stochastic gradient descent for 
training the model with a Gaussian prior.   
The second classifier is built on MART 
(Friedman, 2001), which is a boosting algorithm. 
At each boosting iteration, MART builds a re-
gression tree to model the functional gradient of 
the cost function (which is cross entropy in our 
case), evaluated on all training samples.  MART 
has three main parameters: M, the total number 
of boosting iterations, L, the number of leaf 
nodes for each regression tree, and v, the learning 
rate. The optimal values of these parameters can 
be chosen based on performance on a validation 
set.  In our experiments, we found that the per-
formance of the algorithm is relatively insensi-
tive to these parameters as long as they are in a 
reasonable range: given the training set of a few 
thousand samples or more, as in our experiments, 
M=100, L=15, and v=0.1 usually give good per-
formance. Smaller trees and shrinkage may be 
used if the training data set is smaller. 
The classifiers output a binary decision ac-
cording to P(c|q): positive when P(c|q) > 0.5 and 
negative otherwise.  
5 Experiments 
5.1 Candidate Generation 
We used a set of Japanese query logs collected 
over one year period in 2007 and 2008. More 
specifically, we used two different extracts of log 
data for generating term variation candidate 
pairs:  
Query session data. From raw query logs, we 
extracted pairs of queries q1 and q2 such that they 
are (i) issued by the same user; (ii) q2 follows 
within 3 minutes of issuing q1; and (iii) q2 gener-
ated at least one click of a URL on the result 
1487
page while q1 did not result in any click. We then 
scored each query pair (q1,q2) in this subset using 
the log-likelihood ratio (LLR, Dunning, 1993) 
between q1 and q2, which measures the mutual 
dependence within the context of web search 
queries (Jones et al, 2006a). After applying an 
LLR threshold (LLR > 15) and a count cutoff 
(we used only the top 15 candidate q2 according 
to the LLR value for each q1), we obtained a list 
of 47,139,976 pairs for the 14,929,497 distinct q1, 
on average generating 3.2 candidates per q1
3. We 
took this set as comprising query-candidate pairs 
for our model, along with the set extracted by 
click-through data mining explained below.  
Click-through data. This data extract is based 
on the idea that if two queries led to the same 
URLs being repeatedly clicked, we can reasona-
bly infer that the two queries are semantically 
related. This is similar to computing the distribu-
tional similarity of terms given the context in 
which they appear, where context is most often 
defined as the words co-occurring with the terms. 
Here, the clicked URLs serve as their context.  
One challenge in using the URLs as contex-
tual information is that the contextual representa-
tion in this format is very sparse, as user clicks 
are rare events. To learn query similarities from 
incomplete click-through data, we used the ran-
dom walk algorithm similar to the one described 
in Craswell and Szummer (2007). Figure 1 illu-
strates the basic idea: initially, document ?3 has 
a click-through link consisting of query ?2 only; 
the random walk algorithm adds the link from ?3 
to ?1 , which has a similar click pattern as ?2 . 
Formally, we construct a click graph which is a 
bipartite-graph representation of click-through 
data. We use  ?? ?=1
?  to represent a set of query 
nodes and  ??  ?=1
?
 a set of document nodes. We 
further define an  ? ? ? matrix ? in which ele-
ment ???  represents the click count associated 
with  ?? ,??  . This matrix can be normalized to 
be a query-to-document transition matrix, de-
                                                 
3 We consider each query as an unbreakable term in this 
paper, so term variation is equivalent to query variation. 
noted by ?, where ??? = ?
(1)(?? |??) is the prob-
ability that ??  transits to ??  in one step. Similarly, 
we can normalize the transpose of ?  to be a 
document-to-query transition matrix, denoted by 
?, where ?? ,? = ?
(1)(??|?? ). It is easy to see that 
using ? and ? we can compute the probability of 
transiting from any node to any other node in ? 
steps. In this work, we use a simple measure 
which is the probability that one query transits to 
another in two steps, and the corresponding 
probability matrix is given by ??.  
We used this probability and ranked all pairs 
of queries in the same raw query logs as in the 
query session data described above to generate 
additional candidates for term variation pairs. 
20,308,693 pairs were extracted after applying 
the count cutoff of 5, generating on average 6.8 
candidates for 2,973,036 unique queries. 
It is interesting to note that these two data ex-
tracts are quite complementary: of all the data 
generated, only 4.2% of the pairs were found in 
both the session and click-through data. We be-
lieve that this diversity is attributable to the na-
ture of the extracts: the session data tends to col-
lect the term pairs that are issued by the same 
user as a result of conscious re-writing effort, 
such as typing error corrections and query speci-
fications (Categories 1 and 11 in Table 1), while 
the click-though data collects the terms issued by 
different users, possibly with different intentions, 
and tends to include many spelling variants, syn-
onyms and queries with different specificity 
(Categories 2, 8, 10 and 11).  
5.2 Features 
We used the same set of features for the maxi-
mum entropy and MART models, which are giv-
en in Table 2. They are divided into three main 
types: string similarity features (1-16), semantic 
similarity features (17, 18), and character type 
features (19-39). Among the string similarity 
features, half of them are based on Levenshtein 
distance applied to surface forms (1-8), while the 
other half is based on Levenshtein and string-to-
string edit distance metrics computed over the 
Romanized form of the query, reflecting its pro-
nunciation. The conversion into Roman charac-
ters was done deterministically for kana charac-
ters using a simple mapping table. For Romaniz-
ing kanji characters, we used the function availa-
ble from Windows IFELanguage API (version 
 
Figure 1. Random Walk Algorithm 
1488
2).4 The character equivalence table mentioned in 
the features 3,4,7,8 is a table of 643 pairs of cha-
racters that are known to be equivalent, including 
kanji allography (same kanji in different graphi-
cal styles). The alpha-beta edit distance (11, 12, 
15, 16) is the string-to-string edit distance pro-
posed in Brill and Moore (2001), which we 
trained over about 60K parallel English-to-
katakana Wikipedia title pairs, specifically to 
capture the edit operations between English and 
katakana words, which are different from the edit 
operations between two Japanese words. Seman-
tic similarity features (17, 18) use the LLR score 
from the session data, and the click-though pair 
probability described in the subsection above. 
Finally, features 19-39 capture the script types of 
the query-candidate pair. We first defined six 
basic character types for each query or candidate: 
Hira (hiragana only), Kata (katakana only), Kanji 
(kanji only), Roman (Roman alphabet only), 
MixedNoKanji (includes more than one charac-
ter sets but not kanji) and Mixed (includes more 
than one character sets with kanji). We then de-
rived 21 binary features by concatenating these 
basic character type features for the combination 
                                                 
4 http://msdn.microsoft.com/en-us/library/ms970129.aspx. 
We took the one-best conversion result from the API. The 
conversion accuracy on a randomly sampled 100 kanji que-
ries was 89.6%.  
of query and candidate strings. For example, if 
both the query and candidate are in hiragana, 
BothHira will be on; if the query is Mixed and 
the candidate is Roman, then RomanMixed will 
be on. Punctuation characters and Arabic numer-
als were treated as being transparent to character 
type assignment. The addition of these features is 
motivated by the assumption that appropriate 
types of edit distance operations might depend 
on different character types for the query-
candidate pair.  
Since the dynamic ranges of different features 
can be drastically different, we normalized each 
feature dimension to a normal variable with zero-
mean and unit-variance. We then used the same 
normalized features for both the maximum en-
tropy and the MART classifiers. 
5.3 Training and Evaluation Data 
In order to generate the training data for the bi-
nary classification task, we randomly sampled 
the query session (5,712 samples) and click-
through data (6,228 samples), and manually la-
beled each pair as positive or negative: the posi-
tive label was assigned when the term pair fell 
into Categories 1 through 5 in Table 1; otherwise 
it was assigned a negative label. Only 364 (6.4%) 
and 244 (3.9%) of the samples were positive ex-
amples for the query session and click-through 
data respectively, which makes the baseline per-
String similarity features (16 real-valued features) 
1. Lev distance on surface form 
2. Lev distance on surface form normalized by q1 length 
3. Lev distance on surface form using character equivalence table 
4. Lev distance on surface form normalized by  q1 length using character equivalence table 
5. Lev distance on surface form w/o space 
6. Lev distance on surface form normalized q1 length w/o space 
7. Lev distance on surface form using  character equivalence table w/o space 
8. Lev distance on surface form normalized by q1 using character equivalence table  w/o space 
9. Lev distance on Roman 
10. Lev distance on Roman normalized by q1 length 
11. Alpha-beta edit distance on Roman 
12. Alpha-beta edit distance on Roman normalized by q1 length 
13. Lev distance  on Roman w/o space 
14. Lev distance  on Roman normalized by q1 length w/o space 
15. Alpha-beta edit distance on Roman w/o space 
16. Alpha-beta edit distance on Roman normalized by q1 length w/o space 
Features for semantic similarity (2 real-valued features) 
17. LLR score 
18. Click-though data probability 
Character type features (21 binary features) 
19. BothHira, 20. BothKata, 21. BothRoman, 22. BothKanji, 23. BothMixedNoKanji, 24. BothMixed,  
25. HiraKata, 26. HiraKanji, 27. HiraRoman, 28. HiraMixedNoKanji, 29. HiraMixed, 30. KataKanji, 
31.KataRoman, 32. KataMixedNoKanji, 33. KataMixed, 34. KanjiRoman, 35. KanjiMixedNoKanji,  
36. KanjiMixed, 37. RomanMixedNoKanji, 38. RomanMixed, 39. MixedNoKanjiMixed 
Table 2: Classifier Features 
1489
formance of the classifier quite high (always 
predict the negative label ? the accuracy will be 
95%). Note, however, that these data sets include 
term variation candidates much more efficiently 
than a candidate set generated by the standard 
method that uses an edit distance function with a 
threshold. For example, there is a query-
candidate pair q=???? kafuujouhou 'house-
style information' c= ? ? ? ?  kafunjouhou 
'pollen information') in the session data extract, 
the first one of which is likely to be a mis-
spelling of the second.5 If we try to find candi-
dates for the query ???? using an edit dis-
tance function naively with a threshold of 2 from 
the queries in the log, we end up collecting a 
large amount of completely irrelevant set of can-
didates such as ???? taifuujouhou 'typhoon 
information', ??? kabu jouhou 'stock informa-
tion', ???? kouu jouhou 'rainfall information' 
and so on ? as many as 372 candidates were 
found in the top one million most frequent que-
ries in the query log from the same period; for 
rarer queries these numbers will only be worse. 
Computing the edit distance based on the pro-
nunciation will not help here: the examples 
above are within the edit distance of 2 even in 
terms of Romanized strings.  
Another advantage of generating the annotated 
data using the result of query log data mining is 
that the annotation process is less prone to sub-
jectivity than creating the annotation from 
scratch. As Cucerzan and Brill (2004) point out, 
the process of manually creating a spelling cor-
rection candidate is seriously flawed as the inten-
tion of the original query is completely lost: for 
the query gogle, it is not clear out of context if 
the user meant goggle, google, or gogle. Using 
data mined from query logs solves this problem: 
an annotator can safely assume that if gogle-
goggle appears in the candidate set, it is very 
likely to be a valid term variation intended by the 
user. This makes the annotation more robust and 
efficient: the inter-annotator agreement rate for 
2,000 query pairs by two annotators was 95.7% 
on our data set, each annotator spending only 
about two hours to annotate 2,000 pairs.  
5.4 Results and Discussion 
In order to compare the performance of two clas-
sifiers, we first built maximum entropy and 
MART classifiers as described in Section 4 using 
                                                 
5 ???? does not make any sense in Japanese; on the 
other hand, information about cedar pollen is commonly 
sought after in spring due to widespread pollen allergy.  
all the features in Section 5.2. We run five expe-
riments using different random split of training 
and test data: in each run,  we used 10,000 sam-
ples for training and the remaining 1,940 samples 
for testing, and measured the performance of the 
two classifiers on the task of term variation iden-
tification in terms of the error rate i.e., 1?
accuracy. The results, average over five runs, 
were 4.18 for the maximum entropy model, and 
3.07 for the MART model. In all five runs, the 
MART model outperformed the maximum en-
tropy classifier. This is not surprising given the 
superior performance of tree-boosting algorithms 
previously reported on similar classification 
tasks (e.g., Hastie et al, 2001). In our task where 
different types of features are likely to perform 
better when they are combined (such as semantic 
features and character types features), MART 
would be a better fit than linear classifiers  be-
cause the decision trees generated by MART op-
timally combines features in the local sense. In 
what follows, we only discuss the results pro-
duced by MART for further experiments. Note 
that the baseline classifier, which always predicts 
the label to be negative, achieves 95.04% in ac-
curacy (or 4.96% error rate), which sounds ex-
tremely high, but in fact this baseline classifier is 
useless for the purpose of collecting term varia-
tions, as it learns none of them by classifying all 
samples as negative.  
For evaluating the contribution of different 
types of features in Section 5.2, we performed 
feature ablation experiments using MART. Table 
3 shows the results in error rate by various 
MART classifiers using different combination of 
features. The results in this table are also aver-
aged over five run with random training/test data 
split. From Table 3, we can see that the best per-
formance was achieved by the model using all 
features (line A of the table), which reduces the 
baseline error rate (4.96%) by 38%. The im-
provement is statistically significant according to 
the McNemar test (P < 0.05). Models that use 
string edit distance features only (lines B and C) 
did not perform well: in particular, the model 
that uses surface edit distance features only 
Features Error rate (%) 
A. All features (1-39 in Table 2) 3.07 
B. String features only (1-16) 3.49 
C. Surface string features only (1-8) 4.9 
D. No semantic feats (1-16,19-39) 3.28 
E. No character type feats (1-18) 3.5 
Table 3: Results of Features Ablation Experiments 
Using MART Model 
1490
without considering the term pronunciation per-
formed horribly (line C), which confirms the re-
sults reported by Jones et al (2006b). However, 
unlike Jones et al (2006b), we see a positive 
contribution of semantic features: the use of se-
mantic features reduced the error rate from 3.28 
(line D) to 3.07 (line A), which is statistically 
significant. This may be attributable to the nature 
of semantic information used in our experiments: 
we used the user session and click-though data to 
extract semantic knowledge, which may be se-
mantically more specific than the probability of 
word substitution in a query collection as a 
whole, which is used by Jones et al (2006b). 
Finally, the character type features also contri-
buted to reducing the error rate (lines A and E). 
In particular, the observation that the addition of 
semantic features without the character type fea-
tures (comparing lines B and E) did not improve 
the error rate indicates that the character type 
features are also important in bringing about the 
contribution of semantic features.   
Figure 2 displays the test data precision/recall 
curve of one of the runs of MART that uses all 
features. The x-axis of the graph is the confi-
dence score of classification P(c|q), which was 
set to 0.5 for the results in Table 3. At this confi-
dence, the model achieves 70% precision with 
the recall slightly higher than 60%. In the graph, 
we observe a familiar trade-off between preci-
sion and recall, which is useful for practical ap-
plications that may favor one over the other.  
In order to find out where the weaknesses of 
our classifiers lie, we performed a manual error 
analysis on the same MART run whose results 
are shown in Figure 2. Most of the classification 
errors are false negatives, i.e., the model failed to 
predict a case of term variation as such. The most 
conspicuous error is the failure to capture ab-
breviations, such as failing to capture the altera-
tion between ?????  juujoochuugakkou 
'Juujoo middle school' and ??? juujoochuu, 
which our edit distance-based features fail as the 
length difference between a term and its abbrevi-
ation is significant. Addition of more targeted 
features for this subclass of term variation (e.g., 
Okazaki et al, 2008a) is called for, and will be 
considered in future work. Mistakes in the Ro-
manization of kanji characters were not always 
punished as the query and the candidate string 
may contain the same mistake, but when they 
occurred in either in the query or the candidate 
string (but not in both), the result was destruc-
tive: for example, we assigned a wrong Romani-
zation on ??? as suiginnakari ?mercury lamp?, 
as opposed to the correct suiginntou, which caus-
es the failure to capture the alteration with ??
? suiginntou, (a misspelling of ???). Using 
N-best (N>1) candidate pronunciations for kanji 
terms or using all possible pronunciations for 
kanji characters might reduce this type of error. 
Finally, the features of our models are the edit 
distance functions themselves, rather than the 
individual edit rules or operations. Using these 
individual operations as features in the classifica-
tion task directly has been shown to perform well 
on spelling correction and normalization tasks 
(e.g., Brill and Moore, 2000; Okazaki et al, 
2008b). Okazaki et al?s (2008b) method of gene-
rating edit operations may not be viable for our 
purposes, as they assume that the original and 
candidate strings are very similar in their surface 
representation ? they target only spelling variants 
and inflection in English. One interesting future 
avenue to consider is to use the edit distance 
functions in our current model to select a subset 
of query-candidate pairs that are similar in terms 
of these functions, separately for the surface and 
Romanized forms, and use this subset to align 
the character strings in these query-candidate 
pairs as described in Brill and Moore (2000), and 
add the edit operations derived in this manner to 
the term variation identification classifier as fea-
tures.  
6 Conclusion 
In this paper we have addressed the problem of 
acquiring term variations in Japanese query logs 
for the purpose of query expansion. We generate 
term variation candidates efficiently by mining 
query log data, and our best classifier, based on 
the MART algorithm, can make use of both edit-
distance-based and semantic features, and can 
identify term variation with the precision of 70% 
at the recall slightly higher than 60%. Our next 
 
Figure 2: Precision/Recall Curve of MART 
0
10
20
30
40
50
60
70
80
90
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
confidence
p
r
e
c
is
io
n
/
r
e
c
a
ll
 
(
%
)
precision
recall
1491
goal is to use and evaluate the term variation col-
lected by the proposed method in an actual 
search scenario, as well as improving the per-
formance of our classifier by using individual, 
character-dependent edit operations as features in 
classification.  
 
References  
Ahmad, Farooq, and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. 
In Proceedings of EMNLP, pp.955-962.  
Aitchison, J. and A. Gilchrist. 1987. Thesaurus Con-
struction: A Practical Manual. Second edition. 
ASLIB, London. 
Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazu-
hiko Ohe. 2008. Orthographic disambiguation in-
corporating transliterated probability. In Proceed-
ings of IJCNLP, pp.48-55. 
Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 
1999. Modern Information Retrieval. Addison 
Wesley. 
Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra. 1996. 
A maximum entropy approach to natural language 
processing. Computational Linguistics, 22(1): 39-
72. 
Bilac, Slaven, and Hozumi Tanaka. 2004. A hybrid 
back-transliteration system for Japanese. In Pro-
ceedings of COLING, pp.597-603. 
Brill, Eric, Gary Kacmarcik and Chris Brockett. 2001. 
Automatically harvesting katakana-English term 
pairs from search engine query logs. In Proceed-
ings of the Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS-2001), pp.393-399. 
Brill, Eric, and Robert C. Moore. 2000. An improved 
error model for noisy channel spelling. In Proceed-
ings of ACL, pp.286-293. 
Chen, Qing, Mu Li and Ming Zhou. 2007. Improving 
query spelling correction using web search results. 
In Proceedings of EMNLP-CoNLL, pp.181-189. 
Craswell, Nick, and Martin Szummer. 2007. Random 
walk on the click graph. In Proceedings of SIGIR. 
Cucerzan, Silviu, and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of 
EMNLP, pp.293-300. 
Deerwester, S., S.T. Dumais, T. Landauer and 
Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Infor-
mation Science, 41(6): 391-407. 
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational 
Linguistics, 19(1): 61-74. 
Frakes, W.B. 1992. Stemming algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
8. Prentice Hall. 
Friedman, J. 2001. Greedy function approximation: a 
gradient boosting machine. Annals of Statistics, 
29(5). 
Jones, Rosie, Benjamin Rey, Omid Madani and Wiley 
Greiner. 2006a. Generating query substitutions. In 
Proceedings of WWW, pp.387?396. 
Jones, Rosie, Kevin Bartz, Pero Subasic and Benja-
min Rey. 2006b. Automatically generating related 
aueries in Japanese. Language Resources and 
Evaluation 40: 219-232.  
Hastie, Trevor, Robert Tibshirani and Jerome Fried-
man. 2001. The Elements of Statistical Learning. 
Springer. 
Knight, Kevin, and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 24(4): 
599-612. 
Komachi, Mamoru and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge 
from query logs. In Proceedings of IJCNLP, 
pp.358?365. 
Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou. 
2006. Exploring distributional similarity based 
models for query spelling correction. In Proceed-
ings of COLING/ACL, pp.1025-1032. 
Li, Xiao, Ye-Yi Wang and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In 
Proceedings of SIGIR.  
Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Na-
kagawa. 2004. Automatic construction of Japanese 
katakana variant list from large corpus. In Proceed-
ings COLING, pp.1214-1219. 
Okazaki, Naoaki, Mitsuru Ishizuka and Jun?ichi Tsujii. 
2008a. A discriminative approach to Japanese ab-
breviation extraction. In Proceedings of IJCNLP.  
Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ana-
niadou and Jun?ichi Tsujii. 2008b. A discriminative 
candidate generator for string transformations. In 
Proceedings of EMNLP.  
Rasmussen, E. 1992. Clustering algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
16. Prentice Hall. 
Salton, G., and C. Buckley. 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 41(4): 
288-297. 
Sekine, Satoshi, and Hisami Suzuki. 2007. Acquiring 
ontological knowledge from query logs. In Pro-
ceedings of WWW, pp.1223-1224 
1492
Using Contextual Speller Techniques and Language Modeling for 
ESL Error Correction 
Michael Gamon*, Jianfeng Gao*, Chris Brockett*, Alexandre Klementiev+, William 
B. Dolan*, Dmitriy Belenko*, Lucy Vanderwende* 
 
*Microsoft Research 
One Microsoft Way 
Redmond, WA 98052 
{mgamon,jfgao,chrisbkt,billdol, 
dmitryb,lucyv}@microsoft.com 
+Dept. of Computer Science 
University of Illinois 
Urbana, IL 61801 
klementi@uiuc.edu 
 
 
Abstract 
We present a modular system for detection 
and correction of errors made by non-
native (English as a Second Language = 
ESL) writers. We focus on two error types: 
the incorrect use of determiners and the 
choice of prepositions. We use a decision-
tree approach inspired by contextual 
spelling systems for detection and 
correction suggestions, and a large 
language model trained on the Gigaword 
corpus to provide additional information to 
filter out spurious suggestions. We show 
how this system performs on a corpus of 
non-native English text and discuss 
strategies for future enhancements. 
1 Introduction 
English is today the de facto lingua franca for 
commerce around the globe. It has been estimated 
that about 750M people use English as a second 
language, as opposed to 375M native English 
speakers (Crystal 1997), while as much as 74% of 
writing in English is done by non-native speakers. 
However, the errors typically targeted by 
commercial proofing tools represent only a subset 
of errors that a non-native speaker might make. For 
example, while many non-native speakers may 
encounter difficulty choosing among prepositions, 
this is typically not a significant problem for native 
speakers and hence remains unaddressed in 
proofing tools such as the grammar checker in 
Microsoft Word (Heidorn 2000). Plainly there is an 
opening here for automated proofing tools that are 
better geared to the non-native users.  
One challenge that automated proofing tools 
face is that writing errors often present a semantic 
dimension that renders it difficult if not impossible 
to provide a single correct suggestion. The choice 
of definite versus indefinite determiner?a 
common error type among writers with a Japanese, 
Chinese or Korean language background owing to 
the lack of overt markers for definiteness and 
indefiniteness?is highly dependent on larger 
textual context and world knowledge. It seems 
desirable, then, that proofing tools targeting such 
errors be able to offer a range of plausible 
suggestions, enhanced by presenting real-world 
examples that are intended to inform a user?s 
selection of the most appropriate wording in the 
context1. 
2 Targeted Error Types 
Our system currently targets eight different error 
types: 
1. Preposition presence and choice: 
In the other hand, ... (On the other hand ...) 
2. Definite and indefinite determiner presence 
and choice: 
 I am teacher... (am a teacher) 
3. Gerund/infinitive confusion: 
I am interesting in this book. (interested in) 
4. Auxiliary verb presence and choice: 
My teacher does is a good teacher (my teacher 
is...) 
                                                 
1 Liu et al 2000 take a similar approach, retrieving 
example sentences from a large corpus. 
449
5. Over-regularized verb inflection: 
 I writed a letter (wrote) 
6. Adjective/noun confusion: 
 This is a China book (Chinese book) 
7. Word order (adjective sequences and nominal 
compounds): 
I am a student of university (university student) 
8. Noun pluralization: 
 They have many knowledges (much knowledge) 
In this paper we will focus on the two most 
prominent and difficult errors: choice of 
determiner and prepositions. Empirical 
justification for targeting these errors comes from 
inspection of several corpora of non-native writing. 
In the NICT Japanese Learners of English (JLE) 
corpus (Izumi et al 2004), 26.6% of all errors are 
determiner related, and about 10% are preposition 
related, making these two error types the dominant 
ones in the corpus. Although the JLE corpus is 
based on transcripts of spoken language, we have 
no reason to believe that the situation in written 
English is substantially different. The Chinese 
Learners of English Corpus (CLEC, Gui and Yang 
2003) has a coarser and somewhat inconsistent 
error tagging scheme that makes it harder to isolate 
the two errors, but of the non-orthographic errors, 
more than 10% are determiner and number related. 
Roughly 2% of errors in the corpus are tagged as 
preposition-related, but other preposition errors are 
subsumed under the ?collocation error? category 
which makes up about 5% of errors. 
3 Related Work 
Models for determiner and preposition selection 
have mostly been investigated in the context of 
sentence realization and machine translation 
(Knight and Chander 1994, Gamon et al 2002,  
Bond 2005, Suzuki and Toutanova 2006, 
Toutanova and Suzuki 2007). Such approaches 
typically rely on the fact that preposition or 
determiner choice is made in otherwise native-like 
sentences. Turner and Charniak (2007), for 
example, utilize a language model based on a 
statistical parser for Penn Tree Bank data. 
Similarly, De Felice and Pulman (2007) utilize a 
set of sophisticated syntactic and semantic analysis 
features to predict 5 common English prepositions. 
Obviously, this is impractical in a setting where 
noisy non-native text is subjected to proofing. 
Meanwhile, work on automated error detection on 
non-native text focuses primarily on detection of 
errors, rather than on the more difficult task of 
supplying viable corrections (e.g., Chodorow and 
Leacock, 2000). More recently,  Han et al (2004, 
2006) use a maximum entropy classifier to propose 
article corrections in TESOL essays, while Izumi 
et al (2003) and Chodorow et al (2007) present 
techniques of automatic preposition choice 
modeling. These more recent efforts, nevertheless, 
do not attempt to integrate their methods into a 
more general proofing application designed to 
assist non-native speakers when writing English. 
Finally, Yi et al (2008) designed a system that 
uses web counts to determine correct article usage 
for a given sentence, targeting ESL users. 
4 System Description 
Our system consists of three major components: 
1. Suggestion Provider (SP) 
2. Language Model (LM) 
3. Example Provider (EP) 
The Suggestion Provider contains modules for 
each error type discussed in section 2. Sentences 
are tokenized and part-of-speech tagged before 
they are presented to these modules. Each module 
determines parts of the sentence that may contain 
an error of a specific type and one or more possible 
corrections. Four of the eight error-specific 
modules mentioned in section 2 employ machine 
learned (classification) techniques, the other four 
are based on heuristics. Gerund/infinitive 
confusion and auxiliary presence/choice each use a 
single classifier. Preposition and determiner 
modules each use two classifiers, one to determine 
whether a preposition/article should be present, 
and one for the choice of preposition/article. 
All suggestions from the Suggestion Provider 
are collected and passed through the Language 
Model. As a first step, a suggested correction has 
to have a higher language model score than the 
original sentence in order to be a candidate for 
being surfaced to the user. A second set of 
heuristic thresholds is based on a linear 
combination of class probability as assigned by the 
classifier and language model score. 
The Example Provider queries the web for 
exemplary sentences that contain the suggested 
correction. The user can choose to consult this 
information to make an informed decision about 
the correction. 
450
4.1 Suggestion Provider Modules for 
Determiners and Prepositions 
The SP modules for determiner and preposition 
choice are machine learned components. Ideally, 
one would train such modules on large data sets of 
annotated errors and corrected counterparts. Such a 
data set, however, is not currently available. As a 
substitute, we are using native English text for 
training, currently we train on the full text of the 
English Encarta encyclopedia (560k sentences) and 
a random set of 1M sentences from a Reuters news 
data set. The strategy behind these modules is 
similar to a contextual speller as described, for 
example, in (Golding and Roth 1999). For each 
potential insertion point of a determiner or 
preposition we extract context features within a 
window of six tokens to the right and to the left. 
For each token within the window we extract its 
relative position, the token string, and its part-of-
speech tag. Potential insertion sites are determined 
heuristically from the sequence of POS tags. Based 
on these features, we train a classifier for 
preposition choice and determiner choice. 
Currently we train decision tree classifiers with the 
WinMine toolkit (Chickering 2002). We also 
experimented with linear SVMs, but decision trees 
performed better overall and training and 
parameter optimization were considerably more 
efficient. Before training the classifiers, we 
perform feature ablation by imposing a count 
cutoff of 10, and by limiting the number of features 
to the top 75K features in terms of log likelihood 
ratio (Dunning 1993). 
We train two separate classifiers for both 
determiners and preposition: 
? decision whether or not a 
determiner/preposition should be present 
(presence/absence or pa classifier) 
? decision which determiner/preposition is 
the most likely choice, given that a 
determiner/preposition is present (choice 
or ch classifier) 
In the case of determiners, class values for the ch 
classifier are a/an and the. Preposition choice 
(equivalent to the ?confusion set? of a contextual 
speller) is limited to a set of 13 prepositions that 
figure prominently in the errors observed in the 
JLE corpus: about, as, at, by, for, from, in, like, of, 
on, since, to, with, than, "other" (for prepositions 
not in the list). 
The decision tree classifiers produce probability 
distributions over class values at their leaf nodes. 
For a given leaf node, the most likely 
preposition/determiner is chosen as a suggestion. If 
there are other class values with probabilities 
above heuristically determined thresholds2, those 
are also included in the list of possible suggestions. 
Consider the following example of an article-
related error: 
I am teacher from Korea. 
As explained above, the suggestion provider 
module for article errors consists of two classifiers, 
one for presence/absence of an article, the other for 
article choice. The string above is first tokenized 
and then part-of-speech tagged: 
0/I/PRP   1/am/VBP   2/teacher/NN   3/from/IN   
4/Korea/NNP   5/./.  
Based on the sequence of POS tags and 
capitalization of the nouns, a heuristic determines 
that there is one potential noun phrase that could 
contain an article: teacher. For this possible article 
position, the article presence/absence classifier 
determines the probability of the presence of an 
article, based on a feature vector of pos tags and 
surrounding lexical items: 
p(article + teacher) = 0.54 
Given that the probability of an article in this 
position is higher than the probability of not having 
an article, the second classifier is consulted to 
provide the most likely choice of article: 
p(the) = 0.04 
p(a/an) = 0.96 
Given  this probability distribution, a correction 
suggestion I am teacher from Korea -> I am a 
teacher from Korea is generated and passed on to 
evaluation by the language model component. 
4.2 The Language Model 
The language model is a 5-gram model trained 
on the English Gigaword corpus (LDC2005T12). 
In order to preserve (singleton) context information 
as much as possible, we used interpolated Kneser-
Ney smoothing (Kneser and Ney 1995) without 
count cutoff. With a 120K-word vocabulary, the 
trained language model contains 54 million 
bigrams, 338 million trigrams, 801 million 4-grams 
                                                 
2 Again, we are working on learning these thresholds 
empirically from data. 
451
and 12 billion 5-grams.  In the example from the 
previous section, the two alternative strings  of the 
original user input and the suggested correction are 
scored by the language model: 
I am teacher from Korea. score = 0.19 
I am a teacher from Korea. score = 0.60 
The score for the suggested correction is 
significantly higher than the score for the original, 
so the suggested correction is provided to the user. 
4.3 The Example Provider 
In many cases, the SP will produce several 
alternative suggestions, from which the user may 
be able to pick the appropriate correction reliably. 
In other cases, however, it may not be clear which 
suggestion is most appropriate. In this event, the 
user can choose to activate the Example Provider 
(EP) which will then perform a web search to 
retrieve relevant example sentences illustrating the 
suggested correction. For each suggestion, we 
create an exact string query including a small 
window of context to the left and to the right of the 
suggested correction. The query is issued to a 
search engine, and the retrieved results are 
separated into sentences. Those sentences that 
contain the string query are added to a list of 
example candidates.  The candidates are then 
ranked by two initially implemented criteria: 
Sentence length (shorter examples are preferred in 
order to reduce cognitive load) and context overlap 
(sentences that contain additional words from the 
user input are preferred). We have not yet 
performed a user study to evaluate the usefulness 
of the examples provided by the system. Some 
examples of usage that we retrieve are given below 
with the query string in boldface: 
Original: I am teacher from Korea. 
Suggestion: I am a teacher from Korea. 
All top 3 examples: I am a teacher.  
Original: So Smokers have to see doctor more often 
than non-smokers. 
Suggestion: So Smokers have to see a doctor more 
often than non-smokers. 
Top 3 examples: 
1. Do people going through withdrawal have 
to see a doctor? 
2. Usually, a couple should wait to see a 
doctor until after they've tried to get 
pregnant for a year. 
3. If you have had congestion for over a 
week, you should see a doctor. 
Original: I want to travel Disneyland in March. 
Suggestion: I want to travel to Disneyland in 
March. 
Top 3 examples: 
1. Timothy's wish was to travel to 
Disneyland in California. 
2. Should you travel to Disneyland in 
California or to Disney World in 
Florida? 
3. The tourists who travel to Disneyland in 
California can either choose to stay in 
Disney resorts or in the hotel for 
Disneyland vacations. 
5 Evaluation 
We perform two different types of evaluation on 
our system. Automatic evaluation is performed on 
native text, under the assumption that the native 
text does not contain any errors of the type targeted 
by our system. For example, the original choice of 
preposition made in the native text would serve as 
supervision for the evaluation of the preposition 
module. Human evaluation is performed on non-
native text, with a human rater assessing each 
suggestion provided by the system. 
5.1 Individual SP Modules 
For evaluation, we split the original training data 
discussed in section 4.1 into training and test sets 
(70%/30%). We then retrained the classifiers on 
this reduced training set and applied them to the 
held-out test set. Since there are two models, one 
for preposition/determiner presence and absence 
(pa), and one for preposition/determiner choice 
(ch), we report combined accuracy numbers of the 
two classifiers. Votes(a) stands for the counts of 
votes for class value = absence from pa, votes(p) 
stands for counts of votes for presence from pa. 
Acc(pa) is the accuracy of the pa classifier, acc(ch) 
the accuracy of the choice classifier. Combined 
accuracy is defined as in Equation 1. 
 
 
??? ?? ? ?????(?) + ??? ?? ? ??? ?? ? ?????(?)
????? ?????
 
Equation 1: Combined accuracy of the 
presence/absence and choice models 
452
The total number of cases in the test set is 
1,578,342 for article correction and 1,828,438 for 
preposition correction. 
5.1.1 Determiner choice 
Accuracy of the determiner pa and ch models 
and their combination is shown in Table 1. 
Model pa ch combined 
Accuracy 89.61% 85.97% 86.07% 
Table 1: Accuracy of the determiner pa, ch, and 
combined models. 
The baseline is 69.9% (choosing the most 
frequent class label none). The overall accuracy of 
this module is state-of-the-art compared with 
results reported in the literature (Knight and 
Chander 1994, Minnen et al 2000, Lee 2004, 
Turner and Charniak 2007). Turner and Charniak 
2007 obtained the best reported accuracy to date of 
86.74%, using a Charniak language model 
(Charniak 2001) based on a full statistical parser 
on the Penn Tree Bank. These numbers are, of 
course, not directly comparable, given the different 
corpora. On the other hand, the distribution of 
determiners is similar in the PTB (as reported in 
Minnen et al 2000) and in our data (Table 2). 
 PTB Reuters/Encarta 
mix 
no determiner 70.0% 69.9% 
the 20.6% 22.2% 
a/an 9.4% 7.8% 
Table 2: distribution of determiners in the Penn 
Tree Bank and in our Reuters/Encarta data. 
Precision and recall numbers for both models on 
our test set are shown in Table 3 and Table 4. 
Article 
pa classifier 
precision recall 
presence 84.99% 79.54% 
absence 91.43% 93.95% 
Table 3: precision and recall of the article pa 
classifier. 
Article  
ch classifier 
precision Recall 
the 88.73% 92.81% 
a/an 76.55% 66.58% 
Table 4: precision and recall of the article ch 
classifier. 
5.1.2 Preposition choice 
The preposition choice model and the combined 
model achieve lower accuracy than the 
corresponding determiner models, a result that can 
be expected given the larger choice of candidates 
and hardness of the task. Accuracy numbers are 
presented in Table 5. 
Model pa ch combined 
Accuracy 91.06%% 62.32% 86.07% 
Table 5:Accuracy of the preposition pa, ch, and 
combined models. 
The baseline in this task is 28.94% (using no 
preposition). Precision and recall numbers are 
shown in Table 6 and Table 7. From Table 7 it is 
evident that prepositions show a wide range of 
predictability. Prepositions such as than and about 
show high recall and precision, due to the lexical 
and morphosyntactic regularities that govern their 
distribution. At the low end, the semantically more 
independent prepositions since and at show much 
lower precision and recall numbers. 
 
Preposition  
pa classifier 
precision recall 
presence 90.82% 87.20% 
absence 91.22% 93.78% 
Table 6: Precision and recall of the preposition pa 
classifier. 
Preposition 
ch classifier 
precision recall 
other 53.75% 54.41% 
in 55.93% 62.93% 
for 56.18% 38.76% 
of 68.09% 85.85% 
on 46.94% 24.47% 
to 79.54% 51.72% 
with 64.86% 25.00% 
at 50.00% 29.67% 
by 42.86% 60.46% 
as 76.78% 64.18% 
from 81.13% 39.09% 
since 50.00% 10.00% 
about 93.88% 69.70% 
than 95.24% 90.91% 
Table 7: Precision and recall of the preposition ch 
classifier. 
 
453
Chodorow et al (2007) present numbers on an 
independently developed system for detection of 
preposition error in non-native English. Their 
approach is similar to ours in that they use a 
classifier with contextual feature vectors.  The 
major differences between the two systems are the 
additional use of a language model in our system 
and, from a usability perspective, in the example 
provider module we added to the correction 
process. Since both systems are evaluated on 
different data sets3, however, the numbers are not 
directly comparable. 
5.2 Language model Impact 
The language model gives us an additional piece 
of information to make a decision as to whether a 
correction is indeed valid. Initially, we used the 
language model as a simple filter: any correction 
that received a lower language model score than 
the original was filtered out. As a first approxi-
mation, this was an effective step: it reduced the 
number of preposition corrections by 66.8% and 
the determiner corrections by 50.7%, and increased 
precision dramatically. The language model alone, 
however, does not provide sufficient evidence: if 
we produce a full set of preposition suggestions for 
each potential preposition location and rank these 
suggestions by LM score alone, we only achieve 
58.36% accuracy on Reuters data. 
Given that we have multiple pieces of 
information for a correction candidate, namely the 
class probability assigned by the classifier and the 
language model score, it is more effective to 
combine these into a single score and impose a 
tunable threshold on the score to maximize 
precision. Currently, this threshold is manually set 
by analyzing the flags in a development set. 
5.3 Human Evaluation 
A complete human evaluation of our system would 
have to include a thorough user study and would 
need to assess a variety of criteria, from the 
accuracy of individual error detection and 
corrections to the general helpfulness of real web-
based example sentences. For a first human 
evaluation of our system prototype, we decided to 
                                                 
3 Chodorow et al (2007) evaluate their system on 
proprietary student essays from non-native students, 
where they achieve 77.8% precision at 30.4% recall for 
the preposition substitution task. 
simply address the question of accuracy on the 
determiner and preposition choice tasks on a 
sample of non-native text.  
For this purpose we ran the system over a 
random sample of sentences from the CLEC 
corpus (8k for the preposition evaluation and 6k 
for the determiner evaluation). An independent 
judge annotated each flag produced by the system 
as belonging to one of the following categories: 
? (1) the correction is valid and fixes the 
problem 
? (2) the error is correctly identified, but 
the suggested correction does not fix it 
? (3) the original and the rewrite are both 
equally good 
? (4) the error is at or near the suggested 
correction, but it is a different kind of 
error (not having to do with 
prepositions/determiners) 
? (5) There is a spelling error at or near 
the correction 
? (6) the correction is wrong, the original 
is correct 
Table 8 shows the results of this human 
assessment for articles and prepositions. 
 
Articles (6k 
sentences) 
Prepositions 
(8k 
sentences) 
count ratio count ratio 
(1) correction is 
valid 
240 55% 165 46% 
(2) error identified, 
suggestion does 
not fix it 
10 2% 17 5% 
(3) original and 
suggestion equally 
good 
17 4% 38 10% 
(4) misdiagnosis 65 15% 46 13% 
(5) spelling error 
near correction 
37 8% 20 6% 
(6) original correct 70 16% 76 21% 
Table 8: Article and preposition correction 
accuracy on CLEC data. 
The distribution of corrections across deletion, 
insertion and substitution operations is illustrated 
in Table 9. The most common article correction is 
insertion of a missing article. For prepositions, 
substitution is the most common correction, again 
an expected result given that the presence of a 
454
preposition is easier to determine for a non-native 
speaker than the actual choice of the correct 
preposition. 
 deletion insertion substitution 
Articles 8% 79% 13% 
Prepositions 15% 10% 76% 
Table 9: Ratio of deletion, insertion and 
substitution operations. 
6 Conclusion and Future Work 
Helping a non-native writer of English with the 
correct choice of prepositions and 
definite/indefinite determiners is a difficult 
challenge. By combining contextual speller based 
methods with language model scoring and 
providing web-based examples, we can leverage 
the combination of evidence from multiple 
sources. 
The human evaluation numbers presented in the 
previous section are encouraging. Article and 
preposition errors present the greatest difficulty for 
many learners as well as machines, but can 
nevertheless be corrected even in extremely noisy 
text with reasonable accuracy. Providing 
contextually appropriate real-life examples 
alongside with the suggested correction will, we 
believe, help the non-native user reach a more 
informed decision than just presenting a correction 
without additional evidence and information. 
The greatest challenge we are facing is the 
reduction of ?false flags?, i.e. flags where both 
error detection and suggested correction are 
incorrect. Such flags?especially for a non-native 
speaker?can be confusing, despite the fact that the 
impact is mitigated by the set of examples which 
may clarify the picture somewhat and help the 
users determine that they are dealing with an 
inappropriate correction. In the current system we 
use a set of carefully crafted heuristic thresholds 
that are geared towards minimizing false flags on a 
development set, based on detailed error analysis. 
As with all manually imposed thresholding, this is 
both a laborious and brittle process where each 
retraining of a model requires a re-tuning of the 
heuristics. We are currently investigating a learned 
ranker that combines information from language 
model and classifiers, using web counts as a 
supervision signal. 
7 Acknowledgements 
We thank Claudia Leacock (Butler Hill Group) for 
her meticulous analysis of errors and human 
evaluation of the system output, as well as for 
much invaluable feedback and discussion. 
References 
Bond, Francis. 2005.  Translating the Untranslatable: A 
Solution to the Problem of Generating English 
Determiners. CSLI Publications. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In Proceedingsof the 39th Annual 
Meeting of the Association for Computational 
Linguistics, pp 116-123. 
Chickering, David Maxwell. 2002. The WinMine 
Toolkit.  Microsoft Technical Report 2002-103. 
Chodorow, Martin, Joel R. Tetreault and Na-Rae Han. 
2007. Detection of Grammatical Errors Involving 
Prepositions. In Proceedings of the 4th ACL-SIGSEM 
Workshop on Prepositions, pp 25-30. 
Crystal, David. 1997.  Global English. Cambridge 
University Press. 
Rachele De Felice and Stephen G Pulman. 2007. 
Automatically acquiring models of preposition use. 
Proceedings of the ACL-07 Workshop on 
Prepositions. 
Dunning, Ted. 1993. Accurate Methods for the Statistics 
of Surprise and Coincidence. Computational 
Linguistics, 19:61-74. 
Gamon, Michael, Eric Ringger, and Simon Corston-
Oliver. 2002. Amalgam: A machine-learned 
generation module. Microsoft Technical Report, 
MSR-TR-2002-57. 
Golding, Andrew R. and Dan Roth. 1999. A Winnow 
Based Approach to Context-Sensitive Spelling 
Correction. Machine Learning, pp. 107-130. 
Gui, Shicun and Huizhong Yang (eds.). 2003. Zhongguo 
Xuexizhe Yingyu Yuliaohu. (Chinese Learner English 
Corpus). Shanghai Waiyu Jiaoyu Chubanshe.. 
Han, Na-Rae., Chodorow, Martin and Claudia Leacock. 
2004. Detecting errors in English article usage with a 
maximum entropy classifier trained on a large, 
diverse corpus. Proceedings of the 4th international 
conference on language resources and evaluation, 
Lisbon, Portugal. 
 
 
455
Han, Na-Rae. Chodorow, Martin., and Claudia Leacock. 
(2006). Detecting errors in English article usage by 
non-native speakers. Natural Language Engineering, 
12(2), 115-129. 
Heidorn, George. 2000. Intelligent Writing Assistance. 
In Robert Dale, Herman Moisl, and Harold Somers 
(eds.). Handbook of Natural Language Processing.  
Marcel Dekker.  pp 181 -207. 
Izumi, Emi, Kiyotaka Uchimoto and Hitoshi Isahara. 
2004. The NICT JLE Corpus: Exploiting the 
Language Learner?s Speech Database for Research 
and Education. International Journal of the 
Computer, the Internet and Management 12:2, pp 
119 -125. 
Kneser, Reinhard. and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. 
Proceedings of the IEEE International Conference 
on Acoustics, Speech, and Signal Processing, volume 
1. 1995. pp. 181?184. 
Knight, Kevin and Ishwar Chander. 1994. Automatic 
Postediting of Documents. Proceedings of the 
American Association of Artificial Intelligence, pp 
779-784. 
Lee, John. 2004. Automatic Article Restoration. 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, pp. 31-
36. 
Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, and 
Changning Huan. 2000. PENS: A Machine-Aided 
English Writing System for Chinese Users. 
Proceedings of ACL 2000, pp 529-536. 
Minnen, Guido, Francis Bond and Ann Copestake. 
2000. Memory-Based Learning for Article 
Generation. Proceedings of the Fourth Conference 
on Computational Natural Language Learning and 
of the Second Learning Language in Logic 
Workshop, pp 43-48. 
Suzuki, Hisami and Kristina Toutanova. 2006. Learning 
to Predict Case Markers in Japanese. Proceedings of 
COLING-ACL, pp. 1049-1056. 
Toutanova, Kristina and Hisami Suzuki. 2007 
Generating Case Markers in Machine Translation.  
Proceedings of NAACL-HLT. 
Turner, Jenine and Eugene Charniak. 2007. Language 
Modeling for Determiner Selection. In Human 
Language Technologies 2007: The Conference of the 
North American Chapter of the Association for 
Computational Linguistics; Companion Volume, 
Short Papers, pp 177-180. 
Yi, Xing, Jianfeng Gao and William B. Dolan. 2008. 
Web-Based English Proofing System for English as a 
Second Language Users. To be presented at IJCNLP 
2008. 
456
A Web-based English Proofing System for English as a Second Language
Users
Xing Yi1, Jianfeng Gao2 and William B. Dolan2
1Center for Intelligent Information Retrieval, Department of Computer Science
University of Massachusetts, Amherst, MA 01003-4610, USA
yixing@cs.umass.edu
2Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA
{jfgao,billdol}@microsoft.com
Abstract
We describe an algorithm that relies on
web frequency counts to identify and correct
writing errors made by non-native writers of
English. Evaluation of the system on a real-
world ESL corpus showed very promising
performance on the very difficult problem of
critiquing English determiner use: 62% pre-
cision and 41% recall, with a false flag rate
of only 2% (compared to a random-guessing
baseline of 5% precision, 7% recall, and
more than 80% false flag rate). Performance
on collocation errors was less good, sug-
gesting that a web-based approach should be
combined with local linguistic resources to
achieve both effectiveness and efficiency.
1 Introduction
Proofing technology for native speakers of English
has been a focus of work for decades, and some
tools like spell checkers and grammar checkers have
become standard features of document processing
software products. However, designing an English
proofing system for English as a Second Language
(ESL) users presents a major challenge: ESL writ-
ing errors vary greatly among users with different
language backgrounds and proficiency levels. Re-
cent work by Brockett et al (2006) utilized phrasal
Statistical Machine Translation (SMT) techniques to
correct ESL writing errors and demonstrated that
this data-intensive SMT approach is very promising,
but they also pointed out SMT approach relies on the
availability of large amount of training data. The ex-
pense and difficulty of collecting large quantities of
Search Phrase Google.com Live.com Yahoo.com
English as
Second Language 306,000 52,407 386,000
English as a
Second Language 1,490,000 38,336,308 4,250,000
Table 1: Web Hits for Phrasal Usages
raw and edited ESL prose pose an obstacle to this
approach.
In this work we consider the prospect of using
the Web, with its billions of web pages, as a data
source with the potential to aid ESL writers. Our
research is motivated by the observation that ESL
users already use the Web as a corpus of good En-
glish, often using search engines to decide whether
a particular spelling, phrase, or syntactic construc-
tion is consistent with usage found on the Web. For
example, unsure whether the native-sounding phrase
includes the determiner ?a?, a user might search for
both quoted strings ?English as Second Language?
and ?English as a Second Language?. The counts
obtained for each of these phrases on three different
search engines are shown in Table 1. Note the cor-
rect version, ?English as a Second Language?, has a
much higher number of web hits.
In order to determine whether this approach holds
promise, we implemented a web-based system for
ESL writing error proofing. This pilot study was in-
tended to:
1. identify different types of ESL writing errors and
how often they occur in ESL users? writing samples,
so that the challenges and difficulties of ESL error
proofing can be understood better;
2. explore the advantages and drawbacks of a web-
619
based approach, discover useful web data features,
and identify which types of ESL errors can be reli-
ably proofed using this technique.
We first catalog some major categories of ESL
writing errors, then review related work. Section 3
describes our Web-based English Proofing System
for ESL users (called ESL-WEPS later). Section 4
presents experimental results. Section 5 concludes.
1.1 ESL Writing Errors
In order to get ESL writing samples, we employed
a third party to identify large volumes of ESL web
pages (mostly from Japanese, Korean and Chinese
ESL users? blogs), and cull 1K non-native sen-
tences. A native speaker then rewrote these ESL
sentences ? when possible ? to produce a native-
sounding version. 353 (34.9%) of the original 1012
ESL sentences were labeled ?native-like?, another
347 (34.3%) were rewritten, and the remaining 312
(30.8%) were classified as simply unintelligible.
Table 2 shows some examples from the corpus il-
lustrating some typical types of ESL writing errors
involving: (1) Verb-Noun Collocations (VNC) and
(4) Adjective-Noun Collocations (ANC); (2) incor-
rect use of the transitive verb ?attend?; (3) deter-
miner (article) usage problems; and (5) more com-
plex lexical and style problems. We analyzed all
the pre- and post-edited ESL samples and found 441
ESL errors: about 20% are determiner usage prob-
lems(missing/extra/misused); 15% are VNC errors,
1% are ANC errors; others represent complex syn-
tactic, lexical or style problems. Multiple errors can
co-occur in one sentence. These show that real-
world ESL error proofing is very challenging.
Our findings are consistent with previous research
results on ESL writing errors in two respects:
1. ESL users have significantly more problems
with determiner usage than native speakers be-
cause the use and omission of definite and
indefinite articles varies across different lan-
guages (Schneider and McCoy, 1998)(Lons-
dale and Strong-Krause, 2003).
2. Collocation errors are common among ESL
users, and collocational knowledge contributes
to the difference between native speakers and
ESL learners (Shei and Pain, 2000): in CLEC,
a real-world Chinese English Learner Corpus
(Gui and Yang, 2003), about 30% of ESL writ-
ing errors involve different types of collocation
errors.
In the remainder of the paper, we focus on proofing
determiner usage and VNC errors.
2 Related Work
Researchers have recently proposed some success-
ful learning-based approaches for the determiner se-
lection task (Minnen et al, 2000), but most of this
work has aimed only at helping native English users
correct typographical errors. Gamon et al(2008)
recently addressed the challenging task of proofing
writing errors for ESL users: they propose combin-
ing contextual speller techniques and language mod-
eling for proofing several types of ESL errors, and
demonstrate some promising results. In a departure
from this work, our system directly uses web data
for the ESL error proofing task.
There is a small body of previous work on the
use of online systems aimed at helping ESL learners
correct collocation errors. In Shei and Pain?s sys-
tem (2000), for instance, the British National Cor-
pus (BNC) is used to extract English collocations,
and an ESL learner writing corpus is then used to
build a collocation Error Library. In Jian et al?s sys-
tem (2004), the BNC is also used as a source of col-
locations, with collocation instances and translation
counterparts from the bilingual corpus identified and
shown to ESL users. In contrast to this earlier work,
our system uses the web as a corpus, with string fre-
quency counts from a search engine index used to in-
dicate whether a particular collocation is being used
correctly.
3 Web-based English Proofing System for
ESL Users (ESL-WEPS)
The architecture of ESL-WEPS, which consists of
four main components, is shown in Fig.1.
Parse ESL Sentence and Identify Check Points
ESL-WEPS first tags and chunks (Sang and Buck-
holz, 2000) the input ESL sentence1, and identi-
fies the elements of the structures in the sentence
to be checked according to certain heuristics: when
1One in-house HMM chunker trained on English Penn Tree-
bank was used.
620
ID Pre-editing version Post-editing version
1 Which team can take the champion? Which team will win the championship?
2 I attend to Pyoung Taek University. I attend Pyoung Taek University.
3 I?m a Japanese and studying Info and I?m Japanese and studying Info
Computer Science at Keio University. Computer Science at Keio University.
4 Her works are kinda erotic but they will Her works are kind of erotic, but they will
never arouse any obscene, devil thoughts which might never arouse any obscene, evil thoughts which might
destroy the soul of the designer. destroy the soul of the designer.
5 I think it is so beautiful to go the way of theology I think it is so beautiful to get into theology,
and very attractive too, especially in the area of Christianity. especially Christianity, which attracts me.
Table 2: Some pre- and post-editing ESL writing samples, Bold Italic characters show where the ESL errors
are and how they are corrected/rewritten by native English speaker.
ESL
Sentences
Pre-processing(POS Tagger and Chunk Parser) IdentifyCheck Point
I am learning economics
at university.
[NP I/PRP] [VP am/VBP  learning/
VBG economics/NNS] [PP at/IN] [NP
university/NN] ./.
[VP am/VBP learning/VBG
economics/NNS]
Generate a set of queries, in order to
search correct English usages from Web
Queries:
1.   [economics at university]  AND  [learning]
2.  [economics] AND  [at university] AND
[learning]
3.  [economics]  AND  [university]  AND
[learning]
Search
Engine
Use Web statistics to identify plausible errors, Collect Summaries, Mine collocations or
determiner usages, Generate good suggestions and provide Web example sentences
N-best suggestions:
1. studying 194
2. doing 12
3. visiting 11
Web Examples:
Why Study Economics? - For Lecturers
The design of open days, conferences and other events for school
students  studying economics  and/or thinking of  studying economics at
university . These could be held in a university, in a conference  
http://whystudyeconomics.ac.uk/lecturers/
Figure 1: System Architecture
checking VNC errors, the system searches for a
structure of the form (VP)(NP) or (VP)(PP)(NP) in
the chunked sentence; when checking determiner
usage, the system searches for (NP). Table 3 shows
some examples. For efficiency and effectiveness, the
user can specify that only one specific error type be
critiqued; otherwise it will check both error types:
first determiner usage, then collocations.
Generate Queries In order to find appropriate web
examples, ESL-WEPS generates at each check point
a set of queries. These queries involve three differ-
ent granularity levels, according to sentence?s syntax
structure:
1. Reduced Sentence Level. In order to use
more contextual information, our system pref-
erentially generates a maximal-length query
hereafter called S-Queries, by using the origi-
nal sentence. For the check point chunk, the
verb/adj. to be checked is found and extracted
based on POS tags; other chunks are simply
concatenated and used to formulate the query.
For example, for the first example in Table 3,
the S-Query is [?I have? AND ?this person for
years? AND ?recognized?].
2. Chunk Level. The system segments each ESL
sentence according to chunk tags and utilizes
chunk pairs to generate a query, hereafter re-
ferred to as a C-Query, e.g. the C-Query for the
second example in Table 3 is [?I? AND ?went?
AND ?to climb? AND ?a tall mountain? AND
?last week?]
3. Word Level. The system generates queries by
using keywords from the original string, in the
processing eliminating stopwords used in typ-
ical IR engines, hereafter referred to as a W-
Query, e.g. W-Query for the first example in
Table 3 is [?I? AND ?have? AND ?person? AND
?years? AND ?recognized?]
As queries get longer, web search engines tend to re-
turn fewer and fewer results. Therefore, ESL-WEPS
first segments the original ESL sentence by using
punctuation characters like commas and semicolons,
then generates a query from only the part which con-
tains the given check point. When checking deter-
miner usage, three different cases (a or an/the/none)
621
Parsed ESL sentence Error Type Check Points
(NP I/PRP) (VP have/VBP recognized/VBN) (NP this/DT person/NN) (PP for/IN) (NP years/NNS) ./. VNC recognized this person
(NP I/PRP) (VP went/VBD) (VP to/TO climb/VB) (NP a/DT tall/JJ mountain/NN) (NP last/JJ week/NN) ./. ANC tall mountain, last week
(NP I/PRP) (VP went/VBD) (PP to/TO) (NP coffee/NN) (NP shop/NN) (NP yesterday/NN) ./. Determiner usage coffee, shop, yesterday
(NP Someone/NN) (ADVP once/RB) (VP said/VBD) (SBAR that/IN) Determiner usage meet a right person
(ADVP when/WRB) (NP you/PRP) (VP meet/VBP) (NP a/DT right/JJ person/NN) at the wrong time
(PP at/IN) (NP the/DT wrong/JJ time/NN),/, (NP it/PRP) (VP ?s/VBZ) (NP a/DT pity/NN)./. ?s a pity
Table 3: Parsed ESL sentences and Check Points.
are considered for each check point. For instance,
given the last example in Table 3, three C-Queries
will be generated: [meet a right person],[meet the
right person] and [meet right person]. Note that a
term which has been POS-tagged as NNP (proper
noun) will be skipped and not used for generating
queries in order to obtain more web hits.
Retreive Web Statistics, Collect Snippets To col-
lect enough web examples, three levels of query sets
are submitted to the search engine in the following
order: S-Query, C-Query, and finally W-Query. For
each query, the web hits df returned by search en-
gine is recorded, and the snippets from the top 1000
hits are collected. For efficiency reasons, we follow
Dumais (2002)?s approach: the system relies only
on snippets rather than full-text of pages returned
for each hit; and does not rely on parsing or POS-
tagging for this step. However, a lexicon is used in
order to determine the possible parts-of-speech of a
word as well as its morphological variants. For ex-
ample, to find the correct VNC for a given noun ?tea?
in the returned snippets, the verb drank in the same
clause will be matched before ?tea?.
Identify Errors and Mine Correct Usages To de-
tect determiner usage errors, both the web hit dfq and
the length lq of a given query q are utilized, since
longer query phrases usually lead to fewer web hits.
DFLq, DFLMAX , qmax and Rq are defined as:
DFLq = dfq ? lq, for a given query q;
DFLMAX = max(DFLq),
qmax = argmax
q
(DFLq),
q ? {queries for a given check point};
Rq = DFLq/DFLMAX, given query q and check point.
If DFLMAX is less than a given threshold t1, this
check point will be skipped; otherwise the qmax in-
dicates the best usage. We also calculate the relative
ratio Rq for three usages (a or an/the/none). If Rq is
larger than a threshold t2 for a query q, the system
will not report that usage as an error because it is
sufficiently supported by web data.
For collocation check points, ESL-WEPS may in-
teract twice with the search engine: first, it issues
query sets to collect web examples and identify plau-
sible collocation errors; then, if errors are detected,
new query sets will be issued in the second step in
order to mine correct collocations from new web ex-
amples. For example, for the first sentence in Ta-
ble 3, the S-Query will be [?I have? AND ?this per-
son for years? AND ?recognized?]; the system an-
alyzes returned snippets and identifies ?recognized?
as a possible error. The system then issues a new
S-Query [?I have? AND ?this person for years?], and
finally mines the new set of snippets to discover that
?known? is the preferred lexical option. In contrast
to proofing determiner usages errors, mfreq:
mfreq = frequency of matched collocational verb/adj.
in the snippets for a given noun,
is utilized to both identify errors and suggest correct
VNCs/ANCs. If mfreq is larger than a threshold
t3, the system will conclude that the collocation is
plausible and skip the suggestion step.
4 Experiments
In order to evaluate the proofing algorithm described
above, we utilized the MSN search engine API and
the ESL writing sample set described in Section
1.1 to evaluate the algorithm?s performance on two
tasks: determiner usage and VNC proofing. From
a practical standpoint, we consider precision on the
proofing task to be considerably more important
than recall: false flags are annoying and highly vis-
ible to the user, while recall failures are much less
problematic.
Given the complicated nature of the ESL error
proofing task, about 60% of ESL sentences in our set
that contained determiner errors also contained other
types of ESL errors. As a result, we were forced
to slightly revise the typical precision/recall mea-
surement in order to evaluate performance. First,
622
Good Proofing Examples
Error sentence 1 In my opinion, therefore, when we describe terrorism, its crucially important that
we consider the degree of the influence (i.e., power) on the other countries.
proofing suggestion consider the degree of influence
Error sentence 2 Someone once said that when you meet a right person at the wrong time, it?s a pity.
proofing suggestion meet the right person at the wrong time
Plausible Useful Proofing Examples
Error sentence 3 The most powerful place in Beijing, and in the whole China.
native speaker suggestion in the whole of China
system suggestion in whole China
Error sentence 4 Me, I wanna keep in touch with old friends and wanna talk with anyone who has different thought, etc.
native speaker suggestion has different ideas
system suggestion has a different thought
Table 4: ESL Determiner Usage Proofing by Native Speaker and ESL-WEPS.
Good Proofing Examples
Error sentence 1 I had great time there and got many friends.
proofing suggestion made many friends
Error sentence 2 Which team can take the champion?
proofing suggestion win the champion
Plausible Useful Proofing Examples
Error sentence 3 It may sounds fun if I say my firm resolution of this year is to get a girl friend.
native speaker suggestion sound funny
system suggestion make * fun or get * fun
Table 5: ESL VNC Proofing by Native Speaker and ESL-WEPS.
we considered three cases: (1) the system correctly
identifies an error and proposes a suggestion that ex-
actly matches the native speaker?s rewrite; (2) the
system correctly identifies an error but makes a sug-
gestion that differs from the native speaker?s edit;
and (3) the system incorrectly identifies an error. In
the first case, we consider the proofing good, in the
second, plausibly useful, and in the third case it is
simply wrong. Correspondingly, we introduce the
categories Good Precision (GP), Plausibly Useful
Precision (PUP) and Error Suggestion Rate (ESR),
which were calculated by:
GP = # of Good Proofings# of System?s Proofings ;
PUP = # of Plausibly Useful Proofings# of System?s Proofings ;
ESR = # of Wrong Proofings# of System?s Proofings ;
GP + PUP + ESR = 1
Furthermore, assuming that there are overall NA er-
rors for a given type A of ESL error , the typical
recall and false alarm were calculated by:
recall = # of Good ProofingsNA ;
false alarm = # of Wrong Proofings# of Check points for ESL error A
Table 4 and Table 5 show examples of Good or
Plausibly Useful proofing for determiner usage and
collocation errors, respectively. It can be seen the
system makes plausibly useful proofing suggestions
because some errors types are out of current sys-
tem?s checking range.
The system achieved very promising performance
despite the fact that many of the test sentences con-
tained other, complex ESL errors: using appro-
priate system parameters, ESL-WEPS showed re-
call 40.7% on determiner usage errors, with 62.5%
of these proofing suggestions exactly matching the
rewrites provided by native speakers. Crucially, the
false flag rate was only 2%. Note that a random-
guessing baseline was about 5% precision, 7% re-
call, but more than 80% false flag rate.
For collocation errors, we focused on the most
common VNC proofing task. mfreq and threshold
t3 described in Section 3 are used to control false
alarm, GP and recall. A smaller t3 can reduce recall,
but can increase GP. Table 7 shows how performance
changes with different settings for t3, and Fig. 2(b)
plots the GP/recall curve. Results are not very good:
as recall increases, GP decreases too quickly, so that
at 30.7% recall, precision is only 37.3%. We at-
tribute this to the fact that most search engines only
return the top 1000 web snippets for each query and
our current system relies on this limited number of
snippets to generate and rank candidates.
623
Recall 16.3% 30.2% 40.7% 44.2% 47.7% 50.0%
GP 73.7% 70.3% 62.5% 56.7% 53.3% 52.4%
PUP 15.8% 16.2% 25.0% 29.9% 29.9% 29.3%
false alarm 0.4% 1.4% 2.0% 2.6% 3.7% 4.3%
Table 6: Proofing performance of determiner usage
changes when setting different system parameters.
Recall 11.3% 12.9% 17.8% 25.8% 29.0% 30.7%
GP 77.8% 53.3% 52.4% 43.2% 40.9% 37.3%
PUP 11.11% 33.33% 33.33% 45.10% 48.65% 50.00%
false alarm 0.28% 0.57% 0.85% 0.85% 1.13% 2.55%
Table 7: VNC Proofing performance changes when
setting different system parameters.
5 Conclusion
This paper introduced an approach to the challeng-
ing real-world ESL writing error proofing task that
uses the index of a web search engine for cor-
pus statistics. We validated ESL-WEPS on a web-
crawled ESL writing corpus and compared the sys-
tem?s proofing suggestions to those produced by na-
tive English speakers. Promising performance was
achieved for proofing determiner errors, but less
good results for VNC proofing, possibly because the
current system uses web snippets to rank and gener-
ate collocation candidates. We are currently investi-
gating a modified strategy that exploits high quality
local collocation/synonym lists to limit the number
of proposed Verb/Adj. candidates.
We are also collecting more ESL data to validate
our system and are extending our system to more
ESL error types. Recent experiments on new data
showed that ESL-WEPS can also effectively proof
incorrect choices of prepositions. Later research will
compare the web-based approach to conventional
corpus-based approaches like Gamon et al (2008),
and explore their combination to address complex
ESL errors.
Good Precision vs. Recall
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0%
Good Precision vs. Recall
20.0%
30.0%
40.0%
50.0%
60.0%
70.0%
80.0%
90.0%
5.0% 10.0% 15.0% 20.0% 25.0% 30.0% 35.0%(a)Determiner Usage Error Proofing (b)VNC Error Proofing
Figure 2: GP/recall curves. X and Y axis denotes
GP and Recall respectively.
Acknowledgement The authors have benefited
extensively from discussions with Michael Gamon
and Chris Brockett. We also thank the Butler Hill
Group for collecting the ESL examples.
References
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal smt techniques. In
Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meet-
ing of the ACL, pages 249?256, Sydney, Australia.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: is more always better? In
Proceedings of the 25th Annual International ACM SI-
GIR, pages 291?298, Tampere, Finland.
M. Gamon, J.F. Gao, C. Brockett, A. Klementiev, W.B.
Dolan, and L. Vanderwende. 2008. Using contextual
speller techniques and language modeling for ESL er-
ror correction. In Proceedings of IJCNLP 2008, Hy-
derabad, India, January.
S. Gui and H. Yang, 2003. Zhongguo Xuexizhe Yingyu
Yuliaoku. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe, Shanghai. (In Chinese).
Jia-Yan Jian, Yu-Chia Chang, and Jason S. Chang.
2004. TANGO: bilingual collocational concordancer.
In Proceedings of the ACL 2004, pages 19?23,
Barcelona, Spain.
D. Lonsdale and D. Strong-Krause. 2003. Automated
rating of ESL essays. In Proceedings of the NAACL-
HLT 03 workshop on Building educational applica-
tions using natural language processing, pages 61?67,
Edmonton, Canada.
G. Minnen, F. Bond, and A. Copestake. 2000. Memory-
based learning for article generation. In Proceedings
of the Fourth Conference on Computational Natural
Language Learning and of the Second Learning Lan-
guage in Logic Workshop, pages 43?48.
E. Tjong Kim Sang and S. Buckholz. 2000. Introduction
to the conll-2000 shared task: Chunking. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 127?132,
Lisbon, Portugal.
D. Schneider and K. F. McCoy. 1998. Recognizing syn-
tactic errors in the writing of second language learn-
ers. In Proceedings of the 17th international confer-
ence on Computational linguistics, pages 1198?1204,
Montreal, Quebec, Canada.
C.-C. Shei and H. Pain. 2000. An esl writer?s collo-
cational aid. Computer Assisted Language Learning,
13(2):167?182.
624
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 463?470,
New York, June 2006. c?2006 Association for Computational Linguistics
	

	


	
		
	

Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 225?232,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Approximation Lasso Methods for Language Modeling 
Jianfeng Gao 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
jfgao@microsoft.com 
Hisami Suzuki 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
hisamis@microsoft.com 
Bin Yu 
Department of Statistics 
University of California 
Berkeley., CA 94720 U.S.A. 
binyu@stat.berkeley.edu 
 
Abstract 
Lasso is a regularization method for pa-
rameter estimation in linear models. It op-
timizes the model parameters with respect 
to a loss function subject to model com-
plexities. This paper explores the use of 
lasso for statistical language modeling for 
text input. Owing to the very large number 
of parameters, directly optimizing the pe-
nalized lasso loss function is impossible. 
Therefore, we investigate two approxima-
tion methods, the boosted lasso (BLasso) 
and the forward stagewise linear regres-
sion (FSLR). Both methods, when used 
with the exponential loss function, bear 
strong resemblance to the boosting algo-
rithm which has been used as a discrimi-
native training method for language mod-
eling. Evaluations on the task of Japanese 
text input show that BLasso is able to 
produce the best approximation to the 
lasso solution, and leads to a significant 
improvement, in terms of character error 
rate, over boosting and the traditional 
maximum likelihood estimation. 
1 Introduction 
Language modeling (LM) is fundamental to a 
wide range of applications. Recently, it has been 
shown that a linear model estimated using dis-
criminative training methods, such as the boost-
ing and perceptron algorithms, outperforms 
significantly a traditional word trigram model 
trained using maximum likelihood estimation 
(MLE) on several tasks such as speech recogni-
tion and Asian language text input (Bacchiani et 
al. 2004; Roark et al 2004; Gao et al 2005; Suzuki 
and Gao 2005). 
The success of discriminative training meth-
ods is largely due to fact that unlike the tradi-
tional approach (e.g., MLE) that maximizes the 
function (e.g., likelihood of training data) that is 
loosely associated with error rate, discriminative 
training methods aim to directly minimize the 
error rate on training data even if they reduce the 
likelihood. However, given a finite set of training 
samples, discriminative training methods could 
lead to an arbitrary complex model for the pur-
pose of achieving zero training error. It is 
well-known that complex models exhibit high 
variance and perform poorly on unseen data. 
Therefore some regularization methods have to 
be used to control the complexity of the model. 
Lasso is a regularization method for parame-
ter estimation in linear models. It optimizes the 
model parameters with respect to a loss function 
subject to model complexities. The basic idea of 
lasso is originally proposed by Tibshirani (1996). 
Recently, there have been several implementa-
tions and experiments of lasso on multi-class 
classification tasks where only a small number of 
features need to be handled and the lasso solu-
tion can be directly computed via numerical 
methods. To our knowledge, this paper presents 
the first empirical study of lasso for a realistic, 
large scale task: LM for Asian language text in-
put. Because the task utilizes millions of features 
and training samples, directly optimizing the 
penalized lasso loss function is impossible. 
Therefore, two approximation methods, the 
boosted lasso (BLasso, Zhao and Yu 2004) and 
the forward stagewise linear regression (FSLR, 
Hastie et al 2001), are investigated. Both meth-
ods, when used with the exponential loss func-
tion, bear strong resemblance to the boosting 
algorithm which has been used as a discrimina-
tive training method for LM. Evaluations on the 
task of Japanese text input show that BLasso is 
able to produce the best approximation to the 
lasso solution, and leads to a significant im-
provement, in terms of character error rate, over 
the boosting algorithm and the traditional MLE. 
2 LM Task and Problem Definition 
This paper studies LM on the application of 
Asian language (e.g. Chinese or Japanese) text 
input, a standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for 
225
input method editor, based on the name of the 
commonly used Windows-based application. 
Performance on IME is measured in terms of 
the character error rate (CER), which is the 
number of characters wrongly converted from 
the phonetic string divided by the number of 
characters in the correct transcript.  
Similar to speech recognition, IME is viewed 
as a Bayes decision problem. Let A be the input 
phonetic string. An IME system?s task is to 
choose the most likely word string W* among 
those candidates that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity as the phonetic string is in-
putted by users. Moreover, we can assume a 
unique mapping from W and A in IME as words 
have unique readings, i.e. P(A|W) = 1. So the 
decision of Equation (1) depends solely upon 
P(W), making IME an ideal evaluation test bed 
for LM.  
In this study, the LM task for IME is formu-
lated under the framework of linear models (e.g., 
Duda et al 2001). We use the following notation, 
adapted from Collins and Koo (2005):  
? Training data is a set of example in-
put/output pairs. In LM for IME, training sam-
ples are represented as {Ai, WiR}, for i = 1?M, 
where each Ai is an input phonetic string and WiR 
is the reference transcript of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top n word strings converted from A using a 
baseline IME system that uses only a word tri-
gram model. 
? We assume a set of D+1 features fd(W), for d 
= 0?D. The features could be arbitrary functions 
that map W to real values. Using vector notation, 
we have f(W)??D+1, where f(W) = [f0(W), f1(W), 
?, fD(W)]T. f0(W) is called the base feature, and is 
defined in our case as the log probability that the 
word trigram model assigns to W. Other features 
(fd(W), for d = 1?D) are defined as the counts of 
word n-grams (n = 1 and 2 in our experiments) in 
W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as 
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the num-
ber of conversion errors in W by comparing it 
with a reference transcript WR using an error 
function Er(WR,W), which is the string edit dis-
tance function in our case. We call the sum of 
error counts over the training samples sample risk. 
Our goal then is to search for the best parameter 
set ? which minimizes the sample risk, as in 
Equation (4):  
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
However, (4) cannot be optimized easily since 
Er(.) is a piecewise constant (or step) function of ? 
and its gradient is undefined. Therefore, dis-
criminative methods apply different approaches 
that optimize it approximately. The boosting 
algorithm described below is one of such ap-
proaches.  
3 Boosting 
This section gives a brief review of the boosting 
algorithm, following the description of some 
recent work (e.g., Schapire and Singer 1999; 
Collins and Koo 2005).  
The boosting algorithm uses an exponential 
loss function (ExpLoss) to approximate the sam-
ple risk in Equation (4). We define the margin of 
the pair (WR, W) with respect to the model ? as 
),(),(),( ?? WScoreWScoreWWM RR ?=  (5)
Then, ExpLoss is defined as 
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?  (6)
Notice that ExpLoss is convex so there is no 
problem with local minima when optimizing it. It 
is shown in Freund et al (1998) and Collins and 
Koo (2005) that there exist gradient search pro-
cedures that converge to the right solution.  
Figure 1 summarizes the boosting algorithm 
we used. After initialization, Steps 2 and 3 are 
1 Set ?0 = argmin?0ExpLoss(?); and ?d = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Eq. (6) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
226
repeated N times; at each iteration, a feature is 
chosen and its weight is updated as follows.  
First, we define Upd(?, k, ?) as an updated 
model, with the same parameter values as ? with 
the exception of ?k, which is incremented by ? 
},...,,...,,{),,Upd( 10 Dkk ?????? +=?  
Then, Steps 2 and 3 in Figure 1 can be rewritten 
as Equations (7) and (8), respectively. 
)),,d(ExpLoss(Upminarg*)*,(
,
??
?
kk
k
?=  (7)
*)*,,Upd( 1 ?ktt ?= ??  (8)
The boosting algorithm can be too greedy: 
Each iteration usually reduces the ExpLoss(.) on 
training data, so for the number of iterations 
large enough this loss can be made arbitrarily 
small. However, fitting training data too well 
eventually leads to overfiting, which degrades 
the performance on unseen test data (even 
though in boosting overfitting can happen very 
slowly).  
Shrinkage is a simple approach to dealing 
with the overfitting problem. It scales the incre-
mental step ? by a small constant ?, ? ? (0, 1). 
Thus, the update of Equation (8) with shrinkage 
is 
*)*,,Upd( 1 ??ktt ?= ??  (9)
Empirically, it has been found that smaller values 
of ? lead to smaller numbers of test errors. 
4 Lasso 
Lasso is a regularization method for estimation in 
linear models (Tibshirani 1996). It regularizes or 
shrinks a fitted model through an L1 penalty or 
constraint.  
Let T(?) denote the L1 penalty of the model, 
i.e., T(?) = ?d = 0?D|?d|. We then optimize the 
model ? so as to minimize a regularized loss 
function on training data, called lasso loss defined 
as 
)()ExpLoss(),LassoLoss( ??? T?? +=  (10)
where T(?) generally penalizes larger models (or 
complex models), and the parameter ? controls 
the amount of regularization applied to the esti-
mate. Setting ? = 0 reverses the LassoLoss to the 
unregularized ExpLoss; as ? increases, the model 
coefficients all shrink, each ultimately becoming 
zero. In practice, ? should be adaptively chosen 
to minimize an estimate of expected loss, e.g., ? 
decreases with the increase of the number of 
iterations.  
Computation of the solution to the lasso prob-
lem has been studied for special loss functions. 
For least square regression, there is a fast algo-
rithm LARS to find the whole lasso path for dif-
ferent ?? s (Obsborn et al 2000a; 2000b; Efron et 
al. 2004); for 1-norm SVM, it can be transformed 
into a linear programming problem with a fast 
algorithm similar to LARS (Zhu et al 2003). 
However, the solution to the lasso problem for a 
general convex loss function and an adaptive ? 
remains open. More importantly for our pur-
poses, directly minimizing lasso function of 
Equation (10) with respect to ? is not possible 
when a very large number of model parameters 
are employed, as in our task of LM for IME. 
Therefore we investigate below two methods that 
closely approximate the effect of the lasso, and 
are very similar to the boosting algorithm. 
It is also worth noting the difference between 
L1 and L2 penalty. The classical Ridge Regression 
setting uses an L2 penalty in Equation (10) i.e., 
T(?) = ?d = 0?D(?d)2, which is much easier to 
minimize (for least square loss but not for Ex-
pLoss). However, recent research (Donoho et al 
1995) shows that the L1 penalty is better suited for 
sparse situations, where there are only a small 
number of features with nonzero weights among 
all candidate features. We find that our task is 
indeed a sparse situation: among 860,000 features, 
in the resulting linear model only around 5,000 
features have nonzero weights. We then focus on 
the L1 penalty. We leave the empirical compari-
son of the L1 and L2 penalty on the LM task to 
future work. 
4.1 Forward Stagewise Linear 
Regression (FSLR) 
The first approximation method we used is FSLR, 
described in (Algorithm 10.4, Hastie et al 2001), 
where Steps 2 and 3 in Figure 1 are performed 
according to Equations (7) and (11), respectively. 
)),,d(ExpLoss(Upminarg*)*,(
,
??
?
kk
k
?=  (7) 
*))sign(*,,Upd( 1 ?? ?= ? ktt ??  (11)
Notice that FSLR is very similar to the boosting 
algorithm with shrinkage in that at each step, the 
feature fk* that has largest estimated impact on 
reducing ExpLoss is selected. The only difference 
is that FSLR updates the weight of fk* by a small 
fixed step size ?. By taking such small steps, FSLR 
imposes some implicit regularization, and can 
closely approximate the effect of the lasso in a 
local sense (Hastie et al 2001). Empirically, we 
find that the performance of the boosting algo-
rithm with shrinkage closely resembles that of 
FSLR, with the learning rate parameter ? corre-
sponding to ?. 
227
4.2 Boosted Lasso (BLasso) 
The second method we used is a modified ver-
sion of the BLasso algorithm described in Zhao 
and Yu (2004). There are two major differences 
between BLasso and FSLR. At each iteration, 
BLasso can take either a forward step or a backward 
step. Similar to the boosting algorithm and FSLR, 
at each forward step, a feature is selected and its 
weight is updated according to Equations (12) 
and (13). 
)),,d(ExpLoss(Upminarg*)*,(
,
??
??
kk
k
?
?=
=  (12)
*))sign(*,,Upd( 1 ?? ?= ? ktt ??  (13)
However, there is an important difference be-
tween Equations (12) and (7). In the boosting 
algorithm with shrinkage and FSLR, as shown in 
Equation (7), a feature is selected by its impact on 
reducing the loss with its optimal update ?*. In 
contract, in BLasso, as shown in Equation (12), 
the optimization over ? is removed, and for each 
feature, its loss is calculated with an update of 
either +? or -?, i.e., the grid search is used for 
feature selection. We will show later that this 
seemingly trivial difference brings a significant 
improvement. 
The backward step is unique to BLasso. In 
each iteration, a feature is selected and its weight 
is updated backward if and only if it leads to a 
decrease of the lasso loss, as shown in Equations 
(14) and (15): 
))sign(,,d(ExpLoss(Upminarg*
0,
??
?
??=
?
k
k
kk
k
? (14)
))sign(*,,Upd( *
1 ?? ??= ? ktt k??  
??? >??? ),LassoLoss(),LassoLoss( if 11 tttt ??
(15)
where ?  is a tolerance parameter. 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are re-
peated N times; at each iteration, a feature is 
chosen and its weight is updated either backward 
or forward by a fixed amount ?. Notice that the 
value of ? is adaptively chosen according to the 
reduction of ExpLoss during training. The algo-
rithm starts with a large initial ?, and then at each 
forward step the value of ? decreases until the 
ExpLoss stops decreasing. This is intuitively 
desirable: It is expected that most highly effective 
features are selected in early stages of training, so 
the reduction of ExpLoss at each step in early 
stages are more substantial than in later stages. 
These early steps coincide with the boosting steps 
most of the time. In other words, the effect of 
backward steps is more visible at later stages. 
Our implementation of BLasso differs slightly 
from the original algorithm described in Zhao 
and Yu (2004). Firstly, because the value of the 
base feature f0 is the log probability (assigned by 
a word trigram model) and has a different range 
from that of other features as in Equation (2), ?0 is 
set to optimize ExpLoss in the initialization step 
(Step 1 in Figure 2) and remains fixed during 
training. As suggested by Collins and Koo (2005), 
this ensures that the contribution of the 
log-likelihood feature f0 is well-calibrated with 
respect to ExpLoss. Secondly, when updating a 
feature weight, if the size of the optimal update 
step (computed via Equation (7)) is smaller than 
?, we use the optimal step to update the feature. 
Therefore, in our implementation BLasso does 
not always take a fixed step; it may take steps 
whose size is smaller than ?. In our initial ex-
periments we found that both changes (also used 
in our implementations of boosting and FSLR) 
were crucial to the performance of the methods.  
1 Initialize ?0: set ?0 = argmin?0ExpLoss(?), and ?d = 0 
for d=1?D. 
2 Take a forward step according to Eq. (12) and (13), 
and the updated model is denoted by ?1 
3 Initialize ? = (ExpLoss(?0)-ExpLoss(?1))/? 
4 Take a backward step if and only if it leads to a 
decrease of LassoLoss according to Eq. (14) and 
(15), where ?  = 0; otherwise 
5 Take a forward step according to Eq. (12) and (13); 
update ? = min(?, (ExpLoss(?t-1)-ExpLoss(?t))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
(Zhao and Yu 2004) provides theoretical justi-
fications for BLasso. It has been proved that (1) it 
guarantees that it is safe for BLasso to start with 
an initial ? which is the largest ? that would 
allow an ? step away from 0 (i.e., larger ??s cor-
respond to T(?)=0); (2) for each value of ?, BLasso 
performs coordinate descent (i.e., reduces Ex-
pLoss by updating the weight of a feature) until 
there is no descent step; and (3) for each step 
where the value of ? decreases, it guarantees that 
the lasso loss is reduced.  As a result, it can be 
proved that for a finite number of features and ? 
= 0, the BLasso algorithm shown in Figure 2 
converges to the lasso solution when ? ? 0. 
5 Evaluation 
5.1 Settings 
We evaluated the training methods described 
above in the so-called cross-domain language 
model  adaptation paradigm, where we adapt a 
model trained on one domain (which we call the 
228
background domain) to a different domain (adap-
tation domain), for which only a small amount of 
training data is available. 
The data sets we used in our experiments 
came from five distinct sources of text. A 
36-million-word Nikkei Newspaper corpus was 
used as the background domain, on which the 
word trigram model was trained. We used four 
adaptation domains: Yomiuri (newspaper cor-
pus), TuneUp (balanced corpus containing 
newspapers and other sources of text), Encarta 
(encyclopedia) and Shincho (collection of novels). 
All corpora have been pre-word-segmented us-
ing a lexicon containing 167,107 entries. For each 
of the four domains, we created training data 
consisting of 72K sentences (0.9M~1.7M words) 
and test data of 5K sentences (65K~120K words) 
from each adaptation domain. The first 800 and 
8,000 sentences of each adaptation training data 
were also used to show how different sizes of 
training data affected the performances of vari-
ous adaptation methods. Another 5K-sentence 
subset was used as held-out data for each do-
main.  
We created the training samples for discrimi-
native learning as follows. For each phonetic 
string A in adaptation training data, we pro-
duced a lattice of candidate word strings W using 
the baseline system described in (Gao et al 2002), 
which uses a word trigram model trained via 
MLE on the Nikkei Newspaper corpus. For effi-
ciency, we kept only the best 20 hypotheses in its 
candidate conversion set  GEN(A) for each 
training sample for discriminative training. The 
oracle best hypothesis, which gives the minimum 
number of errors, was used as the reference tran-
script of A.  
We used unigrams and bigrams that occurred 
more than once in the training set as features in 
the linear model of Equation (2). The total num-
ber of candidate features we used was around 
860,000.  
5.2 Main Results 
Table 1 summarizes the results of various model 
training (adaptation) methods in terms of CER 
(%) and CER reduction (in parentheses) over 
comparing models. In the first column, the 
numbers in parentheses next to the domain name 
indicates the number of training sentences used 
for adaptation. 
Baseline, with results shown in Column 3, is 
the word trigram model. As expected, the CER 
correlates very well the similarity between the 
background domain and the adaptation domain, 
where domain similarity is measured in terms of 
cross entropy (Yuan et al 2005) as shown in Col-
umn 2.  
MAP (maximum a posteriori), with results 
shown in Column 4, is a traditional LM adapta-
tion method where the parameters of the back-
ground model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear 
interpolation as described in Bacchiani et al 
(2004): P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), where 
Pb is the probability of the background model, Pa 
is the probability trained on adaptation data 
using MLE and the history h corresponds to two 
preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Boosting, with results shown in Column 5, is 
the algorithm described in Figure 1. In our im-
plementation, we use the shrinkage method 
suggested by Schapire and Singer (1999) and 
Collins and Koo (2005). At each iteration, we 
used the following update for the kth feature 
ZC
ZC
k
k
k ?
?? +
+=
+
_log2
1  (16)
where Ck+ is a value increasing exponentially 
with the sum of margins of (WR, W) pairs over the 
set where fk is seen in WR but not in W; Ck-  is the 
value related to the sum of margins over the set 
where fk is seen in W but not in WR. ? is a 
smoothing factor (whose value is optimized on 
held-out data) and Z is a normalization constant 
(whose value is the ExpLoss(.) of training data 
according to the current model). We see that ?Z in 
Equation (16) plays the same role as ? in Equation 
(9).  
BLasso, with results shown in Column 6, is 
the algorithm described in Figure 2. We find that 
the performance of BLasso is not very sensitive to 
the selection of the step size ? across training sets 
of different domains and sizes. Although small ? 
is preferred in theory as discussed earlier, it 
would lead to a very slow convergence. There-
fore, in our experiments, we always use a large 
step (? = 0.5) and use the so-called early stopping 
strategy, i.e., the number of iterations before 
stopping is optimized on held-out data. 
In the task of LM for IME, there are millions of 
features and training samples, forming an ex-
tremely large and sparse matrix. We therefore 
applied the techniques described in Collins and 
Koo (2005) to speed up the training procedure. 
The resulting algorithms run in around 15 and 30 
minutes respectively for Boosting and BLasso to 
converge on an XEON? MP 1.90GHz machine 
when training on an 8K-sentnece training set. 
229
The results in Table 1 give rise to several ob-
servations. First of all, both discriminative train-
ing methods (i.e., Boosting and BLasso) outper-
form MAP substantially. The improvement mar-
gins are larger when the background and adap-
tation domains are more similar. The phenome-
non is attributed to the underlying difference 
between the two adaptation methods: MAP aims 
to improve the likelihood of a distribution, so if 
the adaptation domain is very similar to the 
background domain, the difference between the 
two underlying distributions is so small that 
MAP cannot adjust the model effectively. Dis-
criminative methods, on the other hand, do not 
have this limitation for they aim to reduce errors 
directly. Secondly, BLasso outperforms Boosting 
significantly (p-value < 0.01) on all test sets. The 
improvement margins vary with the training sets 
of different domains and sizes. In general, in 
cases where the adaptation domain is less similar 
to the background domain and larger training set 
is used, the improvement of BLasso is more visi-
ble.    
Note that the CER results of FSLR are not in-
cluded in Table 1 because it achieves very similar 
results to the boosting algorithm with shrinkage 
if the controlling parameters of both algorithms 
are optimized via cross-validation. We shall dis-
cuss their difference in the next section. 
5.3 Dicussion 
This section investigates what components of 
BLasso bring the improvement over Boosting. 
Comparing the algorithms in Figures 1 and 2, we 
notice three differences between BLasso and 
Boosting: (i) the use of backward steps in BLasso; 
(ii) BLasso uses the grid search (fixed step size) 
for feature selection in Equation (12) while 
Boosting uses the continuous search (optimal 
step size) in Equation (7); and (iii) BLasso uses a 
fixed step size for feature update in Equation (13) 
while Boosting uses an optimal step size in 
Equation (8). We then investigate these differ-
ences in turn. 
To study the impact of backward steps, we 
compared BLasso with the boosting algorithm 
with a fixed step search and a fixed step update, 
henceforth referred to as F-Boosting. F-Boosting 
was implemented as Figure 2, by setting a large 
value to ? in Equation (15), i.e., ? = 103, to prohibit 
backward steps. We find that although the 
training error curves of BLasso and F-Boosting 
are almost identical, the T(?) curves grow apart 
with iterations, as shown in Figure 3. The results 
show that with backward steps, BLasso achieves 
a better approximation to the true lasso solution: 
It leads to a model with similar training errors 
but less complex (in terms of L1 penalty). In our 
experiments we find that the benefit of using 
backward steps is only visible in later iterations 
when BLasso?s backward steps kick in. A typical 
example is shown in Figure 4. The early steps fit 
to highly effective features and in these steps 
BLasso and F-Boosting agree. For later steps, 
fine-tuning of features is required. BLasso with 
backward steps provides a better mechanism 
than F-Boosting to revise the previously chosen 
features to accommodate this fine level of tuning. 
Consequently we observe the superior perform-
ance of BLasso at later stages as shown in our 
experiments.  
As well-known in linear regression models, 
when there are many strongly correlated fea-
tures, model parameters can be poorly estimated 
and exhibit high variance. By imposing a model 
size constraint, as in lasso, this phenomenon is 
alleviated. Therefore, we speculate that a better 
approximation to lasso, as BLasso with backward 
steps, would be superior in eliminating the nega-
tive effect of strongly correlated features in 
model estimation. To verify our speculation, we 
performed the following experiments. For each 
training set, in addition to word unigram and 
bigram features, we introduced a new type of 
features called headword bigram.  
As described in Gao et al (2002), headwords 
are defined as the content words of the sentence. 
Therefore, headword bigrams constitute a special 
type of skipping bigrams which can capture 
dependency between two words that may not be 
adjacent. In reality, a large portion of headword 
bigrams are identical to word bigrams, as two 
headwords can occur next to each other in text. In 
the adaptation test data we used, we find that 
headword bigram features are for the most part 
either completely overlapping with the word bi-
gram features (i.e., all instances of headword 
bigrams also count as word bigrams) or not over-
lapping at all (i.e., a headword bigram feature is 
not observed as a word bigram feature) ? less 
than 20% of headword bigram features displayed 
a variable degree of overlap with word bigram 
features. In our data, the rate of completely 
overlapping features is 25% to 47% depending on 
the adaptation domain. From this, we can say 
that the headword bigram features show moder-
ate to high degree of correlation with the word 
bigram features.  
We then used BLasso and F-Boosting to train 
the linear language models including both word 
bigram and headword bigram features. We find 
that although the CER reduction by adding 
230
headword features is overall very small, the dif-
ference between the two versions of BLasso is 
more visible in all four test sets. Comparing Fig-
ures 5 ? 8 with Figure 4, it can be seen that BLasso 
with backward steps outperforms the one with-
out backward steps in much earlier stages of 
training with a larger margin. For example, on 
Encarta data sets, BLasso outperforms F-Boosting 
after around 18,000 iterations with headword 
features (Figure 7), as opposed to 25,000 itera-
tions without headword features (Figure 4). The 
results seem to corroborate our speculation that 
BLasso is more robust in the presence of highly 
correlated features. 
To investigate the impact of using the grid 
search (fixed step size) versus the continuous 
search (optimal step size) for feature selection, 
we compared F-Boosting with FSLR since they 
differs only in their search methods for feature 
selection. As shown in Figures 5 to 8, although 
FSLR is robust in that its test errors do not in-
crease after many iterations, F-Boosting can reach 
a much lower error rate on three out of four test 
sets. Therefore, in the task of LM for IME where 
CER is the most important metric, the grid search 
for feature selection is more desirable.  
To investigate the impact of using a fixed ver-
sus an optimal step size for feature update, we 
compared FSLR with Boosting. Although both 
algorithms achieve very similar CER results, the 
performance of FSLR is much less sensitive to the 
selected fixed step size. For example, we can 
select any value from 0.2 to 0.8, and in most set-
tings FSLR achieves the very similar lowest CER 
after 20,000 iterations, and will stay there for 
many iterations. In contrast, in Boosting, the 
optimal value of ? in Equation (16) varies with the 
sizes and domains of training data, and has to be 
tuned carefully. We thus conclude that in our 
task FSLR is more robust against different train-
ing settings and a fixed step size for feature up-
date is more preferred. 
6 Conclusion 
This paper investigates two approximation lasso 
methods for LM applied to a realistic task with a 
very large number of features with sparse feature 
space. Our results on Japanese text input are 
promising. BLasso outperforms the boosting 
algorithm significantly in terms of CER reduction 
on all experimental settings. 
We have shown that this superior perform-
ance is a consequence of BLasso?s backward step 
and its fixed step size in both feature selection 
and feature weight update.  Our experimental 
results in Section 5 show that the use of backward 
step is vital for model fine-tuning after major 
features are selected and for coping with strongly 
correlated features; the fixed step size of BLasso 
is responsible for the improvement of CER and 
the robustness of the results. Experiments on 
other data sets and theoretical analysis are 
needed to further support our findings in this 
paper. 
References 
Bacchiani, M., Roark, B., and Saraclar, M. 2004. Lan-
guage model adaptation with MAP estimation and 
the perceptron algorithm. In HLT-NAACL 2004. 21-24. 
Collins, Michael and Terry Koo 2005. Discriminative 
reranking for natural language parsing. Computational 
Linguistics 31(1): 25-69. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 
2001. Pattern classification. John Wiley & Sons, Inc. 
Donoho, D., I. Johnstone, G. Kerkyachairan, and D. 
Picard. 1995. Wavelet shrinkage; asymptopia? (with 
discussion), J. Royal. Statist. Soc. 57: 201-337. 
Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 
2004. Least angle regression. Ann. Statist. 32, 407-499. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. 
An efficient boosting algorithm for combining pref-
erences. In ICML?98.  
Hastie, T., R. Tibshirani and J. Friedman. 2001. The 
elements of statistical learning. Springer-Verlag, New 
York. 
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive 
clustering for language modeling. In EMNLP 2002. 
Gao. J., Yu, H., Yuan, W., and Xu, P. 2005. Minimum 
sample risk methods for language modeling. In 
HLT/EMNLP 2005. 
Osborne, M.R. and Presnell, B. and Turlach B.A. 2000a. 
A new approach to variable selection in least squares 
problems. Journal of Numerical Analysis, 20(3). 
Osborne, M.R. and Presnell, B. and Turlach B.A. 2000b. 
On the lasso and its dual. Journal of Computational and 
Graphical Statistics, 9(2): 319-337. 
Roark, Brian, Murat Saraclar and Michael Collins. 
2004. Corrective language modeling for large vo-
cabulary ASR with the perceptron algorithm. In 
ICASSP 2004. 
Schapire, Robert E. and Yoram Singer. 1999. Improved 
boosting algorithms using confidence-rated predic-
tions. Machine Learning, 37(3): 297-336. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Tibshirani, R. 1996. Regression shrinkage and selection 
via the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Yuan, W., J. Gao and H. Suzuki. 2005. An Empirical 
Study on Language Model Adaptation Using a Met-
ric of Domain Similarity. In IJCNLP 05.  
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, 
Statistics Department, U. C. Berkeley. 
Zhu, J. S. Rosset, T. Hastie, and R. Tibshirani. 2003. 
1-norm support vector machines. NIPS 16. MIT Press. 
231
 Table 1. CER (%) and CER reduction (%) (Y=Yomiuri; T=TuneUp; E=Encarta; S=-Shincho) 
Domain Entropy vs.Nikkei Baseline MAP (over Baseline) Boosting (over MAP) BLasso (over MAP/Boosting) 
Y (800) 7.69 3.70 3.70 (+0.00) 3.13 (+15.41) 3.01 (+18.65/+3.83) 
Y (8K) 7.69 3.70 3.69 (+0.27) 2.88 (+21.95) 2.85 (+22.76/+1.04) 
Y (72K) 7.69 3.70 3.69 (+0.27) 2.78 (+24.66) 2.73 (+26.02/+1.80) 
T (800) 7.95 5.81 5.81 (+0.00) 5.69 (+2.07) 5.63 (+3.10/+1.05) 
T (8K) 7.95 5.81 5.70 (+1.89) 5.48 (+5.48) 5.33 (+6.49/+2.74) 
T (72K) 7.95 5.81 5.47 (+5.85) 5.33 (+2.56) 5.05 (+7.68/+5.25) 
E (800) 9.30 10.24 9.60 (+6.25) 9.82 (-2.29) 9.18 (+4.38/+6.52) 
E (8K) 9.30 10.24 8.64 (+15.63) 8.54 (+1.16) 8.04 (+6.94/+5.85) 
E (72K) 9.30 10.24 7.98 (+22.07) 7.53 (+5.64) 7.20 (+9.77/+4.38) 
S (800) 9.40 12.18 11.86 (+2.63) 11.91 (-0.42) 11.79 (+0.59/+1.01) 
S (8K) 9.40 12.18 11.15 (+8.46) 11.09 (+0.54) 10.73 (+3.77/+3.25) 
S (72K) 9.40 12.18 10.76 (+11.66) 10.25 (+4.74) 9.64 (+10.41/+5.95) 
 
  
 
Figure 3. L1 curves: models are trained 
on the E(8K) dataset. 
Figure 4. Test error curves: models are 
trained on the E(8K) dataset. 
Figure 5. Test error curves: models are 
trained on the Y(8K) dataset, including 
headword bigram features. 
   
Figure 6. Test error curves: models are 
trained on the T(8K) dataset, including 
headword bigram features. 
Figure 7. Test error curves: models are 
trained on the E(8K) dataset, including 
headword bigram features. 
Figure 8. Test error curves: models are 
trained on the S(8K) dataset, including 
headword bigram features. 
232
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 489?496,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A DOM Tree Alignment Model for Mining Parallel Data from the Web 
Lei Shi1, Cheng Niu1, Ming Zhou1, and Jianfeng Gao2 
1Microsoft Research Asia, 5F Sigma Center, 49 Zhichun Road, Beijing 10080, P. R. China 
2Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA 
{leishi,chengniu,mingzhou,jfgao}@microsoft.com 
 
Abstract 
This paper presents a new web mining 
scheme for parallel data acquisition. 
Based on the Document Object Model 
(DOM), a web page is represented as a 
DOM tree. Then a DOM tree alignment 
model is proposed to identify the transla-
tionally equivalent texts and hyperlinks 
between two parallel DOM trees. By 
tracing the identified parallel hyperlinks, 
parallel web documents are recursively 
mined. Compared with previous mining 
schemes, the benchmarks show that this 
new mining scheme improves the mining 
coverage, reduces mining bandwidth, and 
enhances the quality of mined parallel 
sentences. 
1 Introduction 
Parallel bilingual corpora are critical resources 
for statistical machine translation (Brown 1993), 
and cross-lingual information retrieval (Nie 
1999). Additionally, parallel corpora have been 
exploited for various monolingual natural lan-
guage processing (NLP) tasks, such as word-
sense disambiguation (Ng 2003) and paraphrase 
acquisition (Callison 2005). 
However, large scale parallel corpora are not 
readily available for most language pairs. Even 
where resources are available, such as for Eng-
lish-French, the data are usually restricted to 
government documents (e.g., the Hansard corpus, 
which consists of French-English translations of 
debates in the Canadian parliament) or newswire 
texts. The "governmentese" that characterizes 
these document collections cannot be used on its 
own to train data-driven machine translation sys-
tems for a range of domains and language pairs.  
With a sharply increasing number of bilingual 
web sites, web mining for parallel data becomes 
a promising solution to this knowledge acquisi-
tion problem. In an effort to estimate the amount 
of bilingual data on the web, (Ma and Liberman 
1999) surveyed web pages in the de (German 
web site) domain, showing that of 150,000 web-
sites in the .de domain, 10% are German-English 
bilingual. Based on such observations, some web 
mining systems have been developed to auto-
matically obtain parallel corpora from the web 
(Nie et al1999; Ma and Liberman 1999; Chen, 
Chau and Yeh 2004; Resnik and Smith 2003  
Zhang et al2006 ). These systems mine parallel 
web documents within bilingual web sites, ex-
ploiting the fact that URLs of many parallel web 
pages are named with apparent patterns to facili-
tate website maintenance. Hence given a bilin-
gual website, the mining systems use pre-defined 
URL patterns to discover candidate parallel 
documents within the site. Then content-based 
features will be used to verify the translational 
equivalence of the candidate pairs. 
However, due to the diversity of web page 
styles and website maintenance mechanisms, 
bilingual websites use varied naming schemes 
for parallel documents. For example, the United 
Nation?s website, which contains thousands of 
parallel pages, simply names the majority of its 
web pages with some computer generated ad-hoc 
URLs. Such a website then cannot be mined by 
the URL pattern-based mining scheme. To fur-
ther improve the coverage of web mining, other 
patterns associated with translational parallelism 
are called for. 
Besides, URL pattern-based mining may raise 
concerns on high bandwidth cost and slow 
download speed. Based on descriptions of (Nie et 
al 1999; Ma and Liberman 1999; Chen, Chau 
and Yeh 2004), the mining process requires a full 
host crawling to collect URLs before using URL 
patterns to discover the parallel documents. 
Since in many bilingual web sites, parallel 
documents are much sparser than comparable 
documents, a significant portion of internet 
bandwidth is wasted on downloading web pages 
without translational counterparts.  
Furthermore, there is a lack of discussion on 
the quality of mined data. To support machine 
translation, parallel sentences should be extracted 
from the mined parallel documents. However, 
current sentence alignment models, (Brown et al
1991; Gale & Church 1991; Wu 1994; Chen 
489
1993; Zhao and Vogel, 2002; etc.) are targeted 
on traditional textual documents. Due to the 
noisy nature of the web documents, parallel web 
pages may consist of non-translational content 
and many out-of-vocabulary words, both of 
which reduce sentence alignment accuracy. To 
improve sentence alignment performance on the 
web data, the similarity of the HTML tag struc-
tures between the parallel web documents should 
be leveraged properly in the sentence alignment 
model. 
In order to improve the quality of mined data 
and increase the mining coverage and speed, this 
paper proposes a new web parallel data mining 
scheme. Given a pair of parallel web pages as 
seeds, the Document Object Model1  (DOM) is 
used to represent the web pages as a pair of 
DOM trees. Then a stochastic DOM tree align-
ment model is used to align translationally 
equivalent content, including both textual chunks 
and hyperlinks, between the DOM tree pairs. The 
parallel hyperlinks discovered are regarded as 
anchors to new parallel data. This makes the 
mining scheme an iterative process. 
The new mining scheme has three advantages: 
(i) Mining coverage is increased. Parallel hyper-
links referring to parallel web page is a general 
and reliable pattern for parallel data mining. 
Many bilingual websites not supporting URL 
pattern-based mining scheme support this new 
mining scheme. Our mining experiment shows 
that, using the new web mining scheme, the web 
mining throughput is increased by 32%; (ii) The 
quality of the mined data is improved. By lever-
aging the web pages? HTML structures, the sen-
tence aligner supported by the DOM tree align-
ment model outperforms conventional ones by 
7% in both precision and recall;  (iii) The band-
width cost is reduced by restricting web page 
downloads to the links that are very likely to be 
parallel. 
The rest of the paper is organized as follows: 
In the next section, we introduce the related work. 
In Section 3, a new web parallel data mining 
scheme is presented. Three component technolo-
gies, the DOM tree alignment model, the sen-
tence aligner, and the candidate parallel page 
verification model are presented in Section 4, 5, 
and 6. Section 7 presents experiments and 
benchmarks. The paper is finally concluded in 
Section 8. 
                                                 
1
 See http://www.w3.org/DOM/ 
2 Related Work 
The parallel data available on the web have been 
an important knowledge source for machine 
translation. For example, Hong Kong Laws, an 
English-Chinese Parallel corpus released by Lin-
guistic Data Consortium (LDC) is downloaded 
from the Department of Justice of the Hong 
Kong Special Administrative Region website. 
Recently, web mining systems have been built 
to automatically acquire parallel data from the 
web. Exemplary systems include PTMiner (Nie 
et al1999), STRAND (Resnik and Smith, 2003), 
BITS (Ma and Liberman, 1999), and PTI (Chen, 
Chau and Yeh, 2004). Given a bilingual website, 
these systems identify candidate parallel docu-
ments using pre-defined URL patterns. Then 
content-based features are employed for candi-
date verification. Particularly, HTML tag simi-
larities have been exploited to verify parallelism 
between pages. But it is done by simplifying 
HTML tags as a string sequence instead of a hi-
erarchical DOM tree. Tens of thousands parallel 
documents have been acquired with accuracy 
over 90%.  
To support machine translation, parallel sen-
tence pairs should be extracted from the parallel 
web documents. A number of techniques for 
aligning sentences in parallel corpora have been 
proposed. (Gale & Church 1991; Brown et al 
1991; Wu 1994) used sentence length as the ba-
sic feature for alignment. (Kay & Roscheisen 
1993; and Chen 1993) used lexical information 
for sentence alignment. Models combining 
length and lexicon information were proposed in 
(Zhao and Vogel, 2002; Moore 2002). Signal 
processing techniques is also employed in sen-
tence alignment by (Church 1993; Fung & 
McKeown 1994). Recently, much research atten-
tion has been paid to aligning sentences in com-
parable documents (Utiyama et al2003, 
Munteanu et al2004).  
 The DOM tree alignment model is the key 
technique of our mining approach. Although, to 
our knowledge, this is the first work discussing 
DOM tree alignments, there is substantial re-
search focusing on syntactic tree alignment 
model for machine translation. For example, (Wu 
1997; Alshawi, Bangalore, and Douglas, 2000; 
Yamada and Knight, 2001) have studied syn-
chronous context free grammar. This formalism 
requires isomorphic syntax trees for the source 
sentence and its translation. (Shieber and Scha-
bes 1990) presents a synchronous tree adjoining 
grammar (STAG) which is able to align two syn-
490
tactic trees at the linguistic minimal units. The 
synchronous tree substitution grammar (STSG) 
presented in (Hajic etc. 2004) is a simplified ver-
sion of STAG which allows tree substitution op-
eration, but prohibits the operation of tree ad-
junction.  
3 A New Parallel Data Mining Scheme 
Supported by DOM Tree Alignment 
Our new web parallel data mining scheme con-
sists of the following steps:  
 
(1) Given a web site, the root page and web 
pages directly linked from the root page are 
downloaded. Then for each of the 
downloaded web page, all of its anchor texts 
(i.e. the hyperlinked words on a web page) 
are compared with a list of predefined strings 
known to reflect translational equivalence 
among web pages (Nie et al1999). Exam-
ples of such predefined trigger strings in-
clude: (i) trigger words for English transla-
tion {English, English Version,  , 

, etc.}; and (ii) trigger words for Chinese 
translation {Chinese, Chinese Version, Sim-
plified Chinese, Traditional Chinese,   , 


, etc.}. If both categories of trigger 
words are found, the web site is considered 
bilingual, and every web page pair are sent to 
Step 2 for parallelism verification. 
(2) Given a pair of the plausible parallel web 
pages, a verification module is called to de-
termine if the page pair is truly translation-
ally equivalent.  
(3) For each verified pair of parallel web pages, 
a DOM tree alignment model is called to ex-
tract parallel text chunks and hyperlinks. 
(4) Sentence alignment is performed on each 
pair of the parallel text chunks, and the re-
sulting parallel sentences are saved in an 
output file. 
(5) For each pair of parallel hyperlinks, the cor-
responding pair of web pages is downloaded, 
and then goes to Step 2 for parallelism veri-
fication. If no more parallel hyperlinks are 
found, stop the mining process. 
Our new mining scheme is iterative in nature. 
It fully exploits the information contained in the 
parallel data and effectively uses it to pinpoint 
the location holding more parallel data. This ap-
proach is based on our observation that parallel 
pages share similar structures holding parallel 
content, and parallel hyperlinks refer to new par-
allel pages. 
By exploiting both the HTML tag similarity 
and the content-based translational equivalences, 
the DOM tree alignment model extracts parallel 
text chunks. Working on the parallel text chunks 
instead of the text of the whole web page, the 
sentence alignment accuracy can be improved by 
a large margin. 
In the next three sections, three component 
techniques, the DOM tree alignment model, sen-
tence alignment model, and candidate web page 
pair verification model are introduced. 
4 DOM Tree Alignment Model 
The Document Object Model (DOM) is an appli-
cation programming interface for valid HTML 
documents. Using DOM, the logical structure of 
a HTML document is represented as a tree where 
each node belongs to some pre-defined node 
types (e.g. Document, DocumentType, Element, 
Text, Comment, ProcessingInstruction etc.). 
Among all these types of nodes, the nodes most 
relevant to our purpose are Element nodes (cor-
responding to the HTML tags) and Text nodes 
(corresponding to the texts). To simplify the de-
scription of the alignment model, minor modifi-
cations of the standard DOM tree are made: (i) 
Only the Element nodes and Text nodes are kept 
in our document tree model. (ii) The ALT attrib-
ute is represented as Text node in our document 
tree model. The ALT text are textual alternative 
when images cannot be displayed, hence is help-
ful to align images and hyperlinks. (iii) the Text 
node (which must be a leaf) and its parent Ele-
ment node are combined into one node in order 
to concise the representation of  the alignment 
model. The above three modifications are exem-
plified in Fig. 1. 
 
 
Fig. 1 Difference between Standard DOM and 
Our Document Tree 
 
Despite these minor differences, our document 
tree is still referred as DOM tree throughout this 
paper. 
491
4.1 DOM Tree Alignment 
Similar to STSG, our DOM tree alignment model 
supports node deletion, insertion and substitution. 
Besides, both STSG and our DOM tree align-
ment model define the alignment as a tree hierar-
chical invariance process, i.e. if node A is aligned 
with node B, then the children of A are either 
deleted or aligned with the children of B.  
But two major differences exist between 
STSG and our DOM tree alignment model: (i) 
Our DOM tree alignment model requires the 
alignment a sequential order invariant process, 
i.e. if node A is aligned with node B, then the 
sibling nodes following A have to be either de-
leted or aligned with the sibling nodes following 
B.  (ii) (Hajic etc. 2004) presents STSG in the 
context of language generation, while we search 
for the best alignment on the condition that both 
trees are given.  
To facilitate the presentation of the tree align-
ment model, the following symbols are intro-
duced: given a HTML document D, DT refers to 
the corresponding DOM tree; DiN refers to the i
th
 
node of DT (here the index of the node is in the 
breadth-first order), and DiT refers to the sub-tree 
rooted at DiN , so 
D
1N refers to the root of 
DT , 
and DT=D1T ;  [ ]
D
ji,T refers to the forest consisting 
of the sub-trees rooted at nodes from DiT to
D
jT . 
t.N Di refers to the text of node
D
iN ; l.N
D
i refers to 
the HTML tag of the node DiN ; jC.N
D
i  refers to 
the jth child of the node DiN ; [ ]nmC ,Di .N refers to 
the consecutive sequence of DiN ?s children nodes 
from 
m
C.N Di to nC.N
D
i ; the sub-tree rooted at 
jC.N
D
i is represented as jTC.N
D
i  and the forest 
rooted at [ ]nmC ,
D
i .N  is represented as [ ]nmTC ,
D
i .N . 
Finally NULL  refers to the empty node intro-
duced for node deletion.  
To accommodate the hierarchical structure of 
the DOM tree, two different translation prob-
abilities are defined: ( )EiFm TTPr : probability of translating sub-tree 
E
iT into sub-tree
F
m
T ; 
( )EiFm NNPr : probability of translating node 
E
iN into 
F
m
N . 
Besides, [ ] [ ]( )ATT E jiF nm ,Pr ,,  represents the prob-
ability of translating the forest [ ]
E
jiT , into 
[ ]
F
nm
T
,
based on the alignment A. The tree align-
ment A is defined as a mapping from target 
nodes onto source nodes or the null node.  
Given two HTML documents F (in French) 
and E (in English), the tree alignment task is 
defined as searching for A which maximizes the 
following probability: ( ) ( ) ( )EEFEF TAATTTTA Pr,Pr,Pr ?               (1) 
where ( )ETAPr  represents the prior knowledge 
of the alignment configurations.  
By introducing dp  which refers to the prob-
ability of a source or target node deletion occur-
ring in an alignment configuration, the alignment 
prior ( )ETAPr  is assumed as the following bi-
nominal distribution: 
 ( ) ( ) MdLdE ppTA ?? 1Pr  
where L is the count of non-empty alignments in 
A, and M is the count of source and target node 
deletions in A. 
As to ( )ATT EF ,Pr , we can estimate as 
( ) ( )ATTATT EFEF ,Pr,Pr 11= , and ( )ATTr EiFl ,P  
can be calculated recursively depending on the 
alignment configuration of A : 
(1) If FlN is aligned with EiN , and the children of 
F
lN are aligned with the children of 
E
iN , then 
we have ( )
( ) [ ] 



=

	

 ATCNTCNNN
ATT
K
E
iK
F
l
E
i
F
l
E
i
F
l
,..PrPr
,Pr
'
,1,1
    
where K and K? are degree of FlN  and 
E
iN . 
(2) If FlN is deleted, and the children of FlN  is 
aligned with EiT , then we have ( ) ( ) [ ]( )ATTCNNULLNATT EiKFlFlEiFl ,.PrPr,Pr ,1=
where K is the degree of FlN  
(3) If EiN is deleted, and FlN is aligned with the 
children of EiN , then  
( ) ( )ATCTTATT KEiFlEiFl ,.Pr,Pr ],1[=               
where K is the degree of EiN . 
To complete the alignment model, 
[ ]( )ATTr E jiF nm ,P ,],[  is to be estimated. As mentioned 
before, only the alignment configurations with 
unchanged node sequential order are considered 
as valid. So, [ ]( )ATTr E jiF nm ,P ,],[ is estimated recur-
sively according to the following five alignment 
configurations of A: 
(4) If F
m
T is aligned with EiT , and [ ]
F
nm
T
,1+  is 
492
aligned with [ ]
E
jiT ,1+ , then  
[ ]( ) ( ) [ ]( )ATTrNNATTr E jiF nmEiFmE jiF nm ,PPr,P ,1],1[,],[ ++=    
(5) If F
m
T is deleted, and [ ]
F
nm
T
,1+ is aligned with 
[ ]
E
jiT , , then 
[ ]( ) ( ) [ ]( )ATTrNULLNATTr E jiF nmFmE jiF nm ,PPr,P ,],1[,],[ +=
 
(6) If EiT is deleted, and [ ]F nmT , is aligned with 
[ ]
E
jiT ,1+ , then 
[ ]( ) [ ]( )ATTATTr E jiF nmE jiF nm ,Pr,P ,1],[,],[ +=     
(7) If F
m
N  is deleted, and F
m
N ?s children [ ]K
F
m
CN
,1.  
is combined with [ ]
F
nm
T
,1+ to aligned with [ ]
E
jiT , , 
then 
[ ]( )
( ) [ ]( )ATTTCNrNULLN
ATTr
E
ji
F
nmK
F
m
F
m
E
ji
F
nm
,.PPr
,P
,],1[],1[
,],[
+
=
   
where K is the degree of .F
m
N  
(8) EiN  is deleted, and EiN ?s children [ ]KEi CN ,1.  
is combined with [ ]
E
jiT ,1+ to be aligned with 
[ ]
F
nm
T
,
, then 
[ ]( ) [ ]( )ATTCNTATTr EKEiFEF jinmjinm ,.Pr,P ,1],[,],[ ],1[ +=       
where K is the degree of .EiN  
 
Finally, the node translation probability is 
modeled as ( ) ( ) ( )tNtNlNlNNN EiFlEiFlEjFl ..Pr..PrPr ?  . And 
the text translation probability ( )EF ttPr  is model 
using IBM model I (Brown et al1993). 
4.2 Parameter Estimation Using Expecta-
tion-Maximization 
Our tree alignment model involves three catego-
ries of parameters: the text translation probability ( )EF ttPr , tag mapping probability ( )'Pr ll , and 
node deletion probability dp .  
Conventional parallel data released by LDC 
are used to train IBM model I for estimating the 
text translation probability ( )EF ttPr .   
One way to estimate ( )'Pr ll and dp  is to 
manually align nodes between parallel DOM 
trees, and use them as training corpora for 
maximum likelihood estimation. However, this is 
a very time-consuming and error-prone proce-
dure. In this paper, the inside outside algorithm 
presented in (Lari and Young, 1990) is extended 
to train parameters ( )'Pr ll  and dp  by optimally 
fitting the existing parallel DOM trees. 
4.3 Dynamic Programming for Decoding 
It is observed that if two trees are optimally 
aligned, the alignment of their sub-trees must be 
optimal as well. In the decoding process, dy-
namic programming techniques can be applied to 
find the optimal tree alignment using that of the 
sub-trees in a bottom up manner. The following 
is the pseudo-code of the decoding algorithm: 
 
For i= || FT  to 1  (bottom-up) { 
For j= || ET to 1 (bottom-up) { 
derive the best alignments among 
[ ]iK
F
i TCT ,1.  and [ ]jK
E
j TCT ,1. , and then com-
pute the best alignment between 
F
iN and 
E
jN .  
where || FT and || ET are number of nodes in 
FT and ET ; iK and jK are the degrees of 
F
iN and 
E
jN . The time complexity of the decoding algo-
rithm is )))(degree)((degree|||TO(| 2F EFE TTT +?? , 
where the degree of a tree is defined as the larg-
est degree of its nodes. 
5 Aligning Sentences Using Tree Align-
ment Model 
To exploit the HTML structure similarities be-
tween parallel web documents, a cascaded ap-
proach is used in our sentence aligner implemen-
tation.  
First, text chunks associated with DOM tree 
nodes are aligned using the DOM tree alignment 
model. Then for each pair of parallel text chunks, 
the sentence aligner described in (Zhao et al
2002), which combines IBM model I and the 
length model of (Gale & Church 1991) under a 
maximum likelihood criterion, is used to align 
parallel sentences.  
6 Web Document Pair Verification 
Model 
To verify whether a candidate web document 
pair is truly parallel, a binary maximum entropy 
based classifier is used.  
Following (Nie et al1999) and  (Resnik and 
Smith, 2003), three features are used: (i) file 
length ratio;  (ii) HTML tag similarity; (iii) sen-
tence alignment score.  
493
The HTML tag similarity feature is computed 
as follows: all of the HTML tags of a given web 
page are extracted, and concatenated as a string. 
Then, a minimum edit distance between the two 
tag strings associated with the candidate pair is 
computed, and the HMTL tag similarity score is 
defined as the ratio of match operation number to 
the total operation number.  
The sentence alignment score is defined as the 
ratio of the number of aligned sentences and the 
total number of sentences in both files. 
Using these three features, the maximum en-
tropy model is trained on 1,000 pairs of web 
pages manually labeled as parallel or non-
parallel. The Iterative Scaling algorithm (Pietra, 
Pietra and Lafferty 1995) is used for the training. 
7 Experimental Results 
The DOM tree alignment based mining system is 
used to acquire English-Chinese parallel data 
from the web. The mining procedure is initiated 
by acquiring Chinese website list. 
We have downloaded about 300,000 URLs of 
Chinese websites from the web directories at 
cn.yahoo.com, hk.yahoo.com and tw.yahoo.com. 
And each website is sent to the mining system 
for English-Chinese parallel data acquisition. To 
ensure that the whole mining experiment to be 
finished in schedule, we stipulate that it takes at 
most 10 hours on mining each website. Totally 
11,000 English-Chinese websites are discovered, 
from which 63,214 pairs of English-Chinese par-
allel web documents are mined. After sentence 
alignment, totally 1,069,423 pairs of English-
Chinese parallel sentences are extracted. 
In order to compare the system performance, 
100 English-Chinese bilingual websites are also 
mined using the URL pattern based mining 
scheme. Following (Nie et al1999; Ma and 
Liberman 1999; Chen, Chau and Yeh 2004), the 
URL pattern-based mining consists of three steps: 
(i) host crawling for URL collection; (ii) candi-
date pair identification by pre-defined URL pat-
tern matching; (iii) candidate pair verification. 
Based on these mining results, the quality of 
the mined data, the mining coverage and mining 
efficiency are measured.  
First, we benchmarked the precision of the 
mined parallel documents. 3,000 pairs of Eng-
lish-Chinese candidate documents are randomly 
selected from the output of each mining system, 
and are reviewed by human annotators. The 
document level precision is shown in Table 1.  
 
 URL pattern DOM Tree Align-
ment 
Precision 93.5% 97.2% 
Table 1: Precision of Mined Parallel Documents 
 
The document-level mining precision solely 
depends on the candidate document pair verifica-
tion module. The verification modules of both 
mining systems use the same features, and the 
only difference is that in the new mining system 
the sentence alignment score is computed with 
DOM tree alignment support. So the 3.7% im-
provement in document-level precision indirectly 
confirms the enhancement of sentence alignment. 
Secondly, the accuracy of sentence alignment 
model is benchmarked as follows: 150 English-
Chinese parallel document pairs are randomly 
taken from our mining results. All parallel sen-
tence pairs in these document pairs are manually 
annotated by two annotators with cross-
validation. We have compared sentence align-
ment accuracy with and without DOM tree 
alignment support. In case of no tree alignment 
support, all the texts in the web pages are ex-
tracted and sent to sentence aligner for alignment. 
The benchmarks are shown in Table 2. 
 
Alignment 
Method 
Num-
ber 
Right 
Num-
ber 
Wrong 
Num-
ber 
Missed 
Preci-
sion 
Recall 
Eng-Chi 
(no DOM 
tree) 
2172 285 563 86.9% 79.4% 
Eng-Chi 
(with DOM 
tree) 
2369 156 366 93.4% 86.6% 
Table 2: sentence alignment accuracy 
 
Table 2 shows that with DOM tree alignment 
support, the sentence alignment accuracy is 
greatly improved by 7% in both precision and 
recall. We also observed that the recall is lower 
than precision. This is because web pages tend to 
contain many short sentences (one or two words 
only) whose alignment is hard to identify due to 
the lack of content information. 
Although Table 2 benchmarks the accuracy of 
sentence aligner, but the quality of the final sen-
tence pair outputs depend on many other mod-
ules as well, e.g. the document level parallelism 
verification, sentence breaker, Chinese word 
breaker, etc. To further measure the quality of 
the mined data, 2,000 sentence pairs are ran-
domly picked from the final output, and are 
manually classified into three categories: (i) ex-
act parallel, (ii) roughly parallel: two parallel 
sentences involving missing words or erroneous 
additions; (iii) not parallel. Two annotators are 
494
assigned for this task with cross-validation. As is 
shown in Table 3, 93.5% of output sentence pairs 
are either exact or roughly parallel. 
 
Corpus Exact 
Parallel 
Roughly 
Parallel 
Not Parallel 
Mined 1703 167 130 
Table 3  Quality of Mined Parallel Sentences 
As we know, the absolute value of mining sys-
tem recall is hard to estimate because it is im-
practical to evaluate all the parallel data held by 
a bilingual website. Instead, we compare mining 
coverage and efficiency between the two systems. 
100 English-Chinese bilingual website are mined 
by both of the system. And the mining efficiency 
comparison is reported in Table 4. 
 
Mining 
System 
Parallel Page 
Pairs found 
& verified 
# of page 
downloads 
# of 
downloads 
per pair 
URL pat-
tern-based 
Mining 
4383 84942 19.38 
DOM Tree 
Align-
ment-
based 
Mining 
5785 13074 2.26 
 Table 4. Mining Efficiency Comparison on 100 
Bilingual Websites 
 
Although it downloads less data, the DOM 
tree based mining scheme increases the parallel 
data acquisition throughput by 32%. Furthermore, 
the ratio of downloaded page count per parallel 
pair is 2.26, which means the bandwidth usage is 
almost optimal. 
Another interesting topic is the complemen-
tarities between both mining systems. As re-
ported in Table (5),  1797 pairs of parallel docu-
ments mined by the new scheme is not covered 
by the URL pattern-based scheme. So if both 
systems are used, the throughput can be further 
increased by 41%. 
 
# of Parallel Page 
Pairs Mined by 
Both Systems  
# of Parallel Page 
Pairs Mined by 
URL Patterns 
only 
# of Parallel Page 
Pairs Mined by 
Tree Alignment 
only 
3988 395 1797 
 Table 5. Mining Results Complementarities on 
100 Bilingual Website 
8 Discussion and Conclusion 
Mining parallel data from web is a promising 
method to overcome the knowledge bottleneck 
faced by machine translation. To build a practical 
mining system, three research issues should be 
fully studied: (i) the quality of mined data, (ii) 
the mining coverage, and (iii) the mining speed. 
Exploiting DOM tree similarities helps in all the 
three issues. 
Motivated by this observation, this paper pre-
sents a new web mining scheme for parallel data 
acquisition. A DOM tree alignment model is pro-
posed to identify translationally equivalent text 
chunks and hyperlinks between two HTML 
documents. Parallel hyperlinks are used to pin-
point new parallel data, and make parallel data 
mining a recursive process. Parallel text chunks 
are fed into sentence aligner to extract parallel 
sentences.  
Benchmarks show that sentence aligner sup-
ported by DOM tree alignment achieves per-
formance enhancement by 7% in both precision 
and recall. Besides, the new mining scheme re-
duce the bandwidth cost by 8~9 times on average 
compared with the URL pattern-based mining 
scheme. In addition, the new mining scheme is 
more general and reliable, and is able to mine 
more data. Using the new mining scheme alone, 
the mining throughput is increased by 32%, and 
when combined with URL pattern-based scheme, 
the mining throughput is increased by 41%. 
References 
Alshawi, H., S. Bangalore, and S. Douglas. 2000. 
Learning Dependency Translation Models as Col-
lections of Finite State Head Transducers. Compu-
tational Linguistics, 26(1).  
Brown, P. F., J. C. Lai and R. L. Mercer. 1991. Align-
ing Sentences in Parallel Corpora. In Proceedings 
of 29th Annual Meeting of the Association for 
Computational Linguistics.  
Brown, P. E., S. A. D. Pietra, V. J. D. Pietra, and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, V19(2).  
Callison-Burch, C. and C. Bannard. 2005. Paraphras-
ing with Bilingual Parallel Corpora. In Proceed-
ings of 43th Annual Meeting of the Association for 
Computational Linguistics. 
Chen, J., R. Chau, and C.-H. Yeh. 1991. Discovering 
Parallel Text from the World Wide Web. In Pro-
ceedings of the second workshop on Australasian 
Information Security, Data Mining and Web Intel-
ligence, and Software Internationalization. 
Chen, S. 1993. Aligning Sentences in Bilingual Cor-
pora Using Lexical Information. In Proceedings of 
31st Annual Meeting of the Association for Compu-
tational Linguistics. 
Church, K. W. 1993. Char_align: A Program for 
Aligning Parallel Texts at the Character Level. In 
495
Proceedings of 31st Annual Meeting of the Asso-
ciation for Computational Linguistics. 
Fung, P. and K. Mckeown. 1994. Aligning Noisy Par-
allel Corpora across Language Groups: Word Pair 
Feature Matching by Dynamic Time Warping. In 
Proceedings of the First Conference of the Asso-
ciation for Machine Translation in the Americas. 
Gale W. A. and K. Church. 1991. A Program for 
Aligning Sentences in Parallel Corpora. In Pro-
ceedings of 29th Annual Meeting of the Association 
for Computational Linguistics. 
Hajic J., et al 2004.  Final Report: Natural Language 
Generation in the Context of Machine Translation. 
Kay M. and M. Roscheisen. 1993. Text-Translation 
Alignment. Computational Linguistics, 19(1).  
Lari K. and S. J. Young. 1990. The Estimation of Sto-
chastic Context Free Grammars using the Inside-
Outside Algorithm. Computer Speech and Lan-
guage, 4:35?56, 1990. 
Ma, X. and M. Liberman. 1999. Bits: A Method for 
Bilingual Text Search over the Web. In Proceed-
ings of Machine Translation Summit VII. 
Ng, H. T., B. Wang, and Y. S. Chan. 2003. Exploiting 
Parallel Texts for Word Sense Disambiguation: An 
Empirical Study. In Proceedings of 41st Annual 
Meeting of the Association for Computational Lin-
guistics. 
Nie, J. Y., M. S. P. Isabelle, and R. Durand. 1999. 
Cross-language Information Retrieval based on 
Parallel Texts and Automatic Mining of Parallel 
Texts from the Web. In Proceedings of the 22nd 
Annual International ACM SIGIR Conference on 
Research and Development.  
Moore, R. C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings of 
5th Conference of the Association for Machine 
Translation in the Americas. 
Munteanu D. S, A. Fraser, and D. Marcu. D., 2002.  
Improved Machine Translation Performance via 
Parallel Sentence Extraction from Comparable 
Corpora. In Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: HLT-NAACL 2004. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. In-
ducing Features Of Random Fields. In IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence.  
Resnik, P. and N. A. Smith. 2003. The Web as a Par-
allel Corpus. Computational Linguistics, 29(3)  
Shieber, S. M.  and Y. Schabes. 1990. Synchronous 
tree-adjoining grammars. In Proceedings of the 
13th International Conference on Computational 
linguistics.  
Utiyama, M. and H. Isahara 2003. Reliable Measures 
for Aligning Japanese-English News Articles and 
Sentences.  In Proceedings of 41st Annual Meeting 
of the Association for Computational Linguis-
tics.ACL 2003. 
Wu, D. 1994. Aligning a parallel English-Chinese 
corpus statistically with lexical criterias. In Pro-
ceedings of of 32nd Annual Meeting of the Associa-
tion for Computational Linguistics. 
Wu, D. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3).  
Yamada K. and K. Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proceedings of 
39th Annual Meeting of the Association for Com-
putational Linguistics.  
Zhao B. and S. Vogel. 2002. Adaptive Parallel Sen-
tences Mining From Web Bilingual News Collec-
tion. In 2002 IEEE International Conference on 
Data Mining. 
Zhang, Y., K. Wu, J. Gao, and Phil Vines. 2006. 
Automatic Acquisition of Chinese-English Parallel 
Corpus from the Web. In Proceedings of 28th 
European Conference on Information Retrieval. 
496
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824?831,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Comparative Study of Parameter Estimation Methods for 
Statistical Natural Language Processing 
Jianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova* 
*Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com 
&Brown University, Providence, RI 02912,  mj@cs.brown.edu 
 
Abstract 
This paper presents a comparative study of 
five parameter estimation algorithms on four 
NLP tasks. Three of the five algorithms are 
well-known in the computational linguistics 
community: Maximum Entropy (ME) estima-
tion with L2 regularization, the Averaged 
Perceptron (AP), and Boosting.  We also in-
vestigate ME estimation with L1 regularization 
using a novel optimization algorithm, and 
BLasso, which is a version of Boosting with 
Lasso (L1) regularization.  We first investigate 
all of our estimators on two re-ranking tasks: a 
parse selection task and a language model 
(LM) adaptation task.  Then we apply the best 
of these estimators to two additional tasks 
involving conditional sequence models: a 
Conditional Markov Model (CMM) for part of 
speech tagging and a Conditional Random 
Field (CRF) for Chinese word segmentation. 
Our experiments show that across tasks, three 
of the estimators ? ME estimation with L1 or 
L2 regularization, and AP ? are in a near sta-
tistical tie for first place. 
1 Introduction 
Parameter estimation is fundamental to many sta-
tistical approaches to NLP. Because of the 
high-dimensional nature of natural language, it is 
often easy to generate an extremely large number of 
features.  The challenge of parameter estimation is 
to find a combination of the typically noisy, re-
dundant features that accurately predicts the target 
output variable and avoids overfitting. Intuitively, 
this can be achieved either by selecting a small 
number of highly-effective features and ignoring 
the others, or by averaging over a large number of 
weakly informative features.  The first intuition 
motivates feature selection methods such as 
Boosting and BLasso (e.g., Collins 2000; Zhao and 
Yu, 2004), which usually work best when many 
features are completely irrelevant. L1 or Lasso 
regularization of linear models, introduced by 
Tibshirani (1996), embeds feature selection into 
regularization so that both an assessment of the 
reliability of a feature and the decision about 
whether to remove it are done in the same frame-
work, and has generated a large amount of interest 
in the NLP community recently (e.g., Goodman 
2003; Riezler and Vasserman 2004).  If on the other 
hand most features are noisy but at least weakly 
correlated with the target, it may be reasonable to 
attempt to reduce noise by averaging over all of the 
features.  ME estimators with L2 regularization, 
which have been widely used in NLP tasks (e.g., 
Chen and Rosenfeld 2000; Charniak and Johnson 
2005; Johnson et al 1999), tend to produce models 
that have this property.  In addition, the perceptron 
algorithm and its variants, e.g., the voted or aver-
aged perceptron, is becoming increasingly popular 
due to their competitive performance, simplicity in 
implementation and low computational cost in 
training (e.g., Collins 2002). 
While recent studies claim advantages for L1 
regularization, this study is the first of which we are 
aware to systematically compare it to a range of 
estimators on a diverse set of NLP tasks.  Gao et al 
(2006) showed that BLasso, due to its explicit use of 
L1 regularization, outperformed Boosting in the LM 
adaptation task.  Ng (2004) showed that for logistic 
regression, L1 regularization outperforms L2 regu-
larization on artificial datasets which contain many 
completely irrelevant features.  Goodman (2003) 
showed that in two out of three tasks, an ME esti-
mator with a one-sided Laplacian prior (i.e., L1 
regularization with the constraint that all feature 
weights are positive) outperformed a comparable 
estimator using a Gaussian prior (i.e., L2 regulari-
zation).  Riezler and Vasserman (2004) showed that 
an L1-regularized ME estimator outperformed an 
L2-regularized estimator for ranking the parses of a 
stochastic unification-based grammar. 
824
While these individual estimators are well de-
scribed in the literature, little is known about the 
relative performance of these methods because the 
published results are generally not directly compa-
rable.  For example, in the parse re-ranking task, 
one cannot tell whether the L2- regularized ME 
approach used by Charniak and Johnson (2005) 
significantly outperforms the Boosting method by 
Collins (2000) because different feature sets and 
n-best parses were used in the evaluations of these 
methods.  
This paper conducts a much-needed comparative 
study of these five parameter estimation algorithms 
on four NLP tasks: ME estimation with L1 and L2 
regularization, the Averaged Perceptron (AP), 
Boosting, and BLasso, a version of Boosting with 
Lasso (L1) regularization.  We first investigate all of 
our estimators on two re-ranking tasks: a parse 
selection task and a language model adaptation task. 
Then we apply the best of these estimators to two 
additional tasks involving conditional sequence 
models: a CMM for POS tagging and a CRF for 
Chinese word segmentation.  Our results show that 
ME estimation with L2 regularization achieves the 
best performing estimators in all of the tasks, and 
AP achieves almost as well and requires much less 
training time. L1 (Lasso) regularization also per-
forms well and leads to sparser models. 
2 Estimators 
All the four NLP tasks studied in this paper are 
based on linear models (Collins 2000) which re-
quire learning a mapping from inputs ? ? ? to 
outputs ? ? ?.  We are given: 
? Training samples (?? ,??) for ? = 1??, 
? A procedure ??? to generate a set of candi-
dates ???(?) for an input x,  
? A feature mapping ?:? ? ? ? ??  to map 
each (?,?) to a vector of feature values, and 
? A parameter vector ? ? ?? , which assigns a 
real-valued weight to each feature. 
For all models except the CMM sequence model for 
POS tagging, the components ???, ? and ? di-
rectly define a mapping from an input ? to an output 
?(?) as follows: 
? ? = arg max????? ? ? ?,? ? ?. (1) 
In the CMM sequence classifier, locally normalized 
linear models to predict the tag of each word token 
are chained together to arrive at a probability esti-
mate for the entire tag sequence, resulting in a 
slightly different decision rule. 
Linear models, though simple, can capture very 
complex dependencies because the features can be 
arbitrary functions of the input/output pair.  For 
example, we can define a feature to be the log con-
ditional probability of the output as estimated by 
some other model, which may in turn depend on 
arbitrarily complex interactions of ?basic? features.  
In practice, with an appropriate feature set, linear 
models achieve very good empirical results on 
various NLP tasks.  The focus of this paper however 
is not on feature definition (which requires domain 
knowledge and varies from task to task), but on 
parameter estimation (which is generic across 
tasks).  We assume we are given fixed feature 
templates from which a large number of features are 
generated.  The task of the estimator is to use the 
training samples to choose a parameter vector ?, 
such that the mapping ?(?) is capable of correctly 
classifying unseen examples. We will describe the 
five estimators in our study individually. 
2.1 ME estimation with L2 regularization 
Like many linear models, the ME estimator chooses 
? to minimize the sum of the empirical loss on the 
training set and a regularization term: 
? = arg min?  ? ? + ? ?   . (2) 
In this case, the loss term L(w) is the negative con-
ditional log-likelihood of the training data, 
 ? ? = ? log? ??  ??)
?
?=1 ,  where 
? ? ?) =
exp ? ?,? ? ? 
 exp(? ?,? ? ? ?)? ????? ? 
 
and the regularizer term ? ? = ? ??
2
?  is the 
weighted squared L2 norm of the parameters. Here, 
? is a parameter that controls the amount of regu-
larization, optimized on held-out data.  
This is one of the most popular estimators,  
largely due to its appealing computational proper-
ties: both ? ?  and ?(?) are convex and differen-
tiable, so gradient-based numerical algorithms can 
be used to find the global minimum efficiently.  
In our experiments, we used the limited memory 
quasi-Newton algorithm (or L-BFGS, Nocedal and 
Wright 1999) to find the optimal ? because this 
method has been shown to be substantially faster 
than other methods such as Generalized Iterative 
Scaling (Malouf 2002).  
825
Because for some sentences there are multiple 
best parses (i.e., parses with the same F-Score), we 
used the variant of ME estimator described in 
Riezler et al (2002), where ? ?  is defined as the 
likelihood of the best parses ? ? ?(?) relative to 
the n-best parser output ??? ? ,  (i.e., ? ? ?
???(?)): ? ? = ? log ?(?? |??)????(??)
?
?=1 . 
We applied this variant in our experiments of 
parse re-ranking and LM adaptation, and found that 
on both tasks it leads to a significant improvement 
in performance for the L2-regularied ME estimator 
but not for the L1-regularied ME estimator. 
2.2 ME estimation with L1 regularization 
This estimator also minimizes the negative condi-
tional log-likelihood, but uses an L1 (or Lasso) 
penalty. That is, ?(?) in Equation (2) is defined 
according to ? ? = ?  ??  ? . L1 regularization 
typically leads to sparse solutions in which many 
feature weights are exactly zero, so it is a natural 
candidate when feature selection is desirable. By 
contrast, L2 regularization produces solutions in 
which most weights are small but non-zero. 
Optimizing the L1-regularized objective function 
is challenging because its gradient is discontinuous 
whenever some parameter equals zero. Kazama and 
Tsujii (2003) described an estimation method that 
constructs an equivalent constrained optimization 
problem with twice the number of variables.  
However, we found that this method is impracti-
cally slow for large-scale NLP tasks. In this work 
we use the orthant-wise limited-memory qua-
si-Newton algorithm (OWL-QN), which is a mod-
ification of L-BFGS that allows it to effectively 
handle the discontinuity of the gradient (Andrew 
and Gao 2007). We provide here a high-level de-
scription of the algorithm. 
A quasi-Newton method such as L-BFGS uses 
first order information at each iterate to build an 
approximation to the Hessian matrix, ?, thus mod-
eling the local curvature of the function. At each 
step, a search direction is chosen by minimizing a 
quadratic approximation to the function: 
? ? =
1
2
 ? ? ?0 
?? ? ? ?0 + ?0
? (? ? ?0) 
where ?0 is the current iterate, and ?0 is the func-
tion gradient at ?0 .  If ? is positive definite, the 
minimizing value of ? can be computed analytically 
according to: ?? = ?0 ??
?1?0. 
L-BFGS maintains vectors of the change in gradient 
?? ? ???1 from the most recent iterations, and uses 
them to construct an estimate of the inverse Hessian 
???. Furthermore, it does so in such a way that 
??1?0 can be computed without expanding out the 
full matrix, which is typically unmanageably large. 
The computation requires a number of operations 
linear in the number of variables. 
OWL-QN is based on the observation that when 
restricted to a single orthant, the L1 regularizer is 
differentiable, and is in fact a linear function of ?.  
Thus, so long as each coordinate of any two con-
secutive search points does not pass through zero, 
?(?) does not contribute at all to the curvature of 
the function on the segment joining them.  There-
fore, we can use L-BFGS to approximate the Hes-
sian of ? ?  alone, and use it to build an approxi-
mation to the full regularized objective that is valid 
on a given orthant. To ensure that the next point is in 
the valid region, we project each point during the 
line search back onto the chosen orthant.1 At each 
iteration, we choose the orthant containing the 
current point and into which the direction giving the 
greatest local rate of function decrease points. 
This algorithm, although only a simple modifi-
cation of L-BFGS, works quite well in practice. It 
typically reaches convergence in even fewer itera-
tions than standard L-BFGS takes on the analogous 
L2-regularized objective (which translates to less 
training time, since the time per iteration is only 
negligibly higher, and total time is dominated by 
function evaluations). We describe OWL-QN more 
fully in (Andrew and Gao 2007). We also show that 
it is significantly faster than Kazama and Tsujii?s 
algorithm for L1 regularization and prove that it is 
guaranteed converge to a parameter vector that 
globally optimizes the L1-regularized objective. 
2.3 Boosting 
The Boosting algorithm we used is based on Collins 
(2000).  It optimizes the pairwise exponential loss 
(ExpLoss) function (rather than the logarithmic loss 
optimized by ME).  Given a training sample 
(?? ,??), for each possible output ?? ? ???(??), we 
                                                     
1 This projection just entails zeroing-out any coordinates 
that change sign. Note that it is possible for a variable to 
change sign in two iterations, by moving from a negative 
value to zero, and on a the next iteration moving from 
zero to a positive value. 
826
define the margin of the pair (?? ,?? ) with respect to 
? as ? ?? ,??  = ? ?? ,?? ? ? ?  ? ?? ,??  ? ?. 
Then ExpLoss is defined as 
ExpLoss ? =  exp  ?M yi , yj  
?????? ?? ?
 (3) 
Figure 1 summarizes the Boosting algorithm we 
used. It is an incremental feature selection proce-
dure. After initialization, Steps 2 and 3 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated as follows.  
First, we define Upd(?,?, ?)  as an updated 
model, with the same parameter values as ? with 
the exception of ?? , which is incremented by ?: 
Upd ?, ?, ? = (?1 ,? ,?? + ?,? ,??)  
Then, Steps 2 and 3 in Figure 1 can be rewritten as 
Equations (4) and (5), respectively. 
 ??, ?? = arg min
? ,?
ExpLoss(Upd ?, ?, ? ) (4) 
?? = Upd(???1, ??, ??) (5) 
Because Boosting can overfit we update the weight 
of ??? by a small fixed step size ?, as in Equation (6), 
following the FSLR algorithm (Hastie et al 2001).  
?? = Upd(???1, ??, ? ? sign ?? ) (6) 
By taking such small steps, Boosting imposes a 
kind of implicit regularization, and can closely 
approximate the effect of L1 regularization in a local 
sense (Hastie et al 2001).  Empirically, smaller 
values of ? lead to smaller numbers of test errors. 
2.4 Boosted Lasso 
The Boosted Lasso (BLasso) algorithm was origi-
nally proposed in Zhao and Yu (2004), and was 
adapted for language modeling by Gao et al (2006). 
BLasso can be viewed as a version of Boosting with 
L1 regularization. It optimizes an L1-regularized 
ExpLoss function: 
LassoLoss ? = ExpLoss(?) + ?(?) (7) 
where ? ? = ?  ??  ?  . 
BLasso also uses an incremental feature selec-
tion procedure to learn parameter vector ?, just as 
Boosting does.  Due to the explicit use of the regu-
larization term ?(?), however, there are two major 
differences from Boosting.  
At each iteration, BLasso takes either a forward 
step or a backward step.  Similar to Boosting, at 
each forward step, a feature is selected and its 
weight is updated according to Eq. (8) and (9). 
 ??, ?? = ??? ???
? ,?=??
ExpLoss(Upd ?, ?, ? ) (8) 
?? = Upd(???1, ??, ? ? sign ?? ) (9) 
There is a small but important difference between 
Equations (8) and (4). In Boosting, as shown in 
Equation (4), a feature is selected by its impact on 
reducing the loss with its optimal update ?? . By 
contrast, in BLasso, as shown in Equation (8), 
rather than optimizing over ? for each feature, the 
loss is calculated with an update of either +? or ??, 
i.e., grid search is used for feature weight estima-
tion.  We found in our experiments that this mod-
ification brings a consistent improvement. 
The backward step is unique to BLasso.  At each 
iteration, a feature is selected and the absolute value 
of its weight is reduced by ? if and only if it leads to 
a decrease of the LassoLoss, as shown in Equations 
(10) and (11), where ?  is a tolerance parameter. 
?? = arg min
? :???0
ExpLoss(Upd(?, ?,??sign ?? ) (10) 
?? = Upd(???1 , ??,sign(???) ? ?)  (11) 
if LassoLoss ???1,???1 ? LassoLoss ?? ,?? > ? 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are repeated 
T times; at each iteration, a feature is chosen and its 
weight is updated either backward or forward by a 
fixed amount ?.  Notice that the value of ? is adap-
tively chosen according to the reduction of ExpLoss 
during training.  The algorithm starts with a large 
initial ?, and then at each forward step the value of 
? decreases until ExpLoss stops decreasing.  This is 
intuitively desirable: it is expected that most highly 
effective features are selected in early stages of 
training, so the reduction of ExpLoss at each step in 
early stages are more substantial than in later stages.  
These early steps coincide with the Boosting steps 
most of the time.  In other words, the effect of 
backward steps is more visible at later stages.  It can 
be proved that for a finite number of features and 
? =0, the BLasso algorithm shown in Figure 2 
converges to the Lasso solution when ? ? 0. See 
Gao et al (2006) for implementation details, and 
Zhao and Yu (2004) for a theoretical justification 
for BLasso. 
1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Equation (3) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
827
2.5 Averaged Perceptron 
The perceptron algorithm can be viewed as a form 
of incremental training procedure (e.g., using sto-
chastic approximation) that optimizes a minimum 
square error (MSE) loss function (Mitchell, 1997).  
As shown in Figure 3, it starts with an initial pa-
rameter setting and updates it for each training 
example. In our experiments, we used the Averaged 
Perceptron algorithm of Freund and Schapire 
(1999), a variation that has been shown to be more 
effective than the standard algorithm (Collins 
2002).  Let ??,?  be the parameter vector after the ?th 
training sample has been processed in pass ? over 
the training data. The average parameters are de-
fined as?  =
?
??
  ??,???  where T is the number of 
epochs, and N is the number of training samples. 
3 Evaluations 
From the four tasks we consider, parsing and lan-
guage model adaptation are both examples of 
re-ranking.  In these tasks, we assume that we have 
been given a list of candidates ???(?) for each 
training or test sample  ?,? , generated using a 
baseline model.  Then, a linear model of the form in 
Equation (1) is used to discriminatively re-rank the 
candidate list using additional features which may 
or may not be included in the baseline model.  Since 
the mapping from ? to ? by the linear model may 
make use of arbitrary global features of the output 
and is performed ?all at once?, we call such a linear 
model a global model.  
In the other two tasks (i.e., Chinese word seg-
mentation and POS tagging), there is no explicit 
enumeration of ???(?).  The mapping from ? to ? 
is determined by a sequence model which aggre-
gates the decisions of local linear models via a 
dynamic program.  In the CMM, the local linear 
models are trained independently, while in the CRF 
model, the local models are trained jointly.  We call 
these two linear models local models because they 
dynamically combine the output of models that use 
only local features. 
While it is straightforward to apply the five es-
timators to global models in the re-ranking 
framework, the application of some estimators to 
the local models is problematic. Boosting and 
BLasso are too computationally expensive to be 
applied to CRF training and we compared the other 
three better performing estimation methods for this 
model. The CMM is a probabilistic sequence model 
and the log-loss used by ME estimation is most 
natural for it; thus we limit the comparison to the 
two kinds of ME models for CMMs. Note that our 
goal is not to compare locally trained models to 
globally trained ones; for a study which focuses on 
this issue, see (Punyakanok et al 2005). 
In each task we compared the performance of 
different estimators using task-specific measures. 
We used the Wilcoxon signed rank test to test the 
statistical significance of the difference among the 
competing estimators. We also report other results 
such as number of non-zero features after estima-
tion, number of training iterations, and computation 
time (in minutes of elapsed time on an XEONTM MP 
3.6GHz machine). 
3.1 Parse re-ranking 
We follow the experimental paradigm of parse 
re-ranking outlined in Charniak and Johnson 
(2005), and fed the features extracted by their pro-
gram to the five rerankers we developed.  Each uses 
a linear model trained using one of the five esti-
mators. These rerankers attempt to select the best 
parse ?  for a sentence ?  from the 50-best list of 
possible parses ??? ?  for the sentence. The li-
near model combines the log probability calculated 
by the Charniak (2000) parser as a feature with 
1,219,272 additional features.  We trained the fea-
1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 
for d=1?D. 
2 Take a forward step according to Eq. (8) and (9), and 
the updated model is denoted by w1 
3 Initialize ? = (ExpLoss(w0)-ExpLoss(w1))/? 
4 Take a backward step if and only if it leads to a de-
crease of LassoLoss according to Eq. (10) and (11), 
where ?  = 0; otherwise 
5 Take a forward step according to Eq. (8) and (9); 
update ? = min(?, (ExpLoss(wt-1)-ExpLoss(wt))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
1 Set w0 = 1 and wd = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3    For each training sample (xi, yi), i = 1?N 
4 
?? = arg max
????? ?_? 
? ?? , ? ? ? 
Choose the best candidate zi from GEN(xi) using 
the current model w, 
5       w = w +  ?(?(xi, yi) ? ?(xi, zi)), where ? is the size of 
learning step, optimized on held-out data. 
Figure 3: The perceptron algorithm 
 
828
ture weights w on Sections 2-19 of the Penn Tree-
bank, adjusted the regularizer constant ? to max-
imize the F-Score on Sections 20-21 of the Tree-
bank, and evaluated the rerankers on Section 22.  
The results are presented in Tables 12 and 2, where 
Baseline results were obtained using the parser by 
Charniak (2000).  
The ME estimation with L2 regularization out-
performs all of the other estimators significantly 
except for the AP, which performs almost as well 
and requires an order of magnitude less time in 
training.  Boosting and BLasso are feature selection 
methods in nature, so they achieve the sparsest 
models, but at the cost of slightly lower perfor-
mance and much longer training time. The 
L1-regularized ME estimator also produces a rela-
tively sparse solution whereas the Averaged Per-
ceptron and the L2-regularized ME estimator assign 
almost all features a non-zero weight.  
3.2 Language model adaptation 
Our experiments with LM adaptation are based on 
the work described in Gao et al (2006). The va-
riously trained language models were evaluated 
according to their impact on Japanese text input 
accuracy, where input phonetic symbols ?  are 
mapped into a word string ?. Performance of the 
application is measured in terms of character error 
                                                     
2
 The result of ME/L2 is better than that reported in 
Andrew and Gao (2007) due to the use of the variant of 
L2-regularized ME estimator, as described in Section 2.1. 
 CER # features time (min) #train iter 
Baseline 10.24%    
MAP 7.98%    
ME/L2 6.99% 295,337 27 665 
ME/L1 7.01% 53,342 25 864 
AP 7.23% 167,591 6 56 
Boost 7.54% 32,994 175 71,000 
BLasso 7.20% 33,126 238 250,000 
Table 3. Performance summary of estimators 
(lower is better) on language model adaptation 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  ~ >> >> >> 
ME/L1 ~  >> >> >> 
AP << <<  >> ~ 
Boost << << <<  << 
BLasso << << ~ >>  
Table 4. Statistical significance test results. 
rate (CER), which is the number of characters 
wrongly converted from ? divided by the number of 
characters in the correct transcript. 
Again we evaluated five linear rerankers, one for 
each estimator. These rerankers attempt to select the 
best conversions ? for an input phonetic string ? 
from a 100-best list ???(?)of possible conver-
sions proposed by a baseline system. The linear 
model combines the log probability under a trigram 
language model as base feature and additional 
865,190 word uni/bi-gram features.  These 
uni/bi-gram features were already included in the 
trigram model which was trained on a background 
domain corpus (Nikkei Newspaper). But in the 
linear model their feature weights were trained 
discriminatively on an adaptation domain corpus 
(Encarta Encyclopedia). Thus, this forms a cross 
domain adaptation paradigm.  This also implies that 
the portion of redundant features in this task could 
be much larger than that in the parse re-ranking 
task, especially because the background domain is 
reasonably similar to the adaptation domain.  
We divided the Encarta corpus into three sets 
that do not overlap.  A 72K-sentences set was used 
as training data, a 5K-sentence set as development 
data, and another 5K-sentence set as testing data. 
The results are presented in Tables 3 and 4, where 
Baseline is the word-based trigram model trained 
on background domain corpus, and MAP (maxi-
mum a posteriori) is a traditional model adaptation 
method, where the parameters of the background 
model are adjusted so as to maximize the likelihood 
of the adaptation data.  
 F-Score # features time (min) # train iter 
Baseline 0.8986     
ME/L2 0.9176 1,211,026 62     129  
ME/L1 0.9165 19,121 37 174  
AP 0.9164 939,248 2 8  
Boosting 0.9131 6,714 495 92,600  
BLasso 0.9133 8,085 239 56,500  
Table 1: Performance summary of estimators on 
parsing re-ranking (ME/L2: ME with L2 regulari-
zation; ME/L1:  ME with L1 regularization) 
 ME/L2 ME/L1 AP Boost BLasso 
ME/L2  >> ~ >> >> 
ME/L1 <<  ~ > ~ 
AP ~ ~  >> > 
Boost << < <<  ~ 
Blasso << ~ < ~  
Table 2: Statistical significance test results (?>>? 
or ?<<? means P-value < 0.01; > or < means 0.01 < 
P-value ? 0.05; ?~? means P-value > 0.05)  
829
The results are more or less similar to those in 
the parsing task with one visible difference: L1 
regularization achieved relatively better perfor-
mance in this task.  For example, while in the 
parsing task ME with L2 regularization significantly 
outperforms ME with L1 regularization, their per-
formance difference is not significant in this task. 
While in the parsing task the performance differ-
ence between BLasso and Boosting is not signifi-
cant, BLasso outperforms Boosting significantly in 
this task.  Considering that a much higher propor-
tion of the features are redundant in this task than 
the parsing task, the results seem to corroborate the 
observation that L1 regularization is robust to the 
presence of many redundant features. 
3.3 Chinese word segmentation 
Our third task is Chinese word segmentation 
(CWS). The goal of CWS is to determine the 
boundaries between words in a section of Chinese 
text.  The model we used is the hybrid Mar-
kov/semi- Markov CRF described by Andrew 
(2006), which was shown to have state-of-the-art 
accuracy. We tested models trained with the various 
estimation methods on the Microsoft Research Asia 
corpus from the Second International Chinese Word 
Segmentation, and we used the same train/test split 
used in the competition.  The model and experi-
mental setup is identical with that of Andrew (2006) 
except for two differences.  First, we extracted 
features from both positive and negative training 
examples, while Andrew (2006) uses only features 
that occur in some positive training example. 
Second, we used the last 4K sentences of the 
training data to select the weight of the regularizers 
and to determine when to stop perceptron training. 
We compared three of the best performing es-
timation procedures on this task: ME with L2 regu-
larization, ME with L1 regularization, and the Av-
eraged Perceptron.  In this case, ME refers to mi-
nimizing the negative log-probability of the correct 
segmentation, which is globally normalized, while 
the perceptron is trained using at each iteration the 
exact maximum-scoring segmentation with the 
current weights. We observed the same pattern as in 
the other tasks: the three algorithms have nearly 
identical performance, while L1 uses only 6% of the 
features, and the Averaged Perceptron requires 
significantly fewer training iterations.  In this case, 
L1 was also several times faster than L2. The results 
are summarized in Table 5.3 
We note that all three algorithms performed 
slightly better than the model used by Andrew 
(2006), which also used L2 regularization (96.84 
F1).  We believe the difference is due to the use of 
features derived from negative training examples. 
3.4 POS tagging 
Finally we studied the impact of the regularization 
methods on a Maximum Entropy conditional 
Markov Model (MEMM, McCallum et al 2000) for 
POS tagging. MEMMs decompose the conditional 
probability of a tag sequence given a word sequence 
as follows: 
? ?1 ? ??  ?1 ??? = ?(??|???1 ????? ,?1 ???)
?
?=1
 
where the probability distributions for each tag 
given its context are ME models.  Following pre-
vious work (Ratnaparkhi, 1996), we assume that the 
tag of a word is independent of the tags of all pre-
ceding words given the tags of the previous two 
words (i.e., ?=2 in the equation above). The local 
models at each position include features of the 
current word, the previous word, the next word, and 
features of the previous two tags.  In addition to 
lexical identity of the words, we used features of 
word suffixes, capitalization, and number/special 
character signatures of the words. 
We used the standard splits of the Penn Treebank 
from the tagging literature (Toutanova et al 2003) 
for training, development and test sets.  The training 
set comprises Sections 0-18, the development set ? 
Sections 19-21, and the test set ? Sections 22-24.  
We compared training the ME models using L1 and 
L2 regularization.  For each of the two types of 
regularization we selected the best value of the 
regularization constant using grid search to optim-
ize the accuracy on the development set.  We report 
final accuracy measures on the test set in Table 6.  
The results on this task confirm the trends we 
have seen so far.  There is almost no difference in 
                                                     
3 Only the L2 vs. AP comparison is significant at a 0.05 
level according to the Wilcoxon signed rank test. 
 Test F1 # features # train iter 
ME/L2 0.9719 8,084,086 713 
ME/L1 0.9713 317,146 201 
AP 0.9703 1,965,719 162 
Table 5. Performance summary of estimators on 
CWS 
 
830
accuracy of the two kinds of regularizations, and 
indeed the differences were not statistically signif-
icant.  Estimation with L1 regularization required 
considerably less time than estimation with L2, and 
resulted in a model which is more than ten times 
smaller.  
4 Conclusions 
We compared five of the most competitive para-
meter estimation methods on four NLP tasks em-
ploying a variety of models, and the results were 
remarkably consistent across tasks.  Three of the 
methods ? ME estimation with L2 regularization, 
ME estimation with L1 regularization, and the Av-
eraged Perceptron ? were nearly indistinguishable 
in terms of test set accuracy, with ME estimation 
with L2 regularization perhaps enjoying a slight 
lead.  Meanwhile, ME estimation with L1 regulari-
zation achieves the same level of performance while 
at the same time producing sparse models, and the 
Averaged Perceptron provides an excellent com-
promise of high performance and fast training. 
These results suggest that when deciding which 
type of parameter estimation to use on these or 
similar NLP tasks, one may choose any of these 
three popular methods and expect to achieve com-
parable performance.  The choice of which to im-
plement should come down to other considerations: 
if model sparsity is desired, choose ME estimation 
with L1 regularization (or feature selection methods 
such as BLasso); if quick implementation and 
training is necessary, use the Averaged Perceptron; 
and ME estimation with L2 regularization may be 
used if it is important to achieve the highest ob-
tainable level of performance. 
References 
Andrew, G. 2006. A hybrid Markov/semi-Markov condi-
tional random field for sequence segmentation. In EMNLP, 
465-472. 
Andrew, G. and Gao, J. 2007. Scalable training of 
L1-regularized log-linear models. In ICML. 
Charniak, E. 2000. A maximum-entropy-inspired parser. In 
NAACL, 132-139. 
Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best 
parsing and MaxEnt discriminative re-ranking. In ACL. 
173-180. 
Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing 
techniques for ME models. IEEE Trans. On Speech and Audio 
Processing, 8(2): 37-50. 
Collins, M. 2000. Discriminative re-ranking for natural 
language parsing. In ICML, 175-182. 
Collins, M. 2002. Discriminative training methods for hid-
den Markov models: Theory and experiments with per-
ceptron algorithms. In EMNLP, 1-8. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. In 
ICML?98.  
Freund, Y. and Schapire, R. E. 1999. Large margin classifica-
tion using the perceptron algorithm. In Machine Learning, 
37(3): 277-296. 
Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of 
statistical learning. Springer-Verlag, New York. 
Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso 
methods for language modeling. In ACL. 
Goodman, J. 2004. Exponential priors for maximum entropy 
models. In NAACL. 
Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 
1999. Estimators for stochastic ?Unification-based? 
grammars. In ACL. 
Kazama, J. and Tsujii, J. 2003. Evaluation and extension of 
maximum entropy models with inequality constraints. In 
EMNLP. 
Malouf, R. 2002. A comparison of algorithms for maximum 
entropy parameter estimation. In HLT. 
McCallum A, D. Freitag and F. Pereira. 2000. Maximum 
entropy markov models for information extraction and 
segmentation. In ICML. 
Mitchell, T. M. 1997. Machine learning. The McGraw-Hill 
Companies, Inc. 
Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, 
and rotational invariance. In ICML. 
Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. 
Springer, New York. 
Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. 
Learning and inference over constrained output. In IJCAI. 
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech 
tagger. In EMNLP. 
Riezler, S., and Vasserman, A. 2004. Incremental feature 
selection and L1 regularization for relax maximum entro-
py modeling. In EMNLP.  
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., 
and Johnson, M. 2002. Parsing the wall street journal using 
a lexical-functional grammar and discriminative estima-
tion techniques. In ACL. 271-278.  
Tibshirani, R. 1996. Regression shrinkage and selection via 
the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 
2003. Feature-rich Part-of-Speech tagging with a cyclic 
dependency network. In HLT-NAACL, 252-259. 
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics 
Department, U. C. Berkeley. 
 Accuracy (%) # features # train iter 
MEMM/L2 96.39 926,350 467 
MEMM/L1 96.41 84,070 85 
Table 6. Performance summary of estimators on 
POS tagging 
831
Chinese Named Entity Identification Using Class-based
Language Model1
Jian Sun*, Jianfeng Gao, Lei Zhang**, Ming Zhou, Changning Huang
* Beijing University of Posts & Telecommunications, China, jiansun_china@hotmail.com
#Microsoft Research Asia, {jfgao, mingzhou, cnhuang}@microsoft.com Tsinghua University, China
1 This work was done while the author was visiting Microsoft Research Asia
$EVWUDFW
We consider here the problem of Chinese 
named entity (NE) identification using 
statistical language model(LM). In this 
research, word segmentation and NE 
identification have been integrated into a
unified framework that consists of several 
class-based language models. We also adopt a 
hierarchical structure for one of the LMs so 
that the nested entities in organization names 
can be identified. The evaluation on a large test 
set shows consistent improvements. Our 
experiments further demonstrate the 
improvement after seamlessly integrating with 
linguistic heuristic information, cache-based 
model and NE abbreviation identification.
 ,QWURGXFWLRQ
1(LGHQWLILFDWLRQ is the key technique in many
applications such as information extraction, 
question answering, machine translation and so 
on. English NE identification has achieved a 
great success. However, for Chinese, NE 
identification is very different. There is no 
space to mark the word boundary and no 
standard definition of words in Chinese. The 
Chinese NE identification and word 
segmentation are interactional in nature.
This paper presents a unified approach 
that integrates these two steps together using a
class-based LM, and apply Viterbi search to 
select the global optimal solution. The 
class-based LM consists of two sub-models, 
namely the context model and the entity model. 
The context model estimates the probability of 
generating a NE given a certain context, and
the entity model estimates the probability of a 
sequence of Chinese characters given a certain 
kind of NE. In this study, we are interested in 
three kinds of Chinese NE that are most 
commonly used, namely person name (PER), 
location name (LOC) and organization name 
(ORG). We have also adopted a variety of 
approaches to improving the LM. In addition, a 
hierarchical structure for organization LM is 
employed so that the nested PER, LOC in 
ORG can be identified. 
The evaluation is conducted on a large test 
set in which NEs have been manually tagged. 
The experiment result shows consistent 
improvements over existing methods. Our 
experiments further demonstrate the 
improvement after integrating with linguistic 
heuristic information, cache-based model and 
NE abbreviation identification. The precision
of PER, LOC, ORG on the test set is 79.86%, 
80.88%, 76.63%, respectively; and the recall is
87.29%, 82.46%, 56.54%, respectively. 5HODWHG:RUN
Recently, research on English NE 
identification has been focused on the 
machine-learning approaches, including 
hidden Markov model (HMM), maximum 
entropy model, decision tree and 
transformation-based learning, etc. (Bikel et al 
1997; Borthwick et al 1999; Sekine et al 
1998). Some systems have been applied to real 
application.
Research on Chinese NE identification is,
however, still at its early stage. Some 
researches apply methods of English NE 
identification to Chinese. Yu et al(1997) 
applied the HMM approach where the NE 
identification is formulated as a tagging 
problem using Viterbi algorithm. In general, 
current approaches to NE identification (e.g. 
Chen, 1997) usually contain two separate steps: 
word segmentation and NE identification. The 
word segmentation error will definitely lead to 
errors in the NE identification results. Zhang 
(2001) put forward class-based LM for 
Chinese NE identification. We further develop 
this idea with some new features, which leads 
to a new framework. In this framework, we 
integrate Chinese word segmentation and NE 
identification into a unified framework using a 
class-based language model (LM). &ODVVEDVHG/0 IRU1(,GHQWLILFDWLRQ
The n-gram LM is a stochastic model which 
predicts the next word given the previous n-1
words by estimating the conditional probability 
P(w
n
|w1?wn-1). In practice, trigram 
approximation P(wi|wi-2wi-1) is widely used, 
assuming that the word wi depends only on two 
preceding words wi-2 and wi-1. Brown et al(1992) 
put forward and discussed n-gram models 
based on classes of words. In this section, we 
will describe how to use class-based trigram 
model for NE identification. Each kind of NE 
(including PER, LOC and ORG) is defined as a 
class in the model. In addition, we differentiate 
the transliterated person name (FN) from the 
Chinese person name since they have different 
constitution patterns. The four classes of NE 
used in our model are shown in Table 1. All 
other words are also defined as individual 
classes themselves (i.e. one word as one class). 
Consequently, there are _9_+4 classes in our 
model, where _9_ is the size of vocabulary.7DEOH : Classes defined in class-based model7DJ 'HVFULSWLRQ
PN Chinese person name
FN Transliterated person name
LN Location name
ON Organization name 7KH/DQJXDJH0RGHOLQJ
 )RUPXODWLRQ
Given a Chinese character sequence 6=V?VQ, 
the task of Chinese NE identification is to find 
the optimal class sequence &=F?FP (P<=Q) 
that maximizes the probability 3&_6. It can be 
expressed in the equation (1) and we call it 
class-based model.
The class-based model consists of two 
sub-models: the context model 3& and the 
entity model P (S|C). The context model 
indicates the probability of generating a NE
class given a (previous) context. P(C) is a 
priori probability, which is computed 
according to Equation (2):
?
=
--@
P
L LLL FFF3&3 1 12 )|()( (2)
P(C) can be estimated using a NE labeled 
corpus. The entity model can be parameterized 
by Equation (3):
?
=
--
--
@
@
=
P
M MHQGFVWDUWF
PQVWDUWFHQGF
PQ
FVV3
FFVVVV3 FFVV3&63
MM
P
1
11
11
)|]...([
)...|]...]...[...([
)...|...()|(
1 (3)
The entity model estimates the generative 
probability of the Chinese character sequence in
square bracket pair (i.e. starting from FMVWDUW to FMHQG) given the specific NE class.
For different class, we define the different 
entity model.
For the class of PER (including PN and 
FN), the entity model is a FKDUDFWHUEDVHG
trigram model as shown in Equation (4).
?-
-=
--
--
==
=
HQGF
VWDUWFN MNNN
MHQGFVWDUWF
M
M
MM
3(5FVVV3
3(5FVV3
),,|(
)|]...([
12
(4)
where s can be any characters occurred in a 
person name. For example, the generative 
probability of character sequence "?# (Li 
Dapeng) is much larger than that of ??H
(many years) given the PER since " is a 
commonly used family name, and ? and # are 
commonly used first names. The probabilities
can be estimated with the person name list.
For the class of LOC, the entity model is a ZRUGEDVHG trigram model as shown in
Equation (5).
)|(maxarg* 6&3& &=
)|()(maxarg &63&3& ?= (1)
)|]...([ /2&FVV3 MHQGFVWDUWF MM =--
@/2&FZZ_Z3>PD[
/2&F_ZZ3PD[
O
N MNNN:
MO:
?
=
-- ==
=?
(5)
where W = w1?wl is possible segmentation 
result of  character sequence HQGFVWDUWF MM VV -- ... .
For the class of ORG, the construction is 
much more complicated because an ORG often
contain PER and/or LOC. For example, the 
ORG ????N@? ? (Air China 
Corporation) contains the LOC ??? (China).
It is beneficial to such applications as question 
answering, information extraction and so on if 
nested NE can be identified as well . In order to 
identify the nested PER, LOC in ORG 2, we 
adopted class-based LMs for ORG further, in 
which there are three sub models, one is the 
class generative model, and the others are entity 
model: person name model and location name 
model in ORG. Therefore, the entity model of 
ORG is shown in Equation (6) which is almost 
same as Equation (1).
)|]...([ 25*FVV3 MHQGFVWDUWF MM =--
??
??
?
?
?
??
??
?
?
?
=?
=
@
???
?
???
?
=?
==
=@
?
?
=
--
=
--
--
--
N
L MLHQGFVWDUWF
N
L MLLL&
MNHQGFVWDUWF
MN&
MHQGFVWDUWFM&
25*FFVV3
25*FF
F
_3F

25*FFFVV3 25*FFF3
25*F&VV3F&3
LL
MM
MM
1
''
'
1
1
'
'
),'|]...([
max
),'...'|]...([
)|'...'(
max
)],'|]...([)|'([max
(6)
where '...'
1
' NFF& = is the sequence of class 
corresponding to the Chinese character 
sequence.
In addition, if MF is a normal word, 
1)|]...([ =-- MHQGFVWDUWF FVV3 MM .                   (7)
Based on the context model and entity 
models, we can compute the probability 3&_6
2 For simplification, only nested person, location 
names are identified in organization. The nested 
person in location is not identified because of low 
frequency
and can get the optimal class sequence The 
Chinese PER and transliterated PER share the 
same context class model when computing the 
probability. 0RGHOV(VWLPDWLRQ
As discussed in 3.1.1, there are two kinds of 
probabilities to be estimated: P(C) and P(S|C) . 
Both probabilities are estimated using 
Maximum Likelihood Estimation (MLE) with 
the annotated training corpus.
The parser NLPWin3 was used to tag the 
training corpus. As a result, the corpus was 
annotated with NE marks. Four lists were 
extracted from the annotated corpus and each 
list corresponds one NE class. The context 
model 3& was trained with the annotated 
corpus and the four entity models were trained 
with corresponding NE lists. The Figure 1 
shows the training process. (Begin of sentence 
(BOS) and end of sentence (EOS) is added)
NLPWin
Tagged
Sentence
<LOC>b?</LOC>?<PER>??</PER>,$<ORG>???N@?</ORG>X???<LOC>?<LOC>
Context
Class
BOS LN ? PN ,$ ON X??? LN  EOS
LN list b??
FN list ??
ON list ???N@?
ON Class list LN ??N@?
Corresponding 
English
Sentence
<LOC>U.S.</LOC>president 
<PER>Bush</PER> arrived in 
<LOC> P.R. China </LOC> by flight 
No.1 of <ORG>Air China 
Corp.</ORG>
Figure 1:  Example of  Training Process 'HFRGHU
Given a sequence of Chinese characters, the 
decoding process consists of the following 
three steps:6WHS  All possible word segmentations are 
generated using a Chinese lexicon 
containing 120,050 entries. The lexicon is 
only used for segmentation and there is no 
NE tag in it even if one word is PER, LOC or 
3 NLPWin system is a natural language processing 
system developed by Microsoft Research.
ORG. For example, ?? (Beijing) is not 
tagged as LOC.6WHS NE candidates are generated from any 
one or more segmented character strings and 
the corresponding generative probability for 
each candidate is computed using entity 
models described in Equation (4)?(7). 6WHS  Viterbi search is used to select 
hypothesis with the highest probability as 
the best output. Furthermore, in order to 
identify nested named entities, two-pass
Viterbi search is adopted. The inner Viterbi 
search is corresponding to Equation (6) and 
the outer one corresponding to Equation (1). 
After the two-pass searches, the word 
segmentation and the named entities 
(including nested ones) can be obtained. ,PSURYHPHQW
There are some problems with the framework 
of NE identification using the class-based LM.
First, redundant candidates NEs are generated 
in the decoding process, which results in very 
large search space. The second problem is that 
data sparseness will seriously influence the 
performance. Finally, the abbreviation of NEs 
cannot be handled effectively. In the following 
three subsections, we provide solutions to the 
three problems mentioned above. +HXULVWLF,QIRUPDWLRQ
In order to overcome the redundant candidate 
generation problem, the heuristic information 
is introduced into the class-based LM. The 
following resources were used: (1) Chinese 
family name list, containing 373 entries (e.g. ?
(Zhang), _ (Wang)); (2) transliterated name 
character list, containing 618 characters (e.g.?
(shi), S (dun)); and (3) ORG keyword list,
containing 1,355 entries (e.g. ?: (university),@?(corporation)).
The heuristic information is used to 
constrain the generation of NE candidates. For 
PER (PN), only PER candidates beginning with 
the family name is considered. For PER (FN), a 
candidate is generated only if all its composing 
character belongs to the transliterated name 
character list. For ORG, a candidate is excluded 
if it does not contain one ORG keyword.
Here, we do not utilize the LOC keyword to 
generate LOC candidate because of the fact that 
many LOC do not end with keywords.
 &DFKH0RGHO
The cache entity model can address the data 
sparseness problem by adjusting the parameters 
continually as NE identification proceeds. The 
basic idea is to accumulate Chinese character or 
word n-gram so far appeared in the document 
and use them to create a local dynamic entity
model such as )|( 1-LLELFDFKH ZZ3 and 
)( LXQLFDFKH Z3 . We can interpolate the cache 
entity model with the static entity 
LM )...|( 121 -- LLLVWDWLF ZZZZ3 :ZZZ_Z3 LLLFDFKH -- (8)
)....|()1(
)|()(
1121
121
-
-
--+
+=
LLVWDWLF
LLELFDFKHLXQLFDFKH ZZZ3 ZZ3Z3 OO OO
where ]1,0[, 21 ?OO are interpolation weight 
that is determined on the held-out data set. 'HDOLQJZLWK$EEUHYLDWLRQ
We found that many errors result from the 
occurrence of abbreviation of person, location, 
and organization. Therefore, different 
strategies are adopted to deal with 
abbreviations for different kinds of NEs. For 
PER, if Chinese surname is followed by the 
title, then this surname is tagged as PER. For 
example, ??S (President Zuo) is tagged as 
<PER>?</PER> ?S. For LOC, if at least 
two location abbreviations occur consecutive, 
the individual location abbreviation is tagged as 
LOC. For example,?G? (Sino-Japan 
relation) is tagged as <LOC> 
</LOC><LOC>?</LOC> G?. For ORG, if 
organization abbreviation is followed by LOC, 
which is again followed by organization 
keyword, the three units are tagged as one ORG. 
For example,  E ? ? ? ? (Chinese 
Communist Party Committee of Beijing) i s 
tagged as <ORG>E<LOC>??</LOC> ?? </ORG>. At present, we collected 112 
organization abbreviations and 18 location 
abbreviations.
 ([SHULPHQWV
 (YDOXDWLRQ0HWULF
We conduct evaluations in terms of precision (P) 
and recall (R).
1(LGHQWLILHGRIQXPEHU 1(LGHQWLILHGFRUUHFWO\RIQXPEHU3 = (9)
1(DOORIQXPEHU 1(LGHQWLILHGFRUUHFWRIQXPEHU5 = (10)
We also used the F-measure, which is defined 
as a weighted combination of precision and 
recall as Equation (11):
53 53)  +? ??+= EE (11)
where E is the relative weight of precision and 
recall.
There are two differences between MET 
evaluation and ours. First, we include nested 
NE in our evaluation whereas MET does not.
Second, in our evaluation, only NEs with 
correct boundary and type label are considered 
the correct identifications. In MET, the 
evaluation is somewhat flexible. For example, a 
NE may be identified partially correctly if the 
label is correct but the boundary is wrongly 
detected. 'DWD6HWV
The training text corpus contains data from
People?s Daily (Jan.-Jun.1998). It contains
357,544 sentences (about 9,200,000 Chinese 
characters). This corpus includes 104,487
Chinese PER, 51,708 transliterated PER, 
218,904 LOC, and 87,391 ORG. These data 
was obtained after this corpus was parsed with 
NLPWin.
We built the wide coverage test data 
according to the guidelines4 that are just same 
as those of 1999 IEER. The test set (as shown in 
Table 2) contains half a million Chinese 
characters; it is a balanced test set covering 11 
domains. The test set contains 11,844 sentences,
49.84% of the sentences contain at least one NE. 
The number of characters in NE accounts for
8.448% in all Chinese characters.
We can see that the test data is much larger 
than the MET test data and IEER data
4 The difference between IEER?s guidelines and 
ours is that the nested person and location name in 
organization are tagged in our guidelines.
7DEOH: Statistics of Open-Test
Number of NE TokensID Domain
PER LOC ORG
Size
(byte)
1 Army 65 202 25 19k
2 Computer 75 156 171 59k
3 Culture 548 639 85 138k
4 Economy 160 824 363 108k
5 Entertainment 672 575 139 104k
6 Literature 464 707 122 96k
7 Nation 448 1193 250 101k
8 People 1147 912 403 116k
9 Politics 525 1148 218 122k
10 Science 155 204 87 60k
11 Sports 743 1198 628 114k
Total 5002 7758 2491 1037k
 7UDLQLQJ'DWD3UHSDUDWLRQ
The training data produced by NLPWin has 
some noise due to two reasons. First, the NE 
guideline used by NLPWin is different from 
the one we used. For example, in NLPWin, ???(Beijing City) is tagged as <LOC>??
</LOC> ?, whereas ??? should be LOC 
in our definition. Second, there are some errors
in NLPWin results. We utilized 18 rules to 
correct the frequent errors. The following 
shows some examples.
The Table 4 shows the quality of our training 
corpus.
Table 4   Quality of Training Corpus 
NE P (%) R (%) F (%)
PER 61.05 75.26 67.42
LOC 78.14 71.57 74.71
ORG 68.29 31.50 43.11
Total 70.07 66.08 68.02
 ([SHULPHQWV
We conduct incrementally the following four 
experiments:
(1) Class-based LM, we view the results as 
baseline performance;
(2) Integrating heuristic information into (1);
(3) Integrating Cache-based LM with (2);
(4) Integrating NE abbreviation processing 
with (3).
/1/RFDWLRQ.H\ : /1/1O /1Z : 21_b_?_???? : /1?
 &ODVVEDVHG/0%DVHOLQH
Based on the basic class-based models 
estimated with the training data, we can get the 
baseline performance, as is shown in Table 5. 
Comparing Table 4 and Table 5, we found that 
the performance of baseline is better than the 
quality of training data.
Table 5    Baseline Performance
NE P (%) R (%) F (%)
PER 65.70 84.37 73.87
LOC 82.73 76.03 79.24
ORG 56.55 38.56 45.86
Total 72.61 72.44 72.53
 ,QWHJUDWLQJ+HXULVWLF,QIRUPDWLRQ
In this part, we want to see the effects of using 
heuristic information. The results are shown in 
Table 6. In experiments, we found that by 
integrating the heuristic information, we not 
only achieved more efficient decoding, but also 
obtained higher NE identification precision. For 
example, the precision of PER increases from 
65.70% to 77.63%, and precision of ORG 
increases from 56.55% to 81.23%.  The reason 
is that adopting heuristic information reduces 
the noise influence.
However, we noticed that the recall of PER 
and LOC decreased a bit. There are two reasons. 
First, organization names without organization 
ending keywords were not marked as ORG. 
Second, Chinese names without surnames were 
also missed.
Table 6 Results of Heuristic Information Integrated 
into the Class-based LM
NE P (%) R (%) F (%)
PER 77.63 80.89 79.23
LOC 80.05 80.80 80.42
ORG 81.23 36.65 50.51
Total 79.26 73.41 76.23
 ,QWHJUDWLQJ&DFKHEDVHG/0
Table 7 shows the evaluation results after 
cache-based LM was integrated. From Table 6 
and Table 7, we found that almost all the 
precision and recall of PER, LOC, ORG have 
obtained slight improvements.
Table 7   Results of our system
NE P (%) R (%) F (%)
PER 79.12 82.06 80.57
LOC 80.11 81.27 80.69
ORG 79.71 39.89 53.17
Total 79.72 74.58 77.06 ,QWHJUDWLQJ ZLWK 1( $EEUHYLDWLRQ3URFHVVLQJ
In this experiment, we integrated with NE 
abbreviation processing. As shown in Table 8, 
the experiment result indicates that the recall of 
PER, LOC, ORG increased from 82.06%, 
81.27%, 36.65% to 87.29%, 82.46%, 56.54%, 
respectively.
Table 8   Results of our system
NE P (%) R (%) F (%)
PER 79.86 87.29 83.41
LOC 80.88 82.46 81.66
ORG 76.63 56.54 65.07
Total 79.99 79.68 79.83 6XPPDU\
From above data, we observed that (1) the class 
based SLM performs better than the training 
data automatically produced with the parser; (2) 
the distinct improvements is achieved by using 
heuristic information; (3) Furthermore, our 
method of dealing with abbreviation increases 
the recall of NEs.
In addition, the cache-based LM increases 
the performance not so much. The reason is as 
follows: The cache-based LM is based on the 
hypothesis that a word used in the recent past is 
much likely either to be used soon than its 
overall frequency in the language or a 3 -gram 
model would suggest (Kuhn, 1990). However, 
we found that the same NE often vari es its 
morpheme in the same document. For example, 
the same NE  E ? ? ? ? (Chinese 
Communist Party Committee of Beijing),??? ? (Committee of Beijing City), ? ?
(Committee) occur in order.
Furthermore, we notice that the 
segmentation dictionary has an important 
impact on the performance of NE 
identification. We do not think it is better if 
more words are added into dictionary. For 
example, because ??(Chinese) is in our 
dictionary, there is much possibility that ?
(China) in ?? is missed identified.
 (YDOXDWLRQZLWK0(7DQG,((57HVW'DWD
We also evaluated on the MET2 test data and 
IEER test data. The results are shown in Table 
9. The results on MET2 are lower than the 
highest report of MUC7 (PER: Precision 66%, 
Recall 92%; LOC: Precision 89%, Recall 91%; 
ORG: Precision 89%, Recall 88%, 
http://www.itl.nist.gov). We speculate the 
reasons for this in the following. The main 
reason is that our class-based LM was 
estimated with a general domain corpus, which 
is quite different from the domain of MUC 
data. Moreover, we didn?t use a NE dictionary. 
Another reason is that our NE definitions are 
slightly different from MET2.
Table 9 Results on MET2 and IEER
MET2 Data IEER DataNE
P
(%)
R 
(%)
F
(%)
P 
(%)
R 
(%)
F 
(%)
PER 65.86 94.25 77.54 79.38 84.43 81.83
LOC 77.42 89.60 83.07 79.09 80.18 79.63
ORG 88.47 75.33 81.38 88.03 62.30 72.96
Total 77.89 86.09 81.79 80.82 76.78 78.75
 &RQFOXVLRQV 	)XWXUHZRUN
In this research, Chinese word segmentation 
and NE identification has been integrated into 
a framework using class-based language 
models (LM). We adopted a hierarchical 
structure in ORG model so that the nested 
entities in organization names can be identified. 
Another characteristic is that our NE 
identification do not utilize NE dictionary
when decoding.
The evaluation on a large test set shows
consistent improvements. The integration of 
heuristic information improves the precision 
and recall of our system. The cache-based LM 
increases the recall of NE identification to 
some extent. Moreover, some rules dealing 
with abbreviations of NEs have increased 
dramatically the performance. The precision of 
PER, LOC, ORG on the test set is 79.86%, 
80.88%, 76.63%, respectively; and the recall is
87.29%, 82.46%, 56.54%, respectively.
In our future work, we will be focusing 
more on NE coreference using language model. 
Second, we intend to extend our model to 
include the part-of-speech tagging model to 
improve the performance. At present, the 
class-based LM is based on the general domain 
and we may need to fine-tune the model for a 
specific domain. 
ACKNOWLEDGEMENT
I would like to thank Ming Zhou, Jianfeng 
Gao, Changning Huang, Andi Wu, Hang Li
and other colleagues from Microsoft Research 
for their help. And I want to thank especially 
Lei Zhang from Tsinghua University for his 
help in developing the ideas.5HIHUHQFHV
Borthwick. A. (1999) A Maximum Entropy 
Approach to Named Entity Recognition. PhD 
Dissertation
Bikel D., Schwarta R., Weischedel. R. (1997) An 
algorithm that learns what?s in a name. Machine 
Learning 34, pp. 211-231
Brown, P. F., DellaPietra, V. J., deSouza, P. V., Lai, 
J. C., and Mercer, R. L. (1992). Class-based 
n-gram models of natural language. Computational 
Linguistics, 18(4):468--479.
Chinchor. N. (1997) MUC-7 Named Entity Task 
Definition Version 3.5. Available by from 
ftp.muc.saic.com/pub/MUC/MUC7-guidelines
Chen H.H., Ding Y.W., Tsai S.C. and Bian G.W. 
(1997) Description of the NTU System Used for 
MET2
Gao J.F., Goodman J., Li M.J., Lee K.F. (2001)  
Toward a unified Approach to Statistical Language 
Modeling for Chinese. To appear in ACM 
Transaction on Asian Language Processing
Kuhn R., Mori. R.D. (1990) A Cache-Based 
Natural Language Model for Speech Recognition. 
IEEE Transaction on Pattern Analysis and Machine 
Intelligence.Vol.12. No. 6. pp 570-583
Mikheev A., Grover C. and Moens M. (1997)
Description of the LTG System Used for MUC-7
Sekine S., Grishman R. and Shinou H. (1998), ?A 
decision tree method for finding and classifying 
names in Japanese texts?, Proceedings of the Sixth 
Workshop on Very Large Corpora, Canada 
Yu S.H., Bai S.H. and Wu P. (1997) Description of 
the Kent Ridge Digital Labs System Used for 
MUC-7
Zhang L. (2001) Study on Chinese Proofreading 
Oriented Language Modeling, PhD Dissertation
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 209?216, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Minimum Sample Risk Methods for Language Modeling1  
Jianfeng Gao  
Microsoft Research Asia 
jfgao@microsoft.com 
Hao Yu,  Wei Yuan 
 Shanghai Jiaotong Univ., China 
Peng Xu 
John Hopkins Univ., U.S.A. 
xp@clsp.jhu.edu 
                                                     
1
  The work was done while the second, third and fourth authors were visiting Microsoft Research Asia. Thanks to Hisami Suzuki for 
her valuable comments. 
Abstract 
This paper proposes a new discriminative 
training method, called minimum sample risk 
(MSR), of estimating parameters of language 
models for text input. While most existing 
discriminative training methods use a loss 
function that can be optimized easily but 
approaches only approximately to the objec-
tive of minimum error rate, MSR minimizes 
the training error directly using a heuristic 
training procedure. Evaluations on the task 
of Japanese text input show that MSR can 
handle a large number of features and train-
ing samples; it significantly outperforms a 
regular trigram model trained using maxi-
mum likelihood estimation, and it also out-
performs the two widely applied discrimi-
native methods, the boosting and the per-
ceptron algorithms, by a small but statisti-
cally significant margin. 
1 Introduction 
Language modeling (LM) is fundamental to a wide 
range of applications, such as speech recognition 
and Asian language text input (Jelinek 1997; Gao et 
al. 2002). The traditional approach uses a paramet-
ric model with maximum likelihood estimation (MLE), 
usually with smoothing methods to deal with data 
sparseness problems. This approach is optimal 
under the assumption that the true distribution of 
data on which the parametric model is based is 
known. Unfortunately, such an assumption rarely 
holds in realistic applications. 
An alternative approach to LM is based on the 
framework of discriminative training, which uses a 
much weaker assumption that training and test 
data are generated from the same distribution but 
the form of the distribution is unknown. Unlike the 
traditional approach that maximizes the function 
(i.e. likelihood of training data) that is loosely as-
sociated with error rate, discriminative training 
methods aim to directly minimize the error rate on 
training data even if they reduce the likelihood. So, 
they potentially lead to better solutions. However, 
the error rate of a finite set of training samples is 
usually a step function of model parameters, and 
cannot be easily minimized. To address this prob-
lem, previous research has concentrated on the 
development of a loss function that approximates 
the exact error rate and can be easily optimized. 
Though these methods (e.g. the boosting method) 
have theoretically appealing properties, such as 
convergence and bounded generalization error, we 
argue that the approximated loss function may 
prevent them from attaining the original objective 
of minimizing the error rate. 
In this paper we present a new estimation pro-
cedure for LM, called minimum sample risk (MSR). It 
differs from most existing discriminative training 
methods in that instead of searching on an ap-
proximated loss function, MSR employs a simple 
heuristic training algorithm that minimizes the 
error rate on training samples directly. MSR oper-
ates like a multidimensional function optimization 
algorithm: first, it selects a subset of features that 
are the most effective among all candidate features. 
The parameters of the model are then optimized 
iteratively: in each iteration, only the parameter of 
one feature is adjusted. Both feature selection and 
parameter optimization are based on the criterion 
of minimizing the error on training samples. Our 
evaluation on the task of Japanese text input shows 
that MSR achieves more than 20% error rate reduc-
tion over MLE on two newswire data sets, and it 
also outperforms the other two widely applied 
discriminative methods, the boosting method and 
the perceptron algorithm, by a small but statisti-
cally significant margin. 
Although it has not been proved in theory that 
MSR is always robust, our experiments of cross- 
domain LM adaptation show that it is. MSR can 
effectively adapt a model trained on one domain to 
209
different domains. It outperforms the traditional 
LM adaptation method significantly, and achieves 
at least comparable or slightly better results to the 
boosting method and the perceptron algorithm. 
2 IME Task and LM 
This paper studies LM on the task of Asian lan-
guage (e.g. Chinese or Japanese) text input. This is 
the standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for input 
method editor, based on the name of the commonly 
used Windows-based application. 
Performance on IME is measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. Current IME systems make 
about 5-15% CER in conversion of real data in a 
wide variety of domains (e.g. Gao et al 2002).  
Similar to speech recognition, IME is viewed as 
a Bayes decision problem. Let A be the input pho-
netic string. An IME system?s task is to choose the 
most likely word string W* among those candidates 
that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity since the phonetic string is 
inputted by users. Moreover, if we do not take into 
account typing errors, it is reasonable to assume a  
unique mapping from W and A in IME, i.e. P(A|W) 
= 1. So the decision of Equation (1) depends solely 
upon P(W), making IME a more direct evaluation 
test bed for LM than speech recognition. Another 
advantage is that it is easy to convert W to A (for 
Chinese and Japanese), which enables us to obtain 
a large number of training data for discriminative 
learning, as described later.  
The values of P(W) in Equation (1) are tradi-
tionally calculated by MLE: the optimal model 
parameters ?* are chosen in such a way that 
P(W|?*) is maximized on training data. The argu-
ments in favor of MLE are based on the assumption 
that the form of the underlying distributions is 
known, and that only the values of the parameters 
characterizing those distributions are unknown. In 
using MLE for LM, one always assumes a multi-
nomial distribution of language. For example, a 
trigram model makes the assumption that the next 
word is predicted depending only on two preced-
ing words. However, there are many cases in 
natural language where words over an arbitrary 
distance can be related. MLE is therefore not opti-
mal because the assumed model form is incorrect. 
What are the best estimators when the model is 
known to be false then? In IME, we can tackle this 
question empirically. Best IME systems achieve the 
least CER. Therefore, the best estimators are those 
which minimize the expected error rate on unseen 
test data. Since the distribution of test data is un-
known, we can approximately minimize the error 
rate on some given training data (Vapnik 1999). 
Toward this end, we have developed a very simple 
heuristic training procedure called minimum sample 
risk, as presented in the next section. 
3 Minimum Sample Risk 
3.1 Problem Definition 
We follow the general framework of linear dis-
criminant models described in (Duda et al 2001). In 
the rest of the paper we use the following notation, 
adapted from Collins (2002). 
? Training data is a set of example input/output 
pairs. In LM for IME, training samples are repre-
sented as {Ai, WiR}, for i = 1?M, where each Ai is an 
input phonetic string and WiR is the reference tran-
script of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top N word strings converted from A using a base-
line IME system that uses only a word trigram 
model. 
? We assume a set of D+1 features fd(W), for d = 
0?D. The features could be arbitrary functions that 
map W to real values. Using vector notation, we 
have f(W)??D+1, where f(W) = [f0(W), f1(W), ?, 
fD(W)]T. Without loss of generality, f0(W) is called 
the base feature, and is defined in our case as the 
log probability that the word trigram model as-
signs to W. Other features (fd(W), for d = 1?D) are 
defined as the counts of word n-grams (n = 1 and 2 
in our experiments) in W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as  
210
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the number 
of conversion errors in W by comparing it with a 
reference transcript WR using an error function 
Er(WR,W) (i.e.  the string edit distance function in 
our case). We call the sum of error counts over the 
training samples sample risk. Our goal is to mini-
mize the sample risk while searching for the pa-
rameters as defined in Equation (4), hence the name 
minimum sample risk (MSR). Wi* in Equation (4) is 
determined by Equation (3), 
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
We first present the basic MSR training algorithm, 
and then the two improvements we made. 
3.2 Training Algorithm 
The MSR training algorithm is cast as a multidi-
mensional function optimization approach (Press 
et al 1992): taking the feature vector as a set of 
directions; the first direction (i.e. feature) is selected 
and the objective function (i.e. sample risk) is 
minimized along that direction using a line search; 
then from there along the second direction to its 
minimum, and so on, cycling through the whole set 
of directions as many times as necessary, until the 
objective function stops decreasing.  
This simple method can work properly under 
two assumptions. First, there exists an implemen-
tation of line search that optimizes the function 
along one direction efficiently. Second, the number 
of candidate features is not too large, and these 
features are not highly correlated. However, nei-
ther of the assumptions holds in our case. First of 
all, Er(.) in Equation (4) is a step function of ?, thus 
cannot be optimized directly by regular gradient- 
based procedures ? a grid search has to be used 
instead. However, there are problems with simple 
grid search: using a large grid could miss the op-
timal solution whereas using a fine-grained grid 
would lead to a very slow algorithm. Secondly, in 
the case of LM, there are millions of candidate 
features, some of which are highly correlated. We 
address these issues respectively in the next two 
subsections. 
3.3 Grid Line Search 
Our implementation of a grid search is a modified 
version of that proposed in (Och 2003). The modi-
fications are made to deal with the efficiency issue 
due to the fact that there is a very large number of 
features and training samples in our task, compared 
to only 8 features used in (Och 2003). Unlike a 
simple grid search where the intervals between any 
two adjacent grids are equal and fixed, we deter-
mine for each feature a sequence of grids with 
differently sized intervals, each corresponding to a 
different value of sample risk. 
As shown in Equation (4), the loss function (i.e. 
sample risk) over all training samples is the sum of 
the loss function (i.e. Er(.)) of each training sample. 
Therefore, in what follows, we begin with a discus-
sion on minimizing Er(.) of a training sample using 
the line search.  
Let ? be the current model parameter vector, 
and fd be the selected feature. The line search aims to 
find the optimal parameter ?d* so as to minimize 
Er(.). For a training sample (A, WR), the score of each 
candidate word string W?GEN(A), as in Equation 
(2), can be decomposed into two terms: 
)()()(),(
'0'
'' WfWfWWScore dd
D
ddd
dd ?? +== ?
??=
?f? , 
where the first term on the right hand side does not 
change with ?d. Note that if several candidate word 
strings have the same feature value fd(W), their 
relative rank will remain the same for any ?d. Since 
fd(W) takes integer values in our case (fd(W) is the 
count of a particular n-gram in W), we can group the 
candidates using fd(W) so that candidates in each 
group have the same value of fd(W). In each group, 
we define the candidate with the highest value of  
? ??=D ddd dd Wf'0' '' )(?  
as the active candidate of the group because no 
matter what value ?d takes, only this candidate 
could be selected according to Equation (3). 
Now, we reduce GEN(A) to a much smaller list 
of active candidates. We can find a set of intervals 
for ?d, within each of which a particular active 
candidate will be selected as W*. We can compute 
the Er(.) value of that candidate as the Er(.) value for 
the corresponding interval. As a result, for each 
211
training sample, we obtain a sequence of intervals 
and their corresponding Er(.) values. The optimal 
value ?d* can then be found by traversing the se-
quence and taking the midpoint of the interval with 
the lowest Er(.) value.  
305
306
307
308
309
310
311
312
0. 85 0. 9 0. 95 1 1.05 1.1 1. 15 1. 2lambda
SR
(.)
sample risk
smoothed sample risk
 
Figure 1. Examples of line search.  
This process can be extended to the whole 
training set as follows. By merging the sequence of 
intervals of each training sample in the training set, 
we obtain a global sequence of intervals as well as 
their corresponding sample risk. We can then find 
the optimal value ?d* as well as the minimal sample 
risk by traversing the global interval sequence. An 
example is shown in Figure 1. 
The line search can be unstable, however. In 
some cases when some of the intervals are very 
narrow (e.g. the interval A in Figure 1), moving the 
optimal value ?d* slightly can lead to much larger 
sample risk. Intuitively, we prefer a stable solution 
which is also known as a robust solution (with even 
slightly higher sample risk, e.g. the interval B in 
Figure 1). Following Quirk et al (2004), we evaluate 
each interval in the sequence by its corresponding 
smoothed sample risk. Let ? be the midpoint of an 
interval and SR(?) be the corresponding sample risk 
of the interval. The smoothed sample risk of the 
interval is defined as 
???? d
b
b
 )SR(? +?   
where b is a smoothing factor whose value is de-
termined empirically  (0.06 in our experiments). As 
shown in Figure 1, a more stable interval B is se-
lected according to the smoothed sample risk. 
In addition to reducing GEN(A) to an active 
candidate list described above, the efficiency of the 
line search can be further improved. We find that 
the line search only needs to traverse a small subset 
of training samples because the distribution of 
features among training samples are very sparse. 
Therefore, we built an inverted index that lists for 
each feature all training samples that contain it. As 
will be shown in Section 4.2, the line search is very 
efficient even for a large training set with millions of 
candidate features. 
3.4 Feature Subset Selection 
This section describes our method of selecting 
among millions of features a small subset of highly 
effective features for MSR learning. Reducing the 
number of features is essential for two reasons: to 
reduce computational complexity and to ensure the 
generalization property of the linear model. A large 
number of features lead to a large number of pa-
rameters of the resulting linear model, as described 
in Section 3.1. For a limited number of training 
samples, keeping the number of features suffi-
ciently small should lead to a simpler model that is 
less likely to overfit to the training data. 
The first step of our feature selection algorithm 
treats the features independently. The effectiveness 
of a feature is measured in terms of the reduction of 
the sample risk on top of the base feature f0. For-
mally, let SR(f0) be the sample risk of using the base 
feature only, and SR(f0 + ?dfd) be the sample risk of 
using both f0 and fd and the parameter ?d that has 
been optimized using the line search. Then the 
effectiveness of fd, denoted by E(fd), is given by 
))SR()(SR(max
)SR()SR(
)(
00
...1,
00
ii
Dif
dd
d fff
fff
fE
i
?
?
+?
+?=
=
, (5)
where the denominator is a normalization term to 
ensure that E(f) ? [0, 1]. 
The feature selection procedure can be stated as 
follows: The value of E(.) is computed according to 
Equation (5) for each of the candidate features. 
Features are then ranked in the order of descending 
values of E(.). The top l features are selected to form 
the feature vector in the linear model. 
Treating features independently has the ad-
vantage of computational simplicity, but may not 
be effective for features with high correlation. For 
instance, although two features may carry rich 
discriminative information when treated sepa-
rately, there may be very little gain if they are com-
bined in a feature vector, because of the high cor-
relation between them. Therefore, in what follows, 
we describe a technique of incorporating correla-
tion information in the feature selection criterion.  
Let xmd, m = 1?M and d = 1?D, be a Boolean 
value: xmd = 1 if the sample risk reduction of using 
the d-th feature on the m-th training sample, com-
B 
A
212
puted by Equation (5), is larger than zero, and 0 
otherwise. The cross correlation coefficient be-
tween two features fi and fj is estimated as 
??
?
==
==
M
m mj
M
m mi
M
m mjmi
xx
xx
jiC
1
2
1
2
1),( . (6)
It can be shown that C(i, j) ? [0, 1]. Now, similar to  
(Theodoridis and Koutroumbas 2003), the feature 
selection procedure consists of the following steps, 
where fi denotes any selected feature and fj denotes 
any candidate feature to be selected. 
Step 1. For each of the candidate features (fd, for d = 
1?D), compute the value of E(f) according to 
Equation (5). Rank them in a descending order and 
choose the one with the highest E(.) value. Let us 
denote this feature as f1. 
Step 2. To select the second feature, compute the 
cross correlation coefficient between the selected 
feature f1 and each of the remaining M-1 features, 
according to Equation (6). 
Step 3. Select the second feature f according to { } ),1()1()(maxarg*
...2
jCfEj j
Dj
?? ??=
=
 
where ? is the weight that determines the relative 
importance we give to the two terms. The value of 
? is optimized on held-out data (0.8 in our experi-
ments). This means that for the selection of the 
second feature, we take into account not only its 
impact of reducing the sample risk but also the 
correlation with the previously selected feature. It 
is expected that choosing features with less corre-
lation gives better sample risk minimization. 
Step 4. Select k-th features, k = 3?K, according to 
??
?
??
?
?
??= ??
=
1
1
),(
1
1
)(maxarg*
k
i
j
j
jiC
k
fEj
??  (7)
That is, we select the next feature by taking into 
account its average correlation with all previously 
selected features. The optimal number of features, l, 
is determined on held-out data. 
Similarly to the case of line search, we need to 
deal with the efficiency issue in the feature selec-
tion method. As shown in Equation (7), the esti-
mates of E(.) and C(.) need to be computed. Let D 
and K (K << D) be the number of all candidate 
features and the number of features in the resulting 
model, respectively. According to the feature se-
lection method described above, we need to esti-
mate E(.) for each of the D candidate features only 
once in Step 1. This is not very costly due to the 
efficiency of our line search algorithm. Unlike the 
case of E(.), O(K?D) estimates of C(.) are required in 
Step 4. This is computationally expensive even for a 
medium-sized K. Therefore, every time a new fea-
ture is selected (in Step 4), we only estimate the 
value of C(.) between each of the selected features 
and each of the top N remaining features with the 
highest value of E(.). This reduces the number of 
estimates of C(.) to O(K?N). In our experiments we 
set N = 1000, much smaller than D. This reduces the 
computational cost significantly without producing 
any noticeable quality loss in the resulting model. 
The MSR algorithm used in our experiments is 
summarized in Figure 2. It consists of feature se-
lection (line 2) and optimization (lines 3 - 5) steps. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features and select the top K features, using 
the feature selection method described in Section 3.4
3 For t = 1?T (T= total number of iterations) 
4 For each k = 1?K  
5    Update the parameter of fk using line search.  
Figure 2: The MSR algorithm 
4 Evaluation 
4.1 Settings 
We evaluated MSR on the task of Japanese IME. 
Two newspaper corpora are used as training and 
test data: Nikkei and Yomiuri Newspapers. Both 
corpora have been pre-word-segmented using a 
lexicon containing 167,107 entries. A 5,000-sentence 
subset of the Yomiuri Newspaper corpus  was used 
as held-out data (e.g. to determine learning rate, 
number of iterations and features etc.). We tested 
our models on another  5,000-sentence subset of the 
Yomiuri Newspaper corpus.  
We used an 80,000-sentence subset of the Nikkei 
Newspaper corpus as the training set. For each A, 
we produced a word lattice using the baseline 
system described in (Gao et al 2002), which uses a 
word trigram model trained via MLE on anther 
400,000-sentence subset of the Nikkei Newspaper 
corpus. The two subsets do not overlap so as to 
simulate the case where unseen phonetic symbol 
strings are converted by the baseline system. For 
efficiency, we kept for each training sample the 
best 20 hypotheses in its candidate conversion set 
GEN(A) for discriminative training. The oracle best 
hypothesis, which gives the minimum number of 
errors, was used as the reference transcript of A. 
213
4.2 Results 
We used unigrams and bigrams that occurred more 
than once in the training set as features. We did not 
use trigram features because they did not result in a 
significant improvement in our pilot study. The 
total number of candidate features we used was 
around 860,000.  
Our main experimental results are shown in 
Table 1. Row 1 is our baseline result using the word 
trigram model. Notice that the result is much better 
than the state-of-the-art performance currently 
available in the marketplace (e.g. Gao et al 2002), 
presumably due to the large amount of training 
data we used, and to the similarity between the 
training and the test data. Row 2 is the result of the 
model trained using the MSR algorithm described 
in Section 3. We also compared the MSR algorithm 
to two of the state-of-the-art discriminative training 
methods: Boosting in Row 3 is an implementation 
of the improved algorithm for the boosting loss 
function proposed in (Collins 2000), and Percep-
tron in Row 4 is an implementation of the averaged 
perceptron algorithm described in (Collins 2002).  
We see that all discriminative training methods 
outperform MLE significantly (p-value < 0.01). In 
particular, MSR outperforms MLE by more than 
20% CER reduction. Notice that we used only uni-
gram and bigram features that have been included 
in the baseline trigram model, so the improvement 
is solely attributed to the high performance of MSR. 
We also find that MSR outperforms the perceptron 
and boosting methods by a small but statistically 
significant margin. 
The MSR algorithm is also very efficient: using a 
subset of 20,000 features, it takes less than 20 min-
utes to converge on an XEON(TM) MP 1.90GHz 
machine. It is as efficient as the perceptron algo-
rithm and slightly faster than the boosting method. 
4.3 Robustness Issues 
Most theorems that justify the robustness of dis-
criminative training algorithms concern two ques-
tions. First, is there a guarantee that a given algo-
rithm converges even if the training samples are 
not linearly separable? This is called the convergence 
problem. Second, how well is the training error 
reduction preserved when the algorithm is applied 
to unseen test samples? This is called the generali-
zation problem. Though we currently cannot give a 
theoretical justification, we present empirical evi-
dence here for the robustness of the MSR approach. 
As Vapnik (1999) pointed out, the most robust 
linear models are the ones that achieve the least 
training errors with the least number of features. 
Therefore, the robustness of the MSR algorithm are 
mainly affected by the feature selection method. To 
verify this, we created four different subsets of 
features using different settings of the feature se-
lection method described in Section 3.4. We se-
lected different numbers of features (i.e. 500 and 
2000) with and without taking into account the 
correlation between features (i.e. ? in Equation (7) 
is set to 0.8 and 1, respectively). For each of the four 
feature subsets, we used the MSR algorithm to 
generate a set of models. The CER curves of these 
models on training and test data sets are shown in 
Figures 3 and 4, respectively.  
2.08
2.10
2.12
2.14
2.16
2.18
2.20
2.22
2.24
2.26
2.28
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 3. Training error curves of the MSR algorithm 
2.94
2.99
3.04
3.09
3.14
3.19
3.24
1 250 500 750 1000 1250 1500 1750 2000
# of rounds
C
E
R
(%
)
MSR(?=1)-2000
MSR(?=1)-500
MSR(?=0.8)-2000
MSR(?=0.8)-500
 
Figure 4. Test error curves of the MSR algorithm 
The results reveal several facts. First, the con-
vergence properties of MSR are shown in Figure 3 
where in all cases, training errors drop consistently 
with more iterations. Secondly, as expected, using 
more features leads to overfitting, For example, 
MSR(? =1)-2000 makes fewer errors than MSR(? 
=1)-500 on training data but more errors on test 
data. Finally, taking into account the correlation 
between features (e.g. ? = 0.8 in Equation (7)) re-
 Model CER (%) % over MLE 
1. MLE  3.70 -- 
2. MSR (K=2000) 2.95 20.9 
3. Boosting  3.06 18.0 
4. Perceptron 3.07 17.8 
Table 1. Comparison of CER results. 
214
sults in a better subset of features that lead to not 
only fewer training errors, as shown in Figure 3, 
but also better generalization properties (fewer test 
errors), as shown in Figure 4. 
4.4 Domain Adaptation Results  
Though MSR achieves impressive performance in 
CER reduction over the comparison methods, as 
described in Section 4.2, the experiments are all 
performed using newspaper text for both training 
and testing, which is not a realistic scenario if we 
are to deploy the model in an application. This 
section reports the results of additional experi-
ments in which we adapt a model trained on one 
domain to a different domain, i.e., in a so-called 
cross-domain LM adaptation paradigm. See (Su-
zuki and Gao 2005) for a detailed report. 
The data sets we used stem from five distinct 
sources of text. The Nikkei newspaper corpus de-
scribed in Section 4.1 was used as the background 
domain, on which the word trigram model was 
trained. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspapers and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). For each of the four domains, we used an 
72,000-sentence subset as adaptation training data, 
a 5,000-sentence subset as held-out data and an-
other 5,000-sentence subset as test data. Similarly, 
all corpora have been word-segmented, and we 
kept for each training sample, in the four adapta-
tion domains, the best 20 hypotheses in its candi-
date conversion set for discriminative training.  
We compared MSR with three other LM adap-
tation methods:  
Baseline is the background word trigram model, 
as described in Section 4.1. 
MAP (maximum a posteriori) is a traditional LM 
adaptation method where the parameters of the 
background model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear in-
terpolation as P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), 
where Pb is the probability of the background 
model, Pa is the probability trained on adaptation 
data using MLE and the history h corresponds to 
two preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Perceptron, Boosting and MSR are the three 
discriminative methods described in the previous 
sections.  For each of them, the base feature was 
Model Yomiuri TuneUp Encarta Shincho 
Baseline 3.70 5.81 10.24 12.18 
MAP  3.69 5.47 7.98 10.76 
MSR  2.73 5.15 7.40 10.16 
Boosting  2.78 5.33 7.53 10.25 
Perceptron 2.78 5.20 7.44 10.18 
Table 2. CER(%) results on four adaptation test sets . 
derived from the word trigram model trained on 
the background data, and other n-gram features (i.e. 
fd, d = 1?D in Equation (2)) were trained on adap-
tation data. That is, the parameters of the back-
ground model are adjusted in such a way that 
minimizes the errors on adaptation data made by 
background model. 
Results are summarized in Table 2. First of all, 
in all four adaptation domains, discriminative 
methods outperform MAP significantly. Secondly, 
the improvement margins of discriminative 
methods over MAP correspond to the similarities 
between background domain and adaptation do-
mains. When the two domains are very similar to 
the background domain (such as Yomiuri), dis-
criminative methods outperform MAP by a large 
margin. However, the margin is smaller when the 
two domains are substantially different (such as 
Encarta and Shincho). The phenomenon is attrib-
uted to the underlying difference between the two 
adaptation methods: MAP aims to improve the 
likelihood of a distribution, so if the adaptation 
domain is very similar to the background domain, 
the difference between the two underlying distri-
butions is so small that MAP cannot adjust the 
model effectively. However, discriminative meth-
ods do not have this limitation for they aim to 
reduce errors directly. Finally, we find that in most 
adaptation test sets, MSR achieves slightly better 
CER results than the two competing discriminative 
methods. Specifically, the improvements of MSR 
are statistically significant over the boosting 
method in three out of four domains, and over the 
perceptron algorithm in the Yomiuri domain. The 
results demonstrate again that MSR is robust. 
5 Related Work 
Discriminative models have recently been proved 
to be more effective than generative models in 
some NLP tasks, e.g., parsing (Collins 2000), POS 
tagging (Collins 2002) and LM for speech recogni-
tion (Roark et al 2004). In particular, the linear 
models, though simple and non-probabilistic in 
nature, are preferred to their probabilistic coun-
215
terpart such as logistic regression. One of the rea-
sons, as pointed out by Ng and Jordan (2002), is 
that the parameters of a discriminative model can 
be fit either to maximize the conditional likelihood 
on training data, or to minimize the training errors. 
Since the latter optimizes the objective function that 
the system is graded on, it is viewed as being more 
truly in the spirit of discriminative learning. 
The MSR method shares the same motivation: to 
minimize the errors directly as much as possible. 
Because the error function on a finite data set is a 
step function, and cannot be optimized easily, 
previous research approximates the error function 
by loss functions that are suitable for optimization 
(e.g. Collins 2000; Freund et al 1998; Juang et al 
1997; Duda et al 2001). MSR uses an alternative 
approach. It is a simple heuristic training proce-
dure to minimize training errors directly without 
applying any approximated loss function. 
MSR shares many similarities with previous 
methods. The basic training algorithm described in 
Section 3.2 follows the general framework of multi- 
dimensional optimization (e.g., Press et al 1992). 
The line search is an extension of that described in 
(Och 2003; Quirk et al 2005. The extension lies in 
the way of handling large number of features and 
training samples. Previous algorithms were used to 
optimize linear models with less than 10 features. 
The feature selection method described in Section 
3.4 is a particular implementation of the feature 
selection methods described in (e.g., Theodoridis 
and Koutroumbas 2003). The major difference 
between the MSR and other methods is that it es-
timates the effectiveness of each feature in terms of 
its expected training error reduction while previ-
ous methods used metrics that are loosely coupled 
with reducing training errors. The way of dealing 
with feature correlations in feature selection in 
Equation (7), was suggested by Finette et al (1983). 
6 Conclusion and Future Work 
We show that MSR is a very successful discrimina-
tive training algorithm for LM. Our experiments 
suggest that it leads to significantly better conver-
sion performance on the IME task than either the 
MLE method or the two widely applied discrimi-
native methods, the boosting and perceptron 
methods. However, due to the lack of theoretical 
underpinnings, we are unable to prove that MSR 
will always succeed. This forms one area of our 
future work. 
One of the most interesting properties of MSR is 
that it can optimize any objective function (whether 
its gradient is computable or not), such as error rate 
in IME or speech, BLEU score in MT, precision and 
recall in IR (Gao et al 2005). In particular, MSR can 
be performed on large-scale training set with mil-
lions of candidate features. Thus, another area of 
our future work is to test MSR on wider varieties of 
NLP tasks such as parsing and tagging. 
References 
Collins, Michael. 2002. Discriminative training methods 
for Hidden Markov Models: theory and experiments 
with the perceptron algorithm. In EMNLP 2002. 
Collins, Michael. 2000. Discriminative reranking for 
natural language parsing. In ICML 2000. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 2001. 
Pattern classification. John Wiley & Sons, Inc. 
Finette S., Blerer A., Swindel W. 1983. Breast tissue clas-
sification using diagnostic ultrasound and pattern rec-
ognition techniques: I. Methods of pattern recognition. 
Ultrasonic Imaging, Vol. 5, pp. 55-70. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An 
efficient boosting algorithm for combining preferences. 
In ICML?98.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive clus-
tering for language modeling. In EMNLP 2002. 
Gao, J, H. Qin, X. Xiao and J.-Y. Nie. 2005. Linear dis-
criminative model for information retrieval. In SIGIR.  
Jelinek, Fred. 1997. Statistical methods for speech recognition. 
MIT Press, Cambridge, Mass. 
Juang, B.-H., W.Chou and C.-H. Lee. 1997. Minimum 
classification error rate methods for speech recognition. 
IEEE Tran. Speech and Audio Processing 5-3: 257-265.  
Ng, A. N. and M. I. Jordan. 2002. On discriminative vs. 
generative classifiers: a comparison of logistic regres-
sion and na?ve Bayes. In NIPS 2002: 841-848. 
Och, Franz Josef. 2003. Minimum error rate training in 
statistical machine translation. In ACL 2003 
Press, W. H., S. A. Teukolsky, W. T. Vetterling and B. P. 
Flannery. 1992. Numerical Recipes In C: The Art of Scien-
tific Computing. New York: Cambridge Univ. Press. 
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: syntactically informed 
phrasal SMT. In ACL 2005: 271-279.  
Roark, Brian, Murat Saraclar and Michael Collins. 2004. 
Corrective language modeling for large vocabulary ASR 
with the perceptron algorithm. In ICASSP 2004. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Theodoridis, Sergios and Konstantinos Koutroumbas. 
2003. Pattern Recognition. Elsevier. 
Vapnik, V. N. 1999. The nature of statistical learning theory. 
Springer-Verlag, New York. 
216
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 265?272, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Comparative Study on Language Model Adaptation Techniques      
Using New Evaluation Metrics 
 
Hisami Suzuki Jianfeng Gao 
Microsoft Research Microsoft Research Asia 
One Microsoft Way  49 Zhichun Road, Haidian District 
Redmond WA 98052 USA Beijing 100080 China 
hisamis@microsoft.com jfgao@microsoft.com 
  
Abstract 
This paper presents comparative experimen-
tal results on four techniques of language 
model adaptation, including a maximum a 
posteriori (MAP) method and three dis-
criminative training methods, the boosting 
algorithm, the average perceptron and the 
minimum sample risk method, on the task of 
Japanese Kana-Kanji conversion. We evalu-
ate these techniques beyond simply using 
the character error rate (CER): the CER re-
sults are interpreted using a metric of do-
main similarity between background and 
adaptation domains, and are further evalu-
ated by correlating them with a novel metric 
for measuring the side effects of adapted 
models. Using these metrics, we show that 
the discriminative methods are superior to a 
MAP-based method not only in terms of 
achieving larger CER reduction, but also of 
being more robust against the similarity of 
background and adaptation domains, and 
achieve larger CER reduction with fewer 
side effects.  
1 Introduction 
Language model (LM) adaptation attempts to ad-
just the parameters of a LM so that it performs well 
on a particular (sub-)domain of data. Currently, 
most LMs are based on the Markov assumption 
that the prediction of a word depends only on the 
preceding n?1 words, but such n-gram statistics are 
known to be extremely susceptible to the charac-
teristics of training samples. This is true even when 
the data sources are supposedly similar: for exam-
ple, Rosenfeld (1996) showed that perplexity dou-
bled when a LM trained on the Wall Street Journal 
(1987-1989) was tested on the AP newswire stories 
of the same period. This observation, coupled with 
the fact that training data is available in large quan-
tities only in selected domains, facilitates the need 
for LM adaptation.  
There have been two formulations of the LM 
adaptation problem. One is the within-domain ad-
aptation, in which adapted LMs are created for 
different topics in a single domain (e.g., Seymore 
and Rosenfeld, 1997; Clarkson and Robinson, 
1997; Chen et al, 1998). In these studies, a domain 
is defined as a body of text originating from a sin-
gle source, and the main goal of LM adaptation is 
to fine-tune the model parameters so as to improve 
the LM performance on a specific sub-domain (or 
topic) using the training data at hand.  
The other formulation, which is the focus of the 
current study, is to adapt a LM to a novel domain, 
for which only a very small amount of training 
data is available. This is referred to as cross-
domain adaptation. Following Bellegarda (2001), 
we call the domain used to train the original model 
the background domain, and the novel domain 
with a small amount of training data as the adapta-
tion domain. Two major approaches to cross-
domain adaptation have been investigated: maxi-
mum a posteriori (MAP) estimation and discrimi-
native training methods. In MAP estimation 
methods, adaptation data is used to adjust the pa-
rameters of the background model so as to maxi-
mize the likelihood of the adaptation data. Count 
merging and linear interpolation of models are the 
two MAP estimation methods investigated in 
speech recognition experiments (Iyer et al, 1997; 
Bacchiani and Roark, 2003), with count merging 
reported to slightly outperform linear interpolation. 
Discriminative approaches to LM adaptation, on 
the other hand, aim at using the adaptation data to 
directly minimize the errors on the adaptation data 
made by the background model. These techniques 
have been applied successfully to the task of lan-
guage modeling in non-adaptation (Roark et al, 
265
2004) as well as adaptation (Bacchiani et al, 2004) 
scenarios.  
In this paper, we present comparative experi-
mental results on four language model adaptation 
techniques and evaluate them from various angles, 
attempting to elucidate the characteristics of these 
models. The four models we compare are a maxi-
mum a posteriori (MAP) method and three dis-
criminative training methods, namely the boosting 
algorithm (Collins, 2000), the average perceptron 
(Collins, 2002) and the minimum sample risk 
method (Gao et al, 2005). Our evaluation of these 
techniques is unique in that we go beyond simply 
comparing them in terms of character error rate 
(CER): we use a metric of distributional similarity 
to measure the distance between background and 
adaptation domains, and attempt to correlate it with 
the CER of each adaptation method. We also pro-
pose a novel metric for measuring the side effects 
of adapted models using the notion of backward 
compatibility, which is very important from a soft-
ware deployment perspective.  
Our experiments are conducted in the setting of 
Japanese Kana-Kanji conversion, as we believe 
this task is excellently suited for evaluating LMs. 
We begin with the description of this task in the 
following section.  
2 Language Modeling in the Task of IME 
This paper studies language modeling in the con-
text of Asian language (e.g., Chinese or Japanese) 
text input. The standard method for doing this is 
that the users first input the phonetic strings, which 
are then converted into the appropriate word string 
by software. The task of automatic conversion has 
been the subject of language modeling research in 
the context of Pinyin-to-Character conversion in 
Chinese (Gao et al, 2002a) and Kana-Kanji con-
version in Japanese (Gao et al, 2002b). In this pa-
per, we call the task IME (Input Method Editor), 
based on the name of the commonly used Win-
dows-based application.  
The performance of IME is typically measured 
by the character error rate (CER), which is the 
number of characters wrongly converted from the 
phonetic string divided by the number of charac-
ters in the correct transcript. Current IME systems 
exhibit about 5-15% CER on real-world data in a 
wide variety of domains.  
In many ways, IME is a similar task to speech 
recognition. The most obvious similarity is that 
IME can also be viewed as a Bayesian decision 
problem: let A be the input phonetic string (which 
corresponds to the acoustic signal in speech); the 
task of IME is to choose the most likely word 
string W* among those candidates that could have 
been converted from A: 
)|()(maxarg)|(maxarg*
)()(
WAPWPAWPW
AWAW GENGEN ??
==
 
(1) 
where GEN(A) denotes the candidate set given A.  
Unlike speech recognition, however, there is no 
acoustic ambiguity in IME, because the phonetic 
string is provided directly by users. Moreover, we 
can assume a unique mapping from W to A in IME, 
i.e., P(A|W) = 1. So the decision of Equation (1) 
depends solely on P(W), which makes IME ideal 
for testing language modeling techniques. Another 
advantage of using IME for language modeling 
research is that it is relatively easy to convert W to 
A, which facilitates the creation of training data for 
discriminative learning, as described later.  
From the perspective of LM adaptation, IME 
faces the same problem speech recognition faces: 
the quality of the model depends heavily on the 
similarity of the training and test data. This poses a 
serious challenge to IME, as it is currently the most 
widely used method of inputting Chinese or Japa-
nese characters, used by millions of users for in-
putting text of any domain. LM adaptation in IME 
is therefore an imminent requirement for improv-
ing user experience, not only as we build static 
domain-specific LMs, but also in making online 
user adaptation possible in the future.  
3 Discriminative Algorithms for LM Ad-
aptation 
This section describes three discriminative training 
methods we used in this study. For a detailed de-
scription of each algorithm, readers are referred to 
Collins (2000) for the boosting algorithm, Collins 
(2002) for perceptron learning, and Gao et al 
(2005) for the minimum sample risk method. 
3.1 Definition 
The following set-up, adapted from Collins (2002), 
was used for all three discriminative training meth-
ods:  
266
?  Training data is a set of input-output pairs. In the 
task of IME, we have training samples {Ai, WiR}, 
for i = 1?M, where each Ai is an input phonetic 
string and each WiR is the reference transcript of Ai. 
?  We assume a set of D + 1 features fd(W), for d = 
0?D. The features could be arbitrary functions 
that map W to real values. Using vector notation, 
we have f(W)??D+1, where f(W) = {f0(W), f1(W), 
?, fD(W)}. The feature f0(W) is called the base 
model feature, and is defined as the log probability 
that the word trigram model assigns to W. The fea-
tures fd(W) for d = 1?D are defined as the word n-
gram counts (n = 1 and 2 in our experiments) in W. 
?  The parameters of the model form a vector of D 
+ 1 dimensions, one for each feature function, ?= 
{?0, ?1, ?, ?D}. The likelihood score of a word 
string W can then be written as 
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2) 
Given a model ? and an input A, the decision rule 
of Equation (1) can then be rewritten as 
).,(maxarg),(* ??
GEN
WScoreAW
(A)W ?
=
 (3) 
We can obtain the number of conversion errors in 
W by comparing it with the reference transcript WR 
using an error function Er(WR,W), which is an edit 
distance in our case. We call the sum of error 
counts over the training set the sample risk (SR). 
Discriminative training methods strive to optimize 
the parameters of a model by minimizing SR, as in 
Equation (4). 
?
=
==
Mi
ii
R
i AWWSR
...1
* )),(,Er(minarg)(minarg ???
??
 
(4) 
However, (4) cannot be optimized directly by regu-
lar gradient-based procedures as it is a piecewise 
constant function of ? and its gradient is undefined. 
The discriminative training methods described be-
low differ in how they achieve the optimization: 
the boosting and perceptron algorithms approxi-
mate SR by loss functions that are suitable for op-
timization; the minimum sample risk method, on 
the other hand, uses a simple heuristic training pro-
cedure to minimize SR directly without resorting 
to an approximated loss function. 
3.2 The boosting algorithm  
The boosting algorithm we used is based on 
Collins (2000). Instead of measuring the number of 
conversion errors directly, it uses a loss function 
that measures the number of ranking errors, i.e., 
cases where an incorrect candidate W receives a 
higher score than the correct conversion WR. The 
margin of the pair (WR, W) with respect to the 
model ? is given by 
),(),(),( ?? WScoreWScoreWWM RR ?=
 
(5) 
The loss function is then defined as 
 ? ?
= ?
=
Mi iAiW
i
R
i WWMI
...1 )(
)],([)RLoss(
GEN
?
 (6) 
where I[?] = 1 if ? ? 0, and 0 otherwise. Note that 
RLoss takes into account all candidates in GEN(A).  
Since optimizing (6) is NP-complete, the boost-
ing algorithm optimizes its upper bound:  
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?
 
(7) 
Figure 1 summarizes the boosting algorithm we 
used. After initialization, Step 2 and 3 are repeated 
N times; at each iteration, a feature is chosen and 
its weight is updated. We used the following up-
date for the dth feature fd:  
ZC
ZC
d
d
d ?
??
+
+
=
+
_log2
1
 (8) 
where Cd+ is a value increasing exponentially with 
the sum of margins of (WR, W) pairs over the set 
where fd is seen in WR but not in W; Cd-  is the value 
related to the sum of margins over the set where fd 
is seen in W but not in WR. ? is a smoothing factor 
(whose value is optimized on held-out data) and Z 
is a normalization constant. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Select a feature fd which has largest estimated im-
pact on reducing ExpLoss of Equation (7) 
3 Update ?d by Equation (8), and return to Step 2 
Figure 1: The boosting algorithm 
3.3 The perceptron algorithm 
The perceptron algorithm can be viewed as a form 
of incremental training procedure that optimizes a 
minimum square error (MSE) loss function, which 
is an approximation of SR (Mitchell, 1997). As 
shown in Figure 2, it starts with an initial parame-
ter setting and updates it for each training sample. 
We used the average perceptron algorithm of 
Collins (2002) in our experiments, a variation that 
has been proven to be more effective than the stan-
dard algorithm shown in Figure 2. Let ?dt,i be the 
267
value for the dth parameter after the ith training 
sample has been processed in pass t over the train-
ing data. The average parameters are defined as  
)./()()(
1 1
, MT
T
t
M
i
it
davgd ?= ??
= =
??  (9) 
3.4 The minimum sample risk method 
The minimum sample risk (MSR, Gao et al, 2005) 
training algorithm is motivated by analogy with the 
feature selection procedure for the boosting algo-
rithm (Freund et al, 1998). It is a greedy procedure 
for selecting a small subset of the features that 
have the largest contribution in reducing SR in a 
sequential manner. Conceptually, MSR operates 
like any multidimensional function optimization 
approach: a direction (i.e., feature) is selected and 
SR is minimized along that direction using a line 
search, i.e., adjusting the parameter of the selected 
feature while keeping all other parameters fixed. 
This is repeated until SR stops decreasing.  
Regular numerical line search algorithms cannot 
be applied directly because, as described above, 
the value of a feature parameter versus SR is not 
smooth and there are many local minima. MSR 
thus adopts the method proposed by Och (2003). 
Let GEN(A) be the set of n-best candidate word 
strings that could be converted from A. By adjust-
ing ?d for a selected feature fd, we can find a set of 
intervals for ?d within which a particular candidate 
word string is selected. We can compute Er(.) for 
the candidate and use it as the Er(.) value for the 
corresponding interval. As a result, we obtain an 
ordered sequence of Er(.) values and a correspond-
ing sequence of ? intervals for each training sample. 
By summing Er(.) values over all training samples, 
we obtain a global sequence of SR and the corre-
sponding global sequence of ?d intervals. We can 
then find the optimal ?d as well as its correspond-
ing SR by traversing the sequence. 
Figure 3 summarizes the MSR algorithm. See 
Gao et al (2005) for a complete description of the 
MSR implementation and the empirical justifica-
tion for its performance.   
4 Experimental Results 
4.1 Data  
The data used in our experiments come from five 
distinct sources of text. A 36-million-word Nikkei 
newspaper corpus was used as the background 
domain. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspaper and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). The characteristics of these domains are 
measured using the information theoretic notion of 
cross entropy, which is described in the next sub-
section.  
 For the experiment of LM adaptation, we used 
the training data consisting of 8,000 sentences and 
test data of 5,000 sentences from each adaptation 
domain. Another 5,000-sentence subset was used 
as held-out data for each domain, which was used 
to determine the values of tunable parameters. All 
the corpora used in our experiments are pre-
segmented into words using a baseline lexicon 
consisting of 167,107 entries.  
4.2 Computation of domain characteristics 
Yuan et al (2005) introduces two notions of do-
main characteristics: a within-domain notion of 
diversity, and a cross-domain concept of similarity. 
Diversity is measured by the entropy of the corpus 
and indicates the inherent variability within the 
domain. Similarity, on the other hand, is intended 
to capture the difficulty of a given adaptation task, 
and is measured by the cross entropy.  
For the computation of these metrics, we ex-
tracted 1 million words from the training data of 
each domain respectively, and created a lexicon 
consisting of the words in our baseline lexicon plus 
all words in the corpora used for this experiment 
(resulting in 216,565 entries) to avoid the effect of 
out-of-vocabulary items. Given two domains A and 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3 
   For each training sample (Ai, WiR), i = 1?M 
4 
      Choose the best candidate Wi from GEN(Ai)   
      according to Equation (3) 
5 
      For each ?d (? = size of learning step) 
6 
          ?d = ?d + ?(fd(WiR) ? fd(Wi))          
Figure 2: The perceptron algorithm 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features by its expected impact on reduc-
ing SR and select the top N features 
3 For each n = 1?N  
4    Update the parameter of f using line search  
Figure 3: The MSR algorithm 
268
B, we then trained a word trigram model for each 
domain B, and used the resulting model in comput-
ing the cross entropy of domain A. For simplicity, 
we denote this as H(A,B).  
Table 1 summarizes our corpora along this di-
mension. Note that the cross entropy is not sym-
metric, i.e., H(A,B) is not necessarily the same as 
H(B,A), so we only present the average cross en-
tropy in Table 1. We can observe that Yomiuri and 
TuneUp are much more similar to the background 
Nikkei corpus than Encarta and Shincho.  
H(A,A) along the diagonal of Table 1 (in bold-
face) is the entropy of the corpus, indicating the 
corpus diversity. This quantity indeed reflects the 
in-domain variability of text: newspaper and ency-
clopedia articles are highly edited text, following 
style guidelines and often with repetitious content. 
In contrast, Shincho is a collection of novels, on 
which no style or content restriction is imposed. 
We use these metrics in the interpretation of CER 
results in Section 5. 
4.3 Results of LM adaptation 
The discriminative training procedure was carried 
out as follows: for each input phonetic string A in 
the adaptation training set, we produced a word 
lattice using the baseline trigram models described 
in Gao et al (2002b). We kept the top 20 hypothe-
ses from this lattice as the candidate conversion set 
GEN(A). The lowest CER hypothesis in the lattice 
rather than the reference transcript was used as WR. 
We used unigram and bigram features that oc-
curred more than once in the training set.  
We compared the performance of discriminative 
methods against a MAP estimation method as the 
baseline, in this case the linear interpolation 
method. Specifically, we created a word trigram 
model using the adaptation data for each domain, 
which was then linearly interpolated at the word 
level with the baseline model. The probability ac-
cording to the combined model is given by 
)|()1()|()|( hwPhwPhwp iAiBi ?? ?+= ,  
where PB is the probability of the background 
model, PA the probability of the adaptation model, 
and the history h corresponds to two preceding 
words. ? was tuned using the held-out data.  
In evaluating both MAP estimation and dis-
criminative models, we used an N-best rescoring 
approach. That is, we created N best hypotheses 
using the baseline trigram model (N=100 in our 
experiments) for each sentence in the test data, and 
used adapted models to rescore the N-best list. The 
oracle CERs (i.e., the minimal possible CER given 
the available hypotheses) ranged from 1.45% to 
5.09% depending on the adaptation domain.  
The results of the experiments are shown in Ta-
ble 2. We can make some observations from the 
table. First, all discriminative methods signifi-
cantly outperform the linear interpolation (statisti-
cally significant according to the t-test at p < 0.01). 
In contrast, the differences among three discrimi-
native methods are very subtle and most of them 
are not statistically significant. Secondly, the CER 
results correlate well with the metric of domain 
similarity in Table 1 (r=0.94 using the Pearson 
product moment correlation coefficient). This is 
consistent with our intuition that the closer the ad-
aptation domain is to the background domain, the 
easier the adaptation task.  
Regarding the similarity of the adaptation do-
main to the background, we also observe that the 
CER reduction of the linear interpolation model is 
particularly limited when the adaptation domain is 
similar to the background domain: the CER reduc-
tion of the linear interpolation model for Yomiuri 
and TuneUp over the baseline is 0% and 1.89% 
respectively, in contrast to ~22% and ~5.8% im-
provements achieved by the discriminative models. 
The discriminative methods are therefore more 
robust against the similarity of the adaptation and 
background data than the linear interpolation.  
Our results differ from Bacchiani et al (2004) in 
that in our system, the perceptron algorithm alone 
achieved better results than MAP estimation. 
However, the difference may only be apparent, 
given different experimental settings for the two 
 N Y T E S 
Nikkei 3.94 7.46 7.65 9.81 10.10 
Yomiuri  4.09 7.82 8.96 9.29 
TuneUp   4.41 8.82 8.56 
Encarta    4.40 9.20 
Shincho     4.61 
Table 1: Cross entropy 
Domain Base LI MSR Boost Percep 
Yomiuri 3.70 3.69 2.89 2.88 2.85 
TuneUp 5.81 5.70 5.48 5.47 5.47 
Encarta 10.24 8.64 8.39 8.54 8.34 
Shincho 12.18 11.47 11.05 11.09 11.20 
Table 2: CER results (%) (Base=baseline model; 
LI=linear interpolation) 
269
studies. We used the N-best reranking approach 
with the same N-best list for both MAP estimation 
and discriminative training, while in Bacchiani et 
al. (2004), two different lattices were used: the per-
ceptron model was applied to rerank the lattice 
created by the background model, while the MAP 
adaptation model was used to produce the lattice 
itself. The fact that the combination of these mod-
els (i.e., first use the MAP estimation to create hy-
potheses and then use the perceptron algorithm to 
rerank them) produced the best results indicates 
that given a candidate lattice, the perceptron algo-
rithm is effective in candidate reranking, thus mak-
ing our results compatible with theirs. 
5 Discussion 
The results in Section 4 demonstrate that discrimi-
native training methods for adaptation are overall 
superior to MAP adaptation methods. In this sec-
tion, we show additional advantages of discrimina-
tive methods beyond simple CER improvements.   
5.1 Using metrics for side effects  
In the actual deployment of LM adaptation, one 
issue that bears particular importance is the num-
ber of side effects that are introduced by an 
adapted model. For example, consider an adapted 
model which achieves 10% CER improvements 
over the baseline. Such a model can be obtained by 
improving 10%, or by improving 20% and by in-
troducing 10% of new errors. Clearly, the former 
model is preferred, particularly if the models be-
fore and after adaptation are both to be exposed to 
users. This concept is more widely acknowledged 
within the software industry as backward compati-
bility ? a requirement that an updated version of 
software supports all features of its earlier versions. 
In IME, it means that all phonetic strings that can 
be converted correctly by the earlier versions of the 
system should also be converted correctly by the 
new system as much as possible. Users are typi-
cally more intolerant to seeing errors on the strings 
that used to be converted correctly than seeing er-
rors that also existed in the previous version. 
Therefore, it is crucial that when we adapt to a new 
domain, we do so by introducing the smallest 
number of side effects, particularly in the case of 
an incremental adaptation to the domain of a par-
ticular user, i.e., to building a model with incre-
mental learning capabilities.   
5.2 Error ratio 
In order to measure side effects, we introduce the 
notion of error ratio (ER), which is defined as  
||
||
B
A
E
EER = , 
 
where |EA| is the number of errors found only in the 
new (adaptation) model, and |EB| the number of 
errors corrected by the new model. Intuitively, this 
quantity captures the cost of improvement in the 
adaptation model, corresponding to the number of 
newly introduced errors per each improvement. 
The smaller the ratio is, the better the model is at 
the same CER: ER=0 if the adapted model intro-
duces no new errors, ER<1 if the adapted model 
makes CER improvements, ER=1 if the CER im-
provement is zero (i.e., the adapted model makes 
as many new mistakes as it corrects old mistakes), 
and ER>1 when the adapted model has worse CER 
performance than the baseline model.  
Given the notion of CER and ER, a model can 
be plotted on a graph as in Figure 4: the relative 
error reduction (RER, i.e., the CER difference be-
tween the background and adapted models) is plot-
ted along the x-axis, and ER along the y-axis. 
Figure 4 plots the models obtained after various 
numbers of iterations for MSR training and at vari-
ous interpolation weights for linear interpolation 
for the TuneUp domain. The points in the upper-
left quadrant, ER>1 and RER<0, are the models 
that performed worse than the baseline model 
(some of the interpolated models fall into this cate-
gory); the shaded areas (upper-right and lower-left 
quadrants) are by definition empty. The lower-
right quadrant is the area of interest to us, as they 








     
	


 
Figure 4: RER/ER plot for MSR and LI models for 
TuneUp domain 
270
represent the models that led to CER improve-
ments; we will focus only on this area now in 
Figure 5. 
In this figure, a model is considered to have 
fewer side effects when the ER is smaller at the 
same RER (i.e., smaller value of y for a fixed value 
of x), or when the RER is larger at the same ER 
(i.e., larger value of x at the fixed y). That is, the 
closer a model is plotted to the corner B of the 
graph, the better the model is; the closer it is to the 
corner A, the worse the model is.  
5.3 Model comparison using RER/ER 
From Figure 5, we can clearly see that MSR mod-
els have better RER/ER-performance than linear 
interpolation models, as they are plotted closer to 
the corner B. Figure 6 displays the same plot for all 
four domains: the same trend is clear from all 
graphs. We can therefore conclude that a discrimi-
native method (in this case MSR) is superior to 
linear interpolation not only in terms of CER re-
duction, but also of having fewer side effects. This 
desirable result is attributed to the nature of dis-
criminative training, which works specifically to 
adjust feature weights so as to minimize error.  
 
Figure 7: RER/ER plot for MSR, boosting and percep-
tron models (X-axis is normalized to represent relative 
error rate reduction) 
Figure 7 compares the three discriminative 
models with respect to RER/ER by plotting the 
best models (i.e., models used to produce the re-
sults in Table 1) for each algorithm. We can see 
that even though the boosting and perceptron algo-
rithms have the same CER for Yomiuri and 
TuneUp from Table 2, the perceptron is better in 
terms of ER; this may be due to the use of expo-
nential loss function in the boosting algorithm 
which is less robust against noisy data (Hastie et al, 
2001). We also observe that Yomiuri and Encarta 
do better in terms of side effects than TuneUp and 
Shincho for all algorithms, which can be explained 
by corpus diversity, as the former set is less stylis-
tically diverse and thus more consistent within the 
domain.  
5.4 Overfitting and side effects 
The RER/ER graph also casts the problem of over-
fitting in an interesting perspective. Figure 8 is de-
rived from running MSR on the TuneUp test 
corpus, which depicts a typical case of overfitting: 
the CER drops in the beginning, but after a certain 
number of iterations, it goes up again. The models 
indicated by ? and ? in the graph are of the same 
CER, and as such, these models are equivalent. 
When plotted on the RER/ER graph in Figure 5, 











    
	




 
Figure 5: RER/ER plot for the models with ER<1 and 
RER>0 for TuneUp domain. See Figure 8 for the de-
scription of ? and ?  











          











       











      











         

R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 957 ? 968, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
An Empirical Study on Language Model Adaptation 
Using a Metric of Domain Similarity 
Wei Yuan1, Jianfeng Gao2, and Hisami Suzuki3 
1
 Shanghai Jiao Tong University, 1954 Huashan Road, Shanghai 200030 
sunnyuanovo@sjtu.edu.cn 
2
 Microsoft Research, Asia, 49 Zhichun Road, Haidian District, Beijing 100080 
jfgao@microsoft.com 
3
 Microsoft Research, One Microsoft Way, Redmond WA 98052 
hisamis@microsoft.com 
Abstract. This paper presents an empirical study on four techniques of lan-
guage model adaptation, including a maximum a posteriori (MAP) method and 
three discriminative training models, in the application of Japanese Kana-Kanji 
conversion. We compare the performance of these methods from various angles 
by adapting the baseline model to four adaptation domains. In particular, we at-
tempt to interpret the results given in terms of the character error rate (CER) by 
correlating them with the characteristics of the adaptation domain measured us-
ing the information-theoretic notion of cross entropy. We show that such a met-
ric correlates well with the CER performance of the adaptation methods, and 
also show that the discriminative methods are not only superior to a MAP-based 
method in terms of achieving larger CER reduction, but are also more robust 
against the similarity of background and adaptation domains.  
1   Introduction 
Language model (LM) adaptation attempts to adjust the parameters of a LM so that it 
performs well on a particular domain of data. This paper presents an empirical study 
of several LM adaptation methods on the task of Japanese text input. In particular, we 
focus on the so-called cross-domain LM adaptation paradigm, i.e. to adapt a LM 
trained on one domain to a different domain, for which only a small amount of train-
ing data is available. 
The LM adaptation methods investigated in this paper can be grouped into two 
categories: maximum a posterior (MAP) and discriminative training. Linear interpola-
tion is representative of the MAP methods [1]. The other three methods, including the 
boosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method 
[4], are discriminative methods, each of which uses a different training algorithm. 
We carried out experiments over many training data sizes on four distinct adapta-
tion corpora, the characteristics of which were measured using the information-
theoretic notion of cross entropy. We found that discriminative training methods  
                                                          
1
 This research was conducted while the author was visiting Microsoft Research Asia.  
958 W. Yuan, J. Gao, and H. Suzuki 
outperformed the LI method in all cases, and were more robust across different train-
ing sets of different domains and sizes. However, none of the discriminative training 
methods was found to outperform the others in our experiments.  
The paper is organized as follow. Section 2 introduces the task of IME and the role 
of LM. In Section 3, we review related work. After a description of the LM adaptation 
methods in our experiments in Section 4, Sections 5 and 6 present experimental re-
sults and their discussions. We conclude our paper in Section 7. 
2   Language Model and the Task of IME 
Our study falls into the context of Asian language (Japanese in this study) text input. 
The standard method for doing this is that the users first input the phonetic strings, 
which are then converted into the appropriate word string by software. The task of 
automatic conversion is called IME in this paper, which stands for Input Method Edi-
tor, based on the name of the commonly used Windows-based application. 
The performance of IME is typically measured in terms of the character error rate 
(CER), which is the number of characters wrongly converted from the phonetic string 
divided by the number of characters in the correct transcript. Current Japanese IME 
systems exhibit about 5-15% CER in conversion of real-world data in a wide variety 
of domains. In the following, we argue that the IME is a similar problem to speech 
recognition but is a better choice for evaluating language modeling techniques. 
Similar to speech recognition, IME can also be viewed as a Bayesian decision 
problem. Let A be the input phonetic string (which corresponds to the acoustic signal 
in speech). The task of IME is to choose the most likely word string W* among those 
candidates that could have been converted from A: 
)|()(maxarg)(
),(
maxarg)|(maxarg*
)()()(
WAPWP
AP
AWPAWPW
AGENWAGENWAGENW ???
===
 (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, there is almost no acoustic ambiguity in IME, because 
the phonetic string is provided directly by users. Moreover, we can assume a many-to-
one mapping from W to A in IME, i.e. P(A|W) = 1. So the decision of Equation (1) 
depends solely upon P(W), making IME a more direct evaluation test-bed for LM 
than speech recognition. Another advantage is that it is relatively easy to convert W to 
A, making it possible to obtain a large amount of training data for discriminative 
learning, as described later.  
3   Related Work 
Our goal is to quantify the characteristics of different domains of text, and to correlate 
them with the performance of various techniques for LM adaptation to compare their 
effectiveness and robustness. This relates our work to the study of domain similarity 
calculation and to different techniques for LM adaptation. 
 An Empirical Study on Language Model Adaptation 959 
3.1 Measuring Domain Similarity 
Statistical language modeling (SLM) assumes that language is generated from under-
lying distributions. When we discuss different domains of text, we assume that the 
text from each of these domains is generated from a different underlying distribution. 
We therefore consider the problem of distributional similarity in this paper. 
Cross entropy is a widely used measure in evaluating LM. Given a language L 
with its true underlying probability distribution p and another distribution q (e.g. a 
SLM) which attempts to model L, the cross entropy of L with respect to q is 
?
??
?=
nww
nn
n
wwqwwp
n
qLH
...
11
1
)...(log)...(1lim),(  (2) 
where w1?wn is a word string in L. However, in reality, the underlying p is never 
known and the corpus size is never infinite. We therefore make the assumption that L 
is an ergodic and stationary process [5], and approximate the cross entropy by calcu-
lating it for a sufficiently large n instead of calculating it for the limit. 
)...(log1),( 1 nwwq
n
qLH ??  (3) 
This measures how well a model approximates the language L.  
The KL divergence, or relative entropy, is another measure of distributional simi-
larity that has been widely used in NLP and IR [6]. Given the two distributions p and 
q above, the KL divergence is defined as 
?=
nww n
n
nnn
wwq
wwp
wwpwwqwwpD
...1 1
1
111 )...(
)...(log)...())...(||)...((  (4) 
The cross entropy and the KL divergence are related notions. Given the notations 
of L, p and q above, [5] shows that   
)||()(),( qpDLHqLH +=
 (5) 
In other words, the cross entropy takes into account both the similarity between two 
distributions (given by KL divergence) and the entropy of the corpus in question, both 
of which contribute to the complexity of a LM task. In this paper we are interested in 
measuring the complexity of the LM adaptation task. We therefore define the similar-
ity between two domains using the cross entropy. We will also use the metric that 
approximates the entropy of the corpus to capture the in-domain diversity of a corpus 
in Section 5.2.2  
3.2 LM Adaptation Methods 
In this paper, two major approaches to cross-domain adaptation have been investi-
gated: maximum a posteriori (MAP) estimation and discriminative training methods. 
                                                          
2
  There are other well-known metrics of similarity within NLP literature, such as the mutual 
information or cosine similarity [7], which we do not discuss in this paper.  
960 W. Yuan, J. Gao, and H. Suzuki 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data [1]. Dis-
criminative training methods to LM adaptation, on the other hand, aim at using the 
adaptation data to directly minimize the errors on the adaptation data made by the 
background model. These techniques have been applied successfully to the task of 
language modeling in non-adaptation [8] as well as adaptation scenarios [9] for speech 
recognition. But most of them focused on the investigation of performance of certain 
methods for LM adaptation, without analyzing in detail the underlying reasons of 
different performance achieved by different methods. In this paper we attempt to 
investigate the effectiveness of different discriminative methods in an IME adaptation 
scenario, with a particular emphasis on correlating their performance with the charac-
teristics of adaptation domain. 
4   LM Adaptation Methods 
We implement four methods in our experiments. The Linear Interpolation (LI) falls 
into the framework of MAP while the boosting, the perceptron and the MSR methods 
fall into that of discriminative training.  
4.1   Linear Interpolation (MAP) 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data. 
The linear interpolation is a special case of MAP according to [10]. At first, we 
generate trigram models on background data and adaptation data respectively. The 
two models are then combined into one as: 
)|()1()|()|( hwPhwPhwP iAiBi ?? ?+=  (6) 
where PB is the probability of the background model, PA is the probability of the adap-
tation model and the history h corresponds to two preceding words. For simplicity, we 
chose a single ? for all histories and tuned it on held-out data. 
4.2   Discriminative Training 
Discriminative training follows the general framework of linear models [2][3]. We use 
the following notation in the rest of the paper. 
? Training data is a set of example input/output pairs {Ai, WiR} for i = 1?M, where 
each Ai is an input phonetic string and each WiR is the reference transcript of Ai. 
? We assume a set of D+1 features, fd(W), for d=0?D, where each feature is a func-
tion that maps W to a real value. Using vector notation, we have f(W)={ f0(W), 
f1(W)?fD(W)} and f(W) .1+? DR  Without loss of generality, f0(W) is called the base 
model feature, and is defined in this paper as the log probability that the back-
ground trigram model assigns to W. fd(W), for d=1?D, are defined as the counts of 
the word n-gram in W, where n = 1 and 2 in our case. 
 An Empirical Study on Language Model Adaptation 961 
? Finally, the parameters of the model form a vector of D + 1 dimensions, each for 
one feature function, ?= {?0, ?1? ?D}. The likelihood score of a word string W is 
?
=
==
D
d
dd WfWWScore
0
)()(),( ??f?  (7) 
Then the decision rule of Equation (1) can be re-written as  
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
=
 (8) 
Assume that we can measure the number of conversion errors in W by comparing it 
with a reference transcript WR using an error function Er(WR, W), which is an edit 
distance in our case. We call the sum of conversion errors over the training data as 
sample risk (SR). Discriminative training methods strive to minimize the SR by opti-
mizing the model parameters, as defined in Equation (9). 
?
=
==
Mi
ii
R
i
* AWWErSR
...1
)),(,(minarg)(minarg ???
??
 (9) 
However, SR(.) cannot be optimized easily since Er(.) is a piecewise constant (or 
step) function of ? and its gradient is undefined. Therefore, discriminative methods 
apply different approaches that optimize it approximately. As we shall describe be-
low, the boosting and perceptron algorithms approximate SR(.) by loss functions that 
are suitable for optimization, while MSR uses a simple heuristic training procedure to 
minimize SR(.) directly without applying any approximated loss function. We now 
describe each of the discriminative methods in turn. 
The boosting algorithm [2] uses an exponential function to approximate SR(.). We 
define a ranking error in a case where an incorrect candidate conversion W gets a 
higher score than the correct conversion WR. The margin of the pair (WR, W) with 
respect to the model ? is estimated as 
),(),(),( ?? WScoreWScoreWWM RR ?=  (10) 
Then we define an upper bound to the number of ranking errors as the loss function, 
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?  (11) 
Now, ExpLoss(.) is convex with respect to ?, so there are no problems with local 
minima when optimizing it. The boosting algorithm can be viewed as an iterative 
feature selection method: at each iteration, the algorithm selects from all possible 
features the one that is estimated to have the largest impact on reducing the ExpLoss 
function with respect to the current model, and then optimizes the current model by 
adjusting only the parameter of the selected feature while keeping the parameters of 
other features fixed. 
The perceptron algorithm [3] can be viewed as a form of incremental training pro-
cedure that optimizes a minimum square error (MSE) loss function, which is an ap-
proximation of SR(.). As shown in Figure 1, it starts with an initial parameter setting 
and adapts it each time a training sample is wrongly converted.  
962 W. Yuan, J. Gao, and H. Suzuki 
1 Initialize all parameters in the model, i.e. ?0 = 1 and ?d = 0 for d=1?D 
2 For t = 1?T, where T is the total number of iterations 
For each training sample (Ai, WiR), i = 1?M 
Use current model ? to choose some Wi from GEN(Ai) by Equation (8) 
    For d = 1 ? D 
        ?d  = ?d + ?(fd (WiR)- fd (Wi)), where ? is the size of the learning step  
Fig. 1. The standard perceptron algorithm with delta rule 
In our experiments, we used the average perceptron algorithm in [3], a simple refine-
ment to the algorithm in Figure 1, which has been proved to be more robust. Let ?dt,i 
be the value for the dth parameter after the ith training sample has been processed in 
pass t over the training data. Then the ?average parameters? are defined as in Equa-
tion (12). 
)/()()(
1 1
, MT
T
t
M
i
it
davgd ?= ??
= =
??  (12) 
The minimum sample risk (MSR) method [4] can be viewed as a greedy stage-
wise learning algorithm that minimizes the sample risk SR(.) directly as it appears in 
Equation (9). Similar to the boosting method, it is an iterative procedure. In each 
iteration, MSR selects a feature that is estimated to be most effective in terms of re-
ducing SR(.), and then optimizes the current model by adjusting the parameters of the 
selected feature. MSR, however, differs from the boosting method in that MSR tries 
to optimize the sample risk directly while the boosting optimizes the loss function that 
is an upper bound of the sample risk.  
As mentioned earlier, SR(.) can be optimized using regular gradient-based optimi-
zation algorithms. MSR therefore uses a particular implementation of line search, 
originally proposed in [11], to optimize the current model by adjusting the parameter 
of a selected feature while keeping other parameters fixed.  
Assuming fd is the selected feature, its parameter ?d is optimized by line search as 
follows. Recall that Er(WR,W) is the function that measures the number of conversion 
errors in W versus its reference transcript WR. The value of SR(.) is the sum of Er(.) 
over all training samples. For each A in training set, let GEN(A) be the set of n-best 
candidate word strings that could be converted from A. By adjusting ?d, we obtain for 
each training sample an ordered sequence of ?d intervals. For ?d in each interval, a 
particular candidate would be selected according to Equation (8). Then the corre-
sponding Er(.) is associated with the interval. As a result, for each training sample, we 
obtain a sequence of ?d intervals and their corresponding Er(.) values. By combining 
the sequences over all training samples, we obtain a global sequence of ?d intervals, 
each of which is associated with a SR(.) value. Therefore we can find the optimal 
interval of ?d as well as its corresponding sample risk by traversing the sequence and 
taking the center of the interval as the optimal value of ?d. 
 An Empirical Study on Language Model Adaptation 963 
5   Experimental Results 
5.1   Data 
The data used in our experiments stem from five distinct sources of text. A 36-
million-word Nikkei newspaper corpus was used as the background domain. We used 
four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus 
containing newspaper and other sources of text), Encarta (encyclopedia) and Shincho 
(collection of novels).  
For the computation of domain characteristics (Section 5.2), we extracted 1 million 
words from the training data of each domain respectively (corresponding to 13K to 
78K sentences depending on the domain). For this experiment, we also used a lexicon 
consisting of the words in our baseline lexicon (167,107 words) plus all words in the 
corpora used for this experiment (that is, 1M words times 5 domains), which included 
216,565 entries. The use of such a lexicon was motivated by the need to eliminate the 
effect of out-of-vocabulary (OOV) items.  
For the experiment of LM adaptation (Section 5.3), we created training data con-
sisting of 72K sentences (0.9M~1.7M words) and test data of 5K sentences 
(65K~120K words) from each adaptation domain. The first 800 and 8,000 sentences 
of each adaptation training data were also used to show how different sizes of adapta-
tion training data affected the performances of various adaptation methods. Another 
5K-sentence subset was used as held-out data for each domain. For domain adaptation 
experiments, we used our baseline lexicon consisting of 167,107 entries.   
5.2   Computation of Domain Characteristics 
The first domain characteristic we computed was the similarity between two domains 
for the task of LM. As discussed in Section 3, we used the cross entropy as the metric: 
we first trained a word trigram model using the system described in [12] on the 1-
million-word corpus of domain B, and used it in the computations of the cross entropy 
H(LA, qB) following equation (3). For simplicity, we denote H(LA, qB) as H(A,B).  
Table 1 displays the cross entropy between two domains of text. Note that the cross 
entropy is not symmetric, i.e., H(A,B) is not necessarily the same as H(B,A). In order 
to have a representative metric of similarity between two domains, we computed the 
average cross entropy between two domains, shown in Table 2, and used this quantity 
as the metric for domain similarity.   
Along the main diagonal in the tables below, we also have the cross entropy com-
puted for H(A,A), i.e., when two domains we compare are the same (in boldface). This 
value, which we call self entropy for convenience, is an approximation of the entropy 
of the corpus A, and measures the amount of information per word, i.e., the diversity 
of the corpus. Note that the self entropy increases in the order of Nikkei ? Yomiuri 
? Encarta ? TuneUp ? Shincho. This indeed reflects the in-domain variability of 
text: Nikkei, Yomiuri and Encarta are highly edited text, following style guidelines; 
they also tend to have repetitious content. In contrast, Shincho is a collection of nov-
els, on which no style or content restriction is imposed. We expect that the LM task to 
964 W. Yuan, J. Gao, and H. Suzuki 
be more difficult as the corpus is more diverse; we will further discuss the effect of 
diversity in Section 6.3 
Table 1. Cross entropy (rows: corpora; column: models) 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.46 7.65 9.81 10.10 
Yomiuri 7.93 4.09 7.62 9.26 9.97 
TuneUp 8.25 8.03 4.41 9.04 9.06 
Encarta 8.79 8.66 8.60 4.40 9.30 
Shincho 8.70 8.61 8.07 9.10 4.61 
Table 2. Average cross entropy 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.69 7.95 9.30 9.40 
Yomiuri  4.09 7.82 8.96 9.29 
TuneUp   4.41 8.82 8.56 
Encarta    4.40 9.20 
Shincho     4.61 
5.3   Results of LM Adaptation 
We trained our baseline trigram model on our background (Nikkei) corpus using the 
system described in [12]. The CER (%) of this model on each adaptation domain is in 
the second column of Table 3. For the LI adaptation method (the third column of 
Table 3), we trained a word trigram model on the adaptation data, and linearly com-
bined it with the background model, as described in Equation (6).  
For the discriminative methods (the last three columns in Table 3), we produced  a 
candidate word lattice for each input phonetic string in the adaptation training set 
using the background trigram model mentioned above. For efficiency purposes, we 
kept only the best 20 hypotheses from the lattice as the candidate conversion set for 
discriminative training. The lowest CER hypothesis in the lattice, rather than the ref-
erence transcript, was used as the gold standard.  
To compare the performances of different discriminative methods, we fixed the 
following parameter settings: we set the number of iterations N to be 2,000 for the 
boosting and MSR methods (i.e., at most 2,000 features in the final models); for the 
perceptron algorithm, we set T = 40 (in Figure 1). These settings might lead to an 
                                                          
3
  Another derivative notion from Table 1 is the notion of balanced corpus. In Table 1, the 
smallest cross entropy for each text domain (rows) is the self entropy (in boldface), as ex-
pected. Note, however, that the second smallest cross entropy (underlined) is always obtained 
from the TuneUp model (except for Nikkei, for which Yomiuri provides the second smallest 
cross entropy). This reflects the fact that the TuneUp corpus was created by collecting sen-
tences from various sources of text, in order to create a representative test corpus. Using the 
notion of cross entropy, such a characteristic of a test corpus can also be quantified.  
 An Empirical Study on Language Model Adaptation 965 
unfair comparison, as the perceptron algorithm will select far more features than the 
boosting and MSR algorithm. However, we used these settings as they all converged 
under these settings.  All other parameters were tuned empirically. 
In evaluating both MAP and discriminative methods, we used an N-best rescoring 
approach. That is, we created N best hypotheses using the background trigram model 
(N=100 in our experiments) for each sentence in test data, and used domain-adapted 
models to rescore the lattice. The oracle CERs (i.e., the minimal possible CER given 
the hypotheses in the lattice) ranged from 1.45% to 5.09% depending on the adapta-
tion domain. Table 3 below summarizes the results of various adaptation methods in 
terms of CER (%) and CER reduction (in parentheses) over the baseline model. In the 
first column, the numbers in parentheses next to the domain name indicates the num-
ber of training sentences used for adaptation. 
Table 3. CER (%) and CER reduction over Baseline 
Domain Baseline LI Boosting Perceptron MSR 
Yomiuri (800) 3.70 3.70 (0.00%) 3.13 (15.41%) 3.18 (14.05%) 3.17 (14.32%) 
Yomiuri (8K) 3.70 3.69 (0.27%) 2.88 (22.16%) 2.85 (22.97%) 2.88 (22.16%) 
Yomiuri (72K) 3.70 3.69 (0.27%) 2.78 (24.86%) 2.78 (24.86%) 2.73 (26.22%) 
TuneUp (800) 5.81 5.81 (0.00%) 5.69 (2.07%) 5.69 (2.07%) 5.70 (1.89%) 
TuneUp (8K) 5.81 5.70 (1.89%) 5.47 (5.85%) 5.47 (5.85%) 5.47 (5.85%) 
TuneUp (72K) 5.81 5.47 (5.85%) 5.33 (8.26%) 5.20 (10.50%) 5.15 (11.36%) 
Encarta (800) 10.24 9.60 (6.25%) 9.82 (4.10%) 9.43 (7.91%) 9.44 (7.81%) 
Encarta (8K) 10.24 8.64 (15.63%) 8.54 (16.60%) 8.34 (18.55%) 8.42 (17.77%) 
Encarta (72K) 10.24 7.98 (22.07%) 7.53 (26.46%) 7.44 (27.34%) 7.40 (27.73%) 
Shincho (800) 12.18 11.86 (2.63%) 11.91 (2.22%) 11.90 (2.30%) 11.89 (2.38%) 
Shincho (8K) 12.18 11.15 (8.46%) 11.09 (8.95%) 11.20 (8.05%) 11.04 (9.36%) 
Shincho (72K) 12.18 10.76 (11.66%) 10.25 (15.85%) 10.18 (16.42%) 10.16 (16.58%) 
6   Discussion 
6.1   Domain Similarity and CER 
The first row of Table 2 shows that the average cross entropy with respect to the 
background domain (Nikkei) increases in the following order: Yomiuri ? TuneUp ? 
Encarta ? Shincho. This indicates that among the adaptation domains, Yomiuri is the 
most similar to Nikkei, closely followed by TuneUp; Shincho and Encarta are the 
least similar to Nikkei. This is consistent with our intuition, since Nikkei and Yomiuri 
are both newspaper corpora, and TuneUp, which is a manually constructed corpus 
from various representative domains of text, contains newspaper articles.  
966 W. Yuan, J. Gao, and H. Suzuki 
This metric of similarity correlates perfectly with the CER. In Table 3, we see that 
for all sizes of training data for all adaptation methods, the following order of CER 
performance is observed, from better to worse: Yomiuri ? TuneUp ? Encarta ? 
Shincho. In other words, the more similar the adaptation domain is to the background 
domain, the better the CER results are.  
6.2   Domain Similarity and the Effectiveness of Adaptation Methods  
The effectiveness of a LM adaptation method is measured by the relative CER reduc-
tion over the baseline model. Figure 3 shows the CER reduction of various methods 
for each domain when the training data size was 8K.4 
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
Yomiuri TuneUp Encarta Shincho
LI
Boosting
Perceptron
MSR
  
Fig. 2. CER reduction by different adaptation methods 
In Figure 2 the X-axis is arranged in the order of domain similarity with the back-
ground domain, i.e., Yomiuri ? TuneUp ? Encarta ? Shincho. The first thing we 
note is that the discriminative methods outperform LI in all cases: in fact, for all rows 
in Table 3, MSR outperforms LI in a statistically significant manner (p < 0.01 using t-
test);5 the differences among the three discriminative methods, on the other hand, are 
not statistically significant in most cases. 
We also note that the performance of LI is greatly influenced by domain similarity. 
More specifically, when the adaptation domain is similar to the background domain 
(i.e., for Yomiuri and TuneUp corpora), the contribution of the LI model is extremely 
limited. This can be explained as follows: if the adaptation data is too similar to the 
background, the difference between the two underlying distributions is so slight that 
adding adaptation data leads to no or very small improvements.  
Such a limitation is not observed with the discriminative methods. For example, all 
discriminative methods are quite effective on Yomiuri, achieving more than 20% 
CER reduction. We therefore conclude that discriminative methods, unlike LI, are 
robust against the similarity between background and adaptations domains.  
                                                          
4
 Essentially the same trend is observed with other training data sizes.  
5
 The only exception to this was Shincho (800).  
 An Empirical Study on Language Model Adaptation 967 
6.3   Adaptation Data Size and CER Reduction 
We have seen in Table 3 that in all cases, discriminative methods outperform LI. 
Among the discriminative methods, an interesting characteristic regarding the CER 
reduction and the data size is observed. Figure 3 displays the self entropy of four 
adaptation corpora along the X-axis, and the improvement in CER reduction when 
72K-sentence adaptation data is used over when 800 sentences are used along the Y-
axis. In other words, for each adaptation method, each point in the figure corresponds 
to the CER reduction ratio on a domain (corresponding to Yomiuri, Encarta, TuneUp, 
Shincho from left to right) when 90 times more adaptation data was available.  
0
1
2
3
4
5
6
7
4 4.2 4.4 4.6 4.8
self entropy
C
E
R
 
r
e
d
u
c
t
i
o
n
 
r
a
t
i
o
 
(
%
)
Boosting
Perceptron
MSR
  
Fig. 3. Improvement in CER reduction for discriminative methods by increasing the adaptation 
data size from 800 to 72K sentences 
From this figure, we can see that there is a positive correlation between the diversity 
of the adaptation corpus and the benefit of having more training data available. This 
has an intuitive explanation: the less diverse the adaptation data is, the less distinct 
training examples it will include for discriminative training. This result is useful in 
guiding the process of adaptation data collection.  
7   Conclusion and Future Work 
In this paper, we have examined the performance of various LM adaptation methods 
in terms of domain similarity and diversity. We have found that (1) the notion of 
cross-domain similarity, measured by the cross entropy, correlates with the CER of all 
models (Section 6.1), and (2) the notion of in-domain diversity, measured by the self 
entropy, correlates with the utility of more adaptation training data for discriminative 
training methods (Section 6.3). In comparing discriminative methods with a MAP-
based method, we have also found that (1) the former uniformly achieve better CER 
performance than the latter, and (2) are more robust against the similarity of back-
ground and adaptation data (Section 6.2).  
968 W. Yuan, J. Gao, and H. Suzuki 
Though we believe these results are useful in designing the future experiments in 
domain adaptation, some results and correlations indicated in the paper are still incon-
clusive. We hope to run additional experiments to confirm these findings. We also did 
not fully investigate into characterizing the differences among the three discriminative 
methods; such an investigation is also left for future research.  
References 
1. Bellagarda, J. An Overview of Statistical Language Model Adaptation. In ITRW on Adap-
tation Methods for Speech Recognition (2001): 165-174. 
2. Collins, M. Ranking Algorithms for Name-Entity Extraction: Boosting and the Voted Per-
ceptron. ACL (2002).  
3. Collins, M. Discriminative Training Methods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. EMNLP (2002). 
4. Gao. J., H. Yu, P. Xu, and W. Yuan. Minimum Sample Risk Methods for Language Mod-
eling. To Appear (2005). 
5. Manning, C.D., and H. Sch?tze. Foundations of Statistical Natural Language Processing. 
The MIT Press (1999).  
6. Dagan, I., L. Lee, and F. Pereira. Similarity-based models of cooccurrence probabilities. 
Machine Learning, 34(1-3): 43-69 (1999).  
7. Lee, L. Measures of distributional similarity. ACL (1999): 25-32. 
8. Roark, B, M. Saraclar and M. Collins. Corrective Language Modeling for Large Vocabu-
lary ASR with the Perceptron Algorithm. ICASSP (2004): 749-752. 
9. Bacchiani, M., B. Roark and M. Saraclar. Language Model Adaptation with MAP Estima-
tion and the Perceptron Algorithm. HLT-NAACL (2004): 21-24. 
10. Bacchiani, M. and B. Roark. Unsupervised language model adaptation. ICASSP (2003): 
224-227 
11. Och, F.J. Minimum error rate training in statistical machine translation. ACL (2003): 160-
167. 
12. Gao, J., J. Goodman, M. Li, and K.F. Lee. Toward a unified approach to statistical lan-
guage modeling for Chinese. ACM Transactions on Asian Language Information Process-
ing l-1 (2002): 3-33.  
Transformation Based Chinese Entity Detection and Tracking 
Yaqian Zhou? 
Dept. Computer Science and Enginering 
Fudan Univ. 
Shanghai 200433, China 
ZhouYaqian@fudane.edu.cn 
Changning Huang 
Microsoft Research, Asia 
Beijing 100080, China 
cnhuang@msrchina.research.
microsoft.com 
Jianfeng Gao 
Microsoft Research, Asia 
Beijing 100080, China 
jfgao@microsoft.com 
Lide Wu 
Dept. of CSE., Fudan Univ. 
Shanghai 200433, China 
ldwu@fudane.edu.cn 
                                                          
? This work is done while the first author is visiting Microsoft Research Asia. 
 
 
Abstract 
This paper proposes a unified 
Transformation Based Learning (TBL, 
Brill, 1995) framework for Chinese 
Entity Detection and Tracking (EDT). 
It consists of two sub models: a 
mention detection model and an entity 
tracking/coreference model. The first 
sub-model is used to adapt existing 
Chinese word segmentation and Named 
Entity (NE) recognition results to a 
specific EDT standard to find all the 
mentions. The second sub-model is 
used to find the coreference relation 
between the mentions. In addition, a 
feedback technique is proposed to 
further improve the performance of the 
system. We evaluated our methods on 
the Automatic Content Extraction 
(ACE, NIST, 2003) Chinese EDT 
corpus. Results show that it 
outperforms the baseline, and achieves 
comparable performance with the state-
of-the-art methods. 
1 Introduction 
The task of Entity Detection and Tracking (EDT) 
is suggested by the Automatic Content Extrac-
tion (ACE) project (NIST, 2003). The goal is to 
detect all entities in a given text and track all 
mentions that refer to the same entity. The task 
is a fundamental to many Natural Language 
Processing (NLP) applications, such as informa-
tion retrieval and extraction, text classification, 
summarization, question answering, and ma-
chine translation.  
EDT is an extension of the task of 
coreference resolution in that in EDT we not 
only resolve the coreference between mentions 
but also detect the entities. Each of those entities 
may have one or more mentions. In the ACE 
project, there are five types of entities defined in 
EDT: person (PER), geography political Entity 
(GPE), organization (ORG), location (LOC), 
and facility (FAC). Many traditional coreference 
techniques can be extended to EDT for entity 
tracking. 
Early work on pronoun anaphora resolution 
usually uses rule-based methods (e.g. Hobbs 
1976; Ge et al, 1998; Mitkov, 1998), which try 
to mine the cues of the relation between the pro-
nouns and its antecedents. Recent research 
(Soon et al, 2001; Yang et al, 2003; Ng and 
Cardie, 2002; Ittycherah et al, 2003; Luo et al, 
2004) focuses on the use of statistical machine 
learning methods and tries to resolve references 
among all kinds of noun phases, including name, 
nominal and pronoun phrase. One common ap-
proach applied by them is to first train a binary 
statistical model to measure how likely a pair of 
232
mentions corefer; and then followed by a greedy 
procedure to group the mentions into entities. 
Mention detection is to find all the named en-
tity, noun or noun phrase, pronoun or pronoun 
phrase. Therefore, it needs Named Entity Rec-
ognition, but not only. Though the detection of 
entity mentions is an essential problem for 
EDT/coreference, there has been relatively less 
previous research. Ng and Cardie (2002) shows 
that improving the recall of noun phrase identi-
fication can improve the performance of a 
coreference system. Florian et al (2004) formu-
late the mention detection problem as a charac-
ter-based classification problem. They assign for 
each character in the text a label, indicating 
whether it is the start of a specific mention, in-
side a specific mention, or outside of any men-
tion. 
In this paper, we propose a unified EDT 
model based on the Transformation Based 
Learning (TBL, Brill, 1995) framework for Chi-
nese. The model consists of two sub models: a 
mention detection model and a coreference 
model. The first sub-model is used to adapt ex-
isting Chinese word segmentation and Named 
Entity (NE) recognition system to a specific 
EDT standard. TBL is a widely used machine 
learning method, but it is the first time it is ap-
plied to coreference resolution. In addition, a 
feedback technique is proposed to further im-
prove the performance of the system. 
The rest of the paper is organized as follows. 
In section 2, we propose the unified TBL Chi-
nese EDT model framework. We describe the 
four key techniques of our Chinese EDT, the 
word segmentation adaptation model, the men-
tion detection model, the coreference model and 
the feedback technique in section 3, 4, 5 and 6 
accordingly. The experimental results on the 
ACE Chinese EDT corpus are shown in section 
7. 
2 The Unified System Framework 
Our Chinese EDT system consists of two com-
ponents, mention detection module and corefer-
ence module besides a feedback technique 
between them as illustrated in Figure 1. 
MSRSeg (Gao et al, 2003; Gao et al), Mi-
crosoft Research Asia?s Chinese word segmen-
tation system that is integrated with named 
entity recognition, is used to segment Chinese 
words. However MSRSeg can?t well match the 
standard of ACE EDT evaluation for either 
types or boundaries. The difference of the stan-
dard of named entity between MSRSeg and 
ACE cause more than half of the errors for 
NAME mention detection. In order to overcome 
these problems, we integrate a segmentation 
adapter to mention detection model. 
The EDT system is a unified system that uses 
the TBL scheme. The idea of TBL is to learn a 
list of ordered rules while progressively improve 
upon the current state of the training set. An ini-
tial assignment is made based on simple statis-
tics, and then rules are greedily learned to 
correct the mistakes, until no more improvement 
can be made. There are three main problems in 
the TBL framework: An initial state assignment, 
a set of allowable templates for rules, and an 
objective function for learning. 
Figure 1. Entity detection and tracking system 
flow. 
3 Word Segmentation Adaptation 
The method of applying TBL to adapt the Chi-
nese word segmentation standard has been de-
scribed in Gao et al (2004). Our approach is 
slightly different for not have a correctly seg-
mented corpus according to ACE standard. 
From the un-segmented ACE EDT corpus, 
we can only obtain mention boundary informa-
tion. So the adapting objective is to detect the 
mention boundary instead of all words in text, 
correctly. In the corpus, very few mentions? 
boundaries are crossing1. 
The initial state of the segmentation adapta-
tion model is the output of MSRSeg. And we 
                                                          
1 The mentions? extents are frequently crossing, while 
heads not. 
MSRSeg&POS
Tagging 
Mention 
Detection 
Model 
Coreference 
Model 
Raw 
Document
Mentions Entities
Seg/POS/NE 
Document 
233
define two actions in the model, inserting and 
removing a boundary. The prefix or suffix of 
current word is used to define the boundary of 
inserting. Both inserting and removing action 
consider the combination of POS tag and word 
string of current, left and right words.  
When inserting a boundary, the right part of 
the word keeps the old POS tag, and the left part 
introduces a special POS tag ?new?. When re-
moving a boundary, the new formed word intro-
duces a special POS tag ?new?. The following 
two examples illustrate the strategy. 
????? /nt/court of Russia ? ???
/new/Russia ??/nt/court 
?/nr/Bo ?/nr/Pu ???/new/Bopu 
 
4 Mention Detection 
Since the word segmentation adaptation model 
has corrected the boundaries of mentions, our 
mention detection model bases on word and 
only tagging the entity mention types. The 
model detects the mentions by tagging sixteen 
tags (including the combination of five entity 
types and three mention types and ?OTHER? tag) 
for all the words outputted by segmentation ad-
aptation model. The templates, as illustrated in 
table 1, only refer to local features, such as POS 
tag and word string of left, right, and current 
words; the suffix, and single character feature of 
current word. 
Table 1. Templates for mention detection. 
MT1: P0 MT9: R4,P0 
MT2: W0 MT10: R3,P0 
MT3: P0,W0 MT11: R2,P0 
MT4: P_1,W0 MT12: R1,P0 
MT5: P_1,P0 MT13: S0,P0 
MT6: W0,P1 MT14: T_1,W0 
MT7: P0,P1 MT15: T_1,P0 
MT8: W0,W1 MT16: P0,T1 
Table 2. Examples of transformation rules of 
mention detection. 
MR1: MT13 0 ns GPE 
MR2: MT13 0 nr PER 
MR3: MT13 0 nt ORG 
MR4: MT16 n PER NPER 
MR5: MT16 new ORG GPE 
In table 1, ?MT1? et alrepresent the id of the 
templates; ?R1?, ?R2?, ?R3? and ?R4? represent 
the suffix of current word and the number of 
character is 1, 2, 3 and 4 accordingly; other suf-
fix ?_1?, ?0?, ?1? means the left, current and 
right words? feature; ?W? represent the string of 
word; ?P? represent POS tag; ?T? represent 
mention tag; ?S? represent the binary-value sin-
gle character feature. 
Five best transformation rules are illustrated 
in Table 2. For example, MR3 means ?if current 
word?s POS tag is nt, then it is a ORG?. Follow-
ing example well describe the process of apply-
ing these rules. 
???/new/Russia ??/nt/court  
????/new/Russia [??/nt/court]ORG (MR3)
?[ ? ? ? /new/Russia]GPE [ ? ?
/nt/court]ORG 
(MR5)
5 Entity Tracking 
In our entity tracking/coreference model, the 
initial state is let each mention in a document 
form an entity, as shown in Figure 2 (a). And the 
objective function directs the learning process to 
insert or remove chains between mentions (Fig-
ure 2 b and c) to approach the goal state (Figure 
2 f). 
A list of rules is learned in greedy fashion, 
according to the objective function. When no 
rule that improves the current state of the train-
ing set beyond a pre-set threshold can be found, 
the training phrase ends. The objective function 
in our system is driven by the correctness of the 
binary classification for pair-wise mention pairs. 
The TBL entity tracking model has more 
widely clustering/searching space as compare 
with previous strategies (Soon et al 2001; Ng 
and Cardie, 2002; Luo et al, 2004). For example, 
the state shown in Figure 2 (d) is not reachable 
for them. Because they assume one mention 
should refer to its most confidential mentions or 
entities that before it, while A and B are obvi-
ously not in same entity, as we can see in Figure 
2 (d). Thus C can refer to either A or B, but not 
both. While in TBL model, this state is allowed. 
In order to keep our system robust, the trans-
formation templates refer to only six types of 
simple features, as described below.  
All these features do not need any high level 
tools (i.e. syntactic parser) and little external 
knowledge base. In fact, only a country name 
abbreviation list (171 entrances) and a Chinese 
234
province alias list (34 entrances) are used to de-
tect ?alias? relation for String Match feature. 
String Match feature (STRM): Its possible 
values are exact, alias, abbr, left, right, other. If 
two mentions are exact string matched, then re-
turn exact; else if one mention is an alias of the 
other, then return alias; else if one mention is 
the abbreviation of the other, then return abbr; 
else if one mention is the left substring of the 
other, then return left; else if one mention is the 
right substring of the other, then return right; 
else return other. 
Figure 2. The procedure of TBL entity track-
ing/coreference model 
Edit Distance feature I (ED1): Its possible 
values are true or false. If the edit distance of the 
two mentions are less than or equal to 1, then 
return true, else return false. 
Token Distance feature I (TD1): Its possi-
ble values are true or false. If the edit distance of 
the two mentions are less than or equal to 1(i.e., 
there are not more than one token between the 
two mentions), then return true, else return false. 
Mention Type (MT): Its possible values are 
NAME, NOMINAL, or PRONOUN.. 
Entity Type (ET): Its possible values are 
PER, GPE, ORG, LOC, or FAC. 
Mention String (M): Its possible values are 
the actual mention string. 
These six features can be divided into two 
categories: mention pair features (the first three) 
and single mention features (the other three). 
And the single mention features are suffixed 
with ?L? or ?R? to differentiate for left or right 
mentions (i.e. ETL represent the left mention?s 
entity type). 
Based on the six kinds of basic features, four 
simple transformation templates are used in our 
system, as listed in table 3. 
Table 3. Templates for coreference model. 
CT1: MTL,MTR,STRM 
CT2: MTL,MTR,ETL,ETR,ED1 
CT3: MTL,MTR,ETL,ETR,TD1 
CT4: MTL,MTR,ML,MR 
Table 4. Examples of transformation rules of 
coreference model. 
CR1:CT1 NAME NAME EXACT LINK 
CR2:CT2 NOMINAL NAME PER PER 1 LINK 
CR3:CT1 NAME NAME ALIAS LINK 
CR4:CT1 PRONOUN PRONOUN EXACT LINK 
Though trained on different data set will 
learn different rules, the four rules listed in table 
4 is the best rules that always been learned. For 
example, the first rule means that ?If two NAME 
mentions are exact string matched, then insert a 
chain between them?. The following example 
illustrates the process. 
[??/US]GPE??[???/Russia]GPE?
???????[?? /US]GPE[??
/businessman]NPER[??/Bopu]PER 
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER[??
/Bopu]PER 
(CR1)
? [ ? ? /US]GPE-1 ? ? [ ? ? ?
/Russia]GPE ????????[??
/US]GPE-1[?? /businessman]NPER-2[??
/Bopu]PER-2 
(CR2)
6 Feedback 
There are three reasons push us apply feedback 
technique in the EDT system. The first is to de-
termine whether a signal character is an abbre-
viation is discourse depended. For example, 
Chinese character ??? can represents both a 
country name ?China? and a common preposi-
tion ?in?. If it can links to ??? /China? by 
coreference model, it is likely to represent 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B
 
C    D
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
A    B 
 
C    D 
 
E 
(e) 
(a)                     (b)                          (c) 
(d) 
(f) 
235
?China?. The second is the definition of men-
tions is hard to hold, especially the nominal 
mentions. An isolated mention is more likely not 
to be a mention. The third is to pick up lost men-
tion according to its multi-appearance in the dis-
course. In fact, [Ji and Crishman, 2004] has used 
five hubristic rules based on coreference results 
to improve the name recognition result. While in 
this section we will present an automatic method. 
The feedback technique is employed by us-
ing entity features in mention detection model. 
In our model, the transformation templates refer 
to the number of mentions in the entity, the sin-
gle character feature, the entity type feature, the 
mention type feature and mention string, as 
listed follows. 
SDD: Its possible values are the combination 
of the mention type and entity type of the men-
tion string in discourse: PER, GPE, ORG, LOC, 
FAC, NPER (NOMINAL PER), NGPE, NORG, 
NLOC, NFAC, PPER (PROUNOUN PER), 
PGPE, PORG, PLOC, and PFAC. 
SC2, SC3, SC4: Their possible values are 
true or false. If the word string appear not less 
than 2 (3, 4) times in the discourse then return 
true, else return false. 
PDD: presents the combination of the men-
tion type and entity type of the mention in dis-
course. Its possible values are same with ?SDD?. 
PC2: Its possible values are true or false. If 
the mention belong to an entity has not less than 
2 mentions then return true, else return false. 
S0: Its possible values are true or false. If the 
mention is a single character word then return 
true, else return false. 
W0: string of the mention. 
Table 5. Templates for feedback. 
FT1: SDD,SC2 FT4: PDD,PC2,S0 
FT2: SDD,SC3 FT5: PDD,PC2,S0 
FT3: SDD,SC4 FT6: PDD,PC2,W0 
Table 6. Examples of transformation rules of 
feedback. 
FR1: FT1 PER T PER FR4: FT4 NORG F O 
FR2: FT5 GPE F 1 O FR5: FT3 PGPE F O 
FR3: FT4 NFAC F O  
The first rule means that ?if a word in the 
document appears as person name more than 
two times, then it is a person name?. This rule 
can pick up lost person names. The second rule 
means that ?if a GPE mention is isolated and it 
is a single character word, then it is not a men-
tion?. This rule can throw away isolated abbre-
viation of GPE, as illustrated in the following 
example. 
?[??/Bopu]PER-3 ?????[???
/Russia]GPE-2[?? /court]ORG-4[? /by]GPE-6 
????? 20??? ? 
??[??/Bopu]PER-3 ?????[??
?/Russia]GPE-2[??/court]ORG-4?/by ?
???? 20??? ? 
(FR2)
7 Experiments 
Our experiments are conducted on Chinese EDT 
corpus for ACE project from LDC. This corpus 
is the training data for ACE evaluation 2003. 
The corpus has two types, paper news (nwire) 
and broadcast news (bnews). the statistics of the 
corpus is shown in Table 7. 
Table 7. Statistics of the ACE corpus. 
 nwire bnews 
Document 99 122 
Character 55,000 45,000 
Entity 2517 2050 
Mention 5423 4506 
Because the test data for ACE evaluation is 
not public, we randomly and equally divide the 
corpus into 3 subsets: set0, set1, set2. Each con-
sists of about 73 documents and 33K Chinese 
Characters 2 . Cross experiments are conducted 
on these data sets. ACE-value is used to evaluate 
the EDT system; and precision (P), recall (R) 
and F (F=2*P*R/(P+R)) to evaluate the mention 
detection result. 
In the experiments, we first use one data set 
train the mention detection system; then use an-
other set train the coreference model based on 
the output of the mention detection; finally use 
the other set test. In practice, we can retrain the 
mention detection model use the two train set to 
get higher performance. 
Table 8. EDT and mention detection results. 
 EDT Mention Detection 
Method ACE-
value 
R P F 
Tag 55.7?1.6 62.3?1.0 85.0?1.4 71.9?0.6
SegTag 61.6?3.6 70.9?4.5 81.9?1.0 75.9?2.6
SegTag+F 63.3?2.0 68.0?4.8 83.8?1.2 75.0?3.1
                                                          
2 Two of the documents (CTS20001110.1300.0506, and 
XIN20001102.2000.0207) in the corpus are not use for 
serious annotation error. 
236
In Table 8, ?SegTag? represent the mention 
detection system integrated with segmentation 
adaptation, ?Tag? represent the mention detec-
tion system without segmentation adaptation. 
?+F? means with feedback. 
The ACE-value of our Chinese EDT system 
is better than 58.8% of Florian et al (2004). In 
fact, the two systems are not comparable for not 
basing on the same training and test data. How-
ever both corpora are under the same standard 
from ACE project, and our training data (about 
66K) is smaller than Florian et al (2004) (about 
80K). Therefore, it is an encouraging result. 
Segmentation adapting and feedback can im-
prove 7.5% of ACE-value for the whole system. 
As we can see from Table 8, using TBL method 
to adapt standard or correct errors can improve 
the mention detection performance especially 
recall, and word segmentation adapting is essen-
tial for mention detection. Feedback can im-
prove the precision of mention detection with 
loss of recall. The two techniques can signifi-
cantly improve the EDT performance, since the 
p-value of the T-test for the performance of 
?SegTag? to ?Tag? is 96.7%, while for ?Seg-
Tag+F? to ?Tag? is 98.9%. The recall of men-
tion detection is dropped after feedback because 
of the great effect of rule FR2, 3, 4 and 5 as il-
lustrated in table 6. 
8 Conclusion 
In this paper, we integrate the mention detection 
model and entity tracking/coreference model 
into a unified TBL framework. Experimental 
results show segmentation adapting and feed-
back can significantly improve the performance 
of EDT system. And even with very limited 
knowledge and shallow NLP tools, our method 
can reach comparable performance with related 
work. 
References 
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: a case 
study in Part-of-Speech tagging. In: Computa-
tional Lingusitics, 21(4). 
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A 
statistical model for multilingual entity detection 
and tracking. In Proc. of HLT/NAACL-04, pages 
1-8, Boston Massachusetts, USA. 
Jianfeng Gao, Mu Li and Changning Huang. 2003. 
Improved souce-channel model for Chinese word 
segmentation. In Proc. of ACL2003. 
Jianfeng Gao, Andi Wu, Mu Li, Changning Huang, 
Hongqiao Li, Xinsong and Xia, Haowei Qin. 2004. 
Adaptive Chinese word segmentation. In Proc. of 
ACL2004. 
Niyu Ge, John Hale, and Eugene Charniak. 1998. A 
statistical approach to anaphora resolution. In 
Proc. of the Sixth Workshop on Very Large Cor-
pora. 
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven 
J. Maiorano. 2001. Text and knowledge mining 
for coreference resolution. In Proc. of NAACL. 
J. Hobbs. 1976. Pronoun resolution. Technical report, 
Dept. of Computer Science, CUNY, Technical 
Report TR76-1. 
A Ittycheriah, L Lita, N Kambhatla, N Nicolov, S 
Roukos, and M Stys. 2003. Identifying and track-
ing entity mentions in maximum entropy frame-
work. In HLT-NAACL 2003. 
Heng Ji and Ralph Grishman. 2004. Applying 
Coreference to Improve Name Recognition. In 
ACL04 Reference Resolution and its Application Work-
shop. 
Xiaoqiang Luo, A. Ittycheriah, H. Jing, N. Kamb-
hatla, S. Roukos.2004. A Mention-Synchronous 
Coreference Resolution Aogorithm Based on the 
Bell Tree. In Proc. of ACL2004. 
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. In Proc. of the 17th Interna-
tional Conference on Computational Linguistics, 
pages 869-875. 
MUC. 1996. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6). Morgan Kauf-
mann, San Mateo, CA. 
NIST. 2003. The ACE evaluation plan. 
www.nist.gov/speech/tests/ace/index.htm. 
Wee Meng Soon, Hwee Tou Ng, and Chung Yong 
Lim. 2001. A machine learning approach to 
coreference resolution of noun phrases. Computa-
tional Linguistics, 27(4):521-544. 
M. Vilain, J. Burger, J. Aberdeen, D. Connolly and L. 
Hirschman. 1995. A Model-Theoretic coreference 
scoring scheme. In Proc. of MUC-6, page45-52. 
Morgan Kaufmann. 
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew 
Lim Tan. 2003. Coreference resolution using 
competition learning approach. In Proc. of 
ACL2003. 
237
Chinese Word Segmentation and Named
Entity Recognition: A Pragmatic Approach
Jianfeng Gao?
Microsoft Research Asia
Mu Li?
Microsoft Research Asia
Andi Wu+
GrapeCity Inc.
Chang-Ning Huang?
Microsoft Research Asia
This article presents a pragmatic approach to Chinese word segmentation. It differs from most
previous approaches mainly in three respects. First, while theoretical linguists have defined
Chinese words using various linguistic criteria, Chinese words in this study are defined prag-
matically as segmentation units whose definition depends on how they are used and processed
in realistic computer applications. Second, we propose a pragmatic mathematical framework in
which segmenting known words and detecting unknown words of different types (i.e., morpho-
logically derived words, factoids, named entities, and other unlisted words) can be performed
simultaneously in a unified way. These tasks are usually conducted separately in other systems.
Finally, we do not assume the existence of a universal word segmentation standard that is
application-independent. Instead, we argue for the necessity of multiple segmentation standards
due to the pragmatic fact that different natural language processing applications might require
different granularities of Chinese words.
These pragmatic approaches have been implemented in an adaptive Chinese word segmenter,
called MSRSeg, which will be described in detail. It consists of two components: (1) a generic
segmenter that is based on the framework of linear mixture models and provides a unified
approach to the five fundamental features of word-level Chinese language processing: lexicon
word processing, morphological analysis, factoid detection, named entity recognition, and new
word identification; and (2) a set of output adaptors for adapting the output of (1) to different
application-specific standards. Evaluation on five test sets with different standards shows that
the adaptive system achieves state-of-the-art performance on all the test sets.
1. Introduction
This article is intended to address, with a unified and pragmatic approach, two funda-
mental questions in Chinese natural language processing (NLP): What is a ?word? in
Chinese?, and How does a computer identify Chinese words automatically? Our ap-
proach is distinguished from most previous approaches by the following three unique
? Natural Language Computing Group, Microsoft Research Asia, 5F, Sigma Center, No. 49, Zhichun Road,
Beijing, 100080, China. E-mail: jfgao@microsoft.com, muli@microsoft.com, cnhuang@msrchina.research.
microsoft.com.
+ The work reported in this article was done while the author was at Microsoft Research. His current e-mail
address is andi.wu@grapecity.com.
Submission received: 22 November 2004; revised submission received: 20 April 2005; accepted for
publication: 17 June 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
components that are integrated into a single model: a taxonomy of Chinese words, a
unified approach to word breaking and unknown word detection, and a customizable
display of word segmentation.1 We will describe each of these in turn.
Chinese word segmentation is challenging because it is often difficult to define what
constitutes a word in Chinese. Theoretical linguists have tried to define Chinese words
using various linguistic criteria (e.g., Packard 2000). While each of those criteria pro-
vides valuable insights into ?word-hood? in Chinese, they do not consistently lead us to
the same conclusions. Fortunately, this may not be a serious issue in computational lin-
guistics, where the definition of words can vary and can depend to a large degree upon
how one uses and processes these words in computer applications (Sproat and Shih
2002).
In this article, we define the concept of Chinese words from the viewpoint of
computational linguistics. We develop a taxonomy in which Chinese words can be
categorized into one of the following five types: lexicon words, morphologically derived
words, factoids, named entities, and new words.2 These five types of words have
different computational properties and are processed in different ways in our system,
as will be described in detail in Section 3. Two of these five types, factoids and named
entities, are not important to theoretical linguists but are significant in NLP.
Chinese word segmentation involves mainly two research issues: word boundary
disambiguation and unknown word identification. In most of the current systems, these
are considered to be two separate tasks and are dealt with using different components
in a cascaded or consecutive manner.
However, we believe that these two issues are not separate in nature and are
better approached simultaneously. In this article, we present a unified approach to
the five fundamental features of word-level Chinese NLP (corresponding to the five
types of words described earlier): (1) word breaking, (2) morphological analysis, (3)
factoid detection, (4) named entity recognition (NER), and (5) new word identifica-
tion (NWI). This approach is based on a mathematical framework of linear mixture
models in which component models are inspired by the source?channel models of
Chinese sentence generation. There are basically two types of component models: a
source model and a set of channel models. The source model is used to estimate the
generative probability of a word sequence in which each word belongs to one word
type. For each of the word types, a channel model is used to estimate the likelihood
of a character string, given the word type. We shall show that this framework is
flexible enough to incorporate a wide variety of linguistic knowledge and statistical
models in a unified way.
In computer applications, we are more concerned with segmentation units than
words. While words are supposed to be unambiguous and static linguistic entities,
segmentation units are expected to vary from application to application. In fact, dif-
ferent Chinese NLP-enabled applications may have different requirements that request
different granularities of word segmentation. For example, automatic speech recog-
nition (ASR) systems prefer longer ?words? to achieve higher accuracy, whereas in-
1 In this article, we differentiate the terms word breaking and word segmentation. Word breaking refers
to the process of segmenting known words that are predefined in a lexicon. Word segmentation refers
to the process of both lexicon word segmentation and unknown word detection.
2 New words in this article refer to out-of-vocabulary words that are neither recognized as named entities
or factoids nor derived by morphological rules. These words are mostly domain-specific and/or
time-sensitive (see Section 5.5 for details).
532
Gao et al Chinese Word Segmentation: A Pragmatic Approach
formation retrieval (IR) systems prefer shorter ?words? to obtain higher recall rates
(Wu 2003).
Therefore, we do not assume that an application-independent universal word seg-
mentation standard exists. We argue instead for the existence of multiple segmenta-
tion standards, each for a specific application. It is undesirable to develop a set of
application-specific segmenters. A better solution would be to develop a generic seg-
menter with customizable output that is able to provide alternative segmentation units
according to the specification that is either predefined or implied in the application data.
To achieve this, we present a transformation-based learning (TBL; Brill 1995) method,
to be described in Section 6.
We implement the pragmatic approach to Chinese word segmentation in an adapt-
ive Chinese word segmenter called MSRSeg. It consists of two components: (1) a
generic segmenter that is based on the linear mixture model framework of word
breaking and unknown word detection and that can adapt to domain-specific vocab-
ularies, and (2) a set of output adaptors for adapting the output of (1) to different
application-specific standards. Evaluation on five test sets with different standards
shows that the adaptive system achieves state-of-the-art performance on all the test
sets. It thus demonstrates the possibility of a single adaptive Chinese word segmenter
that is capable of supporting multiple applications.
The remainder of this article is organized as follows. Section 2 presents previ-
ous work in this field. Section 3 introduces the taxonomy of Chinese words and de-
scribes the corpora we used in our study. Section 4 presents some of the theoretical
background on which our unified approach is based. Section 5 outlines the general
architecture of the Chinese word segmenter, MSRSeg, and describes each of the com-
ponents in detail, presenting a separate evaluation of each component where appro-
priate. Section 6 presents the TBL method of standards adaptation. While in Section 5
we presume the existence of an annotated training corpus, we focus in Section 7 on
the methods of creating training data in a (semi-)automatic manner, with minimal or
no human annotation. We thus demonstrate the possibilities of unsupervised learning
of Chinese words. Section 8 presents several evaluations of the system on the different
corpora, each corresponding to a different segmentation standard, in comparison with
other state-of-the-art systems. Finally, we conclude the article in Section 9.
2. Previous Work
2.1 Approaches to Word Segmentation
Many methods of Chinese word segmentation have been proposed: reviews include Wu
and Tseng (1993); Sproat and Shih (2002); and Sun and Tsou (2001). These methods can
be roughly classified as either dictionary-based or statistically-based methods, while
many state-of-the-art systems use hybrid approaches.
In dictionary-based methods, given an input character string, only words that are
stored in the dictionary can be identified. One of the most popular methods is maxi-
mum matching (MM), usually augmented with heuristics to deal with ambiguities in
segmentation. Studies that use this method or minor variants include Chen et al (1999)
and Nie, Jin, and Hannan (1994). The performance of these methods thus depends to
a large degree upon the coverage of the dictionary, which unfortunately may never be
complete because new words appear constantly. Therefore, in addition to the dictio-
nary, many systems also contain special components for unknown word identification.
533
Computational Linguistics Volume 31, Number 4
In particular, statistical methods have been widely applied because they use a proba-
bilistic or cost-based scoring mechanism rather than a dictionary to segment the text.
These methods have three drawbacks. First, some of these methods (e.g., Lin et al 1993;
Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their
types. For instance, one might identify a string as a unit but fail to identify that it is
a person name. Second, many current statistical methods do not incorporate linguistic
knowledge effectively into segmentation. For example, Teahan et al (2000) and Dai et al
(1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to
be linguistically implausible, and consequently, additional manual checking is needed
for some subsequent tasks such as parsing. Third, in many current segmenters, OOV
identification is considered a separate process from segmentation (e.g., Chen 2003; Wu
and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words
are usually two or more characters long and are often segmented into single characters.
He then uses different components to detect OOV words of different types in a cascaded
manner after the basic word segmentation.
We believe that the identification of OOV words should not be treated as a problem
separate from word segmentation. We propose a unified approach that solves both
problems simultaneously. A previous work along this line is Sproat et al (1996), which is
based on weighted finite-state transducers (FSTs). Our approach is similarly motivated
but is based on a different mechanism: linear mixture models. As we shall see, the
models provide a more flexible framework for incorporating various kinds of lexical
and statistical information. Many types of OOV words that are not covered in Sproat?s
system can be dealt with in our system. The linear models we used are originally
derived from linear discriminant functions widely used for pattern classification (Duda,
Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and
Duffy (2001). Other frameworks of Chinese word segmentation, which are similar to the
linear models, include maximum entropy models (Xue 2003) and conditional random
fields (Peng, Feng, and McCallum 2004). They also use a unified approach to word
breaking and OOV identification.
2.2 More on New Word Identification
In this article, we use the term ?new words? to refer to OOV words other than named en-
tities, factoids, and morphologically derived words. ?New words? are mostly domain-
specific terms (e.g., ? ?cellular?) and time-sensitive political, social, or cultural
terms (e.g., 	PENS: A Machine-aided English Writing System
for Chinese Users
Ting Liu1 Ming Zhou Jianfeng Gao Endong Xun Changning Huang
Natural Language Computing Group, Microsoft Research China, Microsoft Corporation
5F, Beijing Sigma Center
100080 Beijing, P.R.C.
{ i-liutin, mingzhou, jfgao, i-edxun, cnhuang@microsoft.com }
Abstract
Writing English is a big barrier for most
Chinese users. To build a computer-aided system
that helps Chinese users not only on spelling
checking and grammar checking but also on
writing in the way of native-English is a
challenging task. Although machine translation is
widely used for this purpose, how to find an
efficient way in which human collaborates with
computers remains an open issue. In this paper,
based on the comprehensive study of Chinese
users requirements, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also developed a pilot system, namely PENS
(Perfect ENglish System). Preliminary
experiments show very promising results.
Introduction
With the rapid development of the Internet,
writing English becomes daily work for
computer users all over the world. However, for
Chinese users who have significantly different
culture and writing style, English writing is a big
barrier. Therefore, building a machine-aided
English writing system, which helps Chinese
users not only on spelling checking and grammar
checking but also on writing in the way of
native-English, is a very promising task.
Statistics shows that almost all Chinese
users who need to write in English1 have enough
knowledge of English that they can easily tell the
difference between two sentences written in
Chinese-English and native-English, respectively.
Thus, the machine-aided English writing system
should act as a consultant that provide various
kinds of help whenever necessary, and let users
play the major role during writing. These helps
include:
1) Spelling help: help users input hard-to-spell
words, and check the usage in a certain
context simultaneously;
2) Example sentence help: help users refine the
writing by providing perfect example
sentences.
Several machine-aided approaches have
been proposed recently. They basically fall into
two categories, 1) automatic translation, and 2)
translation memory. Both work at the sentence
level. While in the former, the translation is not
readable even after a lot of manually editing. The
latter works like a case-based system, in that,
given a sentence, the system retrieve similar
sentences from translation example database, the
user then translates his sentences by analogy. To
build a computer-aided English writing system
that helps Chinese users on writing in the way of
native-English is a challenging task. Machine
translation is widely used for this purpose, but
how to find an efficient way in which human
collaborates well with computers remains an
open issue. Although the quality of fully
automatic machine translation at the sentence
level is by no means satisfied, it is hopeful to
1 Now Ting Liu is an associate professor in Harbin
Institute of Technology, P.R.C.
provide relatively acceptable quality translations
at the word or short phrase level. Therefore, we
can expect that combining word/phrase level
automatic translation with translation memory
will achieve a better solution to machine-aided
English writing system [Zhou, 95].
In this paper, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also develop a pilot system, namely PENS.
Preliminary experiments show very promising
results.
The rest of this paper is structured as follows.
In section 2 we give an overview of the system,
introduce the components of the system, and
describe the resources needed. In section 3, we
discuss the word spelling help, and focus the
discussion on Chinese pinyin to English word
translation. In addition, we describe various
kinds of word level help functions, such as
automatic translation of Chinese word in the form
of either pinyin or Chinese characters, and
synonym suggestion, etc. We also describe the
user interface briefly. In section 4, an effective
retrieval algorithm is proposed to implement the
so-called intelligent recommendation function. In
section 5, we present preliminary experimental
results. Finally, concluding remarks is given in
section 6.
1 System Overview
1.1 System Architecture
Figure 1 System Architecture
There are two modules in PENS. The first is
called the spelling help. Given an English word,
the spelling help performs two functions, 1)
retrieving its synonym, antonym, and thesaurus;
or 2) automatically giving the corresponding
translation of Chinese words in the form of
Chinese characters or pinyin. Statistical machine
translation techniques are used for this translation,
and therefore a Chinese-English bilingual
dictionary (MRD), an English language model,
and an English-Chinese word- translation model
(TM) are needed. The English language model is
a word trigram model, which consists of
247,238,396 trigrams, and the vocabulary used
contains 58541 words. The MRD dictionary
contains 115,200 Chinese entries as well as their
corresponding English translations, and other
information, such as part-of-speech, semantic
classification, etc. The TM is trained from a
word-aligned bilingual corpus, which occupies
approximately 96,362 bilingual sentence pairs.
The second module is an intelligent
recommendation system. It employs an effective
sentence retrieval algorithm on a large bilingual
corpus. The input is a sequence of keywords or a
short phrase given by users, and the output is
limited pairs bilingual sentences expressing
relevant meaning with users? query, or just a few
pairs of bilingual sentences with syntactical
relevance.
1.2 Bilingual Corpus Construction
We have collected bilingual texts extracted
from World Wide Web bilingual sites,
dictionaries, books, bilingual news and
magazines, and product manuals. The size of the
corpus is 96,362 sentence pairs. The corpus is
used in the following three cases:
1) Act as translation memory to support the
Intelligent Recommendation Function;
2) To be used to acquire English-Chinese
translation model to support translation at
word and phrase level;
3) To be used to extract bilingual terms to enrich
the Chinese-English MRD;
To construct a sentence aligned bilingual
corpus, we first use an alignment algorithm doing
the automatic alignment and then the alignment
result are corrected.
There have been quite a number of recent
papers on parallel text alignment. Lexically based
techniques use extensive online bilingual
lexicons to match sentences [Chen 93]. In
contrast, statistical techniques require almost no
prior knowledge and are based solely on the
lengths of sentences, i.e. length-based alignment
method. We use a novel method to incorporate
both approaches [Liu, 95]. First, the rough result
is obtained by using the length-based method.
Then anchors are identified in the text to reduce
the complexity. An anchor is defined as a block
that consists of n successive sentences. Our
experiments show best performance when n=3.
Finally, a small, restricted set of lexical cues is
applied to obtain for further improvement.
1.3 Translation Model Training
Chinese sentences must be segmented
before word translation training, because written
Chinese consists of a character stream without
space between words. Therefore, we use a
wordlist, which consists of 65502 words, in
conjunction with an optimization procedure
described in [Gao, 2000]. The bilingual training
process employs a variant of the model in [Brown,
1993] and as such is based on an iterative EM
(expectation-maximization) procedure for
maximizing the likelihood of generating the
English given the Chinese portion. The output of
the training process is a set of potential English
translations for each Chinese word, together with
the probability estimate for each translation.
1.4 Extraction of Bilingual
Domain-specific Terms
A domain-specific term is defined as a string
that consists of more than one successive word
and has certain occurrences in a text collection
within a specific domain. Such a string has a
complete meaning and lexical boundaries in
semantics; it might be a compound word, phrase
or linguistic template. We use two steps to extract
bilingual terms from sentence aligned corpus.
First we extract Chinese monolingual terms from
Chinese part of the corpus by a similar method
described in [Chien, 1998], then we extract the
English corresponding part by using the word
alignment information. A candidate list of the
Chinese-English bilingual terms can be obtained
as the result. Then we will check the list and add
the terms into the dictionary.
2 Spelling Help
The spelling help works on the word or
phrase level. Given an English word or phrase, it
performs two functions, 1) retrieving
corresponding synonyms, antonyms, and
thesaurus; and 2) automatically giving the
corresponding translation of Chinese words in
the form of Chinese characters or pinyin. We will
focus our discussion on the latter function in the
section.
To use the latter function, the user may input
Chinese characters or just input pinyin. It is not
very convenient for Chinese users to input
Chinese characters by an English keyboard.
Furthermore the user must switch between
English input model and Chinese input model
time and again. These operations will interrupt
his train of thought. To avoid this shortcoming,
our system allows the user to input pinyin instead
of Chinese characters. The pinyin can be
translated into English word directly.
Let us take a user scenario for an example to
show how the spelling help works. Suppose that a
user input a Chinese word ?? in the form of
pinyin, say ?wancheng?, as shown in figure1-1.
PENS is able to detect whether a string is a
pinyin string or an English string automatically.
For a pinyin string, PENS tries to translate it into
the corresponding English word or phrase
directly. The mapping from pinyin to Chinese
word is one-to-many, so does the mapping from
Chinese word to English words. Therefore, for
each pinyin string, there are alternative
translations. PENS employs a statistical approach
to determine the correct translation. PENS also
displays the corresponding Chinese word or
phrase for confirmation, as shown in figure 1-2.
Figure 1-1
Figure 1-2
If the user is not satisfied with the English
word determined by PENS, he can browse other
candidates as well as their bilingual example
sentences, and select a better one, as shown in
figure 1-3.
Figure 1-3
2.1 Word Translation Algorithm
based on Statistical LM and TM
Suppose that a user input two English words,
say EW1 and EW2, and then a pinyin string, say
PY. For PY, all candidate Chinese words are
determined by looking up a Pinyin-Chinese
dictionary. Then, a list of candidate English
translations is obtained according to a MRD.
These English translations are English words of
their original form, while they should be of
different forms in different contexts. We exploit
morphology for this purpose, and expand each
word to all possible forms. For instance,
inflections of ?go? may be ?went?, and ?gone?.
In what follows, we will describe how to
determine the proper translation among the
candidate list.
Figure 2-1: Word-level Pinyin-English
Translation
As shown in Figure 2-1, we assume that the
most proper translation of PY is the English word
with the highest conditional probability among
all leaf nodes, that is
According to Bayes? law, the conditional
probability is estimated by
),|(
),|(),,|(
),,|(
21
2121
21
EWEWPYP
EWEWEWPEWEWEWPYP
EWEWPYEWP
ijij
ij
?
= (2-1)
Since the denominator is independent of EWij, we
rewrite (2-1) as
),|(),,|(
),,|(
2121
21
EWEWEWPEWEWEWPYP
EWEWPYEWP
ijij
ij
?
? (2-2)
Since CWi is a bridge which connect the pinyin
and the English translation, we introduce Chinese
word CWi into
We get
),,,|(
),,,|(),,|(
),,|(
21
2121
21
EWEWEWPYCWP
EWEWEWCWPYPEWEWEWCWP
EWEWEWPYP
iji
ijiiji
ij
?
=
(2-3)
For simplicity, we assume that a Chinese word
doesn?t depends on the translation context, so we
can get the following approximate equation:
)|(),,|( 21 ijiiji EWCWPEWEWEWCWP ?
We can also assume that the pinyin of a Chinese
word is not concerned in the corresponding
English translation, namely:
)|(),,,|( 21 iiji CWPYPEWEWEWCWPYP ?
It is almost impossible that two Chinese words
correspond to the same pinyin and the same
English translation, so we can suppose that:
1),,,|( 21 ?EWEWEWPYCWP iji
Therefore, we get the approximation of (2-3) as
follows:
)|()|(
),,|( 21
iiji
ij
CWPYPEWCWP
EWEWEWPYP
?
= (2-4)
According to formula (2-2) and (2-4), we get:
),|()|()|(
),,|(
21
21
EWEWEWPCWPYPEWCWP
EWEWPYEWP
ijiiji
ij
??
= (2-5)
where P(CWi |EWij) is the translation model, and
can be got from bilingual corpus, and P(PY | CWi)
),,|( 21 EWEWEWPYP ij
is the polyphone model, here we suppose
P(PY|CWi) = 1, and P(EWij | EW1, EW2) is the
English trigram language model.
To sum up, as indicated in (2-6), the spelling help
find the most proper translation of PY by
retrieving the English word with the highest
conditional probability.
),|()|(maxarg
),,|(maxarg
21
21
EWEWEWPEWCWP
EWEWPYEWP
ijiji
EW
EW
ij
ij
?
=
(2-6)
3 Intelligent Recommendation
The intelligent recommendation works on
the sentence level. When a user input a sequence
of Chinese characters, the character string will be
firstly segmented into one or more words. The
segmented word string acts as the user query in
IR. After query expansion, the intelligent
recommendation employs an effective sentence
retrieval algorithm on a large bilingual corpus,
and retrieves a pair (or a set of pairs) of bilingual
sentences related to the query. All the retrieved
sentence pairs are ranked based on a scoring
strategy.
3.1 Query Expansion
Suppose that a user query is of the form CW1,
CW2, ? , CWm. We then list all synonyms for
each word of the queries based on a Chinese
thesaurus, as shown below.
mmnnn
m
m
CWCWCW
CWCWCW
CWCWCW
???
????????????
???
???
21 21
22212
12111
We can obtain an expanded query by
substituting a word in the query with its synonym.
To avoid over-generation, we restrict that only
one word is substituted at each time.
Let us take the query ?? for an example.
The synonyms list is as follows:
 =	??
 =
??.
The query consists of two words. By substituting
the first word, we get expanded queries, such as
??????, etc, and by
substituting the second word, we get other
expanded queries, such as ? 
??
???, etc.
Then we select the expanded query, which is
used for retrieving example sentence pairs, by
estimating the mutual information of words with
the query. It is indicated as follows
?
?
=
m
ik
k
ijk
ji
CWCWMI
1,
),(maxarg
where CWk is a the kth Chinese word in the query,
and CWij is the jth synonym of the i-th Chinese
word. In the above example, ? ? is
selected. The selection well meets the common
sense. Therefore, bilingual example sentences
containing ?? will be retrieved as well.
3.2 Ranking Algorithm
The input of the ranking algorithm is a
query Q, as described above, Q is a Chinese
word string, as shown below
Q= T1,T2,T3,?Tk
The output is a set of relevant bilingual
example sentence pairs in the form of,
S={(Chinsent, Engsent) | Relevance(Q,Chinsent)
>Relevance(Q,Engsent) >
where Chinsent is a Chinese sentence, and
Engsent is an English sentence, and 

For each sentence, the relevance score is
computed in two parts, 1) the bonus which
represents the similarity of input query and the
target sentence, and 2) the penalty, which
represents the dissimilarity of input query and the
target sentence.
The bonus is computed by the following formula:
WhereDistribution-Based Pruning of Backoff Language Models
Jianfeng Gao
Microsoft Research China
No. 49 Zhichun Road Haidian District
100080, China,
jfgao@microsoft.com
Kai-Fu Lee
Microsoft Research China
No. 49 Zhichun Road Haidian District
100080, China,
kfl@microsoft.com
Abstract
We propose a distribution-based pruning of
n-gram backoff language models. Instead
of the conventional approach of pruning
n-grams that are infrequent in training data,
we prune n-grams that are likely to be
infrequent in a new document. Our method
is based on the n-gram distribution i.e. the
probability that an n-gram occurs in a new
document. Experimental results show that
our method performed 7-9% (word
perplexity reduction) better than
conventional cutoff methods.
1 Introduction
Statistical language modelling (SLM) has been
successfully applied to many domains such as
speech recognition (Jelinek, 1990), information
retrieval (Miller et al, 1999), and spoken
language understanding (Zue, 1995). In
particular, n-gram language model (LM) has
been demonstrated to be highly effective for
these domains. N-gram LM estimates the
probability of a word given previous words,
P(wn|w1,?,wn-1).
In applying an SLM, it is usually the case
that more training data will improve a language
model. However, as training data size
increases, LM size increases, which can lead to
models that are too large for practical use.
To deal with the problem, count cutoff
(Jelinek, 1990) is widely used to prune
language models. The cutoff method deletes
from the LM those n-grams that occur
infrequently in the training data. The cutoff
method assumes that if an n-gram is infrequent
in training data, it is also infrequent in testing
data. But in the real world, training data rarely
matches testing data perfectly. Therefore, the
count cutoff method is not perfect.
In this paper, we propose a
distribution-based cutoff method. This
approach estimates if an n-gram is ?likely to be
infrequent in testing data?. To determine this
likelihood, we divide the training data into
partitions, and use a cross-validation-like
approach. Experiments show that this method
performed 7-9% (word perplexity reduction)
better than conventional cutoff methods.
In section 2, we discuss prior SLM research,
including backoff bigram LM, perplexity, and
related works on LM pruning methods. In
section 3, we propose a new criterion for LM
pruning based on n-gram distribution, and
discuss in detail how to estimate the
distribution. In section 4, we compare our
method with count cutoff, and present
experimental results in perplexity. Finally, we
present our conclusions in section 5.
2 Backoff Bigram and Cutoff
One of the most successful forms of SLM is the
n-gram LM. N-gram LM estimates the
probability of a word given the n-1 previous
words, P(wn|w1,?,wn-1). In practice, n is usually
set to 2 (bigram), or 3 (trigram). For simplicity,
we restrict our discussion to bigram, P(wn|wn-1),
which assumes that the probability of a word
depends only on the identity of the immediately
preceding word. But our approach extends to
any n-gram.
Perplexity is the most common metric for
evaluating a bigram LM. It is defined as,
?
=
=
?
?
N
i
ii wwPNPP 1
1 )|(log1
2 (1)
where N is the length of the testing data. The
perplexity can be roughly interpreted as the
geometric mean of the branching factor of the
document when presented to the language
model. Clearly, lower perplexities are better.
One of the key issues in language modelling
is the problem of data sparseness. To deal with
the problem, (Katz, 1987) proposed a backoff
scheme, which is widely used in bigram
language modelling. Backoff scheme estimates
the probability of an unseen bigram by utilizing
unigram estimates. It is of the form:
??
? >
=
?
??
?
otherwisewPw
wwcwwP
wwP
ii
iiiid
ii )()(
0),()|()|(
1
11
1 ?
(2)
where c(wi-1,wi) is the frequency of word pair
(wi-1,wi) in training data, Pd represents the
Good-Turing discounted estimate for seen
word pairs, and ?(wi-1) is a normalization
factor.
Due to the memory limitation in realistic
applications, only a finite set of word pairs have
conditional probabilities P(wn|wn-1) explicitly
represented in the model, especially when the
model is trained on a large corpus. The
remaining word pairs are assigned a probability
by back-off (i.e. unigram estimates). The goal
of bigram pruning is to remove uncommon
explicit bigram estimates P(wn|wn-1) from the
model to reduce the number of parameters,
while minimizing the performance loss.
The most common way to eliminate unused
count is by means of count cutoffs (Jelinek,
1990). A cutoff is chosen, say 2, and all
probabilities stored in the model with 2 or
fewer counts are removed. This method
assumes that there is not much difference
between a bigram occurring once, twice, or not
at all. Just by excluding those bigrams with a
small count from a model, a significant saving
in memory can be achieved. In a typical
training corpus, roughly 65% of unique bigram
sequences occur only once.
Recently, several improvements over count
cutoffs have been proposed. (Seymore and
Rosenfeld, 1996) proposed a different pruning
scheme for backoff models, where bigrams are
ranked by a weighted difference of the log
probability estimate before and after pruning.
Bigrams with difference less than a threshold
are pruned.
(Stolcke, 1998) proposed a criterion for
pruning based on the relative entropy between
the original and the pruned model. The relative
entropy measure can be expressed as a relative
change in training data perplexity. All bigrams
that change perplexity by less than a threshold
are removed from the model. Stolcke also
concluded that, for practical purpose, the
method in (Seymore and Rosenfeld, 1996) is a
very good approximation to this method.
All previous cutoff methods described
above use a similar criterion for pruning, that is,
the difference (or information loss) between the
original estimate and the backoff estimate.
After ranking, all bigrams with difference small
enough will be pruned, since they contain no
more information.
3 Distribution-Based Cutoff
As described in the previous section, previous
cutoff methods assume that training data covers
testing data. Bigrams that are infrequent in
training data are also assumed to be infrequent
in testing data, and will be cutoff. But in the
real world, no matter how large the training
data, it is still always very sparse compared to
all data in the world. Furthermore, training data
will be biased by its mixture of domain, time, or
style, etc. For example, if we use newspaper in
training, a name like ?Lewinsky? may have
high frequency in certain years but not others; if
we use Gone with the Wind in training,
?Scarlett O?Hara? will have disproportionately
high probability and will not be cutoff.
We propose another approach to pruning.
We aim to keep bigrams that are more likely to
occur in a new document. We therefore propose
a new criterion for pruning parameters from
bigram models, based on the bigram
distribution i.e. the probability that a bigram
will occur in a new document. All bigrams with
the probability less than a threshold are
removed.
We estimate the probability that a bigram
occurs in a new document by dividing training
data into partitions, called subunits, and use a
cross-validation-like approach. In the
remaining part of this section, we firstly
investigate several methods for term
distribution modelling, and extend them to
bigram distribution modelling. Then we
investigate the effects of the definition of the
subunit, and experiment with various ways to
divide a training set into subunits. Experiments
show that this not only allows a much more
efficient computation for bigram distribution
modelling, but also results in a more general
bigram model, in spite of the domain, style, or
temporal bias of training data.
3.1 Measure of Generality
Probability
In this section, we will discuss in detail how to
estimate the probability that a bigram occurs in
a new document. For simplicity, we define a
document as the subunit of the training corpus.
In the next section, we will loosen this
constraint.
Term distribution models estimate the
probability Pi(k), the proportion of times that of
a word wi appears k times in a document. In
bigram distribution models, we wish to model
the probability that a word pair (wi-1 ,wi) occurs
in a new document. The probability can be
expressed as the measure of the generality of a
bigram. Thus, in what follows, it is denoted by
Pgen(wi-1,wi). The higher the Pgen(wi-1,wi) is, for
one particular document, the less informative
the bigram is, but for all documents, the more
general the bigram is.
We now consider several methods for term
distribution modelling, which are widely used
in Information Retrieval, and extend them to
bigram distribution modelling. These methods
include models based on the Poisson
distribution (Mood et al, 1974), inverse
document frequency (Salton and Michael,
1983), and Katz?s K mixture (Katz, 1996).
3.1.1 The Poisson Distribution
The standard probabilistic model for the
distribution of a certain type of event over units
of a fixed size (such as periods of time or
volumes of liquid) is the Poisson distribution,
which is defined as follows:
!
);()(
k
ekPkP
k
i
ii
i
?? ??== (3)
In the most common model of the Poisson
distribution in IR, the parameter ?i>0 is the
average number of occurrences of wi per
document, that is
N
cfi
i =? , where cfi is the
number of documents containing wi, and N is
the total number of documents in the collection.
In our case, the event we are interested in is the
occurrence of a particular word pair (wi-1,wi)
and the fixed unit is the document. We can use
the Poisson distribution to estimate an answer
to the question: what is the probability that a
word pair occurs in a document. Therefore, we
get
ieiPwwP iigen
?? ?
?
?=?= 1);0(1),( 1 (4)
It turns out that using Poisson distribution, we
have Pgen(wi-1,wi) ? c(wi-1,wi). This means that
this criterion is equivalent to count cutoff.
3.1.2 Inverse Document Frequency (IDF)
IDF is a widely used measure of specificity
(Salton and Michael, 1983). It is the reverse of
generality. Therefore we can also derive
generality from IDF. IDF is defined as follows:
)log(
i
i df
NIDF = (5)
where, in the case of bigram distribution, N is
the total number of documents, and dfi is the
number of documents that the contain word
pair (wi-1,wi). The formula
idf
N
=log gives full
weight to a word pair (wi-1,wi) that occurred in
one document. Therefore, let?s assume,
i
ii
iigen IDF
wwC
wwP
)(),( ,11 ?? ? (6)
It turns out that based on IDF, our criterion is
equivalent to the count cutoff weighted by the
reverse of IDF. Unfortunately, experiments
show that using (6) directly does not get any
improvement. In fact, it is even worse than
count cutoff methods. Therefore, we use the
following form instead,
?
i
ii
iigen IDF
wwC
wwP
)(),( ,11 ?? ? (7)
where ? is a weighting factor tuned to
maximize the performance.
3.1.3 K Mixture
As stated in (Manning and Sch?tze, 1999), the
Poisson estimates are good for non-content
words, but not for content words. Several
improvements over Poisson have been
proposed. These include two-Poisson Model
(Harter, 1975) and Katz?s K mixture model
(Katz, 1996). The K mixture is the better. It is
also a simpler distribution that fits empirical
distributions of content words as well as
non-content words. Therefore, we try to use K
mixture for bigram distribution modelling.
According to (Katz, 1996), K mixture model
estimates the probability that word wi appears k
times in a document as follows:
k
ki kP )1(1)1()( 0, +++?= ?
?
?
??? (8)
where ?k,0=1 iff k=0 and ?k,0=0 otherwise. ?
and ? are parameters that can be fit using the
observed mean ? and the observed inverse
document frequency IDF as follow:
N
cf
=? (9)
df
NIDF log= (10)
df
dfcfIDF ?
=??= 12?? (11)
?
?
? = (12)
where again, cf is the total number of
occurrence of word wi in the collection, df is the
number of documents in the collection that wi
occurs in, and N is the total number of
documents.
The bigram distribution model is a variation
of the above K mixture model, where we
estimate the probability that a word pair
(wi-1,wi) , occurs in a document by:
?
=
?
?=
K
k
iiigen kPwwP
1
1 )(1),( (13)
where K is dependent on the size of the subunit,
the larger the subunit, the larger the value (in
our experiments, we set K from 1 to 3), and
Pi(k) is the probability of word pair (wi-1,wi)
occurs k times in a document. Pi(k) is estimated
by equation (8), where ? , and ? are estimated
by equations (9) to (12). Accordingly, cf is the
total number of occurrence of a word pair
(wi-1,wi) in the collection, df is the number of
documents that contain (wi-1,wi), and N is the
total number of documents.
3.1.4 Comparison
Our experiments show that K mixture is the
best among the three in most cases. Some
partial experimental results are shown in table
1. Therefore, in section 4, all experimental
results are based on K mixture method.
Word PerplexitySize of Bigram
(Number of
Bigrams)
Poisson IDF K
Mixture
2000000 693.29 682.13 633.23
5000000 631.64 628.84 603.70
10000000 598.42 598.45 589.34
Table 1: Word perplexity comparison of
different bigram distribution models.
3.2 Algorithm
The bigram distribution model suggests a
simple thresholding algorithm for bigram
backoff model pruning:
1. Select a threshold ?.
2. Compute the probability that each bigram
occurs in a document individually by
equation (13).
3. Remove all bigrams whose probability to
occur in a document is less than ?, and
recomputed backoff weights.
4 Experiments
In this section, we report the experimental
results on bigram pruning based on distribution
versus count cutoff pruning method.
In conventional approaches, a document is
defined as the subunit of training data for term
distribution estimating. But for a very large
training corpus that consists of millions of
documents, the estimation for the bigram
distribution is very time-consuming. To cope
with this problem, we use a cluster of
documents as the subunit. As the number of
clusters can be controlled, we can define an
efficient computation method, and optimise the
clustering algorithm.
In what follows, we will report the
experimental results with document and cluster
being defined as the subunit, respectively. In
our experiments, documents are clustered in
three ways: by similar domain, style, or time.
In all experiments described below, we use an
open testing data consisting of 15 million
characters that have been proofread and
balanced among domain, style and time.
Training data are obtained from newspaper
(People?s Daily) and novels.
4.1 Using Documents as Subunits
Figure 1 shows the results when we define a
document as the subunit. We used
approximately 450 million characters of
People?s Daily training data (1996), which
consists of 39708 documents.
0
1
2
3
4
5
6
7
8
9
10
550 600 650 700 750 800
M
illi
on
s
Word Perplexity
Si
ze
(N
u
m
be
ro
fB
ig
ra
m
s)
Count Cutoff
Distribution Cutoff
Figure 1: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a document as the subunit.
4.2 Using Clusters by Domain as
Subunits
Figure 2 shows the results when we define a
domain cluster as the subunit. We also used
approximately 450 million characters of
People?s Daily training data (1996). To cluster
the documents, we used an SVM classifier
developed by Platt (Platt, 1998) to cluster
documents of similar domains together
automatically, and obtain a domain hierarchy
incrementally. We also added a constraint to
balance the size of each cluster, and finally we
obtained 105 clusters. It turns out that using
domain clusters as subunits performs almost as
well as the case of documents as subunits.
Furthermore, we found that by using the
pruning criterion based on bigram distribution,
a lot of domain-specific bigrams are pruned. It
then results in a relatively domain-independent
language model. Therefore, we call this
pruning method domain subtraction based
pruning.
0
1
2
3
4
5
6
7
8
9
10
550 600 650 700 750 800
M
ill
io
ns
Word Perplexity
Si
ze
(N
u
m
be
ro
fB
ig
ra
m
s)
Count Cutoff
Distribution Cutoff
Figure 2: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a domain cluster as the subunit.
4.3 Using Clusters by Style as
Subunits
Figure 3 shows the results when we define a
style cluster as the subunit. For this experiment,
we used 220 novels written by different writers,
each approximately 500 kilonbytes in size, and
defined each novel as a style cluster. Just like in
domain clustering, we found that by using the
pruning criterion based on bigram distribution,
a lot of style-specific bigrams are pruned. It
then results in a relatively style-independent
language model. Therefore, we call this
pruning method style subtraction based
pruning.
01
2
3
4
5
6
7
8
9
10
500 520 540 560 580 600 620 640 660 680 700
M
ill
io
ns
Word Perplexity
Si
ze
(N
u
m
be
ro
fB
ig
ra
m
s)
Count Cutoff
Distribution Cutoff
Figure 3: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a style cluster as the subunit.
4.4 Using Clusters by Time as
Subunits
In practice, it is relatively easier to collect large
training text from newspaper. For example,
many Chinese SLMs are trained from
newspaper, which has high quality and
consistent in style. But the disadvantage is the
temporal term phenomenon. In other words,
some bigrams are used frequently during one
time period, and then never used again.
Figure 4 shows the results when we define a
temporal cluster as the subunit. In this
experiment, we used approximately 9,200
million characters of People?s Daily training
data (1978--1997). We simply clustered the
document published in the same month of the
same year as a cluster. Therefore, we obtained
240 clusters in total. Similarly, we found that
by using the pruning criterion based on bigram
distribution, a lot of time-specific bigrams are
pruned. It then results in a relatively
time-independent language model. Therefore,
we call this pruning method temporal
subtraction based pruning.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1200 1300 1400 1500 1600 1700 1800 1900 2000
M
ill
io
n
s
Word Perplexity
Si
ze
(N
um
be
ro
fB
ig
ra
m
s)
Count Cutoff
Distribution Cutoff
Figure 4: Word perplexity comparison of cutoff
pruning and distribution based bigram pruning
using a temporal cluster as the subunit.
4.3 Summary
In our research lab, we are particularly
interested in the problem of pinyin to Chinese
character conversion, which has a memory
limitation of 2MB for programs. At 2MB
memory, our method leads to 7-9% word
perplexity reduction, as displayed in table 2.
Subunit Word Perplexity
Reduction
Document 9.3%
Document Cluster by
Domain
7.8%
Document Cluster by
Style
7.1%
Document Cluster by
Time
7.3%
Table 2: Word perplexity reduction for bigram
of size 2M.
As shown in figure 1-4, although as the size of
language model is decreased, the perplexity
rises sharply, the models created with the
bigram distribution based pruning have
consistently lower perplexity values than for
the count cutoff method. Furthermore, when
modelling bigram distribution on document
clusters, our pruning method results in a more
general n-gram backoff model, which resists to
domain, style or temporal bias of training data.
5 Conclusions
In this paper, we proposed a novel approach for
n-gram backoff models pruning: keep n-grams
that are more likely to occur in a new
document. We then developed a criterion for
pruning parameters from n-gram models, based
on the n-gram distribution i.e. the probability
that an n-gram occurs in a document. All
n-grams with the probability less than a
threshold are removed. Experimental results
show that the distribution-based pruning
method performed 7-9% (word perplexity
reduction) better than conventional cutoff
methods. Furthermore, when modelling n-gram
distribution on document clusters created
according to domain, style, or time, the pruning
method results in a more general n-gram
backoff model, in spite of the domain, style or
temporal bias of training data.
Acknowledgements
We would like to thank Mingjjing Li, Zheng
Chen, Ming Zhou, Chang-Ning Huang and
other colleagues from Microsoft Research,
Jian-Yun Nie from the University of Montreal,
Canada, Charles Ling from the University of
Western Ontario, Canada, and Lee-Feng Chien
from Academia Sinica, Taiwan, for their help
in developing the ideas and implementation in
this paper. We would also like to thank Jian
Zhu for her help in our experiments.
References
F. Jelinek, ?Self-organized language modeling for
speech recognition?, in Readings in Speech
Recognition, A. Waibel and K.F. Lee, eds.,
Morgan-Kaufmann, San Mateo, CA, 1990, pp.
450-506.
D. Miller, T. Leek, R. M. Schwartz, ?A hidden
Markov model information retrieval system?, in
Proc. 22nd International Conference on Research
and Development in Information Retrieval,
Berkeley, CA, 1999, pp. 214-221.
V.W. Zue, ?Navigating the information
superhighway using spoken language interfaces?,
IEEE Expert, S. M. Katz, ?Estimation of
probabilities from sparse data for the language
model component of a speech recognizer?, IEEE
Transactions on Acoustic, Speech and Signal
Processing, ASSP-35(3): 400-401, March, 1987.
K. Seymore, R. Rosenfeld, ?Scalable backoff
language models?, in Porc. International
Conference on Speech and Language Processing,
Vol1. Philadelphia,PA,1996, pp.232-235
A. Stolcke, ?Entropy-based Pruning of Backoff
Language Models? in Proc. DRAPA News
Transcriptionand Understanding Workshop,
Lansdowne, VA. 1998. pp.270-274
M. Mood, A. G. Franklin, and C. B. Duane,
?Introduction to the theory of statistics?, New
York: McGraw-Hill, 3rd edition, 1974.
G. Salton, and J. M. Michael, ?Introduction to
Modern Information Retrieval?, New York:
McGraw-Hill, 1983.
S. M. Katz, ?Distribution of content words and
phrases in text and language modeling?, Natural
Language Engineering, 1996(2): 15-59
C. D. Manning, and H. Sch?tze, ?Foundations of
Statistical Natural Language Processing?, The
MIT Press, 1999.
S. Harter, ?A probabilistic approach to automatic
keyword indexing: Part II. An algorithm for
probabilistic indexing?, Journal of the American
Society for Information Science, 1975(26):
280-289
J. Platt, ?How to Implement SVMs?, IEEE
Intelligent System Magazine, Trends and
Controversies, Marti Hearst, ed., vol 13, no 4,
1998.
Improving Language Model Size Reduction using Better Pruning 
Criteria 
Jianfeng Gao  
 
Microsoft Research, Asia  
Beijing, 100080, China  
jfgao@microsoft.com 
Min Zhang1 
 
State Key Lab of Intelligent Tech & Sys. 
Computer Science & Technology Dept. 
Tsinghua University, China 
 
                                                     
1 This work was done while Zhang was working at Microsoft Research Asia as a visiting student. 
Abstract 
Reducing language model (LM) size is a 
critical issue when applying a LM to 
realistic applications which have memory 
constraints. In this paper, three measures 
are studied for the purpose of LM 
pruning. They are probability, rank, and 
entropy. We evaluated the performance of 
the three pruning criteria in a real 
application of Chinese text input in terms 
of character error rate (CER). We first 
present an empirical comparison, showing 
that rank performs the best in most cases. 
We also show that the high-performance 
of rank lies in its strong correlation with 
error rate. We then present a novel 
method of combining two criteria in 
model pruning. Experimental results 
show that the combined criterion 
consistently leads to smaller models than 
the models pruned using either of the 
criteria separately, at the same CER. 
1 Introduction 
Backoff n-gram models for applications such as 
large vocabulary speech recognition are typically 
trained on very large text corpora.  An 
uncompressed LM is usually too large for practical 
use since all realistic applications have memory 
constraints.  Therefore, LM pruning techniques are 
used to produce the smallest model while keeping 
the performance loss as small as possible.   
Research on backoff n-gram model pruning has 
been focused on the development of the pruning 
criterion, which is used to estimate the performance 
loss of the pruned model. The traditional count 
cutoff method (Jelinek, 1990) used a pruning 
criterion based on absolute frequency while recent 
research has shown that better pruning criteria can 
be developed based on more sophisticated measures 
such as perplexity.  
In this paper, we study three measures for 
pruning backoff n-gram models. They are 
probability, rank and entropy. We evaluated the 
performance of the three pruning criteria in a real 
application of Chinese text input (Gao et al, 2002) 
through CER. We first present an empirical 
comparison, showing that rank performs the best in 
most cases. We also show that the high-performance 
of rank lies in its strong correlation with error rate. 
We then present a novel method of combining two 
pruning criteria in model pruning. Our results show 
that the combined criterion consistently leads to 
smaller models than the models pruned using either 
of the criteria separately. In particular, the 
combination of rank and entropy achieves the 
smallest models at a given CER. 
The rest of the paper is structured as follows: 
Section 2 discusses briefly the related work on 
backoff n-gram pruning.  Section 3 describes in 
detail several pruning criteria. Section 4 presents an 
empirical comparison of pruning criteria using a 
Chinese text input system. Section 5 proposes our 
method of combining two criteria in model pruning. 
Section 6 presents conclusions and our future work. 
2 Related Work 
N-gram models predict the next word given the 
previous n-1 words by estimating the conditional 
probability P(wn|w1?wn-1). In practice, n is usually 
set to 2 (bigram), or 3 (trigram). For simplicity, we 
restrict our discussion to bigrams P(wn| wn-1), but our 
approaches can be extended to any n-gram. 
The bigram probabilities are estimated from the 
training data by maximum likelihood estimation 
(MLE). However, the intrinsic problem of MLE is 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 176-182.
                         Proceedings of the 40th Annual Meeting of the Association for
that of data sparseness: MLE leads to zero-value 
probabilities for unseen bigrams. To deal with this 
problem, Katz (1987) proposed a backoff scheme. 
He estimates the probability of an unseen bigram by 
utilizing unigram estimates as follows 
??
? >=
?
??
? otherwisewPw
wwcwwPwwP
ii
iiiid
ii )()(
0),()|()|(
1
11
1 ? , (1)
where c(wi-1wi) is the frequency of word pair (wi-1wi) 
in the training data, Pd represents the Good-Turing 
discounted estimate for seen word pairs, and ?(wi-1) 
is a normalization factor. 
Due to the memory limitation in realistic 
applications, only a finite set of word pairs have 
conditional probability P(wi|wi-1) explicitly 
represented in the model. The remaining word pairs 
are assigned a probability by backoff (i.e. unigram 
estimates). The goal of bigram pruning is to remove 
uncommon explicit bigram estimates P(wi|wi-1) from 
the model to reduce the number of parameters while 
minimizing the performance loss. 
The research on backoff n-gram model pruning 
can be formulated as the definition of the pruning 
criterion, which is used to estimate the performance 
loss of the pruned model. Given the pruning 
criterion, a simple thresholding algorithm for 
pruning bigram models can be described as follows: 
 
1. Select a threshold ?. 
2. Compute the performance loss due to 
pruning each bigram individually using the 
pruning criterion. 
3. Remove all bigrams with performance loss 
less than ?. 
4. Re-compute backoff weights. 
Figure 1: Thresholding algorithm for bigram 
pruning 
The algorithm in Figure 1 together with several 
pruning criteria has been studied previously 
(Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao 
and Lee, 2000; etc). A comparative study of these 
techniques is presented in (Goodman and Gao, 
2000). 
In this paper, three pruning criteria will be 
studied: probability, rank, and entropy. Probability 
serves as the baseline pruning criterion. It is derived 
from perplexity which has been widely used as a LM 
evaluation measure. Rank and entropy have been 
previously used as a metric for LM evaluation in 
(Clarkson and Robinson, 2001). In the current paper, 
these two measures will be studied for the purpose of 
backoff n-gram model pruning. In the next section, 
we will describe how pruning criteria are developed 
using these two measures. 
3 Pruning Criteria  
In this section, we describe the three pruning criteria 
we evaluated. They are derived from LM evaluation 
measures including perplexity, rank, and entropy. 
The goal of the pruning criterion is to estimate the 
performance loss due to pruning each bigram 
individually. Therefore, we represent the pruning 
criterion as a loss function, denoted by LF below. 
3.1 Probability 
The probability pruning criterion is derived from 
perplexity. The perplexity is defined as 
?= = ??
N
i
ii wwPNPP 1 1
)|(log1
2  (2)
where N is the size of the test data. The perplexity 
can be roughly interpreted as the expected branching 
factor of the test document when presented to the 
LM. It is expected that lower perplexities are 
correlated with lower error rates. 
The method of pruning bigram models using 
probability can be described as follows: all bigrams 
that change perplexity by less than a threshold are 
removed from the model. In this study, we assume 
that the change in model perplexity of the LM can be 
expressed in terms of a weighted difference of the 
log probability estimate before and after pruning a 
bigram. The loss function of probability LFprobability, 
is then defined as 
)]|(log)|(')[log( 111 ??? ?? iiiiii wwPwwPwwP , (3) 
where P(.|.) denotes the conditional probabilities 
assigned by the original model, P?(.|.) denotes the 
probabilities in the pruned model, and P(wi-1 wi) is a 
smoothed probability estimate in the original model.  
We notice that LFprobability of Equation (3) is very 
similar to that proposed by Seymore and Rosenfeld 
(1996), where the loss function is  
)]|(log)|(')[log( 111 ??? ?? iiiiii wwPwwPwwN .  
Here N(wi-1wi) is the discounted frequency that 
bigram wi-1wi was observed in training. N(wi-1wi) is 
conceptually identical to P(wi-1 wi) in Equation (3). 
From Equations (2) and (3), we can see that lower 
LFprobability is strongly correlated with lower 
perplexity. However, we found that LFprobability is 
suboptimal as a pruning criterion, evaluated on CER 
in our experiments. We assume that it is largely due 
to the deficiency of perplexity as a LM performance 
measure. 
Although perplexity is widely used due to its 
simplicity and efficiency, recent researches show 
that its correlation with error rate is not as strong as 
once thought. Clarkson and Robinson (2001) 
analyzed the reason behind it and concluded that the 
calculation of perplexity is based solely on the 
probabilities of words contained within the test text, 
so it disregards the probabilities of alternative 
words, which will be competing with the correct 
word (referred to as target word below) within the 
decoder (e.g. in a speech recognition system). 
Therefore, they used other measures such as rank 
and entropy for LM evaluation. These measures are 
based on the probability distribution over the whole 
vocabulary. That is, if the test text is w1n, then 
perplexity is based on the values of P(wi |wi-1), and 
the new measures will be based on the values of 
P(w|wi-1) for all w in the vocabulary. Since these 
measures take into account the probability 
distribution over all competing words (including the 
target word) within the decoder, they are, hopefully, 
better correlated with error rate, and expected to 
evaluate LMs more precisely than perplexity. 
3.2 Rank 
The rank of the target word w is defined as the 
word?s position in an ordered list of the bigram 
probabilities P(w|wi-1) where w?V, and V is the 
vocabulary. Thus the most likely word (within the 
decoder at a certain time point) has the rank of one, 
and the least likely has rank |V|, where |V| is the 
vocabulary size. 
We propose to use rank for pruning as follows: all 
bigrams that change rank by less than a threshold 
after pruning are removed from the model. The 
corresponding loss function LFrank is defined as 
?
?
??? ?+?
1
)}|(log])|(){log[( 111
ii ww
iiiiii wwRkwwRwwp (4) 
where R(.|.) denotes the rank of the observed bigram 
P(wi|wi-1) in the list of bigram probabilities P(w|wi-1) 
where w?V, before pruning, R?(.|.) is the new rank 
of it after pruning, and the summation is over all 
word pairs (wi-1wi).  k is a constant to assure that 
0)|(log])|(log[ 11 ??+? ?? iiii wwRkwwR . k is set to 
0.1 in our experiments. 
3.3 Entropy 
Given a bigram model, the entropy H of the 
probability distribution over the vocabulary V is 
generally given by 
??= =Vj ijiji wwPwwPwH 1 )|(log)|()( .  
We propose to use entropy for pruning as follows: 
all bigrams that change entropy by less than a 
threshold after pruning are removed from the model. 
The corresponding loss function LFentropy is defined 
as 
? ??? = ??Ni ii wHwHN 1 11 ))()((
1  (5)
where H is the entropy before pruning given history 
wi-1, H? is the new entropy after pruning, and N is the 
size of the test data.  
The entropy-based pruning is conceptually 
similar to the pruning method proposed in (Stolcke, 
1998). Stolcke used the Kullback-Leibler divergence 
between the pruned and un-pruned model 
probability distribution in a given context over the 
entire vocabulary. In particular, the increase in 
relative entropy from pruning a bigram is computed 
by 
?
?
??? ??
ii ww
iiiiii wwPwwPwwP
1
)]|(log)|(')[log( 111 ,  
where the summation is over all word pairs (wi-1wi).  
4 Empirical Comparison 
We evaluated the pruning criteria introduced in the 
previous section on a realistic application, Chinese 
text input.  In this application, a string of Pinyin 
(phonetic alphabet) is converted into Chinese 
characters, which is the standard way of inputting 
text on Chinese computers. This is a similar problem 
to speech recognition except that it does not include 
acoustic ambiguity. We measure performance in 
terms of character error rate (CER), which is the 
number of characters wrongly converted from the 
Pinyin string divided by the number of characters in 
the correct transcript. The role of the language 
model is, for all possible word strings that match the 
typed Pinyin string, to select the word string with the 
highest language model probability. 
The training data we used is a balanced corpus of 
approximately 26 million characters from various 
domains of text such as newspapers, novels, 
manuals, etc. The test data consists of half a million 
characters that have been proofread and balanced 
among domain, style and time.  
The back-off bigram models we generated in this 
study are character-based models. That is, the 
training and test corpora are not word-segmented. 
As a result, the lexicon we used contains 7871 single 
Chinese characters only. While word-based n-gram 
models are widely applied, we used character-based 
models for two reasons. First, pilot experiments 
show that the results of word-based and 
character-based models are qualitatively very 
similar. More importantly, because we need to build 
a very large number of models in our experiments as 
shown below, character-based models are much 
more efficient, both for training and for decoding. 
We used the absolute discount smoothing method 
for model training. 
None of the pruning techniques we consider are 
loss-less. Therefore, whenever we compare pruning 
criteria, we do so by comparing the size reduction of 
the pruning criteria at the same CER.  
Figure 2 shows how the CER varies with the 
bigram numbers in the models. For comparison, we 
also include in Figure 2 the results using count cutoff 
pruning. We can see that CER decreases as we keep 
more and more bigrams in the model. A steeper 
curve indicates a better pruning criterion.  
The main result to notice here is that the 
rank-based pruning achieves consistently the best 
performance among all of them over a wide range of 
CER values, producing models that are at 55-85% of 
the size of the probability-based pruned models with 
the same CER. An example of the detailed 
comparison results is shown in Table 1, where the 
CER is 13.8% and the value of cutoff is 1. The last 
column of Table 1 shows the relative model sizes 
with respect to the probability-based pruned model 
with the CER 13.8%.  
Another interesting result is the good 
performance of count cutoff, which is almost 
overlapping with probability-based pruning at larger 
model sizes 2 . The entropy-based pruning 
unfortunately, achieved the worst performance. 
13.6
13.7
13.8
13.9
14.0
14.1
3.E+05 4.E+05 5.E+05 6.E+05 7.E+05 8.E+05 9.E+05
# of bigrams in the model
av
era
ge
 er
ror
 ra
te
rank
prob
entropy
count cutoff
 
Figure 2: Comparison of pruning criteria 
Table 1: LM size comparison at CER 13.8% 
criterion # of bigram size (MB) % of prob
probability 774483 6.1 100.0% 
cutoff (=1) 707088 5.6 91.8% 
entropy 1167699 9.3 152.5% 
rank 512339 4.1 67.2% 
                                                     
2 The result is consistent with that reported in (Goodman 
and Gao, 2000), where an explanation was offered. 
We assume that the superior performance of 
rank-based pruning lies in the fact that rank (acting 
as a LM evaluation measure) has better correlation 
with CER. Clarkson and Robinson (2001) estimated 
the correlation between LM evaluation measures 
and word error rate in a speech recognition system. 
The related part of their results to our study are 
shown in Table 2, where r is the Pearson 
product-moment correlation coefficient, rs is the 
Spearman rank-order correlation coefficient, and T 
is the Kendall rank-order correlation coefficient. 
Table 2: Correlation of LM evaluation measures 
with word error rates (Clarkson and Robinson, 
2001) 
 r rs T 
Mean log rank 0.967 0.957 0.846
Perplexity 0.955 0.955 0.840
Mean entropy -0.799 -0.792 -0.602
Table 2 indicates that the mean log rank (i.e. 
related to the pruning criterion of rank we used) has 
the best correlation with word error rate, followed by 
the perplexity (i.e. related to the pruning criterion of 
probability we used) and the mean entropy (i.e. 
related to the pruning criterion of entropy we used), 
which support our test results. We can conclude that 
the LM evaluation measures which are better 
correlated with error rate lead to better pruning 
criteria. 
5 Combining Two Criteria 
We now investigate methods of combining pruning 
criteria described above. We begin by examining the 
overlap of the bigrams pruned by two different 
criteria to investigate which might usefully be 
combined. Then the thresholding pruning algorithm 
described in Figure 1 is modified so as to make use 
of two pruning criteria simultaneously. The problem 
here is how to find the optimal settings of the 
pruning threshold pair (each for one pruning 
criterion) for different model sizes. We show how an 
optimal function which defines the optimal settings 
of the threshold pairs is efficiently established using 
our techniques. 
5.1 Overlap 
From the abovementioned three pruning criteria, we 
investigated the overlap of the bigrams pruned by a 
pair of criteria. There are three criteria pairs. The 
overlap results are shown in Figure 3.  
We can see that the percentage of the number of 
bigrams pruned by both criteria seems to increase as 
the model size decreases, but all criterion-pairs have 
overlaps much lower than 100%. In particular, we 
find that the average overlap between probability 
and entropy is approximately 71%, which is the 
biggest among the three pairs. The pruning method 
based on the criteria of rank and entropy has the 
smallest average overlap of 63.6%. The results 
suggest that we might be able to obtain 
improvements by combining these two criteria for 
bigram pruning since the information provided by 
these criteria is, in some sense, complementary. 
0.E+00
2.E+05
4.E+05
6.E+05
8.E+05
1.E+06
0.E+00 2.E+05 4.E+05 6.E+05 8.E+05 1.E+06
# of pruned bigrams
# o
f o
ve
rla
pe
d b
igr
am
s
prob+rank
prob+entropy
rank+entropy
100% overlap
 
Figure 3: Overlap of selected bigrams between 
criterion pairs 
5.2 Pruning by two criteria 
In order to prune a bigram model based on two 
criteria simultaneously, we modified the 
thresholding pruning algorithm described in Figure 
1. Let lfi be the value of the performance loss 
estimated by the loss function LFi, ?i be the 
threshold defined by the pruning criterion Ci. The 
modified thresholding pruning algorithm can be 
described as follows: 
 
1. Select a setting of threshold pair (?1?2) 
2. Compute the values of performance loss lf1 
and lf2 due to pruning each bigram 
individually using the two pruning criteria 
C1 and C2, respectively. 
3. Remove all bigrams with performance loss 
lf1 less than ?1, and lf2 less than ?2. 
4. Re-compute backoff weights. 
Figure 4: Modified thresholding algorithm for 
bigram pruning 
Now, the remaining problem is how to find the 
optimal settings of the pruning threshold pair for 
different model sizes. This seems to be a very 
tedious task since for each model size, a large 
number of settings (?1?2) have to be tried for finding 
the optimal ones. Therefore, we convert the problem 
to the following one: How to find an optimal 
function ?2=f(?1) by which the optimal threshold ?2 
is defined for each threshold ?1. The function can be 
learned by pilot experiments described below. Given 
two thresholds ?1 and ?2 of pruning criteria C1 and 
C2, we try a large number of values of ?1, ?2, and 
build a large number of models pruned using the 
algorithm described in Figure 4. For each model 
size, we find an optimal setting of the threshold 
setting (?1?2) which results in a pruned model with 
the lowest CER. Finally, all these optimal threshold 
settings serve as the sample data, from which the 
optimal function can be learned. We found that in 
pilot experiments, a relatively small set of sample 
settings is enough to generate the function which is 
close enough to the optimal one. This allows us to 
relatively quickly search through what would 
otherwise be an overwhelmingly large search space. 
5.3 Results 
We used the same training data described in Section 
4 for bigram model training. We divided the test set 
described in Section 4 into two non-overlapped 
subsets. We performed testing on one subset 
containing 80% of the test set. We performed 
optimal function learning using the remaining 20% 
of the test set (referred to as held-out data below).  
Take the combination of rank and entropy as an 
example. An uncompressed bigram model was first 
built using all training data.  We then built a very 
large number of pruned bigram models using 
different threshold setting (? rank ?entropy), where the 
values ? rank, ?entropy ?  [3E-12, 3E-6]. By evaluating 
pruned models on the held-out data, optimal settings 
can be found. Some sample settings are shown in 
Table 3. 
Table 3: Sample optimal parameter settings for 
combination of criteria based on rank and entropy
# bigrams ? rank ?entropy 
137987 8.00E-07 8.00E-09 
196809 3.00E-07 8.00E-09 
200294 3.00E-07 5.00E-09 
274434 3.00E-07 5.00E-10 
304619 8.00E-08 8.00E-09 
394300 5.00E-08 3.00E-10 
443695 3.00E-08 3.00E-10 
570907 8.00E-09 3.00E-09 
669051 5.00E-09 5.00E-10 
890664 5.00E-11 3.00E-10 
892214 5.00E-12 3.00E-10 
892257 3.00E-12 3.00E-10 
In experiments, we found that a linear regression 
model of Equation (6) is powerful enough to learn a 
function which is close enough to the optimal one.  
21 )log()log( ???? +?= rankentropy  (6) 
Here ?1 and ?2 are coefficients estimated from the 
sample settings. Optimal functions of the other two 
threshold-pair settings (? rank?probability) and (? 
probability?entropy) are obtained similarly. They are 
shown in Table 4. 
 
Table 4. Optimal functions 
5.6)log(3.0)log( +?= rankentropy ??  
2.6)log( =yprobabilit? , for any rank?  
5.3)log(7.0)log( +?= yprobabilitentropy ??  
 
In Figure 5, we present the results using models 
pruned with all three threshold-pairs defined by the 
functions in Table 4. As we expected, in all three 
cases, using a combination of two pruning criteria 
achieves consistently better performance than using 
either of the criteria separately. In particular, using 
the combination of rank and entropy, we obtained 
the best models over a wide large of CER values. It 
corresponds to a significant size reduction of 
15-54% over the probability-based LM pruning at 
the same CER. An example of the detailed 
comparison results is shown in Table 5.  
 
 
 
Table 5: LM size comparison at CER 13.8% 
Criterion # of bigram size (MB) % of prob
Prob 1036627 8.2 100.0%
Entropy 1291000 10.2 124.4%
Rank 643411 5.1 62.2%
Prob + entropy 542124 4.28 52.2%
Prob + rank 579115 4.57 55.7%
rank + entropy 538252 4.25 51.9%
 
There are two reasons for the superior 
performance of the combination of rank and entropy. 
First, the rank-based pruning achieves very good 
performance as described in Section 4. Second, as 
shown in Section 5.1, there is a relatively small 
overlap between the bigrams chosen by these two 
pruning criteria, thus big improvement can be 
achieved through the combination. 
6 Conclusion 
The research on backoff n-gram pruning has been 
focused on the development of the pruning criterion, 
which is used to estimate the performance loss of the 
pruned model.  
This paper explores several pruning criteria for 
backoff n-gram model size reduction. Besides the 
widely used probability, two new pruning criteria 
have been developed based on rank and entropy. We 
have performed an empirical comparison of these 
pruning criteria. We also presented a thresholding 
algorithm for model pruning, in which two pruning 
criteria can be used simultaneously. Finally, we 
described our techniques of finding the optimal 
setting of the threshold pair given a specific model 
size. 
We have shown several interesting results. They 
include the confirmation of the estimation that the 
measures which are better correlated with CER for 
LM evaluation leads to better pruning criteria. Our 
experiments show that rank, which has the best 
correlation with CER, achieves the best performance 
when there is only one criterion used in bigram 
model pruning. We then show empirically that the 
overlap of the bigrams pruned by different criteria is 
relatively low. This indicates that we might obtain 
improvements through a combination of two criteria 
for bigram pruning since the information provided 
by these criteria is complementary. This hypothesis 
is confirmed by our experiments. Results show that 
using two pruning criteria simultaneously achieves 
13.6
13.7
13.8
13.9
14.0
14.1
14.2
3.E+05 5.E+05 7.E+05 9.E+05 1.E+06
# of bigrams in the model
av
era
ge
 er
ror
 ra
te
rank
prob
entropy
rank+prob
rank+entropy
prob+entropy
 
Figure 5: Comparison of combined pruning 
criterion performance 
better bigram models than using either of the criteria 
separately. In particular, the combination of rank 
and entropy achieves the smallest bigram models at 
the same CER. 
For our future work, more experiments will be 
performed on other language models such as 
word-based bigram and trigram for Chinese and 
English. More pruning criteria and their 
combinations will be investigated as well. 
Acknowledgements 
The authors wish to thank Ashley Chang, Joshua 
Goodman, Chang-Ning Huang, Hang Li, Hisami 
Suzuki and Ming Zhou for suggestions and 
comments on a preliminary draft of this paper. 
Thanks also to three anonymous reviews for 
valuable and insightful comments.  
References 
Clarkson, P. and Robinson, T. (2001), Improved 
language modeling through better language 
model evaluation measures, Computer Speech 
and  Language, 15:39-53, 2001. 
Gao, J. and Lee K.F (2000). Distribution-based 
pruning of backoff language models, 38th Annual 
meetings of the Association for Computational 
Linguistics (ACL?00), HongKong, 2000. 
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). 
Toward a unified approach to statistical language 
modeling for Chinese. ACM Transactions on 
Asian Language Information Processing, Vol. 1, 
No. 1, pp 3-33. Draft available from 
http://www.research.microsoft.com/~jfgao 
Goodman, J. and Gao, J. (2000) Language model 
size reduction by pruning and clustering, 
ICSLP-2000, International Conference on 
Spoken Language Processing, Beijing, October 
16-20, 2000. 
Jelinek, F. (1990). Self-organized language 
modeling for speech recognition. In Readings in 
Speech Recognition, A. Waibel and K. F. Lee, 
eds., Morgan-Kaufmann, San Mateo, CA, pp. 
450-506. 
Katz, S. M., (1987). Estimation of probabilities from 
sparse data for other language component of a 
speech recognizer. IEEE transactions on 
Acoustics, Speech and Signal Processing, 
35(3):400-401, 1987. 
Rosenfeld, R. (1996). A maximum entropy approach 
to adaptive statistical language modeling. 
Computer, Speech and Language, vol. 10, pp. 
187-- 228, 1996. 
Seymore, K., and Rosenfeld, R. (1996). Scalable 
backoff language models. Proc. ICSLP, Vol. 1., 
pp.232-235, Philadelphia, 1996 
Stolcke, A. (1998). Entropy-based Pruning of 
Backoff Language Models. Proc. DARPA News 
Transcription and Understanding Workshop, 
1998, pp. 270-274, Lansdowne, VA. 
 
Exploring Asymmetric Clustering for Statistical Language Modeling 
Jianfeng Gao  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
jfgao@microsoft.com 
Joshua T. Goodman  
Microsoft Research, Redmond 
Washington 98052, USA  
joshuago@microsoft.com 
Guihong Cao1  
Department of Computer 
Science and Engineering of 
Tianjin University, China  
Hang Li  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
hangli@microsoft.com 
                                                     
1 This work was done while Cao was visiting Microsoft Research Asia. 
Abstract 
The n-gram model is a stochastic model, 
which predicts the next word (predicted 
word) given the previous words 
(conditional words) in a word sequence.  
The cluster n-gram model is a variant of 
the n-gram model in which similar words 
are classified in the same cluster. It has 
been demonstrated that using different 
clusters for predicted and conditional 
words leads to cluster models that are 
superior to classical cluster models which 
use the same clusters for both words.  This 
is the basis of the asymmetric cluster 
model (ACM) discussed in our study.  In 
this paper, we first present a formal 
definition of the ACM. We then describe 
in detail the methodology of constructing 
the ACM. The effectiveness of the ACM 
is evaluated on a realistic application, 
namely Japanese Kana-Kanji conversion.  
Experimental results show substantial 
improvements of the ACM in comparison 
with classical cluster models and word 
n-gram models at the same model size.  
Our analysis shows that the 
high-performance of the ACM lies in the 
asymmetry of the model. 
1 Introduction 
The n-gram model has been widely applied in many 
applications such as speech recognition, machine 
translation, and Asian language text input [Jelinek, 
1990; Brown et al, 1990; Gao et al, 2002].  It is a 
stochastic model, which predicts the next word 
(predicted word) given the previous n-1 words 
(conditional words) in a word sequence. 
The cluster n-gram model is a variant of the word 
n-gram model in which similar words are classified 
in the same cluster.  This has been demonstrated as 
an effective way to deal with the data sparseness 
problem and to reduce the memory sizes for realistic 
applications. Recent research [Yamamoto et al, 
2001] shows that using different clusters for 
predicted and conditional words can lead to cluster 
models that are superior to classical cluster models, 
which use the same clusters for both words [Brown 
et al, 1992].  This is the basis of the asymmetric 
cluster model (ACM), which will be formally 
defined and empirically studied in this paper.  
Although similar models have been used in previous 
studies [Goodman and Gao, 2000; Yamamoto et al, 
2001], several issues have not been completely 
investigated. These include: (1) an effective 
methodology for constructing the ACM, (2) a 
thorough comparative study of the ACM with 
classical cluster models and word models when they 
are applied to a realistic application, and (3) an 
analysis of the reason why the ACM is superior. 
The goal of this study is to address the above 
three issues. We first present a formal definition of 
the ACM; then we describe in detail the 
methodology of constructing the ACM including (1) 
an asymmetric clustering algorithm in which 
different metrics are used for clustering the 
predicted and conditional words respectively; and 
(2) a method for model parameter optimization in 
which the optimal cluster numbers are found for 
different clusters.  We evaluate the ACM on a real 
application, Japanese Kana-Kanji conversion, which 
converts phonetic Kana strings into proper Japanese 
orthography. The performance is measured in terms 
of character error rate (CER).  Our results show 
substantial improvements of the ACM in 
comparison with classical cluster models and word 
n-gram models at the same model size.  Our analysis 
shows that the high-performance of the ACM comes 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 183-190.
                         Proceedings of the 40th Annual Meeting of the Association for
from better structure and better smoothing, both of 
which lie in the asymmetry of the model. 
This paper is organized as follows: Section 1 
introduces our research topic, and then Section 2 
reviews related work. Section 3 defines the ACM 
and describes in detail the method of model 
construction. Section 4 first introduces the Japanese 
Kana-Kanji conversion task; it then presents our 
main experiments and a discussion of our findings.  
Finally, conclusions are presented in Section 5. 
2 Related Work 
A large amount of previous research on clustering 
has been focused on how to find the best clusters 
[Brown et al, 1992; Kneser and Ney, 1993; 
Yamamoto and Sagisaka, 1999; Ueberla, 1996; 
Pereira et al, 1993; Bellegarda et al, 1996; Bai et 
al., 1998]. Only small differences have been 
observed, however, in the performance of the 
different techniques for constructing clusters. In this 
study, we focused our research on novel techniques 
for using clusters ? the ACM, in which different 
clusters are used for predicted and conditional words 
respectively. 
The discussion of the ACM in this paper is an 
extension of several studies below. The first similar 
cluster model was presented by Goodman and Gao 
[2000] in which the clustering techniques were 
combined with Stolcke?s [1998] pruning to reduce 
the language model (LM) size effectively. Goodman 
[2001] and Gao et al [2001] give detailed 
descriptions of the asymmetric clustering algorithm.  
However, the impact of the asymmetric clustering 
on the performance of the resulting cluster model 
was not empirically studied there.  Gao et al, [2001] 
presented a fairly thorough empirical study of 
clustering techniques for Asian language modeling. 
Unfortunately, all of the above work studied the 
ACM without applying it to an application; thus 
only perplexity results were presented.  The first real 
application of the ACM was a simplified bigram 
ACM used in a Chinese text input system [Gao et al  
2002]. However, quite a few techniques (including 
clustering) were integrated to construct a Chinese 
language modeling system, and the contribution of 
using the ACM alone was by no means completely 
investigated. 
Finally, there is one more point worth 
mentioning. Most language modeling improvements 
reported previously required significantly more 
space than word trigram models [Rosenfeld, 2000]. 
Their practical value is questionable since all 
realistic applications have memory constraints. In 
this paper, our goal is to achieve a better tradeoff 
between LM performance (perplexity and CER) and 
model size. Thus, whenever we compare the 
performance of different models (i.e. ACM vs. word 
trigram model), Stolcke?s pruning is employed to 
bring the models compared to similar sizes. 
3 Asymmetric Cluster Model 
3.1 Model  
The LM predicts the next word wi given its history h 
by estimating the conditional probability P(wi|h). 
Using the trigram approximation, we have 
P(wi|h)?P(wi|wi-2wi-1), assuming that the next word 
depends only on the two preceding words.  
In the ACM, we will use different clusters for 
words in different positions.  For the predicted word, 
wi, we will denote the cluster of the word by PWi, 
and we will refer to this as the predictive cluster. .For 
the words wi-2 and wi-1 that we are conditioning on, 
we will denote their clusters by CWi-2 and CWi-1 
which we call conditional clusters.  When we which 
to refer to a cluster of a word w in general we will 
use the notation W.  The ACM estimates the 
probability of wi given the two preceeding words wi-2 
and wi-1 as the product of the following two 
probabilities: 
(1) The probability of the predicted cluster PWi 
given the preceding conditional clusters CWi-2 
and CWi-1, P(PWi|CWi-2CWi-1), and 
(2) The probability of the word given its cluster PWi 
and the preceding conditional clusters CWi-2 and 
CWi-1, P(wi|CWi-2CWi-1PWi). 
Thus, the ACM can be parameterized by 
)|()|()|( 1212 iiiiiiii PWCWCWwPCWCWPWPhwP ???? ?? (1) 
The ACM consists of two sub-models: (1) the 
cluster sub-model P(PWi|CWi-2CWi-1), and (2) the 
word sub-model P(wi|CWi-2CWi-1PWi). To deal with 
the data sparseness problem, we used a backoff 
scheme (Katz, 1987) for the parameter estimation of 
each sub-model. The backoff scheme recursively 
estimates the probability of an unseen n-gram by 
utilizing (n-1)-gram estimates. 
The basic idea underlying the ACM is the use of 
different clusters for predicted and conditional 
words respectively.  Classical cluster models are 
symmetric in that the same clusters are employed for 
both predicted and conditional words.  However, the 
symmetric cluster model is suboptimal in practice. 
For example, consider a pair of words like ?a? and 
?an?.  In general, ?a? and ?an? can follow the same 
words, and thus, as predicted words, belong in the 
same cluster. But, there are very few words that can 
follow both ?a? and ?an?. So as conditional words, 
they belong in different clusters. 
In generating clusters, two factors need to be 
considered: (1) clustering metrics, and (2) cluster 
numbers.  In what follows, we will investigate the 
impact of each of the factors. 
3.2 Asymmetric clustering  
The basic criterion for statistical clustering is to 
maximize the resulting probability (or minimize the 
resulting perplexity) of the training data. Many 
traditional clustering techniques [Brown et al, 
1992] attempt to maximize the average mutual 
information of adjacent clusters 
?=
21 , 2
12
2121 )(
)|(log)(),(
WW WP
WWPWWPWWI , (2) 
where the same clusters are used for both predicted 
and conditional words. We will call these clustering 
techniques symmetric clustering, and the resulting 
clusters both clusters.  In constructing the ACM, we 
used asymmetric clustering, in which different 
clusters are used for predicted and conditional 
words. In particular, for clustering conditional 
words, we try to minimize the perplexity of training 
data for a bigram of the form P(wi|Wi-1), which is 
equivalent to maximizing 
?
=
?
N
i
ii WwP
1
1)|( . (3) 
where N is the total number of words in the training 
data.  We will call the resulting clusters conditional 
clusters denoted by CW. For clustering predicted 
words, we try to minimize the perplexity of training 
data of P(Wi|wi-1)?P(wi|Wi). We will call the 
resulting clusters predicted clusters denoted by PW. 
We have2 
??
= ?
?
=
? ?=?
N
i i
ii
i
iiN
i
iiii WP
wWP
wP
WwPWwPwWP
1 1
1
1
1 )(
)(
)(
)()|()|(  
  ?
=
?
?
?= N
i i
ii
i
ii
WP
WwP
wP
wWP
1
1
1 )(
)(
)(
)(  
  ?
= ??
?= N
i
ii
i
i WwPwP
wP
1
1
1
)|()(
)( . 
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, for the selection of the best clusters, it is 
sufficient to try to maximize 
?
=
?
N
i
ii WwP
1
1 )|( . (4) 
This is very convenient since it is exactly the op-
posite of what was done for conditional clustering. It 
                                                     
2 Thanks to Lillian Lee for suggesting this justification of 
predictive clusters. 
means that we can use the same clustering tool for 
both, and simply switch the order used by the 
program used to get the raw counts for clustering.  
The clustering technique we used creates a binary 
branching tree with words at the leaves.  The ACM 
in this study is a hard cluster model, meaning that 
each word belongs to only one cluster.  So in the 
clustering tree, each word occurs in a single leaf.   In 
the ACM, we actually use two different clustering 
trees. One is optimized for predicted words, and the 
other for conditional words. 
The basic approach to clustering we used is a 
top-down, splitting clustering algorithm. In each 
iteration, a cluster is split into two clusters in the 
way that the splitting achieves the maximal entropy 
decrease (estimated by Equations (3) or (4)). Finally, 
we can also perform iterations of swapping all words 
between all clusters until convergence i.e. no more 
entropy decrease can be found3. We find that our 
algorithm is much more efficient than agglomerative 
clustering algorithms ? those which merge words 
bottom up.  
3.3 Parameter optimization 
Asymmetric clustering results in two binary 
clustering trees. By cutting the trees at a certain 
level, it is possible to achieve a wide variety of 
different numbers of clusters.  For instance, if the 
tree is cut after the 8th level, there will be roughly 
28=256 clusters.  Since the tree is not balanced, the 
actual number of clusters may be somewhat smaller. 
We use Wl to represent the cluster of a word w using 
a tree cut at level l.  In particular, if we set l to the 
value ?all?, it means that the tree is cut at infinite 
depth, i.e. each cluster contains a single word. The 
ACM model of Equation (1) can be rewritten as 
 P(PWil|CWi-2jCWi-1j)?P(wi|PWi-2kCWi-1kCWil). (5) 
To optimally apply the ACM to realistic applications 
with memory constraints, we are always seeking the 
correct balance between model size and 
performance. We used Stolcke?s pruning method to 
produce many ACMs with different model sizes. In 
our experiments, whenever we compare techniques, 
we do so by comparing the performance (perplexity 
and CER) of the LM techniques at the same model 
sizes. Stolcke?s pruning is an entropy-based cutoff 
                                                     
3 Notice that for experiments reported in this paper, we 
used the basic top-down algorithm without swapping. 
Although the resulting clusters without swapping are not 
even locally optimal, our experiments show that the 
quality of clusters (in terms of the perplexity of the 
resulting ACM) is not inferior to that of clusters with 
swapping. 
method, which can be described as follows: all 
n-grams that change perplexity by less than a 
threshold are removed from the model. For pruning 
the ACM, we have two thresholds: one for the 
cluster sub-model P(PWil|CWi-2jCWi-1j) and one for 
the word sub-model P(wi|CWi-2kCWi-1kPWil) 
respectively, denoted by tc and  tw below. 
In this way, we have 5 different parameters that 
need to be simultaneously optimized: l, j, k, tc, and 
tw, where j, k, and l are the numbers of clusters, and tc 
and tw are the pruning thresholds.  
A brute-force approach to optimizing such a large 
number of parameters is prohibitively expensive. 
Rather than trying a large number of combinations 
of all 5 parameters, we give an alternative technique 
that is significantly more efficient. Simple math 
shows that the perplexity of the overall model 
P(PWil|CWi-2jCWi-1j)? P(wi|CWi-2kCWi-1kPWil) is 
equal to the perplexity of the cluster sub-model 
P(PWil|CWi-2jCWi-1j) times the perplexity of the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  The size of 
the overall model is clearly the sum of the sizes of 
the two sub-models.  Thus, we try a large number of 
values of j, l, and a pruning threshold tc for 
P(PWil|CWi-2jCWi-1j), computing sizes and 
perplexities of each, and a similarly large number of 
values of l,  k, and a separate threshold tw for 
P(wi|CWi-2kCWi-1kPWil).  We can then look at all 
compatible pairs of these models (those with the 
same value of l) and quickly compute the perplexity 
and size of the overall models.  This allows us to 
relatively quickly search through what would 
otherwise be an overwhelmingly large search space. 
4 Experimental Results and Discussion 
4.1 Japanese Kana-Kanji Conversion Task 
Japanese Kana-Kanji conversion is the standard 
method of inputting Japanese text by converting a 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. The 
performance is generally measured in terms of 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is, 
for all possible word strings that match the typed 
phonetic symbol string, to select the word string 
with the highest language model probability. 
Current products make about 5-10% errors in con-
version of real data in a wide variety of domains. 
4.2 Settings 
In the experiments, we used two Japanese 
newspaper corpora: the Nikkei Newspaper corpus, 
and the Yomiuri Newspaper corpus. Both text 
corpora have been word-segmented using a lexicon 
containing 167,107 entries.  
We performed two sets of experiments: (1) pilot 
experiments, in which model performance is 
measured in terms of perplexity and (2) Japanese 
Kana-Kanji conversion experiments, in which the 
performance of which is measured in terms of CER. 
In the pilot experiments, we used a subset of the 
Nikkei newspaper corpus: ten million words of the 
Nikkei corpus for language model training, 10,000 
words for held-out data, and 20,000 words for 
testing data. None of the three data sets overlapped.  
In the Japanese Kana-Kanji conversion experiments, 
we built language models on a subset of the Nikkei 
Newspaper corpus, which contains 36 million 
words. We performed parameter optimization on a 
subset of held-out data from the Yomiuri Newspaper 
corpus, which contains 100,000 words. We 
performed testing on another subset of the Yomiuri 
Newspaper corpus, which contains 100,000 words. 
In both sets of experiments, word clusters were 
derived from bigram counts generated from the 
training corpora. Out-of-vocabulary words were not 
included in perplexity and error rate computations. 
4.3 Impact of asymmetric clustering 
As described in Section 3.2, depending on the 
clustering metrics we chose for generating clusters, 
we obtained three types of clusters: both clusters 
(the metric of Equation (2)), conditional clusters 
(the metric of Equation (3)), and predicted clusters 
(the metric of Equation (4)). We then performed a 
series of experiments to investigate the impact of 
different types of clusters on the ACM. We used 
three variants of the trigram ACM: (1) the predictive 
cluster model P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) where 
only predicted words are clustered, (2) the 
conditional cluster model P(wi|Wi-2Wi-1) where only 
conditional words are clustered, and (3) the IBM 
model P(wi|Wi)? P(Wi|Wi-2Wi-1) which can be treated 
as a special case of the ACM of Equation (5) by 
using the same type of cluster for both predicted and 
conditional words, and setting k = 0, and l = j. For 
each cluster trigram model, we compared their 
perplexities and CER results on Japanese Kana- 
Kanji conversion using different types of clusters. 
For each cluster type, the number of clusters were 
fixed to the same value 2^6 just for comparison.  The 
results are shown in Table 1. It turns out that the 
benefit of using different clusters in different 
positions is obvious.  For each cluster trigram 
model, the best results were achieved by using the 
?matched? clusters, e.g. the predictive cluster model 
P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) has the best 
performance when the cluster Wi is the predictive 
cluster PWi generated by using the metric of 
Equation (4). In particular, the IBM model achieved 
the best results when predicted and conditional 
clusters were used for predicted and conditional 
words respectively. That is, the IBM model is of the 
form P(wi|PWi)? P(PWi|CWi-2CWi-1). 
 Con Pre Both Con + Pre 
Perplexity 287.7 414.5 377.6 --- Con 
model CER (%) 4.58 11.78 12.56 --- 
Perplexity 103.4 102.4 103.3 --- Pre 
model CER (%) 3.92 3.63 3.82 --- 
Perplexity 548.2 514.4 385.2 382.2 IBM 
model CER (%) 6.61 6.49 5.82 5.36 
Table 1: Comparison of different cluster types 
with cluster-based models 
4.4 Impact of parameter optimization 
In this section, we first present our pilot experiments 
of finding the optimal parameter set of the ACM (l, j, 
k, tc, tw) described in Section 2.3. Then, we compare 
the ACM to the IBM model, showing that the 
superiority of the ACM results from its better 
structure. 
In this section, the performance of LMs was 
measured in terms of perplexity, and the size was 
measured as the total number of parameters of the 
LM: one parameter for each bigram and trigram, one 
parameter for each normalization parameter ? that 
was needed, and one parameter for each unigram.  
We first used the conditional cluster model of the 
form P(wi|CWi-2jCWi-1j). Some sample settings of 
parameters (j, tw) are shown in Figure 1. The 
performance was consistently improved by 
increasing the number of clusters j, except at the 
smallest sizes.  The word trigram model was 
consistently the best model, except at the smallest 
sizes, and even then was only marginally worse than 
the conditional cluster models.  This is not surprising 
because the conditional cluster model always 
discards information for predicting words. 
We then used the predictive cluster model of the 
form P(PWil|wi-2wi-1)?P(wi|wi-2wi-1PWil), where only 
predicted words are clustered. Some sample settings 
of the parameters (l, tc, tw) are shown in Figure 2. For 
simplicity, we assumed tc=tw, meaning that the same 
pruning threshold values were used for both 
sub-models. It turns out that predictive cluster 
models achieve the best perplexity results at about 
2^6 or 2^8 clusters. The models consistently 
outperform the baseline word trigram models.  
We finally returned to the ACM of Equation (5), 
where both conditional words and the predicted 
word are clustered (with different numbers of 
clusters), and which is referred to as the combined 
cluster model below.  In addition, we allow different 
values of the threshold for different sub-models. 
Therefore, we need to optimize the model parameter 
set l, j, k, tc, tw.  
Based on the pilot experiment results using 
conditional and predictive cluster models, we tried 
combined cluster models for values l? [4, 10], j, 
k? [8, 16]. We also allow j, k=all. Rather than plot 
all points of all models together, we show only the 
outer envelope of the points.  That is, if for a given 
model type and a given point there is some other 
point of the same type with both lower perplexity 
and smaller size than the first point, then we do not 
plot the first, worse point.  
The results are shown in Figure 3, where the 
cluster number of IBM models is 2^14 which 
achieves the best performance for IBM models in 
our experiments.  It turns out that when l? [6, 8] and 
j, k>12, combined cluster models yield the best 
results. We also found that the predictive cluster 
models give as good performance as the best 
combined ones while combined models 
outperformed very slightly only when model sizes 
are small. This is not difficult to explain. Recall that 
the predictive cluster model is a special case of the 
combined model where words are used in 
conditional positions, i.e. j=k=all. Our experiments 
show that combined models achieved good 
performance when large numbers of clusters are 
used for conditional words, i.e. large j, k>12, which 
are similar to words. 
The most interesting analysis is to look at some 
sample settings of the parameters of the combined 
cluster models in Figure 3. In Table 2, we show the 
best parameter settings at several levels of model 
size. Notice that in larger model sizes, predictive 
cluster models (i.e. j=k=all) perform the best in 
some cases. The ?prune? columns (i.e. columns 6 and 
7) indicate the Stolcke pruning parameter we used.  
First, notice that the two pruning parameters (in 
columns 6 and 7) tend to be very similar.  This is 
desirable since applying the theory of relative 
entropy pruning predicts that the two pruning 
parameters should actually have the same value.   
Next, let us compare the ACM 
P(PWil|CWi-2jCWi-1j)?P(wi|CWi-2kCWi-1kPWil) to 
traditional IBM clustering of the form 
P(Wil|Wi-2lWi-1l)?P(wi|Wil), which is equal to 
P(Wil|Wi-2lWi-1l)?P(wi|Wi-20Wi-10Wil) (assuming the   
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06
size
pe
rp
lex
ity
2^12 clusters
2^14 clusters
2^16 clusters
word trigram
 
Figure 1. Comparison of conditional models 
applied with different numbers of clusters 
100
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
2^4 clusters
2^6 clusters
2^8 clusters
2^10 clusters
word trigram
 
Figure 2. Comparison of predictive models 
applied with different numbers of clusters 
100
110
120
130
140
150
160
170
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
ACM
IBM
word trigram
predictive model
 
Figure 3. Comparison of ACMs, predictive 
cluster model, IBM model, and word trigram 
model 
same type of cluster is used for both predictive and 
conditional words). Our results in Figure 3 show that 
the performance of IBM models is roughly an order 
of magnitude worse than that of ACMs. This is 
because in addition to the use of the symmetric 
cluster model, the traditional IBM model makes two 
more assumptions that we consider suboptimal.  
First, it assumes that j=l.  We see that the best results 
come from unequal settings of j and l.  Second, more 
importantly, IBM clustering assumes that k=0.  We 
see that not only is the optimal setting for k not 0, but 
also typically the exact opposite is the optimal: k=all 
in which case P(wi|CWi-2kCWi-1kPWil)= 
P(wi|wi-2wi-1PWil), or k=14, 16, which is very 
similar. That is, we see that words depend on the 
previous words and that an independence 
assumption is a poor one.  Of course, many of these 
word dependencies are pruned away ? but when a 
word does depend on something, the previous words 
are better predictors than the previous clusters. 
Another important finding here is that for most of 
these settings, the unpruned model is actually larger 
than a normal trigram model ? whenever k=all or 14, 
16, the unpruned model P(PWil|CWi-2jCWi-1j) ? 
P(wi|CWi-2kCWi-1kPWil) is actually larger than an 
unpruned model P(wi|wi-2wi-1). 
This analysis of the data is very interesting ? it 
implies that the gains from clustering are not from 
compression, but rather from capturing structure.  
Factoring the model into two models, in which the 
cluster is predicted first, and then the word is 
predicted given the cluster, allows the structure and 
regularities of the model to be found. This larger, 
better structured model can be pruned more 
effectively, and it achieved better performance than 
a word trigram model at the same model size. 
Model size Perplexity l j k tc tw 
2.0E+05 141.1 8 12 14 24 24 
2.5E+05 135.7 8 12 14 12 24 
5.0E+05 118.8 6 14 16 6 12 
7.5E+05 112.8 6 16 16 3 6 
1.0E+06 109.0 6 16 16 3 3 
1.3E+06 107.4 6 16 16 2 3 
1.5E+06 106.0 6 All all 2 2 
1.9E+06 104.9 6 All all 1 2 
Table 2: Sample parameter settings for the ACM 
4.5 CER results 
Before we present CER results of the Japanese 
Kana-Kanji conversion system, we briefly describe 
our method for storing the ACM in practice.  
One of the most common methods for storing 
backoff n-gram models is to store n-gram 
probabilities (and backoff weights) in a tree 
structure, which begins with a hypothetical root 
node that branches out into unigram nodes at the first 
level of the tree, and each of those unigram nodes in 
turn branches out into bigram nodes at the second 
level and so on. To save storage, n-gram 
probabilities such as P(wi|wi-1) and backoff weights 
such as ?(wi-2wi-1) are stored in a single (bigram) 
node array (Clarkson and Rosenfeld, 1997). 
Applying the above tree structure to storing the 
ACM is a bit complicated ? there are some 
representation issues. For example, consider the 
cluster sub-model P(PWil|CWi-2jCWi-1j). N-gram 
probabilities such as P(PWil|CWi-1j) and backoff 
weights such as ?(CWi-2jCWi-1j) cannot be stored in a 
single (bigram) node array, because l ? j and 
PW?CW. Therefore, we used two separate trees to 
store probabilities and backoff weights, 
respectively. As a result, we used four tree structures 
to store ACMs in practice: two for the cluster 
sub-model P(PWil|CWi-2jCWi-1j), and two for the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  We found 
that the effect of the storage structure cannot be 
ignored in a real application. 
In addition, we used several techniques to 
compress model parameters (i.e. word id, n-gram 
probability, and backoff weight, etc.) and reduce the 
storage space of models significantly. For example, 
rather than store 4-byte floating point values for all 
n-gram probabilities and backoff weights, the values 
are quantized to a small number of quantization 
levels. Quantization is performed separately on each 
of the n-gram probability and backoff weight lists, 
and separate quantization level look-up tables are 
generated for each of these sets of parameters.  We 
used 8-bit quantization, which shows no 
performance decline in our experiments. 
Our goal is to achieve the best tradeoff between 
performance and model size. Therefore, we would 
like to compare the ACM with the word trigram 
model at the same model size. Unfortunately, the 
ACM contains four sub-models and this makes it 
difficult to be pruned to a specific size. Thus for 
comparison, we always choose the ACM with 
smaller size than its competing word trigram model 
to guarantee that our evaluation is under-estimated. 
Experiments show that the ACMs achieve 
statistically significant improvements over word 
trigram models at even smaller model sizes (p-value 
=8.0E-9). Some results are shown in Table 3.  
Word trigram model ACM 
Size 
(MB) 
CER Size 
(MB) 
CER  CER 
Reduction 
1.8 4.56% 1.7 4.25% 6.8% 
5.8 4.08% 4.5 3.83% 6.1% 
11.7 4.04% 10.7 3.73% 7.7% 
23.5 4.00% 21.7 3.63% 9.3% 
42.4 3.98% 40.4 3.63% 8.8% 
Table 3:  CER results of ACMs and word 
trigram models at different model sizes 
Now we discuss why the ACM is superior to 
simple word trigrams.  In addition to the better 
structure as shown in Section 3.3, we assume here 
that the benefit of our model also comes from its 
better smoothing. Consider a probability such as 
P(Tuesday| party on). If we put the word ?Tuesday? 
into the cluster WEEKDAY, we decompose the 
probability 
When each word belongs to one class, simple math 
shows that this decomposition is a strict equality. 
However, when smoothing is taken into 
consideration, using the clustered probability will be 
more accurate than using the non-clustered 
probability. For instance, even if we have never seen 
an example of ?party on Tuesday?, perhaps we have 
seen examples of other phrases, such as ?party on 
Wednesday?; thus, the probability P(WEEKDAY | 
party on) will be relatively high. Furthermore, 
although we may never have seen an example of 
?party on WEEKDAY Tuesday?, after we backoff or 
interpolate with a lower order model, we may able to 
accurately estimate P(Tuesday | on WEEKDAY). 
Thus, our smoothed clustered estimate may be a 
good one. 
Our assumption can be tested empirically by 
following experiments.  We first constructed several 
test sets with different backoff rates4. The backoff 
rate of a test set, when presented to a trigram model, 
is defined as the number of words whose trigram 
probabilities are estimated by backoff bigram 
probabilities divided by the number of words in the 
test set.  Then for each test set, we obtained a pair of 
CER results using the ACM and the word trigram 
model respectively.  As shown in Figure 4, in both 
cases, CER increases as the backoff rate increases 
from 28% to 40%. But the curve of the word trigram 
model has a steeper upward trend.  The difference of 
the upward trends of the two curves can be shown 
more clearly by plotting the CER difference between 
them, as shown in Figure 5.  The results indicate that 
because of its better smoothing, when the backoff 
rate increases, the CER using the ACM does not 
increase as fast as that using the word trigram model.  
Therefore, we are reasonably confident that some 
portion of the benefit of the ACM comes from its 
better smoothing. 
2.1
2.3
2.5
2.7
2.9
3.1
3.3
3.5
3.7
3.9
0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 0.41backoff rate
er
ro
r r
ate
word trigram model
ACM
 
Figure 4: CER vs. backoff rate. 
                                                     
4  The backoff rates are estimated using the baseline 
trigram model, so the choice could be biased against the 
word trigram model. 
P(Tuesday | party on) = P(WEEKDAY | party on)? 
P(Tuesday | party on WEEKDAY). 
0.25
0.27
0.29
0.31
0.33
0.35
0.37
0.39
0.41
0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42
backoff rate
er
ro
r r
ate
 di
ffe
re
nc
e
 
Figure 5: CER difference vs. backoff rate. 
5 Conclusion 
There are three main contributions of this paper. 
First, after presenting a formal definition of the 
ACM, we described in detail the methodology of 
constructing the ACM effectively. We showed 
empirically that both the asymmetric clustering and 
the parameter optimization (i.e. optimal cluster 
numbers) have positive impacts on the performance 
of the resulting ACM.  The finding demonstrates 
partially the effectiveness of our research focus: 
techniques for using clusters (i.e. the ACM) rather 
than techniques for finding clusters (i.e. clustering 
algorithms).  Second, we explored the actual 
representation of the ACM and evaluate it on a 
realistic application ? Japanese Kana-Kanji 
conversion.  Results show approximately 6-10% 
CER reduction of the ACMs in comparison with the 
word trigram models, even when the ACMs are 
slightly smaller.  Third, the reasons underlying the 
superiority of the ACM are analyzed. For instance, 
our analysis suggests the benefit of the ACM comes 
partially from its better structure and its better 
smoothing. 
All cluster models discussed in this paper are 
based on hard clustering, meaning that each word 
belongs to only one cluster. One area we have not 
explored is the use of soft clustering, where a word w 
can be assigned to multiple clusters W with a 
probability P(W|w) [Pereira et al, 1993]. Saul and 
Pereira [1997] demonstrated the utility of soft 
clustering and concluded that any method that 
assigns each word to a single cluster would lose 
information. It is an interesting question whether our 
techniques for hard clustering can be extended to 
soft clustering.  On the other hand, soft clustering 
models tend to be larger than hard clustering models 
because a given word can belong to multiple 
clusters, and thus a training instance P(wi|wi-2wi-1) 
can lead to multiple counts instead of just 1.  
References 
Bai, S., Li, H., Lin, Z., and Yuan, B. (1998). Building 
class-based language models with contextual statistics. In 
ICASSP-98, pp. 173-176. 
Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N. 
B., and Naik, D. (1996). A novel word clustering algorithm 
based on latent semantic analysis. In ICASSP-96.  
Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., 
Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. 
(1990). A statistical approach to machine translation. 
Computational Linguistics, 16(2), pp. 79-85. 
Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., and 
Mercer, R. L. (1992). Class-based n-gram models of natural 
language. Computational Linguistics, 18(4), pp. 467-479. 
Clarkson, P. R., and Rosenfeld, R. (1997). Statistical language 
modeling using the CMU-Cambridge toolkit. In Eurospeech 
1997, Rhodes, Greece. 
Gao, J. Goodman, J. and Miao, J. (2001). The use of clustering 
techniques for language model ? application to Asian 
language. Computational Linguistics and Chinese Language 
Processing. Vol. 6, No. 1, pp 27-60. 
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a 
unified approach to statistical language modeling for Chinese. 
ACM Transactions on Asian Language Information 
Processing. Vol. 1, No. 1, pp 3-33. 
Goodman, J. (2001). A bit of progress in language modeling.  In 
Computer Speech and Language, October 2001, pp 403-434. 
Goodman, J., and Gao, J. (2000). Language model size 
reduction by predictive clustering. ICSLP-2000, Beijing. 
Jelinek, F. (1990). Self-organized language modeling for speech 
recognition. In Readings in Speech Recognition, A. Waibel 
and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp. 
450-506. 
Katz, S. M. (1987). Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE Transactions on Acoustics, Speech and Signal 
Processing, ASSP-35(3):400-401, March. 
Kneser, R. and Ney, H. (1993).  Improved clustering techniques 
for class-based statistical language modeling. In Eurospeech, 
Vol. 2, pp. 973-976, Berlin, Germany. 
Ney, H., Essen, U., and Kneser, R. (1994). On structuring 
probabilistic dependences in stochastic language modeling. 
Computer, Speech, and Language, 8:1-38. 
Pereira, F., Tishby, N., and Lee L. (1993). Distributional 
clustering of English words. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Rosenfeld, R. (2000). Two decades of statistical language 
modeling: where do we go from here. In Proceeding of the 
IEEE, 88:1270-1278, August. 
Saul, L., and Pereira, F.C.N. (1997). Aggregate and mixed-order 
Markov models for statistical language processing. In 
EMNLP-1997. 
Stolcke, A. (1998). Entropy-based Pruning of Backoff 
Language Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270-274. 
Ueberla, J. P. (1996). An extended clustering algorithm for 
statistical language models. IEEE Transactions on Speech 
and Audio Processing, 4(4): 313-316. 
Yamamoto, H., Isogai, S., and Sagisaka, Y. (2001). Multi-Class 
Composite N-gram Language Model for Spoken Language 
Processing Using Multiple Word Clusters. 39th Annual 
meetings of the Association for Computational Linguistics 
(ACL?01), Toulouse, 6-11 July 2001. 
Yamamoto, H., and Sagisaka, Y. (1999). Multi-class Composite 
N-gram based on Connection Direction, In Proceedings of the 
IEEE International Conference on Acoustics, Speech and 
Signal Processing, May, Phoenix, Arizona. 
Improved Source-Channel Models for Chinese Word Segmentation1  
Jianfeng Gao, Mu Li and Chang-Ning Huang  
Microsoft Research, Asia  
Beijing 100080, China  
{jfgao, t-muli, cnhuang}@microsoft.com 
                                                   
1
 We would like to thank Ashley Chang, Jian-Yun Nie, Andi Wu and Ming Zhou for many useful discussions, and for comments on 
earlier versions of this paper. We would also like to thank Xiaoshan Fang, Jianfeng Li, Wenfeng Yang and Xiaodan Zhu for their 
help with evaluating our system. 
Abstract 
This paper presents a Chinese word segmen-
tation system that uses improved source- 
channel models of Chinese sentence genera-
tion. Chinese words are defined as one of the 
following four types: lexicon words, mor-
phologically derived words, factoids, and 
named entities. Our system provides a unified 
approach to the four fundamental features of 
word-level Chinese language processing: (1) 
word segmentation, (2) morphological analy-
sis, (3) factoid detection, and (4) named entity 
recognition. The performance of the system is 
evaluated on a manually annotated test set, 
and is also compared with several state-of- 
the-art systems, taking into account the fact 
that the definition of Chinese words often 
varies from system to system. 
1 Introduction 
Chinese word segmentation is the initial step of 
many Chinese language processing tasks, and has 
attracted a lot of attention in the research commu-
nity. It is a challenging problem due to the fact that 
there is no standard definition of Chinese words.  
In this paper, we define Chinese words as one of 
the following four types: entries in a lexicon, mor-
phologically derived words, factoids, and named 
entities. We then present a Chinese word segmen-
tation system which provides a solution to the four 
fundamental problems of word-level Chinese lan-
guage processing: word segmentation, morpho-
logical analysis, factoid detection, and named entity 
recognition (NER). 
There are no word boundaries in written Chinese 
text. Therefore, unlike English, it may not be de-
sirable to separate the solution to word segmenta-
tion from the solutions to the other three problems. 
Ideally, we would like to propose a unified ap-
proach to all the four problems. The unified ap-
proach we used in our system is based on the im-
proved source-channel models of Chinese sentence 
generation, with two components: a source model 
and a set of channel models. The source model is 
used to estimate the generative probability of a 
word sequence, in which each word belongs to one 
word type. For each word type, a channel model is 
used to estimate the generative probability of a 
character string given the word type. So there are 
multiple channel models. We shall show in this 
paper that our models provide a statistical frame-
work to corporate a wide variety linguistic knowl-
edge and statistical models in a unified way.  
We evaluate the performance of our system us-
ing an annotated test set. We also compare our 
system with several state-of-the-art systems, taking 
into account the fact that the definition of Chinese 
words often varies from system to system. 
In the rest of this paper: Section 2 discusses 
previous work. Section 3 gives the detailed defini-
tion of Chinese words. Sections 4 to 6 describe in 
detail the improved source-channel models. Section 
8 describes the evaluation results. Section 9 pre-
sents our conclusion.  
2 Previous Work 
Many methods of Chinese word segmentation have 
been proposed: reviews include (Wu and Tseng, 
1993; Sproat and Shih, 2001). These methods can 
be roughly classified into dictionary-based methods 
and statistical-based methods, while many state-of- 
the-art systems use hybrid approaches. 
In dictionary-based methods (e.g. Cheng et al, 
1999), given an input character string, only words 
that are stored in the dictionary can be identified. 
The performance of these methods thus depends to 
a large degree upon the coverage of the dictionary, 
which unfortunately may never be complete be-
cause new words appear constantly. Therefore, in 
addition to the dictionary, many systems also con-
tain special components for unknown word identi-
fication. In particular, statistical methods have been 
widely applied because they utilize a probabilistic 
or cost-based scoring mechanism, instead of the 
dictionary, to segment the text. These methods 
however, suffer from three drawbacks. First, some 
of these methods (e.g. Lin et al, 1993) identify 
unknown words without identifying their types. For 
instance, one would identify a string as a unit, but 
not identify whether it is a person name. This is not 
always sufficient. Second, the probabilistic models 
used in these methods (e.g. Teahan et al, 2000) are 
trained on a segmented corpus which is not always 
available. Third, the identified unknown words are 
likely to be linguistically implausible (e.g. Dai et al, 
1999), and additional manual checking is needed 
for some subsequent tasks such as parsing. 
We believe that the identification of unknown 
words should not be defined as a separate problem 
from word segmentation. These two problems are 
better solved simultaneously in a unified approach. 
One example of such approaches is Sproat et al 
(1996), which is based on weighted finite-state 
transducers (FSTs). Our approach is motivated by 
the same inspiration, but is based on a different 
mechanism: the improved source-channel models. 
As we shall see, these models provide a more 
flexible framework to incorporate various kinds of 
lexical and statistical information. Some types of 
unknown words that are not discussed in Sproat?s 
system are dealt with in our system. 
3 Chinese Words 
There is no standard definition of Chinese words ? 
linguists may define words from many aspects (e.g. 
Packard, 2000), but none of these definitions will 
completely line up with any other. Fortunately, this 
may not matter in practice because the definition 
that is most useful will depend to a large degree 
upon how one uses and processes these words. 
We define Chinese words in this paper as one of 
the following four types: (1) entries in a lexicon 
(lexicon words below), (2) morphologically derived 
words, (3) factoids, and (4) named entities, because 
these four types of words have different function-
alities in Chinese language processing, and are 
processed in different ways in our system. For 
example, the plausible word segmentation for the 
sentence in Figure 1(a) is as shown. Figure 1(b) is 
the output of our system, where words of different 
types are processed in different ways: 
(a) ???/??????/????/?/???/??/?/
?? (Friends happily go to professor Li Junsheng?s 
home for lunch at twelve thirty.) 
(b) [??+? MA_S] [?????? 12:30 TIME] [??
MR_AABB] [?] [??? PN] [??] [?] [??] 
Figure 1: (a) A Chinese sentence. Slashes indicate word 
boundaries. (b) An output of our word segmentation system. 
Square brackets indicate word boundaries. + indicates a 
morpheme boundary. 
? For lexicon words, word boundaries are de-
tected. 
? For morphologically derived words, their 
morphological patterns are detected, e.g. ??
? ?friend+s? is derived by affixation of the 
plural affix ? to the noun ?? (MA_S in-
dicates a suffixation pattern), and ???? 
?happily? is a reduplication of ?? ?happy? 
(MR_AABB indicates an AABB reduplica-
tion pattern). 
? For factoids, their types and normalized 
forms are detected, e.g. 12:30 is the normal-
ized form of the time expression ?????
? (TIME indicates a time expression). 
? For named entities, their types are detected, 
e.g. ??? ?Li Junsheng? is a person name 
(PN indicates a person name). 
In our system, we use a unified approach to de-
tecting and processing the above four types of 
words. This approach is based on the improved 
source-channel models described below. 
4 Improved Source-Channel Models 
Let S be a Chinese sentence, which is a character 
string. For all possible word segmentations W, we 
will choose the most likely one W* which achieves 
the highest conditional probability P(W|S): W* = 
argmaxw P(W|S). According to Bayes? decision rule 
and dropping the constant denominator, we can 
equivalently perform the following maximization: 
)|()(maxarg* WSPWPW
W
=
. 
(1) 
Following the Chinese word definition in Section 3, 
we define word class C as follows: (1) Each lexicon 
Word class Class model Linguistic Constraints 
Lexicon word (LW) P(S|LW)=1 if S forms a word lexicon 
entry, 0 otherwise.  
Word lexicon 
Morphologically derived word 
(MW) 
P(S|MW)=1 if S forms a morph lexicon 
entry, 0 otherwise.  
Morph-lexicon 
Person name (PN) Character bigram  family name list, Chinese PN patterns 
Location name (LN) Character bigram  LN keyword list, LN lexicon, LN abbr. list 
Organization name (ON) Word class bigram ON keyword list, ON abbr. list 
Transliteration names (FN) Character bigram transliterated name character list 
Factoid2 (FT) P(S|FT)=1 if S can be parsed using a 
factoid grammar G, 0 otherwise 
Factoid rules (presented by FSTs). 
Figure 2. Class models 
                                                   
2
 In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone 
number, and WWW. 
word is defined as a class; (2) each morphologically 
derived word is defined as a class; (3) each type of 
factoids is defined as a class, e.g. all time expres-
sions belong to a class TIME; and (4) each type of 
named entities is defined as a class, e.g. all person 
names belong to a class PN. We therefore convert 
the word segmentation W into a word class se-
quence C. Eq. 1 can then be rewritten as: 
)|()(maxarg* CSPCPC
C
=
. 
(2) 
Eq. 2 is the basic form of the source-channel models 
for Chinese word segmentation. The models assume 
that a Chinese sentence S is generated as follows: 
First, a person chooses a sequence of concepts (i.e., 
word classes C) to output, according to the prob-
ability distribution P(C); then the person attempts to 
express each concept by choosing a sequence of 
characters, according to the probability distribution 
P(S|C).  
The source-channel models can be interpreted in 
another way as follows: P(C) is a stochastic model 
estimating the probability of word class sequence. It 
indicates, given a context, how likely a word class 
occurs. For example, person names are more likely 
to occur before a title such as ?? ?professor?. So 
P(C) is also referred to as context model afterwards. 
P(S|C) is a generative model estimating how likely 
a character string is generated given a word class. 
For example, the character string ??? is more 
likely to be a person name than ??? ?Li Jun-
sheng? because ? is a common family name in 
China while ? is not. So P(S|C) is also referred to 
as class model afterwards. In our system, we use the 
improved source-channel models, which contains 
one context model (i.e., a trigram language model in 
our case) and a set of class models of different types, 
each of which is for one class of words, as shown in 
Figure 2. 
Although Eq. 2 suggests that class model prob-
ability and context model probability can be com-
bined through simple multiplication, in practice 
some weighting is desirable. There are two reasons. 
First, some class models are poorly estimated, 
owing to the sub-optimal assumptions we make for 
simplicity and the insufficiency of the training 
corpus. Combining the context model probability 
with poorly estimated class model probabilities 
according to Eq. 2 would give the context model too 
little weight. Second, as seen in Figure 2, the class 
models of different word classes are constructed in 
different ways (e.g. name entity models are n-gram 
models trained on corpora, and factoid models are 
compiled using linguistic knowledge). Therefore, 
the quantities of class model probabilities are likely 
to have vastly different dynamic ranges among 
different word classes. One way to balance these 
probability quantities is to add several class model 
weight CW, each for one word class, to adjust the 
class model probability P(S|C) to P(S|C)CW. In our 
experiments, these class model weights are deter-
mined empirically to optimize the word segmenta-
tion performance on a development set. 
Given the source-channel models, the procedure 
of word segmentation in our system involves two 
steps: First, given an input string S, all word can-
didates are generated (and stored in a lattice). Each 
candidate is tagged with its word class and the class 
model probability P(S?|C), where S? is any substring 
of S. Second, Viterbi search is used to select (from 
the lattice) the most probable word segmentation 
(i.e. word class sequence C*) according to Eq. (2). 
5 Class Model Probabilities 
Given an input string S, all class models in Figure 2 
are applied simultaneously to generate word class 
candidates whose class model probabilities are 
assigned using the corresponding class models: 
? Lexicon words: For any substring S? ? S, we 
assume P(S?|C) = 1 and tagged the class as 
lexicon word if S? forms an entry in the word 
lexicon, P(S?|C) = 0 otherwise. 
? Morphologically derived words: Similar to 
lexicon words, but a morph-lexicon is used 
instead of the word lexicon (see Section 5.1). 
? Factoids: For each type of factoid, we compile 
a set of finite-state grammars G, represented as 
FSTs. For all S? ? S, if it can be parsed using G, 
we assume P(S?|FT) = 1, and tagged S? as a 
factoid candidate. As the example in Figure 1 
shows, ?????? is a factoid (time) can-
didate with the class model probability P(??
????|TIME) =1, and ?? and ?? are 
also factoid (number) candidates, with P(??
|NUM) = P(??|NUM) =1 
? Named entities: For each type of named enti-
ties, we use a set of grammars and statistical 
models to generate candidates as described in 
Section 5.2. 
5.1 Morphologically derived words 
In our system, the morphologically derived words 
are generated using five morphological patterns: (1) 
affixation: ??? (friend - plural) ?friends?; (2) 
reduplication: ?? ?happy? ! ???? ?happily?; 
(3) merging: ?? ?on duty? + ?? ?off duty? !?
?? ?on-off duty?; (4) head particle (i.e. expres-
sions that are verb + comp): ? ?walk? + ?? ?out? 
! ??? ?walk out?; and (5) split (i.e. a set of 
expressions that are separate words at the syntactic 
level but single words at the semantic level): ??? 
?already ate?, where the bi-character word ?? ?eat? 
is split by the particle ? ?already?. 
It is difficult to simply extend the well-known 
techniques for English (i.e., finite-state morphology) 
to Chinese due to two reasons. First, Chinese mor-
morphological rules are not as ?general? as their 
English counterparts. For example, English plural 
nouns can be in general generated using the rule 
?noun + s ! plural noun?. But only a small subset of 
Chinese nouns can be pluralized (e.g. ???) using 
its Chinese counterpart ?noun + ? ! plural noun? 
whereas others (e.g. ??  ?pumpkins?) cannot. 
Second, the operations required by Chinese mor-
phological analysis such as copying in reduplication, 
merging and splitting, cannot be implemented using 
the current finite-state networks3.  
Our solution is the extended lexicalization. We 
simply collect all morphologically derived word 
forms of the above five types and incorporate them 
into the lexicon, called morph lexicon. The proce-
dure involves three steps: (1) Candidate genera-
tion. It is done by applying a set of morphological 
rules to both the word lexicon and a large corpus. 
For example, the rule ?noun + ? ! plural noun? 
would generate candidates like ???. (2) Statis-
tical filtering. For each candidate, we obtain a set 
of statistical features such as frequency, mutual 
information, left/right context dependency from a 
large corpus. We then use an information gain-like 
metric described in (Chien, 1997; Gao et al, 2002) 
to estimate how likely a candidate is to form a 
morphologically derived word, and remove ?bad? 
candidates. The basic idea behind the metric is that 
a Chinese word should appear as a stable sequence 
in the corpus. That is, the components within the 
word are strongly correlated, while the components 
at both ends should have low correlations with 
words outside the sequence. (3) Linguistic selec-
tion. We finally manually check the remaining 
candidates, and construct the morph-lexicon, where 
each entry is tagged by its morphological pattern. 
5.2 Named entities 
We consider four types of named entities: person 
names (PN), location names (LN), organization 
names (ON), and transliterations of foreign names 
(FN). Because any character strings can be in prin-
ciple named entities of one or more types, to limit 
the number of candidates for a more effective 
search, we generate named entity candidates, given 
an input string, in two steps: First, for each type, we 
use a set of constraints (which are compiled by 
                                                   
3
 Sproat et al (1996) also studied such problems (with the same 
example) and uses weighted FSTs to deal with the affixation. 
linguists and are represented as FSTs) to generate 
only those ?most likely? candidates. Second, each of 
the generated candidates is assigned a class model 
probability. These class models are defined as 
generative models which are respectively estimated 
on their corresponding named entity lists using 
maximum likelihood estimation (MLE), together 
with smoothing methods4. We will describe briefly 
the constraints and the class models below. 
5.2.1 Chinese person names 
There are two main constraints. (1) PN patterns: We 
assume that a Chinese PN consists of a family name 
F and a given name G, and is of the pattern F+G. 
Both F and G are of one or two characters long. (2) 
Family name list: We only consider PN candidates 
that begin with an F stored in the family name list 
(which contains 373 entries in our system). 
Given a PN candidate, which is a character 
string S?, the class model probability P(S?|PN) is 
computed by a character bigram model as follows: 
(1) Generate the family name sub-string SF, with the 
probability P(SF|F); (2) Generate the given name 
sub-string SG, with the probability P(SG|G) (or 
P(SG1|G1)); and (3) Generate the second given name, 
with the probability P(SG2|SG1,G2). For example, the 
generative probability of the string ??? given 
that it is a PN would be estimated as P(???|PN) 
= P(?|F)P(?|G1)P(?|?,G2). 
5.2.2 Location names 
Unlike PNs, there are no patterns for LNs. We 
assume that a LN candidate is generated given S? 
(which is less than 10 characters long), if one of the 
following conditions is satisfied: (1) S? is an entry in 
the LN list (which contains 30,000 LNs); (2) S? ends 
in a keyword in a 120-entry LN keyword list such as 
? ?city?5. The probability P(S?|LN) is computed by 
a character bigram model.  
Consider a string ???? ?Wusuli river?. It is a 
LN candidate because it ends in a LN keyword ? 
?river?. The generative probability of the string 
given it is a LN would be estimated as P(????
|LN) = P(? |<LN>) P(?|?) P(? |?) P(? |?) 
                                                   
4
 The detailed description of these models are in Sun et al 
(2002), which also describes the use of cache model and the 
way the abbreviations of LN and ON are handled. 
5
 For a better understanding, the constraint is a simplified 
version of that used in our system. 
P(</LN>|?), where <LN> and </LN> are symbols 
denoting the beginning and the end of a LN, re-
spectively. 
5.2.3 Organization names 
ONs are more difficult to identify than PNs and LNs 
because ONs are usually nested named entities. 
Consider an ON ????????  ?Air China 
Corporation?; it contains an LN ?? ?China?. 
Like the identification of LNs, an ON candidate 
is only generated given a character string S? (less 
than 15 characters long), if it ends in a keyword in a 
1,355-entry ON keyword list such as ?? ?corpo-
ration?. To estimate the generative probability of a 
nested ON, we introduce word class segmentations 
of S?, C, as hidden variables. In principle, the ON 
class model recovers P(S?|ON) over all possible C: 
P(S?|ON) = ?CP(S?,C|ON) = ?CP(C|ON)P(S?|C, 
ON). Since P(S?|C,ON) = P(S?|C), we have P(S?|ON) 
= ?CP(C|ON) P(S?|C). We then assume that the 
sum is approximated by a single pair of terms 
P(C*|ON)P(S?|C*), where C* is the most probable 
word class segmentation discovered by Eq. 2. That 
is, we also use our system to find C*, but the source- 
channel models are estimated on the ON list. 
Consider the earlier example. Assuming that C* 
= LN/??/??/??, where ?? is tagged as a LN, 
the probability P(S?|ON) would be estimated using a 
word class bigram model as: P(????????
|ON) ? P(LN/??/??/??|ON) P(??|LN) =  
P(LN|<ON>)P(??|LN)P(??|??)P(??|??) 
P(</ON>|??)P(??|LN), where P(??|LN) is 
the class model probability of ?? given that it is a 
LN, <ON> and </ON> are symbols denoting the 
beginning and the end of a ON, respectively.  
5.2.4 Transliterations of foreign names 
As described in Sproat et al (1996): FNs are usually 
transliterated using Chinese character strings whose 
sequential pronunciation mimics the source lan-
guage pronunciation of the name. Since FNs can be 
of any length and their original pronunciation is 
effectively unlimited, the recognition of such names 
is tricky. Fortunately, there are only a few hundred 
Chinese characters that are particularly common in 
transliterations. 
Therefore, an FN candidate would be generated 
given S?, if it contains only characters stored in a 
transliterated name character list (which contains 
618 Chinese characters). The probability P(S?|FN) 
is estimated using a character bigram model. Notice 
that in our system a FN can be a PN, a LN, or an ON, 
depending on the context. Then, given a FN can-
didate, three named entity candidates, each for one 
category, are generated in the lattice, with the class 
probabilities P(S?|PN)=P(S?|LN)=P(S?|ON)= 
P(S?|FN). In other words, we delay the determina-
tion of its type until decoding where the context 
model is used. 
6 Context Model Estimation  
This section describes the way the class model 
probability P(C) (i.e. trigram probability) in Eq. 2 is 
estimated. Ideally, given an annotated corpus, 
where each sentence is segmented into words which 
are tagged by their classes, the trigram word class 
probabilities can be calculated using MLE, together 
with a backoff schema (Katz, 1987) to deal with the 
sparse data problem. Unfortunately, building such 
annotated training corpora is very expensive. 
Our basic solution is the bootstrapping approach 
described in Gao et al (2002). It consists of three 
steps: (1) Initially, we use a greedy word segmen-
tor6 to annotate the corpus, and obtain an initial 
context model based on the initial annotated corpus; 
(2) we re-annotate the corpus using the obtained 
models; and (3) re-train the context model using the 
re-annotated corpus. Steps 2 and 3 are iterated until 
the performance of the system converges. 
In the above approach, the quality of the context 
model depends to a large degree upon the quality of 
the initial annotated corpus, which is however not 
satisfied due to two problems. First, the greedy 
segmentor cannot deal with the segmentation am-
biguities, and even after iterations, these ambigui-
ties can only be partially resolved. Second, many 
factoids and named entities cannot be identified 
using the greedy word segmentor which is based on 
the dictionary. 
To solve the first problem, we use two methods 
to resolve segmentation ambiguities in the initial 
segmented training data. We classify word seg-
mentation ambiguities into two classes: overlap 
ambiguity (OA), and combination ambiguity (CA). 
Consider a character string ABC, if it can be seg-
                                                   
6
 The greedy word segmentor is based on a forward maximum 
matching (FMM) algorithm: It processes through the sentence 
from left to right, taking the longest match with the lexicon 
entry at each point. 
mented into two words either as AB/C or A/BC 
depending on different context, ABC is called an 
overlap ambiguity string (OAS). If a character 
string AB can be segmented either into two words, 
A/B, or as one word depending on different context. 
AB is called a combination ambiguity string (CAS). 
To resolve OA, we identify all OASs in the training 
data and replace them with a single token <OAS>. 
By doing so, we actually remove the portion of 
training data that are likely to contain OA errors. To 
resolve CA, we select 70 high-frequent two-char-
acter CAS (e.g. ?? ?talent? and ?/? ?just able?). 
For each CAS, we train a binary classifier (which is 
based on vector space models) using sentences that 
contains the CAS segmented manually. Then for 
each occurrence of a CAS in the initial segmented 
training data, the corresponding classifier is used to 
determine whether or not the CAS should be seg-
mented. 
For the second problem, though we can simply 
use the finite-state machines described in Section 5 
(extended by using the longest-matching constraint 
for disambiguation) to detect factoids in the initial 
segmented corpus, our method of NER in the initial 
step (i.e. step 1) is a little more complicated. First, 
we manually annotate named entities on a small 
subset (call seed set) of the training data. Then, we 
obtain a context model on the seed set (called seed 
model). We thus improve the context model which 
is trained on the initial annotated training corpus by 
interpolating it with the seed model. Finally, we use 
the improved context model in steps 2 and 3 of the 
bootstrapping. Our experiments show that a rela-
tively small seed set (e.g., 10 million characters, 
which takes approximately three weeks for 4 per-
sons to annotate the NE tags) is enough to get a 
good improved context model for initialization. 
7 Evaluation 
To conduct a reliable evaluation, a manually anno-
tated test set was developed. The text corpus con-
tains approximately half million Chinese characters 
that have been proofread and balanced in terms of 
domain, styles, and times. Before we annotate the 
corpus, several questions have to be answered: (1) 
Does the segmentation depend on a particular 
lexicon? (2) Should we assume a single correct 
segmentation for a sentence? (3) What are the 
evaluation criteria? (4) How to perform a fair 
comparison across different systems? 
Word  
segmentation Factoid PN LN ON 
 
System 
P% R% P% R% P% R% P% R% P% R% 
1 FMM 83.7 92.7         
2 Baseline 84.4 93.8         
3 2 + Factoid 89.9 95.5 84.4 80.0       
4 3 + PN 94.1 96.7 84.5 80.0 81.0 90.0     
5 4 + LN 94.7 97.0 84.5 80.0 86.4 90.0 79.4 86.0   
6 5 + ON 96.3 97.4 85.2 80.0 87.5 90.0 89.2 85.4 81.4 65.6 
Table 1: system results 
As described earlier, it is more useful to define 
words depending on how the words are used in real 
applications. In our system, a lexicon (containing 
98,668 lexicon words and 59,285 morphologically 
derived words) has been constructed for several 
applications, such as Asian language input and web 
search. Therefore, we annotate the text corpus based 
on the lexicon. That is, we segment each sentence as 
much as possible into words that are stored in our 
lexicon, and tag only the new words, which other-
wise would be segmented into strings of one 
-character words. When there are multiple seg-
mentations for a sentence, we keep only one that 
contains the least number of words. The annotated 
test set contains in total 247,039 tokens (including 
205,162 lexicon/morph-lexicon words, 4,347 PNs, 
5,311 LNs, 3,850 ONs, and 6,630 factoids, etc.) 
Our system is measured through multiple preci-
sion-recall (P/R) pairs, and F-measures (F?=1, which 
is defined as 2PR/(P+R)) for each word class. Since 
the annotated test set is based on a particular lexicon, 
some of the evaluation measures are meaningless 
when we compare our system to other systems that 
use different lexicons. So in comparison with dif-
ferent systems, we consider only the preci-
sion-recall of NER and the number of OAS errors 
(i.e. crossing brackets) because these measures are 
lexicon independent and there is always a single 
unambiguous answer. 
The training corpus for context model contains 
approximately 80 million Chinese characters from 
various domains of text such as newspapers, novels, 
magazines etc. The training corpora for class mod-
els are described in Section 5. 
7.1 System results 
Our system is designed in the way that components 
such as factoid detector and NER can be ?switched 
on or off?, so that we can investigate the relative 
contribution of each component to the overall word 
segmentation performance.  
The main results are shown in Table 1. For 
comparison, we also include in the table (Row 1) 
the results of using the greedy segmentor (FMM) 
described in Section 6. Row 2 shows the baseline 
results of our system, where only the lexicon is used. 
It is interesting to find, in Rows 1 and 2, that the 
dictionary-based methods already achieve quite 
good recall, but the precisions are not very good 
because they cannot identify correctly unknown 
words that are not in the lexicon such factoids and 
named entities. We also find that even using the 
same lexicon, our approach that is based on the 
improved source-channel models outperforms the 
greedy approach (with a slight but statistically 
significant different i.e., P < 0.01 according to the t 
test) because the use of context model resolves 
more ambiguities in segmentation. The most 
promising property of our approach is that the 
source-channel models provide a flexible frame-
work where a wide variety of linguistic knowledge 
and statistical models can be combined in a unified 
way. As shown in Rows 3 to 6, when components 
are switched on in turn by activating corresponding 
class models, the overall word segmentation per-
formance increases consistently. 
We also conduct an error analysis, showing that 
86.2% of errors come from NER and factoid detec-
tion, although the tokens of these word types consist 
of only 8.7% of all that are in the test set. 
7.2 Comparison with other systems 
We compare our system ? henceforth SCM, with 
other two Chinese word segmentation systems7: 
 
                                                   
7
 Although the two systems are widely accessible in mainland 
China, to our knowledge no standard evaluations on Chinese 
word segmentation of the two systems have been published by 
press time. More comprehensive comparisons (with other well- 
known systems) and detailed error analysis form one area of 
our future work.  
LN PN ON System # OAS 
Errors P % R % F?=1 P % R % F?=1 P % R % F?=1 
MSWS 63 93.5 44.2 60.0 90.7 74.4 81.8 64.2 46.9 60.0 
LCWS 49 85.4 72.0 78.2 94.5 78.1 85.6 71.3 13.1 22.2 
SCM 7 87.6 86.4 87.0 83.0 89.7 86.2 79.9 61.7 69.6 
Table 2. Comparison results 
1. The MSWS system is one of the best available 
products. It is released by Microsoft? (as a set 
of Windows APIs). MSWS first conducts the 
word breaking using MM (augmented by heu-
ristic rules for disambiguation), then conducts 
factoid detection and NER using rules. 
2. The LCWS system is one of the best research 
systems in mainland China. It is released by 
Beijing Language University. The system 
works similarly to MSWS, but has a larger 
dictionary containing more PNs and LNs. 
As mentioned above, to achieve a fair comparison, 
we compare the above three systems only in terms 
of NER precision-recall and the number of OAS 
errors. However, we find that due to the different 
annotation specifications used by these systems, it 
is still very difficult to compare their results auto-
matically. For example, ????? ?Beijing city 
government? has been segmented inconsistently as 
???/?? ?Beijing city? + ?government? or ??/
??? ?Beijing? + ?city government? even in the 
same system. Even worse, some LNs tagged in one 
system are tagged as ONs in another system. 
Therefore, we have to manually check the results. 
We picked 933 sentences at random containing 
22,833 words (including 329 PNs, 617 LNs, and 
435 ONs) for testing. We also did not differentiate 
LNs and ONs in evaluation. That is, we only 
checked the word boundaries of LNs and ONs and 
treated both tags exchangeable. The results are 
shown in Table 2. We can see that in this small test 
set SCM achieves the best overall performance of 
NER and the best performance of resolving OAS. 
8 Conclusion 
The contributions of this paper are three-fold. First, 
we formulate the Chinese word segmentation 
problem as a set of correlated problems, which are 
better solved simultaneously, including word 
breaking, morphological analysis, factoid detection 
and NER. Second, we present a unified approach to 
these problems using the improved source-channel 
models. The models provide a simple statistical 
framework to incorporate a wide variety of linguis-
tic knowledge and statistical models in a unified 
way. Third, we evaluate the system?s performance 
on an annotated test set, showing very promising 
results. We also compare our system with several 
state-of-the-art systems, taking into account the fact 
that the definition of Chinese words varies from 
system to system. Given the comparison results, we 
can say with confidence that our system achieves at 
least the performance of state-of-the-art word seg-
mentation systems. 
References 
Cheng, Kowk-Shing, Gilbert H. Yong and Kam-Fai Wong. 
1999. A study on word-based and integral-bit Chinese text 
compression algorithms. JASIS, 50(3): 218-228. 
Chien, Lee-Feng. 1997. PAT-tree-based keyword extraction for 
Chinese information retrieval. In SIGIR97, 27-31. 
Dai, Yubin, Christopher S. G. Khoo and Tech Ee Loh. 1999. A 
new statistical formula for Chinese word segmentation in-
corporating contextual information. SIGIR99, 82-89. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and Kai-Fu Lee. 
2002. Toward a unified approach to statistical language 
modeling for Chinese. ACM TALIP, 1(1): 3-33.  
Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yi Su. 1993. A 
preliminary study on unknown word problem in Chinese 
word segmentation. ROCLING 6, 119-141. 
Katz, S. M. 1987. Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE ASSP 35(3):400-401. 
Packard, Jerome. 2000. The morphology of Chinese: A Lin-
guistics and Cognitive Approach. Cambridge University 
Press, Cambridge. 
Sproat, Richard and Chilin Shih. 2002. Corpus-based methods 
in Chinese morphology and phonology. In: COOLING 2002.  
Sproat, Richard, Chilin Shih, William Gale and Nancy Chang. 
1996. A stochastic finite-state word-segmentation algorithm 
for Chinese. Computational Linguistics. 22(3): 377-404. 
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou and 
Chang-Ning Huang. 2002. Chinese named entity identifica-
tion using class-based language model. In: COLING 2002. 
Teahan, W. J., Yingying Wen, Rodger McNad and Ian Witten. 
2000. A compression-based algorithm for Chinese word 
segmentation. Computational Linguistics, 26(3): 375-393. 
Wu, Zimin and Gwyneth Tseng. 1993. Chinese text segmenta-
tion for text retrieval achievements and problems. JASIS, 
44(9): 532-542. 
Unsupervised Learning of Dependency Structure for Language Modeling  
Jianfeng Gao  
Microsoft Research, Asia  
49 Zhichun Road, Haidian District 
Beijing 100080 China 
jfgao@microsoft.com  
Hisami Suzuki  
Microsoft Research 
One Microsoft Way 
 Redmond WA 98052 USA  
hisamis@microsoft.com 
 
Abstract 
This paper presents a dependency language 
model (DLM) that captures linguistic con-
straints via a dependency structure, i.e., a set 
of probabilistic dependencies that express 
the relations between headwords of each 
phrase in a sentence by an acyclic, planar, 
undirected graph. Our contributions are 
three-fold. First, we incorporate the de-
pendency structure into an n-gram language 
model to capture long distance word de-
pendency. Second, we present an unsuper-
vised learning method that discovers the 
dependency structure of a sentence using a 
bootstrapping procedure. Finally, we 
evaluate the proposed models on a realistic 
application (Japanese Kana-Kanji conver-
sion). Experiments show that the best DLM 
achieves an 11.3% error rate reduction over 
the word trigram model. 
1 Introduction 
In recent years, many efforts have been made to 
utilize linguistic structure in language modeling, 
which for practical reasons is still dominated by 
trigram-based language models. There are two 
major obstacles to successfully incorporating lin-
guistic structure into a language model: (1) captur-
ing longer distance word dependencies leads to 
higher-order n-gram models, where the number of 
parameters is usually too large to estimate; (2) 
capturing deeper linguistic relations in a language 
model requires a large annotated training corpus 
and a decoder that assigns linguistic structure, 
which are not always available. 
This paper presents a new dependency language 
model (DLM) that captures long distance linguistic 
constraints between words via a dependency 
structure, i.e., a set of probabilistic dependencies 
that capture linguistic relations between headwords 
of each phrase in a sentence. To deal with the first 
obstacle mentioned above, we approximate 
long-distance linguistic dependency by a model that 
is similar to a skipping bigram model in which the 
prediction of a word is conditioned on exactly one 
other linguistically related word that lies arbitrarily 
far in the past. This dependency model is then in-
terpolated with a headword bigram model and a 
word trigram model, keeping the number of pa-
rameters of the combined model manageable. To 
overcome the second obstacle, we used an unsu-
pervised learning method that discovers the de-
pendency structure of a given sentence using an 
Expectation-Maximization (EM)-like procedure. In 
this method, no manual syntactic annotation is 
required, thereby opening up the possibility for 
building a language model that performs well on a 
wide variety of data and languages. The proposed 
model is evaluated using Japanese Kana-Kanji 
conversion, achieving significant error rate reduc-
tion over the word trigram model.   
2 Motivation 
A trigram language model predicts the next word 
based only on two preceding words, blindly dis-
carding any other relevant word that may lie three 
or more positions to the left. Such a model is likely 
to be linguistically implausible: consider the Eng-
lish sentence in Figure 1(a), where a trigram model 
would predict cried from next seat, which does not 
agree with our intuition. In this paper, we define a 
dependency structure of a sentence as a set of 
probabilistic dependencies that express linguistic 
relations between words in a sentence by an acyclic, 
planar graph, where two related words are con-
nected by an undirected graph edge (i.e., we do not 
differentiate the modifier and the head in a de-
pendency). The dependency structure for the sen-
tence in Figure 1(a) is as shown; a model that uses 
this dependency structure would predict cried from 
baby, in agreement with our intuition. 
 
 
(a) [A baby] [in the next seat] cried [throughout the flight] 
 
 
 
(b) [/] [/	] [
/	] [/] [] [/]
 
Figure 1. Examples of dependency structure. (a) A 
dependency structure of an English sentence. Square 
brackets indicate base NPs; underlined words are the 
headwords. (b) A Japanese equivalent of (a). Slashes 
demarcate morpheme boundaries; square brackets 
indicate phrases (bunsetsu).  
A Japanese sentence is typically divided into 
non-overlapping phrases called bunsetsu. As shown 
in Figure 1(b), each bunsetsu consists of one con-
tent word, referred to here as the headword H, and 
several function words F. Words (more precisely, 
morphemes) within a bunsetsu are tightly bound 
with each other, which can be adequately captured 
by a word trigram model. However, headwords 
across bunsetsu boundaries also have dependency 
relations with each other, as the diagrams in Figure 
1 show. Such long distance dependency relations 
are expected to provide useful and complementary 
information with the word trigram model in the task 
of next word prediction.  
In constructing language models for realistic 
applications such as speech recognition and Asian 
language input, we are faced with two constraints 
that we would like to satisfy: First, the model must 
operate in a left-to-right manner, because (1) the 
search procedures for predicting words that corre-
spond to the input acoustic signal or phonetic string 
work left to right, and (2) it can be easily combined 
with a word trigram model in decoding. Second, the 
model should be computationally feasible both in 
training and decoding. In the next section, we offer 
a DLM that satisfies both of these constraints.  
3 Dependency Language Model 
The DLM attempts to generate the dependency 
structure incrementally while traversing the sen-
tence left to right. It will assign a probability to 
every word sequence W and its dependency struc-
ture D. The probability assignment is based on an 
encoding of the (W, D) pair described below. 
Let W be a sentence of length n words to which 
we have prepended <s> and appended </s> so that 
w0 = <s>, and wn+1 = </s>. In principle, a language 
model recovers the probability of a sentence P(W) 
over all possible D given W by estimating the joint 
probability P(W, D): P(W) = ?D P(W, D). In prac-
tice, we used the so-called maximum approximation 
where the sum is approximated by a single term 
P(W, D*): 
? ??=
D
DWPDWPWP ),(),()( . (1) 
Here, D* is the most probable dependency structure 
of the sentence, which is generally discovered by 
maximizing P(W, D): 
D
DWPD ),(maxarg=? . (2) 
Below we restrict the discussion to the most prob-
able dependency structure of a given sentence, and 
simply use D to represent D*. In the remainder of 
this section, we first present a statistical dependency 
parser, which estimates the parsing probability at 
the word level, and generates D incrementally while 
traversing W left to right. Next, we describe the 
elements of the DLM that assign probability to each 
possible W and its most probable D, P(W, D). Fi-
nally, we present an EM-like iterative method for 
unsupervised learning of dependency structure.  
3.1 Dependency parsing 
The aim of dependency parsing is to find the most 
probable D of a given W by maximizing the prob-
ability P(D|W). Let D be a set of probabilistic de-
pendencies d, i.e. d ? D. Assuming that the de-
pendencies are independent of each other, we have 
?
?
=
Dd
WdPWDP )|()|(
 
(3) 
where P(d|W) is the dependency probability condi-
tioned by a particular sentence.1 It is impossible to 
estimate P(d|W) directly because the same sentence 
is very unlikely to appear in both training and test 
data. We thus approximated P(d|W) by P(d), and 
estimated the dependency probability from the 
training corpus. Let dij = (wi, wj) be the dependency 
                                                     
1
 The model in Equation (3) is not strictly probabilistic 
because it drops the probabilities of illegal dependencies 
(e.g., crossing dependencies).  
between wi and wj. The maximum likelihood esti-
mation (MLE) of P(dij) is given by  
),(
),,()(
ji
ji
ij
wwC
RwwC
dP =  
(4) 
where C(wi, wj, R) is the number of times wi and wj 
have a dependency relation in a sentence in training 
data, and C(wi, wj) is the number of times wi and wj 
are seen in the same sentence. To deal with the data 
sparseness problem of MLE, we used the backoff 
estimation strategy similar to the one proposed in 
Collins (1996), which backs off to estimates that 
use less conditioning context. More specifically, we 
used the following three estimates: 
4
4
4
32
32
23
1
1
1 ?
?
??
??
?
?
=
+
+
== EEE , 
(5) 
Where 
 
),,(1 RwwC ji=? , ),(1 ji wwC=? ,  
),*,(2 RwC i=? , ,*)(2 iwC=? ,  
),(*,3 RwC j=? , )(*,3 jwC=? ,  
)(*,*,4 RC=? , (*,*)4 C=? .  
in which * indicates a wild-card matching any 
word. The final estimate E is given by linearly 
interpolating these estimates: 
))1()(1( 42232111 EEEE ???? ?+?+=  (6) 
where ?1 and ?2 are smoothing parameters.  
Given the above parsing model, we used an ap-
proximation parsing algorithm that is O(n2). Tradi-
tional techniques use an optimal Viterbi-style algo-
rithm (e.g., bottom-up chart parser) that is O(n5).2 
Although the approximation algorithm is not 
guaranteed to find the most probable D, we opted 
for it because it works in a left-to-right manner, and 
is very efficient and simple to implement. In our 
experiments, we found that the algorithm performs 
reasonably well on average, and its speed and sim-
plicity make it a better choice in DLM training 
where we need to parse a large amount of training 
data iteratively, as described in Section 3.3.  
The parsing algorithm is a slightly modified 
version of that proposed in Yuret (1998). It reads a 
sentence left to right; after reading each new word 
                                                     
2
 For parsers that use bigram lexical dependencies, Eis-
ner and Satta (1999) presents parsing algorithms that are 
O(n4) or O(n3). We thank Joshua Goodman for pointing 
this out.  
wj, it tries to link wj to each of its previous words wi, 
and push the generated dependency dij into a stack. 
When a dependency crossing or a cycle is detected 
in the stack, the dependency with the lowest de-
pendency probability in conflict is eliminated. The 
algorithm is outlined in Figures 2 and 3. 
DEPENDENCY-PARSING(W) 
1 for j ? 1 to LENGTH(W) 
2 for i ? j-1 downto 1 
3 PUSH dij = (wi, wj) into the stack Dj  
4 if a dependency cycle (CY) is detected in Dj 
(see Figure 3(a)) 
5 REMOVE d, where )(minarg dPd
CYd?
=  
6 while a dependency crossing (CR) is detected 
in Dj (see Figure 3(b)) do 
7 REMOVE d, where )(minarg dPd
CRd?
=  
8 OUTPUT(D) 
Figure 2. Approximation algorithm of dependency 
parsing 
 
 
 
 
 
 
(a) (b) 
Figure 3. (a) An example of a dependency cycle: given 
that P(d23) is smaller than P(d12) and P(d13), d23 is 
removed (represented as dotted line). (b) An example of 
a dependency crossing: given that P(d13) is smaller than 
P(d24), d13 is removed. 
Let the dependency probability be the measure of 
the strength of a dependency, i.e., higher probabili-
ties mean stronger dependencies. Note that when a 
strong new dependency crosses multiple weak 
dependencies, the weak dependencies are removed 
even if the new dependency is weaker than the sum 
of the old dependencies. 3  Although this action 
results in lower total probability, it was imple-
mented because multiple weak dependencies con-
nected to the beginning of the sentence often pre-
                                                     
3
 This operation leaves some headwords disconnected; in 
such a case, we assumed that each disconnected head-
word has a dependency relation with its preceding 
headword.  
w1 w2 w3 w1 
w2 w3 w4 
vented a strong meaningful dependency from being 
created. In this manner, the directional bias of the 
approximation algorithm was partially compen-
sated for.4 
3.2 Language modeling 
The DLM together with the dependency parser 
provides an encoding of the (W, D) pair into a se-
quence of elementary model actions. Each action 
conceptually consists of two stages. The first stage 
assigns a probability to the next word given the left 
context. The second stage updates the dependency 
structure given the new word using the parsing 
algorithm in Figure 2. The probability P(W, D) is 
calculated as:  
=),( DWP  (7) 
?
=
?????
??
n
j
jjj
j
jjjj wDWDPDWwP
1
11111 )),,(|()),(|(  
=?
???
)),,(|( 111 jjjjj wDWDP  (8) 
?
=
???
j
i
j
i
j
jj
j
i ppDWpP
1
1111 ),...,,,|( .  
Here (Wj-1, Dj-1) is the word-parse (j-1)-prefix that 
Dj-1 is a dependency structure containing only those 
dependencies whose two related words are included 
in the word (j-1)-prefix, Wj-1. wj is the word to be 
predicted. Dj-1j is the incremental dependency 
structure that generates Dj = Dj-1 || Dj-1j (|| stands for 
concatenation) when attached to Dj-1; it is the de-
pendency structure built on top of Dj-1 and the 
newly predicted word wj (see the for-loop of line 2 
in Figure 2). pij denotes the ith action of the parser at 
position j in the word string: to generate a new 
dependency dij, and eliminate dependencies with 
the lowest dependency probability in conflict (see 
lines 4 ? 7 in Figure 2). ? is a function that maps the 
history (Wj-1, Dj-1) onto equivalence classes. 
The model in Equation (8) is unfortunately in-
feasible because it is extremely difficult to estimate 
the probability of pij due to the large number of 
parameters in the conditional part. According to the 
parsing algorithm in Figure 2, the probability of 
                                                     
4
 Theoretically, we should arrive at the same dependency 
structure no matter whether we parse the sentence left to 
right or right to left. However, this is not the case with the 
approximation algorithm. This problem is called direc-
tional bias. 
each action pij  depends on the entire history (e.g. 
for detecting a dependency crossing or cycle), so 
any mapping ? that limits the equivalence classi-
fication to less context suitable for model estima-
tion would be very likely to drop critical conditional 
information for predicting pij. In practice, we ap-
proximated P(Dj-1j| ?(Wj-1, Dj-1), wj) by P(Dj|Wj) of 
Equation (3), yielding P(Wj, Dj) ? P(Wj| ?(Wj-1, 
Dj-1)) P(Dj|Wj). This approximation is probabilisti-
cally deficient, but our goal is to apply the DLM to a 
decoder in a realistic application, and the perform-
ance gain achieved by this approximation justifies 
the modeling decision.  
Now, we describe the way P(wj|?(Wj-1,Dj-1)) is 
estimated. As described in Section 2, headwords 
and function words play different syntactic and 
semantic roles capturing different types of de-
pendency relations, so the prediction of them can 
better be done separately. Assuming that each word 
token can be uniquely classified as a headword or a 
function word in Japanese, the DLM can be con-
ceived of as a cluster-based language model with 
two clusters, headword H and function word F. We 
can then define the conditional probability of wj 
based on its history as the product of two factors: 
the probability of the category given its history, and 
the probability of wj given its category. Let hj or fj be 
the actual headword or function word in a sentence, 
and let Hj or Fj be the category of the word wj. 
P(wj|?(Wj-1,Dj-1)) can then be formulated as: 
=?
??
)),(|( 11 jjj DWwP   (9) 
)),,(|()),(|( 1111 jjjjjjj HDWwPDWHP ???? ???  
)),,(|()),(|( 1111 jjjjjjj FDWwPDWFP ???? ???+ . 
We first describe the estimation of headword 
probability P(wj | ?(Wj-1, Dj-1), Hj). Let HWj-1 be the 
headwords in (j-1)-prefix, i.e., containing only 
those headwords that are included in Wj-1. Because 
HWj-1 is determined by Wj-1, the headword prob-
ability can be rewritten as P(wj | ?(Wj-1, HWj-1, Dj-1), 
Hj). The problem is to determine the mapping ? so 
as to identify the related words in the left context 
that we would like to condition on. Based on the 
discussion in Section 2, we chose a mapping func-
tion that retains (1) two preceding words wj-1 and 
wj-2 in Wj-1, (2) one preceding headword hj-1 in 
HWj-1, and (3) one linguistically related word wi 
according to Dj-1. wi is determined in two stages: 
First, the parser updates the dependency structure 
Dj-1 incrementally to Dj assuming that the next word 
is wj. Second, when there are multiple words that 
have dependency relations with wj in Dj, wi is se-
lected using the following decision rule: 
),|(maxarg
),(:
RwwPw ij
Dwww
i
jjii ?
= , (10) 
where the probability P(wj | wi, R) of the word wj 
given its linguistic related word wi is computed 
using MLE by Equation (11): 
?=
jw
ji
ji
ij RwwC
RwwC
RwwP ),,(
),,(),|( . (11) 
We thus have the mapping function ?(Wj-1, HWj-1, 
Dj-1) = (wj-2, wj-1, hj-1, wi). The estimate of headword 
probability is an interpolation of three probabilities:  
=?
??
)),,(|( 11 jjjj HDWwP   (12) 
),|(( 121 jjj HhwP ???   
)),|()1( 2 RwwP ij??+   
),,|()1( 121 jjjj HwwwP ???+ ? .  
Here P(wj|wj-2, wj-1, Hj) is the word trigram prob-
ability given that wj is a headword, P(wj|hj-1, Hj) is 
the headword bigram probability, and ?1, ?2 ? [0,1] 
are  the interpolation weights optimized on held-out 
data. 
We now come back to the estimate of the other 
three probabilities in Equation (9). Following the 
work in Gao et al (2002b), we used the unigram 
estimate for word category probabilities, (i.e., 
P(Hj|?(Wj-1, Dj-1)) ? P(Hj) and P(Fj | ?(Wj-1, Dj-1)) ? 
P(Fj)), and the standard trigram estimate for func-
tion word probability (i.e., P(wj |?(Wj-1,Dj-1),Fj) ? 
P(wj | wj-2, wj-1, Fj)). Let Cj be the category of wj; we 
approximated P(Cj)? P(wj|wj-2, wj-1, Cj) by P(wj | wj-2, 
wj-1). By separating the estimates for the probabili-
ties of headwords and function words, the final 
estimate is given below: 
P(wj | ?(Wj-1, Dj-1))= (13) 
)|()((( 121 ?jjj hwPHP ??
)),|()1( 2 RwwP ij??+
),|()1( 121 ???+ jjj wwwP?  
 
wj: headword  
),|( 12 ?? jjj wwwP   ?
??
?
??
?
?
?
 
wj: function word  
All conditional probabilities in Equation (13) are 
obtained using MLE on training data. In order to 
deal with the data sparseness problem, we used a 
backoff scheme (Katz, 1987) for parameter estima-
tion. This backoff scheme recursively estimates the 
probability of an unseen n-gram by utilizing 
(n?1)-gram estimates. In particular, the probability 
of Equation (11) backs off to the estimate of 
P(wj|R), which is computed as: 
N
RwC
RwP jj
),()|( = , (14) 
where N is the total number of dependencies in 
training data, and C(wj, R) is the number of de-
pendencies that contains wj. To keep the model size 
manageable, we removed all n-grams of count less 
than 2 from the headword bigram model and the 
word trigram model, but kept all long-distance 
dependency bigrams that occurred in the training 
data. 
3.3 Training data creation 
This section describes two methods that were used 
to tag raw text corpus for DLM training: (1) a 
method for headword detection, and (2) an unsu-
pervised learning method for dependency structure 
acquisition. 
In order to classify a word uniquely as H or F, 
we used a mapping table created in the following 
way. We first assumed that the mapping from 
part-of-speech (POS) to word category is unique 
and fixed;5 we then used a POS-tagger to generate a 
POS-tagged corpus, which are then turned into a 
category-tagged corpus.6 Based on this corpus, we 
created a mapping table which maps each word to a 
unique category: when a word can be mapped to 
either H or F, we chose the more frequent category 
in the corpus. This method achieved a 98.5% ac-
curacy of headword detection on the test data we 
used. 
Given a headword-tagged corpus, we then used 
an EM-like iterative method for joint optimization 
of the parsing model and the dependency structure 
of training data. This method uses the maximum 
likelihood principle, which is consistent with lan-
                                                     
5
 The tag set we used included 1,187 POS tags, of which 
102 counted as headwords in our experiments. 
6
 Since the POS-tagger does not identify phrases (bun-
setsu), our implementation identifies multiple headwords 
in phrases headed by compounds.  
guage model training. There are three steps in the 
algorithm: (1) initialize, (2) (re-)parse the training 
corpus, and (3) re-estimate the parameters of the 
parsing model. Steps (2) and (3) are iterated until 
the improvement in the probability of training data 
is less than a threshold. 
Initialize: We set a window of size N and assumed 
that each headword pair within a headword N-gram 
constitutes an initial dependency. The optimal value 
of N is 3 in our experiments. That is, given a 
headword trigram (h1, h2, h3), there are 3 initial 
dependencies: d12, d13, and d23. From the initial 
dependencies, we computed an initial dependency 
parsing model by Equation (4).  
(Re-)parse the corpus: Given the parsing model, 
we used the parsing algorithm in Figure 2 to select 
the most probable dependency structure for each 
sentence in the training data. This provides an up-
dated set of dependencies. 
Re-estimate the parameters of parsing model: 
We then re-estimated the parsing model parameters 
based on the updated dependency set. 
4 Evaluation Methodology 
In this study, we evaluated language models on the 
application of Japanese Kana-Kanji conversion, 
which is the standard method of inputting Japanese 
text by converting the text of a syllabary-based 
Kana string into the appropriate combination of 
Kanji and Kana. This is a similar problem to speech 
recognition, except that it does not include acoustic 
ambiguity. Performance on this task is measured in 
terms of the character error rate (CER), given by the 
number of characters wrongly converted from the 
phonetic string divided by the number of characters 
in the correct transcript.  
For our experiments, we used two newspaper 
corpora, Nikkei and Yomiuri Newspapers, both of 
which have been pre-word-segmented. We built 
language models from a 36-million-word subset of 
the Nikkei Newspaper corpus, performed parameter 
optimization on a 100,000-word subset of the Yo-
miuri Newspaper (held-out data), and tested our 
models on another 100,000-word subset of the 
Yomiuri Newspaper corpus. The lexicon we used 
contains 167,107 entries.  
Our evaluation was done within a framework of 
so-called ?N-best rescoring? method, in which a list 
of hypotheses is generated by the baseline language 
model (a word trigram model in this study), which 
is then rescored using a more sophisticated lan-
guage model. We use the N-best list of N=100,
whose ?oracle? CER (i.e., the CER of the hy-
potheses with the minimum number of errors) is 
presented in Table 1, indicating the upper bound on 
performance. We also note in Table 1 that the per-
formance of the conversion using the baseline tri-
gram model is much better than the state-of-the-art 
performance currently available in the marketplace, 
presumably due to the large amount of training data 
we used, and to the similarity between the training 
and the test data.  
Baseline Trigram Oracle of 100-best 
3.73% 1.51% 
Table 1. CER results of baseline and 100-best list 
5 Results 
The results of applying our models to the task of 
Japanese Kana-Kanji conversion are shown in 
Table 2. The baseline result was obtained by using a 
conventional word trigram model (WTM).7 HBM 
stands for headword bigram model, which does not 
use any dependency structure (i.e. ?2 = 1 in Equation 
(13)). DLM_1 is the DLM that does not use head-
word bigram (i.e. ?
 2 = 0 in Equation (13)). DLM_2 
is the model where the headword probability is 
estimated by interpolating the word trigram prob-
ability, the headword bigram probability, and the 
probability given one previous linguistically related 
word in the dependency structure. 
Although Equation (7) suggests that the word 
probability P(wj|?(Wj-1,Dj-1)) and the parsing model 
probability can be combined through simple multi-
plication, some weighting is desirable in practice, 
especially when our parsing model is estimated 
using an approximation by the parsing score 
P(D|W). We therefore introduced a parsing model 
weight PW: both DLM_1 and DLM_2 models were 
built with and without PW. In Table 2, the PW- 
prefix refers to the DLMs with PW = 0.5, and the 
DLMs without PW- prefix refers to DLMs with PW 
= 0. For both DLM_1 and DLM_2, models with the 
parsing weight achieve better performance; we 
                                                     
7
 For a detailed description of the baseline trigram model, 
see Gao et al (2002a).  
therefore discuss only DLMs with the parsing 
weight for the rest of this section. 
Model ?1 ?2 CER CER reduction 
WTM ---- ---- 3.73% ---- 
HBM 0.2 1 3.40% 8.8% 
DLM_1  0.1 0 3.48% 6.7% 
PW-DLM_1 0.1 0 3.44% 7.8% 
DLM_2 0.3 0.7 3.33% 10.7% 
PW-DLM_2 0.3 0.7 3.31% 11.3% 
Table 2. Comparison of CER results 
By comparing both HBM and PW-LDM_1 models 
with the baseline model, we can see that the use of 
headword dependency contributes greatly to the 
CER reduction: HBM outperformed the baseline 
model by 8.8% in CER reduction, and PW-LDM_1 
by 7.8%. By combining headword bigram and 
dependency structure, we obtained the best model 
PW-DLM_2 that achieves 11.3% CER reduction 
over the baseline. The improvement achieved by 
PW-DLM_2 over the HBM is statistically signifi-
cant according to the t test (P<0.01). These results 
demonstrate the effectiveness of our parsing tech-
nique and the use of dependency structure for lan-
guage modeling. 
6 Discussion 
In this section, we relate our model to previous 
research and discuss several factors that we believe 
to have the most significant impact on the per-
formance of DLM. The discussion includes: (1) the 
use of DLM as a parser, (2) the definition of the 
mapping function ?, and (3) the method of unsu-
pervised dependency structure acquisition. 
One basic approach to using linguistic structure 
for language modeling is to extend the conventional 
language model P(W) to P(W, T), where T is a parse 
tree of W. The extended model can then be used as a 
parser to select the most likely parse by T* = arg-
maxT P(W, T). Many recent studies (e.g., Chelba 
and Jelinek, 2000; Charniak, 2001; Roark, 2001) 
adopt this approach. Similarly, dependency-based 
models (e.g., Collins, 1996; Chelba et al, 1997) use 
a dependency structure D of W instead of a parse 
tree T, where D is extracted from syntactic trees. 
Both of these models can be called grammar-based 
models, in that they capture the syntactic structure 
of a sentence, and the model parameters are esti-
mated from syntactically annotated corpora such as 
the Penn Treebank. DLM, on the other hand, is a 
non-grammar-based model, because it is not based 
on any syntactic annotation: the dependency struc-
ture used in language modeling was learned directly 
from data in an unsupervised manner, subject to two 
weak syntactic constraints (i.e., dependency struc-
ture is acyclic and planar).8 This resulted in cap-
turing the dependency relations that are not pre-
cisely syntactic in nature within our model. For 
example, in the conversion of the string below, the 
word  ban 'evening' was correctly predicted in 
DLM by using the long-distance bigram ~ 
asa~ban 'morning~evening', even though these two 
words are not in any direct syntactic dependency 
relationship:  
	

 
'asks for instructions in the morning and submits 
daily reports in the evening'  
Though there is no doubt that syntactic dependency 
relations provide useful information for language 
modeling, the most linguistically related word in the 
previous context may come in various linguistic 
relations with the word being predicted, not limited 
to syntactic dependency. This opens up new possi-
bilities for exploring the combination of different 
knowledge sources in language modeling.  
Regarding the function ? that maps the left 
context onto equivalence classes, we used a simple 
approximation that takes into account only one 
linguistically related word in left context. An al-
ternative is to use the maximum entropy (ME) 
approach (Rosenfeld, 1994; Chelba et al, 1997). 
Although ME models provide a nice framework for 
incorporating arbitrary knowledge sources that can 
be encoded as a large set of constraints, training and 
using ME models is extremely computationally 
expensive. Our working hypothesis is that the in-
formation for predicting the new word is dominated 
by a very limited set of words which can be selected 
heuristically: in this paper, ? is defined as a heu-
ristic function that maps D to one word in D that has 
the strongest linguistic relation with the word being 
predicted, as in (8). This hypothesis is borne out by 
                                                     
8
 In this sense, our model is an extension of a depend-
ency-based model proposed in Yuret (1998). However, 
this work has not been evaluated as a language model 
with error rate reduction.  
an additional experiment we conducted, where we 
used two words from D that had the strongest rela-
tion with the word being predicted; this resulted in a 
very limited gain in CER reduction of 0.62%, which 
is not statistically significant (P>0.05 according to 
the t test).  
The EM-like method for learning dependency 
relations described in Section 3.3 has also been 
applied to other tasks such as hidden Markov model 
training (Rabiner, 1989), syntactic relation learning 
(Yuret, 1998), and Chinese word segmentation 
(Gao et al, 2002a). In applying this method, two 
factors need to be considered: (1) how to initialize 
the model (i.e. the value of the window size N), and 
(2) the number of iterations. We investigated the 
impact of these two factors empirically on the CER 
of Japanese Kana-Kanji conversion. We built a 
series of DLMs using different window size N and 
different number of iterations. Some sample results 
are shown in Table 3: the improvement in CER 
begins to saturate at the second iteration. We also 
find that a larger N results in a better initial model 
but makes the following iterations less effective. 
The possible reason is that a larger N generates 
more initial dependencies and would lead to a better 
initial model, but it also introduces noise that pre-
vents the initial model from being improved. All 
DLMs in Table 2 are initialized with N = 3 and are 
run for two iterations.  
Iteration N = 2 N = 3 N = 5 N = 7 N = 10 
Init. 3.552% 3.523% 3.540% 3.514 % 3.511% 
1 3.531% 3.503% 3.493% 3.509% 3.489% 
2 3.527% 3.481% 3.483% 3.492% 3.488% 
3 3.526% 3.481% 3.485% 3.490% 3.488% 
Table 3. CER of DLM_1 models initialized with dif-
ferent window size N, for 0-3 iterations 
7 Conclusion 
We have presented a dependency language model 
that captures linguistic constraints via a dependency 
structure ? a set of probabilistic dependencies that 
express the relations between headwords of each 
phrase in a sentence by an acyclic, planar, undi-
rected graph. Promising results of our experiments 
suggest that long-distance dependency relations can 
indeed be successfully exploited for the purpose of 
language modeling.   
There are many possibilities for future im-
provements. In particular, as discussed in Section 6, 
syntactic dependency structure is believed to cap-
ture useful information for informed language 
modeling, yet further improvements may be possi-
ble by incorporating non-syntax-based dependen-
cies. Correlating the accuracy of the dependency 
parser as a parser vs. its utility in CER reduction 
may suggest a useful direction for further research.  
Reference 
Charniak, Eugine. 2001. Immediate-head parsing for 
language models. In ACL/EACL 2001, pp.124-131.  
Chelba, Ciprian and Frederick Jelinek. 2000. Structured 
Language Modeling. Computer Speech and Language, 
Vol. 14, No. 4. pp 283-332.  
Chelba, C, D. Engle, F. Jelinek, V. Jimenez, S. Khu-
danpur, L. Mangu, H. Printz, E. S. Ristad, R. 
Rosenfeld, A. Stolcke and D. Wu. 1997. Structure and 
performance of a dependency language model. In 
Processing of Eurospeech, Vol. 5, pp 2775-2778.  
Collins, Michael John. 1996. A new statistical parser 
based on bigram lexical dependencies. In ACL 
34:184-191. 
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing 
for bilexical context-free grammars and head 
automaton grammars. In ACL 37: 457-464. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002a. Toward a unified approach to sta-
tistical language modeling for Chinese. ACM Trans-
actions on Asian Language Information Processing, 
1-1: 3-33.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002b. 
Exploiting headword dependency and predictive 
clustering for language modeling. In EMNLP 2002: 
248-256. 
Katz, S. M. 1987. Estimation of probabilities from sparse 
data for other language component of a speech recog-
nizer. IEEE transactions on Acoustics, Speech and 
Signal Processing, 35(3): 400-401. 
Rabiner, Lawrence R. 1989. A tutorial on hidden Markov 
models and selected applications in speech recognition. 
Proceedings of IEEE 77:257-286. 
Roark, Brian. 2001. Probabilistic top-down parsing and 
language modeling. Computational Linguistics, 17-2: 
1-28. 
Rosenfeld, Ronald. 1994. Adaptive statistical language 
modeling: a maximum entropy approach. Ph.D. thesis, 
Carnegie Mellon University.  
Yuret, Deniz. 1998. Discovery of linguistic relations 
using lexical attraction. Ph.D. thesis, MIT. 
Adaptive Chinese Word Segmentation1  
Jianfeng Gao*, Andi Wu*, Mu Li*, Chang-Ning Huang*, Hongqiao Li**, Xinsong Xia$, Haowei Qin&  
*
 Microsoft Research. {jfgao, andiwu, muli, cnhuang}@microsoft.com 
**
 Beijing Institute of Technology, Beijing. lhqtxm@bit.edu.cn 
$
 Peking University, Beijing. xia_xinsong@founder.com 
&
 Shanghai Jiaotong university, Shanghai. haoweiqin@sjtu.edu.cn 
                                                   
1
 This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia. We thank 
Xiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al, 2001). 
Abstract 
This paper presents a Chinese word segmen-
tation system which can adapt to different 
domains and standards. We first present a sta-
tistical framework where domain-specific 
words are identified in a unified approach to 
word segmentation based on linear models. 
We explore several features and describe how 
to create training data by sampling. We then 
describe a transformation-based learning 
method used to adapt our system to different 
word segmentation standards. Evaluation of 
the proposed system on five test sets with dif-
ferent standards shows that the system 
achieves state- of-the-art performance on all of 
them. 
1 Introduction 
Chinese word segmentation has been a long- 
standing research topic in Chinese language proc-
essing. Recent development in this field shows that, 
in addition to ambiguity resolution and unknown 
word detection, the usefulness of a Chinese word 
segmenter also depends crucially on its ability to 
adapt to different domains of texts and different 
segmentation standards.  
The need of adaptation involves two research 
issues that we will address in this paper. The first is 
new word detection. Different domains/applications 
may have different vocabularies which contain new 
words/terms that are not available in a general 
dictionary. In this paper, new words refer to OOV 
words other than named entities, factoids and mor-
phologically derived words. These words are 
mostly domain specific terms (e.g. ??? ?cellular?) 
and time-sensitive political, social or cultural terms 
(e.g. ???Three Links?, ?? ?SARS?).  
The second issue concerns the customizable 
display of word segmentation. Different Chinese 
NLP-enabled applications may have different re-
quirements that call for different granularities of 
word segmentation. For example, speech recogni-
tion systems prefer ?longer words? to achieve 
higher accuracy whereas information retrieval 
systems prefer ?shorter words? to obtain higher 
recall rates, etc. (Wu, 2003). Given a word seg-
mentation specification (or standard) and/or some 
application data used as training data, a segmenter 
with customizable display should be able to provide 
alternative segmentation units according to the 
specification which is either pre-defined or implied 
in the data. 
In this paper, we first present a statistical 
framework for Chinese word segmentation, where 
various problems of word segmentation are solved 
simultaneously in a unified approach.  Our ap-
proach is based on linear models where component 
models are inspired by the source-channel models 
of Chinese sentence generation. We then describe in 
detail how the new word identification (NWI) 
problem is handled in this framework. We explore 
several features and describe how to create training 
data by sampling. We evaluate the performance of 
our segmentation system using an annotated test set, 
where new words are simulated by sampling. We 
then describe a transformation-based learning (TBL, 
Brill, 1995) method that is used to adapt our system 
to different segmentation standards. We compare 
the adaptive system to other state-of-the-art systems 
using four test sets in the SIGHAN?s First Interna-
tional Chinese Word Segmentation Bakeoff, each of 
which is constructed according to a different seg-
mentation standard. The performance of our system 
is comparable to the best systems reported on all 
four test sets. It demonstrates the possibility of 
having a single adaptive Chinese word segmenter 
that is capable of supporting multiple user applica-
tions. 
Word Class2 Model Feature Functions, f(S,W) 
Context Model Word class based trigram, P(W). -log(P(W)) 
Lexical Word (LW) --- 1 if S forms a word lexicon entry, 0 otherwise. 
Morphological Word (MW) --- 1 if S forms a morph lexicon entry, 0 otherwise. 
Named Entity (NE) Character/word bigram, P(S|NE). -log(P(S|NE)) 
Factoid (FT) --- 1 if S can be parsed using a factoid grammar, 0 otherwise 
New Word (NW) --- Score of SVM classifier 
Figure 1: Context model, word classes, and class models, and feature functions. 
                                                   
2
 In our system, we define three types of named entity: person name (PN), location name (LN), organization (ON) and translit-
eration name (TN); ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone number, 
and WWW; and five types of morphologically derived words (MDW): affixation, reduplication, merging, head particle and split. 
2 Chinese Word Segmentation with 
Linear Models 
Let S be a Chinese sentence which is a character 
string. For all possible word segmentations W, we 
will choose the most likely one W* which achieves 
the highest conditional probability P(W|S): W* = 
argmaxw P(W|S). According to Bayes? decision rule 
and dropping the constant denominator, we can 
equivalently perform the following maximization: 
)|()(maxarg* WSPWPW
W
=
. 
(1) 
Equation (1) represents a source-channel approach 
to Chinese word segmentation. This approach 
models the generation process of a Chinese sen-
tence: first, the speaker selects a sequence of con-
cepts W to output, according to the probability 
distribution P(W); then he attempts to express each 
concept by choosing a sequence of characters, 
according to the probability distribution P(S|W).  
We define word class as a group of words that 
are supposed to be generated according to the same 
distribution (or in the same manner). For instance, 
all Chinese person names form a word class. We 
then have multiple channel models, each for one 
word class. Since a channel model estimates the 
likelihood that a character string is generated given 
a word class, it is also referred to as class model. 
Similarly, source model is referred to as context 
model because it indicates the likelihood that a word 
class occurs in a context. We have only one context 
model which is a word-class-based trigram model. 
Figure 1 shows word classes and class models that 
we used in our system. We notice that different 
class models are constructed in different ways (e.g. 
name entity models are n-gram models trained on 
corpora whereas factoid models use derivation rules 
and have binary values). The dynamic value ranges 
of different class models can be so different that it is 
improper to combine all models through simple 
multiplication as Equation (1). 
In this study we use linear models. The method 
is derived from linear discriminant functions widely 
used for pattern classification (Duda et al, 2001), 
and has been recently introduced into NLP tasks by 
Collins and Duffy (2001). It is also related to log- 
linear models for machine translation (Och, 2003).  
In this framework, we have a set of M+1 feature 
functions fi(S,W), i = 0,?,M. They are derived from 
the context model (i.e. f0(W)) and M class models, 
each for one word class, as shown in Figure 1: For 
probabilistic models such as the context model or 
person name model, the feature functions are de-
fined as the negative logarithm of the corresponding 
probabilistic models. For each feature function, 
there is a model parameter ?i. The best word seg-
mentation W* is determined by the decision rule as 
?
=
==
M
i
ii
W
M
W
WSfWSScoreW
0
0
* ),(maxarg),,(maxarg ??  (2) 
Below we describe how to optimize ?s. Our 
method is a discriminative approach inspired by the 
Minimum Error Rate Training method proposed in 
Och (2003). Assume that we can measure the 
number of segmentation errors in W by comparing it 
with a reference segmentation R using a function 
Er(R,W). The training criterion is to minimize the 
count of errors over the training data as 
?=
RWS
M
M
SWREr
M
,,
11
^
)),(,(minarg
1
??
?
, (3) 
where W is detected by Equation (2). However, we 
cannot apply standard gradient descent to optimize  
Initialization: ?0=?, ?i=1, i = 1,?,M. 
For t = 1 ? T,  j = 1 ? N 
 Wj = argmax ? ?i fi(Sj,W) 
 For i = 1? M 
  ?i = ?i + ?(Score(?,S,W)-Score(?,S,R))(fi(R) - fi(W)),  
where ?={?0, ?1,?,?M} and ? =0.001. 
Figure 2: The training algorithm for model parameters 
model parameters according to Equation (3) be-
cause the gradient cannot be computed explicitly 
(i.e., Er is not differentiable), and there are many 
local minima in the error surface. We then use a 
variation called stochastic gradient descent (or 
unthresholded perceptron, Mitchell, 1997). As 
shown in Figure 2, the algorithm takes T passes over 
the training set (i.e. N sentences). All parameters are 
initially set to be 1, except for the context model 
parameter ?0 which is set to be a constant ? during 
training, and is estimated separately on held-out 
data. Class model parameters are updated in a sim-
ple additive fashion. Notice that Score(?,S,W) is not 
less than Score(?,S,R). Intuitively the updated rule 
increases the parameter values for word classes 
whose models were ?underestimated? (i.e. expected 
feature value f(W) is less than observed feature 
value f(R)), and decreases the parameter values 
whose models were ?overestimated? (i.e. f(W) is 
larger than f(R)).  Although the method cannot 
guarantee a global optimal solution, it is chosen for 
our modeling because of its efficiency and the best 
results achieved in our experiments. 
Given the linear models, the procedure of word 
segmentation in our system is as follows: First, all 
word candidates (lexical words and OOV words of 
certain types) are generated, each with its word 
class tag and class model score. Second, Viterbi 
search is used to select the best W according to 
Equation (2). Since the resulting W* is a sequence of 
segmented words that are either lexical words or 
OOV words with certain types (e.g. person name, 
morphological words, new words) we then have a 
system that can perform word segmentation and 
OOV word detection simultaneously in a unified 
approach. Most previous works treat OOV word 
detection as a separate step after word segmentation. 
Compared to these approaches, our method avoids 
the error propagation problem and can incorporate a 
variety of knowledge to achieve a globally optimal 
solution. The superiority of the unified approach 
has been demonstrated empirically in Gao et al 
(2003), and will also be discussed in Section 5. 
3 New Word Identification 
New words in this section refer to OOV words that 
are neither recognized as named entities or factoids 
nor derived by morphological rules. These words 
are mostly domain specific and/or time-sensitive. 
The identification of such new words has not been 
studied extensively before. It is an important issue 
that would have substantial impact on the per-
formance of word segmentation. For example, 
approximately 30% of OOV words in the 
SIGHAN?s PK corpus (see Table 1) are new words 
of this type. There has been previous work on de-
tecting Chinese new words from a large corpus in 
an off-line manner and updating the dictionary 
before word segmentation. However, our approach 
is able to detect new words on-line, i.e. to spot new 
words in a sentence on the fly during the process of 
word segmentation where widely-used statistical 
features such as mutual information or term fre-
quency are not available. 
For brevity of discussion, we will focus on the 
identification of 2-character new words, denoted as 
NW_11. Other types of new words such as NW_21 
(a 2-character word followed with a character) and 
NW_12 can be detected similarly (e.g. by viewing 
the 2-character word as an inseparable unit, like a 
character). Below, we shall describe the class model 
and context model for NWI, and the creation of 
training data by sampling. 
3.1 Class Model 
We use a classifier (SVM in our experiments) to 
estimate the likelihood of two adjacent characters to 
form a new word. Of the great number of features 
we experimented, three linguistically-motivated 
features are chosen due to their effectiveness and 
availability for on-line detection. They are Inde-
pendent Word Probability (IWP), Anti-Word Pair 
(AWP), and Word Formation Analogy (WFA). 
Below we describe each feature in turn. In Section 
3.2, we shall describe the way the training data (new 
word list) for the classifier is created by sampling. 
IWP is a real valued feature. Most Chinese 
characters can be used either as independent words 
or component parts of multi-character words, or 
both. The IWP of a single character is the likelihood 
for this character to appear as an independent word 
in texts (Wu and Jiang, 2000): 
)(
) ,()(
xC
WxC
xIWP = . (4) 
where C(x, W) is the number of occurrences of the 
character x as an independent word in training data, 
and C(x) is the total number of x in training data. We 
assume that the IWP of a character string is the 
product of the IWPs of the component characters. 
Intuitively, the lower the IWP value, the more likely 
the character string forms a new word. In our im-
plementation, the training data is word-segmented. 
AWP is a binary feature derived from IWP. For 
example, the value of AWP of an NW_11 candidate 
ab is defined as: AWP(ab)=1 if IWP(a)>? or IWP(b) 
>?, 0 otherwise. ? ? [0, 1] is a pre-set threshold. 
Intuitively, if one of the component characters is 
very likely to be an independent word, it is unlikely 
to be able to form a word with any other characters. 
While IWP considers all component characters in a 
new word candidate, AWP only considers the one 
with the maximal IWP value. 
WFA is a binary feature. Given a character pair 
(x, y), a character (or a multi-character string) z is 
called the common stem of (x, y) if at least one of the 
following two conditions hold: (1) character strings 
xz and yz are lexical words (i.e. x and y as prefixes); 
and (2) character strings zx and zy are lexical words 
(i.e. x and y as suffixes). We then collect a list of 
such character pairs, called affix pairs, of which the 
number of common stems is larger than a pre-set 
threshold. The value of WFA for a given NW_11 
candidate ab is defined as: WFA(ab) = 1 if there 
exist an affix pair (a, x) (or (b, x)) and the string xb 
(or ax) is a lexical word, 0 otherwise. For example, 
given an NW_11 candidate ?? (xia4-gang3, ?out of 
work?), we have WFA(??) = 1 because (?, ?) is 
an affix pair (they have 32 common stems such as _
?,  ?,  ?,  ?,  ?,  ?,  ?) and ?? 
(shang4-gang3, ?take over a shift?) is a lexical word. 
3.2 Context Model 
The motivations of using context model for NWI 
are two-fold. The first is to capture useful contex-
tual information. For example, new words are more 
likely to be nouns than pronouns, and the POS 
tagging is context-sensitive. The second is more 
important. As described in Section 2, with a context 
model, NWI can be performed simultaneously with 
other word segmentation tasks (e.g.: word break, 
named entity recognition and morphological analy-
sis) in a unified approach. 
However, it is difficult to develop a training 
corpus where new words are annotated because ?we 
usually do not know what we don?t know?. Our 
solution is Monte Carlo simulation. We sample a set 
of new words from our dictionary according to the 
distribution ? the probability that any lexical word 
w would be a new word P(NW|w). We then generate 
a new-word-annotated corpus from a word-seg-
mented text corpus.  
Now we describe the way P(NW|w) is estimated. 
It is reasonable to assume that new words are those 
words whose probability to appear in a new docu-
ment is lower than general lexical words. Let Pi(k) 
be the probability of word wi that occurs k times in a 
document. In our experiments, we assume that 
P(NW|wi) can be approximated by the probability of 
wi occurring less than K times in a new document:  
??
=
?
1
0
)()|(
K
k
ii kPwNWP , (5) 
where the constant K is dependent on the size of the 
document: The larger the document, the larger the 
value. Pi(k) can be estimated using several term 
distribution models (see Chapter 15.3 in Manning 
and Sch?tze, 1999). Following the empirical study 
in (Gao and Lee, 2000), we use K-Mixture (Katz, 
1996) which estimate Pi(k) as 
k
ki kP )1(1)1()( 0, +++?= ?
?
?
??? , (6) 
where ?k,0=1 if  k=0, 0 otherwise. ? and ? are pa-
rameters that can be fit using the observed mean ? 
and the observed inverse document frequency IDF 
as follow: 
N
cf
=? , 
df
NIDF log= , 
df
dfcfIDF ?
=??= 12?? , and ?
?
? = , 
where cf is the total number of occurrence of word 
wi in training data, df is the number of documents in 
training data that wi occurs in, and N is the total 
number of documents. In our implementation, the 
training data contain approximately 40 thousand 
documents that have been balanced among domain, 
style and time. 
4 Adaptation to Different Standards 
The word segmentation standard (or standard for 
brevity) varies from system to system because there 
is no commonly accepted definition of Chinese
   
Condition: ?Affixation? Condition: ?Date? Condition: ?PersonName? 
Actions: Insert a boundary 
between ?Prefix? and ?Stem?? 
Actions: Insert a boundary between 
?Year? and ?Mon? ? 
Actions: Insert a boundary be-
tween ?FamilyName? and ?Given-
Name?? 
Figure 3: Word internal structure and class-type transformation templates. 
words and different applications may have different 
requirements that call for different granularities of 
word segmentation. 
It is ideal to develop a single word segmentation 
system that is able to adapt to different standards. 
We consider the following standard adaptation 
paradigm. Suppose we have a ?general? standard 
pre-defined by ourselves. We have also created a 
large amount of training data which are segmented 
according to this general standard. We then develop 
a generic word segmenter, i.e. the system described 
in Sections 2 and 3. Whenever we deploy the seg-
menter for any application, we need to customize 
the output of the segmenter according to an appli-
cation-specific standard, which is not always ex-
plicitly defined. However, it is often implicitly 
defined in a given amount of application data 
(called adaptation data) from which the specific 
standard can be partially learned. 
In our system, the standard adaptation is con-
ducted by a postprocessor which performs an or-
dered list of transformations on the output of the 
generic segmenter ? removing extraneous word 
boundaries, and inserting new boundaries ? to 
obtain a word segmentation that meets a different 
standard. 
The method we use is transformation-based 
learning (Brill, 1995), which requires an initial 
segmentation, a goal segmentation into which we 
wish to transform the initial segmentation and a 
space of allowable transformations (i.e. transfor-
mation templates). Under the abovementioned 
adaptation paradigm, the initial segmentation is the 
output of the generic segmenter. The goal segmen-
tation is adaptation data. The transformation tem-
plates can make reference to words (i.e. lexicalized 
templates) as well as some pre-defined types (i.e. 
class-type based templates), as described below. 
We notice that most variability in word seg-
mentation across different standards comes from 
those words that are not typically stored in the 
dictionary. Those words are dynamic in nature and 
are usually formed through productive morpho-
logical processes. In this study, we focus on three 
categories: morphologically derived words (MDW), 
named entities (NE) and factoids. 
For each word class that belongs to these cate-
gories2, we define an internal structure similar to 
(Wu, 2003). The structure is a tree with ?word class? 
as the root, and ?component types? as the other 
nodes. There are 30 component types. As shown in 
Figure 3, the word class Affixation has three 
component types: Prefix, Stem and Suffix. 
Similarly, PersonName has two component types 
and Date has nine ? 3 as non-terminals and 6 as 
terminals. These internal structures are assigned to 
words by the generic segmenter at run time. 
The transformation templates for words of the 
above three categories are of the form: 
Condition: word class 
Actions:  
z Insert ? place a new boundary 
between two component types. 
z Delete ? remove an existing 
boundary between two component 
types. 
Since the application of the transformations de-
rived from the above templates are conditioned on 
word class and make reference to component types, 
we call the templates class-type transformation 
templates. Some examples are shown in Figure 3. 
In addition, we also use lexicalized transforma-
tion templates as: 
z Insert ? place a new boundary 
between two lemmas. 
Mon Day
Pre_Y Pre_MDig_M Dig_D
Year 
Date
PersonName 
FamilyName GivenName
Affixation 
Prefix Stem Suffix
Pre_DDig_Y 
z Delete ? remove an existing 
boundary between two lemmas. 
Here, lemmas refer to those basic lexical words 
that cannot be formed by any productive morpho-
logical process. They are mostly single characters, 
bi-character words, and 4-character idioms. 
In short, our adaptive Chinese word segmenter 
consists of two components: (1) a generic seg-
menter that is capable of adapting to the vocabu-
laries of different domains and (2) a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards  
5 Evaluation 
We evaluated the proposed adaptive word seg-
mentation system (henceforth AWS) using five 
different standards. The training and test corpora of 
these standards are detailed in Table 1, where MSR 
is defined by ourselves, and the other four are stan-
dards used in SIGHAN?s First International Chi-
nese Word Segmentation Bakeoff (Bakeoff test sets 
for brevity, see Sproat and Emperson (2003) for 
details). 
Corpus Abbrev. # Tr. Word # Te. Word 
?General? standard  MSR 20M 226K 
Beijing University PK 1.1M 17K 
U. Penn Chinese 
Treebank 
CTB 250K 40K 
Hong Kong City U. HK 240K 35K 
Academia Sinica AS 5.8M 12K 
Table 1: standards and corpora. 
MSR is used as the general standard in our ex-
periments, on the basis of which the generic seg-
menter has been developed. The training and test 
corpora were annotated manually, where there is 
only one allowable word segmentation for each 
sentence. The training corpus contains approxi-
mately 35 million Chinese characters from various 
domains of text such as newspapers, novels, maga-
zines etc. 90% of the training corpus are used for 
context model training, and 10% are held-out data 
for model parameter training as shown in Figure 2. 
The NE class models, as shown in Figure 1, were 
trained on the corresponding NE lists that were 
collected separately. The test set contains a total of 
225,734 tokens, including 205,162 lexi-
con/morph-lexicon words, 3,703 PNs, 5,287 LNs, 
3,822 ONs, and 4,152 factoids. In Section 5.1, we 
will describe some simulated test sets that are de-
rived from the MSR test set by sampling NWs from 
a 98,686-entry dictionary. 
The four Bakeoff standards are used as ?specific? 
standards into which we wish to adapt the general 
standard. We notice in Table 1 that the sizes of 
adaptation data sets (i.e. training corpora of the four 
Bakeoff standards) are much smaller than that of the 
MSR training set. The experimental setting turns 
out to be a good simulation of the adaptation para-
digm described in Section 4. 
The performance of word segmentation is 
measured through test precision (P), test recall (R), 
F score (which is defined as 2PR/(P+R)), the OOV 
rate for the test corpus (on Bakeoff corpora, OOV is 
defined as the set of words in the test corpus not 
occurring in the training corpus.), the recall on 
OOV words (Roov), and the recall on in-vocabulary 
(Riv) words. We also tested the statistical signifi-
cance of results, using the criterion proposed by 
Sproat and Emperson (2003), and all results re-
ported in this section are significantly different 
from each other. 
5.1 NWI Results 
This section discusses two factors that we believe 
have the most impact on the performance of NWI. 
First, we compare methods where we use the NWI 
component (i.e. an SVM classifier) as a post- 
processor versus as a feature function in the linear 
models of Equation (2). Second, we compare dif-
ferent sampling methods of creating simulated 
training data for context model. Which sampling 
method is best depends on the nature of P(NW|w). 
As described in Section 3.2, P(NW|w) is unknown 
and has to be approximated by Pi(k) in our study, so 
it is expected that the closer P(NW|w) and Pi(k) are, 
the better the resulting context model. We compare 
three estimates of Pi(k) in Equation (5) using term 
models based on Uniform, Possion, and K- Mixture 
distributions, respectively. 
Table 2 shows the results of the generic seg-
menter on three test sets that are derived from the 
MSR test set using the above three different sam-
pling methods, respectively. For all three distribu-
tions, unified approaches (i.e. using NWI compo-
nent as a feature function) outperform consecutive 
approaches (i.e. using NWI component as a post- 
processor). This demonstrates empirically the 
benefits of using context model for NWI and the 
unified approach to Chinese word segmentation, as 
described in 3.2. We also perform NWI on Bakeoff 
AWS w/o NW AWS w/ NW (post-processor) AWS w/ NW (unified approach) 
word segmentation word segmentation NW word segmentation NW  # of NW 
P% R% P% R% P% R% P% R% P% R% 
Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4 
Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6 
K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4 
Table 2: NWI results on MSR test set, NWI as post-processor versus unified approach 
PK CTB 
 P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .824 .854 .839 .069 .320 .861 .799 .818 .809 .181 .624 .861 
2. AWS .952 .959 .955 .069 .781 .972 .895 .914 .904 .181 .746 .950 
3. AWS w/o NWI .949 .963 .956 .069 .741 .980 .875 .910 .892 .181 .690 .959 
4. FMM w/ adaptation .913 .946 .929 .069 .524 .977 .805 .874 .838 .181 .521 .952 
5. Rank 1 in Bakeoff .956 .963 .959 .069 .799 .975 .907 .916 .912 .181 .766 .949 
6. Rank 2 in Bakeoff .943 .963 .953 .069 .743 .980 .891 .911 .901 .181 .736 .949 
Table 3: Comparison scores for PK open and CTB open. 
HK AS 
 
P R F OOV Roov Riv P R F OOV Roov Riv 
1. AWS w/o adaptation .819 .822 .820 .071 .593 .840 .832 .838 .835 .021 .405 .847 
2. AWS .948 .960 .954 .071 .746 .977 .955 .961 .958 .021 .584 .969 
3. AWS w/o NWI .937 .958 .947 .071 .694 .978 .958 .943 .951 .021 .436 .969 
4. FMM w/ adaptation .818 .823 .821 .071 .591 .841 .930 .947 .939 .021 .160 .964 
5. Rank 1 in Bakeoff .954 .958 .956 .071 .788 .971 .894 .915 .904 .021 .426 .926 
6. Rank 2 in Bakeoff .863 .909 .886 .071 .579 .935 .853 .892 .872 .021 .236 .906 
Table 4: Comparison scores for HK open and AS open. 
test sets. As shown in Tables 3 and 4 (Rows 2 and 3), 
the use of NW functions (via the unified approach) 
substantially improves the word segmentation per-
formance. 
We find in our experiments that NWs sampled 
by Possion and K-Mixture are mostly specific and 
time-sensitive terms, in agreement with our intui-
tion, while NWs sampled by Uniform include more 
common words and lemmas that are easier to detect. 
Consequently, by Uniform sampling, the P/R of 
NWI is the highest but the P/R of the overall word 
segmentation is the lowest, as shown in Table 2. 
Notice that the three sampling methods are not 
comparable in terms of P/R of NWI in Table 2 
because of different sampling result in different sets 
of new words in the test set. We then perform NWI 
on Bakeoff test sets where the sets of new words are 
less dependent on specific sampling methods. The 
results however do not give a clear indication which 
sampling method is the best because the test sets are 
too small to show the difference. We then leave it to 
future work a thorough empirical comparison 
among different sampling methods. 
5.2 Standard Adaptation Results 
The results of standard adaptation on four Bakeoff 
test sets are shown in Tables 3 and 4. A set of 
transformations for each standard is learnt using 
TBL from the corresponding Bakeoff training set. 
For each test set, we report results using our system 
with and without standard adaptation (Rows 1 and 
2). It turns out that performance improves dra-
matically across the board in all four test sets. 
For comparison, we also include in each table 
the results of using the forward maximum matching 
(FMM) greedy segmenter as a generic segmenter 
(Row 4), and the top 2 scores (sorted by F) that are 
reported in SIGHAN?s First International Chinese 
Word Segmentation Bakeoff (Rows 5 and 6). We 
can see that with adaptation, our generic segmenter 
can achieve state-of-the-art performance on dif-
ferent standards, showing its superiority over other 
systems. For example, there is no single segmenter 
in SIGHAN?s Bakeoff, which achieved top-2 ranks 
in all four test sets (Sproat and Emperson, 2003). 
We notice in Table 3 and 4 that the quality of 
adaptation seems to depend largely upon the size of 
adaptation data: we outperformed the best bakeoff 
systems in the AS set because the size of the adap-
tation data is big while we are worse in the CTB set 
because of the small size of the adaptation data. To 
verify our speculation, we evaluated the adaptation 
results using subsets of the AS training set of dif-
ferent sizes, and observed the same trend. However, 
even with a much smaller adaptation data set (e.g. 
250K), we still outperform the best bakeoff results. 
6 Related Work 
Many methods of Chinese word segmentation have 
been proposed (See Wu and Tseng, 1993; Sproat 
and Shih, 2001 for reviews). However, it is difficult 
to compare systems due to the fact that there is no 
widely accepted standard. There has been less work 
on dealing with NWI and standard adaptation. 
All feature functions in Figure 1, except the NW 
function, are derived from models presented in 
(Gao et al, 2003). The linear models are similar to 
what was presented in Collins and Duffy (2001). An 
alternative to linear models is the log-linear models 
suggested by Och (2003). See Collins (2002) for a 
comparison of these approaches. 
The features for NWI were studied in Wu & 
Jiang (2000) and Li et al (2004). The use of sam-
pling was proposed in Della Pietra et al (1997) and 
Rosenfeld et al (2001). There is also a related work 
on this line in Japanese (Uchimoto et al, 2001). 
A detailed discussion on differences among the 
four Bakeoff standards is presented in Wu (2003), 
which also proposes an adaptive system where the 
display of the output can be customized by users. 
The method described in Section 4 can be viewed as 
an improved version in that the transformations are 
learnt automatically from adaptation data. The use 
of TBL for Chinese word segmentation was first 
suggested in Palmer (1997). 
7 Conclusion 
This paper presents a statistical approach to adap-
tive Chinese word segmentation based on linear 
models and TBL. The system has two components: 
A generic segmenter that can adapt to the vocabu-
laries of different domains, and a set of output 
adaptors, learned from application data, for adapt-
ing to different ?application-specific? standards. 
We evaluate our system on five test sets, each cor-
responding to a different standard. We achieve 
state-of-the-art performance on all test sets. 
References 
Brill, Eric. 1995. Transformation-based error-driven 
learning and natural language processing: a case study 
in Part-of-Speech tagging. In: Computational Linguis-
tics, 21(4). 
Collins, Michael and Nigel Duffy. 2001. Convolution 
kernels for natural language. In: Advances in Neural 
Information Processing Systems (NLPS 14). 
Collins, Michael. 2002. Parameter estimation for statis-
tical parsing models: theory and practice of distribu-
tion-free methods. To appear. 
Della Pietra, S., Della Pietra, V., and Lafferty, J. 1997. 
Inducing features of random fields. In: IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 19, 
380-393. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 
2001. Pattern classification. John Wiley & Sons, Inc. 
Gao, Jianfeng and Kai-Fu Lee. 2000. Distribution based 
pruning of backoff language models. In: ACL2000. 
Gao, Jianfeng, Mu Li and Chang-Ning Huang. 2003. 
Improved source-channel model for Chinese word 
segmentation. In: ACL2003.  
Katz, S. M. 1996. Distribution of content words and 
phrases in text and language modeling, In: Natural 
Language Engineering, 1996(2): 15-59 
Li, Hongqiao, Chang-Ning Huang, Jianfeng Gao and 
Xiaozhong Fan. 2004. The use of SVM for Chinese 
new word identification. In: IJCNLP2004. 
Manning, C. D. and H. Sch?tze, 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Press.  
Mitchell, Tom M. 1997. Machine learning. The 
McGraw-Hill Companies, Inc. 
Och, Franz. 2003. Minimum error rate training in statis-
tical machine translation. In: ACL2003. 
Palmer, D. 1997. A trainable rule-based algorithm for 
word segmentation. In: ACL '97. 
Rosenfeld, R., S. F. Chen and X. Zhu. 2001. Whole 
sentence exponential language models: a vehicle for 
linguistic statistical integration. In: Computer Speech 
and Language, 15 (1). 
Sproat, Richard and Chilin Shih. 2002. Corpus-based 
methods in Chinese morphology and phonology. In: 
COLING 2002.  
Sproat, Richard and Tom Emerson. 2003. The first 
international Chinese word segmentation bakeoff. In: 
SIGHAN 2003. 
Uchimoto, K., S. Sekine and H. Isahara. 2001. The 
unknown word problem: a morphological analysis of 
Japanese using maximum entropy aided by a diction-
ary. In: EMNLP2001. 
Wu, Andi and Zixin Jiang. 2000. Statistically-enhanced 
new word identification in a rule-based Chinese system. 
In: Proc of the 2rd ACL Chinese Processing Workshop. 
Wu, Andi. 2003. Customizable segmentation of mor-
phologically derived words in Chinese. In: Interna-
tional Journal of Computational Linguistics and Chi-
nese Language Processing, 8(1): 1-27. 
Wu, Zimin and Gwyneth Tseng. 1993. Chinese text 
segmentation for text retrieval achievements and prob-
lems. In: JASIS, 44(9): 532-542. 
Extraction of Chinese Compound Words - 
An Experimental Study on a Very Large Corpus 
Jian Zhang 
Department of Computer Science and 
Technology of Tsinghua University, China 
ajian@sl000e.cs.tsinghua.edu.cn 
Jianfeng Gao, Ming Zhou 
Microsoft Research China 
{jfgao, mingzhou }@microsoft.corn 
Abstract 
This paper is to introduce a statistical 
method to extract Chinese compound words 
from a very large corpusL This method is 
based on mutual information and context 
dependency. Experimental results show that 
this method is efficient and robust 
compared with other approaches. We also 
examined the impact of different parameter 
settings, corpus size and heterogeneousness 
on the extraction results. We finally present 
results on information retrieval to show the 
usefulness of extracted compounds. 
1 Introduction 
Almost all techniques to statistical anguage 
processing, including speech recognition, machine 
translation and information retrieval, are based on 
words. Although word-based approaches work 
very well for western languages, where words are 
well defined, it is difficult to apply to Chinese. 
Chinese sentences are written as characters strings 
with no spaces between words. Therefore, words 
in Chinese are actually not well marked in 
sentences, and there does not exist a commonly 
accepted Chinese lexicon. 
Furthermore, since new compounds (words 
formed with at least wo characters) are constantly 
created, it is impossible to list them exhaustively 
in a lexicon. Therefore, automatic extraction of 
compounds is an important issue. Traditional 
extraction approaches used rules. However, 
compounds extracted in this way are not always 
desirable. So, human effort is still required to find 
the preferred compounds from a large compound 
l This work was done while the author worked for 
Microsoft Research China as a visiting student. 
candidate list. Some statistical approaches to 
extract Chinese compounds from corpus have 
been proposed (Lee-Feng Chien 1997, WU Dekai 
and Xuanyin XIA 1995, Ming-Wen Wu and Keh- 
Yih Su 1993) as well, but almost all experiments 
are based on relatively small corpus, it is not clear 
whether these methods till work well with large 
corpus. 
In this paper, we investigate statistical 
approaches to Chinese compound extraction from 
very large corpus by using statistical features, 
namely mutual information and context 
dependency. There are three main contributions in
this paper. First, we apply our procedure on a very 
large corpus while other experiments were based 
on small or medium size corpora. We show that 
better esults can be obtained with a large corpus. 
Second, we examine how the results can be 
influenced by parameter settings including mutual 
information and context dependency restrictions. 
It turns out that mutual information mainly affects 
precision while context dependency affects the 
count of extracted items. Third, we test the 
usefulness of the extracted compounds for 
information retrieval. Our experimental results on 
IR show that the new compounds have a positive 
effect on IR. 
The rest of this paper is structured as follows. In 
section 2, we describe the techniques we used. In 
section 3, we present several sets of experimental 
results. In section 4, we outline the related works 
as well as their results. Finally, we give our 
conclusions in section 5. 
2 Technique description 
Statistical extraction of Chinese compounds has 
been used in (Lee-Feng Chien 1997)(WU Dekai 
and Xuanyin XIA 1995) and (Ming-Wen Wu and 
Keh-Yih Su 1993). The basic idea is that a 
132 
Chinese compound should appear as a stable 
sequence in corpus. That is, the components in the 
compound are strongly correlated, while the 
components lie at both ends should have low 
correlations with otiter words. 
The method consists of two steps. At fast, a fist 
of candidate compounds i extracted from a very 
large corpus by using mutual information. Then, 
context dependency is used to remove undesirable 
compounds. In what follows, we will describe 
them in more detail. 
2.1 Mutual Information 
According to our study on Chinese corpora, 
most compounds are of length less than 5 
characters. The average length of words in the 
segmented-corpus i of approximately 1.6 
characters. Therefore, only word bi-gram, tn'- 
gram, and quad-gram in the corpus are of interest 
to us in compound extraction. 
We use a criterion, called mutual inform~on, 
to evaluate the correlation of different components 
in the compound. Mutual information Ml(x,y) of a 
bi-gram (x, y) is estimated by: 
Ml(x ,y )  = f (x ,y )  
f (x )  + f (y ) -  f (x ,y )  
Where f(x) is the occurrence frequency of word 
x in the corpus, and fix, y) is the occurrence 
frequency of the word pair (x,y) in the corpus. The 
higher the value of MI is, the more likely x and y 
are to form a compound. 
The mutual information MI(x,y,z) of tri-gram 
(x,y,z) is estimated by: 
Ml(x,  y, z) = f (x ,  y, z) 
f (x)  + f (y)  + f ( z ) -  f(x,  y, z) 
The estimation of mutual information of quad- 
grams is similar to that of tri-grams. The extracted 
compounds should be of higher value of MI than a 
pre-set threshold. 
2.2 Context Dependency 
Figure 1 
The extracted Chinese compounds hould be 
complete. That is, we should generate a whole 
word, not a part of it. For example, 
~,~-~-~t'~J(missi le defense plan) is a 
complete word, and -~.~0-1~J~ (missile defense) is
not, although both have relatively high value of 
mutual information. 
Therefore, we use another feature, called 
context dependency. The contexts of the word 
l~(defense)  are illustrated by figure 1. 
A compound X has NO left context dependency 
if 
LSize -~ L I > tl or 
f (ctX)  MaxL = MAX ~ - -  < t2 
f (X )  
Where tl, t2 are threshold value, j\[.) is 
frequency, L is the set of left adjacent strings of X, 
tz~L and ILl means the number of unique left 
adjacent strings. Similarly, a compound X has NO 
right context dependency if 
RSize ~ R l> t3 or 
f ( /~)  < t4 MaxR = MAX a f ( X ) 
Where tl, t2, t3, t4 are threshold value, f(.) is 
frequency, R is the set of right adjacent strings of 
X, tiER and \[R I means the number of unique left 
adjacent strings. 
The extracted complete compounds should have 
neither left nor fight context dependency. 
3 Experimental results 
In our experiments, three corpora were used to 
test the performance of the presented approach. 
These corpora re described in table 1. Corpus A 
consists of local news with more than 325 million 
characters. Corpus B consists of documents from 
different domains of novel, news, technique 
report, etc., with approximately 650 million 
characters. Corpus C consists of People's Daily 
news and Xinhua news from TREC5 and TREC6 
(Harman and Voorhees, 1996) with 75 million 
characters. 
Table 1: Characteristics of Corpora 
!Corpus Source Size(char#) 
Corpus political, economic news 325 M 
A 
Corpus Corpus A + novels + 650 M 
B technique reports, etc. 
133 
'cCOrpus \[TREC 5/6 Chinese 75 M 
,, ,coMus I I 
In the first experiment, we test the perfomaance 
of our method on corpus A, which is homogeneity 
in style. We then use corpus B in the second 
experiment totest if the method works as well on 
the corpus that is heterogeneity n style. We also 
use different parameter settings in order to figure 
out the best combination of the two statistical 
features, i.e. mutual information and context 
dependency. In the third experiment, we apply the 
results of the method to information retrieval 
system. We extract new compounds on corpus C, 
and add them to the indexing lexicon, and we 
achieve a higher average precision-recall. In all 
experiments, corpora re segmented automatically 
into words using a lexicon consisting of 65,502 
entries. 
3.1 Compounds Extraction from Homogeneous 
Corpus 
Corpus A contains political and economic news. 
In this series of tests, we gradually loosen the 
conditions to form a compound, i.e. MI threshold 
becomes smaller and MaxL/MaxR becomes larger. 
Results for quad-graras, tri-graras and bi-grams 
are shown in tables 2,3,4. Some compounds 
extracted are fisted in table 5. 
1 
2 
3 
4 
5 
Table2: Performance of quad-sram compounds 
Parameter setting 
(MI, LSize, MaxL, RSize, MaxR) compounds found 
0.01 1 0.75 1 0.75 ! 10 
extraction 
Number of New Precision 
27 
(correct 
compounds/compounds checked) 
0% (27/27) 
0.005 1 0.85 1 0.85 92 98.9% (91/92) 
0.002 1 0.90 1 0.90 513 95.8% (113/118) 
0.001 1 0.95 1 0.95 1648 96.2% (179/186) 
0.0005 1 0.95 1 0.95 4707 96.7% (206/213) 
1 
2 
3 
4 
5 
Table3: Performance of tri-sram compounds extraction 
Parameter setting 
(MI, LSize, MaxL, RSize, MaxR) 
0.02 2 0.70 2 0.70 
Number of New 
compounds found 
167 
Precision (correct 
compounds/compounds checked) 
100% (167/167) 
0.01 2 0.75 2 0.75 538 100% (205/205) 
0.005 2 0.80 2 0.80 1607 100% (262/262) 
0.003 2 0.80 2 0.80 3532 98.3% (341/347) 
0.001 2 0.80 2 0.80 16849 96.6% (488/501) 
1 
2 
3 
4 
5 
Table4: Performance of bi-sram compounds extraction 
Parameter setting Number of New Precision 
(MI, LSize, MaxL, RSize, MaxR) compounds found compound# 
0.05 3 0.5 3 0.5 1622 
(correct 
:~ s/compounds checked) 
98.9% (184/186) 
0.05 3 0.6 3 0.6 1904 98.6% (309/212) 
0.03 3 0.6 3 0.6 3938 97.8% (218/223) 
0.01 3 0.5 3 0.5 14666 97.5% (354/363) 
0.005 3 0.5 3 0.5 32899 97.3% (404/415) 
N-gram 
N=2 
N=3 
N=4 
Table 5: Some N-gram compounds found by our method 
Extracted Compounds 
~\]Jg~(graindepot), ~~J~(CD-ROM Driver), ~ \ [~(B i l l  Gates) 
(XuanWu Gate), (asynchronous t ransfer  model), 
(Amazon) 
~\[~\ [~\ [~ (Eiysee). ~\]~\[~i\[~H (Ohio), ~\ [~\ [~\ [~ (Mr. Dong Jianhua) 
134 
It turns out that our algorithm successfully 
extracted a large number of new compounds 
(>50000) from raw texts. Compared with previous 
methods described in the next section, the 
precision is very high. We can also find that there 
is little precision loss when we loose restriction. 
The result may be due to three reasons. First, the 
two statistical features really characterize the 
nature of compounds, and provide a simple and 
efficient way to estimate the possibility of a word 
sequence being a compound. Second, the corpus 
we use is very large. It is always true that more 
data leads to better esults. Third, the corpus we 
used in this experiment is homogeneity in style. 
The raw corpus is composed of news on politics, 
economy, science and technology. These are 
formal articles, and the sentences and compounds 
are well normalized and strict. This is very helpful 
for compound extraction. 
3.2 Compounds Extraction from Heterogeneous 
Corpus 
In this experiment, we use a heterogeneous 
corpus. It is a combination of corpus A, and some 
other novels, technique reports, etc. For 
simplicity, we discuss the extraction of bi-gram 
compounds only. In comparison with the first 
experiment, we find that the precision is strongly 
affected by the corpus we used. As shown in table 
6, for each corpus, we use the same parameter 
setting, say MI >0.005, LSize >3, MaxL <0.5, 
RSize>3 and MaxR<0.5. 
Table 6: Impact of heterogeneousness of corpora 
Corpus Compounds Extract 
extracted precision 
Corpus A 32899 97.3% 
(4041415) 
Corpus B 36383 88.3% 
(362/410) 
As we mentioned early, the larger the corpus we 
use, the better results we obtain. Therefore, we 
intuitively expect better esult on corpus B, which 
is larger than corpus A. But, the result shown in 
table 6 is just the opposite. 
There are mainly two reasons for this. The first 
one is that our method works better on 
homogeneous corpus than on heterogeneous 
corpus. The second one is that it might not be 
suitable to use the same parameter settings on two 
different corpora. We then try different parameter 
settings on corpus B. 
There are two groups of parameters. MI 
measures the correlation between adjacent words, 
and other four parameters, namely LSize, RSize, 
MaxL, and MaxR, measure the context 
dependency. Therefore, each time, we fix one 
parameter, and relax another from fight to loose to 
see what happens. The Number of extracted 
compounds and precision of each parameter 
setting are shown in table 7. 
MRCD 
0.0002 
0.0004 
0.0006 
0.0008 
0.0010 
0.0012 
0.0014 
Table 7: Extraction results with different parameter settings 
(Ml=Mutual Information, CD = Context Dependency=(LSize, MaxL, RSize, MaxR 
(2, 0.8, 2, (6, 0.7, 6, (10, 0.6, (14, 0.5, 4, (18, 0.4, (22, 0.3, 
0.8) 
1457781 
(39.06%) 
784082 
(48.98%) 
530723 
(51.28%) 
396602 
(54.63%) 
313868 
(59.11%) 
257990 
(58.94%) 
217766 
(58.93%) 
0.7) 
809502 
(42.24%) 
485143 
(46.84%) 
349882 
(53.96%) 
273231 
(58.00%) 
223827 
(66.51%) 
189014 
(59.50%) 
163189 
(67.91%) 
10, 0.6) 
570601 
(43.98%) 
359499 
(52.53%) 
266068 
(60.39%) 
211044 
(55.19%) 
175050 
(61.14%) 
149315 
(60.98%) 
129978 
(60.19%) 
0.5) 
426223 
(44.67%) 
277673 
(49.25%) 
208921 
(52.48%) 
167660 
(65.24%) 
140197 
(57.66%) 
120312 
(65.28%) 
105334 
(65.84%) 
18, 0.4) 
314810 
(43.96%) 
209634 
(53.92%) 
159363 
(49.49%) 
128819 
(60.54%) 
108322 
(67.38%) 
93323 
(70.47%) 
82083 
(66.83%) 
22, 0.3) 
209910 
(43.38%) 
141215 
(49.55%) 
108120 
(63.35%) 
87869 
(64.40%) 
74104 
(63.08%) 
64079 
(65.32%) 
56582 
(67.50%) 
(26, 0.2, 6, 
0.2) 
96383 
(40.93%) 
63907/ 
(52.53%) 
48683 
(61.65%) 
39502 
(54.86%) 
33354 
(67.50%) 
28879 
(64.65%) 
25486 
(65.46%) 
135 
Table 7 shows the extraction results with 
different parameters. These results fit our 
intuition. While parameters become more and 
more strict, less and less compounds are found 
and precisions become higher. This phenomena is
also illustrated in figure 2 and 3, in which the 
"correct compounds extracted" is an estimation 
from tableT, i.e. number of compounds found x 
precision. (These two figures are very useful for 
one who wants to automatically extract a new 
lexicon with pre-defined size from a large corpus.) 
600 
500 
400 
300 
~ 2oo 
~ loo 
o 
o 
2 3 4 5 6 7 
mutual information 
Figure 2 Impact of Parameter Mutual Information 
-(2 0.8 2 0.8) 
-(6 0.7 6 0.7) 
? (I0 0.6 I0 0.6) 
-(14 0.5 14 0.5) 
-(18 0.4 18 0.4) 
-(22 0.3 22 0.3) 
-(26 0.2 26 0.2) 
o 
O) 
o~ 
g 
g 
600 
500 
400 
300 
200 
100 
0 
2 3 4 5 6 7 
context dependency 
ix=0.0002 
ix=0.0004 
ix=0.0006 
ix=0.0008 
ix=0.0010 
ix=0.0012 
ix=0.0014 
Figure 3 Impact of Parameter Context Dependency 
136 
The precision of extraction is estimated in the 
following way. We extract a set of compounds 
based on a seres of pre-defined parameter set. For 
each set of compotinds, we randomly select 200 
compounds. Then we merge those selected 
compounds to a new file for manually check. This 
file consists of about 9,800 new compounds 
because there are 49 compounds lists. One person 
will group these 'compounds' into two sets, say 
set A and set B. Set A contains the items that are 
considered to be correct, and set B contains 
incorrect ones. Then for each original group of 
about 200 compounds we select in the first step, 
we check how many items that also appear in set 
A and how many items in set B. Suppose these 
two values are al and bl, then we estimate the 
precision as al/(al+bl). 
So, there are two important points in our 
evaluation process. First, it is difficult to give a 
definition of the term "compound" to be accepted 
popularly. Different people may have different 
judgement. Only one person takes part in the 
evaluation in our experiment. This can eliminate 
the effect of divergence among different persons. 
Second, we merge those items together. This can 
eliminate the effect of different ime period. One 
may feel tired after checked too many items. If he 
checks those 49 files one by one, the latter results 
are incomparable with the previous one. 
The precisions estimated by the above method 
are not exactly correct. However, as described 
above, the precisions of different parameter 
settings are comparable. In this experiment, what 
we want to show is how the parameter settings 
affect he results. 
Both MI and CD can affect number of extracted 
compounds, as shown in table 7. Compared with 
MI, CD has stronger effect in this aspect. For each 
row in table 7, numbers of extracted compounds 
finally decrease to 10% of that showed in the first 
column. For each column, while MI changes from 
0.0002 to 0.0014, the number is decreased of 
about 20%. This may be explained by the fact that 
it is difficult for candidate to fulfill all four 
restrictions in CD simultaneously. Many 
disqualified candidates are cut off. Table 7 lists 
the precisions of extracted results. It shows that 
there is no clear increasing/decreasing pattern in 
each row. That is to say, CD doesn't strongly 
affect he precision. When we check each column, 
we can see that precision is in a growing progress. 
As we defined above, MI and CD are two 
different measurements. What role they play in 
our extraction procedure? Our conclusion is that 
mutual information mainly affects the precision 
while context dependency mainly affects the count 
of extracted items. This conclusion is also 
confirmed by Fig2 and Fig3. That is, the curves in 
Fig2 are more fiat than corresponding curves in 
Fig3. 
3.3 Testing the Extracted Compounds in 
Information Retrieval 
In this experiment, we apply our method to 
improve information retrieval results. We use 
SMART system (Buckley 1985) for our 
experiments. SMART is a robust, efficient and 
flexible information retrieval system. The corpus 
used in this experiment is TREC Chinese corpus 
(Harman and Voorhees, 1996). The corpus 
contains about 160,000 articles, including articles 
published in the People's Daily from 1991 to 
1993, and a part of the news released by the 
Xinhua News Agency in 1994 and 1995. A set of 
54 queries has been set up and evaluated by 
people in NIST(Nafional Institute of Standards 
and Technology). 
We first use an initial lexicon consisting of 
65,502 entries to segment the corpus. When 
running SMART on the segmented corpus, we 
obtain an average precision of 42.90%. 
Then we extract new compounds from the 
segmented corpus, and add them into the initial 
lexicon. With the new lexicon, the TREC Chinese 
corpus is re-segmented. When running SMART 
on this re-segmented corpus, we obtain an average 
precision of 43.42%, which shows a slight 
improvement of 1.2%. 
Further analysis shows that the new lexicon 
brings positive effect to 10 queries and negative 
effect to 4 queries. For other 40 queries, there is 
no obvious effect. Some improved queries are 
listed in table 8 as well as new compounds being 
contained. 
As an example, we give the segmentation 
results with the two lexicons for query 23 in table 
9. 
137 
Query 
ID 
9 
23 
Base line 
precision 
0.3648 
0.3940 
New 
precision 
0.4173 
0.5154 
Table 8: Improved Query Samples 
Improvement Extracted compounds 
14.4% 
30.8% 
ME(drugs 
sale), ~ \[\] :~li~ Ih\] ~(Drug Problems 
in China) 
I~ -~- \ [ \ ] : '~( the  UN Security 
Council),~l\] ~1~ ,~(peace proposal) 
30 0.3457 0.3639 5.3% 
46 0.3483 0.4192 20.4% ~ ~(Claina nd Vietnam) 
47 0.5369 0.5847 8.9% /~ 1~ ~-~k~ tl.l (Mount 
Minatubo),~U-~(ozone layer), 
~ ~(Subic) 
Table 9: Se~rnented Corpus with the Two Lexicons for Query 23 
Query 23 segment with small lexicon 
, bk 
Query 23 segment with new lexicon 
Another interesting example is query 30. There 
is no new compound extracted from that query. Its 
result is also improved significantly because its 
relevant documents are segmented better than 
before. 
Because the compounds extracted from the 
corpus are not exactly correct, the new lexicon 
will bring negative ffect o some queries, such as 
query 10. The retrieval precision changes from 
0.3086 to 0.1359. The main reason is that 
"~ \ [ \ ]~" (Ch inese  XinJiang) is taken as a new 
compound in the query. 
4 Related works 
Several methods have been proposed for 
extracting compounds from corpus by statistical 
approaches. In this section, we will briefly 
describe some of them. 
(Lee-Feng Chien 1997) proposed an approach 
based on PAT-Tree to automatically extracting 
domain specific terms from online text 
collections. Our method is primary derived from 
(Lee-Feng Chien 1997), and use the similar 
statistical features, i.e. mutual informan'on and 
context dependency. The difference is that we use 
n-gram instead of PAT-Tree, due to the efficiency 
issue. Another difference lies in the experiments. 
In Chien's work, only domain specific terms are 
extracted from domain specific corpus, and the 
size of the corpus is relatively small, namely 
1,872 political news abstracts. 
(Cheng-Huang Tung and His-Jian Lee 1994) 
also presented an efficient method for identifying 
unknown words from a large corpus. The 
statistical features used consist of string (character 
sequence) frequency and entropy of left/fight 
neighbonng characters (similar to left/fight 
context dependency). The corpus consists of 
178,027 sentences, representing a total of more 
than 2 million Chinese characters. 8327 unknown 
words were identified and 5366 items of them 
were confirmed manually. 
(Ming-Wen Wu and Keh-Yih Su 1993) 
presented a method using mutual information and 
relative frequency. 9,124 compounds are extracted 
from the corpus consists of 74,404 words, with the 
precision of 47.43%. In this method, the 
compound extraction problem is formulated as 
classification problem. Each bi-grarn (tri-grarn) is 
assigned to one of those two clusters. It also needs 
a training corpus to estimate parameters for 
classification model. In our method, we didn't 
138 
make use of any training corpus. Another 
difference is that they use the method for English 
compounds extraction while we extract Chinese 
compounds in our experiments. 
(Pascale Fung 1998) presented two simple 
systems for Chinese compound extraction---- 
CXtract. CXtract uses predominantly statistical 
lexical information to find term boundaries in 
large text. Evaluations on the corpus consisting of 
2 million characters show that the average 
precision is 54.09%. 
We should note that since the experiment setup 
and evaluation systems of the methods mentioned 
above are not identical, the results are not 
comparable. However, by showing our 
experimental results on much larger and 
heterogenous corpus, we can say that our method 
is an efficient and robust one. 
5 Conclusion 
In this paper, we investigate a statistical 
approach to Chinese compounds extraction from 
very large corpora using mutual information and 
context dependency. 
We explained how the performance can be 
influenced by different parameter settings, corpus 
size, and corpus heterogeneousness. We also 
refine the lexicon with information retrieval 
system by adding compounds obtained by our 
methods, and achieve 1.2% improvements on 
precision of IR. 
Through our experiments, we conclude that 
statistical method based on mutual information 
and context dependency is efficient and robust for 
Chinese compounds extraction. And, mutual 
information mainly affects the precision while 
context dependency mainly affects the count of 
extracted items. 
Reference 
Lee-Feng Chien, (1997) "PAT-tree-based keyword 
extraction for Chinese Information retrieval", ACM 
SIGIR'97, Philadelphia, USA, 50-58 
WU, Dekai and Xuanyin XIA. (1995). "Large-scale 
automatic extraction of an English-Chinese l xicon", 
Machine Translation 9(3-4), pp.285-313. 
Ming-Wen Wu and Keh-Yih Su. (1993). "Corpus- 
based Automatic Compound Extraction with Mutual 
Information and Relative Frequency Count," 
Proceedings of R. 0. C. Computational Linguistics 
Conference V I .  Nantou, Taiwan, R. O. C., pp.207- 
216. 
Pascale Fung. (1998). "Extracting Key Terms from 
Chinese and Japanese texts ". The International 
Journal on Computer Processing of Oriental 
Language, Special Issue on Information Retrieval on 
Oriental Languages, pp.99-121. 
Cheng-Huang Tung and His-Jian Lee. (1994). 
"Identification of Unknown Words From a Corpus". 
Compouter Processing of Chinese and Oriental 
Languages Vol.8, pp. 131 -145. 
Buckley, C. (1985). Implementation f the SMART 
information retrieval system, Technical report, #85- 
686, Cornell University. 
Harman, D. K. and Voorhees, E. M., Eds. (1996). 
Information Technology: The Fifth Text Retrieval 
Conference(TREC5), NIST SP 500-238. 
Gaithersburg, National Institute fo standards and 
Technology. 
139 
Exploiting Headword Dependency and Predictive Clustering for 
Language Modeling  
 
Jianfeng Gao  
Microsoft Research, Asia  
Beijing, 100080, China  
jfgao@microsoft.com 
Hisami Suzuki  
Microsoft Research 
 Redmond WA 98052, USA  
hisamis@microsoft.com 
Yang Wen* 
Department of Computer & 
Information Sciences of 
Tsinghua University, China 
  
                                                     
* This work was done while the author was visiting Microsoft Research Asia.  
Abstract 
This paper presents several practical ways 
of incorporating linguistic structure into 
language models. A headword detector is 
first applied to detect the headword of each 
phrase in a sentence. A permuted headword 
trigram model (PHTM) is then generated 
from the annotated corpus. Finally, PHTM 
is extended to a cluster PHTM (C-PHTM) 
by defining clusters for similar words in the 
corpus. We evaluated the proposed models 
on the realistic application of Japanese 
Kana-Kanji conversion. Experiments show 
that C-PHTM achieves 15% error rate 
reduction over the word trigram model. This 
demonstrates that the use of simple methods 
such as the headword trigram and predictive 
clustering can effectively capture long 
distance word dependency, and 
substantially outperform a word trigram 
model. 
1 Introduction 
In spite of its deficiencies, trigram-based language 
modeling still dominates the statistical language 
modeling community, and is widely applied to tasks 
such as speech recognition and Asian language text 
input (Jelinek, 1990; Gao et al, 2002).  
Word trigram models are deficient because they 
can only capture local dependency relations, taking 
no advantage of richer linguistic structure. Many 
proposals have been made that try to incorporate 
linguistic structure into language models (LMs), but 
little improvement has been achieved so far in 
realistic applications because (1) capturing longer 
distance word dependency leads to higher-order 
n-gram models, where the number of parameters is 
usually too large to estimate; (2) capturing deeper 
linguistic relations in a LM requires a large amount 
of annotated training corpus and a decoder that 
assigns linguistic structure, which are not always 
available. 
This paper presents several practical ways of 
incorporating long distance word dependency and 
linguistic structure into LMs. A headword detector 
is first applied to detect the headwords in each 
phrase in a sentence. A permuted headword trigram 
model (PHTM) is then generated from the 
annotated corpus. Finally, PHTM is extended to a 
cluster model (C-PHTM), which clusters similar 
words in the corpus.  
Our models are motivated by three assumptions 
about language: (1) Headwords depend on previous 
headwords, as well as immediately preceding 
words; (2) The order of headwords in a sentence can 
freely change in some cases; and (3) Word clusters 
help us make a more accurate estimate of the 
probability of word strings. We evaluated the 
proposed models on the realistic application of 
Japanese Kana-Kanji conversion, which converts 
phonetic Kana strings into proper Japanese 
orthography. Results show that C-PHTM achieves a 
15% error rate reduction over the word trigram 
model. This demonstrates that the use of simple 
methods can effectively capture long distance word 
dependency, and substantially outperform the word 
trigram model. Although the techniques in this 
paper are described in the context of Japanese 
Kana-Kanji conversion, we believe that they can be 
extended to other languages and applications. 
This paper is organized as follows. Sections 2 
and 3 describe the techniques of using headword 
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 248-256.
                         Proceedings of the Conference on Empirical Methods in Natural
dependency and clustering for language modeling. 
Section 4 reviews related work. Section 5 
introduces the evaluation methodology, and Section 
6 presents the results of our main experiments. 
Section 7 concludes our discussion.  
2 Using Headwords 
2.1 Motivation 
Japanese linguists have traditionally distinguished 
two types of words1, content words (jiritsugo) and 
function words (fuzokugo), along with the notion of 
the bunsetsu (phrase). Each bunsetsu typically 
consists of one content word, called a headword in 
this paper, and several function words. Figure 1 
shows a Japanese example sentence and its English 
translation2.  
[??+?][??+??][??+??][??+?][??+?][??+?]
[chiryou+ni]   [sennen+shite]       [zenkai+made] 
[treatment+to][concentration+do][full-recovery+until] 
[juubun+na]      [ryouyou+ni]  [tsutome+ru] 
[enough+ADN] [rest+for]        [try+PRES] 
'(One) concentrates on the treatment and tries to rest 
enough until full recovery' 
Figure 1. A Japanese example sentence with 
bunsetsu and headword tags 
In Figure 1, we find that some headwords in the 
sentence are expected to have a stronger 
dependency relation with their preceding 
headwords than with their immediately preceding 
function words. For example, the three headwords 
??~??~??  (chiryou 'treatment' ~ sennen 
'concentrate' ~ zenkai 'full recovery') form a trigram 
with very strong semantic dependency. Therefore, 
we can hypothesize (in the trigram context) that 
headwords may be conditioned not only by the two 
immediately preceding words, but also by two 
previous headwords. This is our first assumption.  
We also note that the order of headwords in a 
sentence is flexible in some sense. From the 
                                                     
1 Or more correctly, morphemes. Strictly speaking, the 
LMs discussed in this paper are morpheme-based models 
rather than word-based, but we will not make this 
distinction in this paper.  
2 Square brackets demarcate the bunsetsu boundary, and 
+ the morpheme boundary; the underlined words are the 
headwords. ADN indicates an adnominal marker, and 
PRES indicates a present tense marker.  
example in Figure 1, we find that if ??~??~?? 
(chiryou 'treatment' ~ sennen 'concentrate' ~ zenkai 
'full recovery') is a meaningful trigram, then its 
permutations (such as ??~??~?? (zenkai 'full 
recovery' ~ chiryou 'treatment' ~ sennen 
'concentrate')) should also be meaningful, because 
headword trigrams tend to capture an order-neutral 
semantic dependency. This reflects a characteristic 
of Japanese, in which arguments and modifiers of a 
predicate can freely change their word order, a 
phenomenon known as "scrambling" in linguistic 
literature. We can then introduce our second 
assumption: headwords in a trigram are permutable. 
Note that the permutation of headwords should be 
useful more generally beyond Japanese: for 
example, in English, the book Mary bought and 
Mary bought a book can be captured by the same 
headword trigram (Mary ~ bought ~ book) if we 
allow such permutations. 
In this subsection, we have stated two 
assumptions about the structure of Japanese that can 
be exploited for language modeling. We now turn to 
discuss how to incorporate these assumptions in 
language modeling.  
2.2 Permuted headword trigram model 
(PHTM) 
A trigram model predicts the next word wi by 
estimating the conditional probability P(wi|wi-2wi-1), 
assuming that the next word depends only on two 
preceding words, wi-2 and wi-1. The PHTM is a 
simple extension of the trigram model that 
incorporates the dependencies between headwords. 
If we assume that each word token can uniquely be 
classified as a headword or a function word, the 
PHTM can be considered as a cluster-based 
language model with two clusters, headword H and 
function word F. We can then define the conditional 
probability of wi based on its history as the product 
of the two factors: the probability of the category (H 
or F), and the probability of wi given its category. 
Let hi or fi be the actual headword or function word 
in a sentence, and let Hi or Fi be the category of the 
word wi. The PHTM can then be formulated as: 
=? ? ))...(|( 11 ii wwwP   (1) 
))...(|())...(|( 1111 iiiii HwwwPwwHP ?? ???   
))...(|())...(|( 1111 iiiii FwwwPwwFP ?? ???+   
where ? is a function that maps the word history 
(w1?wi-1) onto equivalence classes. 
P(Hi|?(w1?wi-1)) and P(Fi|?(w1?wi-1)) are 
category probabilities, and P(wi|?(w1?wi-1)Fi) is 
the word probability given that the category of wi is 
function word. For these three probabilities, we 
used the standard trigram estimate (i.e., ?(w1?wi-1) 
= (wi-2wi-1)). The estimation of headword 
probability is slightly more elaborate, reflecting the 
two assumptions described in Section 2.1:  
)|(())...(|( 122111 iiiiiii HhhwPHwwwP ??? =? ??   (2) 
))|()1( 212 iiii HhhwP ???+ ?   
)|()1( 121 iiii HwwwP ???+ ? .  
This estimate is an interpolated probability of three 
probabilities: P(wi|hi-2hi-1Hi) and P(wi|hi-1hi-2Hi), 
which are the headword trigram probability with or 
without permutation, and P(wi|wi-2wi-1Hi), which is 
the probability of wi given that it is a headword, 
where hi-1 and hi-2 denote the two preceding 
headwords, and ?1, ?2 ? [0,1] are the interpolation 
weights optimized on held-out data. 
The use of ?1 in Equation (2) is motivated by the 
first assumption described in Section 2.1: 
headwords are conditioned not only on two 
immediately preceding words, but also on two 
previous headwords. In practice, we estimated the 
headword probability by interpolating the 
conditional probability based on two previous 
headwords P(wi|hi-2hi-1Hi) (and P(wi|hi-1hi-2Hi) with 
permutation), and the conditional probability based 
on two preceding words P(wi|wi-2wi-1Hi). If ?1 is 
around zero, it indicates that this assumption does 
not hold in real data. Note that we did not estimate 
the conditional probability P(wi|wi-2wi-1hi-2hi-1Hi) 
directly, because this is in the form of a 5-gram, 
where the number of parameters are too large to 
estimate. 
The use of ?2 in Equation (2) comes from the 
second assumption in Section 2.1: headword 
trigrams are permutable. This assumption can be 
formulated as a co-occurrence model for headword 
prediction: that is, the probability of a headword is 
determined by the occurrence of other headwords 
within a window. However, in our experiments, we 
instead used an interpolated probability 
?2?P(wi|hi-2hi-1Hi) + (1??2)?P(wi|hi-1hi-2Hi) for two 
reasons. First, co-occurrence models do not predict 
words from left to right, and are thus very difficult 
to interpolate with trigram models for decoding. 
Second, if we see n-gram models as one extreme 
that predicts the next word based on a strictly 
ordered word sequence, co-occurrence models go to 
the other extreme of predicting the next word based 
on a bag of previous words without taking word 
order into account at all. We prefer models that lie 
somewhere between the two extremes, and consider 
word order in a more flexible way. In PHTM of 
Equation (2), ?2 represents the impact of word order 
on headword prediction. When ?2 = 1 (i.e., the 
resulting model is a non-permuted headword 
trigram model, referred to as HTM), it indicates that 
the second assumption does not hold in real data. 
When ?2 is around 0.5, it indicates that a headword 
bag model is sufficient. 
2.3 Model parameter estimation  
Assume that all conditional probabilities in 
Equation (1) are estimated using maximum 
likelihood estimation (MLE). Then 
)|( 12 ?? iii wwwP =  
)|()|( 1212 iiiiiii HwwwPwwHP ???? , wi: headword  
??
?
?
??
?
?
?
 
)|()|( 1212 iiiiiii FwwwPwwFP ???? , wi: function word 
is a strict equality when each word token is uniquely 
classified as a headword or a function word. This 
can be trivially proven as follows. Let Ci represent 
the category of wi (Hi or Fi in our case). We have 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?   
)(
)(
)(
)(
12
12
12
2
iii
iiii
ii
iiii
CwwP
wCwwP
wwP
CwwP
??
??
??
?? ?=   
)(
)(
12
12
??
??=
ii
iiii
wwP
wCwwP  (3) 
Since each word is uniquely assigned to a category, 
P(Ci|wi)=1, and thus it follows that 
)|()()( 121212 iiiiiiiiiii wwwCPwwwPwCwwP ?????? ?=  
)|()( 12 iiiii wCPwwwP ?= ??  
)( 12 iii wwwP ??= . (4) 
Substituting Equation (4) into Equation (3), we get 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?  
)|()(
)(
12
12
12
??
??
?? == iii
ii
iii wwwPwwP
wwwP . (5) 
Now, by separating the estimates of probabilities of 
headwords and function words, Equation (1) can be 
rewritten as: 
P(wi|?(w1?wi-1))= (6) 
)|()(|(( 122121 ???? iiiiii hhwPwwHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
 
wi: function word  
There are three probabilities to be estimated in 
Equation (6): word trigram probability 
P(wi|wi-2wi-1), headword trigram probability 
P(wi|hi-2hi-1) and P(wi|hi-1hi-2) (where wi is a 
headword), and category probability P(Hi|wi-2wi-1). 
In order to deal with the data sparseness problem 
of MLE, we used a backoff scheme (Katz, 1987) for 
the parameter estimation. The backoff scheme 
recursively estimates the probability of an unseen 
n-gram by utilizing (n?1)-gram estimates. To keep 
the model size manageable, we also removed all 
n-grams with frequency less than 2.  
In order to classify a word uniquely as H or F, 
we needed a mapping table where each word in the 
lexicon corresponds to a category. The table was 
generated in the following manner. We first 
assumed that the mapping from part-of-speech 
(POS) to word category is fixed. The tag set we 
used included 1,187 POS tags, of which 102 count 
as headwords in our experiments. We then used a 
POS-tagger to generate a POS-tagged corpus, from 
which we generated the mapping table3. If a word 
could be mapped to both H and F, we chose the 
more frequent category in the corpus. Using this 
mapping table, we achieved a 98.5% accuracy of 
headword detection on the test data we used. 
Through our experiments, we found that 
P(Hi|wi-2wi-1) is a poor estimator of category 
probability; in fact, the unigram estimate P(Hi) 
achieved better results in our experiments as shown 
in Section 6.1. Therefore, we also used the unigram 
estimate for word category probability in our 
                                                     
3 Since the POS-tagger does not identify phrases, our 
implementation does not identify precisely one 
headword for a phrase, but identify multiple headwords 
in the case of compounds.  
experiments. The alternative model that uses the 
unigram estimate is given below:  
 
P(wi|?(w1?wi-1))= (7) 
)|()((( 1221 ?? iiii hhwPHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
wi: function word  
We will denote the models using trigram for 
category probability estimation of Equation (6) as 
T-PHTM, and the models using unigram for 
category probability estimation of Equation (7) as 
U-PHTM. 
3 Using Clusters 
3.1 Principle 
Clustering techniques attempt to make use of 
similarities between words to produce a better 
estimate of the probability of word strings 
(Goodman, 2001).  
We have mentioned in Section 2.2 that the 
headword trigram model can be thought of as a 
cluster-based model with two clusters, the 
headword and the function word. In this section, we 
describe a method of clustering automatically 
similar words and headwords. We followed the 
techniques described in Goodman (2001) and Gao 
et al (2001), and performed experiments using 
predictive clustering along with headword trigram 
models.  
3.2 Predictive clustering model 
Consider a trigram probability P(w3|w1w2), where 
w3 is the word to be predicted, called the predicted 
word, and w1 and w2 are context words used to 
predict w3, called the conditional words. Gao et al 
(2001) presents a thorough comparative study on 
various clustering models for Asian languages, 
concluding that a model that uses clusters for 
predicted words, called the predictive clustering 
model, performed the best in most cases.  
Let iw  be the cluster which word wi belongs to. 
In this study, we performed word clustering for 
words and headwords separately. As a result, we 
have the following two predictive clustering models, 
(8) for words and (9) for headwords:  
)|()|()|( 121212 iiiiiiiiii wwwwPwwwPwwwP ?????? ?=  (8) 
)|()|()|( 121212 iiiiiiiiii whhwPhhwPhhwP ?????? ?=   
wi: headword 
(9) 
Substituting Equations (8) and (9) into Equation (7), 
we get the cluster-based PHTM of Equation (10), 
referred to as C-PHTM. 
 
P(wi|?(w1?wi-1))= (10) 
)|()|()((( 121221 iiiiiiii whhwPhhwPHP ???? ???
))|()|()1( 21212 iiiiiii whhwPhhwP ???? ??+ ?  
)|()|()1( 12121 iiiiiii wwwwPwwwP ???? ??+ ?  
 
wi: headword  
)|()|( 1212 iiiiiii wwwwPwwwP ???? ?  
 
??
?
?
??
?
?
?
 
wi: function word  
3.3 Finding clusters: model estimation 
In constructing clustering models, two factors were 
considered: how to find optimal clusters, and the 
optimal number of clusters.  
The clusters were found automatically by 
attempting to minimize perplexity (Brown et al, 
1992). In particular, for predictive clustering 
models, we tried to minimize the perplexity of the 
training data of )|()|( 1 iiii wwPwwP ?? . Letting N be 
the size of the training data, we have 
?
=
? ?
N
i
iiii wwPwwP
1
1 )|()|(  
?
= ?
? ?=
N
i i
ii
i
ii
WP
wwP
wP
wwP
1 1
1
)(
)(
)(
)(  
?
=
?
?
?=
N
i i
ii
i
ii
wP
wwP
wP
wwP
1
1
1 )(
)(
)(
)(  
?
=
?
?
?=
N
i
ii
i
i wwPwP
wP
1
1
1
)|()(
)(  
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, in order to select the best clusters, it is 
sufficient to try to maximize ?= ?Ni ii wwP1 1 )|( . 
The clustering technique we used creates a 
binary branching tree with words at the leaves. By 
cutting the tree at a certain level, it is possible to 
achieve a wide variety of different numbers of 
clusters. For instance, if the tree is cut after the sixth 
level, there will be roughly 26=64 clusters. In our 
experiments, we always tried the numbers of 
clusters that are the powers of 2. This seems to 
produce numbers of clusters that are close enough 
to optimal. In Equation (10), the optimal number of 
clusters we used was 27. 
4 Relation to Previous Work 
Our LMs are similar to a number of existing ones. 
One such model was proposed by ATR (Isotani and 
Matsunaga, 1994), which we will refer to as ATR 
model below. In ATR model, the probability of 
each word in a sentence is determined by the 
preceding content and function word pair. Isotani 
and Matsunaga (1994) reported slightly better 
results over word bigram models for Japanese 
speech recognition. Geutner (1996) interpolated the 
ATR model with word-based trigram models, and 
reported very limited improvements over word 
trigram models for German speech recognition.  
One significant difference between the ATR 
model and our own lies in the use of predictive 
clustering. Another difference is that our models 
use separate probability estimates for headwords 
and function words, as shown in Equations (6) and 
(7). In contrast, ATR models are conceptually more 
similar to skipping models (Rosenfeld, 1994; Ney et 
al., 1994; Siu and Ostendorf, 2000), where only one 
probability estimate is applied for both content and 
function words, and the word categories are used 
only for the sake of finding the content and function 
word pairs in the context. 
Another model similar to ours is Jelinek (1990), 
where the headwords of the two phrases 
immediately preceding the word as well as the last 
two words were used to compute a word 
probability. The resulting model is similar to a 
5-gram model. A sophisticated interpolation 
formula had to be used since the number of 
parameters is too large for direct estimation. Our 
models are easier to learn because they use 
trigrams. They also differ from Jelinek's model in 
that they separately estimate the probability for 
headwords and function words.  
A significant number of sophisticated techniques 
for language modeling have recently been proposed 
in order to capture more linguistic structure from a 
larger context. Unfortunately, most of them suffer 
from either high computational cost or difficulty in 
obtaining enough manually parsed corpora for 
parameter estimation, which make it difficult to 
apply them successfully to realistic applications. 
For example, maximum entropy (ME) models 
(Rosenfeld, 1994) provide a nice framework for 
incorporating arbitrary knowledge sources, but 
training and using ME models is computationally 
extremely expensive.  
Another interesting idea that exploits the use of 
linguistic structure is structured language modeling 
(SLM, Chelba and Jelinek, 2000). SLM uses a 
statistical parser trained on an annotated corpus in 
order to identify the headword of each constituent, 
which are then used as conditioning words in the 
trigram context. Though SLMs have been shown to 
significantly improve the performance of the LM 
measured in perplexity, they also pose practical 
problems. First, the performance of SLM is 
contingent on the amount and quality of 
syntactically annotated training data, but such data 
may not always be available. Second, SLMs are 
very time-intensive, both in their training and use.  
Charniak (2001) and Roark (2001) also present 
language models based on syntactic dependency 
structure, which use lexicalized PCFGs that sum 
over the derivation probabilities. They both report 
improvements in perplexity over Chelba and 
Jelinek (2000) on the Wall Street Journal section of 
the Penn Treebank data, suggesting that syntactic 
structure can be further exploited for language 
modeling. The kind of linguistic structure used in 
our models is significantly more modest than that 
provided by parser-based models, yet offers 
practical benefits for realistic applications, as 
shown in the next section.  
5 Evaluation Methodology 
The most common metric for evaluating a language 
model is perplexity. Perplexity can be roughly 
interpreted as the expected branching factor of the 
test document when presented to a language model. 
Perplexity is widely used due to its simplicity and 
efficiency. However, the ultimate quality of a 
language model must be measured by its effect on 
the specific task to which it is applied, such as 
speech recognition. Lower perplexities usually 
result in lower error rates, but there are numerous 
counterexamples to this in the literature. 
In this study, we evaluated our language models 
on the application of Japanese Kana-Kanji 
conversion, which is the standard method of 
inputting Japanese text by converting the text of 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. Performance 
on this task is generally measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is 
to select the word string (in a combination of Kanji 
and Kana) with the highest probability among the 
candidate strings that match the typed phonetic 
(Kana) string. Current products make about 5-10% 
errors in conversion of real data in a wide variety of 
domains.  
For our experiments, we used two newspaper 
corpora: Nikkei and Yomiuri Newspapers. Both 
corpora have been word-segmented. We built 
language models from a 36-million-word subset of 
the Nikkei Newspaper corpus. We performed 
parameter optimization on a 100,000-word subset 
of the Yomiuri Newspaper (held-out data). We 
tested our models on another 100,000-word subset 
of the Yomiuri Newspaper corpus. The lexicon we 
used contains 167,107 entries.  
In our experiments, we used the so-called 
?N-best rescoring? method. In this method, a list of 
hypotheses is generated by the baseline language 
model (a word trigram model in this study4), which 
is then rescored using a more sophisticated LM. 
Due to the limited number of hypotheses in the 
N-best list, the second pass may be constrained by 
the first pass. In this study, we used the 100-best 
list. The ?oracle? CER (i.e., the CER among the 
hypotheses with the minimum number of errors) is 
presented in Table 1. This is the upper bound on 
performance in our experiments. The performance 
of the conversion using the baseline trigram model 
is much better than the state-of-the-art performance 
currently available in the marketplace. This may be 
due to the large amount of training data we used, 
and to the similarity between the training and the 
test data. We also notice that the ?oracle? CER is 
                                                     
4  For the detailed description of the baseline trigram 
model, see Gao et al (2002).  
relatively high due to the high out-of-vocabulary 
rate, which is 1.14%. Because we have only limited 
room for improvement, the reported results of our 
experiments in this study may be underestimated. 
Baseline Trigram Oracle of 100-best 
3.73% 1.51% 
Table 1. CER results of baseline and 100-best list 
6 Results and Discussion 
6.1 Impact of headword dependency and 
predictive clustering 
We applied a series of language models proposed in 
this paper to the Japanese Kana-Kanji conversion 
task in order to test the effectiveness of our 
techniques. The results are shown in Table 2. The 
baseline result was obtained by using a 
conventional word trigram model. HTM stands for 
the headword trigram model of Equation (6) and (7) 
without permutation (i.e., ?2=1), while PHTM is the 
model with permutation. The T- and U-prefixes 
refer to the models using trigram (Equation (6)) or 
unigram (Equation (7)) estimate for word category 
probability. The C-prefix, as in C-PHTM, refers to 
PHTM with predictive clustering (Equation (10)). 
For comparison, we also include in Table 2 the 
results of using the predictive clustering model 
without taking word category into account, referred 
to as predictive clustering trigram model (PCTM). 
In PCTM, the probability for all words is estimated 
by )|()|( 1212 iiiiiii wwwwPwwwP ???? ? . 
Model ?1 ? 2 CER CER reduction 
Baseline ---- ---- 3.73% ----
T-HTM 0.2 1 3.54% 5.1% 
U-HTM  0.2 1 3.41% 8.6% 
T-PTHM 0.2 0.7 3.53% 5.4% 
U-PHTM  0.2 0.7 3.34% 10.5% 
PCTM ---- ---- 3.44% 7.8% 
C-HTM  0.3 1 3.23% 13.4% 
C-PHTM  0.3 0.7 3.17% 15.0% 
Table 2. Comparison of CER results 
In Table 2, we find that for both PHTM and HTM, 
models U-HTM and U-PHTM achieve better 
performance than models T-HTM and T-PHTM. 
Therefore, only models using unigram for category 
probability estimation are used for further 
experiments, including the models with predictive 
clustering. 
By comparing U-HTM with the baseline model, 
we can see that the headword trigram contributes 
greatly to the CER reduction: U-HTM 
outperformed the baseline model by about 8.6% in 
error rate reduction. HTM with headword 
permutation (U-PHTM) achieves further 
improvements of 10.5% CER reduction against the 
baseline. The contribution of predictive clustering is 
also very encouraging. Using predictive clustering 
alone (PCTM), we reduced the error rate by 7.8%.  
What is particularly noteworthy is that the 
combination of both techniques leads to even larger 
improvements: for both HTM and PHTM, 
predictive clustering (C-HTM and C-PHTM) brings 
consistent improvements over the models without 
clustering, achieving the CER reduction of 13.4% 
and 15.0% respectively against the baseline model, 
or 4.8% and 4.5% against the models without 
clustering.  
In sum, considering the good performance of our 
baseline system and the upper bound on 
performance improvement due to the 100-best list 
as shown in Table 1, the improvements we obtained 
are very promising. These results demonstrate that 
the simple method of using headword trigrams and 
predictive clustering can be used to effectively 
improve the performance of word trigram models. 
6.2 Comparsion with other models 
In this subsection, we present a comparison of our 
models with some of the previously proposed 
models, including the higher-order n-gram models, 
skipping models, and the ATR models.  
Higher-order n-gram models refer to those 
n-gram models in which n>3. Although most of the 
previous research showed little improvement, 
Goodman (2001) showed recently that, with a large 
amount of training data and sophisticated 
smoothing techniques, higher-order n-gram models 
could be superior to trigram models.  
The headword trigram model proposed in this 
paper can be thought of as a variation of a higher 
order n-gram model, in that the headword trigrams 
capture longer distance dependencies than trigram 
models. In order to see how far the dependency goes 
within our headword trigram models, we plotted the 
distribution of headword trigrams (y-axis) against 
the n of the word n-gram were it to be captured by 
the word n-gram (x-axis) in Figure 2. For example, 
given a word sequence w1w2w3w4w5w6, and if w1, w3 
and w6 are headwords, then the headword trigram 
P(w6|w3w1) spans the same distance as the word 
6-gram model.  
0.0E+00
5.0E+06
1.0E+07
1.5E+07
2.0E+07
2.5E+07
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
N of n-gram
Nu
mb
er 
of 
wo
rds
 Figure 2. Distribution of headword trigrams 
against the n of word n-gram 
From Figure 2, we can see that approximately 
95% of the headword trigrams can be captured by 
the higher-order n-gram model with the value of n 
smaller than 7. Based on this observation, we built 
word n-gram models with the values of n=4, 5 and 
6. For all n-gram models, we used the interpolated 
modified absolute discount smoothing method (Gao 
et al, 2001), which, in our experiments, achieved 
the best performance among the state-of-the-art 
smoothing techniques. Results showed that the 
performance of the higher-order word n-gram 
models becomes saturated quickly as n grows: the 
best performance was achieved by the word 5-gram 
model, with the CER of 3.71%. Following 
Goodman (2001), we suspect that the poor 
performance of these models is due to the data 
sparseness problem. 
Skipping models are an extension of an n-gram 
model in that they predict words based on n 
conditioning words, except that these conditioning 
words may not be adjacent to the predicted word. 
For instance, instead of computing P(wi|wi-2wi-1), a 
skipping model might compute P(wi|wi-3wi-1) or 
P(wi|wi-4wi-2). Goodman (2001) performed 
experiments of interpolating various kinds of 
higher-order n-gram skipping models, and obtained 
a very limited gain.  Our results confirm his results 
and suggest that simply extending the context 
window by brute-force can achieve little 
improvement, while the use of even the most 
modest form of structural information such as the 
identification of headwords and automatic 
clustering can help improve the performance.  
We also compared our models with the trigram 
version of the ATR models discussed in Section 4, 
in which the probability of a word is conditioned by 
the preceding content and function word pair. We 
performed experiments using the ATR models as 
described in Isotani and Matsunaga (1994). The 
results show that the CER of the ATR model alone 
is much higher than that of the baseline model, but 
when interpolated with a word trigram model, the 
CER is slightly reduced by 1.6% from 3.73% to 
3.67%. These results are consistent with those 
reported in previous work. The difference between 
the ATR model and our models indicates that the 
predictions of headwords and function words can 
better be done separately, as they play different 
semantic and syntactic roles capturing different 
dependency structure.  
6.3 Discussion 
In order to better understand the effect of the 
headword trigram, we have manually inspected the 
actual improvements given by PHTM. As expected, 
many of the improvements seem to be due to the use 
of larger context: for example, the headword 
trigram?? ~?? ~??  (shouhi 'consume' ~ 
shishutsu 'expense' ~ genshou 'decrease') 
contributed to the correct conversion of the 
phonetic string ?????  genshou into ?? 
genshou 'decrease' rather than ? ?  genshou 
'phenomenon' in the context of ?????????
? shouhi shishutsu hajimete no genshou  'consumer 
spending decreases for the first time'.  
On the other hand, the use of headword trigrams 
and predictive clustering is not without side effects. 
The overall gain in CER was 15% as we have seen 
above, but a closer inspection of the conversion 
results reveals that while C-PHTM corrected the 
conversion errors of the baseline model in 389 
sentences (8%), it also introduced new conversion 
errors in 201 sentences (4.1%). Among the newly 
introduced errors, one type of error is particularly 
worth noting: these are the errors where the 
candidate conversion preferred by the HTM is 
grammatically impossible or unlikely. For example, 
???????? (beikoku-ni shinkou-dekiru, 
USA-to invade-can 'can invade USA') was 
misconverted as ???????? (beikoku-ni 
shinkou-dekiru, USA-to new-can), even though ?
? shinkou 'invade' is far more likely to be preceded 
by the morpheme ? ni 'to', and ?? shinkou 'new' 
practically does not precede ??? dekiru 'can'. 
The HTM does not take these function words into 
account, leading to a grammatically impossible or 
implausible conversion. Finding the types of errors 
introduced by particular modeling assumptions in 
this manner and addressing them individually will 
be the next step for further improvements in the 
conversion task.   
7 Conclusion 
We proposed and evaluated a new language model, 
the permuted headword trigram model with 
clustering (C-PHTM). We have shown that the 
simple model that combines the predictive 
clustering with a headword detector can effectively 
capture structure in language. Experiments show 
that the proposed model achieves an encouraging 
15% CER reduction over a conventional word 
trigram model in a Japanese Kana-Kanji conversion 
system. We also compared C-PTHM to several 
similar models, showing that our model has many 
practical advantages, and achieves substantially 
better performance.  
One issue we did not address in this paper was 
the language model size: the models that use HTM 
are larger than the baseline model we compared the 
performance with. Though we did not pursue the 
issue of size reduction in this paper, there are many 
known techniques that effectively reduce the model 
size while minimizing the loss in performance. One 
area of future work is therefore to reduce the model 
size. Other areas include the application of the 
proposed model to a wider variety of test corpora 
and to related tasks.  
Acknowledgements 
We would like to thank Ciprian Chelba, Bill Dolan, 
Joshua Goodman, Changning Huang, Hang Li and 
Yoshiharu Sato for their comments on early 
thoughts and drafts of the paper. We would also like 
to thank Hiroaki Kanokogi, Noriko Ishibashi and 
Miyuki Seki for their help in our experiments. 
 
References 
Brown, Peter F., Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. 
Class-Based N-gram Models of Natural Language. 
Computational Linguistics, 18-4: 467-479. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In ACL/EACL 2001, pp.124-131.  
Chelba, Ciprian and Frederick Jelinek. 2000. Structured 
Language Modeling. Computer Speech and Language, 
Vol. 14, No. 4. pp 283-332.  
Gao, Jianfeng, Joshua T. Goodman and Jiangbo Miao. 
2001. The use of clustering techniques for language 
model ? application to Asian language. Computational 
Linguistics and Chinese Language Processing. Vol. 6, 
No. 1, pp 27-60. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002. Toward a unified approach to 
statistical language modeling for Chinese. ACM 
Transactions on Asian Language Information 
Processing, Vol. 1, No. 1, pp 3-33.  
Geutner, Petra. 1996. Introducing linguistic constraints 
into statistical language modeling. In International 
Conference on Spoken Language Processing, 
Philadelphia, USA. pp.402-405.  
Goodman, Joshua T. 2001. A bit of progress in language 
modeling. Computer Speech and Language. October, 
2001, pp 403-434. 
Goodman, Joshua T., and Jianfeng Gao. 2000. Language 
model size reduction by pruning and clustering. 
ICSLP-2000, Beijing.  
Isotani, Ryosuke and Shoichi Matsunaga. 1994. A 
stochastic language model for speech recognition 
integrating local and global constraints. ICASSP-94, 
pp. 5-8. 
Jelinek, Frederick. 1990. Self-organized language 
modeling for speech recognition. In A. Waibel and K. 
F. Lee (eds.), Readings in Speech Recognition, 
Morgan-Kaufmann, San Mateo, CA. pp. 450-506. 
Katz, S. M. 1987. Estimation of probabilities from sparse 
data for other language component of a speech 
recognizer. IEEE transactions on Acoustics, Speech 
and Signal Processing, 35(3): 400-401. 
Ney, Hermann, Ute Essen and Reinhard Kneser. 1994. 
On structuring probabilistic dependences in stochastic 
language modeling. Computer Speech and Language, 
8: 1-38. 
Roark, Brian. 2001. Probabilistic top-down parsing and 
language modeling. Computational Linguistics, 17-2: 
1-28. 
Rosenfeld, Ronald. 1994. Adaptive statistical language 
modeling: a maximum entropy approach. Ph.D. thesis, 
Carnegie Mellon University.  
Siu, Manhung and Mari Ostendorf. 2000. Variable 
n-grams and extensions for conversational speech 
language modeling. IEEE Transactions on Speech and 
Audio Processing, 8: 63-75. 
	
			
				
	

					
		Unsupervised Training for Overlapping Ambiguity Resolution in  
Chinese Word Segmentation 
Mu Li, Jianfeng Gao, Changning Huang 
Microsoft Research, Asia  
Beijing 100080, China 
{t-muli,jfgao,cnhuang}@microsoft.com 
Jianfeng Li 
University of Science and Technology of China 
Hefei, 230027, China 
jackwind@mail.ustc.edu.cn 
Abstract 
This paper proposes an unsupervised 
training approach to resolving overlap-
ping ambiguities in Chinese word seg-
mentation. We present an ensemble of 
adapted Na?ve Bayesian classifiers that 
can be trained using an unlabelled Chi-
nese text corpus. These classifiers differ 
in that they use context words within 
windows of different sizes as features. 
The performance of our approach is 
evaluated on a manually annotated test set. 
Experimental results show that the pro-
posed approach achieves an accuracy of 
94.3%, rivaling the rule-based and super-
vised training methods. 
1 Introduction 
Resolving segmentation ambiguities is one of the 
fundamental tasks for Chinese word segmentation, 
and has received considerable attention in the re-
search community. Word segmentation ambigui-
ties can be roughly classified into two classes: 
overlapping ambiguity (OA), and combination am-
biguity (CA). In this paper, we focus on the meth-
ods of resolving overlapping ambiguities.  
Consider a Chinese character string ABC, if it 
can be segmented into two words either as AB/C 
or A/BC depending on different context, ABC is 
called an overlapping ambiguity string (OAS). For 
example, given a Chinese character string ?
? (ge4-guo2-you3), it can be segmented as either 
? | ? (each state-owned) in Sentence (1) of 
Figure 1, or ? | ? (every country has) in 
Sentence (2).  
(1)  (in) |  (each) |  (state-owned) | 
 (enterprise)  |   (middle)  
(in each state-owned enterprise) 
(2)  (in) | 	 (human rights) | 
 (prob-
lem) |  (on) |  (every country) |  
(have) |  (common ground) 
(Regarding human rights, every country has 
some common ground) 
 
Figure 1.  Overlapping  ambiguities of Chinese 
character string ?? 
Our method of resolving overlapping ambigui-
ties contains two procedures. One is to construct an 
ensemble of Na?ve Bayesian classifiers to resolve 
ambiguities. The other is an unsupervised method 
for training the Na?ve Bayesian classifiers which 
compose the ensemble. The main issue of the un-
supervised training is how to eliminate the nega-
tive impact of the OA errors in the training data. 
Our solution is to identify all OASs in the training 
data and replace them with a single special token. 
By doing so, we actually remove the portion of 
training data that are likely to contain OA errors. 
The classifiers are then trained on the processed 
training data. 
Our approach is evaluated on a manually anno-
tated test set with 5,759 overlapping segmentation 
ambiguities. Experimental results show that an ac-
curacy of 94.3% is achieved.  
This remainder of this paper is structured as fol-
lows: Section 2 reviews previous work. Section 3 
defines overlapping ambiguous strings in Chinese. 
Section 4 describes the evaluation results. Section 
5 presents our conclusion. 
 
 
 
2 Previous Work 
Previous methods of resolving overlapping am-
biguities can be grouped into rule-based ap-
proaches and statistical approaches.  
Maximum Matching (MM) based segmentation 
(Huang, 1997) can be regarded as the simplest 
rule-based approach, in which one starts from one 
end of the input sentence, greedily matches the 
longest word towards the other end, and repeats the 
process with the rest unmatched character se-
quences until the entire sentence is processed. If 
the process starts with the beginning of the sen-
tence, it is called Forward Maximum Matching 
(FMM). If the process starts with the end of the 
sentence, it is called Backward Maximum Match-
ing (BMM). Although it is widely used due to its 
simplicity, MM based segmentation performs 
poorly in real text. 
Zheng and Liu (1997) use a set of manually 
generated rules, and reported an accuracy of 81% 
on an open test set. Swen and Yu (1999) presents a 
lexicon-based method. The basic idea is that for 
each entry in a lexicon, all possible ambiguity 
types are tagged; and for each ambiguity types, a 
solution strategy is used. They achieve an accuracy 
of 95%. Sun (1998) demonstrates that most of the 
overlapping ambiguities can be resolved without 
taking into account the context information. He 
then proposes a lexicalized rule-based approach. 
His experiments show that using the 4,600 most 
frequent rules, 51% coverage can be achieved in an 
open test set. 
Statistical methods view the overlapping 
ambiguity resolution as a search or classification 
task. For example, Liu (1997) uses a word unigram 
language model, given all possible segmentations 
of a Chinese character sequence, to search the best 
segmentation with the highest probability. Similar 
approach can be traced back to Zhang (1991). But 
the method does not target to overlapping 
ambiguities. So the disambiguation results are not 
reported. Sun (1999) presents a hybrid method 
which incorporates empirical rules and statistical 
probabilities, and reports an overall accuracy of 
92%. Li (2001) defines the word segmentation dis-
ambiguation as a binary classification problem. Li 
then uses Support Vector Machine (SVM) with 
mutual information between each Chinese charac-
ter pair as a feature. The method achieves an accu-
racy of 92%. All the above methods utilize a su-
pervised training procedure. However, a large 
manually labeled training set is not always avail-
able. To deal with the problem, unsupervised ap-
proaches have been proposed. For example, Sun 
(1997) detected word boundaries given an OAS 
using character-based statistical measures, such as 
mutual information and difference of t-test. He 
reported an accuracy of approximately 90%. In his 
approach, only the statistical information within 4 
adjacent characters is exploited, and lack of word-
level statistics may prevent the disambiguation 
performance from being further improved.  
3 Ensemble of Na?ve Bayesian Classifier 
for Overlapping Ambiguity Resolution 
3.1 Problem Definition 
We first give the formal definition of overlapping 
ambiguous string (OAS) and longest OAS.  
An OAS is a Chinese character string O that 
satisfies the following two conditions: 
a) There exist two segmentations Seg1 and Seg2 
such that 2211 , SegwSegw ??? , where Chinese 
words w1 and w2 are different from either literal 
strings or positions; 
b) 2211 , SegwSegw ??? , where w1 and w2 
overlap.  
The first condition ensures that there are 
ambiguous word boundaries (if more than one 
word segmentors are applied) in an OAS. In the 
example presented in section 1, the string ?
? is an OAS but ?? is not because 
the word ?? remains the same in both FMM 
and BMM segmentations of ? |  | ? and 
? |  | ?. The second condition indicates 
that the ambiguous word boundaries result from 
crossing brackets. As illustrated in Figure 1, words 
?? and ?? form a crossing bracket.  
The longest OAS is an OAS that is not a sub-
string of any other OAS in a given sentence. For 
example, in the case ?? (sheng1-huo2-
shui3-ping2, living standard), both ?? and 
?? are OASs, but only ?? is the 
longest OAS because ?? is a  substring of 
??. In this paper, we only consider the 
longest OAS because both left and right bounda-
ries of the longest OAS are determined.  
Furthermore, we constrain our search space 
within the FMM segmentation Of and BMM seg-
mentation Ob of a given longest OAS. According 
to Huang (1997), two important properties of OAS 
has been identified: (1) if the FMM segmentation 
is the same as its BMM segmentation (Of = Ob), for 
example ?  ? (sou1-suo3-yin3-qing2, 
Search Engine), the probability that the MM seg-
mentation is correct is 99%;  Otherwise, (2) if the 
FMM segmentation differs from its BMM segmen-
tation (Of ? Ob ), for example ??, the prob-
ability that at least one of the MM segmentation is 
correct is also 99%. So such a strategy will not 
lower the coverage of our approach. 
Therefore, the overlapping ambiguity resolution 
can be formulized as a binary classification prob-
lem as follows: 
Given a longest OAS O and its context feature 
set C, let G(Seg, C) be a score function of Seg  for 
},{ bf OOseg ? , the overlapping ambiguity reso-
lution task is to make the binary decision: 
?
?
?
<
>
= ),(),(
),(),(
COGCOGO
COGCOGO
seg
bfb
bff
 
(1) 
Note that Of = Ob means that both FMM and 
BMM arrive at the same result. The classification 
process can then be stated as: 
a) If Of = Ob, then choose either segmentation 
result since they are same; 
b) Otherwise, choose the one with the higher 
score G according to Equation (1). 
For example, in the example of ??, if 
Of = Ob  = ? | ?, then ? | ? is se-
lected as the answer. In another example of ?
? in sentence (1) of Figure 1, Of = ? | ?, 
Ob  = ? | ?. Assume that C = {, }, i.e., 
we used a context window of size 3; then the seg-
mentation ?    |  ? is selected if 
>}),{,"|(" G }),{,"|(" G , 
otherwise ? | ? is selected. 
3.2 Na?ve Bayesian Classifier for Overlapping 
Ambiguity Resolution 
Last section formulates the overlapping ambi-
guity resolution of an OAS O as the binary classi-
fication between Of and Ob. This section describes 
the use of the adapted Na?ve Bayesian Classifier 
(NBC) (Duda and Hart, 1973) to address problem. 
Here, we use the words around O within a window 
as features, with w
-m?w-1 denoting m words on the 
left of the O and w1?wn denoting n words on the 
right of the O. Na?ve Bayesian Classifier assumes 
that all the feature variables are conditionally inde-
pendent. So the joint probability of observing a set 
of context features C = {w
-m?w-1, w1?wn} of a 
segmentation Seg (Of or Ob) of O is as follows: 
?
??=
??
=
nmi
i
nm
SegwpSegp
Segwwwwp
,...1,1,...
1,1
)|()(
),,...,(
 (2) 
Assume that Equation (2) is the score function 
in Equation (1) G, we then have two parameters to 
be estimated: p(Seg) and p(wi|Seg). Since we do 
not have enough labeled training data, we then re-
sort to the redundancy property of natural language. 
Due to the fact that the OAS occupies only in a 
very small portion of the entire Chinese text, it is 
feasible to estimate the word co-occurrence prob-
abilities from the portion of corpus that contains no 
overlapping ambiguities. Consider an OASSingle Character Chinese Named Entity Recognition 
Xiaodan Zhu, Mu Li, Jianfeng Gao and Chang-Ning Huang 
Microsoft Research, Asia  
Beijing 100080, China 
xdzhu@msrchina.research.microsoft.com 
{t-muli,jfgao,cnhuang}@microsoft.com 
 
 
Abstract 
Single character named entity (SCNE) is a 
name entity (NE) composed of one Chinese 
character, such as  ?
 
? (zhong1, China) 
and ?? e2,Russia. SCNE is very 
common in written Chinese text. However, 
due to the lack of in-depth research, SCNE 
is a major source of errors in named entity 
recognition (NER). This paper formulates 
the SCNE recognition within the source-
channel model framework. Our experi-
ments show very encouraging results: an F-
score of 81.01% for single character loca-
tion name recognition, and an F-score of 
68.02% for single character person name 
recognition. An alternative view of the 
SCNE recognition problem is to formulate 
it as a classification task. We construct two 
classifiers based on maximum entropy 
model (ME) and vector space model 
(VSM), respectively. We compare all pro-
posed approaches, showing that the source-
channel model performs the best in most 
cases. 
1 Introduction 
The research of named entity recognition (NER) 
becomes very popular in recent years due to its 
wide applications and the Message Understanding 
Conference (MUC) which provides a standard test-
bed for NER evaluation. Recent research on Eng-
lish NER includes (Collins, 2002; Isozaki, 2002; 
Zhou, 2002; etc.). Chinese NER research includes 
(Liu, 2001; Zheng, 2000; Yu, 1998; Chen, 1998; 
Shen, 1995; Sun, 1994; Zhang, 1992 etc.) 
In Chinese NEs, there is a special kind of NE, 
called single character named entity (SCNE), on 
which there is little in-depth research. SCNE is a 
NE composed of only one Chinese character, such 
as the location name ?
 
? (zhong1,China) and 
? ? (e2,Russia) in the phrase ?  ? 
(zhong1-e2-mao4-yi4, trade between China and 
Russia). SCNE is very common in written Chinese 
text. For instance, SCNE accounts for 8.17% of all 
NE tokens according to our statistics on a 10MB 
corpus. However, due to the lack of research, 
SCNE is a major source of errors in NER. Among 
three state-of-the-art systems we have, the best F-
scores of single character location (SCL) and sin-
gle character person (SCP) are 43.63% and 43.48% 
respectively. This paper formulates the SCNE rec-
ognition within the source-channel model frame-
work. Our results show very encouraging 
performance. We achieve an F-score of 81.01% for 
SCL recognition and an F-score of 68.02% for 
SCP recognition. An alternative view of the SCNE 
recognition problem is to formulate it as a 
classification task. For example, ?
 
? is a SCNE 
in ?
 
?, but not in ?
 
?(bei3-jing1-
si4-zhong1, Beijing No.4 High School). We then 
construct two classifiers respectively based on two 
statistical models: maximum entropy model (ME) 
and vector space model (VSM). We compare these 
two classifiers with the source-channel model, 
showing that the source-channel model is slightly 
better. We then compare the source-channel model 
with other three state-of-the-art NER systems. 
The remainder of this paper is structured as 
follows: Section 2 introduces the task of SCNE 
recognition and related work. Section 3 and 4 pro-
pose the source-channel model and two classifiers 
for SCNE recognition, respectively. Section 5 pre-
sents experimental results and error analysis. Sec-
tion 6 gives conclusion. 
2 SCNE Recognition and Related Work 
We consider three types of SCNE in this paper: 
single character location name (SCL), person 
name (SCP), and organization name (SCO). Be-
low are examples: 
1.    SCL: ?
 
?and ?? in ?
 
? 
2.  SCP: ? ? (zhou1, Zhou) in ? ? 
(zhou1-zong3-li3,Premier Zhou), 
3.  SCO: ?? (guo2, Kuomingtang Party) 
and ?? (gong4, Communist Party ) in 
?  ? (Guo2-gong4-he2-zuo4, 
Cooperation between Kuomingtang Part 
and Communist Party) 
SCNE is very common in written Chinese text. 
As shown in Table 1, SCNE accounts for 8.17% 
of all NE tokens on the 10MB corpus. Especially, 
14.65% of location names are SCLs. However, 
due to the lack of research, SCNE is a major 
source of errors in NER. In our experiments 
described below, we focus on SCL and SCP, 
while SCO is not considered because of its small 
number in the data.  
 
 # SCNE # NE #SCNE / #NE 
PN 5,892 129,317 4.56% 
LN 32,483 221,713 14.65% 
ON 356 122,779 0.29% 
Total 38,731 473,809 8.17% 
 
Table 1.  Proportion of SCNE in NE 
To our knowledge, most NER systems do not 
report SCNE recognition results separately. 
Some systems (e.g. Liu, 2001) even do not in-
clude SCNE in recognition task. SCNE recogni-
tion is achieved using the same technologies as 
for NER, which can be roughly classified into 
rule-based methods and statistical-based methods, 
while most of state-of-the-art systems use hybrid 
approaches. 
Wang (1999) and Chen (1998) used linguistic 
rules to detect NE with the help of the statistics 
from dictionary.  Ji(2001), Zheng (2000), 
Shen(1995) and Sun(1994) used statistics from 
dictionaries and large corpus to generate PN or 
LN candidates, and used linguistic rules to filter 
the result, and Yu (1998) used language model to 
filter. Liu (2001) applied statistical and linguistic 
knowledge alternatively in a seven-step proce-
dure. Unfortunately, most of these results are 
incomparable due to the different test sets used, 
except the results of Chen (1998) and Yu (1998). 
They took part in Multilingual Entity Task 
(MET-2) on Chinese, held together with MUC-7. 
Between them, Yu (1998)?s results are slightly 
better.    However, these two comparable sys-
tems did not report their results on SCNE sepa-
rately. To evaluate our results, we compare with 
three state-of-the-art system we have. These sys-
tems include: MSWS, PBWS and LCWS. The 
former two are developed by Microsoft? and the 
last one comes from by Beijing Language Uni-
versity.  
3 SCNE Recognition Using an Improved 
Source-Channel Model 
3.1 Improved Source-Channel Model1 
We first conduct SCNE recognition within a 
framework of improved source-channel models, 
which is applied to Chinese word segmentation.  
We define Chinese words as one of the following 
four types: (1) entries in a lexicon, (2) morpho-
logically derived words, (3) named entity (NE), 
and (4) factoid.  Examples are 
1. lexicon word:  (peng2-you3, friend). 
2. morph-derived word: 		



 (gao1-gao1-
xing4-xing4 , happily) 
3. named entity:  
(wei1-ruan3-gong1-
si1, Microsoft Corporation) 
4. factoid2:    (yi1-yue4-jiu3-ri4, Jan 9th)  
Chinese NER is achieved within the framework. 
To make our later discussion on SCNE clear, we 
introduce the model briefly. 
We are given Chinese sentence S, which is 
a character string. For all possible word segmen-
tations W, we will choose the one which 
achieves the highest conditional probability W* 
= argmax
 w P(W|S). According to Bayes? law and 
dropping the constant denominator, we acquire 
the following equation: 
                                                     
1
 This follows the description of (Gao, 2003). 
2
 We define ten types of factoid: date, time (TIME), percent-
age, money, number (NUM), measure, e-mail, phone number, 
and WWW. 
)|()(maxarg* WSPWPW
W
=
 
(1) 
Following our Chinese word definition, we de-
fine word class C as follows: (1) each lexicon 
word is defined as a class; (2) each morphologi-
cally derived word is defined as a class; (3) each 
type of named entities is defined as a class, e.g. 
all person names belong to a class PN, and (4) 
each type of factoids is defined as a class, e.g. all 
time expressions belong to a class TIME. We 
therefore convert the word segmentation W into 
a word class sequence C. Eq. 1 can then be 
rewritten as: 
)|()(maxarg* CSPCPC
C
= . 
(2) 
Eq. 2 is the basic form of the source-channel 
models for Chinese word segmentation. The 
models assume that a Chinese sentence S is gen-
erated as follows: First, a person chooses a se-
quence of concepts (i.e., word classes C) to 
output, according to the probability distribution 
P(C); then the person attempts to express each 
concept by choosing a sequence of characters, 
according to the probability distribution P(S|C).  
We use different types of channel models 
for different types of Chinese words. This brings 
several advantages. First, different linguistic 
constraints can be easily added to corresponding 
channel models (see Figure 1). These constraints 
can be dynamic linguistic knowledge acquired 
through statistics or intuitive rules compiled by 
linguists. Second, this framework is data-driven, 
which makes it easy to adapt to other languages. 
We have three channel models for PN, LN and 
ON respectively. (see Figure 1) 
However, although Eq. 2 suggests that channel 
model probability and source model probability 
can be combined through simple multiplication, in 
practice some weighting is desirable. There are two 
reasons. First, some channel models are poorly 
estimated, owing to the sub-optimal assumptions 
we make for simplicity and the insufficiency of the 
training corpus. Combining the channel model 
probability with poorly estimated source model 
probabilities according to Eq. 2 would give the 
context model too little weight. Second, as seen in 
Figure 1, the channel models of different word 
classes are constructed in different ways (e.g. name 
entity models are n-gram models trained on cor-
pora, and factoid models are compiled using lin-
guistic knowledge). Therefore, the quantities of 
channel model probabilities are likely to have 
vastly different dynamic ranges among different 
word classes. One way to balance these probability 
quantities is to add several channel model weight 
CW, each for one word class, to adjust the channel 
model probability P(S|C) to P(S|C)CW. In our ex-
periments, these weights are determined empiri-
cally on a development set. 
Given the source-channel models, the procedure 
of word segmentation involves two steps: first, 
given an input string S, all word candidates are 
generated (and stored in a lattice). Each candidate 
is tagged with its class and the probability P(S?|C), 
where S? is any substring of S. Second, Viterbi 
search is used to select (from the lattice) the most 
probable word segmentation (i.e. word class se-
quence C*) according to Eq. 2.  
Word class Channel model Linguistic Constraints 
Lexicon word (LW) P(S|LW)=1 if S forms a lexicon entry, 
0 otherwise.  
Word lexicon 
Morphologically derived word 
(MW) 
P(S|MW)=1 if S forms a morph lexicon 
entry, 0 otherwise.  
Morph-lexicon 
Person name (PN) Character bigram  family name list, Chinese PN patterns 
Location name (LN) Character bigram  LN keyword list, LN lexicon, LN abbr. list 
Organization name (ON) Word class bigram ON keyword list, ON abbr. List 
Factoid (FT) P(S|G)=1 if S can be parsed using a 
factoid grammar G, 0 otherwise 
Factoid rules (presented by FSTs). 
Figure 1. Channel models (Gao, 2003) 
3.2 Improved Model for SCNE Recognition 
Although our results show that the source-
channel models achieve the state-of-the-art word 
segmentation performance, they cannot handle 
SCNE very well. Error analysis shows that 
11.6% person name errors come from SCP, and 
47.7% location names come from SCL. There 
are two reasons accounting for it: First, SCNE is 
generated in a different way from that of multi-
character NE. Second, the context of SCNE is 
different from other NE. For example, SCNE 
usually appears one after another such as ? 


?. But this is not the case for multi-
character NE.  
To solve the first problem, we add two new 
channel models to Figure 1, that is, define each 
type of SCNE (i.e. SCL and SCP) as a individual 
class (i.e. NE_SCL and NE_SCP) with its chan-
nel probability P(Sj |NE_SCL), and P(Sj 
|NE_SCP). P(Sj |NE_SCL) is calculated by Eq. 3. 

=
=
n
i
i
j
j
SSCL
SSCL
SCLNE
1
|)(|
|)(|
  )_|P(S
 (3) 
Here, Sj is a character in SCL list which is ex-
tracted from training corpus. |SCL(Sj)| is the 
number of tokens Sj , which are labeled as SCL 
in training corpus. n is the size of SCL list, 
which includes 177 SCL. Similarly, P(Sj |NE_SCP) is calculated by Eq. 4, and the SCP 
list includes 151 SCP. 

=
=
n
i
i
j
j
SSCP
SSCP
SCPNE
1
|)(|
|)(|
  )_|P(S
 (4) 
We also use two CW to balance their channel 
probabilities with other NE?s. 
To solve the second problem, we trained a new 
source model P(C) on the re-annotated training 
corpus, where all SCNE are tagged by SCL or SCP.  
For example, ?? in ??is tagged as SCP 
instead of PN, and ? ? in ? 

? is tagged as 
SCL in stead of LN. 
4   Character-based Classifiers 
In this section, SCNE recognition is formulated 
as a binary classification problem. Our motiva-
tions are two folds. First, most NER systems do 
not use source-channel model, so our method 
described in the previous section cannot be ap-
plied. However, if we define SCNE as a binary 
classification problem, it would be possible to 
build a separate recognizer which can be used 
together with any NER systems. Second, we are 
interested in comparing the performance of 
source-channel models with that of other meth-
ods. 
For each Chinese character, a classifier is 
built to estimate how likely an occurrence of this 
Chinese character in a text is a SCNE. Some ex-
amples of these Chinese character as well as 
their probabilities of being a SCNE is shown in 
Table 2.  
 
	
 

	




 



Chinese Chunking with another Type of Spec  
Hongqiao Li 
Beijing Institute of 
Technology 
Beijing 100081 China 
lhqtxm@bit.edu.cn 
Chang-Ning Huang 
Microsoft Research Asia 
Beijing 100080 China 
cnhuang@msrchina. 
research.microsoft.com 
Jianfeng Gao  
Microsoft Research Asia 
Beijing 100080 China 
 jfgao@microsoft.com 
Xiaozhong Fan 
Beijing Institute of 
Technology 
Beijing 100081 China 
fxz@bit.edu.cn 
 
Abstract 
Spec is a critical issue for automatic chunking. 
This paper proposes a solution of Chinese 
chunking with another type of spec, which is 
not derived from a complete syntactic tree but 
only based on the un-bracketed, POS tagged 
corpus. With this spec, a chunked data is built 
and HMM is used to build the chunker. TBL-
based error correction is used to further 
improve chunking performance. The average 
chunk length is about 1.38 tokens, F measure 
of chunking achieves 91.13%, labeling 
accuracy alone achieves 99.80% and the ratio 
of crossing brackets is 2.87%. We also find 
that the hardest point of Chinese chunking is 
to identify the chunking boundary inside 
noun-noun sequences1.  
1 Introduction 
Abney (1991) has proposed chunking as a useful 
and relative tractable median stage that is to divide 
sentences into non-overlapping segments only 
based on superficial analysis and local information. 
(Ramshaw and Marcus, 1995) represent chunking 
as tagging problem and the CoNLL2000 shared 
task (Kim Sang and Buchholz, 2000) is now the 
standard evaluation task for chunking English. 
Their work has inspired many others to study 
chunking for other human languages.  
Besides the chunking algorithm, spec (the 
detailed definitions of all chunk types) is another 
critical issue for automatic chunking development. 
The well-defined spec can induce the chunker to 
perform well. Currently chunking specs are 
defined as some rules or one program to extract 
phrases from Treebank such as (Li, 2003) and (Li, 
2004) in order to save the cost of manual 
annotation. We name it as Treebank-derived spec. 
However, we find that it is more valuable to 
compile another type of chunking spec according 
to the observation from un-bracketed corpus 
instead of Treebank. 
                                                     
1
 This work was done while Hongqiao Li was visiting 
Microsoft Research Asia. 
Based on the problems of chunking Chinese that 
are found with our observation, we explain the 
reason why another type of spec is needed and then 
propose our spec in which the shortening and 
extending strategies are used to resolve these 
problems. We also compare our spec with a 
Treebank-derived spec which is derived from 
Chinese Treebank (CTB) (Xue and Xia, 2000). An 
annotated chunking corpus is built with the spec 
and then a chunker is also constructed accordingly. 
For annotation, we adopt a two-stage processing, 
in which text is first chunked manually and then 
the potential inconsistent annotations are checked 
semi-automatically with a tool. For the chunker, 
we use HMM model and TBL (Transform-based 
Learning) (Brill, 1995) based error correction to 
further improve chunking performance. With our 
spec the overall average length of chunks arrives 
1.38 tokens, in open test, the chunking F measure 
achieves 91.13% and 95.45% if under-combining 
errors are not counted. We also find the hardest 
point of Chinese chunking is to identify the 
chunking boundary inside a noun-noun sequence. 
In the remainder of this paper section 2 describes 
some problems in chunking Chinese text, section 3 
discusses the reason why another type of spec is 
needed and proposes our chunking spec, section 4 
discusses the annotation of our chunking corpus, 
section 5 describes chunking model, section 6 
gives experiment results, section 7, 8 recall some 
related work and give our conclusions respectively. 
2 Problems of Chunking Chinese Text 
The purpose of Chinese chunking is to divide 
sentence into syntactically correlated parts of 
words after word segmentation and part-of-speech 
(POS) tagging. For example: 
[NP ?? /ns ?Zhuhai?] ? /u ?of? [NP ?? /a 
?solid? ??/n ?traffic? ??/n ?frame?] [VP ?/d 
?already? ? ?? ? /v ?achieve considerable 
scale ? ?/u]   ?Zhuhai has achieved considerable 
scale in solid traffic frame.? 
According to Abney?s definition, most chunks 
are modifier-head structures and non-overlapping. 
However, some syntactic structures in Chinese are 
very hard to be chunked correctly due to 
characteristics of Chinese language, for example, 
less using of function words and less inflection 
formats. Table 1 shows the most common 
structural ambiguities occurred during Chinese 
chunking. Their occurrences and distributions of 
each possible structure are also reported. As can be 
seen in Table 1, only 77% neighboring nouns can 
be grouped inside one chunk; if the left word is ??
/of? or a verb, this figure will ascend to 80% and 
94% respectively; but if the left word is an 
adjective or a numeral, it will descend to 70% and 
59% respectively; for ?n_c_n?, only 52%  are word 
level coordination. In contrast with English 
chunking, several hard problems are described in 
detail as following.  
(1) Noun-noun compounds 
Compounds formed by more than two 
neighboring nouns are very common in Chinese 
and not always all the left nouns modify the head 
of the compound. Some compounds consist of 
several shorter sub-compounds. For example: 
( ? ? /younger ??? /volunteer ? ?
/science and technology ???/service team)    
?young volunteer service team of science and 
technology? 
??? ???? and ??? ???? are two sub-
compounds and the former modifies the latter. 
But sometimes it is impossible to distinguish the 
inner structures, for example: 
??/world ??/peace ??/career  
It is impossible to distinguish whether it is {{?
? ??} ??} or {?? {?? ??}}. 
English chunking also shows such problem, and 
the common solution for English is not to identify 
their inner structure and treat them as a flat noun 
phrase. Following is an example in CoNLL2000 
shared task: 
[NP employee assistance program directors] 
(2) Coordination 
Coordination in all cases can be divided into two 
types: with conjunctions and without conjunctions. 
The former can be further divided into two 
subcategories: word-level and phrase-level 
coordinations. For example: 
{ ? ?? /policy ?? /bank ? /and ??
/commercial ? ? /bank} ? /of { ? ?
/relationship ?/and  ??/cooperation}    ?the 
relationship and cooperation between policy 
banks and commercial banks?. 
The former coordination is phrase-level and the 
latter is word-level. Unfortunately, sometimes it is 
difficult or even impossible to distinguish whether 
it is word-level or phrase-level at all, for example: 
?? /least ?? /salary ? /and ??? /living 
maintenance ?the least salary and living 
maintenance? 
It is impossible to distinguish ???? is a shared 
modifier or not. English chunking also has such 
kind of problems. The solution of CoNLL2000 is 
to leave the conjunctions outside chunks for 
phrase-level coordinations and to group the 
conjunction inside a chunk when it is word-level or 
impossibly distinguished phrase-level. For 
example: 
[NP enough food and water] 
In Chinese, some coordinate construction has no 
conjunction or punctuation inside, and also could 
not be distinguished from a modifier-head 
construction with syntactic knowledge only. For 
example: 
??/order (??/police wagon ??/caution 
light ???/alarm whistle)   ?Order the police 
Pattern1 No.2 Distributions Examples 
n_n 951 
77% (modifier head) 
7% (coordination) 
16% (others) 
(??/society ??/phenomenon) ?social phenomena? 
(??/language ??/wordage) ?language and wordage? 
(??/capital ??/art ??/stage) ? the stage of capital art? 
v_n_n 154 6% (v_n modify the last noun) 94 % (others) 
?/enter?/factory??/worker 
??/avoid  ??/law ??/duty ?avoid legal duties? 
?_n_n 98 80% ( n_n is modifier_head) 20% (others) 
??/watch ?/of??/traffic ??/cop ?a orderly traffic cop? 
??/paralytic?/of??/body??/function  
a_n_n 27 70% ( a modify the first n) 30% (others) 
?/high ??/technology ??/company ?high-tech company? 
?/old ??/news ???/worker ?old news worker? 
m_n_n 17 41% ( m modify the first n) 59% (others) 
?/two ?/nation ??/people ?our two peoples? 
??/some ??/country ??/area ?some rural areas? 
n_c_n 88 52%(word level coordination) 48%(others) 
??/economy ?/and ??/society ?economy and society? 
??/quality  ?/and  ??/technology ??/requirement 
 
1
 n, v, a, d, m, q,  p, f , c are the POS tags of noun, verb, adjective, adverb, number, measure, preposition, localizer, 
conjunction respectively, ?_? means neighboring, ??/of? is a common auxiliary word in Chinese.  
2This statistical work is done on our test corpus whose setting is shown in Table 3. 
Table 1: The observation of several common structural ambiguities during Chinese chunking 
wagons, caution lights and alarm whistles? 
Such problem does not exist in English because 
almost all coordinations have certain conjunctions 
or punctuations between words or phrases of the 
same syntactic categories in formal English. 
(3) Structural ambiguities 
In Chinese, some structural ambiguities in 
phrase level are impossible or unnecessary to be 
distinguished during chunking. There is an 
example of ?a_n_n?: 
?? /a ?modern? ?? /n ?industry? ?? /n 
?system? 
{?? {?? ??}} or {{?? ??} ??} are 
identically acceptable. English also has such 
problem. The solution of CoNLL2000 is not to 
distinguish inner structure and group the given 
sequence as a single chunk. For example, the inner 
structure of ?[NP heavy truck production]? is ?{{heavy 
truck} production}?, whereas one reading of ?[NP 
heavy quake damage]? is ?{heavy {quake damage}}?. 
Besides, ?a_n_n?, ?m_n_n? and ?m_q_n_n? also 
have the similar problem. 
3 Chinese Chunking Spec 
As a kind of shallow parsing, the principles of 
chunking are to make chunking much more 
efficient and precise than full parsing. Obviously, 
one can shorten the length of chunks to leave 
ambiguities outside of chunks. For example, if we 
let noun-noun sequences always chunk into single 
word, those ambiguities listed in Table 1 would not 
be encountered and the performance would be 
greatly improved. In fact, there is an implicit 
requirement in chunking, no matter which 
language it is, the average length of chunks is as 
longer as possible without violating the general 
principle of chunking. So a trade-off between the 
average chunk length and the chunking 
performance exists. 
3.1 Why another type of spec is needed 
A convenient spec is to extract the lowest non-
terminal nodes from a Treebank (e.g. CTB) as 
Chinese chunked data. But there are some 
problems. The trees are designed for full parsing 
instead of shallow parsing, thus some of these 
problems listed in section 2 could not be resolved 
well in chunking. Maybe we can compile some 
rules to prune the tree or break some non-terminal 
nodes in order to properly resolve these problems 
just like CoNLL2000. However, just as (Kim Sang 
and Buchholz, 2000) noted: ?some trees are very 
complex and some annotations are inconsistent?. 
So these rules are complex, the extracted data are 
inconsistent and manual check is also needed. In 
addition, the resource of Chinese Treebank is 
limited and the extracted data is not enough for 
chunking. 
So we compile another type of chunking spec 
according to the observation from un-bracket 
corpus instead of Treebank. The only shortcoming 
is the cost of annotation, but there are some 
advantages for us to explore.  
1) It coincides with auto chunking procedure, 
and we can select proper solutions to these 
problems without constraints of the exist Treebank. 
The purpose of drafting another type of chunking 
spec is to keep chunking consistency as high as 
possible without hurting the performance of auto-
chunking in whole. 
2) Through spec drafting and text 
annotating most frequent and significant syntactic 
ambiguities could be studied, and those 
observations are in turn described in the spec 
carefully.  
3) With a proper spec and certain mechanical 
approaches, a large-scale chunked data could be 
produced without supporting from the Treebank. 
3.2 Our spec 
Our spec and chunking annotation are based on 
PK corpus2 (Yu et al 1996). The PK corpus is un-
bracketed, but in which all words are segmented 
and only one POS tag is assigned to each word. 
We define 11 chunk types that are similar with 
CoNLL2000. They are NP (noun chunk), VP (verb 
chunk), ADJP (adjective chunk), ADVP (adverb 
chunk), PP (prepositional chunk), CONJP 
(conjunction), MP (numerical chunk), TP 
(temporal chunk), SP (spatial chunk), INTJP 
(interjection) and INDP (independent chunk).  
During spec drafting we try to find a proper 
chunk spec to solve these problems by two ways: 
either merging neighboring chunks into one chunk 
or shortening them. Besides those structural 
ambiguities, we also extend boundary of the 
chunks with minor structural ambiguities in order 
to make the chunks close to the constituents. 
3.2.1 Shortening 
The auxiliary ??/of? is one of the most frequent 
words in Chinese and used to connect a pre-
modifier with its nominal head. However the left 
boundary of such a ? -construction is quite 
complicated: almost all kinds of preceding clauses, 
phrases and words can be combined with it to form 
such a pre-modifier, and even one ?-construction 
can embed into another. So we definitely leave it 
outside any chunk. Similarly, conjunctions, ??
/and?, ?? /or? and ?? /and? et al, are also left 
outside any chunk no matter they are word-level or 
                                                     
2
 Can be downloaded from www.icl.pku.edu.cn  
phrase-level coordinations. For instances, the 
examples in Section 2 are chunked as ?[NP ??? 
??] ? [NP?? ??] ? [NP ??] ?  [NP?
?]? and ?[ADJP ??] [NP ??]  ?  [NP ??
?]? 
3.2.2 Extending 
(1) NP 
Similar with the shared task of CoNLL2000, 
we define noun compound that is formed by a 
noun-sequence: ?a_n_n?, ?m_n_n? or ?m_q_n_n?, 
as one chunk, even if there are sub-compounds, 
sub-phrase or coordination relations inside it. For 
instances, ?[NP ?? ??? ?? ???]?, 
?[NP ?? ?? ??]?, ?[VP??] [NP?? ?
? ???]?, ?[NP ?? ?? ??] and ?[NP ?
? ?? ??]? are grouped into single chunks 
respectively. 
However, it does not mean that we blindly bind 
all neighboring nouns into a flat NP. If those 
neighboring nouns are not in one constituent or 
cross the phrase boundary, they will be chunked 
separately, such as following two examples in 
Table 1: ?[VP?] [NP ?] [NP??]? and ?[ADJP
??] ?/u [NP ??] [NP ??]?. So our solution 
does not break the grammatical phrase structure in 
a given sentence. 
With this chunking strategy, we not only 
properly resolved these problems, but also get 
longer chunks. Longer chunks can make 
successive parsing easier based on chunking. For 
example, if we chunked the sentence as: 
[NP ??] ? [NP ?? ??] [NP ??] [VP ? 
???? ?]  ?/w 
There would be three possible syntactic trees 
which are difficult to be distinguished: 
1a) {{ [NP ??] ? { [NP ?? ??] [NP ?
?]}} [VP ? ????]} 
1b) {{{ [NP ??] ? [NP ?? ??]} [NP ?
?]}  [VP ? ????]} 
1c) {{ [NP ??] ? [NP ?? ??]} { [NP ?
?]  [VP ? ????]}} 
Whereas with above chunking strategy of our 
spec, there is only one syntactic tree remained: 
{{[NP ??] ? [NP ?? ?? ??]} [VP ?  
???? ?]}  ?/w 
Another reason of the chunking strategy is that 
for some NLP applications such as IR, IE or QA, it 
is unnecessary to analyze these ambiguities at the 
early stage of text analysis. 
(2) PP 
Most PP consists of only the preposition itself 
because the right boundary of a preposition phrase 
is hard to identify or far from the preposition. But 
certain prepositional phrases in Chinese are formed 
with a frame-like construction, such as [PP ?/p 
?at? ??/f ?middle?], [PP ?/p ??/f ?top?], etc. 
Statistics shows that more than 90% of those 
frame-like PPs are un-ambiguous, and others 
commonly have certain formal features such as an 
auxiliary ?  or a conjunction immediately 
following the localizer. Table 2 shows the statistic 
result. Thus with those observations, those frame-
like constructions could be chunked as PP. The 
length of such kind of PP frames is restricted to be 
at most two words inside in order to keep the 
distribution of chunk length more even and the 
chunking annotation more consistent.  
 
Pattern1 No.of occurrence Ratio as a chunk 
p_*_f 45 93.33% 
P_*_*_f 36 97.22% 
*_f 40 92.50% 
*_*_f 9 77.78% 
  
1
 This statistical work is also done on our test 
corpus and ?*? means a wildcard for a POS tag. 
Table 2: The ration of grouping these patterns 
as a chunk without any ambiguity  
(3) SP 
Most spatial chunks consist of only the 
localizer(with POS tag ?/s? or ?/f?). But if the 
spatial phrase is in the beginning of a sentence, or 
there is a punctuation (except ???) in front of it, 
then the localizer and its preceding words could be 
chunked as a SP. And the number of words in front 
of the localizer is also restricted to at most two for 
the same reason. 
(4) VP 
Commonly, a verb chunk VP is a pre-modifier 
verb construction, or a head-verb with its following 
verb-particles which form a morphologically 
derived word sequence. The pre-modifier is 
formed by adverbial phrases and/or auxiliary verbs. 
In order to keep the annotation consistent those 
verb particles and auxiliary verbs could be found in 
a closed list respectively only. Post-modifiers of a 
verb such as object and complement should be 
excluded in a verb chunk. 
We find that although a head verb groups more 
than one preceding adverbial phrases, auxiliary 
verbs and following verb-particles into one VP, its 
chunking performance is still high. For example: 
[CONJP ??/c ?if?] [VP ??/d ?lately? 
?/d ?not? ?/v ?can? ??/v ?build? ?/v 
?up?] [NP ??/n ?diplomat ??/n ?relation?] 
?If we could not build up the foreign relations 
soon? 
3.3 Spec Comparison 
We compare our spec with the Treebank-derived 
spec, named as S1, which is to extract the lowest 
non-terminal nodes from CTB as chunks from the 
aspect of the solutions of these problems in section 
2. Noun-noun compound and the coordination 
which has no conjunction are chunked identically 
in both specs. But for others, there are different. In 
S1, the conjunctions of phrase-level coordination 
are outside of chunks and the ones of word-level 
are inside a chunk, all adjective or numerical 
modifiers are separate from noun head. According 
to S1, the example in 3.2.1 should be chunked as 
following. 
[ADJP ???] [NP ??] ? [NP??] [NP ?
?] ? [NP ?? ?  ??] 
But these phrases that are impossible to 
distinguish inner structures during the early stage 
of text analysis are hard to be chunked and would 
cause some inconsistency. ?[ADJP ??] [NP ??] 
?  [NP ???]? or ?[ADJP ??] [NP ?? ? ?
??]?, ?[ADJP ??] [NP ??] [NP ??]? or 
?[ADJP ??] [NP ?? ??]?, are hard to make 
decisions with S1.  
In addition, with our spec outside words are only 
punctuations, structural auxiliary ? ? /of?, or 
conjunctions, whereas with S1, outside words are 
defined as all left words after lowest non-terminal 
extraction. 
4 Chunking Annotation 
Four graduate students of linguistics were 
assigned to annotate manually the PK corpus with 
the proposed chunking spec. Many discussions 
between authors and those annotators were 
conducted in order to define a better chunking spec 
for Chinese. Through the spec drafting and text 
annotating most significant syntactic ambiguities 
in Chinese, such as those structural ambiguities 
discussed in section 2 and 3, have been studied, 
and those observations are carefully described in 
the spec in turn.  
Consistency control is another important issue 
during annotation. Besides the common methods: 
manual checking, double annotation, post 
annotation checking, we explored a new 
consistency measure to help us find the potential 
inconsistent annotations, which is hinted by 
(Kenneth and Ryszard. 2000), who defined 
consistency gain as a measure of a rule in learning 
from noisy data.  
The consistency of an annotated corpus in whole 
could be divided down into consistency of each 
chunk. If the same chunks appear in the same 
context, they should be identically annotated. So 
we define the consistency of one special chunk as 
the ratio of identical annotation in the same context.  
 
corpusin  ))context( ,( of  No.
)context(in  annotation same of No.
        
 ) )context( ,cons(   
PP
P
PP
=
 
(1) 
?
=
=
N
i
ii PcontextPconsN 1
))(,(1cons(S)  (2) 
Where P represents a pattern of the chunk (POS 
or/and lexical sequence), context(P) represents the 
needed context to annotate this chunk, N represents 
the number of chunks in the whole corpus S. 
In order to improve the efficiency we also 
develop a semi-automatic tool that not only check 
mechanical errors but also detect those potential 
inconsistent annotations. For example, one inputs a 
POS pattern: ?a_n_n?, and an expected annotation 
result: ?B-NP_I-NP_E-NP3?, the tool will list all 
the consistent and inconsistent sentences in the 
annotated text respectively. Based on the output 
one can revise those inconsistent results one by one, 
and finally the consistency of the chunked text will 
be improved step by step. 
5 Chunking Model 
After annotating the corpus, we could use 
various learning algorithms to build the chunking 
model. In this paper, HMM is selected because not 
only its training speed is fast, but also it has 
comparable performance (Xun and Huang, 2000). 
Automatic chunking with HMM should conduct 
the following two steps. 1) Identify boundaries of 
each chunk. It is to assign each word a chunk mark, 
named M, which contains 5 classes: B, I, E, S (a 
single word chunk) and O (outside all chunks). 2) 
Tag the chunk type, named X, which contains 11 
types defined in Section 3.  
So each word will be tagged with two tags: M 
and X (the words excluding from any chunk only 
have M). So the result after chunking is a sequence 
of triples (t, m, x), where t, m, x represent POS tag, 
chunk mark and chunk type respectively. All the 
triples of a chunk are combined as an item ni, 
which also could be named as a chunk rule. Let W 
as the word segmentation result of a given sentence, 
T as POS tagging result and C (C= n1 n2?nj) as the 
chunking result. The statistical chunking model 
could be described as following: 
),(),|(maxarg      
),(/),(),|(maxarg      
),|(maxarg
TCPTCWP
TWPTCPTCWP
TWCPC
C
C
C
=
=
=
?
 
(3) 
Independent assumption is used to approximate 
P(W|C,T), that is: 
                                                     
3
 B, E, I represent the left/right boundary of a chunk 
and inside a chunk respectively, B-NP means this word 
is the beginning of NP. 
?
=
?
m
i
iiii xmtwPTCWP
1
),,|(),|(  (4) 
If the triple is unseen, formula 5 is used. 
2
,
)),,((max
),,(),,|(
kjikj
iii
iiii
xmtcount
xmtcount
xmtwP =  
(5) 
For P(C, T), tri-grams among chunks and outside 
words are used to approximate, that is: 
?
=
??
?
k
i
iii nnnPnnPnPTCP
3
12121 )|()|()(),(
 
(6) 
Smoothing follows the method of (Gao et al, 
2002).  
In order to improve the performance we use N-
fold error correction (Wu, 2004) technique to 
reduce the error rate and TBL is used to learn the 
error correction rules based on the output of HMM. 
6 Data and Evaluation 
The performance of chunking is commonly 
measured with three figures: precision (P), recall 
(R) and F measure that are defined in CoNLL2000. 
Besides these, we also use two other measurements 
to evaluate the performance of bracketing and 
labeling respectively: RCB(ratio of crossing 
brackets), that is the percentage of the found 
brackets which cross the correct brackets; 
LA(labeling accuracy), that is the percentage of the 
found chunks which have the correct labels.   
 
datain test  chunks  of  No.
boundarieschunk  crossed chunks  theof No.
       
  RCB   =
 
 
 boundariescorrect  with chunks   theof  No.
chunkscorrect  of No.
       
LA     =
 
The average length (ALen) of chunks for each 
type is the average number of tokens in each chunk 
of given type. The overall average length is the 
average number of tokens in each chunk. To be 
more disinterested, outside tokens (including 
outside punctuations) are also concerned and each 
of them is counted as one chunk.  
6.1 Chunking performance with our spec 
Training and test was done on the PK corpus. 
Table 3 shows the detail information. We use the 
uni-gram of chunk POS rules as the baseline.  
Data No. of tokens 
No. of 
chunks 
No. of 
outside 
ALen 
(include O) 
Train 444,777 229,989 92,839 1.377 
Test 28,382 13,879 5,493 1.363 
 
Table 3:The information of data set 
Table 4 shows the chunking performance of 
close test and open test when HMM and ten folds 
TBL based error correction (EC) are done 
respectively.  
 
Close Test (%) Open Test (%) 
 
F RCB LA F RCB LA 
Baseline 81.95 6.55 99.46 81.44 6.58 99.47 
HMM 94.79 2.62 99.78 88.39 3.18 99.65 
HMM+EC 95.11 2.38 99.91 91.13 2.87 99.80 
Table 4:The overall performance of chunking 
As can be seen, the performance of open test 
doesn?t drop much. For open test, HMM achieves 
6.9% F improvement, 3.4% RCB reduction on 
baseline; error correction gets another 2.7% F 
improvement, 0.3% RCB reduction. Labeling 
accuracy is so high even with the baseline, which 
indicates that the hard point of chunking is to 
identify the boundaries of each chunk.  
Table 5 shows the performance of each type of 
chunks respectively. NP and VP amount to 
approximately 76% of all chunks, so their 
chunking performance dominates the overall 
performance. Although we extend VP and PP, their 
performances are much better than overall. The 
performance of INDP can arrive 99% although it is 
much longer than other types. Because its surface 
evidences are clear and complete owing to its 
definition: the meta-data of a document, all the 
descriptions inside a pair of parenthesis, and also 
certain fixed phrases which do not act as a 
syntactic constituent in a sentence. From the 
relative lower performance of NP, but the most 
part of all chunks, we can conclude that the hardest 
issue of Chinese chunking is to identify boundaries 
of NPs.  
 
Percent
age(%) 
ALen 
(tokens) 
P 
(%) 
R 
(%) 
F 
(%) 
NP 45.94 1.649 88.82 86.25 87.52 
VP 29.82 1.416 96.60 96.49 96.55 
PP 6.59 1.221 93.67 93.58 93.63 
MP 3.69 1.818 89.51 86.33 87.89 
ADJP 3.77 1.308 86.11 89.43 87.74 
SP 2.71 1.167 84.70 84.03 84.36 
TP 2.59 1.251 93.23 94.30 93.76 
CONJP 2.22 1.000 97.20 98.73 97.96 
INDP 1.41 4.297 99.06 99.06 99.06 
ADVP 1.06 1.117 85.48 85.03 85.25 
INTJP 0.23 1.016 68.75 95.65 80.00 
ALL 100 1.507 91.70 90.55 91.13 
 
Table 5:The result of each type with our spec 
All the chunking errors could be classified into 
four types: wrong labeling, under-combining, over-
combining and overlapping. Table 6 lists the 
number and percentage of each type of errors. 
Under-combining errors count about a half  
number of overall chunking errors, however it is 
not a problem in certain applications because they 
does not cross the brackets, thus there are still 
opportunities to combine them later with additional 
knowledge. If we evaluate the chunking result 
without counting those under-combining errors, the 
F score of the proposed chunker achieves 95.45%. 
Error type No.of the Errors Percentage 
Wrong labeling 22 2.56% 
Under-combine 418 48.71% 
Over-combining 339 39.51% 
Overlapping 59 6.88% 
 
Table 6:The distribution of chunking errors 
With comparison we also use some other 
learning methods, MBL(Bosch and Buchholz, 
2002), SVM(Kudoh and Matsumoto, 2001) and 
TBL to build the chunker. The features for MBL 
and SVM are the POS of current, left two and right 
two words, lexical of current, left one and right one 
word. TiMBL 4  and SVM-light 5  are used as the 
tools. For SVM, we convert the chunk marks 
BIOES to BI and the binary class SVM is used to 
classifier the chunk boundary, then some rules are 
used to identify its label. For TBL, the rule 
templates are all the possible combinations of the 
features and the initial state is that each word is a 
chunk. Table 7 shows the result. As seen, without 
error correction all these models do not perform 
well and our HMM gets the best performance. 
 MBL SVM TBL HMM 
F(%) 85.31 86.25 86.92 88.39 
 
Table 7:Comparison with different algorithms 
6.2 Further applications 
The length of chunks with our spec (AoL is 1.38) 
is longer than other Treebank-derived specs (AoL 
of S1 is 1.239) and closer to the constituents of 
sentence. Thus there are several applications 
benefit from the fact, such as: 
1) The longest/full noun phrase identification. 
According to our statistics, due to including noun-
noun compounds, ?a_n_n? and ?m_n_n? inside NPs, 
65% noun chunks are already the longest/full  noun 
phrases and other 22% could become the longest 
/full noun phrases by only one next combining step. 
2) The predicate-verb identification. 
By extending the average length of VPs, the main 
verb (or predicate-verb, also called tensed verb in 
English) of a given sentence could be identified 
based on certain surface evidences with a relatively 
high accuracy. With certain definition our statistics 
based on our test set show that 84.88% of those 
main verbs are located in the first longest VPs 
among all VPs in a sentence. 
                                                     
4
 http://ilk.kub.nl/software.html  
5
 http://svmlight.joachims.org/ 
7 Related Work 
For chunking spec, the CoNLL2000 shared task 
defines a program chunklink to extract chunks 
from English Treebank. (Li, 2003) defines the 
similar Treebank-derived spec for Chinese and she 
reports manual check is also needed to make data 
consistent. Part of the Sparkle project has 
concentrates on a spec based on un-bracketed 
corpus of English, Italian, French and 
German(Carroll et al, 1997). (Zhou, 2002) defines 
base phrase which is similar as chunk for Chinese, 
but his annotation and experiment are on his own 
corpus. 
For chunking algorithm, many machine learning 
(ML) methods have been applied and got 
promising results after chunking is represented as 
tagging problem, such as: SVM (Kudoh and 
Matsumoto, 2001), Memory-based (Bosch and 
Buchholz, 2002), SNoW (Li and Roth), et al. 
Some rule-base chunking (Kinyon, 2003) and 
combining rules with learning (Park and Zhang, 
2003) are also reported.  
For annotation, (Brants, 2000) reports the inter-
annotator agreement of part-of-speech annotations 
is 98.57%, the one of structural annotations is 
92.43% and some consistency measures. (Xue et 
al., 2002) also address some issues related to 
building a large-scale Chinese corpus. 
8 Conclusion 
We propose a solution of Chinese chunking with 
another type of spec that is based on un-bracketed 
corpus rather than derived from a Treebank. 
Through spec drafting and annotating, most 
significant syntactic ambiguous patterns have been 
studied, and those observations in turn have been 
described in the spec carefully. The proposed 
method of defining a chunking spec helps us find a 
proper solution for the hard problems of chunking 
Chinese. The experiments show that with our spec, 
the overall Chinese chunking F-measure achieves 
91.13% and 95.45% if under-combining errors are 
not counted. 
9 Acknowledgements 
We would like to thank the members of the 
Natural Language Computing Group at Microsoft 
Research Asia. Especial acknowledgement is to the 
anonymous reviewers for their insightful 
comments and suggestions. Based on that we have 
revised the paper accordingly.  
References  
S. Abney. 1991. Parsing by chunks. In Principle-
Based Parsing. Kluwer Academic Publishers, 
Dordrecht: 257?278. 
L.A. Ramshaw and M.P. Marcus. 1995. Text 
chunkingusing transformation-based learning. In 
Proceedings of the 3rd ACL/SIGDAT Workshop, 
Cambridge, Massachusetts, USA: 82?94. 
E. Tjong Kim Sang and S. Buchholz. 2000. 
Introduction to the CoNLL-2000 shared task: 
Chunking. In Proceedings of CoNLL-2000 and 
LLL-2000, Lisbon, Portugal: 127?132.  
Sujian Li, Qun Liu and Zhifeng Yang. 2003. 
Chunking based on maximum entropy. Chinese 
Journal of Computer, 25(12): 1734?1738. 
Heng Li, Jingbo Zhu and Tianshun Yao. 2004. 
SVM based Chinese text chunking. Journal of 
Chinese Information Processing, 18(2): 1?7. 
Nianwen Xue and Fei Xia. 2000. The Bracketing 
Guidelines for the Penn Chinese Treebank(3.0). 
Technical report ,University of Pennsylvania, 
URL http:// www.cis.upenn.edu/~chinese/. 
Eric Brill. Transformation-based error-driven 
learning and natural language processing: A case 
study in part of speech tagging. Computational 
Linguistics,21 (4):543?565. 
Shiwen Yu, Huiming Duan, Xuefeng Zhu, et al 
2002. The basic processing of contemporary 
Chinese corpus at Peking University. Journal of 
Chinese Information Processing, 16(6): 58?65. 
K.A. Kaufman and R.S. Michalski. 1999. Learning 
from inconsistent and noisy data: the AQ18 
approach, Proceedings of the Eleventh 
International Symposium on Methodologies for 
Intelligent Systems, Warsaw: 411?419. 
Endong Xun and Changning Huang. 2000. A 
unified statistical model for the identification of 
English baseNP, In Proceedings of the 38th ACL: 
109?117. 
Jianfeng Gao, Joshua Goodman, Mingjing Li, Kai-
Fu Lee. Toward a unified approach to statistical 
language modeling for Chinese. ACM 
Transactions on Asian Language Information 
Processing, Vol. 1, No. 1, 2002: 3-33.  
Dekai WU, Grace NGAI, Marine CARPUAT. N-
fold Templated Piped Correction. Proceedings of 
the First International Joint Conference on 
Natural Language Processing, SANYA: 632?
637. 
Antal van den Bosch and S. Buchholz. 2002. 
Shallow parsing on the basis of words only: a 
case study, In Proceedings of the 40th ACL: 433?
440. 
Taku Kudo and Yuji Matsumoto. 2000. Use of 
support vector learning for chunk identification, 
In Proceedings of the 4th CoNLL: 142?144. 
J. Carroll, T. Briscoe, G. Carroll et al 1997. 
Phrasal parsing software. Sparkle Work 
Package 3, Deliverable D3.2.  
Yuqi Zhang and Qiang Zhou. 2002. Automatic 
identification of Chinese base phrases. Journal 
of Chinese Information Processing, 16(6):1?8. 
X. Li and D. Roth. 2001. Exploring evidence for 
shallow parsing. In Proceedings of the 5th 
CoNLL. 
Alexandra Kinyon. 2003. A language-independent 
shallow-parser compiler. In Proceedings of 10th 
EACL Conference, Toulouse, France: 322-329. 
S.-B. Park, B.-T. Zhang. 2003. Text chunking by 
combining hand-crafted rules and memory-based, 
In Proceedings of the 41th ACL: 497?504. 
Thorsten Brants. 2000. Inter-annotator agreement 
for a German newspaper corpus, In Second 
International Conference on Language 
Resources and Evaluation LREC-2000, Athens, 
Greece: 69?76. 
Nianwen Xue, Fu-Dong Chiou and M. Palmer. 
2002. Building a large-scale annotated Chinese 
corpus, In Proceedings of COLING.  
 
 
A Semi-Supervised Approach to Build Annotated Corpus for Chinese Named 
Entity Recognition 
Xiaoshan FANG#, Jianfeng GAO*, Huanye SHENG# 
*Microsoft Research Asia, jfgao@microsoft.com 
#Shanghai Jiaotong University, China, {fang-xs, hysheng}@sjtu.edu.cn 
 
 
Abstract1 
This paper presents a semi-supervised ap-
proach to reduce human effort in building an 
annotated Chinese corpus. One of the disad-
vantages of many statistical Chinese named 
entity recognition systems is that training data 
may be in short supply, and manually building 
annotated corpus is expensive. In the proposed 
approach, we construct an 80M hand-
annotated corpus in three steps: (1) 
Automatically annotate training corpus; (2) 
Manually refine small subsets of the automati-
cally annotated corpus; (3) Combine small 
subsets and whole corpus in a bootstrapping 
process. Our approach is tested on a state-of-
the-art Chinese word segmentation system 
(Gao et al, 2003, 2004). Experiments show 
that only a small subset of hand-annotated 
corpus is sufficient to achieve a satisfying per-
formance of the named entity component in 
this system.  
1 Introduction 
The success of applying statistical methods to 
natural language processing tasks depends to a 
large degree upon the quality and amount of avail-
able training data.  
This paper presents our method of creating train-
ing data for the statistical Chinese word segmenter 
proposed in Gao et al (2003). The segmenter is 
based on improved source-channel models, which 
are trained on a large amount of annotated training 
data. Whereas the hand-annotation is a very expen-
sive task, creating the training data automatically 
remains an open research problem. Our approach 
falls somewhere between the two extremes of the 
spectrum. We try to minimize the human effort 
while keeping the quality of the annotation rea-
sonably good for model estimation.  The method to 
be presented has been discussed briefly in Gao et 
al. (2003). This paper presents an extended de-
scription with more details and experimental re-
sults. 
                                                     
1 This work was done while the author was visiting 
Microsoft Research Asia. 
The training data refer to a set of Chinese sen-
tences where word boundaries and types have been 
annotated. Our basic solution is the bootstrapping 
approach described in Gao et al (2002). It consists 
of three steps: (1) Initially, we use a greedy word 
segmenter to annotate the corpus, and obtain initial 
models based on the initial annotated corpus; (2) 
We re-annotate the corpus using the obtained mod-
els; (3) Re-train the models using the re-annotated 
corpus. Steps 2 and 3 are iterated until the per-
formance of the system converges. 
In this approach, the quality of the resulting 
models depends to a large degree upon the quality 
of the initial annotated corpus. Because there are 
many named entities that are not stored in a dic-
tionary, traditional dictionary-based forward 
maximum matching (FMM) algorithm is not suffi-
cient to create a good initial corpus. We thus 
manually annotate named entities on a small subset 
(call seed set) of the training data. Then, we obtain 
a model on the seed set (called seed model). We 
thus improve the initial model which is trained on 
the initial annotated training corpus by interpolat-
ing it with the seed model. Our experiments show 
that a relatively small seed set (e.g., 10 million 
characters, which takes approximately three weeks 
for 4 persons to annotate the NE tags) is enough to 
get a good improved model for initialization. 
The remainder of this paper is organized as fol-
lows: Section 2 summarizes the related work. Sec-
tion 3 deals with our approach to improve model 
estimation for Chinese word segmentation. The 
experiments are presented at Section 4. Finally we 
conclude in Section 5. 
2 Related work 
Traditional statistical approaches use a paramet-
ric model with maximum likelihood estimation 
(MLE), usually with smoothing methods to deal 
with data sparseness problems. These approaches 
have been introduced for the task of Chinese word 
segmentation. According to the training data used 
(word-segmented or not), the Chinese word seg-
mentation can be achieved in a supervised or unsu-
pervised manner. 
As an example of unsupervised training, Ge et al 
(1999) presents a simple zero-th order Markov 
model of the words in Chinese text. They devel-
oped an efficient algorithm to train their model on 
an unsegmented corpus. Their basic assumption is 
that Chinese words are usually 1 to 4 characters 
long. They however did not take into account a 
large amount of named entities (e.g. Chinese or-
ganization name, transliterate name and some per-
son names) most of which are longer than 4 char-
acters (e.g., ??????? Microsoft Research 
Asia, ????? California, ?????  a 
woman?s name which puts her husband?s surname 
ahead). 
An and Wong used Hidden Markov Models 
(HMM) for segmentation. Their system is solely 
trained on a corpus which has been manually anno-
tated with word boundaries and Part-of-Speech 
tags. Wu (2003) also used the training data to tune 
the segmentation parameters of their MSR-NLP 
Chinese system. He used the annotated training 
data to deal with the morphologically derived 
words. 
In this paper we present a semi-supervised train-
ing method where we use both an auto-segmented 
training corpus and a small hand-annotated subset 
of it. Comparing to unsupervised approaches, our 
approach leads to a better segmenter that can 
identify much more named entities which are not 
in the dictionary. Comparing to supervised ap-
proaches, our method requires much less human 
effort for data annotation. 
The Chinese word segmenter used in this study 
is described in Gao et al (2003). The segmenter 
provides a unified approach to word segmentation 
and named entity (NE) recognition. This unified 
approach is based on the improved source-channel 
models of Chinese sentence generation, with two 
components: a source model and a set of channel 
models. For each word class (e.g. a person name), 
there is a channel model (referred to as class model 
afterwards) that estimates the generative probabil-
ity of a character string given the word type. The 
source model is used to estimate the generative 
probability of a word sequence, in which each 
word belongs to one word class (e.g. a word in a 
lexicon or a named entity). In another word, it in-
dicates, given a context, how likely a word occurs. 
So the source model is also referred to as context 
model afterwards. This paper focuses the discus-
sion on how to create annotated corpus for context 
model estimation. 
3 A semi-supervised approach to improve 
context model estimation 
In this study the context model is a trigram 
model which estimates the probability of a word 
class. 
Ideally, given an annotated corpus, where each 
sentence is segmented into words which are tagged 
by their word types, the trigram word class prob-
abilities can be calculated using MLE, together 
with a backoff schema (Katz, 1987) to deal with 
the sparse data problem. Unfortunately, building 
such annotated training corpora is very expensive. 
Our basic solution is the bootstrapping approach 
described in Gao et al (2002). It consists of three 
steps: (1) Initially, a greedy word segmenter (i.e. 
FMM) is used to annotate the corpus, and an initial 
context model is obtained based on the initial an-
notated corpus; (2) Re-annotate the corpus using 
the obtained models; (3) Re-train the context 
model using the re-annotated corpus. Steps 2 and 3 
are iterated until the performance of the system 
converges. 
In the above approach, the quality of the context 
model depends to a large degree upon the quality 
of the initial annotated corpus, which is however 
not satisfied due to the fact that many named enti-
ties cannot be identifying using the greedy word 
segmenter which is based on the dictionary. As a 
consequence, the above approach achieves a low 
accuracy in detecting Chinese named entities. 
A straightforward solution to the above problem 
is to obtain large amount of high-quality annotated 
corpus for context model estimation. Unfortunately, 
manually creating such annotated corpus  is very 
expensive. For example, Douglas (1999) pointed 
out that at least up to about 1.2 million words of 
training data are necessary to train an HMM name 
recognizer. To guarantee a high degree of accuracy 
(e.g. 90% F-measure), it requires about 800 hours, 
or 20 person*weeks of labor to annotate and check 
the amount of data. This is almost certainly more 
time than would be required by a skilled rule writer 
to write a rule-based name recognizer achieving 
the same level of performance, assuming all the 
necessary resources, such as lexicons and name 
lists, are already available. 
Our training data contains approximately 80 
million Chinese characters from various domains 
of text. We are facing three questions in annotating 
the training data. (1) How to generate a high qual-
ity hand-annotated corpus? (2) How to best use the 
valuable hand-annotated corpus so as to achieve a 
satisfying performance? (3) What is the optimal 
size of the hand-annotated corpus, considering the 
tradeoff between the cost of human labor and the 
performance of the resulting segmenter? 
We leave the answers to the first and third ques-
tions to Section 4. In what follows, we describe our 
method of using small set of human-annotated cor-
pus to boost the quality of the annotation of the 
entire corpus. It consists of 6 steps. 
Step 1: Manually annotate named entities on a 
small subset (call seed set) of the training data. 
Step 2: Obtain a context model on the seed set 
(called seed model). 
Step 3: Re-annotate the training corpus using the 
seed model and then train an improved context 
model using the re-annotated corpus. 
Step 4: Manually annotate another small subset 
of the training data. Repeat Steps (2) and (3) until 
the entire training data have been annotated. 
Step 5: Repeat steps 1 to 4 using different seed 
sets (we used three seed sets in our experiments, as 
we shall describe in Section 4). 
Step 6: Combine all context models obtained in 
step 5 via linear interpolation: 
P(xyz) =? ?i? Pi(xyz) (1)
Here Pi(xyz) is the trigram probability of the i-th 
context model. ?s is the interpolation weights 
which vary from 0 to 1. 
4 Experiments 
In this section, we first present our experiments 
on the generation and evaluation of hand-annotated 
corpus to answer the first two questions. Then, the 
answer to the third question is given in subsection 
4.2. 
4.1 The generation and evaluation of hand-
annotated corpus 
4.1.1 The generation of hand-annotated corpus 
Four students, whose major is Chinese language, 
annotate the corpus according to a pre-defined 
MSRA?s guideline of Chinese named entities. We 
find that we have to revise the guideline when they 
were annotating the corpus. For example, Chinese 
character string ? ? ? ? ? ? (Shanghai 
Exposition)?can be tagged as either ?[L ?]???
?? or ?[L ??]????. Here ??? is the abbre-
viation of ???(Shanghai)?. ??? is the abbrevia-
tion of ???(city)?. L is the tag of location name. 
It is not clearly described in the guideline where 
the named entity?s right boundary is. 
We obtain in total three manually annotated sub-
sets (i.e. seed sets) by the following process: 
1. Annotate the training data using a greedy 
word segmenter. Highlight the NEs and their 
tags. 
2. Randomly select 10 million characters from 
the annotated training data and then ask the 
students to manually refine these 10 million 
characters. The refinement includes correcting 
the wrong NE tags and adding missing NE 
tags.  
3. Repeat the second step, and then combine the 
obtained new 10-million-character subset with 
the first one. Hence, a 20-million-character 
subset of the training data is obtained. 
4. Repeat the second step, and then combine the 
obtained new 10-million-character subset with 
the 20-million-character subset. Hence, a 30-
million-character subset of the training data is 
obtained. 
A manually annotated test set was developed as 
well. The text corpus contains approximately a half 
million Chinese characters that have been proof-
read and balanced in terms of domain, styles, and 
times. 
4.1.2 The evaluation of hand-annotated corpus 
To evaluate the quality of our annotated corpus, 
we trained a context model using the method de-
scribed in Section 3, with the first-obtained 10-
million-character seed set. We then compare the 
performance of the resulting segmenter with those 
of other state-of-the-art segmenters and the FMM 
segmenter. 
4.1.2.1 Evaluation metrics 
We conduct evaluations in terms of precision (P) 
and recall (R). 
NEsidentifiedofnumber
NEsidentifiedcorrectlyofnumberP ???
????=  (2)
NEsallofnumber
NEsidentifiedcorrectlyofnumberR ???
????=  (3)
4.1.2.2 Segmenters in Comparison 
1. The MSWS system is one of the best 
available products. It is released by Micro-
soft? (as a set of Windows APIs). MSWS first 
conducts the word-breaking using MM (aug-
mented by heuristic rules for disambiguation), 
and then conducts factoid detection and NER 
using rules. 
2. The LCWS system is one of the best re-
search systems in mainland China. It is re-
leased by Beijing Language University. The 
system works similarly to MSWS, but has a 
larger dictionary containing more PNs and 
LNs. 
3. The PBWS system is a rule-based Chinese 
parser which can also output the word seg-
mentation results. It explores high-level lin-
guistic knowledge, such as syntactic structure 
for Chinese word segmentation and NER. 
4.1.2.3 Results 
The performance of the resulting segmenter is 
compared with those of three state-of-the-art seg-
menters and FMM segmenter in Table 1. Here PN, 
LN and ON stand for person name, location name 
and organization name respectively. The first col-
umn lists the segmenters. 
As can be seen from Table 1, the resulting seg-
menter (SSSC.10m) achieves comparable results 
with those of the other three state-of-the-art word 
segmenters. From Table 1 we also find that our 
semi-supervised approach makes a 2.4%-49% im-
provement over FMM. 
 
Segmenter PN LN ON 
 R % P % R % P % R % P %
MSWS 74.4 90.7 44.2 93.5 46.9 64.2
LCWS 78.1 94.5 72.0 85.4 13.1 71.3
PBWS 78.7 78.0 73.6 76.7 21.6 81.7
FMM 65.7 84.4 82.7 76.0 56.6 38.6
SSSC.10m 73.6 86.6 80.7 89.5 84.3 56.8
Impr. (%) 12.0 2.6 2.4 17.7 49.0 47.1
Table 1: Results on different Chinese word seg-
menters 
The results show a moderate amount of hand-
annotated corpus leads our segmenter to a state-of-
the-art performance. 
4.2 The optimal size of the hand-annotated 
corpus 
Regarding the third question: what is the optimal 
size of the hand-annotated subset, considering the 
tradeoff between the cost of human labor and the 
performance of the resulting segmenter? We obtain 
a series of results using 10-30-million-character 
subsets as seed sets, and then plot three graphs 
showing the relationship between the performances 
and their corresponding human efforts of 
constructing the seed set. 
4.2.1 Baselines 
We use two baselines. 
One is the FMM method, which does not use 
annotated training data. It is used to evaluate the 
performances using 10-30-million-character sub-
sets (see Figure 1). 
The other is to use all the human effort of anno-
tating the whole training data, which takes about 
1920 person*hours human effort. It is used to cal-
culate how much labor we would save by using the 
semi-supervised approach described in section 3.  
4.2.2 Results 
The relationship between the performances and 
their corresponding human efforts of constructing 
the seed sets is shown in Figure 1. The X-axes give 
the human efforts on building 10, 20, and 30-
million-character subsets. They are 360, 720, and 
1080 person*hours. The Y-axes show the recall 
and precision results on person name, location 
name and organization name, separately.  
 
60
70
80
90
0 360 720 1080
PN
Recall (%) Precision (%)
30
45
60
75
90
0 360 720 1080
O
N
70
80
90
100
0 360 720 1080
Human effort (person*hour)
LN
 
Figure 1: The relationship between the perform-
ances and their corresponding human efforts 
We observe that both the recall and precision re-
sults first go upwards, and level off after the use of 
720 person*hours, which is the corresponding hu-
man effort of constructing 20 million characters. 
This means that 20 million characters is a satura-
tion point, because more human effort does not 
lead to any improvement in performance, and less 
human effort leads to lower performance. 
From the fact that manually annotating the 
whole training data costs 1920 person*hours, we 
indicate that by using our semi-supervised ap-
proach we save 62.5% human labor in corpus an-
notation. 
5 Conclusion 
This paper presents a semi-supervised method to 
save human effort in building annotated corpus. 
This method uses a small set of human-annotated 
corpus to boost the quality of the annotation of the 
entire corpus. We test this method on Gao?s Chi-
nese word segmentation system, which achieves a 
state-of-the-art performance on SIGHAN backoff 
data sets (Gao et al 2004). 
Several conclusions can be drawn from our ex-
periments: 
z The obtained corpus is of high quality. 
z 20-million-characters is the optimal size of 
hand-annotated subset to boost the 80-million-
character training data, considering the trade-
off between the cost of human labor and the 
performance of the resulting segmenter. 
z We save 62.5% human labor in corpus anno-
tation. 
References 
An, Q. and Wong, W. S. 1996. Automatic segmen-
tation and tagging of Hanzi text using a hybrid 
algorithm. In Proceedings of the 9th Interna-
tional Conference on Indus-trial & Engineering 
Applications of AI & Expert Systems. 
Borthwick, Andrew. 1999. A Maximum Entropy 
Approach to Named Entity Recognition. Ph.D. 
thesis, New York University. 
Douglas Appelt. 1999. Introduction to Information 
Extraction Technology. A Tutorial Prepared for 
IJCAI-99. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002. Toward a unified approach to 
statistical language modeling for Chinese. ACM 
TALIP, 1(1): 3-33. 
Gao, Jianfeng, Mu Li and Changning Huang. 2003. 
Improved source-channel models for Chinese 
word segmentation. In ACL-2003. Sapporo, Ja-
pan. 
Gao, Jianfeng, Andi Wu, Mu Li, Chang-Ning 
Huang, Hongqiao Li, Haowei Qin and Xinsong 
Xia. 2004. Adaptive Chinese Word Segmentation. 
In proceedings of ACL 2004. Barcelona, Spain. 
Ge, X., Pratt, W. and Smyth, P. 1999. Discovering 
Chinese Words from Unsegmented Text. SIGIR-
99, pages 271-272. 
Hockenmaier, J. and Brew, C. 1998. Error-driven 
learning of Chinese word segmentation. In J. 
Guo, K. T. Lua, and J. Xu, editors, 12th Pacific 
Conference on Language and Information, pp. 
218?229, Singapore. Chinese and Oriental Lan-
guages Processing Society. 
Katz, S. M. 1987. Estimation of probabilities from 
sparse data for the language model component 
of a speech recognizer. IEEE Trans. Acoustics, 
Speech Signal Process. ASSP-35, 3 (March), 
400-401. 
Palmer, David. 1997. A Trainable Rule-Based Al-
gorithm for Word Segmentation.  In Proceedings 
of the 35th Annual Meeting of the Association 
for Computational Linguistics (ACL ?97), Ma-
drid. 
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou, 
and Changning Huang. 2002. Chinese named en-
tity identification using class-based language 
model. In: COLING 2002. Taipei, Taiwan. 
Wu, Andi. 2003. Chinese Word Segmentation in 
MSR-NLP. In Proceedings of the Second 
SIGHAN Workshop on Chinese Language Proc-
essing, Sapporo, Japan. 
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,
Beijing, August 2010
A Large Scale Ranker-Based System  
for Search Query Spelling Correction 
 
Jianfeng Gao 
Microsoft Research, Redmond 
jfgao@microsoft.com 
Xiaolong Li 
Microsoft Corporation 
xiaolong.li@microsoft.com 
Daniel Micol 
Microsoft Corporation 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research, Redmond 
chrisq@microsoft.com 
Xu Sun 
University of Tokyo 
xusun@mist.i.u-tokyo.ac.jp 
 
 
Abstract 
This paper makes three significant extensions to a 
noisy channel speller designed for standard writ-
ten text to target the challenging domain of search 
queries. First, the noisy channel model is sub-
sumed by a more general ranker, which allows a 
variety of features to be easily incorporated. Se-
cond, a distributed infrastructure is proposed for 
training and applying Web scale n-gram language 
models. Third, a new phrase-based error model is 
presented. This model places a probability distri-
bution over transformations between multi-word 
phrases, and is estimated using large amounts of 
query-correction pairs derived from search logs. 
Experiments show that each of these extensions 
leads to significant improvements over the state-
of-the-art baseline methods. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods. New 
search queries emerge constantly. As a result, 
many queries contain valid search terms, such as 
proper nouns and names, which are not well es-
tablished in the language. Therefore, recent re-
search has focused on the use of Web corpora 
and search logs, rather than human-compiled lex-
icons, to infer knowledge about spellings and 
word usages in search queries (e.g., Whitelaw et 
al., 2009; Cucerzan and Brill, 2004).  
The spelling correction problem is typically 
formulated under the framework of the noisy 
channel model. Given an input query   
       , we want to find the best spelling correc-
tion           among all candidates: 
         
 
       (1) 
Applying Bayes' Rule, we have 
         
 
           (2) 
where the error model        models the trans-
formation probability from C to Q, and the lan-
guage model (LM)      models the likelihood 
that C is a correctly spelled query. 
This paper extends a noisy channel speller de-
signed for regular text to search queries in three 
ways: using a ranker (Section 3), using Web scale 
LMs (Section 4), and using phrase-based error 
models (Section 5). 
First of all, we propose a ranker-based speller 
that covers the noisy channel model as a special 
case. Given an input query, the system first gen-
erates a short list of candidate corrections using 
the noisy channel model. Then a feature vector is 
computed for each query and candidate correc-
tion pair. Finally, a ranker maps the feature vec-
tor to a real-valued score, indicating the likeli-
hood that this candidate is a desirable correction. 
We will demonstrate that ranking provides a flex-
ible modeling framework for incorporating a 
wide variety of features that would be difficult to 
model under the noisy channel framework. 
Second, we explore the use of Web scale LMs 
for query spelling correction. While traditional 
LM research focuses on how to make the model 
?smarter? via how to better estimate the probabil-
ity of unseen words (Chen and Goodman, 1999); 
and how to model the grammatical structure of 
language (e.g., Charniak, 2001), recent studies 
show that significant improvements can be 
achieved using ?stupid? n-gram models trained 
on very large corpora (e.g., Brants et al, 2007). 
We adopt the latter strategy in this study. We pre-
sent a distributed infrastructure to efficiently train 
and apply Web scale LMs. In addition, we ob-
serve that search queries are composed in a lan-
guage style different from that of regular text. We 
thus train multiple LMs using different texts as-
sociated with Web corpora and search queries. 
Third, we propose a phrase-based error model 
that captures the probability of transforming one 
358
multi-term phrase into another multi-term phrase. 
Compared to traditional error models that account 
for transformation probabilities between single 
characters or substrings (e.g., Kernighan et al, 
1990; Brill and Moore, 2000), the phrase-based 
error model is more effective in that it captures 
inter-term dependencies crucial for correcting 
real-word errors, prevalent in search queries. We 
also present a novel method of extracting large 
amounts of query-correction pairs from search 
logs. These pairs, implicitly judged by millions of 
users, are used for training the error models. 
Experiments show that each of the extensions 
leads to significant improvements over its base-
line methods that were state-of-the-art until this 
work, and that the combined method yields a sys-
tem which outperforms the noisy channel speller 
by a large margin: a 6.3% increase in accuracy on 
a human-labeled query set. 
2 Related Work 
Prior research on spelling correction for regular 
text can be grouped into two categories: correct-
ing non-word errors and real-word errors. The 
former focuses on the development of error mod-
els based on different edit distance functions (e.g., 
Kucich, 1992; Kernighan et al, 1990; Brill and 
Moore, 2000; Toutanova and Moore, 2002). Brill 
and Moore?s substring-based error model, con-
sidered to be state-of-the-art among these models, 
acts as the baseline against which we compare 
our models. On the other hand, real-word spelling 
correction tries to detect incorrect usages of a 
valid word based on its context, such as "peace" 
and "piece" in the context "a _ of cake". N-gram 
LMs and na?ve Bayes classifiers are commonly 
used models (e.g., Golding and Roth, 1996; 
Mangu and Brill, 1997; Church et al, 2007). 
While almost all of the spellers mentioned 
above are based on a pre-defined dictionary (ei-
ther a lexicon against which the edit distance is 
computed, or a set of real-word confusion pairs), 
recent research on query spelling correction fo-
cuses on exploiting noisy Web corpora and query 
logs to infer knowledge about spellings and word 
usag in queries (Cucerzan and Brill 2004; Ahmad 
and Kondrak, 2005; Li et al, 2006; Whitelaw et 
al., 2009).  Like those spellers designed for regu-
lar text, most of these query spelling systems are 
also based on the noisy channel framework. 
3 A Ranker-Based Speller 
The noisy channel model of Equation (2) does 
not have the flexibility to incorporate a wide va-
riety of features useful for spelling correction, 
e.g., whether a candidate appears as a Wikipedia 
document title. We thus generalize the speller to 
a ranker-based system. Let f be a feature vector 
of a query and candidate correction pair (Q, C). 
The ranker maps f to a real value y that indicates 
how likely C is a desired correction. For example, 
a linear ranker maps f to y with a weight vector w 
such as      , where w is optimized for accu-
racy on human-labeled       pairs. Since the 
logarithms of the LM and error model probabili-
ties can be included as features, the ranker covers 
the noisy channel model as a special case. 
For efficiency, our speller operates in two dis-
tinct stages: candidate generation and re-ranking. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. For each term 
q, we consult a lexicon to identify a list of 
spelling suggestions c whose edit distance from q 
is lower than some threshold. Our lexicon con-
tains around 430,000 high frequency query uni-
gram and bigrams collected from 1 year of query 
logs. These suggestions are stored in a lattice.  
We then use a decoder to identify the 20-best 
candidates from the lattice according to Equation 
(2), where the LM is a backoff bigram model 
trained on 1 year of query logs, and the error 
model is approximated by weighted edit distance:  
                         (3) 
The decoder uses a standard two-pass algorithm. 
The first pass uses the Viterbi algorithm to find 
the best C according to the model of Equations 
(2) and (3).  The second pass uses the A-star al-
gorithm to find the 20-best corrections, using the 
Viterbi scores computed at each state in the first 
pass as heuristics. 
The core component in the second stage is a 
ranker, which re-ranks the 20-best candidate cor-
rections using a set of features extracted from 
     . If the top C after re-ranking is different 
from Q, C is proposed as the correction. We use 
96 features in this study. In addition to the two 
features derived from the noisy channel model, 
the rest of the features can be grouped into the 
following 5 categories. 
1. Surface-form similarity features, which 
check whether C and Q differ in certain patterns, 
359
e.g., whether C is transformed from Q by adding 
an apostrophe, or by adding a stop word at the 
beginning or end of Q. 
2. Phonetic-form similarity features, which 
check whether the edit distance between the met-
aphones (Philips, 1990) of a query term and its 
correction candidate is below some thresholds. 
3. Entity features, which check whether the 
original query is likely to be a proper noun based 
on an in-house named entity recognizer. 
4. Dictionary features, which check whether 
a query term or a candidate correction are in one 
or more human-compiled dictionaries, such as the 
extracted Wiki, MSDN, and ODP dictionaries. 
5. Frequency features, which check whether 
the frequency of a query term or a candidate cor-
rection is above certain thresholds in different 
datasets, such as query logs and Web documents. 
4 Web Scale Language Models 
An n-gram LM assigns a probability to a word 
string   
            according to  
    
   ? (  |  
   )
 
   
 ? (  |      
   )
 
   
 (4) 
where the approximation is based on a Markov 
assumption that each word depends only upon the 
immediately preceding n-1 words. In a speller, 
the log of n-gram LM probabilities of an original 
query and its candidate corrections are used as 
features in the ranker.  
While recent research reports the benefits of 
large LMs trained on Web corpora on a variety of 
applications (e.g. Zhang et al, 2006; Brants et al, 
2007), it is also clear that search queries are com-
posed in a language style different from that of 
the body or title of a Web document. Thus, in this 
study we developed a set of large LMs from dif-
ferent text streams of Web documents and query 
logs. Below, we first describe the n-gram LM 
collection used in this study, and then present a 
distributed n-gram LM platform based on which 
these LMs are built and served for the speller. 
4.1 Web Scale Language Models 
Table 1 summarizes the data sets and Web scale 
n-gram LMs used in this study. The collection is 
built from high quality English Web documents 
containing trillions of tokens, served by a popular 
commercial search engine. The collection con-
sists of several data sets built from different Web 
sources, including the different text fields from 
the Web documents (i.e., body, title, and anchor 
texts) and search query logs. The raw texts ex-
tracted from these different sources were pre- 
processed in the following manner: texts are to-
kenized based on white-space and upper case let-
ters are converted to lower case. Numbers are 
retained, and no stemming/inflection is per-
formed. The n-gram LMs are word-based backoff 
models, where the n-gram probabilities are esti-
mated using Maximum Likelihood Estimation 
with smoothing. Specifically, for a trigram mod-
el, the smoothed probability is computed as 
                (5) 
{
               (             )
           
                   
                              
 
where      is the count of the n-gram in the train-
ing corpus and   is a normalization factor.      
is a discount function for smoothing. We use 
modified absolute discounting (Gao et al, 2001), 
whose parameters can be efficiently estimated 
and performance converges to that of more elabo-
rate state-of-the-art techniques like Kneser-Ney 
smoothing in large data (Nguyen et al 2007).  
4.2 Distributed N-gram LM Platform 
The platform is developed on a distributed com-
puting system designed for storing and analyzing 
massive data sets, running on large clusters con-
sisting of hundreds of commodity servers con-
nected via high-bandwidth network.  
We use the SCOPE (Structured Computations 
Optimized for Parallel Execution) programming 
model (Chaiken et al, 2008) to train the Web 
scale n-gram LMs shown in Table 1. The SCOPE 
scripting language resembles SQL which many 
programmers are familiar with. It also supports 
Dataset Body Anchor Title Query 
Total tokens 1.3T 11.0B 257.2B 28.1B 
Unigrams 1.2B 60.3M 150M 251.5M 
Bigrams 11.7B 464.1M 1.1B 1.3B 
Trigrams 60.0B 1.4B 3.1B 3.1B 
4-grams 148.5B 2.3B 5.1B 4.6B 
Size on disk# 12.8TB 183GB 395GB 393GB 
# N-gram entries as well as other model parameters are 
stored. 
Table 1: Statistics of the Web n-gram LMs collection (count 
cutoff = 0 for all models). These models will be accessible at 
Microsoft (2010). 
360
C# expressions so that users can easily plug-in 
customized C# classes. SCOPE supports writing 
a program using a series of simple data transfor-
mations so that users can simply write a script to 
process data in a serial manner without wonder-
ing how to achieve parallelism while the SCOPE 
compiler and optimizer are responsible for trans-
lating the script into an efficient, parallel execu-
tion plan. We illustrate the usage of SCOPE for 
building LMs using the following example of 
counting 5-grams from the body text of English 
Web pages. The flowchart is shown in Figure 1.  
The program is written in SCOPE as a step-
by- step of computation, where a command takes 
the output of the previous command as its input. 
ParsedDoc=SELECT docId, TokenizedDoc 
FROM @?/shares/?/EN_Body.txt? 
USING DefaultTextExtractor; 
NGram=PROCESS ParsedDoc 
PRODUCE NGram, NGcount 
USING NGramCountProcessor(-stream       
TokenizedDoc -order 5 ?bufferSize 
20000000); 
NGramCount=REDUCE NGram 
ON NGram 
PRODUCE NGram, NGcount 
USING NGramCountReducer; 
 
OUTPUT TO @?Body-5-gram-count.txt?; 
The first SCOPE command is a SELECT 
statement that extracts parsed Wed body text. The 
second command uses a build-in Processor 
(NGramCountProcessor) to map the parsed doc-
uments into separate n-grams together with their 
counts. It generates a local hash at each node 
(i.e., a core in a multi-core server) to store the (n-
gram, count) pairs. The third command (RE-
DUCE) aggregates counts from different nodes 
according to the key (n-gram string). The final 
command (OUTPUT) writes out the resulting to a 
data file. 
The smoothing method can be implemented 
similarly by the customized smoothing Proces-
sor/Reducer. They can be imported from the ex-
isting C# codes (e.g., developed for building LMs 
in a single machine) with minor changes.  
It is straightforward to apply the built LMs for 
the ranker in the speller. The n-gram platform 
provides a DLL for n-gram batch lookup. In the 
server, an n-gram LM is stored in the form of 
multiple lists of key-value pairs, where the key is 
the hash of an n-gram string and the value is ei-
ther the n-gram probability or backoff parameter.  
5 Phrase-Based Error Models 
The goal of an error model is to transform a cor-
rectly spelled query C into a misspelled query Q. 
Rather than replacing single words in isolation, 
the phrase-based error model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. The training pro-
cedure closely follows Sun et al (2010). For in-
stance, we might learn that ?theme part? can be 
replaced by ?theme park? with relatively high 
probability, even though ?part? is not a mis-
spelled word. We use this generative story: first 
the correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence 
q1, ?, qk, finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S de-
note the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as bi-
phrases. Finally, let M denote a permutation of K 
elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution 
over rewrite pairs. Let B(C, Q) denote the set of S, 
T, M triples that transform C into Q. Assuming a 
uniform probability over segmentations, the 
phrase-based probability can be defined as: 
Recursive 
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
Web Pages
Parsing
Counting
Local 
Hash
Tokenize
 
Figure 1. Distributed 5-gram counting. 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative procedure 
behind the phrase-based error model. 
361
       ?                    
            
 (6) 
As is common practice in SMT, we use the max-
imum approximation to the sum:  
          
            
                    (7) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs that will act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be identi-
fied with little ambiguity. Thus we restrict our 
attention to those phrase transformations con-
sistent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1?aJ  be a hidden variable representing 
the word alignment between them. Each ai takes 
on a value ranging from 1 to L indicating its cor-
responding word position in C, or 0 if the ith 
word in Q is unaligned. The cost of assigning k 
to ai is equal to the Levenshtein edit distance 
(Levenshtein, 1966) between the ith word in Q 
and the kth word in C, and the cost of assigning 0 
to ai is equal to the length of the i
th word in Q. 
The least cost alignment A* between Q and C is 
computed efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, 
which we denote as B(C, Q, A*). Here, consisten-
cy requires that if two words are aligned in A*, 
then they must appear in the same bi-phrase (ci, 
qi). Once the word alignment is fixed, the final 
permutation is uniquely determined, so we can 
safely discard that factor. Thus we have: 
          
       
       
         (8) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
         ?         
 
   , (9) 
where          is a phrase transformation prob-
ability, the estimation of which will be described 
in Section 5.2.  
To find the maximum probability assignment 
efficiently, we use a dynamic programming ap-
proach, similar to the monotone decoding algo-
rithm described in Och (2002).  
5.2 Training the Error Model  
Given a set of (Q, C) pairs as training data, we 
follow a method commonly used in SMT (Och 
and Ney, 2004) to extract bi- phrases and esti-
mate their replacement probabilities. A detailed 
description is discussed in Sun et al (2010). 
We now describe how (Q, C) pairs are gener-
ated automatically from massive query reformu-
lation sessions of a commercial Web browser. 
A query reformulation session contains a list 
of URLs that record user behaviors that relate to 
the query reformulation functions, provided by a 
Web search engine. For example, most commer-
cial search engines offer the "did you mean" 
function, suggesting a possible alternate interpre-
tation or spelling of a user-issued query. Figure 3 
shows a sample of the query reformulation ses-
sions that record the "did you mean" sessions 
from three of the most popular search engines. 
These sessions encode the same user behavior: A 
user first queries for "harrypotter sheme part", 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+part&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+part& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+part&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 3.  A sample of query reformulation sessions from 3 
popular search engines. These sessions show that a user first 
issues the query "harrypotter sheme part", and then clicks on 
the resulting spell suggestion "harry potter theme park". 
362
and then clicks on the resulting spelling sugges-
tion "harry potter theme park". We can "reverse-
engineer" the parameters from the URLs of these 
sessions, and deduce how each search engine en-
codes both a query and the fact that a user arrived 
at a URL by clicking on the spelling suggestion 
of the query ? an strong indication that the 
spelling suggestion is desired. In this study, from 
1 year of sessions, we extracted ~120 million 
pairs. We found the data set very clean because 
these spelling corrections are actually clicked, 
and thus judged implicitly, by many users. 
In addition to the "did you mean" functionali-
ty, recently some search engines have introduced 
two new spelling suggestion functions. One is the 
"auto-correction" function, where the search en-
gine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results. The other is the "split 
pane" result page, where one half portion of the 
search results are produced using the original 
query, while the other half, usually visually sepa-
rate portion of results, are produced using the 
auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the session data, is already able to 
correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of un-
derestimating the self-transformation probability 
of a query P(Q2=Q1|Q1), because we only includ-
ed in the training data the pairs where the query is 
different from the correction. To deal with this 
problem, we augmented the training data by in-
cluding correctly spelled queries, i.e., the pairs 
(Q1, Q2) where Q1 = Q2.  First, we extracted a set 
of queries from the sessions where no spell sug-
gestion is presented or clicked on. Second, we 
removed from the set those queries that were rec-
ognized as being auto-corrected by a search en-
gine. We do so by running a sanity check of the 
queries against our baseline noisy channel 
speller, which will be described in Section 6. If 
the system consider a query misspelled, we as-
sumed it an obvious misspelling, and removed it. 
The remaining queries were assumed to be cor-
rectly spelled and were added to the training data. 
6 Experiments 
We perform the evaluation using a manually an-
notated data set containing 24,172 queries sam-
pled from one year?s query logs from a commer-
cial search engine. The spelling of each query is 
manually corrected by four independent annota-
tors. The average length of queries in the data 
sets is 2.7 words. We divided the data set into 
non-overlapped training and test data sets. The 
training data contain 8,515       pairs, among 
which 1,743 queries are misspelled (i.e.    ). 
The test data contain 15,657       pairs, among 
which 2,960 queries are misspelled.  
The speller systems we developed in this 
study are evaluated using the following metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, a t-test 
with a significance level of 0.05. 
In our experiments, all the speller systems are 
ranker-based. Unless otherwise stated, the ranker 
is a two-layer neural net with 5 hidden nodes. 
The free parameters of the neural net are trained 
to optimize accuracy on the training data using 
the back propagation algorithm (Burges et al, 
2005) .  
6.1 System Results 
Table 1 summarizes the main results of different 
spelling systems. Row 1 is the baseline speller 
where the noisy channel model of Equations (2) 
363
and (3) is used. The error model is based on the 
weighted edit distance function and the LM is a 
backoff bigram model trained on 1 year of query 
logs, with count cutoff 30. Row 2 is the speller 
using a linear ranker to incorporate all ranking 
features described in Section 3. The weights of 
the linear ranker are optimized using the Aver-
aged Perceptron algorithm (Freund and Schapire, 
1999). Row 3 is the speller where a nonlinear 
ranker (i.e., 2-layer neural net) is trained atop the 
features. Rows 4, 5 and 6 are systems that incor-
porate the additional features derived from the 
phrase-based error model (PBEM) described in 
Section 5 and the four Web scale LMs (WLMs) 
listed in Table 1. 
The results show that (1) the ranker is a very 
flexible modeling framework where a variety of 
fine-grained features can be easily incorporated, 
and a ranker-based speller outperforms signifi-
cantly (p < 0.01) the traditional system based on 
the noisy channel model (Row 2 vs. Row 1); (2) 
the speller accuracy can be further improved by 
using more sophisticated rankers and learning 
algorithms (Row 3 vs. Row 2); (3) both WLMs 
and PBEM bring significant improvements 
(Rows 4 and 5 vs. Row 3); and (4) interestingly, 
the gains from WLMs and PBEM are additive 
and the combined leads to a significantly better 
speller (Row 6 vs. Rows 4 and 5) than that of 
using either of them individually. 
In what follows, we investigate in detail how 
the WLMs and PBEM trained on massive Web 
content and search logs improve the accuracy of 
the speller system. We will compare our models 
with the state-of-the-art models proposed previ-
ously. From now on, the system listed in Row 3 
of Table 1 will be used as baseline. 
6.2 Language Models 
The quality of n-gram LMs depends on the order 
of the model, the size of the training data, and 
how well the training data match the test data. 
Figure 4 illustrates the perplexity results of the 
four LMs trained on different data sources tested 
on a random sample of 733,147 queries. The re-
sults show that (1) higher order LMs produce 
lower perplexities, especially when moving be-
yond unigram models; (2) as expected, the query 
LMs are most predictive for the test queries, 
though they are from independent query log 
snapshots; (3) although the body LMs are trained 
on much larger amounts of data than the title and 
anchor LMs, the former lead to much higher per-
plexity values, indicating that both title and an-
chor texts are quantitatively much more similar to 
queries than body texts. 
Table 2 summarizes the spelling results using 
different LMs. For comparison, we also built a 4-
gram LM using the Google 1T web 5-gram cor-
pus (Brants and Franz, 2006). This model is re-
ferred to as the G1T model, and is trained using 
the ?stupid backoff? smoothing method (Brants et 
al., 2007). Due to the high count cutoff applied 
by the Google corpus (i.e., n-grams must appear 
at least 40 times to be included in the corpus), we 
found the G1T model results to a higher OOV 
rate (i.e., 6.5%) on our test data than that of the 4 
Web scale LMs (i.e., less than 1%). 
The results in Table 2 are more or less con-
sistent with the perplexity results: the query LM 
is the best performer; there is no significant dif-
ference among the body, title and anchor LMs 
though the body LM is trained on a much larger 
amount of data; and all the 4 Web scale LMs out-
perform the G1T model substantially due to the 
significantly lower OOV rates. 
6.3 Error Models 
This section compares the phrase-based error 
model (PBEM) described in Section 5, with one 
of the state-of-the-art error models, proposed by 
Brill and Moore (2000), henceforth referred to as 
# System Accuracy Precision Recall 
1 Noisy channel 85.3 72.1 35.9 
2 Linear ranker 88.0 74.0 42.8 
3 Nonlinear ranker 89.0 74.1 49.6 
4 3 + PBEM 90.7 78.7 58.2 
5 3 + WLMs 90.4 75.1 58.7 
6 3 + PBEM + WLMs  91.6 79.1 63.9 
Table 1. Summary of spelling correction results. 
 
Figure 4. Perplexity results on test queries, using n-
gram LMs with different orders, derived from differ-
ent data sources. 
 
364
the B&M model. B&M is a substring error mod-
el. It estimates        as 
          
    
           
?        
   
   
  (10) 
where R is a partitioning of correction term c into 
adjacent substrings, and T is a partitioning of 
query term q, such that |T|=|R|. The partitions are 
thus in one-to-one alignment. To train the B&M 
model, we extracted 1 billion term-correction 
pairs       from the set of 120 million query-
correction pairs      , derived from the search 
logs as described in Section 5.2.  
Table 3 summarizes the comparison results. 
Rows 1 and 2 are our ranker-based baseline sys-
tems with and without the error model (EM) fea-
ture. The error model is based on weighted edit 
distance of Eq. (3), where the weights are learned 
on some manually annotated word-correction 
pairs (which is not used in this study). Rows 3 
and 4 are the B&M models using different maxi-
mum substring lengths, specified by L. L=1 re-
duces B&M to the weighted edit distance model 
in Row 2. Rows 5 and 6 are PBEMs with differ-
ent maximum phrase lengths. L=1 reduces PBEM 
to a word-based error model. The results show 
the benefits of capturing context information in 
error models. In particular, the significant im-
provements resulting from PBEM demonstrate 
that the dependencies between words are far 
more effective than that between characters 
(within a word) for spelling correction. This is 
largely due to the fact that there are many real-
word spelling errors in search queries. We also 
notice that PBEM is a more powerful model  than   
# # of word pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 1M 89.15 73.71 50.74 
3 10M 89.22 74.11 50.92 
4 100M 89.20 73.60 51.06 
5 1B 89.21 73.72 50.99 
Table 4. The performance of B&M error model (L=3) as a 
function of the size of training data (# of word pairs). 
# # of (Q, C) pairs Accuracy Precision Recall 
1 Baseline w/o EM 88.55 71.95 46.97 
2 5M 89.59 77.01 52.34 
3 15M 90.23 77.87 56.67 
4 45M 90.45 78.56 57.02 
5 120M 90.70 78.49 58.12 
Table 5. The performance of PBEM (L=3) as a function of 
the size of training data (# of (Q, C) pairs). 
B&M in that it can benefit more from increasing-
ly larger training data. As shown in Tables 4 and 
5, whilst the performance of B&M saturates 
quickly with the increase of training data, the per-
formance of PBEM does not appear to have 
peaked ? further improvements are likely given a 
larger data set. 
7 Conclusions and Future Work 
This paper explores the use of massive Web cor-
pora and search logs for improving a ranker- 
based search query speller. We show significant 
improvements over a noisy channel speller using 
fine-grained features, Web scale LMs, and a 
phrase-based error model that captures intern- 
word dependencies. There are several techniques 
we are exploring to make further improvements. 
First, since a query speller is developed for im-
proving the Web search results, it is natural to use 
features from search results in ranking, as studied 
in Chen et al (2007). The challenge is efficiency. 
Second, in addition to query reformulation ses-
sions, we are exploring other search logs from 
which we might extract more       pairs for er-
ror model training. One promising data source is 
clickthrough data (e.g., Agichtein et al 2006; 
Gao et al, 2009). For instance, we might try to 
learn a transformation from the title or anchor 
text of a document to the query that led to a click 
on that document. Finally, the phrase-based error 
model is inspired by phrase-based SMT systems. 
We are introducing more SMT techniques such 
as alignment and translation rule exaction. In a 
broad sense, spelling correction can be viewed as 
a monolingual MT problem where we translate 
bad English queries into good ones. 
# System Accuracy Precision Recall 
1 Baseline 89.0 74.1 49.6 
2 1+ query 4-gram 90.1 75.6 56.3 
3 1 + body 4-gram 89.9 75.7 54.4 
4 1 + title 4-gram 89.8 75.4 54.7 
5 1 + anchor 4-gram 89.9 75.1 55.6 
6 1 + G1T 4-gram 89.4 75.1 51.5 
Table 2. Spelling correction results using different LMs 
trained on different data sources. 
# System Accuracy Precision Recall 
1 Baseline w/o EM 88.6 72.0 47.0 
2 Baseline 89.0 74.1 49.6 
3 1 + B&M, L=1 89.0 73.3 50.1 
4 1 + B&M, L=3 89.2 73.7 51.0 
5 1 + PBEM, L=1 90.1 76.7 55.6 
6 1 + PBEM, L=3 90.7 78.5 58.1 
Table 3. Spelling correction results using different error 
models. 
365
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Kuansan Wang for the 
very helpful discussions and collaboration. The 
work was done when Xu Sun was visiting Mi-
crosoft Research Redmond. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Improv-
ing web search ranking by incorporating user be-
havior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a spelling 
error model from search query logs. In HLT-
EMNLP, pp. 955-962. 
Brants, T., and Franz, A. 2006. Web 1T 5-gram corpus 
version 1.1. Technical report, Google Research. 
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. 
2007. Large language models in machine translation. 
In EMNLP-CoNLL, pp. 858 - 867. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In ACL, 
pp. 286-293. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In ICML, 
pp. 89-96.  
 Chaiken, R., Jenkins, B., Larson, P., Ramsey, B., 
Shakib, D., Weaver, S., and Zhou, J. 2008. SCOPE: 
easy and efficient parallel processing f massive data 
sets. In Proceedings of the VLDB Endowment, pp. 
1265-1276. 
Charniak, E. 2001. Immediate-head parsing for lan-
guage models. In ACL/EACL, pp. 124-131. 
Chen, S. F., and Goodman, J. 1999. An empirical 
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13(10):359-
394. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving que-
ry spelling correction using web search results. In 
EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compressing 
trigram language models with Golomb coding. In 
EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Freund, Y. and Schapire, R. E. 1999. Large margin 
classification using the perceptron algorithm. In 
Machine Learning, 37(3): 277-296. 
Gao, J., Goodman, J., and Miao, J. 2001. The use of 
clustering techniques for language modeling -
application to Asian languages. Computational Lin-
guistics and Chinese Language Processing, 
6(1):27?60, 2001.  
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web search 
ranking. In SIGIR, pp. 355-362.  
Golding, A. R., and Roth, D. 1996. Applying winnow 
to context-sensitive spelling correction. In ICML, pp. 
182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 127-
133. 
Kucich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys, 
24(4):377-439. 
Levenshtein, V. I. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. Ex-
ploring distributional similarity based models for 
query spelling correction. In ACL, pp. 1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule acquisi-
tion for spelling correction. In ICML, pp. 187-194. 
Microsoft Microsoft web n-gram services. 2010. 
http://research.microsoft.com/web-ngram 
Nguyen, P., Gao, J., and Mahajan, M. 2007. MSRLM: 
a scalable language modeling toolkit. Technical re-
port TR-2007-144, Microsoft Research. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Philips, L. 1990. Hanging on the metaphone. Comput-
er Language Magazine, 7(12):38-44. 
Sun, X., Gao, J., Micol, D., and Quirk, C. 2010. 
Learning phrase-based spelling error models from 
clickthrough data. In ACL.  
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In ACL, 
pp. 144-151.  
Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis, 
G. 2009. Using the web for language independent 
spellchecking and autocorrection. In EMNLP, pp. 
890-899. 
Zhang, Y., Hildebrand, Al. S., and Vogel, S. 2006. 
Distributed language modeling for n-best list re-
ranking. In EMNLP, pp. 216-233. 
366
Coling 2010: Poster Volume, pages 135?143,
Beijing, August 2010
A comparison of unsupervised methods for  
Part-of-Speech Tagging in Chinese 
 
Alex Cheng 
Microsoft Corporation 
alcheng@microsoft.com 
 Fei Xia 
Univ. of Washington 
fxia@uw.edu 
 Jianfeng Gao 
Microsoft Research 
jfgao@microsoft.com 
 
Abstract 
We conduct a series of Part-of-Speech 
(POS) Tagging experiments using Ex-
pectation Maximization (EM), Varia-
tional Bayes (VB) and Gibbs Sampling 
(GS) against the Chinese Penn Tree-
bank.  We want to first establish a base-
line for unsupervised POS tagging in 
Chinese, which will facilitate future re-
search in this area.  Secondly, by com-
paring and analyzing the results between 
Chinese and English, we highlight some 
of the strengths and weaknesses of each 
of the algorithms in POS tagging task 
and attempt to explain the differences 
based on some preliminary linguistics 
analysis.  Comparing to English, we find 
that all algorithms perform rather poorly 
in Chinese in 1-to-1 accuracy result but 
are more competitive in many-to-1 accu-
racy.  We attribute one possible explana-
tion of this to the algorithms? inability to 
correctly produce tags that match the 
desired tag count distribution. 
1 Introduction 
Recently, there has been much work on 
unsupervised POS tagging using Hidden 
Markov Models (Johnson, 2007; Goldwater & 
Griffiths, 2007).  Three common approaches are 
Expectation Maximization (EM), Variational 
Bayes (VB) and Gibbs Sampling (GS).  EM was 
first used in POS tagging in (Merialdo, 1994) 
which showed that except in conditions where 
there are no labeled training data at all, EM 
performs very poorly.  Gao and Johnson (2008) 
compared EM, VB and GS in English against 
the Penn Treebank Wall Street Journal (WSJ) 
text.  Their experiments on English showed that 
GS outperforms EM and VB in almost all cases.  
Other notable studies in the unsupervised and 
semi-supervised POS domain include the use of 
prototype examples (Haghighi & Klien, 2006), 
dictionary constraints to guide the algorithms 
(Elworthy 1994; Banko & Moore 2004) and 
Bayseian LDA-based model (Toutanova and 
Johnson, 2007). 
   To our knowledge, little work has been done 
on unsupervised POS tagging in Chinese against 
the Chinese Penn Treebank (CTB).  The work 
in Chinese POS tagging has been predominately 
in the supervised fashion (Huang et al 2009; 
Chang & Chen, 1993; Ng & Low, 2004) and 
achieve accuracy of 92.25% using a traditional 
ngram HMM tagger.  For English, a supervised 
trigram tagger achieves an accuracy of  96.7% 
against the Penn Treebank (Thorsten, 2000). 
   In this study, we analyze and compare the 
performance of three classes of unsupervised 
learning algorithms on Chinese and report the 
experimental results on the CTB.  We establish 
a baseline for unsupervised POS tagging in 
Chinese.  We then compare and analyze the 
results between Chinese and English, we 
explore some of the strengths and weaknesses 
of each of the algorithms in POS tagging task 
and attempt to explain the differences based on 
some preliminary linguistics analysis.   
2 Models 
In this section, we provide a brief overview of 
the three unsupervised learning methods for 
POS tagging as described in (Gao & Johnson, 
2008), which all uses a traditional bigram Hid-
den Markov Model (HMM).  HMM is a well-
135
known statistical model, used for sequential 
modeling. To put it formally, let   
                      be the set of possible 
states and                  be the set 
of possible observations.  In the case for POS 
tagging using a bigram model, the set   corres-
ponds to the set of POS tags and the set  cor-
responds to the set of words in the language.  
 
 
Figure 1: Graphical model of an HMM for a 
bigram POS tagger.  The top row represents a 
sequence of hidden states where each is condi-
tionally dependent only on the previous state 
and the bottom row represents a sequence of 
observations where each is conditionally depen-
dent only on the current state. 
 
  An HMM models a sequence of discrete ob-
servations                where       
that are produced by a sequence of hidden 
states                 where       .  The  
sequence of states is produced by a first order 
Markov process such that the current state     
depends only on its previous state     ; corres-
pondingly each of the observations    depends 
only on the state   : 
 
                        
                           
 
where                 is the probability of 
transition to state       from         and 
                is the probability of observa-
tion       produced by      .  The para-
meter   for the HMM is defined by the transi-
tion probability distribution        , emission 
(observation) probability distribution           
and the initial probability            . 
Direct calculation of the likelihood          is 
computationally inefficient, and we can use dy-
namic programming techniques to speed up the 
calculation by calculating the forward probabili-
ty: 
 
                      
 
and backward probability  
                     . 
See (Mannings & Schutze, 1999) for details on 
the calculation. 
2.1 Expectation Maximization (EM) 
EM is a general class of algorithms for finding 
the maximum likelihood estimator of 
parameters in probabilistic models.  It is an 
iterative algorithm where we alternate between 
calculating the expectation of the log likelihood 
of the model given the parameters: 
 
                            
and then finding the parameters that maximizes 
the expected log likelihood.  Using Lagrange 
multipliers with constraint that each parameter 
is a probability distribution, we have these 
update steps for the well-known forward-
backward Algorithm for EM HMM: 
 
                    
 
          
                       
            
 
   
 
 
          
                          
           
 
   
 
   
 
 
where          
       
      
 . 
2.2 Variational Bayes (VB) 
One of the drawbacks of EM is that the result-
ing distribution is very uniform; that is, EM ap-
plies roughly the same number of observations 
for each state.  Instead of using only the best 
model for decoding, the Bayesian approach uses 
and considers all the models; that is, the model 
is treated as a hidden variable.  This is done by 
assigning a probability distribution over the 
model parameters as a prior distribution,      . 
   In HMM, we calculate the probability of the 
observation by considering all models and inte-
grating over the distribution over the priors:  
(5) 
(1) 
(2) 
(3) 
(4) 
136
                          
where                  . 
 
As with the standard in the literature, we use 
Dirichlet Prior as it allows us to model the tag 
distribution more closely and because they are 
in the same conjugate exponential family as the 
log likelihood.  The Dirichlet distribution is pa-
rameterized by a vector of real values   (hyper-
parameters).  There are two ways that we can 
view the vector  .  First, the parameter controls 
the sharpness of distribution for each of the 
components.  This is in contrast to the EM mod-
el where we essentially have a uniform prior.  
Thus, we can view   as our prior beliefs on the 
shape of the distribution and we can make our 
choices based on our linguistics knowledge.  
Second, we can view the role of   in terms of 
predictive distribution based on the statistics 
from observed counts.  For HMM, we can set a 
separate prior for each state-state transition and 
word-state emission distribution, effectively 
giving us control over the distribution of each 
entry in the transition matrix.  However, to sim-
plify the model and without the need to fine 
tune each parameters, we use two fixed hyper-
parameters: all of the state-state probability will 
have the hyper-parameter      and all of the 
word-state probability will have hyper-
parameter    .    
   To begin our estimation and maximization 
procedure, we create            
            as an approximation of the post-
erior of the log likelihood: 
 
         
              
                  
         
  
 
 
 
By taking the functional derivative with respect 
to      to find the distribution that maximizes 
the log likelihood, and following the derivation 
from (Beal, 2003), we arrive at the following 
EM-like procedure: 
 
               
                                     
        
                   
           
 
   
 
                            
 
                           
           
 
   
 
 
This is the Expectation step where   and   is 
the forward and backward probabilities and 
         is the indicator function as in EM. 
   The Maximization step is as follows: 
 
        
 
                            
                           
 
     
 
 
        
  
                    
 
     
                     
 
   
 
     
 
 
        
 
                     
 
    
                      
 
   
 
     
 
   
where                                     , 
                                   and    is 
the digamma function.  
2.3 Gibbs Sampling (GS) 
Gibbs sampling (Geman & Geman, 1984) is a 
widely used MCMC algorithm designed espe-
cially for cases where we can sample from the 
conditional probability easily. It is a 
straightforward application of the Metropolis 
Hasting algorithm where we sample a variable 
   while keeping     constant where     
                     .  We set the proposal 
distribution to  
 
    
               .   
 
So the sampling procedure is the following:  
initialize the components of             .  
Then sample    from              ,    from 
                , and so on for each compo-
nent of  .  For POS tagging, the main idea is 
that we sample the tag   based on the         
and        distribution.  
   The main idea for using GS for POS tagging 
is that in  each iteration, we sample the tag   
based on the         and        distribution.  
(7) 
(6) 
(8) 
(9) 
(10) 
137
Then from the samples, we count the number 
for each state-state and word-state pairs and up-
date the probabilities accordingly.  How we 
sample the data depends on whether we are us-
ing word based or sentence based sampling (the 
Expectation Step).  Whereas how we update the 
probabilities depend on whether we are using a 
collapsed or explicit Gibbs sampler (the Max-
imization Step).  
 
Word Based vs. Sentence Based: Word-based 
and sentence-based approaches to GS determine 
how we sample the each tag   at position   in 
the data set.  For the word-based approach, in-
stead of going through sentence by sentence (as 
in EM and VB procedures), we pick a word po-
sition in the corpus at random (without repeti-
tion) and sample a new tag    at position   using 
the probability: 
 
        
                                    
  
    
Notice that since we are selecting each position 
at random, the tag      at position n-1 and      
at position n+1 are our samples at the previous 
iteration or an already updated samples at the 
current iteration.   
   The sentence-based approach use the forward 
and backward probability to sample the tag 
based on the sentence (Besag, 2004). Specifical-
ly, we use the backward probability       
                to sample the sentence from 
start (     to finish (    .  We sample a 
new tag    at position   using the probability: 
 
                            
                                   
 
where the transition and emission probability 
distribution are from the current model parame-
ters.  Again      is our ?guess? at the previous 
sampling step of the tag of     . 
 
Explicit vs. Collapsed Based: We use the tags 
estimated at the previous step to maximize the 
parameters.  Our choice of using Dirichlet dis-
tributions over the parameters      and      
give us some nice mathematical properties.  We 
show that           and           also calcu-
late to be Dirichlet distributions.  Following  
(MacKay & Peto, 1994), the posterior probabili-
ty of   can be derived as follows: 
 
          
             
       
  
 
       
         
 
   
  
   
         
         
 
 
 
 
   
   
                             
                   
 
   
 
   
                        
 
 
where          is the number of times    is fol-
lowed by    in the sample from the previous 
iteration.   
 
Similarly, we can define           using the 
count          to show that: 
 
                                 
 
 
 
For the collapsed Gibbs sampler, we want to 
integrate over all possible model parameters   
to maximize the new transition probabilities 
using Maximum a posteriori (MAP) estimator: 
 
                                           
                             
 
   
 
            
                
 
 
The last equality uses the following result: 
 
             
 
    
 
 
We can derive a similar result for 
             .  Then we can use the sample 
count to update the new parameter values.   
   An explicit sampler samples the HMM para-
meters   in addition to the states.  Specifically, 
in the Bayesian model, we will need to sample 
from the Dirichlet distribution for the parame-
ters  
 
(11) 
(12) 
(13) 
(14) 
(15) 
(16) 
138
                                 
 
 
 
                                 
 
 
derived above.  An  -dimensional Dirichlet dis-
tribution variable can be generated from gamma 
variate (Wolfram Mathematica, 2009): 
 
                        
                 
                  
               
 
 
we can update the transition probability by ge-
nerating the gamma variate for the Dirichlet 
distribution:  
    
   
     
. 
 
Similarly, we sample the emission probability 
using the count for word-tag with          
    as the hyper-parameter. 
3 Experiment Setup 
Our experiment setup is similar to the ones used 
in (Gao & Johnson, 2007).  They are summa-
rized in Table 1: 
 
Parameters Values 
Data Size 24k, 120k, 500k 
Algorithm EM, VB, GS(c,w), GS(c,s), 
GS(e,s), GS(e,w) 
# of states Chinese: 33  English: 50 
    0.0001, 0.1, 0.5, 1 
    0.0001, 0.1, 0.5, 1 
Table 1: The list of experiments conducted.  For 
the hyper-parameters          , we try the 
combination of the adjacent pairs ? 
(0.0001,0.0001), (0.1,0.0001), (0.0001,0.1), (0.1, 
0.1), (0.1, 0.5), etc.  
3.1 Data  
For our experiments, we use the data set Chi-
nese Penn Treebank (CTB) v5.0.  The Chinese 
Treebank project began at the University of 
Pennsylvania in 1998 and the team created a set 
of annotation guidelines for word segmentation, 
POS tagging and bracketing (Xia, 2000; Xue et 
al., 2002; Xue et al, 2005).  The version used in 
this paper is the Chinese Treebank 5.0 which 
consists of over 500k words and over 800k Chi-
nese characters.  The text comes from various 
sources including newswire, magazine articles, 
website news, transcripts from various broad-
cast news program.   
   Chinese POS tagging faces additional chal-
lenges because it has very little, if any, inflec-
tional morphology. Words are not inflected with 
number, gender, case, or tense. For example, a 
word such as ?? in Chinese corresponds to 
destroy /destroys /destroyed/destruction in Eng-
lish. This fuels the discussion in Chinese NLP 
communities on whether the POS tags should be 
based on meaning or on syntactic distribution 
(Xia, 2000). If only the meaning is used, ?? 
should be a verb all the time. If syntactic distri-
bution is used, the word is a verb or a noun de-
pending on the context.  For the CTB, syntactic 
distribution is used, which complies with the 
principles of contemporary linguistics theories. 
   Following the experiment done for English in 
(Gao & Johnson, 2008), we split the data into 
three sizes: 24k words, 120k words and all 
words (500k), and used the same data set for 
training and testing. The idea is to track the ef-
fectiveness of an algorithm across different cor-
pus sizes.  Instead of using two different tag set 
sizes (17 and 50) as it is done for English POS 
tagging, we opt to keep the original 33 tag set 
for Chinese without further modification.  In 
addition to reporting the results for English 
from (Gao & Johnson, 2008), we run additional 
experiments on English using only 500k words 
for comparison. 
3.2 Decoding 
For decoding, we use max marginal likelihood 
estimator (as opposed to using Viterbi algorithm) 
to assign a tag for each word in the result tag. 
(Gao & Johnson, 2008) finds that max marginal 
decoder performs as well as Viterbi algorithm 
and runs significantly faster as we can reuse the 
forward and backwards probabilities already 
calculated during the estimation and update step.   
3.3    Hyperparameters 
For the Bayesian approaches (VB and GS), we 
have a choice of hyperparameters. We choose 
uniform hyperparameters     and     instead 
(17) 
(18) 
(19) 
139
of choosing a specific hyper-parameter for each 
of the tag-tag and word-tag distribution.  The 
values for the hyper-parameters are chosen such 
that we can see more clearly the interactions 
between the two values.  For GS, we use the 
notation GS(c,s) to denote collapsed sentence-
based approach, GS(e,s) for explicit sentence 
based, GS(c,w) for collapsed word-based and 
GS(e,w) for explicit word based. 
3.4   Evaluation Metrics 
We use POS tagging accuracy as our primary 
evaluation method. There are two commonly 
used methods to map the state sequences from 
the system output to POS tags.  In both methods, 
we first create a matrix where each row corres-
ponds to a hidden state, each column corres-
ponds to a POS tag, and each cell       
represents the number of times a word position 
in the test data comes from the hidden state    
according to the system output and the position 
has tag    according to the gold standard.  In 
greedy 1-to-1 mapping, we find the largest val-
ue in the table ? suppose the value is for the cell 
     . We map state i to tag j, and remove both 
row i and column j from the table. We repeat 
the process until all the rows have been re-
moved. Greedy many-to-1 allow multiple hid-
den states to map to a single POS tag. That is, 
when the highest value in the table is found, 
only the corresponding row is removed. In other 
words, we simply map each hidden state to the 
POS tag that the hidden state co-occurs with the 
most.   
4 Results and Analysis 
We compare and analyze the results between 
the different algorithms and between Chinese 
and English using Greedy 1-to-1 accuracy, 
Greedy many-to-1 accuracy.  
4.1 Greedy 1-to-1 accuracy 
When measure using 1-to-1 mapping, the best  
algorithm ? Collapsed word based Gibbs Sam-
pling GS(c,w) - achieve 0.358 in Chinese on the 
full data set but remains close to 0.499 in Eng-
lish for the full dataset.  GS(c,w) outperforms 
other algorithm in almost all categories.  But 
EM posts the highest  relative improvement 
with an increase of 70% when the data size in-
creases  from  24k to 500k words.  The full re-
sult is listed in Table 2. 
 Greedy 1-to-1 
 24k 120k 500k 
C
h
in
es
e 
EM 0.1483 0.1838 0.2406 
VB 0.1925 0.2498 0.3105 
GS(e,w) 0.2167 0.3108 0.3475 
GS(e,s) 0.2262 0.2596 0.3572 
GS(c,s) 0.2351 0.2931 0.3577 
GS(c,w) 0.2932 0.3289 0.3558 
E
n
g
 EM 0.1862 0.2930 0.3837 
VB 0.2382 0.3468 0.4327 
GS(c,w) 0.3918 0.4276 0.4348 
Table 2: Tagging accuracy for Chinese and 
English with greedy 1-to-1 mapping.  The Eng-
lish 24k and 120k results are taken from (Gao & 
Johnson 2008) with the 50-tag set.  
 
 
Figure 2: Tag distribution for 1-to-1 greedy 
mapping in Chinese 500k.  Only the top 18 tags 
are shown.  The figure compares the tag distri-
bution between the gold standard for Chinese 
(33 tags) and the algorithm?s results.  The gold 
tags are shown as lines, and each algorithm?s 
result is shown as bar graphs.   
 
As expected, the increase in data size improves 
the accuracy as EM algorithm optimizes the 
likelihood better with more data.  We ran addi-
tional experiments on English using a reduced 
500k dataset to match the dataset used for Chi-
nese; EM in this setting achieve an accuracy of 
0.384 on average for 50 tags (down from 
0.405).  So even in the reduced data size setting, 
EM on English performs better than Chinese 
although the difference is reduced.  We analyze 
the tag distribution of the 1-to-1 mapping.  
(Johnson, 2007) finds that EM generally assigns 
roughly as equal number of words for each 
state.  In Figure 2, we find the same phenome-
non for Chinese.  
0
50000
100000
150000
EM
VB
GS(c,w)
Gold
140
   One of the advantages of Bayesian approaches 
(VB and GS) is that we can assign a prior to 
attempt to encourage a sparse model distribu-
tion.  Despite using small values 0.0001 as 
hyperparameters, we find that the resulting dis-
tribution for number of words mapping to a par-
ticular state is very different  from the gold 
standard. 
4.2 Greedy many-to-1 accuracy 
Collapsed Word Based Gibbs Sampler GS(c,w) 
is the clear winner for both English and Chinese 
unsupervised POS tagging.  Table 3 shows the 
result of Greedy many-to-1 mapping for Chi-
nese in different data size as well as English 
with the full data set.  In Greedy many-to-1 
mapping, GS(c,w) in both Chinese and English 
achieve 60%+ accuracy.  In addition, the size of 
the dataset does not affect GS(c,w) as much as 
the other algorithms.  In fact, the change from 
24k to 500k dataset only increases the relative 
accuracy by less than 6%.   
 
 Greedy many-to-1 
 24k 120k 500k 
C
h
in
es
e 
EM 0.4049 0.4564 0.4791 
VB 0.4411 0.5023 0.5390 
GS(e,w) 0.4758 0.4969 0.5499 
GS(e,s) 0.4904 0.5369 0.5658 
GS(c,s) 0.5070 0.5701 0.5757 
GS(c,w) 0.5874 0.6180 0.6213 
E
n
g
 EM 0.2828 0.44135 0.5872 
VB 0.3595 0.48427 0.6025 
GS(c,w) 0.5815 0.6529 0.6644 
Table  3: Many-to-1 accuracy for Chinese and 
English. The English 24k and 120k results are 
taken from (Gao & Johnson 2008) with the 50-
tag set.  
 
However, despite the relatively high  accuracy, 
when analyzing the result, we notice that there 
are overwhelmingly many states which maps to 
a single POS tag (NN).  Figure 3 shows the 
number of states mapping to different POS tags 
in Chinese over the 500k data size.  There are a 
large number of states mapping to relatively few 
POS tags.  In the most extreme example, for the 
POS tag NN, GS(e,s) assigns 18 (the most) hid-
den states, accounts for 44% of the word tokens 
mapping to NN whereas GS(e,w) assigns 13 
states, which is actually the least among all the 
algorithms and accounts for 31% of the word 
tokens mapping to NN.  Notice that we have 
only a total of 33 hidden states in our model.  
This means that over half the states are mapped 
to NN, which is a rather disappointing result.  
The actual empirical result for the gold standard 
in CTB is that only 27% of the word should be 
mapped to NN.  For EM in particular, we see 17 
states accounting for 42% of the words tagged 
as NN.  
 
 
Figure 3: The distribution of POS tags based on 
the output EM algorithm in Chinese using the  
500k dataset.  Tag T-N-y% means that there are 
N hidden states mapped to the specific POS tag 
T accounting for y% of word tokens tagged with 
these N states by the EM algorithm. 
 
 
Figure 4: English tag distribution for EM using 
500k dataset with 50 states mapping to the 17 
pos tag set. Tag T-N-y% means that there are N 
hidden states mapped to the specific POS tag T 
accounting for y% of word tokens tagged with 
these N states. 
 
We also ran additional experiments on the algo-
rithms for English using a reduced data size of 
500k to match that of our Chinese experiment to 
see whether we see the same phenomena.  We 
notice that the tag distribution for English EM is 
more consistent to the empirical distribution 
found in the gold standard. 
AD-3 
state(s)
9%
NN-17 
state(s)
42%
NR-2 
state(s)
8%
PU-3 
state(s)
14%
VV-3 
state(s)
10%
N-16 
state(s)
37%
DET-7 
state(s)
15%
V-7 
state(s)
12%
PREP-4 
state(s)
10%
ADJ-7 
state(s)
6%
141
With the English 50 tag set with 500k words, 
we experiment with mapping the English 50 tag 
set result to the 17 tag set, we see that in Figure 
4, 16 (of 50) states mapped to the N tag, ac-
counting for 37% of the words in the dataset.  
This is close to the actual empirical distribution 
for English for 17 tags where N accounts for 
about 32%. 
4.3 Convergence 
We analyze how each algorithm converges to its 
local maxima.  Figure 5 shows the change in 
greedy 1-to-1 accuracy over the 50% of the run.   
 
 
Figure 5: Greedy 1-to-1 accuracy of EM, VB 
and GS(c,w) over the first 50% of the algo-
rithms' iterations for the Chinese 500k dataset.  
Note: the percentage of iterations is used here 
because each algorithms converge at a different 
number of iterations, thus the progress is scaled 
accordingly. 
 
The greedy 1-to-1 accuracy actually fluctuates 
through the run.  VB has an interesting dip at 
around 80% of its iteration before climbing to 
its max (not showing in the graph).  All the 
Gibbs sampling variations follow a relatively 
steady hill climb before converging (only 
GS(c,w) is shown in Figure 5).  EM is particu-
larly interesting; Looking at the initial 15% of 
the algorithm?s run, we can see that EM climbs 
to a ?local? max very quickly before dropping 
and then slowly improving in its accuracy.  The 
greedy 1-to-1 accuracy in the initial top is ac-
tually higher than the final convergence value in 
most runs.  This initial peak in value following 
by a drop and then a slow hill climb in EM for 
Chinese POS tagging is consistent with the find-
ing in (Johnson, 2007) for English POS tagging.  
5 Conclusion and Future Work 
We have only scratched the surface of the re-
search in unsupervised techniques in Chinese 
NLP.  We have established a baseline of EM, 
VB and GS against the CTB 5.0.  The experi-
ment shows that for both Chinese and English, 
GS(c,w) produces the best result.  We have also 
found that Chinese performs rather poorly in the 
1-to-1 accuracy  when comparing against Eng-
lish in the same data size.  We find that in 
many-to-1 mapping, we have a disproportionate 
large number of states mapping to individual 
POS tags comparing to the gold distribution and 
also in comparison to English against its gold 
distribution.   
   Gra?a et al (2009) addresses the problem we 
observe in our resulting tag distributions in our 
model where EM, VB and GS fails to capture 
the shape of the true distribution.  They propose 
a Posterior Regularization framework where it 
poses linear constraints on the posterior expec-
tation.  They define a set distributions Q over 
hidden states with a constraint on the expecta-
tion over the features.  The log likelihood is pe-
nalized using the KL-divergence between the Q 
distribution and the model.  The distributions 
that their model predicted are far more similar 
to the gold standard than traditional EM.   
   Liang and Klein (2009) propose some inter-
esting error analysis techniques for unsuper-
vised POS tagging.  One of their analyses on 
EM is done by observing the approximation 
errors being created during each iteration of the 
algorithm?s execution.  We can also perform 
these analyses on VB and GS and observe the 
changes of output tags by starting from the Gold 
Standard distribution in EM and VB, and gold 
standard tags in GS.  We can then follow how 
and which set of tags start to deviate from the 
gold standard.  This will allow us to see which 
categories of errors (ex. noun-verb, adj-adv er-
rors) occur most in these algorithms and how 
the error progresses. 
 
Acknowledgment: This work is partly sup-
ported by the National Science Foundation 
Grant BCS-0748919. We would also like to 
thank three anonymous reviewers for their valu-
able comments.   
15%
20%
25%
30%
35%
0% 16% 32% 48%
% of Total Iterations
EM
VB
GS(c,w)
142
References 
Banko, M., & Moore, R. C. 2004. Part of Speech 
Tagging in Context. In Proc. of the 20th Interna-
tional Conference on Computational Linguistics 
(COLING), pp 556-561. 
Beal, M. 2003. Variational Algorithms for Approx-
imate Bayesian Inference. Ph.D. thesis, Gatsby-
Computational Neuroscience unit, University Col-
lege London. 
Besag, J. 2004. An introduction to Markov Chain 
Monte Carlo methods. Mathematical Foundations 
of Speech and Language Processing, pages 247?
270. Springer, New York. 
Chang, C.-H., & Chen, C.-D. 1993. HMM-Based 
Part-Of-Speech Tagging For Chinese Corpora. 
Workshop On Very Large Corpora: Academic 
And Industrial Perspectives.  
Elworthy, D. 1994. Does Baum-Welch Re-
estimation Help Taggers? In Proc. of Applied 
Natural Language Processing Conference 
(ANLP),  pp 53-58. 
Gao, J., & Johnson, M.  2008. A comparison of 
Bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proceedings of 
the 2008 Conference on Empirical Methods in 
Natural Language Processing (EMNLP), pp 344-
352. 
Geman, S., & Geman, D. 1984. Stochastic Relaxa-
tion, Gibbs Distributions, and the Bayesian Resto-
ration of Images. IEEE Transactions on Pattern 
Analysis and Machine Intelligence, pp 721?741. 
Goldwater, S., & Griffiths, T. 2007. A Fully Baye-
sian Approach to Unsupervised Part-of-Speech 
Tagging. Proceedings of the 45th Annual Meeting 
of the Association of Computational Linguistics 
(ACL) , pp 744-751. 
Graca, M.J., Ganchev, K., Taskar B. & Pereira, F. 
2009. Posterior vs. Parameter Sparsity in Latent 
Variable. Advances in Neural Information 
Processing Systems 22 (NIPS). MIT Press. 
Haghighi, A., & Klein, D. 2006. Prototype-driven 
learning for sequence models. In Proceedings of 
the Human Language Technology Conference  
(HLT- NAACL) , pp 320-327. 
Huang, Z., Eidelman, V., Harper, M.  2009. Improv-
ing A Simple Bigram HMM Part-of-Speech Tag-
ger by Latent Annotation and Self-Training. In 
Proc. of  Annual Conference of the North Ameri-
can Chapter of the Association for Computational 
Linguistics (NAACL), Companion Volume: Short 
Papers. 
Johnson, M. 2007. Why Doesn?t EM Find Good 
HMM POS-Taggers? In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL), pp 
296-305.  
Liang, P., & Klein, D. 2008. Analyzing the errors of 
unsupervised learning. The Forty Sixth Annual 
Meeting of the Association for Computational 
Linguistics (ACL), pp 879?887. Columbus, OH. 
MacKay, D. J., & Peto, L. C. 1994. A Hierarchical 
Dirichlet Language Model. Natural Language 
Engineering, 1-19. 
Manning, C. &  Schutze, H. 1999. Foundations of 
Statistical Natural Language Processing. The 
MIT Press, Cambridge, Massachusetts. 
Merialdo, B. 1994. Tagging English text with a 
probabilistic model. Computational Linguistics, 
20(2). 
Ng, H. T., & Low, J. K. 2004. Chinese Part-Of-
Speech Tagging: One-At-A-Time Or All-At-
Once? Word-Based Or Character-Based? In Proc. 
of  EMNLP.  
Thorsten Brants, 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Ap-
plied Natural Language Processing Conference 
(ANLP), Seattle, WA. 
Toutanova, K., & Johnson, M. 2007. A Bayesian 
LDA-based model for semi-supervised. In Pro-
ceedings of NIPS 21 . 
Wolfram Mathematica. (2009, 10 3). Random Num-
ber Generation. http://reference.wolfram.com/ 
mathematica/tutorial/RandomNumberGeneration 
.html . 
Xia, F. 2000. The Part-of-Speech Guidelines for the 
Penn Chinese Treebank (3.0). University  of 
Pennsylvania: IRCS Report 00-07. 
Xue, N., Chiou, F.-D., & Palmer, M. 2002. Building 
a Large-Scale Annotated Chinese Corpus. Pro-
ceedings of the 19th. International Conference on 
Computational Linguistics (COLING). Taipei, 
Taiwan. 
Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. 2005. 
The Penn Chinese TreeBank: Phrase Structure 
Annotation of a Large Corpus. Natural Language 
Engineering, 11(2), pp 207-238. 
 
 
 
143
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 355?362,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Domain Adaptation via Pseudo In-Domain Data Selection
Amittai Axelrod
University of Washington
Seattle, WA 98105
amittai@uw.edu
Xiaodong He
Microsoft Research
Redmond, WA 98052
xiaohe@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98052
jfgao@microsoft.com
Abstract
We explore efficient domain adaptation for the
task of statistical machine translation based
on extracting sentences from a large general-
domain parallel corpus that are most relevant
to the target domain. These sentences may
be selected with simple cross-entropy based
methods, of which we present three. As
these sentences are not themselves identical
to the in-domain data, we call them pseudo
in-domain subcorpora. These subcorpora ?
1% the size of the original ? can then used
to train small domain-adapted Statistical Ma-
chine Translation (SMT) systems which out-
perform systems trained on the entire corpus.
Performance is further improved when we use
these domain-adapted models in combination
with a true in-domain model. The results
show that more training data is not always
better, and that best results are attained via
proper domain-relevant data selection, as well
as combining in- and general-domain systems
during decoding.
1 Introduction
Statistical Machine Translation (SMT) system per-
formance is dependent on the quantity and quality
of available training data. The conventional wisdom
is that more data is better; the larger the training cor-
pus, the more accurate the model can be.
The trouble is that ? except for the few all-purpose
SMT systems ? there is never enough training data
that is directly relevant to the translation task at
hand. Even if there is no formal genre for the text
to be translated, any coherent translation task will
have its own argot, vocabulary or stylistic prefer-
ences, such that the corpus characteristics will nec-
essarily deviate from any all-encompassing model of
language. For this reason, one would prefer to use
more in-domain data for training. This would em-
pirically provide more accurate lexical probabilities,
and thus better target the task at hand. However, par-
allel in-domain data is usually hard to find1, and so
performance is assumed to be limited by the quan-
tity of domain-specific training data used to build the
model. Additional parallel data can be readily ac-
quired, but at the cost of specificity: either the data
is entirely unrelated to the task at hand, or the data is
from a broad enough pool of topics and styles, such
as the web, that any use this corpus may provide is
due to its size, and not its relevance.
The task of domain adaptation is to translate a text
in a particular (target) domain for which only a small
amount of training data is available, using an MT
system trained on a larger set of data that is not re-
stricted to the target domain. We call this larger set
of data a general-domain corpus, in lieu of the stan-
dard yet slightly misleading out-of-domain corpus,
to allow a large uncurated corpus to include some
text that may be relevant to the target domain.
Many existing domain adaptation methods fall
into two broad categories. Adaptation can be done at
the corpus level, by selecting, joining, or weighting
the datasets upon which the models (and by exten-
sion, systems) are trained. It can be also achieved at
the model level by combining multiple translation or
language models together, often in a weighted man-
ner. We explore both categories in this work.
1Unless one dreams of translating parliamentary speeches.
355
First, we present three methods for ranking the
sentences in a general-domain corpus with respect to
an in-domain corpus. A cutoff can then be applied to
produce a very small?yet useful? subcorpus, which
in turn can be used to train a domain-adapted MT
system. The first two data selection methods are ap-
plications of language-modeling techniques to MT
(one for the first time). The third method is novel
and explicitly takes into account the bilingual na-
ture of the MT training corpus. We show that it is
possible to use our data selection methods to subse-
lect less than 1% (or discard 99%) of a large general
training corpus and still increase translation perfor-
mance by nearly 2 BLEU points.
We then explore how best to use these selected
subcorpora. We test their combination with the in-
domain set, followed by examining the subcorpora
to see whether they are actually in-domain, out-of-
domain, or something in between. Based on this, we
compare translation model combination methods.
Finally, we show that these tiny translation mod-
els for model combination can improve system per-
formance even further over the current standard way
of producing a domain-adapted MT system. The re-
sulting process is lightweight, simple, and effective.
2 Related Work
2.1 Training Data Selection
An underlying assumption in domain adaptation is
that a general-domain corpus, if sufficiently broad,
likely includes some sentences that could fall within
the target domain and thus should be used for train-
ing. Equally, the general-domain corpus likely in-
cludes sentences that are so unlike the domain of the
task that using them to train the model is probably
more harmful than beneficial. One mechanism for
domain adaptation is thus to select only a portion of
the general-domain corpus, and use only that subset
to train a complete system.
The simplest instance of this problem can be
found in the realm of language modeling, using
perplexity-based selection methods. The sentences
in the general-domain corpus are scored by their per-
plexity score according to an in-domain language
model, and then sorted, with only the lowest ones
being retained. This has been done for language
modeling, including by Gao et al(2002), and more
recently by Moore and Lewis (2010). The ranking
of the sentences in a general-domain corpus accord-
ing to in-domain perplexity has also been applied to
machine translation by both Yasuda et al(2008), and
Foster et al(2010). We test this approach, with the
difference that we simply use the source side per-
plexity rather than computing the geometric mean
of the perplexities over both sides of the corpus. We
also reduce the size of the training corpus far more
aggressively than Yasuda et als 50%. Foster et al
(2010) do not mention what percentage of the cor-
pus they select for their IR-baseline, but they con-
catenate the data to their in-domain corpus and re-
port a decrease in performance. We both keep the
models separate and reduce their size.
A more general method is that of (Matsoukas et
al., 2009), who assign a (possibly-zero) weight to
each sentence in the large corpus and modify the em-
pirical phrase counts accordingly. Foster et al(2010)
further perform this on extracted phrase pairs, not
just sentences. While this soft decision is more flex-
ible than the binary decision that comes from includ-
ing or discarding a sentence from the subcorpus, it
does not reduce the size of the model and comes
at the cost of computational complexity as well as
the possibility of overfitting. Additionally, the most
effective features of (Matsoukas et al, 2009) were
found to be meta-information about the source doc-
uments, which may not be available.
Another perplexity-based approach is that taken
by Moore and Lewis (2010), where they use the
cross-entropy difference as a ranking function rather
than just cross-entropy. We apply this criterion for
the first time to the task of selecting training data
for machine translation systems. We furthermore ex-
tend this idea for MT-specific purposes.
2.2 Translation Model Combination
In addition to improving the performance of a sin-
gle general model with respect to a target domain,
there is significant interest in using two translation
models, one trained on a larger general-domain cor-
pus and the other on a smaller in-domain corpus, to
translate in-domain text. After all, if one has ac-
cess to an in-domain corpus with which to select
data from a general-domain corpus, then one might
as well use the in-domain data, too. The expectation
is that the larger general-domain model should dom-
356
inate in regions where the smaller in-domain model
lacks coverage due to sparse (or non-existent) ngram
counts. In practice, most practical systems also per-
form target-side language model adaptation (Eck et
al., 2004); we eschew this in order to isolate the ef-
fects of translation model adaptation alone.
Directly concatenating the phrase tables into one
larger one isn?t strongly motivated; identical phrase
pairs within the resulting table can lead to unpre-
dictable behavior during decoding. Nakov (2008)
handled identical phrase pairs by prioritizing the
source tables, however in our experience identical
entries in phrase tables are not very common when
comparing across domains. Foster and Kuhn (2007)
interpolated the in- and general-domain phrase ta-
bles together, assigning either linear or log-linear
weights to the entries in the tables before combining
overlapping entries; this is now standard practice.
Lastly, Koehn and Schroeder (2007) reported
improvements from using multiple decoding paths
(Birch et al, 2007) to pass both tables to the Moses
SMT decoder (Koehn et al, 2003), instead of di-
rectly combining the phrase tables to perform do-
main adaptation. In this work, we directly com-
pare the approaches of (Foster and Kuhn, 2007) and
(Koehn and Schroeder, 2007) on the systems gener-
ated from the methods mentioned in Section 2.1.
3 Experimental Framework
3.1 Corpora
We conducted our experiments on the Interna-
tional Workshop on Spoken Language Translation
(IWSLT) Chinese-to-English DIALOG task 2, con-
sisting of transcriptions of conversational speech in
a travel setting. Two corpora are needed for the
adaptation task. Our in-domain data consisted of the
IWSLT corpus of approximately 30,000 sentences
in Chinese and English. Our general-domain cor-
pus was 12 million parallel sentences comprising a
variety of publicly available datasets, web data, and
private translation texts. Both the in- and general-
domain corpora were identically segmented (in Chi-
nese) and tokenized (in English), but otherwise un-
processed. We evaluated our work on the 2008
IWSLT spontaneous speech Challenge Task3 test
2http://iwslt2010.fbk.eu/node/33
3Correct-Recognition Result (CRR) condition
set, consisting of 504 Chinese sentences with 7 En-
glish reference translations each. This is the most
recent IWSLT test set for which the reference trans-
lations are available.
3.2 System Description
In order to highlight the data selection work, we
used an out-of-the-box Moses framework using
GIZA++ (Och and Ney, 2003) and MERT (Och,
2003) to train and tune the machine translation sys-
tems. The only exception was the phrase table
for the large out-of-domain system trained on 12m
sentence pairs, which we trained on a cluster us-
ing a word-dependent HMM-based alignment (He,
2007). We used the Moses decoder to produce all
the system outputs, and scored them with the NIST
mt-eval31a 4 tool used in the IWSLT evalutation.
3.3 Language Models
Our work depends on the use of language models to
rank sentences in the training corpus, in addition to
their normal use during machine translation tuning
and decoding. We used the SRI Language Model-
ing Toolkit (Stolcke, 2002) was used for LM train-
ing in all cases: corpus selection, MT tuning, and
decoding. We constructed 4gram language mod-
els with interpolated modified Kneser-Ney discount-
ing (Chen and Goodman, 1998), and set the Good-
Turing threshold to 1 for trigrams.
3.4 Baseline System
The in-domain baseline consisted of a translation
system trained using Moses, as described above, on
the IWSLT corpus. The resulting model had a phrase
table with 515k entries. The general-domain base-
line was substantially larger, having been trained on
12 million sentence pairs, and had a phrase table
containing 1.5 billion entries. The BLEU scores of
the baseline single-corpus systems are in Table 1.
Corpus Phrases Dev Test
IWSLT 515k 45.43 37.17
General 1,478m 42.62 40.51
Table 1: Baseline translation results for in-domain and
general-domain systems.
4http://www.itl.nist.gov/iad/mig/tools/
357
4 Training Data Selection Methods
We present three techniques for ranking and select-
ing subsets of a general-domain corpus, with an eye
towards improving overall translation performance.
4.1 Data Selection using Cross-Entropy
As mentioned in Section 2.1, one established
method is to rank the sentences in the general-
domain corpus by their perplexity score accord-
ing to a language model trained on the small in-
domain corpus. This reduces the perplexity of the
general-domain corpus, with the expectation that
only sentences similar to the in-domain corpus will
remain. We apply the method to machine trans-
lation, even though perplexity reduction has been
shown to not correlate with translation performance
(Axelrod, 2006). For this work we follow the proce-
dure of Moore and Lewis (2010), which applies the
cosmetic change of using the cross-entropy rather
than perplexity.
The perplexity of some string s with empirical n-
gram distribution p given a language model q is:
2?
?
x p(x) log q(x) = 2H(p,q) (1)
where H(p, q) is the cross-entropy between p and
q. We simplify this notation to just HI(s), mean-
ing the cross-entropy of string s according to a lan-
guage model LMI which has distribution q. Se-
lecting the sentences with the lowest perplexity is
therefore equivalent to choosing the sentences with
the lowest cross-entropy according to the in-domain
language model. For this experiment, we used a lan-
guage model trained (using the parameters in Sec-
tion 3.3) on the Chinese side of the IWSLT corpus.
4.2 Data Selection using Cross-Entropy
Difference
Moore and Lewis (2010) also start with a language
model LMI over the in-domain corpus, but then fur-
ther construct a language modelLMO of similar size
over the general-domain corpus. They then rank the
general-domain corpus sentences using:
HI(s)?HO(s) (2)
and again taking the lowest-scoring sentences. This
criterion biases towards sentences that are both like
the in-domain corpus and unlike the average of the
general-domain corpus. For this experiment we re-
used the in-domain LM from the previous method,
and trained a second LM on a random subset of
35k sentences from the Chinese side of the general
corpus, except using the same vocabulary as the in-
domain LM.
4.3 Data Selection using Bilingual
Cross-Entropy Difference
In addition to using these two monolingual criteria
for MT data selection, we propose a new method
that takes in to account the bilingual nature of the
problem. To this end, we sum cross-entropy differ-
ence over each side of the corpus, both source and
target:
[HI?src(s)?HO?src(s)]+[HI?tgt(s)?HO?tgt(s)]
(3)
Again, lower scores are presumed to be better. This
approach reuses the source-side language models
from Section 4.2, but requires similarly-trained ones
over the English side. Again, the vocabulary of the
language model trained on a subset of the general-
domain corpus was restricted to only cover those
tokens found in the in-domain corpus, following
Moore and Lewis (2010).
5 Results of Training Data Selection
The baseline results show that a translation system
trained on the general-domain corpus outperforms a
system trained on the in-domain corpus by over 3
BLEU points. However, this can be improved fur-
ther. We used the three methods from Section 4 to
identify the best-scoring sentences in the general-
domain corpus.
We consider three methods for extracting domain-
targeted parallel data from a general corpus: source-
side cross-entropy (Cross-Ent), source-side cross-
entropy difference (Moore-Lewis) from (Moore and
Lewis, 2010), and bilingual cross-entropy difference
(bML), which is novel.
Regardless of method, the overall procedure is
the same. Using the scoring method, We rank the
individual sentences of the general-domain corpus,
select only the top N . We used the top N =
{35k, 70k, 150k} sentence pairs out of the 12 mil-
358
lion in the general corpus 5. The net effect is that of
domain adaptation via threshhold filtering. New MT
systems were then trained solely on these small sub-
corpora, and compared against the baseline model
trained on the entire 12m-sentence general-domain
corpus. Table 2 contains BLEU scores of the sys-
tems trained on subsets of the general corpus.
Method Sentences Dev Test
General 12m 42.62 40.51
Cross-Entropy 35k 39.77 40.66
Cross-Entropy 70k 40.61 42.19
Cross-Entropy 150k 42.73 41.65
Moore-Lewis 35k 36.86 40.08
Moore-Lewis 70k 40.33 39.07
Moore-Lewis 150k 41.40 40.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
Table 2: Translation results using only a subset of the
general-domain corpus.
All three methods presented for selecting a sub-
set of the general-domain corpus (Cross-Entropy,
Moore-Lewis, bilingual Moore-Lewis) could be
used to train a state-of-the-art machine transla-
tion system. The simplest method, using only the
source-side cross-entropy, was able to outperform
the general-domain model when selecting 150k out
of 12 million sentences. The other monolingual
method, source-side cross-entropy difference, was
able to perform nearly as well as the general-
domain model with only 35k sentences. The bilin-
gual Moore-Lewis method proposed in this paper
works best, consistently boosting performance by
1.8 BLEU while using less than 1% of the available
training data.
5.1 Pseudo In-Domain Data
The results in Table 2 show that all three meth-
ods (Cross-Entropy, Moore-Lewis, bilingual Moore-
Lewis) can extract subsets of the general-domain
corpus that are useful for the purposes of statistical
machine translation. It is tempting to describe these
as methods for finding in-domain data hidden in a
5Roughly 1x, 2x, and 4x the size of the in-domain corpus.
general-domain corpus. Alas, this does not seem to
be the case.
We trained a baseline language model on the in-
domain data and used it to compute the perplexity
of the same (in-domain) held-out dev set used to
tune the translation models. We extracted the top
N sentences using each ranking method, varying N
from 10k to 200k, and then trained language models
on these subcorpora. These were then used to also
compute the perplexity of the same held-out dev set,
shown below in Figure 1.
020406080100120140
'0
20
25
30
35
40
50
70
100
125
150
175
Top-r
anked
 
gener
al-dom
ain se
ntenc
es (in k
)
Devset Perplexity
In-dom
ain ba
seline
Cross
-
Entrop
y
Moore
-
Lewis
bilingu
al M-L
Figure 1: Corpus Selection Results
The perplexity of the dev set according to LMs
trained on the top-ranked sentences varied from 77
to 120, depending on the size of the subset and the
method used. The Cross-Entropy method was con-
sistently worse than the others, with a best perplex-
ity of 99.4 on 20k sentences, and bilingual Moore-
Lewis was consistently the best, with a lowest per-
plexity of 76.8. And yet, none of these scores are
anywhere near the perplexity of 36.96 according to
the LM trained only on in-domain data.
From this it can be deduced that the selection
methods are not finding data that is strictly in-
domain. Rather they are extracting pseudo in-
domain data which is relevant, but with a differing
distribution than the original in-domain corpus.
As further evidence, consider the results of con-
catenating the in-domain corpus with the best ex-
tracted subcorpora (using the bilingual Moore-
Lewis method), shown in Table 3. The change in
359
both the dev and test scores appears to reflect dissim-
ilarity in the underlying data. Were the two datasets
more alike, one would expect the models to rein-
force each other rather than cancel out.
Method Sentences Dev Test
IWSLT 30k 45.43 37.17
bilingual M-L 35k 39.59 42.31
bilingual M-L 70k 40.84 42.29
bilingual M-L 150k 42.64 42.22
IWSLT + bi M-L 35k 47.71 41.78
IWSLT + bi M-L 70k 47.80 42.30
IWSLT + bi M-L 150k 48.44 42.01
Table 3: Translation results concatenating the in-domain
and pseudo in-domain data to train a single model.
6 Translation Model Combination
Because the pseudo in-domain data should be kept
separate from the in-domain data, one must train
multiple translation models in order to advanta-
geously use the general-domain corpus. We now ex-
amine how best to combine these models.
6.1 Linear Interpolation
A common approach to managing multiple transla-
tion models is to interpolate them, as in (Foster and
Kuhn, 2007) and (Lu? et al, 2007). We tested the
linear interpolation of the in- and general-domain
translation models as follows: Given one model
which assigns the probability P1(t|s) to the trans-
lation of source string s into target string t, and a
second model which assigns the probability P2(t|s)
to the same event, then the interpolated translation
probability is:
P (t|s) = ?P1(t|s) + (1? ?)P2(t|s) (4)
Here ? is a tunable weight between 0 and 1, which
we tested in increments of 0.1. Linear interpolation
of phrase tables was shown to improve performance
over the individual models, but this still may not be
the most effective use of the translation models.
6.2 Multiple Models
We next tested the approach in (Koehn and
Schroeder, 2007), passing the two phrase tables di-
rectly to the decoder and tuning a system using both
phrase tables in parallel. Each phrase table receives
a separate set of weights during tuning, thus this
combined translation model has more parameters
than a normal single-table system.
Unlike (Nakov, 2008), we explicitly did not at-
tempt to resolve any overlap between the phrase ta-
bles, as there is no need to do so with the multiple
decoding paths. Any phrase pairs appearing in both
models will be treated separately by the decoder.
However, the exact overlap between the phrase ta-
bles was tiny, minimizing this effect.
6.3 Translation Model Combination Results
Table 4 shows baseline results for the in-domain
translation system and the general-domain system,
evaluated on the in-domain data. The table also
shows that linearly interpolating the translation
models improved the overall BLEU score, as ex-
pected. However, using multiple decoding paths,
and no explicit model merging at all, produced even
better results, by 2 BLEU points over the best indi-
vidual model and 1.3 BLEU over the best interpo-
lated model, which used ? = 0.9.
System Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
Interpolate IWSLT, General 48.46 41.28
Use both IWSLT, General 49.13 42.50
Table 4: Translation model combination results
We conclude that it can be more effective to not
attempt translation model adaptation directly, and
instead let the decoder do the work.
7 Combining Multi-Model and Data
Selection Approaches
We presented in Section 5 several methods to im-
prove the performance of a single general-domain
translation system by restricting its training corpus
on an information-theoretic basis to a very small
number of sentences. However, Section 6.3 shows
that using two translation models over all the avail-
able data (one in-domain, one general-domain) out-
performs any single individual translation model so
far, albeit only slightly.
360
Method Dev Test
IWSLT 45.43 37.17
General 42.62 40.51
both IWSLT, General 49.13 42.50
IWSLT, Moore-Lewis 35k 48.51 40.38
IWSLT, Moore-Lewis 70k 49.65 40.45
IWSLT, Moore-Lewis 150k 49.50 41.40
IWSLT, bi M-L 35k 48.85 39.82
IWSLT, bi M-L 70k 49.10 43.00
IWSLT, bi M-L 150k 49.80 43.23
Table 5: Translation results from using in-domain and
pseudo in-domain translation models together.
It is well and good to use the in-domain data
to select pseudo in-domain data from the general-
domain corpus, but given that this requires access
to an in-domain corpus, one might as well use it.
As such, we used the in-domain translation model
alongside translation models trained on the subcor-
pora selected using the Moore-Lewis and bilingual
Moore-Lewis methods in Section 4. The results are
in Table 5.
A translation system trained on a pseudo in-
domain subset of the general corpus, selected with
the bilingual Moore-Lewis method, can be further
improved by combining with an in-domain model.
Furthermore, this system combination works better
than the conventional multi-model approach by up
to 0.7 BLEU on both the dev and test sets.
Thus a domain-adapted system comprising two
phrase tables trained on a total of 180k sen-
tences outperformed the standard multi-model sys-
tem which was trained on 12 million sentences. This
tiny combined system was also 3+ points better than
the general-domain system by itself, and 6+ points
better than the in-domain system alone.
8 Conclusions
Sentence pairs from a general-domain corpus that
seem similar to an in-domain corpus may not actu-
ally represent the same distribution of language, as
measured by language model perplexity. Nonethe-
less, we have shown that relatively tiny amounts of
this pseudo in-domain data can prove more useful
than the entire general-domain corpus for the pur-
poses of domain-targeted translation tasks.
This paper has also explored three simple yet
effective methods for extracting these pseudo in-
domain sentences from a general-domain corpus. A
translation model trained on any of these subcorpora
can be comparable ? or substantially better ? than a
translation system trained on the entire corpus.
In particular, the new bilingual Moore-Lewis
method, which is specifically tailored to the ma-
chine translation scenario, is shown to be more ef-
ficient and stable for MT domain adaptation. Trans-
lation models trained on data selected in this way
consistently outperformed the general-domain base-
line while using as few as 35k out of 12 million sen-
tences. This fast and simple technique for discarding
over 99% of the general-domain training corpus re-
sulted in an increase of 1.8 BLEU points.
We have also shown in passing that the linear in-
terpolation of translation models may work less well
for translation model adaptation than the multiple
paths decoding technique of (Birch et al, 2007).
These approaches of data selection and model com-
bination can be stacked, resulting in a compact, two
phrase-table, translation system trained on 1% of the
available data that again outperforms a state-of-the-
art translation system trained on all the data.
Besides improving translation performance, this
work also provides a way to mine very large corpora
in a computationally-limited environment, such as
on an ordinary computer or perhaps a mobile device.
The maximum size of a useful general-domain cor-
pus is now limited only by the availability of data,
rather than by how large a translation model can be
fit into memory at once.
References
Amittai Axelrod. 2006. Factored Language Models for
Statistical Machine Translation. M.Sc. Thesis. Univer-
sity of Edinburgh, Scotland.
Alexandra Birch, Miles Osborne and Philipp Koehn.
2007. CCG Supertags in Factored Translation Models.
Workshop on Statistical Machine Translation, Associ-
ation for Computational Linguistics.
Stanley Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
Modeling. Technical Report 10-98, Computer Science
Group, Harvard University.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
361
Translation based on Information Retrieval. Language
Resources and Evaluation.
George Foster and Roland Kuhn. 2007. Mixture-Model
Adaptation for SMT. Workshop on Statistical Machine
Translation, Association for Computational Linguis-
tics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instatnce Weighting for Domain Adap-
tation in Statistical Machine Translation. Empirical
Methods in Natural Language Processing.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu
Lee. 2002. Toward a Unified Approach to Statistical
Language Modeling for Chinese. ACM Transactions
on Asian Language Information Processing.
Xiaodong He. 2007. Using Word-Dependent Transition
Models in HMM-based Word Alignment for Statisti-
cal Machine Translation. Workshop on Statistical Ma-
chine Translation, Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2003. Moses: Open Source
Toolkit for Statistical Machine Translation. Demo Ses-
sion, Association for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. Workshop on Statistical Machine Translation,
Association for Computational Linguistics.
Yajuan Lu?, Jin Huang and Qun Liu. 2007. Improving
Statistical Machine Translation Performance by Train-
ing Data Selection and Optimization. Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning.
Spyros Matsoukas, Antti-Veikko Rosti, Bing Zhang.
2009. Discriminative Corpus Weight Estimation for
Machine Translation. Empirical Methods in Natural
Language Processing.
Robert Moore and William Lewis. 2010. Intelligent Se-
lection of Language Model Training Data. Association
for Computational Linguistics.
Preslav Nakov. 2008. Improving English-Spanish Sta-
tistical Machine Translation: Experiments in Domain
Adaptation, Sentence Paraphrasing, Tokenization, and
Recasing. Workshop on Statistical Machine Transla-
tion, Association for Computational Linguistics.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. Association for Compu-
tational Linguistics
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. Spoken Language Process-
ing.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, Ei-
ichiro Sumita. 2008. Method of Selecting Train-
ing Data to Build a Compact and Efficient Transla-
tion Model. International Joint Conference on Natural
Language Processing.
362
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 609?618, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Unified Approach to Transliteration-based Text Input                     
with Online Spelling Correction 
 
Hisami Suzuki              Jianfeng Gao 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,jfgao}@microsoft.com 
 
Abstract 
This paper presents an integrated, end-to-end 
approach to online spelling correction for text 
input. Online spelling correction refers to the 
spelling correction as you type, as opposed to 
post-editing. The online scenario is 
particularly important for languages that 
routinely use transliteration-based text input 
methods, such as Chinese and Japanese, 
because the desired target characters cannot 
be input at all unless they are in the list of 
candidates provided by an input method, and 
spelling errors prevent them from appearing 
in the list. For example, a user might type 
suesheng by mistake to mean xuesheng ?? 
'student' in Chinese; existing input methods 
fail to convert this misspelled input to the 
desired target Chinese characters. In this 
paper, we propose a unified approach to the 
problem of spelling correction and 
transliteration-based character conversion 
using an approach inspired by the phrase-
based statistical machine translation 
framework. At the phrase (substring) level, k 
most probable pinyin (Romanized Chinese) 
corrections are generated using a monotone 
decoder; at the sentence level, input pinyin 
strings are directly transliterated into target 
Chinese characters by a decoder using a log-
linear model that refer to the features of both 
levels. A new method of automatically 
deriving parallel training data from user 
keystroke logs is also presented. Experiments 
on Chinese pinyin conversion show that our 
integrated method reduces the character error 
rate by 20% (from 8.9% to 7.12%) over the 
previous state-of-the art based on a noisy 
channel model.  
1 Introduction 
This paper addresses the problem of online 
spelling correction, which tries to correct users' 
misspellings as they type, rather than post-editing 
them after they have already been input. This 
online scenario is particularly important for 
languages that routinely use transliteration-based 
text input methods, including Chinese and 
Japanese: in these languages, characters (called 
hanzi in Chinese and kanji/kana in Japanese) are 
typically input by typing how they are pronounced 
in Roman alphabet (called pinyin in Chinese, 
romaji in Japanese), and selecting a conversion 
candidate among those that are offered by an input 
method system, often referred to as IMEs or input 
method editors. One big challenge posed by 
spelling mistakes is that they prevent the desired 
candidates from appearing as conversion 
candidates, as in Figure 1: suesheng is likely to be 
a spelling error of xuesheng?? 'student', but it is 
not included as one of the candidates.  
                  
Figure 1: Spelling mistake prevents the desired output 
(??) from appearing in the list of candidates 
This severely limits the utility of an IME, as 
spelling errors are extremely common. Speakers of 
a non-standard dialect and non-native speakers 
have a particularly hard time, because they may 
not know the standard pronunciation of the word to 
begin with, preventing them from inputting the 
word altogether. Error-tolerant word completion 
and next word prediction are also highly desirable 
features for text input on software (onscreen) 
keyboards for any language, making the current 
work relevant beyond Chinese and Japanese.  
In this paper, we propose a novel, unified 
system of text input with spelling correction, using 
609
Chinese pinyin-to-hanzi conversion as an example. 
We first formulate the task of pinyin spelling 
correction as a substring-based monotone 
translation problem, inspired by phrase-based 
statistical machine translation (SMT) systems 
(Koehn et al2003; Och and Ney, 2004): we 
consider the pinyin input (potentially with errors) 
as the source language and the error-corrected 
pinyin as the target, and build a log-linear model 
for spelling correction. In doing so, we also 
propose a novel, unsupervised method of 
collecting parallel training data from user input 
logs. We then build an integrated end-to-end text 
input system that directly converts a potentially 
erroneous input pinyin sequence into a desired 
hanzi sequence, also formulated as a monotone 
phrase-based SMT problem, in which the feature 
functions of the substring-based error correction 
component are integrated and jointly optimized 
with the sentence-level feature functions for 
character conversion  
Our method generalizes and improves over the 
previous state-of-the-art methods for the task of 
error correction and text input in several crucial 
respects. First, our error correction model is 
designed and implemented as a substring-based, 
fully trainable system based on a log-linear model, 
which has been shown effective for related tasks 
such as transliteration and letter-to-phone 
conversion, but has not been attempted for the task 
of spelling correction. Second, we build an end-to-
end pinyin-to-hanzi conversion system by 
combining all the feature functions used in the 
error correction and character conversion 
components in an SMT-style log-linear model, 
where the feature weights are trained 
discriminatively for the end-to-end task. This 
integration method generalizes the previous 
approach based on a noisy channel model (Chen 
and Lee, 2000; Zheng et al011b), in which only 
the error model and the conversion model 
probabilities are used and combined with equal 
weights. Finally, like other statistical systems, the 
amount and quality of training data control the 
quality of the outcome; we thus propose a new, 
language-independent method of deriving parallel 
data for spelling correction from user keystroke 
logs.  
We performed experiments on various methods 
of integrating the error correction and character 
conversion sub-components. Our best system, a 
fully integrated SMT-based approach, reduces the 
character error rate by 35% on test data that is 
completely independent of the creation of error 
correction and character conversion models.  
In what follows, we first give the background of 
this research in Section 2. We then describe our 
approach to the spelling correction task (Section 3) 
and the end-to-end conversion task (Section 4). We 
summarize our contribution and conclude with 
remarks for future directions in Section 5.  
2 Related Work  
The current work builds on many previous works 
on the task of monotone substring-based 
transduction, including spelling correction, letter-
to-phone conversion and transliteration between 
different scripts. In particular, our substring-based 
approach to spelling correction is motivated by the 
success on transliteration (e.g., Sherif and Kondrak, 
2007; Cherry and Suzuki, 2009) and letter-to-
phoneme conversion (e.g., Jiampojamarn et al
2007; Rama et al2009). One big challenge of the 
spelling correction research is the general lack of 
naturally occurring paired data of contextual 
spelling errors and their correction. Previous work 
has therefore either focused on the task of 
correcting out-of-vocabulary words out of context 
(e.g., Brill and Moore, 2000; Toutanova and 
Moore, 2002), or has resorted to innovative 
methods of data collection. For example, Banko 
and Brill (2001) generate data artificially by 
substituting words from a confusion word set in 
text for building a contextual speller; Whitelaw et 
al. (2009) use word frequency and edit distance 
information to harvest error pairs from a web 
corpus in an unsupervised manner; Bertoldi et al
(2010) intentionally corrupt clean text by adding 
noise to the data. Another approach to spelling 
error data collection uses web search query logs, 
available in large quantity (albeit to limited 
institutions), and limit its focus on the task of 
correcting misspelled queries (e.g., Cucerzan and 
Brill, 2004; Gao et al2010; Sun et al2010; 
Duan and Hsu, 2011). The problem of data 
collection is particularly difficult for pinyin error 
correction, as pinyin is not a final form of text in 
Chinese, so it is not recorded in final text. Zheng et 
al. (2011a) study a log of pinyin input method and 
use the backspace key to learn the user mistyping 
behavior, but they do so only for the purpose of 
610
data analysis, and do not build a statistical model 
from this data.  
Text input methods have been commercially 
available for decades for inputting Chinese and 
Japanese, but have also recently become available 
for other non-Roman script languages including 
Arabic and the languages of India.1 Early research 
work on text input methods includes e.g., Mori et 
al. (1998), Chen and Lee (2000) and Gao et al
(2002), all of which approach the problem using a 
noisy channel model. Discriminative approaches 
have also been proposed, e.g., Suzuki and Gao 
(2005); Tokunaga et al2011). There is only a 
very limited amount of work that deals with 
spelling correction in the context of text input: 
Zheng et al2011b) represents a recent work 
based on a noisy channel model, which defines our 
baseline. Their work is strictly word-based and 
only handles the correction of out-of-vocabulary 
pinyin words into in-vocabulary pinyin words, 
while our substring-based model is not limited by 
these constraints.  
The current work also has an affinity to the task 
of speech translation in that the parallel data 
between the input (speech signal) and the output 
(text in foreign language) is not directly available, 
but is mediated by a corrected (transcribed) form 
of input. Zhang et al2011) is thus relevant to our 
study, though their approach differs from ours in 
that we build an integrated system that include the 
feature functions of both error correction and 
character conversion sub-systems.  
3 Substring-based Spelling Correction 
using a Log-linear Model 
In this section, we describe our approach to pinyin 
error correction within a log-linear framework. 
Though our current target is pinyin error correction, 
the method described in this section is applicable 
to any language of interest.  
The spelling correction problem has been 
standardly formulated within the framework of 
noisy channel model (e.g., Kernighan et al1990). 
Let A be the input phonetic string in pinyin. The 
task of spelling correction is to search for the best 
                                                          
1 A few examples include Google Transliterate 
(http://www.google.com/transliterate/) and Microsoft Maren 
(http://www.microsoft.com/middleeast/egypt/cmic/maren/) / 
ILIT (http://specials.msn.co.in/ilit/Hindi.aspx). Quillpad 
(http://quillpad.in/) is also popularly used in India. 
correction candidate in pinyin C* among all 
possible corrections for each potentially misspelled 
pinyin A: 
         
        
   |                                               
Applying Bayes' Rule and dropping the constant 
denominator, we have 
         
        
   |                                         
where the error model    |  models the 
translation probability from C to A, and the 
language model      models how likely the 
output C is a correctly spelled pinyin sequence. 
Many variations on the error model have been 
proposed, including substring-based (Brill and 
Moore, 2000) and pronunciation-based (Toutanova 
and Moore, 2002) models.  
Our model is inspired by the SMT framework, 
in which the error correction probability    |   of 
Equation (1) is directly modeled using a log-linear 
model of the following form:   
   |   
 
    
   ?                               
 
 
where Z(A) is the normalization factor, hi is a 
feature function and ?i is the feature weight. 
Similarly to phrase-based SMT, many feature 
functions are derived from the translation and 
language models, where the translation model-
derived features are trained using a parallel corpus 
of original pinyin and correction pairs. The argmax 
of Equation (1) defines the search operation: we 
use a left-to-right beam search decoder to seek for 
each input pinyin the best correction according to 
Equation (3).   
We first describe how the paired data for 
deriving the error model probabilities is generated 
from user logs in Section 3.1, and then how the 
models are trained and the model weights are 
learned in Section 3.2. We discuss the results of 
pinyin error correction as an independent task in 
Section 3.3.  
3.1 Generating error correction pairs from 
keystroke logs 
Unlike English text, which includes instances of 
misspelled words explicitly, pinyin spelling errors 
are not found in a corpus, because pinyin is used as 
a means of inputting text, and is not part of the 
611
final written form of the language. Therefore, 
pinyin error correction pairs must be created 
intentionally. We chose the method of 
implementing a version of an input method which 
records the keystrokes of users while they are 
asked to type a particular Chinese text in hanzi; in 
doing so, we captured each keystroke issued by the 
user behind the scene. Such keystroke logs include 
the use of the backspace key, from which we 
compute the pinyin strings after the usage of the 
backspace keys as well as the putative pinyin string 
had the user not corrected it using the backspace 
key.2 Table 1 shows a few examples of the entries 
in the keystroke log, along with the computed 
pinyin strings before and after correction. Each 
entry (or phrase) in the log represents the unit that 
corresponds to the sequence the user input at once, 
at the end of which the user committed to a 
conversion candidate, which typically consists of 
one or more words. While the post-correction 
string can be straightforwardly derived by deleting 
the same number of characters preceding the 
backspaces, the computation of the pre-correction 
string is trickier and ambiguous, because the 
backspace key is used for the purpose of both 
deletion and substitution (delete and replace) 
operations. In Table 1, a backspace usage is 
indicated by _ in the original keystroke sequence 
that is logged. In the second example, a deletion 
interpretation will generate zhonguo as a pre-
correction string, while substitution interpretation 
will generate zhonguoo. In order to recover the 
desired pre-correcting string, we compared the 
prefix of the backspace usage (zhonguo) with the 
substrings after error correction (zhong, zhongg, 
zhonggu?). We considered that the prefix was 
spell-corrected into the substring which is the 
longest and with the smallest edit distance: in this 
case, zhonguo is considered an error for 
zhongguo, therefore recovering the pre-correction 
string of the whole sequence as zhonguo. Note 
that this method of error data extraction is general 
                                                          
2 Zheng et al2011a) also uses the backspace key in the IME 
log to generate error-correction pairs, but they focus on the 
usage of a backspace after the desired hanzi characters have 
been input, i.e., the backspace key is used to delete one or 
more hanzi characters. In contrast, our method focuses on the 
use of backspace to delete one or more pinyin characters 
before conversion. This simulates the scenario of online error 
correction more truthfully, and can collect paired data in large 
quantity faster.  
and is language-independent. Since paired error 
correction data do not exist naturally and is 
expensive to collect for any language, we believe 
that the proposed method is useful beyond the case 
of Chinese text input and applicable to the data 
collection of the spelling correction task in general. 
In a related work (Baba and Suzuki, 2012), we 
collected such keystroke data using Amazon's 
Mechanical Turk for English and Japanese, and 
released the error-correction pairs for research 
purposes.3  
The extracted pairs are still quite noisy, because 
one error correction behavior might not completely 
eliminate the errors in typing a word. For example, 
in trying to type women ?? 'we', a user might 
first type wmen, hit the backspaces key four times, 
retype womeen, and commit to a conversion 
candidate by mistake. We extract the pair (wmen, 
womeen) from this log incorrectly, which is one of 
the causes of the noise in the data. Despite these 
remaining errors, we use the data without further 
cleaning, as we expect our approach to be robust 
against a certain amount of noise.  
Keystroke data was collected for three text 
domains (chat, blog and online forum) from 60 
users, resulting in 86,783 pairs after removing 
duplicates. The data includes the pairs with the 
same source and target, with about 41% 
representing the case of correction. We used 5,000 
pairs for testing, 1,000 pairs for tuning the log-
linear model weights (see the next subsection), and 
the remaining portion for training the error 
correction component.  
3.2 Training the log-linear model 
The translation model captures substring-based 
spelling error patterns and their transformation 
probabilities. The model is learned from large 
amounts of pinyin-correction pairs mined from 
user keystroke logs discussed above. Take the 
                                                          
3 Available at http://research.microsoft.com/en-
us/downloads/4eb8d4a0-9c4e-4891-8846-
7437d9dbd869/default.aspx.  
keystroke pre-
correction 
post-
correction 
n a n s _ r e n nansen nanren 
z h o n g u o _ _ g u o zhonguo 
(*zhonguoo) 
zhongguo 
Table 1: Computation of pre- and post-correction 
strings from keystroke log 
612
following pinyin-correction pair as an example, 
where the input pinyin and its correction are 
aligned at the character level: given a pair (A,C), 
we align the letters in A with those in C so as to 
minimize the edit distance between A and C based 
on single character insertions, deletions and 
substitutions. 
 
From this pair, we learn a set of error patterns that 
are consistent with the character alignment,4 each 
of which is a pair of substrings indicating how the 
spelling is transformed from one to another. Some 
examples of extracted phrases are (wanmian, 
waimian) and (andshi, andeshi). In our 
implementation, we extract all patterns with a 
substring length of up to 9 characters. We then 
learn the translation probabilities for each pair 
using maximum likelihood estimation (MLE). Let 
(a,c) denote a pair. For each pair, we learn the 
translation probabilities P(c|a) and P(a|c), 
estimated using MLE, as well as lexical weights in 
two directions following Koehn et al2003).  Our 
error correction model is completely substring-
based and does not use a word-based lexicon, 
which gives us the flexibility of generating unseen 
correction targets as well as supporting pinyin 
input consisting of multiple words at a time. For 
the language model, we use a character 9-gram 
model to capture the knowledge of correctly 
spelled pinyin words and phrases. We trained the 
language model using the target portion of the 
parallel data described in Section 3.1, though it is 
possible to train it with an arbitrary text in pinyin 
when such data is available.  
In addition to the feature functions derived from 
the error and language models, we also use word 
and phrase penalties as feature functions, which are 
commonly used in SMT. These features also make 
sense in the current context, as using fewer phrase 
means encouraging longer ones with more context, 
and the target character length can capture 
tendencies to delete or insert words in errors. 
                                                          
4 Consistency here implies two things. First, there must be at 
least one aligned character pair in the aligned phrase. Second, 
there must not be any alignments from characters inside the 
aligned phrase to characters outside the phrase. That is, we do 
not extract a phrase pair if there is an alignment from within 
the phrase pair to outside the phrase pair. 
Overall, the log-linear model uses 7 feature 
functions: 4 derived from the translation models, 
word and phrase penalties, and the language model. 
The model weights were trained using the 
minimum error rate training algorithm (MERT, 
Och, 2003). We tried MERT with two objective 
functions: one that uses the 4-gram BLEU score as 
straightforwardly adapted from SMT, and the other 
that minimizes the character error rate (CER). CER 
is based on the edit distance between the reference 
and system output, which is used for evaluating the 
IME accuracy (Section 4.3). It is more directly 
related with the word/phrase-level accuracy, which 
we used to evaluate the error correction module in 
isolation, than the BLEU metric. As we will show 
below, however, using different objective 
functions turned out to have only a minimal impact 
on the spelling correction accuracy.  
3.3 Experiments and results 
The performance of pinyin error correction was 
evaluated on two data sets: (1) log-test: the test set 
of the data in Section 3.1, which is derived in the 
same way as the training data but is noisy, 
consisting of 5,000 phrases of which 2,020 are 
misspelled; (2) CHIME: the gold standard from the 
CHIME data set made available by Zheng et al
(2011b), 5  which is also used in the end-to-end 
evaluation in Section 4. This data set consists of 
2,000 sentence pairs of pinyin input with errors 
and the target hanzi characters, constructed by 
collecting actual user typing logs of the Lancaster 
corpus (McEnery and Xiao, 2004), which includes 
text from newspaper, fiction, and essays. 6  The 
CHIME data set does not include the corrected 
pinyin string; we therefore generated this by 
running a text-to-pinyin utility, 7  and created the 
pairs before and after error correction for 
evaluating our pinyin spelling correction module. 
The set contains 11,968 words of which 908 are 
misspelled. 
The results of the evaluation are given in Table 
2. They are for phrase/word-level accuracy, as the 
log-derived data set is for each phrase (a user-
                                                          
5 Available from http://chime.ics.uci.edu/ 
6 Details on the Lancaster corpus are found at 
http://www.lancs.ac.uk/fass/projects/corpus/LCMC/.  
7 We used an in-house tool, but many tools are available 
online. Unlike pinyin-to-hanzi, hanzi-to-pinyin is relatively 
straightforward as most characters have a unique 
pronunciation. 
613
defined unit of conversion, consisting of one to a 
few words), while the CHIME data set is word-
segmented. The baseline accuracy is the accuracy 
of not correcting any error, which is very strong in 
this task: 59.6% and 92.41% for the two data sets, 
respectively. The accuracy on the log-test data is 
generally much lower than the CHIME data, 
presumably because the latter is cleaner, contains 
less errors to begin with, and the unit of evaluation 
is smaller (word) than the log-test (phrase). 
Though CHIME is an out-of-domain data set, the 
proposed model works very well on this set, 
achieving more than 93% accuracy with the best 
output, significantly (at p<0.001 using McNemar's 
test) improving on the strong baseline of not 
correcting any error. The proposed log-linear 
approach is also compared against the noisy 
channel model baseline, which is simulated by 
only using one error model-derived feature 
function    |   and the language model, weighted 
equally, using the same beam search decoder. 
Somewhat surprisingly, the noisy channel model 
results fall below the baseline in both data sets, 
while the log-linear model improves over the 
baseline, especially on the 1-best accuracy: all 
differences between the noisy channel model and 
the log-linear model outputs are significant. Finally, 
regarding the effect of using the CER as the 
objective function of MERT, we only observe 
minimal impact: none of the differences in 
accuracy between the BLEU and CER objectives is 
statistically significant on either data set. For a 
monotone decoding task such as spelling 
correction, using either objective function therefore 
seems to suffice, even though BLEU is more 
indirect and redundant in capturing the phrase-
level accuracy.   
4 A Unified Model of Character 
Conversion with Spelling Correction  
In this section we describe our unified model of 
spelling correction and transliteration-based 
character conversion. Analogous to the spelling 
correction task, the character conversion problem 
can also be considered as a substring-based 
translation problem. The novelty of our approach 
lies in the fact that we take advantage of the 
parallelism between these tasks, and build an 
integrated model that performs spelling correction 
and character conversion at the same time, within 
the log-linear framework. This allows us to 
optimize the feature weights directly for the end 
goal, from which from we can expect a better 
overall conversion accuracy.  
4.1 Noisy channel model approach to 
incorporating error correction in 
character conversion 
The task of pinyin-to-hanzi conversion consists of 
converting the input phonetic strings provided by 
the user into the appropriate word string using 
ideographic characters. This has been formulated 
within the noisy channel model (Chen and Lee, 
2000), in exactly the same manner as the spelling 
correction, as describe in Equations (1) and (2) in 
Section 3. Given the pinyin input A, the task is to 
find the best output hanzi sequence W*: 
  
       
        
   |                                                     
       
        
       |   
In traditional conversion systems which do not 
consider spelling errors, P(A|W) is usually set to 1 
if the word is found in a dictionary of word-
pronunciation pairs, which also defines GEN(A). 
Therefore, the ranking of the candidates relies 
exclusively on the language model probability 
P(W).  
An extension of this formulation to handle 
spelling errors can be achieved by incorporating an 
actual error model P(A|W). Assuming a conditional 
independence of A and W given the error-corrected 
pinyin sequence C, Equation (4) can be re-written 
as: 
 1-best 3-best 20-best 
log-test: No correction  59.6   
log-test: Noisy Channel 49.5 67.86 84.8 
log-test: Proposed (BLEU) 62.46 74.58 86.66 
log-test: Proposed (CER) 62.82 75.06 86.8 
CHIME: No correction 92.41   
CHIME: Noisy Channel 91.29 95.75 98.82 
CHIME: Proposed (BLEU) 93.51 97.38 99.06 
CHIME: Proposed (CER) 93.49 97.29 99.08 
Table 2: Pinyin error correction accuracy (in %) 
614
         
 
   |  
       
 
?   |     |                                      
 
       
 
?   |         |  
 
 
Here, P(C|W) corresponds to the channel model of 
traditional input methods, P(W) the language 
model, and P(C|A) the pinyin error correction 
model. There have been attempts to use this 
formulation in text input: for example, Chen and 
Lee (2000) trained a syllable-based model for 
P(C|A) with user keystroke data,8 and Zheng et al
(2011b) used a model based on a weighted 
character edit distance whose weights are manually 
assigned. This noisy channel integration of error 
correction and character conversion is the state-of-
the-art in the task of error-correcting text input, 
and will serve as our baseline.  
4.2 Log-linear model for error-correcting 
character conversion 
 Similar to the formulation of our error correction 
model in Section 3, we adopt the log-linear model 
for modeling the character conversion probability 
in (4):  
   |   
 
    
   ?         
 
 
where A = a1,?,an is a sequence of phrases in 
pinyin, and W = w1,?,wn is the corresponding 
sequence in hanzi. A unique challenge of the 
current task is that the parallel data for A and W do 
not exist directly. Therefore, we generated the 
translation phrase table offline by merging the 
                                                          
8 No detail of this data is available in Chen and Lee (2000).  
substring-based phrase table generated for the 
pinyin error correction task in Section 3 with the 
results of character conversion. This process is 
described in detail in Figure 2: k-best candidates 
for each input pinyin phrase a are generated by the 
error model in Section 3, which are then submitted 
offline to an IME system to obtain n-best 
conversion candidates with probabilities. For the 
IME system, we used an in-house conversion 
system, which only uses a word trigram language 
model for ranking. In the resulting translation table, 
defined for each (a, w) pair, the feature functions 
and their values are inherited from the pinyin error 
correction translation table mediated by the 
correction candidates c1?k for a, plus the function 
that defines the IME conversion probability for (cj, 
w). Note that in this final phrase table, the 
correction candidates for a are latent, only 
affecting the values of the feature functions.9 The 
final end-to-end system uses the following 11 
features:  
- 7 error correction model features at the phrase 
level  
- IME conversion probability at the phrase level 
- language model probability at the sentence level 
- word/phrase penalty features at the sentence 
level 
The language model at the sentence level is trained 
on a large monolingual corpus of Chinese in hanzi, 
consisting of about 13 million sentences (176 
million words). The IME conversion probability 
                                                          
9 The final phrase table needs to be unique for each phrase pair 
(a, w), though the process described here results in multiple 
entries with the same pair having different feature values, 
because the generation of (a, w) is mediated by multiple 
correction candidates c1?k. These entries need to be added up 
to remove duplicates; we used a heuristic approximation of 
taking the pair where a equals cj (i.e., no spelling correction) 
when multiple entries are found.  
c1  xuesheng f1 ... f7
c2  xueshereng   f1 ... f7
c3  xueshusheng  f1 ... f7
...
+
c1  xuesheng    w1 ?? 1
c2  xueshereng w1 ??? 0.103
        w2 ??? 0.101
        w3 ??? 0.101
             ...
c3  xueshusheng  w1 ??? 0.102 
            w2 ??? 0.101 
            w3 ??? 0.101
           ...
...
?
w11 xueshseng ?? f1 ... f7 1
w21 xueshseng ??? f1 ... f7 0.103
w22 xueshseng ??? f1 ... f7 0.101
w23 xueshseng ??? f1 ... f7 0.101
...
w31 xueshseng ??? f1 ... f7 0.102
w32 xueshseng ??? f1 ... f7 0.101
w33 xueshseng ??? f1 ... f7 0.101
...
k-best error correction candidates c1...k 
n-best IME conversion 
candidates w1...n for c1...k
combined translation table w11...kn
 
Figure 2: Generation of integrated translation table for the pinyin input a = xueshseng 
615
also uses a word trigram model, but it is trained on 
a different data set which we did not have access 
to; we therefore used both of these models. The 
values for k and n can be determined empirically; 
we used 20 for both of them. 10  This generates 
maximally 400 conversion candidates for each 
input pinyin.  
The feature weights of the log-linear model are 
tuned using MERT. As running MERT on a CER-
based target criterion on the similar, monotone 
translation task of spelling correction did not lead 
to a significant improvement (Section 3.3), we 
simply report the results of using the 4-gram 
BLEU as the training criterion in this task.  
4.3 Experiments and results 
For the evaluation of the end-to-end conversion 
task, we used the CHIME corpus mentioned above. 
In order to use the word trigram language model 
that is built in-house, we re-segmented the CHIME 
corpus using our word-breaker, resulting in 12,102 
words in 2,000 sentences. We then divided the 
sentences in the corpus randomly into two halves, 
and performed a two-fold cross validation 
evaluation. The development portion of the data is 
used to tune the weights of the feature functions in 
MERT-style training. We measured our results 
using character error rate (CER), which is based on 
the longest common subsequence match in 
characters between the reference and the best 
system output. This is a standard metric used in 
evaluating IME systems (e.g., Mori et al1998; 
Gao et al2002). Let NREF be the number of 
characters in a reference sentence, NSYS be the 
character length of a system output, and NLCS be 
the length of the longest common subsequence 
between them. Then the character-level recall is 
defined as NLCS/NREF, and the precision as NLCS/NSYS. 
The CER based on recall and on precision are then 
defined as 1 ? recall and 1 ? precision, respectively. 
We report the harmonic mean of these values, 
similarly to the widely used F1-measure. 
As our goal is to show the effectiveness of the 
unified approach, we used simpler methods of 
integrating pinyin error correction with character 
conversion to create baselines. The simplest 
                                                          
10 From Table 2, we observe that the accuracy of the 20-best 
output of the spelling correction component is over 99%. An 
offline run with the IME system on an independent data set 
also showed that the accuracy of the 20-best IME output is 
over 99%.  
baseline is a pre-processing approach: we use the 
pinyin error correction model to convert A into a 
single best candidate C, and run an IME system on 
C. Another more realistic baseline is the noisy 
channel integration discussed in Section 4.1. We 
approximated this integration method by re-
ranking all the candidates generated by the 
proposed log-linear model with only the channel 
and language model probabilities, equally 
weighted.  
The results are shown Table 3. 5-best results as 
well as the 1-best results are shown, because in an 
IME application, providing the correct candidate in 
the candidate list is particularly important even if it 
is not the best candidate. Let us first discuss the 1-
best results. The CER of this test corpus using the 
in-house IME system without correcting any errors 
is 10.91. The oracle CER, which is the result of 
applying the IME on the gold standard pinyin input 
derived from the reference text using a hanzi-to-
pinyin converter (as mentioned in Section 3.3), is 
4.08, which is the upper-bound imposed by the 
IME conversion accuracy. The simple pipeline 
approach of concatenating the pinyin correction 
component with the character conversion 
component improves the CER by 1% to 9.93. 
Assuming that there are on average 20 words in a 
sentence, and each word consists of 2 characters, 
1% CER reduction means one improvement every 
2.5 sentences. Noisy channel integration improves 
over this quite substantially, achieving a CER of 
7.92, demonstrating the power of the word 
language model in character conversion. 
Incidentally, the CER of the output by Zheng et al
(2011b)'s model is 8.90.11 Their results are not as 
good as our noisy channel integration, as their 
system uses a manually defined error model and a 
word bigram language model. With the use of 
additional feature functions weighted 
discriminatively for the final conversion task, the 
                                                          
11 Available at http://chime.ics.uci.edu/. 
 CER on 
1-best 
CER on 
5-best 
Baseline: No correction 10.91 7.76 
Baseline: Pre-processing 9.93 6.75 
Baseline: Zheng et al2011b) 8.90  
Baseline: Noisy channel 7.92 3.93 
Proposed: SMT model 7.12 3.63 
Oracle 4.08 1.51 
Table 3: CER results for the conversion task (%) 
616
proposed method outperforms all these baselines to 
reduce the CER to 7.12, a 35% relative error rate 
reduction compared with the no correction baseline, 
a 20% reduction against Zheng et al011b) and a 
10% reduction from our noisy channel baseline. 
The 5-best results follow the same trend of steady 
improvement as we use a more integrated system.  
In order to understand the characteristics of the 
errors and remaining issues, we ran an error 
analysis on the 1-best results of the proposed 
system. For each word in the test data (all 2,000 
sentences) for which the system output had an 
error, we classified the reasons of failure into one 
of the four categories: (1) character conversion 
error: correct pinyin was input to the IME but the 
conversion failed; (2) over-correction of pinyin 
input: the system corrected the pinyin input when 
it should not have; (3) under-correction of pinyin 
input: the system did not correct an error in the 
input pinyin when it should have; (4) wrong 
correction: input pinyin string had a spelling error 
but it was corrected incorrectly.   
Table 4 shows the results of the error analysis. 
We find that somewhat contrary to our expectation, 
over-correction of the spelling mistakes was not a 
conspicuous problem, even though the pinyin 
correction rate of the training data is much higher 
than that of the test data. We therefore conclude 
that the error correction model adapts very well to 
the characteristics of the test data in our integrated 
SMT-based approach, which trains the unified 
feature weights to optimize the end goal.  
5 Conclusion and Future Work 
In this paper we have presented a unified approach 
to error-tolerant text input, inspired by the phrase-
based SMT framework, and demonstrated its 
effectiveness over the traditional method based on 
the noisy channel model. We have also presented a 
new method of automatically collecting parallel 
data for spelling correction from user keystroke 
logs, and showed that the log-linear model works 
well on the task of spelling correction in isolation 
as well.  
In this study, we isolated the problem of spelling 
errors and studied the effectiveness of error 
correction over a basic IME system that does not 
include advanced features such as abbreviated 
input (e.g., typing only "py" for ?? pengyou 
'friend' or ?? pinyin in Chinese) and auto-
completion (e.g., typing only "ari" for ????? 
arigatou 'thank you' in Japanese). Integrating data-
driven error correction feature with these advanced 
features for the benefit of users is the challenge we 
face in the next step.  
Acknowledgements 
We are indebted to many colleagues at Microsoft 
and MSR for their help in conducting this research, 
particularly to Xi Chen, Pallavi Choudhury, Chris 
Quirk, Mei-Yuh Hwang and Kristina Toutanova. 
We are also grateful for the comments we received 
from the reviewers of this paper.  
References  
Baba, Y. and H. Suzuki. 2012. How are spelling errors 
generated and corrected? A study of corrected and 
uncorrected spelling errors using keystroke logs. In 
Proceedings of ACL. 
Banko, M. and E. Brill. 2001. Scaling to very very large 
corpora for natural language disambiguation. In 
Proceedings of ACL.  
Bertoldi, N., M. Cettolo, and M. Federico. 2010. 
Statistical machine translation of texts with 
misspelled words. In Proceedings of HLT-NAACL.  
Brill, E., and R. C. Moore. 2000. An improved error 
model for noisy channel spelling correction. In 
Proceedings of ACL.  
Chen, Z., and K. F. Lee. 2000. A new statistical 
approach to Chinese Pinyin input. In Proceedings of 
ACL.  
Cherry, C., and H. Suzuki. 2009. Discriminative 
substring decoding for transliteration. In Proceedings 
of EMNLP. 
Cucerzan, S., and E. Brill. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In Proceedings of EMNLP.  
Duan, H., and P. Hsu. 2011. Online spelling correction 
for query completion. In Proceedings of WWW.  
Gao, J., J. Goodman, M. Li and K.-F. Lee. 2002. 
Toward a unified approach to statistical language 
modeling for Chinese. In ACM Transactions on 
Overall errors (words) 1,074 / 12,102 
  Conversion 646 (60.14%) 
  Over-corrections 155 (14.43%) 
  Under-correction 161 (14.99%) 
  Wrong correction 112 (10.42%) 
Table 4: Classification of errors  
617
Asian Language Information Processing, Vol. 1, No. 
1, pp 3-33. 
Gao, J., X. Li, D. Micol, C. Quirk and X. Sun. 2010. A 
large scale ranker-based system for search query 
spelling correction. In Proceedings of COLING. 
Jiampojamarn, S., G. Kondrak and T. Sherif, 2007. 
Applying many-to-many alignments and hidden 
markov models to letter-to-phoneme conversion. In 
Proceedings of HLT/NAACL. 
Kernighan, M., K. Church, and W. Gale. 1990. A 
spelling correction program based on a noisy channel 
model. In Proceedings of COLING. 
Koehn, P., F. Och and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of HLT-
NAACL.  
McEnery, A. and Xiao, Z. 2004. The Lancaster Corpus 
of Mandarin Chinese: A Corpus for Monolingual and 
Contrastive Language Study. In Proceedings of 
LREC.  
Mori, S., M. Tsuchiya, O. Yamaji and M. Nagao. 1998. 
Kana-kanji conversion by a stochastic model. In 
Proceedings of Information Processing Society of 
Japan, SIG-NL-125-10 (in Japanese). 
Och, F. J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL.  
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Rama, T., A. K. Singh and S. Kolachina. 2009. 
Modeling letter-to-phoneme conversion as a phrase 
based statistical machine translation problem with 
minimum error rate training. In Proceedings of the 
NAACL HLT Student Research Workshop and 
Doctoral Consortium. 
Sherif, T. and G. Kondrak. 2007. Substring-based 
transliteration. In Proceedings of ACL. 
Sun, X., J. Gao, D. Micol and C. Quirk. 2010. Learning 
phrase-based spelling error models from clickthrough 
data. In Proceedings of ACL.  
Suzuki, H. and J. Gao. 2005. A comparative study on 
language model adaptation using new evaluation 
metrics. In Proceedings of EMNLP.  
Toutanova, K., and R. C. Moore. 2002. Pronunciation 
modeling for improved spelling correction. In 
Proceedings of ACL.  
Tokunaga, H., D. Okanohara and S. Mori. 2011. 
Discriminative method for Japanese kana-kanji input 
method. In Proceedings of the Workshop on 
Advances in Text Input Methods (WTIM 2011).  
Whitelaw, C., B. Hutchinson, G. Y. Chung, and G. 
Ellis. 2009. Using the web for language independent 
spellchecking and autocorrection. In Proceedings of 
ACL. 
Zhang, Y., L. Deng, X. He and A. Acero. 2011. A novel 
decision function and the associated decision-
feedback learning for speech translation. In 
Proceedings of ICASSP.  
Zheng, Y., L. Xie, Z. Liu, M. Sun. Y. Zhang and L. Ru. 
2011a. Why press backspace? Understanding user 
input behaviors in Chinese pinyin input method. In 
Proceedings of ACL.  
Zheng, Y., C. Li and M. Sun. 2011b. CHIME: An 
efficient error-tolerant Chinese pinyin input method. 
In Proceedings of IJCAI.  
 
618
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 666?676, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Lexicon Models from Search Logs for Query Expansion 
 
Jianfeng Gao 
Microsoft Research, Redmond 
Washington 98052, USA 
jfgao@microsoft.com 
 
Shasha Xie 
Educational Testing Service, Princeton 
New Jersey 08540, USA 
sxie@ets.org 
 
Xiaodong He 
Microsoft Research, Redmond 
Washington 98052, USA 
xiaohe@microsoft.com 
 
 
Alnur Ali 
Microsoft Bing, Bellevue 
Washington 98004, USA 
alnurali@microsoft.com 
 
Abstract 
This paper explores log-based query expan-
sion (QE) models for Web search. Three 
lexicon models are proposed to bridge the 
lexical gap between Web documents and 
user queries. These models are trained on 
pairs of user queries and titles of clicked 
documents. Evaluations on a real world data 
set show that the lexicon models, integrated 
into a ranker-based QE system, not only 
significantly improve the document retriev-
al performance but also outperform two 
state-of-the-art log-based QE methods. 
1 Introduction 
Term mismatch is a fundamental problem in Web 
search, where queries and documents are com-
posed using different vocabularies and language 
styles. Query expansion (QE) is an effective strate-
gy to address the problem. It expands a query is-
sued by a user with additional related terms, called 
expansion terms, so that more relevant documents 
can be retrieved.  
In this paper we explore the use of clickthrough 
data and translation models for QE. We select ex-
pansion terms for a query according to how likely 
it is that the expansion terms occur in the title of a 
document that is relevant to the query. Assuming 
that a query is parallel to the titles of documents 
clicked for that query (Gao et al
icon models are trained on query-title pairs ex-
tracted from clickthrough data. The first is a word 
model that learns the translation probability be-
tween single words. The second model uses lexi-
calized triplets to incorporate word dependencies 
for translation. The third is a bilingual topic model, 
which represents a query as a distribution of hid-
den topics and learns the translation between a 
query and a title term at the semantic level. We 
will show that the word model provides a rich set 
of expansion candidates while the triplet and topic 
models can effectively select good expansion 
terms, and that a ranker-based QE system which 
incorporates all three of these models not only sig-
nificantly improves Web search result but outper-
forms other log-based QE methods that are state-
of-the-art. 
There is growing interest in applying user logs 
to improve QE. A recent survey is due to Baeze-
Yates and Ribeiro-Neto (2011). Below, we briefly 
discuss two log-based QE methods that are closest 
to ours and are re-implemented in this study for 
comparison. Both systems use the same type of log 
data that we used to train the lexicon models. The 
term correlation model of Cui et al
to our knowledge the first to explore query-
document relations for direct extraction of expan-
sion terms for Web search. The method outper-
forms traditional QE methods that do not use log 
data e.g. the local analysis model of Xu and Croft 
(1996). In addition, as pointed out by Cui et al
(2003) there are three important advantages that 
make log-based QE a promising technology to im-
prove the performance of commercial search en-
gines. First, unlike traditional QE methods that are 
based on relevance feedback, log-based QE derives 
expansion terms from search logs, allowing term 
correlations to be pre-computed offline. Compared 
to methods that are based on thesauri either com-
piled manually (Prager et alu-
666
tomatically from document collections (Jing and 
Croft 1994), the log-based method is superior in 
that it explicitly captures the correlation between 
query terms and document terms, and thus can 
bridge the lexical gap between them more effec-
tively. Second, since search logs retrain query-
document pairs clicked by millions of users, the 
term correlations reflect the preference of the ma-
jority of users. Third, the term correlations evolve 
along with the accumulation of user logs, thus can 
reflect updated user interests at a specific time. 
However, as pointed out by Riezler et al
(2008), Cui et ald method suf-
fers low precision of QE partly because the corre-
lation model does not explicitly capture context 
information and is susceptible to noise. Riezler et 
al. developed a QE system by retraining a standard 
phrase-based statistical machine translation (SMT) 
system using query-snippet pairs extracted from 
clickthrough data (Riezler et al
Liu 2010). The SMT-based system can produce 
cleaner, more relevant expansion terms because 
rich context information useful for filtering noisy 
expansions is captured by combining language 
model and phrase translation model in its decoder. 
Furthermore, in the SMT system all component 
models are properly smoothed using sophisticated 
techniques to avoid sparse data problems while the 
correlation model relies on pure counts of term 
frequencies. However, the SMT system is used as a 
black box in their experiments. So the relative con-
tribution of different SMT components is not veri-
fied empirically. In this study we break this black 
box in order to build a better, simpler QE system. 
We will show that the proposed lexicon models 
outperform significantly the term correlation mod-
el, and that a simpler QE system that incorporates 
the lexicon models can beat the sophisticated, 
black-box SMT system. 
2 Lexicon Models 
We view search queries and Web documents as 
two different languages, and cast QE as a means to 
bridge the language gap by translating queries to 
documents, represented by their titles. In this sec-
tion, we will describe three translation models that 
are based on terms, triplets, and topics, respective-
ly, and the way these models are learned from que-
ry-title pairs extracted from clickthrough data. 
2.1 Word Model 
The word model takes the form of IBM Model 1 
(Brown et alafferty 1999). Let 
            be a query,   be an expansion term 
candidate, the translation probability from   to   is 
defined as  
   |     ? ( |  ) (  | )
 
   
 (1) 
where    |   is the unsmoothed unigram proba-
bility of word   in query  . The word translation 
probabilities    |   are estimated on the query-
title pairs derived from the clickthrough data by 
assuming that the title terms are likely to be the 
desired expansions of the paired query. Our train-
ing method follows the standard procedure of 
training statistical word alignment models pro-
posed by Brown et al we opti-
mize the model parameters   by maximizing the 
probability of generating document titles from que-
ries over the entire training corpus: 
           ?    |     
 
   
 (2) 
where both the titles   and the paired queries   are 
viewed as bag of words. The translation probability 
    |      takes the form of IBM Model 1 as  
   |     
 
      
?? (  |    )
 
   
 
   
 (3) 
where   is a constant,   is the length of  , and   is 
the length of  . To find the optimal word transla-
tion probabilities of IBM Model 1, we used the EM 
algorithm, where the number of iterations is deter-
mined empirically on held-out data. 
2.2 Triplet Model 
The word model is context independent. The triplet 
model, which is originally proposed for SMT (Ha-
san et al to capture inter-term 
dependencies for selecting expansion terms. The 
model is based on lexicalized triplets (       ) 
which can be understood as two query terms trig-
gering one expansion term. The translation proba-
bility of   given   for the triplet model is parame-
terized as 
667
   |     
 
 
? ?  ( |     )
 
     
   
   
 (4) 
where Z is a normalization factor based on the cor-
responding query length, i.e.,   
      
 
, and 
 (  |     ) is the probability of translating    into 
   given another query word   . Since    can be 
any word in   that is not necessary to be adjacent 
to   , the triple model is able to combine local (i.e. 
word and phrase level) and global (i.e. query level) 
contextual information useful for word translation.  
Similar to the case of word model, we used the 
EM algorithm to estimate the translation probabili-
ties    |      on the query-title pairs.  Since the 
number of all possible triplets (      ) is large and 
as a consequence the model training could suffer 
the data sparseness problem, in our experiments 
count-based cutoff is applied to prune the model to 
a manageable size. 
2.3 Bilingual Topic Model (BLTM) 
The BLTM was originally proposed for Web doc-
ument ranking by Gao et aln-
derlying the model is that a search query and its 
relevant Web documents share a common distribu-
tion of (hidden) topics, but use different (probably 
overlapping) vocabularies to express these topics. 
Intuitively, BLTM-based QE works as follows. 
First, a query is represented as a vector of topics. 
Then, all the candidate expansion terms, which are 
selected from document, are ranked by how likely 
it is that these document terms are selected to best 
describe those topics. In a sense, BLTM is similar 
to the word model and the triplet model since they 
all map a query to a document word. BLTM differs 
in that the mapping is performed at the topic level 
(via a language independent semantic representa-
tion) rather than at the word level. In our experi-
ments BLTM is found to often select a different set 
of expansion terms and is complementary to the 
word model and the triplet model. 
Formally, BLTM-based QE assumes the follow-
ing story of generating   from  : 
1. First, for each topic  , a pair of different 
word distributions    
    
   are selected 
from a Dirichlet prior with concentration pa-
rameter ?, where  
 
 is a topic-specific query 
term distribution, and   
  a topic-specific 
document term distribution. Assuming there 
are   topics, we have two sets of distribu-
tions       
      
   and    
   
      
  . 
2. Given  , a topic distribution    is drawn 
from a Dirichlet prior with concentration pa-
rameter  . 
3. Then a document term (i.e., expansion term 
candidate)   is generated by first selecting a 
topic   according to the topic distribution   , 
and then drawing a word from   
 . 
By summing over all possible topics, we end up 
with the following model form 
       |   ?   |  
     |   
 
 (5) 
The BLTM training follows the method described 
in Gao et ale EM algorithm to 
estimate the parameters (         of BLTM by 
maximizing the joint log-likelihood of the query-
title pairs and the parameters. In training, we also 
constrain that the paired query and title have simi-
lar fractions of tokens assigned to each topic. The 
constraint is enforced on expectation using posteri-
or regularization (Ganchev et al
3 A Ranker-Based QE System 
This section describes a ranker-based QE system in 
which the three lexicon models described above 
are incorporated. The system expands an input 
query in two distinct stages, candidate generation 
and ranking, as illustrated by an example in Figure 
1. 
Original query jaguar locator 
Ranked expansion  jaguar finder 
candidates 
(altered words are in 
car locator 
jaguar location 
italic) jaguar directory 
 ? 
 jaguar list 
Expanded query OR(jaguar, car)  
(selected expansion 
terms are in italic) 
OR(locator, finder, location, 
directory) 
Figure 1. An example of an original query, its expan-
sion candidates and the expanded query generated by 
the ranker-based QE system. 
 
668
In candidate generation, an input query   is 
first tokenized into a sequence of terms. For each 
term   that is not a stop word, we consult a word 
model described in Section 2.1 to identify the best 
  altered words according to their word transla-
tion probabilities from  . Then, we form a list of 
expansion candidates, each of which contains all 
the original words in   except for the word that is 
substituted by one of its altered words. So, for a 
query with   terms, there are at most    candi-
dates. 
In the second stage, all the expansion candidates 
are ranked using a ranker that is based on the Mar-
kov Random Field (MRF) model in which the 
three lexicon models are incorporated as features.  
Expansion terms of a query are taken from those 
terms in the  -best (     in our experiments) 
expansion candidates of the query that have not 
been seen in the original query string. 
In the remainder of this section we will describe 
in turn the MRF-based ranker, the ranking features, 
and the way the ranker parameters are estimated. 
3.1 MRF-Based Ranker 
The ranker is based on the MRF model that models 
the joint distribution of         over a set of ex-
pansion term random variables             and 
a query random variable  . It is constructed from a 
graph   consisting of a query node and nodes for 
each expansion term. Nodes in the graph represent 
random variables and edges define the independ-
ence semantics between the variables. An MRF 
satisfies the Markov property (Bishop 2006), 
which states that a node is independent of all of its 
non-neighboring nodes given observed values of 
its neighbors, defined by the clique configurations 
of  . The joint distribution over the random varia-
bles in   is defined as  
        
 
  
?       
      
 (6) 
where      is the set of cliques in  , and each 
       is a non-negative potential function de-
fined over a clique configuration c that measures 
the compatibility of the configuration,   is a set of 
parameters that are used within the potential func-
tion, and    normalizes the distribution. For rank-
ing expansion candidates, we can drop the expen-
sive computation of    since it is independent of 
E, and simply rank each expansion candidate   by 
its unnormalized joint probability with   under the 
MRF. It is common to define MRF potential func-
tions of the exponential form as        
           , where      is a real-valued feature 
function over clique values and    is the weight of 
the feature function. Then, we can compute the 
posterior     |   as 
    |   
       
     
 (7) 
    
?   ?          
      
 ?       
      
  
which is essentially a weighted linear combination 
of a set of features. 
Therefore, to instantiate the MRF model, one 
needs to define a graph structure and a set of po-
tential functions. In this paper, the graphical model 
representation we propose for QE is a fully con-
nected graph shown in Figure 2, where all expan-
sion terms and the original query are assumed de-
pendent with each other. In what follows, we will 
define six types of cliques that we are interested in 
defining features (i.e., potential functions) over. 
3.2 Features 
The cliques and features are inspired by the com-
ponent models used in SMT systems. The cliques 
defined in   for MRF can be grouped into two cat-
egories. The first includes three types of cliques 
involving both the query node and one or more 
expansion terms. The potential functions defined 
over these cliques attempt to abstract the idea be-
hind the query to title translation models. The other 
three types, belonging to the second category, in-
volve only expansion terms. Their potential func-
 
 
Figure 2: The structure of the Markov random field for 
representing the term dependency among the query   
and the expansion terms        . 
669
tions attempt to abstract the idea behind the target 
language models.  
The first type of cliques involves a single ex-
pansion term and the query node. The potentials 
functions for these cliques are defined as 
                             (6) 
               
                   
where the three feature functions of the form 
       are defined as the log probabilities of 
translating   to   according to the word, triplet and 
topic models defined in Equations (1), (4) and (5), 
respectively. 
                   |    
                   |    
                       |    
The second type of cliques contains the query 
node and two expansion terms,    and     , which 
appear in consecutive order in the expansion. The 
potential functions over these cliques are defined 
as 
                                        (7) 
where the feature        is defined as the log prob-
ability of generating an expansion bigram given   
           |               |    
Unlike the language models used for document 
ranking (e.g., Zhai and Lafferty 2001), we cannot 
compute the bigram probability by simply counting 
the relative frequency of           in   because 
the query is usually very short and the bigram is 
unlikely to occur. Thus, we approximate the bi-
gram probability by assuming that the words in   
are independent with each other. We thus have 
         |   
            
    
 
 
           |   ?     |        
 
   
?      
 
   
  
where     |         is the translation probability 
computed using a variant of the triplet model de-
scribed  in Section 2.2. The model variation differs 
from the one of Equation (4) in two respects. First, 
it models the translation in a different direction i.e., 
from expansion to query. Second, we add a con-
straint to the triplets such that (       ) must be an 
ordered, contiguous bigram. The model variation is 
also trained using EM on query-title pairs.       
and       |    are assigned respectively by the 
unigram and bigram language models, estimated 
from the collection of document titles of the click-
through data, and        is the unigram probability 
of the query term, estimated from the collection of 
queries of the clickthrough data. 
The third type of cliques contains the query 
node and two expansion terms,    and   , which 
occur unordered within the expansion. The poten-
tial functions over these cliques are defined as 
                                    (8) 
where the feature        is defined as the log prob-
ability of generating a pair of expansion terms 
        given   
         |             |  .  
Unlike            |   defined in Equation (7), this 
class of features captures long-span term depend-
ency in the expansion candidate. Similar to the 
computation of          |   in Equation (7), we 
approximate        |   as     
       |   
          
    
 
 
         |   ?     |      
 
   
?      
 
   
  
where     |       is the translation probability 
computed using the triplet model described  in Sec-
tion 2.2, but in the expansion-to-query direction. 
      is assigned by a unigram language model 
estimated from the collection of document titles of 
the clickthrough data.     |    is assigned by a co-
occurrence model, estimated as  
    |    
        
?         
  
where         is the number of times that the two 
terms occur in the same title in clickthrough data.  
We now turn to the other three types of cliques 
that do not contain the query node. The fourth type 
of cliques contains only one expansion term. The 
potential functions are defined as 
670
                          (9) 
                  
where       is the unigram probability computed 
using a unigram language model trained on the 
collection of document titles. 
The fifth type of cliques contains a pair of terms 
appearing in consecutive order in the expansion. 
The potential functions are defined as 
                                    (10) 
                      |     
where       |    is the bigram probability com-
puted using a bigram language model trained on 
the collection of document titles. 
The sixth type of cliques contains a pair of 
terms appearing unordered within the expansion. 
The potential functions are defined as 
                                (11) 
                  |     
where     |    is the assigned by a co-occurrence 
model trained on the collection of document titles. 
3.3 Parameter Estimation 
The MRF model uses 8 classes of features defined 
on 6 types of cliques, as in Equations (6) to (11). 
Following previous work (e.g., Metzler and Croft 
2005; Bendersky et alhat all 
features within the same feature class are weighted 
by the same tied parameter   . Thus, the number of 
free parameters of the MRF model is significantly 
reduced. This not only makes the model training 
easier but also improves the robustness of the 
model. After tying the parameters and using the 
exponential potential function form, the MRF-
based ranker can be parameterized as  
    |  
    
?      ?          
 
   
  (12) 
   ?          
 
   
  
     ?            
 
   
  
   ?               
   
   
  
   ? ?             
 
     
   
   
  
   ?        
 
   
  
   ?             
   
   
  
   ? ?           
 
     
   
   
 
where there are in total 8  ?s to be estimated. 
Although the MRF is by nature a generative 
model, it is not always appropriate to train the pa-
rameters using conventional likelihood based ap-
proaches due to the metric divergence problem 
(Morgan et alaximum likelihood 
estimate is unlikely to be the one that optimizes the 
evaluation metric. In this study the effectiveness of 
a QE method is evaluated by first issuing a set of 
queries which are expanded using the method to a 
search engine and then measuring the Web search 
performance. Better QE methods are supposed to 
lead to better Web search results using the corre-
spondingly expanded query set. 
For this reason, the parameters of the MRF-
based ranker are optimized directly for Web 
search. In our experiments, the objective in train-
ing is Normalized Discounted Cumulative Gain 
(NDCG, Jarvelin and Kekalainen 2000), which is 
widely used as quality measure for Web search. 
Formally, we view parameter training as a multi-
dimensional optimization problem, with each fea-
ture class as one dimension. Since NDCG is not 
differentiable, we tried in our experiments numeri-
cal algorithms that do not require the computation 
of gradient. Among the best performers was the 
Powell Search algorithm (Press et al
constructs a set of   virtual directions that are con-
jugate (i.e., independent with each other), then it 
uses line search  times (    in our case), each 
on one virtual direction, to find the optimum. Line 
search is a one-dimensional optimization algo-
rithm. Our implementation follows the one de-
scribed in Gao et alsed to opti-
mize averaged precision. 
4 Experiments 
We evaluate the performance of a QE method by 
first issuing a set of queries which are expanded 
using the method to a search engine and then 
671
measuring the Web search performance. Better QE 
methods are supposed to lead to better Web search 
results using the correspondingly expanded query 
set.  
Due to the characteristics of our QE methods, 
we cannot conduct experiments on standard test 
collections such as the TREC data because they do 
not contain related user logs we need. Therefore, 
following previous studies of log-based QE (e.g., 
Cui et allthe 
proprietary datasets that have been developed for 
building a commercial search engine, and demon-
strate the effectiveness of our methods by compar-
ing them against previous state-of-the-art log-
based QE methods. 
The relevance judgment set consists of 4,000 
multi-term English queries. On average, each que-
ry is associated with 197 Web documents (URLs). 
Each query-URL pair has a relevance label. The 
label is human generated and is on a 5-level rele-
vance scale, 0 to 4, with 4 meaning document D is  
the  most  relevant  to  query Q  and 0 meaning  D 
is  not  relevant to Q.  
The relevance judgment set is constructed as 
follows. First, the queries are sampled from a year 
of search engine logs. Adult, spam, and bot queries 
are all removed. Queries are ?de-duped? so that 
only unique queries remain. To reflect a natural 
query distribution, we do not try to control the 
quality of these queries. For example, in our query 
sets, there are roughly 20% misspelled queries, 20% 
navigational queries, and 10% transactional que-
ries. Second, for each query, we collect Web doc-
uments to be judged by issuing the query to several 
popular search engines (e.g., Google, Bing) and 
fetching retrieval results from each. Finally, the 
query-document pairs are judged by a group of 
well-trained assessors. In this study all the queries 
are preprocessed as follows. The text is white-
space tokenized and lowercased, numbers are re-
tained, and no stemming/inflection treatment is 
performed. We split the judgment set into two non-
overlapping datasets, namely training and test sets, 
respectively. Each dataset contains 2,000 queries. 
The query-title pairs used for model training are 
extracted from one year of query log files using a 
procedure similar to Gao et al
periments we used a randomly sampled subset of 
20,692,219 pairs that do not overlap the queries 
and documents in the test set. 
Our Web document collection consists of ap-
proximately 2.5 billion Web pages. In the retrieval 
experiments we use the index based on the content 
fields (i.e., body and title text) of each Web page. 
The Web search performance is evaluated by 
mean NDCG. We report NDCG scores at trunca-
tion levels of 1, 3, and 10.  We also perform a sig-
nificance test using the paired t-test. Differences 
are considered statistically significant when p-
value is less than 0.05. 
4.1 Comparing Systems 
Table 1 shows the main document ranking results 
using different QE systems, developed and evalu-
ated using the datasets described above.  
NoQE (Row 1) is the baseline retrieval system 
that uses the raw input queries and the BM25 doc-
ument ranking model. Rows 2 to 4 are different QE 
systems. Their results are obtained by first expand-
ing a query, then using BM25 to rank the docu-
ments with respect to the expanded query.  
TC (Row 2) is our implementation of the corre-
lation-based QE system (Cui et al
takes the following steps to expand an input query 
 : 
# QE methods NDCG@1 NDCG@3 NDCG@10 
1 NoQE 34.70 36.50 41.54 
2 TC 33.78 36.57 42.33
 ? 
3 SMT 34.79
 ? 36.98 ?? 42.84 ?? 
4 MRF 36.10 ??? 38.06 ??? 43.71 ??? 
5 MRFum+bm+cm 33.31 36.12 42.26
 ? 
6 MRFtc 34.50
 ? 36.59 42.33 ? 
7 MRFwm 34.73
 ? 36.62 42.73 ?? 
8 MRFtm 35.13
 ?? 37.46 ??? 42.82 ?? 
9 MRFbltm 34.34
 ? 36.19 41.98 ? 
10 MRFwm+tm 35.21
 ??? 37.46 ??? 42.83 ?? 
11 MRFwm+tm+bltm 35.84
 ??? 37.70 ??? 43.14 ??? 
Table 1: Ranking results using BM25 with different 
query expansion systems. The superscripts      and    
indicate statistically significant improvements 
         over NoQE, TC, and SMT, respectively. 
Rows 5 to 11 are different versions of MRF in Row 5, 
They use the same candidate generator but use in the 
ranker different feature classes, as specified by the 
subscript. tc specifies the feature class defined as the 
scoring function in Equation (13). Refer to Equation 
(12) for the names of other feature classes. 
 
 
672
1. Extract all query terms   (eliminating 
stopwords) from  . 
2. Find all documents that have clicks on a 
query that contains one or more of these 
query terms. 
3. For each title term   in these documents, 
calculate its evidence of being selected as 
an expansion term according to the whole 
query via a scoring function        |   . 
4. Select n title terms with the highest score 
(where the value of n is optimized on train-
ing data) and formulate the expanded que-
ry by adding these terms into  . 
5. Use the expanded query to rank documents. 
The scoring function is based on the term correla-
tion model, and is defined as 
       |     (?   |    
   
) (13) 
   |   ?    |     |  
    
  
where    is the set of documents clicked for the 
queries containing the term   and is collected from 
search logs,    |   is a normalized tf-idf weight 
of the document term in  , and    |   is the rela-
tive occurrence of   among all the documents 
clicked for the queries containing  . Table 1 shows 
that TC leads to significant improvement over 
NoQE in NDCG@10, but not in NDCG@1 and 
NDCG@3 (Row 2 vs. Row 1). The result is not 
entirely consistent with what reported in Cui et al
(2003). A possible reason is that Cui et al
formed the evaluation using documents and search 
logs collected from the Encarta website, which is 
much cleaner and more homogenous than the data 
sets we used. The result suggests that although QE 
improves the recall of relevant documents, it is 
also likely to introduce noise that hurts the preci-
sion of document retrieval. 
SMT (Row 3) is a SMT-based QE system. Fol-
lowing Riezler et al is an im-
plementation of a phrase-based SMT system with a 
standard set of features for translation model and 
language model, combined under a log linear mod-
el framework (Koehn et alrom 
Riezler et al translation model 
is trained on query-snippet pairs and the language 
model on queries, in our implementation the trans-
lation model is trained on query-title pairs and the 
language model on titles. To apply the system to 
QE, expansion terms of a query are taken from 
those terms in the 10-best translations of the query 
that have not been seen in the original query string. 
We see that SMT significantly outperforms TC in 
NDCG at all levels. The result confirms the con-
clusion of Riezler et alt context 
information is crucial for improving retrieval pre-
cision by filtering noisy expansions.  
Both TC and SMT, considered as state-of-the-
art QE methods, have been frequently used for 
comparison in related studies. Thus, we also used 
them as baselines in our experiments. 
MRF (Row 4) is the ranker-based QE system 
described in Section 3, which uses a MRF-based 
ranker to incorporate all 8 classes of features de-
rived from a variety of lexicon translation models 
and language models as in Equation (12). Results 
show that the ranker-based QE system significantly 
outperforms both NoQE and the two state-of-the-
art QE methods. The fact that MRF beats SMT 
with a statistically significant margin although the 
former is a much simpler system indicates that text 
translation and QE are different tasks and some 
SMT components, designed for the task of regular 
text translation, are not as effective in selecting 
expansion terms. We will explore this in more de-
tail in the next section. 
4.2 Comparing Models 
The experiments presented in this section investi-
gate in detail the effectiveness of different models, 
e.g., the lexicon models and the language models 
described in Sections 2 and 3, in ranking expansion 
candidates for QE. The results are summarized in 
Rows 5 to 11 in Table 1, where a number of differ-
ent versions of the ranker-based QE system are 
compared. These versions, labeled as MRFf, use 
the same candidate generator, and differ in the fea-
ture classes (which are specified by the subscript f) 
incorporated in the MRF-based ranker. In what 
follows, we focus our discussion on the results of 
the three lexicon models. 
MRFwm (Row 7) uses the word translation 
model described in Section 2.1. Both the word 
model and term correlation model used in MRFtm 
(Row 6) are context independent. They differ 
mainly in the training methods. For the sake of 
comparison, in our experiment the word model is 
673
EM-trained with the correlation model as initial 
point. Rezler et al that statisti-
cal translation model is superior to correlation 
model because the EM training captures the hidden 
alignment information when mapping document 
terms to query terms, leading to a better smoothed 
probability distribution. Our result (Row 7 vs. Row 
6) verifies the hypothesis. Notice that MRFtc out-
performs TC in NDCG@1 (Row 6 vs. Row 2) 
mainly because in the former the expansion candi-
dates are generated by a word translation model 
and are less noisy. 
It is encouraging to observe that the rankers us-
ing the triplet model features achieve the QE per-
formance either in par with or better than that of 
SMT (Rows 8, 10 and 11 vs. Row 3), although the 
latter is a much more sophisticated system. The 
result suggests that not all SMT components are 
useful for QE. For example, language models are 
indispensable for translation but are less effective 
than word models for QE (Row 5 vs. Rows 6 and 
7). We also observe that the triplet model not only 
outperforms significantly the word model due to 
the use of contextual information (Row 8 vs. Row 
7), but also seems to subsume the latter in that 
combining the features derived from both models 
in the ranker leads to little improvement over the 
ranker that uses only the triplet model features 
(Row 10 vs. Row 8).  
The bilingual topic model underperforms the 
word model and the triplet model (Row 9 vs. Rows 
7 and 8). However, we found that the bilingual top-
ic model often selects a different set of expansion 
terms and is complementary to the other two lexi-
con models. As a result, unlike the case of combin-
ing the word model and triplet model features, in-
corporating the bilingual topic model features in 
the ranker leads to some visible improvement in 
NDCG at all positions (Row 11 vs. Row 10). 
To better understand empirically how the MRF-
based QE system achieves the improvement, we 
analyzed the expansions generated by our system 
in detail and obtained several interesting findings. 
First, as expected, in comparison with the word 
model, the triplet translation model is more effec-
tive in benefitting long queries, e.g., notably que-
ries containing questions and queries containing 
song lyrics. Second, unlike the two lexicon models, 
the bilingual topic model tends to generate expan-
sions that are more likely to relate to an entire que-
ry rather than individual query terms. Third, the 
features involving the order of the expansion terms 
benefitted queries containing named entities. 
5 Related Work 
In comparison with log-based methods studied in 
this paper, the QE methods based on automatic 
relevance feedback have been studied much more 
extensively in the information retrieval (IR) com-
munity, and have been proved useful for improving 
IR performance on benchmark datasets such as 
TREC (e.g., Rocchio 1971; Xu and Croft 1996; 
Lavrenko 2001; Zhai and Lafferty 2001). Howev-
er, these methods cannot be applied directly to a 
commercial Web search engine because the rele-
vant documents are not always available and gen-
erating pseudo-relevant documents requires multi-
phase retrieval, which is prohibitively expensive. 
Although automatic relevance feedback is not the 
focus of this study, our method shares a lot of simi-
larities with some of them. For example, similar to 
the way the parameters of our QE ranker are esti-
mated, Cao et alethod of se-
lecting expansion terms to directly optimize aver-
age precision. The MRF model has been previous-
ly used for QE, in the form of relevance feedback 
and pseudo-relevance feedback (Metzler et al
2007; Lang et al MRF models 
use the features derived from IR systems such as 
Indri, we use the SMT-inspired features.  
Using statistical translation models for IR is not 
new (e.g., Berger and Lafferty 1999; Jin et al
Xue et alveness of the statistical 
translation-based approach to Web search has been 
demonstrated empirically in recent studies where 
word-based and phrase-based translation models 
are trained on large amounts of clickthrough data 
(e.g., Gao et alwork extends 
these studies and constructs QE-oriented transla-
tion models that capture more flexible dependen-
cies. 
In addition to QE, search logs have also been 
used for other Web search tasks, such as document 
ranking (Joachims 2002; Agichtein et al
search query processing and spelling correction 
(Huang et alre-
trieval (Craswell and Szummer 2007), and user 
query clustering (Baeza-Yates and Tiberi 2007; 
Wen et al
6 Conclusions 
674
In this paper we extend the previous log-based QE 
methods in two directions. First, we formulate QE 
as the problem of translating a source language of 
queries into a target language of documents, repre-
sented as titles. This allows us to adapt the estab-
lished techniques developed for SMT to QE. Spe-
cially, we propose three lexicon models based on 
terms, lexicalized triplets, and topics, respectively. 
These models are trained on pairs of user queries 
and the titles of clicked documents using EM. Se-
cond, we present a ranker-based QE system, the 
heart of which is a MRF-based ranker in which the 
lexicon models are incorporated as features. We 
perform experiments on the Web search task using 
a real world data set. Results show that the pro-
posed system outperforms significantly other state-
of-the-art QE systems. 
This study is part of a bigger, ongoing project, 
aiming to develop a real-time QE system for Web 
search, where simplicity is the key to the success. 
Thus, what we learned from this study is particu-
larly encouraging. We demonstrate that with large 
amounts of clickthrough data for model training, 
simple lexicon models can achieve state-of-the-art 
QE performance, and that the MRF-based ranker 
provides a simple and flexible framework to incor-
porate a variety of features capturing different 
types of term dependencies in such an effective 
way that the Web search performance can be di-
rectly optimized. 
References  
Agichtein, E., Brill, E., and Dumais, S. 2006. Im-
proving web search ranking by incorporating us-
er behavior information. In SIGIR, pp. 19-26. 
Baeze-Yates, R., and Ribeiro-Neto, B. 2011. Mod-
ern Information Retrieval. Addison-Wesley. 
Baeza-Yates, R. and Tiberi, A. 2007. Extracting 
semantic relations from query logs. In SIGKDD, 
pp. 76-85. 
Bai, J., Song, D., Bruza, P., Nie, J-Y., and Cao, G. 
2005. Query expansion using term relationships 
in language models for information retrieval. In 
CIKM, pp. 688-695. 
Bendersky, M., Metzler, D., and Croft, B. 2010. 
Learning concept importance using a weighted 
dependence model. In WSDM, pp. 31-40.  
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229. 
Bishop, C. M. 2006. Patten recognition and ma-
chine learning. Springer.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet al Machine 
Learning Research, 3: 993-1022. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., 
and Mercer, R. L. 1993. The mathematics of sta-
tistical machine translation: parameter estimation. 
Computational Linguistics, 19(2): 263-311. 
Cao, G., Nie, J-Y., Gao, J., and Robertson, S. 2008. 
Selecting good expansion terms for pseudo-
relevance feedback. In SIGIR, pp. 289-305. 
Craswell, N. and Szummer, M. 2007. Random 
walk on the click graph. In SIGIR. pp. 239-246. 
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2002. 
Probabilistic query expansion using query logs. 
In WWW, pp. 325-332.  
Cui, H., Wen, J-R., Nie, J-Y. and Ma, W-Y. 2003. 
Query expansion by mining user log. IEEE 
Trans on Knowledge and Data Engineering. Vol. 
15, No. 4. pp. 1-11. 
Dempster, A., Laird, N., and Rubin, D. 1977. Max-
imum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical 
Society, 39: 1-38. 
Ganchev, K., Graca, J., Gillenwater, J., and Taskar, 
B. 2010. Posterior regularization for structured 
latent variable models. Journal of Machine 
Learning Research, 11 (2010): 2001-2049. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684. 
Gao, J., He, X., and Nie, J-Y. 2010a. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM, pp. 
1139-1148. 
Gao, J., Li, X., Micol, D., Quirk, C., and Sun, X. 
2010b. A large scale ranker-based system for 
query spelling correction. In COLING, pp. 358-
366. 
675
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR, pp. 355-362. 
Gao, J., Qi, H., Xia, X., and Nie, J-Y. 2005. Linear 
discriminant model for information retrieval. In 
SIGIR, pp. 290-297. 
Hasan, S., Ganitkevitch, J., Ney, H., and Andres-
Fnerre, J. 2008. Triplet lexicon models for statis-
tical machine translation. In EMNLP, pp. 372-
381. 
Huang, J., Gao, J., Miao, J., Li, X., Wang, K., and 
Behr, F. 2010. Exploring web scale language 
models for search query processing. In WWW, pp. 
451-460. 
Jarvelin, K. and Kekalainen, J. 2000. IR evaluation 
methods for retrieving highly relevant docu-
ments. In SIGIR, pp. 41-48 
Jin, R., Hauptmann, A. G., and Zhai, C. 2002. Title 
language model for information retrieval. In 
SIGIR, pp. 42-48. 
Jing, Y., and Croft., B. 1994. An association 
thesaurus for information retrieval. In RIAO, pp. 
146-160. 
Joachims, T. 2002. Optimizing search engines us-
ing clickthrough data. In SIGKDD, pp. 133-142. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Lang, H., Metzler, D., Wang, B., and Li, J-T. 2010. 
Improving latent concept expansion using 
markov random fields. In CIKM, pp. 249-258. 
Lavrenko, V., and Croft, B. 2001. Relevance-based 
language models. In SIGIR, pp. 120-128. 
Lease, M. 2009. An improved markov random 
field model for supporting verbose queries. In 
SIGIR, pp. 476-483 
Metzler, D., and Croft, B. 2005. A markov random 
field model for term dependencies. In SIGIR, pp. 
472-479. 
Metzler, D., and Croft, B. 2007. Latent concept 
expansion using markov random fields. In 
SIGIR, pp. 311-318. 
Morgan, W., Greiff, W., and Henderson, J.  2004.  
Direct maximization of average precision by 
hill-climbing with a comparison to a maximum 
entropy approach.  Technical report.  MITRE. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Prager, J., Chu-Carroll, J., and Czuba, K. 2001. 
Use of Wordnet hypernyms for answering what 
is questions. In TREC 10. 
Press, W. H., Teukolsky, S. A., Vetterling, W. T., 
and Flannery, B. P. 1992. Numerical Recipes in 
C. Cambridge Univ. Press. 
Rocchio, J. 1971. Relevance feedback in infor-
mation retrieval. In The SMART retrieval system: 
experiments in automatic document processing, 
pp. 313-323, Prentice-Hall Inc. 
Riezler, S., Liu, Y. and Vasserman, A. 2008. 
Translating queries into snippets for improving 
query expansion. In COLING 2008. 737-744. 
Riezler, S., and Liu, Y. 2010. Query rewriting us-
ing monolingual statistical machine translation. 
Computational Linguistics, 36(3): 569-582. 
Wen, J., Nie, J-Y., and Zhang, H. 2002. Query 
clustering using user logs. ACM TOIS, 20(1): 59-
81. 
Xu, J., and Croft, B. 1996. Query expansion using 
local and global document analysis. In SIGIR. 
Xue, X., Jeon, J., Croft, W. B. 2008. Retrieval 
models for Question and answer archives.  In 
SIGIR, pp. 475-482. 
Zhai, C., and Lafferty, J. 2001a. Model-based 
feedback in the kl-divergence retrieval model. In 
CIKM, pp. 403-410. 
Zhai, C., and Lafferty, J. 2001b. A study of 
smoothing methods for language models applied 
to ad hoc information retrieval. In SIGIR, pp. 
334-342. 
676
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Modeling Interestingness with Deep Neural Networks 
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.com 
 
 
Abstract 
This paper presents a deep semantic simi-
larity model (DSSM), a special type of 
deep neural networks designed for text 
analysis, for recommending target docu-
ments to be of interest to a user based on a 
source document that she is reading. We 
observe, identify, and detect naturally oc-
curring signals of interestingness in click 
transitions on the Web between source and 
target documents, which we collect from 
commercial Web browser logs. The DSSM 
is trained on millions of Web transitions, 
and maps source-target document pairs to 
feature vectors in a latent space in such a 
way that the distance between source doc-
uments and their corresponding interesting 
targets in that space is minimized. The ef-
fectiveness of the DSSM is demonstrated 
using two interestingness tasks: automatic 
highlighting and contextual entity search. 
The results on large-scale, real-world da-
tasets show that the semantics of docu-
ments are important for modeling interest-
ingness and that the DSSM leads to signif-
icant quality improvement on both tasks, 
outperforming not only the classic docu-
ment models that do not use semantics but 
also state-of-the-art topic models. 
1 Introduction 
Tasks of predicting what interests a user based on 
the document she is reading are fundamental to 
many online recommendation systems. A recent 
survey is due to Ricci et al. (2011). In this paper, 
we exploit the use of a deep semantic model for 
two such interestingness tasks in which document 
semantics play a crucial role: automatic highlight-
ing and contextual entity search. 
Automatic Highlighting. In this task we want 
a recommendation system to automatically dis-
cover the entities (e.g., a person, location, organi-
zation etc.) that interest a user when reading a doc-
ument and to highlight the corresponding text 
spans, referred to as keywords afterwards. We 
show in this study that document semantics are 
among the most important factors that influence 
what is perceived as interesting to the user. For 
example, we observe in Web browsing logs that 
when a user reads an article about a movie, she is 
more likely to browse to an article about an actor 
or character than to another movie or the director. 
Contextual entity search. After identifying 
the keywords that represent the entities of interest 
to the user, we also want the system to recommend 
new, interesting documents by searching the Web 
for supplementary information about these enti-
ties. The task is challenging because the same key-
words often refer to different entities, and interest-
ing supplementary information to the highlighted 
entity is highly sensitive to the semantic context. 
For example, ?Paul Simon? can refer to many peo-
ple, such as the singer and the senator. Consider 
an article about the music of Paul Simon and an-
other about his life. Related content about his up-
coming concert tour is much more interesting in 
the first context, while an article about his family 
is more interesting in the second. 
At the heart of these two tasks is the notion of 
interestingness. In this paper, we model and make 
use of this notion of interestingness with a deep 
semantic similarity model (DSSM). The model, 
extending from the deep neural networks shown 
recently to be highly effective for speech recogni-
tion (Hinton et al., 2012; Deng et al., 2013) and 
computer vision (Krizhevsky et al., 2012; Mar-
koff, 2014), is semantic because it maps docu-
ments to feature vectors in a latent semantic space, 
also known as semantic representations. The 
model is deep because it employs a neural net-
work with several hidden layers including a spe-
cial convolutional-pooling structure to identify 
keywords and extract hidden semantic features at 
different levels of abstractions, layer by layer. The 
semantic representation is computed through a 
deep neural network after its training by back-
propagation with respect to an objective tailored 
2
to the respective interestingness tasks. We obtain 
naturally occurring ?interest? signals by observ-
ing Web browser transitions, from a source docu-
ment to a target document, in Web usage logs of a 
commercial browser. Our training data is sampled 
from these transitions. 
The use of the DSSM to model interestingness 
is motivated by the recent success of applying re-
lated deep neural networks to computer vision 
(Krizhevshy et al. 2012; Markoff, 2014), speech 
recognition (Hinton et al. 2012), text processing 
(Collobert et al. 2011),  and Web search (Huang 
et al. 2013). Among them, (Huang et al. 2013) is 
most relevant to our work. They also use a deep 
neural network to map documents to feature vec-
tors in a latent semantic space. However, their 
model is designed to represent the relevance be-
tween queries and documents, which differs from 
the notion of interestingness between documents 
studied in this paper. It is often the case that a user 
is interested in a document because it provides 
supplementary information about the entities or 
concepts she encounters when reading another 
document although the overall contents of the sec-
ond documents is not highly relevant. For exam-
ple, a user may be interested in knowing more 
about the history of University of Washington af-
ter reading the news about President Obama?s 
visit to Seattle. To better model interestingness, 
we extend the model of Huang et al. (2013) in two 
significant aspects. First, while Huang et al. treat 
a document as a bag of words for semantic map-
ping, the DSSM treats a document as a sequence 
of words and tries to discover prominent key-
words. These keywords represent the entities or 
concepts that might interest users, via the convo-
lutional and max-pooling layers which are related 
to the deep models used for computer vision 
(Krizhevsky et al., 2013) and speech recognition 
(Deng  et al., 2013a) but are not used in Huang et 
al.?s model. The DSSM then forms the high-level 
semantic representation of the whole document 
based on these keywords. Second, instead of di-
rectly computing the document relevance score 
using cosine similarity in the learned semantic 
space, as in Huang et al. (2013), we feed the fea-
tures derived from the semantic representations of 
documents to a ranker which is trained in a super-
vised manner. As a result, a document that is not 
highly relevant to another document a user is read-
ing (i.e., the distance between their derived feature 
                                                          
1 We stress here that, although the click signal is available to 
form a dataset and a gold standard ranker (to be described in 
vectors is big) may still have a high score of inter-
estingness because the former provides useful in-
formation about an entity mentioned in the latter. 
Such information and entity are encoded, respec-
tively, by (some subsets of) the semantic features 
in their corresponding documents. In Sections 4 
and 5, we empirically demonstrate that the afore-
mentioned two extensions lead to significant qual-
ity improvements for the two interestingness tasks 
presented in this paper.  
Before giving a formal description of the 
DSSM in Section 3, we formally define the inter-
estingness function, and then introduce our data 
set of naturally occurring interest signals. 
2 The Notion of Interestingness 
Let ?  be the set of all documents. Following 
Gamon et al. (2013), we formally define the inter-
estingness modeling task as learning the mapping 
function: 
	 ?: ? ? ? ? ??	 		
where the function ???, ?? is the quantified degree 
of interest that the user has  in the target document 
? ? ? after or while reading the source document 
? ? ?. 
Our notion of a document is meant in its most 
general form as a string of raw unstructured text. 
That is, the interestingness function should not 
rely on any document structure such as title tags, 
hyperlinks, etc., or Web interaction data. In our 
tasks, documents can be formed either from the 
plain text of a webpage or as a text span in that 
plain text, as will be discussed in Sections 4 and 5. 
2.1 Data 
We can observe many naturally occurring mani-
festations of interestingness on the Web. For ex-
ample, on Twitter, users follow shared links em-
bedded in tweets. Arguably the most frequent sig-
nal, however, occurs in Web browsing events 
where users click from one webpage to another 
via hyperlinks. When a user clicks on a hyperlink, 
it is reasonable to assume that she is interested in 
learning more about the anchor, modulo cases of 
erroneous clicks. Aggregate clicks can therefore 
serve as a proxy for interestingness. That is, for a 
given source document, target documents that at-
tract the most clicks are more interesting than doc-
uments that attract fewer clicks1.  
Section 4), our task is to model interestingness between un-
structured documents, i.e., without access to any document 
structure or Web interaction data. Thus, in our experiments, 
3
We collect a large dataset of user browsing 
events from a commercial Web browser. Specifi-
cally, we sample 18 million occurrences of a user 
click from one Wikipedia page to another during 
a one year period. We restrict our browsing events 
to Wikipedia since its pages tend to contain many 
anchors (79 on average, where on average 42 have 
a unique target URL). Thus, they attract enough 
traffic for us to obtain robust browsing transition 
data2. We group together all transitions originat-
ing from the same page and randomly hold out 
20% of the transitions for our evaluation data 
(EVAL), 20% for training the DSSM described in 
Section 3.2 (TRAIN_1), and the remaining 60% 
for training our task specific rankers described in 
Section 3.3 (TRAIN_2). In our experiments, we 
used different settings for the two interestingness 
tasks. Thus, we postpone the detailed description 
of these datasets and other task-specific datasets 
to Sections 4 and 5. 
3 A Deep Semantic Similarity Model 
(DSSM) 
This section presents the architecture of the 
DSSM, describes the parameter estimation, and 
the way the DSSM is used in our tasks. 
                                                          
we remove all structural information (e.g., hyperlinks and 
XML tags) in our documents, except that in the highlighting 
experiments (Section 4) we use anchor texts to simulate the 
candidate keywords to be highlighted. We then convert each 
3.1 Network Architecture 
The heart of the DSSM is a deep neural network 
with convolutional structure, as shown in Figure 
1. In what follows, we use lower-case bold letters, 
such as ?, to denote column vectors, ???? to de-
note the ??? element of ?, and upper-case letters, 
such as ?, to denote matrices. 
Input Layer ?. It takes two steps to convert a doc-
ument ?, which is a sequence of words, into a vec-
tor representation ? for the input layer of the net-
work: (1) convert each word in ? to a word vector, 
and (2) build ? by concatenating these word vec-
tors. To convert a word ? into a word vector, we 
first represent ? by a one-hot vector using a vo-
cabulary that contains ?  high frequent words 
(? ? 150K in this study). Then, following Huang 
et al. (2013), we map ? to a separate tri-letter vec-
tor. Consider the word ?#dog#?, where # is a word 
boundary symbol. The nonzero elements in its tri-
letter vector are ?#do?, ?dog?, and ?og#?. We then 
form the word vector of ? by concatenating its 
one-hot vector and its tri-letter vector. It is worth 
noting that the tri-letter vector complements the 
one-hot vector representation in two aspects. First, 
different OOV (out of vocabulary) words can be 
represented by tri-letter vectors with few colli-
sions. Second, spelling variations of the same 
word can be mapped to the points that are close to 
each other in the tri-letter space. Although the 
number of unique English words on the Web is 
extremely large, the total number of distinct tri-
letters in English is limited (restricted to the most 
frequent 30K in this study). As a result, incorpo-
rating tri-letter vectors substantially improves the 
representation power of word vectors while keep-
ing their size small.  
To form our input layer ? using word vectors, 
we first identify a text span with a high degree of 
relevance, called focus, in ?  using task-specific 
heuristics (see Sections 4 and 5 respectively). Sec-
ond, we form ? by concatenating each word vec-
tor in the focus and a vector that is the summation 
of all other word vectors, as shown in Figure 1. 
Since the length of the focus is much smaller than 
that of its document, ? is able to capture the con-
textual information (for the words in the focus) 
Web document into plain text, which is white-space to-
kenized and lowercased. Numbers are retained and no stem-
ming is performed. 
2 We utilize the May 3, 2013 English Wikipedia dump con-
sisting of roughly 4.1 million articles from http://dumps.wiki-
media.org. 
Figure 1: Illustration of the network architec-
ture and information flow of the DSSM 
 
4
useful to the corresponding tasks, with a manage-
able vector size. 
Convolutional Layer ? . A convolutional layer 
extracts local features around each word ??	in a 
word sequence of length ?  as follows. We first 
generate a contextual vector ??  by concatenating 
the word vectors of ?? and its surrounding words defined by a window (the window size is set to 3 
in this paper). Then, we generate for each word a 
local feature vector ??  using a tanh  activation 
function and a linear projection matrix ??, which 
is the same across all windows ? in the word se-
quence, as: 
?? ? tanh??????? , where	? ? 1? 1) ?) 
Max-pooling Layer ?. The size of the output ? 
depends on the number of words in the word se-
quence. Local feature vectors have to be com-
bined to obtain a global feature vector, with a 
fixed size independent of the document length, in 
order to apply subsequent standard affine layers. 
We design ? by adopting the max operation over 
each ?time? ? of the sequence of vectors computed 
by (1), which forces the network to retain only the 
most useful, partially invariant local features pro-
duced by the convolutional layer: 
???? ? max???,?,??u????? (2) 
where the max operation is performed for each di-
mension of ? across ? ? 1,? , ? respectively.  
That convolutional and max-pooling layers are 
able to discover prominent keywords of a docu-
ment can be demonstrated using the procedure in 
Figure 2 using a toy example. First, the convolu-
tional layer of (1) generates for each word in a 5-
word document a 4-dimensional local feature vec-
tor, which represents a distribution of four topics. 
For example, the most prominent topic of ?? within its three word context window is the first 
topic, denoted by ???1?, and the most prominent 
topic of ?? is ???3?. Second, we use max-pooling of (2) to form a global feature vector, which rep-
resents the topic distribution of the whole docu-
ment. We see that ??1? and ??3? are two promi-
nent topics. Then, for each prominent topic, we 
trace back to the local feature vector that survives 
max-pooling: 
??1? ? max???,?,?????1?? ? ???1?  
??3? ? max???,?,?????3?? ? ???3?.  
Finally, we label the corresponding words of these 
local feature vectors, ?? and ??, as keywords of the document.  
Figure 3 presents a sample of document snip-
pets and their keywords detected by the DSSM ac-
cording to the procedure elaborated in Figure 2. It 
is interesting to see that many names are identified 
as keywords although the DSSM is not designed 
explicitly for named entity recognition. 
Fully-Connected Layers ?  and ? . The fixed 
sized global feature vector ? of (2) is then fed to 
several standard affine network layers, which are 
stacked and interleaved with nonlinear activation 
functions, to extract highly non-linear features ? 
at the output layer. In our model, shown in Figure 
1, we have: 
? ? tanh?????? (3) 
? ? tanh?????? (4) 
where ?? and ?? are learned linear projection matri-ces. 
3.2 Training the DSSM 
To optimize the parameters of the DSSM of Fig-
ure 1, i.e., ? ? ???,??,???, we use a pair-wise rank loss as objective (Yih et al. 2011). Consider 
a source document ?  and two candidate target 
documents ??	and ??, where ?? is more interesting 
than ??  to a user when reading ?. We construct 
two pairs of documents ??, ??? and ??, ???, where the former is preferred and should have a higher 
u1 u2 u3 u4 u5
w1 w2 w3 w4 w5
2
3
4
1
 
w1 w2 w3 w4 w5
v
2
3
4
1
Figure 2: Toy example of (upper) a 5-word 
document and its local feature vectors ex-
tracted using a convolutional layer, and (bot-
tom) the global feature vector of the document 
generated after max-pooling. 
 
 
5
interestingness score. Let ? be the difference of 
their interestingness scores: ?	? ???, ??? ?
???, ??? , where ?  is the interestingness score, computed as the cosine similarity: 
???, ?? ? sim???, ?? ?
?????
???????? 
(5) 
where ?? and ?? are the feature vectors of ? and ?, respectively, which are generated using the 
DSSM, parameterized by ?. Intuitively, we want 
to learn ? to maximize ?. That is, the DSSM is 
learned to represent documents as points in a hid-
den interestingness space, where the similarity be-
tween a document and its interesting documents is 
maximized.  
We use the following logistic loss over ? , 
which can be shown to upper bound the pairwise 
accuracy: 
???; ?? ? log?1 ? exp?????? (6) 
                                                          
3 In our experiments, we observed better results by sampling 
more negative training examples (e.g., up to 100) although 
this makes the training much slower. An alternative approach 
The loss function in (6) has a shape similar to the 
hinge loss used in SVMs. Because of the use of 
the cosine similarity function, we add a scaling 
factor ? that magnifies ? from [-2, 2] to a larger 
range. Empirically, the value of ? makes no dif-
ference as long as it is large enough. In the exper-
iments, we set ? ? 10. Because the loss function 
is differentiable, optimizing the model parameters 
can be done using gradient-based methods. Due to 
space limitations, we omit the derivation of the 
gradient of the loss function, for which readers are 
referred to related derivations (e.g., Collobert et 
al. 2011; Huang et al. 2013; Shen et al. 2014). 
In our experiments we trained DSSMs using 
mini-batch Stochastic Gradient Descent. Each 
mini-batch consists of 256 source-target docu-
ment pairs. For each source document ?, we ran-
domly select from that batch four target docu-
ments which are not paired with ?  as negative 
training samples3. The DSSM trainer is imple-
mented using a GPU-accelerated linear algebra li-
brary, which is developed on CUDA 5.5. Given 
the training set (TRAIN_1 in Section 2), it takes 
approximately 30 hours to train a DSSM as shown 
in Figure 1, on a Xeon E5-2670 2.60GHz machine 
with one Tesla K20 GPU card. 
In principle, the loss function of (6) can be fur-
ther regularized (e.g. by adding a term of 2? norm) 
to deal with overfitting. However, we did not find 
a clear empirical advantage over the simpler early 
stop approach in a pilot study, hence we adopted 
the latter in the experiments in this paper. Our ap-
proach adjusts the learning rate ?  during the 
course of model training. Starting with ? ? 1.0, 
after each epoch (a pass over the entire training 
data), the learning rate is adjusted as ? ? 0.5 ? ? 
if the loss on validation data (held-out from 
TRAIN_1) is not reduced. The training stops if 
either ?  is smaller than a preset threshold 
(0.0001) or the loss on training data can no longer 
be reduced significantly. In our experiments, the 
DSSM training typically converges within 20 
epochs. 
3.3 Using the DSSM 
We experiment with two ways of using the DSSM 
for the two interestingness tasks. First, we use the 
DSSM as a feature generator. The output layer of 
the DSSM can be seen as a set of semantic fea-
tures, which can be incorporated in a boosted tree 
is to approximate the partition function using Noise Contras-
tive Estimation (Gutmann and Hyvarinen 2010). We leave it 
to future work.  
? the comedy festival formerly known as 
the us comedy arts festival is a comedy 
festival held each year in las vegas 
nevada from its 1985 inception to 2008 
. it was held annually at the wheeler 
opera house and other venues in aspen 
colorado . the primary sponsor of the 
festival was hbo with co-sponsorship by 
caesars palace . the primary venue tbs 
geico insurance twix candy bars and 
smirnoff vodka hbo exited the festival 
business in 2007 and tbs became the pri-
mary sponsor the festival includes 
standup comedy performances appearances 
by the casts of television shows? 
 
? bad samaritans is an american comedy
series produced by walt becker kelly
hayes and ross putman . it premiered on 
netflix on march 31 2013 cast and char-
acters . the show focuses on a community 
service parole group and their parole 
officer brian kubach as jake gibson an 
aspiring professional starcraft player 
who gets sentenced to 2000 hours of com-
munity service for starting a forest 
fire during his breakup with drew prior 
to community service he had no real am-
bition in life other than to be a pro-
fessional gamer and become wealthy 
overnight like mark zuckerberg as in 
life his goal during ? 
Figure 3: A sample of document snippets and 
the keywords (in bold) detected by the DSSM. 
 
 
6
based ranker (Friedman 1999) trained discrimina-
tively on the task-specific data. Given a source-
target document pair ??, ??, the DSSM generates 
600 features (300 from the output layers ?? and ?? 
for each ? and ?, respectively). 
Second, we use the DSSM as a direct imple-
mentation of the interestingness function ?. Re-
call from Section 3.2 that in model training, we 
measure the interestingness score for a document 
pair using the cosine similarity between their cor-
responding feature vectors (?? and ??). Similarly 
at runtime, we define	? ? 	sim???, ?? as (5). 
4 Experiments on Highlighting 
Recall from Section 1 that in this task, a system 
must select ? most interesting keywords in a doc-
ument that a user is reading. To evaluate our mod-
els using the click transition data described in Sec-
tion 2, we simulate the task as follows. We use the 
set of anchors in a source document ? to simulate 
the set of candidate keywords that may be of in-
terest to the user while reading ?, and treat the text 
of a document that is linked by an anchor in ? as a 
target document ?. As shown in Figure 1, to apply 
DSSM to a specific task, we need to define the fo-
cus in source and target documents. In this task, 
the focus in s is defined as the anchor text, and the 
focus in t is defined as the first 10 tokens in t. 
We evaluate the performance of a highlighting 
system against a gold standard interestingness 
function ?? which scores the interestingness of an 
anchor as the number of user clicks on ? from the 
anchor in ? in our data. We consider the ideal se-
lection to then consist of the ?  most interesting 
anchors according to ??. A natural metric for this 
task is Normalized Discounted Cumulative Gain 
(NDCG) (Jarvelin and Kekalainen 2000). 
We evaluate our models on the EVAL dataset 
described in Section 2. We utilize the transition 
distributions in EVAL to create three other test 
sets, following the stratified sampling methodol-
ogy commonly employed in the IR community, 
for the frequently, less frequently, and rarely 
viewed source pages, referred to as HEAD, 
TORSO, and TAIL, respectively. We obtain 
these sets by first sorting the unique source docu-
ments according to their frequency of occurrence 
in EVAL. We then partition the set so that HEAD 
corresponds to all transitions from the source 
pages at the top of the list that account for 20% of 
the transitions in EVAL; TAIL corresponds to the 
transitions at the bottom also accounting for 20% 
of the transitions in EVAL; and TORSO corre-
sponds to the remaining transitions. 
4.1 Main Results 
Table 1 summarizes the results of various models 
over the three test sets using NDCG at truncation 
levels 1, 5, and 10. 
Rows 1 to 3 are simple heuristic baselines. 
RAND selects ?  random anchors, 1stK selects 
the first ? anchors and LastK the last ? anchors.  
The other models in Table 1 are boosted tree 
based rankers trained on TRAIN_2 described in 
Section 2. They vary only in their features. The 
ranker in Row 4 uses Non-Semantic Features 
(NSF) only. These features are derived from the 
 # Models HEAD TORSO TAIL 
   @1 @5 @10 @1 @5 @10 @1 @5 @10 
src
  o
nly
 
1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.258 
2 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.348 
3 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.219
4 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.365 
5 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.386 
6 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.372 
8 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382 
src
+ta
r 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.380 
12 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398 
Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test (? ?
0.05). 
 
 
7
source document s and from user session infor-
mation in the browser log. The document features 
include: position of the anchor in the document, 
frequency of the anchor, and anchor density in the 
paragraph.  
The rankers in Rows 5 to 12 use the NSF and 
the semantic features computed from source and 
target documents of a browsing transition. We 
compare semantic features derived from three dif-
ferent sources. The first feature source comes 
from our DSSMs (DSSM and DSSM_BOW) us-
ing the output layers as feature generators as de-
scribed in Section 3.3. DSSM is the model de-
scribed in Section 3 and DSSM_BOW is the 
model proposed by Huang et al. (2013) where 
documents are view as bag of words (BOW) and 
the convolutional and max-pooling layers are not 
used. The two other sources of semantic features 
are used as a point of comparison to the DSSM. 
One is a generative semantic model (Joint Transi-
tion Topic model, or JTT) (Gamon et al. 2013). 
JTT is an LDA-style model (Blei et al. 2003) that 
is trained jointly on source and target documents 
linked by browsing transitions. JTT generates a 
total of 150 features from its latent variables, 50 
each for the source topic model, the target topic 
model and the transition model. The other seman-
tic model of contrast is a manually defined one, 
which we use to assess the effectiveness of auto-
matically learned models against human model-
ers. To this effect, we use the page categories that 
editors assign in Wikipedia as semantic features 
(WCAT). These features number in the multiple 
thousands. Using features such as WCAT is not a 
viable solution in general since Wikipedia catego-
ries are not available for all documents. As such, 
we use it solely as a point of comparison against 
DSSM and JTT. 
We also distinguish between two types of 
learned rankers: those which draw their features 
only from the source (src only) document and 
those that draw their features from both the source 
and target (src+tar) documents. Although our 
task setting allows access to the content of both 
source and target documents, there are practical 
scenarios where a system should predict what in-
terests the user without looking at the target doc-
ument because the extra step of identifying a suit-
able target document for each candidate concept 
or entity of interest is computationally expensive.  
4.2 Analysis of Results 
As shown in Table 1, NSF+DSSM, which incor-
porates our DSSM, is the overall best performing 
system across test sets. The task is hard as evi-
denced by the weak baseline scores. One reason is 
the large average number of candidates per page. 
On HEAD, we found an average of 170 anchors 
(of which 95 point to a unique target URL). For 
TORSO and TAIL, we found the average number 
of anchors to be 94 (52 unique targets) and 41 (19 
unique targets), respectively. 
Clearly, the semantics of the documents form 
important signals for this task: WCAT, JTT, 
DSSM_BOW, and DSSM all significantly boost 
the performance over NSF alone. There are two 
interesting comparisons to consider: (a) manual 
semantics vs. learned semantics; and (b) deep se-
mantic models vs. generative topic models. On 
(a), we observe somewhat surprisingly that the 
learned DSSM produces features that outperform 
the thousands of features coming from manually 
(editor) assigned Wikipedia category features 
(WCAT), in all but the TAIL where the two per-
form statistically the same. In contrast, features 
from the generative model (JTT) perform worse 
than WCAT across the board except on TAIL 
where JTT and WCAT are statistically tied. On 
(b), we observe that DSSM outperforms a state-
of-the-art generative model (JTT) on HEAD and 
TORSO. On TAIL, they are statistically indistin-
guishable. 
We turn now to inspecting the scenario where 
features are only drawn from the source document 
(Rows 1-8 in Table 1). Again we observe that se-
mantic features significantly boost the perfor-
mance against NSF alone, however they signifi-
cantly deteriorate when compared to using fea-
tures from both source and target documents. In 
this scenario, the manual semantics from WCAT 
outperform all other models, but with a diminish-
ing effect as we move from HEAD through 
TORSO to TAIL. DSSM is the best performing 
learned semantic model. 
Finally, we present the results to justify the two 
modifications we made to extend the model of 
Huang et al. (2013) to the DSSM, as described in 
Section 1. First, we see in Table 1 that 
DSSM_BOW, which has the same network struc-
ture of Huang et al.?s model, is much weaker than 
DSSM, demonstrating the benefits of using con-
volutional and max-pooling layers to extract se-
mantic features for the highlighting task. Second, 
we conduct several experiments by using the co-
sine scores between the output layers of DSSM 
for ? and ? as features (following the procedure in 
Section 3.3 for using the DSSM as a direct imple-
mentation of ?). We found that adding the cosine 
8
features to NSF+DSSM does not lead to any im-
provement. We also combined NSF with solely 
the cosine features from DSSM (i.e., without the 
other semantic features drawn from its output lay-
ers). But we still found no improvement over us-
ing NSF alone. Thus, we conclude that for this 
task it is much more effective to feed the features 
derived from DSSM to a supervised ranker than 
directly computing the interestingness score using 
cosine similarity in the learned semantic space, as 
in Huang et al. (2013). 
5 Experiments on Entity Search 
We construct the evaluation data set for this sec-
ond task by randomly sampling a set of documents 
from a traffic-weighted set of Web documents. In 
a second step, we identify the entity names in each 
document using an in-house named entity recog-
nizer. We issue each entity name as a query to a 
commercial search engine, and retain up to the 
top-100 retrieved documents as candidate target 
documents. We form for each entity a source doc-
ument which consists of the entity text and its sur-
rounding text defined by a 200-word window. We 
define the focus (as in Figure 1) in ? as the entity 
text, and the focus in ? as the first 10 tokens in ?. 
The final evaluation data set contains 10,000 
source documents. On average, each source docu-
ment is associated with 87 target documents. Fi-
nally, the source-target document pairs are labeled 
in terms of interestingness by paid annotators. The 
label is on a 5-level scale, 0 to 4, with 4 meaning 
the target document is the most interesting to the 
source document and 0 meaning the target is of no 
interest. 
We test our models on two scenarios. The first 
is a ranking scenario where ?  interesting docu-
ments are displayed to the user. Here, we select 
the top-? ranked documents according to their in-
terestingness scores. We measure the performance 
via NDCG at truncation levels 1 and 3. The sec-
ond scenario is to display to the user all interesting 
results. In this scenario, we select all target docu-
ments with an interestingness score exceeding a 
predefined threshold. We evaluate this scenario 
using ROC analysis and, specifically, the area un-
der the curve (AUC). 
5.1 Main Results 
The main results are summarized in Table 2. Rows 
1 to 6 are single model results, where each model 
is used as a direct implementation of the interest-
ingness function ?. Rows 7 to 9 are ranker results, 
where ? is defined as a boosted tree based ranker 
that incorporates different sets of features ex-
tracted from source and target documents, includ-
ing the features derived from single models. As in 
the highlighting experiments, all the machine-
learned single models, including the DSSM, are 
trained on TRAIN_1, and all the rankers are 
trained on TRAIN_2. 
5.2 Analysis of Results 
BM25 (Rows 1 and 2 in Table 2) is the classic 
document model (Robertson and Zaragoza 2009). 
It uses the bag-of-words document representation 
and the BM25 term weighting function. In our set-
ting, we define the interestingness score of a doc-
ument pair as the dot product of their BM25-
weighted term vectors. To verify the importance 
of using contextual information, we compare two 
different ways of forming the term vector of a 
source document. The first only uses the entity 
text (Row 1). The second (Row 2) uses both the 
entity text and and its surrounding text in a 200-
word window (i.e., the entire source document). 
Results show that the model using contextual in-
formation is significantly better. Therefore, all the 
other models in this section use both the entity 
texts and their surrounding text. 
WTM (Row 3) is our implementation of the 
word translation model for IR (Berger and Laf-
ferty 1999; Gao et al. 2010). WTM defines the in-
terestingness score as: 
???, ?? ? ? ? ????|???????|?????????? ,  
# Models @1 @3 AUC 
1 BM25 (entity)  0.133 0.195 0.583 
2 BM25 0.142 0.227 0.675 
3 WTM 0.191 0.287 0.678 
4 BLTM 0.214 0.306 0.704 
5 DSSM 0.259* 0.356* 0.711* 
6 DSSM_BOW 0.223 0.322 0.699 
7 Baseline ranker 0.283 0.360 0.723 
8 7 + DSSM(1) 0.301# 0.385# 0.758# 
9 7 + DSSM(600) 0.327## 0.402## 0.782##
Table 2: Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test (? ?
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. 
 
 
 
9
where ????|?? is the unigram probability of word 
?? in ?, and ????|??? is the probability of trans-
lating ?? into ??, trained on source-target docu-ment pairs using EM (Brown et al. 1993). The 
translation-based approach allows any pair of 
non-identical but semantically related words to 
have a nonzero matching score. As a result, it sig-
nificantly outperforms BM25. 
BTLM (Row 4) follows the best performing 
bilingual topic model described in Gao et al. 
(2011), which is an extension of PLSA (Hofmann 
1999). The model is trained on source-target doc-
ument pairs using the EM algorithm with a con-
straint enforcing a source document ? and its tar-
get document ? to not only share the same prior 
topic distribution, but to also have similar frac-
tions of words assigned to each topic. BLTM de-
fines the interestingness score between s and t as: 
???, ?? ? ? ? ????|??????|???????? .  
The model assumes the following story of gener-
ating ? from ?. First, for each topic ? a word dis-
tribution ?? is selected from a Dirichlet prior with 
concentration parameter ? . Second, given ? , a 
topic distribution ??  is drawn from a Dirichlet 
prior with parameter ? . Finally, ?  is generated 
word by word. Each word ?? is generated by first 
selecting a topic ?  according to ?? , and then 
drawing a word from ?? . We see that BLTM models interestingness by taking into account the 
semantic topic distribution of the entire docu-
ments. Our results in Table 2 show that BLTM 
outperforms WTM by a significant margin in 
both NDCG and AUC. 
DSSM (Row 5) outperforms all the competing 
single models, including the state-of-the-art topic 
model BLTM. Now, we inspect the difference be-
tween DSSM and BLTM in detail. Although both 
models strive to generate the semantic representa-
tion of a document, they use different modeling 
approaches. BLTM by nature is a generative 
model. The semantic representation in BLTM is a 
distribution of hidden semantic topics. Such a dis-
tribution is learned using Maximum Likelihood 
Estimation in an unsupervised manner, i.e., max-
imizing the log-likelihood of the source-target 
document pairs in the training data. On the other 
hand, DSSM represents documents as points in a 
hidden semantic space using a supervised learning 
method, i.e., paired documents are closer in that 
latent space than unpaired ones. We believe that 
the superior performance of DSSM is largely due 
to the fact that the model parameters are discrimi-
natively trained using an objective that is tailored 
to the interestingness task.  
In addition to the difference in training meth-
ods, DSSM and BLTM also use different model 
structures. BLTM treats a document as a bag of 
words (thus losing some important contextual in-
formation such as word order and inter-word de-
pendencies), and generates semantic representa-
tions of documents using linear projection. 
DSSM, on the other hand, treats text as a sequence 
of words and better captures local and global con-
text, and generates highly non-linear semantic 
features via a deep neural network. To further ver-
ify our analysis, we inspect the results of a variant 
of DSSM, denoted as DSSM_BOW (Row 6), 
where the convolution and max-pooling layers are 
removed. This model treats a document as a bag 
of words, just like BLTM. These results demon-
strate that the effectiveness of DSSM can also be 
attributed to the convolutional architecture in the 
neural network, in addition to being deep and be-
ing discriminative. 
We turn now to discussing the ranker results in 
Rows 7 to 9. The baseline ranker (Row 7) uses 158 
features, including many counts and single model 
scores, such as BM25 and WMT. DSSM (Row 5) 
alone is quite effective, being close in perfor-
mance to the baseline ranker with non-DSSM fea-
tures. Integrating the DSSM score computed in (5) 
as one single feature into the ranker (Row 8) leads 
to a significant improvement over the baseline. 
The best performing combination (Row 9) is ob-
tained by incorporating the DSSM feature vectors 
of source and target documents (i.e., 600 features 
in total) in the ranker. 
We thus conclude that on both tasks, automatic 
highlighting and contextual entity search, features 
drawn from the output layers of our deep semantic 
model result in significant gains after being added 
to a set of non-semantic features, and in compari-
son to other types of semantic models used in the 
past. 
6 Related Work 
In addition to the notion of relevance as described 
in Section 1, related to interestingness is also the 
notion of salience (also called aboutness) (Gamon 
et al. 2013; 2014; Parajpe 2009; Yih et al. 2006). 
Salience is the centrality of a term to the content 
of a document. Although salience and interesting-
ness interact, the two are not the same. For exam-
ple, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average 
user would probably not be interested in learning 
more about Obama while reading that article.  
10
There are many systems that identify popular 
content in the Web or recommend content (e.g., 
Bandari et al. 2012; Lerman and Hogg 2010; 
Szabo and Huberman 2010), which is closely re-
lated to the highlighting task. In contrast to these 
approaches, we strive to predict what term a user 
is likely to be interested in when reading content, 
which may or may not be the same as the most 
popular content that is related to the current docu-
ment. It has empirically been demonstrated in 
Gamon et al. (2013) that popularity is in fact a ra-
ther poor predictor for interestingness. The task of 
contextual entity search, which is formulated as an 
information retrieval problem in this paper, is also 
related to research on entity resolution (Stefanidis 
et al. 2013).  
Latent Semantic Analysis (Deerwester et al. 
1990) is arguably the earliest semantic model de-
signed for IR. Generative topic models widely 
used for IR include PLSA (Hofmann 1990) and 
LDA (Blei et al. 2003). Recently, these models 
have been extended to handle cross-lingual cases, 
where there are pairs of corresponding documents 
in different languages (e.g., Dumais et al. 1997; 
Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). 
By exploiting deep architectures, deep learning 
techniques are able to automatically discover from 
training data the hidden structures and the associ-
ated features at different levels of abstraction use-
ful for a variety of tasks (e.g., Collobert et al. 
2011; Hinton et al. 2012; Socher et al. 2012; 
Krizhevsky et al., 2012; Gao et al. 2014). Hinton 
and Salakhutdinov (2010) propose the most origi-
nal approach based on an unsupervised version of 
the deep neural network to discover the hierar-
chical semantic structure embedded in queries and 
documents. Huang et al. (2013) significantly ex-
tends the approach so that the deep neural network 
can be trained on large-scale query-document 
pairs giving much better performance. The use of 
the convolutional neural network for text pro-
cessing, central to our DSSM, was also described 
in Collobert et al. (2011) and Shen et al. (2014) 
but with very different applications. The DSSM 
described in Section 3 can be viewed as a variant 
of the deep neural network models used in these 
previous studies. 
7 Conclusions 
Modeling interestingness is fundamental to many 
online recommendation systems. We obtain natu-
rally occurring interest signals by observing Web 
browsing transitions where users click from one 
webpage to another. We propose to model this 
?interestingness? with a deep semantic similarity 
model (DSSM), based on deep neural networks 
with special convolutional-pooling structure, 
mapping source-target document pairs to feature 
vectors in a latent semantic space. We train the 
DSSM using browsing transitions between docu-
ments. Finally, we demonstrate the effectiveness 
of our model on two interestingness tasks: auto-
matic highlighting and contextual entity search. 
Our results on large-scale, real-world datasets 
show that the semantics of documents computed 
by the DSSM are important for modeling interest-
ingness and that the new model leads to signifi-
cant improvements on both tasks. DSSM is shown 
to outperform not only the classic document mod-
els that do not use (latent) semantics but also state-
of-the-art topic models that do not have the deep 
and convolutional architecture characterizing the 
DSSM. 
One area of future work is to extend our 
method to model interestingness given an entire 
user session, which consists of a sequence of 
browsing events. We believe that the prior brows-
ing and interaction history recorded in the session 
provides additional signals for predicting interest-
ingness. To capture such signals, our model needs 
to be extended to adequately represent time series 
(e.g., causal relations and consequences of ac-
tions). One potentially effective model for such a 
purpose is based on the architecture of recurrent 
neural networks (e.g., Mikolov et al. 2010; Chen 
and Deng, 2014), which can be incorporated into 
the deep semantic model proposed in this paper. 
Additional Authors 
Yelong Shen (Microsoft Research, One Microsoft 
Way, Redmond, WA 98052, USA, email: 
yeshen@microsoft.com). 
Acknowledgments 
The authors thank Johnson Apacible, Pradeep 
Chilakamarri, Edward Guo, Bernhard Kohlmeier, 
Xiaolong Li, Kevin Powell, Xinying Song and 
Ye-Yi Wang for their guidance and valuable dis-
cussions. We also thank the three anonymous re-
viewers for their comments. 
References 
Bandari, R., Asur, S., and Huberman, B. A. 2012. 
The pulse of news in social media: forecasting 
popularity. In ICWSM. 
11
Bengio, Y., 2009. Learning deep architectures for 
AI. Fundamental Trends in Machine Learning, 
2(1):1?127. 
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet allocation. Journal of Machine 
Learning Research, 3. 
Broder, A., Fontoura, M., Josifovski, V., and 
Riedel, L. 2007. A semantic approach to contex-
tual advertising. In SIGIR. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. 
J., and Mercer, R. L. 1993. The mathematics of 
statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263-
311. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML, pp. 89-96.  
Chen, J. and Deng, L. 2014. A primal-dual method 
for training recurrent neural networks con-
strained by the echo-state property. In ICLR. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P., 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 12. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Indexing 
by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
Deng, L., Hinton, G., and Kingsbury, B. 2013. 
New types of deep neural network learning for 
speech recognition and related applications: An 
overview. In ICASSP. 
Deng, L., Abdel-Hamid, O., and Yu, D., 2013a. A 
deep convolutional neural network using heter-
ogeneous pooling for trading acoustic invari-
ance with phonetic confusion. In ICASSP. 
Dumais, S. T., Letsche, T. A., Littman, M. L., and 
Landauer, T. K. 1997. Automatic cross-linguis-
tic information retrieval using latent semantic 
indexing. In AAAI-97 Spring Symposium Series: 
Cross-Language Text and Speech Retrieval. 
Friedman, J. H. 1999. Greedy function approxi-
mation: a gradient boosting machine. Annals of 
Statistics, 29:1189-1232. 
Gamon, M., Mukherjee, A., Pantel, P. 2014. Pre-
dicting interesting things in text. In COLING. 
Gamon, M., Yano, T., Song, X., Apacible, J. and 
Pantel, P. 2013. Identifying salient entities in 
web pages. In CIKM. 
Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM. pp. 
1139-1148. 
Gao, J., He, X., Yih, W-t., and Deng, L. 2014. 
Learning continuous phrase representations for 
translation modeling. In ACL. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR. pp. 675-684.  
Graves, A., Mohamed, A., and Hinton, G. 2013. 
Speech recognition with deep recurrent neural 
networks. In ICASSP. 
Gutmann, M. and Hyvarinen, A. 2010. Noise-con-
trastive estimation: a new estimation principle 
for unnormalized statistical models. In Proc. 
Int. Conf. on Artificial Intelligence and Statis-
tics (AISTATS2010). 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, 29:82-97. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering binary codes for documents by learning 
deep generative models. Topics in Cognitive 
Science, pp. 1-18. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR. pp. 50-57. 
Huang, P., He, X., Gao, J., Deng, L., Acero, A., 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM. 
Jarvelin, K. and Kekalainen, J. 2000. IR evalua-
tion methods for retrieving highly relevant doc-
uments. In SIGIR. pp. 41-48. 
Krizhevsky, A., Sutskever, I. and Hinton, G. 
2012. ImageNet classification with deep convo-
lutional neural networks. In NIPS. 
Lerman, K., and Hogg, T. 2010. Using a model of 
social dynamics to predict popularity of news. 
In WWW. pp. 621-630. 
Markoff, J. 2014. Computer eyesight gets a lot 
more accurate. In New York Times. 
Mikolov, T.. Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In 
INTERSPEECH. pp. 1045-1048. 
Paranjpe, D. 2009. Learning document aboutness 
from implicit user feedback and document 
structure. In CIKM. 
12
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual document representations from 
discriminative projections. In EMNLP. pp. 251-
261. 
Ricci, F., Rokach, L., Shapira, B., and Kantor, P. 
B. (eds) 2011. Recommender System Handbook, 
Springer. 
Robertson, S., and Zaragoza, H. 2009. The proba-
bilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information 
Retrieval, 3(4):333-389. 
Shen, Y., He, X., Gao. J., Deng, L., and Mesnil, G. 
2014. A latent semantic model with convolu-
tional-pooling structure for information re-
trieval. In CIKM. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic compositionality through recursive 
matrix-vector spaces. In EMNLP. 
Stefanidis, K., Efthymiou, V., Herschel, M., and 
Christophides, V. 2013. Entity resolution in the 
web of data.  CIKM?13 Tutorial. 
Szabo, G., and Huberman, B. A. 2010. Predicting 
the popularity of online content. Communica-
tions of the ACM, 53(8). 
Wu, Q., Burges, C.J.C., Svore, K., and Gao, J. 
2009. Adapting boosting for information re-
trieval measures. Journal of Information Re-
trieval, 13(3):254-270. 
Yih, W., Goodman, J., and Carvalho, V. R. 2006. 
Finding advertising keywords on web pages. In 
WWW. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
 
13
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250?1260,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Large-scale Expected BLEU Training of Phrase-based Reordering Models
Michael Auli, Michel Galley, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,mgalley,jfgao}@microsoft.com
Abstract
Recent work by Cherry (2013) has shown
that directly optimizing phrase-based re-
ordering models towards BLEU can lead
to significant gains. Their approach is lim-
ited to small training sets of a few thou-
sand sentences and a similar number of
sparse features. We show how the ex-
pected BLEU objective allows us to train
a simple linear discriminative reordering
model with millions of sparse features on
hundreds of thousands of sentences re-
sulting in significant improvements. A
comparison to likelihood training demon-
strates that expected BLEU is vastly more
effective. Our best results improve a hi-
erarchical lexicalized reordering baseline
by up to 2.0 BLEU in a single-reference
setting on a French-English WMT 2012
setup.
1 Introduction
Modeling reordering for phrase-based machine
translation has been a long standing problem.
Contrary to synchronous context free grammar-
based translation models (Wu, 1997; Galley et al.,
2004; Galley et al., 2006; Chiang, 2007), phrase-
based models (Koehn et al., 2003; Och and Ney,
2004) have no in-built notion of reordering beyond
what is captured in a single phrase pair, and the
first phrase-based decoders simply scored inter-
phrase reorderings using a restricted linear dis-
tortion feature, which scores a phrase reordering
proportionally to the length of its displacement.
While phrase-based models allow in theory com-
pletely unrestricted reordering patterns, move-
ments are generally limited to a finite distance for
complexity reasons. To address this limitation,
extensive prior work focused on richer feature
sets, in particular on lexicalized reordering mod-
els trained with maximum likelihood-based ap-
proaches (Tillmann, 2003; Xiong et al., 2006; Gal-
ley and Manning, 2008; Nguyen et al.,2009;?2).
More recently, Cherry (2013) proposed a very
effective sparse ordering model relying on a set
of only a few thousand indicator features which
are trained towards a task-specific metric such as
BLEU (Papineni et al., 2002). These features
are simply added to the log-linear framework of
translation that is trained with the Margin Infused
Relaxed Algorithm (MIRA; Chiang et al., 2009)
on a small development set of a few thousand
sentences. While simple, the approach outper-
forms the state-of-the-art hierarchical reordering
model of Galley and Manning (2008), a maximum
likelihood-based model trained on millions of sen-
tences to fit millions of parameters.
Ideally, we would like to scale sparse reorder-
ing models to similar dimensions but recent at-
tempts to increase the amount of training data for
MIRA was met with little success (Eidelman et
al., 2013). In this paper we propose much larger
sparse ordering models that combine the scalabil-
ity of likelihood-based approaches with the higher
accuracy of maximum BLEU training (?3). We
train on the output of a hierarchical reordering
model-based system and scale to millions of fea-
tures learned on hundreds of thousands of sen-
tences (?4). Specifically, we use the expected
BLEU objective function (Rosti et al., 2010; Rosti
et al., 2011; He and Deng, 2012; Gao and He,
2013; Gao et al., 2014; Green et al., 2014) which
allows us to train models that use training data and
feature sets that are two to three orders of magni-
tudes larger than in previous work (?5).
Our models significantly outperform the
state-of-the-art hierarchical lexicalized reordering
model on two language pairs and we demonstrate
that richer feature sets result in significantly
higher accuracy than with a feature set similar
to Cherry (2013). We also demonstrate that our
1250
approach greatly benefits from more training
data than is typically used for maximum BLEU
training. Previous work concluded that sparse
reordering models perform better than maximum
entropy models, however, the two approaches
do not only differ in the objective function but
also the type of training data (Cherry, 2013). Our
analysis isolates the objective function and shows
that expected BLEU optimization is the most
important factor to train accurate ordering models.
Finally, we compare expected BLEU training to
pair-wise ranked optimization (PRO) on a feature
set similar to Cherry (2013; ?7).
2 Reordering Models
Reordering models for phrase-based translation
are typically part of the log-linear framework
which forms the basis of many statistical machine
translation systems (Och and Ney, 2004).
Formally, we are given K training pairs D =
(f
(1)
, e
(1)
)...(f
(K)
, e
(K)
), where each f
(i)
? F
is drawn from a set of possible foreign sentences,
and each English sentence e
(i)
? E(f
(i)
) is drawn
from a set of possible English translations of f
(i)
.
The log-linear model is parameterized by m pa-
rameters ? where each ?
k
? ? is the weight of
an associated feature h
k
(f, e) such as a language
model or a reordering model. Function h(f, e)
maps foreign and English sentences to the vector
h
1
(f, e)...h
m
(f, e), and we usually choose trans-
lations e? according to the following decision rule:
e? = arg max
e?E(f)
?
T
h(f, e) (1)
In practice, computing e? exactly is intractable and
we resort to an approximate but more efficient
beam search (Och and Ney, 2004).
Early phrase-based models simply relied on a
linear distortion feature, which measures the dis-
tance between the first word of the current source
phrase and the last word of the previous source
phrase (Koehn et al., 2003; Och and Ney, 2004).
Unfortunately, this approach is agnostic to the ac-
tual phrases being reordered, and does not take
into account that certain phrases are more likely
to be reordered than others. This shortcoming led
to a range of lexicalized reordering models that
capture exactly those preferences for individual
phrases (Tillmann, 2003; Koehn et al., 2007).
Reordering models generally assume a se-
quence of English phrases e = {e?
1
, . . . , e?
n
} cur-
rently hypothesized by the decoder, a phrase align-
ment a = {a
1
, . . . , a
n
} that defines a foreign
phrase
?
f
a
i
for each English phrase e?
i
, and an ori-
entation o
i
which describes how a phrase pair
should be reordered with respect to the previous
phrases. There are typically three orientation types
and the exact definition depends on the specific
models which we describe below. Orientations can
be determined during decoding and from word-
aligned training corpora. Most models estimate
a probability distribution p(o
i
|pp
i
, a
1
, . . . , a
i
) for
the i-th phrase pair pp
i
= ?e?
i
,
?
f
a
i
? and the align-
ments a
1
, . . . , a
i
of the previous target phrases.
Lexicalized Reordering. This model defines the
three orientation types based only on the posi-
tion of the current and previously translated source
phrase a
i
and a
i?1
, respectively (Tillmann, 2003;
Koehn et al., 2007). The orientation types gen-
erally are: monotone (M), indicating that a
i?1
is
directly followed by a
i
. swap (S) assumes that a
i
precedes a
i?1
, i.e., the two phrases swap places.
Finally, discontinuous (D) indicates that a
i
is not
adjacent to a
i?1
. The probability distribution over
these reordering events is based on a maximum
likelihood estimate:
p(o|pp, a
i?1
, a
i
) =
cnt(o, pp)
cnt(pp)
(2)
where o ? {M,S,D} and cnt returns smoothed
frequency counts over a word-aligned corpus.
Hierarchical Reordering. An extension of the
lexicalized reordering model better handles long-
distance reordering by conditioning the orientation
of the current phrase on a context larger than just
the previous phrase (Galley and Manning, 2008).
In particular, the hierarchical reordering model
does so by building a compact representations
of the preceding context using an efficient shift-
reduce parser. During translation new phrases get
moved on a stack and are then combined with any
previous phrase if they are adjacent. Figure 1
shows an illustrative example: when the decoder
shifts phrase pp
8
onto the stack, this phrase is then
merged with pp
7
(reduce operation), which then
can be merged with previous phrases to finally
form a hierarchical block h
1
. These merge opera-
tions stop once we reach a phrase (here, pp
3
) that
is not contiguous with the current block. Then, as
another phrase (pp
9
) is hypothesized, the decoder
uses the hierarchical block at the top of the stack
(h
1
) to determine the orientation of the current
1251
    	 
       therussiansidehopestoholdconsultationswithiranonthisissueinthenearfuture..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
h1
pp9
pp8
pp7
pp3
d
e
c
o
d
e
d
 
o
u
t
p
u
t
input sentence
Figure 1: The hierarchical reordering model
(HRM) analyzes a non-local context to determine
the orientation of the current phrase. For exam-
ple, the phrase pair pp
9
has a swap orientation
(o
9
= S) with respect to a hierarchical block (h
1
)
that comprises the five preceding phrase pairs.
phrase pp
9
, which in this case is a swap (S) orien-
tation.
1
The model has the advantage that the ori-
entations computed are more robust to derivational
ambiguity of the underlying translation model. A
given surface translation may be derived through
different phrases but the shift-reduce parser com-
bines them into a single representation which is
more consistent with the orientations observed in
the word-aligned training data.
Maximum Entropy-based models. The statis-
tics used to estimate the lexicalized and the hierar-
chical reordering models are based on very sparse
estimates, simply because certain phrases are not
very frequent. Maximum entropy models address
this problem by estimating Eq. 2 through sparse
indicator features over phrase pairs instead, but
prior work with such models still relies on word
aligned corpora for estimation (Xiong et al., 2006;
Nguyen et al., 2009). However, recent evalua-
tions of the approach show little gain over the sim-
pler frequency-based estimation method (Cherry,
2013).
Sparse Hierarchical Reordering model. All of
the models so far are trained to maximize the like-
lihood of reordering decisions observed in word
aligned corpora. Cherry (2013) argues that it
is probably too difficult to learn human reorder-
ing patterns through noisy word alignments that
1
Galley and Manning (2008) provide a more formal ex-
planation.
were generated by unsupervised methods. Instead,
he proposes to learn a discriminative reordering
model based on the outputs of the actual machine
translation system, adjusting the feature weights
to maximize a task-specific objective, which is
BLEU in their case. Their model is based on a
set of sparse features derived from the hierarchi-
cal reordering model which we scale to millions
of features (?6).
3 A Simple Linear Reordering Model
Our reordering model is defined as a simple linear
model over the basic orientation types, similar to
Cherry (2013). In particular, our model defines
score s
?
(o, e, f) over orientations o = {M,S,D},
and a sentence pair {e, f, a} with alignment a as a
linear combination of weighted indicator features:
s
?
(o, e, f, a) = ?
T
u(o, e, f, a)
=
I
?
i=1
?
T
u(o, pp
i
, c
i
)
=
I
?
i=1
s
?
(o, pp
i
, c
i
) (3)
where ? is a vector of weights, {pp
i
}
I
i=1
is a
set of phrases that decompose the sentence pair
{e, f, a}, and u(o, pp
i
, c
i
) is a function that maps
orientation o, phrase pair pp
i
and local context c
i
to a sparse vector of indicator features. The lo-
cal context c
i
represents information used by the
model that is in addition to the phrase pair. For
example, the features of Cherry (2013) condition
on the top-stack of the hierarchical shift reduce
parser, information that is non-local with respect
to the phrase pair. In our experiments, we use fea-
tures that go beyond the top-stack, in order to con-
dition on various parts of the source and target side
contexts (?7).
4 Model Training
Optimization of our model is based on standard
stochastic gradient descent (SGD; Bottou, 2004)
with an expected BLEU loss l(?) which we detail
next (?5). The update is:
?
t
= ?
t?1
? ?
?l(?
t?1
)
??
t?1
(4)
where ?
t
and ?
t?1
are model weights at time t and
t? 1 respectively, and ? is a learning rate.
We add the model as a small number of dense
features to the log-linear framework of translation
1252
(Eq. 1). Specifically, we extend the m baseline
features by a set of new features h
m+1
, . . . , h
m+j
,
where each represents a linear combination of
sparse indicator features corresponding to one of
the orientation types. Exposing each orientation
as a separate dense feature within the log-linear
model is common practice for lexicalized reorder-
ing models (Koehn et al., 2005):
h
m+j
= s
?
(o
j
, e, f, a)
where o
j
? {M,S,D}.
The translation model is then parameterized by
both ?, the log-linear weights of the baseline fea-
tures, as well as ?, the weights of the reordering
model. The reordering model is learned as follows
(Gao and He, 2013; Gao et al., 2014):
1. We first train a baseline translation system to
learn ?, without the discriminative reordering
model, i.e., we set ?
m+1
= 0, . . . , ?
m+j
= 0.
2. Using these weights, we generate n-best lists
for the foreign sentences in the training data
using the setup described in the experimental
section (?7). The n-best lists serve as an ap-
proximation to E(f), the set of possible trans-
lations of f , used in the next step for expected
BLEU training of the reordering model (?5).
3. Next, we fix ?, set ?
m+1
= 1, . . . ?
m+j
= 1
and optimize ? with respect to the loss func-
tion on the training data using stochastic gra-
dient descent.
2
4. Finally, we fix ? and re-optimize ? in the
presence of the discriminative reordering
model using Minimum Error Rate Training
(MERT; Och 2003; ?7).
We found that re-optimizing ? after a few iter-
ations of stochastic gradient descent in step 3 did
not improve accuracy.
5 Expected BLEU Objective Function
The expected BLEU objective (Gao and He, 2013;
Gao et al., 2014) allows us to efficiently optimize
a large scale discriminative reordering model to-
wards the desired task-specific metric, which in
our setting is BLEU.
2
We tuned ?
m+1
, . . . ?
m+j
on the development set but
found that setting them uniformly to one resulted in faster
training and equal accuracy.
Formally, we define our loss function l(?) as
the negative expected BLEU score, denoted as
xBLEU(?), for a given foreign sentence f and a
log-linear parameter set ?:
l(?) =? xBLEU(?)
=?
?
e?E(f)
p
?,?
(e|f) sBLEU(e, e
(i)
) (5)
where sBLEU(e, e
(i)
) is a smoothed sentence-
level BLEU score with respect to the reference
translation e
(i)
, and E(f) is the generation set ap-
proximated by an n-best list. In our experiments
we use n-best lists with unique entries and there-
fore our definitions do not take into account mul-
tiple derivations of the same translation. Specif-
ically, our n-best lists are generated by choosing
the highest scoring derivation e? amongst string
identical translations e for f . We use a sentence-
level BLEU approximation similar to Gao et al.
(2014).
3
Finally, p
?,?
(e|f) is the normalized prob-
ability of translation e given f , defined as:
p
?,?
(e|f) =
exp{??
T
h(f, e)}
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
(6)
where ?
T
h(f, e) includes the discriminative re-
ordering model h
m+1
(e, f), . . . , h
m+j
(e, f) pa-
rameterized by ?, and ? ? [0, inf) is a tuned scal-
ing factor that flattens the distribution for ? < 1
and sharpens it for ? > 1 (Tromble et al., 2008).
4
Next, we define the gradient of the expected
BLEU loss function l(?). To simplify our notation
we omit the local context c in s
?
(o, pp, c) (Eq. 3)
from now on and assume it to be part of pp. Us-
ing the observation that the loss does not explicitly
depend on ?, we get:
?l(?)
??
=
?
o,pp
?l(?)
?s
?
(o, pp)
?s
?
(o, pp)
??
=
?
o,pp
??
o,pp
u(o, pp)
where ?
o,pp
is the error term for orientation o of
phrase pair pp:
?
o,pp
= ?
?l(?)
?s
?
(o, pp)
3
We found in early experiments that the BLEU+1 approx-
imation used by Liang et al. (2006) and Nakov et. al (2012)
worked equally well in our setting.
4
? is only used during expected BLEU training.
1253
The error term indicates how the expected BLEU
loss changes with the reordering score which we
derive in the next section.
Finally, the gradient of the reordering score
s
?
(o, pp) with respect to ? is simply given by this:
?s
?
(o, pp)
??
=
??
T
u(o, pp)
??
= u(o, pp)
5.1 Derivation of the Error Term ?
o,pp
We rewrite the loss function (Eq. 5) using Eq. 6
and separate it into two terms G(?) and Z(?):
l(?) = ?xBLEU(?) = ?
G(?)
Z(?)
(7)
= ?
?
e?E(f)
exp{??
T
h(f, e)} sBLEU(e, e
(i)
)
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
Next, we apply the quotient rule of differentiation:
?
o,pp
=
?xBLEU(?)
?s
?
(o, pp)
=
?(G(?)/Z(?))
?s
?
(o, pp)
=
1
Z(?)
(
?G(?)
?s
?
(o, pp)
?
?Z(?)
?s
?
(o, pp)
xBLEU(?)
)
The gradients for G(?) and Z(?) with respect to
s
?
(o, pp) are:
?G(?)
?s
?
(o, pp)
=
?
e?E(f)
sBLEU(e, e
(i)
)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
?Z(?)
?s
?
(o, pp)
=
?
e?E(f)
? exp{??
T
h(f, e)}
?s
?
(o, pp)
By using the following definition:
U(?, e) = sBLEU(e, e
(i)
)? xBLEU(?)
together with the chain rule, Eq. 6 and Eq. 7, we
can rewrite ?
o,pp
as follows:
?
o,pp
=
1
Z(?)
?
e?E(f)
(
? exp{??
T
h(f, e)}
?s
?
(o, pp)
U(?, e)
)
=
?
e?E(f)
(
p
?,?
(e|f)
???
T
h(f, e)
?s
?
(o, pp)
U(?, e)
)
Because ? is only relevant to the reordering
model, represented by h
m+1
, . . . , h
m+j
, we have:
???
T
h(f, e)
?s
?
(o, pp)
= ??
k
?h
k
(e, f)
?s
?
(o, pp)
= ??
k
N (o, pp, e, f)
1: function TRAINSGD(D, ?)
2: t? 0
3: for all (f
(i)
, e
(i)
) in D do
4: xBLEU = 0 . Compute xBLEU
5: for all e in E(f
(i)
) do
6: wBLEU? p
?,?
t
(e|f) sBLEU(e, e
(i)
)
7: xBLEU? xBLEU + wBLEU
8: end for
9: for all e in E(f
(i)
) do
10: D = sBLEU(e, e
(i)
)? xBLEU
11: for all o, pp in ?e, f
(i)
? do
12: N = N (o, pp, e, f)
13: ?
o,pp
= p
?,?
t
(e|f
(i)
)??
k
ND
14: ?
t+1
= ?
t
? ??
o,pp
u(o, pp))
15: end for
16: end for
17: t? t+ 1
18: end for
19: end function
Figure 2: Algorithm for computing the expected
BLEU loss with SGD updates (Eq. 4) based on
training data D and learning rate ?.
where m + 1 ? k ? m + j and N (o, pp, e, f) is
the number of times pp with orientation o occurs
in the current sentence pair.
This simplifies the error term to:
?
o,pp
=
?
e?E(f)
p
?,?
(e|f)??
k
N (o, pp, e, f)U(?, e)
(8)
where ?
k
is the weight of the dense feature sum-
marizing orientation o in the log-linear model. We
use Eq. 8 in a simple algorithm to train our model
(Figure 2). Our SGD trainer uses a mini-batch size
of a single sentence (?7) which entails all hypoth-
esis in the n-best list for this sentence and the pa-
rameters are updated after each mini-batch.
6 Feature Sets
Our features are inspired by Cherry (2013)
who bases his features on the local phrase-pair
pp = ?e?,
?
f? as well as the top stack of the shift re-
duce parser of the baseline hierarchical ordering
model. We experiment with these variants and ex-
tensions:
? SparseHRMLocal: This feature set is exclu-
sively based on the local phrase-pair and
1254
consists of features over the first and last
word of both the source and target phrase.
5
We use four different word representations:
The word identity itself, but only for the
80 most common source and target language
words. The three other word representations
are based on Brown clustering with either 20,
50 or 80 classes (Brown et al., 1992). There
is one feature for every orientation type.
? SparseHRM: The main feature set of Cherry
(2013). This is an extension of SparseHRM-
Local adding features based on the first and
last word of both the source and the target of
the hierarchical block at the top of the stack.
There are also features based on the source
words in-between the current phrase and the
hierarchical block at the top of the stack.
? SparseHRM+UncommonWords: This set is
identical to SparseHRM, except that word-
identity features are not restricted to the 80
most frequent words, but can be instantiated
for all words, regardless of frequency.
? SparseHRM+BiPhrases: This augments
SparseHRM by phrase-identity features re-
sulting in millions of instances compared to
only a few thousand for SparseHRM. We add
three features for each possible phrase pair:
the source phrase, the target phrase, and the
whole phrase pair.
The baseline hierarchical lexicalized reorder-
ing model is most similar to SparseHRM+BiPhrases
feature set since both have parameters for phrase,
orientation pairs.
6
The feature set closest to
Cherry (2013) is SparseHRM. However, while
Cherry had to severely restrict his features for
batch lattice MIRA-based training, our maximum
expected BLEU approach can handle millions of
features.
7 Experiments
Baseline. We experiment with a phrase-based
system similar to Moses (Koehn et al., 2007),
5
Phrase-local features allow pre-computation which re-
sults in significant speed-ups at run-time. Cherry (2013)
shows that local features are responsible for most of his gains.
6
Although, our model is likely to learn significantly fewer
parameters since many phrase, orientation pairs will only be
seen in the word-aligned data but not in actual machine trans-
lation output.
scoring translations by a set of common fea-
tures including maximum likelihood estimates
of source given target phrases p
MLE
(e|f) and
vice versa, p
MLE
(f |e), lexically weighted esti-
mates p
LW
(e|f) and p
LW
(f |e), word and phrase-
penalties, as well as a linear distortion feature.
The baseline uses a hierarchical reordering model
with five orientation types, including monotone
and swap, described in ?2, as well as two discon-
tinuous orientations, distinguishing if the previous
phrase is to the left or right of the current phrase.
Finally, monotone global indicates that all previ-
ous phrases can be combined into a single hier-
archical block. The baseline includes a modified
Kneser-Ney word-based language model trained
on the target-side of the parallel data, which is de-
scribed below. Log-linear weights are estimated
with MERT (Och, 2003). We regard the 1-best
output of the phrase-based decoder with the hierar-
chical reordering model as the baseline accuracy.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English translation
(Callison-Burch et al., 2012). Translation mod-
els are estimated on 102M words of parallel data
for French-English and 91M words for German-
English; between 7.5-8.2M words are newswire,
depending on the language pair, and the remainder
are parliamentary proceedings. All discrimina-
tive reordering models are trained on the newswire
subset since we found this portion of the data to be
most useful in initial experiments. We evaluate on
six newswire domain test sets from 2008, 2010 to
2013 as well as the 2010 system combination test
set containing between 2034 to 3003 sentences.
Log-linear weights are estimated on the 2009 data
set comprising 2525 sentences. We evaluate using
BLEU with a single reference.
Discriminative Reordering Model. We use 100-
best lists generated by the phrase-based decoder
to train the discriminative reordering model. The
n-best lists are generated by ten systems, each
trained on 90% of the available data in order to de-
code the remaining 10%. The purpose of this pro-
cedure is to avoid a bias introduced by generating
n-best lists for sentences on which the translation
model was previously trained.
7
Unless otherwise
7
Later, we found that the bias has only a negligible effect
on end-to-end accuracy since we obtained very similar results
when decoding with a system trained on all data. This setting
increased the training data BLEU score from 27.5 to 37.8. We
used a maximum source and target phrase length of 7 words.
1255
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93 -
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72 -
SparseHRMLocal 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77 4,407
SparseHRM 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95 9,463
+UncommonWords 25.32 21.76 26.30 26.29 27.15 26.77 27.18 26.12 897,537
+BiPhrases 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26 3,043,053
Table 1: French-English results of expected BLEU trained sparse reordering models compared to no
reordering model at all (noRM) and the likelihood trained baseline hierarchical reordering model (HRM)
on WMT test sets; sc2010 is the 2010 system combination test set. FeatTypes is the number of different
types and AllTest is the average BLEU score over all the test sets, weighted by corpus size. All results
for our sparse reordering models include a likelihood-trained hierarchical reordering model.
dev 2008 2010 sc2010 2011 2012 2013 AllTest FeatTypes
noRM 18.54 19.28 20.14 20.01 18.90 18.87 21.60 19.81 -
HRM (baseline) 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58 -
SparseHRMLocal 19.89 19.86 21.11 20.84 20.04 20.21 22.93 20.88 4,410
SparseHRM 19.83 20.27 21.26 21.05 20.22 20.44 23.17 21.11 9,477
+UncommonWords 20.06 20.35 21.45 21.31 20.28 20.55 23.30 21.24 1,136,248
+BiPhrases 20.09 20.33 21.62 21.47 20.66 20.75 23.27 21.40 3,640,693
Table 2: German-English results of expected BLEU trained sparse reordering models (cf. Table 1).
mentioned, we train our reordering model on the
news portion of the parallel data, corresponding to
136K-150K sentences, depending on the language
pair. We tuned the various hyper-parameters on a
held-out set, including the learning rate, for which
we found a simple setting of 0.1 to be useful. To
prevent overfitting, we experimented with `
2
regu-
larization, but found that it did not improve test ac-
curacy. We also tuned the probability scaling pa-
rameter ? (Eq. 6) but found ? = 1 to be very good
among other settings. We evaluate the perfor-
mance on a held-out validation set during training
and stop whenever the objective changes less than
a factor of 0.0003. For our PRO experiments, we
tuned three hyper-parameters controlling `
2
reg-
ularization, sentence-level BLEU smoothing, and
length. The latter is important to eliminate PRO?s
tendency to produce too short translations (Nakov
et al., 2012).
7.1 Scaling the Feature Set
We first compare our baseline, a likelihood trained
hierarchical reordering model (HRM; Galley &
Manning, 2008), to various expected BLEU
trained models, starting with SparseHRMLocal,
inspired by Cherry (2013) and compare it to
SparseHRM+BiPhrases, a set that is three orders of
magnitudes larger.
Our results on French-English translation (Ta-
ble 1) and German-English translation (Table 2)
show that the expected BLEU trained models scale
to millions of features and that we outperform the
baseline by up to 2.0 BLEU on newstest2012 for
French-English and by up to 1.1 BLEU on new-
stest2011 for German-English.
8
Increasing the
size of the feature set improves accuracy across
the board: The average accuracy over all test sets
improves from 1.0 BLEU for the most basic fea-
ture set to 1.5 BLEU for the largest feature set
on French-English and from 0.3 BLEU to 0.8
BLEU on German-English.
9
The most compa-
rable setting to Cherry (2013) is the feature set
SparseHRM, which we outperform by up to 0.5
BLEU on French-English and by 0.3 BLEU on av-
erage on both language pairs, demonstrating the
benefit of being able to effectively train large fea-
ture sets. Furthermore, the increase in the num-
ber of features does not affect runtime, since most
8
Different to the setups of Galley & Manning (2008) and
Cherry (2013) our WMT evaluation framework uses only one
instead of four references, which makes our BLEU score im-
provements not directly comparable.
9
We attribute smaller improvements on German-English
to the low distortion limit of only six words of our system and
the more difficult reordering patterns when translating from
German which may require more elaborate features.
1256
features can be pre-computed and stored in the
phrase-table, only requiring a constant time table-
lookup, similar to traditional reordering models.
Another appeal of our approach is that train-
ing is very fast given a set of n-best lists for the
training data. The SparseHRM model with 4,407
features is trained in only 26 minutes, while the
SparseHRM+BiPhrases model with over three mil-
lion parameters can be trained in just over two
hours (136K sentences and 100 epochs in both
cases). We attribute this to the training regime
(?4), which does not iteratively re-decode the
training data for expected BLEU training.
10
7.2 Varying Training Set Size
Previous work on sparse reordering models was
restricted to small data sets (Cherry, 2013) due
to the limited ability of standard machine trans-
lation optimizers to handle more than a few thou-
sand sentences. In particular, recent attempts to
scale the margin-infused relaxation algorithm, a
variation which was also used by Cherry (2013),
to larger data sets showed that more data does not
necessarily help to improve test set accuracy for
large feature sets (Eidelman et al., 2013).
In the next set of experiments, we shed light on
the advantage of training discriminative reordering
models with expected BLEU on large training sets.
Specifically, we start off by estimating a reorder-
ing model on only 2,000 sentences, similar to the
size of the development set used by Cherry (2013),
and incrementally increase the amount of training
data to nearly three hundred thousand sentences.
To avoid overfitting to small data sets we experi-
ment with our most basic feature set SparseHRM-
Local, comprising of just over 4,400 types.
For this experiment only, we measure accuracy
in a re-ranking framework for faster experimen-
tation where we use the 100-best output of the
baseline system relying on a likelihood-based hi-
erarchical reordering model. We re-estimate the
log-linear weights by running a further iteration of
MERT on the n-best list of the development set
which is augmented by scores corresponding to
the discriminative reordering model. The weights
of those features are initially set to one and we
use 20 random restarts for MERT. At test time we
rescore the 100-best list of the test set using the
new set of log-linear weights learned previously.
10
We would expect better accuracy when iteratively decod-
ing the training data but did not do so in this study for effi-
ciency reasons.
24.4
24.6
24.8
25.0
25.2
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
dev
26.6 Training set size
25.6
25.8
26.0
26.2
26.4
26.6
2K 4K 8K 16K 32K 64K 136K 272K
BLE
U
Training set size
news2011
Figure 3: Effect of increasing the training set size
from 2,000 to 272,000 sentences measured on the
dev set (top) and news2011 (bottom) in an n-best
list rescoring setting.
Figure 3 confirms that more training data in-
creases accuracy and that the best model requires
a substantially larger amount of training data than
what is typically used for maximum BLEU train-
ing. We expect an even steeper curve for larger
feature sets where more parameters need to be es-
timated and where the amount of training data is
likely to have an even larger effect.
7.3 Likelihood versus BLEU Optimization
Previous research has shown that directly training
a reordering model for BLEU can vastly outper-
form a likelihood trained maximum entropy re-
ordering model (Cherry, 2013). However, the two
approaches do not only differ in the objectives
used, but also in the type of training data. The
maximum entropy reordering model is trained on
a word-aligned corpus, trying to learn human re-
ordering patterns, whereas the sparse reordering
model is trained on machine translation output,
trying to learn from the mistakes made by the ac-
tual system. It is therefore not clear how much
either one contributes to good accuracy.
Our next experiment teases those two aspects
apart and clearly shows the effect of the objec-
tive function. Specifically, we compare the tra-
ditionally used conditional log-likelihood (CLL)
objective to expected BLEU on the French-
English translation task in a small feature con-
dition (SparseHRM) of about 9K features and
1257
dev 2008 2010 sc2010 2011 2012 2013 AllTest
noRM 23.37 20.18 24.24 24.18 24.83 24.23 24.85 23.93
HRM (baseline) 24.11 20.85 24.92 24.83 25.68 25.11 25.76 24.72
SparseHRM (CLL) 24.28 21.02 25.11 25.10 25.92 25.24 25.76 24.88
SparseHRM (xBLEU) 25.29 21.43 26.17 26.14 26.99 26.63 27.01 25.95
SparseHRM+BiPhrases (CLL) 24.42 21.17 25.12 25.00 25.86 25.36 26.18 24.98
SparseHRM+BiPhrases (xBLEU) 25.46 21.67 26.19 26.19 27.55 27.07 27.41 26.26
Table 3: French-English results comparing the baseline hierarchical reordering model (HRM) to sparse
reordering model trained towards conditional log-likelihood (CLL) and expected BLEU (xBLEU).
dev 2008 2010 sc2010 2011 2012 2013 AllTest
PRO 24.05 20.90 25.42 25.28 25.79 25.09 26.07 24.94
xBLEU 25.24 21.26 25.99 25.93 26.98 26.34 26.77 25.77
Table 4: French-English results on the SparseHRMLocal feature set when when trained with pair-wise
ranked optimization (PRO) and expected BLEU (xBLEU).
a large feature setting of over 3M features
(SparseHRM+BiPhrases). In the CLL setting, we
maximize the likelihood of the hypothesis with the
highest BLEU score in the n-best list of each train-
ing sentence.
Our results (Table 3) show that CLL training
achieves only a fraction of the gains yielded by
the expected BLEU objective. For SparseHRM,
CLL improves the baseline by less than 0.2 BLEU
on average across all test sets, whereas expected
BLEU achieves 1.2 BLEU. Increasing the number
of features to 3M (SparseHRM+BiPhrases) results
in a slightly better average gain of 0.3 BLEU for
CLL but but expected BLEU still achieves a much
higher improvement of 1.5 BLEU. Because our
gains with likelihood training are similar to what
Cherry (2013) reported for his maximum entropy
model, we conclude that the objective function is
the most important factor to achieving good accu-
racy.
7.4 Comparison to PRO
In our final experiment we compare expected
BLEU training to pair-wise ranked optimization
(PRO), a popular off the shelf trainer for ma-
chine translation models with large feature sets
(Hopkins and May, 2011).
11
Previous work has
shown that PRO does not scale to truly large fea-
ture sets with millions of types (Yu et al., 2013)
and we therefore restrict ourselves to our smallest
11
MIRA is another popular optimizer but as previously
mentioned, even the best publicly available implementation
does not scale to large training sets (Eidelman et al., 2013).
set (SparseHRMLocal) of just over 4.4K features.
We train PRO on the development set compris-
ing of 2,525 sentences, a setup that is commonly
used by standard machine translation optimizers.
In this setting, PRO directly learns weights for the
baseline features (?7) as well as the 4.4K indica-
tor features corresponding to the sparse reordering
model. For expected BLEU training we use the
full 136K sentences from the training data. The
results (Table 4) demonstrate that expected BLEU
outperforms a typical setup commonly used to
train large feature sets.
8 Conclusion and Future Work
The expected BLEU objective is a simple and ef-
fective approach to train large-scale discriminative
reordering models. We have demonstrated that
it scales to millions of features, which is orders
of magnitudes larger than other modern machine
translation optimizers can currently handle.
Empirically, our sparse reordering model im-
proves machine translation accuracy across the
board, outperforming a strong hierarchical lexi-
calized reordering model by up to 2.0 BLEU on
a French to English WMT2012 setup, where the
baseline was trained on over two million sentence
pairs. We have shown that scaling to large train-
ing sets is crucial to good performance and that
the best performance is reached when hundreds
of thousands of training sentences are used. Fur-
thermore, we demonstrate that task-specific train-
ing towards expected BLEU is much more effec-
tive than optimizing conditional log-likelihood as
1258
is usually done. We attribute this to the fact that
likelihood is a strict zero-one loss that does not as-
sign credit to partially correct solutions, whereas
expected BLEU does.
In future work we plan to extend expected
BLEU training to lattices and to evaluate the ef-
fect of estimating weights for the dense baseline
features as well. Our current training procedure
(Gao and He, 2013; Gao et al., 2014) decodes
the training data only once. In future work, we
would like to compare this to repeated decoding
as done by conventional optimization methods as
well as other large-scale discriminative training
approaches (Yu et al., 2013). We expect this to
yield additional accuracy gains.
Acknowledgements
We would like to thank Arul Menezes and Xi-
aodong He for helpful discussion related to this
work and the three anonymous reviewers for their
comments.
References
L?eon Bottou. 2004. Stochastic learning. In
Olivier Bousquet and Ulrike von Luxburg, edi-
tors, Advanced Lectures in Machine Learning, Lec-
ture Notes in Artificial Intelligence, pages 146?168.
Springer Verlag, Berlin.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, pages 10?51.
Association for Computational Linguistics, June.
Colin Cherry. 2013. Improved Reordering for Phrase-
Based Translation using Sparse Features. In Proc. of
NAACL, pages 9?14. Association for Computational
Linguistics, June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. of NAACL, pages 218?226. Associ-
ation for Computational Linguistics, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Vladimir Eidelman, Ke Wu, Ferhan Ture1, Philip
Resnik, and Jimmy Lin. 2013. Mr. MIRA:
Open-Source Large-Margin Structured Learning on
MapReduce. In Proc. of ACL, pages 199?204. As-
sociation for Computational Linguistics, August.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of HLT-NAACL, pages 273?280, Boston,
MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of ACL, pages 961?968, Sydney, Australia,
June.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450?459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Spence Green, Daniel Cer, and Christopher Manning.
2014. An Empirical Comparison of Features and
Tuning for Phrase-based Machine Translation. In
Proc. of WMT. Association for Computational Lin-
guistics, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8?14. Association
for Computational Linguistics, July.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proc. of EMNLP. Association for Com-
putational Linguistics, July.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proc. of IWSLT.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761?768, Jul.
1259
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Vinh Van Nguyen, Akira Shimazu, Minh Le Nguyen,
and Thai Phuong Nguyen. 2009. Improving A Lex-
icalized Hierarchical Reordering Model Using Max-
imum Entropy. In MT Summit XII. Association for
Computational Linguistics, August.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to machine translation.
Computational Linguistics, 30(4):417?449, June.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Philadelphia, PA, USA, Jul.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321?326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159?165. Association for Computa-
tional Linguistics, July.
Christoph Tillmann. 2003. A Unigram Orientation
Model for Statistical Machine Translation. In Proc.
of NAACL, pages 106?108. Association for Compu-
tational Linguistics, June.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620?629. Associ-
ation for Computational Linguistics, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proc. of ACL-
COLING, pages 521?528, Sydney, Jul.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112?1123. Association for Computational
Linguistics, October.
1260
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Minimum Translation Modeling with Recurrent Neural Networks
Yuening Hu
Department of Computer Science
University of Maryland, College Park
ynhu@cs.umd.edu
Michael Auli, Qin Gao, Jianfeng Gao
Microsoft Research
Redmond, WA, USA
{michael.auli,qigao,jfgao}@microsoft.com
Abstract
We introduce recurrent neural network-
based Minimum Translation Unit (MTU)
models which make predictions based on
an unbounded history of previous bilin-
gual contexts. Traditional back-off n-gram
models suffer under the sparse nature of
MTUs which makes estimation of high-
order sequence models challenging. We
tackle the sparsity problem by modeling
MTUs both as bags-of-words and as a
sequence of individual source and target
words. Our best results improve the out-
put of a phrase-based statistical machine
translation system trained on WMT 2012
French-English data by up to 1.5 BLEU,
and we outperform the traditional n-gram
based MTU approach by up to 0.8 BLEU.
1 Introduction
Classical phrase-based translation models rely
heavily on the language model and the re-
ordering model to capture dependencies between
phrases. Sequence models over Minimum Trans-
lation Units (MTUs) have been shown to com-
plement both syntax-based (Quirk and Menezes,
2006) as well as phrase-based (Zhang et al., 2013)
models by explicitly modeling relationships be-
tween phrases. MTU models have been tradi-
tionally estimated using standard back-off n-gram
techniques (Quirk and Menezes, 2006; Crego and
Yvon, 2010; Zhang et al., 2013), similar to word-
based language models (?2).
However, the estimation of higher-order n-gram
models becomes increasingly difficult due to data
sparsity issues associated with large n-grams, even
when training on over one hundred billion words
(Heafield et al., 2013); bilingual units are much
sparser than words and are therefore even harder
to estimate. Another drawback of n-gram mod-
els is that future predictions are based on a limited
amount of previous context that is often not suf-
ficient to capture important aspects of human lan-
guage (Rastrow et al., 2012).
Recently, several feed-forward neural network-
based models have achieved impressive improve-
ments over traditional back-off n-gram models in
language modeling (Bengio et al., 2003; Schwenk
et al., 2007; Schwenk et al., 2012; Vaswani et al.,
2013), as well as translation modeling (Allauzen et
al., 2011; Le et al., 2012; Gao et al., 2013). These
models tackle the data sparsity problem by rep-
resenting words in continuous space rather than
as discrete units. Similar words are grouped in
the same sub-space rather than being treated as
separate entities. Neural network models can be
seen as functions over continuous representations
exploiting the similarity between words, thereby
making the estimation of probabilities over higher-
order n-grams easier.
However, feed-forward networks do not directly
address the limited context issue either, since pre-
dictions are based on a fixed-size context, similar
to back-off n-gram models. We therefore focus
in this paper on recurrent neural network architec-
tures, which address the limited context issue by
basing predictions on an unbounded history of pre-
vious events which allows to capture long-span de-
pendencies. Recurrent architectures have recently
advanced the state of the art in language model-
ing (Mikolov et al., 2010; Mikolov et al., 2011a;
Mikolov, 2012) outperforming multi-layer feed-
forward based networks in perplexity and word er-
ror rate for speech recognition (Arisoy et al., 2012;
Sundermeyer et al., 2013). Recent work has also
shown successful applications to machine transla-
tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-
ner and Blunsom, 2013). We extend this work by
modeling Minimum Translation Units with recur-
rent neural networks.
Specifically, we introduce two recurrent neu-
ral network-based MTU models to address the is-
20
M1 M2 M3 M4 M5
Yu        ZuoTian JuXing Le HuiTan
held
=> null
=> Yesterday
=> held
=> the
=> meeting
? ?? ?? ? ??
Yu
ZuoTian
JuXing_Le
null
HuiTan
null        
the meeting null yesterday
M1: 
M2: 
M3: 
M4: 
M5: 
Figure 1: Example Minimum Translation Unit
partitioning based on Zhang et al. (2013).
sues regarding data sparsity and limited context
sizes by leveraging continuous representations and
the unbounded history of the recurrent architec-
ture. Our first approach frames the problem as a
sequence modeling task over minimal units (?3).
The second model improves over the first by mod-
eling an MTU as a bag-of-words, thereby allow-
ing us to learn representations over sub-structures
of minimal units that are shared across MTUs
(?4). Our models significantly outperform the tra-
ditional back-off n-gram based approach and we
show that they act complementary to a very strong
recurrent neural network-based language model
based solely on target words (?5).
2 Minimum Translation Units
Banchs et al. (2005) introduced the idea of framing
translation as a sequence modeling problem where
a sentence pair is generated in left-to-right order as
a sequence of bilingual n-grams. Minimum Trans-
lation Units (Quirk and Menezes, 2006; Zhang
et al., 2013) are an extension which additionally
permit tuples with empty source or target sides,
thereby allowing insertion or deletion phrase pairs.
The two basic requirements for MTUs are that
there are no overlapping word alignment links be-
tween phrase pairs and it should not be possible to
extract smaller phrase pairs without violating the
word alignment constraints. Informally, we can
think of MTUs as small phrase pairs that cannot
be broken down any further without violating the
two requirements.
Minimum Translation Units partition a sentence
pair into a set of minimal bilingual units or tu-
Words MTUs
Tokens 34,769,416 14,853,062
Types 143,524 1,315,512
Singleton types 34.9% 80.1%
Table 1: Token and type counts for both source
and target words as well as MTUs based on the
WMT 2006 German to English data set (cf. ?5).
ples obtained by an algorithm similar to phrase-
extraction (Koehn et al., 2003). Figure 1 illus-
trates such a partitioning. Modeling minimal units
has two advantages over considering larger phrase
pairs that are effectively composed of MTUs:
First, minimal units result in a unique partition-
ing of a sentence pair. This has the advantage that
we avoid modeling spurious derivations, that is,
multiple derivations generating the same sentence
pair. Second, minimal units result in smaller mod-
els with a smoother distribution than models based
on composed units (Zhang et al., 2013).
Sentence pairs can be generated in multiple or-
ders, such as left-to-right or right-to-left, either in
source or target order. For example, the source
left-to-right order of the sentence pair in Figure 1
is simply M1, M2, M3, M4, M5, while the tar-
get left-to-right order is M3, M4, M5, M1, M2.
We deal with inserted or deleted words similar to
Zhang et al. (2013): The source side null token of
an inserted target phrase is placed next to the last
source word aligned to the closest preceding non-
null aligned target phrase; a similar rule is applied
to null tokens on the target side. For example, in
Figure 1 we place M4 straight after M3 because
?the?, the aligned target phrase, is after ?held?, the
previous non-null aligned target phrase.
We can straightforwardly estimate an n-gram
model over MTUs to estimate the probability
of a sentence pair using standard back-off tech-
niques commonly employed in language mod-
eling. For example, a trigram model in tar-
get left-to-right order factors the sentence pair in
Figure 1 as p(M3) p(M4|M3) p(M5|M3,M4)
p(M1|M4,M5)p(M2|M5,M1).
If we would like to model larger contexts, then
we quickly run into data sparsity issues. To illus-
trate this point, consider the parameter growth of
an n-gram model which is driven by the vocabu-
lary size |V | and the n-gram order n: O(|V |
n
).
Clearly, the exact estimation of higher-order n-
21
gram probabilities becomes more difficult with
large n, leading to the estimation of events with
increasingly sparse statistics, or having to rely
on statistics from lower-order events with back-
off models, which is less desirable. Even word-
based language models rarely ventured so far
much beyond 5-gram statistics as demonstrated
by Heafield et al. (2013) who trained a, by to-
day?s standards, very large 5-gram model on 130B
words. Data sparsity is therefore an even more sig-
nificant issue for MTU models relying on much
larger vocabularies. In our setting, the MTU vo-
cabulary is an order of magnitude larger than a
word vocabulary obtained from the same data (Ta-
ble 1). Furthermore, most MTUs are observed
only once making the reliable estimation of prob-
abilities very challenging.
Neural network-based sequence models tackle
the data sparsity problem by learning continuous
word representations, that group similar words to-
gether in continuous space. For example, the
distributional representations induced by recurrent
neural networks have been found to have interest-
ing syntactic and semantic regularities (Mikolov
et al., 2013). Furthermore, these representations
can be exploited to estimate more reliable statis-
tics over higher-order n-grams than with discrete
word units. Recurrent neural networks go beyond
fixed-size contexts and allow the model to keep
track of long-span dependencies that are important
for future predictions. In the next sections we will
present Minimum Translation Unit models based
on recurrent architectures.
3 Atomic MTU RNN Model
The first model we introduce is based on the recur-
rent neural network language model of Mikolov
et al. (2010). We frame the problem as a tradi-
tional sequence modeling task which treats MTUs
as atomic units, similar to the approach taken by
the traditional back-off n-gram models.
The model is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 2). The input layer encodes the
MTU at time t as a 1-of-N vector m
t
with all val-
ues being zero except for the entry representing
the MTU. The output layer y
t
represents a proba-
bility distribution over possible next MTUs; both
the input and output layers are of size |V |, the size
of the MTU vocabulary. The hidden layer state h
t
encodes the history of all MTUs observed in the
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
Figure 2: Structure of the atomic recurrent neu-
ral network MTU model following the word-based
RNN model of Mikolov (2012).
sequence up to time step t.
The state of the hidden layer is determined by
the input layer and the hidden layer configuration
of the previous time step h
t?1
. The weights of the
connections between the layers are summarized in
a number of matrices: U represents weights from
the input layer to the hidden layer, and W repre-
sents connections from the previous hidden layer
to the current hidden layer. Matrix V contains
weights between the current hidden layer and the
output layer.
The hidden and output layers are computed
via a series of matrix-vector products and non-
linearities:
h
t
= s(Um
t
+Wh
t?1
)
y
t
= g(Vh
t
)
where
s(z) =
1
1 + exp {?z}
, g(z
m
) =
exp {z
m
}
?
k
exp {z
k
}
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram fea-
tures over input MTUs (Mikolov et al., 2011a).
The maximum entropy weights D are added to
the output activations before applying the softmax
function and are estimated jointly with all other
parameters (Figure 3).
1
1
While these features depend on multiple input MTUs, we
22
mt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
0
0
T
D
c t
Figure 3: Structure of atomic recurrent neural net-
work MTU model with classing layer c
t
and direct
connections D between the input and output lay-
ers (cf. Figure 2).
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the truncated back
propagation through time algorithm, which unrolls
the network and then computes error gradients
over multiple time steps (Rumelhart et al., 1986);
we use a cross entropy criterion to obtain the error
vector with respect to the output activations and
the desired prediction. After training, the output
layer represents posteriors p(m
t+1
|m
t
t?n+1
,h
t
),
the probability of the next MTU given the previ-
ous n input MTUs m
t
t?n+1
= m
t
, . . . ,m
t?n+1
and the current hidden layer configuration h
t
.
Na??ve computation of the probability distribu-
tion over the next MTU is very expensive for large
vocabularies, such as commonly encountered for
MTU models (Table 1). A well established ef-
ficiency trick assigns each possible output to a
unique class and then uses a two-step process to
find the probability of an MTU, instead of comput-
ing the probability of all possible outputs (Good-
man, 2001; Emami and Jelinek, 2005; Mikolov et
al., 2011b). Under this scheme we compute the
probability of an MTU by multiplying the prob-
ability of its class c
i
t
with the probability of the
depicted them for simplicity as a connection between the
current input vectorm
t
and the output layer.
minimal unit conditioned on the class:
p(m
t+1
|m
t
t?n+1
,h
t
) =
p(c
i
t
|m
t
t?n+1
,h
t
) p(m
t+1
|c
i
t
,m
t
t?n+1
,h
t
)
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C| + max
i
|c
i
|) where |C| is the number of
classes and |c
i
| is the number of minimal units
in class c
i
. The best case complexity O(
?
|V |)
requires the number of classes and MTUs to be
evenly balanced, i.e., each class contains exactly
as many minimal units as there are classes.
Figure 3 illustrates how classing changes the
structure of the network by adding an additional
output layer for the class probabilities.
4 Bag-of-words MTU RNN Model
The previous model treats MTUs as atomic sym-
bols which leads to large vocabularies requir-
ing large parameter sets and expensive inference.
However, similar MTUs may share the same
words, or words which are related in continuous
space. The atomic MTU model does not exploit
this since it cannot access the internal structure of
a minimal unit.
The approach we pursue next is to break MTUs
into individual source and target words (Le et al.,
2012) in order to exploit structural similarities be-
tween infrequently observed minimal units. Sin-
gletons represent the vast majority of our MTU
vocabulary (Table 1). This resembles the word-
hashing trick of Huang et al. (2013) who repre-
sented individual words as a bag-of-character n-
grams to reduce the vocabulary size of a neural
network-based model in an information retrieval
setting.
2
We first describe a theoretically appealing but
computationally expensive model and then discuss
a more practical variation. The input layer of this
model accepts the current minimal unit as a K-of-
N vector representing K source and target words
as opposed to the 1-of-N encoding of entire MTUs
in the previous model (Figure 4). Larger MTUs
may contain the same word more than once and we
simply adjust their count to one.
3
Different to the
2
Applying the same technique would likely result in too many
collisions since we are dealing with multi-word units instead
of single words.
3
We found no effect on accuracy when using the unmodified
count in initial experiments.
23
x t
ht-1
ht
w t
V
W
U
1
0
1
1
0
1
0
0
D yt
C
. . .
. . .
. . .
sr c
tgt
MT U
Figure 4: Structure of MTU bag-of-words recur-
rent neural network model. The input layer rep-
resents a minimal unit as a bag-of-words and the
output layer y
t
is a probability distribution over
possible next MTUs depending on the activations
of the word layer w
t
representing source and tar-
get words of minimal units.
previous model, the input vector has now multiple
active entries whose signals are absorbed into the
new hidden layer configuration.
This bag-of-words encoding of minimal units
dramatically reduces the vocabulary size but it in-
evitably maps different MTUs to the same encod-
ing. On our data set, we observe less than 0.2% of
minimal units that are involved in collisions, a rate
that is similar to Huang et al. (2013). In practice
collisions are unlikely to affect accuracy in our set-
ting because MTUs that are mapped to the same
encoding usually do not differ much in semantic
meaning as illustrated by the following examples:
erfolg haben ? succeed collides with haben er-
folg? succeed, or damit ,? to and , damit? to;
in both examples either the auxiliary verb haben or
the comma changes position, neither of which sig-
nificantly changes the meaning for this particular
pair of MTUs.
The structure of the bag-of-words MTU RNN
models is shown in Figure 4. Similar to the atomic
MTU RNN model (?3), the hidden layer combines
the signal from the input layer and the previous
hidden layer configuration. The hidden layer acti-
vations feed into a word layer w
t
representing the
source and target words that part of all possible
MTUs; it is of the same size as the input layer. The
word layer is connected to a convolutional out-
put layer y
t
by weights summarized in the sparse
matrix C. The output layer represents all possi-
ble next minimal units, where each MTU entry is
only connected to neurons in the word layer repre-
senting its source and target words. The word and
MTU layers are then computed as follows:
w
t
= s(Vh
t
)
y
t
= g(Cw
t
)
However, there are a number of computational
issues with this model: First, we cannot efficiently
factor the word layer w
t
into classes such as for
the atomic MTU RNN model because we require
all its activations to compute the MTU output
layer y
t
. This reduces the best case complex-
ity of computing the word layer from O(
?
|V |)
back to linear in the number of source and tar-
get words |V |. In practice this results in between
200-1000 more activations that need to be com-
puted, depending on the word vocabulary size.
Second, turning the MTU output layer into a con-
volutional layer is not enough to sufficiently re-
duce the computational effort to compute the out-
put activations since the number of connections
between the word and MTU layers is very imbal-
anced. This is because frequent words, such as
function words, are part of many MTUs and there-
fore have a very high out-degree, e.g., the neuron
representing ?the? has over 82K outgoing edges.
On the other hand, infrequent words, have a very
low out-degree. This imbalance makes it hard
to efficiently compute activations and error gradi-
ents, even on a GPU, since some neurons require
substantially more work than others.
4
For these reasons we decided to design a sim-
pler, more tractable version of this model (Fig-
ure 5). The simplified model still represents an
input MTU as a bag-of-words but minimal units
are generated word-by-word, first emitting source
words and then target words. This is in contrast
to the original model which predicted an MTU as
a single unit. Decomposing the next MTU into
individual words dramatically reduces the size of
the output layer, thereby resulting in faster com-
putation of the outputs and making normalization
4
In initial experiments we found this model to be over twenty
times slower than the atomic MTU RNN model with esti-
mated training times of over 6 weeks. This was despite us-
ing a vastly smaller vocabulary and by computing the word
layer on a, by current standards, high-end GPU (NVIDIA
Tesla K20c) using sparse matrix optimizations (cuSPARSE)
for the convolutional layer.
24
mt
ht-1
ht
yt
V
W
U
1
0
1
1
0
1
0
0
T
D
c t
mt+ 1
sr c
tgt
MT U
Figure 5: Simplified MTU bag-of-words recurrent
neural network model (cf. Figure 4). An MTU is
input as bag-of-words and the next MTU is pre-
dicted as a sequence of both source and target
words.
into probabilities easier. Furthermore, the output
layer can be factorized into classes requiring only
a fraction of the neurons to be computed, a much
more efficient solution compared to the original
model which required calculation of the entire out-
put layer.
The simplified model computes the probability
of the next MTU m
t+1
as a product of individual
word probabilities:
p(m
t+1
|m
t
t?n+1
,h
t
) = (1)
?
a
1
,...,a
u
?m
t+1
p(c
k
|m
t
t?n+1
,h
t
)
p(a
k
|c
k
,m
t
t?n+1
,h
t
)
where we predict a sequence of source and target
words a
1
, . . . , a
u
? m
t+1
with a class-structured
output layer, similar to the atomic model (?3).
Training still uses a cross entropy criterion and
back propagation through time, however, error
vectors are computed on a per-word basis, instead
of a per-MTU basis. Direct connections between
the input and output layers are based on source and
target words which is less sparse than basing direct
features on entire MTUs such as for the original
bag-of-words model.
Overall, the simplified model retains the bag-of-
words input representation of the original model,
while permitting the efficient factorization of the
word-output layer into classes.
5 Experiments
We evaluate the effectiveness of both the atomic
MTU RNN model (?3) and the simplified bag-of-
words MTU RNN model (?4) in an n-best rescor-
ing setting, comparing against a trigram back-off
MTU model as well as the phrasal decoder 1-best
output which we denote as the baseline.
5.1 Experimental Setup
Baselines. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007), scoring translations by a set of common
features including maximum likelihood estimates
of source given target mappings p
MLE
(e|f) and
vice versa p
MLE
(f |e), as well as lexical weight-
ing estimates p
LW
(e|f) and p
LW
(f |e), word and
phrase-penalties, a linear distortion feature and
a lexicalized reordering feature. The baseline
includes a standard modified Kneser-Ney word-
based language model trained on the target-side of
the parallel corpora described below. Log-linear
weights are estimated with minimum error rate
training (MERT; Och, 2003).
The 1-best output by the phrase-based decoder
is the baseline accuracy. As a second baseline we
experiment with a trigram back-off MTU model
trained on all extracted MTUs, denoted as n-gram
MTU. The trigram MTU model is estimated with
the same modified Kneser-Ney framework as the
target side language model. All MTU models are
trained in target left-to-right MTU order which
performed well in initial experiments.
Evaluation. We test our approach on two differ-
ent data sets. First, we train a German to English
system based on the data of the WMT 2006 shared
task (Koehn and Monz, 2006). The parallel corpus
includes about 35M words of parliamentary pro-
ceedings for training, a development set and two
test sets with 2000 sentences each.
Second, we experiment with a French to En-
glish system based on 102M words of training data
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for about 5m words which are newswire; all
MTU models are trained on the newswire subset
since we found similar accuracy to using all data in
initial experiments. We evaluate on four newswire
domain test sets from 2008, 2010 and 2011 as well
as the 2010 system combination test set contain-
ing between 2034 to 3003 sentences. Log-linear
weights are estimated on the 2009 data set com-
25
prising 2525 sentences. We evaluate all systems
in a single reference BLEU setting.
Rescoring Setup. We rescore the 1000-best out-
put of the baseline phrase-based decoder by ei-
ther the trigram back-off MTU model or the
RNN models. The baseline accuracy is obtained
by choosing the 1-best decoder output. We re-
estimate the log-linear weights for rescoring by
running a further iteration of MERT with the ad-
ditional feature values; we initialize the rescoring
feature weight to zero and try 20 random restarts.
At test time we use the new set of log-linear
weights to rescore the test set n-best list.
Neural Network Setup. We trained the recur-
rent neural network models on between 88% and
93% of each data set and used the remainder as
validation data. The vocabulary of the atomic
MTU RNN model is comprised of all MTU types
which were observed more than once in the train-
ing data.
5
Similarly, we modeled all non-singleton
words for the bag-of-words MTU RNN model.
We obtain classes for words or MTUs using a
version of Brown-Clustering with an additional
regularization term to optimize the runtime of
the language model (Brown et al., 1992; Zweig
and Makarychev, 2013). Direct connections use
features over unigrams, bigrams and trigrams of
words or MTUs, depending on the model. Fea-
tures are hashed to a table with at most 500 million
values following Mikolov et al. (2011a). We use
the standard settings for the model with the default
learning rate ? = 0.1 that decays exponentially if
the validation set entropy does not decrease. Back
propagation through time computes error gradi-
ents over the past twenty time steps. Training
is stopped after 20 epochs or when the valida-
tion entropy does not decrease over two epochs.
Throughout, we use a hidden layer size of 100
which provided a good trade-off between time and
accuracy in initial experiments.
5.2 Results
We first report the decoder 1-best output as the
first baseline and then rescore our two data sets
(Table 2 and Table 3) with the n-gram back-off
MTU model to establish a second baseline (n-
gram MTU). The n-gram model improves by 0.4
BLEU over the decoder 1-best on all test sets for
German to English. On French-English accuracy
5
We tried modeling all MTUs which did not contain a single-
ton word but observed no significant effect on accuracy.
dev test1 test2
Baseline 25.8 26.0 26.0
n-gram MTU 26.3 26.6 26.4
atomic MTU RNN 26.5 26.8 26.5
BoW MTU RNN 26.5 27.0 26.9
word RNNLM 26.5 27.1 26.8
Combined 26.8 27.3 27.1
Table 2: German to English BLEU results for
the decoder 1-best output (Baseline) compared to
rescoring with a target left-to-right trigram MTU
model (n-gram MTU), our two recurrent neural
network-based MTU models, a word-based RNN-
based language model (word RNNLM), as well
as a combination of the three RNN-based models
(Combined).
improves on three out of five sets by up to 0.7
BLEU.
Next, we evaluate the accuracy of the MTU
RNN models. The atomic MTU RNN model im-
proves over the n-gram MTU model on all test sets
for German to English, however, for French to En-
glish the back-off model performs better on two
out of four test sets.
The next question we answer is if breaking
MTUs into individual units to leverage similarities
in the internal structure can help accuracy. The re-
sults (Table 2 and Table 3) for the bag-of-words
model (BoW MTU RNN) clearly show that this is
the case for both language pairs. We significantly
improve over the n-gram MTU model as well as
the atomic RNN model on all test sets. We observe
gains of up to 0.5 BLEU over the n-gram MTU
model for German to English as well as French to
English; improvements over the decoder baseline
are up to 1.2 BLEU for French to English.
How do our models compare to other neural net-
work approaches that rely only on target side in-
formation? To answer this question we compare
to the strong language model of Mikolov (2012;
RNNLM) which has recently improved the state-
of-the-art in language modeling perplexity. The
results (Table 2 and Table 3) show that RNNLM
performs competitively. However, our approaches
model translation since we use both source and tar-
get information as opposed to scoring only the flu-
ency of the target side, such as done by RNNLM.
Can our models act complementary to a strong
RNN language model? Our final experiment com-
bines the atomic MTU RNN model, the BoW
26
dev news2008 news2010 news2011 newssyscomb2010
Baseline 24.3 20.5 24.4 25.1 24.3
n-gram MTU 24.6 20.8 24.4 25.8 24.3
atomic MTU RNN 24.6 20.7 24.4 25.5 24.3
BoW MTU RNN 25.2 21.2 24.8 26.3 24.6
word RNNLM 25.1 21.4 25.1 26.4 24.9
Combined 25.4 21.4 25.1 26.6 24.9
Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various
MTU models (cf. Table 2).
MTU RNN model, and the RNNLM (Combined).
The results (Table 2 and Table 3) confirm that this
is the case. For German to English translation
accuracy improves by 0.2 to 0.3 BLEU over the
RNNLM alone, with gains of up to 1.3 BLEU over
the baseline and up to 0.7 BLEU over the n-gram
MTU model. Improvements for French to English
are lower but we can see some gains on news2011
and on the dev set. Overall, we improve accuracy
on the French to English task by up to 1.5 BLEU
over the decoder 1-best, and by up to 0.8 BLEU
over the n-gram MTU model.
6 Related Work
Our approach of modeling Minimum Translation
Units is very much in line with recent work on n-
gram-based translation models (Crego and Yvon,
2010), and more recently, continuous space-based
translation models (Le et al., 2012). The mod-
els presented in this paper differ in a number of
key aspects: We use a recurrent architecture repre-
senting an unbounded history of MTUs rather than
a feed-forward style network. Feed-forward net-
works as well as back-off n-gram models rely on a
finite history which results in predictions indepen-
dent of anything but a short context of words. A
recent side-by-side comparison between recurrent
and feed-forward style neural networks (Sunder-
meyer et al., 2013) has shown that recurrent ar-
chitectures outperform feed-forward networks in
a language modeling task, a similar problem to
modeling sequences over Minimum Translation
Units.
Furthermore, the input of our best model is a
bag-of-words representation of an MTU, unlike
the ordered source and target word n-grams used
by Crego and Yvon (2010) as well as Le et al.
(2012). Finally, we model both source and target
words in a single recurrent neural network. The
approach of Le et al. (2012) factorizes the joint
probability over an MTU sequence in a way that
suggests the use of separate neural network mod-
els for the source and the target sides, where each
model generates words on the respective side only.
Other work on applying recurrent neural net-
works to machine translation (Mikolov, 2012; Auli
et al., 2013; Kalchbrenner and Blunsom, 2013)
concentrated on word-based language and transla-
tion models, whereas we model Minimum Trans-
lation Units.
7 Conclusion and Future Work
Minimum Translation Unit models based on recur-
rent neural networks lead to substantial gains over
their classical n-gram back-off models. We intro-
duced two models of which the best improves ac-
curacy by up to 1.5 BLEU over the 1-best decoder
output, and by 0.8 BLEU over a trigram MTU
model in an n-best rescoring setting.
Our experiments have shown that representing
MTUs as bags-of-words leads to better accuracy
since this exploits similarities in the internal struc-
ture of Minimum Translation Units, which is not
possible when modeling them as atomic symbols.
We have also shown that our models are comple-
mentary to a very strong RNN language model
(Mikolov, 2012).
In future work, we would like to make the initial
version of the bag-of-words model computation-
ally more tractable using a better GPU implemen-
tation. This model combines the efficient bag-of-
words input representation with the ability to pre-
dict MTUs as single units while explicitly model-
ing the constituent words in an intermediate layer.
8 Acknowledgements
We would like to thank Kristina Toutanova for
providing a dataset and for helpful discussions re-
lated to this work. We also thank the four anony-
mous reviewers for their comments.
27
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,
Patrik Lambert, and Jos?e B. Mari?no. 2005. Statis-
tical Machine Translation of Euparl Data by Using
bilingual n-grams. In Proc. of ACL Workshop on
Building and Using Parallel Texts, pages 133?136,
Jun.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467?479,
Dec.
Josep Crego and Franc?ois Yvon. 2010. Factored bilin-
gual n-gram language models for statistical machine
translation. Machine Translation, 24(2):159?175.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60(1-3):195?227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning Semantic Representations
for the Phrase Translation Model. Technical Report
MSR-TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum
Entropy Training. In Proc. of ICASSP.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL, August.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning Deep
Structured Semantic Models for Web Search using
Clickthrough Data. In Proc. of CIKM, October.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. of NAACL Workshop
on Statistical Machine Translation, pages 102?121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov, Stefan Kombrink, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proc. of ICASSP, pages 5528?5531.
Tom?a?s Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746?751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proc. of NAACL,
pages 8?16, New York, Jun.
28
Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.
2012. Revisiting the Case for Explicit Syntactic
Information in Language Models. In NAACL-HLT
Workshop on the Future of Language Modeling for
HLT, pages 50?58. Association for Computational
Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Marta R. Costa-juss`a, and Jos?e A. R.
Fonollosa. 2007. Smooth Bilingual N -Gram Trans-
lation. In Proc. of EMNLP, pages 430?438, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, pages 8430?8434, Vancouver,
Canada, May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proc. of NAACL,
pages 12?21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Geoff Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proc. of ICASSP.
29
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of NAACL-HLT 2013, pages 12?21,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Beyond Left-to-Right: Multiple Decomposition Structures for SMT
Hui Zhang?
USC/ISI
Los Angeles, CA 90089
hzhang@isi.edu
Kristina Toutanova
Microsoft Research
Redmond, WA 98502
kristout@microsoft.com
Chris Quirk
Microsoft Research
Redmond, WA 98502
chrisq@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA 98502
jfgao@microsoft.com
Abstract
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
1 Introduction
The translation procedure of a classical phrase-
based translation model (Koehn et al, 2003) first di-
vides the input sentence into a sequence of phrases,
translates each phrase, explores reorderings of these
translations, and then scores the resulting candi-
dates with a linear combination of models. Conven-
tional models include phrase-based channel models
that effectively model each phrase as a large uni-
gram, reordering models, and target language mod-
els. Of these models, only the target language model
?This research was conducted during the author?s internship
at Microsoft Research
(and, to some weak extent, the lexicalized reordering
model) captures some lexical dependencies that span
phrase boundaries, though it is not able to model in-
formation from the source side. Larger phrases cap-
ture more contextual dependencies within a phrase,
but individual phrases are still translated almost in-
dependently.
To address this limitation, several researchers
have proposed bilingual n-gram Markov models
(Marino et al, 2006) to capture contextual depen-
dencies between phrase pairs. Much of their work
is limited by the requirement ?that the source and
target side of a tuple of words are synchronized, i.e.
that they occur in the same order in their respective
languages? (Crego and Yvon, 2010).
For language pairs with significant typological di-
vergences, such as Chinese-English, it is quite dif-
ficult to extract a synchronized sequence of units;
in the limit, the smallest synchronized unit may be
the whole sentence. Other approaches explore incor-
poration into syntax-based MT systems or replacing
the phrasal translation system altogether.
We investigate the addition of MTUs to a phrasal
translation system to improve modeling of con-
text and to provide more robust estimation of long
phrases. However, in a phrase-based system there
is no single synchronized traversal order; instead,
we may consider the translation units in many pos-
sible orders: left-to-right or right-to-left according
to either the source or the target are natural choices.
Alternatively we consider translating a particularly
unambiguous unit in the middle of the sentence
and building outwards from there. We investigate
both consistent and dynamic decomposition orders
in several language pairs, looking at distinct orders
in isolation and combination.
12
2 Related work
Marino et al (2006) proposed a translation model
using a Markov model of bilingual n-grams, demon-
strating state-of-the-art performance compared to
conventional phrase-based models. Crego and
Yvon (2010) further explored factorized n-gram ap-
proaches, though both models considered rather
large n-grams; this paper focuses on small units with
asynchronous orders in source and target. Durrani
et al (2011) developed a joint model that captures
translation of contiguous and gapped units as well as
reordering. Two prior approaches explored similar
models in syntax based systems. MTUs have been
used in dependency translation models (Quirk and
Menezes, 2006) to augment syntax directed trans-
lation systems. Likewise in target language syntax
systems, one can consider Markov models over min-
imal rules, where the translation probability of each
rule is adjusted to include context information from
parent rules (Vaswani et al, 2011).
Most prior work tends to replace the existing
probabilities rather than augmenting them. We be-
lieve that Markov rules provide an additional sig-
nal but are not a replacement. Their distributions
should be more informative than the so-called ?lex-
ical weighting? models, and less sparse than rela-
tive frequency estimates, though potentially not as
effective for truly non-compositional units. There-
fore, we explore the inclusion of all such informa-
tion. Also, unlike prior work, we explore combina-
tions of multiple decomposition orders, as well as
dynamic decompositions. The most useful context
for translation differs by language pair, an important
finding when working with many language pairs.
We build upon a standard phrase-based approach
(Koehn et al, 2003). This acts as a proposal dis-
tribution for translations; the MTU Markov models
provide additional signal as to which translations are
correct.
3 MTU n-gram Markov models
We begin by defining Minimal Translation Units
(MTUs) and describing how to identify them in
word-aligned text. Next we define n-gram Markov
models over MTUs, which requires us to define
traversal orders over MTUs.
?Yu ??ZuoTian ??JuXing
held the meeting
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 1: Word alignment and minimum translation units.
3.1 Definition of an MTU
Informally, the notion of a minimal translation unit
is simple: it is a translation rule that cannot be
broken down any further without violating the con-
straints of the rules. We restrict ourselves to contigu-
ous MTUs. They are similar to small phrase pairs,
though unlike phrase pairs we allow MTUs to have
either an empty source or empty target side, thereby
allowing insertion and deletion phrases. Conven-
tional phrase pairs may be viewed as compositions
of these MTUs up to a given size limit.
Consider a word-aligned sentence pair consisting
of a sequence of source words s = s1 . . . sm, a se-
quence of target words t = t1 . . . tn, and a word align-
ment relation between the source and target words
? ? {1..m} ? {1..n}. A translation unit is a sequence
of source words si..s j and a sequence of target words
tk..tl (one of which may be empty) such that for all
aligned pairs i? ? k?, we have i ? i? ? j if and only
if k ? k? ? l. This definition, nearly identical to
that of a phrase pair (Koehn et al, 2003), relaxes the
constraint that one aligned word must be present.
A set of translation units is a partition of the sen-
tence pair if each source and target word is covered
exactly once. Minimal translation units is the par-
tition with the smallest average unit size, or, equiv-
alently, the largest number of units. For example,
Figure 1 shows a word-aligned sentence pair and its
corresponding set of MTUs. We extract these min-
imal translation units with an algorithm similar to
that of phrase extraction.
We train n-gram Markov models only over min-
13
imal rules for two reasons. First, the segmentation
of the sentence pair is not unique under composed
rules, which makes probability estimation compli-
cated. Second, some phrase pairs are very large,
which results in sparse data issues and compromises
the model quality. Therefore, training an n-gram
model over minimal translation units turns out to
be a simple and clean choice: the resulting segmen-
tation is unique, and the distribution is smooth. If
we want to capture more context, we can simply in-
crease the order of the Markov model.
Such Markov models address issues in large
phrase-based translation approaches. Where stan-
dard phrase-based models rely upon large unigrams
to capture contextual information, n-grams of mini-
mal translation units allow a robust contextual model
that is less constrained by segmentation.
3.2 MTU enumeration orders
When defining a joint probability distribution over
MTUs of an aligned sentence pair, it is necessary
to define a decomposition, or generation order for
the sentence pair. For a single sequence in lan-
guage modeling or synchronized sequences in chan-
nel modeling, the default enumeration order has
been left-to-right.
Different decomposition orders have been used
in part-of-speech tagging and named entity recog-
nition (Tsuruoka and Tsujii, 2005). Intuitively, in-
formation from the left or right could be more use-
ful for particular disambiguation choices. Our re-
search on different decomposition orders was moti-
vated by this work. When applying such ideas to
machine translation, there are additional challenges
and opportunities. The task exhibits much more am-
biguity ? the number of possible MTUs is in the
millions. An opportunity arises from the reordering
phenomenon in machine translation: while in POS
tagging the natural decomposition orders to study
are only left-to-right and right-to-left, in machine
translation we can further distinguish source and tar-
get sentence orders.
We first define the source left-to-right and the tar-
get left-to-right orders of the aligned sets of MTUs.
The definition is straightforward when there are no
inserted or deleted word. To place the nulls corre-
sponding to such word we use the following defi-
nition: the source position of the null for a target
inserted word is just after the position of the last
source word aligned to the closest preceding non-
null aligned target word. The target position for a
null corresponding to a source deleted MTU is de-
fined analogously. In Figure 1 we define the posi-
tion of M4 to be right after M3 (because ?the? is
after ?held? in left-to-right order on the target side).
The complete MTU sequence in source left-to-
right order is M1-M2-M3-M4-M5. The sequence
in target left-to-right order is M3-M4-M5-M1-M2.
This illustrates that decomposition structure may
differ significantly depending on which language is
used to define the enumeration order.
Once a sentence pair is represented as a sequence
of MTUs, we can define the probability of the
sentence pair using a conventional n-gram Markov
model (MM) over MTUs. For example, the 3-gram
MM probability of the sentence pair in Figure 1
under the source left-to-right order is as follows:
P(M1)?P(M2|M1)?P(M3|M1,M2)?P(M4|M2,M3)?
P(M5|M3,M4).
Different decomposition orders use different con-
text for disambiguation and it is not clear apriori
which would perform best. We compare all four
decomposition orders (source order left-to-right and
right-to-left, and target order left-to-right and right-
to-left). Although the independence assumptions of
left-to-right and right-to-left are the same, the result-
ing models may be different due to smoothing.
In addition to studying these four basic decompo-
sition orders, we report performance of two cyclic
orders: cyclic in source or target sentence order.
These models are inspired by the cyclic depen-
dency network model proposed for POS tagging
(Toutanova et al, 2003) and also used as a baseline
in previous work on dynamic decomposition orders
(Tsuruoka and Tsujii, 2005). 1
The probability according to the cyclic orders is
defined by conditioning each MTU on both its left
and right neighbor MTUs. For example, the prob-
ability of the sentence pair in Figure 1 under the
source cyclic order, using a 3-gram model is defined
as: P(M1|M2) ? P(M2|M1,M3) ? P(M3|M2,M4) ?
P(M4|M3,M5) ? P(M5|M4).
All n-gram Markov models over MTUs are esti-
1The correct application of such models requires sampling
to find the highest scoring sequence, but we apply the max prod-
uct approximation as done in previous work.
14
mated using Kneser-Ney smoothing. Each MTU is
treated as an atomic unit in the vocabulary of the
n-gram model. Counts of all n-grams are obtained
from the parallel MT training data, using different
MTU enumeration orders.
Note that if we use a target-order decomposition,
the model provides a distribution over target sen-
tences and the corresponding source sides of MTUs,
albeit unordered. Likewise source order based mod-
els provide distributions over source sentences and
unordered target sides of MTUs. We attempted to
introduce reordering models to predict an order over
the resulting MTU sequences using approaches sim-
ilar to reordering models for phrases. Although
these models produced gains in some language pairs
when used without translation MTU MMs, there
were no additional gains over a model using mul-
tiple translation MTU MMs.
4 Lexical selection
We perform an empirical evaluation of different
MTU decomposition orders on a simplified machine
translation task: lexical selection. In this task we
assume that the source sentence segmentation into
minimal translation units is given and that the or-
der of the corresponding target sides of the minimal
translation units is also given. The problem is to
predict the target sides of the MTUs, called target
MTUs for brevity (see Figure 2). The lexical selec-
tion task is thus similar to sequence tagging tasks
like part-of-speech tagging, though much more dif-
ficult: the predicted variables are sequences of target
language words with millions of possible outcomes.
?Yu ??ZuoTian ??JuXing
held the meet ng
??HuiTan
yesterdaynull
null
M1 M2 M3 M5M4
M1: Yu => null                               M2: ZuoTian => yesterdayM3:                                  JuXing => heldM4:                                       null => theM5:                                 HuiTan => meeting
?Yu ??ZuoTian ??JuXing
? ? ?
??HuiTan
??
null
Figure 2: Lexical selection.
We use this constrained MT setting to evaluate the
performance of models using different MTU decom-
position orders and models using combinations of
decomposition orders. The simplified setting allows
controlled experimentation while lessening the im-
pact of complicating factors in a full machine trans-
lation setting (search error, reordering limits, phrase
table pruning, interaction with other models).
To perform the tagging task, we use trigram MTU
models. The four basic decomposition orders for
MTU Markov models we use are left-to-right in tar-
get sentence order, right-to-left in target sentence or-
der, left-to-right in source sentence order, and right-
to-left in source sentence order. We also consider
cyclic orders in source and target.
Regardless of the decomposition order used, we
perform decoding using a beam search decoder, sim-
ilar to ones used in phrase-based machine transla-
tion. The decoder builds target hypotheses in left-
to-right target sentence order. At each step, it fills in
the translation of the next source MTU, in the con-
text of the already predicted MTUs to its left. The
top scoring complete hypotheses covering the first m
MTUs are maintained in a beam. When scoring with
a target left-to-right MTU Markov model (L2RT),
we can score each partial hypothesis exactly at each
step. When scoring using a R2LT model or a source
order model, we use lower-order approximations to
the trigram MTU Markov model scores as future
scores, since not all needed context is available for a
hypothesis at the time of construction. As additional
context becomes available, the exact score can be
computed. 2
4.1 Basic decomposition order combinations
We first introduce two methods of combining differ-
ent decomposition orders: product and system com-
bination.
The product method arises naturally in the ma-
chine translation setting, where probabilities from
different models are multiplied together and further
weighted to form the log-linear model for machine
translation (Och and Ney, 2002). We define a similar
scoring function using a set of MTU Markov models
MM1, ...,MMk for a hypothesis h as follows:
Score(h) = ?1logPMM1(h) + ... + ?klogPMMk (h)
2We apply hypothesis recombination, which can merge hy-
potheses that are indistinguishable with respect to future contin-
uations. This is similar to recombination in a standard-phrase
based decoder with the difference that it is not always the last
two target MTUs that define the context needed by future ex-
tensions.
15
The weights ? of different models are trained on a
development set using MER training to maximize
the BLEU score of the resulting model. Note that
this method of model combination was not consid-
ered in any of the previous works comparing differ-
ent decompositions.
The system combination method is motivated
by prior work in machine translation which com-
bined left-to-right and right-to-left machine trans-
lation systems (Finch and Sumita, 2009). Simi-
larly, we perform sentence-level system combina-
tion between systems using different MTU Markov
models to come up with most likely translations.
If we have k systems guessing hypotheses based
on MM1, . . . ,MMk respectively, we generate 1000-
best lists from each system, resulting in a pool of
up to 1000k possible distinct translations. Each of
the candidate hypotheses from MMi is scored with
its Markov model log-probability logPMMi(h). We
compute normalized probabilities for each system?s
n-best by exponentiating and normalizing: Pi(h) ?
PMMi(h). If a hypothesis h is not in system i?s n-
best list, we assume its probability is zero according
to that system. The final scoring function for each
hypothesis in the combined list of candidates is:
Score(h) = ?1P1(h) + ... + ?kPk(h)
The weights ? for the combination are tuned using
MERT as for the product model.
4.2 Dynamic decomposition orders
A more complex combination method chooses the
best possible decomposition order for each transla-
tion dynamically, using a set of constraints to de-
fine the possible decomposition orders, and a set of
features to score the candidate decompositions. We
term this method dynamic combination. The score
of each translation is defined as its score according
to the highest-scoring decomposition order for that
translation.
This method is very similar to the bidirectional
tagging approach of Tsuruoka and Tsujii (2005).
For this approach we only explored combinations of
target language orders (L2RT, CycT, and R2LT). If
source language orders were included, the complex-
ity of decoding would increase substantially.
Figure 3 shows two possible decompositions for
a short MTU sequence. The structures displayed are
1
 1
2
1  2| 1
3
2  3| 2,	 1
4
2  4| 3,	 2
 1  2| 1,	 3 1  3| 4  4
Figure 3: Different decompositions.
directed graphical models. They define the set of
parents (context) used to predict each target MTU.
The decomposition structures we consider are lim-
ited to acyclic graphs where each node can have one
of the following parent configurations: no parents
(C = 0 in the Figure), one left parent (C = 1L),
one right parent (C = 1R), one left and one right
parent (C = LR), two left parents (C = 2L), and
two right parents (C = 2R). If all nodes have two
left parents, we recover the left-to-right decomposi-
tion order, and if all nodes have two right parents,
the right-to-left decomposition order. A mixture of
parent configurations defines a mixed, dynamic de-
composition order. The decomposition order chosen
varies from translation to translation.
A directed graphical model defines the probability
of an assignment of MTUs to the variable nodes as a
product of local probabilities of MTUs given their
parents. Here we extend this definition to scores
of assignments by using a linear model with con-
figuration features and log-probability features. The
configuration features are indicators of which par-
ent configuration is active at a node and the settings
of these features for the decompositions in Figure
3 are shown as assignments to the C variables. The
log-probability feature values are obtained by query-
ing the appropriate n-gram model: L2RT, CycT, or
R2LT. For a node with one or two left parents, the
log-probability is computed according to the L2RT
model. For a node with one or two right parents, the
R2LT model is queried. The CycT model is used for
nodes with one left and one right parent.
To find the best translation of a sentence the
model now searches over hidden decomposition or-
16
ders in addition to assignments to target MTUs. The
final score of a translation and decomposition is a
linear combination of the two types of feature values
? model log-probabilities and configuration types.
There is one feature weight for each parent con-
figuration (six configuration weights) and one fea-
ture weight for each component model (three model
weights). The final score of the second decomposi-
tion and assignment in Figure 3 is:
Score(h)
= 2 ? wC0 + wCLR + wC1R
+ wL2RlogPLR(m1) + wCyclogPCyc(m2|m1,m3)
+ wR2LlogPRL(m3|m4) + wL2RlogPLR(m4)
There are two main differences between our ap-
proach and that of Tsuruoka and Tsujii (2005): we
perform beam search with hypothesis recombination
instead of exact decoding (due to the larger size of
the hypothesis set), and we use parameters to be
able to globally weight the probabilities from dif-
ferent models and to develop preferences for using
certain types of decompositions. For example, the
model can learn to prefer right-to-left decomposi-
tions for one language pair, and left-to-right decom-
positions for another. An additional difference from
prior work is the definition of the possible decompo-
sition orders that are searched over.
Compared to the structures allowed in (Tsuruoka
and Tsujii, 2005) for a trigram baseline model, our
allowed structures are a subset; in (Tsuruoka and
Tsujii, 2005) there are sixteen possible parent con-
figurations (up to two left and two right parents),
whereas we allow only six. We train and use only
three n-gram Markov models to assign probabilities:
L2RT, R2LT, and CycT, whereas the prior work used
sixteen models. One could potentially see additional
gains from considering a larger space of structures
but the training time and runtime memory require-
ments might become prohibitive for the machine
translation task.
Because of the maximization over decomposition
structures, the score of a translation is not a simple
linear function of the features, but rather a maximum
over linear functions. The score of a translation for
a fixed decomposition is a linear function of the fea-
tures, but the score of a translation is a maximum of
linear functions (over decompositions). Therefore,
if we define hypotheses as just containing transla-
tions, MERT training does not work directly for op-
timizing the weights of the dynamic combination
method. 3 We used a combination of approaches;
we did MERT training followed by local simplex-
method search starting from three starting points:
the MERT solution, a weight vector that strongly
prefers left-to-right decompositions, and a weight-
vector that strongly prefers right-to-left decomposi-
tions. In the Experiments section, we report results
for the weights that achieved the best development
set performance.
5 N-best reranking
To evaluate the impact of these models in a full MT
system, we investigate n-best reranking. We use a
phrase-based MT system to output 1000-best can-
didate translations. For each candidate translation,
we have access to the phrase pairs it used as well as
the alignments inside each phrase pair. Thus, each
source sentence and its candidate translation form a
word-aligned parallel sentence pair. We can extract
MTU sequences from this sentence pair and com-
pute its probability according to MTU Markov mod-
els. These MTU MM log-probabilities are appended
to the original MT features and used to rerank the
1000-best list. The weight vectors for systems using
the original features along with one or more MTU
Markov model log-probabilities are trained on the
development set using MERT.
6 Experiments
We report experimental results on the lexical selec-
tion task and the reranking task on three language
pairs. The datasets used for the different languages
are described in detail in Section 6.2.
6.1 Lexical selection experiments
The data used for the lexical selection experiments
consists of the training portion of the datasets used
for MT. These training sets are split into three sec-
tions: lex-train, for training MTU Markov models
and extracting possible translations for each source
3If we include the decompositions in the hypotheses we
could use MERT but then the n-best lists used for training might
not contain much variety in terms of translation options. This is
an interesting direction for future research.
17
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline 06.45 06.30 11.60 10.98 15.09 14.40
Oracle 69.79 70.78 72.28 75.39 85.15 84.32
L2RT 24.02 25.09 28.69 28.70 49.86 46.45
R2LT 23.79 24.91 30.14 30.14* 49.22 46.58
CycT 18.59 20.33 25.91 26.83 41.30 38.85
L2RS 25.81 27.89* 25.52 25.10 45.69 43.98
R2LS 26.48 27.96* 26.03 26.30 47.36 43.91
CycS 21.62 23.38 22.68 23.58 39.11 36.44
Table 1: Lexical selection results for individual MTU
Markov models.
MTU, lex-dev for tuning combination weights for
systems using several MTU MMs, and lex-test, for
final evaluation results. The possible translations for
each source MTU are defined as the most frequent
100 translations seen in lex-train. The lex-dev sets
contain 200 sentence pairs each and the lex-test sets
contains 1000 sentence pairs each. These develop-
ment and test sets consist of equally spaced sen-
tences taken from the full MT training sets.
We start by reporting BLEU scores of the six in-
dividual MTU MMs on the three language pairs in
Table 1. The baseline predicts the most frequent tar-
get MTU for each source MTU (unigram MM not
using context). The oracle looks at the correct trans-
lation and always chooses the correct target MTU if
it is in the vocabulary of available MTUs.
We can see that there is a large difference between
the baseline and oracle performance, underscoring
the importance of modeling context for accurate pre-
diction. The best decomposition order varies from
language to language: right-to-left in source order is
best for Chinese-English, right-to-left in target order
is best for German-English and left-to-right or right-
to-left in target order are best in English-Bulgarian.
We computed statistical significance tests, testing
the difference between the L2RT model (the stan-
dard in prior work) and models achieving higher test
set performance. The models that are significantly
better at significance ? < 0.01 are marked with a
star in the table. We used a paired bootstrap test with
10,000 trials (Koehn, 2004).
Next we evaluate the methods for combining de-
composition orders introduced in Sections 4.1 and
4.2. The results are reported in Table 2. The up-
per part of the table focuses on combining different
Model Chs-En Deu-En En-Bgr
Dev Test Dev Test Dev Test
Baseline-1 24.04 25.09 30.14 30.14 49.86 46.45
TgtProduct 25.27 25.84* 30.47 30.49 51.04 47.27*
TgtSysComb 24.49 25.27 30.20 30.15 50.46 46.31
TgtDynamic 24.07 25.10 30.60 30.41 49.99 46.52
Baseline-2 26.48 27.96 30.14 30.14 49.86 46.45
AllProduct 28.68 29.59* 31.54 31.36* 51.50 48.10*
AllSyscomb 27.02 28.30 30.20 30.17 50.90 46.53
Table 2: Lexical selection results for combinations of
MTU Markov models.
target-order decompositions. The lower part of the
table looks at combining all six decomposition or-
ders. The baseline for the target order combinations,
Baseline-1, is the best single target MTU Markov
model from Table 1. Baseline-2 in the lower part
of the table is the best individual model out of all
six. We can see that the product models TgtProduct
(a product of the three target-order MTU MMs) and
AllProduct (a product of all six MTU MMs) are con-
sistently best. The dynamic decomposition models
TgtDynamic achieve slight but not significant gains
over the baseline. The combination models that are
statistically significantly better than corresponding
baselines (? < 0.01) are marked with a star.
Our takeaway from these experiments is that mul-
tiple decomposition orders are good, and thus taking
a product (which encourages agreement among the
models) is a good choice for this task. The dynamic
decomposition method shows some promise, but it
does not outperform the simpler product approach.
Perhaps a lager space of decompositions would
achieve better results, especially given a larger pa-
rameter set to trade off decompositions and better
tuning for those parameters.
6.2 Datasets and reranking settings
For Chinese-English, the training corpus consists
of 1 million sentence pairs from the FBIS and
HongKong portions of the LDC data for the NIST
MT evaluation. We used the union of the NIST
2002 and 2003 test sets as the development set and
the NIST 2005 test set as our test set. The baseline
phrasal system uses a 5-gram language model with
modified Kneser-Ney smoothing (Kenser and Ney,
1995), trained on the Xinhua portion of the English
Gigaword corpus (238M English words).
For German-English we used the dataset from
18
Language Training Dev Test
Chs-En 1 Mln NIST02+03 NIST05
Deu-En 751 K WMT06dev WMT06test
En-Bgr 4 Mln 1,497 2,498
Table 3: Data sets for different language pairs.
the WMT 2006 shared task on machine translation
(Koehn and Monz, 2006). The parallel training set
contains approximately 751K sentences. We also
used the English monolingual data of around 1 mil-
lion sentences for language model training. The de-
velopment set contains 2000 sentences. The final
test set (the in-domain test set for the shared task)
also contains 2000 sentences. Two Kneser-Ney lan-
guage models were used as separate features: a 4-
gram LM trained on the parallel portion of the data,
and a 5-gram LM trained on the monolingual corpus.
For English-Bulgarian we used a dataset con-
taining sentences from several data sources: JRC-
Acquis (Steinberger et al, 2006), TAUS4, and web-
scraped data. The development set consists of 1,497
sentences, the English side from WMT 2009 news
test data, and the Bulgarian side a human translation
thereof. The test set comes from the same mixture of
sources as the training set. For this system we used
a single four-gram target language model trained on
the target side of the parallel corpus.
All systems used phrase tables with a maximum
length of seven words on either side and lexicalized
reordering models. For the Chinese-English sys-
tem we used GIZA++ alignments, and for the other
two we used alignments by an HMM model aug-
mented with word-based distortion (He, 2007). The
alignments were symmetrized and then combined
with the heuristics ?grow-diag-final-and?. 5 We tune
parameters using MERT (Och, 2003) with random
restarts (Moore and Quirk, 2008) on the develop-
ment set. Case-insensitive BLEU-4 is our evaluation
metric (Papineni et al, 2002).
3-gram models 5-gram models
Model Dev Test Dev Test
Baseline 32.58 31.78 32.58 31.78
L2RT 33.05 32.78* 33.16 32.88*
R2LT 33.05 32.96* 33.16 32.81*
L2RS 32.90 33.00* 32.98 32.98*
R2LS 32.94 32.98* 33.09 32.96*
4 MMs 33.22 33.07* 33.37 33.00*
4 MMs phrs 32.58 31.78 32.58 31.78
Table 4: Reranking with 3-gram and 5-gram MTU trans-
lation models on Chinese-English. Starred results on the
test set indicate significantly better performance than the
baseline.
6.3 MT reranking experiments
We first report detailed experiments on Chinese-
English, and then verify our main conclusions on the
other language pairs. Table 4 looks at the impact of
individual 3-gram and 5-gram MTU Markov models
and their combination. Amongst the decomposition
orders tested (L2RT, R2LT, L2RS, and R2LS), each
of the individual MTU MMs was able to achieve
significant improvement over the baseline, around 1
BLEU point.6 The results achieved by the individ-
ual models differ, and the combination of four direc-
tions is better than the best individual direction, but
the difference is not statistically significant.
We ran an additional experiment to test whether
MTU MMs make effective use of context across
phrase boundaries, or whether they simply pro-
vide better smoothed estimates of phrasal transla-
tion probabilities. The last row of the table reports
the results achieved by a combination of MTU MMs
that do not use context across the phrasal bound-
aries. Since an MTU MM limited to look only inside
phrases can provide improved smoothing compared
to whole phrase relative frequency counts, it is con-
ceivable it could provide a large improvement. How-
ever, there is no improvement in practice for this lan-
guage pair; the additional improvements from MTU
MMs stem from modeling cross-phrase context.
4www.tausdata.org
5The combination heuristic was further refined to disallow
crossing one-to-many alignments, which would result in the ex-
traction of larger minimum translation units. We found that this
further refinement on the combination heuristic consistently im-
proved the BLEU scores by between 0.3 and 0.7.
6Here again we call a difference significant if the paired
bootstrap p-value is less than 0.01.
19
Table 5 shows the test set results of individ-
ual 3-gram MTU Markov models and the com-
bination of 3-gram and 5-gram models on the
English-Bulgarian and German-English datasets.
For English-Bulgarian all individual 3-gram Markov
models achieve significant improvements of close to
one point; their combination is better than the best
individual model (but not significantly). The indi-
vidual 5-gram models and their combination bring
much larger improvement, for a total increase of
2.82 points over the baseline. We believe the 5-
gram models were more effective in this setting be-
cause the larger training set alowed for successful
training of models of larger capacity. Also the in-
creased context size helps to resolve ambiguity in
the forms of morphologically-rich Bulgarian words.
For German-English we see a similar pattern, with
the combination of models outperforming the in-
dividual ones, and the 5-gram models being better
than the 3-gram. Here the individual 3-gram models
are better than the baseline at significance level 0.02
and their combination is better than the baseline at
our earlier defined threshold of 0.01. The within-
phrase MTU MMs (results shown in the last two
rows) improve upon the baseline slightly, but here
again the improvements mostly stem from the use of
context across phrase boundaries. Our final results
on German-English are better than the best result of
27.30 from the shared task (Koehn and Monz, 2006).
Thanks to the reviewers for referring us to re-
cent work by (Clark et al, 2011) that pointed out
problems with significance tests for machine trans-
lation, where the randomness and local optima in the
MERT weight tuning method lead to a large vari-
ance in development and test set performance across
different runs of optimization (using a different ran-
dom seed or starting point). (Clark et al, 2011) pro-
posed a stratified approximate randomization statis-
tical significance test, which controls for optimizer
instability. Using this test, for the English-Bulgarian
system, we confirmed that the combination of four
3-gram MMs and the combination of 5-gram MMs
is better than the baseline (p = .0001 for both, using
five runs of parameter tuning). We have not run the
test for the other language pairs.
Model En-Bgr Deu-En
Baseline 45.75 27.92
L2RT 3-gram 47.07* 28.15
R2LT 3-gram 47.06* 28.19
L2RS 3-gram 46.44* 28.15
R2LS 3-gram 47.04* 28.18
4 3-gram 47.17* 28.37*
4 5-gram 48.57* 28.47*
4 3-gram phrs 46.08 27.92
4 5-gram phrs 46.17* 27.93
Table 5: English-Bulgarian and German-English test set
results: reranking with MTU translation models.
7 Conclusions
We introduced models of Minimal Translation Units
for phrasal systems, and showed that they make a
substantial and statistically significant improvement
on three distinct language-pairs. Additionally we
studied the importance of decomposition order when
defining the probability of MTU sequences. In a
simplified lexical selection task, we saw that there
were large differences in performance among the
different decompositions, with the best decomposi-
tions differing by language. We investigated multi-
ple methods to combine decompositions and found
that a simple product approach was most effective.
Results in the lexical selection task were consistent
with those obtained in a full MT system, although
the differences among decompositions were smaller.
In future work, perhaps we would see larger gains
by including additional decomposition orders (e.g.,
top-down in a dependency tree), and taking this idea
deeper into the machine translation model, down to
the word-alignment and language-modeling levels.
We were surprised to find n-best reranking so ef-
fective. We are incorporating the models into first
pass decoding, in hopes of even greater gains.
References
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. ACL-11.
JM Crego and F Yvon. 2010. Factored bilingual n-
gram language models for statistical machine transla-
tion. Machine Translation, Special Issue: Pushing the
frontiers of SMT, 24(2):159?175.
20
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1045?
1054, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Andrew Finch and Eiichiro Sumita. 2009. Bidirectional
phrase-based machine translation. In In proceedings
of EMNLP.
Xiaodong He. 2007. Using word-dependent transition
models in hmm based word alignment for statistical
machine translation. In WMT workshop.
Reinhard Kenser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP 1995, pages 181?184.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, June.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In In Proceedings of
EMNLP.
JB Marino, RE Banchs, JM Crego, A de Gispert, P Lam-
bert, JA Fonollosa, and MR Costa-Jussa. 2006. N-
gram-based machine translation. Computational Lin-
guistics, 32(4):527?549.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error training for statistical ma-
chine translation. In Proc. Coling-08.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In In Proceedings of ACL,
pages 295?302.
Franz Joseph Och. 2003. Minimum error training in sta-
tistical machine translation. In Proc. ACL-03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. 40th Annual
Meeting of the ACL, pages 311?318.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 9?16, New York City, USA,
June. Association for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, Dan Tufis, and Dniel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In LREC,
Genoa, Italy.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In In Pro-
ceedings of HLT-NAACL.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In In proceedings of
HLT/EMNLP.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 856?864,
Portland, Oregon, USA, June. Association for Compu-
tational Linguistics.
21
Proceedings of NAACL-HLT 2013, pages 450?459,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Training MRF-Based Phrase Translation Models using Gradient Ascent 
 
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Xiaodong He 
Microsoft Research 
Redmond, WA, USA 
xiaohe@microsoft.com 
 
Abstract 
This paper presents a general, statistical 
framework for modeling phrase translation 
via Markov random fields. The model al-
lows for arbituary features extracted from a 
phrase pair to be incorporated as evidence. 
The parameters of the model are estimated 
using a large-scale discriminative training 
approach that is based on stochastic gradi-
ent ascent and an N-best list based expected 
BLEU as the objective function. The model 
is easy to be incoporated into a standard 
phrase-based statistical machine translation 
system, requiring no code change in the 
runtime engine. Evaluation is performed on 
two Europarl translation tasks, German-
English and French-English. Results show 
that incoporating the Markov random field 
model significantly improves the perfor-
mance of a state-of-the-art phrase-based 
machine translation system, leading to a 
gain of  0.8-1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of a 
phrase-based statistical machine translation (SMT) 
system. The most common method of constructing 
the phrase table takes a two-phase approach. First, 
the bilingual phrase pairs are extracted heuristical-
ly from an automatically word-aligned training da-
ta. The second phase is parameter estimation, 
where each phrase pair is assigned with some 
scores that are estimated based on counting of 
words or phrases on the same word-aligned train-
ing data. 
There has been a lot of research on improving 
the quality of the phrase table using more princi-
pled methods for phrase extraction (e.g., Lamber 
and Banchs 2005), parameter estimation (e.g., 
Wuebker et al 2010; He and Deng 2012), or both 
(e.g., Marcu and Wong 2002; Denero et al 2006). 
The focus of this paper is on the parameter estima-
tion phase. We revisit the problem of scoring a 
phrase translation pair by developing a new phrase 
translation model based on Markov random fields 
(MRFs) and large-scale discriminative training. 
We strive to address the following three primary 
concerns. 
First of all, instead of parameterizing a phrase 
translation pair using a set of scoring functions that 
are learned independently (e.g., phrase translation 
probabilities and lexical weights) we use a general, 
statistical framework in which arbitrary features 
extracted from a phrase pair can be incorporated to 
model the translation in a unified way. To this end, 
we propose the use of a MRF model.  
Second, because the phrase model has to work 
with other component models in an SMT system in 
order to produce good translations and the quality 
of translation is measured via BLEU score, it is de-
sirable to optimize the parameters of the phrase 
model jointly with other component models with 
respect to an objective function that is closely re-
lated to the evaluation metric under consideration, 
i.e., BLEU in this paper. To this end, we resort to a 
large-scale discriminative training approach, fol-
lowing the pioneering work of Liang et al (2006). 
Although there are established methods of tuning a 
handful of features on small training sets, such as 
the MERT method (Och 2003), the development of 
discriminative training methods for millions of fea-
tures on millions of sentence pairs is still an ongo-
ing area of research. A recent survey is due to 
Koehn (2010). In this paper we show that by using 
stochastic gradient ascent and an N-best list based 
450
expected BLEU as the objective function, large-
scale discriminative training can lead to significant 
improvements. 
The third primary concern is the ease of adop-
tion of the proposed method. To this end, we use a 
simple and well-established learning method, en-
suring that the results can be easily reproduced. 
We also develop the features for the MRF model in 
such a way that the resulting model is of the same 
format as that of a traditional phrase table. Thus, 
the model can be easily incorporated into a stand-
ard phrase-based SMT system, requiring no code 
change in the runtime engine. 
In the rest of the paper, Section 2 presents the 
MRF model for phrase translation. Section 3 de-
scribes the way the model parameters are estimated. 
Section 4 presents the experimental results on two 
Europarl translation tasks. Section 5 reviews pre-
vious work that lays the foundation of this study. 
Section 6 concludes the paper. 
2 Model 
The traditional translation models are directional 
models that are based on conditional probabilities. 
As suggested by the noisy-channel model for SMT 
(Brown et al 1993): 
? = argmax

| = argmax

()| (1) 
The Bayes rule leads us to invert the conditioning 
of translation probability from a foreign (source) 
sentence  to an English (target) translation .  
However, in practice, the implementation of 
state-of-the-art phrase-based SMT systems uses a 
weighted log-linear combination of several models 
?(,,)  including the logarithm of the phrase 
probability (and the lexical weight) in source-to-
target and target-to-source directions (Och and Ney 
2004) 
? = argmax ? 	?(,,)   (2) 
= argmax


(,) 
 
where   in ?(,,)  is a hidden structure that 
best derives  from , called the Viterbi derivation 
afterwards. In phrase-based SMT,  consists of (1) 
the segmentation of the source sentence into 
phrases, (2) the segmentation of the target sentence 
into phrases, and (3) an alignment between the 
source and target phrases. 
In this paper we use Markov random fields 
(MRFs) to model the joint distribution (, ) 
over a source-target translation phrase pair (, ), 
parameterized by . Different from the directional 
translation models, as in Equation (1), the MRF 
model is undirected, which we believe upholds the 
spirit of the use of bi-directional translation proba-
bilities under the log-linear framework. That is, the 
agreement or the compatibility of a phrase pair is 
more effective to score translation quality than a 
directional translation probability which is mod-
eled based on an imagined generative story does. 
2.1 MRF 
MRFs, also known as undirected graphical models, 
are widely used in modeling joint distributions of 
spatial or contextual dependencies of physical phe-
nomena (Bishop 2006). A Markov random field is 
constructed from a graph  . The nodes of the 
graph represent random variables, and edges define 
the independence semantics between the random 
variables. An MRF satisfies the Markov property, 
which states that a node is independent of all of its 
non-neighbors, defined by the clique configura-
tions of . In modeling a phrase translation pair, 
we define two types of nodes, (1) two phrase nodes 
and (2) a set of word nodes, each for a word in the-
se phrases, such as the graph in Figure 1. Let us 
denote a clique by  and the set of variables in that 
clique by ,  . Then, the joint distribution over 
the random variables in  is defined as 
(, ) = 	? (, ;)
() , (3) 
where  = , ? , || ,  = , ? , ||  and ()  is 
the set of cliques in , and each (, ;) is a 
non-negative potential function defined over a 
clique  that measures the compatibility of the var-
iables in ,  is a set of parameters that are used 
within the potential function.   in Equation (3), 
sometimes called the partition function, is a nor-
malization constant and is given by  
 = ? ? ? (, ;)
()   (4) 
= ? ? 
(, ) ,  
which ensures that the distribution (, ) given 
by Equation (3) is correctly normalized. The pres-
451
ence of  is one of the major limitations of MRFs 
because it is generally not feasible to compute due 
to the exponential number of terms in the summa-
tion. However, we notice that   is a global con-
stant which is independent of  and . Therefore, in 
ranking phrase translation hypotheses, as per-
formed by the decoder in SMT systems, we can 
drop   and simply rank each hypothesis by its 
unnormalized joint probability. In our implementa-
tion, we only store in the phrase table for each 
translation pair ,  its unnormalized probability, 
i.e., 
(, ) as defined in Equation (4). 
It is common to define MRF potential functions 
of the exponential form as , ; =
exp (), where  is a real-valued feature 
function over clique  and  is the weight of the 
feature function. In phrase-based SMT systems, the 
sentence-level translation probability from   to  
is decomposed as the product of a set of phrase 
translation probabilities. By dropping the phrase 
segmentation and distortion model components, we 
have  
(|) ? max

(|,) (5) 
(|,) = ? (|)(,)? ,  
where   is the Viterbi derivation. Similarly, the 
joint probability (,) can be decomposed as 
, ? max

(,,) (6) 
,, = ? (, )(,)?   
? ? log, ,?   
? ? ? ()?((,)),?   
= ?  ?Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266?274,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
  Learning Phrase-Based Spelling Error Models  
from Clickthrough Data 
 
 
 
Xu Sun? 
Dept. of Mathematical Informatics 
University of Tokyo, Tokyo, Japan 
xusun@mist.i.u-tokyo.ac.jp
Jianfeng Gao 
Microsoft Research 
Redmond, WA, USA 
jfgao@microsoft.com 
 
Daniel Micol 
Microsoft Corporation 
Munich, Germany 
danielmi@microsoft.com 
Chris Quirk 
Microsoft Research 
Redmond, WA, USA 
chrisq@microsoft.com 
 
                                                     
? The work was done when Xu Sun was visiting Microsoft Research Redmond. 
Abstract 
This paper explores the use of clickthrough data 
for query spelling correction. First, large amounts 
of query-correction pairs are derived by analyzing 
users' query reformulation behavior encoded in 
the clickthrough data. Then, a phrase-based error 
model that accounts for the transformation 
probability between multi-term phrases is trained 
and integrated into a query speller system. Expe-
riments are carried out on a human-labeled data 
set. Results show that the system using the 
phrase-based error model outperforms signifi-
cantly its baseline systems. 
1 Introduction 
Search queries present a particular challenge for 
traditional spelling correction methods for three 
main reasons (Ahmad and Kondrak, 2004).  First, 
spelling errors are more common in search queries 
than in regular written text: roughly 10-15% of 
queries contain misspelled terms (Cucerzan and 
Brill, 2004). Second, most search queries consist 
of a few key words rather than grammatical sen-
tences, making a grammar-based approach inap-
propriate. Most importantly, many queries con-
tain search terms, such as proper nouns and names, 
which are not well established in the language. 
For example, Chen et al (2007) reported that 
16.5% of valid search terms do not occur in their 
200K-entry spelling lexicon. 
Therefore, recent research has focused on the 
use of Web corpora and query logs, rather than 
human-compiled lexicons, to infer knowledge 
about misspellings and word usage in search 
queries (e.g., Whitelaw et al, 2009). Another 
important data source that would be useful for this 
purpose is clickthrough data. Although it is 
well-known that clickthrough data contain rich 
information about users' search behavior, e.g., 
how a user (re-) formulates a query in order to 
find the relevant document, there has been little 
research on exploiting the data for the develop-
ment of a query speller system. 
In this paper we present a novel method of 
extracting large amounts of query-correction pairs 
from the clickthrough data.  These pairs, impli-
citly judged by millions of users, are used to train 
a set of spelling error models. Among these 
models, the most effective one is a phrase-based 
error model that captures the probability of 
transforming one multi-term phrase into another 
multi-term phrase. Comparing to traditional error 
models that account for transformation probabili-
ties between single characters (Kernighan et al, 
1990) or sub-word strings (Brill and Moore, 
2000), the phrase-based model is more powerful 
in that it captures some contextual information by 
retaining inter-term dependencies. We show that 
this information is crucial to detect the correction 
of a query term, because unlike in regular written 
text, any query word can be a valid search term 
and in many cases the only way for a speller 
system to make the judgment is to explore its 
usage according to the contextual information. 
We conduct a set of experiments on a large 
data set, consisting of human-labeled 
266
query-correction pairs. Results show that the error 
models learned from clickthrough data lead to 
significant improvements on the task of query 
spelling correction. In particular, the speller sys-
tem incorporating a phrase-based error model 
significantly outperforms its baseline systems. 
To the best of our knowledge, this is the first 
extensive study of learning phase-based error 
models from clickthrough data for query spelling 
correction. The rest of the paper is structured as 
follows. Section 2 reviews related work. Section 3 
presents the way query-correction pairs are ex-
tracted from the clickthrough data. Section 4 
presents the baseline speller system used in this 
study. Section 5 describes in detail the phrase- 
based error model. Section 6 presents the expe-
riments. Section 7 concludes the paper. 
2 Related Work 
Spelling correction for regular written text is a 
long standing research topic. Previous researches 
can be roughly grouped into two categories: 
correcting non-word errors and real-word errors. 
In non-word error spelling correction, any 
word that is not found in a pre-compiled lexicon is 
considered to be misspelled.  Then, a list of lexical 
words that are similar to the misspelled word are 
proposed as candidate spelling corrections. Most 
traditional systems use a manually tuned similar-
ity function (e.g., edit distance function) to rank 
the candidates, as reviewed by Kukich (1992). 
During the last two decades, statistical error 
models learned on training data (i.e., 
query-correction pairs) have become increasingly 
popular, and have proven more effective (Ker-
nighan et al, 1990; Brill and Moore, 2000; Tou-
tanova and Moore, 2002; Okazaki et al, 2008).  
Real-word spelling correction is also referred 
to as context sensitive spelling correction (CSSC). 
It tries to detect incorrect usages of a valid word 
based on its context, such as "peace" and "piece" 
in the context "a _ of cake". A common strategy in 
CSSC is as follows. First, a pre-defined confusion 
set is used to generate candidate corrections, then 
a  scoring model, such as a trigram language 
model or na?ve Bayes classifier, is used to rank the 
candidates according to their context (e.g., 
Golding and Roth, 1996; Mangu and Brill, 1997; 
Church et al, 2007). 
When designed to handle regular written text, 
both CSSC and non-word error speller systems 
rely on a pre-defined vocabulary (i.e., either a 
lexicon or a confusion set). However, in query 
spelling correction, it is impossible to compile 
such a vocabulary, and the boundary between the 
non-word and real-word errors is quite vague. 
Therefore, recent research on query spelling 
correction has focused on exploiting noisy Web 
data and query logs to infer knowledge about 
misspellings and word usage in search queries. 
Cucerzan and Brill (2004) discuss in detail the 
challenges of query spelling correction, and 
suggest the use of query logs. Ahmad and Kon-
drak (2005) propose a method of estimating an 
error model from query logs using the EM algo-
rithm. Li et al (2006) extend the error model by 
capturing word-level similarities learned from 
query logs. Chen et al (2007) suggest using web 
search results to improve spelling correction. 
Whitelaw et al (2009) present a query speller 
system in which both the error model and the 
language model are trained using Web data. 
Compared to Web corpora and query logs, 
clickthrough data contain much richer informa-
tion about users? search behavior.  Although there 
has been a lot of research on using clickthrough 
data to improve Web document retrieval (e.g., 
Joachims, 2002; Agichtein et al, 2006; Gao et al, 
2009), the data have not been fully explored for 
query spelling correction. This study tries to learn 
error models from clickthrough data. To our 
knowledge, this is the first such attempt using 
clickthrough data. 
Most of the speller systems reviewed above are 
based on the framework of the source channel 
model. Typically, a language model (source 
model) is used to capture contextual information, 
while an error model (channel model) is consi-
dered to be context free in that it does not take into 
account any contextual information in modeling 
word transformation probabilities. In this study 
we argue that it is beneficial to capture contextual 
information in the error model. To this end, in-
spired by the phrase-based statistical machine 
translation (SMT) systems (Koehn et al, 2003; 
Och and Ney, 2004), we propose a phrase-based 
error model where we assume that query spelling 
correction is performed at the phrase level. 
In what follows, before presenting the phrase- 
based error model, we will first describe the 
clickthrough data and the query speller system we 
used in this study. 
3 Clickthrough Data and Spelling Cor-
rection 
This section describes the way the 
query-correction pairs are extracted from click-
267
through data. Two types of clickthrough data are 
explored in our experiment. 
The clickthrough data of the first type has been 
widely used in previous research and proved to be 
useful for Web search (Joachims, 2002; Agichtein 
et al, 2006; Gao et al, 2009) and query refor-
mulation (Wang and Zhai, 2008; Suzuki et al, 
2009). We start with this same data with the hope 
of achieving similar improvements in our task. 
The data consist of a set of query sessions that 
were extracted from one year of log files from a 
commercial Web search engine. A query session 
contains a query issued by a user and a ranked list 
of links (i.e., URLs) returned to that same user 
along with records of which URLs were clicked. 
Following Suzuki et al (2009), we extract 
query-correction pairs as follows. First, we extract 
pairs of queries Q1 and Q2 such that (1) they are 
issued by the same user; (2) Q2 was issued within 
3 minutes of Q1; and (3) Q2 contained at least one 
clicked URL in the result page while Q1 did not 
result in any clicks.  We then scored each query 
pair (Q1, Q2) using the edit distance between Q1 
and Q2, and retained those with an edit distance 
score lower than a pre-set threshold as query 
correction pairs. 
Unfortunately, we found in our experiments 
that the pairs extracted using the method are too 
noisy for reliable error model training, even with a 
very tight threshold, and we did not see any sig-
nificant improvement. Therefore, in Section 6 we 
will not report results using this dataset. 
The clickthrough data of the second type con-
sists of a set of query reformulation sessions 
extracted from 3 months of log files from a 
commercial Web browser.  A query reformulation 
session contains a list of URLs that record user 
behaviors that relate to the query reformulation 
functions, provided by a Web search engine. For 
example, almost all commercial search engines 
offer the "did you mean" function, suggesting a 
possible alternate interpretation or spelling of a 
user-issued query. Figure 1 shows a sample of the 
query reformulation sessions that record the "did 
you mean" sessions from three of the most pop-
ular search engines. These sessions encode the 
same user behavior: A user first queries for 
"harrypotter sheme park", and then clicks on the 
resulting spelling suggestion "harry potter theme 
park". In our experiments, we "reverse-engineer" 
the parameters from the URLs of these sessions, 
and deduce how each search engine encodes both 
a query and the fact that a user arrived at a URL 
by clicking on the spelling suggestion of the query 
? an important indication that the spelling sug-
gestion is desired. From these three months of 
query reformulation sessions, we extracted about 
3 million query-correction pairs. Compared to the 
pairs extracted from the clickthrough data of the 
first type (query sessions), this data set is much 
cleaner because all these spelling corrections are 
actually clicked, and thus judged implicitly, by 
many users. 
In addition to the "did you mean" function, 
recently some search engines have introduced two 
new spelling suggestion functions. One is the 
"auto-correction" function, where the search 
engine is confident enough to automatically apply 
the spelling correction to the query and execute it 
to produce search results for the user.  The other is 
the "split pane" result page, where one half por-
tion of the search results are produced using the 
original query, while the other half, usually vi-
sually separate portion of results are produced 
using the auto-corrected query. 
In neither of these functions does the user ever 
receive an opportunity to approve or disapprove 
of the correction. Since our extraction approach 
focuses on user-approved spelling suggestions, 
Google: 
http://www.google.com/search? 
hl=en&source=hp& 
q=harrypotter+sheme+park&aq=f&oq=&aqi= 
http://www.google.com/search? 
hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA& 
sa=X&oi=spell&resnum=0&ct= 
result&cd=1&ved=0CA4QBSgA& 
q=harry+potter+theme+park&spell=1 
Yahoo: 
http://search.yahoo.com/search; 
_ylt=A0geu6ywckBL_XIBSDtXNyoA? 
p=harrypotter+sheme+park& 
fr2=sb-top&fr=yfp-t-701&sao=1 
http://search.yahoo.com/search? 
ei=UTF-8&fr=yfp-t-701& 
p=harry+potter+theme+park 
&SpellState=n-2672070758_q-tsI55N6srhZa. 
qORA0MuawAAAA%40%40&fr2=sp-top 
Bing: 
http://www.bing.com/search? 
q=harrypotter+sheme+park&form=QBRE&qs=n 
http://www.bing.com/search? 
q=harry+potter+theme+park&FORM=SSRE 
Figure 1.  A sample of query reformulation sessions 
from three popular search engines. These sessions 
show that a user first issues the query "harrypotter 
sheme park", and then clicks on the resulting spell 
suggestion "harry potter theme park". 
268
we ignore the query reformulation sessions re-
cording either of the two functions. Although by 
doing so we could miss some basic, obvious 
spelling corrections, our experiments show that 
the negative impact on error model training is 
negligible. One possible reason is that our base-
line system, which does not use any error model 
learned from the clickthrough data, is already able 
to correct these basic, obvious spelling mistakes. 
Thus, including these data for training is unlikely 
to bring any further improvement. 
We found that the error models trained using 
the data directly extracted from the query refor-
mulation sessions suffer from the problem of 
underestimating the self-transformation probabil-
ity of a query P(Q2=Q1|Q1), because we only 
included in the training data the pairs where the 
query is different from the correction. To deal 
with this problem, we augmented the training data 
by including correctly spelled queries, i.e., the 
pairs (Q1, Q2) where Q1 = Q2.  First, we extracted a 
set of queries from the sessions where no spell 
suggestion is presented or clicked on. Second, we 
removed from the set those queries that were 
recognized as being auto-corrected by a search 
engine. We do so by running a sanity check of the 
queries against our baseline spelling correction 
system, which will be described in Section 6. If 
the system thinks an input query is misspelled, we 
assumed it was an obvious misspelling, and re-
moved it. The remaining queries were assumed to 
be correctly spelled and were added to the training 
data. 
4 The Baseline Speller System 
The spelling correction problem is typically 
formulated under the framework of the source 
channel model. Given an input query ? ?
??. . . ??, we want to find the best spelling correc-
tion ? ? ??. . . ??  among all candidate spelling 
corrections: 
?? ? argmax
?
???|?? (1) 
Applying Bayes' Rule and dropping the constant 
denominator, we have 
?? ? argmax
?
???|?????? (2) 
where the error model ???|?? models the trans-
formation probability from C to Q, and the lan-
guage model ????  models how likely C is a 
correctly spelled query. 
The speller system used in our experiments is 
based on a ranking model (or ranker), which can 
be viewed as a generalization of the source 
channel model. The system consists of two 
components: (1) a candidate generator, and (2) a 
ranker. 
In candidate generation, an input query is first 
tokenized into a sequence of terms. Then we scan 
the query from left to right, and each query term q 
is looked up in lexicon to generate a list of spel-
ling suggestions c whose edit distance from q is 
lower than a preset threshold. The lexicon we 
used contains around 430,000 entries; these are 
high frequency query terms collected from one 
year of search query logs. The lexicon is stored 
using a trie-based data structure that allows effi-
cient search for all terms within a maximum edit 
distance. 
The set of all the generated spelling sugges-
tions is stored using a lattice data structure, which 
is a compact representation of exponentially many 
possible candidate spelling corrections. We then 
use a decoder to identify the top twenty candi-
dates from the lattice according to the source 
channel model of Equation (2).  The language 
model (the second factor) is a backoff bigram 
model trained on the tokenized form of one year 
of query logs, using maximum likelihood estima-
tion with absolute discounting smoothing.  The 
error model (the first factor) is approximated by 
the edit distance function as 
?log???|?? ? EditDist??, ?? (3) 
The decoder uses a standard two-pass algorithm 
to generate 20-best candidates. The first pass uses 
the Viterbi algorithm to find the best C according 
to the model of Equations (2) and (3).  In the 
second pass, the A-Star algorithm is used to find 
the 20-best corrections, using the Viterbi scores 
computed at each state in the first pass as heuris-
tics. Notice that we always include the input query 
Q in the 20-best candidate list. 
The core of the second component of the 
speller system is a ranker, which re-ranks the 
20-best candidate spelling corrections. If the top 
C after re-ranking is different than the original 
query Q, the system returns C as the correction.   
Let f be a feature vector extracted from a query 
and candidate spelling correction pair (Q, C). The 
ranker maps f to a real value y that indicates how 
likely C is a desired correction of Q.  For example, 
a linear ranker simply maps f to y with a learned 
weight vector w such as ? ? ? ? ?, where w is 
optimized w.r.t. accuracy on a set of hu-
269
man-labeled (Q, C) pairs. The features in f are 
arbitrary functions that map (Q, C) to a real value. 
Since we define the logarithm of the probabilities 
of the language model and the error model (i.e., 
the edit distance function) as features, the ranker 
can be viewed as a more general framework, 
subsuming the source channel model as a special 
case. In our experiments we used 96 features and a 
non-linear model, implemented as a two-layer 
neural net, though the details of the ranker and the 
features are beyond the scope of this paper. 
5 A Phrase-Based Error Model 
The goal of the phrase-based error model is to 
transform a correctly spelled query C into a 
misspelled query Q. Rather than replacing single 
words in isolation, this model replaces sequences 
of words with sequences of words, thus incorpo-
rating contextual information. For instance, we 
might learn that ?theme part? can be replaced by 
?theme park? with relatively high probability, 
even though ?part? is not a misspelled word. We 
assume the following generative story: first the 
correctly spelled query C is broken into K 
non-empty word sequences c1, ?, ck, then each is 
replaced with a new non-empty word sequence q1, 
?, qk, and finally these phrases are permuted and 
concatenated to form the misspelled Q. Here, c 
and q denote consecutive sequences of words. 
To formalize this generative process, let S 
denote the segmentation of C into K phrases c1?cK, 
and let T denote the K replacement phrases 
q1?qK ? we refer to these (ci, qi) pairs as 
bi-phrases. Finally, let M denote a permutation of 
K elements representing the final reordering step. 
Figure 2 demonstrates the generative procedure. 
Next let us place a probability distribution over 
rewrite pairs. Let B(C, Q) denote the set of S, T, M 
triples that transform C into Q. If we assume a 
uniform probability over segmentations, then the 
phrase-based probability can be defined as: 
???|?? ? ? ???|?, ?? ? ???|?, ?, ??
??,?,???
???,??
 (4) 
As is common practice in SMT, we use the 
maximum approximation to the sum:  
???|?? ? max
??,?,???
???,??
???|?, ?? ? ???|?, ?, ?? (5) 
5.1 Forced Alignments 
Although we have defined a generative model for 
transforming queries, our goal is not to propose 
new queries, but rather to provide scores over 
existing Q and C pairs which act as features for 
the ranker. Furthermore, the word-level align-
ments between Q and C can most often be iden-
tified with little ambiguity. Thus we restrict our 
attention to those phrase transformations consis-
tent with a good word-level alignment. 
Let J be the length of Q, L be the length of C, 
and A = a1, ?, aJ be a hidden variable 
representing the word alignment. Each ai takes on 
a value ranging from 1 to L indicating its corres-
ponding word position in C, or 0 if the ith word in 
Q is unaligned. The cost of assigning k to ai is 
equal to the Levenshtein edit distance (Levensh-
tein, 1966) between the ith word in Q and the kth 
word in C, and the cost of assigning 0 to ai is equal 
to the length of the ith word in Q. We can deter-
mine the least cost alignment A* between Q and C 
efficiently using the A-star algorithm. 
When scoring a given candidate pair, we fur-
ther restrict our attention to those S, T, M triples 
that are consistent with the word alignment, which 
we denote as B(C, Q, A*). Here, consistency re-
quires that if two words are aligned in A*, then 
they must appear in the same bi-phrase (ci, qi). 
Once the word alignment is fixed, the final per-
mutation is uniquely determined, so we can safely 
discard that factor. Thus we have: 
???|?? ? max
??,?,???
???,?,???
???|?, ?? (6) 
For the sole remaining factor P(T|C, S), we 
make the assumption that a segmented query T = 
q1? qK is generated from left to right by trans-
forming each phrase c1?cK independently: 
C: ?disney theme park? correct query 
S: [?disney?, ?theme park?] segmentation 
T: [?disnee?, ?theme part?] translation 
M: (1 ? 2, 2? 1) permutation 
Q: ?theme part disnee? misspelled query 
Figure 2: Example demonstrating the generative 
procedure behind the phrase-based error model. 
 
270
???|?, ?? ? ? ????|???
?
??? , (7) 
where ????|???  is a phrase transformation 
probability, the estimation of which will be de-
scribed in Section 5.2.  
To find the maximum probability assignment 
efficiently, we can use a dynamic programming 
approach, somewhat similar to the monotone 
decoding algorithm described in Och (2002). 
Here, though, both the input and the output word 
sequences are specified as the input to the algo-
rithm, as is the word alignment. We define the 
quantity ?? to be the probability of the most likely 
sequence of bi-phrases that produce the first j 
terms of Q and are consistent with the word 
alignment and C. It can be calculated using the 
following algorithm: 
1. Initialization:  
 ?? ? 1 (8) 
2. Induction:  
 ?? ? max
????,???
????
???
??
?????????? (9) 
3. Total:   
 ???|?? ? ?? (10) 
The pseudo-code of the above algorithm is 
shown in Figure 3. After generating Q from left to 
right according to Equations (8) to (10), we record 
at each possible bi-phrase boundary its maximum 
probability, and we obtain the total probability at 
the end-position of Q. Then, by back-tracking the 
most probable bi-phrase boundaries, we obtain B*.  
The algorithm takes a complexity of O(KL2), 
where K is the total number of word alignments in 
A* which does not contain empty words, and L is 
the maximum length of a bi-phrase, which is a 
hyper-parameter of the algorithm. Notice that 
when we set L=1, the phrase-based error model is 
reduced to a word-based error model which as-
sumes that words are transformed independently 
from C to Q, without taking into account any 
contextual information. 
 
5.2 Model Estimation 
We follow a method commonly used in SMT 
(Koehn et al, 2003) to extract bi-phrases and 
estimate their replacement probabilities.  From 
each query-correction pair with its word align-
ment (Q, C, A*), all bi-phrases consistent with the 
word alignment are identified. Consistency here 
implies two things. First, there must be at least 
one aligned word pair in the bi-phrase. Second, 
there must not be any word alignments from 
words inside the bi-phrase to words outside the 
bi-phrase. That is, we do not extract a phrase pair 
if there is an alignment from within the phrase 
pair to outside the phrase pair. The toy example 
shown in Figure 4 illustrates the bilingual phrases 
we can generate by this process. 
 After gathering all such bi-phrases from the 
full training data, we can estimate conditional 
relative frequency estimates without smoothing. 
For example, the phrase transformation probabil-
ity ???|?? in Equation (7) can be estimated ap-
proximately as 
Input: biPhraseLattice ?PL? with length = K & height 
= L;  
Initialization: biPhrase.maxProb = 0; 
for (x = 0; x <= K ? 1; x++) 
      for (y = 1; y <= L; y++) 
            for (yPre = 1; yPre <= L; yPre++) 
            { 
                  xPre = x ? y;  
                  biPhrasePre = PL.get(xPre, yPre); 
                  biPhrase = PL.get(x, y); 
                  if (!biPhrasePre || !biPhrase) 
                         continue; 
                  probIncrs = PL.getProbIncrease(biPhrasePre,  
                                                                      biPhrase); 
                  maxProbPre = biPhrasePre.maxProb;  
                  totalProb = probIncrs + maxProbPre; 
                  if  (totalProb > biPhrase.maxProb)  
                  { 
                        biPhrase.maxProb = totalProb;  
                        biPhrase.yPre = yPre; 
                   } 
             } 
Result: record at each bi-phrase boundary its maxi-
mum probability (biPhrase.maxProb) and optimal 
back-tracking biPhrases (biPhrase.yPre). 
 
Figure 3: The dynamic programming algorithm for 
Viterbi bi-phrase segmentation. 
 A B C D E F  a A 
a #       adc ABCD 
d    #    d D 
c   #     dc CD 
f      #  dcf CDEF 
        c C 
        f F 
 
Figure 4: Toy example of (left) a word alignment 
between two strings "adcf" and "ABCDEF"; and (right) 
the bi-phrases containing up to four words that are 
consistent with the word alignment. 
 
 
271
???|?? ?
???, ??
? ???, ?????
 (11) 
where ???, ?? is the number of times that c is 
aligned to q in training data. These estimates are 
useful for contextual lexical selection with suffi-
cient training data, but can be subject to data 
sparsity issues. 
An alternate translation probability estimate 
not subject to data sparsity issues is the so-called 
lexical weight estimate (Koehn et al, 2003). 
Assume we have a word translation distribution 
???|??  (defined over individual words, not 
phrases), and a word alignment A between q and c; 
here, the word alignment contains (i, j) pairs, 
where  ? ? 1. . |?| and ? ? 0. . |?|, with 0 indicat-
ing an inserted word.  Then we can use the fol-
lowing estimate: 
????|?, ?? ??
1
|??|??, ?? ? ??|
? ????| ???
???,????
|?|
???
 (12) 
We assume that for every position in q, there is 
either a single alignment to 0, or multiple align-
ments to non-zero positions in c. In effect, this 
computes a product of per-word translation scores; 
the per-word scores are averages of all the trans-
lations for the alignment links of that word. We 
estimate the word translation probabilities using 
counts from the word aligned corpus: ???|?? ?
???,??
? ???,?????
. Here ???, ?? is the number of times that 
the words (not phrases as in Equation 11) c and q 
are aligned in the training data. These word based 
scores of bi-phrases, though not as effective in 
contextual selection, are more robust to noise and 
sparsity. 
Throughout this section, we have approached 
this model in a noisy channel approach, finding 
probabilities of the misspelled query given the 
corrected query. However, the method can be run 
in both directions, and in practice SMT systems 
benefit from also including the direct probability 
of the corrected query given this misspelled query 
(Och, 2002). 
5.3 Phrase-Based Error Model Features 
To use the phrase-based error model for spelling 
correction, we derive five features and integrate 
them into the ranker-based query speller system, 
described in Section 4. These features are as 
follows. 
? Two phrase transformation features: 
These are the phrase transformation scores 
based on relative frequency estimates in two 
directions. In the correction-to-query direc-
tion, we define the feature as  ?????, ?, ?? ?
log ???|?? , where ???|??  is computed by 
Equations (8) to (10), and ??????? is the rel-
ative frequency estimate of Equation (11).   
? Two lexical weight features: These are the 
phrase transformation scores based on the 
lexical weighting models in two directions. 
For example, in the correction-to-query di-
rection, we define the feature 
as ?????, ?, ?? ? log ???|??, where ???|?? 
is computed by Equations (8) to (10), and the 
phrase transformation probability is the 
computed as lexical weight according to Eq-
uation (12). 
? Unaligned word penalty feature: the feature 
is defined as the ratio between the number of 
unaligned query words and the total number 
of query words. 
6 Experiments 
We evaluate the spelling error models on a large 
scale real world data set containing 24,172 queries 
sampled from one year?s worth of query logs from 
a commercial search engine. The spelling of each 
query is judged and corrected by four annotators.  
We divided the data set into training and test 
data sets. The two data sets do not overlap. The 
training data contains 8,515 query-correction 
pairs, among which 1,743 queries are misspelled 
(i.e., in these pairs, the corrections are different 
from the queries). The test data contains 15,657 
query-correction pairs, among which 2,960 que-
ries are misspelled. The average length of queries 
in the training and test data is 2.7 words.  
The speller systems we developed in this study 
are evaluated using the following three metrics. 
? Accuracy: The number of correct outputs 
generated by the system divided by the total 
number of queries in the test set. 
? Precision: The number of correct spelling 
corrections for misspelled queries generated 
by the system divided by the total number of 
corrections generated by the system. 
? Recall: The number of correct spelling cor-
rections for misspelled queries generated by 
the system divided by the total number of 
misspelled queries in the test set. 
We also perform a significance test, i.e., a t-test 
with a significance level of 0.05. A significant 
difference should be read as significant at the 95% 
level. 
272
In our experiments, all the speller systems are 
ranker-based. In most cases, other than the base-
line system (a linear neural net), the ranker is a 
two-layer neural net with 5 hidden nodes. The free 
parameters of the neural net are trained to optim-
ize accuracy on the training data using the back 
propagation algorithm, running for 500 iterations 
with a very small learning rate (0.1) to avoid 
overfitting. We did not adjust the neural net 
structure (e.g., the number of hidden nodes) or 
any training parameters for different speller sys-
tems. Neither did we try to seek the best tradeoff 
between precision and recall. Since all the sys-
tems are optimized for accuracy, we use accuracy 
as the primary metric for comparison. 
Table 1 summarizes the main spelling correc-
tion results.  Row 1 is the baseline speller system 
where the source-channel model of Equations (2) 
and (3) is used. In our implementation, we use a 
linear ranker with only two features, derived 
respectively from the language model and the 
error model models. The error model is based on 
the edit distance function. Row 2 is the rank-
er-based spelling system that uses all 96 ranking 
features, as described in Section 4. Note that the 
system uses the features derived from two error 
models.  One is the edit distance model used for 
candidate generation. The other is a phonetic 
model that measures the edit distance between the 
metaphones (Philips, 1990) of a query word and 
its aligned correction word. Row 3 is the same 
system as Row 2, with an additional set of features 
derived from a word-based error model. This 
model is a special case of the phrase-based error 
model described in Section 5 with the maximum 
phrase length set to one.  Row 4 is the system that 
uses the additional 5 features derived from the 
phrase-based error models with a maximum 
bi-phrase length of 3. 
In phrase based error model, L is the maxi-
mum length of a bi-phrase (Figure 3). This value 
is important for the spelling performance. We 
perform experiments to study the impact of L; 
the results are displayed in Table 2. Moreover, 
since we proposed to use clickthrough data for 
spelling correction, it is interesting to study the 
impact on spelling performance from the size of 
clickthrough data used for training. We varied 
the size of clickthrough data and the experi-
mental results are presented in Table 3. 
The results show first and foremost that the 
ranker-based system significantly outperforms 
the spelling system based solely on the 
source-channel model, largely due to the richer 
set of features used (Row 1 vs. Row 2).  Second, 
the error model learned from clickthrough data 
leads to significant improvements (Rows 3 and 4 
vs. Row 2).  The phrase-based error model, due to 
its capability of capturing contextual information, 
outperforms the word-based model with a small 
but statistically significant margin (Row 4 vs. 
Row 3), though using phrases longer (L > 3) does 
not lead to further significant improvement (Rows 
6 and 7 vs. Rows 8 and 9). Finally, using more 
clickthrough data leads to significant improve-
ment (Row 13 vs. Rows 10 to 12). The benefit 
does not appear to have peaked ? further im-
provements are likely given a larger data set. 
7 Conclusions 
Unlike conventional textual documents, most 
search queries consist of a sequence of key words, 
many of which are valid search terms but are not 
stored in any compiled lexicon. This presents a 
challenge to any speller system that is based on a 
dictionary.  
This paper extends the recent research on using 
Web data and query logs for query spelling cor-
rection in two aspects. First, we show that a large 
amount of training data (i.e. query-correction 
pairs) can be extracted from clickthrough data, 
focusing on query reformulation sessions. The 
resulting data are very clean and effective for 
error model training. Second, we argue that it is 
critical to capture contextual information for 
query spelling correction. To this end, we propose 
# System Accuracy Precision Recall 
1 Source-channel 0.8526 0.7213 0.3586 
2 Ranker-based 0.8904 0.7414 0.4964 
3 Word model 0.8994 0.7709 0.5413 
4 Phrase model (L=3) 0.9043 0.7814 0.5732 
Table 1. Summary of spelling correction results. 
# System Accuracy Precision Recall 
5 Phrase model (L=1) 0.8994 0.7709 0.5413 
6 Phrase model (L=2) 0.9014 0.7795 0.5605 
7 Phrase model (L=3) 0.9043 0.7814 0.5732 
8 Phrase model (L=5) 0.9035 0.7834 0.5698 
9 Phrase model (L=8) 0.9033 0.7821 0.5713 
Table 2. Variations of spelling performance as a func-
tion of phrase length. 
 
# System Accuracy Precision Recall 
10 L=3; 0 month data 0.8904 0.7414 0.4964 
11 L=3; 0.5 month data 0.8959 0.7701 0.5234 
12 L=3; 1.5 month data 0.9023 0.7787 0.5667 
13 L=3; 3 month data 0.9043 0.7814 0.5732 
Table 3. Variations of spelling performance as a func-
tion of the size of clickthrough data used for training. 
 
 
273
a new phrase-based error model, which leads to 
significant improvement in our spelling correc-
tion experiments.  
There is additional potentially useful informa-
tion that can be exploited in this type of model. 
For example, in future work we plan to investigate 
the combination of the clickthrough data collected 
from a Web browser with the noisy but large 
query sessions collected from a commercial 
search engine. 
Acknowledgments 
The authors would like to thank Andreas Bode, 
Mei Li, Chenyu Yan and Galen Andrew for the 
very helpful discussions and collaboration. 
References 
Agichtein, E., Brill, E. and Dumais, S. 2006. Im-
proving web search ranking by incorporating user 
behavior information. In SIGIR, pp. 19-26. 
Ahmad, F., and Kondrak, G. 2005. Learning a 
spelling error model from search query logs. In 
HLT-EMNLP, pp 955-962. 
Brill, E., and Moore, R. C. 2000. An improved error 
model for noisy channel spelling correction. In 
ACL, pp. 286-293. 
Chen, Q., Li, M., and Zhou, M. 2007. Improving 
query spelling correction using web search results. 
In EMNLP-CoNLL, pp. 181-189. 
Church, K., Hard, T., and Gao, J. 2007. Compress-
ing trigram language models with Golomb cod-
ing. In EMNLP-CoNLL, pp. 199-207. 
Cucerzan, S., and Brill, E. 2004. Spelling correction 
as an iterative process that exploits the collective 
knowledge of web users. In EMNLP, pp. 293-300. 
Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y. 
2009. Smoothing clickthrough data for web 
search ranking. In SIGIR.  
Golding, A. R., and Roth, D. 1996. Applying win-
now to context-sensitive spelling correction. In 
ICML, pp. 182-190. 
Joachims, T. 2002. Optimizing search engines using 
clickthrough data. In SIGKDD, pp. 133-142. 
Kernighan, M. D., Church, K. W., and Gale, W. A. 
1990. A spelling correction program based on a 
noisy channel model. In COLING, pp. 205-210. 
Koehn, P., Och, F., and Marcu, D. 2003. Statistical 
phrase-based translation. In HLT/NAACL, pp. 
127-133. 
Kukich, K. 1992. Techniques for automatically 
correcting words in text. ACM Computing Sur-
veys. 24(4): 377-439. 
Levenshtein, V. I. 1966. Binary codes capable of 
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707-710. 
Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006. 
Exploring distributional similarity based models 
for query spelling correction. In ACL, pp. 
1025-1032. 
Mangu, L., and Brill, E. 1997. Automatic rule ac-
quisition for spelling correction. In ICML, pp. 
187-194. 
Och, F. 2002. Statistical machine translation: from 
single-word models to alignment templates. PhD 
thesis, RWTH Aachen. 
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Okazaki, N., Tsuruoka, Y., Ananiadou, S., and 
Tsujii, J. 2008. A discriminative candidate gene-
rator for string transformations. In EMNLP, pp. 
447-456. 
Philips, L. 1990. Hanging on the metaphone. 
Computer Language Magazine, 7(12):38-44. 
Suzuki, H., Li, X., and Gao, J. 2009. Discovery of 
term variation in Japanese web search queries. In 
EMNLP. 
Toutanova, K., and Moore, R. 2002. Pronunciation 
modeling for improved spelling correction. In 
ACL, pp. 144-151. 
Wang, X., and Zhai, C. 2008. Mining term associa-
tion patterns from search logs for effective query 
reformulation. In CIKM, pp. 479-488. 
Whitelaw, C., Hutchinson, B., Chung, G. Y., and 
Ellis, G. 2009. Using the web for language inde-
pendent spellchecking and autocorrection. In 
EMNLP, pp. 890-899.  
274
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699?709,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Learning Continuous Phrase Representations for                                     
Translation Modeling 
 
Jianfeng Gao    Xiaodong He    Wen-tau Yih    Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,xiaohe,scottyih,deng}@microsoft.com 
 
 
 
Abstract 
This paper tackles the sparsity problem in 
estimating phrase translation probabilities 
by learning continuous phrase representa-
tions, whose distributed nature enables the 
sharing of related phrases in their represen-
tations. A pair of source and target phrases 
are projected into continuous-valued vec-
tor representations in a low-dimensional 
latent space, where their translation score 
is computed by the distance between the 
pair in this new space. The projection is 
performed by a neural network whose 
weights are learned on parallel training 
data. Experimental evaluation has been 
performed on two WMT translation tasks. 
Our best result improves the performance 
of a state-of-the-art phrase-based statistical 
machine translation system trained on 
WMT 2012 French-English data by up to 
1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of 
phrase-based statistical machine translation (SMT) 
systems. The most common method of construct-
ing the phrase table takes a two-phase approach 
(Koehn et al 2003). First, the bilingual phrase 
pairs are extracted heuristically from an automat-
ically word-aligned training data. The second 
phase, which is the focus of this paper, is parame-
ter estimation where each phrase pair is assigned 
with some scores that are estimated based on 
counting these phrases or their words using the 
same word-aligned training data. 
Phrase-based SMT systems have achieved 
state-of-the-art performance largely due to the fact 
that long phrases, rather than single words, are 
used as translation units so that useful context in-
formation can be captured in selecting translations. 
However, longer phrases occur less often in train-
ing data, leading to a severe data sparseness prob-
lem in parameter estimation. There has been a 
plethora of research reported in the literature on 
improving parameter estimation for the phrase 
translation model (e.g., DeNero et al 2006; 
Wuebker et al 2010; He and Deng 2012; Gao and 
He 2013).  
This paper revisits the problem of scoring a 
phrase translation pair by developing a Continu-
ous-space Phrase Translation Model (CPTM). 
The translation score of a phrase pair in this model 
is computed as follows. First, we represent each 
phrase as a bag-of-words vector, called word vec-
tor henceforth. We then project the word vector, 
in either the source language or the target lan-
guage, into a respective continuous feature vector 
in a common low-dimensional space that is lan-
guage independent. The projection is performed 
by a multi-layer neural network. The projected 
feature vector forms the continuous representa-
tion of a phrase. Finally, the translation score of a 
source-target phrase pair is computed by the dis-
tance between their feature vectors.  
The main motivation behind the CPTM is to 
alleviate the data sparseness problem associated 
with the traditional counting-based methods by 
grouping phrases with a similar meaning across 
different languages. This style of grouping is 
made possible because of the distributed nature of 
the continuous-space representations for phrases. 
No such sharing was possible in the original sym-
bolic space for representing words or phrases.  In 
this model, semantically or grammatically related 
phrases, in both the source and the target lan-
guages, would tend to have similar (close) feature 
vectors in the continuous space, guided by the 
training objective. Since the translation score is a 
smooth function of these feature vectors, a small 
699
change in the features should only lead to a small 
change in the translation score. 
The primary research task in developing the 
CPTM is learning the continuous representation 
of a phrase that is effective for SMT. Motivated 
by recent studies on continuous-space language 
models (e.g., Bengio et al 2003; Mikolov et al 
2011; Schwenk et al, 2012), we use a neural net-
work to project a word vector to a feature vector. 
Ideally, the projection would discover those latent 
features that are useful to differentiate good trans-
lations from bad ones, for a given source phrase. 
However, there is no training data with explicit 
annotation on the quality of phrase translations. 
The phrase translation pairs are hidden in the par-
allel source-target sentence pairs, which are used 
to train the traditional translation models. The 
quality of a phrase translation can only be judged 
implicitly through the translation quality of the 
sentences, as measured by BLEU, which contain 
the phrase pair. In order to overcome this chal-
lenge and let the BLEU metric guide the projec-
tion learning, we propose a new method to learn 
the parameters of a neural network. This new 
method, via the choice of an appropriate objective 
function in training, automatically forces the fea-
ture vector of a source phrase to be closer to the 
feature vectors of its candidate translations. As a 
result, the BLEU score is improved when these 
translations are selected by an SMT decoder to 
produce final, sentence-level translations. The 
new learning method makes use of the L-BFGS 
algorithm and the expected BLEU as the objective 
function defined on N-best lists. 
To the best of our knowledge, the CPTM pro-
posed in this paper is the first continuous-space 
phrase translation model that makes use of joint 
representations of a phrase in the source language 
and its translation in the target language (to be de-
tailed in Section 4) and that is shown to lead to 
significant improvement over a standard phrase-
based SMT system (to be detailed in Section 6).  
Like the traditional phrase translation model, 
the translation score of each bilingual phrase pair 
is modeled explicitly in our model. However, in-
stead of estimating the phrase translation score on 
aligned parallel data, our model intends to capture 
the grammatical and semantic similarity between 
a source phrase and its paired target phrase by pro-
jecting them into a common, continuous space 
that is language independent. 
                                                          
1 Niehues et al (2011) use different translation units in order 
to integrate the n-gram translation model into the phrase-
based approach. However, it is not clear how a continuous 
The rest of the paper is organized as follows. 
Section 2 reviews previous work. Section 3 re-
views the log-linear model for phrase-based SMT 
and Sections 4 presents the CPTM. Section 5 de-
scribes the way the model parameters are esti-
mated, followed by the experimental results in 
Section 6. Finally, Section 7 concludes the paper. 
2 Related Work 
Representations of words or documents as contin-
uous vectors have a long history. Most of the ear-
lier latent semantic models for learning such vec-
tors are designed for information retrieval 
(Deerwester et al 1990; Hofmann 1999; Blei et al 
2003). In contrast, recent work on continuous 
space language models, which estimate the prob-
ability of a word sequence in a continuous space 
(Bengio et al 2003; Mikolov et al 2010), have ad-
vanced the state of the art in language modeling, 
outperforming the traditional n-gram model on 
speech recognition (Mikolov et al 2012; Sunder-
meyer et al 2013) and machine translation 
(Mikolov 2012; Auli et al 2013). 
Because these models are developed for mono-
lingual settings, word embedding from these mod-
els is not directly applicable to translation. As a 
result, variants of such models for cross-lingual 
scenarios have been proposed so that words in dif-
ferent languages are projected into the shared la-
tent vector space (Dumais et al 1997; Platt et al 
2010; Vinokourov et al 2002; Yih et al 2011; 
Gao et al 2011; Huang et al 2013; Zou et al 
2013). In principle, a phrase table can be derived 
using any of these cross-lingual models, although 
decoupling the derivation from the SMT training 
often results in suboptimal performance (e.g., 
measured in BLEU), as we will show in Section 6. 
Recently, there is growing interest in applying 
continuous-space models for translation. The 
most related to this study is the work of continu-
ous space n-gram translation models (Schwenk et 
al. 2007; Schwenk 2012; Son et al 2012), where 
the feed-forward neural network language model 
is extended to represent translation probabilities. 
However, these earlier studies focused on the n-
gram translation models, where the translation 
probability of a phrase or a sentence is decom-
posed as a product of n-gram probabilities as in a 
standard n-gram language model. Therefore, it is 
not clear how their approaches can be applied to 
the phrase translation model1, which is much more 
version of such a model can be trained efficiently because the 
factor models used by Son et al cannot be applied directly. 
700
widely used in modern SMT systems. In contrast, 
our model learns jointly the representations of a 
phrase in the source language as well as its trans-
lation in the target language. The recurrent contin-
uous translation models proposed by Kalchbren-
ner and Blunsom (2013) also adopt the recurrent 
language model (Mikolov et al 2010). But unlike 
the n-gram translation models above, they make 
no Markov assumptions about the dependency of 
the words in the target sentence. Continuous space 
models have also been used for generating trans-
lations for new words (Mikolov et al 2013a) and 
ITG reordering (Li et al 2013). 
There has been a lot of research on improving 
the phrase table in phrase-based SMT (Marcu and 
Wong 2002; Lamber and Banchs 2005; Denero et 
al. 2006; Wuebker et al 2010; Zhang et al, 2011; 
He and Deng 2012; Gao and He 2013). Among 
them, (Gao and He 2013) is most relevant to the 
work described in this paper. They estimate 
phrase translation probabilities using a discrimi-
native training method under the N-best reranking 
framework of SMT. In this study we use the same 
objective function to learn the continuous repre-
sentations of phrases, integrating the strengths as-
sociated with these earlier studies. 
3 The Log-Linear Model for SMT 
Phrase-based SMT is based on a log-linear model 
which requires learning a mapping between input 
? ? ? to output ? ? ?. We are given 
? Training samples (?? , ??)  for ? = 1??,  
where each source sentence ?? is paired with 
a reference translation in target language ??; 
? A procedure GEN to generate a list of N-best 
candidates GEN(??) for an input ?? , where 
GEN  in this study is the baseline phrase-
based SMT system, i.e., an in-house 
implementation of the Moses system (Koehn 
et al 2007) that does not use the CPTM, and 
each ? ? GEN(??)  is labeled by the 
sentence-level BLEU score (He and Deng 
2012), denoted by sBleu(??, ?) , which 
measures the quality of ? with respect to its 
reference translation ??; 
? A vector of features ? ? ?? that maps each 
(??, ?) to a vector of feature values
2; and 
? A parameter vector ? ? ??, which assigns a 
real-valued weight to each feature. 
                                                          
2 Our baseline system uses a set of standard features sug-
gested in Koehn et al (2007), which is also detailed in Sec-
tion 6. 
The components GEN(. ), ? and ?  define a log-
linear model that maps ?? to an output sentence as 
follows: 
?? = argmax
(?,?)?GEN(??)
?T?(??, ?, ?) (1) 
which states that given ? and ?, argmax returns 
the highest scoring translation ??,  maximizing 
over  correspondences ?. In phrase-based SMT, ? 
consists of a segmentation of the source and target 
sentences into phrases and an alignment between 
source and target phrases. Since computing the 
argmax  exactly is intractable, it is commonly 
performed approximatedly by beam search (Och 
and Ney 2004). Following Liang et al (2006), we 
assume that every translation candidate is always 
coupled with a corresponding ?, called the Viterbi 
derivation, generated by (1). 
4 A Continuous-Space Phrase Transla-
tion Model (CPTM) 
The architecture of the CPTM is shown in Figures 
1 and 2, where for each pair of source and target 
phrases (??, ??)  in a source-target sentence pair, 
we first project them into feature vectors ??? and 
??? in a latent, continuous space via a neural net-
work with one hidden layer (as shown in Figure 
2), and then compute the translation score, 
score(??, ??), by the distance of their feature vec-
tors in that space. 
We start with a bag-of-words representation of 
a phrase ? ? ??, where ? is a word vector and ? 
is the size of the vocabulary consisting of words 
in both source and target languages, which is set 
to 200K in our experiments. We then learn to pro-
ject ? to a low-dimensional continuous space ??: 
?(?): ?? ? ??  
The projection is performed using a fully con-
nected neural network with one hidden layer and 
tanh activation functions. Let ?1 be the projec-
tion matrix from the input layer to the hidden layer 
and ?2  the projection matrix from the hidden 
layer to the output layer, we have 
? ? ?(?) = tanh (?2
T(tanh(?1
T?))) (2) 
 
 
701
 
Figure 2. A neural network model for phrases 
giving rise to their continuous representations. 
The model with the same form is used for both 
source and target languages. 
 
  
The translation score of a source phrase f and a 
target phrase e can be measured as the similarity   
(or distance) between their feature vectors. We 
choose the dot product as the similarity function3: 
score(?, ?) ? sim?(?? , ??) = ??
T?? (3) 
According to (2), we see that the value of the scor-
ing function is determined by the projection ma-
trices ? = {?1,?2}. 
The CPTM of (2) and (3) can be incorporated 
into the log-linear model for SMT (1) by 
                                                          
3 In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for 
nonlinear multi-layer neural networks, and the latter works 
better for linear neural networks. For the sake of clarity, we 
choose dot product when we describe the CPTM and its train-
ing in Sections 4 and 5, respectively. 
4 The baseline SMT needs to be reasonably good in the 
sense that the oracle BLEU score on the generated n-best 
introducing a new feature ??+1  and a new feature 
weight ??+1. The new feature is defined as 
??+1(??, ?, ?) = ? sim?(?? , ??)(?,? )??   (4) 
Thus, the phrase-based SMT system, into which 
the CPTM is incorporated, is parameterized by 
(?, ?), where ? is a vector of a handful of param-
eters used in the log-linear model of (1), with one 
weight for each feature; and ? is the projection 
matrices used in the CPTM defined by (2) and (3). 
In our experiments we take three steps to learn 
(?, ?): 
1. We use a baseline phrase-based SMT sys-
tem to generate for each source sentence in 
training data an N-best list of translation hy-
potheses4. 
2. We set ? to that of the baseline system and 
let ??+1 = 1, and optimize ? w.r.t. a loss 
function on training data5. 
3. We fix ? , and optimize ?  using MERT 
(Och 2003) to maximize BLEU on dev data. 
In the next section, we will describe Step 2 in de-
tail as it is directly related to the CPTM training. 
 
lists needs to be significantly higher than that of the top-1 
translations so that the CPTM can be effectively trained. 
5 The initial value of ??+1 can also be tuned using the dev 
set. However, we find in a pilot study that it is good enough 
to set it to 1 when the values of all the baseline feature 
weights, used in the log-linear model of (1), are properly nor-
malized, such as by setting ?? = ??/?  for ? = 1?? , 
where ? is the unnormalized weight value of the target lan-
guage model. 
 
Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous repre-
sentation is shown in Figure 2. 
 
 
200K (d) 
100 
100 (?) 
(?1???) 
Word vector 
Neural network 
Feature vector 
?1 
?2 
? 
? 
Raw phrase  ? or ? 
  ?(the process of)         (machine translation)     
(consists of). . . 
 ?(le processus de)    
(traduction automatique)  (consiste en). . .  
???1 ?? ??+1 
???1 ?? ??+1 
??? 
 
 
??? 
 
??(?)
score(??, ??) = ???
T ???  
 
Target phrases  
Continuous representations of  
target phrases  
Source phrases  
 
Continuous representations of  
source phrases  
Translation score as dot product of 
feature vectors in the continuous space 
702
5 Training CPTM 
This section describes the loss function we em-
ploy with the CPTM and the algorithm to train the 
neural network weights. 
We define the loss function ?(?) as the nega-
tive of the N-best list based expected BLEU, de-
noted by xBleu(?). In the reranking framework of 
SMT outlined in Section 3, xBleu(?) over one 
training sample (??, ??) is defined as 
xBleu(?) = ? ?(?|??)sBleu(??, ?)??GEN(??)  (5) 
where sBleu(??, ?)  is the sentence-level BLEU 
score, and  ?(?|??) is the translation probability 
from ?? to ? computed using softmax as  
?(?|??) =
exp(??T?(??,?,?))
? exp(??T?(??,??,?))???GEN(??)
  (6) 
where ?T? is the log-linear model of (1), which 
also includes the feature derived from the CPTM 
as defined by (4), and ? is a tuned smoothing fac-
tor. 
Let ?(?) be a loss function which is differen-
tiable w.r.t. the parameters of the CPTM, ?. We 
can compute the gradient of the loss and learn ? 
using gradient-based numerical optimization al-
gorithms, such as L-BFGS or stochastic gradient 
descent (SGD).  
5.1 Computing the Gradient 
Since the loss does not explicitly depend on ?, we 
use the chain rule for differentiation: 
??(?)
??
= ?
??(?)
?sim?(?? , ??)
?sim?(?? , ??)
??
(?,? )
 
= ? ??(?,?)
?sim?(?? , ??)
??
(?,? )
 (7) 
which takes the form of summation over all phrase 
pairs occurring either in a training sample (sto-
chastic mode) or in the entire training data (batch 
mode). ?(?,?) in (7) is known as the error term of 
the phrase pair (?, ?), and is defined as   
?(?,?) = ?
??(?)
?sim?(??,??)
  (8) 
It describes how the overall loss changes with the 
translation score of the phrase pair (?, ?). We will 
leave the derivation of ?(?,?) to Section 5.1.2, and 
will first describe how the gradient of 
sim?(?? , ??) w.r.t. ? is computed. 
5.1.1 Computing ?????(??, ??)/?? 
Without loss of generality, we use the following 
notations to describe a neural network: 
? ?? is the projection matrix for the l-th layer 
of the neural network; 
? ? is the input word vector of a phrase; 
? ?? is the sum vector of the l-th layer; and  
? ?? = ?(??) is the output vector of the l-th 
layer, where ? is an activation function; 
Thus, the CPTM defined by (2) and (3) can be rep-
resented as  
?1 = ?1
T?  
?1 = ?(?1)  
?2 = ?2
T?1  
?2 = ?(?2)  
sim?(?? , ??) = (??
2)
T
??
2  
The gradient of the matrix ?2 which projects the 
hidden vector to the output vector is computed as: 
?sim?(?? , ??)
??2
=
?(??
2)
T
??2
??
2 + (??
2)
T ???
2
??2
 
= ??
1 (??
2 ? ??(??
2))
T
+ ??
1 (??
2 ? ??(??
2))
T
 (9) 
where ? is the element-wise multiplication (Hada-
mard product). Applying the back propagation 
principle, the gradient of the projection matrix 
mapping the input vector to the hidden vector ?1 
is computed as 
?sim?(?? , ??)
??1
 
= ?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  
+?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  (10) 
The derivation can be easily extended to a neural 
network with multiple hidden layers.  
5.1.2 Computing ?(?,?) 
To simplify the notation, we rewrite our loss func-
tion of (5) and (6) over one training sample as  
703
?(?) = ?xBleu(?) = ?
G(?)
Z(?)
 (11) 
where 
G(?) = ? sBleu(?, ??) exp(?
T?(??, ?, ?))?   
Z(?) = ? exp(?T?(??, ?, ?))?   
Combining (8) and (11), we have 
?(?,?) =
?xBleu(?)
?sim?(?? , ??)
 (12) 
=
1
Z(?)
(
?G(?)
?sim?(?? , ??)
?
?Z(?)
?sim?(?? , ??)
xBleu(?)) 
Because ? is only relevant to ??+1 which is de-
fined in (4), we have 
??T?(??, ?, ?)
?sim?(?? , ??)
= ??+1
???+1(??, ?, ?)
?sim?(?? , ??)
  
= ??+1?(?, ?; ?) (13) 
where ?(?, ?; ?)  is the number of times the 
phrase pair (?, ?)  occurs in ? . Combining (12) 
and (13), we end up with the following equation 
?(?,?)
= ? U(?,?)?(?|??)??+1?(?, ?; ?)
(?,?)????(??)
 
where  (14) 
U(?, ?) = sBleu(??, ?) ? xBleu(?).  
5.2 The Training Algorithm 
In our experiments we train the parameters of the 
CPTM, ?, using the L-BFGS optimizer described 
in Andrew and Gao (2007), together with the loss 
function described in (5). The gradient is com-
puted as described in Sections 5.1. Although SGD 
has been advocated for neural network training 
due to its simplicity and its robustness to local 
minima (Bengio 2009), we find that in our task 
that the L-BFGS minimizes the loss in a desirable 
fashion empirically when iterating over the com-
plete training data (batch mode). For example, the 
convergence of the algorithm was found to be 
smooth, despite the non-convexity in our loss. An-
other merit of batch training is that the gradient 
over all training data can be computed efficiently. 
As shown in Section 5.1, computing 
?sim?(x? , x?)/??  requires large-scale matrix 
multiplications, and is expensive for multi-layer 
neural networks. Eq. (7) suggests that 
?sim?(x? , x?)/??  and ?(?,?)  can be computed 
separately, thus making the computation cost of 
the former term only depends on the number of 
phrase pairs in the phrase table, but not the size of 
training data. Therefore, the training method de-
scribed here can be used on larger amounts of 
training data with little difficulty.  
As described in Section 4, we take three steps 
to learn the parameters for both the log-linear 
model of SMT and the CPTM. While steps 1 and 
3 can be easily parallelized on a computer cluster, 
the CPTM training is performed on a single ma-
chine. For example, given a phrase table contain-
ing 16M pairs and a 1M-sentence training set, it 
takes a couple of hours to generate the N-best lists 
on a cluster, and about 10 hours to train the CPTM 
on a Xeon E5-2670 2.60GHz machine.   
For a non-convex problem, model initialization 
is important. In our experiments we always initial-
ize ?1 using a bilingual topic model trained on 
parallel data (see detail in Section 6.2), and ?2 as 
an identity matrix. In principle, the loss function 
of (5) can be further regularized (e.g. by adding a 
term of ?2 norm) to deal with overfitting. How-
ever, we did not find clear empirical advantage 
over the simpler early stop approach in a pilot 
study, which is adopted in the experiments in this 
paper.   
6 Experiments 
This section evaluates the CPTM presented on 
two translation tasks using WMT data sets. We 
first describe the data sets and baseline setup. 
Then we present experiments where we compare 
different versions of the CPTM and previous 
models. 
6.1 Experimental Setup 
Baseline. We experiment with an in-house 
phrase-based system similar to Moses (Koehn et 
al. 2007), where the translation candidates are 
scored by a set of common features including 
maximum likelihood estimates of source given 
target phrase mappings ????(?|?) and vice versa 
????(?|?), as well as lexical weighting estimates 
???(?|?) and ???(?|?), word and phrase penal-
ties, a linear distortion feature, and a lexicalized 
reordering feature. The baseline includes a stand-
ard 5-gram modified Kneser-Ney language model 
trained on the target side of the parallel corpora 
described below. Log-linear weights are estimated 
with the MERT algorithm (Och 2003). 
704
Evaluation. We test our models on two different 
data sets. First, we train an English to French sys-
tem based on the data of WMT 2006 shared task 
(Koehn and Monz 2006). The parallel corpus in-
cludes 688K sentence pairs of parliamentary pro-
ceedings for training. The development set con-
tains 2000 sentences, and the test set contains 
other 2000 sentences, all from the official WMT 
2006 shared task. 
Second, we experiment with a French to Eng-
lish system developed using 2.1M sentence pairs 
of training data, which amounts to 102M words, 
from the WMT 2012 campaign. The majority of 
the training data set is parliamentary proceedings 
except for 5M words which are newswire. We use 
the 2009 newswire data set, comprising 2525 sen-
tences, as the development set. We evaluate on 
four newswire domain test sets from 2008, 2010 
and 2011 as well as the 2010 system combination 
test set, containing 2034 to 3003 sentences. 
In this study we perform a detailed empirical 
comparison using the WMT 2006 data set, and 
verify our best models and results using the larger 
WMT 2012 data set. 
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al 2002). We also 
perform a significance test using the Wilcoxon 
signed rank test. Differences are considered statis-
tically significant when the p-value is less than 
0.05. 
6.2 Results of the CPTM 
Table 1 shows the results measured in BLEU eval-
uated on the WMT 2006 data set, where Row 1 is 
the baseline system. Rows 2 to 4 are the systems 
enhanced by integrating different versions of the 
CPTM. Rows 5 to 7 present the results of previous 
models. Row 8 is our best system. Table 2 shows 
the main results on the WMT 2012 data set. 
CPTM is the model described in Sections 4. 
As illustrated in Figure 2, the number of the nodes 
in the input layer is the vocabulary size ?. Both 
the hidden layer and the output layer have 100 
nodes6. That is, ?1 is a ? ? 100 matrix and ?2 
a 100 ? 100  matrix. The result shows that 
CPTM leads to a substantial improvement over 
the baseline system with a statistically significant 
margin of 1.0 BLEU points as in Table 1.  
We have developed a set of variants of CPTM 
to investigate two design choices we made in de-
veloping the CPTM: (1) whether to use a linear 
                                                          
6 We can achieve slightly better results using more nodes in 
the hidden and output layers, say 500 nodes. But the model 
projection or a multi-layer nonlinear projection; 
and (2) whether to compute the phrase similarity 
using word-word similarities as suggested by e.g., 
the lexical weighting model (Koehn et al 2003). 
We compare these variants on the WMT 2006 
data set, as shown in Table 1. 
CPTML (Row 3 in Table 1) uses a linear neural 
network to project a word vector of a phrase ? to 
a feature vector ?: ? ? ?(?) = ?T?, where ? is 
a ? ? 100  projection matrix. The translation 
score of a source phrase f and a target phrase e is 
measured as the similarity of their feature vectors. 
We choose cosine similarity because it works bet-
ter than dot product for linear projection. 
CPTMW (Row 4 in Table 1) computes the phrase 
similarity using word-word similarity scores. This 
follows the common smoothing strategy of ad-
dressing the data sparseness problem in modeling 
phrase translations, such as the lexical weighting 
model (Koehn et al 2003) and the word factored 
n-gram translation model (Son et al 2012). Let ? 
denote a word, and ? and ? the source and target 
phrases, respectively. We define 
sim(?, ?) =
1
|?|
? sim?(?, ?) +???
1
|?|
? sim?(?, ?)???   
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
sim?(?, ?)
=
? sim(?,??) exp(?sim(?,??))????
? exp(?sim(?,??))????
 
 
training is too slow to perform a detailed study within a rea-
sonable time. Therefore, all the models reported in this paper 
use 100 nodes.   
# Systems WMT test2006 
1 Baseline 33.06 
2 CPTM 34.10? 
3 CPTML 33.60
?? 
4 CPTMW 33.25
? 
5 BLTMPR 33.15
? 
6 DPM 33.29? 
7 MRFP 33.91
? 
8 Comb (2 + 7) 34.39?? 
Table 1: BLEU results for the English to French 
task using translation models and systems built 
on the WMT 2006 data set. The superscripts ? 
and ? indicate statistically significant difference 
(p < 0.05) from Baseline and CPTM, respec-
tively. 
 
705
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
where ? is the tuned smoothing parameter.  
Similar to CPTM, CPTMW also uses a nonlin-
ear projection to map each word (not a phrase vec-
tor as in CPTM) to a feature vector. 
Two observations can be made by comparing 
CPTM in Row 2 to its variants in Table 1. First of 
all, it is more effective to model the phrase trans-
lation directly than decomposing it into word-
word translations in the CPTMs. Second, we see 
that the nonlinear projection is able to generate 
more effective features, leading to better results 
than the linear projection. 
We  also compare the best version of the CPTM 
i.e., CPTM, with three related models proposed 
previously. We start the discussion with the re-
sults on the WMT 2006 data set in Table 1. 
Rows 5 and 6 in Table 1 are two state-of-the-
art latent semantic models that are originally 
trained on clicked query-document pairs (i.e., 
clickthrough data extracted from search logs) for 
query-document matching (Gao et al 2011). To 
adopt these models for SMT, we view source-tar-
get sentence pairs as clicked query-document 
pairs, and trained both models using the same 
methods as in Gao et al (2011) on the parallel bi-
lingual training data described earlier. Specifi-
cally, BTLMPR is an extension to PLSA, and is 
the best performer among different versions of the 
Bi-Lingual Topic Model (BLTM) described in 
Gao et al (2011). BLTM with Posterior Regular-
ization (BLTMPR) is trained on parallel training 
data using the EM algorithm with a constraint en-
forcing a source sentence and its paralleled target 
sentence to not only share the same prior topic dis-
tribution, but to also have similar fractions of 
words assigned to each topic. We incorporated the 
model into the log-linear model for SMT (1) as 
                                                          
7 Gao and He (2013) reported results of MRF models with 
different feature sets. We picked the MRF using phrase fea-
tures only (MRFP) for comparison since we are mainly inter-
ested in phrase representation. 
follows. First of all, the topic distribution of a 
source sentence ?? , denoted by ?(?|??) , is in-
duced from the learned topic-word distributions 
using EM. Then, each translation candidate ? in 
the N-best list GEN(??) is scored as 
?(?|??) = ? ? ?(?|?)?(?|??)????    
?(??|?) can be similarly computed. Finally, the 
logarithms of the two probabilities are incorpo-
rated into the log-linear model of (1) as two addi-
tional features. DPM is the Discriminative Projec-
tion Model described in Gao et al (2011), which 
is an extension of LSA. DPM uses a matrix to pro-
ject a word vector of a sentence to a feature vector. 
The projection matrix is learned on parallel train-
ing data using the S2Net alorithm (Yih et al 
2011). DPM can be incorporated into the log-lin-
ear model for SMT (1) by introducing a new fea-
ture ??+1 for each phrase pair, which is defined 
as the cosine similarity of the phrases in the pro-
ject space.  
As we see from Table 1, both latent semantic 
models, although leading to some slight improve-
ment over Baseline, are much less effective than 
CPTM. 
Finally, we compare the CPTM with the Mar-
kov Random Field model using phrase features 
(MRFP in Tables 1 and 2), proposed by Gao and 
He (2013)7, on both the WMT 2006 and WMT 
2012 datasets. MRFp is a state-of-the-art large 
scale discriminative training model that uses the 
same expected BLEU training criterion, which 
has proven to give superior performance across a 
range of MT tasks recently (He and Deng 2012, 
Setiawan and Zhou 2013, Gao and He 2013).  
Unlike CPTM, MRFp is a linear model that 
simply treats each phrase pair as a single feature. 
Therefore, although both are trained using the 
# Systems dev news2011 news2010 news2008 newssyscomb2010 
1 Baseline 23.58 25.24 24.35 20.36 24.14 
2 MRFP 24.07
? 26.00? 24.90 20.84? 25.05? 
3 CPTM 24.12? 26.25? 25.05? 21.15?? 24.91? 
4 Comb (2 + 3) 24.46?? 26.56?? 25.52?? 21.64?? 25.22? 
Table 2:   BLEU results for the French to English task using translation models and systems built on 
the WMT 2012 data set. The superscripts ? and ? indicate statistically significant difference (p < 
0.05) from Baseline and MRFp, respectively. 
 
 
706
same expected BLEU based objective function, 
CPTM and MRFp model the translation relation-
ship between two phrases from different angles. 
MRFp estimates one translation score for each 
phrase pair explicitly without parameter sharing, 
while in CPTM, all phrases share the same neural 
network that projects raw phrases to the continu-
ous space, providing a more smoothed estimation 
of the translation score for each phrase pair.  
The results in Tables 1 and 2 show that CPTM 
outperforms MRFP on most of the test sets across 
the two WMT data sets, but the difference be-
tween them is often not significant. Our interpre-
tation is that although CPTM provides a better 
smoothed estimation for low-frequent phrase 
pairs, which otherwise suffer the data sparsity is-
sue, MRFp provides a more precise estimation for 
those high-frequent phrase pairs. That is, CPTM 
and MRFp capture complementary information 
for translation. We thus combine CPTM and 
MRFP (Comb in Tables 1 and 2) by incorporating 
two features, each for one model, into the log-lin-
ear model of SMT (1). We observe that for both 
translation tasks, accuracy improves by up to 0.8 
BLEU over MRFP alone (e.g., on the news2008 
test set in Table 2). The results confirm that 
CPTM captures complementary translation infor-
mation to MRFp. Overall, we improve accuracy 
by up to 1.3 BLEU over the baseline on both 
WMT data sets. 
7 Conclusions 
The work presented in this paper makes two major 
contributions. First, we develop a novel phrase 
translation model for SMT, where joint represen-
tations are exploited of a phrase in the source lan-
guage and of its translation in the target language, 
and where the translation score of the pair of 
source-target phrases are represented as the dis-
tance between their feature vectors in a low-di-
mensional, continuous space. The space is derived 
from the representations generated using a multi-
layer neural network. Second, we present a new 
learning method to train the weights in the multi-
layer neural network for the end-to-end BLEU 
metric directly. The training method is based on 
L-BFGS. We describe in detail how the gradient 
in closed form, as required for efficient optimiza-
tion, is derived. The objective function, which 
takes the form of the expected BLEU computed 
from N-best lists, is very different from the usual 
objective functions used in most existing architec-
tures of neural networks, e.g., cross entropy (Hin-
ton et al 2012) or mean square error (Deng et al 
2012). We hence have provided details in the der-
ivation of the gradient, which can serve as an ex-
ample to guide the derivation of neural network 
learning with other non-standard objective func-
tions in the future. 
Our evaluation on two WMT data sets show 
that incorporating the continuous-space phrase 
translation model into the log-linear framework 
significantly improves the accuracy of a state-of-
the-art phrase-based SMT system, leading to a 
gain up to 1.3 BLEU. Careful implementation of 
the L-BFGS optimization based on the BLEU-
centric objective function, together with the asso-
ciated closed-form gradient, is a key to the suc-
cess.  
A natural extension of this work is to expand 
the model and learning algorithm from shallow to 
deep neural networks. The deep models are ex-
pected to produce more powerful and flexible se-
mantic representations (e.g., Tur et al, 2012), and 
thus greater performance gain than what is pre-
sented in this paper. 
8 Acknowledgements 
We thank Michael Auli for providing a dataset 
and for helpful discussions. We also thank the four 
anonymous reviewers for their comments.  
References 
Andrew, G. and Gao, J. 2007. Scalable training 
of L1-regularized log-linear models. In 
ICML.  
Auli, M., Galley, M., Quirk, C. and Zweig, G. 
2013 Joint language and translation modeling 
with recurrent neural networks. In EMNLP. 
Bengio, Y. 2009. Learning deep architectures for 
AI. Fundamental Trends Machine Learning, 
vol. 2, no. 1, pp. 1?127. 
Bengio, Y., Duharme, R., Vincent, P., and Janvin, 
C. 2003. A neural probabilistic language 
model. JMLR, 3:1137-1155. 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3: 993-1022. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P. 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 
12. 
707
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Index-
ing by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
DeNero, J., Gillick, D., Zhang, J., and Klein, D. 
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statis-
tical Machine Translation, pp. 31-38. 
Deng, L., Yu, D., and Platt, J. 2012. Scalable 
stacking and learning for building deep archi-
tectures. In ICASSP. 
Diamantaras, K. I., and Kung, S. Y. 1996. Princi-
ple Component Neural Networks: Theory and 
Applications. Wiley-Interscience. 
Dumais S., Letsche T., Littman M. and Landauer 
T. 1997. Automatic cross-language retrieval us-
ing latent semantic indexing. In AAAI-97 
Spring Symposium Series: Cross-Language 
Text and Speech Retrieval. 
Ganchev, K., Graca, J., Gillenwater, J., and 
Taskar, B. 2010. Posterior regularization for 
structured latent variable models. Journal of 
Machine Learning Research, 11 (2010): 2001-
2049. 
Gao, J., and He, X. 2013. Training MRF-based 
translation models using gradient ascent. In 
NAACL-HLT, pp. 450-459. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684.  
He, X., and Deng, L. 2012. Maximum expected 
bleu training of phrase and lexicon translation 
models. In ACL, pp. 292-301. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering Binary Codes for Documents by Learn-
ing Deep Generative Models. Topics in Cogni-
tive Science, pp. 1-18. 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, vol. 29, no. 6, pp. 82-97. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR, pp. 50-57. 
Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM.  
Kalchbrenner, N. and Blunsom, P. 2013. Recur-
rent continuous translation models. In EMNLP. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, 
C., Federico, M., Bertoldi, N., Cowan, B., Shen, 
W., Moran, C., Zens, R., Dyer, C., Bojar, O., 
Constantin, A., and Herbst, E. 2007. Moses: 
open source toolkit for statistical machine trans-
lation. In ACL 2007, demonstration session. 
Koehn, P. and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation be-
tween European languages. In Workshop on 
Statistical Machine Translation, pp. 102-121. 
Koehn, P., Och, F., and Marcu, D. 2003. Statisti-
cal phrase-based translation. In HLT-NAACL, 
pp. 127-133. 
Lambert, P. and Banchs, R. E. 2005. Data inferred 
multi-word expressions for statistical machine 
translation. In MT Summit X, Phuket, Thailand. 
Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-
encoders for ITG-based translation. In EMNLP. 
Liang,P., Bouchard-Cote,A., Klein, D. and 
Taskar, B. 2006. An end-to-end discriminative 
approach to machine translation. In COLING-
ACL. 
Marcu, D., and Wong, W. 2002. A phrase-based, 
joint probability model for statistical machine 
translation. In EMNLP. 
Mikolov, T., Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In INTER-
SPEECH, pp. 1045-1048. 
Mikolov, T., Kombrink, S., Burget, L., Cernocky, 
J., and Khudanpur, S. 2011. Extensions of re-
current neural network language model. In 
ICASSP, pp. 5528-5531. 
Mikolov, T. 2012. Statistical Language Model 
based on Neural Networks. Ph.D. thesis, Brno 
University of Technology. 
Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. 
Exploiting similarities among languages for 
machine translation. CoRR. 2013; 
abs/1309.4148. 
Mikolov, T., Yih, W. and Zweig, G. 2013b. Lin-
guistic Regularities in Continuous Space Word 
Representations. In NAACL-HLT. 
Mimno, D., Wallach, H., Naradowsky, J., Smith, 
D. and McCallum, A. 2009. Polylingual topic 
models. In EMNLP. 
708
Niehues J., Herrmann, T., Vogel, S., and Waibel, 
A. 2011. Wider context by using bilingual lan-
guage models in machine translation. 
Och, F. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL, pp. 160-
167.  
Och, F., and Ney, H. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 29(1): 19-51. 
Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 
2002. BLEU: a method for automatic evaluation 
of machine translation. In ACL. 
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual Document Representations from 
Discriminative Projections. In EMNLP. 
Rosti, A-V., Hang, B., Matsoukas, S., and 
Schwartz, R. S. 2011. Expected BLEU training 
for graphs: bbn system description for WMT 
system combination task. In Workshop on Sta-
tistical Machine Translation. 
Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. 
A. R. 2007. Smooth bilingual n-gram transla-
tion. In EMNLP-CoNLL, pp. 430-438. 
Schwenk, H. 2012. Continuous space translation 
models for phrase-based statistical machine 
translation. In COLING. 
Schwenk, H., Rousseau, A., and Mohammed A. 
2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine 
translation. In NAACL-HLT Workshop on the 
future of language modeling for HLT, pp. 11-
19. 
Setiawan, H. and Zhou, B., 2013. Discriminative 
training of 150 million translation parameters 
and its application to pruning. In NAACL. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic Compositionality through Recursive 
Matrix-Vector Spaces. In EMNLP. 
Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 
2011. Parsing natural scenes and natural lan-
guage with recursive neural networks. In ICML. 
Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-
tinuous space translation models with neural 
networks. In NAACL-HLT, pp. 29-48. 
Sundermeyer, M., Oparin, I., Gauvain, J-L. 
Freiberg, B., Schluter, R. and Ney, H. 2013. 
Comparison of feed forward and recurrent neu-
ral network language models. In ICASSP, pp. 
8430?8434. 
Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 
2012. Towards deeper understanding: deep con-
vex networks for semantic utterance classifica-
tion. In ICASSP. 
Vinokourov,A., Shawe-Taylor,J. and Cristia-
nini,N. 2002. Inferring a semantic representa-
tion of text via cross-language correlation anal-
ysis. In NIPS. 
Weston, J., Bengio, S., and Usunier, N. 2011. 
Large scale image annotation: learning to rank 
with joint word-image embeddings. In IJCAI. 
Wuebker, J., Mauser, A., and Ney, H. 2010. Train-
ing phrase translation models with leaving-one-
out. In ACL, pp. 475-484. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
Zhang, Y., Deng, L., He, X., and Acero, A. 2011. 
A novel decision function and the associated de-
cision-feedback learning for speech translation. 
In ICASSP. 
Zhila, A., Yih, W., Meek, C., Zweig, G. and 
Mikolov, T. 2013. Combining heterogeneous 
models for measuring relational similarity. In 
NAACL-HLT. 
Zou, W. Y., Socher, R., Cer, D., and Manning, C. 
D. 2013. Bilingual word embeddings for 
phrase-based machine translation. In EMNLP. 
709
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136?142,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Decoder Integration and Expected BLEU Training
for Recurrent Neural Network Language Models
Michael Auli
Microsoft Research
Redmond, WA, USA
michael.auli@microsoft.com
Jianfeng Gao
Microsoft Research
Redmond, WA, USA
jfgao@microsoft.com
Abstract
Neural network language models are often
trained by optimizing likelihood, but we
would prefer to optimize for a task specific
metric, such as BLEU in machine trans-
lation. We show how a recurrent neural
network language model can be optimized
towards an expected BLEU loss instead
of the usual cross-entropy criterion. Fur-
thermore, we tackle the issue of directly
integrating a recurrent network into first-
pass decoding under an efficient approxi-
mation. Our best results improve a phrase-
based statistical machine translation sys-
tem trained on WMT 2012 French-English
data by up to 2.0 BLEU, and the expected
BLEU objective improves over a cross-
entropy trained model by up to 0.6 BLEU
in a single reference setup.
1 Introduction
Neural network-based language and translation
models have achieved impressive accuracy im-
provements on statistical machine translation tasks
(Allauzen et al, 2011; Le et al, 2012b; Schwenk
et al, 2012; Vaswani et al, 2013; Gao et al, 2014).
In this paper we focus on recurrent neural network
architectures which have recently advanced the
state of the art in language modeling (Mikolov et
al., 2010; Mikolov et al, 2011; Sundermeyer et al,
2013) with several subsequent applications in ma-
chine translation (Auli et al, 2013; Kalchbrenner
and Blunsom, 2013; Hu et al, 2014). Recurrent
models have the potential to capture long-span de-
pendencies since their predictions are based on an
unbounded history of previous words (?2).
In practice, neural network models for machine
translation are usually trained by maximizing the
likelihood of the training data, either via a cross-
entropy objective (Mikolov et al, 2010; Schwenk
et al, 2012) or more recently, noise-contrastive es-
timation (Vaswani et al, 2013). However, it is
widely appreciated that directly optimizing for a
task-specific metric often leads to better perfor-
mance (Goodman, 1996; Och, 2003; Auli and
Lopez, 2011). The expected BLEU objective pro-
vides an efficient way of achieving this for ma-
chine translation (Rosti et al, 2010; Rosti et al,
2011; He and Deng, 2012; Gao and He, 2013;
Gao et al, 2014) instead of solely relying on tra-
ditional optimizers such as Minimum Error Rate
Training (MERT) that only adjust the weighting
of entire component models within the log-linear
framework of machine translation (?3).
Most previous work on neural networks for ma-
chine translation is based on a rescoring setup
(Arisoy et al, 2012; Mikolov, 2012; Le et al,
2012a; Auli et al, 2013), thereby side stepping
the algorithmic and engineering challenges of di-
rect decoder-integration. One recent exception is
Vaswani et al (2013) who demonstrated that feed-
forward network-based language models are more
accurate in first-pass decoding than in rescoring.
Decoder integration has the advantage for the neu-
ral network to directly influence search, unlike
rescoring which is restricted to an n-best list or lat-
tice. Decoding with feed-forward architectures is
straightforward, since predictions are based on a
fixed size input, similar to n-gram language mod-
els. However, for recurrent networks we have to
deal with the unbounded history, which breaks the
usual dynamic programming assumptions for effi-
cient search. We show how a simple but effective
approximation can side step this issue and we em-
pirically demonstrate its effectiveness (?4).
We test the expected BLEU objective by train-
ing a recurrent neural network language model
and obtain substantial improvements. We also find
that our efficient approximation for decoder inte-
gration is very accurate, clearly outperforming a
rescoring setup (?5).
136
wt
ht-1
ht
yt
V
W
U
0
0
1
0
0
0
Figure 1: Structure of the recurrent neural network
language model.
2 Recurrent Neural Network LMs
Our model has a similar structure to the recurrent
neural network language model of Mikolov et al
(2010) which is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 1). The input layer encodes the
word at position t as a 1-of-N vector w
t
. The out-
put layer y
t
represents scores over possible next
words; both the input and output layers are of size
|V |, the size of the vocabulary. The hidden layer
state h
t
encodes the history of all words observed
in the sequence up to time step t. The state of
the hidden layer is determined by the input layer
and the hidden layer configuration of the previous
time step h
t?1
. The weights of the connections
between the layers are summarized in a number
of matrices: U represents weights from the in-
put layer to the hidden layer, and W represents
connections from the previous hidden layer to the
current hidden layer. Matrix V contains weights
between the current hidden layer and the output
layer. The activations of the hidden and output
layers are computed by:
h
t
= tanh(Uw
t
+ Wh
t?1
)
y
t
= tanh(Vh
t
)
Different to previous work (Mikolov et al, 2010),
we do not use the softmax activation function to
output a probability over the next word, but in-
stead just compute a single unnormalized score.
This is computationally more efficient than sum-
ming over all possible outputs such as required
for the cross-entropy error function (Bengio et al,
2003; Mikolov et al, 2010; Schwenk et al, 2012).
Training is based on the back propagation through
time algorithm, which unrolls the network and
then computes error gradients over multiple time
steps (Rumelhart et al, 1986); we use the expected
BLEU loss (?3) to obtain the error with respect to
the output activations. After training, the output
layer represents scores s(w
t+1
|w
1
. . . w
t
,h
t
) for
the next word given the previous t input words and
the current hidden layer configuration h
t
.
3 Expected BLEU Training
We integrate the recurrent neural network lan-
guage model as an additional feature into the stan-
dard log-linear framework of translation (Och,
2003). Formally, our phrase-based model is pa-
rameterized by M parameters ? where each ?
m
?
?, m = 1 . . .M is the weight of an associated
feature h
m
(f, e). Function h(f, e) maps foreign
sentences f and English sentences e to the vector
h
1
(f, e) . . . (f, e), and the model chooses transla-
tions according to the following decision rule:
e? = arg max
e?E(f)
?
T
h(f, e)
We summarize the weights of the recurrent neural
network language model as ? = {U,W,V} and
add the model as an additional feature to the log-
linear translation model using the simplified nota-
tion s
?
(w
t
) = s(w
t
|w
1
. . . w
t?1
,h
t?1
):
h
M+1
(e) = s
?
(e) =
|e|
?
t=1
log s
?
(w
t
) (1)
which computes a sentence-level language model
score as the sum of individual word scores. The
translation model is parameterized by ? and ?
which are learned as follows (Gao et al, 2014):
1. We generate an n-best list for each foreign
sentence in the training data with the baseline
translation system given ? where ?
M+1
= 0
using the settings described in ?5. The n-best
lists serve as an approximation to E(f) used
in the next step for expected BLEU training
of the recurrent neural network model (?3.1).
2. Next, we fix ?, set ?
M+1
= 1 and opti-
mize ? with respect to the loss function on
the training data using stochastic gradient de-
scent (SGD).
1
1
We tuned ?
M+1
on the development set but found that
?
M+1
= 1 resulted in faster training and equal accuracy.
137
3. We fix ? and re-optimize ? in the presence
of the recurrent neural network model using
Minimum Error Rate Training (Och, 2003)
on the development set (?5).
3.1 Expected BLEU Objective
Formally, we define our loss function l(?) as
the negative expected BLEU score, denoted as
xBLEU(?) for a given foreign sentence f :
l(?) =? xBLEU(?)
=
?
e?E(f)
p
?,?
(e|f)sBLEU(e, e
(i)
) (2)
where sBLEU(e, e
(i)
) is a smoothed sentence-
level BLEU score with respect to the reference
translation e
(i)
, and E(f) is the generation set
given by an n-best list.
2
We use a sentence-level
BLEU approximation similar to He and Deng
(2012).
3
The normalized probability p
?,?
(e|f) of
a particular translation e given f is defined as:
p
?,?
(e|f) =
exp{??
T
h(f, e)}
?
e
?
?E(f)
exp{??
T
h(f, e
?
)}
(3)
where ?
T
h(f, e) includes the recurrent neural net-
work h
M+1
(e), and ? ? [0, inf) is a scaling factor
that flattens the distribution for ? < 1 and sharp-
ens it for ? > 1 (Tromble et al, 2008).
4
Next, we define the gradient of the expected
BLEU loss function l(?) using the observation that
the loss does not explicitly depend on ?:
?l(?)
??
=
?
e
|e|
?
t=1
?l(?)
?s
?
(w
t
)
?s
?
(w
t
)
??
=
?
e
|e|
?
t=1
??
w
t
?s
?
(w
t
)
??
where ?
w
t
is the error term for English word w
t
.
5
The error term indicates how the loss changes with
the translation probability which we derive next.
6
2
Our definitions do not take into account multiple derivations
for the same translation because our n-best lists contain only
unique entries which we obtain by choosing the highest scor-
ing translation among string identical candidates.
3
In early experiments we found that the BLEU+1 approxi-
mation used by Liang et al (2006) and Nakov et. al (2012)
worked equally well in our setting.
4
The ? parameter is only used during expected BLEU training
but not for subsequent MERT tuning.
5
A sentence may contain the same word multiple times and
we compute the error term for each occurrence separately
since the error depends on the individual history.
6
We omit the gradient of the recurrent neural network score
?s
?
(w
t
)
??
since it follows the standard form (Mikolov, 2012).
3.2 Derivation of the Error Term ?
w
t
We rewrite the loss function (2) using (3) and sep-
arate it into two terms G(?) and Z(?) as follows:
l(?) = ?xBLEU(?) = ?
G(?)
Z(?)
(4)
= ?
?
e?E(f)
exp{??
T
h(f, e)} sBLEU(e, e
(i)
)
?
e?E(f)
exp{??
T
h(f, e)}
Next, we apply the quotient rule of differentiation:
?
w
t
=
?xBLEU(?)
?s
?
(w
t
)
=
?(G(?)/Z(?))
?s
?
(w
t
)
=
1
Z(?)
(
?G(?)
?s
?
(w
t
)
?
?Z(?)
?s
?
(w
t
)
xBLEU(?)
)
Using the observation that ? is only relevant to the
recurrent neural network h
M+1
(e) (1) we have
???
T
h(f, e)
?s
?
(w
t
)
= ??
M+1
?h
M+1
(e)
?s
?
(w
t
)
=
??
M+1
s
?
(w
t
)
which together with the chain rule, (3) and (4) al-
lows us to rewrite ?
w
t
as follows:
?
w
t
=
1
Z(?)
?
e?E(f),
s.t.w
t
?e
(
? exp{??
T
h(f, e)}
?s
?
(w
t
)
U(?, e)
)
=
?
e?E(f),
s.t.w
t
?e
(
p
?,?
(e|f)U(?, e)?
M+1
?
s
?
(w
t
)
)
where U(?, e) = sBLEU(e, e
i
)? xBLEU(?).
4 Decoder Integration
Directly integrating our recurrent neural network
language model into first-pass decoding enables us
to search a much larger space than would be pos-
sible in rescoring.
Typically, phrase-based decoders maintain a set
of states representing partial and complete transla-
tion hypothesis that are scored by a set of features.
Most features are local, meaning that all required
information for them to assign a score is available
within the state. One exception is the n-gram lan-
guage model which requires the preceding n ? 1
words as well. In order to accommodate this fea-
ture, each state usually keeps these words as con-
text. Unfortunately, a recurrent neural network
makes even weaker independence assumptions so
138
that it depends on the entire left prefix of a sen-
tence. Furthermore, the weaker independence as-
sumptions also dramatically reduce the effective-
ness of dynamic programming by allowing much
fewer states to be recombined.
7
To solve this problem, we follow previous work
on lattice rescoring with recurrent networks that
maintained the usual n-gram context but kept a
beam of hidden layer configurations at each state
(Auli et al, 2013). In fact, to make decoding as
efficient as possible, we only keep the single best
scoring hidden layer configuration. This approx-
imation has been effective for lattice rescoring,
since the translations represented by each state are
in fact very similar: They share both the same
source words as well as the same n-gram context
which is likely to result in similar recurrent his-
tories that can be safely pruned. As future cost
estimate we score each phrase in isolation, reset-
ting the hidden layer at the beginning of a phrase.
While simple, we found our estimate to be more
accurate than no future cost at all.
5 Experiments
Baseline. We use a phrase-based system simi-
lar to Moses (Koehn et al, 2007) based on a set
of common features including maximum likeli-
hood estimates p
ML
(e|f) and p
ML
(f |e), lexically
weighted estimates p
LW
(e|f) and p
LW
(f |e),
word and phrase-penalties, a hierarchical reorder-
ing model (Galley and Manning, 2008), a linear
distortion feature, and a modified Kneser-Ney lan-
guage model trained on the target-side of the paral-
lel data. Log-linear weights are tuned with MERT.
Evaluation. We use training and test data from
the WMT 2012 campaign and report results on
French-English and German-English. Transla-
tion models are estimated on 102M words of par-
allel data for French-English, and 99M words
for German-English; about 6.5M words for each
language pair are newswire, the remainder are
parliamentary proceedings. We evaluate on six
newswire domain test sets from 2008 to 2013 con-
taining between 2034 to 3003 sentences. Log-
linear weights are estimated on the 2009 data set
comprising 2525 sentences. We evaluate accuracy
in terms of BLEU with a single reference.
Rescoring Setup. For rescoring we use ei-
7
Recombination only retains the highest scoring state if there
are multiple identical states, that is, they cover the same
source span, the same translation phrase and contexts.
ther lattices or the unique 100-best output of
the phrase-based decoder and re-estimate the log-
linear weights by running a further iteration of
MERT on the n-best list of the development set,
augmented by scores corresponding to the neural
network models. At test time we rescore n-best
lists with the new weights.
Neural Network Training. All neural network
models are trained on the news portion of the
parallel data, corresponding to 136K sentences,
which we found to be most useful in initial exper-
iments. As training data we use unique 100-best
lists generated by the baseline system. We use the
same data both for training the phrase-based sys-
tem as well as the language model but find that
the resulting bias did not hurt end-to-end accu-
racy (Yu et al, 2013). The vocabulary consists of
words that occur in at least two different sentences,
which is 31K words for both language pairs. We
tuned the learning rate ? of our mini-batch SGD
trainer as well as the probability scaling parameter
? (3) on a held-out set and found simple settings of
? = 0.1 and ? = 1 to be good choices. To prevent
over-fitting, we experimented with L2 regulariza-
tion, but found no accuracy improvements, prob-
ably because SGD regularizes enough. We evalu-
ate performance on a held-out set during training
and stop whenever the objective changes less than
0.0003. The hidden layer uses 100 neurons unless
otherwise stated.
5.1 Decoder Integration
We compare the effect of direct decoder integra-
tion to rescoring with both lattices and n-best lists
when the model is trained with a cross-entropy ob-
jective (Mikolov et al, 2010). The results (Ta-
ble 1 and Table 2) show that direct integration im-
proves accuracy across all six test sets on both lan-
guage pairs. For French-English we improve over
n-best rescoring by up to 1.1 BLEU and by up to
0.5 BLEU for German-English. We improve over
lattice rescoring by up to 0.4 BLEU on French-
English and by up to 0.3 BLEU on German-
English. Compared to the baseline, we achieve
improvements of up to 2.0 BLEU for French-
English and up to 1.3 BLEU for German-English.
The average improvement across all test sets is
1.5 BLEU for French-English and 1.0 BLEU for
German-English compared to the baseline.
139
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53
RNN n-best rescore 24.83 21.41 25.17 25.06 26.53 25.74 26.31 25.25
RNN lattice rescore 24.91 21.73 25.56 25.43 27.04 26.43 26.75 25.72
RNN decode 25.14 22.03 25.86 25.74 27.32 26.86 27.15 26.06
Table 1: French-English accuracy of decoder integration of a recurrent neural network language model
(RNN decode) compared to n-best and lattice rescoring as well as the output of a phrase-based system
using an n-gram model (Baseline); Alltest is the corpus-weighted average BLEU across all test sets.
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 19.35 19.96 20.87 20.66 19.60 19.80 22.48 20.58
RNN n-best rescore 20.17 20.29 21.35 21.27 20.51 20.54 23.03 21.21
RNN lattice rescore 20.24 20.38 21.55 21.43 20.77 20.63 23.23 21.38
RNN decode 20.13 20.51 21.79 21.71 20.91 20.93 23.53 21.61
Table 2: German-English results of direct decoder integration (cf. Table 1).
dev 2008 2010 syscomb2010 2011 2012 2013 AllTest
Baseline 24.11 20.73 24.68 24.59 25.62 24.85 25.54 24.53
CE RNN 24.80 21.15 25.14 25.06 26.45 25.83 26.69 25.29
+ xBLEU RNN 25.11 21.74 25.52 25.42 27.06 26.42 26.72 25.71
Table 3: French-English accuracy of a decoder integrated cross-entropy recurrent neural network model
(CE RNN) and a combination with an expected BLEU trained model (xBLEU RNN). Results are not
comparable to Table 1 since a smaller hidden layer was used to keep training times manageable (?5.2).
5.2 Expected BLEU Training
Training with the expected BLEU loss is compu-
tationally more expensive than with cross-entropy
since each training example is an n-best list in-
stead of a single sentence. This increases the num-
ber of words to be processed from 3.5M to 340M.
To keep training times manageable, we reduce the
hidden layer size to 30 neurons, thereby greatly
increasing speed. Despite slower training, the ac-
tual scoring at test time of expected BLEU mod-
els is about 5 times faster than for cross-entropy
models since we do not need to normalize the out-
put layer anymore. The results (Table 3) show
improvements of up to 0.6 BLEU when combin-
ing a cross-entropy model with an expected BLEU
variant. Average gains across all test sets are 0.4
BLEU, demonstrating that the gains from the ex-
pected BLEU loss are additive.
6 Conclusion and Future Work
We introduce an empirically effective approxima-
tion to integrate a recurrent neural network model
into first pass decoding, thereby extending pre-
vious work on decoding with feed-forward neu-
ral networks (Vaswani et al, 2013). Our best re-
sult improves the output of a phrase-based decoder
by up to 2.0 BLEU on French-English translation,
outperforming n-best rescoring by up to 1.1 BLEU
and lattice rescoring by up to 0.4 BLEU. Directly
optimizing a recurrent neural network language
model towards an expected BLEU loss proves ef-
fective, improving a cross-entropy trained variant
by up 0.6 BLEU. Despite higher training complex-
ity, our expected BLEU trained model has five
times faster runtime than a cross-entropy model
since it does not require normalization.
In future work, we would like to scale up to
larger data sets and more complex models through
parallelization. We would also like to experiment
with more elaborate future cost estimates, such as
the average score assigned to all occurrences of a
phrase in a large corpus.
7 Acknowledgments
We thank Michel Galley, Arul Menezes, Chris
Quirk and Geoffrey Zweig for helpful discussions
related to this work as well as the four anonymous
reviewers for their comments.
140
References
Alexandre Allauzen, H?el`ene Bonneau-Maynard, Hai-
Son Le, Aur?elien Max, Guillaume Wisniewski,
Franc?ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309?315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20?28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli and Adam Lopez. 2011. Training a
Log-Linear Parser with Loss Functions via Softmax-
Margin. In Proc. of EMNLP, pages 333?343. Asso-
ciation for Computational Linguistics, July.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137?1155.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP, pages 848?856.
Jianfeng Gao and Xiaodong He. 2013. Training MRF-
Based Phrase Translation Models using Gradient
Ascent. In Proc. of NAACL-HLT, pages 450?459.
Association for Computational Linguistics, June.
Jianfeng Gao, Xiaodong He, Scott Wen tau Yih, and
Li Deng. 2014. Learning Continuous Phrase Rep-
resentations for Translation Modeling. In Proc.
of ACL. Association for Computational Linguistics,
June.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. In Proc. of ACL, pages 177?183, Santa Cruz,
CA, USA, June.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proc. of ACL, pages 8?14. Association
for Computational Linguistics, July.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum Translation Modeling with Recur-
rent Neural Networks. In Proc. of EACL. Associa-
tion for Computational Linguistics, April.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700?1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39?48, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In Proc.
of WMT, pages 330?337, Montr?eal, Canada, June.
Association for Computational Linguistics.
Percy Liang, Alexandre Bouchard-C?ot?e, Ben Taskar,
and Dan Klein. 2006. An end-to-end discriminative
approach to machine translation. In Proc. of ACL-
COLING, pages 761?768, Jul.
Tom?a?s Mikolov, Karafi?at Martin, Luk?a?s Burget, Jan
Cernock?y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045?1048.
Tom?a?s Mikolov, Anoop Deoras, Daniel Povey, Luk?a?s
Burget, and Jan
?
Cernock?y. 2011. Strategies for
Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196?201.
Tom?a?s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING. As-
sociation for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160?167, Sapporo, Japan, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN System De-
scription for WMT10 System Combination Task.
In Proc. of WMT, pages 321?326. Association for
Computational Linguistics, July.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proc. of
WMT, pages 159?165. Association for Computa-
tional Linguistics, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
141
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11?19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, pages 8430?8434, May.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice Minimum
Bayes-Risk Decoding for Statistical Machine Trans-
lation. In Proc. of EMNLP, pages 620?629. Associ-
ation for Computational Linguistics, October.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-Violation Perceptron and Forced Decod-
ing for Scalable MT Training. In Proc. of EMNLP,
pages 1112?1123. Association for Computational
Linguistics, October.
142
