Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 183?191,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Text Classification by a Sense Spectrum Approach to Term
Expansion
Peter Wittek
Department of Computer Science
National University of Singapore
Computing 1, Law Link
Singapore 117590
wittek@comp.nus.edu.sg
Sa?ndor Dara?nyi
Swedish School of Library
and Information Science
Go?teborg University &
University of Bora?s
Alle?gatan 1
50190 Bora?s, Sweden
sandor.daranyi@hb.se
Chew Lim Tan
Department of Computer Science
National University of Singapore
Computing 1, Law Link
Singapore 117590
tancl@comp.nus.edu.sg
Abstract
Experimenting with different mathematical
objects for text representation is an important
step of building text classification models. In
order to be efficient, such objects of a for-
mal model, like vectors, have to reasonably re-
produce language-related phenomena such as
word meaning inherent in index terms. We in-
troduce an algorithm for sense-based seman-
tic ordering of index terms which approxi-
mates Cruse?s description of a sense spectrum.
Following semantic ordering, text classifica-
tion by support vector machines can benefit
from semantic smoothing kernels that regard
semantic relations among index terms while
computing document similarity. Adding ex-
pansion terms to the vector representation can
also improve effectiveness. This paper pro-
poses a new kernel which discounts less im-
portant expansion terms based on lexical re-
latedness.
1 Introduction
Generally, building an automated text classification
system consists of two key subtasks. The first task
is text representation which converts the content of
documents into compact format so that they can be
further processed by the text classifiers. Another
task is to learn the model of a text classifier which
is used to classify the unlabeled documents. This
paper proposes a substantially new model for text
representation to improve effectiveness of text clas-
sification by semantic ordering.
Our motivation for the research presented here
came from (Dorrer et al, 2001) who demonstrated
the viability of database searching by visible light
using a quantum algorithm, albeit on meaningless
items. The question was, what kind of document
representation would be necessary to extend their
in-principle results to include semantics, one that
has been leading us to test both periodic and non-
periodic functions for this purpose. Since represen-
tation and retrieval by colors was implied in their
method, we speculated that the following compo-
nents could be useful in a rephrased model: (a)
a metaphorically presented spectral expression of
lexical semantic phenomena, (b) a ranked one-
dimensional condensate of multidimensional sense
structure, and (c) representation of documents and
queries by functions in L2 space with a similarity
measure. Our anticipation was that by matching
these components, a new model could demonstrate
new capacities in general, and contribute to comput-
ing meaning by waves in particular.
Semantic ordering (component b) is an approxi-
mation of what (Cruse, 1986) referred to as a sense
spectrum, i.e. a series of points - called local senses
and constituting lexical units -, in a one-dimensional
semantic continuum (component a). Apart from dif-
ferentiating between the conceptual content of the
same word in terms of its senses in word pairs, i.e.
their semantic relatedness, it also compresses the
result in spectral form. The scalar values of this
spectrum have the double potential of being a con-
densed measure for semantic weighting, and, ten-
tatively, they can play the role of mass in experi-
ments where gravity is called in as a metaphor for
text categorization and information retrieval (Paij-
mans, 1997; Shi et al, 2005; Wittek et al, 2009).
183
This paper addresses text categorization by means
of non-periodical functions only.
In support of Cruse?s point, recently it has been
demonstrated by measurements that sense classifi-
cation errors made by their maximum entropy based
word sense disambiguation system were partly
remedied once instead of a fine-grained view, a more
coarse-grained view of senses was adopted (Palmer
et al, 2006). Improvement of sense classification ac-
curacy linked with ?zooming out? in terms of obser-
vation granularity indicates, in our eyes, the ?fluid?,
perhaps spectral nature of sense inasmuch as it is
impossible to precisely distinguish between the bor-
derlines and some fuzziness is implied both in the
phenomenon and its perception. This ?fluidity of
language?, as Palmer et al call it, is in accord
with the theory of shared semantic representations in
psycholinguistics (Rodd et al, 2002), according to
which related senses share a portion of their mean-
ing representation in the mental lexicon; it also sup-
ports an earlier observation of two of the present au-
thors based on the same methodology as outlined in
this paper, namely that using continuous functions
for information retrieval leads to content representa-
tion without exact term or document locations, one
which is regional in its nature and subject to a math-
ematical uncertainty principle (Wittek and Dara?nyi,
2007).
We approach our problem in three steps: (1)
whether distributional semantics alone is enough for
the representation of word meaning, (2) whether se-
mantic relatedness between word pairs can be ex-
pressed in an ordered form while preserving lexical
field structure, and if (3) the uniqueness of entries in
such an order can be expressed by functions rather
than scalars such as distance. As we will show, this
line of thought leads to performance improvement
in text classification by using kernel-based feature
weighting.
Since the early days of the vector space model,
it has been debated whether it is a proper carrier
of meaning of texts (Raghavan and Wong, 1986),
arguing if distributional similarity is an adequate
proxy for lexical semantic relatedness (Budanitsky
and Hirst, 2006). We argue for the need to enrich
distributional semantics-based text representation by
other components because with the statistical, i.e.
devoid of word semantics approaches there is gen-
erally no way to improve both precision and recall
at the same time, increasing one is done at the ex-
pense of the other. For example, casting a wider net
of search terms to improve recall of relevant items
will also bring in an even greater proportion of ir-
relevant items, lowering precision. In the mean-
time, practical approaches have been proliferating,
especially with developments in kernel methods in
the last decade (Joachims, 1998; Cristianini et al,
2002). Some researchers suggested a more general
mathematical framework to accommodate the needs
that the vector space model cannot satisfy (van Rijs-
bergen, 2004). This paper explores the opportunities
of this representation in the domain of text classifi-
cation by introducing it as a new nonlinear semantic
kernel.
Another aspect of the same problem is term ex-
pansion for document classification and retrieval.
By automatically selecting expansion terms for a
text classification system to expand a document vec-
tor by adding terms that are related to the terms
already in the document, performance can be im-
proved (Hu et al, 2008). Such new terms can ei-
ther be statistically related to the original terms or
chosen from lexical resources such as thesauri, con-
trolled vocabularies, ontologies and the like.
However, in doing so the fundamental question
often overlooked is whether the expansion terms ex-
tracted are equally related to the document and are
useful for text classification. In what follows we
propose a form of term expansion with decreasing
importance of those terms that are less related, as
contrasted with rigid term expansion. This can be
carried out by a combination of semantic ordering
and using function space for classification.
This paper is organized as follows. Section 2
overviews text classification by support vector ma-
chines, expanding on traditional text similarity mea-
sures (Section 2.1), semantic smoothing kernels
(Section 2.2), term expansion strategies (Section
2.3), and finally introduces our semantic kernels in
the L2 space (Section 2.4). Section 3 discusses ex-
perimental results and Section 4 concludes the pa-
per.
184
2 Text Classification with Support Vector
Machines
Text categorization is the task of assigning unlabeled
documents into predefined categories. Given a col-
lection of {d1, d2, . . . , dN} documents, and a C =
{c1, c2, ..., c|C|} set of predefined categories, the
task is, for each document dj (j ? {1, 2, . . . , N}),
to assign a decision to file dj under ci or a deci-
sion not to file dj under ci (ci ? C) by virtue of
a function ?, where the function ? is also referred
to as the classifier, or model, or hypothesis, or rule.
Supervised text classification is a machine learning
technique for creating the function ? from training
data. The training data consist of pairs of input doc-
uments, and desired outputs (i.e., classes).
Support vector machines have been found the
most effective by several authors (Joachims, 1998).
The proposed semantic text classification method is
grounded in the kernel methods underlying support
vector machines.
A support vector machine is a kind of supervised
learning algorithm. In its simplest, linear form, a
support vector machine is a hyperplane that sepa-
rates a set of positive examples from a set of nega-
tive examples with maximum margin (Shawe-Taylor
and Cristianini, 2004). The strength of kernel meth-
ods is that they allow a mapping ?(.) of x to a higher
dimensional space. In the dual formulation of the
mathematical programming problem, only the ker-
nel matrix K(xi,xj) = ?(xi)??(xi) is needed in
the calculations.
2.1 Traditional Text Similarity Measure
Intuitively, if a text fragment of two documents ad-
dress similar topics, it is highly possible that they
share lots of substantive terms. After having re-
moved the stopwords and stemmed the rest, the
stemmed terms construct a vector representation for
each text document. Let aj be a document vector in
the vector space model, that is, aj = ?Mk=1 akjek,
where M is the number of index terms, akj is some
weighting (e.g., term frequency), and ek is a basis
vector of the M -dimensional Euclidean space. This
representation is also referred to as the bag-of-words
(BOW) model.
Given this representation, semantic relatedness of
a pair of text fragments is computed as the cosine
similarity of their corresponding term vectors which
is defined as:
S(ai,aj) = aiaj|ai||a|j . (1)
2.2 Linear Semantic Kernels
One enrichment strategy is to use a semantic
smoothing kernel while calculating the similarity
between two documents. Any linear kernel for texts
is characterized by K(ai,aj) = a?iS?Saj , where
S is an appropriately shaped matrix commonly re-
ferred to as semantic smoothing matrix (Siolas and
d?Alche? Buc, 2000; Shawe-Taylor and Cristianini,
2004; Basili et al, 2005; Mavroeidis et al, 2005;
Bloehdorn et al, 2006). The presence of S changes
the orthogonality of the vector space model, as this
mapping should introduce term dependence. A re-
cent attempt tried to manually construct S with the
help of a lexical resource (Siolas and d?Alche? Buc,
2000). The entries in the symmetric matrix S ex-
press the semantic similarity between the terms i and
j. Entries in this matrix are inversely proportional
to the length of the WordNet hierarchy path linking
the two terms. The performance, measured over the
20NewsGroups corpus, showed an improvement of
2 % over the the basic vector space method. More-
over, the semantic matrix S is almost fully dense,
hence computational issues arise. In a similar con-
struction, (Bloehdorn et al, 2006) defined the ma-
trix entries as weights of superconcepts of the two
terms in the WordNet hierarchy. Focusing on special
subcategories of Reuters-21578 and on the TREC
Question Answering Dataset, they showed consis-
tent improvement over the baseline. As (Mavroei-
dis et al, 2005) pointed out, polysemy will remain
a problem in semantic smoothing kernels. A more
complex way of calculating the semantic similarity
as the matrix entries was also proposed (Basili et al,
2005). For a more general discussion on semantic
similarity see Section 2.4.1.
An early attempt to overcome the untenable or-
thogonality assumption of the vector space model
was proposed under the name of generalized vec-
tor space model (Wong et al, 1985). The article
which proposed the model did not provide empiri-
cal results, and since then the model has been re-
garded of large theoretical importance with less im-
pact on actual applications. The model takes a distri-
185
butional approach, focusing on term co-occurrences.
The underlying assumption is that term correlations
are captured by the co-occurrence information. That
is, two terms are semantically related if they co-
occur often in the same documents. By eliminat-
ing orthogonality, documents can be seen as similar
even if they do not share any terms. The term co-
occurrence matrix is AA?, hence the model takes A?
as the semantic similarity matrix S. A major draw-
back of the generalized vector space model is that it
replaces the orthogonality assumption with another
questionable assumption. The computational needs
are tremendous too, if the dimensions of A are con-
sidered. Moreover, the co-occurrence matrix is not
sparse anymore.
Latent semantic indexing (or latent semantic anal-
ysis) was another attempt to bring more linguis-
tic and psychological aspects to language process-
ing via a kernel. Conceptually, latent semantic in-
dexing is similar to the generalized vector space
model, it measures semantic information through
co-occurrence analysis in the corpus. From the al-
gorithmic perspective it is an enormous problem that
textual data have a large number of relevant fea-
tures. This results in huge computational needs and
the classification models may overfit the data. The
number of features can be reduced by multivariate
feature extraction methods. In latent semantic in-
dexing, the dimension of the vector space is reduced
by singular value decomposition (Deerwester et al,
1990).
Using rank reduction, terms that occur together
very often in the same documents are merged into
a single dimension of the feature space. The di-
mensions of the reduced space correspond to the
axes of greatest variance. For latent semantic in-
dexing, by dual representation the kernel matrix is
K = V ?2kV ?, where ?k is a diagonal matrix con-
taining the k largest singular values of the singu-
lar value decomposition of the vector space, and V
holds the right singular vectors of the decomposi-
tion. The new kernel matrix can be obtained directly
from K by applying an eigenvalue decomposition
of K (Cristianini et al, 2002). The computational
complexity of performing an eigenvalue decompo-
sition on the kernel matrix is a major drawback of
latent semantic indexing.
2.3 Text Representation Enrichment Strategies
by Term Expansion
In order to eliminate the bottleneck of the traditional
BOW representation, previous approaches in term
expansion enriched this convention by external lexi-
cal resources such as WordNet.
As a first step, these methods generate new fea-
tures for each document in the dataset. These new
features can be synonyms or homonyms of docu-
ment terms as in (Hotho et al, 2003; Rodriguez
and Hidalgo, 1997), or expanded features for terms,
sentences and documents as in (Gabrilovich and
Markovitch, 2005), or term context information for
word sense disambiguation such as topic signatures
(Agirre and De Lacalle, 2003; Agirre et al, 2004).
Then, the generated new features replace the old
ones or are appended to the document representa-
tion, and construct a new vector representation a?i
for each text document. The similarity measure of
document pairs is defined as:
S(a?i, a?j) = a?ia?j|a?i||a?j | . (2)
2.4 Our Framework
The basic assumption of our framework is that terms
can be arranged in an order such that consecutive
terms are semantically related. Hence each term ac-
quires a unique position, and this position ties the
term to its semantically related neighbors. However,
given a BOW representation with a cosine similarity
measure, this position would not improve classifica-
tion performance. Therefore we suggest to associate
a mathematical function with each term, thus map-
ping terms and documents to theL2 space, and using
the inner product of this space to express similar-
ity. The choice of function will determine to which
extent neighboring terms, i.e., the enriching terms,
are considered in calculating the similarity between
two documents. This section first introduces an al-
gorithm that produces the aforementioned semantic
order, then the semantic kernels in the L2 space are
discussed.
2.4.1 An Algorithm for a Semantic Ordering of
Terms
The proposed kernels assume that there is a se-
mantic order between terms. Let V denote a set of
186
terms {t1, t2, . . . , tn} and let d(ti, tj) denote the se-
mantic distance between the terms ti and tj . The
initial order of the terms is not relevant, though it is
assumed to be alphabetic. Let G = (V,E) denote
a weighted undirected graph, where the weights in
the set E are defined by the distances between the
terms.
Various lexical resource-based (Budanitsky and
Hirst, 2006) and distributional measures (Moham-
mad and Hirst, 2005) have been proposed to mea-
sure semantic relatedness and distance between
terms. Terms can be corpus- or genre-specific. Man-
ually constructed general-purpose lexical resources
include many usages that are infrequent in a partic-
ular corpus or genre of documents. For example,
one of the 8 senses of company in WordNet is a
visitor/visitant, which is a hyponym of person (Lin,
1998). This sense of the term is practically never
used in newspaper articles, hence distributional at-
tributes should be taken into consideration. Com-
posite measures that combine the advantages of both
approaches have also been developed (Resnik, 1995;
Jiang and Conrath, 1997). This paper relies on the
Jiang-Conrath composite measure (Jiang and Con-
rath, 1997), which has been shown to be superior
to other measures (Budanitsky and Hirst, 2006), and
we also found that this measure works the best for
the purpose. The Jiang-Conrath metric measures
the distance between two senses by using the hier-
archy of WordNet. By denoting the lowest super-
ordinate of two senses s1 and s2 in the hierarchy
with LSuper(s1,s2), the metric is calculated as fol-
lows:
d(s1, s2) = IC(s1)+IC(s2)?2IC(LSuper(s1, s2)),
where IC(s) is the information content of a sense
s based on a corpus. Distance between two terms
is calculated according to the following equation:
d(t1, t2) = maxs1?sen(t1),s2?sen(t2) d(s1, s2), where
t1 and t2 are two terms, and sen(ti) is the set of
senses of ti. The distance between two terms is
usually defined as the minimum of the sense dis-
tances. We chose maximum because it ensures that
only closely related terms will be placed to adjacent
positions by the algorithm below.
Finding a semantic ordering of terms can be trans-
lated to a graph problem: a minimum-weight Hamil-
tonian path G? of G gives the ordering by reading
the nodes from one of the paths to the other. G is
a complete graph, therefore such a path always ex-
ists, but finding it is an NP-complete problem. The
following greedy algorithm is similar to the nearest
neighbor heuristic for the solution of the traveling
salesman problem. It creates a graph G? = (V ?, E?),
where V ? = V and E? ? E. This G? graph is a
spanning tree of G in which the maximum degree of
a node is two, that is, the minimum spanning tree is
a path between two nodes.
Step 1 Find the term at the highest stage of the hi-
erarchy in a lexical resource.
ts = argminti?V depth(ti).
This seed term is the first element of V ?, V ? =
{ts}. Remove it from the set V :
V := V \{ts}.
Using WordNet, this seed term is entity, if the
vocabulary of the text collection contains it.
Step 2 Let tl denote the leftmost term of the order-
ing and tr the rightmost one. Find the next two
elements of the ordering:
t?l = argminti?V d(ti, tl),
t?r = argminti?V \{t?l}d(ti, tr).
Step 3 If d(tl, t?l) < d(tr, t?r) then add t?l to V ?,
E? := E? ? {e(tl, t?l)}, and V := V \{t?l}.
Else add t?r to V ?, E? := E? ? {e(tr, t?r)} and
V := V \{t?r}.
Step 4 Repeat from Step 2 until V = ?.
The above algorithm can be thought of as a modi-
fied Prim?s algorithm, but it does not find the optimal
minimum-weight spanning tree.
2.4.2 Semantic Kernels in the L2 Space
The L2 space shares resemblance with a real
vector space. Real-valued vectors are replaced by
square-integrable functions, and the dot product is
replaced by the following inner product: (fi, fj) =?
fifjdx, for some fi, fj in the given L2 space.
Lately, Hoenkamp has also pointed out that the
L2 space can be used for information retrieval when
187
he introduced a Haar basis for the document space
(Hoenkamp, 2003). He utilized a signal processing
framework within the context of latent semantic in-
dexing. In order to apply an L2 representation for
text classification, the problem is approached from a
different angle than by Hoenkamp, taking discount-
ing expansion terms as our point of departure.
Assigning a function w(x ? k) to the term in the
kth position in a semantic order, a document j can
be expressed as follows:
fj(x) =
M?
k=1
akjw(x? k), (3)
where x is in [1,M ], and it is the variable of inte-
gration in calculating the inner product of the L2;
x can be regarded as a ?dummy? variable carrying
no meaning in itself. The above formula will be re-
ferred to as a document function. In the experiments,
the function exp(?bx2) was used as w(x), with b as
a free parameter reflecting the width of the function
expressing how many neighboring expansion terms
are considered.
The inner product of theL2[1,M ] space is applied
to express similarity between two documents in sim-
ilar vein as the dot product does in a real-valued vec-
tor space:
(fi, fj) =
?
[1,M ]
fi(x)fj(x)dx, (4)
where fi and fj are the representations of the docu-
ments in the L2 space (fi, fj ? L2([1,M ])).
0.5
1
1.5
2
bran
d bran
d
name trade name
Figure 1: Two documents with matching term brand
name. Dotted line: Document-1. Dashed line:
Document-2. Solid line: Their product.
With the above formula, a matching term in two
documents will be counted to its full term frequency
or tfidf score, while semantically related terms will
be counted less and less according their semantic
similarity to the matching term. Assuming that the
terms brand, brand name, and trade name follow
each other in the semantic order, consider the fol-
lowing example. The first document has the term
brand name, and so does the second document. In
Figure 1, it can be seen brand name is counted the
same way as it would be in a BOW model with its
full term frequency score, brand and trade name are
counted to a lesser extent, while other related terms
are considered even less.
0.5
1
1.5
2
bran
d bran
d
name trade name
Figure 2: Two documents with no matching term but
with related terms brand and trade name. Dotted line:
Document-1. Dashed line: Document-2. Solid line:
Their product.
Now if the two documents do not share the exact
term, only related terms occur, for instance, trade
name and brand, respectively, then the term brand
name, placed between trade name and brand in the
s semantic order, will be considered only to some
extent for the calculation of similarity (see Figure
2).
3 Experimental Results
The most widely used benchmark corpus is the
Reuters-21578 collection. For benchmarking pur-
poses, the ModApte split was adopted. 9603 doc-
uments were used as the training set and 3299 as the
test set in the experiments. Only those ninety text
categories which had at least one positive example
in the training set were included in the benchmark.
Another benchmark data corpus we used was the 20
188
Newsgroups corpus, which is a collection of approx-
imate 20,000 newsgroup documents nearly evenly
divided among 20 discussion groups and each doc-
ument is labeled as one of the 20 categories corre-
sponding to the name of the newsgroup that the doc-
ument was posted to.
In preparing the index terms, we restricted the vo-
cabulary to the terms of WordNet 3.0 in order to be
able to calculate the similarity score between any
two terms. Stop words were removed in advance.
Multiple word expressions were used to fully utilize
WordNet. We used the built-in stemmer of WordNet,
which is able to distinguish between different parts-
of-speeches if the form of the word is unambiguous.
For example, {accommodates, accommodated, ac-
commodation} was stemmed to {accommodate, ac-
commodate, accommodation}. We used term fre-
quency as term weighting.
Prior to the semantic ordering, terms were as-
sumed to be in alphabetic order. Measuring the
Jiang-Conrath distance between adjacent terms, the
average distance was 1.68 on the Reuters corpus.
Note that the Jiang-Conrath distance was normal-
ized to the interval [0, 2]. There were few terms with
zero or little distance between them. This is due to
terms which are related and start with the same word
or stem. For example, account, account executive,
account for, accountable.
The same average distance after reordering the
terms with the proposed algorithm and the Jiang-
Conrath distance was 0.56 on the same corpus.
About one third of the terms had very little distance
between each other. Nevertheless, over 10 % of the
total terms still had the maximum distance. This is
due to the non-optimal nature of the proposed term-
ordering algorithm. These terms add noise to the
classification. The noisy terms occur typically at the
two sides of the scale, that is, the leftmost terms and
the rightmost terms. While it is easy to find close
terms in the beginning, as the algorithm proceeds,
fewer terms remain in the pool to be chosen. For in-
stance, brand, brand name, trade name, label are in
the 33rd, 34th, 35th and 36th position on the left side
counting from the seed respectively, while windy,
widespread, willingly, whatsoever, worried, worth-
while close the left side, apparently sharing little in
common. The noise can be reduced by the appropri-
ate choice of the parameter b in exp(?bx2), so that
Kernel Reuters Reuters 20News 20News
Micro Macro Micro Macro
Linear 0.900 0.826 0.801 0.791
Poly 0.903 0.824 0.796 0.788
L2 0.911 0.835 0.813 0.799
Table 1: Micro- and macro-average F1 results
the impact of adjacent but distantly related terms can
be minimized.
Table 1 shows the results on the two benchmark
corpora with the baseline linear kernel. Precision
and recall with regard to a class ck, the F1 score
shown is their average. For all the kernels, the results
with the best parameter settings are shown. Polyno-
mial kernels were benchmarked between degrees 2
and 5. L2 kernels were benchmarked with width b
between 1 and 8, the performance peaking at 4 in
all cases. The model is able to outperform the base-
line kernels, and the differences in micro-averaged
results are statistically significant. In all cases of the
L2 kernel, the increase of F1 was due the increase in
both precision and recall.
4 Conclusions
Information systems are in great need of automated
intelligent tools, but existing algorithms and meth-
ods cannot be pushed much further. Most tech-
niques in current use are impaired by the semanti-
cally poor but widespread representation of informa-
tion and knowledge. For this reason, we propose a
new formalism that combines Cruse?s idea about a
sense spectrum, approximated by semantic ordering,
and its calculation by functions.
The suggested model combines term expansion
with the semantic relations and semantic relatedness
used in semantic smoothing kernels. This slightly
unusual approach needs to transform the real vector
representation to the L2 space, and the experimen-
tal results show that this new representation can im-
prove text classification effectiveness.
Our new model also blends insights from differ-
ent approaches to lexical semantics theory at its dif-
ferent levels. First, during the semantic ordering
of terms the distributional hypothesis meets hand-
crafted lexical resources of word meaning that relate
to term occurrences as if they were their referents,
189
a component external to term context. While high-
quality lexical resources enable such an ordering in
themselves, the procedure can benefit from data de-
rived from the specific corpora in study ? seman-
tic relatedness measures such as the Jiang-Conrath
similarity operate this way. Secondly, once the or-
dering is done and a sense spectrum is constructed,
weights expressing statistical relationships between
terms and documents are borrowed from the vector
space model to form the basis for constructing hypo-
thetical signals of content, documents as continuous
functions.
5 Future research
Figure 3: A hypothetical spectrum of terms.
As we have shown, a spectral interpretation of
sense granularity can lead to improved text catego-
rization results by utilizing L2 space for informa-
tion representation. Whether non-periodic functions
other than the variant tested in this paper can be ap-
plied to the same end needs to be explored.
Turning back to the use of the spectrum of visi-
ble light for representing meaning, this raises new
research questions. On the one hand, translat-
ing one-dimensional semantic ordering into colors
is straightforward. Consider the following map-
ping. Assume that a language has a finite N num-
ber of terms, so the 1-dimensional result is an or-
dered list o1, o2, . . . , oN . Calculate the following:
? = ?N?1i=1 d(oi, oi+1), where ? is the sum of
distances between consecutive words. Further let
F : [0,?] ? [400, 700] be the following map-
ping: F (x) = 400 + x300? . The visible spectrumis between 400 and 700 nm, F maps the cumulative
distances of terms from [0,?] to the visible spec-
trum congruently, i.e. without distorting the dis-
tances. With this bijective (one-to-one) mapping,
each term is assigned a physical wavelength and fre-
quency. Figure 3 shows an example of such a term
spectrum.
On the other hand, we have only begun to test the
applicability of periodic functions in L2 space (Wit-
tek and Dara?nyi, 2007), hence a well-established
link to semantic computing by waves is missing for
the time being. A possible compromise between the
non-periodic vs. periodic approaches can be to ap-
ply wavelets instead of waves, a direction where our
ongoing research shows promising results. These
will be reported elsewhere. In a broader frame of
thought, we are also working on the optical equiv-
alents of the vector space model and the general-
ized vector space model as a first step toward coding
more semantics in mathematical objects, and putting
them to work in novel computing environments.
6 Acknowledgments
The authors are grateful to Martha Palmer (Univer-
sity of Colorado Boulder) for her inspiring sugges-
tions and advice on sense granularity.
References
E. Agirre and O.L. De Lacalle. 2003. Clustering Word-
Net word senses. In Proceedings of RANLP-03, 4th
International Conference on Recent Advances in Nat-
ural Language Processing, pages 121?130.
E. Agirre, E. Alfonseca, and O.L. de Lacalle. 2004. Ap-
proximating hierarchy-based similarity for WordNet
nominal synsets using topic signatures. In Proceed-
ings of GWC-04, 2nd Global WordNet Conference,
pages 15?22.
R. Basili, M. Cammisa, and A. Moschitti. 2005. Effec-
tive use of WordNet semantics via kernel-based learn-
ing. In Proceedings of CoNLL-05, 9th Conference on
Computational Natural Language Learning, pages 1?
8.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. Proceed-
ings of ICDM-06, 6th IEEE International Conference
on Data Mining.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of lexical semantic relatedness. Com-
putational Linguistics, 32(1):13?47.
190
N. Cristianini, J. Shawe-Taylor, and H. Lodhi. 2002. La-
tent semantic kernels. Journal of Intelligent Informa-
tion Systems, 18(2):127?152.
D.A. Cruse. 1986. Lexical semantics.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for Informa-
tion Science, 41(6):391?407.
C. Dorrer, P. Londero, M. Anderson, S. Wallentowitz, and
IA Walmsley. 2001. Computing with interference:
all-optical single-query 50-element database search.
In Proceedings of QELS-01, Quantum Electronics and
Laser Science Conference, pages 149?150.
E. Gabrilovich and S. Markovitch. 2005. Feature gen-
eration for text categorization using world knowledge.
In Proceedings of IJCAI-05, 19th International Joint
Conference on Artificial Intelligence, volume 19.
E. Hoenkamp. 2003. Unitary operators on the document
space. Journal of the American Society for Informa-
tion Science and Technology, 54(4):314?320.
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet
improves text document clustering. In Proceedings of
SIGIR-03, 26th ACM International Conference on Re-
search and Development in Information Retrieval.
J. Hu, L. Fang, Y. Cao, H.J. Zeng, H. Li, Q. Yang, and
Z. Chen. 2008. Enhancing text clustering by lever-
aging Wikipedia semantics. In Proceedings of SIGIR-
08, 31st ACM International Conference on Research
and Development in Information Retrieval, pages 179?
186.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, pages 19?33.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
In Proceedings of ECML-98, 10th European Confer-
ence on Machine Learning, pages 137?142.
D. Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL Workshop
on Usage of WordNet in Natural Language Processing
Systems, volume 98, pages 768?773.
D. Mavroeidis, G. Tsatsaronis, M. Vazirgiannis,
M. Theobald, and G. Weikum. 2005. Word sense
disambiguation for exploiting hierarchical thesauri
in text classification. Proceedings of PKDD-05,
9th European Conference on the Principles of Data
Mining and Knowledge Discovery, pages 181?192.
S. Mohammad and G. Hirst. 2005. Distributional mea-
sures as proxies for semantic relatedness.
H. Paijmans. 1997. Gravity wells of meaning: detecting
information-rich passages in scientific texts. Journal
of Documentation, 53(5):520?536.
M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Natural Language
Engineering, 13(02):137?163.
V.V. Raghavan and S.K.M. Wong. 1986. A critical anal-
ysis of vector space model for information retrieval.
Journal of the American Society for Information Sci-
ence, 37(5):279?287.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In Proceedings of
IJCAI-95, 14th International Joint Conference on Ar-
tificial Intelligence, volume 1, pages 448?453.
J. Rodd, G. Gaskell, and W. Marslen-Wilson. 2002.
Making sense of semantic ambiguity: Semantic com-
petition in lexical access. Journal of Memory and Lan-
guage, 46(2):245?266.
M.D.E.B. Rodriguez and J.M.G. Hidalgo. 1997. Using
WordNet to complement training information in text
categorisation. In Procedings of RANLP-97, 2nd In-
ternational Conference on Recent Advances in Natural
Language Processing.
J. Shawe-Taylor and N. Cristianini. 2004. Kernel Meth-
ods for Pattern Analysis.
S. Shi, J.R. Wen, Q. Yu, R. Song, and W.Y. Ma. 2005.
Gravitation-based model for information retrieval. In
Proceedings of SIGIR-05, 28th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 488?495. ACM New York, NY,
USA.
G. Siolas and F. d?Alche? Buc. 2000. Support vector ma-
chines based on a semantic kernel for text categoriza-
tion. In Proceedings of IJCNN-00, IEEE International
Joint Conference on Neural Networks.
C. J. van Rijsbergen. 2004. The Geometry of Information
Retrieval.
P. Wittek and S. Dara?nyi. 2007. Representing word
semantics for IR by continuous functions. In S. Do-
minich and F. Kiss, editors, Proceedings of ICTIR-07,
1st International Conference of the Theory of Informa-
tion Retrieval, pages 149?155.
P. Wittek, C.L. Tan, and S. Dara?nyi. 2009. An or-
dering of terms based on semantic relatedness. In
H. Bunt, editor, Proceedings of IWCS-09, 8th Inter-
national Conference on Computational Semantics.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of SIGIR-85, 8th ACM International
Conference on Research and Development in Informa-
tion Retrieval, pages 18?25.
191
Proceedings of the 8th International Conference on Computational Semantics, pages 235?247,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
An Ordering of Terms Based on Semantic
Relatedness
Peter Wittek
Department of Computer Science
National University of Singapore
Sa?ndor Dara?nyi
Swedish School of Library and Information Science
Go?teborg University
Sandor.Daranyi@hb.se
Chew Lim Tan
Department of Computer Science
National University of Singapore
tancl@comp.nus.edu.sg
Abstract
Term selection methods typically employ a statistical measure to
filter or weight terms. Term expansion for IR may also depend on
statistics, or use some other, non-metric method based on a lexical
resource. At the same time, a wide range of semantic similarity mea-
sures have been developed to support natural language processing tasks
such as word sense disambiguation. This paper combines the two ap-
proaches and proposes an algorithm that provides a semantic order of
terms based on a semantic relatedness measure. This semantic order
can be exploited by term weighting and term expansion methods.
1 Introduction
Since the early days of the vector space model, it has been debated whether
it is a proper carrier of meaning of texts [23], arguing if distributional sim-
ilarity is an adequate proxy for lexical semantic relatedness [3]. With the
statistical, i.e. devoid of word semantics approaches there is generally no
way to improve both precision and recall at the same time, increasing one
is done at the expense of the other. For example, casting a wider net of
235
search terms to improve recall of relevant items will also bring in an even
greater proportion of irrelevant items, lowering precision. In the meantime,
practical applications in information retrieval and text classification have
been proliferating, especially with developments in kernel methods in the
last decade [9, 4].
Ordering of terms based on semantic relatedness seeks an answer to the
simple question, can statistical term weighting be eclipsed? Namely, vari-
ants of weighting schemes based on term occurrences and co-occurrences
dominate the information retrieval and text classification scenes. However,
they also have a number of limitations. The connection between statistics
and word semantics is in general not understood very well. In other words,
a systematic discussion of mappings between theories of word meaning and
modeling them by mathematical objects is missing for the time being. Fur-
ther, enriching weighting schemes by importing their sense content from
lexical resources such as WordNet lacks a theoretical interpretation in terms
of lexical semantics. Combining co-occurrence and lexical resource-based
approaches for term weighting and term expansion may offer further theo-
retical insights, as well as performance benefits.
Using vectors in the vector space model as such mathematical objects
for the representation of term, document or query meaning necessarily ex-
presses content mapped on form as a set of coordinates. These coordinates,
at least in the case of the tfidf scheme, are corpus-specific, i.e. term weights
are neither constant over time nor database independent. Introducing a se-
mantic ordering of terms, hence loading a coordinate with semantic content,
reduces the dependence on a specific corpus.
In what follows, we will argue that:
? By assigning specific scalar values to terms in an ontology, terms rep-
resented by sets of geometric coordinates can be outdone;
? Such values result from a one-dimensional ordering based on the idea
of a sense-preserving distance between terms in a conceptual hierarchy;
? Sense-preserving distances mapped onto a line condense lexical rela-
tions and express them as a kind of within-language referential mean-
ing pertinent to individual terms, quasi charging their occurrences
independent of their occurrence rates, i.e. from the outside;
? This linear order can be used to assist term expansion and term weight-
ing.
236
This paper is organized as follows. Section 2 discusses the most impor-
tant measures for semantic relatedness with regard to the major linguistic
theories. Section 3 introduces an algorithm that creates a linear semantic
order of terms of a corpus, and Section 4 both offers first results in text
classification and discusses some implications. Finally, Section 5 concludes
the paper.
2 Measuring Semantic Relatedness
Several methods have been proposed for measuring similarity. One of such
early proposals was the semantic differential which analyzes the affective
meaning of terms into a range of different dimensions with the opposed
adjectives at both ends, and locates the terms within semantic space [20].
Semantic similarity as proposed by Miller and Charles is a continuous
variable that describes the degree of synonymy between two terms [16]. They
argue that native speakers can order pairs of terms by semantic similarity,
for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house,
ship-dog, ship-sun. This concept may be extended to quantify relations
between non-synonymous but closely related terms, for example airplane-
wing. Semantic distance is the inverse of semantic similarity [17].
Semantic relatedness is defined between senses of terms. Given a relat-
edness formula rel(s
1
, s
2
) between two senses s
1
and s
2
, term relatedness
between two terms t
1
and t
2
can be calculated as
rel(t
1
, t
2
) = max
s
1
?sen(t
1
),s
2
?sen(t
2
)
rel(s
1
, s
2
),
where sen(t) is a set of senses of term t [3].
Automated systems assign a score of semantic relatedness to a given pair
of terms calculated from a relatedness measure. The absolute score itself
is typically irrelevant on its own, what is important is that the measure
assigns a higher score to term pairs which humans think are more related
and comparatively lower scores to term pairs that are less related [17].
The best known theories of word semantics fall in three major groups:
1. ?Meaning is use? [30]: habitual usage provides indirect contextual in-
terpretation of any term. In accord with Carnap, frequency of use
expresses aspects of a conceptual hierarchy. In terms of logical seman-
tics, one regards document groups as value extensions (classes) and
index terms as value intensions (properties) of a (semantic) function
237
?f?. Extensions and intensions are inverse proportional: the more prop-
erties defined, the less entities they apply to - there are more flowers
in general than tulips in particular, for instance.
2. ?Meaning is change?: the stimulus-response theory by Bloomfield and
the biological theory of meaning by von Uexku?ll both stress that the
meaning of any action is its consequences.
3. ?Meaning is equivalence?: referential or ostensional theories of mean-
ing suggest that ?X = Y for/as long as Z? [22].
Point 2 refers to theories which assign a temporal structure to word
meaning, they are not discussed here. Measures that rely on distributional
measures (Point 1) and those that use knowledge-rich resources (Point 3)
both exist, and they have been individually shown to good quantifiers of
term similarity each [17], These theories have been individually shown to be
good, therefore their combination must be a valid research alternative.
A lexical resource in computer science is a structure that captures se-
mantic relations among terms. Such a resource necessarily entails some sort
of world view with respect to a given domain. This is often conceived as a set
of concepts, their definitions and their inter-relationships; this is referred to
as a conceptualization. The following types of resources are commonly used
in measuring semantic similarity between terms: dictionary [12], semantic
networks, such as WordNet [5], thesauri modeled on Roget?s Thesaurus [19].
All approaches to measuring semantic relatedness that use a lexical re-
source regard the resource as a network or a directed graph, making use of
the structural information embedded in the graph [8, 3].
Distributional similarity, as studied by language technology, covers an
important kind of theories of word meaning and can be hence seen as con-
tributing to semantic document indexing and retrieval. Its predecessors go
back a long way, building on the notion of term dependence and structures
derived therefrom [2, 18]. Also called the contextual theory of meaning (see
[15] for the historical development of the concept), the underlying distri-
butional hypothesis is often cited for explaining how word meaning enters
information processing [10], and basically equals the claim ?meaning is use?
in language philosophy. Before attempts to utilize lexical resources for the
same purpose, this used to be the sole source of word semantics in informa-
tion retrieval, inherent in the exploitation of term occurrences (tfidf) and
term co-occurrences [7, 21, 27], including multiple-level term co-occurrences
[11].
238
Statistical techniques typically suffer from the sparse data problem: they
perform poorly when the terms are relatively rare, due to the scarcity of data.
Hybrid approaches attempt to address this problem by supplementing sparse
data with information from a lexical database [24, 8]. In a semantic network,
to differentiate between the weights of edges connecting a node and all its
child nodes, one needs to consider the link strength of each specific child
link. This is a situation in which corpus statistics can contribute. Ideally
the method chosen should be both theoretically sound and computationally
efficient [8].
Following the notation in information theory, the information content
(IC) of a concept c can be quantified as follows.
IC(c) =
1
logP (c)
.
where P (c) is the probability of encountering an instance of concept c. In the
case of the hierarchical structure, where a concept in the hierarchy subsumes
those ones below it, this implies that P (c) is monotonic as one moves up in
the hierarchy. As the node?s probability increases, its information content or
its informativeness decreases. If there is a unique top node in the hierarchy,
then its probability is 1, hence its information content is 0. Given the
monotonic feature of the information content value, the similarity of two
concepts can be formally defined as follows.
sim(c
1
, c
2
) = max
c?Sup(c
1
,c
2
)
IC(c) = max
c?Sup(c
1
,c
2
)
? log p(c)
where Sup(c
1
, c
2
) is the set of concepts that subsume both c
1
and c
2
. To
maximize the representativeness, the similarity value is the information con-
tent value of the node whose IC value is the largest among those higher order
classes.
The information content method requires less information on the detailed
structure of a lexical resource and it is insensitive to varying link types [24].
On the other hand, it does not differentiate between the similarity values of
any pair of concepts in a sub-hierarchy as long as their lowest super-ordinate
class is the same. Moreover, in the calculation of information content, a
polysemous term will have a large content value if only term frequency data
are used.
The distance function between two terms can be written as follows:
d(t
1
, t
2
) = IC(c
1
) + IC(c
2
)? 2IC(LSuper(c
1
, c
2
)),
239
where LSuper(c1, c2) denotes the lowest super-ordinate of c
1
and c
2
in a
lexical resource. This distance measure also satisfies the properties of a
metric [8].
3 A Semantic Ordering of Terms
Traditional distributional term clustering methods do not provide signifi-
cantly improved text representation [13]. Distributional clustering has also
been employed to compress the feature space while compromising document
classification accuracy [1]. Applying the information bottleneck method to
find term clusters that preserve the information about document categories
has been shown to increase text classification accuracy in certain cases [28].
On the other hand, term expansion has been widely researched, with
varying results [21]. These methods generate new features for each docu-
ment in the data set. These new features can be synonyms or homonyms of
document terms [26], or expanded features for terms, sentences and docu-
ments as in [6]. Several distributional criteria have been used to select terms
related to the query. For instance, [25] proposed the principle that the se-
lected terms should have a higher probability in the relevant documents than
in the irrelevant documents. Others examined the impact of determining ex-
pansion terms using a minimum spanning tree and some simple linguistic
analysis [29].
This section proposes an algorithm that connects term clustering and
term expansion. It employs a pairwise comparison between the terms to
find a linear order, instead of finding clusters. In this order, the transition
from a term to an adjacent one is ?smooth? if the semantic distance between
two neighboring terms is small. The dimension of the feature space is not
compressed, yet, groups of adjacent terms can be regarded as semantic clus-
ters. Hence, following the idea of term expansion, adjacent terms can help
to improve the effectiveness of any vector space-based language technology.
Let V denote a set of terms {t
1
, t
2
, . . . , t
n
} and let d(t
i
, t
j
) denote the
semantic distance between the terms t
i
and t
j
.
Let G = (V,E) denote a weighted undirected graph, where the weights
on the set E are defined by the distances between the terms.
Finding a semantic ordering of terms can be translated to a graph prob-
lem: a minimum-weight Hamiltonian path S of G gives the ordering by
reading the nodes from one end of the path to the other. G is a complete
graph, therefore such a path always exists, but finding it is an NP-complete
problem. The following greedy algorithm is similar to the nearest neighbor
240
heuristic for the solution of the traveling salesman problem. It creates a
graph G
?
= (S, T ), where S = V and T ? E. This G
?
graph is a span-
ning tree of G in which the maximum degree of a node is two, that is, the
minimum spanning tree is a path between two nodes.
Step 1 Find the term at the highest stage of the hierarchy in a lexical
resource.
t
s
= argmin
t
i
?V
depth(t
i
).
This seed term is the first element of V
?
, V
?
= {t
s
}. Remove it from
the set V :
V := V \{t
s
}.
Step 2 Let t
l
denote the leftmost term of the ordering and t
r
the rightmost
one. Find the next two elements of the ordering:
t
?
l
= argmin
t
i
?V
d(t
i
, t
l
),
t
?
r
= argmin
t
i
?V \{t
?
l
}
d(t
i
, t
r
).
Step 3 If d(t
l
, t
?
l
) < d(t
r
, t
?
r
) then add t
?
l
to V
?
, E
?
:= E
?
? {e(t
l
, t
?
l
)}, and
V := V \{t
?
l
}. Else add t
?
r
to V
?
, E
?
:= E
?
?{e(t
r
, t
?
r
)} and V := V \{t
?
r
}.
Step 4 Repeat from Step 2 until V = ?.
The computational cost of the algorithm is O(n
2
). The above algorithm
can be thought of as a modified Prim?s algorithm, but it does not find the
optimal minimum-weight spanning tree.
The validity of the ordering algorithm is discussed as follows.
1. The ordering is possible. Starting from the seed term, the candidate
sets will always contain elements, which either share the same hyper-
nym or are hypernyms of each other.
2. The ordering is good enough. The quality will also depend on the
lexical resource in question. Further, the complexity of human lan-
guages makes the creation of even a near perfect semantic network of
its concepts impossible. Thus in many ways the lexical resource-based
measures are as good as the networks on which they are based.
3. The distance between adjacent terms is uniform. By the construction
of the ordering, it is obvious that the distances will not be uniform.
241
4 Discussion
We were interested in how the distances of consecutive index terms change if
we apply the semantic ordering. We indexed the ModApte split of Reuters-
21578 benchmark corpus with a WordNet-based stemmer. The indexing
found 12643 individual terms. Prior to the semantic ordering, terms were
assumed to be in an arbitrary order. Measuring the Jiang-Conrath distance
between the arbitrarily ordered terms, the average distance was 1.68. Note
that the Jiang-Conrath distance was normalized to the interval [0, 2]. Fig-
ure 1 shows the distribution of distances. The histogram has a high peak
at the maximum distance, indicating that the original arrangment had little
to do with semantic distance. However, there were few terms with zero or
little distance between them. This is due to terms which are related and
start with the same word or stem. For example, account, account execu-
tive, account for, accountable, accountant, accounting principle, accounting
standard, accounting system, accounts payable, accounts receivable.
Figure 1: Distribution of Distances Between Adjacent Terms in an Arbitrary
Order
After the semantic ordering of the term by the proposed algorithm, both
the average distance and the Jiang-Conrath distance were 0.56. About one
third of the terms had very little distance between each other (see Figure 2).
Nevertheless, over 10 % of the total terms still had the maximum distance.
This is due to the non-optimal nature of the proposed term-ordering algo-
rithm. These terms add noise to the classification. The noisy terms occur
242
typically at the two sides of the scale, being the leftmost and the rightmost
ones. While it is easy to find terms close to each other in the beginning, as
the algorithm proceeds, fewer terms remain in the pool to be chosen. For
instance, brand, brand name, trade name, label are in the 33rd, 34th, 35th
and 36th position on the left side counting from the seed respectively, while
windy, widespread, willingly, whatsoever, worried, worthwhile close the left
side, apparently sharing little in common.
Figure 2: Distribution of Distances Between Adjacent Terms in a Semantic
Order Based on Jiang-Conrath Distance
We conducted experiments on the ten most common categories of the
ModApte split of Reuters-21578. We trained support vector machines with
a linear kernel to compare the micro- and macro-average F
1
measures for
different methods. Table 1 summarizes the results. The baseline vector
space model has zero expansion terms. Neighboring terms of the semantic
order were chosen as expansion terms. We found that increasing the number
of expansion terms also increases the effectiveness of classification, however,
effectiveness decreases after 4 expansions for micro-F1 and after 6 expansions
for macro-F1.
5 Conclusions
Terms can be corpus- or genre-specific. Manually constructed general-purpose
lexical resources include many usages that are infrequent in a particular cor-
243
Number of
Expansion Micro-F
1
Macro-F
1
Terms
0 0.900 0.826
2 0.901 0.826
4 0.905 0.828
6 0.898 0.830
8 0.896 0.827
Table 1: Micro-Average and Macro F
1
-measure, Reuters-21578
pus or genre of documents, and therefore of little use. For example, one of
the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym
of person [14]. This usage of the term is practically never used in newspaper
articles, hence distributional attributes should be taken into consideration
when creating a linear ordering of terms.
Integrating lexical resources into an upgraded semantic weighting scheme
that could augment statistical term weighting is a prospect that cannot be
overlooked in information retrieval and text categorization. Our first results
with such a scheme in text categorization. At the same time, the results
also raise the question, does assigning specific scalar values to terms in an
ontology, this far represented by their geometric coordinates only, turn them
metaphorically into band lines of elements in a conceptual spectrum. We
anticipate that applying other types of kernels to the task may bring a new
set of challenging results.
References
[1] L.D. Baker and A.K. McCallum. Distributional clustering of words
for text classification. In Proceedings of SIGIR-98, 21st ACM Inter-
national Conference on Research and Development in Information Re-
trieval, pages 96?103, Melbourne, Australia, August 1998. ACM Press,
New York, NY, USA.
[2] M.A. Ba?rtschi. Term dependence in information retrieval models. Mas-
ter?s thesis, Swiss Federal Institute of Technology, 1984.
244
[3] A. Budanitsky and G. Hirst. Evaluating WordNet-based measures of
lexical semantic relatedness. Computational Linguistics, 32(1):13?47,
2006.
[4] N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent semantic kernels.
Journal of Intelligent Information Systems, 18(2):127?152, 2002.
[5] C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press,
Cambridge, MA, USA, 1998.
[6] E. Gabrilovich and S. Markovitch. Feature generation for text cate-
gorization using world knowledge. In Proceedings of IJCAI-05, 19th
International Joint Conference on Artificial Intelligence, volume 19,
Edinburgh, UK, 2005. Lawrence Erlbaum Associates Ltd.
[7] S. I. Gallant. A practical approach for representing context and for
performing word sense disambiguation using neural networks. Neural
Computation, 3:293?309, 1991.
[8] J.J. Jiang and D.W. Conrath. Semantic similarity based on corpus
statistics and lexical taxonomy. In Proceedings of the International Con-
ference on Research in Computational Linguistics, pages 19?33, Taipei,
Taiwan, 1997.
[9] T. Joachims. Text categorization with support vector machines: Learn-
ing with many relevant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning, pages 137?142, Chemnitz,
Germany, April 1998. Springer-Verlag, London, UK.
[10] J. Karlgren and M. Sahlgren. From words to understanding. Founda-
tions of Real-World Intelligence, pages 294?308, 2001.
[11] A. Kontostathis and W.M. Pottenger. A framework for understanding
latent semantic indexing (LSI) performance. Information Processing
and Management, 42(1):56?73, 2006.
[12] M. Lesk. Automatic sense disambiguation using machine readable dic-
tionaries: How to tell a pine cone from an ice cream cone? In Proceed-
ings of SIGDOC-86, 5th Annual International Conference on Systems
Documentation, pages 24?26, New York, NY, USA, 1986. ACM Press.
[13] D.D. Lewis. An evaluation of phrasal and clustered representations
on a text categorization task. In Proceedings of SIGIR-92, 15th ACM
245
International Conference on Research and Development in Information
Retrieval, pages 37?50, Copenhagen, Denmark, June 1992. ACM Press,
New York, NY, USA.
[14] D. Lin. Automatic retrieval and clustering of similar words. In Pro-
ceedings of COLING-ACL Workshop on Usage of WordNet in Natu-
ral Language Processing Systems, volume 98, pages 768?773, Montre?al,
Que?bec, Canada, August 1998. ACL, Morristown, NJ, USA.
[15] J. Lyons. Semantics. Cambridge University Press, New York, NY,
USA, 1977.
[16] G. Miller and W. Charles. Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1?28, 1991.
[17] S. Mohammad and G. Hirst. Distributional measures as proxies for
semantic relatedness. Submitted for publication, 2005.
[18] J. Morris, C. Beghtol, and G. Hirst. Term relationships and their contri-
bution to text semantics and information literacy through lexical cohe-
sion. In Proceedings of the 31st Annual Conference of the Canadian As-
sociation for Information Science, Halifax, Nova Scotia, Canada, May
2003.
[19] J. Morris and G. Hirst. Lexical cohesion computed by thesaural rela-
tions as an indicator of the structure of text. Computational Linguistics,
17(1):21?48, 1991.
[20] C.E. Osgood. The nature and measurement of meaning. Psychological
Bulletin, 49(3):197?237, 1952.
[21] H.J. Peat and P. Willett. The limitations of term co-occurrence data for
query expansion in document retrieval systems. Journal of the Ameri-
can Society for Information Science, 42(5):378?383, 1991.
[22] C.S. Peirce. Logic as semiotic: The theory of signs. Philosophical
Writings of Peirce, pages 98?119, 1955.
[23] V.V. Raghavan and S.K.M. Wong. A critical analysis of vector space
model for information retrieval. Journal of the American Society for
Information Science, 37(5):279?287, 1986.
246
[24] P. Resnik. Using information content to evaluate semantic similarity in
a taxonomy. In Proceedings of IJCAI-95, 14th International Joint Con-
ference on Artificial Intelligence, volume 1, pages 448?453, Montre?al,
Que?bec, Canada, August 1995.
[25] S.E. Robertson. On term selection for query expansion. Journal of
Documentation, 46(4):359?364, 1990.
[26] M.D.E.B. Rodriguez and J.M.G. Hidalgo. Using WordNet to com-
plement training information in text categorisation. In Procedings of
RANLP-97, 2nd International Conference on Recent Advances in Natu-
ral Language Processing. John Benjamins Publishing, Amsterdam, The
Netherlands, 1997.
[27] H. Schutze and T. Pedersen. A co-occurrence-based thesaurus and
two applications to information retrieval. Information Processing and
Management, 3(33):307?318, 1997.
[28] N. Slonim and N. Tishby. The power of word clusters for text clas-
sification. In Proceedings of ECIR-01, 23rd European Colloquium on
Information Retrieval Research, Darmstadt, Germany, 2001.
[29] A.F. Smeaton and C.J. van Rijsbergen. The retrieval effects of query
expansion on a feedback document retrieval system. The Computer
Journal, 26(3):239?246, 1983.
[30] L. Wittgenstein. Philosophical Investigations. Blackwell Publishing,
Oxford, UK, 1967.
247
