Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 52?61,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Feasibility of HMEANT as a Human MT Evaluation Metric
Alexandra Birch Barry Haddow Ulrich Germann
a.birch@ed.ac.uk bhaddow@inf.ed.ac.uk ugermann@inf.ed.ac.uk
Maria Nadejde Christian Buck Philipp Koehn
maria.nadejde@gmail.com cbuck@lantis.de pkoehn@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
There has been a recent surge of interest in
semantic machine translation, which stan-
dard automatic metrics struggle to evalu-
ate. A family of measures called MEANT
has been proposed which uses semantic
role labels (SRL) to overcome this prob-
lem. The human variant, HMEANT, has
largely been evaluated using correlation
with human contrastive evaluations, the
standard human evaluation metric for the
WMT shared tasks. In this paper we claim
that for a human metric to be useful, it
needs to be evaluated on intrinsic proper-
ties. It needs to be reliable; it needs to
work across different language pairs; and
it needs to be lightweight. Most impor-
tantly, however, a human metric must be
discerning. We conclude that HMEANT
is a step in the right direction, but has
some serious flaws. The reliance on verbs
as heads of frames, and the assumption
that annotators need minimal guidelines
are particularly problematic.
1 Introduction
Human evaluation is essential in machine transla-
tion (MT) research because it is the ultimate way
to judge system quality. Furthermore, human eval-
uation is used to evaluate automatic metrics which
are necessary for tuning system parameters. Un-
fortunately, there is no clear consensus on which
evaluation strategy is best. Humans have been
asked to judge if translations are correct, to grade
them and to rank them. But it is often very difficult
to decide how good a translation is, when there are
so many possible ways of translating a sentence.
Another problem is that different types of evalua-
tion might be useful for different purposes. If the
MT is going to be the basis of a human transla-
tor?s work-flow, then post-editing effort seems like
a natural fit. However, for people using MT for
gisting, what we really want is some measure of
how much meaning has been retained.
We clearly need a metric which tries to answer
the question, how much of the meaning does the
translation capture. In this paper, we explore the
use of human evaluation metrics which attempt
to capture the extent of this meaning retention.
In particular, we consider HMEANT (Lo and Wu,
2011a), a metric that uses semantic role labels
to measure how much of the ?who, why, when,
where? has been preserved. For HMEANT evalua-
tion, annotators are instructed to identify verbs as
heads of semantic frames. Then they attach role
fillers to the heads and finally they align heads
and role fillers in the candidate translation with
those in a reference translation. In a series of pa-
pers, Lo and Wu (2010, 2011b,a, 2012) explored a
number of questions, evaluating HMEANT by us-
ing correlation statistics to compare it to judge-
ments of human adequacy and contrastive evalu-
ations. Given the drawbacks of those evaluation
measures, which we discuss in Sec. 2, they could
just as well have been evaluating the human ade-
quacy and contrastive judgements using HMEANT.
Human evaluation metrics need to be judged on
other intrinsic qualities, which we describe below.
The aim of this paper is to evaluate the effec-
tiveness of HMEANT, with the goal of using it to
judge the relative merits of different MT systems,
for example in the shared task of the Workshop on
Machine Translation.
In order to be useful, an MT evaluation metric
must be reliable, be language independent, have
discriminatory power, and be efficient. We address
each of these criteria as follows:
52
Reliability We produce extensive IAA (Inter-
annotator agreement) for HMEANT, breaking it
down into the different stages of annotation. Our
experimental results show that whilst the IAA for
HMEANT is acceptable at the individual stages of
the annotation, the compounding effect of dis-
agreement at each stage of the pipeline greatly re-
duces the effective overall IAA ? to 0.44 on role
alignment for German, and, only slightly better,
0.59 for English. This raises doubts about the reli-
ability of HMEANT in its current form.
Discriminatory Power We consider output of
three types of MT system (Phrase-based, Syntax-
based and Rule-based) to attempt to gain insight
into the different types of semantic information
preserved by the different systems. The Syntax-
based system seems to have a slight edge overall,
but since IAA is so low, this result has to be taken
with a grain of salt.
Language Independence We apply HMEANT
to both English and German translation outputs,
showing that the guidelines can be adapted to the
new language.
Efficiency Whilst HMEANT evaluation will
never be as fast as, for example, the contrastive
judgements used for the WMT shared task,
it is still reasonably efficient considering the
fine-grained nature of the evaluation. On average,
annotators evaluated about 10 sentences per hour.
2 Related Work
Even though the idea that machine translation re-
quires a semantic representation of the translated
content is as old as the idea of computer-based
translation itself (Weaver, 1955), it has not been
until recently that people have begun to combine
statistical models with semantic representations.
Jones et al (2012), for example, represent mean-
ing as directed acyclic graphs and map these to
PropBank (Palmer et al, 2005) style dependen-
cies. To evaluate such approaches properly, we
need evaluation metrics that capture the accuracy
of the translation.
Current automatic metrics of machine trans-
lation, such as BLEU (Papineni et al, 2002),
METEOR (Lavie and Denkowski, 2009) and
TER (Snover et al, 2009b), which have greatly
accelerated progress in MT research, rely on shal-
low surface properties of the translations, and
only indirectly capture whether or not the trans-
lation preserves the meaning. This has meant that
potentially more sophisticated translation models
are pitted against the flatter phrase-based mod-
els, based on metrics which cannot reflect their
strengths. Callison-Burch et al (2011) provide ev-
idence that automatic metrics are inconsistent with
human judgements when comparing rule-based
against statistical machine translation systems.
Automatic evaluation metrics are evaluated and
calibrated based on their correlation with human
judgements. However, after more than 60 years
of research into machine translation, there is still
no consensus on how to evaluate machine transla-
tion based on human judgements. (Hutchins and
Somers, 1992; Przybocki et al, 2009).
One obvious approach is to ask annotators to
rate translation candidates on a numerical scale.
Under the DARPA TIDES program, the Linguistic
Data Consortium (2002) developed an evaluation
scheme that relies on two five-point scales repre-
senting fluency and adequacy. This was also the
human evaluation scheme used in the annual MT
competitions sponsored by NIST (2005).
In an analysis of human evaluation results for
the WMT ?07 workshop, however, Callison-Burch
et al (2007) found high correlation between flu-
ency and adequacy scores assigned by individual
annotators, suggesting that human annotators are
not able to separate these two evaluation dimen-
sions easily. Furthermore these absolute scores
show low inter-annotator agreement. Instead of
giving absolute quality assessments, annotators
appeared to be using their ratings to rank trans-
lation candidates according to their overall prefer-
ence for one over the other.
In line with these findings, Callison-Burch et al
(2007) proposed to let annotators rank translation
candidates directly, without asking them to assign
an absolute quality assessment to each candidate.
This type of human evaluation has been performed
in the last six Workshops on Statistical Machine
Translation.
Although it is useful to have a score or a rank
for a particular sentence, especially for evaluat-
ing automatic metrics, these ratings are necessar-
ily a simplification of the real differences between
translations. Translations can contain a large num-
ber of different types of errors of varying severity.
Even if we put aside difficulties with selecting one
preferred sentence, ranking judgements are diffi-
cult to generalise. Humans are shown five transla-
tions at a time, and there is a high cognitive cost to
ranking these at once. Furthermore, these repre-
53
sent a subset of the competing systems, and these
rankings must be combined with other annotators
judgements on five other system outputs to com-
pute an overall ranking. The methodology for in-
terpreting the contrastive evaluations has been the
subject of much recent debate in the community
(Bojar et al, 2011; Lopez, 2012).
There has been some effort to overcome these
problems. HTER (Snover et al, 2009a) is a met-
ric which counts the number of edits needed by a
human to convert the machine translation so as to
convey the same meaning as the reference. This
type of evaluation is of some use when one is us-
ing MT to aid human translation (although the re-
lationship between number of edits and actual ef-
fort is not straightforward (Koponen, 2012)), but
it is not so helpful when one?s task is gisting. The
number of edits need not correlate with the sever-
ity of the semantic differences between the two
sentences. The loss of a negative, for instance, is
only one edit away from the original, but the se-
mantics change completely.
Alternatively, HyTER (Dreyer and Marcu,
2012) is an annotation tool which allows a user
to create an exponential number of correct trans-
lations for a given sentence. These references are
then efficiently exploited to compare with machine
translation output. The authors argue that the cur-
rent metrics fail simply because they have access
to sets of reference translations which are simply
too small. However, the fact is that even if one
does have access to large numbers of translations,
it is very difficult to determine whether the refer-
ence correctly captures the essential semantic con-
tent of the references.
The idea of using semantic role labels to evalu-
ate machine translation is not new. Gime?nez and
Ma`rquez (2007) proposed using automatically as-
signed semantic role labels as a feature in a com-
bined MT metric. The main difference between
this application of semantic roles and MEANT is
that arguments for specific verbs are taken into ac-
count, instead of just applying the subset agent,
patient and benefactor. This idea would probably
help human annotators to handle sentences with
passives, copulas and other constructions which
do not easily match the most basic arguments. On
the other hand, verb specific arguments are lan-
guage dependent.
Bojar and Wu (2012), applying HMEANT to
English-to-Czech MT output, identified a number
of problems with HMEANT, and suggested a vari-
ety of improvements. In some respects, this work
is very similar, except that our goal is to evaluate
HMEANT along a range of intrinsic properties, to
determine how useful the metric really is to evalu-
ation campaigns such as the workshop on machine
translation.
3 Evaluation with HMEANT
3.1 Annotation Procedure
The goal of the HMEANT metric is to capture es-
sential semantic content, but still be simple and
fast. There are two stages to the annotation, the
first of which is semantic role labelling (SRL).
Here the annotator is directed to select the actions,
or frame heads, by marking all the verbs in the sen-
tence except for auxilliaries and modals. The roles
(or slot fillers) within the frame are then marked
and each is linked with a unique action. Each role
is given a type from an inventory of 11 (Table 1),
and an action with its collection of corresponding
roles is known as a frame. In the role annotation
the idea is to get the annotator to recognise who
did what to who, when, where and why in both the
references and the MT outputs.
who what whom when where
agent patient benefactive temporal locative
why how
purpose degree, manner, modal, negation, other
Table 1: Semantic roles
The second stage in the annotation is alignment,
where the annotators match elements of the SRL
annotation in the reference with that in the MT
output. The annotators link both actions and roles,
and these alignments can be matched as ?Correct?
or ?Partial? matches, depending on how well the
action or role is translated. The guidelines for the
annotators are deliberately minimalistic, with the
argument being that non-experts can get started
quickly. Lo and Wu (2011a) claim that unskilled
annotators can be trained within 15 minutes.
In all such human evaluation, there is a trade-
off between simplicity and accuracy. Clearly when
evaluating bad machine translation output, we do
not want to label too much. However, sometimes
having so little choice of semantic roles can lead
to confusion and slow down the annotator when
more complicated examples do not fit the scheme.
Therefore, common exceptions need to be handled
either in the roles provided, or in the annotator
guidelines.
54
3.2 Calculation of Score
The overall HMEANT score for MT evaluation
is computed as the f-score from the counts of
matches of frames and their role fillers between
the reference and the MT output. Unmatched
frames are excluded from the calculation together
with all their corresponding roles.
In recognition that preservation of some types
of semantic relations may be more important than
others for a human to understand a sentence, one
may want to weight them differently in the com-
putation of the HMEANT score. Lo and Wu (2012)
train weights for each role filler type to optimise
correlation with human adequacy judgements. As
an unsupervised alternative, they suggest weight-
ing roles according to their frequency as approxi-
mation to their importance.
Since the main focus of the current paper is the
annotation of the actions, roles and alignments that
HMEANT depends on, we do not explore such dif-
ferent weight-setting schemes, but set the weights
uniformly, with the exception of a partial align-
ment, which is given a weight of 0.5. HMEANT is
thus defined as follows:
Fi = # correct or partially correct fillers
for PRED i in MT
MTi = total # fillers for PRED i in MT
REFi = total # fillers for PRED i in REF
P =
?
matched i
Fi
MTi
R =
?
matched i
Fi
REFi
Ptotal =
Pcorrect + 0.5Ppartial
total # predicates in MT
Rtotal =
Pcorrect + 0.5Ppartial
total # predicates in REF
HMEANT = 2 ? Ptotal ?RtotalPtotal +Rtotal
3.3 Automating HMEANT
One of the main directions taken by the authors of
HMEANT is in creating a fully automated version
of the metric (MEANT) in (Lo et al, 2012). The
metric combines shallow semantic parsing with a
simple maximum weighted bipartite matching al-
gorithm for aligning semantic frames. They use
approximate matching schemes (Cosine and Jac-
card similarity) for matching roles, with the lat-
ter producing better alignments (Tumuluru et al,
2012). They demonstrate that MEANT corre-
lates with human adequacy judgements better than
other commonly used automatic metrics. In this
paper we focus on human evaluation, as it is es-
sential for building better automatic metrics, and
therefore a more fundamental problem.
4 Experimental Setup
4.1 Systems and Data Sets
We performed HMEANT evaluation on three
systems selected from 2013 WMT evaluation1.
The systems we selected were uedin-wmt13,
uedin-syntax and rbmt-3, which were cho-
sen to provide us with a high performing phrase-
based system, a high performing syntax-based
system and the top performing rule-based system,
respectively. The cased BLEU scores of the three
systems are shown in Table 2.
System Type de-en en-de
uedin-wmt13 Phrase 26.6 20.1
uedin-syntax Syntax 26.3 19.4
rbmt-3 Rule 18.8 16.5
Table 2: Cased BLEU on the full newstest2013
test set for the systems used in this study
We randomly selected sentences from the en-de
and de-en newstest2013 tasks, and extracted
the corresponding references and system outputs
for these sentences. For the en-de task, 75% of our
selected sentences were selected from the section
of newstest2013 that was originally in Ger-
man, with the other 25% from the section that was
originally in English. The sentence selection for
the de-en task was performed in a similar man-
ner. For presentation to the annotators, the sen-
tences were split into segments of 12. We found
that with practice, annotators could complete one
of these segments in around 100-120 minutes. In
total, with close to 70 hours of annotator effort,
we evaluated 142 sentences of German, and 72
sentences of English. The annotation for each
sentence includes 1 reference, 3 system outputs,
and their corresponding alignments. Apart from 5
singly-annotated German sentences, and 1 singly-
annotated English sentence, all sentences were an-
notated by exactly 2 annotators.
1www.statmt.org/wmt13
55
4.2 Annotation
The annotation for English was performed by 3
different annotators (E1, E2 and E3), and the Ger-
man annotation by 2 annotators (D1 and D2).
All the English annotators were machine transla-
tion researchers, with E1 and E2 both native En-
glish speakers whereas E3 is not a native speaker,
but lives and works in an English-speaking coun-
try. The two German annotators were both native
speakers of German, with no background in com-
putational linguistics, although D2 is a teacher of
German as a second language and has had linguis-
tic training.
The HMEANT evaluation task was carried out
following the framework described in Lo and Wu
(2011a) and Bojar and Wu (2012). For each sen-
tence in the evaluation set, the annotators were first
asked to mark the semantic frames and roles (i.e.,
slot fillers within the frame) in a human reference
translation of the respective sentence. They were
then presented with the output of several machine
translation systems for the same source sentence,
one system at a time, with the reference transla-
tion and its annotations visible in the left half of
the screen (cf. Fig. 1). For each system, the an-
notators were asked to annotate semantic frames
and slot fillers in the translation first, and then
align them with frame heads and slot fillers in
the human reference translation. Annotations and
alignment were performed with Edi-HMEANT2,
a web-based annotation tool for HMEANT that
we developed on the basis of Yawat (Germann,
2008). The tool allows the alignment of slots from
different semantic frames, and the alignment of
slots of different types; however, such alignments
are not considered in the computation of the final
HMEANT score.
The annotation guidelines were essentially
those used in Bojar and Wu (2012), with some ad-
ditional English examples, and a complete set of
German examples. For ease of comparison with
prior work, we used the same set of semantic role
labels as Bojar and Wu (2012), shown in Table 1.
Given the restriction that the head of a frame can
consist of only one word, a convention was made
that all other verbs attached to the main verb such
as modals, auxiliaries or separable particles for
German verbs, would be labelled as modal. This
was the only change we made to the HMEANT
2Edi-HMEANT is part of the Edinburgh
Multi-text Annotation and Alignment Tool Suite
(http://www.statmt.org/edimtaats).
scheme.
5 Results and Discussion
5.1 Inter-Annotator Agreement
We first measured IAA on role identification, as
in Lo and Wu (2011a), except that we use exact
match on word spans as opposed to the approx-
imate match employed in that reference. Whilst
exact match is a harsher measure, penalising dis-
agreements related to punctuation and articles, us-
ing any sort of approximate match would mean
having to deal with N:M matches. IAA is defined
as follows:
IAA = 2 ? P ?RP +R
Where P is defined as the number of labels (ei-
ther heads, roles, or alignments) that match be-
tween annotators, divided by the total number of
labels given by annotator 1. And R is defined the
same way for annotator 2. This is similar to an
F-measure (f1), where we consider one of the an-
notators as the gold standard. The IAA for role
identification is shown in Table 3.
Reference Hypothesis
Lang. matches f1 matches f1
de 865 0.846 2091 0.737
en 461 0.759 1199 0.749
Table 3: IAA for role identification. This is calcu-
lated by considering exact endpoint matches on all
spans (predicates and arguments).
The agreements in Table 3 are not too differ-
ent from those reported in earlier work. We note
that the IAA for the German annotators drops for
the MT system outputs, but this may be because
the English annotators (as MT researchers) are less
bothered by bad MT output than their counterparts
working on the German texts.
Next we looked at the IAA on role classifica-
tion, the other IAA figure provided by Lo and Wu
(2011a). We only considered roles where both an-
notators had marked the same span in the same
frame, with the frame being identified by its ac-
tion. The IAA for role classification is shown in
Table 4.
Again, we show similar levels of IAA to those
reported in (Lo and Wu, 2011a). Examining the
disagreements in more detail, we produced counts
of the most common role type disagreements, by
56
Figure 1: Example of a sentence pair annotated with Edi-HMEANT. The reference translation is on
the left, the machine translation output on the right. Head and slot fillers for each semantic frame are
marked by selecting spans in the text and automatically listed in tables below the respective sentences.
Frames and slot fillers are aligned by clicking on table cells. The alignments of the semantic frames are
highlighted: green (grey in black and white version) for exact match and grey (light grey) for partial
match.
Reference Hypothesis
Lang. matches f1 matches f1
de 425 0.717 1050 0.769
en 245 0.825 634 0.826
Table 4: IAA for role classification. We only con-
sider cases where annotators had marked the same
span in the same frame.
Role 1 Role 2 Count
Agent Experiencer-Patient 110
Degree-Extent Modal 92
Beneficiary Experiencer-Patient 45
Experiencer-Patient Manner 26
Manner Other 25
Table 5: Most common role type disagreements,
for German
language. We show the top 5 disagreements in Ta-
bles 5 and 6. Essentially these show that the most
common role types provide the most confusions.
In order to shed more light on the role type dis-
agreements, we examined a random sample of 10
of the English annotations where the annotators
had disagreed about ?Agent? versus ?Experiencer-
Patient?. In 7 of these cases, there was a definite
correct answer, according to the annotation guide-
lines. Of the other 3, there were 2 cases of poor
MT output making the semantic interpretation dif-
ficult, and one case of existential ?there?. Of the 7
cases where one annotator appears in error, 3 were
passive, 1 was a copula, and 1 involved the verb
Role 1 Role 2 Count
Agent Experiencer-Patient 44
Manner Other 22
Degree-Extent Temporal 12
Degree-Extent Other 12
Beneficiary Experiencer-Patient 11
Table 6: Most common role type disagreements,
for English
?receive?. For the other 2 there was no clear rea-
son for the error. From this small sample, we sug-
gest that passive constructions are still difficult to
annotate semantically.
The last of elements of the semantic frames to
be considered for IAA are the actions, i.e. the
frame heads or predicates. In this case identifying
a match was straightforward as actions are identi-
fied by a single token. The IAA for action identi-
fication is shown in Table 7.
Reference Hypothesis
Lang. matches f1 matches f1
de 238 0.937 592 0.826
en 126 0.818 362 0.868
Table 7: IAA for action identification.
We see fairly high IAA for actions, which seems
encouraging, but given the importance of actions
in HMEANT, we probably need the scores to be
higher. Most of the problems with the identifica-
tion of actions centre around multiple-verb con-
structions and participles.
We now turn our attention to the second stage
of the annotation process where the annotators
marked alignments between slots and roles. These
provide the relevant statistics for the calculation of
the HMEANT score so it is important that they are
annotated reliably.
Firstly, we consider the alignment of actions. In
this case, we use pipelined statistics, in that if one
annotator marks actions in the reference and hy-
pothesis, then aligns them, whilst the other anno-
tator does not mark the corresponding actions, we
still count this as an action alignment mismatch.
This creates a harsher measure on action align-
ment, but gives a better idea of the overall relia-
bility of the annotation task. In Table 8 we show
the IAA (as F1) on action alignments. Comparing
Tables 8 and 7 we see that, for English at least, the
57
Lang. matches f1
de 300 0.655
en 275 0.769
Table 8: IAA for action alignment, collapsing par-
tial and full alignment
agreement on action alignment is not much lower
than that on action identification, indicating that if
annotators agree on the actions then they generally
agree on how they align. For German, however,
the IAA on action alignment is a bit lower, ap-
parently because one of the annotators was much
stricter about which actions they aligned.
In order to calculate the IAA on role align-
ments, we only consider those alignments that
connect two roles in aligned frames, of the same
type, since these are the only role alignments that
count for computing the HMEANT score. This
means that if one of the annotators does not align
the frames, then all the contained role alignments
are counted as mismatches. We do not consider
the spans when calculating the agreement on role
alignments, meaning that if one annotator has an
alignment between roles of type T in frame F ,
and the other annotator also aligns the same types
of roles in the same frame, then they are consid-
ered as a match. This is done because it is only the
counts of alignments that are relevant for HMEANT
scoring. The IAA on the role alignments is quite
Lang. matches f1
de 448 0.442
en 506 0.596
Table 9: IAA for role alignment.
low, dipping below 0.5 for German. This is mainly
because of the pipelining effect, where annota-
tion disagreements at each stage are compounded.
Since the final HMEANT score is computed essen-
tially by counting role alignments, this level of
IAA causes problems for this score calculation.
We computed HMEANT and BLEU scores for the
hypotheses annotated by each annotator pair. The
HMEANT scores were calculated as described in
Section 3.2. The two metrics are calculated for
each sentence (we apply +1 smoothing for BLEU),
then averaged across all sentences. Table 10 shows
the scores organised by annotator pair and sys-
tem type. The agreement in the overall scores is
not good, but really just reflects the compounded
Annotator System BLEU HMEANT HMEANT
Pair (Annot. 1) (Annot. 2)
Phrase 0.310 0.626 (2) 0.672 (3)
E1, E2 Syntax 0.291 0.635 (1) 0.730 (1)
Rule 0.252 0.578 (3) 0.673 (2)
Phrase 0.378 0.569 (1) 0.602 (3)
E1, E3 Syntax 0.376 0.553 (2) 0.627 (2)
Rule 0.320 0.546 (3) 0.646 (1)
Phrase 0.360 0.669 (2) 0.696 (3)
E2, E3 Syntax 0.362 0.751 (1) 0.739 (1)
Rule 0.308 0.624 (3) 0.716 (2)
Phrase 0.296 0.327 (1) 0.631 (3)
D1, D2 Syntax 0.321 0.312 (2) 0.707 (1)
Rule 0.242 0.274 (3) 0.648 (2)
Table 10: Scores assigned by each annotator pair.
The numbers in brackets after the HMEANT scores
show the relative ranking assigned by each anno-
tator.
agreement problems in the role alignments (Table
9). In no case do the annotators choose a consis-
tent ranking of the 3 systems, and in 2 of the 4 an-
notator pairs, the annotators disagree about which
is the top performing system.
5.2 Overall Scores
In this section we report the overall HMEANT
scores of the three systems whose output we an-
notated. Our main focus on this paper was on the
annotation task, so we do not wish to emphasise
the scoring, but it is nevertheless an important end-
product of the HMEANT annotation process. The
overall scores (HMEANT and +1 smoothed sen-
tence BLEU, averaged across sentences and anno-
tators) are given in Table 11.
Language System BLEU HMEANT
Phrase 0.351 0.634
en Syntax 0.344 0.667
Rule 0.295 0.625
Phrase 0.294 0.482
de Syntax 0.302 0.517
Rule 0.242 0.464
Table 11: Comparison of mean HMEANT and
(smoothed sentence) BLEU for the three systems.
From the table we can observe that, whilst
BLEU shows similar scores for the phrase-based
and syntax-based systems, with lower scores for
the rule-based system, HMEANT shows the syntax-
based system as being ahead, with the other two
showing similar performance. We would caution
against reading too much into this, considering the
relatively small number of sentences annotated,
58
and the issues with IAA exposed in the previous
section, but it is an encouraging results for syntax-
based MT.
5.3 Discussion
Machine translation research needs a reliable
method for evaluating and comparing different
machine translation systems. The performance of
HMEANT as shown in the previous section is dis-
appointing. The fact that the final role IAA, in Ta-
ble 9, is 0.442 for German and 0.596 for English,
demonstrates that there are fundamental problems
with the scheme. One of the areas of greatest con-
fusion is between what seems like one of the eas-
iest role types to distinguish: agent and patient.
Here is an example of a passive where one anno-
tator has marked ?tea? wrongly as agent, and the
other annotator correctly labelled it as patient:
Reference: In the kitchen, tea is prepared for
the guests
ACTION prepared
LOCATIVE In the kitchen
AGENT / PATIENT tea
MODAL is
BENEFICIARY for the guests
We would argue that the most important change
to HMEANT must be in creating more comprehen-
sive annotation guidelines, with examples of diffi-
cult cases. Bojar and Wu (2012) listed a number of
problems and improvements to HMEANT, which
we largely agree with. We list the most important
limitations of HMEANT that we have encountered:
? Single Word Heads Verbal predicates often
consist of multiple words, which can be split.
For example: ?Take him up on his offer?.
? Heads being limited to verbs The semantics
of verbs can often be carried by an equivalent
noun and should be allowed by HMEANT. For
example ?My father broke down and cried .?,
the verb ?cried? is correctly paraphrased in
?My father collapsed in tears .?
? Copular Verbs These do not fit in to the lim-
ited list of role types. For example forcing
this sentence ?The story is plausible?, to have
and agent and patient is confusing.
? Prepositional Phrases attaching to a noun
These can greatly affect the semantics of a
sentence, but HMEANT has no way of captur-
ing this.
? Semantics not on head This frequently oc-
curs with light verbs, for example ?Bouson
did the review of the paper? is equivalent to
?Bouson reviewed the paper?.
? Hierarchy of frames There are often frames
which are embedded in other frames, for ex-
ample in reported speech. It is not clear
whether errors at the lowest level should be
marked wrong just at that point, or whether
they should be marked wrong all the way up
the semantic tree. For example: ?Arafat said
?Isreal suffocates such a hope in the germ? ?.
The frame headed by ?said? is largely cor-
rect, but the reported speech is not. The pa-
tient role of the verb ?said? could be aligned
as correct, as the error is already captured in
relation to the verb ?suffocates?.
? No discourse markers These are impor-
tant for capturing the relationships between
frames and should be labelled.
6 Conclusion
HMEANT represents an attempt to create a human
evaluation for machine translation which directly
measures the semantic content preserved by the
MT. It partly succeeds. However we have cast
doubt on the claim that HMEANT can be reliably
annotated with minimal annotator training and
guidelines. In the most extensive study of inter-
annotator agreement yet performed for HMEANT,
across two language pairs, we have shown that the
disagreements between annotators make it diffi-
cult to reliably compare different MT systems with
HMEANT scores.
Furthermore, the fact that HMEANT is restricted
to annotating purely verbal predicates results in
some important disadvantages. Ideally we need a
more general definition of a frame, not restricted
to purely verbal predicates, and we would like
to be able to link frames. We should explore
the feasibility of a semantic framework which at-
tempts to overcome reliance on syntactic proper-
ties such as Universal Conceptual Cognitive An-
notation (Abend and Rappoport, 2013).
7 Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE).
59
References
Abend, Omri and Ari Rappoport. 2013. ?Univer-
sal Conceptual Cognitive Annotation (UCCA).?
Proceedings of ACL.
Bojar, Ondrej, Milos? Ercegovc?evic?, Martin Popel,
and Omar Zaidan. 2011. ?A Grain of Salt for the
WMT Manual Evaluation.? Proceedings of the
Sixth Workshop on Statistical Machine Transla-
tion, 1?11. Edinburgh, Scotland.
Bojar, Ondrej and Dekai Wu. 2012. ?Towards a
Predicate-Argument Evaluation for MT.? Pro-
ceedings of SSST, 30?38.
Callison-Burch, Chris, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
2007. ?(Meta-) evaluation of machine trans-
lation.? Proceedings of the Second Workshop
on Statistical Machine Translation, 136?158.
Prague, Czech Republic.
Callison-Burch, Chris, Philipp Koehn, Christof
Monz, and Omar F Zaidan. 2011. ?Findings of
the 2011 workshop on statistical machine trans-
lation.? Proceedings of the Sixth Workshop on
Statistical Machine Translation, 22?64.
Dreyer, Markus and Daniel Marcu. 2012. ?Hyter:
Meaning-equivalent semantics for translation
evaluation.? Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguis-
tics: Human Language Technologies, 162?171.
Montre?al, Canada.
Germann, Ulrich. 2008. ?Yawat: Yet Another
Word Alignment Tool.? Proceedings of the
ACL-08: HLT Demo Session, 20?23. Colum-
bus, Ohio.
Gime?nez, Jesu?s and Llu??s Ma`rquez. 2007. ?Lin-
guistic features for automatic evaluation of het-
erogenous mt systems.? Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, StatMT ?07, 256?264. Stroudsburg, PA,
USA.
Hutchins, W. J. and H. L. Somers. 1992. An intro-
duction to machine translation. Academic Press
New York.
Jones, Bevan, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
?Semantics-based machine translation with hy-
peredge replacement grammars.? Proceedings
of COLING.
Koponen, Maarit. 2012. ?Comparing human per-
ceptions of post-editing effort with post-editing
operations.? Proceedings of the Seventh Work-
shop on Statistical Machine Translation, 181?
190. Montre?al, Canada.
Lavie, Alon and Michael Denkowski. 2009. ?The
METEOR metric for automatic evaluation of
machine translation.? Machine Translation.
Linguistic Data Consortium. 2002. ?Lin-
guistic data annotation specification: As-
sessment of fluency and adequacy in
Chinese-English translation.? http:
//projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf.
Lo, Chi-kiu, Anand Karthik Tumuluru, and Dekai
Wu. 2012. ?Fully automatic semantic MT eval-
uation.? Proceedings of WMT, 243?252.
Lo, Chi-kiu and Dekai Wu. 2010. ?Evaluating
machine translation utility via semantic role la-
bels.? Proceedings of LREC, 2873?2877.
Lo, Chi-kiu and Dekai Wu. 2011a. ?MEANT : An
inexpensive , high-accuracy , semi-automatic
metric for evaluating translation utility via se-
mantic frames.? Proceedings of ACL, 220?229.
Lo, Chi-kiu and Dekai Wu. 2011b. ?Structured vs.
flat semantic role representations for machine
translation evaluation.? Proceedings of SSST,
10?20.
Lo, Chi-kiu and Dekai Wu. 2012. ?Unsupervised
vs. supervised weight estimation for semantic
MT evaluation metrics.? Proceedings of SSST,
49?56.
Lopez, Adam. 2012. ?Putting human assessments
of machine translation systems in order.? Pro-
ceedings of WMT, 1?9.
NIST. 2005. ?The 2005 NIST machine
translation evaluation plan (MT-05).?
http://www.itl.nist.gov/iad/
mig/tests/mt/2005/doc/mt05_
evalplan.v1.1.pdf.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. ?The proposition bank: An anno-
tated corpus of semantic roles.? Computational
Linguistics, 31(1):71?106.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. ?BLEU: a method for au-
tomatic evaluation of machine translation.? Pro-
ceedings of the Association for Computational
Linguistics, 311?318. Philadelphia, USA.
Przybocki, Mark, Kay Peterson, Se?bastien Bron-
sart, and Gregory Sanders. 2009. ?The NIST
60
2008 metrics for machine translation challen-
geoverview, methodology, metrics, and results.?
Machine Translation, 23(2):71?103.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009a. ?Fluency, ad-
equacy, or HTER? exploring different human
judgments with a tunable MT metric.? Proceed-
ings of the Workshop on Statistical Machine
Translation at the Meeting of the European
Chapter of the Association for Computational
Linguistics (EACL-2009). Athens, Greece.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009b. ?TER-plus:
paraphrase, semantic, and alignment enhance-
ments to translation edit rate.? Machine Trans-
lation.
Tumuluru, Anand Karthik, Chi-kiu Lo, and Dekai
Wu. 2012. ?Accuracy and robustness in measur-
ing the lexical similarity of semantic role fillers
for automatic semantic MT evaluation.? Pro-
ceedings of PACLIC, 574?581.
Weaver, Warren. 1955. ?Translation.? William N.
Locke and Andrew D. Booth (eds.), Machine
Translation of Languages; Fourteen Essays,
15?23. Cambridge, MA: MIT Press. Reprint of
a memorandum written in 1949.
61
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 170?176,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Edinburgh?s Syntax-Based Machine Translation Systems
Maria Nadejde, Philip Williams, and Philipp Koehn
School of Informatics, University of Edinburgh, Scotland, United Kingdom
maria.nadejde@gmail.com, P.J.Williams-2@sms.ed.ac.uk, pkoehn@inf.ed.ac.uk
Abstract
We present the syntax-based string-to-
tree statistical machine translation systems
built for the WMT 2013 shared transla-
tion task. Systems were developed for
four language pairs. We report on adapting
parameters, targeted reduction of the tun-
ing set, and post-evaluation experiments
on rule binarization and preventing drop-
ping of verbs.
1 Overview
Syntax-based machine translation models hold
the promise to overcome some of the fundamen-
tal problems of the currently dominating phrase-
based approach, most importantly handling re-
ordering for syntactically divergent language pairs
and grammatical coherence of the output.
We are especially interested in string-to-tree
models that focus syntactic annotation on the tar-
get side, especially for morphologically rich target
languages (Williams and Koehn, 2011).
We have trained syntax-based systems for the
language pairs
? English-German,
? German-English,
? Czech-English, and
? Russian-English.
We have also tried building systems for French-
English and Spanish-English but the data size
proved to be problematic given the time con-
straints. We give a brief description of the syntax-
based model and its implementation within the
Moses system. Some of the available features are
described as well as some of the pre-processing
steps. Several experiments are described and final
results are presented for each language pair.
2 System Description
The syntax-based system used in all experiments
is the Moses string-to-tree toolkit implementing
GHKM rule extraction and Scope-3 parsing previ-
ously described in by Williams and Koehn (2012)
2.1 Grammar
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side. In this paper, we write
these rules in the form
LHS ? RHSs | RHSt
where LHS is a target-side non-terminal label and
RHSs and RHSt are strings of terminals and non-
terminals for the source and target sides, respec-
tively. We use subscripted indices to indicate the
correspondences between source and target non-
terminals.
For example, a translation rule to translate the
German Haus into the English house is
NN ? Haus | house
If our grammar also contains the translation rule
S ? das ist ein X1 | this is a NN1
then we can apply the two rules to an input das ist
ein Haus to produce the output this is a house.
2.2 Rule Extraction
The GHKM rule extractor (Galley et al, 2004,
2006) learns translation rules from a word-aligned
parallel corpora for which the target sentences are
syntactically annotated. Given a string-tree pair,
the set of minimally-sized translation rules is ex-
tracted that can explain the example and is consis-
tent with the alignment. The resulting rules can be
composed in a non-overlapping fashion in order to
cover the string-tree pair.
Two or more minimal rules that are in a parent-
child relationship can be composed together to ob-
tain larger rules with more syntactic context. To
avoid generating an exponential number of com-
posed rules, several limitation have to be imposed.
One such limitation is on the size of the com-
posed rules, which is defined as the number of
non-part-of-speech, non-leaf constituent labels in
the target tree (DeNeefe et al, 2007). The corre-
sponding parameter in the Moses implementation
is MaxRuleSize and its default value is 3.
170
Another limitation is on the depth of the rules?
target subtree. The rule depth is computed as the
maximum distance from its root node to any of its
children, not counting pre-terminal nodes (param-
eter MaxRuleDepth, default 3).
The third limitation considered is the number of
nodes in the composed rule, not counting target
words (parameter MaxNodes, default 15).
These parameters are language-dependent and
should be set to values that best represent the char-
acteristics of the target trees on which the rule ex-
tractor is trained on. Therefore the style of the
treebanks used for training the syntactic parsers
will also influence these numbers. The default
values have been set based on experiments on
the English-German language pair (Williams and
Koehn, 2012). It is worth noting that the Ger-
man parse trees (Skut et al, 1997) tend to be
broader and shallower than those for English. In
Section 3 we present some experiments where we
choose different settings of these parameters for
the German-English language pair. We use those
settings for all language pairs where the target lan-
guage is English.
2.3 Tree Restructuring
The coverage of the extracted grammar depends
partly on the structure of the target trees. If the
target trees have flat constructions such as long
noun phrases with many sibling nodes, the rules
extracted will not generalize well to unseen data
since there will be many constraints given by the
types of different sibling nodes.
In order to improve the grammar coverage to
generalize over such cases, the target tree can be
restructured. One restructuring strategy is tree
binarization. Wang et al (2010) give an exten-
sive overview of different tree binarization strate-
gies applied for the Chinese-English language
pair. Moses currently supports left binarization
and right binarization.
By left binarization all the left-most children
of a parent node n except the right most child
are grouped under a new node. This node is in-
serted as the left child of n and receives the la-
bel n?. Left binarization is then applied recursively
on all newly inserted nodes until the leaves are
reached. Right binarization implies a similar pro-
cedure but in this case the right-most children of
the parent node are grouped together except the
left most child.
Another binarization strategy that is not cur-
rently integrated in Moses, but is worth investigat-
ing for different language pairs, is parallel head
binarization.
The result of parallel binarization of a parse
tree is a binarization forest. To generate a bina-
rization forest node, both right binarization and
left binarization are applied recursively to a parent
node with more than two children. Parallel head
binarization is a case of parallel binarization with
the additional constraint that the head constituent
is part of all the new nodes inserted by either left
or right binarization steps.
In Section 3 we give example of some initial ex-
periments carried out for the German-English lan-
guage pair.
2.4 Pruning The Grammar
Decoding for syntax-based model relies on a
bottom-up chart parsing algorithm. Therefore de-
coding efficiency is influenced by the following
combinatorial problem: given an input sentence
of length n and a context-free grammar rule with
s consecutive non-terminals, there are (n+1s
) ways
to choose subspans, or application contexts (Hop-
kins and Langmead, 2010), that the rule can ap-
plied to. The asymptotic running time of chart
parsing is linear in this number O(ns).
Hopkins and Langmead (2010) maintain cubic
decoding time by pruning the grammar to remove
rules for which the number of potential applica-
tion contexts is too large. Their key observation is
that a rule can have any number of non-terminals
and terminals as long as the number of consecutive
non-terminal pairs is bounded. Terminals act to
anchor the rule, restricting the number of potential
application contexts. An example is the rule X ?
WyY Zz for which there are at most O(n2) appli-
cation contexts, given that the terminals will have
a fixed position and will play the role of anchors
in the sentence for the non-terminal spans. The
number of consecutive non-terminal pairs plus the
number of non-terminals at the edge of a rule is
referred to as the scope of the rule. The scope of a
grammar is the maximum scope of any of its rules.
Moses implements scope-3 pruning and therefore
the resulting grammar can be parsed in cubic time.
2.5 Feature Functions
Our feature functions are unchanged from last
year. They include the n-gram language model
probability of the derivation?s target yield, its word
171
count, and various scores for the synchronous
derivation. Our grammar rules are scored accord-
ing to the following functions:
? p(RHSs|RHSt,LHS), the noisy-channel
translation probability.
? p(LHS,RHSt|RHSs), the direct translation
probability.
? plex (RHSt|RHSs) and plex (RHSs|RHSt),
the direct and indirect lexical weights (Koehn
et al, 2003).
? ppcfg(FRAGt), the monolingual PCFG prob-
ability of the tree fragment from which
the rule was extracted. This is defined
as ?ni=1 p(ri), where r1 . . . rn are the con-
stituent CFG rules of the fragment. The
PCFG parameters are estimated from the
parse of the target-side training data. All lex-
ical CFG rules are given the probability 1.
This is similar to the pcfg feature proposed
by Marcu et al (2006) and is intended to en-
courage the production of syntactically well-
formed derivations.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
3 Experiments
This section describes details for the syntax-based
systems submitted by the University of Edinburgh.
Additional post-evaluation experiments were car-
ried out for the German-English language pair.
3.1 Data
We made use of all available data for each lan-
guage pair except for the Russian-English where
the Commoncrawl corpus was not used. Table 1
shows the size of the parallel corpus used for each
language pair. The English side of the paral-
lel corpus was parsed using the Berkeley parser
(Petrov et al, 2006) and the German side of the
parallel corpus was parsed using the BitPar parser
(Schmid, 2004). For German-English, German
compounds were split using the script provided
with Moses. The parallel corpus was word-aligned
using MGIZA++ (Gao and Vogel, 2008).
All available monolingual data was used for
training the language models for each language
Lang. pair Sentences Grammar Size
en-de 4,411,792 31,568,480
de-en 4,434,060 55,310,162
cs-en 14,425,564 209,841,388
ru-en 1,140,359 7,946,502
Table 1: Corpus statistics for parallel data.
pair. 5-gram language models were trained us-
ing SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned
on the newstest2011 development set.
The feature weights for each system were tuned
on development sets using the Moses implementa-
tion of minimum error rate training (Och, 2003).
The size of the tuning data varied for different lan-
guages depending on the amount of available data.
In the case of the the German-English pair a filter-
ing criteria based on sentence level BLEU score
was applied which is briefly described in Section
3.5. Table 2 shows the size of the tuning set for
each language pair.
Lang. pair Sentences
en-de 7,065
de-en 2,400
cs-en 10,068
ru-en 1,501
Table 2: Corpus statistics for tuning data.
3.2 Pre-processing
Some attention was given to pre-processing of the
English side of the corpus prior to parsing. This
was done to avoid propagating parser errors to the
rule-extraction step. These particular errors arise
from a mismatch in punctuation and tokenization
between the corpus used to train the parser, the
PennTree bank, and the corpus which is being
parsed and passed on to the rule extractor. There-
fore we changed the quotation marks, which ap-
pear quite often in the parallel corpora, to opening
and closing quotation marks. We also added some
PennTree bank style tokenization rules1. These
rules split contractions such as I?ll, It?s, Don?t,
Gonna, Commissioner?s in order to correctly sep-
arate the verbs, negation and possessives that are
1The PennTree bank tokenization rules considered were
taken from http://www.cis.upenn.edu/?treebank/
tokenizer.sed. Further examples of contractions were
added.
172
Grammar Size BLEU
Parameters Full Filtered 2009-40 2010-40 2011-40 Average
Depth=3, Nodes=15, Size=3 2,572,222 751,355 18.57 20.43 18.51 19.17
Depth=4, Nodes=20, Size=4 3,188,970 901,710 18.88 20.38 18.63 19.30
Depth=5, Nodes=20, Size=5 3,668,205 980,057 19.04 20.47 18.75 19.42
Depth=5, Nodes=30, Size=5 3,776,961 980,061 18.90 20.59 18.77 19.42
Depth=5, Nodes=30, Size=6 4,340,716 1,006,174 18.98 20.52 18.80 19.43
Table 3: Cased BLEU scores for various rule extraction parameter settings for German-English language
pair. The parameters considered are MaxRuleDepth, MaxRuleSize, MaxNodes. Grammar sizes are given
for the full extracted grammar and after filtering for the newstest2008 dev set.
newstest2012 newstest2013
System Sentences BLEU Glue Rule Tree Depth BLEU Glue Rule Tree Depth
Baseline 5,771 23.21 5.42 4.03 26.27 4.23 3.80
Big tuning set 10,068 23.52 3.41 4.34 26.33 2.49 4.03
Filtered tuning set 2,400 23.54 3.21 4.37 26.30 2.37 4.05
Table 4: Cased BLEU scores for German-English systems tuned on different data. Scores are emphasized
for the system submitted to the shared translation task.
parsed as separate constituents.
For German?English, we carried out the usual
compound splitting (Koehn and Knight, 2003), but
not pre-reordering (Collins et al, 2005).
3.3 Rule Extraction
Some preliminary experiments were carried out
for the German-English language pair to deter-
mine the parameters for the rule extraction step:
MaxRuleDepth, MaxRuleSize, MaxNodes. Table 3
shows the BLEU score on different test sets for
various parameter settings. For efficiency rea-
sons less training data was used, therefore the
grammar sizes, measured as the total number of
extracted rules, are smaller than the final sys-
tems (Table 1). The parameters on the third line
Depth=5, Nodes=20, Size=4 were chosen as the
average BLEU score did not increase although the
size of the extracted grammar kept growing. Com-
paring the rate of growth of the full grammar and
the grammar after filtering for the dev set (the
columns headed ?Full? and ?Filtered?) suggests
that beyond this point not many more usable rules
are extracted, even while the total number of rules
stills increases.
3.4 Decoder Settings
We used the following non-default decoder param-
eters:
max-chart-span=25: This limits sub deriva-
tions to a maximum span of 25 source words. Glue
rules are used to combine sub derivations allowing
the full sentence to be covered.
ttable-limit=200: Moses prunes the translation
grammar on loading, removing low scoring rules.
This option increases the number of translation
rules that are retained for any given source side
RHSs.
cube-pruning-pop-limit=1000: Number of hy-
potheses created for each chart span.
3.5 Tuning sets
One major limitation for the syntax-based systems
is that decoding becomes inefficient for long sen-
tences. Therefore using large tuning sets will slow
down considerably the development cycle. We
carried out some preliminary experiments to de-
termine how the size of the tuning set affects the
quality and speed of the system.
Three tuning sets were considered. The tun-
ing set that was used for training the baseline sys-
tem was built using the data from newstest2008-
2010 filtering out sentences longer than 30 words.
The second tuning set was built using all data
from newstest2008-2011. The final tuning set
was also built using the concatenation of the sets
newstest2008-2011. All sentences in this set were
decoded with a baseline system and the output was
scored according to sentence-BLEU scores. We se-
173
lected examples with high sentence-BLEU score in
a way that penalizes excessively short examples2.
Results of these experiments are shown in Table 4.
Results show that there is some gain in BLEU
score when providing longer sentences during tun-
ing. Further experiments should consider tuning
the baseline with the newstest2008-2011 data, to
eliminate variance caused by having different data
sources. Although the size of the third tuning set is
much smaller than that of the other tuning sets, the
BLEU score remains the same as when using the
largest tuning set. The glue rule number, which
shows how many times the glue rule was applied,
is lowest when tuning with the third data set. The
tree depth number, which shows the depth of the
resulting target parse tree, is higher for the third
tuning set as compared to the baseline and similar
to that resulted from using the largest tuning set.
These numbers are all indicators of better utilisa-
tion of the syntactic structure.
Regarding efficiency, the baseline tuning set and
the filtered tuning set took about a third of the time
needed to decode the larger tuning set.
Therefore we could draw some initial conclu-
sions that providing longer sentences is useful,
but sentences for which some baseline system per-
forms very poorly in terms of BLEU score can be
eliminated from the tuning set.
3.6 Results
Table 5 summarizes the results for the systems
submitted to the shared task. The BLEU scores for
the phrase-based system submitted by the Univer-
sity of Edinburgh are also shown for comparison.
The syntax-based system had BLEU scores similar
to those of the phrase-based system for German-
English and English-German language pairs. For
the Czech-English and Russian-English language
pairs the syntax-based system was 2 BLEU points
behind the phrase-based system.
However, in the manual evaluation, the
German?English and English?German syntax
based systems were ranked higher than the phrase-
based systems. For Czech?English, the syntax
systems also came much closer than the BLEU
score would have indicated.
The Russian-English system performed worse
because we used much less of the available data
for training (leaving out Commoncrawl) and there-
2Ongoing work by Eva Hasler. Filtered data set was pro-
vided in order to speed up experiment cycles.
phrase-based syntax-based
BLEU manual BLEU manual
en-de 20.1 0.571 19.4 0.614
de-en 26.6 0.586 26.3 0.608
cs-en 26.2 0.562 24.4 0.542
ru-en 24.3 0.507 22.5 0.416
Table 5: Cased BLEU scores and manual evalua-
tion scores (?expected wins?) on the newstest2013
evaluation set for the phrase-based and syntax-
based systems submitted by the University of Ed-
inburgh.
fore the extracted grammar is less reliable. An-
other reason was the mismatch in data format-
ting for the Russian-English parallel corpus. All
the training data was lowercased which resulted in
more parsing errors.
3.7 Post-Submission Experiments
Table 6 shows results for some preliminary ex-
periments carried out for the German-English lan-
guage pair that were not included in the final sub-
mission. The baseline system is trained on all
available parallel data and tuned on data from
newstest2008-2010 filtered for sentences up to 30
words.
Tree restructuring ? In one experiment the
parse trees were restructured before training by
left binarization. Tree restructuring is need to im-
prove generalization power of rules extracted from
flat structures such as base noun phrases with sev-
eral children. The second raw in Table 6 shows
that the BLEU score did not improve and more
glue rules were applied when using left binariza-
tion. One reason for this result is that the rule ex-
traction parameters MaxRuleDepth, MaxRuleSize,
MaxNodes had the same values as in the baseline.
Increasing this parameters should improve the ex-
tracted grammar since binarizing the trees will in-
crease these three dimensions.
Verb dropping ? A serious problem of
German?English machine translation is the ten-
dency to drop verbs, which shatters sentence struc-
ture. One cause of this problem is the failure of the
IBM Models to properly align the German verb to
its English equivalent, since it is often dislocated
with respect to English word order. Further prob-
lems appear when the main verb is not reordered in
the target sentence, which can result in lower lan-
174
newstest2012 newstest2013
System Grammar size BLEU glue rule tree depth BLEU glue rule tree depth
Baseline 55,310,162 23.21 5.42 4.03 26.27 4.23 3.80
Left binarized 57,151,032 23.17 7.79 4.09 26.13 6.57 3.85
Realigned vb 53,894,112 23.26 4.88 4.19 26.26 3.73 3.96
Table 6: Cased BLEU scores for various German-English systems.
System Vb droprules
Vb Count
nt2012
Vb Count
nt2013
Baseline 1,038,597 9,216 8,418
Realigned
verbs 391,231 9,471 8,614
Reference
translation - 9,992 9,207
Table 7: Statistics about verb dropping.
guage model scores and BLEU scores. However
the syntax models handle the reordering of verbs
better than phrase-based models.
In an experiment we investigated how the num-
ber of verbs dropped by the translation rules can
be reduced. In order to reduce the number of
verb dropping rules we looked at unaligned verbs
and realigned them before rule extraction. An un-
aligned verb in the source sentence was aligned
to the verb in the target sentence for which IBM
model 1 predicted the highest translation probabil-
ity. The third row in Table 6 shows the results of
this experiment. While there is no change in BLEU
score the number of glue rules applied is lower.
Further analysis shows in Table 7 that the number
of verb dropping rules in the grammar is almost
three times lower and that there are more trans-
lated verbs in the output when realigning verbs.
4 Conclusion
We describe in detail the syntax-based machine
translation systems that we developed for four Eu-
ropean language pairs. We achieved competitive
results, especially for the language pairs involving
German.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 287658 (EU BRIDGE) and
grant agreement 288487 (MosesCore).
References
Chen, S. F. and Goodman, J. (1998). An empiri-
cal study of smoothing techniques for language
modeling. Technical report, Harvard University.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
DeNeefe, S., Knight, K., Wang, W., and Marcu,
D. (2007). What can syntax-based MT learn
from phrase-based MT? In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-
CoNLL 2007). June 28-30, 2007. Prague, Czech
Republic.
Galley, M., Graehl, J., Knight, K., Marcu, D., De-
Neefe, S., Wang, W., and Thayer, I. (2006).
Scalable inference and training of context-rich
syntactic translation models. In ACL-44: Pro-
ceedings of the 21st International Conference
on Computational Linguistics and the 44th an-
nual meeting of the Association for Computa-
tional Linguistics, pages 961?968, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Galley, M., Hopkins, M., Knight, K., and Marcu,
D. (2004). What?s in a translation rule? In HLT-
NAACL ?04.
Gao, Q. and Vogel, S. (2008). Parallel implemen-
tations of word alignment tool. In Software En-
gineering, Testing, and Quality Assurance for
Natural Language Processing, SETQA-NLP
?08, pages 49?57, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hopkins, M. and Langmead, G. (2010). SCFG
decoding without binarization. In Proceedings
of the 2010 Conference on Empirical Methods
175
in Natural Language Processing, pages 646?
655, Cambridge, MA. Association for Compu-
tational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Koehn, P., Och, F. J., and Marcu, D. (2003). Sta-
tistical phrase-based translation. In NAACL
?03: Proceedings of the 2003 Conference of
the North American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology, pages 48?54, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Marcu, D., Wang, W., Echihabi, A., and Knight,
K. (2006). SPMT: statistical machine transla-
tion with syntactified target language phrases.
In EMNLP ?06: Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 44?52, Morristown,
NJ, USA. Association for Computational Lin-
guistics.
Och, F. J. (2003). Minimum error rate training
in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL
?03, pages 160?167, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Petrov, S., Barrett, L., Thibaux, R., and Klein, D.
(2006). Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of
the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting
of the Association for Computational Linguis-
tics, ACL-44, pages 433?440, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Schmid, H. (2004). Efficient parsing of highly am-
biguous context-free grammars with bit vectors.
In Proceedings of the 20th international con-
ference on Computational Linguistics, COL-
ING ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Skut, W., Krenn, B., Brants, T., and Uszkoreit, H.
(1997). An annotation scheme for free word or-
der languages. In Proceedings of the Fifth Con-
ference on Applied Natural Language Process-
ing (ANLP-97).
Stolcke, A. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. Spo-
ken Language Processing, Denver, Colorado,
September 2002.
Wang, W., May, J., Knight, K., and Marcu, D.
(2010). Re-structuring, re-labeling, and re-
aligning for syntax-based machine translation.
Comput. Linguist., 36(2):247?277.
Williams, P. and Koehn, P. (2011). Agreement
constraints for statistical machine translation
into german. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages
217?226, Edinburgh, Scotland. Association for
Computational Linguistics.
Williams, P. and Koehn, P. (2012). Ghkm rule
extraction and scope-3 parsing in moses. In
Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, pages 388?394,
Montre?al, Canada. Association for Computa-
tional Linguistics.
176
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105?113,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
EU-BRIDGE MT: Combined Machine Translation
?
Markus Freitag,
?
Stephan Peitz,
?
Joern Wuebker,
?
Hermann Ney,
?
Matthias Huck,
?
Rico Sennrich,
?
Nadir Durrani,
?
Maria Nadejde,
?
Philip Williams,
?
Philipp Koehn,
?
Teresa Herrmann,
?
Eunah Cho,
?
Alex Waibel
?
RWTH Aachen University, Aachen, Germany
?
University of Edinburgh, Edinburgh, Scotland
?
Karlsruhe Institute of Technology, Karlsruhe, Germany
?
{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
?
{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
?
v1rsennr@staffmail.ed.ac.uk
?
maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
?
{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
Abstract
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German?English
and English?German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
1 Introduction
EU-BRIDGE
1
is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
1
http://www.eu-bridge.eu
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
2 RWTH Aachen University
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH?s
publicly available translation toolkit Jane (Vilar
105
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
3 University of Edinburgh
UEDIN contributed phrase-based and syntax-
based systems to both the German?English and
the English?German joint submission.
3.1 Phrase-based Systems
UEDIN?s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA
++
align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English?German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German?English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
3.2 Syntax-based Systems
UEDIN?s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses? hierarchical decoder (Hoang et al.,
2009).
UEDIN?s German?English syntax-based setup
is a string-to-tree system with compound splitting
106
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English?German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English?German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
4 Karlsruhe Institute of Technology
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German?English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German?English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German?English is
generated using the GIZA
++
toolkit (Och and Ney,
2003). For English?German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English?German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
107
In total, the English?German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German?English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
5 System Combination
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH?s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as ?primary? hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
6 Results
In this section, we present our experimental results
on the two translation tasks, German?English
and English?German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
6.1 German?English
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German?English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
6.2 English?German
The results of all English?German system setups
are given in Table 3. For the English?German
translation task, only UEDIN and KIT are con-
108
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
Table 1: Results for the German?English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p < 0.05.
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
Table 2: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
Table 3: Results for the English?German translation task. The system combination is tuned on news-
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p< 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p < 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English?German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English?German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
109
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
Table 4: Cross BLEU scores for the German?English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 1: Sentence distribution for the
German?English newstest2013 test set compar-
ing system combination output against the best
individual system.
As for the German?English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
7 Conclusion
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  20  40  60  80  100
amo
unt 
sent
ence
s
sBLEU
bettersameworse
Figure 2: Sentence distribution for the
English?German newstest2013 test set compar-
ing system combination output against the best
individual system.
based, hierarchical phrase-based, and syntax-
based. For English?German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.
2
Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German?English and
English?German without adding any new data.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658.
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2
http://matrix.statmt.org/
110
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65?72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409?412, Montr?eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh?s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh?s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53?61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847?855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
/ North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273?280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268?275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292?301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
111
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152?159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452?463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868?876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177?180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. / North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169?176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222?229, San
Francisco, CA, USA, December.
Arne Mauser, Sa?sa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210?217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL?99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
112
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433?440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk?ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388?394,
Montr?eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh?s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ?12: The 24th
Int. Conf. on Computational Linguistics, pages 483?
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, WA, USA, Oc-
tober.
113
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207?214,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Syntax-Based Systems at WMT 2014
Philip Williams
1
, Rico Sennrich
1
, Maria Nadejde
1
,
Matthias Huck
1
, Eva Hasler
1
, Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Speech and Language Processing, The Johns Hopkins University
Abstract
This paper describes the string-to-tree sys-
tems built at the University of Edin-
burgh for the WMT 2014 shared trans-
lation task. We developed systems for
English-German, Czech-English, French-
English, German-English, Hindi-English,
and Russian-English. This year we
improved our English-German system
through target-side compound splitting,
morphosyntactic constraints, and refine-
ments to parse tree annotation; we ad-
dressed the out-of-vocabulary problem us-
ing transliteration for Hindi and Rus-
sian and using morphological reduction
for Russian; we improved our German-
English system through tree binarization;
and we reduced system development time
by filtering the tuning sets.
1 Introduction
For this year?s WMT shared translation task we
built syntax-based systems for six language pairs:
? English-German ? German-English
? Czech-English ? Hindi-English
? French-English ? Russian-English
As last year (Nadejde et al., 2013), our systems are
based on the string-to-tree pipeline implemented
in the Moses toolkit (Koehn et al., 2007).
We paid particular attention to the production of
grammatical German, trying various parsers and
incorporating target-side compound splitting and
morphosyntactic constraints; for Hindi and Rus-
sian, we employed the new Moses transliteration
model to handle out-of-vocabulary words; and for
German to English, we experimented with tree bi-
narization, obtaining good results from right bina-
rization.
We also present our first syntax-based results
for French-English, the scale of which defeated us
last year. This year we were able to train a sys-
tem using all available training data, a task that
was made considerably easier through principled
filtering of the tuning set. Although our system
was not ready in time for human evaluation, we
present BLEU scores in this paper.
In addition to the five single-system submis-
sions described here, we also contributed our
English-German and German-English systems for
use in the collaborative EU-BRIDGE system com-
bination effort (Freitag et al., 2014).
This paper is organised as follows. In Sec-
tion 2 we describe the core setup that is com-
mon to all systems. In subsequent sections we de-
scribe language-pair specific variations and exten-
sions. For each language pair, we present results
for both the development test set (newstest2013
in most cases) and for the filtered test set (new-
stest2014) that was provided after the system sub-
mission deadline. We refer to these as ?devtest?
and ?test?, respectively.
2 System Overview
2.1 Pre-processing
The training data was normalized using the WMT
normalize-punctuation.perl script then
tokenized and truecased. Where the target lan-
guage was English, we used the Moses tokenizer?s
-penn option, which uses a tokenization scheme
that more closely matches that of the parser. For
the English-German system we used the default
Moses tokenization scheme, which is similar to
that of the German parsers.
For the systems that translate into English, we
used the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007) to parse the target-side of
the training corpus. As we will describe in Sec-
tion 3, we tried a variety of parsers for German.
We did not perform any corpus filtering other
than the standard Moses method, which removes
207
sentence pairs with dubious length ratios and sen-
tence pairs where parsing fails for the target-side
sentence.
2.2 Translation Model
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side.
The grammar was extracted from the word-
aligned parallel data using the Moses implemen-
tation (Williams and Koehn, 2012) of the GHKM
algorithm (Galley et al., 2004; Galley et al., 2006).
For word alignment we used MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
Minimal GHKM rules were composed into
larger rules subject to parameterized restrictions
on size defined in terms of the resulting target tree
fragment. A good choice of parameter settings
depends on the annotation style of the target-side
parse trees. We used the settings shown in Table 1,
which were chosen empirically during the devel-
opment of last years? systems:
Parameter Value
Rule depth 5
Node count 20
Rule size 5
Table 1: Parameter settings for rule composition.
Further to the restrictions on rule composition,
fully non-lexical unary rules were eliminated us-
ing the method described in Chung et al. (2011)
and rules with scope greater than 3 (Hopkins and
Langmead, 2010) were pruned from the trans-
lation grammar. Scope pruning makes parsing
tractable without the need for grammar binariza-
tion.
2.3 Language Model
We used all available monolingual data to train
5-gram language models. Language models
for each monolingual corpus were trained using
the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned to
minimize perplexity on the development set.
2.4 Feature Functions
Our feature functions are unchanged from the pre-
vious two years. They include the n-gram lan-
guage model probability of the derivation?s target
yield, its word count, and various scores for the
synchronous derivation.
Each grammar rule has a number of pre-
computed scores. For a grammar rule r of the form
C ? ??, ?,??
where C is a target-side non-terminal label, ? is a
string of source terminals and non-terminals, ? is
a string of target terminals and non-terminals, and
? is a one-to-one correspondence between source
and target non-terminals, we score the rule accord-
ing to the following functions:
? p (C, ? | ?,?) and p (? | C, ?,?), the direct
and indirect translation probabilities.
? p
lex
(? | ?) and p
lex
(? | ?), the direct and
indirect lexical weights (Koehn et al., 2003).
? p
pcfg
(pi), the monolingual PCFG probability
of the tree fragment pi from which the rule
was extracted.
? exp(?1/count(r)), a rule rareness penalty.
? exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
2.5 Tuning
The feature weights were tuned using the Moses
implementation of MERT (Och, 2003) for all sys-
tems except English-to-German, for which we
used k-best MIRA (Cherry and Foster, 2012) due
to the larger number of features.
We used tuning sentences drawn from all of
the previous years? test sets (except newstest2013,
which was used as the development test set). In
order to speed up the tuning process, we used sub-
sets of the full tuning sets with sentence pairs up
to length 30 (Max-30) and further applied a fil-
tering technique to reduce the tuning set size to
2,000 sentence pairs for the language pairs involv-
ing German, French and Czech
1
. We also experi-
mented with random subsets of size 2,000.
For the filtering technique, we make the as-
sumption that finding suitable weights for all the
feature functions requires the optimizer to see a
range of feature values and to see hypotheses that
can partially match the reference translations in
order to rank the hypotheses. For example, if a
1
For Russian and Hindi, the development sets are smaller
and no filtering was applied.
208
tuning example contains many out-of-vocabulary
words or is difficult to translate for other reasons,
this will result in low quality translation hypothe-
ses and provide the system with little evidence for
which features are useful to produce good transla-
tions. Therefore, we select high quality examples
using a smooth version of sentence-BLEU com-
puted on the 1-best output of a single decoder run
on the development set. Standard sentence-BLEU
tends to select short examples because they are
more likely to have perfect n-gram matches with
the reference translation. Very short sentence pairs
are less informative for tuning but also tend to have
more extreme source-target length ratios which
can affect the weight of the word penalty. Thus,
we penalize short examples by padding the de-
coder output with a fixed number of non-matching
tokens
2
to the left and right before computing
sentence-BLEU. This has the effect of reducing
the precision of short sentences against the refer-
ence translation while affecting longer sentences
proportionally less. Experiments on phrase-based
systems have shown that the resulting tuning sets
are of comparable diversity as randomly selected
sets in terms of their feature vectors and maintain
BLEU scores in comparison with tuning on the en-
tire development set.
Table 2 shows the size of the full tuning sets
and the size of the subsets with up to length 30,
Table 3 shows the results of tuning with different
sets. Reducing the tuning sets to Max-30 results
in a speed-up in tuning time but affects the per-
formance on some of the devtest/test sets (mostly
for Czech-English). However, tuning on the full
set took more than 18 days using 12 cores for
German-English which is not feasible when try-
ing out several model variations. Further filter-
ing these subsets to a size of 2,000 sentence pairs
as described above maintains the BLEU scores in
most cases and even improves the scores in some
cases. This indicates that the quality of the se-
lected examples is more important than the total
number of tuning examples. However, the exper-
iments with random subsets from Max-30 show
that random selection also yields results which im-
prove over the results with Max-30 in most cases,
though are not always as good as with the filtered
sets.
3
The filtered tuning sets yield reasonable per-
2
These can be arbitrary tokens that do not match any ref-
erence token.
3
For random subsets from the full tuning set the perfor-
mance was similar but resulted in standard deviations of up
formance compared to the full tuning sets except
for the German-English devtest set where perfor-
mance drops by 0.5 BLEU
4
.
Tuning set Cs-En En-De De-En
Full 13,055 13,071 13,071
Max-30 10,392 9,151 10,610
Table 2: Size of full tuning sets and with sentence
length up to 30.
devtest
Tuning set Cs-En En-De De-En
Full 25.1 19.9 26.7
Max-30 24.7 19.8 26.2
Filtered 24.9 19.8 26.2
Random 24.8 19.7 26.4
test
Tuning set Cs-En En-De De-En
Full 27.5 19.2 26.9
Max-30 27.2 19.2 27.0
Filtered 27.5 19.1 27.2
Random 27.3 19.4 27.0
Table 3: BLEU results on devtest and test sets with
different tuning sets: Full, Max-30, filtered subsets
of Max-30 and average of three random subsets of
Max-30 (size of filtered/random subsets: 2,000).
3 English to German
We use the projective output of the dependency
parser ParZu (Sennrich et al., 2013) for the syn-
tactic annotation of our primary submission. Con-
trastive systems were built with other parsers: Bit-
Par (Schmid, 2004), the German Stanford Parser
(Rafferty and Manning, 2008), and the German
Berkeley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
The set of syntactic labels provided by ParZu
has been refined to reduce overgeneralization phe-
nomena. Specifically, we disambiguate the labels
ROOT (used for the root of a sentence, but also
commas, punctuation marks, and sentence frag-
ments), KON and CJ (coordinations of different
constituents), and GMOD (pre- or postmodifying
genitive modifier).
to 0.36 across three random sets.
4
Note however that due to the long tuning times, we are
reporting single tuning runs.
209
NN
SEGMENT
gericht
COMP
JUNC
@s@
SEGMENT
berufung
COMP
JUNC
@es@
SEGMENT
Bund
Figure 1: Syntactic representation of split com-
pound Bundesberufungsgericht (Engl: federal ap-
peals court).
We discriminatively learn non-terminal labels
for unknown words using sparse features, rather
than estimating a probability distribution of non-
terminal labels from singleton statistics in the
training corpus.
We perform target-side compound splitting, us-
ing a hybrid method described by Fritzinger and
Fraser (2010) that combines a finite-state mor-
phology and corpus statistics. As finite-state mor-
phology analyzer, we use Zmorge (Sennrich and
Kunz, 2014). An original contribution of our
experiments is a syntactic representation of split
compounds which eliminates typical problems
with target-side compound splitting, namely er-
roneous reorderings and compound merging. We
represent split compounds as a syntactic tree with
the last segment as head, preceded by a modifier.
A modifier consists of an optional modifier, a seg-
ment and a (possibly empty) joining element. An
example is shown in Figure 1. This hierarchical
representation ensures that compounds can be eas-
ily merged in post-processing (by removing the
spaces and special characters around joining ele-
ments), and that no segments are placed outside of
a compound in the translation.
We use unification-based constraints to model
morphological agreement within German noun
phrases, and between subjects and verbs (Williams
and Koehn, 2011). Additionally, we add con-
straints that operate on the internal tree structure of
the translation hypotheses, to enforce several syn-
tactic constraints that were frequently violated in
the baseline system:
? correct subcategorization of auxiliary/modal
verbs in regards to the inflection of the full
verb.
? passive clauses are not allowed to have ac-
cusative objects.
system
BLEU
devtest test
Stanford Parser 19.0 18.3
Berkeley Parser 19.3 18.6
BitPar 19.5 18.6
ParZu 19.6 19.1
+ modified label set 19.8 19.1
+ discriminative UNK weights 19.9 19.2
+ German compound splitting 20.0 19.8
+ grammatical constraints 20.2 20.1
Table 4: English to German translation results
on devtest (newstest2013) and test (newstest2014)
sets.
? relative clauses must contain a relative (or in-
terrogative) pronoun in their first constituent.
Table 4 shows BLEU scores with systems
trained with different parsers, and for our exten-
sions of the baseline system.
4 Czech to English
For Czech to English we used the core setup de-
scribed in Section 2 without modification. Table 5
shows the BLEU scores.
BLEU
system devtest test
baseline 24.8 27.0
Table 5: Czech to English results on the devtest
(newstest2013) and test (newstest2014) sets.
5 French to English
For French to English, alignment of the parallel
corpus was performed using fast_align (Dyer et
al., 2013) instead of MGIZA++ due to the large
volume of parallel data.
Table 6 shows BLEU scores for the system and
Table 7 shows the resulting grammar sizes after
filtering for the evaluation sets.
BLEU
system devtest test
baseline 29.4 32.3
Table 6: French to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
210
system devtest test
baseline 86,341,766 88,657,327
Table 7: Grammar sizes of the French to En-
glish system after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
6 German to English
German compounds were split using the script
provided with Moses.
For training the primary system, the target parse
trees were restructured before rule extraction by
right binarization. Since binarization strategies
increase the tree depth and number of nodes by
adding virtual non-terminals, we increased the ex-
traction parameters to: Rule Depth = 7, Node
Count = 100, Rule Size = 7. A thorough in-
vestigation of binarization methods for restructur-
ing Penn Treebank style trees was carried out by
Wang et al. (2007).
Table 8 shows BLEU scores for the baseline
system and two systems employing different bi-
narization strategies. Table 9 shows the result-
ing grammar sizes after filtering for the evaluation
sets. Results on the development set showed no
improvement when left binarization was used for
restructuring the trees, although the grammar size
increased significantly.
BLEU
system devtest test
baseline 26.2 27.2
+ right binarization (primary) 26.8 28.2
+ left binarization 26.3 -
Table 8: German to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
system devtest test
baseline 11,462,976 13,811,304
+ right binarization 24,851,982 29,133,910
+ left binarization 21,387,976 -
Table 9: Grammar sizes of the German to En-
glish systems after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
7 Hindi to English
English-Hindi has the least parallel training data
of this year?s language pairs. Out-of-vocabulary
(OOV) input words are therefore a comparatively
large source of translation error: in the devtest set
(newsdev2014) and filtered test set (newstest2014)
the average OOV rates are 1.08 and 1.16 unknown
words per sentence, respectively.
Assuming a significant fraction of OOV words
to be named entities and thus amenable to translit-
eration, we applied the post-processing translitera-
tion method described in Durrani et al. (2014) and
implemented in Moses. In brief, this is an unsuper-
vised method that i) uses EM to induce a corpus of
transliteration examples from the parallel training
data; ii) learns a monotone character-level phrase-
based SMT model from the transliteration corpus;
and iii) substitutes transliterations for OOVs in the
system output by using the monolingual language
model and other features to select between translit-
eration candidates.
5
Table 10 shows BLEU scores with and without
transliteration on the devtest and filtered test sets.
Due to a bug in the submitted system, the language
model trained on the HindEnCorp corpus was used
for transliteration candidate selection rather than
the full interpolated language model. This was
fixed subsequent to submission.
BLEU
system devtest test
baseline 12.9 14.7
+ transliteration (submission) 13.3 15.1
+ transliteration (fixed) 13.6 15.5
Table 10: Hindi to English results with and with-
out transliteration on the devtest (newsdev2014)
and test (newstest2014) sets.
Transliteration increased 1-gram precision from
48.1% to 49.4% for devtest and from 49.1% to
50.6% for test. Of the 2,913 OOV words in test,
938 (32.2%) of transliterations exactly match the
reference. Manual inspection reveals that there are
also many near matches. For instance, translitera-
tion produces Bernat Jackie where the reference is
Jacqui Barnat.
8 Russian to English
Compared to Hindi-English, the Russian-English
language pair has over six times as much parallel
data. Nonetheless, OOVs remain a problem: the
average OOV rates are approximately half those
5
This is the variant referred to as Method 2 in Dur-
rani et al. (2014).
211
of Hindi-English, at 0.47 and 0.51 unknown words
per sentence for the devtest (newstest2013) and fil-
tered test (newstest2014) sets, respectively. We
address this in part using the same transliteration
method as for Hindi-English.
Data sparsity issues for this language pair are
exacerbated by the rich inflectional morphology of
Russian. Many Russian word forms express gram-
matical distinctions that are either absent from En-
glish translations (like grammatical gender) or are
expressed by different means (like grammatical
function being expressed through syntactic config-
uration rather than case). We adopt the widely-
used approach of simplifying morphologically-
complex source forms to remove distinctions that
we believe to be redundant. Our method is simi-
lar to that of Weller et al. (2013) except that ours
is much more conservative (in their experiments,
Weller et al. (2013) found morphological reduc-
tion to harm translation indicating that useful in-
formation was likely to have been discarded).
We used TreeTagger (Schmid, 1994) to obtain
a lemma-tag pair for each Russian word. The tag
specifies the word class and various morphosyn-
tactic feature values. For example, the adjective
??????????????? (?republican?) gets the lemma-
tag pair ??????????????? + Afpfsnf, where
the code A indicates the word class and the re-
maining codes indicate values for the type, degree,
gender, number, case, and definiteness features.
Like Weller et al. (2013), we selectively re-
placed surface forms with their lemmas and re-
duced tags, reducing tags through feature dele-
tion. We restricted morphological reduction to ad-
jectives and verbs, leaving all other word forms
unchanged. Table 11 shows the features that
were deleted. We focused on contextual inflec-
tion, making the assumption that inflectional dis-
tinctions required by agreement alone were the
least likely to be useful for translation (since the
same information was marked elsewhere in the
sentence) and also the most likely to be the source
of ?spurious? variation.
Table 12 shows the BLEU scores for Russian-
English with transliteration and morphological re-
duction. The effect of transliteration was smaller
than for Hindi-English, as might be expected from
the lower baseline OOV rate. 1-gram precision in-
creased from 57.1% to 57.6% for devtest and from
62.9% to 63.6% for test. Morphological reduction
decreased the initial OOV rates by 3.5% and 4.1%
Adjective Verb
Type 7 Type 7
Degree 3 VForm 3
Gender 7 Tense 3
Number 7 Person 3
Case 7 Number 3
Definiteness 7 Gender 7
Voice 3
Definiteness 7
Aspect 3
Case 3
Table 11: Feature values that are retained (3)
or deleted (7) during morphological reduction of
Russian.
BLEU
system devtest test
baseline 23.3 29.7
+ transliteration 23.7 30.3
+ morphological reduction 23.8 30.3
Table 12: Russian to English results on the devtest
(newstest2013) and test (newstest2014) sets.
on the devtest and filtered test sets. After both
morphological and transliteration the 1-gram pre-
cisions for devtest and test were 57.7% and 63.8%.
9 Conclusion
We have described Edinburgh?s syntax-based sys-
tems in the WMT 2014 shared translation task.
Building upon the already-strong string-to-tree
systems developed for previous years? shared
translation tasks, we have achieved substantial im-
provements over our baseline setup: we improved
translation into German through target-side com-
pound splitting, morphosyntactic constraints, and
refinements to parse tree annotation; we have ad-
dressed unknown words using transliteration (for
Hindi and Russian) and morphological reduction
(for Russian); and we have improved our German-
English system through tree binarization.
Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement n
o
287658 (EU-BRIDGE).
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1_148717.
212
References
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montr?al, Canada, June. Association for
Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 413?417, Portland, Oregon, USA, June.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April. To appear.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In In Proc. NAACL/HLT 2013,
pages 644?648.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ?10, pages 224?234, Uppsala,
Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a Translation Rule?
In HLT-NAACL ?04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961?968, Morristown, NJ, USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646?655, Cambridge,
MA, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Morristown, NJ, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s Syntax-Based Machine Transla-
tion Systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ?03, pages
160?167, Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ?08, pages
33?39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
213
Workshop on Parsing German at ACL ?08, pages 40?
46, Columbus, OH, USA, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601?609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Intl. Conf. Spoken
Language Processing, Denver, Colorado, September
2002.
Wei Wang, Kevin Knight, Daniel Marcu, and Marina
Rey. 2007. Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 746?754.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Rich?rd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 232?239, Sofia, Bul-
garia, August.
Philip Williams and Philipp Koehn. 2011. Agreement
Constraints for Statistical Machine Translation into
German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 217?226, Ed-
inburgh, Scotland, July.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388?394, Montr?al,
Canada, June.
214
