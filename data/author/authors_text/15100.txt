Proceedings of the ACL 2010 Student Research Workshop, pages 31?36,
Uppsala, Sweden, 13 July 2010.
c?2010 Association for Computational Linguistics
Unsupervised Search for The Optimal Segmentation for Statistical
Machine Translation
Cos?kun Mermer
1,3
and Ahmet Afs??n Ak?n
2,3
1
Bo?gazic?i University, Bebek, Istanbul, Turkey
2
Istanbul Technical University, Sar?yer, Istanbul, Turkey
3
T
?
UB
?
ITAK-UEKAE, Gebze, Kocaeli, Turkey
{coskun,ahmetaa}@uekae.tubitak.gov.tr
Abstract
We tackle the previously unaddressed
problem of unsupervised determination of
the optimal morphological segmentation
for statistical machine translation (SMT)
and propose a segmentation metric that
takes into account both sides of the SMT
training corpus. We formulate the objec-
tive function as the posterior probability of
the training corpus according to a genera-
tive segmentation-translation model. We
describe how the IBM Model-1 transla-
tion likelihood can be computed incremen-
tally between adjacent segmentation states
for efficient computation. Submerging the
proposed segmentation method in a SMT
task from morphologically-rich Turkish to
English does not exhibit the expected im-
provement in translation BLEU scores and
confirms the robustness of phrase-based
SMT to translation unit combinatorics.
A positive outcome of this work is the
described modification to the sequential
search algorithm of Morfessor (Creutz and
Lagus, 2007) that enables arbitrary-fold
parallelization of the computation, which
unexpectedly improves the translation per-
formance as measured by BLEU.
1 Introduction
In statistical machine translation (SMT), words
are normally considered as the building blocks of
translation models. However, especially for mor-
phologically complex languages such as Finnish,
Turkish, Czech, Arabic etc., it has been shown
that using sub-lexical units obtained after morpho-
logical preprocessing can improve the machine
translation performance over a word-based sys-
tem (Habash and Sadat, 2006; Oflazer and Durgar
El-Kahlout, 2007; Bisazza and Federico, 2009).
However, the effect of segmentation on transla-
tion performance is indirect and difficult to isolate
(Lopez and Resnik, 2006).
The challenge in designing a sub-lexical SMT
system is the decision of what segmentation to use.
Linguistic morphological analysis is intuitive, but
it is language-dependent and could be highly am-
biguous. Furthermore, it is not necessarily opti-
mal in that (i) manually engineered segmentation
schemes can outperform a straightforward linguis-
tic morphological segmentation, e.g., (Habash and
Sadat, 2006), and (ii) it may result in even worse
performance than a word-based system, e.g., (Dur-
gar El-Kahlout and Oflazer, 2006).
A SMT system designer has to decide what
segmentation is optimal for the translation task
at hand. Existing solutions to this problem are
predominantly heuristic, language-dependent, and
as such are not easily portable to other lan-
guages. Another point to consider is that the op-
timal degree of segmentation might decrease as
the amount of training data increases (Lee, 2004;
Habash and Sadat, 2006). This brings into ques-
tion: For the particular language pair and training
corpus at hand, what is the optimal (level of) sub-
word segmentation? Therefore, it is desirable to
learn the optimal segmentation in an unsupervised
manner.
In this work, we extend the method of Creutz
and Lagus (2007) so as to maximize the transla-
tion posterior in unsupervised segmentation. The
learning process is tailored to the particular SMT
task via the same parallel corpus that is used in
training the statistical translation models.
2 Related Work
Most works in SMT-oriented segmentation are su-
pervised in that they consist of manual experimen-
tation to choose the best among a set of segmen-
tation schemes, and are language(pair)-dependent.
For Arabic, Sadat and Habash (2006) present sev-
eral morphological preprocessing schemes that en-
tail varying degrees of decomposition and com-
31
pare the resulting translation performances in an
Arabic-to-English task. Shen et al (2007) use a
subset of the morphology and apply only a few
simple rules in segmenting words. Durgar El-
Kahlout and Oflazer (2006) tackle this problem
when translating from English to Turkish, an ag-
glutinative language. They use a morphologi-
cal analyzer and disambiguation to arrive at mor-
phemes as tokens. However, training the trans-
lation models with morphemes actually degrades
the translation performance. They outperform
the word-based baseline only after some selec-
tive morpheme grouping. Bisazza and Federico
(2009) adopt an approach similar to the Arabic
segmentation studies above, this time in a Turkish-
to-English translation setting.
Unsupervised segmentation by itself has gar-
nered considerable attention in the computational
linguistics literature (Poon et al, 2009; Snyder and
Barzilay, 2008; Dasgupta and Ng, 2007; Creutz
and Lagus, 2007; Brent, 1999). However, few
works report their performance in a translation
task. Virpioja et al (2007) used Morfessor (Creutz
and Lagus, 2007) to segment both sides of the par-
allel training corpora in translation between Dan-
ish, Finnish, and Swedish, but without a consistent
improvement in results.
Morfessor, which gives state of the art results in
many tests (Kurimo et al, 2009), uses only mono-
lingual information in its objective function. It is
conceivable that we can achieve a better segmenta-
tion for translation by considering not one but both
sides of the parallel corpus. A posssible choice is
the post-segmentation alignment accuracy. How-
ever, Elming et al (2009) show that optimizing
segmentation with respect to alignment error rate
(AER) does not improve and even degrades ma-
chine translation performance. Snyder and Barzi-
lay (2008) use bilingual information but the seg-
mentation is learned independently from transla-
tion modeling.
In Chang et al (2008), the granularity of the
Chinese word segmentation is optimized by train-
ing SMT systems for several values of a granular-
ity bias parameter and it is found that the value that
maximizes translation performance (as measured
by BLEU) is different than the value that maxi-
mizes segmentation accuracy (as measured by pre-
cision and recall).
One motivation in morphological preprocess-
ing before translation modeling is ?morphology
matching? as in Lee (2004) and in the scheme
?EN? of Habash and Sadat (2006). In Lee (2004),
the goal is to match the lexical granularities of the
two languages by starting with a fine-grained seg-
mentation of the Arabic side of the corpus and
then merging or deleting Arabic morphemes us-
ing alignments with a part-of-speech tagged En-
glish corpus. But this method is not completely
unsupervised since it requires external linguistic
resources in initializing the segmentation with the
output of a morphological analyzer and disam-
biguator. Talbot and Osborne (2006) tackle a spe-
cial case of morphology matching by identifying
redundant distinctions in the morphology of one
language compared to another.
3 Method
Maximizing translation performance directly
would require SMT training and decoding for
each segmentation hypothesis considered, which
is computationally infeasible. So we make some
conditional independence assumptions using a
generative model and decompose the posterior
probability P (M
f
|e, f). In this notation e and f
denote the two sides of a parallel corpus and M
f
denotes the segmentation model hypothesized for
f . Our approach is an extension of Morfessor
(Creutz and Lagus, 2007) so as to include the
translation model probability in its cost calcula-
tion. Specifically, the segmentation model takes
into account the likelihood of both sides of the
parallel corpus while searching for the optimal
segmentation. The joint likelihood is decomposed
into a prior, a monolingual likelihood, and a
translation likelihood, as shown in Eq. 1.
P (e, f,M
f
) = P (M
f
)P (f |M
f
)P (e|f,M
f
)
(1)
Assuming conditional independence between
e and M
f
given f , the maximum a posteriori
(MAP) objective can be written as:
?
M
f
= arg max
M
f
P (M
f
)P (f |M
f
)P (e|f) (2)
The role of the bilingual component P (e|f)
in Eq. 2 can be motivated with a simple exam-
ple as follows. Consider an occurrence of two
phrase pairs in a Turkish-English parallel corpus
and the two hypothesized sets of segmentations
for the Turkish phrases as in Table 1. Without ac-
cess to the English side of the corpus, a monolin-
gual segmenter can quite possibly score Seg. #1
32
Phrase #1 Phrase #2
Turkish phrase: anahtar anahtar?m
English phrase: key my key
Seg. #1: anahtar anahtar? +m
Seg. #2: anahtar anahtar +?m
Table 1: Example segmentation hypotheses
higher than Seg. #2 (e.g., due to the high fre-
quency of the observed morph ?+m?). On the
other hand, a bilingual segmenter is expected to
assign a higher alignment probability P (e|f) to
Seg. #2 than Seg. #1, because of the aligned words
key||anahtar, therefore ranking Seg. #2 higher.
The two monolingual components of Eq. 2 are
computed as in Creutz and Lagus (2007). To sum-
marize briefly, the prior P (M
f
) is assumed to only
depend on the frequencies and lengths of the indi-
vidual morphs, which are also assumed to be in-
dependent. The monolingual likelihood P (f |M
f
)
is computed as the product of morph probabilities
estimated from their frequencies in the corpus.
To compute the bilingual (translation) likeli-
hood P (e|f), we use IBM Model 1 (Brown et
al., 1993). Let an aligned sentence pair be rep-
resented by (s
e
, s
f
), which consists of word se-
quences s
e
= e
1
, ..., e
l
and s
f
= f
1
, ..., f
m
. Us-
ing a purely notational switch of the corpus labels
from here on to be consistent with the SMT lit-
erature, where the derivations are in the form of
P (f |e), the desired translation probability is given
by the expression:
P (f |e) =
P (m|e)
(l + 1)
m
m
?
j=1
l
?
i=0
t(f
j
|e
i
), (3)
The sentence length probability distribution
P (m|e) is assumed to be Poisson with the ex-
pected sentence length equal to m.
3.1 Incremental computation of Model-1
likelihood
During search, the translation likelihood P (e|f)
needs to be calculated according to Eq. 3 for every
hypothesized segmentation.
To compute Eq. 3, we need to have at hand the
individual morph translation probabilities t(f
j
|e
i
).
These can be estimated using the EM algorithm
given by (Brown, 1993), which is guaranteed to
converge to a global maximum of the likelihood
for Model 1. However, running the EM algorithm
to optimization for each considered segmentation
model can be computationally expensive, and can
result in overtraining. Therefore, in this work we
used the likelihood computed after the first EM
iteration, which also has the nice property that
P (f |e) can be computed incrementally from one
segmentation hypothesis to the next.
The incremental updates are derived from the
equations for the count collection and probability
estimation steps of the EM algorithm as follows.
In the count collection step, in the first iteration,
we need to compute the fractional counts c(f
j
|e
i
)
(Brown et al, 1993):
c(f
j
|e
i
) =
1
l + 1
(#f
j
)(#e
i
), (4)
where (#f
j
) and (#e
i
) denote the number of occur-
rences of f
j
in s
f
and e
i
in s
e
, respectively.
Let f
k
denote the word hypothesized to be seg-
mented. Let the resulting two sub-words be f
p
and
f
q
, any of which may or may not previously exist
in the vocabulary. Then, according to Eq. (4), as a
result of the segmentation no update is needed for
c(f
j
|e
i
) for j = 1 . . . N , j 6= p, q, i = 1 . . .M
(note that f
k
no longer exists); and the necessary
updates ?c(f
j
|e
i
) for c(f
j
|e
i
), where j = p, q;
i = 1 . . .M are given by:
?c(f
j
|e
i
) =
1
l + 1
(#f
k
)(#e
i
). (5)
Note that Eq. (5) is nothing but the previous
count value for the segmented word, c(f
k
|e
i
). So,
all needed in the count collection step is to copy
the set of values c(f
k
|e
i
) to c(f
p
|e
i
) and c(f
q
|e
i
),
adding if they already exist.
Then in the probability estimation step, the nor-
malization is performed including the newly added
fractional counts.
3.2 Parallelization of search
In an iteration of the algorithm, all words are pro-
cessed in random order, computing for each word
the posterior probability of the generative model
after each possible binary segmentation (splitting)
of the word. If the highest-scoring split increases
the posterior probability compared to not splitting,
that split is accepted (for all occurrences of the
word) and the resulting sub-words are explored re-
cursively for further segmentations. The process is
repeated until an iteration no more results in a sig-
nificant increase in the posterior probability.
The search algorithm of Morfessor is a greedy
algorithm where the costs of the next search points
33
Wor
d-ba
sed
Mor
fess
or
Mor
fess
or-p
Mor
fess
or-b
i
51.451.651.85252.252.452.652.85353.253.4
Seg
men
tatio
n m
etho
d
BLEU score
Figure 1: BLEU scores obtained with different
segmentation methods. Multiple data points for
a system correspond to different random orders in
processing the data (Creutz and Lagus, 2007).
are affected by the decision in the current step.
This leads to a sequential search and does not lend
itself to parallelization.
We propose a slightly modified search proce-
dure, where the segmentation decisions are stored
but not applied until the end of an iteration. In
this way, the cost calculations (which is the most
time-consuming component) can all be performed
independently and in parallel. Since the model is
not updated at every decision, the search path can
differ from that in the sequential greedy search and
hence result in different segmentations.
4 Results
We performed in vivo testing of the segmenta-
tion algorithm on the Turkish side of a Turkish-
to-English task. We compared the segmenta-
tions produced by Morfessor, Morfessor modi-
fied for parallel search (Morfessor-p), and Mor-
fessor with bilingual cost (Morfessor-bi) against
the word-based performance. We used the ATR
Basic Travel Expression Corpus (BTEC) (Kikui
et al, 2006), which contains travel conversa-
tion sentences similar to those in phrase-books
for tourists traveling abroad. The training cor-
pus contained 19,972 sentences with average sen-
tence length 5.6 and 7.7 words for Turkish and
English, respectively. The test corpus consisted
of 1,512 sentences with 16 reference translations.
We used GIZA++ (Och and Ney, 2003) for post-
segmentation token alignments and the Moses
toolkit (Koehn et al, 2007) with default param-
eters for phrase-based translation model genera-
tion and decoding. Target language models were
1.55
8
1.56
1.56
21
.564
1.56
61
.568
1.57 x 10
6
51.451.651.85252.252.452.652.85353.253.4
Mor
fess
or c
ost
BLEU score
 
 
1.072
1.074
1.076
1.078
1.08
1.082
1.084 x 10
6
51.85252.252.452.652.85353.253.453.6
Mor
fess
or-b
i cos
t
BLEU score
 
 
Figure 2: Cost-BLEU plots of Morfessor and
Morfessor-bi. Correlation coefficients are -0.005
and -0.279, respectively.
trained on the English side of the training cor-
pus using the SRILM toolkit (Stolcke, 2002). The
BLEU metric (Papineni et al, 2002) was used for
translation evaluation.
Figure 1 compares the translation performance
obtained using the described segmentation meth-
ods. All segmentation methods generally im-
prove the translation performance (Morfessor and
Morfessor-p) compared to the word-based models.
However, Morfessor-bi, which utilizes both sides
of the parallel corpus in segmenting, does not con-
vincingly outperform the monolingual methods.
In order to investigate whether the proposed
bilingual segmentation cost correlates any better
than the monolingual segmentation cost of Mor-
fessor, we show several cost-BLEU pairs obtained
from the final and intermediate segmentations of
Morfessor and Morfessor-bi in Fig. 2. The cor-
relation coefficients show that the proposed bilin-
gual metric is somewhat predictive of the trans-
lation performance as measured by BLEU, while
the monolingual Morfessor cost metric has almost
no correlation. Yet, the strong noise in the BLEU
scores (vertical variation in Fig. 2) diminishes the
effect of this correlation, which explains the incon-
sistency of the results in Fig. 1. Indeed, in our ex-
periments even though the total cost kept decreas-
ing at each iteration of the search algorithm, the
BLEU scores obtained by those intermediate seg-
mentations fluctuated without any consistent im-
provement.
Table 2 displays sample segmentations pro-
duced by both the monolingual and bilingual seg-
mentation algorithms. We can observe that uti-
lizing the English side of the corpus enabled
34
Count Morfessor Morfessor-bi English Gloss
7 anahtar anahtar (the) key
6 anahtar + ?m? anahtar + ?m? my key (ACC.)
5 anahtarla anahtar + la with (the) key
4 anahtar? anahtar + ?
1
(the) key (ACC.);
2
his/her key
3 anahtar? + m anahtar + ?m my key
3 anahtar? + n anahtar + ?n
1
your key;
2
of (the) key
1 anahtar? + n?z anahtar + ?n?z your (pl.) key
1 anahtar? + n? anahtar + ?n?
1
your key (ACC.);
2
his/her key (ACC.)
1 anahtar + ?n?z? anahtar + ?n?z? your (pl.) key (ACC.)
1 oyun + lar oyunlar (the) games
2 oyun + lar? oyunlar + ?
1
(the) games (ACC.);
2
his/her games;
3
their game(s)
1 oyun + lar?n oyunlar + ? + n
1
of (the) games;
2
your games
1 oyun + lar?n?z? oyunlar + ? + n + ?z? your (pl.) games (ACC.)
Table 2: Sample segmentations produced by Morfessor and Morfessor-bi
Morfessor-bi: (i) to consistently identify the root
word ?anahtar? (top portion), and (ii) to match the
English plural word form ?games? with the Turk-
ish plural word form ?oyunlar? (bottom portion).
Monolingual Morfessor is unaware of the target
segmentation, and hence it is up to the subsequent
translation model training to learn that ?oyun? is
sometimes translated as ?game? and sometimes as
?games? in the segmented training corpus.
5 Conclusion
We have presented a method for determining opti-
mal sub-word translation units automatically from
a parallel corpus. We have also showed a method
of incrementally computing the first iteration pa-
rameters of IBM Model-1 between segmentation
hypotheses. Being language-independent, the pro-
posed algorithm can be added as a one-time pre-
processing step prior to training in a SMT system
without requiring any additional data/linguistic re-
sources. The initial experiments presented here
show that the translation units learned by the
proposed algorithm improves on the word-based
baseline in both translation directions.
One avenue for future work is to relax some of
the several independence assumptions made in the
generative model. For example, independence of
consecutive morphs could be relaxed by an HMM
model for transitions between morphs (Creutz and
Lagus, 2007). Other future work includes optimiz-
ing the segmentation of both sides of the corpus
and experimenting with other language pairs.
It is also possible that the probability distribu-
tions are not discriminative enough to outweigh
the model prior tendencies since the translation
probabilities are estimated only crudely (single it-
eration of Model-1 EM algorithm). A possible
candidate solution would be to weigh the transla-
tion likelihood more in calculating the overall cost.
In fact, this idea could be generalized into a log-
linear modeling (e.g., (Poon et al, 2009)) of the
various components of the joint corpus likelihood
and possibly other features.
Finally, integration of sub-word segmentation
with the phrasal lexicon learning process in SMT
is desireable (e.g., translation-driven segmenta-
tion in Wu (1997)). Hierarchical models (Chiang,
2007) could cover this gap and provide a means to
seamlessly integrate sub-word segmentation with
statistical machine translation.
Acknowledgements
The authors would like to thank Murat Sarac?lar
for valuable discussions and guidance in this work,
and the anonymous reviewers for very useful com-
ments and suggestions. Murat Sarac?lar is sup-
ported by the T
?
UBA-GEB
?
IP award.
References
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological Pre-Processing for Turkish to English
Statistical Machine Translation. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 129?135, Tokyo, Japan.
M.R. Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1):71?105.
35
P.F. Brown, V.J. Della Pietra, S.A. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232, Columbus, Ohio.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learn-
ing. ACM Transactions on Speech and Language
Processing, 4(1):1?34.
Sajib Dasgupta and Vincent Ng. 2007. High-
performance, language-independent morphological
segmentation. In Proceedings of HLT-NAACL,
pages 155?163, Rochester, New York.
?
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006.
Initial explorations in English to Turkish statistical
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, pages 7?
14, New York City, New York, USA.
Jakob Elming, Nizar Habash, and Josep M. Crego.
2009. Combination of statistical word alignments
based on multiple preprocessing schemes. In Cyrill
Goutte, Nicola Cancedda, Marc Dymetman, and
George Foster, editors, Learning Machine Transla-
tion, chapter 5, pages 93?110. MIT Press.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proc. of the HLT-NAACL, Companion Volume:
Short Papers, pages 49?52, New York City, USA.
G. Kikui, S. Yamamoto, T. Takezawa, and E. Sumita.
2006. Comparative study on corpora for speech
translation. IEEE Transactions on Audio, Speech
and Language Processing, 14(5):1674?1682.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics, Companion
Volume: Proceedings of the Demo and Poster Ses-
sions, pages 177?180, Prague, Czech Republic.
M. Kurimo, S. Virpioja, V.T. Turunen, G.W. Black-
wood, and W. Byrne. 2009. Overview and Results
of Morpho Challenge 2009. In Working notes of the
CLEF workshop.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, Companion Volume: Short Papers, pages
57?60, Boston, Massachusetts, USA.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the
link? In Proceedings of the 7th Conference of the
Association for Machine Translation in the Ameri-
cas (AMTA-06), pages 90?99.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kemal Oflazer and
?
Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 25?32, Prague,
Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of HLT-
NAACL, pages 209?217, Boulder, Colorado.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
chine translation. In Proc. of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?8, Sydney, Australia.
Wade Shen, Brian Delaney, and Tim Anderson. 2007.
The MIT-LL/AFRL IWSLT-2007 MT system. In
Proc. of the International Workshop on Spoken Lan-
guage Translation, Trento, Italy.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics: HLT, pages 737?745, Columbus, Ohio.
A. Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing, volume 3.
David Talbot and Miles Osborne. 2006. Modelling
lexical redundancy for machine translation. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 969?976, Sydney, Australia.
S. Virpioja, J.J. V?ayrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsuper-
vised manner. In Machine Translation Summit XI,
pages 491?498, Copenhagen, Denmark.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
36
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182?187,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bayesian Word Alignment for Statistical Machine Translation
Cos?kun Mermer1,2
1BILGEM
TUBITAK
Gebze 41470 Kocaeli, Turkey
coskun@uekae.tubitak.gov.tr
Murat Sarac?lar2
2Electrical and Electronics Eng. Dept.
Bogazici University
Bebek 34342 Istanbul, Turkey
murat.saraclar@boun.edu.tr
Abstract
In this work, we compare the translation
performance of word alignments obtained
via Bayesian inference to those obtained via
expectation-maximization (EM). We propose
a Gibbs sampler for fully Bayesian inference
in IBM Model 1, integrating over all possi-
ble parameter values in finding the alignment
distribution. We show that Bayesian inference
outperforms EM in all of the tested language
pairs, domains and data set sizes, by up to 2.99
BLEU points. We also show that the proposed
method effectively addresses the well-known
rare word problem in EM-estimated models;
and at the same time induces a much smaller
dictionary of bilingual word-pairs.
1 Introduction
Word alignment is a crucial early step in the training
of most statistical machine translation (SMT) sys-
tems, in which the estimated alignments are used for
constraining the set of candidates in phrase/grammar
extraction (Koehn et al, 2003; Chiang, 2007; Galley
et al, 2006). State-of-the-art word alignment mod-
els, such as IBM Models (Brown et al, 1993), HMM
(Vogel et al, 1996), and the jointly-trained symmet-
ric HMM (Liang et al, 2006), contain a large num-
ber of parameters (e.g., word translation probabili-
ties) that need to be estimated in addition to the de-
sired hidden alignment variables.
The most common method of inference in such
models is expectation-maximization (EM) (Demp-
ster et al, 1977) or an approximation to EM when
exact EM is intractable. However, being a maxi-
mization (e.g., maximum likelihood (ML) or max-
imum a posteriori (MAP)) technique, EM is gen-
erally prone to local optima and overfitting. In
essence, the alignment distribution obtained via EM
takes into account only the most likely point in the
parameter space, but does not consider contributions
from other points.
Problems with the standard EM estimation of
IBM Model 1 was pointed out by Moore (2004) and
a number of heuristic changes to the estimation pro-
cedure, such as smoothing the parameter estimates,
were shown to reduce the alignment error rate, but
the effects on translation performance was not re-
ported. Zhao and Xing (2006) note that the param-
eter estimation (for which they use variational EM)
suffers from data sparsity and use symmetric Dirich-
let priors, but they find the MAP solution.
Bayesian inference, the approach in this paper,
have recently been applied to several unsupervised
learning problems in NLP (Goldwater and Griffiths,
2007; Johnson et al, 2007) as well as to other tasks
in SMT such as synchronous grammar induction
(Blunsom et al, 2009) and learning phrase align-
ments directly (DeNero et al, 2008).
Word alignment learning problem was addressed
jointly with segmentation learning in Xu et al
(2008), Nguyen et al (2010), and Chung and Gildea
(2009). The former two works place nonparametric
priors (also known as cache models) on the param-
eters and utilize Gibbs sampling. However, align-
ment inference in neither of these works is exactly
Bayesian since the alignments are updated by run-
ning GIZA++ (Xu et al, 2008) or by local maxi-
mization (Nguyen et al, 2010). On the other hand,
182
Chung and Gildea (2009) apply a sparse Dirichlet
prior on the multinomial parameters to prevent over-
fitting. They use variational Bayes for inference, but
they do not investigate the effect of Bayesian infer-
ence to word alignment in isolation. Recently, Zhao
and Gildea (2010) proposed fertility extensions to
IBM Model 1 and HMM, but they do not place any
prior on the parameters and their inference method is
actually stochastic EM (also known as Monte Carlo
EM), a ML technique in which sampling is used to
approximate the expected counts in the E-step. Even
though they report substantial reductions in align-
ment error rate, the translation BLEU scores do not
improve.
Our approach in this paper is fully Bayesian in
which the alignment probabilities are inferred by
integrating over all possible parameter values as-
suming an intuitive, sparse prior. We develop a
Gibbs sampler for alignments under IBM Model 1,
which is relevant for the state-of-the-art SMT sys-
tems since: (1) Model 1 is used in bootstrapping
the parameter settings for EM training of higher-
order alignment models, and (2) many state-of-the-
art SMT systems use Model 1 translation probabil-
ities as features in their log-linear model. We eval-
uate the inferred alignments in terms of the end-to-
end translation performance, where we show the re-
sults with a variety of input data to illustrate the gen-
eral applicability of the proposed technique. To our
knowledge, this is the first work to directly investi-
gate the effects of Bayesian alignment inference on
translation performance.
2 Bayesian Inference with IBM Model 1
Given a sentence-aligned parallel corpus (E,F), let
ei (fj) denote the i-th (j-th) source (target)1 word
in e (f ), which in turn consists of I (J) words and
denotes the s-th sentence in E (F).2 Each source
sentence is also hypothesized to have an additional
imaginary ?null? word e0. Also let VE (VF ) denote
the size of the observed source (target) vocabulary.
In Model 1 (Brown et al, 1993), each target word
1We use the ?source? and ?target? labels following the gen-
erative process, in which E generates F (cf. Eq. 1).
2Dependence of the sentence-level variables e, f , I , J (and
a and n, which are introduced later) on the sentence index s
should be understood even though not explicitly indicated for
notational simplicity.
fj is associated with a hidden alignment variable aj
whose value ranges over the word positions in the
corresponding source sentence. The set of align-
ments for a sentence (corpus) is denoted by a (A).
The model parameters consist of a VE ? VF ta-
ble T of word translation probabilities such that
te,f = P (f |e).
The joint distribution of the Model-1 variables is
given by the following generative model3:
P (E,F,A;T) =
?
s
P (e)P (a|e)P (f |a, e;T) (1)
=
?
s
P (e)
(I + 1)J
J?
j=1
teaj ,fj (2)
In the proposed Bayesian setting, we treat T as a
random variable with a prior P (T). To find a suit-
able prior for T, we re-write (2) as:
P (E,F,A|T) =
?
s
P (e)
(I + 1)J
VE?
e=1
VF?
f=1
(te,f )
ne,f (3)
=
VE?
e=1
VF?
f=1
(te,f )
Ne,f
?
s
P (e)
(I + 1)J
(4)
where in (3) the count variable ne,f denotes the
number of times the source word type e is aligned
to the target word type f in the sentence-pair s, and
in (4) Ne,f =
?
s ne,f . Since the distribution over
{te,f} in (4) is in the exponential family, specifically
being a multinomial distribution, we choose the con-
jugate prior, in this case the Dirichlet distribution,
for computational convenience.
For each source word type e, we assume the prior
distribution for te = te,1 ? ? ? te,VF , which is itself
a distribution over the target vocabulary, to be a
Dirichlet distribution (with its own set of hyperpa-
rameters ?e = ?e,1 ? ? ? ?e,VF ) independent from the
priors of other source word types:
te ? Dirichlet(te;?e)
fj |a, e,T ? Multinomial(fj ; teaj )
We choose symmetric Dirichlet priors identically
for all source words e with ?e,f = ? = 0.0001 to
obtain a sparse Dirichlet prior. A sparse prior favors
3We omit P (J |e) since both J and e are observed and so
this term does not affect the inference of hidden variables.
183
distributions that peak at a single target word and
penalizes flatter translation distributions, even for
rare words. This choice addresses the well-known
problem in the IBM Models, and more severely in
Model 1, in which rare words act as ?garbage col-
lectors? (Och and Ney, 2003) and get assigned ex-
cessively large number of word alignments.
Then we obtain the joint distribution of all (ob-
served + hidden) variables as:
P (E,F,A,T;?) = P (T;?) P (E,F,A|T) (5)
where ? = ?1 ? ? ??VE .
To infer the posterior distribution of the align-
ments, we use Gibbs sampling (Geman and Ge-
man, 1984). One possible method is to derive the
Gibbs sampler from P (E,F,A,T;?) obtained in
(5) and sample the unknowns A and T in turn, re-
sulting in an explicit Gibbs sampler. In this work,
we marginalize out T by:
P (E,F,A;?) =
?
T
P (E,F,A,T;?) (6)
and obtain a collapsed Gibbs sampler, which sam-
ples only the alignment variables.
Using P (E,F,A;?) obtained in (6), the Gibbs
sampling formula for the individual alignments is
derived as:4
P (aj = i|E,F,A?j ;?)
=
N?jei,fj + ?ei,fj
?VF
f=1N
?j
ei,f
+
?VF
f=1 ?ei,f
(7)
where the superscript ?j denotes the exclusion of
the current value of aj .
The algorithm is given in Table 1. Initialization
of A in Step 1 can be arbitrary, but for faster conver-
gence special initializations have been used, e.g., us-
ing the output of EM (Chiang et al, 2010). Once the
Gibbs sampler is deemed to have converged after B
burn-in iterations, we collect M samples of A with
L iterations in-between5 to estimate P (A|E,F). To
obtain the Viterbi alignments, which are required for
phrase extraction (Koehn et al, 2003), we select for
each aj the most frequent value in the M collected
samples.
4The derivation is quite standard and similar to other
Dirichlet-multinomial Gibbs sampler derivations, e.g. (Resnik
and Hardisty, 2010).
5A lag is introduced to reduce correlation between samples.
Input: E, F; Output: K samples of A
1 Initialize A
2 for k = 1 to K do
3 for each sentence-pair s in (E,F) do
4 for j = 1 to J do
5 for i = 0 to I do
6 Calculate P (aj = i| ? ? ? )
according to (7)
7 Sample a new value for aj
Table 1: Gibbs sampling algorithm for IBM Model 1 (im-
plemented in the accompanying software).
3 Experimental Setup
For Turkish?English experiments, we used the
20K-sentence travel domain BTEC dataset (Kikui
et al, 2006) from the yearly IWSLT evaluations6
for training, the CSTAR 2003 test set for develop-
ment, and the IWSLT 2004 test set for testing7. For
Czech?English, we used the 95K-sentence news
commentary parallel corpus from the WMT shared
task8 for training, news2008 set for development,
news2009 set for testing, and the 438M-word En-
glish and 81.7M-word Czech monolingual news cor-
pora for additional language model (LM) training.
For Arabic?English, we used the 65K-sentence
LDC2004T18 (news from 2001-2004) for training,
the AFP portion of LDC2004T17 (news from 1998,
single reference) for development and testing (about
875 sentences each), and the 298M-word English
and 215M-word Arabic AFP and Xinhua subsets of
the respective Gigaword corpora (LDC2007T07 and
LDC2007T40) for additional LM training. All lan-
guage models are 4-gram in the travel domain exper-
iments and 5-gram in the news domain experiments.
For each language pair, we trained standard
phrase-based SMT systems in both directions (in-
cluding alignment symmetrization and log-linear
model tuning) using Moses (Koehn et al, 2007),
SRILM (Stolcke, 2002), and ZMERT (Zaidan,
2009) tools and evaluated using BLEU (Papineni et
al., 2002). To obtain word alignments, we used the
accompanying Perl code for Bayesian inference and
6International Workshop on Spoken Language Translation.
http://iwslt2010.fbk.eu
7Using only the first English reference for symmetry.
8Workshop on Machine Translation.
http://www.statmt.org/wmt10/translation-task.html
184
Method TE ET CE EC AE EA
EM-5 38.91 26.52 14.62 10.07 15.50 15.17
EM-80 39.19 26.47 14.95 10.69 15.66 15.02
GS-N 41.14 27.55 14.99 10.85 14.64 15.89
GS-5 40.63 27.24 15.45 10.57 16.41 15.82
GS-80 41.78 29.51 15.01 10.68 15.92 16.02
M4 39.94 27.47 15.47 11.15 16.46 15.43
Table 2: BLEU scores in translation experiments. E: En-
glish, T: Turkish, C: Czech, A: Arabic.
GIZA++ (Och and Ney, 2003) for EM.
For each translation task, we report two EM es-
timates, obtained after 5 and 80 iterations (EM-5
and EM-80), respectively; and three Gibbs sampling
estimates, two of which were initialized with those
two EM Viterbi alignments (GS-5 and GS-80) and a
third was initialized naively9 (GS-N). Sampling set-
tings were B = 400 for T?E, 4000 for C?E and
8000 for A?E; M = 100, and L = 10. For refer-
ence, we also report the results with IBM Model 4
alignments (M4) trained in the standard bootstrap-
ping regimen of 15H53343.
4 Results
Table 2 compares the BLEU scores of Bayesian in-
ference and EM estimation. In all translation tasks,
Bayesian inference outperforms EM. The improve-
ment range is from 2.59 (in Turkish-to-English)
up to 2.99 (in English-to-Turkish) BLEU points in
travel domain and from 0.16 (in English-to-Czech)
up to 0.85 (in English-to-Arabic) BLEU points in
news domain. Compared to the state-of-the-art IBM
Model 4, the Bayesian Model 1 is better in all travel
domain tasks and is comparable or better in the news
domain.
Fertility of a source word is defined as the num-
ber of target words aligned to it. Table 3 shows the
distribution of fertilities in alignments obtained from
different methods. Compared to EM estimation, in-
cluding Model 4, the proposed Bayesian inference
dramatically reduces ?questionable? high-fertility (4
? fertility? 7) alignments and almost entirely elim-
9Each target word was aligned to the source candidate that
co-occured the most number of times with that target word in
the entire parallel corpus.
Method TE ET CE EC AE EA
All 140K 183K 1.63M 1.78M 1.49M 1.82M
EM-80 5.07K 2.91K 52.9K 45.0K 69.1K 29.4K
M4 5.35K 3.10K 36.8K 36.6K 55.6K 36.5K
GS-80 755 419 14.0K 10.9K 47.6K 18.7K
EM-80 426 227 10.5K 18.6K 21.4K 24.2K
M4 81 163 2.57K 10.6K 9.85K 21.8K
GS-80 1 1 39 110 689 525
EM-80 24 24 28 30 44 46
M4 9 9 9 9 9 9
GS-80 8 8 13 18 20 19
Table 3: Distribution of inferred alignment fertilities. The
four blocks of rows from top to bottom correspond to (in
order) the total number of source tokens, source tokens
with fertilities in the range 4?7, source tokens with fertil-
ities higher than 7, and the maximum observed fertility.
The first language listed is the source in alignment (Sec-
tion 2).
Method TE ET CE EC AE EA
EM-80 52.5K 38.5K 440K 461K 383K 388K
M4 57.6K 40.5K 439K 441K 422K 405K
GS-80 23.5K 25.4K 180K 209K 158K 176K
Table 4: Sizes of bilingual dictionaries induced by differ-
ent alignment methods.
inates ?excessive? alignments (fertility ? 8)10.
The number of distinct word-pairs induced by an
alignment has been recently proposed as an objec-
tive function for word alignment (Bodrumlu et al,
2009). Small dictionary sizes are preferred over
large ones. Table 4 shows that the proposed in-
ference method substantially reduces the alignment
dictionary size, in most cases by more than 50%.
5 Conclusion
We developed a Gibbs sampling-based Bayesian in-
ference method for IBM Model 1 word alignments
and showed that it outperforms EM estimation in
terms of translation BLEU scores across several lan-
guage pairs, data sizes and domains. As a result
of this increase, Bayesian Model 1 alignments per-
form close to or better than the state-of-the-art IBM
10The GIZA++ implementation of Model 4 artificially limits
fertility parameter values to at most nine.
185
Model 4. The proposed method learns a compact,
sparse translation distribution, overcoming the well-
known ?garbage collection? problem of rare words
in EM-estimated current models.
Acknowledgments
Murat Sarac?lar is supported by the TU?BA-GEBI?P
award.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
782?790, Suntec, Singapore, August.
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
A new objective function for word alignment. In Pro-
ceedings of the NAACL HLT Workshop on Integer Lin-
ear Programming for Natural Language Processing,
pages 28?35, Boulder, Colorado, June. Association for
Computational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for finite-state transducers. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 447?455, Los Angeles, Cali-
fornia, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 718?726,
Singapore, August.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 314?323, Honolulu, Hawaii, October.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 961?
968, Sydney, Australia, July.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions, and the Bayesian restora-
tion of images. IEEE Transactions On Pattern Analy-
sis And Machine Intelligence, 6(6):721?741, Novem-
ber.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751, Prague, Czech Republic, June.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 139?146, Rochester, New York, April.
Genichiro Kikui, Seiichi Yamamoto, Toshiyuki
Takezawa, and Eiichiro Sumita. 2006. Com-
parative study on corpora for speech translation.
IEEE Transactions on Audio, Speech and Language
Processing, 14(5):1674?1682.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003, Main Papers, pages 48?54,
Edmonton, May-June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA,
June.
Robert C. Moore. 2004. Improving IBM word alignment
Model 1. In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics (ACL?04),
Main Volume, pages 518?525, Barcelona, Spain, July.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for ma-
186
chine translation. In Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 815?823, Beijing, China, August.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling
for the uninitiated. University of Maryland Computer
Science Department; CS-TR-4956, June.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing, volume 3.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised Chinese
word segmentation for statistical machine translation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
1017?10124, Manchester, UK, August.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79?88.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden Markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 596?605, Cambridge, MA, October.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 969?976, Sydney, Australia,
July. Association for Computational Linguistics.
187
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 109?113,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
T ?UB?ITAK-B?ILGEM German-English Machine Translation Systems for
WMT?13
?Ilknur Durgar El-Kahlout and Cos?kun Mermer
T ?UB?ITAK-B?ILGEM
Gebze 41470, Kocaeli, TURKEY
{ilknur.durgar,coskun.mermer}@tubitak.gov.tr
Abstract
This paper describes T ?UB?ITAK-B?ILGEM
statistical machine translation (SMT) sys-
tems submitted to the Eighth Work-
shop on Statistical Machine Transla-
tion (WMT) shared translation task for
German-English language pair in both di-
rections. We implement phrase-based
SMT systems with standard parameters.
We present the results of using a big tun-
ing data and the effect of averaging tun-
ing weights of different seeds. Addition-
ally, we performed a linguistically moti-
vated compound splitting in the German-
to-English SMT system.
1 Introduction
T ?UB?ITAK-B?ILGEM participated for the first time
in the WMT?13 shared translation task for the
German-English language pairs in both directions.
We implemented a phrase-based SMT system by
using the entire available training data. In the
German-to-English SMT system, we performed a
linguistically motivated compound splitting. We
tested different language model (LM) combina-
tions by using the parallel data, monolingual data,
and Gigaword v4. In each step, we tuned systems
with five different tune seeds and used the average
of tuning weights in the final system. We tuned
our systems on a big tuning set which is generated
from the last years? (2008, 2009, 2010, and 2012)
development sets. The rest of the paper describes
the details of our systems.
2 German-English
2.1 Baseline
All available data was tokenized, truecased, and
the maximum number of tokens were fixed to
70 for the translation model. The Moses open
SMT toolkit (Koehn et al, 2007) was used with
MGIZA++ (Gao and Vogel, 2008) with the stan-
dard alignment heuristic grow-diag-final (Och and
Ney, 2003) for word alignments. Good-Turing
smoothing was used for phrase extraction. Sys-
tems were tuned on newstest2012 with MERT
(Och, 2003) and tested on newstest2011. 4-
gram language models (LMs) were trained on
the target side of the parallel text and the mono-
lingual data by using SRILM (Stolcke, 2002)
toolkit with Kneser-Ney smoothing (Kneser and
Ney, 1995) and then binarized by using KenLM
toolkit (Heafield, 2011). At each step, systems
were tuned with five different seeds with lattice-
samples. Minimum Bayes risk decoding (Kumar
and Byrne, 2004) and -drop-unknown parameters
were used during the decoding.
This configuration is common for all of the ex-
periments decribed in this paper unless stated oth-
erwise. Table 1 shows the number of sentences
used in system training after the clean-corpus pro-
cess.
Data Number of sentences
Europarl 1908574
News-Commentary 177712
Commoncrawl 726458
Table 1: Parallel Corpus.
We trained two baseline systems in order to as-
sess the effects of this year?s new parallel data,
commoncrawl. We first trained an SMT system
by using only the training data from the previ-
ous WMT shared translation tasks that is europarl
and news-commentary (Baseline1). As the second
baseline, we also included the new parallel data
commoncrawl only in the translation model (Base-
line2). Then, we included commoncrawl corpus
both to the translation model and the language
model (Baseline3).
Table 2 compares the baseline results. For all
109
experiments throughout the paper, we present the
minimum and the maximum BLEU scores ob-
tained after five different tunes. As seen in the
table, the addition of the commoncrawl corpus re-
sultedin a 1.1 BLEU (Papineni et al, 2002) points
improvement (on average) on the test set. Al-
though Baseline2 is slightly better than Baseline3,
we used Baseline3 and kept commoncrawl corpus
in LMs for further experiments.
System newstest12 newstest11
Baseline1 20.58|20.74 19.14|19.29
Baseline2 21.37|21.58 20.16|20.46
Baseline3 21.28|21.58 20.22|20.49
Table 2: Baseline Results.
2.2 Bayesian Alignment
In the original IBM models (Brown et al, 1993),
word translation probabilities are treated as model
parameters and the expectation-maximization
(EM) algorithm is used to obtain the maximum-
likelihood estimates of the parameters and the
resulting distributions on alignments. However,
EM provides a point-estimate, not a distribu-
tion, for the parameters. The Bayesian align-
ment on the other hand takes into account all
values of the model parameters by treating them
as multinomial-distributed random variables with
Dirichlet priors and integrating over all possible
values. A Bayesian approach to word alignment
inference in IBM Models is shown to result in sig-
nificantly less ?garbage collection? and a much
more compact alignment dictionary. As a result,
the Bayesian word alignment has better transla-
tion performances and obtains significant BLEU
improvements over EM on various language pairs,
data sizes, and experimental settings (Mermer et
al., 2013).
We compared the translation performance of
word alignments obtained via Bayesian inference
to those obtained via EM algorithm. We used a
a Gibbs sampler for fully Bayesian inference in
HMM alignment model, integrating over all pos-
sible parameter values in finding the alignment
distribution by using Baseline3 word alignments
for initialization. Table 3 compares the Bayesian
alignment to the EM alignment. The results show
a slight increase in the development set newstest12
but a decrease of 0.1 BLEU points on average in
the test set newstest11.
System newstest12 newstest11
Baseline3 21.28|21.58 20.22|20.49
Gibbs Sampling 21.36|21.59 19.98|20.40
Table 3: Bayesian Alignment Results.
2.3 Development Data in Training
Development data from the previous years (i.e.
newstest08, newstest09, newstest10), though being
a small set of corpus (7K sentences), is in-domain
data and can positively affect the translation sys-
tem. In order to make use of this data, we exper-
imented two methods: i) adding the development
data in the translation model as described in this
section and ii) using it as a big tuning set for tun-
ing the parameters more efficiently as explained in
the next section.
Similar to including the commoncrawl corpus,
we first add the development data both to the train-
ing and language models by concatenating it to the
biggest corpus europarl (DD(tm+lm)) and then
we removed this corpus from the language models
(DD(tm)). Results in Table 4 show that including
the development data both the tranining and lan-
guage model increases the performance in devel-
opment set but decreases the performance in the
test set. Including the data only in the translation
model shows a very slight improvement in the test
set.
System newstest12 newstest11
Baseline3 21.28|21.58 20.22|20.49
DD(tm+lm) 21.28|21.65 20.00|20.49
DD(tm) 21.23|21.52 20.26|20.49
Table 4: Development Sets Results.
2.4 Tuning with a Big Development Data
The second method of making use of the develop-
ment data is to concatenate it to the tuning set. As
a baseline, we tuned the system with newstest12
as mentioned in Section 2.1. Then, we concate-
nated the development data of the previous years
with the newstest12 and built a big tuning set. Fi-
nally, we obtained a tuning set of 10K sentences.
We excluded the newstest11 as an internal test set
to see the relative improvements of different sys-
tems. Table 5 shows the results of using a big tun-
ing set. Tuning the system with a big tuning set
resulted in a 0.13 BLEU points improvement.
110
System newstest12 newstest11
newstest12 21.28|21.58 20.22|20.49
Big Tune 20.93|21.19 20.32|20.58
Table 5: Tuning Results.
2.5 Effects of Different Language Models
In this set of experiments, we tested the effects
of different combinations of parallel and monolin-
gual data as language models. As the baseline, we
trained three LMs, one from each parallel corpus
as europarl, news-commentary, and commoncrawl
and one LM from the monolingual data news-
shuffled (Baseline3). We then trained two LMs,
one from the whole parallel data and one from the
monolingual data (2LMs). Table 6 shows that us-
ing whole parallel corpora as one LM performs
better than individual corpus LMs and results in
0.1 BLEU points improvement on the baseline. Fi-
nally, we trained Gigaword v4 (LDC2009T13) as a
third LM (3LMs) which gives a 0.16 BLEU points
improvement over the 2LMs.
System newstest12 newstest11
Baseline3 21.28|21.58 20.22|20.49
2LMs 21.46|21.70 20.28|20.57
3LMs 21.78|21.93 20.54|20.68
Table 6: Language Model Results.
2.6 German Preprocessing
In German, compounding is very common. From
the machine translation point of view, compounds
increase the vocabulary size with high number of
the singletons in the training data and hence de-
crease the word alignment quality. Moreover, high
number of out-of-vocabulary (OOV) words in tun-
ing and test sets results in several German words
left as untranslated. A well-known solution to this
problem is compound splitting.
Similarly, having different word forms for a
source side lemma for the same target lemma
causes the lexical redundancy in translation. This
redundancy results in unnecessary large phrase
translation tables that overload the decoder, as a
separate phrase translation entry has to be kept for
each word form. For example, German definite de-
terminer could be marked in sixteen different ways
according to the possible combinations of genders,
case and number, which are fused in six different
tokens (e.g., der, das, die, den, dem, des). Except
for the plural and genitive cases, all these forms
are translated to the same English word ?the?.
In the German preprocessing, we aimed both
normalizing lexical redundancy and splitting Ger-
man compounds with corpus driven splitting al-
gorithm based on Koehn and Knight (2003). We
used the same compound splitting and lexical re-
dundancy normalization methods described in Al-
lauzen et al (2010) and Durgar El-Kahlout and
Yvon (2010) with minor in-house changes. We
used only ?addition? (e.g., -s, -n, -en, -e, -es) and
?truncation? (e.g., -e, -en, -n) affixes for com-
pound splitting. We selected minimum candidate
length to 8 and minimum split length to 4. By us-
ing the Treetagger (Schmid, 1994) output, we in-
cluded linguistic information in compound split-
ting such as not splitting named entities and for-
eign words (CS1). We also experimented adding
# as a delimiter for the splitted words except the
last word (e.g., Finanzkrisen is splitted as finanz#
krisen) (CS2).
On top of the compound splitting, we
applied the lexical redundancy normalization
(CS+Norm1). We lemmatized German articles,
adjectives (only positive form), for some pronouns
and for nouns in order to remove the lexical re-
dundancy (e.g., Bildes as Bild) by using the fine-
grained part-of-speech tags generated by RFTag-
ger (Schmid and Laws, 2008). Similar to CS2, We
tested the delimited version of normalized words
(CS+Norm2).
Table 7 shows the results of compound split-
ting and normalization methods. As a result, nor-
malization on top of compounding did not per-
form well. Besides, experiments showed that com-
pound word decomposition is crucial and helps
vastly to improve translation results 0.43 BLEU
points on average over the best system described
in Section 2.5.
System newstest12 newstest11
3LMs 21.78|21.93 20.54|20.68
CS1 22.01|22.21 20.63|20.89
CS2 22.06|22.22 20.74|20.99
CS+Norm2 21.96|22.16 20.70|20.88
CS+Norm1 20.63|20.76 22.01|22.16
Table 7: Compound Splitting Results.
111
2.7 Average of Weights
As mentioned in Section 2.1, we performed tun-
ing with five different seeds. We averaged the five
tuning weights and directly applied these weights
during the decoding. Table 8 shows that using the
average of several tuning weights performs better
than each individual tuning (0.2 BLEU points).
System newstest12 newstest11
CS2 22.06|22.22 20.74|20.99
Avg. of Weights 22.27 21.07
Table 8: Average of Weights Results.
2.8 Other parameters
In addition to the experiments described in the
earlier sections, we removed the -drop-unknown
parameter which gave us a 0.5 BLEU points im-
provement. We also included the monotone-at-
punctuation, -mp in decoding. We handled out-
of-vocabulary (OOV) words by lemmatizing the
OOV words. Moreover, we added all development
data in training after fixing the parameter weights
as described in Section 2.7. Although each of
these changes increases the translation scores each
gave less than 0.1 BLEU point improvement. Ta-
ble 9 shows the results of the final system after
including all of the approaches except the ones de-
scribed in Section 2.2 and 2.3.
System newstest12 newstest11
Final System 22.59|22.77 21.86|21.93
Avg. of Weights 22.66 22.00
+ tune data in train ?? 22.09
Table 9: German-to-English Final System Results.
3 English-German
For English-to-German translation system, the
baseline setting is the same as described in Sec-
tion 2.1. We also added the items that showed
positive improvement in the German to English
SMT system such as using 2 LMs, tuning with five
seeds and averaging tuning parameters, using -mp,
and not using -drop-unknown. Table 10 shows the
experimental results for English-to-German SMT
systems. Similar to the German-to-English direc-
tion, tuning with a big development data outper-
forms the baseline 0.26 BLEU points (on average).
Additionally, averaging the tuning weights of dif-
ferent seeds results in 0.2 BLEU points improve-
ment.
System newstest12 newstest11
Baseline 16.95|17.03 15.93|16.13
+ Big Tune 16.82|17.01 16.22|16.37
Avg. of Weights 16.99 16.47
Table 10: English to German Final System Re-
sults.
4 Final System and Results
Table 11 shows our official submission scores for
German-English SMT systems submitted to the
WMT?13.
System newstest13
De-En 25.60
En-De 19.28
Table 11: German-English Official Test Submis-
sion.
5 Conclusion
In this paper, we described our submissions to
WMT?13 Shared Translation Task for German-
English language pairs. We used phrase-based
systems with a big tuning set which is a com-
bination of the development sets from last four
years. We tuned the systems on this big tuning
set with five different tunes. We averaged these
five tuning weights in the final system. We trained
4-gram language models one from parallel data
and one from monolingual data. Moreover, we
trained a 4-gram language model with Gigaword
v4 for German-to-English direction. For German-
to-English, we performed a different compound
splitting method instead of the Moses splitter. We
obtained a 1.7 BLEU point increase for German-
to-English SMT system and a 0.5 BLEU point in-
crease for English-to-German SMT system for the
internal test set newstest2011. Finally, we sub-
mitted our German-to-English SMT system with
a BLEU score 25.6 and English-to-German SMT
system with a BLEU score 19.3 for the official test
set newstest2013.
112
References
Alexandre Allauzen, Josep M. Crego, ?Ilknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi?s statisti-
cal translation systems for wmt?10. In Proceedings
of the Fifth Workshop on Statistical Machine Trans-
lation, pages 54?59.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19:263?311.
?Ilknur Durgar El-Kahlout and Francois Yvon. 2010.
The pay-offs of preprocessing German-English sta-
tistical machine translation. In Proceedings of the
Seventh International Workshop on Spoken Lan-
guage Translation (IWSLT), pages 251?258.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Proceedings of
ACL WSETQANLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech and Signal Processing, pages 181?184.
Philipp Koehn and Kevin Knight. 2003. Emprical
methods for compound splitting. In Proceedings of
European Chapter of the ACL (EACL), pages 187?
194.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL Demo and Poster Ses-
sion, pages 177?180.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Cos?kun Mermer, Murat Sarac?lar, and Ruhi Sarkaya.
2013. Improving statistical machine translation us-
ing bayesian word alignment and gibbs sampling.
IEEE Transactions on Audio, Speech and Language
Processing, 21:1090?1101.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 1:19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-granined pos tagging. In Pro-
ceedings of COLING.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing (ICSLP), pages 257?286.
113
