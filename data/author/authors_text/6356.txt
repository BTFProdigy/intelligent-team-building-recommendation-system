Feature Weighting for Co-occurrence-based Classification of Words
Viktor PEKAR
CLG, U. of Wolverhampton
Wolverhampton
UK, WV1 1SB
v.pekar@wlv.ac.uk
Michael KRKOSKA
Mentasys GmbH
Schonfeldstrasse 8
Karlsruhe, Germany, 76131
michael@mentasys.de
Steffen STAAB
Ontoprise GmbH & Institute
AIFB, U. of Karlsruhe
Karlsruhe, Germany, 76128
staab@aifb.uni-karlsruhe.de
Abstract*
The paper comparatively studies methods of
feature weighting in application to the task of
cooccurrence-based classification of words
according to their meaning. We explore parameter
optimization of several weighting methods
frequently used for similar problems such as text
classification. We find that successful application
of all the methods crucially depends on a number
of parameters; only a carefully chosen weighting
procedure allows to obtain consistent improvement
on a classifier learned from non-weighted data.
1 Introduction
Lexical repositories like thesauri and lexicons are
today a key component of many NLP technologies,
where they serve as background knowledge for
processing the semantics of text. But, as is well
known, manual compilation of such resources is a
very costly procedure, and their automated
construction is an important research issue.
One promising possibility to speed up the lexical
acquisition process is to glean the semantics of
words from a corpus by adopting the co-
occurrence model of word meaning. Previous
research has investigated a wide range of its
applications, including automatic construction of
thesauri, their enrichment, acquisition of bilingual
lexicons, learning of information extraction
patterns, named entity classification and others..
The basic idea behind the approach is that the
distribution of a word across lexical contexts (other
words and phrases it co-occurs with) is highly
indicative of its meaning. The method represents
the meaning of a word as a vector where each
feature corresponds to a context and its value to the
frequency of the word?s occurring in that context.
Once such representation is built, machine learning
techniques can be used to perform various lexical
acquisition tasks, e.g. automatically classify or
cluster words according to their meaning.
However, using natural language words as
features inevitably results in very noisy
                                                
.* The study was partially supported by the Russian
Foundation Basic Research grant #03-06-80008.
representations. Because of their inherent
polysemy and synonymy, many context words
become ambiguous or redundant features. It is
therefore desirable to determine a measure of
usefulness of each feature and weight it
accordingly. Still, despite a wide variety of feature
weighting methods existing in machine learning,
these methods are poorly explored in application to
lexical acquisition. There have been a few studies
(e.g., Lin, 1998; Ciaramita, 2002; Alfonseca and
Manandhar, 2002) where word representations are
modified through this or that kind of feature
weighting. But in these studies it is performed only
as a standard pre-processing step on the analogy
with similar tasks like text categorization, and the
choice of a particular weighting procedure is
seldom motivated. To our knowledge, there is no
work yet on evaluation and comparison of different
weighting methods for lexical acquisition.
The goal of this paper is to comparatively study
a number of popular feature weighting methods in
application to the task of word classification.
The structure of the paper is the following.
Section 2 more formally describes the task of
feature weighting. Section 3 describes the
weighting methods under study. Section 4 details
the experimental data, classification algorithms
used, and evaluation methods. Section 5 is
concerned with the results of the experiments and
their discussion. Section 6 presents conclusions
from the study.
2 Two feature weighting strategies
In machine learning, feature weighting before
classification is performed with the purpose to
reflect how much particular features reveal about
class membership of instances. The weights of
features are determined from their distribution
across training classes, which is why the weighting
procedure can be called supervised. In the context
of word classification this procedure can be
formalized as follows.
Let us assume that each word n?N of the
training set is represented as a feature vector,
consisting of features f ? F, and that each n is
assigned a class label c?C, i.e. "n$c?C: n?c. For
each f, from its distribution across C, a certain
function computes its relevance score, specific to
each class. This score can be used directly as its
local weight w(f,c). Alternatively, from class-
specific weights of a feature, one can compute its
single global weight, using some globalization
policy. For example, as a global weight one can
use the maximum local weight of f across all
classes wglob(f)= ),(max cfwCc? . After the weights
have been applied to the training data, a classifier
is learned and evaluated on the test data.
A key decision in the weighting procedure is to
choose a function computing w(f,c). Such functions
typically try to capture the intuition that the best
features for a class are the ones that best
discriminate  the sets of its positive and negative
examples. They determine w(f,c) from the
distribution of f between c and c , attributing
greater weights to those f that correlate with c or c
most. In the present study we include three such
functions widely used in text categorization:
mutual information, information gain ratio and
odds ratio.
There is another view on feature scoring that it is
sometimes adopted in classification tasks.
According to this view, useful are those features
that are shared by the largest number of positive
examples of c. The purpose of emphasizing these
features is to characterize the class without
necessarily discriminating it from other classes.
Functions embodying this view assess w(f,c) from
the distribution of f across n ? c, giving greater
weight to those f that are distributed most uniformly.
Although they do not explicitly aim at underpinning
differences between classes, these functions were
shown to enhance text retrieval (Wilbur and
Sirotkin, 1992) and text categorization (Yang and
Pedersen, 1997). In this paper we experimented with
term strength, a feature scoring function previously
shown to be quite competitive in information
retrieval. Since term strength is an unsupervised
function, we develop two supervised variants of it
tailoring them for the classification task.
3 Feature Weighting Functions
3.1 Mutual Information
Mutual information (MI) is an information-
theoretic measure of association between two
words, widely used in statistical NLP. Pointwise
MI between class c and feature f measures how
much information presence of f contains about c:
)()(
),(log),(
cPfP
cfPcfMI =             (1)
3.2 Gain Ratio
Gain Ratio (GR) is a normalized variant of
Information Gain (IG), introduced into machine
learning from information theory (Quinlan, 1993).
IG measures the number of bits of information
obtained about presence and absence of a class by
knowing the presence or absence of the feature1:
? ?
? ?
=
},{ },{ )()(
),(log),(),(
ccd ffg dPgP
dgPdgPcfIG    (2)
Gain Ratio aims to overcome one disadvantage
of IG which is the fact that IG grows not only with
the increase of dependence between f and c, but
also with the increase of the entropy of f. That is
why features with low entropy receive smaller IG
weights although they may be strongly correlated
with a class. GR removes this factor by
normalizing IG by the entropy of the class:
?
?
-
=
},{
)(log)(
),(),(
ffg
gPgP
cgIGcfGR             (3)
3.3 Odds Ratio
Odds Ratio (OR) is used in information retrieval
to rank documents according to their relevance on
the basis of association of their features with a set
of positive documents. Mladenic (1998) reports
OR to be a particularly successful method of
selecting features for text categorization. The OR
of a feature f, given the set of positive examples
and negative examples for class c, is defined as2:
)|())|(1(
))|(1()|(),(
cfpcfp
cfpcfpcfOR
?-
-?=               (4)
3.4 Term Strength
Term Strength (TS) was introduced by Wilbur
and Sirotkin (1992) for improving efficiency of
document retrieval by feature selection. It was later
studied in a number of works by Yang and her
colleagues (e.g., Yang and Pedersen, 1997), who
found that it performs on par with best
discriminative functions on the document
categorization task. This method is based on the
idea that most valuable features are shared by
related documents. It defines the weight of a
                                                
1 Strictly speaking, the definition does not define IG,
but conditional entropy H(c|f) ; the other ingredient of
the IG function, the entropy of c, being constant and
thus omitted from actual weight calculation.
2 In cases when p(f|c) equals 1 or p(f|c ) equals 0, we
mapped the weight to the maximum OR weight in the class.
feature as the probability of finding it in some
document d given that it has also appeared in the
document d?, similar to d. To calculate TS for
feature f, for each n we first retrieved several
related words n? using a distributional similarity
measure, thus preparing a set of pairs (n, n?). The
TS weight for f was then calculated as the
conditional probability of f appearing in n given
that f appears also in n? (the ordering of words
inside a pair is ignored):
)'|()( nfnfPfTS ??=                          (5)
An important parameter in TS is the threshold on
the similarity measure used to judge two words to
be sufficiently related. Yang and Pedersen
determined this threshold by first deciding how
many documents can be related to a given one and
then finding the average minimum similarity
measure for this number of neighbors over all
documents in the collection. It should be noted that
TS does not make use of the information about
feature-class associations and therefore is
unsupervised and can be used only for global
feature weighting.
We introduce two supervised variants of TS,
which can be applied locally: TSL1 and TSL2. The
first one is different from TS in that, firstly, related
words for n are looked for not in the entire training
set, but within the class of n; secondly, the weight
for a feature is estimated from the distribution of
the feature across pairs of members of only that
class:
c  ,with ),'|(),(1 ???= n'nnfnfPcfTSL       (6)
Thus, by weighting features using TSL1 we aim
to increase similarity between members of a class
and disregard possible similarities across classes.
Both TS and TSL1 require computation of
similarities between a large set of words and thus
incur significant computational costs. We therefore
tried another, much more efficient method to
identify features characteristic of a class, called
TSL2. As TSL1, it looks at how many members of
a class share a feature. But instead of computing a
set of nearest neighbors for each member, it
simply uses all the words in the class as the set
of related words. TSL2 is the proportion of
instances which possess feature f to the total
number of instances in c :
|}{|
|}|{|),(2 cn
nfcncfTSL
?
??=                         (7)
Table 1 illustrates the 10 highest scored features
according to five supervised functions for the class
{ambulance, car, bike, coupe, jeep, motorbike,
taxi, truck} (estimated from the BNC co-
occurrence data described in Section 4).
MI GR OR TSL1 TSL2
see_into
die_after
drive_into
remand_to
run_from
privatise
release_into
switch_to
make_about
entrust_to
knock_by
climb _of
die_after
drive_into
remand_to
privatise
make_about
force_of
plan_with
recover_in
die_after
drive_into
remand_to
privatise
make_about
force_of
plan_with
recover_in
start_up
explode_in
see
drive
take
get
get_into
hear
need
call
send
go_by
see
drive
get
take
get_into
park
hear
wait_for
need
call
Table 1. 10 highest scored features for class
{ambulance, car, bike, coupe, jeep, motorbike,
taxi, truck} according to MI, GR, OR, TSL1, TSL2
The examples vividly demonstrate the basic
differences between the functions emphasizing
discriminative features vs. those emphasizing
characteristic features. The former attribute
greatest weights to very rare context words, some
of which seem rather informative (knock_by,
climb_of, see_into), some also appear to be
occasional collocates (remand_to, recover_in ) or
parsing mistakes (entrust_to, force_of). In contrast,
the latter encourage frequent context words.
Among them are those that are intuitively useful
(drive, park, get_into), but also those that are too
abstract (see, get, take). The inspection of the weights
suggests that both feature scoring strategies are able
to identify different potentially useful features, but at
the same time often attribute great relevance to quite
non-informative features. We next describe an
empirical evaluation of these functions.
4 Experimental Settings
4.1 Data
The evaluation was carried out on the task of
classifying English nouns into predefined semantic
classes. The meaning of each noun n?N was
represented by a vector where features are verbs
v?V with which the nouns are used as either direct
or prepositional objects. The values of the features
were conditional probabilities p(v|n). Two different
datasets were used in the experiments: verb-noun
co-occurrence pairs extracted from the British
National Corpus (BNC)3 and from the Associated
Press 1988 corpus (AP)4. Rare nouns were filtered:
the BNC data contained nouns that appeared with
at least 5 different verbs and the AP data contained
1000 most frequent nouns, each of which appeared
                                                
3 http://www.wlv.ac.uk/~in8113/data/bnc.tar.gz
4 http://www.cs.cornell.edu/home/llee/data/sim.html
with at least 19 different verbs. Co-occurrences
that appeared only once were removed.
To provide the extracted nouns with class labels
needed for training and evaluation, the nouns were
arranged into classes using WordNet in the
following manner. Each class was made up of
those nouns whose most frequent senses are
hyponyms to a node seven edges below the root
level of WordNet. Only those classes were used in
the study that had 5 or more members. Thus, from
the BNC data we formed 60 classes with 514 nouns
and from the AP data 42 classes with 375 nouns.
4.2 Classification algorithms
Two classification algorithms were used in the
study: k  nearest neighbors (kNN) and Na?ve Bayes,
which were previously shown to be quite robust on
highly dimensional representations on tasks
including word classification (e.g., Ciaramita 2002).
The kNN algorithm classifies a test instance by
first identifying its k  nearest neighbors among the
training instances according to some similarity
measure and then assigning it to the class that has
the majority in the set of nearest neighbors. We
used the weighted kNN algorithm: the vote of each
neighbor was weighted by the score of its
similarity to the test instance.
As is well known, kNN?s performance is highly
sensitive to the choice of the similarity metric.
Therefore, we experimented with several similarity
metrics and found that on both datasets Jensen-
Shannon Divergence yields the best classification
results (see Table 1). Incidentally, this is in
accordance with a study by (Dagan et al, 1997)
who found that it consistently performed better
than a number of other popular functions.
Similarity function BNC AP
Jensen-Shannon 41.67 41.33
L1 distance 38.15 39.72
Jaccard 36.82 37.01
Cosine 36.80 34.95
Skew Divergence 35.82 37.34
L2 distance 24.15 26.62
Table 2. Comparison of similarity functions for
the kNN algorithm.
Jensen-Shannon Divergence measures the
(dis)similarity between a train instance n and test
instance m as:
)]||()||([
2
1),( ,, mnmn avgmDavgnDmnJ +=   (8)
where D is the Kullback Leibler divergence
between two probability distributions x and y:
? ?= Vv yvp
xvpxvpyxD
)|(
)|(log)|()||(                (9)
and avgn,m is the average of the distributions of n
and m.
In testing each weighting method, we
experimented with k = 1, 3, 5, 7, 10, 15, 20, 30, 50,
70, and 100 in order to take into account the fact
that feature weighting typically changes the
optimal value of k . The results for kNN reported
below indicate the highest effectiveness measures
obtained among all k  in a particular test.
The Na?ve Bayes algorithm classifies a test
instance m by finding a class c that maximizes
p(c|Vm?m). Assuming independence between
features, the goal of the algorithm can be stated as:
)|()(maxarg)|(maxarg i
Vv
iimii cvpcpVcp
m
?
?
? (10)
where p(ci) and p(v|ci) are estimated during the
training process from the corpus data.
The Na?ve Bayes classifier adopted in the study
was the binary independence model, which
estimates p(v|ci) assuming the binomial distribution
of features across classes. In order to introduce the
information inherent in the frequencies of features
into the model all input probabilities were
calculated from the real values of features, as
suggested in (Lewis, 1998).
4.3 Evaluation method
To evaluate the quality of classifications, we
adopted the ten-fold cross-validation technique. The
same 10 test-train splits were used in all experiments.
Since we found that the difficulty of particular test
sets can vary quite a lot, using the same test-train
splits allowed for estimation of the statistical
significance of differences between the results of
particular methods (one-tailed paired t-test was used
for this purpose). Effectiveness was first measured
in terms of precision and recall, which were then
used to compute the Fb score5. The reported
evaluation measure is microaveraged F scores.
As a baseline, we used the k-nn and the Na?ve
Bayes classifiers trained and tested on non-
weighted instances.
5 Results
5.1 Term Strength
We first describe experiments on finding the
most optimal parameter settings for Term Strength.
As was mentioned in Section 3.4, an important
parameter of term strength that needs to be tuned for
                                                
5 b was set to 1.
a task is the similarity threshold which is used to
judge a pair of words to be semantically related.
Since in both datasets the minimum number of
words in a class was 5, we chose 4 to be the number
of words that can be related to any given word.
Finding the four nearest neighbors for each word in
the collection, we calculated the average minimum
similarity score that a pair of words must have in
order to be considered related. However, since words
vary a lot in terms of the amount of corpus data
available on them, the average similarity threshold
might be inappropriate for many words. Therefore
we tried also another way to select pairs of related
words by simply taking the four most similar words
for each particular word. Table 4 compares the two
methods of locating related words (significant
improvements at a=0.05 are shown in bold).
kNN Na?ve Bayes
TS TSL1 TS TSL1
BNC
Threshold 39.54 36.84 41.67 37.97
Top 4 words 40.90 39.74 41.86 42.64
AP
Threshold 42.12 40.22 37.80 33.82
Top 4 words 42.12 44.45 38.07 36.47
Table 4. Two methods of identifying semantically
related words for TS and TSL1.
We see that using a similarity threshold indeed
produces worse results, significantly so for TSL1. In
the rest of the experiments we used a fixed number
of related words in calculating TS and TSL1.
5.2 Globalization methods
Before comparing global and local variants of
the functions, we studied three ways to derive a
global weight for a feature: (1) using the maximum
local relevance score of a feature across all classes,
(2) its weighted average score (the contribution of
each class-specific score is weighted by the size of
the class), and (3) the sum of all local scores. The
results are shown on Tables 5 and 6 (in bold are
the figures that are significantly different from the
second-best achievement at a=0.05).
MI GR OR TSL1 TSL2
BNC
max 42.83 48.88 46.35 37.9 41.52
wavg 41.29 45.95 42.65 26.47 27.24
sum 41.29 45.95 42.65 26.46 27.24
AP
max 43.17 43.44 44.77 32.48 35.95
wavg 42.93 43.98 41. 37.31 38.12
sum 43.20 43.99 41.61 37.04 37.85
Table 5. Globalization methods on kNN.
MI GR OR TSL1 TSL2
BNC
max 46.52 42.82 43.96 37.17 38.53
wavg 43.4 41.45 43.98 21. 24.5
sum 40.48 43.59 45.15 18.66 22.96
AP
max 39.68 38.6 42.07 38.1 38.64
wavg 39.15 42.1 40.23 33.82 35.15
sum 39.68 42.38 40.76 34.1 35.96
Table 6. Globalization methods on Na?ve Bayes.
As one can see, using a maximum local weight is
usually the best method of globalization. Its
performance is often significantly higher than that
of the other methods. The explanation for this can
be the fact that a feature often has very high scores
relative to specific classes, while in the rest of the
classes its weight is low. Using its weighted
average score or a sum of local scores results in
obscuring its high relevance to some classes. In
contrast, the maximum local score does reflect
high relevance of the feature to these classes. If, in
addition to that, the feature appears in very few
classes, it is unlikely that its being weighted too
highly interferes with the representations of
irrelevant classes. This is confirmed by the fact
that the maximum weight is noticeably better on
the BNC dataset, which contains much more rare
features than the AP one.
5.3 Global vs. Local Weighting
In carrying out either local or global weighting,
there is a choice either to weight only training
instances or also test instances before their
classification. The test instance can be weighted
either by the global weights or by the local weights
of the class it is compared with. Tables 7 and 8
present the results of the evaluation of the
functions along two dimensions: (1) local versus
global weighting and (2) weighted versus un-
weighted test instances. As before, the results for
those methods whose superiority over other ones is
statistically significant appear in bold.
MI GR OR TS TSL1 TSL2
BNC
gl y 42.83 48.88 46.35 34.72 32.48 35.95
loc y 28.43 35.84 34.29 - 15.38 20.75
gl n 40.12 39.74 38.36 40.90 38.35 36.99
loc n 40.32 40.33 39.72 - 39.74 41.29
AP
gl y 43.17 43.44 44.77 38.12 37.9 41.52
loc y 37.31 31.74 37.04 - 33.06 36.68
gl n 41.59 40.78 40.51 42.12 39.25 40.24
loc n 40.74 37.86 41.34 - 44.45 43.70
Table 7. Local vs. global weighting schemas
on kNN.
MI GR OR TS TSL1 TSL2
BNC
gl y 46.52 42.82 43.96 33.87 37.17 38.53
loc y 41.84 43.79 38.32 - 36.01 39.53
gl n 45.54 42.63 40.87 41.86 41.65 45.54
loc n 43.99 38.93 44.38 - 42.64 46.53
AP
gl y 39.68 38.60 42.07 36.22 38.10 38.64
loc y 36.50 31.72 37.04 - 33.56 35.66
gl n 39.16 35.44 38.65 38.07 39.43 39.95
loc n 38.89 27.96 38.15 - 36.47 39.42
Table 8. Local vs. global weighting schemas on
Na?ve Bayes.
The results are largely consistent both across the
datasets and across the classification methods.
Discriminative functions are almost always best in
their global variants; when applying them globally,
it is also advisable to weight test instances. In
contrast, the characteristic functions TSL1 and
TSL2 are usually better when applied locally. It is
also noteworthy that all the variants of TS fare
better when test instances are not weighted.
We believe that the good performance of the
global versions of MI, GR, and OR should be
explained by the fact that features they weight
highest are rare and likely to appear only in one
class so that using the same weight for all classes
does not cause confusion between them. It is also
beneficial to weight test instances globally,
because this guarantees that most features of a test
instance always have a non-zero weight. With
characteristic functions, however, highest weighted
are rather frequent features which are often present
in other classes as well. Using the same weight of
these features for all classes therefore fails to
differentiate classes from each other. Local TSL1
and TSL2 are more advantageous. Although
individual features they weight highest may be
mediocre separators, usually several such features
are given prominence within a class. Taken
collectively they appear to be able to successfully
discriminate a class from other classes.
An interesting observation is that the
combination of a local schema with weighted test
instances is very undesirable with all the functions.
The reason for this is that very often a test instance
has many features different from those in the
training class to which it is being compared.
Because of this, these features receive zero local
weights, which renders the representation of the
test instance extremely sparse.
Table 9 shows how the performance of the most
optimal settings for the six studied function
compares with the baseline (improvements on the
baseline are in bold).
kNN Na?ve Bayes
BNC AP BNC AP
MI 42.83 43.17 46.52 39.68
GR 48.88 43.44 43.79 38.60
OR 46.35 44.77 43.96 42.07
TS 40.90 42.12 41.86 38.07
TSL1 39.74 44.45 42.64 39.43
TSL2 41.29 43.70 46.53 39.95
baseline 41.67 41.33 45.55 39.16
Table 9. The most optimal settings for MI, GR,
OR, TS, TSL1 and TSL2 compared to the baselines.
All the functions often show superiority over the
baseline, except for TS which only once slightly
outperformed it. However, statistical significance
of the improvement was registered only for MI and
OR on the BNC data, using the kNN classifier,
which was 17% and 11% better than the baseline
correspondingly.
Comparing discriminative and characteristic
weighting functions we see that the supervised
variants of TS frequently perform on a par with
MI, GR, and OR. Particularly, TSL2 was the best
performer on Naive Bayes, BNC and the second
best on kNN, AP. We also see that the supervised
variants of TS very often surpass its original
unsupervised variant, but the improvement is
significant only for TSL2, on the BNC dataset
using Naive Bayes (at a=0.001).
5.4 Correlations between the functions
As was mentioned before, an informal inspection
of features emphasized by different functions
suggests that the discriminative functions tend to
give greater weights to rare features, while
characteristic ones to frequent features. In order to
see if this results in disagreement between them as
to the classifications of test instances, we measured
the extent to which classifications resulting from
MI, GR, OR, TSL1, and TSL2 overlap. For this
purpose, we calculated the Kappa coefficient for
all the 10 possible pairs of these functions. The
results are reported in Table 10.
GR OR TSL1 TSL2
MI 0.676
0.762
0.711
0.729
0.584
0.668
0.801
0.788
GR 0.873
0.855
0.483
0.617
0.571
0.703
OR 0.473
0.614
0.588
0.695
TSL1 0.658
0.721
Table 10. The agreement in classifications using
Na?ve Bayes between MI, GR, OR, TSL1, and
TSL2 on the BNC and AP datasets.
On results from both datasets, we see that the
highest agreement is indeed between MI, OR, and
GR and between TSL1 and TSL2. Interestingly,
there is also a relatively strong correlation between
classification resulting from using MI and TSL2.
The lowest agreement is between the
discriminative functions and TSL1.
kNN Na?ve Bayes
BNC AP BNC AP
MI 42.83 43.17 46.52 39.68
GR 48.88 43.44 43.79 38.60
OR 46.35 44.77 43.96 42.07
TSL1 39.74 44.45 42.64 39.43
TSL2 41.29 43.70 46.53 39.95
MI*TSL1 40.51 45.27 44.2 38.1
GR*TSL1 42.47 43.2 43.6 34.37
OR*TSL1 41.49 44.72 46.32 37.3
MI*TSL2 44.81 45.04 46.73 42.36
GR*TSL2 47.53 44.77 44.17 37.55
OR*TSL2 46.35 45.29 46.5 40.47
Table 11. Combinations of TSL1 and TSL2 with
MI, GR, and OR.
5.5 Combination of the functions
An obvious question is whether the effectiveness
of classifications can be increased by combining
the discriminative and the characteristic weights of
a feature given that both provide useful, but
different kinds of evidence about the correct class
label of test instances. To investigate this, we tried
combining each of the discriminative weights of a
feature with each of its supervised characteristic
weights in the following manner. First, both kinds
of weights were estimated from non-weighted
training data. Then they were applied one after the
other to the training data. During the test procedure,
test instances were weighted only with the global
weights. The results of these experiments are
shown in Table 11. Results for those combined
weighting methods which outperformed both of the
component functions are shown in bold.
Certain combined weighting procedures did
improve on both of the component methods.
However, none of them showed an improvement
over 2.5% on the best of the component weighting
methods (no significance for any of the
improvements could be established).
6 Conclusion
In the paper we studied several feature weighting
methods in application to automatic word
classification. Our particular focus was on the
differences between those weighting methods
which encourage features discriminating classes
from each other (odds ratio, gain ratio, mutual
information) and those which favor features that
best characterize classes (term strength).
We find that classification of words into flatly
organized classes is a very challenging task with
quite low upper and lower bounds, which suggests
that a considerable improvement on the baseline is
hard to achieve. We explicitly explored
parameterization of the weighting functions,
finding that the choice of certain parameters,
notably the application of local vs. global weights
and weighted vs. un-weighted test instances, is
critical for the performance of the classifier. We
find that the most optimal weighting procedure
often brings the performance of a classifier
significantly closer to the upper bound, achieving
up to 17% improvement on the baseline.
We find that discriminative and characteristic
weighting procedures are able to identify different
kinds of features useful for learning a classifier,
both consistently enhancing the classification
accuracy. These findings indicate that although
individual characteristic features may be less
powerful class separators, several such features,
taken collectively, are helpful in differentiating
between classes.
References
E. Alfonseca and S. Manandhar. 2002. Extending a
lexical ontology by a combination of
distributional semantics signatures. In
Proceedings of EKAW?02, pp.1-7.
M. Ciaramita. 2002. Boosting automatic lexical
acquisition with morphological information. In
Proceedings of the ACL-02 Workshop on
Unsupervised Lexical Acquisition. pp.17-25.
I. Dagan, L. Lee, and F. C. N. Pereira. 1997.
Similarity-based methods for word sense dis-
ambiguation. In Proceedings of ACL?97, pp. 56-63.
D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information re-
trieval. In Proceedings of ECML?98, pp.4-15.
D. Lin (1998) Automatic retrieval and clustering of
similar words. In Proceedings of COLING-
ACL?98, pp. 768-773.
D. Mladenic. 1998. Feature subset selection in text
learning. In Proceedings of ECML?98, pp.95-100.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
J.W. Wilbur and K. Sirotkin. 1992. The automatic
identification of stopwords. Journal of
Information Science, (18):45-55.
Y. Yang and J.O. Pedersen. 1997. A comparative
study on feature selection in text categorization.
Proceedings of ICML?97, pp. 412-420.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 49?56,
New York, June 2006. c?2006 Association for Computational Linguistics
Acquisition of Verb Entailment from Text
Viktor Pekar
Computational Linguistics Group
University of Wolverhampton
MB109 Stafford Street
Wolverhampton WV1 1SB, UK
v.pekar@wlv.ac.uk
Abstract
The study addresses the problem of auto-
matic acquisition of entailment relations
between verbs. While this task has much
in common with paraphrases acquisition
which aims to discover semantic equiva-
lence between verbs, the main challenge
of entailment acquisition is to capture
asymmetric, or directional, relations. Mo-
tivated by the intuition that it often under-
lies the local structure of coherent text, we
develop a method that discovers verb en-
tailment using evidence about discourse
relations between clauses available in a
parsed corpus. In comparison with earlier
work, the proposed method covers a much
wider range of verb entailment types and
learns the mapping between verbs with
highly varied argument structures.
1 Introduction
The entailment relations between verbs are a natural
language counterpart of the commonsense knowl-
edge that certain events and states give rise to other
events and states. For example, there is an entail-
ment relation between the verbs buy and belong,
which reflects the commonsense notion that if some-
one has bought an object, this object belongs to that
person.
A lexical resource encoding entailment can serve
as a useful tool in many tasks where automatic in-
ferencing over natural language text is required. In
Question Answering, it has been used to establish
that a certain sentence found in the corpus can serve
as a suitable, albeit implicit answer to a query (Cur-
tis et al, 2005), (Girju, 2003), (Moldovan and Rus,
2001). In Information Extraction, it can similarly
help to recognize relations between named entities
in cases when the entities in the text are linked by
a linguistic construction that entails a known extrac-
tion pattern, but not by the pattern itself. A lexical
entailment resource can contribute to information re-
trieval tasks via integration into a textual entailment
system that aims to recognize entailment between
two larger text fragments (Dagan et al, 2005).
Since entailment is known to systematically inter-
act with the discourse organization of text (Hobbs,
1985), an entailment resource can be of interest to
tasks that deal with structuring a set of individual
facts into coherent text. In Natural Language Gener-
ation (Reiter and Dale, 2000) and Multi-Document
Summarization (Barzilay et al, 2002) it can be used
to order sentences coming from multiple, possibly
unrelated sources to produce a coherent document.
The knowledge is essential for compiling answers
for procedural questions in a QA system, when sen-
tences containing relevant information are spread
across the corpus (Curtis et al, 2005).
The present paper is concerned with the prob-
lem of automatic acquisition of verb entailment from
text. In the next section we set the background
for the study by describing previous work. We
then define the goal of the study and describe our
method for verb entailment acquisition. After that
we present results of its experimental evaluation. Fi-
nally, we draw conclusions and outline future work.
2 Previous Work
The task of verb entailment acquisition appears to
have much in common with that of paraphrase ac-
quisition (Lin and Pantel, 2001), (Pang et al, 2003),
(Szpektor et al, 2004). In both tasks the goal is
to discover pairs of related verbs and identify map-
49
pings between their argument structures. The impor-
tant distinction is that while in a paraphrase the two
verbs are semantically equivalent, entailment is a di-
rectional, or asymmetric, relation: one verb entails
the other, but the converse does not hold. For ex-
ample, the verbs buy and purchase paraphrase each
other: either of them can substitute its counterpart in
most contexts without altering their meaning. The
verb buy entails own so that buy can be replaced with
own without introducing any contradicting content
into the original sentence. Replacing own with buy,
however, does convey new meaning.
To account for the asymmetric character of entail-
ment, a popular approach has been to use lexico-
syntactic patterns indicative of entailment. In
(Chklovski and Pantel, 2004) different types of se-
mantic relations between verbs are discovered us-
ing surface patterns (like ?X-ed by Y-ing? for en-
ablement1, which would match ?obtained by bor-
rowing?, for example) and assessing the strength
of asymmetric relations as mutual information be-
tween the two verbs. (Torisawa, 2003) collected
pairs of coordinated verbs, i.e. matching patterns
like ?X-ed and Y-ed?, and then estimated the prob-
ability of entailment using corpus counts. (Inui
et al, 2003) used a similar approach exploiting
causative expressions such as because, though, and
so. (Girju, 2003) extracted causal relations between
nouns like ?Earthquakes generate tsunami? by first
using lexico-syntactic patterns to collect relevant
data and then using a decision tree classifier to learn
the relations. Although these techniques have been
shown to achieve high precision, their reliance on
surface patterns limits their coverage in that they ad-
dress only those relations that are regularly made
explicit through concrete natural language expres-
sions, and only within sentences.
The method for noun entailment acquisition by
(Geffet and Dagan, 2005) is based on the idea of dis-
tributional inclusion, according to which one noun
is entailed by the other if the set of occurrence con-
texts of the former subsumes that of the latter. How-
ever, this approach is likely to pick only a particular
kind of verb entailment, that of troponymy (such as
1In (Chklovski and Pantel, 2004) enablement is defined to
be a relation where one event often, but not necessarily always,
gives rise to the other event, which coincides with our definition
of entailment (see Section 3).
march-walk) and overlook pairs where there is little
overlap in the occurrence patterns between the two
verbs.
In tasks involving recognition of relations be-
tween entities such as Question Answering and In-
formation Extraction, it is crucial to encode the
mapping between the argument structures of two
verbs. Pattern-matching often imposes restrictions
on the syntactic configurations in which the verbs
can appear in the corpus: the patterns employed by
(Chklovski and Pantel, 2004) and (Torisawa, 2003)
derive pairs of only those verbs that have identical
argument structures, and often only those that in-
volve a subject and a direct object. The method
for discovery of inference rules by (Lin and Pantel,
2001) obtains pairs of verbs with highly varied argu-
ment structures, which also do not have to be iden-
tical for the two verbs. While the inference rules
the method acquires seem to encompass pairs re-
lated by entailment, these pairs are not distinguished
from paraphrases and the direction of relation in
such pairs is not recognized.
To sum up, a major challenge in entailment ac-
quisition is the need for more generic methods that
would cover an unrestricted range of entailment
types and learn the mapping between verbs with
varied argument structures, eventually yielding re-
sources suitable for robust large-scale applications.
3 Verb Entailment
Verb entailment relations have been traditionally at-
tracting a lot of interest from lexical semantics re-
search and their various typologies have been pro-
posed (see, e.g., (Fellbaum, 1998)). In this study,
with the view of potential practical applications, we
adopt an operational definition of entailment. We
define it to be a semantic relation between verbs
where one verb, termed premise P , refers to event
Ep and at the same time implies event Eq, typically
denoted by the other verb, termed consequence Q.
The goal of verb entailment acquisition is then
to find two linguistic templates each consisting of
a verb and slots for its syntactic arguments. In the
pair, (1) the verbs are related in accordance with
our definition of entailment above, (2) there is a
mapping between the slots of the two templates and
(3) the direction of entailment is indicated explic-
50
itly. For example, in the template pair ?buy(obj:X)
? belong(subj:X)? the operator ? specifies that the
premise buy entails the consequence belong, and X
indicates a mapping between the object of buy and
the subject of belong, as in The company bought
shares. - The shares belong to the company.
As opposed to logical entailment, we do not re-
quire that verb entailment holds in all conceivable
contexts and view it as a relation that may be more
plausible in some contexts than others. For each
verb pair, we therefore wish to assign a score quan-
tifying the likelihood of its satisfying entailment in
some random context.
4 Approach
The key assumption behind our approach is that the
ability of a verb to imply an event typically denoted
by a different verb manifests itself in the regular co-
occurrence of the two verbs inside locally coherent
text. This assumption is not arbitrary: as discourse
investigations show (Asher and Lascarides, 2003),
(Hobbs, 1985), lexical entailment plays an impor-
tant role in determining the local structure of dis-
course. We expect this co-occurrence regularity to
be equally characteristic of any pair of verbs related
by entailment, regardless of is type and the syntactic
behavior of verbs.
The method consists of three major steps. First,
it identifies pairs of clauses that are related in the
local discourse. From related clauses, it then cre-
ates templates by extracting pairs of verbs along
with relevant information as to their syntactic be-
havior. Third, the method scores each verb pair
in terms of plausibility of entailment by measuring
how strongly the premise signals the appearance of
the consequence inside the text segment at hand. In
the following sections, we describe these steps in
more detail.
4.1 Identifying discourse-related clauses
We attempt to capture local discourse relatedness
between clauses by a combination of several surface
cues. In doing so, we do not build a full discourse
representation of text, nor do we try to identify the
type of particular rhetorical relations between sen-
tences, but rather identify pairs of clauses that are
likely to be discourse-related.
Textual proximity. We start by parsing the cor-
pus with a dependency parser (we use Connexor?s
FDG (Tapanainen and Ja?rvinen, 1997)), treating
every verb with its dependent constituents as a
clause. For two clauses to be discourse-related, we
require that they appear close to each other in the
text. Adjacency of sentences has been previously
used to model local coherence (Lapata, 2003). To
capture related clauses within larger text fragments,
we experiment with windows of text of various sizes
around a clause.
Paragraph boundaries. Since locally related
sentences tend to be grouped into paragraphs, we
further require that the two clauses appear within the
same paragraph.
Common event participant. Entity-based theo-
ries of discourse (e.g., (Grosz et al, 1995)) claim
that a coherent text segment tends to focus on a
specific entity. This intuition has been formalized
by (Barzilay and Lapata, 2005), who developed an
entity-based statistical representation of local dis-
course and showed its usefulness for estimating co-
herence between sentences. We also impose this as
a criterion for two clauses to be discourse-related:
their arguments need to refer to the same participant,
henceforth, anchor. We identify the anchor as the
same noun lemma appearing as an argument to the
verbs in both clauses, considering only subject, ob-
ject, and prepositional object arguments. The anchor
must not be a pronoun, since identical pronouns may
refer to different entities and making use of such cor-
respondences is likely to introduce noise.
4.2 Creating templates
Once relevant clauses have been identified, we cre-
ate pairs of syntactic templates, each consisting of a
verb and the label specifying the syntactic role the
anchor occupies near the verb. For example, given
a pair of clauses Mary bought a house. and The
house belongs to Mary., the method will extract two
pairs of templates: {buy(obj:X), belong(subj:X)}
and {buy(subj:X), belong(to:X).}
Before templates are constructed, we automati-
cally convert complex sentence parses to simpler,
but semantically equivalent ones so as to increase
the amount of usable data and reduce noise:
? Passive constructions are turned into active
51
ones: X was bought by Y ? Y bought X;
? Phrases with coordinated nouns and verbs are
decomposed: X bought A and B ? X bought A,
X bought B; X bought and sold A ? X bought A,
X sold A.
? Phrases with past and present participles are
turned into predicate structures: the group led
by A ? A leads the group; the group leading the
market ? the group leads the market.
The output of this step is V ? P ?Q, a set of pairs
of templates {p, q}, where p ? P is the premise,
consisting of the verb vp and rp ? the syntactic re-
lation between vp and the anchor, and q ? Q is the
consequence, consisting of the verb vq and rq ? its
syntactic relation to the anchor.
4.3 Measuring asymmetric association
To score the pairs for asymmetric association, we
use a procedure similar to the method by (Resnik,
1993) for learning selectional preferences of verbs.
Each template in a pair is tried as both a premise
and a consequence. We quantify the ?preference?
of the premise p for the consequence q as the con-
tribution of q to the amount of information p con-
tains about its consequences seen in the data. First,
we calculate Kullback-Leibler Divergence (Cover.
and Thomas, 1991) between two probability distrib-
utions, u ? the prior distribution of all consequences
in the data and w ? their posterior distribution given
p, thus measuring the information p contains about
its consequences:
Dp(u||w) =
?
n
u(x) log u(x)w(x) (1)
where u(x) = P (qx|p), w(x) = P (qx), and x ranges
over all consequences in the data. Then, the score for
template {p, q} expressing the association of q with
p is calculated as the proportion of q?s contribution
to Dp(u||w):
Score(p, q) = P (q|p) log P (q|p)P (p) Dp(u||w)
?1 (2)
In each pair we compare the scores in both di-
rections, taking the direction with the greater score
to indicate the most likely premise and consequence
and thus the direction of entailment.
5 Evaluation Design
5.1 Task
To evaluate the algorithm, we designed a recognition
task similar to that of pseudo-word disambiguation
(Schu?tze, 1992), (Dagan et al, 1999). The task was,
given a certain premise, to select its correct conse-
quence out of a pool with several artificially created
incorrect alternatives.
The advantages of this evaluation technique are
twofold. On the one hand, the task mimics many
possible practical applications of the entailment re-
source, such as sentence ordering, where, given a
sentence, it is necessary to identify among several
alternatives another sentence that either entails or is
entailed by the given sentence. On the other hand,
in comparison with manual evaluation of the direct
output of the system, it requires minimal human in-
volvement and makes it possible to conduct large-
scale experiments.
5.2 Data
The experimental material was created from the
BLLIP corpus, a collection of texts from the Wall
Street Journal (years 1987-89). We chose 15 tran-
sitive verbs with the greatest corpus frequency and
used a pilot run of our method to extract 1000
highest-scoring template pairs involving these verbs
as a premise. From them, we manually selected 129
template pairs that satisfied entailment.
For each of the 129 template pairs, four false con-
sequences were created. This was done by randomly
picking verbs with frequency comparable to that of
the verb of the correct consequence. A list of parsed
clauses from the BLLIP corpus was consulted to se-
lect the most typical syntactic configuration of each
of the four false verbs. The resulting five template
pairs, presented in a random order, constituted a test
item. Figure 1 illustrates such a test item.
The entailment acquisition method was evaluated
on entailment templates acquired from the British
National Corpus. Even though the two corpora are
quite different in style, we assume that the evalua-
tion allows conclusions to be drawn as to the relative
quality of performance of the methods under consid-
eration.
52
1* buy(subj:X,obj:Y)?own(subj:X,obj:Y)
2 buy(subj:X,obj:Y)?approve(subj:X,obj:Y)
3 buy(subj:X,obj:Y)?reach(subj:X,obj:Y)
4 buy(subj:X,obj:Y)?decline(subj:X,obj:Y)
5 buy(subj:X,obj:Y)?compare(obj:X,with:Y)
Figure 1: An item from the test dataset. The tem-
plate pair with the correct consequence is marked
by an asterisk.
5.3 Recognition algorithm
During evaluation, we tested the ability of the
method to select the correct consequence among the
five alternatives. Our entailment acquisition method
generates association scores for one-slot templates.
In order to score the double-slot templates in the
evaluation material, we used the following proce-
dure.
Given a double-slot template, we divide it into
two single-slot ones such that matching arguments
of the two verbs along with the verbs themselves
constitute a separate template. For example, ?buy
(subj:X, obj:Y) ? own (subj:X, obj:Y)? will be de-
composed into ?buy (subj:X) ? own (subj:X)? and
?buy (obj:Y) ? own (obj:Y)?. The scores of these
two templates are then looked up in the generated
database and averaged. In each test item, the five
alternatives are scored in this manner and the one
with the highest score was chosen as containing the
correct consequence.
The performance was measured in terms of accu-
racy, i.e. as the ratio of correct choices to the total
number of test items. Ties, i.e. cases when the cor-
rect consequence was assigned the same score as one
or more incorrect ones, contributed to the final accu-
racy measure proportionate to the number of tying
alternatives.
This experimental design corresponds to a ran-
dom baseline of 0.2, i.e. the expected accuracy when
selecting a consequence template randomly out of 5
alternatives.
6 Results and Discussion
We now present the results of the evaluation of the
method. In Section 6.1, we study its parameters and
determine the best configuration. In Section 6.2, we
compare its performance against that of human sub-
jects as well as that of two state-of-the-art lexical re-
sources: the verb entailment knowledge contained in
WordNet2.0 and the inference rules from the DIRT
database (Lin and Pantel, 2001).
6.1 Model parameters
We first examined the following parameters of the
model: the window size, the use of paragraph
boundaries, and the effect of the shared anchor on
the quality of the model.
6.1.1 Window size and paragraph boundaries
As was mentioned in Section 4.1, a free parame-
ter in our model is a threshold on the distance be-
tween two clauses, that we take as an indicator that
the clauses are discourse-related. To find an opti-
mal threshold, we experimented with windows of
1, 2 ... 25 clauses around a given clause, taking
clauses appearing within the window as potentially
related to the given one. We also looked at the ef-
fect paragraph boundaries have on the identification
of related clauses. Figure 2 shows two curves de-
picting the accuracy of the method as a function of
the window size: the first one describes performance
when paragraph boundaries are taken into account
(PAR) and the second one when they are ignored
(NO PAR).
Figure 2: Accuracy of the algorithm as a function
of window size, with and without paragraph bound-
aries used for delineating coherent text.
One can see that both curves rise fairly steeply up
to window size of around 7, indicating that many en-
tailment pairs are discovered when the two clauses
appear close to each other. The rise is the steepest
53
between windows of 1 and 3, suggesting that entail-
ment relations are most often explicated in clauses
appearing very close to each other.
PAR reaches its maximum at the window of 15,
where it levels off. Considering that 88% of para-
graphs in BNC contain 15 clauses or less, we take
this as an indication that a segment of text where
both a premise and its consequence are likely to be
found indeed roughly corresponds to a paragraph.
NO PAR?s maximum is at 10, then the accuracy
starts to decrease, suggesting that evidence found
deeper inside other paragraphs is misleading to our
model.
NO PAR performs consistently better than PAR
until it reaches its peak, i.e. when the window size is
less than 10. This seems to suggest that several ini-
tial and final clauses of adjacent paragraphs are also
likely to contain information useful to the model.
We tested the difference between the maxima
of PAR and NO PAR using the sign test, the non-
parametric equivalent of the paired t-test. The test
did not reveal any significance in the difference be-
tween their accuracies (6-, 7+, 116 ties: p = 1.000).
6.1.2 Common anchor
We further examined how the criterion of the
common anchor influenced the quality of the model.
We compared this model (ANCHOR) against the one
that did not require that two clauses share an anchor
(NO ANCHOR), i.e. considering only co-occurrence
of verbs concatenated with specific syntactic role la-
bels. Additionally, we included into the experiment
a model that looked at plain verbs co-occurring in-
side a context window (PLAIN). Figure 3 compares
the performance of these three models (paragraph
boundaries were taken into account in all of them).
Compared with ANCHOR, the other two models
achieve considerably worse accuracy scores. The
differences between the maximum of ANCHOR and
those of the other models are significant according
to the sign test (ANCHOR vs NO ANCHOR: 44+, 8-,
77 ties: p < 0.001; ANCHOR vs PLAIN: 44+, 10-,
75 ties: p < 0.001). Their maxima are also reached
sooner (at the window of 7) and thereafter their per-
formance quickly degrades. This indicates that the
common anchor criterion is very useful, especially
for locating related clauses at larger distances in the
text.
Figure 3: The effect of the common anchor on the
accuracy of the method.
The accuracy scores for NO ANCHOR and PLAIN
are very similar across all the window size settings.
It appears that the consistent co-occurrence of spe-
cific syntactic labels on two verbs gives no addi-
tional evidence about the verbs being related.
6.2 Human evaluation
Once the best parameter settings for the method
were found, we compared its performance against
human judges as well as the DIRT inference rules
and the verb entailment encoded in the WordNet 2.0
database.
Human judges. To elicit human judgments on
the evaluation data, we automatically converted the
templates into a natural language form using a num-
ber of simple rules to arrange words in the correct
grammatical order. In cases where an obligatory
syntactic position near a verb was missing, we sup-
plied the pronouns someone or something in that po-
sition. In each template pair, the premise was turned
into a statement, and the consequence into a ques-
tion. Figure 4 illustrates the result of converting the
test item from the previous example (Figure 1) into
the natural language form.
During the experiment, two judges were asked
to mark those statement-question pairs in each test
item, where, considering the statement, they could
answer the question affirmatively. The judges? deci-
sions coincided in 95 of 129 test items. The Kappa
statistic is ?=0.725, which provides some indication
about the upper bound of performance on this task.
54
X bought Y. After that:
1* Did X own Y?
2 Did X approve Y?
3 Did X reach Y?
4 Did X decline Y?
5 Did someone compare X with Y?
Figure 4: A test item from the test dataset. The cor-
rect consequence is marked by an asterisk.
DIRT. We also experimented with the inference
rules contained in the DIRT database (Lin and Pan-
tel, 2001). According to (Lin and Pantel, 2001), an
inference rule is a relation between two verbs which
are more loosely related than typical paraphrases,
but nonetheless can be useful for performing infer-
ences over natural language texts. We were inter-
ested to see how these inference rules perform on
the entailment recognition task.
For each dependency tree path (a graph linking a
verb with two slots for its arguments), DIRT con-
tains a list of the most similar tree paths along with
the similarity scores. To decide which is the most
likely consequence in each test item, we looked up
the DIRT database for the corresponding two depen-
dency tree paths. The template pair with the greatest
similarity was output as the correct answer.
WordNet. WordNet 2.0 contains manually en-
coded entailment relations between verb synsets,
which are labeled as ?cause?, ?troponymy?, or ?en-
tailment?. To identify the template pair satisfying
entailment in a test item, we checked whether the
two verbs in each pair are linked in WordNet in
terms of one of these three labels. Because Word-
Net does not encode the information as to the rela-
tive plausibility of relations, all template pairs where
verbs were linked in WordNet, were output as cor-
rect answers.
Figure 5 describes the accuracy scores achieved
by our entailment acquisition algorithm, the two hu-
man judges, DIRT and WordNet. For comparison
purposes, the random baseline is also shown.
Our algorithm outperformed WordNet by 0.38
and DIRT by 0.15. The improvement is significant
vs. WordNet (73+, 27-, 29 ties: p<0.001) as well as
vs. DIRT (37+, 20-, 72 ties: p=0.034).
We examined whether the improvement on DIRT
was due to the fact that DIRT had less extensive
Figure 5: A comparison of performance of the
proposed algorithm, WordNet, DIRT, two human
judges, and a random baseline.
coverage, encoding only verb pairs with similarity
above a certain threshold. We re-computed the ac-
curacy scores for the two methods, ignoring cases
where DIRT did not make any decision, i.e. where
the database contained none of the five verb pairs
of the test item. On the resulting 102 items, our
method was again at an advantage, 0.735 vs. 0.647,
but the significance of the difference could not be
established (21+, 12-, 69 ties: p=0.164).
The difference in the performance between our al-
gorithm and the human judges is quite large (0.103
vs. Judge 1 and 0.088 vs Judge 2), but significance
to the 0.05 level could not be found (vs. Judge 1:
17-, 29+, 83 ties: p=0.105; vs. Judge 2: 15-, 27+,
ties 87: p=0.09).
7 Conclusion
In this paper we proposed a novel method for au-
tomatic discovery of verb entailment relations from
text, a problem that is of potential benefit for many
NLP applications. The central assumption behind
the method is that verb entailment relations mani-
fest themselves in the regular co-occurrence of two
verbs inside locally coherent text. Our evaluation
has shown that this assumption provides a promis-
ing approach for discovery of verb entailment. The
method achieves good performance, demonstrating
a closer approximation to the human performance
than inference rules, constructed on the basis of dis-
tributional similarity between paths in parse trees.
A promising direction along which this work
55
can be extended is the augmentation of the current
algorithm with techniques for coreference reso-
lution. Coreference, nominal and pronominal, is
an important aspect of the linguistic realization of
local discourse structure, which our model did not
take into account. As the experimental evaluation
suggests, many verbs related by entailment occur
close to one another in the text. It is very likely that
many common event participants appearing in such
proximity are referred to by coreferential expres-
sions, and therefore noticeable improvement can
be expected from applying coreference resolution
to the corpus prior to learning entailment patterns
from it.
Acknowledgements
We are grateful to Nikiforos Karamanis and Mirella Lapata
as well as three anonymous reviewers for valuable comments
and suggestions. We thank Patrick Pantel and Dekang Lin for
making the DIRT database available for this study.
References
N. Asher and A. Lascarides. 2003. Logics of Conversation.
Cambridge University Press.
R. Barzilay and M. Lapata. 2005. Modeling local coherence:
an entity-based approach. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 141?148.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. Inferring
strategies for sentence ordering in multidocument summa-
rization. JAIR.
T. Chklovski and P. Pantel. 2004. VERBOCEAN: Mining the
web for fine-grained semantic verb relations. In In Proceed-
ings of Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?04).
T.M. Cover. and J.A. Thomas. 1991. Elements of Information
Theory. Wiley-Interscience.
J. Curtis, G. Matthews, and D. Baxter. 2005. On the effective
use of cyc in a question answering system. In Proceedings
the IJCAI?05 Workshop on Knowledge and Reasoning for
Answering Questions.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-based mod-
els of cooccurrence probabilities. Machine Learning, 34(1-
3):43?69.
I. Dagan, O. Glickman, and B. Magnini. 2005. The pascal
recognising textual entailment challenge. In PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
C. Fellbaum, 1998. WordNet: An Electronic Lexical Database,
chapter Semantic network of English verbs. MIT Press.
M. Geffet and I. Dagan. 2005. The distributional inclusion hy-
potheses and lexical entailment. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 107?114.
R. Girju. 2003. Automatic detection of causal relations for
question answering. In Proceedings of the ACL?03 Work-
shop on ?Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond?.
B. Grosz, A. Joshi, and S.Weinstein. 1995. Centering : a frame-
work for modeling the local coherence of discourse. Com-
putational Linguistics, 21(2):203?225.
J.R. Hobbs. 1985. On the coherence and structure of discourse.
Technical Report CSLI-85-37, Center for the Study of Lan-
guage and Information.
T. Inui, K.Inui, and Y.Matsumoto. 2003. What kinds and
amounts of causal knowledge can be acquired from text by
using connective markers as clues? In Proceedings of the
6th International Conference on Discovery Science, pages
180?193.
M. Lapata. 2003. Probabilistic text structuring: experiments
with sentence ordering. In Proceedings of the 41rd Annual
Meeting of the Association for Computational Linguistics
(ACL?03), pages 545?552.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question answering. Natural Language Engineering,
7(4):343?360.
D. Moldovan and V. Rus. 2001. Logic form transformation
of WordNet and its applicability to question answering. In
Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL?01).
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: extracting paraphrases
and generating new sentences. In Proceedings of HLT-
NAACL?2003.
E. Reiter and R. Dale. 2000. Building Natural Language Gen-
eration Systems. Cambidge University Press.
P. Resnik. 1993. Selection and Information: A Class-Based
Approach to Lexical Relationships. Ph.D. thesis, University
of Pennsylvania.
H. Schu?tze. 1992. Context space. In Fall Symposium on Prob-
abilistic Approaches to Natural Language, pages 113?120.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004. Scaling
web-based acquisition of entailment relations. In Proceed-
ings of Empirical Methods in Natural Language Processing
(EMNLP?04).
P. Tapanainen and T. Ja?rvinen. 1997. A non-projective depen-
dency parser. In Proceedings of the 5th Conference on Ap-
plied Natural Language Processing, pages 64?71.
K. Torisawa, 2003. Questions and Answers: Theoretical
and Applied Perspectives, chapter An unsupervised learning
method for commonsensical inference rules on events. Uni-
versity of Utrecht.
56
Linguistic Preprocessing for Distributional Classification of Words
Viktor PEKAR
CLG, University of Wolverhampton
MB 114, Stafford Road
Wolverhampton, UK, WV1 1SB
v.pekar@wlv.ac.uk
Abstract
The paper is concerned with automatic
classification of new lexical items into
synonymic sets on the basis of their co-
occurrence data obtained from a corpus. Our
goal is to examine the impact that different
types of linguistic preprocessing of the co-
occurrence material have on the classification
accuracy. The paper comparatively studies
several preprocessing techniques frequently
used for this and similar tasks and makes
conclusions about their relative merits. We
find that a carefully chosen preprocessing
procedure achieves a relative effectiveness
improvement of up to 88% depending on the
classification method in comparison to the
window-based context delineation, along with
using much smaller feature space.
1 Introduction
With the fast development of text mining
technologies, automated management of lexical
resources is presently an important research issue.
A particular text mining task often requires a
lexical database (e.g., a thesaurus, dictionary, or a
terminology) with a specific size, topic coverage,
and granularity of encoded meaning. That is why a
lot of recent NLP and AI research has been
focusing on finding ways to speedily build or
extend a lexical resource ad hoc for an application.
One attractive idea to address this problem is to
elicit the meanings of new words automatically
from a corpus relevant to the application domain.
To do this, many approaches to lexical acquisition
employ the distributional model of word meaning
induced from the distribution of the word across
various lexical contexts of its occurrence found in
the corpus. The approach is now being actively
explored for a wide range of semantics-related
tasks including automatic construction of thesauri
(Lin, 1998; Caraballo, 1999), their enrichment
(Alfonseca and Manandhar, 2002; Pekar and Staab,
2002), acquisition of bilingual lexica from non-
aligned (Kay and R?scheisen, 1993) and non-
parallel corpora (Fung and Yee, 1998), learning of
information extraction patterns from un-annotated
text (Riloff and Schmelzenbach, 1998).
However, because of irregularities in corpus
data, corpus statistics cannot guarantee optimal
performance, notably for rare lexical items. In
order to improve robustness, recent research has
attempted a variety of ways to incorporate external
knowledge into the distributional model. In this
paper we investigate the impact produced by the
introduction of different types of linguistic
knowledge into the model.
Linguistic knowledge, i.e., the knowledge about
linguistically relevant units of text and relations
holding between them, is a particularly convenient
way to enhance the distributional model. On the
one hand, although describing the ?surface?
properties of the language, linguistic notions
contain conceptual information about the units of
text they describe. It is therefore reasonable to
expect that the linguistic analysis of the context of
a word yields additional evidence about its
meaning. On the other hand, linguistic knowledge
is relatively easy to obtain: linguistic analyzers
(lemmatizers, PoS-taggers, parsers, etc) do not
require expensive hand-encoded resources, their
application is not restricted to particular domains,
and their performance is not dependent on the
amount of the textual data. All these characteristics
fit very well with the strengths of the distributional
approach: while enhancing it with external
knowledge, linguistic analyzers do not limit its
coverage and portability.
This or that kind of linguistic preprocessing is
carried out in many previous applications of the
approach. However, these studies seldom motivate
the choice of a particular preprocessing procedure,
concentrating rather on optimization of other
parameters of the methodology. Very few studies
exist that analyze and compare different techniques
for linguistically motivated extraction of
distributional data. The goal of this paper is to
exploire in detail a range of variables in the
morphological and syntactic processing of the
context information and reveal the merits and
drawbacks of their particular settings.
The outline of the paper is as follows. Section 2
describes the preprocessing methods under study.
Section 3 describes the settings for their empirical
evaluation. Section 4 details the experimental
results. Section 5 discusses related work. Section 6
summarizes the results and presents the
conclusions from the study.
2 Types of Linguistic Preprocessing
In order to prepare a machine-processable
representation of a word from particular instances
of its occurrence, one needs to decide on, firstly,
what is to be understood by the context of a word?s
use, and, secondly, which elements of that context
will constitute distributional features. A
straightforward decision is to take a certain number
of words or characters around the target word to be
its occurrence context, and all uninterrupted letter
sequences within this delineation to be its features.
However, one may ask the question if elements of
the text most indicative of the target word?s
meaning can be better identified by looking at the
linguistic analysis of the text.
In this paper we empirically study the following
types of linguistic preprocessing.
1. The use of original word forms vs. their stems
vs. their lemmas as distributional features. It is not
evident what kind of morphological preprocessing
of context words should be performed, if at all.
Stemming of context words can be expected to
help better abstract from their particular
occurrences and to emphasize their invariable
meaning. It also relaxes the stochastic dependence
between features and reduces the dimensionality of
the representations. In addition to these
advantages, lemmatization also avoids confusing
words with similar stems (e.g., car vs. care, ski vs.
sky, aide vs. aid). On the other hand,
morphological preprocessing cannot be error-free
and it may seem safer to simply use the original
word forms and preserve their intended meaning as
much as possible. In text categorization, stemming
has not been conclusively shown to improve
effectiveness in comparison to using original word
forms, but it is usually adopted for the sake of
shrinking the dimensionality of the feature space
(Sebastiani, 2002). Here we will examine both the
effectiveness and the dimensionality reduction that
stemming and lemmatization of context words
bring about.
2. Morphological decomposition of context
words. A morpheme is the smallest meaningful
unit of the language. Therefore decomposing
context words into morphemes and using them as
features may eventually provide more fine-grained
evidence about the target word. Particularly, we
hypothesize that using roots of context words
rather than their stems or lemmas will highlight
lexical similarities between context words
belonging to different parts of speech (e.g.,
different, difference, differentiate) or differing only
in affixes (e.g., build  and rebuild ).
3. Different syntactically motivated methods of
delimiting the context of the word?s use. The
lexical context permitting occurrence of the target
word consists of words and phrases whose
meanings have something to do with the meaning
of the target word. Therefore, given that syntactic
dependencies between words presuppose certain
semantic relations between them, one can expect
syntactic parsing to point to most useful context
words. The questions we seek answers to are: Are
syntactically related words indeed more revealing
about the meaning of the target word than spatially
adjacent ones? Which types of syntactic
dependencies should be preferred for delimiting
the context of a target word?s occurrence?
4. Filtering out rare context words. The typical
practice of preprocessing distributional data is to
remove rare word co-occurrences, thus aiming to
reduce noise from idiosyncratic word uses and
linguistic processing errors and at the same time
form more compact word representations (e.g.,
Grefenstette, 1993; Ciaramita, 2002). On the other
hand, even single occurrence word pairs make up a
very large portion of the data and many of them are
clearly meaningful. We compare the quality of the
distributional representations with and without
context words that occurred only once with the
target word.
3 Evaluation
3.1 Experimental Task
The preprocessing techniques were evaluated on
the task of automatic classification of nouns into
semantic classes. The evaluation of each
preprocessing method consisted in the following.
A set of nouns N each belonging to one semantic
class c?C was randomly split into ten equal parts.
Co-occurrence data on the nouns was collected and
preprocessed using a particular method under
analysis. Then each noun n?N was represented as
a vector of distributional features: nr = (vn,1, vn,2, ?
vn,i), where the values of the features are the
frequencies of n occurring in the lexical context
corresponding to v. At each experimental run, one
of the ten subsets of the nouns was used as the test
data and the remaining ones as the train data. The
reported effectiveness measures are microaveraged
precision scores averaged over the ten runs. The
statistical significance of differences between
performance of particular preprocessing methods
reported below was estimated by means of the one-
tailed paired t-test.
3.2 Data
The set of nouns each provided with a class
label to be used in the experiments was obtained as
follows. We first extracted verb-noun
dependencies from the British National Corpus,
where nouns are either direct or prepositional
objects to verbs. Each noun that occurred with
more than 20 different verbs was placed into a
semantic class corresponding to the WordNet
synset of its most frequent sense. The resulting
classes with less than 2 nouns were discarded.
Thus we were left with 101 classes, each
containing 2 or 3 nouns.
3.3 Classification Methods
Two classification algorithms were used in the
study: Na?ve Bayes and Rocchio, which were
previously shown to be quite robust on highly
dimensional representations on tasks including
word classification (e.g., Tokunaga et al, 1997,
Ciaramita, 2002).
The Na?ve Bayes algorithm classifies a test
instance n by finding a class c that maximizes
p(c|nr ). Assuming independence between features,
the goal of the algorithm can be stated as:
)|()(maxarg)|(maxarg i
nv
iiii cvpcpncp ?
?
?
where p(ci) and p(v|ci) are estimated during the
training process from the corpus data.
The Na?ve Bayes classifier was the binary
independence model, which estimates p(v|ci)
assuming the binomial distribution of features
across classes. In order to introduce the
information inherent in the frequencies of features
into the model all input probabilities were
calculated from the real values of features, as
suggested in (Lewis, 1998).
The Rocchio classifier builds a vector for each
class c?C from the vectors of training instances.
The value of jth feature in this vector is computed as:
||||
,,
, c
v
c
v
v ci jici jijc
?? ?? ?-?= gb
where the first part of the equation is the average
value of the feature in the positive training
examples of the class, and the second part is its
average value in the negative examples. The
parameters b and g control the influence of the
positive and negative examples on the computed
value, usually set to 16 and 4, correspondingly.
Once vectors for all classes are built, a test instance
is classified by measuring the similarity between
its vector and the vector of each class and
assigning it to the class with the greatest similarity.
In this study, all features of the nouns were
modified by the TFIDF weight before the training.
4 Results
4.1 Syntactic Contexts
The context of the target word?s occurrence can
be delimited syntactically. In this view, each
context word is a word that enters in a syntactic
dependency relation with the target word, being
either the head or the modifier in the dependency.
For example, in the sentence She bought a nice hat
context words for hat are bought (the head of the
predicate-object relation) and nice (the attributive
modifier).
We group typical syntactic relations of a noun
together based on general semantic relations they
indicate. We define five semantic types of
distributional features of nouns that can be
extracted by looking at the dependencies they
participate in.
A. verbs in the active form, to which the target
nouns are subjects (e.g., the committee
discussed (the issue), the passengers  got on (a
bus), etc);
B. active verbs, to which the target nouns are
direct or prepositional objects (e.g., hold  a
meeting; depend on a friend); passive verbs to
which the nouns are subjects (e.g., the meeting
is held);
C. adjectives and nouns used as attributes or
predicatives to the target nouns (e.g., a tall
building, the building is tall; amateur actor,
the actor is an amateur);
D. prepositional phrases, where the target nouns
are heads (e.g., the box in the room); we
consider three possibilities to construct
distributional features from such a
dependency: with the preposition (in_room,
D1), without it (room, D2), and creating to
separate features for the preposition and the
noun (in and room, D3).
E. prepositional phrases, where the target nouns
are modifiers (the ball in the box); as with type
D, three subtypes are identified: E1 (ball_in ),
E2 (ball), and E3 (ball and in);
We compare these feature types to each other
and to features extracted by means of the window-
based context delineation. The latter were collected
by going over occurrences of each noun with a
window of three words around it. This particular
size of the context window was chosen following
findings of a number of studies indicating that
small context windows, i.e. 2-3 words, best capture
the semantic similarity between words (e.g., Levy
et al, 1998; Ciaramita, 2002). Thereby, a common
stoplist was used to remove too general context
words. All the context words experimented with at
this stage were lemmatized; those, which co-
occurred with the target noun only once, were
removed.
We first present the results of evaluation of
different types of features formed from
prepositional phrases involving target nouns (see
Table 1).
Na?ve Bayes Rocchio #dim
D1  23.405  16.574 11271
D2  18.571  13.879 5876
D3  19.095  13.879 5911
E1  28.166  17.619 7642
E2  25.31  13.067 3433
E3  26.714  13.067 3469
Table 1. Different kinds of features derived from
prepositional phrases involving target nouns.
On both classifiers and for both types D and E,
the performance is noticeably higher when the
collocation of the noun with the preposition is used
as one single feature (D1 and E1). Using only the
nouns as separate features decreases classification
accuracy. Adding the prepositions to them as
individual features improves the performance very
slightly on Na?ve Bayes, but has no influence on
the performance of Rocchio. Comparing types D1
and E1, we see that D1 is clearly more effective,
particularly on Na?ve Bayes, and uses around 30%
less features than E1.
NB Rocchio #dim
A 21.052 15.075 1533
B 34.88 29.889 4039
C 36.357 28.242 4607
D1 23.405 16.574 11271
E1 28.166 17.619 7642
Window 38.261 18.767 35902
Table 2. Syntactically-defined types of features.
Table 2 describes the results of the evaluation of
all the five feature types described above. On
Na?ve Bayes, each of the syntactically-defined
types yields performance inferior to that of the
window-based features. On Rocchio, window-
based is much worse than B and C, but is
comparable to A, D1 and E1. Looking at the
dimensionality of the feature space each method
produces, we see that the window-based features
are much more numerous than any of the
syntactically-defined ones, although collected from
the same corpus. The much larger feature however
space does not yield a proportional increase in
classification accuracy. For example, there are
around seven times less type C features than
window-based ones, but they are only 1.9% less
effective on Na?ve Bayes and significantly more
effective on Rocchio.
Among the syntactically-defined features,
types B and C perform equally well, no statistical
significance between their performances was found
on either NB or Rocchio. In fact, the ranking of the
feature types wrt their performance is the same for
both classifiers: types B and C trail E1 by a large
margin, which is followed by D1, type A being the
worst performer. The results so far suggest that
adjectives and verbs near which target nouns are
used as objects provide the best evidence about the
target nouns? meaning.
We further tried collapsing different types of
features together. In doing so, we appended a tag to
each feature describing its type so as to avoid
confusing context words linked by different
syntactic relations to the target noun (see Table 3).
The best result was achieved by combining all the
five syntactic feature types, clearly outperforming
the window-based context delineation on both
Na?ve Bayes (26% improvement, p<0.05) and
Rocchio (88% improvement, p<0.001) and still
using 20% smaller feature space. The combination
of B and C produced only slightly worse results (the
differences not significant for either classifiers), but
using over 3 times smaller feature space.
NB Rocchio #dim
B+C 43.071 35.426 8646
B+C+D1+E1 47.357 36.469 27559
A+B+C+D1+E1 48.309 36.829 29092
D1+E1 30.095 22.26 18913
Window 38.261 18.767 35902
Table 3. Combinations of syntactically-defined
feature types.
4.2 Original word forms vs. stems vs. lemmas
We next looked at the performance resulting
from stemming and lemmatization of context
words. Since morphological preprocessing is likely
to differently affect nouns, verbs, and adjectives,
we study them on data of types B (verbs), C
(adjectives), and the combination of D1 and E1
(nouns) from the previous experiment. Stemming
was carried out using the Porter stemmer.
Lemmatization was performed using a pattern-
matching algorithm which operates on PoS-tagged
text and consults the WordNet database for
exceptions. As before, context words that occurred
only once with a target noun were discarded. Table
4 describes the results of these experiments.
NB Rocchio #dim
Verbs
Original 35.333 31.648 9906
Stem 35.357 27.665 7506
Lemma 34.88 29.889 4039
Adjectives
Original 37.309 28.911 4765
Stem 36.833 29.168 4390
Lemma 36.357 28.242 4607
Nouns
Original 28.69 23.076 19628
Stem 29.19 22.176 19141
Lemma 20.976 22.26 15642
Table 4. Morphological preprocessing of verbs,
adjectives, and nouns.
There is very little difference in effectiveness
between these three methods (except for
lemmatized nouns on NB). As a rule, the
difference between them is never greater than 1%.
In terms of the size of feature space, lemmatization
is most advisable for verbs (32% reduction of
feature space compared with the original verb
forms), which is not surprising since the verb is the
most infected part of speech in English. The
feature space reduction for nouns was around 25%.
Least reduction of feature space occurs when
applying lemmatization to adjectives, which inflect
only for degrees of comparison.
4.3 Morphological decomposition
We further tried constructing features for a target
noun on the basis of morphological analysis of
words occurring in its context. As in the
experiments with stemming and lemmatization, in
order to take into account morphological
differences between parts of speech, the effects of
morphological decomposition of context words
was studied on the distributional data of types B
(verbs), C (adjectives), and D1+E1 (nouns).
The decomposition of words into morphemes
was carried out as follows. From ?Merriam-
Webster's Dictionary of Prefixes, Suffixes, and
Combining Forms?1 we extracted a list of 12
verbal, 59 adjectival and 138 nounal suffixes, as
well as 80 prefixes, ignoring affixes consisting of
only one character. All suffixes for a particular
part-of-speech and all prefixes were sorted
according to their character length. First, all
context words were lemmatized. Then, examining
the part-of-speech of the context word, presence of
each affix with it was checked by simple string
matching, starting from the top of the corres-
ponding array of affixes. For each word, only one
prefix and only one suffix was matched. In this
way, every word was broken down into maximum
three morphemes: the root, a prefix and a suffix.
Two kinds of features were experimented with:
one where features corresponded to the roots of the
context words and one where all morphemes of the
context word (i.e., the root, prefix and suffix)
formed separate features. When combining features
created from context words belonging to different
parts-of-speech, no tags were used in order to map
roots of cognate words to the same feature. The
results of these experiments are shown in Table 5.
roots roots+
affixes
lemmas
Na?ve bayes
B 37.261 35.833 34.88
C 38.738 39.214 36.357
D1+E1 29.119 25.785 30.095
B+C 43.976 42.071 43.547
B+C+D1+E1 46.88 45.452 48.309
Rocchio
B 24.241 24.061 29.889
C 27.803 27.901 28.242
D1+E1 13.267 12.87 22.26
B+C 28.747 28.019 35.426
B+C+D1+E1 28.863 30.752 36.469
Table 5. Distributional features derived from the
morphological analysis of context words.
On Na?ve Bayes, using only roots increases the
classification accuracy for B, C, and B+C
compared to the use of lemmas. The improvement,
however, is not significant. Inclusion of affixes
does not produce any perceptible effect on the
performance. In all other cases and when the
Rocchio classifier is used, decomposition of words
into morphemes consistently decreases
performance compared to the use of their lemmas.
These results seem to suggest that the union of
the root with the affixes constitutes the most
                                                
1 Available at www.spellingbee.com/pre_suf_comb.pdf
optimal ?container? for distributional information.
Decomposition of words into morphemes often
causes loss of a part of this information. It seems
there are few affixes with the meaning so abstract
that they can be safely discarded.
4.4 Filtering out rare context words
To study the effect of removing singleton
context words, we compared the quality of
classifications with and without them. The results
are shown in Table 6.
NB Rocchio #dim
Without singletons
B 34.88 29.889 4039
C 36.357 28.242 4607
B+C 43.547 35.426 8646
A+B+C+D1+E1 48.309 36.829 29092
Window 38.261 18.767 35902
With singletons
B 38.361 25.164 14024
C 39.261 28.387 9898
B+C 45. 29.535 23922
A+B+C+D1+E1 44. 25.31 98703
Window 41.142 19.037 94606
Table 4: The effect of removing rare context words.
The results do not permit making any
conclusions as to the enhanced effectiveness
resulting from discarding rare co-occurrences.
Discarding singletons, however, does considerably
reduce the feature space. The dimensionality
reduction is especially large for the datasets
involving types B, D1 and E1, where each feature is
a free collocation of a noun or a verb with a
preposition, whose multiple occurrences are much
less likely than multiple occurrences of an
individual context word.
5 Related work
A number of previous studies compared different
kinds of morphological and syntactic
preprocessing performed before inducing a co-
occurrence model of word meaning.
Grefenstette (1993) studied two context
delineation methods of English nouns: the
window-based and the syntactic, whereby all the
different types of syntactic dependencies of the
nouns were used in the same feature space. He
found that the syntactic technique produced better
results for frequent nouns, while less frequent
nouns were more effectively modeled by the
windowing technique. He explained these results
by the fact that the syntactic technique extracts
much fewer albeit more useful features and the
small number of features extracted for rare nouns
is not sufficient for representing their distributional
behavior.
Alfonseca and Manandhar (2002) compared
different types of syntactic dependencies of a noun
as well as its ?topic signature?, i.e. the features
collected by taking the entire sentence as the
context of its occurrence, in terms of their
usefulness for the construction of its distributional
representation. They found that the best
effectiveness is achieved when using a
combination of the topic signature with the ?object
signature? (a list of verbs and prepositions to
which the target noun is used as an argument) and
the ?subject signature? (a list of verbs to which the
noun is used as a subject). The ?modifier
signature? containing co-occurring adjectives and
determiners produced the worst results.
Pado and Lapata (2003) investigated different
possibilities to delimit the context of a target word
by considering the syntactic parse of the sentence.
They examined the informativeness of features
arising from using the window-based context
delineation, considering the sum of dependencies
the target word is involved in, and considering the
entire argument structure of a verb as the context
of the target word, so that, e.g. an object can be a
feature for a subject of that verb. Their study
discovered that indirect syntactic relations within
an argument structure of a verb generally yield
better results than using only direct syntactic
dependencies or the windowing technique.
Ciaramita (2002) looked at how the performance
of automatic classifiers on the word classification
task is affected by the decomposition of target
words into morphologically relevant features. He
found that the use of suffixes and prefixes of target
nouns is indeed more advantageous, but this was
true only when classifying words into large word
classes. These classes are formed on the basis of
quite general semantic distinctions, which are often
reflected in the meanings of their affixes. In addition
to that, the classification method used involved
feature selection, which ensured that useless
features resulting from semantically empty affixes
and errors of the morphological decomposition did
not harm the classification accuracy.
6 Conclusion
In this study we examined the impact which
linguistic preprocessing of distributional data
produce on the effectiveness and efficiency of
semantic classification of nouns.
Our study extends previous work along the
following lines. First, we have compared different
types of syntactic dependencies of the target noun
in terms of the informativeness of the distributional
features constructed from them. We find that the
most useful dependencies are the adjectives and
nouns used as attributes to the target nouns and the
verbs near which the target nouns are used as
direct or prepositional objects. The most effective
representation overall is obtained when using all
the syntactic dependencies of the noun. We find
that it is clearly more advantageous than the
windowing technique both in terms of
effectiveness and efficiency. The combination of
the attribute and object dependencies also produces
very good classification accuracy, which is only
insignificantly worse than that of the combination
of all the dependency types, while using several
times more compact feature space.
We further looked at the influence of stemming
and lemmatization of context words on the
performance. The study did not reveal any
considerable differences in effectiveness obtained
by stemming or lemmatization of context words
versus the use of their original forms. Lemma-
tization, however, allows to achieve the greatest
reduction of the feature space. Similarly, the
removal of rare word co-occurrences from the
training data could not be shown to consistently
improve effectiveness, but was very beneficial in
terms of dimensionality reduction, notably for
features corresponding to word collocations.
Finally, we examined whether morphological
decomposition of context words helps to obtain
more informative features, but found that
indiscriminative decomposition of all context
words into morphemes and using them as separate
features actually more often decreases performance
rather than increases it. These results seem to
indicate that morphological analysis of context
words should be accompanied by some feature
selection procedure, which would identify those
affixes which are too general and can be safely
stripped off and those which are sufficiently
specific and whose unity with the root best
captures relevant context information.
7 Acknowledgements
The research was supported by the Russian
Foundation Basic Research grant #03-06-80008.
We thank our colleauges Steffen Staab and
Andreas Hotho for fruitful discussions during the
work on this paper.
References
E. Alfonseca and S. Manandhar. 2002. Extending a
lexical ontology by a combination of
distributional semantics signatures. In
Proceedings of 13th International Conference on
Knowledge Engineering and Knowledge
Management, pages 1-7.
S. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In
Proceedings of ACL?99, pages 120-126.
M. Ciaramita. 2003. Boosting automatic  lexical
acquisition with morphological information.  In
Proceedings of the ACL?02 Workshop on
Unsupervised Lexical Acquisition, pages 17-25.
P. Fung and L.Y. Yee. An IR approach for
translating new words from nonparallel,
comparable texts In Proceedings of COLING-
ACL?98, pages 414-420.
G. Grefenstette. 1993. Evaluation techniques for
automatic semantic extraction: comparing
syntactic and window based approaches. In
Proceedings of the SIGLEX Workshop on
Acquisition of Lexical Knowledge from Text,
Columbus, Ohio.
M. Kay and M. R?scheisen. 1993. Text-translation
alignment. Computational Linguistics.
19(1):121-142.
J. Levy, J. Bullinaria, and M. Patel. 1998.
Explorations in the derivation of word co-
occurrence statistics. South Pacific Journal of
Psychology, 10(1), 99-111.
D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information re-
trieval. In Proceedings of ECML?98, pages 4-15.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the COLING-
ACL?98, pages 768-773.
S. Pado and M. Lapata. 2003. Constructing
semantic space models from parsed corpora. In
Proceedings of ACL?03, pages 128-135.
V. Pekar and S. Staab. 2002. Factoring the
structure of a taxonomy into a semantic
classification decision. In: Proceedings of
COLING?02, pages 786-792.
E. Riloff and M. Schmelzenbach. 1998. An
empirical approach to conceptual case frame
acquisition. In: Proceedings of the 6th Workshop
on Very Large Corpora.
F. Sebastiani. 2002. Machine learning in
automated text categorization. ACM Computing
Surveys, 34(1): 1-47.
T. Tokunaga, A. Fujii, M. Iwayama, N. Sakurai,
and H. Tanaka. 1997. Extending a thesaurus by
classifying words. In Proceedings of the ACL-
EACL Workshop on Automatic Information
Extraction and Building of Lexical Semantic
Resources, pages 16-21.
Taxonomy learning ? factoring the structure of a taxonomy into a 
semantic classification decision 
 
Viktor PEKAR 
Bashkir State University  
Ufa, Russia, 450000 
vpekar@ufanet.ru 
Steffen STAAB 
Institute AIFB, University of Karlsruhe 
http://www.aifb.uni-karlsruhe.de/WBS  
& Learning Lab Lower Saxony 
http://www.learninglab.de 
 
Abstract  
The paper examines different possibilities 
to take advantage of the taxonomic or-
ganization of a thesaurus to improve the 
accuracy of classifying new words into its 
classes. The results of the study demon-
strate that taxonomic similarity between 
nearest neighbors, in addition to their dis-
tributional similarity to the new word, 
may be useful evidence on which classifi-
cation decision can be based. 
1. Introduction 
Machine-readable thesauri are now an indispen-
sable part for a wide range of NLP applications 
such as information extraction or semantics-
sensitive information retrieval. Since their man-
ual construction is very expensive, a lot of recent 
NLP research has been aiming to develop ways 
to automatically acquire lexical knowledge from 
corpus data. 
In this paper we address the problem of large-
scale augmenting a thesaurus with new lexical 
items. The specifics of the task are a big number 
of classes into which new words need to be clas-
sified and hence a lot of poorly predictable se-
mantic distinctions that have to be taken into 
account. For this reason, knowledge-poor ap-
proaches such as the distributional approach are 
particularly suited for this task. Its previous ap-
plications (e.g., Grefenstette 1993, Hearst and 
Schuetze 1993, Takunaga et al1997, Lin 1998, 
Caraballo 1999) demonstrated that cooccurrence 
statistics on a target word is often sufficient for 
its automatical classification into one of numer-
ous classes such as synsets of WordNet. 
Distributional techniques, however, are poorly 
applicable to rare words, i.e., those words for 
which a corpus does not contain enough cooc-
currence data to judge about their meaning. Such 
words are the primary concern of many practical 
NLP applications: as a rule, they are semanti-
cally focused words and carry a lot of important 
information. If one has to do with a specific 
domain of lexicon, sparse data is a problem par-
ticularly difficult to overcome. 
The major challenge for the application of the 
distributional approach in this area is, therefore, 
the development of ways to minimize the 
amount of corpus data required to successfully 
carry out a task. In this study we focus on opti-
mization possibilities of an important phase in 
the process of automatically augmenting a the-
saurus ? the classification algorithm. The main 
hypothesis we test here is that the accuracy of 
semantic classification may be improved by 
taking advantage of information about taxo-
nomic relations between word classes contained 
in a thesaurus. 
On the example of a domain-specific thesaurus 
we compare the performance of three state-of-
the-art classifiers which presume flat organiza-
tion of thesaurus classes and two classification 
algorithms, which make use of taxonomic or-
ganization of the thesaurus: the "tree descend-
ing" and the "tree ascending" algorithms. We 
find that a version of the tree ascending algo-
rithm, though not improving on other methods 
overall, is much better at choosing a supercon-
cept for the correct class of the new word. We 
then propose to use this algorithm to first narrow 
down the search space and then apply the kNN 
method to determine the correct class among 
fewer candidates. 
The paper is organized as follows. Sections 2 
and 3 describe the classification algorithms un-
der study. Section 4 describes the settings and 
data of the experiments. Section 5 details the 
evaluation method. Section 6 presents the results 
of the experiments. Section 7 concludes. 
2. Classification methods 
Classification techniques previously applied to 
distributional data can be summarized according 
to the following methods: the k nearest neighbor 
(kNN) method, the category-based method and 
the centroid-based method. They all operate on 
vector-based semantic representations, which 
describe the meaning of a word of interest (tar-
get word) in terms of counts1 of its coocurrence 
with context words, i.e., words appearing within 
some delineation around the target word. The 
key differences between the methods stem from 
different underlying ideas about how a semantic 
class of words is represented, i.e. how it is de-
rived from the original cooccurrence counts, and, 
correspondingly, what defines membership in a 
class. 
The kNN method is based on the assumption 
that membership in a class is defined by the new 
instance?s similarity to one or more individual 
members of the class. Thereby, similarity is 
defined by a similarity score as, for instance, by 
the cosine between cooccurrence vectors. To 
classify a new instance, one determines the set 
of k training instances that are most similar to 
the new instance. The new instance is assigned 
to the class that has the biggest number of its 
members in the set of nearest neighbors. In addi-
tion, the classification decision can be based on 
the similarity measure between the new instance 
and its neighbors: each neighbor may vote for its 
class with a weight proportional to its closeness 
to the new instance. When the method is applied 
to augment a thesaurus, a class of training in-
stances is typically taken to be constituted by 
words belonging to the same synonym set, i.e. 
lexicalizing the same concept (e.g., Hearst and 
Schuetze 1993). A new word is assigned to that 
synonym set that has the biggest number of its 
members among nearest neighbors. 
                                                     
1 Or, probabilities determined via Maximum Likeli-
hood Estimation. 
The major disadvantage of the kNN method that 
is often pointed out is that it involves significant 
computational expenses to calculate similarity 
between the new instance and every instance of 
the training set. A less expensive alternative is 
the category-based method (e.g., Resnik 1992). 
Here the assumption is that membership in a 
class is defined by the closeness of the new item 
to a generalized representation of the class. The 
generalized representation is built by adding up 
all the vectors constituting a class and normalis-
ing the resulting vector to unit length, thus com-
puting a probabilistic vector representing the 
class. To determine the class of a new word, its 
unit vector is compared to each class vector.  
Thus the number of calculations is reduced to 
the number of classes. Thereby, a class represen-
tation may be derived from a set of vectors cor-
responding to one synonym set (as is done by 
Takunaga et al 1997) or a set of vectors corre-
sponding to a synonym set and some or all sub-
ordinate synonym sets (Resnik 1992). 
Another way to prepare a representation of a 
word class is what may be called the centroid-
based approach (e.g., Pereira et al 1993). It is 
almost exactly like the category-based method, 
the only difference being that a class vector is 
computed slightly differently. All n vectors cor-
responding to class members are added up and 
the resulting vector is divided by n to compute 
the centroid between the n vectors. 
3. Making use of the structure of the the-
saurus 
The classification methods described above pre-
suppose that semantic classes being augmented 
exist independently of each other. For most ex-
isting thesauri this is not the case: they typically 
encode taxonomic relations between word 
classes. It seems worthwhile to employ this in-
formation to enhance the performance of the 
classifiers. 
3.1 Tree descending algorithm 
One way to factor the taxonomic information 
into the classification decision is to employ the 
?tree-descending? classification algorithm, 
which is a familiar technique in text categoriza-
tion. The principle behind this approach is that 
the semantics of every concept in the thesaurus 
tree retains some of the semantics of all its hy-
ponyms in such a way that the upper the concept, 
the more relevant semantic characteristics of its 
hyponyms it reflects. It is thus feasible to deter-
mine the class of a new word by descending the 
tree from the root down to a leaf. The semantics 
of concepts in the thesaurus tree can be repre-
sented by means of one of the three methods to 
represent a class described in Section 2. At every 
tree node, the decision which path to follow is 
made by choosing the child concept that has the 
biggest distributional similarity to the new word. 
After the search has reached a leaf, the new 
word is assigned to that synonym set, which 
lexicalizes the concept that is most similar to the 
new word. This manner of search offers two 
advantages. First, it allows to gradually narrow 
down the search space and thus save on compu-
tational expenses. Second, it ensures that, in a 
classification decision, more relevant semantic 
distinctions of potential classes are given more 
preference than less relevant ones. As in the case 
with the category-based and the centroid-based 
representations, the performance of the method 
may be greatly dependent on the number of sub-
ordinate synonyms sets included to represent a 
concept. 
3.2 Tree ascending algorithm 
Another way to use information about inter-class 
relations contained in a thesaurus is to base the 
classification decision on the combined meas-
ures of distributional similarity and taxonomic 
similarity (i.e., semantic similarity induced from 
the relative position of the words in the thesau-
rus) between nearest neighbors. Suppose words 
in the nearest neighbors set for a given new 
word, e.g., trailer, all belong to different classes 
as in the following classification scenario: box 
(similarity score  to trailer: 0.8), house (0.7), 
barn (0.6), villa (0.5) (Figure 1). In this case, 
kNN will classify trailer into the class 
CONTAINER, since it appears to have biggest 
similarity to box. However, it is obvious that the 
most likely class of trailer is in a different part 
of the thesaurus: in the nearest neighbors set 
there are three words which, though not belong-
ing to one class, are semantically close to each 
other. It would thus be safer to assign the new 
word to a concept that subsumes one or all of the 
three semantically similar neighbors. For exam-
ple, the concepts DWELLING or BUILDING could 
be feasible candidates in this situation. 
Figure 1. A semantic classification scenario. 
The crucial question here is how to calculate the 
total of votes for these two concepts to be able to 
decide which of them to choose or whether to 
prefer CONTAINER. Clearly, one cannot sum or 
average the distributional similarity measures of 
neighbors below a candidate concept. In the first 
case the root will always be the best-scoring 
concept. In the second case the score of the can-
didate concept will always be smaller than the 
score of its biggest-scoring hyponym. 
We propose to estimate the total of votes for 
such candidate concepts based on taxonomic 
similarity between relevant nodes. The taxo-
nomic similarity between two concepts is meas-
ured according to the procedure elaborated in 
(Maedche & Staab, 2000). Assuming that a tax-
onomy is given as a tree with a set of nodes N, a 
set of edges E ? N?N, a unique root ROOT ? N, 
one first determines the least common supercon-
cept of a pair of concepts a,b being compared. It 
is defined by 
)),(),(),((minarg),( crootcbcabalcs
Nc
??? ++=
?
(1) 
where ?(a,b) describes the number of edges on 
the shortest path between a and b. The taxonomic 
similarity between a and b is then given by 
( ) ),(),(),(
),(, crootcbca
crootba ???
?
++
=?        (2) 
where c = lcs(a,b). T is such that 0? T ? 1, with 1 
standing for the maximum taxonomic similarity. 
T is directly proportional to the number of edges 
from the least common superconcept to the root, 
which agrees with the intuition that a given num-
ber of edges between two concrete concepts sig-
nifies greater similarity than the same number of 
edges between two abstract concepts. 
We calculate the total of votes for a candidate 
concept by summing the distributional similarity 
measures of its hyponyms to the target word t 
each weighted by the taxonomic similarity 
measure between the hyponym and the candi-
date node: 
?
?
?=
nIh
hnThtsimnW ),(),()(     (3) 
where In is the set of hyponyms below the can-
didate concept n, sim(t,h) is the distributional 
similarity between a hyponym h and the word to 
be classified t, and T(n,h) is the taxonomic simi-
larity between the candidate concept and the 
hyponym h. 
4. Data and settings of the experiments 
The machine-readable thesaurus we used in this 
study was derived from GETESS2, an ontology 
for the tourism domain. Each concept in the 
ontology is associated with one lexical item, 
which expresses this concept.  From this ontol-
ogy, word classes were derived in the following 
manner. A class was formed by words lexicaliz-
ing all child concepts of a given concept. For 
example, the concept CULTURAL_EVENT in the 
ontology has successor concepts PERFORMANCE, 
OPERA, FESTIVAL, associated with words per-
formance, opera, festival correspondingly. 
Though these words are not synonyms in the 
traditional sense, they are taken to constitute one 
semantic class, since out of all words of the on-
tology?s lexicon their meanings are closest. The 
thesaurus thus derived contained 1052 words 
and phrases (the corpus used in the study had 
data on 756 of them). Out of the 756 concepts, 
182 were non-final; correspondingly, 182 word 
classes were formed. The average depth level of 
the thesaurus is 5.615, the maximum number of 
levels is 9. The corpus from which distributional 
data was obtained was extracted from a web site 
advertising hotels around the world 3 . It con-
tained around 1 million words. 
Collection of distributional data was carried out 
in the following settings. The preprocessing of 
corpus included a very simple stemming (most 
                                                     
2 http://www.daml.org/ontologies/171 
3 http://www.placestostay.com 
common inflections were chopped off; irregular 
forms of verbs, adjectives and nouns were 
changed to their first forms). The context of 
usage was delineated by a window of 3 words on 
either side of the target word, without 
transgressing sentence boundaries. In case a stop 
word other than a proper noun appeared inside 
the window, the window was accordingly ex-
panded. The stoplist included 50 most frequent 
words of the British National Corpus, words 
listed as function words in the BNC, and proper 
nouns not appearing in the sentence-initial posi-
tion. The obtained frequencies of cooccurrence 
were weighted by the 1+log weight function. 
The distributional similarity was measured by 
means of three different similarity measures: the 
Jaccard?s coefficient, L1 distance, and the skew 
divergence. This choice of similarity measures 
was motivated by results of studies by (Levy et 
al 1998) and (Lee 1999) which compared several 
well known measures on similar tasks and found 
these three to be superior to many others. An-
other reason for this choice is that there are dif-
ferent ideas underlying these measures: while 
the Jaccard?s coefficient is a binary measure, L1 
and the skew divergence are probabilistic, the 
former being geometrically motivated and the 
latter being a version of the information theo-
retic Kullback Leibler divergence (cf., Lee 
1999).  
5. Evaluation method 
The performance of the algorithms was assessed 
in the following manner. For each algorithm, we 
held out a single word of the thesaurus as the 
test case, and trained the system on the remain-
ing 755 words. We then tested the algorithm on 
the held-out vector, observing if the assigned 
class for that word coincided with its original 
class in the thesaurus, and counting the number 
of correct classifications (?direct hits?). This was 
repeated for each of the words of the thesaurus. 
However, given the intuition that a semantic 
classification may not be simply either right or 
wrong, but rather of varying degrees of appro-
priateness, we believe that a clearer idea about 
the quality of the classifiers would be given by 
an evaluation method that takes into account 
?near misses? as well. We therefore evaluated 
the performance of the algorithms also in terms 
of Learning Accuracy (Hahn & Schnattinger 
1998), i.e., in terms of how close on average the 
proposed class for a test word was to the correct 
class. For this purpose the taxonomic similarity 
between the assigned and the correct classes is 
measured so that the appropriateness of a par-
ticular classification is estimated on a scale be-
tween 0 and 1, with 1 signifying assignment to 
the correct class. Thus Learning Accuracy is 
compatible with the counting of direct hits, 
which, as will be shown later, may be useful for 
evaluating the methods. 
In the following, the evaluation of the classifica-
tion algorithms is reported both in terms of the 
average of direct hits and Learning Accuracy (?di-
rect+near hits?) over all words in the thesaurus. 
To have a benchmark for evaluation of the algo-
rithms, a baseline was calculated, which was the 
average hit value a given word gets, when its 
class label is chosen at random. The baseline for 
direct hits was estimated at 0.012; for di-
rect+near hits, it was 0.15741. 
6. Results 
We first conducted experiments evaluating per-
formance of the three standard classifiers. To 
determine the best version for each particular 
classifier, only those parameters were varied that, 
as described above, we deemed to be critical in 
the setting of thesaurus augmentation.  
In order to get a view on how the accuracy of the 
algorithms was related to the amount of avail-
able distributional data on the target word, all 
words of the thesaurus were divided into three 
groups depending on the amount corpus data 
available on them (see Table 1). The amount of 
distributional data for a word (the ?frequency? in 
the left column) is the total of frequencies of its 
context words. 
Table 1. Distribution of words of the thesaurus 
into frequency ranges 
Frequency range # words in the range 
0-40 274 
40-500 190 
>500 292 
 
The results of the evaluation of the methods are 
summarized in the tables below. Rows specify 
the measures used to determine distributional 
similarity (JC for Jaccard?s coefficient, L1 for 
the L1 distance and SD for the skew divergence) 
and columns specify frequency ranges. Each cell 
describes the average of direct+near hits / the 
average of direct hits over words of a particular 
frequency range and over all words of the the-
saurus. The statistical significance of the results 
was measured in terms of the one-tailed chi-
square test. 
kNN. Evaluation of the method was conducted 
with k=1, 3, 5, 7, 10, 15, 20, 25, and 30. The 
accuracy of classifications increased with the 
increase of k. However, starting with k=15 the 
increase of k yielded only insignificant im-
provement. Table 2 describes results of evalua-
tion of kNN using 30 nearest neighbors, which 
was found to be the best version of kNN. 
Table 2. kNN, k=30. 
 0-40 40-500 >500 Overall 
JC .33773 
/.17142 
.33924 
/.15384 
.40181 
/.12457 
.37044 
/.15211 
L1  .33503 
/.16428 
.38424 
/.21025 
.38987 
/.14471 
.37636 
/.17195 
SD .31505 
/.14285 
.36316 
/.18461 
.45234 
/.17845 
.38806 
/.17063 
 
Category-based method. To determine the best 
version of this method, we experimented with 
the number of levels of hyponyms below a con-
cept that were used to build a class vector). The 
best results were achieved when a class was 
represented by data from its hyponyms at most 
three levels below it (Table 3).  
Table 3. Category-based method, 3 levels 
 0-40 40-500 >500 Overall 
JC .26918 
/.12142 
.34743 
/.17948 
.47404 
/.28282 
.37554 
/.2023 
L1 .27533 
/.125 
.41736 
/.25128 
.56711 
/.38383 
.43242 
/.26190 
SD .28589 
/.12857 
.34932 
/.18461 
.51306 
/.31649 
.39755 
/.21957 
 
Centroid-based method. As in the case with 
the category-based method, we varied the num-
ber of levels of hyponyms below the candidate 
concept. Table 4 details results of evaluation of 
the best version of this method (a class is repre-
sented by 3 levels of its hyponyms). 
 
Table 4. Centroid-based method, 3 levels. 
 0-40 40-500 >500 Overall 
JC .17362 
/.07831 
.18063 
/.08119 
.30246 
/.14434 
.22973 
/.10714 
L1 .21711 
/.09793 
.30955 
/.13938 
.37411 
/.1687 
.30723 
/.12698 
SD .22108 
/.09972 
.23814 
/.11374 
.36486 
/.16147 
.28665 
/.10714 
 
Comparing the three algorithms we see that 
overall, kNN and the category-based method 
exhibit comparable performance (with the ex-
ception of measuring similarity by L1 distance, 
when the category-based method outperforms 
kNN by a margin of about 5 points; statistical 
significance p<0.001). However, their perform-
ance is different in different frequency ranges: 
for lower frequencies kNN is more accurate (e.g., 
for L1 distance, p<0.001). For higher frequen-
cies, the category-based method improves on 
kNN (L1, p<0.001). The centroid-based method 
exhibited performance, inferior to both those of 
kNN and the category-based method. 
Tree descending algorithm. In experiments 
with the algorithm, candidate classes were repre-
sented in terms of the category-based method, 3 
levels of hyponyms, which proved to be the best 
generalized representation of a class in previous 
experiments. Table 5 specifies the results of its 
evaluation. 
Table 5. Tree descending algorithm. 
 0-40 40-500 >500 Overall 
JC .00726 
/0 
.01213 
/.00512 
.02312 
/.0101 
.014904 
/.005291 
L1 .08221 
/.03214 
.05697 
/.02051 
.21305 
/.11111 
.128844 
/.060846 
SD .08712 
/.03214 
.07739 
/.03589 
.16731 
/.06734 
.011796 
/.047619 
 
Its performance turns out to be much worse than 
that of the standard methods. Both direct+near 
and direct hits scores are surprisingly low, for 0-
40 and 40-500 much lower than chance. This 
can be explained by the fact that some of top 
concepts in the tree are represented by much less 
distributional data than other ones. For example, 
there are less than 10 words that lexicalize the 
top concepts MASS_CONCEPT and 
MATHEMATICAL_CONCEPT and all of their 
hyponyms (compare to more than 150 words 
lexicalizing THING and its hyponyms up to 3 
levels below it). As a result, at the very begin-
ning of the search down the tree, a very large 
portion of test words was found to be similar to 
such concepts. 
Tree ascending algorithm. The experiments 
were conducted with the same number of nearest 
neighbors as with kNN. Table 6 describes the 
results of evaluation of the best version (formula 
3, k=15). 
Table 6. Tree ascending algorithm, total of votes 
according to (3), k=15. 
 0-40 40-500 >500 Overall 
JC .32112 
/.075 
.33553 
/.0923 
.40968 
/.08754 
.36643 
/.08597 
L1 .33369 
/.07142 
.34504 
/.0923 
.42627 
/.09764 
.38005 
/.08862 
SD .31809 
/.06785 
.32489 
/.05128 
.45529 
/.11111 
.38048 
/.08201 
 
There is no statistically significant improvement 
on kNN overall, or in any of the frequency 
ranges. The algorithm favored more upper con-
cepts and thus produced about twice as few di-
rect hits than kNN. At the same time, its di-
rect+near hits score was on par with that of kNN! 
This algorithm thus produced much more near 
hits than kNN, what can be interpreted as its 
better ability to choose a superconcept of the 
correct class. Based on this observation, we 
combined the best version of the tree ascending 
algorithm with kNN in one algorithm in the 
following manner. First the former was used to 
determine a superconcept of the class for the 
new word and thus to narrow down the search 
space. Then the kNN method was applied to 
pick a likely class from the hyponyms of the 
concept determined by the tree ascending 
method. Table 7 specifies the results of evalua-
tion of the proposed algorithm. 
Table 7. Tree ascending algorithm combined with 
kNN, k=30. 
 0-40 40-500 >500 Overall 
JC .34444 
/.16428 
.35858 
/.14358 
.41260 
/.10774 
.38215 
/.14021 
L1 .35147 
/.16428 
.36545 
/.15384 
.41086 
/.11784 
.38584 
/.14682 
SD .32613 
/.13571 
.36485 
/.1641 
.45732 
/.16498 
.39456 
/.1574 
 
The combined algorithm demonstrated impro-
vement both on kNN and the tree ascending 
method of 1 to 3 points in every frequency range 
and overall for direct+near hits (except for the 
40-500 range, L1). The improvement was statis-
tically significant only for L1, ?>500? (p=0.05) 
and for L1, overall (p=0.011). For other similari-
ty measures and frequency ranges it was insigni-
ficant (e.g., for JC, overall, p=0.374; for SD, 
overall, p=0.441). The algorithm did not im-
prove on kNN in terms of direct hits. The hits 
scores set in bold in Table 7 are those which are 
higher than those for kNN in corresponding 
frequency ranges and similarity measures. 
7. Discussion 
In this paper we have examined different possi-
bilities to take advantage of the taxonomic or-
ganization of a thesaurus to improve the accu-
racy of classifying new words into its classes. 
The study demonstrated that taxonomic similar-
ity between nearest neighbors, in addition to 
their distributional similarity to the new word, 
may be a useful evidence on which classification 
decision can be based. We have proposed a ?tree 
ascending? classification algorithm which ex-
tends the kNN method by making use of the 
taxonomic similarity between nearest neighbors. 
This algorithm was found to have a very good 
ability to choose a superconcept of the correct 
class for a new word. On the basis of this finding, 
another algorithm was developed that combines 
the tree ascending algorithm and kNN in order 
to optimize the search for the correct class. Al-
though only limited statistical significance of its 
improvement on kNN was found, the results of 
the study indicate that this algorithm is a promis-
ing possibility to incorporate the structure of a 
thesaurus into the decision as to the class of the 
new word. We conjecture that the tree ascending 
algorithm leaves a lot of room for improvements 
and combinations with other algorithms like 
kNN. 
The tree descending algorithm, a technique 
widely used for text categorization, proved to be 
much less efficient than standard classifiers 
when applied to the task of augmenting a do-
main-specific thesaurus. Its poor performance is 
due to the fact that in such a thesaurus there are 
great differences between top concepts in the 
amount of distributional data used to represent 
them, which very often misleads the top-down 
search. 
We believe that a study of the two algorithms on 
the material of a larger thesaurus, where richer 
taxonomic information is available, can yield a 
further understanding of its role in the perform-
ance of the algorithms. 
References 
Caraballo S. A. (1999) Automatic construction of a 
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 120-126. 
Hahn U. and Schnattinger K. (1998) Towards text 
knowledge engineering. In Proc. of AAAI/IAAI, pp. 
524-531. 
Hearst M. and Schuetze H. (1993) Customizing a 
lexicon to better suit a computational task. In Proc. 
of the SIGLEX Workshop on Acquisition of Lexical 
Knowledge from Text, Columbus Ohio, pp. 55--69. 
Grefenstette G. (1993) Evaluation techniques for 
automatic semantic extraction: comparing syntac-
tic and window based approaches. In Proc. of the 
SIGLEX Workshop on Acquisition of Lexical 
Knowledge from Text, Columbus Ohio. 
Lin D. (1998) Automatic retrieval and clustering of 
similar words. In Proc. of the COLING-ACL?98, 
pp. 768-773. 
Lee L. (1999) Measures of distributional similarity. 
In Proc. of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 25-32. 
Levy J., Bullinaria J., and Patel M. (1998) Explora-
tions in the derivation of word co-occurrence sta-
tistics. South Pacific Journal of Psychology, 10/1, 
pp. 99-111.  
Maedche A. and Staab S. (2000) Discovering concep-
tual relations from text. In Proc. of ECAI-2000, 
IOS Press, pp. 321-324.  
Pereira F., Tishby N., and Lee L. (1993) Distribu-
tional clustering of English words. In Proc. of the 
31st Annual Meeting of the ACL, pp. 183-190. 
Resnik P. (1992) Wordnet and distributional analysis: 
A class-based approach to lexical discovery. AAAI 
Workshop on Statistically-based Natural Language 
Processing Techniques. 
Tokunaga T., Fujii A., Iwayama M., Sakurai N., and 
Tanaka H. (1997) Extending a thesaurus by classi-
fying words. In Proc. of the ACL-EACL Workshop 
on Automatic Information Extraction and Building 
of Lexical Semantic Resources, pp. 16-21.  
147
148
149
150
Specification in terms of interactional properties as a way to optimize
the representation of spatial expressions
Viktor Pekar
Bashkir State University
Okt.Revolutsii 3a
450000 Ufa, Russia
vpekar@ufanet.ru
Abstract
The results of the study demonstrate
that numerous object-specific
restrictions on the use of projective
prepositions in English and Russian are
predicted by their interactional
(functional) semantic properties.
Object-independent perceptual
properties (such as distance between
objects, direction of their motion, etc)
that seemingly guide the use of the
expressions, are also found to be
presupposed by their interactional
properties. Based on these findings, it
is suggested that in addition to a basic
geometrical specification, the semantic
representation should contain
functional information. A
computational procedure of matching
an expression with a spatial scene
should thus include detection of the
interactional properties of the scene.
They can be determined through (1)
retrieval of information about
interactional properties of specific
objects and (2) determining
functionally relevant object-
independent perceptual properties of
the scene.
1 Introduction
By now a number of computational models of
spatial semantics have been developed, which
aim to generate spatial references (e.g. Gapp,
1994; Logan and Sadler, 1996; Regier, 1996). In
these models, spatial meaning is represented in
terms of geometric constructs such as shapes,
center of mass, distance, overlapping between
shapes, etc. The models are able to appropriately
match an expression with a novel spatial
arrangement of shapes. However, the great
disparity of real-world scenes a spatial
expression can refer to presents a serious
problem for this approach. There seems to be a
virtually illimitable number of object- and even
situation-specific restrictions on the usage of a
particular expression (Herskovits, 1986).
Consider, for example, the preposition in, the
meaning of which is often represented in terms of
the mathematical notion "inclusion". A scene
where an overturned bowl is placed over a potato
cannot be described by in (*the potato in the
bowl), even though the potato is within the
physical boundaries of the bowl. The spatial
relation must be described by under: the potato is
under the bowl. Hence, the lexical entry for in has
to contain a comment that the preposition is not
used when the Ground is a concave object
positioned with its concavity opening
downwards. On the other hand, a bulb, being
within the boundaries of a socket, which
concavity does open downwards, is said to be in
the socket, but not under it. This, in its turn, has
to be registered in the lexical entry as well.
Taking into account the fact that the restrictions
are imposed on specific orientation of specific
objects, such comments in semantic
representations should proliferate infinitely.
In a number of lexical semantics studies
(Miller and Johnson-Laird, 1976; Herskovits,
1986; Cuyckens, 1993; Coventry, 1998; Malyar
and Seliverstova, 1998), these restrictions are
accounted by the fact that spatial meaning
reflects not only perceptual experience of the
referent scenes, but also their interactional
(functional) conceptualization. For example,
Miller and Johnson-Laird (1976) argue that the
usage of the English preposition at cannot be
predicted by such notions as "contiguity" or
"juxtaposition". There are situations, where
these types of relations hold, but the preposition
cannot be used: *The chair is at the ocean liner.
*Bill is at Australia. To define the meaning of
at, they introduce the notion "region" of an
object x1 ? a portion of space "where x can
interact with y socially, physically, or in
whatever way x?s conventionally interact with
y?s". The notion of  "interaction" explains
inappropriateness of at in the two sentences: a
chair and an ocean liner, as well as a person and
a big body of land normally do not interact with
each other.
However, it remains unclear how the
perceptual and functional properties relate to
each other. Malyar and Seliverstova (1998) for
some English and Russian prepositions propose
that function is a factor complementing
geometry in predicting the usage: in those uses,
where the available geometrical semantic
information does not correctly indicate the
Figure's position, it is delineated by the notion
"function" of the Ground. Cuyckens' (1993)
analysis of the Dutch in similarly suggests that
in most cases its meaning can be represented by
geometric constructs like "three-dimensional",
"porous", "bounded", etc; and in cases, where
geometric specification fails to explain the
usage, the notion "function" is employed
(notably for the case with the overturned bowl
and potato). This solution, however, seems
counter-intuitive: it requires that some portion of
uses is determined by geometry and some by
function and thus the meaning representation
still has to explicitly encode specific cases,
where geometrical description is inappropriate.
From the computational perspective, this
representation is also hardly plausible.
                                                          
1 Miller and Johnson-Laird (1976) use ?x? to refer to the
object denoted by the left argument of the preposition and
?y? for the object denoted by the right argument of the
preposition. In the present paper to designate these entities
the terms ?Figure? and ?Ground?, respectively, will be used.
 The present study attempts to verify the
hypothesis that a functional property of a
preposition presupposes its different perceptual
properties. If the hypothesis is verified, the
finding will explain why numerous perceptually
diverse uses of a preposition fall under one
linguistic category. The meaning of a
preposition could then be specified in terms of
function while omitting the many restrictions
necessary in a geometry-based semantic
representation.
The paper focuses on projective prepositions
? the English above and over and the Russian
??????? and ?????2. Projective prepositions
present an interesting subject matter in the
context of study of spatial reference. Coventry?s
(1998) analysis of a number of synonymic pairs
of English projective prepositions (including
above and over) demonstrated that function
plays a significant role in their semantics. The
present study is to check if the usage of the
Russian prepositions is also influenced by
function. Comparison between semantics of the
vertical and frontal prepositions may reveal
certain cross-domain regularities of correlation
between perceptual and functional semantic
properties.
2 Methodology
The first stage of the study was concerned with
formulation of hypotheses about particular
character of perceptual and functional properties
of the prepositions. For this purpose examples of
the authentic usage of the prepositions in
electronic corpora (the Brown corpus, the
Lancaster-Oslo-Bergen corpus, the British
National Corpus, the Times (March 1995)) were
collected. The corpus data were supplemented
by usage examples found in literary English and
Russian texts. This stage revealed important
distributional characteristics of the prepositions.
For example, it was found that over very rarely
combines with verbs denoting an upward
motion, such as rise, raise, lift, heave, soar,
while above often does. Hypotheses about a
given preposition were formulated in terms of
properties of referent scenes that tend to be
described by this preposition. The hypotheses
                                                          
2 The both prepositions denote the Figure's position on the
frontal axis of the Ground.
were tested in experiments with native speaking
subjects.
During the experiments subjects' judgements
about appropriateness of the use of the
prepositions in selected contexts were obtained.
The methodology is based on the assumption
that an expression is judged to be semantically
acceptable in a context, if this context possesses
semantic features that are either the same as
those of the expression or do not contradict
them; the expression is unacceptable, if the
context contains semantic properties,
contradicting those of the expression. Thus,
presence of a semantic property in the meaning
of a preposition was verified in the following
manner. In the context, which possessed the
examined semantic property, first one and then
the other of the contrasted prepositions were
placed. If there was a statistically significant
difference in distribution between subjects'
evaluations of the two sentences, presence of the
property in the semantics of a preposition was
taken to be verified. The use of linguistic data in
the experiments is particularly suited for the
purposes of the study, because it allows for
dealing with non-perceptual semantic properties,
as opposed to obtaining linguistic responses to
purely perceptual stimuli, e.g. pictures of
geometric shapes.
23 English-speaking subjects participated in
the experiments. They represented the American
and British varieties of English (20 and 3
subjects, respectively). Their age ranged
between 25 and 60. All of them were college
graduates. As Russian-speaking subjects, 45
graduate and undergraduate students of the
English Language Department of Bashkir State
University (Ufa, Russia) were recruited, their age
ranging between 18 and 30.
During the experiment the subjects were
presented with questionnaires, each containing
about 30 pairs of identical sentences, which
differed only in the prepositions used. The
sentences used were edited authentic examples
of the use of the prepositions. The subjects were
instructed to evaluate appropriateness of the use
of the prepositions in the sentences according to
a 5-degree scale. In case they perceived a
sentence as ambiguous, they were asked to point
it out and leave it unevaluated.
The difference between the pairs of sentences
thus formed was analyzed in terms of the
Student t criterion and the chi-square criterion.
The difference in evaluations between the
sentences (and hence between the semantics of
the prepositions) was taken to be established, if
the ? value was smaller than 0.05. In the
following discussion, paired sentences with
?<0.05 are used as examples, with the
preposition having smaller mean of evaluations
marked by an asterisk.
3 Perceptual Properties
The section describes the revealed perceptual
semantic properties of the four prepositions,
which do not depend on specific objects and
namely those properties which pertain to motion
of the Figure, distance between the Figure and
the Ground and choice of reference frame3. The
usage of over and ?????, one the one hand, and
above and ???????, on the other, was found to
be sensitive to largely the same perceptual
properties of referent scenes.
3.1 Choice of Reference Frame
The prepositions ??????? and above were found
to be used when the position of the Figure is
described in the egocentric reference frame:
(1) ??????? ????? ????? ????. ?There is a stool
ahead of the table.?
(2) The roof of my school could be seen above
(*over) those trees.
Example 1 implies that the stool and the table
are located on the frontal axis of the observer,
the table being closer to the observer than the
stool (Figure1) (compare: ????? ??????
????? ????. ?There is a stool in front of the
table.? which does not imply the presence of the
observer). In 2 the position of the Figure roof
relative to the Ground trees is described
simultaneously in the absolute reference frame
(relative to the gravity axis) and the egocentric
reference frame: the roof is not directly over the
trees; the roof and the trees are vertically co-
related only from the point of view of the
                                                          
3 Strictly speaking, reference frame is not a perceptual
property of a scene, but a strategy of organizing the
perceptual input. For our purposes, it is important that
descriptions in different reference frames invoke different
percepts. For example, John is to the left of Mary in the
egocentric reference frame denotes different spatial
relations than the same sentence in the intrinsic reference
frame.
observer, whose position is implied ? it is on
one horizontal line with positions of the trees
and the school (Figure2).
Figure 1. Egocentric reference frame in the
usage of ??????? in (1).
Figure 2. Egocentric reference frame in the
usage of above in (2).
The prepositions ????? and over are used
when the position of the Figure is described in
the intrinsic reference frame:
(3) ????????? ????? ???????? ??????. ?The
postman is in front of the mailbox.?
(4) He held the hammer over the nail.
In 3 the position of the postman is defined
relative to the frontal axis of the mailbox; the
position of the observer is not implied ? he or
she can in principle be viewing the scene from
any point (Figure 3). In 4 the position of the
hammer is defined relative to the gravity axis
and the vertical axis of the nail, also irrespective
of the observer?s position (Figure 4).
Figure 3. Intrinsic reference frame in the
usage of ????? in (3).
Figure 4. Intrinsic reference frame in the
usage of over in (4).
3.2 Motion of the Figure
The preposition over is found to be used to
denote Goal (i.e. the end-point of a trajectory) of
a downward motion of the Figure toward the
Ground (5). It can also denote Source (i.e. the
starting point of a trajectory) of an upward
motion of the Figure away from the Ground (6):
(5) He pulled his cap down over (*above) his eyes.
(6) The executioner removed his sword from over
(*above) Peter?s head.
The preposition above, on the contrary, is
used to denote Goal of an upward motion of the
Figure away from the Ground (7) and Source of a
downward motion of the Figure toward the
Ground (8):
(7) He raised his cap above (*over) his eyes.
(8) A meteorite falling from above (*over) us.
In Russian to denote Goals and Sources of
the Figure?s motion on a horizontal plane the
prepositions ? (Goal of motion toward the
Ground), ?? (Source of motion away from the
Ground), ?????? (Goal of motion away from the
Ground) and ??????? (Source of motion toward
the Ground) are used. However, the prepositions
????? and ??????? have different degrees of
acceptability when different Paths (i.e.
trajectories) of the Figure?s motion along the
Ground?s frontal axis are in question. The
preposition ????? is preferred, when motion is
toward the Ground (9) and ??????? when
motion is away from the Ground (10):
(9) ??-?? ???? ????? ?? ???? ???????? ?????? ?
???????????? ?????? ????? ??? (*???????????
?????? ??????? ????). ?A car raced from around the
corner and stopped just in front of him.?
(10) ??????? ???? ??????? ????? ??????
????????? (*????????? ?????? ????). ?The skiers
ahead of me started to quickly enlarge the gap.?
Thus, ?????, like over, emphasizes Goal of
the Figure?s approaching the Ground, while
???????, like above, emphasizes Goal of the
Figure?s departing from the Ground.
3.3 Distance between the Figure and the
Ground
The prepositions ??????? and above are used
when distance between the Figure and the
Ground is conceptualized as great.
(11) ??????, ??????? ?????? ??????? ??? (*??????
????? ????). ?A tree standing far ahead of us.?
(12) Keep the grill high enough above (*over) the fire!
The prepositions can be used when between
the Figure and the Ground there are some other
objects of a size, comparable to that of the
spatially co-related objects.
(13) He lives three floors above me (*three floors
over me).
(14) ?? ????? ? ??????? ? ???????, ??? ????? ???
???????? ??????? ???? ????? ???????? (*?????
??? ???????? ????? ???). ?He took a place in the
line and saw Vladimir standing three people ahead of
him.?
Presumably the presence of such objects
between the Figure and the Ground motivates
conceptualization of distance / remoteness
between them. Of conceptualization of distance
between the Figure and the Ground in
occurrences of above and ??????? also speaks
the fact that these prepositions, unlike their
synonyms, are used when degree of remoteness
of one object from the other is specified. These
are those cases when the prepositions are
combined with words and phrases like one inch,
several feet, five hundred meters, slightly, a little
(15 and 16)4, etc.
(15) She stuck the rose in her hair a little above the
left ear (*a little over the left ear).
(16) ???? ??????? ??????? ????? ????????
(*???? ?????). ?Vladimir was running a little ahead
of Aleksey.?
The prepositions ????? and over are used
when the distance between the Figure and the
Ground is not conceptualized at all:
                                                          
4 It is of interest to note that above and ??????? can
combine with words like slightly or a little, but not with
very, too, considerably, quite: *quite above, *????????
???????.
(17) ????? ??? ????? ? ???? ????? ????????
(*??????? ???? ????? ? ????). ?In front of him
face to face stood Vladimir.?
(18) His cap was low over (*above) his eyes.
That the prepositions are used when distance
is negligible is also seen from the fact that they
are inappropriate in cases when there are some
other sizeable objects between the Figure and
the Ground (13-14) or when degree of
remoteness is emphasized (15-16).
The prepositions over and ????? are also
used to denote the point where the Figure is
traversing a projective axis of the Ground,
typically at such a distance from it, that
conceptualized as negligibly small and also not
excluding a contact between the objects5.
(19) ??????? ????????? ?????? ????? ?????
?????????? (*? ????????? ?????? ????? ???????
?????????.) ?The child dashed across the street right
in front of a truck.?
(20) The ball flew over the fence. (*The ball flew
above the fence.)
Their synonyms cannot denote this point on a
projective axis of the Ground: ??????? is
unacceptable in 19 and above in 20 conveys
information of the Figure?s moving along a
projective axis of the Ground away from it.
4 Functional Properties
The study revealed that both the Russian and the
English prepositions possess functional semantic
properties, and namely their meaning is
characterized by information about certain
interaction, currently taking place or anticipated,
between the spatially co-related objects. The
prepositions ????? and over convey information
about functional interdependence between the
objects, whereas the prepositions ??????? and
above ? of functional separation or
independence of the objects.
The first type of functional information is
physical interaction between the Figure and the
Ground, which is taking place at the moment in
question. The result of this interaction is usually
change of some physical (integrity, temperature,
etc) or interactional (visibility, possibility to
manipulate, etc) characteristic of one of the
objects. One of the objects ? which may be both
                                                          
5 Such uses of over can be considered boundary cases
between the ?above? and ?across? senses of the preposition.
the Figure and the Ground ? is seen as a source
of influence or an agent of an action directed at
its counterpart. Animate objects can act in the
role of the influencing object, as well as those
inanimate objects that are able to disperse light,
heat, etc or used as instruments:
(21) She bent over (*above) the puppies to see them
better.
(22) ?? ???????, ???? ????? (*???????) ??????. ?We
were keeping warm standing in front of the stove.?
(23) Footsteps thumping heavily above (*over) the
boys.
(24) ??????? ???? ?????????? ???????????
????? (*????? ??? ?????????? ???????????
?????). ?Bombs were constantly exploding ahead of
him.?
In 21 and 22 there are functional relations
between the objects. In 21 the Figure she is
directing the action of inspecting at the Ground
puppies. Note that here not a simple act of
perceptually spotting an object is implied, but an
active examination of it. In 22 the Figure ??
?we? experiences influence exerted by the
Ground ????? ?stove?. In 23 and 24 the objects
are functionally disjunct. In 23 the action of the
Figure footsteps does not effect the Ground the
boys, if above is used; and does so, if over is
replaced for above. In 24 the process
??????????? ????? ?exploding? is understood
as not effecting the Figure ???? ?him?, cf. ??
??? ???? ????????????? ????? ??? ??????.
?He was killed by a bomb that exploded in front
of him.?
Second, the prepositions can describe such
functional relations, whereby one object is
conceptualized as protecting the other one from
influence from without the functional unity:
(25) He carefully held his hat over (*above) the
candle to protect it from the rain.
(26) The fog over (*above) the river prevented
detection from airplanes.
(27) ??????? ???? ?? ?????????, ?? ??????
????? ????? ????? (*??????? ????). ?Trying to
conceal himself from the cameras he held his file in
front of his face.?
(28) ????? ???????? ??????? ?????????-
???????? ?????? (*??????? ???????). ?A
bulletproof shield was held in front of the speaker.?
Third, the relations between objects can be
conceptualized in such a manner that one object
is seen as having a potential to establish an
influence over the other one; at the moment in
question, however, no influence is exerted. To
describe this type of relations the term
?functional potential? will be used, which was
introduced by Miller and Johnson-Laird (1976)
to designate a similar interactional property of
artifacts.
(29) Trying to threaten Peter, the executioner held his
sword over (*above) his head.
(30) A black stormy cloud over (*above) the farm.
(31) ????? ??? ????????? ???????, ????????
???????, ?? ???? ????? ? ????? ???? (*???????
???? ????????? ???????). ?A plate was placed in
front of him, he waited a little, then took the spoon
and started to eat.?
(32) ?? ???? ??????? ????? ?????? ???? ?????
?????? (*??????? ????). ?There is a letter on my
desk, right in front of me.?
To sum up, the prepositions over and ?????
are found to convey information of three
identical types of functional relations between
the Figure and the Ground: actual physical
interaction, protection, functional potential; the
prepositions above and ??????? convey
information of absence / impossibility to
establish such functional relations. As is clear,
the functional properties presuppose perceptual
properties that are specific for particular objects.
5 Correlation between Functional and
Perceptual Properties
The study revealed that each of the three types
of the object-independent perceptual properties
of each of the prepositions can be present in one
referent scene simultaneously together with the
functional property of this preposition, but not
with the one contradicting it. This regularity is
established from subjects? acceptability
judgements of sentences where information
about either presence or absence of functional
relations is added to information about a
particular object-independent perceptual
property.
The study replicated results of Carlson-
Radvansky and Irwin (1993), who demonstrated
correlation between presence of functional
relations and choice of the intrinsic reference
frame, on the one hand, and between absence of
functional relations and choice of the egocentric
reference frame, on the other. For example, in 3,
where, as it was shown, choice of the intrinsic
reference frame necessitates the use of ?????,
the postman?s position is such that allows
functional interaction between him and the
mailbox (the postman may be taking mail out of
the mailbox). If it is manifestly shown that the
postman cannot interact with the mailbox,
??????? (or some other preposition like ?????
?near?) is preferred to ?????:
(37) ????????? ????? ??????? / ????? ?????????
?????, ???????????? ? ?????????. (*?????????
????? ????? ???????? ??????, ???????????? ?
?????????.) ?The postman was standing ahead of /
near the mailbox, talking to a roadsweeper.?
In 4, where the intrinsic reference frame is
also chosen, the position of the hammer is such
that allows its functional interaction with the
nail. Both in 3 and 4 the Figures (the postman
and the hammer) and the Grounds (the mailbox
and the nail) are conceptualized as facing each
other by their functionally relevant sides.
In uses of ??????? and above, absence of
functional relations between the Figure and the
Ground can combine with choice of the
egocentric reference frame. In 1 the stool and
the table do not necessarily face each other by
their functional sides, the two objects do not
have to constitute a functional unity. For
example, the stool may be at such a distance
away from the table that a person seated on it
cannot interact with the table. The co-position of
the trees and the roof described in 2 does not
allow any sort of interaction between them.
The Figure?s approaching the Ground (38a)
and an insignificantly small distance between
the two objects (39a) are found to correlate with
the emergence and the presence of functional
relations between them, correspondingly.
(38a) He pulled his lamp down over his sheets to see
the scheme better.
(38b) *He pulled his lamp down over his sheets so
that it does not obstruct the view.
(39a) The helicopter was hovering low over the boat
to save the crew.
(39b) *The helicopter was hovering low over the
boat in order not to be detected.
The Figure?s departing from (40a) and a big
distance between the two objects (41a) correlate
with discontinuation and absence of functional
relations between them.
(40a) To let the sun rays play on his face he pulled
his cap up above his eyes.
(40b) *To conceal his face from the sunrays, he
pulled his cap up above his eyes.
(41a) She sat on a branch high above the lions so that
they could not get at her.
(41b) *She sat on a branch high above the lions and
fed them.
Examples 38b, 39b, 40b and 41b show that
the opposite is not possible. In a given scene,
perceptual properties, corresponding to those of
a given preposition, cannot combine with the
functional properties that contradict those of this
preposition. As is easily seen, this regularity is
not just a peculiarity of the prepositions, but
rather a matter of common sense. Removing the
prepositions and expressing the idea in any other
words cannot make the sentences semantically
well-formed: *The helicopter was hovering low
in order not to be detected from the boat. *She
sat high on a tree branch and fed the lions. It
can be expected that other prepositions, which
possess the same functional properties, also
possess corresponding perceptual properties.
From the correspondences between the
object-independent perceptual properties of a
preposition and its functional property, one can
conclude that the two types of semantic
properties presuppose each other. The
perceptual properties can be said to be
functionally relevant, that is, perceiving these
properties, the observer forms an idea about the
functional aspect of the scene. For example, one
object?s approaching some other one
automatically triggers conceptualization of a
possibility that the former influences the latter.
6 Conclusion
The English vertical above and over and the
Russian frontal ????? and ??????? are found to
possess similar object-independent perceptual
properties, which pertain to motion, distance,
and choice of the reference frame. All the four
prepositions are also found to have similar
functional properties ? their usage is sensitive to
(1) interaction between the Figure and the
Ground that is currently taking place; (2) the
Figure?s preventing the Ground from being
physically influenced (?protection?); (3)
potential interaction between the objects
(?functional potential?). Conceptualization of
interaction in these cases hinges on experiential
knowledge about functionality of specific
objects. For example, a human is usually
conceptualized as (potentially) acting upon an
object, located near him/her. The fact that
specific objects are perceptually different, but
have the same functional status, explains object-
specific restrictions necessary in a geometry-
based semantic specification.
The object-independent perceptual properties
of referent scenes that determine the usage of
the prepositions are found to be functionally
relevant, i.e. by perceiving these properties the
observer is able to form an idea about the
functional aspect of the scene. That is why
functional relations are conceptualized even in
those cases when the spatially correlated objects
do not have any inherent function (i.e. "natural
kind objects" like clouds, fog, or rocks). These
are such cases when, for example, small distance
triggers off conceptualization of potential
interaction between objects, or great distance -
impossibility of interaction.
Thus, the object-specific and object-
independent perceptual properties of each of the
four prepositions are presupposed by its
functional property. The established correlation
between the perceptual and functional properties
allows not to keep geometry and function
distinct in the lexical entries, and thus to avoid
the necessity to explicitly specify cases that are
determined by geometry and cases that are
determined by function. On this account, an
optimal semantic representation will include
only functional semantic properties and
information about the particular projective axis
on which the Figure is located.
Based on these findings, a computationally
plausible procedure of matching a preposition
with a referent scene may be proposed. The
procedure should include detection of the
functional properties of the scene. They can be
determined through (1) retrieval of interactional
information about specific objects, e.g. their
functionally relevant sides; (2) functionally
relevant object-independent perceptual
properties of the scene, e.g. distance between
objects, direction of their motion, etc. After that
the scene is described by that preposition that
possesses the corresponding functional semantic
property.
The conducted study, however, does not
allow one to think that meanings of the vertical
and the frontal prepositions are entirely
identical. The frontal prepositions ??????? and
?????, unlike the vertical prepositions above and
over, cannot denote Goal and Source of the
Figure?s motion: there are special prepositions
to denote these entities. Nonetheless, such cases
are not numerous and can be presented as
exceptions in the lexical entries without making
the semantic representation too bulky and
unmanageable.
References
Laura A. Carlson-Radvansky and D. E. Irwin. 1993:
Frames of reference in vision and language:
Where is above? Cognition 46:223-244.
Kenny R. Coventry. Spatial prepositions, functional
relations and lexical specification. The
representation and processing of spatial
expressions. Mahwah, 247-262.
Hubert Cuyckens. 1993: The Dutch spatial
preposition ?in?: a cognitive semantic analysis.
The semantics of prepositions: from mental
processing to natural language processing. Berlin,
27-71.
Klaus P. Gapp. 1994. Basic meanings for spatial
relations: Computation and evaluation of 3D
space. Proceedings of AAAI-94: 1393-1398.
Annette Herskovits. 1986: Language and spatial
cognition: an interdisciplinary study of the
prepositions in English. Cambridge.
Gordon D. Logan and Daniel D. Sadler. 1996: A
computational analysis of the apprehension of
spatial relations. Language and Space. Cambridge,
MA, 493-529.
Tatiana N. Malyar and Olga N.  Seliverstova. 1998:
Prostranstvenno-distanzionnye predlogi i
narechiya v russkom i angliyskom yazykakh.
Slavistische Beitr?ge 362. M?nchen.
George Miller and Phillip Johnson-Laird. 1976:
Language and perception. Cambridge, MA.
Terry Regier. 1996: The human semantic potential.
Spatial language and constrained connectionism.
Cambridge, MA.
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 683?687,
Dublin, Ireland, August 23-24, 2014.
UBham: Lexical Resources and Dependency Parsing for Aspect-Based
Sentiment Analysis
Viktor Pekar
School of Computer Science
University of Birmingham
Birmingham, UK
v.pekar@cs.bham.ac.uk
Naveed Afzal
FCIT, North Branch
King Abdulaziz University
Jeddah, KSA
nafzal@kau.edu.sa
Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, UK
b.bohnet@cs.bham.ac.uk
Abstract
This paper describes the system devel-
oped by the UBham team for the SemEval-
2014 Aspect-Based Sentiment Analysis
task (Task 4). We present an approach
based on deep linguistic processing tech-
niques and resources, and explore the pa-
rameter space of these techniques applied
to the different stages in this task and ex-
amine possibilities to exploit interdepen-
dencies between them.
1 Introduction
Aspect-Based Sentiment Analysis (ASBA) is con-
cerned with detection of the author?s sentiment to-
wards different issues discussed in a document,
such as aspects or features of a product in a cus-
tomer review. The specific ASBA scenario we ad-
dress in this paper is as follows. Given a sentence
from a review, identify (1) aspect terms, specific
words or multiword expressions denoting aspects
of the product; (2) aspect categories, categories of
issues being commented on; (3) aspect term po-
larity, the polarity of the sentiment associated with
each aspect term; and (4) aspect category polarity,
the polarity associated with each aspect category
found in the sentence. For example, in:
I liked the service and the staff, but not the food.
aspect terms are service, staff and food, where the
first two are evaluated positively and the last one
negatively; and aspect categories are SERVICE and
FOOD, where the former is associated with pos-
itive sentiment and the latter with negative. It
should be noted that a given sentence may contain
This work is licenced under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The research was partially supported by FP7 ICT project
?Workbench for Interactive Contrastive Analysis of Patent
Documentation? under grant no. FP7-SME-606163.
one, several, or no aspect terms, one, several, or no
aspect categories, and may express either positive,
negative, neutral, or conflicted sentiment.
While the ASBA task is usually studied in the
context of documents (e.g., online reviews), pecu-
liarities of this scenario are short input texts, com-
plex categorization schemas, and a limited amount
of annotated data. Therefore we focused on ways
to exploit deep linguistic processing techniques,
which we use for both creating complex classifi-
cation features and rule-based processing.
2 Related Work
2.1 Aspect Term Extraction
To recognize terms that express key notions in a
product or service review, a common general ap-
proach has been to extract nouns and noun phrases
as potential terms and then apply a certain filtering
technique to ensure only the most relevant terms
remain. These techniques include statistical asso-
ciation tests (Yi et al., 2003), associative mining
rules with additional rule-based post-processing
steps (Hu and Liu, 2004), and measures of asso-
ciation with certain pre-defined classes of words,
such as part-whole relation indicators (Popescu
and Etzioni, 2005).
2.2 Aspect Category Recognition
Aspect category recognition is often addressed as
a text classification problem, where a classifier
is learned from reviews manually tagged for as-
pects (e.g., Snyder and Barzilay, 2007, Ganu et al.,
2009). Titov and McDonald (2008) present an ap-
proach which jointly detects aspect categories and
their sentiment using a classifier trained on top-
ics discovered via Multi-Grain LDA and star rat-
ings available in training data. Zhai et al. (2010)
presented an approach based on Expectation-
Maximization to group aspect expressions into
user-defined aspect categories.
683
2.3 Sentence Sentiment
Lexicon-based approaches to detecting sentiment
in a sentence rely on a lexicon where words and
phrases are provided with sentiment labels as well
as on techniques to recognize ?polarity shifters?,
phrases causing the polarity of a lexical item
to reverse. Early work on detection of polarity
shifters used surface-level patterns (Yu and Hatzi-
vassilouglu, 2003; Hu and Liu, 2004). Moila-
nen and Pulman (2007) provide a logic-oriented
framework to compute the polarity of grammatical
structures, that is capable of dealing with phenom-
ena such as sentiment propagation, polarity rever-
sal, and polarity conflict. Several papers looked at
different ways to use syntactic dependency infor-
mation in a machine learning framework, to better
account for negations and their scope (Nakagawa
et al., 2010; Socher et al., 2013).
To adapt a generic sentiment lexicon to a new
application domain, previous work exploited se-
mantic relations encoded in WordNet (Kim and
Hovy, 2006), unannotated data (Li et al, 2012), or
queries to a search engine (Taboada et al., 2006).
3 Our Approach
In the following sections, we will describe our ap-
proach to each stage of the Shared Task, reporting
experiments on the provided training data using a
10-fold cross-validation.
3.1 Aspect Term Extraction
During pre-processing training data was parsed
using a dependency parser (Bohnet and Nivre,
2012), and sentiment words were recognized in it
using a sentiment lexicon (see Section 6.1). Can-
didate terms were extracted as single nouns, noun
phrases, adjectives and verbs, enforcing certain
exceptions as detailed in the annotation guidelines
for the Shared Task (Pontiki et al., 2014), namely:
? Sentiment words were not allowed as part of
terms;
? Noun phrases with all elements capitalized
and acronyms were excluded, under the as-
sumption they refer to brands rather than
product aspects;
? Nouns referring to the product class as a
whole (?restaurant?, ?laptop?, etc) were ex-
cluded.
Candidate terms that exactly overlapped with
manually annotated terms were discarded, while
those that did not were used as negative examples
of aspect terms.
In order to provide the term extraction process
with additional lexical knowledge, from the train-
ing data we extracted those manually annotated
terms that corresponded to a single aspect cate-
gory. Then the set of terms belonging to each
category was augmented using WordNet: first we
determined the 5 most prominent hyperonyms of
these terms in the WordNet hierarchy using Resnik
(1992)?s algorithm for learning a class in a seman-
tic hierarchy that best represents selectional pref-
erences of a verb, additionally requiring that each
hypernym is at least 7 nodes away from the root, to
make them sufficiently specific. Then we obtained
all lexical items that belong to children synsets of
these hypernyms, and further extended these lexi-
cal items with their meronyms and morphological
derivatives. The resulting set of lexical items was
later used as an extended aspect term lexicon. We
additionally created a list of all individual lemmas
of content words found in this lexicon.
For each term, we extracted the following fea-
tures to be used for automatic classification:
? Normalized form: the surface form of the
term after normalization;
? Term lemmas: lemmas of content words
found in the term;
? Lexicon term: if the term is in the lexicon;
? Lexicon lemmas ratio: the ratio of lexicon
lemmas in the term;
? Unigram: 3 unigrams on either side of the
term;
? Bigrams: The two bigrams around the term;
? Adj+term: If an adjective depends on the
term
1
or related to it via a link verb (?be?,
?get?, ?become?, etc);
? Sentiment+term: If a sentiment word de-
pends on the term or related via a link verb;
? Be+term: If the term depends on a link verb;
? Subject term: If the term is a subject;
1
In case the term was a multi-word expression, the rela-
tion to the head of the phrase was used.
684
? Object term: If the term is an object.
We first look at how well the manually designed
patterns extracted potential terms. We are primar-
ily interested in recall at this stage, since after that
potential terms are classified into terms and non-
terms with an automatic classifier. The recall on
the restaurants was 70.5, and on the laptops ?
56.9. These are upper limits on recall for the over-
all task of aspect term recognition.
Table 1 and Table 2 compare the performance of
several learning algorithms on the restaurants and
the laptops dataset, respectively
2
.
P R F
Linear SVM 94.42 95.51 94.96
Decision Tree 94.24 92.90 93.56
Na??ve Bayes 84.97 95.67 89.99
kNN (k=5) 82.71 93.50 87.76
Table 1: Learning algorithms on the aspect term
extraction task, restaurants dataset.
P R F
Linear SVM 88.14 94.07 91.00
Na??ve Bayes 93.61 79.46 85.92
Decision Tree 83.87 82.99 83.39
kNN (k=5) 82.83 83.31 83.03
Table 2: Learning algorithms on the aspect term
extraction task, laptops dataset.
On both datasets, linear SVMs performed best,
and so they were used in the subsequent experi-
ments on term recognition. To examine the qual-
ity of each feature used for term classification, we
ran experiments where a classifier was built and
tested without that feature, see Tables 3 and 4, for
the restaurants and laptops datasets respectively,
where a greater drop in performance compared to
the entire feature set, indicates a more informative
feature.
The results show the three most useful features
are the same in both datasets: the occurrence of the
candidate term in the constructed sentiment lexi-
con, the lemmas found in the term, and the nor-
malized form of the term account.
We ran further experiments manually selecting
several top-performing features, but none of the
2
This and the following experiments were run on the train
data supplied by the shared task organizers using 10-fold
cross-validation.
P R F
Lexicon term 91.74 95.01 93.33
Term lemmas 92.43 95.00 93.69
Normalized form 93.45 95.36 94.39
Be+term 93.99 95.28 94.63
Left bigram 94.21 95.09 94.64
All features 94.42 95.51 94.96
Table 3: Top 5 most informative features for the
term extraction subtask, restaurants dataset.
P R F
Lexicon term 88.82 88.61 88.69
Term lemmas 85.02 95.16 89.79
Normalized form 87.79 92.13 89.89
Left bigram 87.83 93.62 90.62
Term is obj 87.79 94.43 90.97
All features 88.14 94.07 91.00
Table 4: Top 5 most informative features for the
term extraction subtask, laptops dataset.
configurations produced significant improvements
on the use of the whole feature set.
Table 5 shows the results of evaluation of the as-
pect term extraction on the test data of the Shared
Task (baseline algorithms were provided by the or-
ganizers). The results correspond to what can be
expected based on the upper limits on recall for
the pattern-based extraction of candidate terms as
well as precision and recall for the classifier.
P R F
Restaurants 77.9 61.1 68.5
Restaurants, baseline 53.9 51.4 52.6
Laptops 60.3 39.1 47.5
Laptops, baseline 40.1 38.1 39.1
Table 5: Aspect term extraction on the test data of
the Shared Task.
3.2 Aspect Category Recognition
To recognize aspect categories in a sentence, we
classified individual clauses found in it, assuming
that each aspect category would be discussed in
a separate clause. Features used for classification
were lemmas of content words; to account for the
fact that aspect terms are more indicative of aspect
categories than other words, we additionally used
entire terms as features, weighting them twice as
much as other features. Table 6 compares the per-
685
formance of several learning algorithms when au-
tomatically recognized aspect terms were not used
as an additional feature; Table 7 shows results
when terms were used as features.
P R F
Linear SVM 66.37 58.07 60.69
Decision Tree 58.07 51.22 53.05
Na??ve Bayes 74.34 46.07 48.63
kNN (k=5) 58.65 43.77 46.57
Table 6: Learning algorithms on the aspect cate-
gory recognition task, aspect terms not weighted.
P R F
Linear SVM 67.23 59.43 61.90
Decision Tree 64.41 55.84 58.36
Na??ve Bayes 78.02 49.57 52.87
kNN (k=5) 67.92 47.91 51.94
Table 7: Learning algorithms on the aspect cate-
gory recognition task, aspect terms weighted.
The addition of aspect terms as separate features
increased F-scores for all the learning methods,
sometimes by as much as 5%. Based on these re-
sults, we used the linear SVM method for the task
submission. Table 8 reports results achieved on
the test data of the Shared Task.
P R F
Restaurants 81.8 67.9 74.2
Baseline 64.8 52.5 58.0
Table 8: Aspect category extraction on the test
data of the Shared Task.
3.3 Aspect Term Sentiment
To recognize sentiment in a sentence, we take a
lexicon-based approach. The sentiment lexicon
we used encodes the lemma, the part-of-speech
tag, and the polarity of the sentiment word. It was
built by combining three resources: lemmas from
SentiWordNet (Baccianella et al., 2010), which do
not belong to more than 3 synsets; the General
Inquirer lexicon (Stone et al., 1966), and a sub-
section of the Roget thesaurus annotated for sen-
timent (Heng, 2004). In addition, we added sen-
timent expressions that are characteristic of the
restaurants and laptop domains, obtained based on
manual analysis of the restaurants corpus used in
(Snyder and Barzilay (2007) and the laptop re-
views corpus used in (Jindal and Liu, 2008).
To detect negated sentiment, we used a list of
negating phrases such as ?not?, ?never?, etc., and
two types of patterns to determine the scope of a
negation. The first type detected negations on the
sentence level, checking for negative phrases at
the start of the sentence; negations detected on the
sentence level were propagated to the clause level.
The second type of patterns detected negated sen-
timent within a clause, using patterns specific to
the part-of-speech of the sentiment word (e.g.,
?AUXV + negation + VB + MAINV?, where
MAINV is a sentiment verb). The output of this
algorithm is the sentence split into clauses, with
each clause being assigned one of four sentiment
labels: ?positive?, ?negative?, ?neutral?, ?con-
flict?. Thus, each term was associated with the
sentiment of the clause it appeared in.
On the test data of the Shared Task, the algo-
rithm achieved the accuracy scores of 76.0 (the
restaurants data, for the baseline of 64.3) and 63.6
(the laptops data, for the baseline of 51.1).
3.4 Category Sentiment
Recall that aspect categories were recognized in a
sentence by classifying its individual clauses. Cat-
egory sentiment was determined from the senti-
ment of the clauses where the category was found.
In case more than one clause was assigned to the
same category and at least one clause expressed
positive sentiment and at least one ? negative,
such cases were classified as conflicted sentiment.
This method achieved the accuracy of 72.8 (on the
restaurants data), with the baseline being 65.65.
4 Conclusion
Our study has shown that aspect terms can be de-
tected with a high accuracy using a domain lexicon
derived from WordNet, and a set of classification
features created with the help of deep linguistic
processing techniques. However, the overall accu-
racy of aspect term recognition is greatly affected
by the extraction patterns that are used to extract
initial candidate terms. We also found that au-
tomatically extracted aspect terms are useful fea-
tures in the aspect category recognition task. With
regards to sentiment detection, our results suggest
that reasonable performance can be achieved with
a lexicon-based approach coupled with carefully
designed rules for the detection of polarity shifts.
686
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SENTIWORDNET 3.0: An Enhanced
Lexical Resource for Sentiment Analysis and Opin-
ion Mining. Proceedings of LREC-2010.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. Pro-
ceedings of EMNLP-CoNLL.
Gayatree Ganu, No?emie Elhadad, and Ameli?e Mar-
ian. 2009. Beyond the Stars: Improving Rating
Predictions using Review Text Content. Proceedings
of Twelfth International Workshop on the Web and
Databases (WebDB 2009).
Adrian Heng. 2004. An exploratory study into the use
of faceted classification for emotional words. Mas-
ter Thesis. Nanyang Technological University, Sin-
gapore.
Minqing Hu and Bing Liu. 2004. Mining opinion
features in customer reviews. Proceedings of the
9th National Conference on Artificial Intelligence
(AAAI-2004).
Nitin Jindal and Bing Liu. 2008. Opinion Spam and
Analysis Proceedings of WWW-2008.
Soo-Min Kim and Eduard Hovy. 2006. Identifying
and analyzing judgment opinions. Proceedings of
HLT/NAACL-2006.
Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang and
Xiaoyan Zhu. 2012. Cross-Domain Co-Extraction
of Sentiment and Topic Lexicons. Proceedings of
ACL-2012.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using CRFs with hidden variables. Proceedings
of NAACL/HLT-2010.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
composition. Proceedings of the Recent Advances
in Natural Language Processing (RANLP 2007).
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect Based Sentiment Analysis. Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014).
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. Pro-
ceedings HLT/EMNLP-2005.
Philip Resnik. 1992. A class-based approach to lexi-
cal discovery Proceedings of the Proceedings of the
30th Annual Meeting of the Association for Compu-
tational Linguists.
Benjamin Snyder and Regina Barzilay 2007. Multi-
ple Aspect Ranking using the Good Grief Algorithm.
Proceedings of NAACL-2007.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng
and Christopher Potts 2013. Recursive Deep Mod-
els for Semantic Compositionality Over a Sentiment
Treebank. Proceedings of EMNLP-2013.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
Cambridge, MA: The MIT Press.
Maite Taboada, Caroline Anthony, and Kimberly Voll.
2006. Creating semantic orientation dictionaries
Proceedings of 5th International Conference on Lan-
guage Resources and Evaluation (LREC).
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. Proceedings of ACL-2008.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen and Jun
Zhao. 2013. Mining Opinion Words and Opinion
Targets in a Two-Stage Framework. Proceedings of
ACL-2013.
Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, and
Wayne Niblack. 2003. Sentiment analyzer: Ex-
tracting sentiments about a given topic using natural
language processing techniques. Proceedings of the
3rd IEEE International Conference on Data Mining
(ICDM-2003), pp. 423-434.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards Answering Opinion Questions: Separating
Facts from Opinions and Identifying the Polarity of
Opinion Sentences. Proceedings of EMNLP-03.
Zhongwu Zhai, Bing Liu, Hua Xu and Peifa Jia. 2011.
Clustering product features for opinion mining. Pro-
ceedings of the 4th ACM International Conference
on Web Search and Data Mining, ACM, pp 347354.
687
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 54?65 Dublin, Ireland, August 23-29 2014.
Exploring Options for Fast Domain Adaptation of Dependency Parsers
Viktor Pekar, Juntao Yu, Mohab El-karef, Bernd Bohnet
School of Computer Science
University of Birmingham
Birmingham, UK
{v.pekar,jxy362,mxe346,b.bohnet}@cs.bham.ac.uk
Abstract
The paper explores different domain-independent techniques to adapt a dependency parser
trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, we-
blogs): co-training, word clusters, and a crowd-sourced dictionary. We examine the relative
utility of these techniques as well as different ways to put them together to achieve maximum
parsing accuracy. While we find that co-training and word clusters produce the most promising
results, there is little additive improvement when combining the two techniques, which suggests
that in the absence of large grammatical discrepancies between the training and test domains,
they address largely the same problem, that of unknown vocabulary, with word clusters being
a somewhat more effective solution for it. Our highest results were achieved by a combination
of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Eval-
uation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012)
showed that they outperform all the shared task submissions that used a single parser to parse
test data, averaging the results across all the test sets.
1 Introduction
Domain adaptation of a statistical dependency parser is a problem that is of much importance for many
practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops
when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomed-
ical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al.,
2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-of-
the-art parsers ? Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et
al., 2013) ? trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes
v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending
on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.
Domain MST MALT Mate
Newswire 84.8 81.7 87.1
Pivot Texts 84.9 83.0 86.6
Broadcast News 79.4 78.1 81.2
Magazines 77.1 74.7 79.3
Broadcast Conversation 73.4 70.5 74.4
CoNLL09 test 86.9 84.7 90.1
Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09
data and tested on different specialist domains.
In a typical domain adaptation scenario, there are in-domain texts that are manually annotated and
that are used to train a general-language parser, and out-of-domain or target domain texts that are
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
54
parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be
available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem
of domain adaption, previous work focused on weakly supervised methods to re-train parsers on auto-
matically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman
et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et
al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target
domain (Plank and Van Noord, 2011; S?gaard and Plank, 2012; Khan et al., 2013b). Another line of
research aims specifically to overcome the lexical gap between the training data and the target domain
texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010),
the use of external lexica and morphological clues to predict PoS tags of unknown target domain words
(Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled
target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from
word co-occurrences obtained from unannotated texts (Zhou et al., 2011).
The goal of this paper is to investigate a combination of such techniques to adapt a general-language
parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual
annotation. In our study we include several techniques that have been shown to be reasonably effective
for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon,
as well as automatically annotated texts produced with the help of co-training. All these techniques
are domain-independent and can be applied to new target domains given unlabelled texts form these
domains. We explore the relative utility of these methods and ways to combine them for maximum
parser accuracy.
2 Related work
2.1 Text normalization
User-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsis-
tent grammar and spelling. Foster (2010) investigated lexical phenomena that appear on online discus-
sion forums that present common problems for parsing and compiled a list of such phenomena along
with their transformations. Applying the transformations to test sentences helped to bring the F-score up
by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than
spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method
and language modelling in order to align misspelled words with their regular spelling. Their method of
cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%.
2.2 External lexica
To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from
the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting
the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10%
reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS
tagging (Li et al., 2012).
2.3 Word clusters
In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used
word clusters computed from unlabelled data as features for training a parser. The same approach has
proved to be effective for out-of-domain parsing, where there are many words in the test data unseen
during training, and word clusters computed from in-domain data similarly help to deal with the vocab-
ulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et
al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts
(Candito et al., 2011) and web texts (?vrelid and Skj?rholt, 2012). Word clusters created with Brown
clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013).
Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing
accuracy both on the Penn Treebank and on web data.
55
2.4 Co-training
Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification
problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or
more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed
that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training
algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed
the both precision and recall of a phrase structure parser can be increased using a co-training procedure
that iteratively adds the most confidently parsed sentences from two different views to the training set.
Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrap-
ping manner.
A number of studies specifically aimed to use co-training for domain adaptation of a dependency
parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a
one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-of-
domain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was
that the agreement between the two classifiers during testing was a very good predictor of accuracy. More
recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm
uses three learners and each learner was designed to learn from those automatically classified unlabelled
data where the other two learners agreed on the classification label.
3 Experimental set-up
3.1 Parsers
In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira,
2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins
et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all
the parsers were the same and came from the Mate parser.
3.2 Baseline
As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation
techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain
test data.
3.3 Data
The experiments were conducted on annotated data on web-related domains available in the Ontonotes
v.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptation
techniques is widely available.
OntoNotes. In experiments with weblog texts, we used the CoNLL09 training dataset (Hajic? et al.,
2009) as the general-language training data. The CoNLL09 test dataset was used to evaluate in-domain
parsing. To create an out-of-domain test set, we selected the last 10% of the weblogs section of the
OntoNotes v5.0 corpus1, in order to make the size of the out-of-domain test data comparable to that of
the in-domain test data, i.e. of CoNLL09 test. The OntoNotes corpus was converted to the CoNLL09
format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007).
SANCL. In order to compare our results with the results achieved by participants in the SANCL-2012
shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers,
newsgroups and reviews). In these experiments we used the training set, test sets, unlabelled data,
as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012).
Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measures
of lexical and grammatical characteristics of the data. The average sentence length (in tokens) and the
average number of subjects, roughly corresponding to the number of clauses in the sentence, aim to
characterize the syntactic complexity of the sentences: the higher these values, the more complex the
1https://catalog.ldc.upenn.edu/LDC2013T19
56
structure of the sentences is likely to be. The ratio of word forms absent from training data describes
how different the train and test data are in terms of vocabulary.
We see that in the OntoNotes test set the average sentence length and the number of subjects per
sentence is very similar to those in the train data. In SANCL test sets, these measures are more different,
but the values indicate a smaller syntactic complexity than in the train data. The amount of unknown
vocabulary in all the four test sets is between 5% and 8%.
CoNLL09 train CoNLL09 test OntoNotes test
Sentences 39,279 2,399 2,150
Tokens 958,167 57,676 42,144
Sentence length 24.61 24.59 23.4
Subjects 1.8 1.83 1.89
Unk. wordforms ratio 0.0 0.011 0.05
Table 2: The size of OntoNotes train and test datasets.
SANCL train Answers test Newsgroups test Reviews test
Sentences 30,060 1,744 1,195 1,906
Tokens 731,678 28,823 20,651 28,086
Sentence length 24.56 18.44 22.79 16.35
Subjects 1.69 1.78 1.62 1.5
Unk. wordforms ratio 0.0 0.064 0.084 0.051
Table 3: The size of SANCL train and test datasets.
Unlabelled Data. As unlabelled target domain data we used the unlabelled dataset from the SANCL-
2012 shared task. In experiments with word clusters, the entire dataset was used without any pre-
processing. In the co-training experiments, we pre-processed the data by removing sentences that are
longer than 500 tokens, or contained non-English words (this reduced the test set by 2%). Table 4 de-
scribes the size of the subsets of the unlabelled data.
Emails Weblogs Answers Newsgroups Reviews
Sentences 1,194,173 524,834 27,274 1,000,000 1,965,350
Tokens 17,047,731 10,356,284 424,299 18,424,657 29,289,169
Table 4: The size of unlabelled datasets.
3.4 Evaluation method
As a measure of parser accuracy, we report labeled attachment scores (LAS), the percentage of depen-
dencies which are attached and labeled correctly. Significance testing was performed using paired t-test.
4 Results and Discussion
4.1 Text normalization
We used a manually compiled lexicon containing Internet-specific spellings of certain words aligned
with their traditional spellings, e.g. u? you, gr8? great, don,t? don?t, as well as a number of regular
expressions to deal with extra symbols usually added for emphasis (This is sooooo good., This *is*
great.). After the original word forms were read by the parser, the lexicon and the regular expressions
were applied to normalize the spelling of the words. This produced only a very insignificant gain on the
baseline. A manual examination of the test data in both OntoNotes and SANCL has shown that in fact
although it comes from the web it contains very few examples of ?Internet speak?.
57
4.2 Word clusters
We used Liang?s (2005) implementation of the Brown clustering algorithm to create clusters of words
found in unlabelled domain texts. The output of the algorithm are word types assigned to discrete hi-
erarchical clusters, with clusters assigned ids in the form of bit strings of varying length corresponding
to clusters of different granularity. We experimentally set the maximum length of the bit string to 6,
collapsing more fine-grained clusters. Instead of replacing the original word forms and/or PoS tags with
cluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; Ta?ckstro?m et
al., 2013), the ids of clusters were used to generate additional features in the representations of the word
forms, as this also produced better results in the preliminary runs. Below we describe experiments with
several other parameters of the clustering algorithm.
Number of clusters. As an input parameter, the Brown clustering algorithm requires a desired number
of clusters. Initially discarding all word types with a count of less than 3, we experimented with different
numbers of clusters and found that an optimal settings lies around 600 and 800 clusters, which gives an
improvement on the baseline of 0.9% for out-of-domain texts; but there does not seem to be noticeable
differences between specific numbers of clusters (see Table 5, statistically significant differences to the
baseline are indicated by stars2).
Number of clusters CoNLL09 OntoNotes
50 90.46** 78.10*
100 90.28* 78.40**
200 90.27 78.39**
400 90.37** 78.20**
600 90.40** 78.43**
800 90.30* 78.14**
Baseline 90.07 77.54
Table 5: The effect of the number of word clusters on in- and out-of-domain parsing, using the reviews
and weblogs subsets of the SANCL-2012 unlabelled data.
Filtering rare words. Due to the inevitable data sparseness, the algorithm is likely to mis-cluster
infrequent words. At the same time, it is rare words that are not seen during parser training and are
potentially of greatest value if included into word clusters. We examined several thresholds on word
frequency and their impact on parsing accuracy (see Table 6; statistically significant differences to the
baseline are indicated by stars). We found very slight differences between these three thresholds, al-
though the cut-off point of 3 showed the best results. Hence in further experiments with word clusters
we used this cut-off point.
Min. freq. CoNLL09 OntoNotes
1 90.36** 78.12*
3 90.40** 78.43**
5 90.22 78.24**
Table 6: The effect of filtering out rare words on word clusters, using the reviews and weblogs subsets
of the SANCL-2012 unlabelled data.
Amount of unlabelled data. To examine the effect that the size of unlabelled data from which word
clusters are computed, has on parser accuracy, we compared parser accuracy achieved when using only
the reviews and weblogs subsets of the SANCL corpus (39.6 mln word tokens), and when using the
entire SANCL dataset (75.2 mln tokens). These results are shown in Table 7, significant improvements
on the smaller set are indicated by stars. As expected, a larger amount of data does improve the parsing
accuracy, and the improvement is greater for out-of-domain parsing (+0.55% vs. +0.32%).
2In this and the following tables, one star indicates significance at the p < 0.05 level, two stars at the p < 0.01 level.
58
CoNLL09 OntoNotes
Reviews and Weblogs 90.30 78.14
Entire SANCL dataset 90.62* 78.69*
Table 7: The effect of the size of unlabelled data on word clusters, discarding word types with count less
than 3.
Relevant domain data. Furthermore, we were interested if simply adding more unlabelled data, not
necessarily from the relevant domain, produced the same increase in accuracy. We obtained the plain-
text claims and description parts of 13,600 patents freely available in the Global IP Database which
is based on the Espacenet3, creating a corpus with 42.5 mln tokens, i.e. which was similar in size to
the reviews and weblogs sections of the SANCL unlabelled dataset. Table 8 compares results achieved
when building clusters from the patents corpus and when using the reviews and weblogs texts from the
SANCL unlabelled dataset. Despite the fact that the size of the two datasets is comparable, we find that
while creating clusters from an irrelevant domain does gain on the baseline (+0.25%), the improvement
for clusters built from the relevant domain texts is noticeably higher (+0.6%). The difference between
the accuracy on the legal texts and the accuracy on the reviews and weblogs texts is significant at the
p < 0.05 level.
CoNLL09 OntoNotes
Legal texts 90.19 77.77
Reviews and Weblogs 90.30 78.14*
Table 8: The effect of the domain of unlabelled data on word clusters, discarding word types with count
less than 3.
4.3 External lexicon
It is possible to supply to the dependency parser an external lexicon, where word forms are provided
with PoS tags. Wiktionary, a companion project for Wikipedia that aims to produce a free, large-scale
multilingual dictionary, is a large and constantly growing crowd-sourced resource that appears attrac-
tive for NLP research. Wiktionary encodes word definitions, pronunciation, translations, etymology,
word forms and part-of-speech information. PoS tag dictionaries derived from Wiktionary have been
previously used for out-of-domain PoS tagging (Li et al., 2012) and for PoS tagging of resource-poor
languages (Ta?ckstro?m et al., 2013).
To create a lexicon for the parser, we extracted 753,970 English word forms and their PoS tags from
a dump of Wiktionary4. Wiktionary uses a rather informal set of PoS labels; to convert them to the
CoNLL09 tag set, we manually aligned all unique PoS tags found in Wiktionary with those of the
CoNLL09 tag set. We compared the accuracy achieved by the parser when the lexicon was supplied,
as well as when the lexicon was supplied together with the best configuration word clusters (800 clusters
built from the entire SANCL dataset after filtering words with the count less than 3). Table 9 shows
results achieved with these settings in comparison to the baseline (improvements on the baseline are in-
dicated with stars). When the lexicon is used on its own, we observe only slight gains on the baseline,
on both in-domain and out-domain data, and neither are statistically significant. When combining the
lexicon and word clusters, the accuracy actually decreases compared to using word clusters on their own.
Thus the best combination of domain adaptation techniques so far included the use of 800 word clusters
built from the entire SANCL unlabelled dataset, after filtering out word forms with the count less than 3,
with text normalization, but without the Wiktionary lexicon (+1.15% on the baseline).
3http://www.epo.org/searching/free/espacenet.html
4http://wiki.dbpedia.org/Wiktionary
59
CoNLL09 OntoNotes
Wiktionary 90.22 77.73
Clusters 90.62** 78.69**
Wiktionary+Clusters 90.44 78.49**
Baseline 90.07 77.54
Table 9: The effect of the Wiktionary lexicon on parsing accuracy.
4.4 Co-Training
Following Sagae (2007), the overall approach to parser co-training we adopted was as follows. First,
several parsers were combined to generate additional training data from unlabelled data, i.e. were used
as source learners for co-training. Then, the Mate parser was re-trained on the augmented training set
and tested on a test set, i.e. used as the evaluation learner. The reason Mate was selected the evaluation
learner was that it achieved the best results on the test data in its default settings (see Table 10).
CoNLL09 OntoNotes
Mate 90.07 77.54
MST 86.9 75.35
Turbo 85.94 74.85
Malt 84.72 72.63
Table 10: The baselines of parsers used in co-training experiments.
Agreement-based co-training. We first experimented with three pairwise parser combinations: using
Mate as one source learner and each of the other three parsers as the other source learner in order to obtain
additional training data. If two learners agreed on the parse of an unlabelled sentence, i.e. assigned each
word form the same dependency label and attached it to the same head, this was taken as an indication of
a correct parse, and the sentence was added to the training set. We experimented with different amounts
of the additional training sentences added to the main training set in such a manner: 10k, 20k, and 30k
sentences. The results of these experiments are shown in Table 11 (significant differences to the baseline
results are indicated by stars). The best result is obtained by Mate Malt pair, which outperforms the
baseline by just above 1%.
+10k +20k +30k
Mate+Malt 78.22** 78.61** 78.61**
Mate+MST 78.10** 78.23** 78.31**
Mate+Turbo 77.94** 77.84* 77.99**
Baseline 77.54
Table 11: Agreement-based co-training using two parsers.
Removing short sentences from unlabelled data. We noticed that among those sentences where
two parsers agreed, many tended to be very short: the average number of tokens in generated additional
training data was 8 per sentence, while both the training and test set contain much longer sentences
on average: the OntoNotes test set had 19.6 tokens/sentence and the CoNLL09 training set had 24.4
tokens/sentence. Such short sentences in the additional training data may be less useful or even harmful
for learning an accurate model of the target domain, than those that approximate both training and test
data. We experimented with several thresholds (4, 5, and 6 tokens) on the sentence length below which
sentences were removed from the additional training data. Table 12 shows that discarding short sentences
did improve accuracy by up to 0.25%, though none of the improvements were significant.
Three learners co-training. In the previous experiments, the Mate parser was used both as a source
learner and as the evaluation learner. Therefore it was likely that the additional training data did not
60
Mate+Malt, +30k Avg. Length
>6 tokens 78.88 13.1
>5 tokens 78.61 12.67
>4 tokens 78.67 11.94
All sentences 78.61 8.35
Table 12: The effect of removing short sentences from generated training data.
contain sufficiently novel examples based on which the evaluation parser could adapt better to the new
domain. Thus we next tried the tri-training algorithm (Zhou and Li, 2005), where two parsers are used
as source learners and a third as the evalaution learner. We used Malt and MST as source learners,
identifying sentences which they parsed in the same manner, and using these sentences to retrain the
Mate parser. We find that the tri-training algorithm performs better than the set-up with two parsers:
on 10k and 20k additional sentences, it achieves an accuracy increase on Mate+Malt, significant at the
p < 0.05 level (see Table 13).
+10k +20k +30k
Mate+Malt+MST 78.70* 79.12* 78.95
Mate+Malt 78.43 78.70 78.88
Table 13: Accuracy scores for tri-training (Mate+Malt+MST) and the best two-parser co-training algo-
rithm (Mate+Malt).
5 Combining co-training with clusters and an external lexicon
5.1 OntoNotes test set
We explored several possibilities to combine co-training with word clusters and an external lexicon, each
time supplying word clusters and/or the lexicon to the Mate parser when it is being retrained on additional
training data and applied to the test data. The following configurations of each of the techniques were
used:
? Word clusters: 800 clusters generated from the entire SANCL unlabelled dataset, after discarding
word types with the count less than 3.
? Lexicon: Wiktionary
? Co-training: Retraining the Mate parser on the combination of initial training set and 20k automat-
ically parsed sentences (agreed by Malt and MST) which contained more than 6 tokens.
The results showed that all three combinations failed to obtain significant improvements over co-
training alone. The best result is achieved by combining co-training and clusters, which obtains an
increase of only 0.09% on co-training; this is however, the greatest overall improvement on the baseline
(+1.67%). The combination of co-training and a Wiktionary lexicon in fact harms accuracy (see Table
14).
5.2 SANCL test set
In order to compare different technique combinations with the results achieved by participants of the
SANCL-2012 shared task, we evaluated them on the SANCL test set5.
As the results in Table 15 indicate, similarly to the results on OntoNotes, word clusters usually fare
much better than the Wiktionary-based lexicon, while the latter fails to produce statistically significant
5Note that the data was annotated in the Stanford format.
61
OntoNotes
Co-training 79.12**
Clusters 78.69**
Wiktionary 77.73
Co-training+Clusters 79.21**
Co-training+Wiktionary 78.89*
Co-training+Clusters+Wiktionary 79.19**
Baseline 77.54
Table 14: Combination of co-training with word clusters and an external lexicon, OntoNotes test set.
improvements on the baseline. The best accuracy overall was achieved by combinations of techniques,
in all the three subdomains, improving on the baseline by up to 1.3%.
Comparing the results achieved by our best configurations with the results of the shared task, we see
that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31
vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our
results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results
were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al.,
2012); our highest results only produced by the Mate parser, hence our best configuration achieved the
best performance of a single parser.
Answers Newsgroups Reviews Average
Co-training 77.18 82.72** 78.21 79.37
Clusters 78.04** 83.06* 79.03** 80.04
Wiktionary 77.61 82.8 78.32 79.57
Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31
Co-training+Clusters 78.05* 83.29** 78.8** 80.04
Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17
Baseline 77.03 82.4 78.12 79.18
SANCL Stanford-2 77.5 83.56 79.7 80.25
SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46
Table 15: Combination of co-training with word clusters and an external lexicon, SANCL test set.
The results on both the OntoNotes and SANCL datasets show that on their own, word clusters and co-
training often improve significantly on the baseline, but their combination results only in minor further
improvements (only up to 0.32%). Word clusters aim specifically to deal with the unknown vocabulary
problem, and, since there seem to be no major grammatical differences between the train and test do-
mains (see Section 3.3), it is likely that the main benefit derived from co-training is the compensation
for unknown domain vocabulary. Word clusters also seem a better way to approach this problem: they
perform better than co-training on three out of four subdomains. The explanation that unknown vocab-
ulary is the main issue for domain adaptation in this domain pair is further supported by the fact that
combinations of word clusters with a Wiktionary lexicon sometimes performed better than combinations
involving co-training (on newsgroups and reviews).
6 Conclusion
In this paper we described experiments with several domain adaptation techniques, in order to quickly
adapt a general-language parser to parse web data. We find that the best combination of the techniques
improves significantly on the baseline (up to 1.67%), and achieves very promising results on the SANCL-
2012 shared task data, outperforming all submissions that used a single parser, in terms of labelled
accuracy score averaged across three test sets.
62
Our experiments with word clusters showed that word clusters derived from unlabelled domain texts
consistently contribute to a greater parsing accuracy, and that both the domain relevance of the unlabelled
data and its quantity are major factors for successful exploitation of word clusters. Experiments with a
crowd-sourced PoS lexicon however were not as conclusive: whereas supplying the lexicon to the parser
often resulted in certain accuracy gains, they were not as large as those for word clusters. This suggests
word clusters created automatically from relevant domain texts are a better tool to deal with unknown
vocabulary than a generic hand-crafted and wide-coverage lexicon. Another interesting finding was
that co-training was most effective when the evaluation parser was not used for creating extra training
data (the so-called tri-training technique), and when removing very short sentences from automatically
labelled data before re-training the evaluation parser.
With respect to combining co-training with word clusters, we could not find clear evidence for additive
improvement. This suggests that co-training solves largely the same problem as word clusters, i.e.,
unknown target domain vocabulary, and that for the web texts under study unknown vocabulary is a much
more significant impediment for domain adaptation than grammatical differences between domains.
Acknowledgements
The research was supported by FP7 ICT project ?Workbench for Interactive Contrastive Analysis of
Patent Documentation? under grant no. FP7-SME-606163.
References
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency
parsing. In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT? 98, pages 92?100, New York, NY,
USA. ACM.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richa?rd Farkas Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Associtation for Computational
Linguistics, 1.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Alicia Burga, Joan Codina, Gabriella Ferraro, Horacio Saggion, and Leo Wanner. 2013. The challenge of syntactic
dependency parsing adaptation for the patent domain. In ESSLLI-13 Workshop on Extrinsic Parse Improvement.
Marie Candito, Enrique Henestroza Anguiano, and Djam Seddah. 2011. A word clustering approach to domain
adaptation: Effective parsing of biomedical texts. In IWPT, pages 37?42. The Association for Computational
Linguistics.
Jennifer Foster. 2010. ?cba to check the spelling?: Investigating parser performance on discussion forum posts. In
HLT-NAACL, pages 381?384. The Association for Computational Linguistics.
Phani Gadde, L. V. Subramaniam, and Tanveer A. Faruquie. 2011. Adapting a WSJ trained part-of-speech tag-
ger to noisy text: Preliminary results. In Proceedings of the 2011 Joint Workshop on Multilingual OCR and
Analytics for Noisy Unstructured Text Data, MOCRAND11, pages 51?58, New York, NY, USA. ACM.
Daniel Gildea. 2001. Corpus variation and parser performance. In Lillian Lee and Donna Harman, editors,
Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ?01,
pages 167?202, Stroudsburg. Association for Computational Linguistics.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s Ma`rquez,
Adam Meyers, Joakim Nivre, Sebastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In 16th
Nordic Conference of Computational Linguistics, pages 105?112. University of Tartu.
63
Mohammad Khan, Markus Dickinson, and Sandra Ku?bler. 2013a. Does size matter? text and grammar revision
for parsing social media data. In Proceedings of the Workshop on Language Analysis in Social Media, pages
1?10, Atlanta, Georgia, June. Association for Computational Linguistics.
Mohammad Khan, Markus Dickinson, and Sandra Ku?bler. 2013b. Towards domain adaptation for parsing web
data. In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP, pages 357?364. RANLP 2011
Organising Committee / ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In In Proc.
ACL/HLT.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. Dcu-
paris13 systems for the sancl 2012 shared task.
Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Robert Dale, Kam-Fai Wong, Jian
Su, and Oi Yee Kwong, editors, IJCNLP, volume 3651 of Lecture Notes in Computer Science, pages 58?69.
Springer.
Shen Li, Joo Graa, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP-CoNLL, pages
1389?1398. ACL.
Percy Liang. 2005. Semi-supervised learning for natural language. In MASTERS THESIS, MIT.
Andre? FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and Ma?rio AT Figueiredo. 2010. Turbo parsers:
Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34?44. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-training for biomedical parsing. In ACL (Short Papers), pages
101?104. The Association for Computer Linguistics.
David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning.
2012. Stanfords system for parsing the english web. In Workshop on the Syntactic Analysis of Non-Canonical
Language (SANCL 2012). Montreal, Canada.
Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algo-
rithms. In EACL.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932. sn.
Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages 351?359. Association for Computational Linguistics.
Lilja ?vrelid and Arne Skj?rholt. 2012. Lexical categories for improved parsing of web data. In Proceedings of
COLING 2012: Posters, pages 903?912, Mumbai, India, December. The COLING 2012 Organizing Committee.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380?
390. The Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the
First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 705?713, Stroudsburg, PA, USA. Association for Computational Linguistics.
Barbara Plank and Gertjan Van Noord. 2011. Effective measures of domain similarity for parsing. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1, pages 1566?1576. Association for Computational Linguistics.
Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006. Lexical adaptation of link
grammar to the biomedical sublanguage: a comparative evaluation of three approaches. BMC Bioinformat-
ics, 7(Suppl 3).
64
Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, SPMRL ?11, pages 62?67, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenji Sagae. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In In
Proceedings of the Eleventh Conference on Computational Natural Language Learning.
Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting
of the North American Chapter of the Association for Computational Linguistics on Language Technologies,
NAACL ?01, pages 1?8, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anders S?gaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Workshop on the Syntactic
Analysis of Non-Canonical Language (SANCL2012), Montreal, Canada.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL, pages
331?338. The Association for Computer Linguistics.
Peter Szolovits. 2003. Adding a medical lexicon to an English parser. In AMIA Annual Symposium Proceedings,
volume 2003, page 639. American Medical Informatics Association.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging. TACL, 1:1?12.
Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu. 2012. Hit dependency parsing: Bootstrap
aggregating heterogeneous parsers. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. Knowledge and
Data Engineering, IEEE Transactions on, 17(11):1529?1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve
statistical dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1, HLT ?11, pages 1556?1565, Stroudsburg, PA, USA.
Association for Computational Linguistics.
65
