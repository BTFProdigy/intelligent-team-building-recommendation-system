Memory-Inductive Categorial Grammar:
An Approach to Gap Resolution in Analytic-Language Translation
Prachya Boonkwan Thepchai Supnithi
Human Language Technology Laboratory
National Electronics and Computer Technology Center (NECTEC)
112 Thailand Science Park, Phaholyothin Road,
Khlong 1, Pathumthani 12120, Thailand
{prachya.boonkwan, thepchai.supnithi}@nectec.or.th
Abstract
This paper presents a generalized frame-
work of syntax-based gap resolution in ana-
lytic language translation using an extended
version of categorial grammar. Translat-
ing analytic languages into Indo-European
languages suffers the issues of gapping,
because ?deletion under coordination? and
?verb serialization? are necessary to be re-
solved beforehand. Rudimentary operations,
i.e. antecedent memorization, gap induction,
and gap resolution, were introduced to the
categorial grammar to resolve gapping is-
sues syntactically. Hereby, pronominal ref-
erences can be generated for deletion under
coordination, while sentence structures can
be properly selected for verb serialization.
1 Background
Analytic language, such as Chinese, Thai, and Viet-
namese, is any language whose syntax and meaning
relies on particles and word orders rather than inflec-
tion. Pronouns and other grammatical information,
such as tense, aspect, and number, expressed by use
of adverbs and adjectives, are often omitted. In addi-
tion to deletion under coordination and verb serial-
ization, called gapping (Hendriks, 1995), translation
from analytic languages into Indo-European ones
becomes a hard task because (1) an ordinary parser
cannot parse some problematic gapping patterns and
(2) these omissions are necessary to be resolved be-
forehand. We classify resolution of the issue into
two levels: syntactic/semantic and pragmatic. Gap-
ping, which we considered as a set of bound vari-
ables, can be resolved in syntactic/semantic level
(Partee, 1975). Omission of other grammatical in-
formation is, on the contrary, to be resolved in prag-
matic level because some extra-linguistic knowledge
is required. Consequently, we concentrate in this pa-
per the resolution of gapping by means of syntax and
semantics.
Many proposals to gap resolution were intro-
duced, but we classify them into two groups: non-
ellipsis-based and ellipsis-based. Non-ellipsis-based
approach is characterized by: (a) strong proof sys-
tem (Lambek, 1958), and (b) functional composition
and type raising that allow coordination of incom-
plete constituents, such as CG (Ajdukiewicz, 1935;
Bar-Hillel, 1953; Moortgat, 2002), CCG (Steed-
man, 2000), and multimodal CCG (Baldridge and
Kruijff, 2003). Proposals in this approach, such
as (Hendriks, 1995; Ja?ger, 1998a; Ja?ger, 1998b),
introduced specialized operators to resolve overt
anaphora, while covert anaphora is left unsolved.
Ellipsis-based approach is characterized by treat-
ing incomplete constituents as if they are of the
same simple type but contain ellipsis inside (Yatabe,
2002; Cryssmann, 2003; Beavers and Sag, 2004).
However, Beavers and Sag (2004) evidenced that
ellipsis-based analysis possibly reduces the accept-
ability of language, because the resolution is per se
completely uncontrolled.
In this paper, we introduce an integration of the
two approaches that incorporates strong proof sys-
tem and ellipsis-based analysis. Antecedent memo-
rization and gap induction are introduced to imitate
ellipsis-based analysis. The directions of ellipsis are
80
also used to improve the acceptability of language.
The rest of the paper is structured as follows. Sec-
tion 2 describes the formalization of our method.
Section 3 evidences the coverage of the framework
on coping with the gapping issues in analytic lan-
guages. Section 4 further discusses coverage and
limitations of the framework comparing with CG
and its descendants. Section 5 explains relevance
of the proposed formalism to MT. Finally, Section 6
concludes the paper and lists up future work.
2 Memory-Inductive Categorial Grammar
Memory-Inductive Categorial Grammar, abbrevi-
ated MICG, is a version of pure categorial grammar
extended by ellipsis-based analysis. On the con-
trary, it relies on antecedent memorization, gap in-
duction, and gap resolution that outperform CCG?s
functional composition and type raising.
All grammatical expressions of MICG are, like
CG, distinguished by a syntactic category identify-
ing them as either a function from arguments of one
type to result another (a.k.a. function), or an argu-
ment (a.k.a. primitive category). Let us exemplify
the MICG by defining an example grammar G be-
low.
John,Mary,sandwich,noodle ? np
eats ? (np\s)/np
and ? &
The lexicons John, Mary, sandwich, and noodle are as-
signed with a primitive category np. The lexicon
eats is assigned with a function that forms a sentence
s after taking np from the right side (/np) and then
taking np from the left side (np\). The lexicon and is
assigned with a conjunction category (&). By means
of syntactic categories assigned to each lexicon, the
derivation for a simple sentence ?John eats noodle? is
shown in (1).
John eats noodle
John ? np eats ? (np\s)/np noodle ? np
eats?noodle ? np\s
John ? (eats?noodle) ? s
(1)
CG suffers some patterns of coordination e.g.
SVO&SO as exemplified in (2).
John eats noodle, and Mary, sandwich.(2)
One should find that the second conjunct cannot be
reduced into s by means of CG, because it lacks of
the main verb ?eats.? The main verb in the first con-
junct should be remembered and then filled up to
the ellipsis of the second conjunct to accomplish the
derivation. This matter of fact motivated us to de-
velop MICG by introducing to CG the process of
remembering an antecedent from a conjunct, called
memorization, and filling up an ellipsis in the other
conjunct, called induction. There are three manda-
tory operations in MICG: antecedent memorization,
gap induction, and gap resolution.
One of two immediate formulae combined in
the derivation can be memorized as an antecedent.
The resulted syntactic category is modalized by the
modality 2DF , where D is a direction of memoriza-
tion (< for the left side and > for the right side),
and F is the memorized formula. The syntactic
structure of the memorized formula is also modal-
ized with the notation 2 to denote the memoriza-
tion. It is restricted in MICG that the memorized for-
mula must be unmodalized to maintain mild context-
sensitivity. For example, let us consider the deriva-
tion of the first conjunct of (2), ?John eats noodle,?
with antecedent memorization at the verb ?eats? in
(3). As seen, a modalized formula can combine with
another unmodalized formula while all modalities
are preserved.
John eats noodle
John ? np 2eats ? (np\s)/np noodle ? np
2eats?noodle ? 2<eats?(np\s)/np(np\s)
John ? (2eats?noodle) ? 2<eats?(np\s)/nps
(3)
Any given formula can be induced for a missing
formula, or a gap, at any direction, and the induced
gap contains a syntactic category that can be com-
bined to that of the formula. The resulted syntactic
category of combining the formula and the gap is
modalized by the modality 3DF , where D is a direc-
tion of induction, and F is the induced formula at the
gap. The syntactic structure of F is an uninstantiated
variable and also modalized with the notation 3 to
denote the induction. The induced formula is neces-
sary to be unmodalized for mild context-sensitivity.
For example, let us consider the derivation of the
second conjunct of (2), ?Mary, sandwich,? with gap in-
duction before the word ?sandwich? in (4). The vari-
81
able of syntactic structure will be resolved with an
appropriate antecedent containing the same syntac-
tic category in the gap resolution process.
Mary sandwich
Mary ? np sandwich ? np
3X ? sandwich ? 3<X?(np\s)/np(np\s)
Mary ? (3X ? sandwich) ? 3<X?(np\s)/nps
(4)
Gap resolution matches between memorized an-
tecedents and induced gaps to associate ellipses to
their antecedents during derivation of coordination
and serialization. That is, two syntactic categories
2D1F1 C and 3
D2
F2 C are matched up and canceled from
the resulted syntactic category, if they have the same
syntactic categories C, their directions D1 and D2
are equal, and their memorized/induced formulae F1
and F2 are unified. For example, let us consider the
derivation of ?John eats noodle, and Mary, sandwich?
in Figure 1. The modalities 2<eats?(np\s)/nps and
3<X?(np\s)/nps are matched up together. Their mem-
orized/induced formulae are also unified by instan-
tiating the variable X with ?eats?. Eventually, af-
ter combining them and the conjunction ?and,? the
derivation yields out the formula (John ? (2eats ?
noodle))? (and? (Mary? (3eats? sandwich))) ? s.
Gap resolution could also indicate argument shar-
ing in coordination and serialization. 3D1F1 C and
3D2F2 C can be also matched up, if they have the same
syntactic categories C, their directions D1 and D2
are equal, and their memorized/induced formulae F1
and F2 are unified. However, they must be preserved
in the resulted syntactic category. For example, let
us consider the derivation in Figure 2. By means of
unification of induced formulae, the variables X and
Y are unified into the variable Z.
A formal definition of MICG is given in Ap-
pendix A. MICG is applied to resolve deletion under
coordination and serialization in analytic languages
in the next section.
3 Gap Resolution in Analytic Languages
There are two causes of gapping in analytic lan-
guages: coordination and serial verb construction.
Each of which complicates the analysis module of
MT to resolve such issue before transferring. In this
section, problematic gapping patterns are analyzed
in forms of generalized patterns by MICG. For sim-
plification reason, syntactic structure is suppressed
during derivation.
3.1 To resolve gapping under coordination
Coordination in analytic languages is more com-
plex than that of Indo-European ones. Multi-
conjunct coordination is suppressed here because
biconjunct coordination can be applied. Besides
SVO&VO and SV&SVO patterns already resolved
by CCG (Steedman, 2000), there are also SVO&SV,
SVO&V, SVO&SO (already illustrated in Figure 1),
and SVO&SA patterns.
The pattern SVO&SV exhibits ellipsis at the ob-
ject position of the second conjunct. The analysis of
SVO&SV is illustrated in (5). It shows that the ob-
ject of the first conjunct is memorized while the verb
of the second conjunct is induced for the object.
S V O & S V
np (np\s)/np np & np (np\s)/np
2>np(np\s) 3>np(np\s)
2>nps 3>nps
s
(5)
Analysis of the sentence pattern SVO&V, illus-
trated in (6), exhibits ellipses at the subject and the
object positions of the second conjunct. The subject
and the object of the first conjunct are memorized,
while the verb of the second conjunct is induced
twice for the object and for the subject, respectively.
S V O & V
np (np\s)/np np & (np\s)/np
2>np(np\s) 3>np(np\s)
2<np2>nps 3<np3>nps
s
(6)
The pattern SVO&SA exhibits ellipsis at the pred-
icate position of the second conjunct, because only
the adverb (A) is left. Suppose the adverb, typed
(np\s)/(np\s), precedes the predicate. Illustrated in
(7), the predicate of the first conjunct is memorized,
while the adverb of the second conjunct is inducted
for the predicate.
S V O & S A
np (np\s)/np np & np (np\s)/(np\s)
np\s 3>
np\s(np\s)
2>
np\ss 3
>
np\ss
s
(7)
82
John eats noodle and Mary, sandwich
John? (2eats ?noodle) ? 2<eats?(np\s)/nps and ? & Mary? (3X ? sandwich) ? 3
<
X?(np\s)/nps
(John ? (2eats?noodle))? (and ? (Mary ? (3eats ? sandwich))) ? s
Figure 1: Derivation of ?John eats noodle, and Mary, sandwich.?
eats noodle and drinks coke
3X ? (eats?noodle) ? 3<X?nps and ? & 3Y ? (drinks?coke) ? 3<Y?nps
(3Z ? (eats ?noodle))? (and ? (3Z ? (drinks?coke))) ? 3<Z?nps
Figure 2: Preservation of modalities in derivation
3.2 To resolve gapping under serial verb
construction
Serial verb construction (SVC) (Baker, 1989) is con-
struction in which a sequence of verbs appears in
what seems to be a single clause. Usually, the
verbs have a single structural object and share log-
ical arguments (Baker, 1989). Following (Li and
Thompson, 1981; Wang, 2007; Thepkanjana, 2006),
we classify SVC into three main types: consecu-
tive/concurrent events, purpose, and circumstance.
No operation specialized for tracing antecedent
projection in consecutive/concurrent event construc-
tion has been proposed in CG or its descendants. In
MICG, the serialization operation is specialized for
this construction. For example, a Chinese sentence
from (Wang, 2007) in (8) is analyzed as in (9).
ta? ma?i pia`o j??n qu`
he buy ticket enter go
?He buys a ticket and then goes inside.?
(8)
ta? ma?i pia`o j??n qu`
np (np\s)/np np np\s np\s
np\s 3<nps 3<nps
2<nps 3<nps
s
(9)
Illustrated in (9), the subject argument ta? ?he? is pro-
jected through the verb sequence by means of mem-
orization and induction modalities.
Purpose construction can also be handled by
MICG. For example, a Thai sentence in (10) is ana-
lyzed as in (11).
kha?V tO`: thO?: paj Cha?j naj ba?:n
he attach pipe go use in house
?He attaches pipes to use in the house.?
(10)
kha?V tO`: thO?: paj Cha?j naj ba?:n
np (np\s)/np np s\s (np\s)/np (s\s)/np np
2>np(np\s) 3>np(np\s) s\s
2<np2>nps 3<np3>nps
2<np2>nps 3<np3>nps
s
(11)
Illustrated in (11), the two logical arguments, i.e. the
subject kha?V ?he? and the object thO?: ?pipe,? are pro-
jected through the construction.
SVC expressing circumstance of action is syntac-
tically considered much as consecutive event con-
struction. For example, a Chinese sentence from
(Wang, 2007) in (12) is analyzed as in (13).
wo? yo`ng kua`izi ch?? fa`n
I use chopstick eat meal
?I eat meal with chopsticks.?
(12)
wo? yo`ng kua`izi ch?? fa`n
np (np\s)/np np (np\s)/np np
np\s np\s
2<nps 3<nps
s
(13)
4 Coverage and Limitations
Proven in Theorem 1 in Appendix A, memorized
constituents and induced constituents are cross-
serially associated. Controlled by order and di-
rection, each memorized constituent is guaranteed
to be cross-serially associated to its corresponding
induced gap, while each gap pair is also cross-
serially associated revealing argument sharing. This
causes cross-serial association, illustrated in Fig-
ure 3, among memorized constituents and induced
gaps. Since paired modalities are either eliminated
or preserved and no modalities are left on the start
83
symbol, it guarantees that there is eventually no
modality in derivation. In conclusion, no excessive
gap is over-generated in the language.
p1 q1 p2 q2 . . . pn qn pn+1 qn+1 pn+2 qn+2 . . . p2n q2n p2n+1
Figure 3: Cross-serial association
MICG?s antecedent memorization and gap induc-
tion perform well in handling node raising. Node
raising is analyzed in terms of MICG by memorizing
the raised constituent at the conjunct it occurs and
inducing a gap at the other conjunct. For example,
the right node ?ice cream? is raised in the sentence ?I
like but you don?t like ice cream.? The sentence can
be analyzed in terms of MICG in (14).
I like but you don?t like ice cream
np (np\s)/np & np (np\s)/np np
3>np(np\s) 2>np(np\s)
3>nps 2>nps
s
(14)
Topicalization and contraposition are still the is-
sues to be concerned for coverage over CCG. For
example, in an example sentence ?Bagels, Yo said
that Jan likes? from (Beavers and Sag, 2004), the
NP ?Bagels? is topicalized from the object position
of the relative clause?s complement. (15) shows un-
parsability of the sentence.
Bagels, Yo said that Jan likes
np np (np\s)/cl cl/s np (np\s)/np
3>np(np\s)
3>nps
3>nps
3>np(np\s)
3>nps
?????
(15)
Furthermore, constituent shifting, such as dative
shift and adjunct shift, is not supported by MICG.
We found that it is also constituent extraction as
consecutive constituents other than the shifted one
are extracted from the sentence. For example, the
adjunct ?skillfully? is shifted next to the main verb
in the sentence ?Kahn blocked skillfully a powerful
shot by Ronaldo? from (Baldridge, 2002) in (16).
a powerful shot
Kahn blocked skillfully by Ronaldo
np (np\s)/np (np\s)\(np\s) np
3>np(np\s)
3>np(np\s)
3>nps
?????
(16)
Since MICG was inspired by reasons other than
those of CCG, the coverage of MICG is therefore
different from CCG. Let us compare CG, CCG, and
MICG in Table 1. CCG initially attempted to han-
dle linguistic phenomena in English and other Indo-
European languages, in which topicalization and da-
tive shift play an important role. Applied to many
other languages such as German, Dutch, Japanese,
and Turkish, CCG is still unsuitable for analytic lan-
guages. MICG instead was inspired by deletion un-
der coordination and serial verb construction in ana-
lytic languages. We are in progress to develop an ex-
tension of MICG that allows topicalization and da-
tive shift avoiding combinatoric explosion.
5 Relevance to RBMT
Major issues of MT from analytic languages into
Indo-European ones include three issues: anaphora
generation, semantic duplication, and sentence
structuring. Both syntax and semantics are used to
solve such problems by MICG?s capability of gap
resolution. Case studies from our RBMT are exem-
plified for better understanding.
Our Thai-English MT system is rule-based and
consists of three modules: analysis, transfer, and
generation. MICG is used to tackle sentences with
deletion under coordination and SVC which cannot
be parsed by ordinary parsers. For good speed effi-
ciency, an MICG parser was implemented in GLR-
based approach and used to analyze the syntactic
structure of a given sentence before transferring.
The parser detects zero anaphora and resolves their
antecedents in coordinate structure, and reveals ar-
gument sharing in SVC. Therefore, coordinate struc-
ture and SVC can be properly translated.
No experiment has been done on our system yet,
but we hope to see an improvement of translation
quality. We planned to evaluate the translation accu-
racy by using both statistical and human methods.
84
Table 1: Coverage comparison among CG, CCG, and MICG (Y = supported, N = not supported)
Linguistic phenomena CG CCG MICG
Basic application Y Y Y
Node raising N Y Y
Topicalization/contraposition N Y N
Constituent shifting N Y N
Deletion under coordination N N Y
Serial verb construction N N Y
5.1 Translation of deletion under coordination
Coordinate structures in Thai drastically differ from
those of English. This is because Thai allows zero
anaphora at subject and object positions while En-
glish does not. Pronouns and VP ellipses must there-
fore be generated in place of deletion under coordi-
nation for grammaticality of English. Moreover, se-
mantic duplication is often made use to emphasize
the meaning of sentence, but its direct translation be-
comes redundant.
MICG helps us detect zero anaphora and resolve
their antecedents, so that appropriate pronouns and
ellipses can be generated at the right positions. By
tracing resolved antecedents and ellipses, argument
projections are disclosed and they can be used to
control verb fusion. We exemplify three cases of
translation of coordinate structure.
Case 1: Pronouns are generated to maintain
grammaticality of English translation if the two
verbs are not postulated in the verb-fusion table. For
example, a Thai sentence in (17) is translated, while
pronouns ?he? and ?it? are generated from Thai NPs
na?k;rian ?student? and kha`;no?m ?candy,? respectively.
na?k;rianS sW?:V kha`;no?mO lE?:V& kinV
student buy candy then eat
?A student buys candy, then he eats it.?
(17)
Case 2: Two verbs V1 and V2 are fused togeth-
erif they are postulated in the verb-fusion table to
eliminate semantic duplication in English transla-
tion. The object form of S2 is necessary to be gener-
ated in some cases. For example, in (18), the trans-
lation becomes ?He reports her this matter? instead
of ?He tells her to know this matter.? Two verbs bO`:k
?tell? and sa?:b ?know? are fused into a single verb ?re-
port.? The object form of ?she,? ?her,? is also gener-
ated.
kha?VS bO`:kV ha?j& th@:S sa?:pV rW?:@N n??:O
he tell TO she know this matter
?He reports her this matter.?
(18)
Case 3: A VP ellipsis is generated to main-
tain English grammaticality. For example, in (19),
a VP ellipsis ?do? is generated from a Thai VP
ma?i ChO?:b don;tri: rO?k ?not like rock music.?
CO:nS ChO?:pV don;tri: rO?kO tE`:& Cha?nS ma?iA
John like rock music but I not
?John likes rock music, but I do not.?
(19)
5.2 Translation of SVC
Sentence structuring is also nontrivial for translation
of Thai SVC. Thai uses SVC to describe consecu-
tive/concurrent events, purposes, and circumstances.
On the other hand, English describes each of those
with different sentence structure. A series of verbs
with duplicated semantics can be also clustered to
emphasize the meaning of sentence in Thai, while
English does not allow this phenomenon.
Because MICG reveals argument sharing in SVC,
appropriate sentence structures can be selected by
tracing argument sharing between two consecutive
verbs. We exemplify two cases of translation of
SVC.
Case 1: The second verb is participialized if the
first verb is intransitive and its semantic concept is
an action. For example, the present participial form
of the verb ?see,? ?seeing,? is generated in (20) .
so?m;Cha:jS d@:nV ChomV pha?:p;khia?nO
Somchai walk see paintings
?Somchai walks seeing paintings.?
(20)
Case 2: If the two cases above do not apply to
the two verbs, they are translated directly by de-
fault. The conjunction ?and? is automatically added
85
to conjoin two verb phrases. In case of multiple-
conjunct coordination, the conjunction will be added
only before the last conjunct. For example, in (21),
a pronoun ?it? is generated from the NP kho?:k ?coke,?
while the conjunction ?and? is automatically added.
ph?i:;sa?:VS sW?:V kho?:kO dW`:mV
my elder sister buy coke drink
?My elder sister buys coke and drinks it.?
(21)
6 Conclusion and Future Work
This paper presents Memory-Inductive Categorial
Grammar (MICG), an extended version of catego-
rial grammar, for gap resolution in analytic language
translation. Antecedent memorization, gap induc-
tion, and gap resolution, are proposed to cope with
deletion under coordination and serial verb construc-
tion. By means of MICG, anaphora can be gen-
erated for deletion under coordination, while sen-
tence structure can be properly selected for serial
verb construction. No experiment has been done to
show improvement of translation quality by MICG.
The following future work remains. First, we will
experiment on our Thai-English RBMT to measure
improvement of translation quality. Second, crite-
ria for pronominal reference generation in place of
deletion under coordination will be studied. Third,
once serial verb construction is analyzed, criteria of
sentence structuring will further be studied based on
an analysis of antecedent projection. Fourth and fi-
nally, constituent extraction and the use of extraction
direction in the extraction resolution will be studied
to avoid combinatoric explosion.
References
K. Ajdukiewicz. 1935. Die Syntaktische Konnexita?t.
Polish Logic, pages 207?231.
M. C. Baker. 1989. Object Sharing and Projection in Se-
rial Verb Constructions. Linguistic Inquiry, 20:513?
553.
J. Baldridge and G. J. M. Kruijff. 2003. Multimodal
combinatory categorial grammar. In Proceedings of
the 10th Conference of the European Chapter of the
ACL 2003, Budapest, Hungary.
J. Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Y. Bar-Hillel. 1953. A Quasi-Arithmetical Notation for
Syntactic Description. Language, 29:47?58.
J. Beavers and I. A. Sag. 2004. Coordinate ellipsis and
apparent non-constituent coordination. In Proceed-
ings of the HPSG04 Conference. Center for Compu-
tational Linguistics, Katholieke Universiteit Leuven,
CSLI Publications.
B. Cryssmann. 2003. An asymmetric theory of periph-
eral sharing in HPSG: Conjunction reduction and coor-
dination of unlikes. In Proceedings of Formal Gram-
mar Conference.
P. Hendriks. 1995. Ellipsis and multimodal categorial
type logic. In Proceedings of Formal Grammar Con-
ference. Barcelona, Spain.
G. Ja?ger. 1998a. Anaphora and ellipsis in type-logical
grammar. In Proceedings of the 1th Amsterdam Col-
loquium, Amsterdam, the Netherland. ILLC, Univer-
siteit van Amsterdam.
G. Ja?ger. 1998b. Anaphora and quantification in cate-
gorial grammar. In Lecture Notes in Computer Sci-
ence; Selected papers from the 3rd International Con-
ference, on logical aspects of Computational Linguis-
tics, volume 2014, pages 70?89.
J. Lambek. 1958. The Mathematics of Sentence Struc-
ture. American Mathematical Monthly, 65:154?170.
C. N. Li and S. A. Thompson. 1981. Mandarin Chinese:
A Functional Reference Grammar. Berkeley: Univer-
sity of California Press.
M. Moortgat. 2002. Categorial grammar and formal se-
mantics. In Encyclopedia of Cognitive Science, vol-
ume 1, pages 435?447. Nature Publishing Group.
B. H. Partee. 1975. Bound variables and other anaphors.
In Theoretical Issues in Natural Language Processing-
2 (TINLAP-2), pages 79?85, University of Illinois at
Urbana Champaign, July.
M. Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, Massachusetts.
K. Thepkanjana. 2006. Properties of events expressed
by serial verb constructions in Thai. In Proceedings
of the 11th Biennial Symposium: Intertheoretical Ap-
proaches to Complex Verb Constructions, Rice Univer-
sity.
X. Wang. 2007. Notes about Serial Verb Constructions
in Chinese. California Linguistic Notes, 32(1).
S. Yatabe. 2002. A linearization-based theory of sum-
mative agreement in peripheral-node raising construc-
tions. In Proceedings of the HPSG02 Conference,
Standford, California. CSLI Publications.
86
A Formal Definition of MICG
Definition 1 (Closure of MICG) Let VA of category symbols,
a finite set VT of terminal symbols, and a set of directions D =
{<,>}.
The set C of all category symbols is given by: (1) For all
x ? VA, x ? C. (2) If x,y ? C, then so are x\y and x/y. (3) If
x ?C, then so are 2<f x, 2>f x, 3<f x, and 3>f x, where f ? F is a
formula (described below). (4) Nothing else is in C.
The set T of all grammatical structures is given by: (1) For
all x ? VT , x ? T . (2) If x,y ? T , then so are x? y. (3) If x ? T ,
then so are 2x and 3x. (4) Nothing else is in T .
The set F of all formulae is a set of terms t ? x, where t ? T
and x ?C. The set Q of all modalities is a set of all terms 2<f ,
2>f , 3
<
f , and 3
>
f , where f ? F.
Definition 2 (Modality resolution) For any directions d ? D,
any formulae f ? F, and any modality sequences M,M1,M2 ?
Q?, the function ? : Q??Q? 7? Q? is defined as follows:
2df M1 ?3df M2 ? M1 ?M2
3df M1 ?2df M2 ? M1 ?M2
2df M1 ?2df M2 ? 2df (M1 ?M2)
3df M1 ?3df M2 ? 3df (M1 ?M2)
??M ? M? ? ? M
Definition 3 (MICG) A memory-inductive categorial gram-
mar (MICG) is defined as a quadruple G = ?VT ,VA,s,R?,
where: (1) VT and VA are as above. (2) s ?VA is the designated
symbol called ?start symbol.? (3) R :VT 7?P(F) is a function as-
signing to each terminal symbol a set of formulae from F. The
set of all strings generated from G is denoted as L(G).
Definition 4 (Acceptance of strings) For any formulae x,y ?
F, any grammatical structures t1,t2,t3 ? T , any variables
v of grammatical structures, and any modality sequences
M,M1,M2 ? Q?, the binary relation |=? F??F controls com-
bination of formulae as follows:
t1 ? y t2 ? y\x |= t1 ? t2 ? x
t1 ? x/y t2 ? y |= t1 ? t2 ? x
t1 ? y t2 ? My\x |= 2t1 ? t2 ? 2<t1?yMx
t1 ? My t2 ? y\x |= t1 ?2t2 ? 2>t2?y\xMx
t1 ? x/y t2 ? My |= 2t1 ? t2 ? 2<t1?x/yMx
t1 ? Mx/y t2 ? y |= t1 ?2t2 ? 2>t2?yMx
t2 ? My\x |= 3v? t2 ? 3<v?yMx
t1 ? My |= t1 ?3v ? 3>v?y\xMx
t2 ? My |= 3v? t2 ? 3<v?x/yMx
t1 ? Mx/y |= t1 ?3v ? 3>v?yMx
t1 ? M1x t3 ? & t2 ? M2x |= t1 ? (t3 ? t2) ? (M1 ?M2)x
t1 ? M1x t2 ? M2x |= t1 ? t2 ? (M1 ?M2)x
The binary relation ?? F??F? holds between two strings
of formulae ?X? and ?Y ?, denoted ?X? ? ?Y ?, if and only if
X |=Y , where X ,Y,?,? ? F? and |X | ? |Y |. The relation ?? is
the reflexive transitive closure of ?.
A string w ? V ?T is generated by G, denoted by w ? L(G), if
and only if w = w1 . . .wn and there is some sequence of formulae
f1 . . . fn such that fi ?R(wi) for all 1 ? i ? n, and f1 . . . fn ?? s.
That is, w1 . . .wn is generated if and only if there is some choice
of formula assignments by R to the symbols in w1 . . .wn that
reduces to s.
Definition 5 Correspondence between a grammatical struc-
ture and its syntactic category can be viewed as a tree with spe-
cialized node types. Each node is represented (m,S), where m
is a node type { /0,2,3}, and S is a modality sequence attached
to the node?s syntactic category.
Definition 6 A node that has the type m is said to be marked m
where m ? {2, 3}, while a node that has the type /0 is said to
be unmarked.
Definition 7 The function ? : Q 7? {2,3} maps a modality to
a node modality, where ?(2df ) = 2 and ?(3df ) = 3 for all d ?D
and f ? F.
Definition 8 A substring generated from a node marked ?(M)
beneath the node n is said to be unpaired under n, if and only if
n has the modality sequence S and M ? S.
Definition 9 Every string w generated from MICG can be
rewritten in the form w = p1q1 . . . plql pl+1ql+1 . . . p2lq2l p2l+1,
where qi is a substring unpaired under n, p j is a substring gen-
erated from unmarked nodes beneath n, 1 ? i ? l, 1 ? j ? l +1,
and l ? 0.
Theorem 1 (Cross-serial association) For every string gener-
ated from MICG w = p1q1 . . . plql p j(l)q j(1) . . . p j(l)q j(l) p j(l)+1,
every couple qi and q j(i) are associated by ? for all 1 ? i ? l,
where j(i) = l + i and l ? 0.
Proof Let us prove this property by mathematical induction.
Basic step: Let l = 0. We obtain that w0 = p1. Since there is
no unpaired substring, this case is trivially proven.
Hypothesis: Let l = k. Suppose that wk =
p1q1 . . . p j(k)q j(k) p j(k)+1. We rewrite wk = w1kw2k , where w1k =
p1q1 . . . pkqk p?j(1) and w
2
k = p??j(1)q j(1) . . . p j(k)q j(k) p j(k)+1.
Every couple qi and q j(k) are associated by ? for all 1 ? i ? k.
Induction: Let l = k + 1; wk+1 = p1q1 . . . p j(k)+2q j(k)+2
p j(k)+3, consequently. Let the formulae of the substrings
wk+1 = w1k+1w2k+1 be t1k+1 ? m1M1 and t2k+1 ? m2M2, respec-
tively. We can rewrite the substrings wk+1 = w1k+1w2k+1 in terms
of wk = w1kw2k in three cases.
Case I: Suppose w1k+1 = pqw1k . It follows that the direction
of q is <. Since w1k+1 combines w2k+1, we can conclude that
w2k+1 = p?q?w2k . Therefore, q and q? are also associated by ?.
Case II: Suppose w1k+1 = w1kqp. It follows that the direction
of q is >. Since w1k+1 combines w2k+1, we can conclude that
w2k+1 = w2kq?p?. Therefore, q and q? are also associated by ?.
Case III: w1k+1 = p1q1 . . . pmqm pqpm+1qm+1 . . . pnqn pk+1 and
w2k+1 = p j(1)q j(1) . . . p j(m?)q j(m?) p?q?p j(m?)+1q j(m?)+1 . . . p j(k)
q j(k) p j(k)+1, where 1 < m,m? < k. Since w1k+1 and w2k+1
combine and every qi and q j(i) are associated, we can conclude
that m = m?. Therefore, q and q? are also associated by ?.
From Case I, Case II, and Case III, we can rewrite w1k+1 =
p?1q
?
1 p
?
2q
?
2 . . . p?k+1 and w2k+1 = p?j(1)q?j(1) p?j(2)q?j(2) . . . p?j(k+1).
Since each qi in w1k and q j(i) in w
2
k are already associated by
?, it follows that all qi and q j(i)+1 are also associated. 
87
Speech-to-Speech Translation Activities in Thailand 
Chai Wutiwiwatchai, Thepchai Supnithi, Krit Kosawat 
Human Language Technology Laboratory 
National Electronics and Computer Technology Center 
112 Pahonyothin Rd., Klong-luang, Pathumthani 12120 Thailand 
{chai.wut, thepchai.sup, krit.kos}@nectec.or.th 
 
 
Abstract 
A speech-to-speech translation project 
(S2S) has been conducted since 2006 by 
the Human Language Technology labora-
tory at the National Electronics and Com-
puter Technology Center (NECTEC) in 
Thailand. During the past one year, there 
happened a lot of activities regarding tech-
nologies constituted for S2S, including 
automatic speech recognition (ASR), ma-
chine translation (MT), text-to-speech syn-
thesis (TTS), as well as technology for lan-
guage resource and fundamental tool de-
velopment. A developed prototype of Eng-
lish-to-Thai S2S has opened several re-
search issues, which has been taken into 
consideration. This article intensively re-
ports all major research and development 
activities and points out remaining issues 
for the rest two years of the project. 
1 Introduction 
Speech-to-speech translation (S2S) has been ex-
tensively researched since many years ago. Most of 
works were on some major languages such as 
translation among European languages, American 
English, Mandarin Chinese, and Japanese. There is 
no initiative of such research for the Thai language. 
In the National Electronics and Computer Tech-
nology Center (NECTEC), Thailand, there is a 
somewhat long history of research on Thai speech 
and natural language processing. Major technolo-
gies include Thai automatic speech recognition 
(ASR), Thai text-to-speech synthesis (TTS), Eng-
lish-Thai machine translation (MT), and language 
resource and fundamental tool development. These 
basic technologies are ready to seed for S2S re-
search. The S2S project has then been conducted in 
NECTEC since the end of 2006. 
The aim of the 3-year S2S project initiated by 
NECTEC is to build an English-Thai S2S service 
over the Internet for a travel domain, i.e. to be used 
by foreigners who journey in Thailand. In the first 
year, the baseline system combining the existing 
basic modules applied for the travel domain was 
developed. The prototype has opened several re-
search issues needed to be solved in the rest two 
years of the project. This article summarizes all 
significant activities regarding each basic technol-
ogy and reports remaining problems as well as the 
future plan to enhance the baseline system.
The rest of article is organized as follows. The 
four next sections describe in details activities 
conducted for ASR, MT, TTS, and language re-
sources and fundamental tools. Section 6 summa-
rizes the integration of S2S system and discusses 
on remaining research issues as well as on-going 
works. Section 7 concludes this article. 
2 Automatic Speech Recognition (ASR) 
Thai ASR research focused on two major topics. 
The first topic aimed to practice ASR in real envi-
ronments, whereas the second topic moved to-
wards large vocabulary continuous speech recogni-
tion (LVCSR) in rather spontaneous styles such as 
news broadcasting and telephone conversation. 
Following sub-sections give more details. 
2.1 Robust speech recognition 
To tackle the problem of noisy environments, 
acoustic model selection was adopted in our sys-
tem. A tree structure was constructed with each 
leaf node containing speaker-, noise-, and/or SNR-
specific acoustic model. The structure allowed ef-
ficient searching over a variety of speech environ-
ments. Similar to many robust ASR systems, the 
selected acoustic model was enhanced by adapting 
by the input speech using any adaptation algorithm 
such as MLLR or MAP. In our model, however, 
simulated-data adaptation was proposed (That-
phithakkul et al, 2006). The method synthesized 
an adaptation set by adding noise extracted from 
the input speech to a pre-recorded set of clean 
speech. A speech/non-speech detection module 
determined in the input speech the silence portions, 
which were assumed to be the environmental noise. 
This approach solved the problem of incorrect 
transcription in unsupervised adaptation and en-
hanced the adaptation performance by increasing 
the size of adaptation data. 
2.2 Large-vocabulary continuous speech rec-
ognition (LVCSR) 
During the last few years, researches on continuous 
speech recognition were based mainly on two da-
tabases, the NECTEC-ATR (Kasuriya et al, 
2003a) and the LOTUS (Kasuriya et al, 2003b). 
The former corpus was for general purposes, 
whereas the latter corpus was well designed for 
research on acoustic phonetics as well as research 
on 5,000-word dictation systems. A number of re-
search works were reported, starting by optimizing 
the Thai phoneme inventory (Kanokphara, 2003). 
Recently, research has moved closer to real and 
spontaneous speech. The first task collaborated 
with a Thai telephone service provider was to build 
a telephone conversation corpus (Cotsomrong et al, 
2007). To accelerate the corpus development, 
Thatphithakkul et al (2007) developed a speaker 
segmentation model which helped separating 
speech from two speakers being conversed. The 
model was based on the simple Hidden Markov 
model (HMM), which achieved over 70% accuracy.
Another on-going task is a collection of broadcast 
news video. The aim of the task is to explore the 
possibility to use the existing read-speech model to 
boot broadcast news transcription. More details 
will be given in Section 5. 
3 Machine Translation (MT) 
It was a long history of the NECTEC English-to-
Thai machine translation (MT) which has been 
publicly serviced online. The ?Parsit? 1  system 
modified from the engine developed by NEC, Ja-
pan, which was a rule-based MT (RBMT). Over 
900 parsed rules were coded by Thai linguists. The 
system recognized more than 70,000 lexical words 
and 120,000 meanings. 
 
 
 
Figure 1. Examples of using MICG to solve two 
major problems of parsing Thai, (a) coordination 
with gapping and (b) verb serialization. 
3.1 Thai-to-English MT 
Recently, there has been an effort to develop the 
first rule-based system for Thai-to-English MT. 
The task is much more difficult than the original 
English-to-Thai translation since the Thai word 
segmentation, sentence breaking, and grammar 
parser are all not complete. Coding rules for pars-
ing Thai is not trivial and the existing approach 
used to translate English to Thai cannot be applied 
counter wise. Last year, a novel rule-based ap-
proach appropriate for Thai was proposed 
(Boonkwan and Supnithi, 2007). The approach, 
called memory-inductive categorical grammar 
(MICG), was derived from the categorical gram-
mar (CG). The MICG introduced memorization 
and induction symbols to solve problems of ana-
lytic languages such as Thai as well as many spo-
                                                 
1
 Parsit MT, http://www.suparsit.com/ 
ken languages. In parsing Thai, there are two major 
problems, coordination with gapping and verb se-
rialization. Figure 1 shows examples of the two 
problems with the MICG solution, where the 
square symbol denotes the chunk to be memorized 
and the diamond symbol denotes the chunk to be 
induced. A missing text chunk can be induced by 
seeking for its associated memorized text chunk.  
3.2 TM and SMT 
In order to improve the performance of our transla-
tion service, we have adopted a translation memory 
(TM) module in which translation results corrected 
by users are stored and reused. Moreover, the ser-
vice system is capable to store translation results of 
individual users. A na?ve user can select from the 
list of translation results given by various users. 
Figure 3 captures the system interface.  
Due to powerful hardware today, research has 
turned to rely more on statistical approaches. This 
is also true for the machine translation issue. Sta-
tistical machine translation (SMT) has played an 
important role on modeling translation given a 
large amount of parallel text. In NECTEC, we also 
realize the benefit of SMT especially on its adapta-
bility and naturalness of translation results. How-
ever, a drawback of SMT compared to RBMT is 
that it works quite well on a limited domain, i.e. 
translating in a specific domain. This is actually 
suitable to the S2S engine which has been designed 
to work in only a travel domain. Therefore, in par-
allel to RBMT, SMT is being explored for limited 
domains. Two parallel text corpora have been con-
structed. The first one, collected by ATR under the 
Asian speech translation advanced research (A-
STAR)2 consortium, is a Thai incorporated version 
of the Basic travel expression (BTEC) corpus (Ki-
kui et al, 2003). This corpus will seed the devel-
opment of S2S in the travel domain. The second 
parallel corpus contains examples of parallel sen-
tences given in several Thai-English dictionaries.  
The latter corpus has been used for a general 
evaluation of Thai-English SMT. Details of both 
corpora will be given in the Section 5. 
4 Text-to-Speech Synthesis (TTS) 
Thai TTS research has begun since 2000. At pre-
sent, the system utilizes a corpus-based unit-
                                                 
2
 A-STAR consortium, http://www.slc.atr.jp/AStar/ 
selection technique. A well-constructed phoneti-
cally-balanced speech corpus, namely ?TSynC-1?, 
containing approximately 13 hours is embedded in 
the TTS engine, namely ?Vaja?3. Although the lat-
est version of Vaja achieved a fair speech quality, 
there are still a plenty of rooms to improve the sys-
tem. During the past few years, two major issues 
were considered; reducing the size of speech cor-
pus and improving unit selection by prosody in-
formation. Following sub-sections describe the 
detail of each issue. 
4.1 Corpus space reduction 
A major problem of corpus-based unit-selection 
TTS is the large size of speech corpus required to 
obtain high-quality, natural synthetic-speech. Scal-
ability and adaptability of such huge database be-
come a critical issue. We then need the most com-
pact speech corpus that still provides acceptable 
speech quality. An efficient way to reduce the size 
of corpus was recently proposed (Wutiwiwatchai et 
al., 2007). The method incorporated Thai phonetics 
knowledge in the design of phoneme/diphone in-
ventory. Two assumptions on diphone characteris-
tics were proved and used in the new design. One 
was to remove from the inventory the diphone 
whose coarticulation strength between adjacent 
phonemes was very weak. Normally, the corpus 
was designed to cover all tonal diphones in Thai. 
The second strategy to reduce the corpus was to 
ignore tonal levels of unvoiced phonemes. Ex-
periments showed approximately 30% reduction of 
the speech corpus with the quality of synthesized 
speech remained. 
4.2 Prosody-based naturalness improvement 
The baseline TTS system selected speech units by 
considering only phoneme and tone context. In the 
past few years, analyses and modeling Thai pro-
sodic features useful for TTS have been exten-
sively explored. The first issue was to detect 
phrasal units given an input text. After several ex-
periments (Tesprasit et al, 2003; Hansakunbun-
theung et al, 2005), we decided to develop a clas-
sification and decision tree (CART) for phrase 
break detection.  
The second issue was to model phoneme dura-
tion. Hansakunbuntheung et al (2003) compared 
several models to predict the phoneme duration. 
                                                 
3
 Vaja TTS, http://vaja.nectec.or.th/ 
Mainly, we found linear regression appropriate for 
our engine as its simplicity and efficiency. Both 
two prosody information were integrated in our 
Vaja TTS engine, which achieved a better synthe-
sis quality regarding subjective and objective 
evaluations (Rugchatjaroen et al, 2007). 
5 Language Resources and Tools 
A lot of research issues described in previous sec-
tions definitely requires the development and as-
sessment of speech and language corpora. At the 
same time, there have been attempts to enhance the 
existing language processing tools that are com-
monly used in a number of advanced applications. 
This section explains the activities on resource and 
tool development. 
5.1 Speech and text corpora 
Table 1 summarizes recent speech and text corpora 
developed in NECTEC. Speech corpora in NEC-
TEC have been continuously developed since 2000. 
The first official corpus under the collaboration 
with ATR was for general purpose (Kasuriya et al, 
2003a). The largest speech corpus, called LOTUS 
(Kasuriya et al, 2003b), was well-designed read 
speech in clean and office environments. It con-
tained both phonetically balanced utterances and 
news paper utterances covering 5,000 lexical 
words. The latter set was designed for research on 
Thai dictation systems. Several research works 
utilizing the LOTUS were reported as described in 
the Section 2.2. 
The last year was the first-year collaboration of 
NECTEC and a telephone service provider to de-
velop the first Thai telephone conversation speech 
corpus (Cotsomrong et al, 2007). The corpus has 
been used to enhance the ASR capability in dealing 
with various noisy telephone speeches. 
Regarding text corpora, as already mentioned in 
the Section 3, two parallel text corpora were de-
veloped. The first corpus was a Thai version of the 
Basic travel expression corpus (BTEC), which will 
be used to train a S2S system. The second corpus 
developed ourselves was a general domain. It will 
be used also in the SMT research. Another impor-
tant issue of corpus technology is to create golden 
standards for several Thai language processing top-
ics. Our last year attempts focused on two sets; a 
golden standard set for evaluating MT and a 
golden standard set for training and evaluating 
Thai word segmentation. Finally, the most basic 
but essential in all works is the dictionary. Within 
the last year, we have increased the number of 
word entries in our lexicon from 35,000 English-
to-Thai and 53,000 Thai-to-English entries to over 
70,000 entries both. This incremental dictionary 
will be very useful in sustaining improvement of 
many language processing applications. 
 
Table 1. Recent speech/text corpora in NECTEC. 
 
Corpus Purpose Details 
LOTUS 
 
Well-designed 
speech utterances for 
5,000-word dictation 
systems 
- 70 hours of pho-
netically balanced 
and 5,000-word 
coverage sets 
TSynC-1 Corpus-based unit-
selection Thai speech 
synthesis 
- 13 hours pros-
ody-tagged fluent 
speech 
Thai BTEC Parallel text and 
speech corpora for 
travel-domain S2S 
- 20,000 textual 
sentences and a 
small set of speech 
in travel domain 
Parallel text Pairs of Thai-English 
sample sentences 
from dictionaries 
used for SMT 
- 0.2M pairs of 
sentences 
NECTEC-
TRUE 
Telephone conversa-
tion speech for 
acoustic modeling 
- 10 hours conver-
sational speech in 
various telephone 
types 
5.2 Fundamental language tools 
Two major language tools have been substantially 
researched, word segmentation and letter-to-sound 
conversion. These basic tools are very useful in 
many applications such as ASR, MT, TTS, as well 
as Information retrieval (IR). 
Since Thai writing has no explicit word and sen-
tence boundary marker. The first issue on process-
ing Thai is to perform word segmentation. Our 
baseline morphological analyzer determined word 
boundaries and word part-of-speech (POS) simul-
taneously using a POS n-gram model and a prede-
fined lexicon. Recently, we have explored Thai 
named-entity (NE) recognition, which is expected 
to help alleviating the problem of incorrect word 
segmentation. Due to the difficulty of Thai word 
segmentation, we initiated a benchmark evaluation 
on Thai word segmentation, which will be held in 
2008. This will gather researchers who are inter-
ested in Thai language processing to consider the 
problem on a standard text corpus. 
The problem of incorrect word segmentation 
propagates to the letter-to-sound conversion (LTS) 
module which finds pronunciations on the word 
basis. Our original LTS algorithm was based on 
probabilistic generalized LR parser (PGLR). Re-
cently, we proposed a novel method to automati-
cally induce syllable patterns from a large text with 
no need for any preprocessing (Thangthai et al, 
2006). This approach largely helped alleviating the 
tedious work on text corpus annotation. 
Another important issue we took into account 
was an automatic approach to find pronunciations 
of English words using Thai phonology. The issue 
is particularly necessary in many languages where 
their local scripts are always mixed with English 
scripts. We proposed a new model that utilized 
both English graphemes and English phonemes, if 
found in an English pronunciation dictionary, to 
predict Thai phonemes of the word (Thangthai et 
al., 2007). 
6 Speech-to-Speech Translation (S2S) 
In parallel to the research and development of in-
dividual technology elements, some efforts have 
been on the development of Thai-English speech-
to-speech translation (S2S). Wutiwiwatchai (2007) 
already explained in details about the activities, 
which will be briefly reported in this section. 
As described briefly in the Introduction, the aim 
of our three-year S2S project is to develop an S2S 
engine in the travel domain, which will be given 
service over the Internet. In the last year, we de-
veloped a prototype English-to-Thai S2S engine, 
where major tasks turned to be the development of 
English ASR in the travel domain and the integra-
tion of three core engines, English ASR, English-
to-Thai RBMT, and Thai TTS. 
6.1 System development 
Our current prototype of English ASR adopted a 
well-known SPHINX toolkit, developed by Carne-
gie Mellon University. An American English 
acoustic model has been provided with the toolkit. 
An n-gram language model was trained by a small 
set of sentences in travel domain. The training text 
contains 210 patterns of sentences spanning over 
480 lexical words, all prepared by hands. Figure 2 
shows some examples of sentence pattern. 
 
 
Figure 2. Examples of sentence patterns for lan-
guage modeling (uppercases are word classes, 
bracket means repetition). 
 
In the return direction, a Thai ASR is required. 
Instead of using the SPHINX toolkit4, we built our 
own Thai ASR toolkit, which accepts an acoustic 
model in the Hidden Markov toolkit (HTK)5 for-
mat proposed by Cambridge University. The ?iS-
peech?6 toolkit that supports an n-gram language 
model is currently under developing.  
The English ASR, English-to-Thai RBMT, and 
Thai TTS were integrated simply by using the 1-
best result of ASR as an input of MT and generat-
ing a sound of the MT output by TTS. The proto-
type system, run on PC, utilizes a push-to-talk in-
terface so that errors made by ASR can be allevi-
ated.  
6.2 On-going works 
To enhance the acoustic and language models, a 
Thai speech corpus as well as a Thai-English paral-
lel corpus in the travel domain is constructing as 
mentioned in the Section 5.1, the Thai version of 
BTEC corpus. Each monolingual part of the paral-
lel text will be used to train a specific ASR lan-
guage model. 
For the MT module, we can use the parallel text 
to train a TM or SMT. We expect to combine the 
trained model with our existing rule-based model, 
which will be hopefully more effective than each 
individual model. Recently, we have developed a 
TM engine. It will be incorporated in the S2S en-
gine in this early stage. 
In the part of TTS, several issues have been re-
searched and integrated in the system. On-going 
works include incorporating a Thai intonation 
                                                 
4
 CMU SPHINX, http://cmusphinx.sourceforge.net/ 
5
 HTK, Cambridge University, http://htk.eng.cam.ac.uk/ 
6
 iSpeech ASR, http://www.nectec.or.th/rdi/ispeech/ 
model in unit-selection, improving the accuracy of 
Thai text segmentation, and learning for hidden 
Markov model (HMM) based speech synthesis, 
which will hopefully provide a good framework 
for compiling TTS on portable devices. 
7 Conclusion 
There have been a considerable amount of research 
and development issues carried out under the 
speech-to-speech translation project at NECTEC, 
Thailand. This article summarized and reported all 
significant works mainly in the last few years. In-
deed, research and development activities in each 
technology element, i.e. ASR, MT, and TTS have 
been sustained individually. The attempt to inte-
grate all systems forming an innovative technology 
of S2S has just been carried out for a year. There 
are many research and development topics left to 
explore. Major challenges include at least but not 
limited to the following issues: 
 The rapid development of Thai-specific ele-
ments such as robust Thai domain-specific 
ASR and MT 
 Migration of the existing written language 
translation to spoken language translation 
Recently, there have been some initiations of 
machine translation among Thai and other lan-
guages such as Javi, a minor language used in the 
southern part of Thailand and Mandarin Chinese. 
We expect that some technologies carried out in 
this S2S project will be helpful in porting to the 
other pairs of languages. 
Acknowledgement 
The authors would like to thank the ATR, Japan, in 
initiating the fruitful A-STAR consortium and in 
providing some resources and tools for our re-
search and development. 
References 
Boonkwan, P., Supnithi, T., 2008. Memory-inductive 
categorial grammar: an approach to gap resolution 
in analytic-language translation, To be presented in 
IJCNLP 2008. 
Cotsomrong, P., Saykham, K., Wutiwiwatchai, C., 
Sreratanaprapahd, S., Songwattana, K., 2007. A Thai 
spontaneous telephone speech corpus and its appli-
cations to speech recognition, O-COCOSDA 2007. 
Hansakunbuntheung, C., Tesprasit, V., Siricharoenchai, 
R., Sagisaka, Y., 2003. Analysis and modeling of syl-
lable duration for Thai speech synthesis, EU-
ROSPEECH 2003, pp. 93-96. 
Hansakunbuntheung, C., Thangthai, A., Wutiwiwatchai, 
C., Siricharoenchai, R., 2005. Learning methods and 
features for corpus-based phrase break prediction on 
Thai, EUROSPEECH 2005, pp. 1969-1972. 
Kanokphara, S., 2003. Syllable structure based phonetic 
units for context-dependent continuous Thai speech 
recognition, EUROSPEECH 2003, pp. 797-800. 
Kasuriya, S., Sornlertlamvanich, V., Cotsomrong, P., 
Jitsuhiro, T., Kikui, G., Sagisaka, Y., 2003a. NEC-
TEC-ATR Thai speech corpus, O-COCOSDA 2003. 
Kasuriya, S., Sornlertlamvanich, V., Cotsomrong, P., 
Kanokphara, S., Thatphithakkul, N., 2003b. Thai 
speech corpus for speech recognition, International 
Conference on Speech Databases and Assessments 
(Oriental-COCOSDA). 
Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., 
2003. Creating corpora for speech-to-speech transla-
tion, EUROSPEECH 2003. 
Tesprasit, V., Charoenpornsawat, P., Sornlertlamvanich, 
V., 2003. Learning phrase break detection in Thai 
text-to-speech, EUROSPEECH 2003, pp. 325-328. 
Rugchatjaroen, A., Thangthai, A., Saychum, S., That-
phithakkul, N., Wutiwiwatchai, C., 2007. Prosody-
based naturalness improvement in Thai unit-selection 
speech synthesis, ECTI-CON 2007, Thailand. 
Thangthai, A., Hansakunbuntheung, C., Siricharoenchai, 
R., Wutiwiwatchai, C., 2006. Automatic syllable-
pattern induction in statistical Thai text-to-phone 
transcription, INTERSPEECH 2006. 
Thangthai, A., Wutiwiwatchai, C., Ragchatjaroen, A., 
Saychum, S., 2007. A learning method for Thai pho-
netization of English words, INTERSPEECH 2007. 
Thatphithakkul, N., Kruatrachue, B., Wutiwiwatchai, C., 
Marukatat, S., Boonpiam, V., 2006. A simulated-data 
adaptation technique for robust speech recognition, 
INTERSPEECH 2006. 
Wutiwiwatchai, C., 2007. Toward Thai-English speech 
translation, International Symposium on Universal 
Communications (ISUC 2007), Japan. 
Wutiwiwatchai, C., Saychum, S., Rugchatjaroen, A., 
2007. An intensive design of a Thai speech synthesis 
corpus, To be presented in International Symposium 
on Natural Language Processing (SNLP 2007). 
 
Automatic error detection in the Japanese learners? English spoken data
Emi IZUMI?? 
emi@crl.go.jp 
Kiyotaka UCHIMOTO? 
uchimoto@crl.go.jp 
Toyomi SAIGA? 
hoshi@karl.tis.co.jp
Thepchai Supnithi* 
thepchai@nectec.or.th
Hitoshi ISAHARA?? 
isahara@crl.go.jp 
Abstract 
This paper describes a method of 
detecting grammatical and lexical errors 
made by Japanese learners of English 
and other techniques that improve the 
accuracy of error detection with a limited 
amount of training data. In this paper, we 
demonstrate to what extent the proposed 
methods hold promise by conducting 
experiments using our learner corpus, 
which contains information on learners? 
errors. 
1 Introduction 
One of the most important things in keeping up 
with our current information-driven society is the 
acquisition of foreign languages, especially 
English for international communications. In 
developing a computer-assisted language teaching 
and learning environment, we have compiled a 
large-scale speech corpus of Japanese learner 
English, which provides a great deal of useful 
information on the construction of a model for the 
developmental stages of Japanese learners? 
speaking abilities.  
In the support system for language learning, 
we have assumed that learners must be informed 
of what kind of errors they have made, and in 
which part of their utterances. To do this, we need 
to have a framework that will allow us to detect 
learners? errors automatically.  
In this paper, we introduce a method of detect-
ing learners? errors, and we examine to what ex-
tent this could be accomplished using our learner 
corpus data including error tags that are labeled 
with the learners? errors.  
2 SST Corpus 
The corpus data was based entirely on audio-
recorded data extracted from an interview test, the 
?Standard Speaking Test (SST)?. The SST is a 
face-to-face interview between an examiner and 
the test-taker. In most cases, the examiner is a 
native speaker of Japanese who is officially 
certified to be an SST examiner. All the 
interviews are audio-recorded, and judged by two 
or three raters based on an SST evaluation scheme 
(SST levels 1 to 9). We recorded 300 hours of 
data, totaling one million words, and transcribed 
this. 
2.1 Error tags 
We designed an original error tagset for 
learners? grammatical and lexical errors, which 
were relatively easy to categorize. Our error tags 
contained three pieces of information, i.e., the part 
of speech, the grammatical/lexical system and the 
corrected form. We prepared special tags for some 
errors that cannot be categorized into any word 
class, such as the misordering of words. Our error 
tagset currently consists of 45 tags. The following 
example is a sentence with an error tag. 
*I lived in <at 
crr="">the</at> New Jersey. 
at indicates that it is an article error, and 
crr=?? means that the corrected form does not 
?Computational Linguistics Group, Communications Research Laboratory, 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 
?Graduate School of Science and Technology, Kobe University, 1-1 Rokkodai, Nada-ku, Kobe, Japan 
?TIS Inc., 9-1 Toyotsu, Suita, Osaka, Japan 
*National Electronics and Computer Technology Center, 
112 Pahonyothin Road, Klong 1, Klong Luang, Pathumthani, 12120, Thailand 
need an article. By referring to information on the 
corrected form indicated in an error tag, the sys-
tem can convert erroneous parts into corrected 
equivalents. 
3 Error detection method 
In this section, we would like to describe how 
we proceeded with error detection in the learner 
corpus. 
3.1 Types of errors 
We first divided errors into two groups de-
pending on how their surface structures were dif-
ferent from those of the correct ones. The first was 
an ?omission?-type error, where the necessary 
word was missing, and an error tag was inserted to 
interpolate it. The second was a ?replacement?-
type error, where the erroneous word was en-
closed in an error tag to be replaced by the cor-
rected version. We applied different methods to 
detecting these two kinds of errors. 
3.2 Detection of omission-type errors 
Omission-type errors were detected by estimat-
ing whether or not a necessary word string was 
missing in front of each word, including delimit-
ers. We also estimated to which category the error 
belonged during this process. What we call ?error 
categories? here means the 45 error categories that 
are defined in our error tagset. (e.g. article and 
tense errors) These are different from ?error 
types? (omission or replacement). As we can see 
from Fig. 1, when more than one error category is 
given, we have two ways of choosing the best one. 
Method A allows us to estimate whether there is a 
missing word or not for each error category. This 
can be considered the same as deciding which of 
the two labels (E: ?There is a missing word.? or C: 
?There is no missing word.?) should be inserted in 
front of each word. Here, there is an article miss-
ing in front of ?telephone?, so this can be consid-
ered an omission-type error, which is categorized 
as an article error (?at? is a label that indicates that 
this is an article error.). In Method B, if N error 
categories come up, we need to choose the most 
appropriate error category ?k? from among N+1 
categories, which means we have added one more 
category (+1) of ?There is no missing word.? (la-
beled with ?C?) to the N error categories. This can 
be considered the same as putting one of the N+1 
labels in front of each word. If there is more than 
one error tag inserted at the same location, they 
are combined to form a new error tag. 
As we can see from Fig. 2, we referred to 23 
pieces of information to estimate the error cate-
gory: two preceding and following words, their 
word classes, their root forms, three combinations 
of these (one preceding word and one following 
word/two preceding words and one following 
word/one preceding word and two following 
words), and the first and last letter of the word 
immediately following. (In Fig. 2, ?t? and ?e? in 
?telephone?.) The word classes and root forms 
were acquired with ?TreeTagger?. (Shmid 1994) 
3.3 Detection of replacement-type errors 
Replacement-type errors were detected by es-
timating whether or not each word should be de-
leted or replaced with another word string. The 
error category was also estimated during this 
process. As we did in detecting omission-type er-
rors, if more than one error category was given, 
we use two methods of detection. Method C was 
used to estimate whether or not the word should 
be replaced with another word for each error cate-
gory, and if it was to be replaced, the model esti-
mated whether the word was located at the 
beginning, middle or end of the erroneous part. As 
we can see from Fig. 3, this can be considered the Figure 2. Features used for detecting omission-type errors 
Word   POS    Root form 
there     EX       there 
is      VBZ       be 
telephone     NN       telephone 
and      CC       and 
the  DT       the 
books NNS       books 
.  SENT       . t e
?:feature combination      :single feature 
?Erroneous 
part
Figure 1. Detection of omission-type errors when 
there are more than one (N) error categories. 
Method A 
* there is telephone and the books . 
 
E: There is a missing word 
C: There is no missing word (=correct) 
Mehod B 
* there is telephone and the books . 
 
Ek: There is a missing word and the related error 
category is k (1?k?N) 
C: There is no missing word (=correct) 
? 
C 
? 
C 
? 
Ek 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
E 
? 
C 
? 
C 
? 
C 
? 
C 
same as deciding which of the three labels (Eb: 
?The word is at the beginning of the erroneous 
part.?, Ee: ?The word is in the middle or end.? or 
C: ?The word is correct.?) must be applied to each 
word. Method D was used if N error categories 
came up and we chose an appropriate one for the 
word from among 2N+1 categories. ?2N+1 cate-
gories? means that we divided N categories into 
two groups, i.e., where the word was at the begin-
ning of the erroneous part and where the word was 
not at the beginning, and we added one more 
where the word neither needed to be deleted nor 
replaced. This can be considered the same as at-
taching one of the 2N+1 labels to each word. To 
do this, we applied Ramshaw?s IOB scheme 
(Lance 1995). If there was more than one error tag 
attached to the same word, we only referred to the 
tag that covered the highest number of words. 
As Fig. 4 reveals, 32 pieces of information are 
referenced to estimate an error category, i.e., the 
targeted word and the two preceding and follow-
ing words, their word classes, their root forms, 
five combinations of these (the targeted word, the 
one preceding and one following/ the targeted 
word and the one preceding/ the targeted word 
and the one following/ the targeted word and the 
two preceding/ the targeted word and the two fol-
lowing), and the first and last letters of the word. 
3.4 Use of machine learning model 
The Maximum Entropy (ME) model (Jaynes 
1957) is a general technique that is used to esti-
mate the probability distributions of data. The 
over-riding principle in ME is that when nothing 
is known, the distribution should be as uniform as 
possible, i.e., maximum entropy. We calculated 
the distribution of probabilities p(a,b) with this 
method when Eq. 1 was satisfied and Eq. 2 was 
maximized. We then selected the category with 
maximum probability, as calculated from this dis-
tribution of probabilities, to be the correct cate-
gory. 
 
(2)   )),(log(),(             )(             
)1(                                          
(1)          ),(),(~       ),(),(
,
  
, ,
?
? ?
??
?? ??
?=
???
=
BbAa
j
BbAa BbAa
jj
bapbappH
kjffor
bagbapbagbap  
We assumed that the constraint of feature sets 
fi (i?j?k) was defined by Eq. 1. This is where A 
is a set of categories and B is a set of contexts,  
and gj(a,b) is a binary function that returns value 1 
when feature fj exists in context b and the category 
is a. Otherwise, gj(a,b) returns value 0. p~ (a,b) is 
the occurrence rate of the pair (a,b) in the training 
data. 
4 Experiment 
4.1 Targeted error categories 
We selected 13 error categories for detection.  
Table 1. Error categories to be detected 
Noun Number error, Lexical error 
Verb Erroneous subject-verb agreement, Tense error, 
Compliment error 
Adjective Lexical error 
Adverb Lexical error 
Preposition Lexical error on normal and dependent preposition 
Article Lexical error 
Pronoun Lexical error 
Others Collocation error 
 
Figure 4. The features used for detecting replace-
ment-type errors 
?:feature combination      :single feature 
Word     POS         Root form 
there     EX         there 
is      VBZ         be 
telephone     NN         telephone 
and      CC         and 
the      DT         the 
books     NNS         book 
on      IN         on 
the      DT         the 
desk      NN         NN 
.      SENT         . 
t e
?Erroneous
part 
Figure 3. Detection of replacement-type errors 
when there are more than one (N) error categories.
Method C 
* there is telephone and the books on the desk. 
 
 
Eb: The word in the beginning of the part which 
should be replaced. 
Ee: The word in the middle or the end of the part 
which should be replaced. 
C: no need to be replaced (=correct) 
Mehod D 
* there is telephone and the books on the desk. 
 
 
Ebk: The word in the beginning of the part which 
should be replaced and which error category is k. 
Eek: The word in the middle or the end of the part 
which should be replaced and which error category 
is k. (1?k?N) 
C: no need to be replaced (=correct) 
? 
C 
? 
C 
? 
C 
?
Eb
? 
C 
? 
C 
? 
C 
? 
C 
?
C
? 
C 
? 
C 
? 
C 
? 
Ebk 
? 
C 
? 
C 
? 
C 
? 
C 
?
C
4.2 Experiment based on tagged data 
We obtained data from 56 learners? with error 
tags. We used 50 files (5599 sentences) as the 
training data, and 6 files (617 sentences) as the 
test data. 
We tried to detect each error category using the 
methods discussed in Sections 3.2 and 3.3. There 
were some error categories that could not be de-
tected because of the lack of training data, but we 
have obtained the following results for article er-
rors which occurred most frequently. 
Article errors 
Omission- Recall rate 8/71 * 100 = 32.39(%) 
type errors Precision rate 8/11 * 100 = 52.27(%) 
Replacement- Recall rate 0/43 * 100 =  9.30(%) 
type errors Precision rate 0/ 1 * 100 =  22.22(%) 
Results for 13 errors were as follows. 
All errors 
Omission- Recall rate 21/ 93 * 100 = 22.58(%) 
type errors Precision rate 21/ 38 * 100 = 55.26(%) 
Replacement- Recall rate 5/224 * 100 =  2.23(%) 
type errors Precision rate 5/ 56 * 100 =  8.93(%) 
We assumed that the results were inadequate 
because we did not have sufficient training data. 
To overcome this, we added the correct sentences 
to see how this would affect the results. 
4.3 Addition of corrected sentences 
As discussed in Section 2.1, our error tags pro-
vided a corrected form for each error. If the erro-
neous parts were replaced with the corrected 
forms indicated in the error tags one-by-one, ill-
formed sentences could be converted into cor-
rected equivalents. We did this with the 50 items 
of training data to extract the correct sentences 
and then added them to the training data. We also 
added the interviewers? utterances in the entire 
corpus data (totaling 1202 files, excluding 6 that 
were used as the test data) to the training data as 
correct sentences. We added a total of 104925 
correct new sentences. The results we obtained by 
detecting article errors with the new data were as 
follows. 
Article errors 
Omission- Recall rate 8/71 * 100 = 11.27(%) 
type errors Precision rate 8/11 * 100 = 72.73(%) 
Replacement- Recall rate 0/43 * 100 =  0.00(%) 
type errors Precision rate 0/ 1 * 100 =  0.00(%) 
We found that although the recall rate de-
creased, the precision rate went up through adding 
correct sentences to the training data. 
We then determined how we could improve 
the results by adding the artificially made errors to 
the training data. 
4.4 Addition of sentences with artificially 
made errors 
We did this only for article errors. We first ex-
amined what kind of errors had been made with 
articles and found that ?a?, ?an?, ?the? and the 
absence of articles were often confused. We made 
up pseudo-errors just by replacing the correctly 
used articles with one of the others. The results of 
detecting article errors using the new training data, 
including the new corrected sentences described 
in Section 4.2, and 7558 sentences that contained 
artificially made errors were as follows. 
Article errors 
Omission- Recall rate 24/71 * 100 = 33.80(%) 
type errors Precision rate 24/30 * 100 = 80.00(%) 
Replacement- Recall rate 2/43 * 100 =  4.65(%) 
type errors Precision rate 2/ 9 * 100 = 22.22(%) 
We obtained a better recall and precision rate 
for omission-type errors. 
There were no improvements for replacement-
type errors. Since some more detailed context 
might be necessary to decide whether ?a? or ?the? 
must be used, the features we used here might be 
insufficient. 
5 Conclusion 
In this paper, we explained how errors in 
learners? spoken data could be detected and in the 
experiment, using the corpus as it was, the recall 
rate was about 30% and the precision rate was 
about 50%. By adding corrected sentences and 
artificially made errors, the precision rate rose to 
80% while the recall rate remained the same.  
References 
Helmut  Schmid  Probabilistic  part-of-Speech 
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. pp. 44-49, 1994. 
Lance A. Ramshaw and Mitchell P. Marcus. Text 
chunking using transformation-based learning. In 
Proceedings of the Third ACL Workshop on Very 
Large Corpora, pp. 82-94, 1995. 
Jaynes, E. T. ?Information Theory and Statistical Me-
chanics? Physical Review, 106, pp. 620-630, 1957. 
Abstract
The rapid growth of Internet Technology,
especially user friendliness approach, helps
increase the number of Internet users and the
amount of information in the cyberspace.
There is a countless amount of information in
languages. This has spread developments of
MT systems. The focus of our approach is to
increase the reusability of those MT systems
by using Cross System machine translation.
Using natural language as an intermediate
language, such as English, will help us use the
information in Internet qualitatively. In this
paper, we point out some problems that may
cause the efficiency to decrease when a
sentence is translated from a second language
to a third language. A novel method is
proposed to solve this problem.
 
1. Introduction
Machine Translation (MT) is an automatic
system that provides an ability to convert a
message written in one language (source
language: SL) to another (target language:
TL)[1]. The interlingua approach [2,3], a
methodology of constructing an intermediate
language, is a dominant approach in
standalone system to support multi-language.
Many products such as, SYSTRAN [4],
BESTILAND [5], are implemented using this
approach. Interlingua approach is helpful for
a central server, but it is difficult to complete
concepts in Interlingua.
The rapid growth of Internet Technology,
especially user friendliness approach, helps
increase the population of users who access
the Internet and the amount of information in
the cyberspace. With the increasing amount of
online information and the rapid growth of
non-English speaking Internet hosts, it is
becoming increasingly important to offer
users universal access to valuable information
resources in different languages. The
European Multilingual Information Retrieval
(EMIR) project [6], the MULINEX project[7],
the TwentyOne project[8], and the
cross-language retrieval track in TREC[9]
conference all reflect people?s interest in
A Cross System Machine Translation
Thepchai Supnithi, Virach Sornlertlamvanich, Thatsanee Charoenporn
Information Research and Development Division
National Electronics and Computer Technology Center
112 Thailand Science Park, Paholyothin Rd.,
Klong 1, Klong Luang, Pathumthani 12120
THAILAND
{thepchai , virach ,thatsanee}@nectec.or.th
providing interoperability among different
language processing environments and
multilingual information retrieval.
Distributed system technology plays an
important role to enable us to manage
information from various places. This makes
it unnecessary to access only the central
server. It helps machine translation
developers to work individually. Yasuhara
[10] wrote that many machine translation
systems were developed, especially from local
language to English, and the language has an
important role as an intermediate language.
Our paper tries to apply a distributed
technique by using English language, which is
mostly used by non-English speakers as a
second language to be an intermediate
language. Our approach is not aimed to show
that it is better than the interlingua approach,
but it is another solution for us to use existing
resources in cyberspace. We hope that it is
possible to help developers build the
machine translation that will support
all languages taking into account of
cost, quantity, and time consumption.
In section 2, we show cross system
MT approach. In section 3, an
example of our approach is given. In
section 4 we illustrate drawbacks of
this technique and give an example
about how to examine these problems.
2. Cross System MT
The major significance of Asian
languages is the variation of languages
in the region; most of which use their
own unique set of characters. In terms of
grammar, some (Thai, Laotian, Japanese,
Chinese, etc.) do not indicate word boundary,
some (Thai, Laotian, etc.) do not inflect while
others(Japanese, Korean, etc.) provide
particles to indicate the word grammatical
function, some are not distinguishable
between sentences and phrases, etc. These are
the basic difficulties that interest the
researchers in the field of machine translation
and the application.
Due to these varieties, it is difficult to build
an MT system that supports all languages
taking into account of cost, quantity, and time
consumption. Cross system machine
translation approach is, therefore, an essential
concept that helps reduce these problems by
reusing the large amount of information
existing in Internet.
Figure 1 shows an idea of our cross system
machine translation approach. Since the
Text Processing Common Platform
VisualizationRepresentation Extraction Retrieval Summarization MT Mining
English
Language Processing
Chinese
Language Processing
Japanese
Language Processing
French
Language Processing
Korean
Language Processing
Myanmar
Language Processing
Vietnam
Language Processing
Indonesia
Language Processing
Thai
Language Processing
??
Language Processing
??
Language Processing
MT
MT MT MT MT
MT MT MT
MTMT
e-Content Dictionary e-Content Dictionary e-Content Dictionary e-Content Dictionary
e-Content Dictionary e-Content Dictionary
e-Content Dictionary
e-Content Dictionary e-Content Dictionary e-Content Dictionary e-Content Dictionary
Figure 1.Cross System Architecture 
???? ?? ??????? ?? ??
???? ?? ??????? ?? ???Computer Computer?


?EnglishEnglish???????
TE??ET System JE??EJ System
S
earching
Word transfer
Web transfer
1 2 3
4
567
TE??ET System: Thai-English MT System
JE??EJ System : Japanese-English MT System
?? ?? ?? ??
?? ?? ?? ??
S
earching
technology of building MT can be transferred
from us to other countries in this region and
we know that English is broadly used as a
bridge to communicate among different
languages. It is simpler for a local developer
to build an MT system from his/her local
language to English (L1??E). If all
countries have their own Ln??E MT system,
sharing English as an intermediate
representation language reduces problems
shown above. Moreover, there are many
different ways to develop a MT system. Our
approach is to encapsulate the type differences
among MT systems. Thus we can decrease the
gap among languages by connecting the MT
system of each local language.
Our cross system MT also offers a good
infrastructure for many future applications
such as e-commerce, digital archive,
e-publishing, and so on as shown in figure 1.
Next we show an example of the usage of a
cross system MT.
3. A Usage Sample of
Cross System MT
This chapter shows an
application of using our
cross system MT. Figure 2
shows an example of our
expected application tool
for information retrieval.
We have two MT systems
in our workgroup, a
bilingual Thai??English
MT system and Japanese
??English MT system.
When a user starts to search by input a
keyword in Thai, such as a word ??????????? ?
[kom pyu ter]?(step1). The word
?????????????will be sent to the Thai?English
MT system to translate into
?computer?(step2). The word ?computer?
will be sent to the English?Japanese MT
system to translate into ?     
?(step3). The word ?? will
be used as a keyword to search for Japanese
web pages by a Japanese search engine
(step4). The result of Japanese web pages
from the search engine will be sent to
Japanese ? English MT system to translate
into English web pages (step 5). The result of
English homepages will again be sent to
English ? Thai MT system to translate into
Thai pages (step 6). Finally, the output of the
workgroup is web pages that contain the
keyword????????????? (step 7). These web pages
are selected from Japanese web pages.
This approach helps us to develop a MT
system that supports all languages taking into
Figure 2.Cross System Information Retrieval 
account of cost, quantity, and time
consumption. If each pair of languages can be
translated perfectly, it should produce a
satisfactory result for cross system technique.
There is, however, a major problem that we
have to consider about the efficiency due to
the fact that the efficiency of each pair of
machine translation is not completed. The
more languages we include in our system, the
less efficient the system becomes. In order to
find out the solution for this problem, we
show the linguistic problems and an example
for solving those problems in the next section.
4. Problems and Solution to Improve
Efficiency from Cross System Approach
The cross system MT approach seems to
be another solution to develop an MT system
that is possible to connect to other languages.
However, it has a major problem of efficiency
decreasing. When we consider the efficiency
of translation from the SL to TL, we find that
the machine translation cannot be transferred
completely. Section 4.1 gives the problem that
is possible in linguistics and in section 4.2 our
approach to transfer information from the first
SL to TL in order to examine these problems
is described.
4.1 Linguistic Problems
Manisara Meechoonuk and Somporn
Rakchonlatee [11] evaluated the result of
machine translation developed in Thailand,
they define the linguistic problems as shown
in table 1. In the investigation, they show that
the result from MT that is perfect translation is
about 29%, comprehensible translation is
about 55%, and incomprehensible translation
for the remaining . They also state that
?Mismatch Concept? is about 34% found and
is the most common linguistic problems .
These linguistic problems cause the
Table 1. List of Linguistic Problems and Meaning
Linguistics Problems Meaning
Mismatch Concept Inappropriate concept is selected
Misplaced Modifiers Wrong position of words, phrases or modifiers in TL resulting in
distortion of meaning
Inappropriate Literal Translation An inappropriate translation that follows closely the form of SL.
It can be categorized into 1) part of speech, 2) order, 3) idiom.
Addition of words or phrases Some words in TL that are not stated in SL are added.
Omission of words The meaning of a word or words when translating from SL to TL
is/are leaved out.
Insufficient definitions of idioms, two
word verbs, and phrasal verbs
The scope or number of words in electronic storage is either
limited or inaccurate according to the meanings of words in SL
Translation which does not conform to
Target language grammar
A difference sentence structure in TL that may cause an
incomprehensible translation.
Implicit in both SL and TL The implied meaning of a word in the SL is not expressed clearly
or fully in TL
Active in SL but passive in TL The participles appear in SL as active forms but are translated
into passive forms in TL
Insufficient Dictionary Definitions The scope or number of words in the electronic data dictionary is
limited
Different Semantic Segmentation
between SL and TL
Using difference marker, such as punctuation or space in SL and
TL may cause the incomprehensible translation
Specific in SL but generic in TL A specific word in SL is referred as a general meaning in TL
incomplete translation. An MT system cannot
correctly translate from second language to
third language if the result of translation from
the MT system from first language to second
language is not perfect . We, however, find
that ?Insufficient definitions of idioms,
two-word verbs, and phrasal verbs? and
?Insufficient Dictionary Definitions?
problems cannot be fixed by the cross
language system because of the lack of
information before the translation in SL.
We examine this problem by adding the
information from the first language together
with the result of second language. When the
MT system translates from the second
language to the third one , it can request
additional information that is attached from
the first language as a reference. Next we
show some examples of using our method.
4.2 Examples of Information Transfer
In section 4.1, we give linguistic problems
that cause a decrease of efficiency of
translation. In order to increase the efficiency,
we illustrate how to give the additional
information in order to help the translation
when the second language functions as a
source language. Our approach is that the
information we receive from the first
language is the most appropriate information.
If we can add additional information from the
first language as much as we can, it will help
us increase the efficiency of translation. We
use an XML as a language to transfer from
first language to other languages.
For example, we have two MT systems, a
Thai??English MT system and
English??Japanese MT system.
(1) Looking at the first sentence,
??????????? (dek duum ya)?
means ?A child drinks a medicine?.
But it is translated into ?A child drinks a
drug? by the Thai?English MT system.
The problem of this sentence is classified as
a ?Mismatch Concept? problem. A word
????has several meanings, such as medicine,
drug, cure, tonic and so on. For this problem
we can add all concepts as a reference as
follows.
?A child <AGT> drinks<? > a drug
<OBJ:c#drug, c#medicine, c#pill,
c#tonic> ?
This will help the second MT system not to
translate ???? as ?drug?, but refer all concepts
of ???? before the translation. The result of
translation should be, ?? by
English?Japanese MT system.
(2) Looking at another sentence in
Japanese.
? 	
  
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 96?102,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Syntactic Resource for Thai: CG Treebank
Taneth Ruangrajitpakorn      Kanokorn Trakultaweekoon      Thepchai Supnithi
Human Language Technology Laboratory
National Electronics and Computer Technology Center
112 Thailand Science Park, Phahonyothin Road, Klong 1, 
Klong Luang Pathumthani, 12120, Thailand
 +66-2-564-6900 Ext.2547, Fax.: +66-2-564-6772
{taneth.ruangrajitpakorn, kanokorn.trakultaweekoon, thep-
chai.supnithi}@nectec.or.th
Abstract
This  paper  presents  Thai  syntactic  re-
source:  Thai  CG treebank,  a  categorial 
approach  of  language  resources.  Since 
there  are  very  few  Thai  syntactic  re-
sources,  we designed to create treebank 
based on CG formalism. Thai corpus was 
parsed  with  existing  CG  syntactic  dic-
tionary  and  LALR  parser.  The  correct 
parsed trees were collected as prelimin-
ary  CG  treebank.  It  consists  of  50,346 
trees  from 27,239 utterances.  Trees  can 
be  split  into  three  grammatical  types. 
There are 12,876 sentential trees, 13,728 
noun  phrasal  trees,  and  18,342  verb 
phrasal trees. There are 17,847 utterances 
that obtain one tree, and an average tree 
per an utterance is 1.85.
1 Introduction
Syntactic lexical resources such as POS tagged 
corpus and treebank play one of  the  important 
roles in NLP tools for instance machine transla-
tion (MT), automatic POS tagger, and statistical 
parser. Because of a load burden and lacking lin-
guistic expertise to manually assign syntactic an-
notation to sentence, we are currently limited to a 
few  syntactical  resources.  There  are  few  re-
searches  (Satayamas  and  Kawtrakul,  2004)  fo-
cused  on  developing  system to  build  treebank. 
Unfortunately,  there is  no further report on the 
existing treebank in Thai  so far.  Especially for 
Thai,  Thai  belongs  to  analytic  language  which 
means  grammatical  information  relying  in  a 
word  rather  than  inflection  (Richard,  1964). 
Function words represent grammatical  informa-
tion such as tense, aspect, modal, etc. Therefore, 
to recognise word order is a key to syntactic ana-
lysis  for  Thai.  Categorial  Grammar  (CG)  is  a 
formalism which focuses on principle of syntact-
ic behaviour. It can be applied to solve word or-
der  issues  in  Thai.  To  apply  CG  for  machine 
learning and statistical based approach,  CG tree-
bank, is initially required.
CG is a based concept that can be applied to 
advance  grammar  such  as  Combinatory  Cat-
egorial  Grammar  (CCG)  (Steedman,  2000). 
Moreover,  CCG is  proved  to  be  superior  than 
POS for CCG tag consisting of fine grained lex-
ical  categories and its  accuracy rate (Curran et 
al., 2006; Clark and Curran, 2007). 
Nowadays,  CG and CCG become popular in 
NLP researches. There are several researches us-
ing them as a main theoretical approach in Asia. 
For example, there is a research in China using 
CG with Type Lifting (Dowty, 1988) to find fea-
tures interpretations of undefined words as syn-
tactic-semantic  analysis  (Jiangsheng,  2000).  In 
Japan,  researchers  also works  on Japanese cat-
egorial grammar (JCG) which gives a foundation 
of  semantic  parsing  of  Japanese  (Komatsu, 
1999). Moreover, there is a research in Japan to 
improve CG for solving Japanese particle shift-
ing phenomenon and using CG to focus on Ja-
panese particle (Nishiguchi, 2008).
This paper is organised as follows. Section 2 
reviews  categorial  grammar  and  its  function. 
Section  3  explains  resources  for  building  Thai 
CG treebank. Section 4 describes experiment res-
ult. Section 5 discusses issues of Thai CG tree-
bank. Last, Section 6 summarises paper and lists 
up future work.
96
2 Categorial Grammar
Categorial  grammar  (Aka.  CG or  classical  cat-
egorial  grammar)  (Ajdukiewicz,  1935;  Car-
penter,  1992;  Buszkowski,  1998;  Steedman, 
2000) is a formalism in natural language syntax 
motivated  by  the  principle  of  constitutionality 
and  organised  according  to  the  syntactic  ele-
ments. The syntactic elements are categorised in 
terms of their ability to combine with one anoth-
er to form larger constituents as functions or ac-
cording to a function-argument relationship. All 
syntactic categories in CG are distinguished by a 
syntactic category identifying them as one of the 
following two types: 
1. Argument: this type is a basic category, 
such  as  s  (sentence)  and  np  (noun 
phrase).
2. Functor  (or  functor  category):  this  cat-
egory type is a combination of argument 
and  operator(s)  '/'  and  '\'.  Functor  is 
marked to a complex lexicon to assist ar-
gument   to  complete  sentence  such  as 
s\np  (intransitive  verb)  requires  noun 
phrase from the left  side to complete a 
sentence.
CG captures the same information by associat-
ing a functional type or category with all gram-
matical entities. The notation ?/? is a rightward-
combining  functor  over  a  domain  of  ?  into  a 
range of ?. The notation ?\? is a leftward-com-
bining functor over ? into ?. ? and ? are both ar-
gument  syntactic  categories  (Hockenmaier  and 
Steedman,  2002;  Baldridge  and  Kruijff,  2003). 
The basic concept is to find the core of the com-
bination  and  replace  the  grammatical  modifier 
and complement with set of categories based on 
the same concept with fractions. For example, in-
transitive verb is needed to combine with a sub-
ject to complete a sentence therefore intransitive 
verb is written as s\np which means it  needs a 
noun phrase from the left side to complete a sen-
tence. If there is a noun phrase exists on the left 
side, the rule of fraction cancellation is applied 
as np*s\np = s. With CG, each lexicon can be an-
notated  with  its  own  syntactic  category. 
However,  a  lexicon could have more  than one 
syntactic category if it is able to be used in dif-
ferent appearances.
Furthermore,  CG  does  not  only  construct  a 
purely  syntactic  structure  but  also  delivers  a 
compositional  interpretation.  The  identification 
of derivation with interpretation becomes an ad-
vantage over others.
Example of CG derivation of Thai sentence is 
illustrated in Figure 1.
Recently,  there are many researches on com-
binatory categorial grammar (CCG) which is an 
improved  version  of  CG.  With  the  CG  based 
concept and notation, it is possible to easily up-
grade  it  to  advance  formalism.  However,  Thai 
syntax still remains unclear since there are sever-
al points on Thai grammar that are yet not com-
pletely  researched  and  found  absolute  solvent 
(Ruangrajitpakorn et al, 2007). Therefore, CG is 
currently set for Thai to significantly reduce over 
generation rate of complex composition or am-
biguate usage.
Figure 1. CG derivation tree of Thai sentence
3 Resources
To collect CG treebank, CG dictionary and pars-
er  are  essentially required.  Firstly,  Thai  corpus 
was parsed with the parser using CG dictionary 
as a syntactic resource. Then, the correct trees of 
each sentence were manually determined by lin-
guists and collected together as treebank.
3.1 Thai CG Dictionary
Recently, we developed Thai CG dictionary to be 
a syntactic dictionary for several purposes since 
CG is new to Thai NLP. CG was adopted to our 
syntactic  dictionary because  of  its  focusing  on 
lexicon's behaviour and its fine grained lexical-
ised  grammar.  CG is  proper  to  nature  of  Thai 
language since Thai belongs to analytic language 
typology; that is, its syntax and meaning depend 
on  the  use  of  particles  and word orders  rather 
than inflection (Boonkwan, and Supnithi, 2008). 
Moreover,  pronouns  and other  grammatical  in-
formation, such as tenses, aspects, numbers, and 
voices, are expressed by function words such as 
97
determiners, auxiliary verbs, adverbs and adject-
ives, which are in fix word order. With CG, it is 
possible  to  well  capture  Thai  grammatical  in-
formation. Currently we only aim to improve an 
accuracy of Thai syntax parsing since it still re-
mains unresearched ambiguities in Thai syntax. 
A list of grammatical Thai word orders which are 
handled with CG is shown in Table 1.
Thai 
utilisation Word-order 
Sentence - Subject + Verb + (Object)1 [rigid order]
Compound 
noun - Core noun + Attachment
Adjective 
modification - Noun + Adjective2
Predicate Ad-
jective - Noun + Adjective3
Determiner - Noun + (Classifier) + Determiner
Numeral ex-
pression
- Noun + (Modifier) + Number + Classifier + 
(Modifier)
Adverb 
modification
- Sentence + Adverb
- Adverb + Sentence 
Several aux-
iliary verbs  - Subject + (Aux verbs) + VP + (Aux verbs)
Negation
- Subject + Negator + VP
- Subject + (Aux verb) + Negator + (Aux verb) + 
VP
- Subject + VP + (Aux verb) + Negator + (Aux 
verb) 
Passive - Actee + Passive marker + (Actor) + Verb
Ditransitive  - Subject + Ditransitive verb + Direct object + In-direct  object
Relative 
clause - Noun + Relative marker + Clause
Compound 
sentence
- Sentence + Conjunction + Sentence
- Conjunction + Sentence +  Sentence
Complex 
sentence
- Sentence + Conjunction + Sentence
- Conjunction + Sentence +  Sentence
Subordinate 
clause that 
begins with 
word ? ????
- Subject + Verb + ? ?  ??? + Sentence
Table 1. Thai word orders that CG can solve
1 Information in parentheses is able to be omitted.
2 Adjective modification is a form of an adjective per-
forms as a modifier to a  noun, and they combine as a 
noun phrase.
3 Predicate adjective is a form of an adjective acts as a 
predicate of a sentence.
In addition, there are many multi-sense words 
in Thai. These words have the same surface form 
but  they have different  meanings  and  different 
usages. This issue can be solved with CG formal-
ism. The different usages are separated because 
the annotation of syntactic information. For ex-
ample,  Thai  word  ? ? ???? /k???/  can  be  used  to 
refer to noun as an 'island' and it is marked as 
np, and this word can also be denoted an action 
which  means  'to  clink'  or  'to  attach'  and  it  is 
marked as s\np/np.
After observation Thai word usage, the list of 
CG  was  created  according  to  CG  theory  ex-
plained in Section 2.
Thai  argument  syntactic  categories  were  ini-
tially created.  For Thai  language,  six argument 
syntactic  categories  were  determined.  Thai  CG 
arguments  are  listed  with  definition  and  ex-
amples in Table 2. Additionally,  np,  num, and 
spnum are a Thai  CG arguments  that  can dir-
ectly tag to a word, but  other can not and they 
can only be used as a combination for other argu-
ment.
With  the  arguments,  other  type  of  word  are 
created as functor by combining the arguments 
together  following  its  behaviour  and  environ-
mental  requirements.  The  first  argument  in  a 
functor is a result of combination. There are only 
two main operators in CG which are slash '/' and 
backslash '\' before an argument. A slash '/' refers 
to  argument  requirement  from the  right,  and  a 
backslash '\' refers to argument requirement from 
the left.  For instance,  a transitive verb requires 
one np from the left and one np from the right to 
complete a sentence. Therefore, it can be written 
as  s\np/np in CG form. However, several Thai 
words have many functions even it has the same 
word sense. For example, Thai word ? ?   ???? /c??? ?/
(to believe) is capable to use as intransitive verb, 
transitive  verb,  and  verb  that  can  be  followed 
with subordinate clause. This word therefore has 
three  different  syntactic  categories.  Currently, 
there are 72 functors for Thai.
With an argument and a functor, each word in 
the word list is annotated with CG. This informa-
tion is  sufficient  for  parser  to analyse  an input 
sentence into a grammatical tree. In conclusion, 
CG dictionary presently contains 42,564 lexical 
entries with 75 CG syntactic categories. All Thai 
CG categories are shown in Appendix A.
98
Thai ar-
gument 
category
definition example
np a noun phrase ? ??? (elephant), ?? (I, me) 
num A both digit and word cardinal number
???? (one), 
2  (two)
spnum
a number which is suc-
ceeding to classifier in-
stead of proceeding clas-
sifier like ordinary num-
ber 
???  (one),  
?????  (one)4
pp a prepositional phrase ????   (in car),??????   (on table)
s a sentence
? ???????? ??? 
(elephant eats ba-
nana) 
ws
a specific category for 
Thai which is assigned 
to a sentence that begins 
with Thai word ??? (that : 
sub-ordinate clause 
marker).
* ????????????? 5
'that he will come 
late' 
Table 2. List of Thai CG arguments
3.2 Parser
Our implemented lookahead LR parser (LALR)
(Aho and Johnson, 1974; Knuth, 1965) was used 
as a tool to syntactically parse input from corpus. 
For  our  LALR  parser,  a  grammar  rule  is  not 
manually determined, but it is automatically pro-
duced by a any given syntactic notations aligned 
with lexicons in a dictionary therefore this LALR 
parser has a coverage including a CG formalism 
parsing. Furthermore, our LALR parser has po-
tential to parse a tree from sentence, noun phrase 
and verb phrase.  However,  the parser  does not 
only return the best first tree, but also all parsable 
trees  to  gather  all  ambiguous  trees  since  Thai 
language tends to be ambiguous because of lack-
ing explicit sentence and word boundary.
3.3 Tree Visualiser
To reduce load burden of linguist to seek for the 
correct tree among all  outputs,  we developed a 
tree visualiser. This tool was developed by using 
an open source library provided by  NLTK: The 
4 This spnum category has a different usage from other 
numerical use, e.g. ? ??[noun,'horse'] ?? [classifier] 
?????[spnum,'one'] 'lit: one horse'. This case is different 
from normal numerical usage, e.g. ? ??[noun,'horse'] ???? 
[num,'one'] ?? [classifier] 'lit: one horse'
5 This example is a part of a sentence ?????????????????
??? 'lit: I believe that he will come late'
Natural  Language Toolkit  (http://www.nltk.org/
Home; Bird and Loper, 2004).
A tree visualiser is a tool to transform a textual 
tree structure to graphic tree.  This tool reads a 
tree  marking  with  parentheses  form and  trans-
mutes it into graphic. This tool can transform all 
output types of tree including sentence tree, noun 
phrase tree, and verb phrase tree. For example, 
Thai  sentence   "|???|???|????|??? ?|???|??????|" 
/ka:n l?: s?? ? p?n ka:n p?a? con p?ai/  'lit: Tiger 
hunting  is  an  adventure'  was  parsed  to  a  tree 
shown in Figure 2. With a tree visualiser, the tree 
in Figure 2 was transformed to a graphic tree il-
lustrated in Figure 3.
4 Experiment Result
In the preliminary experiment, 27,239 Thai utter-
ances with a mix of sentences and phrases from a 
general domain corpus are tested. The input was 
word-segmented  by  JwordSeg  (http://www.su-
parsit.com/nlp-tools) and approved by  linguists. 
In the test corpus, the longest utterance contains 
seventeen words, and the shortest utterance con-
tains two words. 
    s
      (np
        (np/(s\np)[???] 
          s\np(
            (s\np)/np[???] 
            np[????]
          )
        ) 
        s\np(
          (s\np)/np[??? ?] 
          np(
            np/(s\np)[???] 
           s\np[????? ]
          )
        )
      ) 
Figure 2. An example of CG tree output
Figure 3.  An example of graphic tree
99
All trees are manually observed by linguists to 
evaluate accuracy of the parser.  The criteria of 
accuracy are:
? A tree is correct if sentence is success-
fully parsed and syntactically correct ac-
cording to Thai grammar.
? In case of syntactic ambiguity such as a 
usage of preposition or phrase and sen-
tence  ambiguity,  any  tree  following 
those ambiguity is acceptable and coun-
ted as correct.
The parser returns 50,346 trees from  27,239 
utterances  as  1.85  trees  per  input  in  average. 
There are 17,874 utterances that returns one tree. 
The outputs can be divided into three different 
output  types:   12,876  sentential  trees,  13,728 
noun phrasal trees, and 18,342 verb phrasal trees. 
From the parser output, tree amount collecting 
in the CG tree bank in details is shown in Table 
3.
Tree type Utterance 
amount
Tree 
amount
Average
Only S 8,184 12,798 1.56
Only NP 7,211 12,407 1.72
Only VP 8,006 11,339 1.42
Both NP 
and S
1,583 5,188 3.28
Both VP 
and S
1,725 6,816 3.95
Both NP 
and VP
397 1,140 2.87
S, NP, VP 133 658 4.95
Total 27,239 50,346 1.85
Table 3. Amount of tree categorised by a dif-
ferent kind of grammatical tree
5 Discussion
After  observation  of  our  result,  we  found  two 
main issues.
First, some Thai inputs were parsed into sever-
al correct outputs due to ambiguity of an input. 
The use  of  an adjective  can be parsed to  both 
noun phrase  and  sentence  since  Thai  adjective 
can be used either a noun modifier or predicate. 
For example, Thai input ?|?????|????|??|????|? /
d??k d??k sod sai bon sa? na:m/  can be literally 
translated as follows:
1. Children is cheerful on a playground.
2. Cheerful children on a playground 
For  this  problem,  we  decided  to  keep  both 
trees in our treebank since they are both gram-
matically correct.
Second, the next issue is a variety of syntactic 
usages of Thai word.  It is the fact that Thai has a 
narrow range of word's surface but a lot of poly-
symy words. The more the word in Thai is gener-
ally used, the more utilisation of word becomes 
varieties. With the several combination, there are 
more chances to generate trees in a wrong con-
ceptual meaning even they form a correct  syn-
tactic word order. For example, Thai noun phrase 
???????| ? ?????? /kam la? ma? ha: sa:n/ 'lit: great 
power' can automatically be parsed to three trees 
for a sentence, a noun phrase, and a verb phrase 
because of polysymy of the first word. The first 
word "?? ? ??" has two syntactic usages as a noun 
which  conceptually refers  to  power and a  pre-
auxiliary verb to imply progressive aspect.  The 
word "??????" is an adjective which can per-
form two options in Thai as noun modifier and 
predicate. These affect parser to result three trees 
as follows:
np: np(np[??????] np\np[??????])
s: s(np[????? ] s\np[??????])
vp: s\np((s\np)/(s\np)[??????] s\np[??????])
Even though all trees are syntactically correct, 
only  noun  phrasal  tree  is  fully  acceptable  in 
terms of semantic sense as great power. The oth-
er trees are awkward and out of certain meaning 
in Thai. Therefore, the only noun phrase tree is 
collected into our CG treebank for such case.
6 Conclusion and Future Work
This paper presents Thai CG treebank which is a 
language resource for developing Thai NLP ap-
plication. This treebank consists of  50,346 syn-
tactic trees  from 27,239 utterances  with CG tag 
and  composition. Trees  can  be  split  into  three 
grammatical  types.  There  are  12,876 sentential 
trees, 13,728 noun phrasal trees, and 18,342 verb 
phrasal  trees.  There  are  17,847  utterances  that 
obtain one tree, and an average tree per an utter-
ance is 1.85.
In  the  future,  we  plan  to  improve  Thai  CG 
treebank to Thai CCG treebank. We also plan to 
reduce a variety of trees by extending semantic 
feature  into  CG.  We  will  improve  our  LALR 
parser to be GLR and PGLR parser respectively 
to reduce a missing word and named entity prob-
lem.  Moreover,  we  will  develop  parallel  Thai-
English  treebank  by  adding  a  parallel  English 
treebank  aligned  with  Thai  since  parallel  tree-
bank is useful resource for learning to statistical 
100
machine translation. Furthermore, we will apply 
obtained CG treebank for automatic CG tagging 
development.
Reference
Alfred  V.  Aho,  and  Stephen  C.  Johnson.  1974  LR 
Parsing,  In  Proceedings  of  Computing  Surveys, 
Vol. 6, No. 2.
Bob Carpenter. 1992. ?Categorial Grammars, Lexical 
Rules,and the English Predicative?,  In  R. Levine, 
ed., Formal Grammar: Theory and Implementation. 
OUP.
David Dowty,  Type raising,  functional  composition, 
and non-constituent conjunction, In Richard Oehrle 
et al, ed., Categorial Grammars and Natural Lan-
guage Structures. D. Reidel, 1988.
Donald  E.  Knuth.  1965.  On the  translation  of  lan-
guages from left to right, Information and Control 
86.
Hisashi Komatsu. 1999. ?Japanese Categorial Gram-
mar Based on Term and Sentence?.  In Proceeding  
of The 13th Pacific Asia Conference on Language,  
Information and Computation, Taiwan.
James R.  Curran,  Stephen Clark,  and David Vadas. 
2006.  Multi-Tagging  for  Lexicalized-Grammar 
Parsing. In Proceedings of the Joint Conference of  
the  International  Committee  on  Computational  
Linguistics and the Association for Computational  
Linguistics (ACL), Paris, France.
Jason  Baldridge,  and  Geert-Jan.  M.  Kruijff.  2003. 
?Multimodal combinatory categorial grammar?. In 
Proceeding  of  10th  Conference  of  the  European  
Chapter of the ACL-2003, Budapest, Hungary.
Julia Hockenmaier, and Mark Steedman. 2002. ?Ac-
quiring  Compact  Lexicalized  Grammars  from  a 
Cleaner Treebank?.  In Proceeding of 3rd Interna-
tional  Conference  on  Language  Resources  and  
Evaluation (LREC-2002), Las Palmas, Spain.
JWordSeg,  word-segmentation  toolkit.  Available 
from: http://www.suparsit.com/nlp-tools), 2007.
Kazimierz Ajdukiewicz. 1935. Die Syntaktische Kon-
nexitat, Polish Logic.
Mark  Steedman.  2000.  The  Syntactic  Process,  The 
MIT Press, Cambridge Mass.
NLTK:  The  Natural  Language  Toolkit.  Available 
from: http://www.nltk.org/Home
Noss B. Richard. 1964. Thai Reference Grammar, U. 
S. Government Printing Office, Washington DC.
Prachya  Boonkwan,  and  Thepchai  Supnithi.  2008. 
Memory-inductive  categorial  grammar:  An  ap-
proach to gap resolution in analytic-language trans-
lation.  In  Proceeding  of  3rd  International  Joint  
Conference  on  Natural  Language  Processing  
(IJCNLP-2008), Hyderabad, India.
Stephen Clark and James R.  Curran.  2007. Formal-
ism-Independent Parser Evaluation with CCG and 
DepBank.  In  Proceedings  of  the  45th  Annual  
Meeting of the Association for Computational Lin-
guistics (ACL),  Prague, Czech Republic.
Steven G. Bird, and Edward Loper. 2004. NLTK: The 
Natural Language Toolkit, In Proceedings of 42nd 
Meeting of the Association for Computational Lin-
guistics (Demonstration Track), Barcelona, Spain.
Sumiyo  Nishiguchi.  2008.  Continuation-based  CCG 
of  Japanese  Quantifiers.  In  Proceeding  of  6th 
ICCS,  The Korean  Society of  Cognitive Science, 
Seoul, South Korea.
Taneth  Ruangrajitpakorn, Wasan.  na  Chai, Prachya 
Boonkwan,  Montika  Boriboon,  and  Thepchai. 
Supnithi. 2007. The Design of Lexical Information 
for Thai  to English MT,  In  Proceeding of  SNLP 
2007, Pattaya, Thailand.
Vee Satayamas, and Asanee Kawtrakul. 2004. Wide-
Coverage  Grammar  Extraction  from  Thai  Tree-
bank. In Proceedings of Papillon 2004 Workshops  
on  Multilingual  Lexical  Databases,  Grenoble, 
France.
Wojciech  Buszkowski, Witold Marciszewski, and Jo-
han van Benthem, ed.,  Categorial Grammar, John 
Benjamin, Amsterdam, 1998.
Yu Jiangsheng. 2000. Categorial Grammar based on 
Feature Structures, dissersion in In-stitute of Com-
putational Linguistics, Peking University.
101
Appendix A
Type CG Category Type CG Category Type CG Category
Conjoiner ws/s Verb (s\np)/ws Function word ((s\np)\(s\np))/(np\np)
Conjoiner ws/(s/np) Verb, Adjective (s\np)/pp Function word ((s\np)\(s\np))/((s\np)\(s\np))
Function word spnum Determiner (s\np)/num Verb ((s\np)/ws)/pp
Particle, Adverb s\s Verb, Adjective (s\np)/np Verb ((s\np)/ws)/np
Verb s\np/(s\np)/np
Function word, Verb, 
Adverb, Auxiliary verb (s\np)/(s\np)
Adverb, Auxiliary 
verb ((s\np)/pp)\((s\np)/pp)
Verb s\np Function word (s\np)/(np\np) Verb ((s\np)/pp)/np
Function word, Particle s/s Auxiliary verb (s\np)/((s\np)/np)
Function word, 
Adverb ((s\np)/pp)/((s\np)/pp)
Function word s/np Conjunction (s/s)/s Auxiliary verb ((s\np)/np)\((s\np)/np)
Auxiliary verb s/(s/np) Function word (s/s)/np Verb ((s\np)/np)/np
Sentence s Function word (s/s)/(s/np) Verb ((s\np)/np)/(s\np)
Conjoiner pp/s Classifier (np\np)\num Adverberb ((s\np)/(s\np))\((s\np)/(s\np))
Conjoiner pp/np
Function word, Adverb, 
Auxiliary verb (np\np)\(np\np) Function word ((np\np)\(np\np))/np
Conjoiner pp/(s\np) Classifier (np\np)/spnum Conjoiner ((np\np)\(np\np))/(np\np)
Function word num Function word (np\np)/s
Adverb, Auxiliary 
verb ((np\np)/pp)\((np\np)/pp)
Classifier np\num Determiner (np\np)/num
Adverb, Function 
word ((np\np)/pp)/((np\np)/pp)
Adjective np\np Adjective, Conjoiner (np\np)/np Auxiliary verb ((np\np)/np)\((np\np)/np)
Noun, Pronoun np/pp Function word (np\np)/(s\np) Conjoiner ((np/pp)\(np/pp))/(np/pp)
Adjective, Determiner np/np
Classifier, Function word, 
Adverb, Auxiliary verb (np\np)/(np\np) Verb (((s\np)\np)
Function word np/(s\np) Auxiliary verb (np\np)/((np\np)/np) Verb (((s\np)/ws)/pp)/np
Auxiliary verb np/(np/np) Adjective, Determiner (np/pp)\(np/pp) Conjoiner (((s\np)/pp)\((s\np)/pp))/((s\np)/pp)
Function word np/((s\np)/np) Determiner (np/pp)/(np/pp) Function word (((s\np)/pp)\((s\np)/pp))/(((s\np)/pp)\((s\np)/pp))
Noun, Pronoun np Classifier ((s\np)\(s\np))\num Verb (((s\np)/pp)/np)/np
Conjunction (s\s)/s Classifier ((s\np)\(s\np))/spnum Function word (((s\np)/pp)/np)/(((s\np)/pp)/np)
Adverb, Auxiliary verb (s\np)\(s\np)
Function word ((s\np)\(s\np))/np Verb (((s\np)/np)/(s\np))/pp
Conjoiner ((s\np)\(s\np))/(s\np) Conjoiner (((np\np)/pp)\((np\np)/pp))/((np\np)/pp)
102
Proceedings of the 8th Workshop on Asian Language Resources, pages 129?136,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Supervised Learning based Chunking in Thai  
using Categorial Grammar 
Thepchai Supnithi, Peerachet Porkaew, 
Taneth Ruangrajitpakorn, Kanokorn 
Trakultaweekool 
Human Language Technology, 
National Electronics and Computer  
Technology Center 
{thepchai.sup, peera-
chet.por, taneth.rua, ka-
nokorn.tra}@nectec.or.th 
 
Chanon Onman, Asanee Kaw-
trakul  
Department of Computer Engineer-
ing, Kasetsart University and  
National Electronics and Computer  
Technology Center 
 
chanon.onman@gmail.com, 
asanee.kaw@nectec.or.th 
Abstract 
One of the challenging problems in Thai 
NLP is to manage a problem on a syn-
tactical analysis of a long sentence.  
This paper applies conditional random 
field and categorical grammar to devel-
op a chunking method, which can group 
words into larger unit. Based on the ex-
periment, we found the impressive re-
sults. We gain around 74.17% on sen-
tence level chunking. Furthermore we 
got a more correct parsed tree based on 
our technique. Around 50% of tree can 
be added. Finally, we solved the prob-
lem on implicit sentential NP which is 
one of the difficult Thai language pro-
cessing.  58.65% of sentential NP is cor-
rectly detected. 
1 Introduction 
Recently, many languages applied chunking, or 
shallow parsing, using supervised learning ap-
proaches. Basili (1999) utilized clause boundary 
recognition for shallow parsing. Osborne (2000) 
and McCallum et al (2000) applied Maximum 
Entropy tagger for chunking. Lafferty (2001) 
proposed Conditional Random Fields for se-
quence labeling. CRF can be recognized as a 
generative model that is able to reach global 
optimum while other sequential classifiers focus 
on making the best local decision. Sha and Pe-
reira (2003) compared CRF to other supervised 
learning in CoNLL task. They achieved results 
better than other approaches. Molina et al 
(2002) improved the accuracy of HMM-based 
shallow parser by introducing the specialized 
HMMs. 
In Thai language processing, many research-
es focus on fundamental level of NLP, such as 
word segmentation, POS tagging. For example, 
Kruengkrai et al (2006) introduced CRF for 
word segmentation and POS tagging trained 
over Orchid corpus (Sornlertlamvanich et al, 
1998.). However, the number of tagged texts in 
Orchid is specific on a technical report, which is 
difficult to be applied to other domains such as 
news, document, etc. Furthermore, very little 
researches on other fundamental tools, such as 
chunking, unknown word detection and parser, 
have been done. Pengphon et al (2002) ana-
lyzed chunks of noun phrase in Thai for infor-
mation retrieval task. All researches assume that 
sentence segmentation has been primarily done 
in corpus. Since Thai has no explicit sentence 
boundary, defining a concrete concept of sen-
tence break is extremely difficult. 
Most sentence segmentation researches con-
centrate on "space" and apply to Orchid corpus 
(Meknavin 1987, Pradit 2002). Because of am-
biguities on using space, the accuracy is not im-
pressive when we apply into a real application. 
Let consider the following paragraph which 
is a practical usage from news: 
129
"??????????????????????????????? ????????????????????????????????????
??????????????? | ?????????????????  | ??????????????????????????"  
lit: ?The red shirts have put bunkers around 
the assembly area and put oil and tires. The 
traffic is opened normally.? 
We found that three events are described in 
this paragraph. We found that both the first and 
second event do not contain a subject. The third 
event does not semantically relate to the previ-
ous two events. With a literal translation to Eng-
lish, the first and second can be combined into 
one sentence; however, the third events should 
be separated. 
As we survey in BEST corpus (Kosawat 
2009), a ten-million word Thai segmented cor-
pus. It contains twelve genres. The number of 
word in sentence is varied from one word to 
2,633 words and the average word per line is 
40.07 words. Considering to a News domain, 
which is the most practical usage in BEST, we 
found that the number of words are ranged from 
one to 415 words, and the average word length 
in sentence is 53.20. It is obvious that there is a 
heavy burden load for parser when these long 
texts are applied. 
Example 1:  
   ??                  ???                 ????????               ??                ???????????? ?
man(n) drive(v)   taxi(n)  find(v)   wallet(n) 
 
lit1: A man drove a taxi and found a wallet. 
lit2: A taxi chauffeur found a wallet. 
Example 2: 
   ???            ??          ????      ??????              ?????               ?????? 
should will must    can    develop(v) country(n) 
 
lit: possibly have to develop country. 
 
Figure 1. Examples of compounds in Thai 
Two issues are raised in this paper. The first 
question is "How to separate a long paragraph 
into a larger unit than word effectively?" We are 
looking at the possibility of combining words 
into a larger grain size. It enables the system to 
understand the complicate structure in Thai as 
explained in the example. Chunking approach in 
this paper is closely similar to the work of Sha 
and Pereira (2003). Second question is "How to 
analyze the compound noun structure in Thai?" 
Thai allows a compound construction for a noun 
and its structures can be either a sequence of 
nouns or a combination of nouns and verbs. The 
second structure is unique since the word order 
is as same as a word order of a sentence. We 
call this compound noun structure as a ?senten-
tial NP?. 
Let us exemplify some Thai examples related to 
compound word and serial construction problem 
in Figure 1. The example 1 shows a sentence 
which contains a combination of nouns and 
verbs. It can be ambiguously represented into 
two structures. The first alternative is that this 
sentence shows an evidence of a serial verb 
construction. The first word serves as a subject 
of the two following predicates. Another alter-
native is that the first three word can be formed 
together as a compound noun and they refer to 
?a taxi driver? which serve as a subject of the 
following verb and noun. The second alternative 
is more commonly used in practical language. 
However, to set the ?N V N? pattern as a noun 
can be very ambiguous since in the example 1 
can be formed a sentential NP from either the 
first three words or the last three words. 
From the Example 2, an auxiliary verb serial-
ization is represented. It is a combination of 
auxiliary verbs and verb. The word order is 
shown in Aux Aux Aux Aux V N sequence. 
The given examples show complex cases that 
require chunking to reduce an ambiguity while 
Thai text is applied into a syntactical analysis 
such as parsing. Moreover, there is more chance 
to get a syntactically incorrect result from either 
rule-based parser or statistical parser with a high 
amount of word per input. 
This paper is organized as follows. Section 2 
explains Thai categorial grammar. Section 3 
130
illustrates CRF, which is supervised method 
applied in this work.  Section 4 explains the 
methodology and experiment framework. Sec-
tion 5 shows experiments setting and result. 
Section 6 shows discussion. Conclusion and 
future work are illustrated in section 7. 
2 Linguistic Knowledge 
2.1 Categorial Grammar 
Categorial grammar (Aka. CG or classical cate-
gorial grammar) (Ajdukiewicz, 1935; Bar-
Hillel, 1953; Carpenter, 1992; Buszkowski, 
1998; Steedman, 2000) is formalism in natural 
language syntax motivated by the principle of 
constitutionality and organized according to the 
syntactic elements. The syntactic elements are 
categorised in terms of their ability to combine 
with one another to form larger constituents as 
functions or according to a function-argument 
relationship. All syntactic categories in CG are 
distinguished by a syntactic category identifying 
them as one of the following two types: 
1. Argument: this type is a basic category, 
such as s (sentence) and np (noun 
phrase).  
2. Functor (or function category): this cat-
egory type is a combination of argu-
ment and operator(s) '/' and '\'. Functor 
is marked to a complex constituent to 
assist argument to complete sentence 
such as s\np (intransitive verb) requires 
noun phrase from the left side to com-
plete a sentence. 
CG captures the same information by associ-
ating a functional type or category with all 
grammatical entities. The notation ?/? is a 
rightward-combining functor over a domain of ? 
into a range of ?. The notation ?\? is a leftward-
combining functor over ? into ?. ? and ? are 
both argument syntactic categories 
(Hockenmaier and Steedman, 2002; Baldridge 
and Kruijff, 2003). 
The basic concept is to find the core of the 
combination and replace the grammatical modi-
fier and complement with set of categories 
based on the same concept with fractions. For 
example, intransitive verb is needed to combine 
with a subject to complete a sentence therefore 
intransitive verb is written as s\np which means  
Figure 2 Example of Thai CG-parsed Tree. 
it needs a noun phrase from the left side to  
complete a sentence. If there is a noun phrase 
exists on the left side, the rule of fraction can-
cellation is applied as np*s\np = s. With CG, 
each constituent is annotated with its own syn-
tactic category as its function in text. Currently 
there are 79 categories in Thai. An example of 
CG derivation from Thai is shown in Figure 2.  
2.2 CG-Set 
CG-Set are used as a feature when no CG are 
tagged to the input. We aim to apply our chunk-
er to a real world application. Therefore, in case 
that we have only sentence without CG tags, we 
will use CG-Set instead.           
Cat-
Set 
Index 
Cat-Set Member 
0 np ????????? 
2 s\np/pp,s\np/np,s\np/pp/np,s\np ????, ???? 
3 
(np\np)/(np\np), 
((s\np)\(s\np))/spnum, 
np, 
(np\np)\num,np\num, 
(np\np)/spnum, 
((s\np)\(s\np))\num 
????, 
?????? 
62 (s\np)\(s\np),s\s ??'?, ??'?, ??? 
134 np/(s\np), 
np/((s\np)/np) ???, ???? 
Table 1 Example of CG-Set  
131
The concept of CG-Set is to group words that 
their all possible CGs are equivalent to the 
other. Therefore every word will be assigned to 
only one CG-Set. By using CG-Set we use the 
lookup table for tagging the input. Table 1 
shows examples of CG-set. Currently, there are 
183 CG set. 
3 Conditional Random Field (CRF) 
CRF is an undirected graph model in which 
each vertex represents a random variable whose 
distribution is to be inferred, and edge 
represents a dependency between two random 
variables. It is a supervised framework for 
labeling a sequence data such as POS tagging 
and chunking. Let X  is a random variable of 
observed input sequence, such as sequence of 
words, and Y is a random variable of label 
sequence corresponding to X , such as sequence 
of POS or CG. The most probable label 
sequence ( y? ) can be obtain by 
                     )|(maxarg? xypy =  
 Where nxxxx ,...,, 21= and nyyyy ,...,, 21=  
)|( xyp  is the conditional probability 
distribution of a label sequence given by an 
input sequence. CRF defines )|( xyp as 
                  ?
?
??
?
?= ?
=
n
i
ixyF
Z
xyP
1
),,(exp1)|(  
where ( )? ? == y ni ixyFZ 1 ),,(exp  is a 
normalization factor over all state sequences. 
),,( ixyF  is the global feature vector of CRF 
for sequence x and y at position i . ),,( ixyF  
can be calculated by using summation of local 
features. 
?? += ?
j
jj
i
iiii tyxgtyyfixyF ),,(),,(),,( 1 ??
Each local feature consists of transition feature 
function ),,( 1 tyyf iii ?  and per-state feature 
function ),,( tyxg j . Where i? and j? are 
weight vectors of transition feature function and 
per-state feature function respectively.  
The parameter of CRF can be calculated by 
maximizing the likelihood function on the 
training data. Viterbi algorithm is normally 
applied for searching the most suitable output. 
4 Methodology 
Figure 3 shows the methodology of our 
experiments. To prepare the training set, we 
start with our corpus annotated with CG tag. 
Then, each sentence in the corpus was parsed by 
Figure 3 Experimental Framework 
132
our Thai CG parser, developed by GLR tech-
technique. However, not all sentences can be 
parsed successfully due to the complexity of the 
sentence. We kept parsable sentences and 
unparsable sentences separately. The parsable 
sentences were selected to be the training set.  
There are four features ? surface, CG, CG-set 
and chunk marker ? in our experiments. CRF is 
applied using 5-fold cross validation over 
combination of these features. Accuracy in term 
of averaged precision and recall are reported. 
We select the best model from the experiment 
to implement the chunker. To investigate 
performance of the chunker, we feed the 
unparsable sentences to the chunker and 
evaluate them manually.  
After that, the sentences which are correctly 
chunked will be sent to our Thai CG parser. We 
calculate the number of successfully-parsed 
sentences and the number of correct chunks. 
5 Experiment Settings and Results 
5.1 Experiment on chunking 
5.1.1 Experiment setting 
To develop chunker, we apply CG Dictionary 
and CG tagged corpus as input. Four features 
are provided to CRF. Surface is a word surface. 
CG is a categorial grammar of the word. CG-set 
is a combination of CG of the word. IOB 
represents a method to mark chunk in a 
sentence. "I" means "inner" which represents 
the word within the chunk. "O" means "outside" 
which represents the word outside the chunk. 
"B" means "boundary" which represents the 
word as a boundary position. It accompanied 
with five chunk types. "NP" stands for noun 
phrase, "VP" stands for verb phrase, "PP" stands 
for preposition phrase, "ADVP" stands for 
adverb phrase and S-BAR stands for 
complementizer that link two phrases.  
Surface and CG-set are developed from CG 
dictionary. CG is retrieved from CG tagged 
corpus. IOB is developed by parsing tree. We 
apply Thai CG parser to obtain the parsed tree. 
Figure 4 shows an example of our prepared 
data. We provide 4,201 sentences as a training 
data in CRF to obtain a chunked model. In this 
experiment, we use 5-fold cross validation to 
evaluation the model in term of F-measure.  
surface cg_set cg chunk_label 
?? 74 s/s/np B-ADVP 
??? 3 np I-ADVP 
?? 180 (np\np)/(s\np) I-ADVP 
?? ? 54 (s\np)/(s\np) I-ADVP 
???? 7 s\np I-ADVP 
???? 130 ((s/s)\(s/s))/(s/s) I-ADVP 
?? 74 s/s/np I-ADVP 
??????? 0 np I-ADVP 
??? 0 np B-NP 
??? 8 s\np/np B-VP 
???'? 0 np B-NP 
?? 148 (s\np)/(s\np) B-VP 
???????? 2 s\np I-VP 
Figure 4 An example of prepared data 
 
Table 2 Chunking accuracy of each chunk 
133
  
5.1.2 Experiment result 
From Table 2, considering on chunk based lev-
el, we found that CG gives the best result 
among surface, CG-set, CG and their combina-
tion. The average on three types in terms of F-
measure is 86.20.  When we analyze infor-
mation in detail, we found that NP, VP and PP 
show the same results. Using CG shows the F-
measure for each of them, 81.15, 90.96 and 
99.56 respectively.   
From Table 3, considering in both word level 
and sentence level, we got the similar results, 
CG gives the best results. F-measure is 93.24 in 
word level and 74.17 in sentence level. This 
shows the evidence that CG plays an important 
role to improve the accuracy on chunking. 
5.2 Experiment on parsing 
5.2.1 Experiment setting 
We investigate the improvement of parsing con-
sidering unparsable sentences.  There are 14,885 
unparsable sentences from our CG parser. These 
sentences are inputted in chunked model to ob-
tain a chunked corpus. We manually evaluate 
the results by linguist. Linguists evaluate the 
chunked output in three types. 0 means incorrect 
chunk. 1 means correct chunk and 2 represents a 
special case for Thai NP, a sentential NP. 
5.2.2 Experiment result 
From the experiment, we got an impressive re-
sult. We found that 11,698 sentences (78.59%) 
are changed from unparsable to parsable sen-
tence. Only 3,187 (21.41%) are unparsable.  We 
manually evaluate the parsable sentence by ran-
domly select 7,369 sentences. Linguists found 
3,689 correct sentences (50.06%). In addition, 
we investigate the number of parsable chunk 
calculated from the parsable result and found 
37,743 correct chunks from 47,718 chunks 
(78.47%).  We also classified chunk into three 
types NN VP and PP and gain the accuracy in 
each type 79.14% ,74.66% and 92.57% respec-
tively. 
6 Discussion 
6.1 Error analysis 
From the experiment results, we found the fol-
lowing errors. 
6.1.1 Chunking Type missing 
Some chunk missing types are found in experi-
ment results. For example, [PP ?????? (rec-
ord)][NP ????????????????? (character about)]. [PP 
Table 3 Chunking accuracy based on  
word and sentence. 
Figure 4 An Example of sentential NP 
134
?????? (record)] should be defined as VP instead 
of PP. 
6.1.2 Over-grouping 
In the sentence ?[VP ?? ? (Using)][NP 
(medicine)][VP ????? (treat) ][NP ???????????' ?????
?????? (each disease have to)][PP ??? (follow) ]
[NP ???????????????? ?(doctor?s instruction)] ?, we 
found that ?NP ???????????' ??????????? (each disease 
have to) ? has over-grouping. IT is necessary to 
breakdown to NP ???????????' ?(each disease)  and  
VP ??????????(have to). The reason of this error is 
due to allow the sentential structure NP VP NP, 
and then NP and VP are combined. 
6.1.3 Sentential NP 
We investigated the number of sentential NP. If 
the number of chunk equal to 1, sentence should 
not be recognized as NP. Other cases are de-
fined as NP. We found that 929 from 1,584 sen-
tences (58.65 % of sentences) are correct sen-
tential NP. This evidence shows the impressive 
results to solve implicit NP in Thai. Figure 4 
shows an example of sentential NP.  
6.1.4 CG-set  
Since CG-set is another representation of word 
and can only detect from CG dictionary. It is 
very easy to develop a tag sequence using CG-
set. We found that CG-set is more powerful than 
surface. It might be another alternative for less 
language resource situation. 
6.2 The Effect of Linguistic Knowledge on 
chunking 
Since CG is formalism in natural language syn-
tax motivated by the principle of constitutionali-
ty and organised according to the syntactic ele-
ments, we would like to find out whether lin-
guistic knowledge effects to the model. We 
grouped 89 categorial grammars into 17 groups, 
called CG-17.  
It is categorized into Noun, Prep, Noun 
Modifier, Number modifier for noun, Number 
modifier for verb, Number, Clause Marker, 
Verb with no argument, Verb with 1 argument, 
Verb with 2 or more arguments, Prefix noun, 
Prefix predicate, Prefix predicate modifier, 
Noun linker, Predicate Modification, Predicate 
linker, and Sentence Modifier.  
We found that F-measure is slightly improved 
from 74.17% to 75.06%. This shows the evi-
dence that if we carefully categorized data based 
on linguistics viewpoint, it may improve more 
accuracy.  
7 Conclusions and Future Work 
In this paper, we stated Thai language problems 
on the long sentence pattern and find the novel 
method to chunk sentence into smaller unit, 
which larger than word. We concluded that us-
ing CRF accompanied with categorical grammar 
show the impressive results. The accuracy of 
chunking in sentence level is 74.17%. We are 
possible to collect 50% more on correct tree. 
This technique enables us to solve the implicit 
sentential NP problem. With our technique, we 
found 58% of implicit sentential NP. In the fu-
ture work, there are several issues to be im-
proved. First, we have to trade-off between 
over-grouping problem and implicit sentential 
problem. Second, we plan to consider ADVP, 
SBAR, which has a very small size of data. It is 
not adequate to train for a good result. Finally, 
we plan to apply more linguistics knowledge to 
assist more accuracy. 
References 
Abney S., and Tenny C., editors, 1991. Parsing 
by chunks, Priciple-based Parsing. Kluwer 
Academic Publishers. 
Awasthi P., Rao D., Ravindram B., 2006. Part 
of Speech Tagging and Chunking with HMM 
and CRF, Proceeding of the NLPAI Machine 
Learning Competition. 
Basili R., Pazienza T., and Massio F., 1999. 
Lexicalizing a shallow parser, Proceedings of 
135
Traitement Automatique du Langage Naturel 
1999. Corgese, Corsica. 
Charoenporn Thatsanee, Sornlertlamvanich Vi-
rach,  and Isahara Hitoshi. 1997. Building A 
Large Thai Text Corpus - Part-Of-Speech 
Tagged Corpus: ORCHID. Proceedings of 
Natural Language Processing Pacific Rim 
Symposium. 
Kosawat Krit, Boriboon Monthika, Chootrakool 
Patcharika, Chotimongkol Ananlada, Klaithin 
Supon, Kongyoung Sarawoot, Kriengket 
Kanyanut, Phaholphinyo Sitthaa, Puroda-
kananda Sumonmas,Thanakulwarapas 
Tipraporn, and Wutiwiwatchai Chai. 2009. 
BEST 2009: Thai Word Segmentation Soft-
ware Contest. The Eigth International Sym-
posium on Natural Language Processing  : 
83-88. 
Kruengkrai C., Sornlertlumvanich V., Isahara H, 
2006. A Conditional Random Field Frame-
work for Thai Morphological Analysis, Pro-
ceedings of 5th International Conference on 
Language Resources and Evaluation (LREC-
2006). 
Kudo T., and Matsumoto Y., 2001. Chunking 
with support vector machines, Proceeding of 
NAACL. 
Lafferty J., McCallum A., and Pereira F., 2001. 
Conditional Random Fields : Probabilistic 
models for segmenting and labeling sequence 
data. In Proceeding of ICML-01, 282-289. 
McCallum A., Freitag D., and Pereira F. 2000. 
Maximum entropy markov model for infor-
mation extraction and segmentation. Pro-
ceedings of ICML. 
Molina A., and Pla F., 2002. Shallow Parsing 
using Specialized HMMs, Journal of Machine 
Learning Research 2,595-613 
Nguyen L. Minh, Nguyen H. Thao, and Nguyen 
P., Thai. 2009. An Empirical Study of Viet-
namese Noun Phrase Chunking with Discrim-
inative Sequence Models, Proceedings of the 
7th Workshop on Asian Language Resources, 
ACL-IJCNLP 2009,9-16 
Osborne M. 2000. Shallow Parsing as Part-of-
Speech Tagging. Proceedings of CoNLL-
2000 and LLL-2000, Lisbon, Portugal. 
Pengphon N., Kawtrakul A., Suktarachan M., 
2002. Word Formation Approach to Noun 
Phrase Analysis for Thai,  Proceedings of 
SNLP2002. 
Sha F. and Pereira F., 2003. Shallow Parsing 
with Conditional Random Fields, Proceeding 
of HLT-NAACL. 
 
136
Proceedings of the 8th Workshop on Asian Language Resources, pages 161?168,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
A Current Status of Thai Categorial Grammars and 
Their Applications
Taneth Ruangrajitpakorn and Thepchai Supnithi
Human Language Technology Laboratory
National Electronics and Computer Technology Center 
{taneth.ruangrajitpakorn,thepchai.supnithi}@nec-
tec.or.th
Abstract
This  paper presents a current  status of 
Thai resources and tools for CG  devel-
opment.  We also  proposed  a  Thai  cat-
egorial dependency grammar (CDG), an 
extended version of CG which includes 
dependency  analysis  into  CG notation. 
Beside, an idea of how to group a word 
that has the same functions are presen-
ted to gain a certain type of category per 
word. We also discuss about a difficulty 
of  building  treebank  and  mention  a 
toolkit for assisting on a Thai CGs tree 
building  and  a  tree  format  representa-
tions. In this paper, we also give a sum-
mary  of  applications  related  to  Thai 
CGs.
1 Introduction
Recently, CG formalism was applied to several 
Thai NLP applications such as syntactic inform-
ation  for  Thai  to  English  RBMT (Ruangrajit-
pakorn et al, 2007), a CG treebank (Ruangrajit-
pakorn et al, 2009), and an automatic CG tag-
ger (Supnithi et al, 2010). CG shows promises 
to handle Thai syntax expeditiously since it can 
widely  control  utilisations  of  function  words 
which are the main grammatical expression of 
Thai. 
In the previous research, CG was employed 
as a feature for an English to Thai SMT and it 
resulted better accuracy in term of BLEU score 
for  1.05% (Porkaew and  Supnithi,  2009).  CG 
was  also  used  in  a  research  of  translation  of 
noun phrase from English to Thai using phrase-
based SMT with CG reordering rules, and it re-
turned 75% of better and smoother translation 
from human evaluation (Porkaew et al, 2009).
Though CG has a high potential in immediate 
constituency analysis for Thai, it sill lacks of a 
dependency analysis which is also important in 
syntactical parsing. In this paper, we propose a 
category dependency grammar which is an up-
graded version of CG to express a dependency 
relation  alongside  an  immediate  constituency 
bracketing.  Moreover,  some  Thai  dependency 
banks  such  as  NAIST  dependency  bank 
(Satayamas and Kawtrakul, 2004) have been de-
veloped.  It  is  better  to  be  able  to interchange 
data between a Thai CG treebank and a Thai de-
pendency bank in order to increase an amount 
of data since building treebank from scratch has 
high cost.
In  the  point  of  resources  and  applications, 
Thai CG and CDG still have a few number of 
supported tools. Our CG treebank still contains 
insufficient  data  and  they  are  syntactically 
simple and do not reflect a natural Thai usage. 
To add complex Thai tree, we found that Thai 
practical usage such as news domain contains a 
number of word and very complex.
An example of  natural  Thai text from news, 
which contains 25 words including nine under-
lined function words, is instanced with transla-
tion in Figure 1. 
?? ????|???|???|?? ???|???|??|?????|???| |???|??|???|???|????????|???|???????|??????| |???|??? ???|???| |???????|???|??????|??|???????|????|????
lit: The red-shirts have put bunkers around the as-
sembly area and poured oil and worn-out tires.
Figure 1. An example of Thai usage in natural 
language
161
We parsed the example in  Figure 1 with CG 
and our parser returned 1,469 trees. The result is 
in a large number because many Thai structural 
issues in a syntactic level cause ambiguity. 
The first issue is many Thai words can have 
multiple  functions  including  employing  gram-
matical usage and representing a meaning. For 
instance, a word ????? /tee/ can be a noun, a relat-
ive clause marker, a classifier, a preposition, and 
an  adjective  marker.  A word  ????  /kon/  can 
refer  to a person,  a  classifier  of  human being 
and  it  can  denote  an  action.  A word  ??? ???? 
/kumlung/ can serve as an auxiliary verb to ex-
press progressive aspect and also refers a mean-
ing as a noun. A function word is a main gram-
matical representation and it hints an analyser to 
clarify an overall context structure. Regretfully, 
it is difficult for system to instantly indicate the 
Thai function words by focusing on the lexical 
surface and their surrounding lexicons. This cir-
cumstance is  stimulates  an over  generation  of 
many improper trees.
The second issue is a problem of Thai verb 
utilisations. Thai ordinarily allows to omit either 
a  subject  or  an  object  of  a  verb.  Moreover,  a 
Thai  intransitive  verb  is  occasionally  attached 
its  indirect  object  without  a  preposition.  Fur-
thermore, Thai adjective allows to perform as a 
predicate without a marker. With an allowance 
of verb serialisation, these complexify linguists 
to design a category into well-crafted category 
set for verb. Therefore, many Thai verbs contain 
several syntactic categories to serve their many 
functions.
The last issues is a lack of an explicit bound-
ary for a word, a phrase and a sentence in Thai. 
A Thai  word  and  phrase  boundary  is  implicit 
and  a  space  is  not  significantly  signified  a 
boundary  in  the  context.  In  addition,  most  of 
modifiers are attached after a core element. This 
leads  to  ambiguity  of  finding  an  ending  of  a 
subject with an attached adjective and relative 
clause since the verbs in attachment can be seri-
alised and consequently placed with following 
main verb phrase (which is likely to be serial-
ised either) without a signified indicator.
With these issues, a parser with only syntactic 
information merely returns a large number of all 
possible  trees.  It  becomes  difficulty  and  time 
consuming for linguists to select the correct one 
among them. Moreover, with many lexical ele-
ments, using a statistical parser has a very low 
chance to generate a correct tree and a  manual 
tree  construction  is  also  required as  a  gold 
standard. Thus, we recently implemented an as-
sistant toolkit for tree construction and tree rep-
resentation  to  reduce  linguists'  work  load  and 
time consumption.
This paper aims to explain the current status 
of resource and tool for CG and CDG develop-
ment  for  Thai  language.  We  also  listed  open 
tools and applications that relate to CGs in this 
paper.
The rest of the paper is organised as follows. 
Section  2  presents  a  Thai  categorial  grammar 
and  its  related  formalism.  Section  3  explains 
status of CGs resources including syntactic dic-
tionary and treebank. Section 4 shows details of 
a toolkit  which assists  linguist  to manage and 
construct CGs derivation tree and tree represent-
ations. Section 5 provides information of applic-
ations that involve Thai CGs. Lastly, Section 6 
concludes this paper and lists future works.
2 Thai Categorial Grammars
2.1 Categorial Grammar
Categorial  grammar  (Aka.  CG  or  classical 
categorial  grammar)  (Ajdukiewicz,  1935; 
Carpenter,  1992;  Buszkowski,  1998)  is  a 
formalism in natural language syntax motivated 
by  the  principle  of  constitutionality  and 
organised according  to the  syntactic  elements. 
The syntactic elements are categorised in terms 
of their ability to combine with one another to 
form  larger  constituents  as  functions  or 
according to a function-argument relationship. 
CG captures the same information by associ-
ating  a  functional  type  or  category  with  all 
grammatical  entities.  Each  word  is  assigned 
with at least one syntactic category, denoted by 
an argument symbol (such as np and num) or a 
functional  symbol  X/Y and  X\Y that  require  Y 
from the right and the left respectively to form 
X.
The basic concept is to find the core of the 
combination and replace the grammatical modi-
fier  and  complement  with  set  of  categories 
based on the same concept of the rule of frac-
tion cancellation as follow:
162
Upon applying to Thai, we have modified ar-
gument set and designed eight arguments shown 
in Table 1.
From the last  version,  two arguments  were 
additionally designed. ?ut? argument was added 
to denote utterance that is followed after a word 
?????. The word ????? has a special function to let 
the word after it perform as an exemplified ut-
terance and ignore its appropriate category as it 
is signified an example in context. Comparing 
to ?ws? argument, the word ?? ???  is functioned 
in a different sense which is used to denote a be-
ginner of subordinate clause.
For ?X? category, it is used for punctuation or 
symbol  which  takes  the same categories  from 
the left  or  right  sides  and produces  the  taken 
category. For instance, ??? is a marker to denote 
after many types of content word. In details, this 
symbol signifies plurality while it is after noun 
but it intensifies a degree of meaning while it is 
placed after adjective.
Upon  CG  design,  we  allowed  only  binary 
bracketing  of  two  immediate  constituents.  To 
handle serial construction in Thai including seri-
al  verb construction,  we permitted the exactly 
same  categories  which  are  consequent  to  be 
combined.  For  example,  Thai  noun  phrase 
'???(np)|???????????(np)' (lit: a consensus of the 
government)  contains  two  consequent  nouns 
without a joint word to form a noun phrase. Un-
fortunately, there still remain limits of syntactic 
parsing in CG that can not handle long depend-
ency and word omission in this state.
2.2 Categorial Dependency Grammar
Categorial  dependency  grammar  (CDG)  is  an 
extension of CG. CDG differs from CG in that a 
dependency  direction  motivated  by  Collins 
(1999)  is  additionally  annotated  to  each  slash 
notation  in  syntactic  category.  The  derivation 
rules of CDG are listed as follow:
X/<Y : d1 Y : d2 => X : h(d1) ? h(d2)
X/>Y : d1 Y : d2 => X : h(d1) ? h(d2)
Y : d1 X\<Y : d2 => X : h(d1) ? h(d2)
Y : d1 X\>Y : d2 => X : h(d1) ? h(d2)
where the notations h(d1)  ? h(d2) and h(d1)  ? 
h(d2) mean a dependency linking from the head 
of the dependency structure d1 to the head of d2, 
and that linking from the head of d2 to the head 
of d1, respectively. Throughout this paper, a con-
stituent type of the syntactic category c and the 
dependency structure d is represented by c:d.
Let us exemplify a dependency driven deriva-
tion  of  CDG  of  sentence  'Mary  drinks  fresh 
milk'  in  Figure 2. In  Figure 2(a),  each pair of 
constituents is combined to form a larger con-
stituent with its head word. Figure 2(b) shows a 
dependency structure equivalent  to the deriva-
tion in Figure 2(a).
Comparing  to  PF-CCG  (Koller  and  Kuhl-
mann, 2009), there is different in that their PF-
CCG dependency markers are fixed to the direc-
tion of slashes while CDG dependency markers 
are  customised  based  on  behaviour  of  a  con-
stituent.
CDG offers an efficient way to represent de-
pendency structures alongside syntactic deriva-
tions. Apart from immediate constituency ana-
lysis,  we  can  also  investigate  the  correspond-
ence between the syntactic derivations and the 
dependency  structures.  It  benefits  linguists  in 
details  a  grammar for  a specific  language be-
argu-
ment 
category
definition example
np a noun phrase ???? (elephant), ?? (I, me) 
num a digit and a spelled-out number
????? (one), 
2 (two)
spnum a number which is suc-ceeding to classifier 
??? (one), ????? (one)
pp a prepositional phrase ???? (in car),?????? (on table)
s a sentence
???????????? 
(an elephant eats 
a banana) 
ws
a specific category for 
Thai which is assigned to 
a sentence or a phrase that 
begins with Thai word ??? 
(that : sub-ordinate clause 
marker).
* ?????????????
'that he will 
come late'
* ??????????
'that (he) will 
come late'
ut
an utterance using to ex-
emplify a specific word 
after a word ???
??  ???   ? ?
'the word ?good?'
X
an undefined category that 
takes the same categories 
from the left or right sides 
and produces the taken 
category.
???? ?
(plural marker)????? ?
(intensifier)
Table 1. A list of Thai CDG arguments
163
cause  we  can  restrain  the  grammar  in  lexical 
level.
In  this  paper,  our  Thai  CG was  applied  to 
CDG. For the case of serial construction, we set 
the  left  word  as  a  head  of  dependency  since 
Thai modifiers and dependents are ordinarily at-
tached on right side.
2.3 Categorial Set
A categorial set is a group of lexicons that ex-
actly contains the same function(s) in terms of 
their  category amount and all  their  same syn-
tactic categories. With a specific surface, each 
word certainly is in one categorial set. For ex-
ample,  suppose that  we have following words 
and categories:
word category POS
?????????,????,???,???? ? np noun
???,????,????,???,???? ? s\np/np verb
???,????,??? ? np\np/num classifier
We can group the given words into five groups 
based on the concept of categorial set shown in 
Table 2.
Set-
index Category member Word member
1 np ?????????
2 s\np/np ???,????
3 nps\np/np ????
4
np
s\np/np
np\np/num
???,????
5 np\np/num ???
Table 2. An example of categorial set
For  current  status,  we  attain  183 categorial 
sets  in total and the maximum amount of cat-
egory member in a categorial set is 22 categor-
ies.
3 Categorial Grammars Resources
To apply categorial grammars to Thai NLP, syn-
tactic dictionary and treebank are a mandatory.
3.1 Categorial Grammars Dictionary
For using in other work and researches, we col-
lected  all  CGs  information  into  one  syntactic 
dictionary.  An  example  of  CGs  dictionary  is 
shown in Table 3. In a summary, our Thai CGs 
dictionary  currently  contain  70,193  lexical 
entries with 82 categories for both CG and CDG 
and 183 categorial sets.
Lexicon CG CDG Cset no.
???? np np 0
???? np,s\np/np,np\n
p/num
np,s\<np/>np,np\>
np/<num
15
??? s\np/np,s\np s\<np/>np,s\<np 13
??? s\s/s,s/s/s s\<s/>s,s/>s/>s 43
??? s\np/pp,s\np,s\
np/ws
s\<np/>pp,s\<np,s\
<np/>ws
19
????? np\np,s\np np\>np,s\<np 3
???? s\np s\<np 1
??????? np\np,s\np np\>np,s\<np 3
??? s\np s\<np 1
??? s\np/np,s\np s\<np/>np,s\<np 13
????? np np 0
??? s\np/np,s\np/ws
,np\np/ut
s\<np/>np,s\<np/>
ws,np\>np/>ut
136
????? s\s/s,s/s/s s\<s/>s,s/>s/>s 43
Table 3. An example of Thai CGs dictionary
3.2 Thai CDGTreebank
Our CG treebank was recently transformed into 
dependency-driven  derivation  tree  with  CDG. 
An example of derivation tree of sentence |???|
Figure 2. Syntactic derivation of ?Mary drinks fresh milk? based on CDG
164
???|????|????|???|??????| 'lit: Tiger hunting is an 
adventure' comparing between CG and CDG is 
illustrated in Figure 3.
s
  (np
    (np/(s\np)[???]
    s\np(
      (s\np)/np[???]
      np[????]
    )
  )
  s\np(
    (s\np)/np[????]
    np(
      np/(s\np)[???]
      s\np[??????]
    )
  )
)
(a) CG derivation tree
s
  (np
    (np/>(s\<np)[???]
    s\<np(
      (s\<np)/>np[???]
      np[????]
    )
  )
  s\<np(
    (s\<np)/>np[????]
    np(
      np/>(s\<np)[???]
      s\<np[??????]
    )
  )
)
(b) CDG derivation tree
Figure 3. An example of a derivation tree in 
treebank comparing between CG and CDG
A status  of  transformed  CDG  treebank  is 
30,340  text  lines  which  include  14,744  sen-
tences,  9,651  verb  phrases  or  subject-omitted 
sentences and 5,945 noun phrases. However, the 
average word amount of  this  treebank is  3.41 
words per tree which is obviously short.
Upon  an  attempt  to  increase  a  number  of 
trees, we considered that the existing trees are 
simple and not signify all utilisations of natural 
Thai  text.  Therefore,  news  domain  of  BEST 
(Kosawat et al, 2009) corpus was chosen to ful-
fil  such  issues  because  of  its  practical  usage. 
From our  observation,  we  found that  most  of 
data are  ranged from 25 to 68 words  and the 
longest  line  contains  415  words  which  is  ex-
tremely long for parser to handle it efficiently.
After a prior experiment, we found that  our 
GLR  parser  with  CDG  information  resulted 
514.62  tree  alternatives  in  average  from  the 
range  of  three  to  fifteen  words  per  sentence 
from  news  domain  in  BEST.  With  problems 
from ambiguous syntax of Thai, to automatic-
ally select  a correct  tree is extremely difficult 
since  several  resulted  trees  are  grammatically 
correct and semantically sound but they are not 
proper  for  their  context.  It  becomes  difficulty 
for linguists to select an appropriate one among 
them. In order to solve that problem, we imple-
mented a toolkit to assist linguists on construct-
ing treebank with such a long and complicated 
sentence.  The  manual  annotated  tree  will  be 
used as a gold standard and confidentially apply 
for statistical parser development.
4 CGs Tree Supported Tool
Building  a  resource  is  a  laboured  work  espe-
cially  a  treebank  construction.  For  Thai  lan-
guage which uses several function words to ex-
press grammatical function in context, an imme-
diate  constituency  analysis  and  a  dependency 
analysis become difficult since many word pair 
can  cause  ambiguity  and  complexity  among 
them. Additionally, a representation of a deriva-
tion tree in textual format is excessively com-
plex to be  analysed or approved.  To reduce a 
burden of linguists,  we developed a toolkit  to 
help a linguists with graphical user-interface in 
manual tree construction.
4.1 CGs Toolkit
The proposed toolkit supports multi-tasks which 
are annotating CG tag to a word, bracketing in-
termediate constituents, generating dependency-
driven derivation tree in multiple formats, and 
visualising graphical tree.
4.1.1 Category Annotator
Category annotator supports users to select  an 
appropriate CDG category for each word. The 
system  takes  word-segmented  input  text.  It 
starts with checking possible categories with the 
given CDG dictionary and lists  all  of them to 
each word. Users only select a correct category 
for each. Unless the word is known or the re-
quired category for the word is present, user has 
to add a new category for the word and the sys-
tem  contiguously  updates  the  dictionary  with 
the given data for further usage.
4.1.2 Dependency-driven  Derivation  Tree 
Generator
This system is implemented for manual annotat-
ing tree information and dependency relation to 
a text that is difficult for parser to generate tree 
such as a text with multiple serial verb construc-
tions, a complex head-dependent relation word 
pairs,  etc.  A captured picture  of  user-interface 
165
working  on  immediate  constituency  and  de-
pendency annotation is illustrated in Figure 4. 
We provide a user-interface for linguists and 
experts  to  easily  annotate  brackets  covering. 
Users  begin  a  process  by  selecting  a  pair  of 
words that are a terminal of leaf node. The sys-
tem  apparently  shows  only  categories  of  the 
word that can be possibly combined within the 
bracket for selecting. After choosing categories 
of those two constituents, the system automatic-
ally generates a result category. Next, users will 
continue the process for other constituents until 
one top result category is left.
After users finish the bracketing process, de-
pendency relation will be generated from annot-
ated  dependency  marker  within  categories 
without manual assignment.
4.1.3 Tree Visualiser
The  system  includes  a  function  to  create  a 
graphical tree from a file in textual formats. It 
provides a function to modify a tree by editing a 
word  spelling  and  its  syntactic  category  and 
shifting a branch of syntactic tree to another.
4.2 Tree Representation
The CGs  toolkit  allows users  to export  a  tree 
output in two representations; traditional textual 
tree format and XML format.
Throughout all tree format examples, we ex-
emplify a Thai sentence '? ??? ??????  ????  ?? 
????? ???????' (lit: an expert discovers corona vir-
us.) with the following categories:
Word CDG category
?????????? (expert)????? (virus)??????? (corona) ? np
???? (diagnose) ? s\<np
?? (discover) ? s\<np/>np
4.2.1 Traditional Textual Tree Format
A traditional textual tree format represents a ter-
minal (w) with its category (c) in form of c[w]. 
The brackets are enclosed two constituents split 
by  space  with  parentheses  and  the  result  cat-
egory (cr) is placed before the open parenthesis 
in format  cr(c[w] c[w]).  Figure 5 shows an ex-
ample of a traditional textual tree format.
s(np[??????????] s\<np(s\<np[????] 
s\<np(s\<np(s\<np/>np[??] np(np[?????] np[???????])))
Figure 5. An example of a traditional textual 
tree format of '?????????? ???? ?? ????? ???????'
4.2.2 XML Tree Format
For XML tree format, we design three tag sets, 
i.e., word tag,  tree tag and input tag. The word 
Figure 4. A snapshot of dependency-driven derivation tree generator
166
tag bounds a terminal to mark a word. In a start-
tag of  word tag, there are two attributes which 
are  cat to assign a category in a value and text 
to assign a given text in a value. For tree tag, it 
marks a combination of either word tags or tree 
tags to form another result category. It contains 
two previous attributes with an additional attrib-
ute, i.e., a head attribute to fill in a notation that 
which word has a head-outward relation value 
where '0' value indicates head from left constitu-
ent and '1' value indicates head from right con-
stituent. The input tag shows a boundary of all 
input and it has attributes to show line number, 
raw input text and status of tree building pro-
cess. Figure 6 illustrates an XML tree represent-
ation.
5 Thai CGs Related Applications
Several applications related to Thai CGs or used 
Thai  CGs  as  their  syntactic  information  have 
been  implemented  recently.  Below  is  a  sum-
mary of their methodology and result.
5.1 CG AutoTagger for Thai
To reduce an amount of trees generated from a 
parser with all possible categories, an automatic 
syntactic category tagger (Supnithi et al, 2010) 
was developed to disambiguate  unappropriated 
combinations of impossible categories. The sys-
tem was developed based on CRF and Statistic-
al Alignment Model based on information the-
ory (SAM) algorithm. The accuracy 89.25% in 
word level was acquired. This system also has a 
function to predict  a  syntactic category for an 
unknown word and 79.67% of unknown word 
are predicted correctly.
5.2 Chunker
With  a  problem  of  a  long  sentence  in  Thai, 
chunker  was  implemented  to  group  a  con-
sequent of words to larger unit in order to re-
duce  a  difficulty  on  parsing  too  many  lexical 
elements. CRD method with syntactic informa-
tion from CG and categorial set was applied in 
the  system to  chunk a  text  into  noun phrase, 
verb phrase, prepositional phrase, and adverbial 
phrase.  Moreover,  the system also  attempts to 
handle a compound word that has a form like 
sentence.  The  result  was  impressive  as  it  im-
proved  74.17% of  accuracy  on  sentence  level 
chunking  and  58.65%  on  sentence-form  like 
compound noun.
5.3 GLR parser for Thai CG and CDG
Our implemented LALR parser (Aho and John-
son,  1974)  was  improved  to  GLR  parser  for 
syntactically  parse  Thai  text.  This  parser  was 
developed to return all possible trees form input 
to show a baseline that covers all syntactic pos-
sibilities. For our GLR parser, a grammar rule is 
not manually determined, but it is automatically 
produced  by  any  given  syntactic  notations 
aligned  with  lexicons  in  a  dictionary.  Hence, 
this  GLR parser has a  coverage including CG 
and CDG formalism parsing. Furthermore, our 
GLR parser accepts a sentence, a noun phrase, a 
verb phrase and prepositional phrase. However, 
the parser does not only return the best first tree, 
but also all  parsable trees to gather all ambigu-
ous trees since Thai language tends to be  am-
biguous because  of  lacking  explicit  sentence, 
phrase and word boundary. This parser includes 
a  pre-process to handle named-entities,  numer-
ical expression and time expression.
Figure 6. An example of XML tree format of '?????????? ???? ?? ????? ???????'
167
6 Conclusion and Future Work
In this paper, we update our Thai CG informa-
tion and a status of its resources. We also pro-
pose CDG for Thai, an extended version of CG. 
CDG offers  an  efficient  way  to  represent  de-
pendency structures with syntactic derivations. 
It benefits linguists in terms of they can restrain 
Thai  grammar in lexical  level.  With CDG de-
pendency-driven  derivation  tree,  both bracket-
ing information and dependency relation are an-
notated  to  every  lexical  units.  In  the  current 
state, we transformed our CG dictionary and CG 
treebank into CDG formalism. 
With an attempt to increase an amount of our 
treebank with a complex text, CDG tree toolkit 
was developed for linguists to manual managing 
a derivation tree. This toolkit  includes a CDG 
category tagger tool, dependency-driven deriva-
tion  tree  generator,  and  tree  visualiser.  This 
toolkit  can generate  an  output  in  two formats 
which are traditional textual tree and XML tree. 
The XML tree format is an option for standard-
ised format  or  further  usage such as  applying 
tree for ontology.
We also summarised CGs related works and 
their accuracy. They included an automatic CG 
tagger and a Thai phrase chunker.
In the future, we plan to increase an amount 
of  CGs  derivation  trees  of  complex  sentence 
and practical language. Moreover, we will im-
plement a system to transform an existing Thai 
dependency bank to CDG format to gain more 
number  of  trees.  We  also  plan  to  include  se-
mantic meaning into derivation tree and repres-
ent  such trees  in an RDF format.  In addition, 
statistical parser will be implemented based on 
the CDG derivation trees.
References
Ajdukiewicz  Kazimierz.  1935.  Die  Syntaktische 
Konnexitat, Polish Logic.
Aho Alfred, and Johnson Stephen. 1974. LR Parsing, 
Proceedings of Computing Surveys, Vol. 6, No. 2.
Bar-Hillel  Yehoshua.  1953.  A  quasi-arithmetical 
notation for syntactic description. 29(1): 47-58.
Carpenter Bob. 1992. Categorial Grammars, Lexical 
Rules,and the English Predicative, In R. Levine, 
ed.,  Formal  Grammar:  Theory  and  Implementa-
tion. OUP.
Collins Micheal. 1999. Head-Driven Statistical Mod-
els for  Natural  Language Parsing. Ph.D. Thesis, 
University of Pennsylvania.
Koller Alexander, and Kuhlmann Marco. 2009. De-
pendency trees and the strong generative capacity 
of ccg, Proceedings of the 12th Conference of the 
European Chapter of the Association for Compu-
tational Linguistics: 460-468.
Kosawat  Krit,  Boriboon  Monthika,  Chootrakool 
Patcharika,  Chotimongkol  Ananlada,  Klaithin 
Supon, Kongyoung Sarawoot,  Kriengket  Kanya-
nut,  Phaholphinyo  Sitthaa,  Purodakananda 
Sumonmas,  Thanakulwarapas  Tipraporn,  and 
Wutiwiwatchai  Chai.  2009.  BEST  2009:  Thai 
Word Segmentation Software Contest. The 8th In-
ternational Symposium on Natural Language Pro-
cessing: 83-88.
Porkaew Peerachet, Ruangrajitpakorn Taneth, Trak-
ultaweekoon Kanokorn,  and  Supnithi  Thepchai.. 
2009. Translation of Noun Phrase from English to 
Thai using Phrase-based SMT with CCG Reorder-
ing Rules, Proceedings of the 11th conference of 
the  Pacific  Association  for  Computational  Lin-
guistics (PACLING).
Porkaew  Peerachet,  and  Supnithi  Thepchai.  2009. 
Factored  Translation  Model  in  English-to-Thai 
Translation, Proceedings of  the 8th International 
Symposium on Natural Language Processing. 
Ruangrajitpakorn Taneth, Na Chai Wasan , Boonk-
wan Prachya, Boriboon Monthika, Supnithi Thep-
chai. 2007. The Design of Lexical Information for 
Thai to English MT, Proceedings of the 7th Inter-
national  Symposium  on  Natural  Language  Pro-
cessing.
Ruangrajitpakorn  Taneth,  Trakultaweekoon  Kan-
okorn, and Supnithi Thepchai. 2009. A Syntactic 
Resource for Thai: CG Treebank, Proceedings of 
the 7th Workshop on Asian Language Resources, 
(ACL-IJCNLP): 96?102.
Satayamas Vee, and Kawtrakul Asanee . 2004. Wide-
Coverage  Grammar  Extraction  from Thai  Tree-
bank. Proceedings of Papillon 2004 Workshops on 
Multilingual Lexical Databases, Grenoble, France.
Supnithi  Thepchai,  Ruangrajitpakorn  Taneth,  Trak-
ultaweekoon Kanokorn,  and Porkaew Peerachet. 
2010. AutoTagTCG : A Framework for Automatic 
Thai CG Tagging, Proceedings of the 7th interna-
tional  conference  on  Language  Resources  and 
Evaluation (LREC).
168
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 94?101,
Dublin, Ireland, August 23-29 2014.
Character-Cluster-Based Segmentation using Monolingual and Bilingual Information for Statistical Machine Translation 
Vipas Sutantayawalee  Peerachet Porkeaw  Thepchai Supnithi Prachya Boonkwan  Sitthaa Phaholphinyo  National Electronics and Computer Technology Center, Thailand  {vipas.sutantayawalee, peerachet.porkeaw,prachya.boonkwan, sitthaa.phaholphinyo,thepchai}@nectec.or.th   Abstract We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing character co-occurrence statistics and orthographic insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster alignment, co-occurrence frequency and alignment confidence into that result. We evaluate the effectiveness of our method on PB-SMT task using English-Thai language pair and report the best improvement of 8.1% increase in BLEU score. There are two main advantages of our approach. First, our method requires less effort on developing the corpus and can be applied to unsegmented corpus or poor-quality manually segmented corpus. Second, this technique does not only limited to specific language pair but also capable of automatically adjust the character cluster boundaries to be suitable for other language pairs.  1 Introduction Nowadays, it is admitted that word segmentation is a crucial part of Statistical Machine Translation (SMT) especially in the languages where there are no explicit word boundaries such as Chinese, Japanese or Thai. The writing system of these languages allow each word can be written continuously with no space appearing between words. Consequently, word ambiguities will arise if word boundary has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator is required to disambiguate each word separator before processing another task in SMT. Several word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal.  In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches make use of either manually segmented [4]  or unsegment1ed bilingual corpus [5] as a guideline information to perform a word segmentation task and improve the performance of SMT system. 
                                                            This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
94
In this paper, we propose a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information on English-Thai language pair and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result to manually segmented corpus in PB-SMT task when the good heuristics character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing heuristic algorithm and language insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster (CC) alignment, CC co-occurrence frequency and alignment confidence into that result. These two tasks can be performed repeatedly. The remainder of this paper is organized as follows. Section 2 provides some information related to our work. Section 3 describes the methodology of our approach. Section 4 present the experiments setting. Section 5 present the experimental results and empirical analysis. Section 6 and 7 gives a conclusion and future work respectively.  2 Related Work 2.1 Thai Character Clustering  In Thai writing system, there are no explicit word boundaries as in English, and a single Thai character does not have specific meanings like Chinese, Japanese and Korean. Thai characters could be consonants, vowels and tone marks and a word can be formed by combining these characters. From our observation, we found that the average length of Thai words on BEST2010 corpus (National Electronics and Computer Technology Center, Thailand 2010) is 3.855. This makes the search space of Thai word segmentation very large. To alleviate this issue, the notion of Thai character cluster (TCC), is introduced [1] to reduce the search space with predetermined unambiguious constraints for cluster formation. A cluster may not be meaningful and has to combine with other consecutive clusters to form a word. Characters in the cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result.  2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word segmentation. Several methods have used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between English and Chinese language pair. On the other hands, [4] is based on the manually segmented bilingual corpus and then try to ?repack? the word from existing alignment by using alignment confidence. Both works evaluated the performance in BLEU metric and reported the promising result of PB-SMT task.    3 Methodology This paper aim to compare translation quality based on SMT task between the systems trained on bilingual corpus that contains both segmented source and target, and on the same bilingual corpus with segmented source but unsegmented target. First, we make use of monolingual information by employing several character cluster algorithms on unsegmented data. Second, we use bilingual-guided alignment information retrieved from alignment extraction process for improving character cluster segmentation. Then, we evaluate our performance based on translation accuracy by using BLEU metric. We want to prove that (1) the result of PB-SMT task using unsegmented corpus (unsupervised) 
95
is nearly identical result to manually segmented (supervised) data and (2) when bilingual information are also applied, the performance of PB-SMT is also improved. 3.1 Notation Given a target ????  ? sentence ??? consisting of ? clusters ? ??,? , ?? , where ?? ? 1. If ?? = 1, we call ??as a single character ??. Otherwise, we call is as a character cluster ? . In addition, given a English sentence ???  consisting of ? words ? ?,? , ?? , ????? denotes a set of English-to-Thai language word alignments between ???  and ????. In addition, since we concentrate on one-to-many alignments, ????  , can be rewritten as a set of pairs ?? and ?? = ?< ?? , ?? > noting a link between one single English word and several Thai characters that are formed to one cluster ? 	 3.2 Monolingual Information Due to the issue mentioned in section 2.1, we apply character clustering (CC) technique on target text in order to reduce the search space. After performing CC, it will yield several character clusters ?which can be grouped together to obtain a larger unit which approaches the notion of word. However, for Thai and Lao, we do not only receive ? but also ? which usually has no meaning by itself. Moreover, Thai, Burmese and Lao writing rule does not allow ? to stand alone in most case. Thus, we are required to develop various adapted versions of CC by using orthographic insight and heuristic algorithm to automatically pack the characters that reside in a pre-defined grammatical word list handcrafted by linguists. Then, all of single consonants in Thai Burmese, and Lao are forced to group with either left or right cluster due to the Thai writing system. The decision has been made by consulting on character co-occurrence statistics (unigram and bigram frequency). Eventually, we obtain different character cluster alignments from the system trained on various CC approaches which effect to translation quality as shown in section 5.1 3.3 Bilingually-Guided Alignment Information We begin with the sequence of small clusters resulting from previous character clustering process.  These small clusters can be grouped together in order to form ?word? using bilingually-guided alignment information. Generally, small consecutive clusters in target side which are aligned to the same word in source data should be grouped together. Therefore, this section describes our one-to-many alignment extraction process.   For one-to-many alignment, we applied processes similar to those in phrase extraction algorithm [7] which is described as follows.  With English sentence ???  and a Thai character cluster ???, we apply IBM model 1-5 to extract word-to-cluster translation probability of source-to-target ?(?|?) and target-to-source ??(?|?). Next, the alignment points which have the highest probability are greedily selected from both ?(?|?) and ?(?|?). Figure 1.a and 1.b show examples of alignment points of source-to-target and target-to-source respectively. After that we selected the intersection of alignment pairs from both side. Then, additional alignment points are added according to the growing heuristic algorithm (grow additional alignment points, [8])  
 (a)   (b)  
96
 (c)   (d)   Figure 1. The process of one-to-many alignment extraction (a) Source-to-Target word alignment (b) Target-to-Source word alignment (c) Intersection between (a) and (b).  (d) Result of (c) after applying the growing heuristic algorithm.   Finally, we select consecutive clusters which are aligned to the same English word as candidates. From the Figure 1.d, we obtain these candidates (red, ?????) and (bicycle, ??? ? ?? ?). 3.4 Character Cluster Repacking  Although the alignment information obtained from the previous step is very helpful for the PB-SMT task, there is still plenty of room to enhance the PB-SMT performance. One way of doing that is by using word repacking [4]. However, in this paper, we perform a character cluster repacking (CCR) instead of word. The main purpose of repacking technique is to group all small consecutive clusters (or word) in target side that frequently align with one word in source data. Repacking approaches uses two simple calculations which are a co-occurrence frequency (???? ?(?? , ??)) and alignment confidence (??( ???)). (???? ?(?? , ??)) is the number of times ?? and ??? co-occurr in the bilingual corpus [4] [9] and ??( ???) is a measure of how often the aligner aligns ?? and ???  when they co-occur. AC is defined as  ??(??)  ?=  ? ?(??)???? ?(?? , ??) ?  where ?(??) denotes the number of alignments suggested by the previous-step word aligner.  Unfortunately, due to the limited memory in our experiment machine, we cannot find ????? ?(?? , ??)) for all possible < ?? , ?? > ?pairs. We, therefore, slightly modified the above equation by finding ?(??) first. Secondly, we begin searching ????? ?(?? , ??)) from all possible alignments in ??? instead of finding all occurrences in corpus. By applying this modification, we eliminate < ?? , ?? > ?pairs that co-occur together but never align to each other by previous-step aligner (?? ??  ?equals to zero) so as to reduce the search space and complexity in our algorithm. Thirdly, we choose ?? with the highest ??(??) and repack all character clusters in target side that similar to ?? to be a new single cluster unit. This process can be done repeatedly. However, we have run this task less than twice since there are few new cluster unit appear after two iterations have passed. The running example of this algorithm is described as follows  Suppose previous step aligner (GIZA++) produce two alignments ??? = ?< ??, ??,? > and ?? = ?<??, ??,?,? > ?CCR will find the frequency of each aligment and number of times ?? and ??? co-occurr in the bilingual corpus ( ???? ? ??, ??,?  and  ???? ? ??, ??,?,?  ?). Then, we will have ??(??) ? score for each alignment and the aligment with the highest ?? will  be selected. The CCR will group these cluster ( e.g. ??,? ) to be a new single cluster unit.       
97
4 Experimental Setting 4.1 Data The bilingual corpus1 we used in our experiment is constructed from several sources and consists of multiple domains (e.g news, travel, article, entertainment, computer, etc.). We divided this corpus into three sets plus one additional test set as shown below            Table 1. Information of bilingual corpus  4.2 Tools and Evaluation We evaluate our system in term of translation quality based on phrase-based SMT.  Source sentences are sequence of English words while target sentences are sequences of Thai character clusters and each cluster size depends on which approach used in the experiment.   Translation model and language model are train based on the standard phrase-based SMT. Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++ [8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data.  Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score [11]. Therefore, we re-segment the reference data (manually segmented) and the translation result data based on TCC. Some may concern about using TCC will lead to over estimation (higher than actual) due to the BLEU score is design based on word and not based on character. However, we used this BLEU score only for comparing translation quality among our experiments. Comparing to other SMT system still require running BLEU score based on the same segmentation guideline. 5 Results and Discussion We conducted all experiments on PB-SMT task and reported the performance of PB-SMT system based on the BLEU measure. First, we use a method proposed in section 3.2 followed by the approach in section 3.3 in order to the receive first translation result set (without CCR). Then, we perform a method describe in 3.4 and also follow by approach in section 3.3 in order to receive another translation result set (with CCR). Table 1 shows the number of character clusters that are decreasing over time when several different character clustering approaches are applied.     
                                                            1 Currently, the corpus we used is a proprietary of NECTEC and does not available to public yet due to the licensing issue. However, for the educational purpose, this corpus is available upon by request. 
 Data Set No. of sentence pairs Train  633,589 Dev 12,568 Test #1 3,426 Test #2 500 
98
  Table 2. Number of character clusters when different character clustering approaches are applied on the bilingual corpus	  Next, we present all translation results of PB-SMT task that using different character clustering approaches. Each training set is trained with only one character clustering method which are (1) TCC (baseline), (2) TCC with CCR, (3) TCC with only orthographic insight (TCC-FN), (4) TCC-Fn with CCR, (5) TCC with language insight and heuristic algorithm (TCC-FN-B) and (6) TCC-FN-B with CCR. The results are shown in Table 3.   
 No. of Character Clusters (or word in original data) Approaches Without CCR With CCR TCC (baseline) 9,862,271 7,187,862 TCC with language insight (TCC-FN) 8,953,437 6,636,305 TCC with language insight and heuristic algorithm (TCC-FN-B) 6,545,617 5,448,437 Manually segmented corpus (Upper bound) 5,311,648 N/A 
 Table 3. BLEU score of each character clustering method  and the percentage of the improvement when we applied CCR to the data   
 Test #1 BLEU % of BLEU Improvement Test #2 BLEU % of BLEU Improvement Approaches Without CCR With CCR Without CCR With CCR Baseline 37.12 40.13 8.11 36.78 38.87 5.68 TCC-FN 40.23 41.90 4.15 38.36 39.09 1.90 TCC-FN-B 44.69 44.43 -0.58 40.45 40.81 0.89 Upper bound 47.04 N/A N/A 40.73 N/A N/A 
 (a)  
35	 ?
36	 ?
37	 ?
38	 ?
39	 ?
40	 ?
41	 ?
42	 ?
43	 ?
44	 ?
45	 ?
46	 ?
47	 ?
48	 ?
Upperbound	 ?
No	 ?CCR	 ?
With	 ?CCR	 ?
Upperbound	 ?
99
 (b)   Figure 2. The BLEU score of (a) test set no.1 and (b) test set no.2   As seen from Table 3, when we apply the enhanced version of TCCs into the data with no CCR, BLEU score have gradually increased and almost reached the same level as original in test set #2. Furthermore, when CCR have been also deployed on each training dataset, the results of BLEU are also rise in the same manner with Without CCR method. There are certain significant points that should be noticed. First, CCR method is able to yield maximum of 8.1 % BLEU score increase. Second, when we apply the CCR methods and reach at some point, few improvement or minor degradation is received as shown in TCC-FN-B without and with CCR result. This is because the number of clusters produced by this character clustering algorithm is almost equal to number of words in original data as shown in Table 2 and this approach might suffer from the word boundary misplacement problem. Third, character clustering that use TCC with orthographic insight and heuristic algorithm combined with CCR approach is able to overcome the translation result from original data for the first time.   6 Conclusion In this paper, we introduce a new approach for performing word segmentation task for SMT. Instead of starting with word level, we focus on character cluster level because this approach can perform on unsegmented corpus or multiple-guideline manually segmented corpus. First, we apply several adapted versions of TCC on unsegmented data. Next, we use a bilingual corpus to find alignment information for all < ?? , ?? > ?  pairs and then employ character cluster repacking method in order to form the large cluster of Thai characters.   We evaluate our approach on translation task on several sources and different domain corpus and report the result in BLEU metric. Our technique demonstrates that (1) we can achieve a dramatically improvement of BLUE as of 8.1% when we apply adapted TCC with CCR and (2) it is possible to overcome the manually segmented corpus by using TCC with orthographic insight and heuristic algorithm character clustering method combined with CCR. The advantage of our approach is a reduction in time and effot for construct a billinugal corpus because we are no longer required to manually segment all sentences in target side. In addition, our approach is able to cope with larger data information (e.g. 1 million sentences pairs) and adaptable to other language pairs (e.g. English-Chinese, English-Japanese or English-Lao) 	 
35	 ?
36	 ?
37	 ?
38	 ?
39	 ?
40	 ?
41	 ?
42	 ?
Baseline	 ? TCCAFn	 ? TCCAFnB	 ?
No	 ?CCR	 ?
With	 ?CCR	 ?
Upperbound	 ?
100
7 Future Work There are some tasks that can be added into this approaches. Firstly, we can make use of trigram (and n-gram) statistics, maximum entropy or conditional random field on heuristic algorithm in adapted version of TCC. Secondly, we might report the result from another language pair in order to confirm our approach.Thirdly, we can modify CCR process to be able to rerank the alignment confidence by using discriminative approach. Lastly, name entity recognition system can be integrated with our approach in order to improve the SMT performance.  Reference 	 ?[1]  T. Teeramunkong, V. Sornlertlamvanich, T. Tanhermhong and W. Chinnan, ?Character cluster based Thai information retrieval,? in IRAL '00 Proceedings of the fifth international workshop on on Information retrieval with Asian languages, 2000.  [2]  C. Kruengkrai, K. Uchimoto, J. Kazama, K. Torisawa, H. Isahara and C. Jaruskulchai, ?A Word and Character-Cluster Hybrid Model for Thai Word Segmentation,? in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.  [3]  Y. Liu, W. Che and T. Liu, ?Enhancing Chinese Word Segmentation with Character Clustering,? in Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, China, 2013.  [4]  Y. Ma and A. Way, ?Bilingually motivated domain-adapted word segmentation for statistical machine translation,? in Proceeding EACL '09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pp. 549-557, Stroudsburg, PA, USA, 2009.  [5]  J. Xu, R. Zens and H. Ney, ?Do We Need Chinese Word Segmentation for Statistical Machine Translation?,? ACL SIGHAN Workshop 2004, pp. 122-129, 2004.  [6]  P. Limcharoen, C. Nattee and T. Theeramunkong, ?Thai Word Segmentation based-on GLR Parsing Technique and Word N-gram Model,? in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.  [7]  P. Koehn, F. J. Och and D. Marcu, ?Statistical phrase-based translation,? in NAACL '03 Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Stroudsburg, PA, USA, 2003.  [8]  F. J. Och and H. Ney, ?A systematic comparison of various statistical alignment models,? Computational Linguistics, vol. 29, no. 1, pp. 19-51, 2003.  [9]  I. D. Melamed, ?Models of translational equivalence among words,? Computational Linguistics, vol. 26, no. 2, pp. 221-249, 2000.  [10]  ?SRILM -- An extensible language modeling toolkit,? in Proceeding of the International Conference on Spoken Language Processing, 2002.  [11]  P.-C. Chang, M. Galley and C. D. Manning, ?Optimizing Chinese word segmentation for machine translation performance,? in Proceedings of the Third Workshop on Statistical Machine Translation, Columbus, Ohio, 2008.     
101
