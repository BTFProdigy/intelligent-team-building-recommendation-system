Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 636?646,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Phrasal Verbs Using Many Bilingual Corpora
Karl Pichotta?
Department of Computer Science
University of Texas at Austin
pichotta@cs.utexas.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
We address the problem of identifying mul-
tiword expressions in a language, focus-
ing on English phrasal verbs. Our poly-
glot ranking approach integrates frequency
statistics from translated corpora in 50 dif-
ferent languages. Our experimental eval-
uation demonstrates that combining statisti-
cal evidence from many parallel corpora us-
ing a novel ranking-oriented boosting algo-
rithm produces a comprehensive set of English
phrasal verbs, achieving performance compa-
rable to a human-curated set.
1 Introduction
A multiword expression (MWE), or noncomposi-
tional compound, is a sequence of words whose
meaning cannot be composed directly from the
meanings of its constituent words. These idiosyn-
cratic phrases are prevalent in the lexicon of a lan-
guage; Jackendoff (1993) estimates that their num-
ber is on the same order of magnitude as that of sin-
gle words, and Sag et al (2002) suggest that they
are much more common, though quantifying them
is challenging (Church, 2011). The task of identify-
ing MWEs is relevant not only to lexical semantics
applications, but also machine translation (Koehn et
al., 2003; Ren et al, 2009; Pal et al, 2010), informa-
tion retrieval (Xu et al, 2010; Acosta et al, 2011),
and syntactic parsing (Sag et al, 2002). Awareness
of MWEs has empirically proven useful in a num-
ber of domains: Finlayson and Kulkarni (2011), for
example, use MWEs to attain a significant perfor-
mance improvement in word sense disambiguation;
Venkatapathy and Joshi (2006) use features associ-
ated with MWEs to improve word alignment.
?Research conducted during an internship at Google.
We focus on a particular subset of MWEs, English
phrasal verbs. A phrasal verb consists of a head
verb followed by one or more particles, such that
the meaning of the phrase cannot be determined by
combining the simplex meanings of its constituent
words (Baldwin and Villavicencio, 2002; Dixon,
1982; Bannard et al, 2003).1 Examples of phrasal
verbs include count on [rely], look after [tend], or
take off [remove], the meanings of which do not in-
volve counting, looking, or taking. In contrast, there
are verbs followed by particles that are not phrasal
verbs, because their meaning is compositional, such
as walk towards, sit behind, or paint on.
We identify phrasal verbs by using frequency
statistics calculated from parallel corpora, consist-
ing of bilingual pairs of documents such that one
is a translation of the other, with one document in
English. We leverage the observation that a verb
will translate in an atypical way when occurring as
the head of a phrasal verb. For example, the word
look in the context of look after will tend to trans-
late differently from how look translates generally.
In order to characterize this difference, we calculate
a frequency distribution over translations of look,
then compare it to the distribution of translations of
look when followed by the word after. We expect
that idiomatic phrasal verbs will tend to have unex-
pected translation of their head verbs, measured by
the Kullback-Leibler divergence between those dis-
tributions.
Our polyglot ranking approach is motivated by the
hypothesis that using many parallel corpora of dif-
ferent languages will help determine the degree of
semantic idiomaticity of a phrase. In order to com-
1Nomenclature varies: the term verb-particle construction
is also used to denote what we call phrasal verbs; further, the
term phrasal verb is sometimes used to denote a broader class
of constructions.
636
bine evidence from multiple languages, we develop
a novel boosting algorithm tailored to the task of
ranking multiword expressions by their degree of id-
iomaticity. We train and evaluate on disjoint subsets
of the phrasal verbs in English Wiktionary2. In our
experiments, the set of phrasal verbs identified au-
tomatically by our method achieves held-out recall
that nears the performance of the phrasal verbs in
WordNet 3.0, a human-curated set. Our approach
strongly outperforms a monolingual system, and
continues to improve when incrementally adding
translation statistics for 50 different languages.
2 Identifying Phrasal Verbs
The task of identifying phrasal verbs using corpus
information raises several issues of experimental de-
sign. We consider four central issues below in moti-
vating our approach.
Types vs. Tokens. When a phrase is used in con-
text, it takes a particular meaning among its pos-
sible senses. Many phrasal verbs admit composi-
tional senses in addition to idiomatic ones?contrast
idiomatic ?look down on him for his politics? with
compositional ?look down on him from the balcony.?
In this paper, we focus on the task of determining
whether a phrase type is a phrasal verb, meaning that
it frequently expresses an idiomatic meaning across
its many token usages in a corpus. We do not at-
tempt to distinguish which individual phrase tokens
in the corpus have idiomatic senses.
Ranking vs. Classification. Identifying phrasal
verbs involves relative, rather than categorical, judg-
ments: some phrasal verbs are more compositional
than others, but retain a degree of noncomposition-
ality (McCarthy et al, 2003). Moreover, a poly-
semous phrasal verb may express an idiosyncratic
sense more or less often than a compositional sense
in a particular corpus. Therefore, we should expect
a corpus-driven system not to classify phrases as
strictly idiomatic or compositional, but instead as-
sign a ranking or relative scoring to a set of candi-
dates.
Candidate Phrases. We distinguish between the
task of identifying candidate multiword expressions
2http://en.wiktionary.org
Feature Description
?L (?50) KL Divergence for each language L
?1 frequency of phrase given verb
?2 PMI of verb and particles
?3 ?1 with interposed pronouns
Table 1: Features used by the polyglot ranking system.
and the task of ranking those candidates by their se-
mantic idiosyncracy. With English phrasal verbs, it
is straightforward to enumerate all desired verbs fol-
lowed by one or more particles, and rank the entire
set.
Using Parallel Corpora. There have been a num-
ber of approaches proposed for the use of multilin-
gual resources for MWE identification (Melamed,
1997; Villada Moiro?n and Tiedemann, 2006; Caseli
et al, 2010; Tsvetkov and Wintner, 2012; Salehi
and Cook, 2013). Our approach differs from pre-
vious work in that we identify MWEs using transla-
tion distributions of verbs, as opposed to 1?1, 1?m,
or m?n word alignments, most-likely translations,
bilingual dictionaries, or distributional entropy. To
the best of our knowledge, ours is the first approach
to use translational distributions to leverage the ob-
servation that a verb typically translates differently
when it heads a phrasal verb.
3 The Polyglot Ranking Approach
Our approach uses bilingual and monolingual statis-
tics as features, computed over unlabeled corpora.
Each statistic characterizes the degree of idiosyn-
crasy of a candidate phrasal verb, using a single
monolingual or bilingual corpus. We combine fea-
tures for many language pairs using a boosting algo-
rithm that optimizes a ranking objective using a su-
pervised training set of English phrasal verbs. Each
of these aspects of our approach is described in de-
tail below; for reference, Table 1 provides a list of
the features used.
3.1 Bilingual Statistics
One of the intuitive properties of an MWE is that
its individual words likely do not translate literally
when the whole expression is translated into another
language (Melamed, 1997). We capture this effect
637
by measuring the divergence between how a verb
translates generally and how it translates when head-
ing a candidate phrasal verb.
A parallel corpus is a collection of document
pairs ?DE , DF ?, where DE is in English, DF is in
another language, one document is a translation of
the other, and all documents DF are in the same
language. A phrase-aligned parallel corpus aligns
those documents at a sentence, phrase, and word
level. A phrase e aligns to another phrase f if some
word in e aligns to some word in f and no word in
e or f aligns outside of f or e, respectively. As a
result of this definition, the words within an aligned
phrase pair are themselves connected by word-level
alignments.
Given an English phrase e, define F (e) to be the
set of all foreign phrases observed aligned to e in a
parallel corpus. For any f ? F (e), let P (f |e) be the
conditional probability of the phrase e translating to
the phrase f . This probability is estimated as the
relative frequency of observing f and e as an aligned
phrase pair, conditioned on observing e aligned to
any phrase in the corpus:
P (f |e) = N(e, f)?
f ? N(e, f ?)
with N(e, f) the number of times e and f are ob-
served occurring as an aligned phrase pair.
Next, we assign statistics to individual verbs
within phrases. The first word of a candidate phrasal
verb e is a verb. For a candidate phrasal verb e and
a foreign phrase f , let pi1(e, f) be the subphrase of
f that is most commonly word-aligned to the first
word of e. As an example, consider the phrase pair
e = talk down to and f = hablar con menosprecio.
Suppose that when e is aligned to f , the word talk is
most frequently aligned to hablar. Then pi1(e, f) =
hablar.
For a phrase e and its set F (e) of aligned trans-
lations, we define the constituent translation proba-
bility of a foreign subphrase x as:
Pe(x) =
?
f?F (e)
P (f |e) ? ? (pi1(e, f), x) (1)
where ? is the Kronecker delta function, taking value
1 if its arguments are equal and 0 otherwise. Intu-
itively, Pe assigns the probability mass for every f
to its subphrase most commonly aligned to the verb
in e. It expresses how this verb is translated in the
context of a phrasal verb construction.3 Equation (1)
defines a distribution over all phrases x of a foreign
language.
We also assign statistics to verbs as they are trans-
lated outside of the context of a phrase. Let v(e)
be the verb of a phrasal verb candidate e, which
is always its first word. For a single-word verb
phrase v(e), we can compute the constituent transla-
tion probability Pv(e)(x), again using Equation (1).
The difference between Pe(x) and Pv(e)(x) is that
the latter sums over all translations of the verb v(e),
regardless of whether it appears in the context of e:
Pv(e)(x) =
?
f?F (v(e))
P (f |v(e)) ? ? (pi1(v(e), f), x)
For a one-word phrase such as v(e), pi1(v(e), f)
is the subphrase of f that most commonly directly
word-aligns to the one word of v(e).
Finally, for a phrase e and its verb v(e), we calcu-
late the Kullback-Leibler (KL) divergence between
the translation distribution of v(e) and e:
DKL
(
Pv(e)?Pe
)
=
?
x
Pv(e)(x) ln
Pv(e)(x)
Pe(x)
(2)
where the sum ranges over all x such that Pv(e)(x) >
0. This quantifies the difference between the trans-
lations of e?s verb when it occurs in e, and when it
occurs in general. Figure 1 illustrates this computa-
tion on a toy corpus.
Smoothing. Equation (2) is defined only if, for ev-
ery x such that Pv(e)(x) > 0, it is also the case
that Pe(x) > 0. In order to ensure that this con-
dition holds, we smooth the translation distributions
toward uniform. Let D be the set of phrases with
non-zero probability under either distribution:
D = {x : Pv(e)(x) > 0 or Pe(x) > 0}
Then, let UD be the uniform distribution over D:
UD(x) =
{
1/|D| if x ? D
0 if x /? D
3To extend this statistic to other types of multiword expres-
sions, one could compute a similar distribution for other content
words in the phrase.
638
0%
10%
20%
30%
40%
50%
0 5 10 15 20 25 30 35 40 45 50
T
e
s
t
 
s
e
t
 
r
e
c
a
l
l
-
a
t
-
1
2
2
0
Number of languages (k)
Combined with AdaBoost
Individual Bilingual Statistics
looking forward to
mirando adelante a
looking forward to
deseando
looking
mirando
looking
buscando  a
3
1
5
3
Aligned Phrase Pair
N(e, f) ?1(e, f)
mirando
deseando
mirando
buscando
\begin{tabular}{rrrr}
         &\textit{mirando}   &\textit{deseando} &\textit{buscando} \\ [2ex]
$P_{v(e)}(x)$ &$\frac{5}{8}=0.625 $  &$0$       &$\frac{3}{8}=0.375 $ \\ [1ex]
\hline \\ [-1ex]
$P'_{v(e)}(x)$&$0.610     $         &$0.02$     &$0.373$ \\ [1ex]
\hline \\ [-1ex]
$P_e(x)$ &$\frac{3}{4}=0.75 $  &$\frac{1}{4}=0.25 $ &$0$ \\ [1.5ex]
\hline \\ [-1ex]
$P'_e(x)$&$0.729     $         &$0.254    $     &$0.02$ \\ [1ex]
\hline \\ [-1ex]
\end{tabular}
DKL(P 0v(ei)kP 0ei) =  0.109 + 0.045 + 1.159 = 1.005
D_{KL} (P'_{v(e_i)} \| P'_{e_i}) = -0.109 + -0.045 + 1.159 = 1.005
mirando deseando buscando
Pv(e)(x) 58 = 0.625 0 38 = 0.375
P 0v(e)(x) 0.610 0.02 0.373
Pe(x) 34 = 0.75 14 = 0.25 0
P 0e(x) 0.729 0.254 0.02
Figure 1: The computation of DKL(P ?v(ei)?P
?
ei) using a
toy corpus, for e = looking forward to. Note that the sec-
ond aligned phrase pair contains the third, so the second?s
count of 3 must be included in the third?s count of 5.
When computing divergence in Equation (2), we use
the smoothed distributions P ?e and P ?v(e):
P ?e(x) = ?Pe(x) + (1? ?)UD(x)
P ?v(e)(x) = ?Pv(e)(x) + (1? ?)UD(x).
We use ? = 0.95, which distributes 5% of the total
probability mass evenly among all events in D.
Morphology. We calculate statistics for morpho-
logical variants of an English phrase. For a candi-
date English phrasal verb e (for example, look up),
letE denote the set of inflections of that phrasal verb
(for look up, this will be [look|looks|looked|looking]
up). We extract the variants in E from the verb en-
tries in English Wiktionary. The final score com-
puted from a phrase-aligned parallel corpus translat-
ing English sentences into a language L is the aver-
age KL divergence of smoothed constituent transla-
tion distributions for any inflected form ei ? E:
?L(e) =
1
|E|
?
ei?E
DKL
(
P ?v(ei)?P
?
ei
)
3.2 Monolingual Statistics
We also collect a number of monolingual statistics
for each phrasal verb candidate, motivated by the
considerable body of previous work on the topic
(Church and Hanks, 1990; Lin, 1999; McCarthy et
al., 2003). The monolingual statistics are designed
to identify frequent collocations in a language. This
set of monolingual features is not comprehensive, as
we focus our attention primarily on bilingual fea-
tures in this paper.
As above, define E to be the set of morpholog-
ically inflected variants of a candidate e, and let
V be the set of inflected variants of the head verb
v(e) of e. We define three statistics calculated from
the phrase counts of a monolingual English corpus.
First, we define ?1(e) to be the relative frequency of
the candidate e, given e?s head verb, summed over
morphological variants:
?1(e) = lnP (E|V )
= ln
?
ei?E
N(ei)
?
vi?V
N(vi)
where N(x) is the number of times phrase x was
observed in the monolingual corpus.
Second, define ?2(e) to be the pointwise mutual
information (PMI) between V (the event that one of
the inflections of the verb in e is observed) and R,
the event of observing the rest of the phrase:
?2(e)
= PMI(V,R)
= lgP (V,R)? lg (P (V )P (R))
= lgP (E)? lg (P (V )P (R))
= lg
?
ei?E
N(ei)?lg
?
vi?V
N(vi)?lgN(r)+lgN
where N is the total number of tokens in the corpus,
and logarithms are base-2. This statistic character-
izes the degree of association between a verb and
its phrasal extension. We only calculate ?2 for two-
word phrases, as it did not prove helpful for longer
phrases.
639
Finally, define ?3(e) to be the relative frequency
of the phrasal verb e augmented by an accusative
pronoun, conditioned on the verb. Let A be the
set of phrases in E with an accusative pronoun (it,
them, him, her, me, you) optionally inserted either at
the end of the phrase or directly after the verb. For
e = look up, A = {look up, look X up, look up X,
looks up, looks X up, looks up X, . . . }, with X an
accusative pronoun. The ?3 statistic is similar to ?1,
but allows for an intervening or following pronoun:
?3(e) = lnP (A|V )
= ln
?
ei?A
N(ei)
?
vi?V
N(vi)
.
This statistic is designed to exploit the intuition that
phrasal verbs frequently have accusative pronouns
either inserted into the middle (e.g. look it up) or at
the end (e.g. look down on him).
3.3 Ranking Phrasal Verb Candidates
Our goal is to assign a single real-valued score to
each candidate e, by which we can rank candidates
according to semantic idiosyncrasy. For each lan-
guage L for which we have a parallel corpus, we
defined, in section 3.1, a function ?L(e) assigning
real values to candidate phrasal verbs e, which we
hypothesize is higher on average for more idiomatic
compounds. Further, in section 3.2, we defined real-
valued monolingual functions ?1, ?2, and ?3 for
which we hypothesize the same trend holds. Be-
cause each score individually ranks all candidates,
it is natural to view each ?L and ?i as a weak rank-
ing function that we can combine with a supervised
boosting objective. We use a modified version of
AdaBoost (Freund and Schapire, 1995) that opti-
mizes for recall.
For each ?L and ?i, we compute a ranked list
of candidate phrasal verbs, ordered from highest to
lowest value. To simplify learning, we consider only
the top 5000 candidate phrasal verbs according to
?1, ?2, and ?3. This pruning procedure excludes
candidates that do not appear in our monolingual
corpus.
We optimize the ranker using an unranked, in-
complete training set of phrasal verbs. We can eval-
uate the quality of the ranker by outputting the top
N ranked candidates and measuring recall relative
Algorithm 1 Recall-Oriented Ranking AdaBoost
1: for i = 1 : |X| do
2: w[i]? 1/|X|
3: end for
4: for t = 1 : T do
5: for all h ? H do
6: h ? 0
7: for i = 1 : |X| do
8: if xi 6? h then
9: h ? h + w[i]
10: end if
11: end for
12: end for
13: ht ? argmaxh?H |B ? h|
14: ?t ? ln(B/ht)
15: for i = 1 : |X| do
16: if xi ? ht then
17: w[i]? 1Zw[i] exp (??t)
18: else
19: w[i]? 1Zw[i] exp (?t)
20: end if
21: end for
22: end for
to this gold-standard training set. We choose this
recall-at-N metric so as to not directly penalize pre-
cision errors, as our training set is incomplete.
DefineH to be the set of N -element sets contain-
ing the top proposals for each weak ranker (we use
N = 2000). That is, each element ofH is a set con-
taining the 2000 highest values for some ?L or ?i.
We define the baseline error B to be 1?E[R], with
R the recall-at-N of a ranker ordering the candidate
phrases in the set ?H at random. The value E[R] is
estimated by averaging the recall-at-N of 1000 ran-
dom orderings of ?H.
Algorithm 1 gives the formulation of the Ada-
Boost training algorithm that we use to combine
weak rankers. The algorithm maintains a weight
vector w (summing to 1) which contains a positive
real number for each gold standard phrasal verb in
the training set X . Initially, w is uniformly set to
1/|X|. At each iteration of the algorithm, w is mod-
ified to take higher values for recently misclassi-
fied examples. We repeatedly choose weak rankers
ht ? H (and corresponding real-valued coefficients
?t) that correctly rank examples with high w values.
640
Lines 5?12 of Algorithm 1 calculate the weighted
error values h for every weak ranker set h ? H.
The error h will be 1 if h contains none of X and 0
if h contains all of X , as w always sums to 1. Line
13 picks the ranker ht ? H whose weighted error is
as far as possible from the random baseline error B .
Line 14 calculates a coefficient ?t for ht, which will
be positive if ht < B and negative if ht > B .
Intuitively, ?t encodes the importance of ht?it will
be high if ht performs well, and low if it performs
poorly. The Z in lines 17 and 19 is the normalizing
constant ensuring the vector w sums to 1.
After termination of Algorithm 1, we have
weights ?1, . . . , ?T and lists h1, . . . , hT . Define ft
as the function that generated the list ht (each ft will
be some ?L or ?i). Now, we define a final combined
function ?, taking a phrase e and returning a real
number:
?(e) =
T?
t=1
?tft(e).
We standardize the scores of individual weak
rankers to have mean 0 and variance 1, so that their
scores are comparable.
The final learned ranker outputs a real value, in-
stead of the class labels frequently found in Ada-
Boost. This follows previous work using boosting
for learning to rank (Freund et al, 2003; Xu and Li,
2007). Our algorithm differs from previous methods
because we are seeking to optimize for Recall-at-N ,
rather than a ranking loss.
4 Experimental Evaluation
4.1 Training and Test Set
In order to train and evaluate our system, we con-
struct a gold-standard list of phrasal verbs from
the freely available English Wiktionary. We gather
phrasal verbs from three sources within Wiktionary:
1. Entries labeled as English phrasal verbs4,
2. Entries labeled as English idioms5, and
3. The derived terms6 of English verb entries.
4http://en.wiktionary.org/wiki/Category:
English_phrasal_verbs
5http://en.wiktionary.org/wiki/Category:
English_idioms
6For example, see http://en.wiktionary.org/
wiki/take#Derived_terms
about across after against along
among around at before behind
between beyond by down for
from in into like off
on onto outside over past
round through to towards under
up upon with within without
Table 2: Particles and prepositions allowed in phrasal
verbs gathered from Wiktionary.
Many of the idioms and derived terms are not
phrasal verbs (e.g. kick the bucket, make-or-break).
We filter out any phrases not of the form V P+, with
V a verb, and P+ denoting one or more occurrences
of particles and prepositions from the list in Table 2.
We omit prepositions that do not productively form
English phrasal verbs, such as amid and as. This
process also omits some compounds that are some-
times called phrasal verbs, such as light verb con-
structions, e.g. have a go (Butt, 2003), and noncom-
positional verb-adverb collocations, e.g. look for-
ward.
There are a number of extant phrasal verb cor-
pora. For example, McCarthy et al (2003) present
graded human compositionality judgments for 116
phrasal verbs, and Baldwin (2008) presents a large
set of candidates produced by an automated system,
with false positives manually removed. We use Wik-
tionary instead, in an attempt to construct a maxi-
mally comprehensive data set that is free from any
possible biases introduced by automatic extraction
processes.
4.2 Filtering and Data Partition
The merged list of phrasal verbs extracted from Wik-
tionary included some common collocations that
have compositional semantics (e.g. know about), as
well as some very rare constructions (e.g. cheese
down). We removed these spurious results system-
atically by filtering out very frequent and very infre-
quent entries. First, we calculated the log probability
of each phrase, according to a language model built
from a large monolingual corpus of news documents
and web documents, smoothed with stupid back-
off (Brants et al, 2007). We sorted all Wiktionary
phrasal verbs according to this value. Then, we se-
lected the contiguous 75% of the sorted phrases that
minimize the variance of this statistic. This method
641
Recall-at-1220
Dev Test
Frequent Candidates 17.0 19.3
B
as
el
in
e
WordNet 3.0 Frequent 41.6 43.7
WordNet 3.0 Filtered 49.4 48.8
Monolingual Only 30.1 30.2
B
oo
st
ed
Bilingual Only 47.1 43.9
Monolingual+Bilingual 50.8 47.9
Table 3: Our boosted ranker combining monolingual
and bilingual features (bottom) compared to three base-
lines (top) gives comparable performance to the human-
curated upper bound.
removed a few very frequent phrases and a large
number of rare phrases. The remaining phrases were
split randomly into a development set of 694 items
and a held-out test set of 695 items.
4.3 Corpora
Our monolingual English corpus consists of news ar-
ticles and documents collected from the web. Our
parallel corpora from English to each of 50 lan-
guages also consist of documents collected from
the web via distributed data mining of parallel doc-
uments based on the text content of web pages
(Uszkoreit et al, 2010).
The parallel corpora were segmented into aligned
sentence pairs and word-aligned using two iterations
of IBM Model 1 (Brown et al, 1993) and two iter-
ations of the HMM-based alignment model (Vogel
et al, 1996) with posterior symmetrization (Liang et
al., 2006). This training recipe is common in large-
scale machine translation systems.
4.4 Generating Candidates
To generate the set of candidate phrasal verbs con-
sidered during evaluation, we exhaustively enumer-
ated the Cartesian product of all verbs present in the
previously described Wiktionary set (V), all parti-
cles in Table 2 (P) and a small set of second parti-
cles T = {with, to, on, }, with  the empty string.
The set of candidate phrasal verbs we consider dur-
ing evaluation is the product V ?P ?T , which con-
tains 96,880 items.
4.5 Results
We optimize a ranker using the boosting algorithm
described in section 3.3, using the features from Ta-
ble 1, optimizing performance on the Wiktionary de-
velopment set described in section 4.2. Monolingual
and bilingual statistics are calculated using the cor-
pora described in section 4.3, with candidate phrasal
verbs being drawn from the set described in section
4.4.
We evaluate our method of identifying phrasal
verbs by computing recall-at-N . This statistic is the
fraction of the Wiktionary test set that appears in the
top N proposed phrasal verbs by the method, where
N is an arbitrary number of top-ranked candidates
held constant when comparing different approaches
(we use N = 1220). We do not compute precision,
because the test set to which we compare is not an
exhaustive list of phrasal verbs, due to the develop-
ment/test split, frequency filtering, and omissions in
the original lexical resource. Proposing a phrasal
verb not in the test set is not necessarily an error, but
identifying many phrasal verbs from the test set is an
indication of an effective method. Recall-at-N is a
natural way to evaluate a ranking system where the
gold-standard data is an incomplete, unranked set.
Table 3 compares our approach to three baselines
using the Recall-at-1220 metric evaluated on both
the development and test sets. As a lower bound, we
evaluated the 1220 most frequent candidates in our
Monolingual corpus (Frequent Candidates).
As a competitive baseline, we evaluated the set of
phrasal verbs in WordNet 3.0 (Fellbaum, 1998). We
selected the most frequent 1220 out of 1781 verb-
particle constructions in WordNet (WordNet 3.0 Fre-
quent). A stronger baseline resulted from apply-
ing the same filtering procedure to WordNet that
we did to Wiktionary: sorting all verb-particle en-
tries by their language model score and retaining the
1220 consecutive entries that minimized language
model variance (WordNet 3.0 Filtered). WordNet
is a human-curated resource, and yet its recall-at-N
compared to our Wiktionary test set is only 48.8%,
indicating substantial divergence between the two
resources. Such divergence is typical: lexical re-
sources often disagree about what multiword expres-
sions to include (Lin, 1999).
The three final lines in Table 3 evaluate our
642
0%
10%
20%
30%
40%
50%
0 5 10 15 20 25 30 35 40 45 50
T
e
s
t
 
s
e
t
 
r
e
c
a
l
l
-
a
t
-
1
2
2
0
Number of languages (k)
Combined with AdaBoost
Individual Bilingual Statistics
Figure 2: The solid line shows recall-at-1220 when com-
bining the k best-performing bilingual statistics and three
monolingual statistics. The dotted line shows the indi-
vidual performance of the kth best-performing bilingual
statistic, when applied in isolation to rank candidates.
boosted ranker. Automatically detecting phrasal
verbs using monolingual features alone strongly out-
performed the frequency-based lower bound, but un-
derperformed the WordNet baseline. Bilingual fea-
tures, using features from 50 languages, proved sub-
stantially more effective. The combination of both
types of features yielded the best performance, out-
performing the human-curated WordNet baseline on
the development set (on which our ranker was opti-
mized) and approaching its performance on the held-
out test set.
4.6 Feature Analysis
The solid line in Figure 2 shows the recall-at-1220
for a boosted ranker using all monolingual statistics
and k bilingual statistics, for increasing k. Bilin-
gual statistics are added according to their individual
recall, from best-performing to worst. That is, the
point at k = 0 uses only ?1, ?2, and ?3, the point at
k = 1 adds the best individually-performing bilin-
gual statistic (Spanish) as a weak ranker, the next
point adds the second-best bilingual statistic (Ger-
man), etc. Boosting maximizes performance on the
development set, and evaluation is performed on the
test set. We use T = 53 (equal to the total number
of weak rankers).
Recall-at-1220
Dev Test
Bilingual only 47.1 43.9
Bilingual+?1 48.1 46.9
Bilingual+?2 50.1 48.3
Bilingual+?3 48.4 46.3
Bilingual+?1 + ?2 50.2 47.9
Bilingual+?1 + ?3 49.0 47.4
Bilingual+?2 + ?3 50.4 49.4
Bilingual+?1 + ?2 + ?3 50.8 47.9
Table 4: An ablation of monolingual statistics shows that
they are useful in addition to the 50 bilingual statistics
combined, and no single statistic provides maximal per-
formance.
The dotted line in Figure 2 shows that individual
bilingual statistics have recall-at-1220 ranging from
34.4% to 5.0%. This difference reflects the differ-
ent sizes of parallel corpora and usefulness of dif-
ferent languages in identifying English semantic id-
iosyncrasy. Combining together the signal of mul-
tiple languages is clearly beneficial, and including
many low-performing languages still offers overall
improvements.
Table 4 shows the effect of adding different sub-
sets of the monolingual statistics to the set of all
50 bilingual statistics. Monolingual statistics give
a performance improvement of up to 5.5% recall
on the test set, but the comparative behavior of the
various combinations of the ?i is somewhat unpre-
dictable when training on the development set and
evaluating on the test set. The pointwise mutual in-
formation of a verb and its particles (?2) appears to
be the most useful feature. In fact, the test set per-
formance of using ?2 alone outperforms the combi-
nation of all three. The best combination even out-
performs the WordNet 3.0 baseline on the test set,
though optimizing on the development set would not
select this model.
4.7 Error Analysis
Table 5 shows the 100 highest ranked phrasal verb
candidates by our system that do not appear in either
the development or test sets. Most of these candi-
dates are in fact English phrasal verbs that happened
to be missing from Wiktionary; some are present
in Wiktionary but were removed from the reference
643
pick up pat on tap into fit for charge with suit against
catch up burst into muck up haul up give up get off
get through get up get in tack on buzz about do like
plump for haul in keep up with strap on catch up with suck into
get round chop off slap on pitch into get into inquire into
drop behind get on catch up on pass on cue from carry around
get around get over shoot at pick over shoot by shoot in
make up to get past cast down set up with rule off hand round
piss on hit by break down move for lead off pluck off
flip through edge over strike off plug into keep up go past
set off pull round see about stay on put up sidle up to
buzz around take off set up slap in head towards shoot past
inquire for tuck up lie with well before go on with reel from
drive along snap off barge into whip on put down instance through
bar from cut down on let in tune in to move off suit in
lean against well beyond get down to go across sail into lie over
hit with chow down on look after catch at
Table 5: The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set.
Candidates are presented in decreasing rank; ?pat on? is the second highest ranked candidate.
sets during filtering, and the remainder are in fact
not phrasal verbs (true precision errors).
These errors fall largely into two categories.
Some candidates are compositional, but contain pol-
ysemous verbs, such as hit by, drive along, and head
towards. In these cases, prepositions disambiguate
the verb, which naturally affects translation distri-
butions. Other candidates are not phrasal verbs, but
instead phrases that tend to have a different syntac-
tic role, such as suit against, instance through, fit
for, and lie over (conjugated as lay over). A care-
ful treatment of part-of-speech tags when computing
corpus statistics might address this issue.
5 Related Work
The idea of using word-aligned parallel corpora
to identify idiomatic expressions has been pur-
sued in a number of different ways. Melamed
(1997) tests candidate MWEs by collapsing them
into single tokens, training a new translation model
with these tokens, and using the performance of
the new model to judge candidates? noncomposi-
tionality. Villada Moiro?n and Tiedemann (2006)
use word-aligned parallel corpora to identify Dutch
MWEs, testing the assumption that the distributions
of alignments of MWEs will generally have higher
entropies than those of fully compositional com-
pounds. Caseli et al (2010) generate candidate mul-
tiword expressions by picking out sufficiently com-
mon phrases that align to single target-side tokens.
Tsvetkov and Wintner (2012) generate candidate
MWEs by finding one-to-one alignments in paral-
lel corpora which are not in a bilingual dictionary,
and ranking them based on monolingual statistics.
The system of Salehi and Cook (2013) is perhaps
the closest to the current work, judging noncompo-
sitionality using string edit distance between a can-
didate phrase?s automatic translation and its com-
ponents? individual translations. Unlike the current
work, their method does not use distributions over
translations or combine individual bilingual values
with boosting; however, they find, as we do, that in-
corporating many languages is beneficial to MWE
identification.
A large body of work has investigated the identifi-
cation of noncompositional compounds from mono-
lingual sources (Lin, 1999; Schone and Jurafsky,
2001; Fazly and Stevenson, 2006; McCarthy et
al., 2003; Baldwin et al, 2003; Villavicencio,
2003). Many of these monolingual statistics could
be viewed as weak rankers and fruitfully incorpo-
rated into our framework.
There has also been a substantial amount of work
addressing the problem of differentiating between
literal and idiomatic instances of phrases in con-
text (Katz and Giesbrecht, 2006; Li et al, 2010;
644
Sporleder and Li, 2009; Birke and Sarkar, 2006;
Diab and Bhutada, 2009). We do not attempt this
task; however, techniques for token identification
could be used to improve type identification (Bald-
win, 2005).
6 Conclusion
We have presented the polyglot ranking approach
to phrasal verb identification, using parallel corpora
from many languages to identify phrasal verbs. We
proposed an evaluation metric that acknowledges the
inherent incompleteness of reference sets, but dis-
tinguishes among competing systems in a manner
aligned to the goals of the task. We developed a
recall-oriented learning method that integrates mul-
tiple weak ranking signals, and demonstrated exper-
imentally that combining statistical evidence from a
large number of bilingual corpora, as well as from
monolingual corpora, produces the most effective
system overall. We look forward to generalizing
our approach to other types of noncompositional
phrases.
Acknowledgments
Special thanks to Ivan Sag, who argued for the
importance of handling multi-word expressions in
natural language processing applications, and who
taught the authors about natural language syntax
once upon a time. We would also like to thank the
anonymous reviewers for their helpful suggestions.
References
Otavio Acosta, Aline Villavicencio, and Viviane Moreira.
2011. Identification and treatment of multiword ex-
pressions applied to information retrieval. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proceedings of the Sixth Conference on
Natural Language Learning.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin. 2005. Deep lexical acquisition of
verb-particle constructions. Computer Speech & Lan-
guage, Special Issue on Multiword Expressions.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of english verb-particle con-
structions. In Proceedings of the LREC Workshop To-
wards a Shared Task for Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL Workshop on
Multiword Expressions.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of European Chapter
of the Association for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1).
Kenneth Church. 2011. How many multiword expres-
sions do people know? In Proceedings of the ACL
Workshop on Multiword Expressions.
Mona T. Diab and Pravin Bhutada. 2009. Verb noun
construction MWE token supervised classification. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Robert Dixon. 1982. The grammar of english phrasal
verbs. Australian Journal of Linguistics.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the European Chap-
ter of the Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting
multiword expressions improves word sense disam-
biguation. In Proceedings of the ACL Workshop on
Multiword Expressions.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. In Proceedings of the Confer-
ence on Computational Learning Theory.
645
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. The Journal of Machine
Learning Research.
Ray Jackendoff. 1993. The Architecture of the Language
Faculty. MIT Press.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL Workshop on Multiword Expres-
sions.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the Asso-
ciation for Computational Linguistics.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL Workshop on Multi-
word Expressions.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Santanu Pal, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In Proceedings of the
COLING 2010 Workshop on Multiword Expressions.
Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine transla-
tion using domain bilingual multiword expressions. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the CICLING Conference on Intelligent Text Pro-
cessing and Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting the com-
positionality of multiword expressions using transla-
tions in multiple languages. In Second Joint Confer-
ence on Lexical and Computational Semantics.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the European Chapter
of the Association for Computational Linguistics.
Yulia Tsvetkov and Shuly Wintner. 2012. Extraction
of multi-word expressions from small parallel corpora.
In Natural Language Engineering.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of the Conference
on Computational Linguistics.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the ACL
Workshop on Multiword Expressions.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL Work-
shop on Multiword Expressions in a Multilingual Con-
text.
Aline Villavicencio. 2003. Verb-particle constructions
and lexical resources. In Proceedings of the ACL
workshop on Multiword expressions.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the SIGIR Conference on Research and Development
in Information Retrieval.
Ying Xu, Randy Goebel, Christoph Ringlstetter, and
Grzegorz Kondrak. 2010. Application of the tightness
continuum measure to chinese information retrieval.
In Proceedings of the COLING Workshop on Multi-
word Expressions.
646
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220?229,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Statistical Script Learning with Multi-Argument Events
Karl Pichotta
Department of Computer Science
The University of Texas at Austin
pichotta@cs.utexas.edu
Raymond J. Mooney
Department of Computer Science
The University of Texas at Austin
mooney@cs.utexas.edu
Abstract
Scripts represent knowledge of stereotyp-
ical event sequences that can aid text un-
derstanding. Initial statistical methods
have been developed to learn probabilis-
tic scripts from raw text corpora; how-
ever, they utilize a very impoverished rep-
resentation of events, consisting of a verb
and one dependent argument. We present
a script learning approach that employs
events with multiple arguments. Unlike
previous work, we model the interactions
between multiple entities in a script. Ex-
periments on a large corpus using the task
of inferring held-out events (the ?narrative
cloze evaluation?) demonstrate that mod-
eling multi-argument events improves pre-
dictive accuracy.
1 Introduction
Scripts encode knowledge of stereotypical events,
including information about their typical ordered
sequences of sub-events and corresponding argu-
ments (Schank and Abelson, 1977). The clas-
sic example is the ?restaurant script,? which en-
codes knowledge about what normally happens
when dining out. Such knowledge can be used
to improve text understanding by supporting in-
ference of missing actions and events, as well as
resolution of lexical and syntactic ambiguities and
anaphora (Rahman and Ng, 2012). For example,
given the text ?John went to Olive Garden and or-
dered lasagna. He left a big tip and left,? an infer-
ence that scripts would ideally allow us to make is
?John ate lasagna.?
There is a small body of recent research on auto-
matically learning probabilistic models of scripts
from large corpora of raw text (Manshadi et al.,
2008; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Jans et al., 2012). However,
this work uses a very impoverished representation
of events that only includes a verb and a single de-
pendent entity. We propose a more complex multi-
argument event representation for use in statistical
script models, capable of directly capturing inter-
actions between multiple entities. We present a
method for learning such a model, and provide ex-
perimental evidence that modeling entity interac-
tions allows for better prediction of events in docu-
ments, compared to previous single-entity ?chain?
models. We also compare to a competitive base-
line not used in previous work, and introduce a
novel evaluation metric.
2 Background
The idea of representing stereotypical event se-
quences for textual inference originates in the
seminal work of Schank and Abelson (1977).
Early scripts were manually engineered for spe-
cific domains; however, Mooney and DeJong
(1985) present an early knowledge-based method
for learning scripts from a single document. These
early scripts (and methods for learning them) were
non-statistical and fairly brittle.
Chambers and Jurafsky (2008) introduced a
method for learning statistical scripts that, using a
much simpler event representation that allows for
efficient learning and inference. Jans et al. (2012)
use the same simple event representation, but in-
troduce a new model that more accurately predicts
test data. These methods only model the actions of
a single participant, called the protagonist. Cham-
bers and Jurafsky (2009) extended their approach
to the multi-participant case, modeling the events
in which all of the entities in a document are in-
volved; however, their method cannot represent in-
teractions between multiple entities.
Balasubramanian et al. (2012; 2013) describe
the Rel-gram system, a Markov model similar to
that of Jans et al. (2012), but with tuples instead
of (verb, dependency) pairs. Our approach is sim-
220
ilar, but instead of modeling a distribution over co-
occurring verbs and nominal arguments, we model
interactions between entities directly by incorpo-
rating coreference information into the model.
Previous statistical script learning systems pro-
ceed broadly as follows. For a document D:
1. Run a dependency parser on D, to match up
verbs with their argument NPs.
2. Run a coreference resolver onD to determine
which NPs likely refer to the same entity.
3. Construct a sequence of event objects, using
syntactic and coreference information.
One can then build a statistical model of the event
sequences produced by Step 3. Such a model may
be evaluated using the narrative cloze evaluation,
described in Section 4.1, in which we hold out an
event from a sequence and attempt to infer it.
The major difference between the current work
and previous work is that the event sequences pro-
duced in Step 3 are of a different sort from those
in other models. Our events are more structured,
as described in Section 3.1, and we produce one
event sequence per document, instead of one event
sequence per entity. This requires a different sta-
tistical model, as described in Section 3.2.
3 Script Models
In Section 3.1, we describe the multi-argument
events we use as the basis of our script models.
Section 3.2 describes a script model using these
events, and Section 3.3 describes the baseline sys-
tems to which we compare.
3.1 Multi-Argument Events
Statistical scripts are models of stereotypical se-
quences of events. In Chambers and Juraf-
sky (2008; 2009) and Jans et al. (2012), events
are (verb, dependency) pairs, forming ?chains,?
grouped according to the entity involved. For ex-
ample, the text
(1) Mary emailed Jim and he responded to her
immediately.
yields two chains. First, there is a chain for Mary:
(email, subject)
(respond, object)
indicating that Mary was the subject of an email-
ing event and the object of a responding event.
Second, there is a chain for Jim:
(email, object)
(respond, subject)
indicating that Jim was the object of an emailing
event and the subject of a responding event. Thus,
one document produces many chains, each cor-
responding to an entity. Note that a single verb
may produce multiple pair events, each present
in a chain corresponding to one of the verb?s ar-
guments. Note also that there is no connection
between the different events produced by a verb:
there is nothing connecting (email, subject) in
Mary?s chain with (email, object) in Jim?s chain.
We propose a richer event representation, in
which a document is represented as a single se-
quence of event tuples, the arguments of which are
entities. Each entity may be mentioned in many
events, and, unlike previous work, each event may
involve multiple entities. For example, sentence
(1) will produce a single two-event sequence, the
first event representing Mary emailing Jim, and the
second representing Jim responding to Mary.
Formally, an entity is represented by a con-
stant, and noun phrases are mapped to entities,
where two noun phrases are mapped to the same
constant if and only if they corefer. A multi-
argument event is a relational atom v(e
s
, e
o
, e
p
),
where v is a verb lemma, and e
s
, e
o
, and e
p
are
possibly-null entities. The first entity, e
s
, stands
in a subject relation to the verb v; the second, e
o
,
is the direct object of v; the third e
p
stands in
a prepositional relation to v. One of these enti-
ties is null (written as ???) if and only if no noun
phrase stands in the appropriate relation to v. For
example, Mary hopped would be represented as
hop(mary, ?, ?), while Mary gave the book to John
would be give(mary, book, john). In this formula-
tion, Example (1) produces the sequence
email(m, j, ?)
respond(j,m, ?)
where m and j are entity constants representing
all mentions of Mary and Jim, respectively. Note
that this formulation is capable of capturing inter-
actions between entities: we directly encode the
fact that after one person emails another, the lat-
ter responds to the former. In contrast, pair events
can capture only that after an entity emails, they
are responded to (or after they are emailed, they
respond). Multi-argument events capture more of
the basic event structure of text, and are therefore
well-suited as a representation for scripts.
221
3.2 Multi-argument Statistical Scripts
We now describe our script model. Section 3.2.1
describes our method of estimating a joint prob-
ability distribution over pairs of events, modeling
event co-occurrence, and Section 3.2.2 shows how
this co-occurrence probability can be used to infer
new events from a set of known events.
3.2.1 Estimating Joint Probabilities
Suppose we have a sequence of multi-argument
events, each of which is a verb with entities as ar-
guments. We are interested in predicting which
event is most likely to have happened at some
point in the sequence. Our model will require
a conditional probability P (a|a
?
), the probability
of seeing event a after event a
?
, given we have
observed a
?
. However, as described below, di-
rectly estimating this probability is more compli-
cated than in previous work because events now
have additional structure.
By definition, we have
P (a
2
|a
1
) =
P (a
1
, a
2
)
P (a
1
)
where P (a
1
, a
2
) is the probability of seeing a
1
and a
2
, in order. The most straightforward way
to estimate P (a
1
, a
2
) is, if possible, by counting
the number of times we observe a
1
and a
2
co-
occurring and normalizing the function to sum to
1 over all pairs (a
1
, a
2
). For Chambers and Ju-
rafsky (2008; 2009) and Jans et al. (2012), such a
Maximum Likelihood Estimate is straightforward
to arrive at: events are (verb, dependency) pairs,
and two events co-occur when they are in the same
event chain, relating to the same entity (Jans et al.
(2012) further require a
1
and a
2
to be near each
other). One need simply traverse a training corpus
and count the number of times each pair (a
1
, a
2
)
co-occurs. The Rel-grams of Balasubramanian et
al. (2012; 2013) admit a similar strategy: to arrive
at a joint distribution of pairwise co-occurrence,
one can simply count co-occurrence of ground re-
lations in a corpus and normalize.
However, given two multi-argument events of
the form v(e
s
, e
o
, e
p
), this strategy will not suffice.
For example, if during training we observe the two
co-occurring events
(2) ask(mary, bob, question)
answer(bob, ?, ?)
we would like this to lend evidence to the
co-occurrence of events ask(x, y, z) and
Algorithm 1 Learning with entity substitution
1: for a
1
, a
2
? evs do
2: N(a
1
, a
2
)? 0
3: end for
4: for D ? documents do
5: for a
1
, a
2
? coocurEvs(D) do
6: for ? ? subs(a
1
, a
2
) do
7: N(?(a
1
), ?(a
2
)) += 1
8: end for
9: end for
10: end for
answer(y, ?, ?) for all distinct entities x, y,
and z. If we were to simply keep the entities as
they are and calculate raw co-occurrence counts,
we would get evidence only for x = mary,
y = bob, and z = question.
One approach to this problem would be to de-
ploy one of many previously described Statistical
Relational Learning methods, for example Logi-
cal Hidden Markov Models (Kersting et al., 2006)
or Relational Markov Models (Anderson et al.,
2002). These methods can learn various statisti-
cal relationships between relational logical atoms
with variables, of the sort considered here. How-
ever, we investigate a simpler option.
The most important relationship between the
entities in two multi-argument events concerns
their overlapping entities. For example, to de-
scribe the relationship between the three entities
in (2), it is most important to note that the object
of the first event is identical with the subject of the
second (namely, both are bob). The identity of the
non-overlapping entities mary and question is not
important for capturing the relationship between
the two events.
We note that two multi-argument events
v(e
s
, e
o
, e
p
) and v
?
(e
?
s
, e
?
o
, e
?
p
), share at most three
entities. We thus introduce four variables x, y, z,
and O. The three variables x, y, and z repre-
sent arbitrary distinct entities, and the fourth, O,
stands for ?Other,? for entities not shared between
the two events. We can rewrite the entities in our
two multi-argument events using these variables,
with the constraint that two identical (i.e. corefer-
ent) entities must be mapped to the same variable
in {x, y, z}, and no two distinct entities may map
to the same variable in {x, y, z}. This formulation
simplifies calculations while still capturing pair-
wise entity relationships between events.
Algorithm 1 gives the pseudocode for the learn-
222
ing method. This populates a co-occurrence
matrix N , where entry N(a
1
, a
2
) gives the co-
occurrence count of events a
1
and a
2
. The vari-
able evs in line 1 is the set of all events in our
model, which are of the form v(e
s
, e
o
, e
p
), with v
a verb lemma and e
s
, e
o
, e
p
? {x, y, z, O}. The
variable documents in line 4 is the collection
of documents in our training corpus. The func-
tion cooccurEvs in line 5 takes a document D
and returns all ordered pairs of co-occurring events
in D, where, following the 2-skip bigram model
of Jans et al. (2012), and similar to Balasubrama-
nian et al. (2012; 2013), two events a
1
and a
2
are
said to co-occur if they occur in order, in the same
document, with at most two intervening events be-
tween them.
1
The function subs in line 6 takes
two events and returns all variable substitutions ?
mapping from entities mentioned in the events a
1
and a
2
to the set {x, y, z, O}, such that two coref-
erent entities map to the same element of {x, y, z}.
A substitution ? applied to an event v(e
s
, e
o
, e
p
),
as in line 7, is defined as v(?(e
s
), ?(e
o
), ?(e
p
)),
with the null entity mapped to itself.
Once we have calculatedN(a
1
, a
2
) using Algo-
rithm 1, we may define P (a
1
, a
2
) for two events
a
1
and a
2
, giving an estimate for the probability
of observing a
2
occurring after a
1
, as
P (a
1
, a
2
) =
N(a
1
, a
2
)
?
a
?
1
,a
?
2
N(a
?
1
, a
?
2
)
. (3)
We may then define the conditional probability of
seeing a
2
after a
1
, given an observation of a
1
:
P (a
2
|a
1
) =
P (a
1
, a
2
)
?
a
?
P (a
1
, a
?
)
=
N(a
1
, a
2
)
?
a
?
N(a
1
, a
?
)
. (4)
3.2.2 Inferring Events
Suppose we have a sequence of multi-argument
events extracted from a document. A natural task
for a statistical script model is to infer what other
events likely occurred, given the events explic-
itly stated in a document. Chambers and Jurafsky
(2008; 2009) treat the events involving an entity
as an unordered set, inferring the most likely ad-
ditional event, with no relative ordering between
the inferred event and known events. We adopt
the model of Jans et al. (2012), which was demon-
strated to give better empirical performance. This
1
Other notions of co-occurrence could easily be substi-
tuted here.
model takes an ordered sequence of events and
a position in that sequence, and guesses events
that likely occurred at that position. In that work,
events are (verb, dependency) pairs, and an event
sequence consists of all such pairs involving a par-
ticular entity. We use this model in the multi-
argument event setting, in which a document pro-
duces a single sequence of multi-argument events.
LetA be an ordered list of events, and let p be an
integer between 1 and |A|, the length ofA. For i =
1, . . . , |A|, define a
i
to be the ith element of A.
We follow Jans et al. (2012) by scoring a candidate
event a according to its probability of following all
of the events before position p, and preceding all
events after position p. That is, we rank candidate
events a by maximizing S(a), defined as
S(a) =
p?1
?
i=1
logP (a|a
i
) +
|A|
?
i=p
logP (a
i
|a) (5)
with conditional probabilities P (a|a
?
) calculated
using (4). Each event in a
i
? A independently
contributes to a candidate a?s score; the ordering
between a and a
i
is taken into account, but the or-
dering between the different events a
i
? A does
not directly affect a?s score.
3.3 Baseline Systems
We describe the baseline systems against which
we compare the performance of the multi-
argument script system described in section 3.2.
These systems infer new events (either multi-
argument or pair events) given the events con-
tained in a document.
Performance of these systems is measured using
the narrative cloze task, in which we hold out a sin-
gle event (either a multi-argument or pair event),
and rate a system by its ability to infer this event,
given the other events in a document. The narra-
tive cloze task is described in detail in Section 4.1.
3.3.1 Random Model
The simplest baseline we compare to is the ran-
dom baseline, which outputs randomly selected
events observed during training. This model can
guess either multi-argument or pair events.
3.3.2 Unigram Model
The unigram system guesses events ordered by
prior probability, as calculated from the train-
ing set. If scripts are viewed as n-gram models
223
over events, this baseline corresponds to a bag-of-
words unigram model. In this model, events are
assumed to occur independently, drawn from a sin-
gle distribution. This model can be used to guess
either multi-argument or pair events.
3.3.3 Single Protagonist Model
We refer to the system of Jans et al. (2012) as the
single protagonist system. This model takes a
single sequence of (verb, dependency) pair events,
all relating to a single entity. It then produces
a list of pair events, giving the model?s top pre-
dictions for additional events involving the entity.
This model maximizes the objective given in (5),
with the sequence A (and the candidate guesses a)
comprised of pair events.
3.3.4 Multiple Protagonist Model
The multiple protagonist system infers multi-
argument events. While this method is not de-
scribed in previous work, it is the most direct way
of guessing a full multi-argument event using a
single protagonist script model.
The multiple protagonist system uses a single-
protagonist model, which models pair events, to
predict multi-argument events, given a sequence
of known multi-argument events. Suppose we
have a non-empty set E of entities mentioned in
the known events. We describe the most direct
method of using a single-protagonist system to in-
fer additional multi-argument events involving E.
A multi-argument event a = v(e
s
, e
o
, e
p
) repre-
sents three pairs: (v, e
s
), (v, e
o
), and (v, e
p
). The
multiple protagonist model scores an event a ac-
cording to the score the single protagonist model
assigns to these three pairs individually.
For entity e ? E in some multi-argument event
in a document, we first extract the sequence of
(verb, dependency) pairs corresponding to e from
all known multi-argument events. For a pair d,
we calculate the score S
e
(d), the score the sin-
gle protagonist system assigns the pair d, given the
known pairs corresponding to e. If e has no known
pairs corresponding to it (in the cloze evaluation
described below, this will happen if e occurs only
in the held-out event), we fall back to calculating
S
e
(d) with a unigram model, as described in Sec-
tion 3.3.2, over (verb, dependency) pair events.
We then rank a multi-argument event a =
v(e
s
, e
o
, e
p
), with e
s
, e
o
, e
p
? E, with the follow-
ing objective function:
M(a) =S
e
s
((v, subj)) + S
e
o
((v, obj))+
S
e
p
((v, prep)) (6)
where, for null entity e, we define S
e
(d) = 0 for
all d. In the cloze evaluation, E will be the entities
in the held-out event. Each entity in a contributes
independently to the score M(a), based on the
known (verb, dependency) pairs involving that en-
tity. This model scores a multi-argument event a
by combining one independent single-protagonist
model for every entity in a.
This model is similar to the multi-participant
narrative schemas described in Chambers and Ju-
rafsky (2009), but whereas they infer bare verbs,
we infer an entire multi-argument event.
4 Evaluation
4.1 Evaluation Task
We follow previous work in using the narrative
cloze task to evaluate statistical scripts (Chambers
and Jurafsky, 2008; Chambers and Jurafsky, 2009;
Jans et al., 2012). The task is as follows: given
a sequence of events a
1
, . . . , a
n
from a document,
hold out some event a
p
and attempt to predict that
event, given the other events in the sequence. As
we cannot automatically evaluate the prediction of
truly unmentioned events in a document, this eval-
uation acts as a straightforward proxy.
In the aforementioned work, the cloze task is
to guess a pair event, given the other events in
which the held-out pair?s entity occurs. In Section
4.2.2, we evaluate directly on this task of guess-
ing pair events. However, in Section 4.2.1, we
evaluate on the task of guessing a multi-argument
event, given all other events in a document and the
entities mentioned in the held-out event. This is,
we argue, the most natural way to adapt the cloze
evaluation to the multi-argument event setting: in-
stead of guessing a held-out pair event based on
the other events involving its lone entity, we will
guess a held-out multi-argument event based on
the other events involving any of its entities.
A document may contain arbitrarily many enti-
ties. The script model described in Section 3.2.1,
however, only models events involving entities
from a closed class of four variables {x, y, z, O}.
We therefore rewrite entities in a document?s se-
quences of events to the variables {x, y, z, O} in
a way that maintains all pairwise relationships be-
tween the held-out event and others. That is, if the
224
held-out event shares an entity with another event,
this remains true after rewriting.
We perform entity rewriting relative to a single
held-out event, proceeding as follows:
? Any entity in the held-out event that is men-
tioned at least once in another event gets
rewritten consistently to one of x, y, or z,
such that distinct entities never get rewritten
to the same variable.
? Any entity mentioned only in the held-out
event is rewritten as O.
? All entities not present in the held-out event
are rewritten as O.
This simplification removes structure from the
original sequence, but retains the important pair-
wise entity relationships between the held-out
event and the other events.
4.2 Experimental Evaluation
For each document, we use the Stanford depen-
dency parser (De Marneffe et al., 2006) to get syn-
tactic information about the document; we then
use the Stanford coreference resolution engine
(Raghunathan et al., 2010) to get (noisy) equiva-
lence classes of coreferent noun phrases in a doc-
ument.
2
We train on approximately 1.1M arti-
cles from years 1994-2006 of the NYT portion
of the Gigaword Corpus, Third Edition (Graff et
al., 2007), holding out a random subset of the arti-
cles from 1999 for development and test sets. Our
test set consists of 10,000 randomly selected held-
out events, and our development set is 500 disjoint
randomly selected held-out events. To remove du-
plicate documents, we hash the first 500 characters
of each article and remove any articles with hash
collisions. We use add-one smoothing on all joint
probabilities. To reduce the size of our model, we
remove all events that occur fewer than 50 times.
3
We evaluate performance using the following
two metrics:
1. Recall at 10: Following Jans et al. (2012),
we measure performance by outputting the
top 10 guesses for each held-out event and
calculating the percentage of such lists con-
2
We use version 1.3.4 of the Stanford CoreNLP system.
3
A manual inspection reveals that the majority of these
removed events come from noisy text or parse errors.
taining the correct answer.
4
This value will
be between 0 and 1, with 1 indicating perfect
system performance.
2. Accuracy: A multi-argument event
v(e
s
, e
o
, e
p
) has four components; a pair
event has two components. For a held-out
event, we may judge the accuracy of a
system?s top guess by giving one point for
getting each of its components correct and
dividing by the number of possible points.
We average this value over the test set,
yielding a value between 0 and 1, with 1
indicating perfect system performance. This
is a novel evaluation metric for the script
learning task.
These metrics target a system?s most confident
predicted events: we argue that a script system is
best evaluated by its top inferences.
In Section 4.2.1, we evaluate on the task of in-
ferring multi-argument events. In Section 4.2.2,
we evaluate on the task of guessing pair events.
4.2.1 System Comparison on Multi-argument
Events
We first compare system performance on inferring
multi-argument events, evaluated on the narrative
cloze task as described in Section 4.1, using the
corpora and metrics described in Section 4.2. We
compare against three baselines: the uninformed
random baseline from Section 3.3.1, the unigram
system from 3.3.2, and the multiple protagonist
system from Section 3.3.4.
The joint system guesses the held-out event,
given the other events in the document that involve
the entities in that held-out tuple. The system or-
ders candidate events a by their scores S(a), as
given in Equation (5). This is the primary sys-
tem described in this paper, modeling full multi-
argument events directly.
Table 1 gives the recall at 10 (?R@10?) and ac-
curacy scores for the different systems. The uni-
gram system is quite competitive, achieving per-
formance comparable to the multiple protagonist
system on accuracy, and superior performance on
recall at 10.
Evaluating by the recall at 10 metric, the joint
system provides a 2.9% absolute (13.2% relative)
improvement over the unigram system, and a 3.6%
4
Jans et al. (2012) instead use recall at 50, but we observe,
as they also report, that the comparative differences between
systems using recall at k for various values of k is similar.
225
Method R@10 Accuracy
Random 0.001 0.334
Unigram 0.216 0.507
Multiple Protagonist 0.209 0.504
Joint 0.245 0.549
Table 1: Results for multi-argument events.
absolute (17.2% relative) improvement over the
multiple protagonist system. These differences
are statistically significant (p < 0.01) by McNe-
mar?s test. By accuracy, the joint system provides
a 4.2% absolute (8.3% relative) improvement over
the unigram model, and a 4.5% absolute (8.9%
relative) improvement over the multiple protago-
nist model. Accuracy differences are significant
(p < 0.01) by a Wilcoxon signed-rank test.
These results provide evidence that directly
modeling full multi-argument events, as opposed
to modeling chains of (verb, dependency) pairs for
single entities, allows us to better infer held-out
verbs with all participating entities.
4.2.2 System Comparison on Pair Events
In Section 4.2.1, we adapted a baseline pair-event
system to the task of guessing multi-argument
events. We may also do the converse, adapting our
multi-argument event system to the task of guess-
ing the simpler pair events. That is, we infer a full
multi-argument event and extract from it a (sub-
ject,verb) pair relating to a particular entity. This
allows us to compare directly to previously pub-
lished methods.
The random, unigram, and single protagonist
systems are pair-event systems described in Sec-
tions 3.3.1, 3.3.2, and 3.3.3, respectively. The
joint pair system takes the multi-argument events
guessed by the joint system of Section 4.2.1 and
converts them to pair events by discarding any in-
formation not related to the target entity; that is, if
the held-out pair event relates to an entity e, then
every occurrence of e as an argument of a guessed
multi-argument event will be converted into a sin-
gle pair event, scored identically to its original
multi-argument event. Ties are broken arbitrarily.
Table 2 gives the comparative results for these
four systems. The test set is constructed by ex-
tracting one pair event from each of the 10,000
multi-argument events in the test set used in Sec-
tion 4.2.1, such that the extracted pair event relates
to an entity with at least one additional known pair
Method R@10 Accuracy
Random 0.001 0.495
Unigram 0.297 0.552
Single Protagonist 0.282 0.553
Joint Pair 0.336 0.561
Table 2: Results for pair events.
event. Evaluating by recall at 10, the joint sys-
tem provides a 3.9% absolute (13.1% relative) im-
provement over the unigram baseline, and a 5.4%
absolute (19.1% relative) improvement over the
single protagonist system. These differences are
significant (p < 0.01) by McNemar?s test. By
accuracy, the joint system provides a 0.9% abso-
lute (1.6% relative) improvement over the unigram
model, and a 0.8% absolute (1.4% relative) im-
provement over the single protagonist model. Ac-
curacy differences are significant (p < 0.01) by a
Wilcoxon signed-rank test.
These results indicate that modeling multi-
argument event sequences allows better inference
of simpler pair events. These performance im-
provements may be due to the fact that the joint
model conditions on information not representable
in the single protagonist model (namely, all of the
events in which a multi-argument event?s entities
are involved).
5 Related Work
The procedural encoding of common situations
for automated reasoning dates back decades. The
frames of Minsky (1974), schemas of Rumelhart
(1975), and scripts of Schank and Abelson (1977)
are early examples. These models use quite com-
plex representations for events, with many differ-
ent relations between events. They are not statis-
tical, and use separate models for different scenar-
ios (e.g. the ?restaurant script? is different from
the ?bank script?). Generally, they require humans
to encode procedural information by hand; see,
however, Mooney and DeJong (1985) for an early
method for learning scripts automatically from a
document. Miikkulainen (1990; 1993) gives a hi-
erarchical Neural Network system which stores
sequences of events from text in episodic memory,
capable of simple question answering.
Regneri et al. (2010) and Li et al. (2012)
give methods for using crowdsourcing to cre-
ate situation-specific scripts. These methods
226
help alleviate the bottleneck of the knowledge-
engineering required for traditionally conceived
script systems. These systems are precision-
oriented: they create small, highly accurate scripts
for very limited scenarios. The current work,
in contrast, focuses on building high-recall mod-
els of general event sequences. There are also a
number of systems addressing the related problem
of modeling domain-specific human-human dia-
log for building dialog systems (Bangalore et al.,
2006; Chotimongkol, 2008; Boyer et al., 2009).
There have been a number of recent approaches
to learning statistical scripts. Chambers and Ju-
rafsky (2008) and Jans et al. (2012) give methods
for learning models of (verb, dependency) pairs,
as described above. Manshadi et al. (2008) give
an n-gram model for sequences of verbs and their
patients. McIntyre and Lapata (2009; 2010) use
script objects learned from corpora of fairy tales
to automatically generate stories. Chambers and
Jurafsky (2009) extend their previous model to
incorporate multiple entities, but do not directly
model the different arguments of an event. Bam-
man et al. (2013) learn latent character personas
from film summaries, associating character types
with stereotypical actions; they focus on identify-
ing persona types, rather than event inference.
Manshadi et al. (2008) and Balasubramanian et
al. (2012; 2013) give approaches similar to the
current work for modeling sequences of events as
n-grams. These methods differ from the current
work in that they do not model entities directly, in-
stead modeling co-occurrence of particular nouns
standing as arguments to particular verbs. Lewis
and Steedman (2013) build clusters of relations
similar to these events, finding such clusters help-
ful to question answering and textual inference.
There has also been recent interest in the related
problem of automatically learning event frames
(Bejan, 2008; Chambers and Jurafsky, 2011; Che-
ung et al., 2013; Chambers, 2013). These ap-
proaches focus on identifying frames for infor-
mation extraction tasks, as opposed to inferring
events directly. Balasubramanian et al. (2013) give
an event frame identification method, developed in
parallel with the current work, using sequences of
tuples similar to our multi-argument events, noting
coherence issues with pair events. Their formu-
lation differs from ours primarily in that they do
not incorporate coreference information into their
event co-occurrence distribution, and evaluate us-
ing human judgments of frame coherence rather
than a narrative cloze test.
6 Future Work
We have evaluated only one type of multi-
argument event inference, in which a script infers
an event given a set of entities and the events in-
volving those entities. We claim that this is the
most natural adaptation of the cloze evaluation to
the multi-argument event setting. However, other
types of inferences would be useful as well for
question-answering. Additional script inferences,
and their applications to question answering, are
worth investigating more fully.
The evaluation methodology used here has two
serious benefits: it is totally automatic, and it does
not require labeled data. The cloze evaluation is
intuitively reasonable: a good script system should
be able to predict stated events as having taken
place. Basic pragmatic reasoning, however, tells
us that the most obvious inferable events are not
typically stated in text. This evaluation thus fails
to capture some of the most important common-
sense inferences. Further investigation into evalu-
ation methodologies for script systems is needed.
7 Conclusion
We described multi-argument events for statisti-
cal scripts, which can directly encode the pair-
wise entity relationships between events in a doc-
ument. We described a script model that can han-
dle the important aspects of the additional com-
plexity introduced by these events, and a baseline
model that can infer multi-argument events using
single-protagonist chains instead of directly mod-
eling full relations. We introduced the novel uni-
gram baseline model for comparison, as well as
the novel accuracy metric, and provided empir-
ical evidence that modeling full multi-argument
events provides more predictive power than mod-
eling event chains individually.
Acknowledgments
Thanks to Katrin Erk, Amelia Harrison, and the
DEFT group at UT Austin for helpful discussions.
Thanks also to the anonymous reviewers for their
helpful comments. This research was supported in
part by the DARPA DEFT program under AFRL
grant FA8750-13-2-0026. Some of our experi-
ments were run on the Mastodon Cluster, sup-
ported by NSF Grant EIA-0303609.
227
References
Corin R Anderson, Pedro Domingos, and Daniel S
Weld. 2002. Relational Markov models and their
application to adaptive web navigation. In Proceed-
ings of the Eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD-2002), pages 143?152.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2012. Rel-grams: a
probabilistic model of relations in text. In Proceed-
ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge
Extraction at NAACL-HLT 2012 (AKBC-WEKEX
2012), pages 101?105.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP-2013).
David Bamman, Brendan O?Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-13), pages 352?361.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of task-
driven human?human dialogs. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING/ACL-
06), pages 201?208.
Cosmin Adrian Bejan. 2008. Unsupervised discov-
ery of event scenarios from texts. In Prodeedings of
the 21st International Florida Artificial Intelligence
Research Society Conference (FLAIRS-2008), pages
124?129.
Kristy Elizabeth Boyer, Robert Phillips, Eun Young
Ha, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2009. Modeling dialogue structure
with adjacency pair analysis and Hidden Markov
Models. In Proceedings of Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Paper (NAACL-
HLT-09 Short), pages 49?52.
Nathanael Chambers and Daniel Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 789?797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 602?610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT-
11), pages 976?986.
Nathanael Chambers. 2013. Event schema induc-
tion with a probabilistic entity-driven model. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2013).
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-13).
Ananlada Chotimongkol. 2008. Learning the struc-
ture of task-oriented conversations from the corpus
of in-domain dialogs. Ph.D. thesis, Carnegie Mellon
University.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources & Evaluation
(LREC-2006), volume 6, pages 449?454.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium.
Bram Jans, Steven Bethard, Ivan Vuli?c, and
Marie Francine Moens. 2012. Skip n-grams
and ranking functions for predicting script events.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-12), pages 336?344.
Kristian Kersting, Luc De Raedt, and Tapani Raiko.
2006. Logical Hidden Markov Models. Journal of
Artificial Intelligence Research, 25:425?456.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179?192.
Boyang Li, Stephen Lee-Urban, Darren Scott Appling,
and Mark O Riedl. 2012. Crowdsourcing narrative
intelligence. Advances in Cognitive Systems, 2:25?
42.
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Prodeed-
ings of the 21st International Florida Artificial In-
telligence Research Society Conference (FLAIRS-
2008), pages 159?164.
228
Neil McIntyre and Mirella Lapata. 2009. Learn-
ing to tell tales: A data-driven approach to story
generation. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 217?225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-10),
pages 1562?1572.
Risto Miikkulainen. 1990. DISCERN: A Distributed
Artificial Neural Network Model of Script Process-
ing and Memory. Ph.D. thesis, University of Cali-
fornia.
Risto Miikkulainen. 1993. Subsymbolic Natural Lan-
guage Processing: An Integrated Model of Scripts,
Lexicon, and Memory. MIT Press, Cambridge, MA.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, MIT-AI Laboratory.
Raymond J. Mooney and Gerald F. DeJong. 1985.
Learning schemata for natural language processing.
In Proceedings of the Ninth International Joint Con-
ference on Artificial Intelligence (IJCAI-85), pages
681?687, Los Angeles, CA, August.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2010), pages
492?501.
Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: the Winograd
schema challenge. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL-12), pages 777?789.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-10), Uppsala, Sweden, July.
David Rumelhart. 1975. Notes on a schema for sto-
ries. Representation and Understanding: Studies in
Cognitive Science.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: An Inquiry into
Human Knowledge Structures. Lawrence Erlbaum
and Associates, Hillsdale, NJ.
229
