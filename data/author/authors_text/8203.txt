 The ARC A3 Project: 
Terminology Acquisition Tools: Evaluation Method and Task 
Widad Mustafa El Hadi 
mustafa@univ-lille3.fr  
I s m a ? l  T i m i m i 
timimi@univ-lille3.fr  
A n n e t t e  B ? g u i n 
beguin@univ-lille3.fr 
Marcilio De Brito 
mdebrito@noos.fr 
 
UFR IDIST & CERSATES (CNRS UMR 8529) 
Universit? Charles De Gaulle, Lille 3 
BP 149, F-59 653 Villeneuve D'Ascq, France 
 
Abstract 
This paper describes the work achieved in 
the Concerted Research Project ARC A3 
supported and coordinated by the AUF1, 
former Aupelf-Uref2. The project deals 
with the evaluation of term and semantic 
relation extraction from corpora in French. 
Eight participants, both from public 
institutions and industrial corporations 
were involved in this project and were 
responsible for producing corpora suitable 
for extraction tasks and elaborating a 
protocol in order to evaluate objectively 
terminology acquisition tools. This 
expression covers respectively, term 
extractors, classifiers and semantic relation 
extractors. The paper also reports on the 
methodology used for comparing four term 
extractors, one classifier and three 
semantic relation extractors during the 
2000 evaluation campaign. There are also 
several by-products of this campaign: first, 
two corpora which can be used for NLP 
system development and evaluation as the 
AUF recommended; and then terminology 
products: for each corpus a list of terms 
characterizing the field is available. We are 
not giving details about the results but 
rather an assessment of what the evaluation 
of Terminology Extraction Tools is: how 
was it done, what were the difficulties, 
which are the advantages and 
disadvantages of the adopted protocol, 
what are the limits and how should we 
proceed for future testing. 
                                                          
1
 The Association des Universit?s Francophones 
2
 AUPELF is the "Association des Universit?s Enti?rement 
ou Partiellement de Langue Fran?aise", an NGO whose 
mission is to promote the dissemination of French as a 
scientific medium. 
1 The ARC A3 Program 
ARC A3 is a project of the ILEC3 group 
coordinated and founded by AUF. It was started 
in 1995 in order to promote research in the field 
of terminology acquisition. The ARC A3, 
?Term and Semantic Relation Extraction from 
Corpora in French? project aim is to test 
software capabilities in term and semantic 
relation extraction from corpora in French. 
Systems submitted to this evaluation are 
designed by French and Canadian research 
institutions (National Scientific Research Center 
and Universities) and/or private businesses. 
These systems have been extensively described 
in our previous work (cf. B?guin, et al, 1997, 
2000; Jouis et al, 1997; Mustafa El Hadi et al, 
1996a, 1996b, 1997 & 1998;). The first phase of 
the project has been directed towards testing the 
systems on one corpus4 (trial run) and towards 
elaborating a workable protocol based on this 
experience. The first results were presented 
during the first conference of JST5 (cf. B?guin et 
al., 1997, 2000). This article reports on the 
second and final evaluation campaign. 
2 ARC A3 Organization 
ARC A3 brings together four kinds of actors: a 
coordinator who plays an organizational role 
(schedule, quality control of corpora, data 
production, etc.), corpora providers; participants 
of the test and two scientific advisors. The 
action has been coordinated by the University of 
Lille 3. The organizing team in cooperation with 
the discussion group made up of representatives 
of each participating team and two scientific 
                                                          
3
 Ing?nierie de la Langue, Linguistique-informatique et 
Corpus ?crits. 
4
 SPIRALE, a periodical dealing with education and 
pedagogy issues. Each periodical sizes around 200 pages. 
5
 Journ?es Scientifiques e Techniques de Francil, Avignon, 
France, 1997. 
 advisors are supposed to co-operate in defining a 
methodology for testing the systems. 
2.1 Participating Systems 
The systems are designed by French and 
Canadian research institutions. There were ten 
registered participants at the beginning of the 
project and three withdrew later for a variety of 
reasons. The organizers then launched another 
call for participation in July 1999 and three 
more participants joined the project (two private 
enterprises (Xerox and Logos) and a public 
institution (the University of Grenoble). Logos 
and the University team later dropped out for 
reasons unrelated to the program. When the final 
campaign was launched in 2000 there were eight 
systems remaining under evaluation. 
 
Fig. 1. Participating Systems 
Software Affiliation 
Acabit IRIN (Nantes) 
Ana IRIN (Nantes) 
Conterm LANCI (Montr?al) 
Iota CLIPS-IMAG (Grenoble) 
Lexter ERSS (Toulouse) 
Seek-Java CAMS-LALIC (Paris 4) 
Loria6 LORIA (Nancy) 
Xerox-Termfinder XEROX (Grenoble) 
 
2.2 Overview of the Tested Tools 
Terminology plays a major role in information 
processing and management and in specialized 
communication. Its role has been enhanced by 
the spread of automation and by the availability 
of electronic corpora. These two factors have 
had a massive impact on many different 
applications: systematic terminology7 building, 
natural-language interface design, lexical units 
management for specific use in some sub-
                                                          
6
 This name is used for practical reasons since no final 
software name has yet been chosen  
7
 A holistic list of terms drawn from a representative corpus 
characterizing and describing a field of knowledge. In 
order to be of any use this type of list must be subject to a 
structuring which is an important step towards exploiting 
extraction results. 
languages and technical writing, thesaurus 
construction, translation and indexing as well as 
the recent growth of cross-language information 
retrieval (CLIR). 
If we focus on the tools, presented in our 
evaluation project, from the point of view of 
their functions and of the purposes for which 
they were designed), there are three categories: 
"Term Extractors", "Classifying Tools", and 
"Semantic relations extraction tools". As we 
already mentioned, these systems were 
extensively described in our previous 
publications. 
2.2.1 Term Extractors (TE) 
We will briefly describe the basic idea 
underlying TE tools. Most of the extracting tools 
consider terms as noun phrases. Systems 
identify terms by using frequency, distribution 
and category-pattern matching (Daille et al 
1995; Dagan, 1996; Lauriston, 1994). All lexical 
units contained in a given text are analyzed and 
matched to patterns (typical forms of 
terminological units) described in rules. More 
term extractors are accounted for elsewhere 
(L'Homme, 1996; Kageura et al, 1996; Dagan 
et al, 1994). Some of the systems described by 
these authors are tested in the framework of our 
valuation project (Acabit, Lexter, and Ana). 
2.2.2 Classifiers and Semantic Relation 
Extractors (SRE) 
Terminology resources are increasingly seen as 
structured data i.e. as a network of terms 
organized by relations. Pure alphabetical lists 
can hardly be used except for bilingual reference 
tools. The variety of tools, their functions and 
the different possible uses offered within the 
framework of ARC A3 shows this need. 
Consequently such lists of terms are quite 
difficult to evaluate except by specialists in the 
relevant fields which makes it a rather 
constraining process. 
Structuring terms by semantic relations or in 
classes is useful for the following applications: 
Index-making for on-line technical 
documentation; browsing; information access 
and retrieval; building thesaurus and ontologies 
for information systems. 
Many applications and extraction methods 
relevant to these tools have been described in 
the literature. The systems tested in the AUF 
framework are geared towards a variety of 
 applications ranging from rough semantic 
relation extraction, through indexing, thesaurus 
construction to knowledge-based system 
modeling (see figure 2). 
Classifiers and semantic relation extractors are 
tested within the same framework as the one 
used for evaluating term extractors. The first 
category is characterized as classifying tools. 
Their role is to build classes of networks of 
terms linked to a major one. This category 
consists of statistical and/or connectionist 
models such as Conterm. It is the only classifier 
tested within the framework of this campaign. 
The second category includes semantic relation 
extractors which focus particularly on semantic 
relations (Iota, Loria and Seek-Java). A 
complete description of all the systems which 
were tested (main characteristics and purposes, 
description as far as approaches are concerned) 
is documented in previous work. 
3 Evaluation paradigm 
Evaluation activities are a corollary of the quick 
development of NLP tools in general and of 
terminology extraction in particular. It thus 
became necessary to evaluate these tools on 
objectively based criteria in order to have a clear 
picture of the state-of-the-art, assess the needs in 
this sector and hence promote research in this 
specific field. Moreover, the principal aim of 
existing testing methods, as reported in the 
literature, is to come across software errors and 
then try to adapt them for a particular user 
environment. 
Evaluation paradigm is basically dependant 
upon two major steps: (i) Creation of textual 
data: raw or tagged corpora and test material. A 
corpus-based research is part of the 
infrastructure for the development of advanced 
language processing applications; (ii) Test and 
comparison of systems on a similar data 
(Cavazza, 1993; Adda et al, 2000). 
3.1 The ARC A3 Evaluation Approach 
The approach we adopted is a black-box 
qualitative approach8 The results are compared 
                                                          
8
 This approach is adopted and validated by the vast 
majority of participants to the test in June 1999. The 
organizers have slightly adapted the protocol because more 
participants joined the ARC after the validation of the 
protocol. 
with the human performance of a task (either 
experts examining results or using reference lists 
or both). Moreover comparisons are made with 
other systems performing the same task. The 
results are finally calculated and translated in 
terms of traditional IR measures9. 
The conventional distinction between black-box 
and glass-box is the following: the former 
considers only system input-out-put relations 
without regard to the specific mechanisms by 
which the outputs were obtained while the latter 
examines the mechanisms linking input and 
output. (Sparck-Jones, 1996 p. 26; King, 1996; 
1999, among many others). 
The qualitative evaluation measures as described 
by Sparck-Jones 1996, pp. 61-122, are based on 
observation or interviewing and are broadly 
designed to obtain a more holistic, less reductive 
or fragmented view of the situation. It is 
moreover more naturalistic. This type of 
evaluation naturally fits an end-free style. In our 
case the quality of the results is evaluated by 
domain experts. We distinguish two types of 
experts: experts for the three applications tested 
(systematic terminology, translation and 
indexing); and experts in the two domains of 
corpora (biotechnology and pedagogy). 
Both quantitative and qualitative approaches are 
goal-oriented, that is focusing on discrepancies 
between performance results and initial system 
requirements. Sparck-Jones points out how the 
two types of measures are deeply interwoven 
although different in their nature: 
- Recall is a quantitative measure of system 
performance while 
- Declared Satisfaction is a qualitative one (i.e. 
such a measure is really qualitative even if the 
result of applying it to a set of users is a 
percentage figure). 
The qualitative approach in the evaluation 
process is the easiest one for end users. It means 
giving a value judgment on how the system 
globally works (Cavazza, 1993; Chaudiron, 
2000). The dominant approach today is towards 
quantitative evaluations which are considered as 
more objective and reproducible than the 
qualitative approach (EAGLES-1 1996; ISLE 
2001). The main attempt of these approaches is 
                                                          
9
 We chose to accompany the qualitative approach (mainly 
based on manual evaluations) by a translation of the 
manual evaluations into numerical scales of values (see 
below for more details). 
 to translate the concepts of relevance and quality 
into numerical data. Statistical approaches such 
as MUC 2 and TREC 3 are frequently used for 
this type of evaluation. (Chaudiron, 2000). 
3.1.1 The merits of a black-box evaluation 
Obviously this approach has its pros and cons. 
But it can be justified on the following basis: 
- Since most developers cannot provide us (as 
test organizers) with their systems, the only way 
was to send them the text corpora and let them 
provide us with the results. A glass-box 
evaluation would have required an examination 
of the systems by the organizers which would 
have been impossible except for Xerox?s 
TermFinder and Logos System?s Knowledge 
Discovery, two commercialized systems. 
- Even if this approach may be criticized on 
account of its subjective side, end-users like it 
because of its usefulness when comparing two 
or more systems which differ in all their 
parameter settings. (Chaudiron 2000; Cavazza 
1993). 
- A black-box evaluation is more oriented 
towards system?s end-user when compared to a 
glass-box evaluation. For the latter the test will 
involve analyzing the system?s functioning by 
looking at its different components. Each 
component is evaluated separately in itself. Such 
an approach allows for spotting and 
understanding the causes of dysfunctional 
results. It is a long term process which requires 
access to the internal parts of the system and an 
understanding of the architecture and global 
strategy of the software. This is obviously a 
developer oriented approach and not an end-
user one (Chaudiron 2000; Cavazza 1993). 
- In spite of its limited scope the evaluation 
protocol we adopted is used in more 
complicated NLP tools, such as MT tools. 
Evaluators examine the systems? output without 
considering the differences between them (cf. 
L?Homme, 2001). Last Spring our team took 
part in a workshop organized by ISSCO 
(University of Geneva) where we and all the 
other participants adopted this approach. 
 
3.2 Elements of the Evaluation Protocol of 
the 2000 Campaign 
3.2.1 Evaluation Task 
The extraction of terms, of classes and of 
semantic relations was necessary to test the tools 
performance in the three following tasks: 
Systematic terminology (characterizing the 
tested corpora); (ii) Translation; (iii) Indexing. 
This means in practice: what is the relevance of 
terms, classes and semantic relations provided 
by the systems being tested? Do the terms, 
classes and semantic relations satisfy minimum 
requirements? Do we need to define a minimum 
level of terms, classes, semantic production? 
Are discrepancies meaningful? For example, it 
could be that most of the systems being tested 
are having qualitatively poor outputs, while only 
one or two produce worthwhile results. Within 
this perspective the idea was to submit the 
results to specialists. We distinguished for the 
purpose of this campaign two types of human 
expertise as we mentioned above. 
3.2.2 Test material 
Evaluation data can normally be divided into 
two different categories (i) representative 
samples of the tested corpora (ii) test material, 
which, in our evaluation framework, is made up 
of both custom-designed lists and real life lists / 
thesaurus. 
3.2.2.1 Corpus 
Two corpora were tested: Spirale10 and INRA11. 
We have chosen a sample representing 10% of 
each corpus: for Spirale n? 19 was chosen. As 
for INRA corpus, the providers of this corpus 
suggested 8 articles (603, 604, 607, 609, 631, 
666, 732, 740). 
3.2.2.2 Reference Lists 
These lists are standard human professional 
results which can be used as performance 
exemplars or norms for comparison. This type 
of data is considered to be a gold standard (see 
SensEval, Kilgarrif 1998; ISLE 2001). 
For the INRA corpus the following lists have 
been created: 
For translation two lists were processed (i) a list 
created by a novice translator (ii) another one by 
a confirmed professional translator. 
                                                          
10
 423 texts, 16 mega bytes  
11
 51 texts, 2,2 mega bytes. 
 For indexing: six lists were created both by 
professional and by non professional indexers. 
We are not developing these lists in this paper 
given the limited scope of this type of evaluation 
from an indexing point of view. Hence the 
limited interest of term extraction tools for 
human indexing. We will however comment on 
the terminology lists provided by the two corpus 
providers, INRA (Institut National pour la 
Recherche Agronomique i.e. National Institute 
for Agronomic Research), the Francis list of 
INIST12) and the translation lists. 
As far as INRA corpus is concerned: 
We think that our evaluation task could have 
given better results if the lists had been more 
representative of a systematic terminology 
activity. For the INRA corpora, for example, 
only 113 terms were chosen by the experts to 
represent their terminology. Our estimation is 
that, 113 terms only constitute a poor 
representation of an activity. It would have been 
a good idea to have specialists establish the lists 
of terms and to compare those to the systems? 
output. Even if this work is time consuming it 
makes for a better evaluation of the systems? 
productivity. As far as indexing is concerned the 
interest of these lists is quite limited and we 
think that a lot of time has been lost in drawing 
them up and even grooming them. From a 
general point of view the tools we have 
considered, especially term extraction ones, only 
have a limited interest for indexing contrary to 
other tools (semantic relation extractors) they 
have not been conceived for this purpose. This 
point of view is shared by their own designers. 
However, some of the semantic extraction tools 
are adapted for indexing among their other 
applications (Iota and Loria, for instance). 
As for Spirale corpus: 
Terminology (i) Thesaurus Mobis, (educational 
sciences section) (ii) Francis list (of the INIST, 
covering the complete volume on educational 
sciences section). 
Three lists for indexing: - Dictionnaire 
encyclop?dique de l'?ducation et de la 
formation13. - CRDP list14 de Lille. - Br?hier list 
(PRCE in documentation ). 
                                                          
12
 INIST is the National Institute of Scientific and 
Technical Information. The list they provided is used to 
index their data-base to complete this part. 
13
 P. Champy et C. Etev?. Index pp 1059-1097. 
14
 Centre R?gional de la Documentation P?dagogique. 
3.2.2.3 Unified Presentation Format 
The protocol we suggested was based on the 
previous evaluation sessions. The layout of 
some results could at times make the task of 
evaluation difficult. In some cases, good graphic 
presentation (conceptual graphs, etc.) could hide 
a poor term extraction and hence influence the 
evaluation. Conversely a system which has the 
capacity to extract relevant terms and semantic 
relations but whose layout is poor can influence 
the evaluation process. To prevent this, 
participants have been asked to adopt a unified 
format for their presentations for 2000 
evaluation campaign. 
3.2.2.4. Non-unified Tagging 
Given the fact that system designers have 
different processing possibilities, some of the 
systems use an independent tagger, others have 
an integrated one which is part and parcel of 
their system. The organizers decided to allow 
the participants their own choice in terms of 
tagging methods. 
3.2.2.5. Evaluation Measures 
Given the three tasks to be performed (indexing, 
systematic terminology and translation), the 
usual notions of recall and precision can be used 
to evaluate the quality of results when matched 
with a manually-produced reference list. 
Performance failure at this level can be 
interpreted in terms of silence and noise (see 
below). 
3.2.2.6. Automatic Matching by EvalTerm 
If the qualitative approach offers the easiest 
form of systems evaluation it nevertheless 
retains two major drawbacks: (i) it makes up for 
a very boring job when there are too many 
results (ii) judgments can easily be slanted by 
the subjective approach of the expert. 
Our protocol being based on the qualitative 
black-box principle where parameters are hard 
to quantify we chose to apply traditional IR 
measures, recall and precision which normally 
accompany qualitative evaluations: 
R = number of correct extractions / number of 
reference extractions. 
P = number of correct extractions / number of 
proposed extractions15 
Since the manual matching of lists proved to be 
long and complicated due to the huge size of the 
                                                          
15
 Or their equivalents in terms of noise and silence: 
Silence = 1 ? Recall, Noise = 1 ? Precision  
 processed data and to a variety of other 
inconveniences, we chose to automatically 
calculate these measures. We then decided to 
duplicate the manual evaluation with its 
conversion into numerical scales of values.  
For this purpose we developed a program which 
matches the results provided by the software 
with the reference lists16 The program compares 
two lists: L1 represents the results given by a 
software and list L2 is a reference list proposed 
by an expert17. The program output consists of 
two files: file (a)
 
which contains the elements of 
L2 found in L1 (the relevant terms which the 
software was able to find). And file (b) which 
contains some elements of L2 which have not 
been identified and consequently were not 
mentioned in L1 (the correct terms not found by 
the software). Through a simple subtraction we 
can get a file containing the noisy terms of each 
software. 
In our automatic matching we have not included 
any linguistic treatment for fear of introducing 
new parameters which would influence the 
results. Right from the beginning we have 
noticed that over-productive systems such as 
Ana or Term Finder are difficult to compare 
with reference lists because the noise rate 
becomes irrelevant. 
4 An Overview of the Results 
4.1 Term Extraction on the two Corpora 
We will now comment globally on how the term 
extractors performed when run on the two 
corpora for the three different tasks (indexing, 
systematic terminology and translation): 
First, automatic matching concurred with human 
experience which notices that the systems 
produce many ? noisy ? terms while on the 
contrary there are many terms not included in 
the reference lists but which the experts 
considered as relevant for systematic 
terminology. Hence the interest of some of these 
? noisy ? terms for enriching and updating 
reference lists and terminology data bases. 
Matching the results of the different systems has 
                                                          
16These lists can be: a) existing lists, real-life lists ( thesauri 
or alphabetical lists, such as Francis List);  b) established 
by the evaluators/indexers (specifically tailored for the 
three tasks, indexing, terminology and translation). 
17
 They are many lists proposed by our experts. 
showed a great similarity between Lexter and 
Acabit. 
As for indexing, if the systems could generally 
provide relevant and effective help for 
terminology (systematic terminology, and 
translation) their contribution to indexing is less 
obvious. Indexing supposes other mental 
operations than those needed for terminology 
construction and simply picking out candidate-
descriptors is not enough to supply a reliable 
form of indexing. 
The three core criteria of good indexing are: 
reliability, selectivity and exhaustiveness. The 
indexer must hold a balance between 
exhaustiveness and selectivity. Having too many 
terms leads to noise and too few to silence. It is 
on this criteria of selectivity that human 
processing varies. 
Softwares based on term extraction offer a large 
number of potential candidate terms, connecting 
them with more or less precise criteria of 
relevance, mostly of a statistical nature. At this 
level of processing the indexer has recourse to 
authorized lists and thesauri i.e. he or she refers 
to the work of terminologists in structuring the 
field and attributing a label to each and every 
concept. The systems which we tried to assess 
are not yet likely to provide a very effective help 
to indexing since the results are over-productive 
in view of the needs. 
5 The Classifier and the Semantic Relation 
Extraction (SRE) Tools 
The protocol we adopted specifies the 
evaluation of semantic relation and class 
validity, coherence and comprehensiveness on 
all of the three tasks (i.e. semantic relations 
examined from the point of view of systematic 
terminology, translation and indexing). The 
classes and semantic relations extracted were 
subject to a comparison with the human 
performance of these tasks (experts and 
reference lists), plus a comparison with other 
systems performing the same task. This 
qualitative evaluation is measured by the 
traditional IR performance measures (silence, 
noise, recall and precision). The first thing we 
can remark on is that it is very difficult to fulfill 
the evaluation within our proposed terms of 
reference. We are presenting hereunder the 
reasons limiting the scope of our protocol when 
applied to SRE results. 
 5.1 An Overview of the Results of SRE on 
the Two Corpora 
What we observed is that these tools are too 
different to allow a useful comparison for the 
following reasons: 
- SRE extract different types of relations and 
hence are incomparable. 
- This difference is linked to the different forms 
of semantic model implementation. Conversely 
some extractors are based on models that will 
not allow the type of relations required for the 
three evaluation tasks. 
- SRE are designed for different functions and 
have different objectives or carry out different 
tasks. 
- These differences are reflected in the type of 
output or results. 
- Another problem came from the fact that 
INRA could not provide us with a structured list 
corresponding to the eight selected texts. Even if 
this list had been available, comparing it to the 
results would have been of limited interest only. 
The remaining solution was to submit the results 
to a field specialist. 
- Difficulties in interpreting the non-labeled 
Semantic Relations. Fig. Two shows these 
differences: 
 
Fig. 2. Synthetic comparison table for SRE 
 IOTA LORIA SEEK-JAVA 
O
bje
ct
iv
es
 
Building 
indexes of 
one or more 
levels (layers) 
for document 
retrieval 
Scientific & 
technical 
watch 
(identifying 
rare or new 
information 
a) Cognitive text 
organization 
b) Extraction of Labeled 
Semantic relations between 
terms in a thesaurus or a 
network of terms 
c)Constructing/modeling 
Knowledge-based systems  
Fu
n
ct
io
n
s 
Information 
seeking 
systems, 
automatic 
extraction of 
complex 
indexes  
Semantic 
relations 
extraction 
Semantic relations 
extraction and 
representation 
Presentation in a conceptual 
graph fashion 
Building relational data 
bases 
O
u
t-
pu
t 
Lists of 
potential 
candidate 
terms ranked 
by with 
frequency 
Terminology 
networks  
non-labeled 
Logico-
Semantic 
Relations 
between terms  
Classes of 
terms  
A descriptive network/graph 
of terms linked with 
semantic relations between 
complex or simple terms, 
on the one hand and a triplet 
of argument-relation-
argument on the other 
assembled in a relational 
data-base 
- Moreover, it is difficult or even impossible to 
measure silence using a protocol based on IR 
systems performance measure. 
? Without a prior knowledge of the missing 
possible relations one cannot account for the 
silence measure. 
? To account for noise, a thorough knowledge 
of both the semantic model and the field of 
knowledge is required. 
? These observation are also valid for recall 
and precision measures. 
We can thus say for the time being that SRE 
cannot be assessed by the protocol since their 
results cannot be matched. 
The field specialist18 gave the following account: 
?It is essential to have an interface to manipulate 
and interpret the relations. Everything seemed 
somewhat inconclusive. At times the relation 
?fits well?, at times it does not at all. Results are 
not always relevant and it is difficult to trust this 
type of analysis on its own if one is not at the 
same time be conversant with the domain, since 
some of the relations can be wrong. 
For Iota, concept extraction seems generally 
quite relevant. However one has to wonder 
about the relevance of a number of extracted 
concepts which are not at all relevant to the 
field. How did these non-specific concepts get 
extracted more easily than others ? 
As for the table on Conceptual Semantic 
Dependence19 it is hard to draw any conclusions 
from it since it offers only one semantic label for 
any relation. 
The Iota approach is more global than the Seek-
Java one since the relations are based on the 
whole document and not only at the level of one 
sentence. These two softwares are thus difficult 
to compare since their purpose is not the same?. 
5.2 Conterm, the Classifier: an ad hoc 
Evaluation 
Given the difficulties we listed above and the 
fact that it was impossible to compare Conterm 
with other systems performing the same task. 
The only possible evaluation for Conterm would 
have been a progress evaluation for this sole 
classifier of the campaign20. This problem shows 
again the limits of our Protocol. The Conterm 
lists were matched to an automatically produced 
                                                          
18
 Patricia Volland-Neil, from INRA-Tours 
19
 The evaluator is referring to the tables accompanying the 
results provided by the system?s designer. 
20
 The protocol is not suitable for its evaluation. After the 
withdrawal of another participant who had also presented a 
classifier, only this one remained. 
 untagged list of terms which corresponds to the 
eight texts of the INRA corpus. The most 
important element in its evaluation is not that we 
matched its results with a tagged list but that the 
results had been matched with indexers? and/or 
experts lists and that we could observe the 
correspondence between Conterm?s output and 
the lists. It does not mean that Conterm is good 
for indexing but that the classes suggested by 
this tool embody conceptual attributes which are 
close to the logic underlying the human 
selection of candidate-terms suitable for 
indexing, namely its rich lexico-semantic 
network. 
6 Concluding Remarks 
- This evaluating action provided us with an 
awareness of the State-of-the-art in the field of 
terminology acquisition tools. It also allowed us 
to test evaluation paradigms, demonstrating how 
difficult it was to apply a single evaluation 
protocol to a variety of systems operating along 
different lines. 
- The discussions among participants aiming at 
the creation of a testing protocol resulted in the 
definition of an evaluation procedure and in an 
assessment of their relative merits. The 
comparative study of the systems? out-put also 
enabled a better understanding of the 
performances of the wide range of techniques 
involved. As by-products of the project two 
corpora can be used in further evaluation 
campaigns and a set of material tests (real-life 
and constructed or specifically tailored one that 
can be shared during future evaluations). 
- The evaluation results can be used predictively 
for system design, development or modification 
The limits of our evaluation approach can be 
sketched in the following manner: 
- If the adopted protocol based upon reference 
list can be applicable to the two tasks 
(translation and terminology) it is hardly 
applicable to indexing tasks. 
- It is not adequate to account neither for the 
classifiers nor for the SRE. 
- Several questions remain unanswered: 
a) first, is it possible to fully automate 
evaluation procedures? Then is it possible to 
abandon test material, such as reference lists or 
other type of human-made data, which are 
considered as a kind of gold standard reusable 
for other evaluation campaigns? (see our recent 
experience in MT evaluation workshop, April 
200121. 
b) As far as semantic relation extraction is 
concerned, is it possible to automate SRE 
valuation procedure in the way Grefensttete 
(1994) does? 
7 Future Directions 
1. Exploiting Results: the Campaign?s Side 
Benefits: 
Full treatment of the Spirale corpus will allow 
the creation of an index of all the reviews past 
numbers, which fulfills  the moral contract made 
with its Editorial Board in exchange for getting 
the corpus free of charge. In addition, these 
results can help broaden the terminological 
repository for the education sciences, especially 
in drawing up the Francis Thesaurus which 
covers all education sciences. 
2. Towards Trans-Systemic Integration: The 
output of the systems are divergent but can in 
some cases be complementary. In fact the 
preliminary results drawn from the first 
evaluation in 1997 (cf. B?guin et al 2000) have 
led us to consider the feasibility of trans-
systemic integration for strengthening their 
automatic terms identification capabilities. The 
idea is to combine two or three different types of 
systems in order to specify various integrated 
production processes. Systems could 
                                                          
21
 ?Setting a methodology for Machine Translation 
evaluation?. The context: evaluation of a translation 
made by an MT System on the following source text: 
INRA corpus text N?604 ? corpus biotechnologique 
sur la reproduction chez l?animal ? Source language: 
French - Target language: English. We carried  out 
some manual testing but with the objective of setting 
a rough methodology that might be irrelevant for 
translating huge size corpora. The tool we used was a 
non interactive French / English MT System with a 
basic French/English dictionary that does not include 
any specific terminology. We had two indexes (a 
French index and an English index of domain 
specific expressions, but they are not aligned). They 
have been provided by the INRA and considered as 
gold standard. We used the indexes to create a 
specific dictionary in order to feed the MT systems 
with this specific lexical data. The next step is to 
assess the impact of specific terminology when 
integrated to an MT system by comparing the results 
of the two translations we get: with and without 
specific terminology. 
 increasingly be seen as parts of these integrated 
production processes. 
3. Towards User-Oriented Evaluations: in the 
light of the results obtained in this campaign the 
most suitable type of evaluation would be a 
user-oriented one. Other types of approaches22 
can be designed, such as adequacy evaluation23 
which can to some extent be adopted for our 
case but we have to define a more strict user 
profile. 
4. Towards developing interfaces for validating 
the results: even if we opted for a unified 
presentation format for the reasons mentioned in 
section 3.2.2.3, we however think it is essential  
for future campaign organizers to have an 
interface to manipulate and interpret the results 
(validating term, relations and classes). This 
type of interface can dramatically facilitate the 
interaction with the evaluators and the end-user 
of these tools. 
5. Designing tools for generic bi-lingual 
production, allowing ad hoc extractions through 
ad hoc interfaces. 
6. Capability to share resources in the future 
(test material such as gold standard lists, real-
life and/or constructed ones). 
7. Developing automatic evaluation tools such 
as Evalterm which can be reused in similar 
future evaluations. 
8. Hypothesis are still to be tested for semantic 
relations extraction: results of the various 
semantic extractors will be of different quality 
depending on the type and nature of corpora 
(domain and genre) chosen (cf. also Condamines 
et al 98; Davidson et al98, among many 
others). 
8 Acknowledgements 
The authors gratefully acknowledge the 
financial assistance provided by the AUF 
(Association des Universit?s Francophones) in 
funding the general research project within 
which this paper was written. 
 
                                                          
22
 Spark-Jones et al 1996; King 1999, among many others, 
identified more three types of evaluation processes: the 
progress evaluation,  the adequacy evaluation and the 
diagnostic evaluation. The first and second types are used 
for comparative benchmarking. 
23
 Adequacy evaluation aims at finding out whether a 
system or product is adequate to someone?s needs. This 
type is typically done when thinking of acquiring a system. 
9. References 
Adda, G., Lecompte, J., Mariani, J., Paroubek, P., 
Rajman, M. (2000). Les proc?dures de mesure 
automatique de l?action GRACE pour l??valuation 
des assignateurs de partie du discours pour le 
fran?ais, Chibout, K., Mariani, J., Masson, N., 
Neel, F. ?ds., (2000). Ressources et ?valuation en 
ing?nierie de la langue, Duculot, Coll. Champs 
linguistiques, et Collection Universit?s 
Francophones (AUF), pp. 645-664. 
B?guin, A, Jouis, C, Mustafa El Hadi , W, (1997): 
"Evaluation d?outils d?aide ? la construction de 
terminologie et de relations s?mantiques entre 
termes ? partir de corpus", In JST'97, FRANCIL, 
AUPELF-UREF, Avignon, avril 1997, pp. 419-
426. This article is published in Chibout et al 
2001 (eds.) pp 161-179. 
B?guin, A., Jouis, Ch., Mustafa Elhadi, W. (2000). 
Evaluation d?outils d?aide ? l?extraction et ? la 
construction automatiques de termes et de relations 
s?mantiques. In Chibout, K., Mariani, J., Masson, 
N., Neel, F. ?ds., (2000). Ressources et ?valuation 
en ing?nierie de la langue, Duculot, Coll. Champs 
linguistiques, et Collection Universit?s 
Francophones (AUF), pp 161-179. 
Bourigault, D. (1993). ?Analyse syntaxique locale 
pour le rep?rage de termes complexes dans un 
texte?, Traitement automatique des langues 34(2), 
pp. 105-117. 
Bourigault, D., Jacquemin Ch. &. L?Homme M-C. 
?ds. (1998). Computerm ?98, First Workshop on 
Computational Terminology, COLING-ACL?98, 
15 August 1998. 
Bruandet, M.F. (1989). Outline of a knowledge base 
model for an intelligent Information Retrieval 
system. In Information Processing and 
management, Vol 25, N? 3, 1989. 
Cavazza, M., (1993). M?thodes d'?valuation des 
logiciels incorporant des technologies 
d'informatique linguistique, Paris, Rapport MRE-
DIST, 1993. 
Chaudiron, S. (2000). The Relevance of Quality 
Model for NLP Applications, in Proceedings of 
RIAO, pp 1568-1577, Paris 12-I4 April 2000. 
Condamines, A. et Rebeyrolle, J. (1998). CTKB: A 
corpus-based approch to a Terminological 
Knowledge Base, In Bourrigault, D., Jacquemin 
Ch. &. L?Homme M-C, ?ds.(1998). 
Computerm?98, First Workshop on Computational 
Terminology, COLING-ACL?98, 15 August 1998, 
pp. 29-35. 
Dagan. I. and Church, K. (1994). Termight: 
Identifying and Translating Technical 
 Terminology. In: Proceedings of 4th Applied NLP 
Conference 1994, 34-40. 
Daille, B., (1994). ACABIT: une maquette d?aide ? 
la construction automatique de banques 
terminologiques monolingues ou bilingues. In 
Class, A., Thoiron, P, , B?joint (eds) 
Lexicomatique et Dictionnairiques, pp. 123-136, 
Beyrouth 1996. 
Daille, B. B. Habert, C. Jacquemin et J. Royaut? 
(1995). ?Empirical Observation of Term Variation 
and Principles for their Description?, Terminology 
3(2), pp. 197-257. 
Davidson, L., Kavanagh, J., Mackintoch, K., Meyer, 
I., Skuce, D. (1998). Semi-Automatic Extraction of 
Knowledge-Rich Contents from Corpora, In 
Bourrigault, D., Jacquemin, Ch. &. L?Homme M-
C, ?ds. (1998). Computerm?98, First Workshop on 
Computational Terminology, COLING-ACL?98, 
15 August 1998, pp. 51-56.  
EAGLES (1996)  
http://www.issco.unige.ch/projects/eagles. 
Enguehard, C. (1993). Acquisition de terminologie ? 
partir de gros corpus. Informatique et langue 
naturelle ILN? 93, Nantes, pp. 373-384, d?cembre 
1993.  
Grefenstette, G. (1994). Explorations in Automatic 
Thesaurus Discovery, Boston: Kluwer Academic- 
Press. 
[Gaussier, E. (1995). Mod?les statistiques et patrons 
morphosyntaxiques pour l?extraction de lexiques 
bilingues de termes, Th?se de doctorat, Paris: 
Universit? Paris VII. 
ISLE (2001) MT Evaluation Classification, 
Expanded Classification (2001). 
http://www.isi.edu/natural-language/mteval/2b-
MT-classification.htm. 
Jouis, C., (1993). Contribution ? la Conceptualisation 
et ? la Mod?lisation des connaissances ? partir 
d'une analyse linguistique de textes. R?alisation 
d'un prototype: le syst?me SEEK. Th?se de 
doctorat. EHESS. Paris. 1993. 
Jouis, C., Mustafa El Hadi, W. (1997), AUPELF 
Project: Term and Semantic Relation Extraction 
Tools. Evaluation Paradigms, In Proc. of the 
Speech and Language Technology Club Workshop 
? Evaluation in Speech and Language 
Technology ?, Univ. of Sheffied, June 17-18, 
Sheffield, UK, pp. 106-113. 
Kageura, K. and Umino, B. (1996). Methods of 
Automatic Term Recognition: A Review. In: 
Terminology Vol. 3(2), 1996, 259-289. 
Kilgarrif, A., Rosenzweig, J. (1998). English 
SENSVAL: Reports and Results. 
King (1999) EAGLES Evaluation Working Group, 
report, http://www.issco.unige.ch/projects/eagles. 
King M. (1996) EAGLES, Workshop, University of 
Geneva http://www.issco.unige.ch/projects/eagles. 
Le Priol, F, Chavallet, J-P., Bruandet, M-F., Descl?s, 
J-P. (1998). Int?gration d?un syst?me statistique 
(IOTA) et d?un syst?me s?mantique (SEEK) dans 
une cha?ne de traitement permettant l?extraction de 
terminologies. Actes Ing?nierie des Connaissances, 
Pont-?-Mousson, pages 33-40. 1998. 
L?Homme, Marie-Claude, Benali, Loubna, Bertrand, 
Claudine and Lauduique, Claudine. (1996). 
Definition of an Evaluation Grid for Term-
Extraction Software. In: Terminology Vol. 3(2), 
1996, 291-312.
 
L?Homme M-C. (2001). ?valuation d?outils d?aide ? 
la construction automatique de terminologie et de 
relations s?mantiques entre termes ? partir de 
corpus ARC-A3 Rapport final, Montr?al, mars 
2001. 
Lauriston, A. (1994). Automatic Recognition of 
Complex Terms: Problems and the TERMINO 
Solution. In: Terminology 1(1), 147-170. 
Manzi S. (1999) Test Material for Evaluation, Sandra 
Manzi, ISSCO - University of Geneva 
http://www.issco.unige.ch/projects/eagles/ewg99. 
Mariani, J. Masson, N., Neel, F., ?ds. (2000). 
Ressources et ?valuation en ing?nierie de la 
langue, Duculot, Coll. Champs linguistiques, et 
Collection Universit?s Francophones (AUF), 2000, 
pp. 13-24, actes des 1?res Journ?es Francil autour 
du th?me: L'ing?nierie de la Langue: de la 
Recherche au produit, Avignon 15-16 avril 1997. 
MUC-3, (1991). Proceedings of the Third Message 
Understanding Conference. Morgan Kaufmann. 
MUC-4, (1992). Proceedings of the Fourth Message 
Understanding Conference. Morgan Kaufmann. 
Mustafa El Hadi, W. & Jouis, C. (1998), 
Terminology Extraction and Acquisition from 
Textual Data: Criteria for Evaluating Tools and 
Method, Proceedings of the First International 
Conference on Language Resources and 
Evaluation, Grenada, Spain may 1998, pp. 11750-
1178. 
Mustafa El Hadi, W. & Jouis, C. (1997), "Natural 
language processing techniques and their use in 
data modeling and information retrieval". In: 
Proceedings of the sixth international study 
conference on classification research, Knowledge 
Organization for Information Retrieval, University 
College London, London, 16-18 June 1997. The 
Hague: FID, 157-161. 
Mustafa El Hadi, W., Jouis, C. (1996a), Evaluating 
Natural Language Processing Systems as a Tool 
for Building Terminological Databases, In 
Proceedings of the Fourth International ISKO 
Conference: Knowledge Organization and 
 Change, Washington, Library of Congress, July 
1996, Advances in Knowledge Organization, Vol. 
5, INDEX Verlag, Frankfurt/Main, pp. 346-355. 
Mustafa El Hadi, W., Jouis, C. (1996b), Natural 
Language Processing-based Systems for 
Terminological Construction and their 
Contribution to Information Retrieval. In 
Proceedings of the Fourth International Congress 
on Terminology and Knowledge Engineering 
(TKE'96), Vienna, INDEX Verlag, 
Frankfurt/Main. pp. 118-130. 
Seffah, A. Meunier, J.G. (1995). ALADIN :un atelier 
orient? objet pour l?analyse et la lecture de textes 
assist?e par ordinateur. In : International 
Conference on statistics and texts  
Sparck-Jones K., Gallier, J.R. (1996). Evaluating 
Natural Language Processing Systems: An Analysis 
and Review, Springer, Berlin. 
Toussaint, Y. Namer F., Daille, B., Jacquemein, C., 
Royaut?, J., Hathou N., (1998). Une approche 
linguistique et statistique pour l?analyse de 
l?information en corpus. In : TALN? 98, Paris, 
France, 1998. 
Work-in-Progress  project report : CESTA - Machine Translation Evaluation 
Campaign 
 
Widad Mustafa El Hadi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
mustafa@univ-lille3.fr  
Marianne Dabbadie 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
dabbadie@univ-lille3.fr 
Isma?l Timimi 
IDIST / CERSATES 
Universit? de Lille 3 
Domaine universitaire 
du "Pont de Bois" 
rue du Barreau 
BP 149 
59653 Villeneuve d'Ascq 
Cedex - France 
timimi@univ-lille3.fr  
Martin Rajman 
LIA 
Ecole 
Polytechnique 
F?d?rale de Lausanne 
B?t. INR 
CH-1015 Lausanne 
Switzerland 
martin.rajman@epfl.ch 
 
 
Philippe Langlais 
RALI / DIRO - 
Universit? de Montr?al 
C.P. 6128, 
succursale Centre-ville 
Montr?al (Qu?bec) - 
Canada, H3C 3J7 
felipe@IRO.UMontreal.
CA
Antony Hartley 
University of Leeds 
Centre for Translation 
Studies 
Woodhouse Lane 
LEEDS LS2 9JT 
UK 
a.hartley@leeds.ac.uk
Andrei Popescu Belis 
University of Geneva 40 
bvd du Pont d'Arve CH-
1211 Geneva 4 
Switzerland 
Andrei.Popescu-
Belis@issco.unige.ch  
 
Abstract 
CESTA, the first European Campaign 
dedicated to MT Evaluation, is a project 
labelled by the French Technolangue action. 
CESTA provides an evaluation of six 
commercial and academic MT systems using a 
protocol set by an international panel of 
experts. CESTA aims at producing reusable 
resources and information about reliability of 
the metrics. Two runs will be carried out: one 
using the system?s basic dictionary, another 
after terminological adaptation. Evaluation 
task, test material, resources, evaluation 
measures, metrics, will be detailed in the full 
paper. The protocol is the combination of a 
contrastive reference to: IBM ?BLEU? 
protocol (Papineni, K., S. Roukos, T. Ward 
and Z. Wei-Jing, 2001); ?BLANC? protocol 
derived from (Hartley, Rajman, 2002).; 
?ROUGE? protocol (Babych, Hartley, Atwell, 
2003). The results of the campaign will be 
published in a final report and be the object of 
two intermediary and final workshops. 
1 Introduction 
1.1 CESTA and the Technolangue Action in 
France 
 
This article is a collective paper written by the 
CESTA scientific committee that aims at 
presenting the CESTA evaluation campaign, a 
project labelled in 2002 by the French Ministry of 
Research and Education within the framework of 
the Technolangue call for projects and integrated 
to the EVALDA evaluation platform. It reports 
work in progress and therefore is the description of 
an on-going campaign for which system results are 
not yet available.  
 
In France, EVALDA is the new Evaluation 
platform, a joint venture between the French 
Ministry of Research and Technology and ELRA 
(European Language Resources and Evaluation 
Association, Paris, France). Within the framework 
of this initiative eight evaluation projets are being 
conducted:  ARCADE II: campagne d??valuation 
de l?alignement de corpus multilingues; CESART:
 campagne d'Evaluation de Syst?mes 
d?Acquisition de Ressources Terminologiques; 
CESTA : campagne d'Evaluation de Syst?mes de 
Traduction automatique; Easy: Evaluation des 
Analyseurs Syntaxiques du fran?ais; Campagne 
EQueR, Evaluation en question-r?ponse; 
Campagne ESTER, Evaluation de transcriptions 
d??missions radio; Campagne EvaSY, Evaluation 
en synth?se vocale; and Campagne MEDIA, 
Evaluation du dialogue hors et en contexte. 
 
Regarding evaluation, the objectives of the 
Action as Joseph Mariani pointed out in his 
presentation at the LREC 2002 conference are to: 
? Improve the present evaluation 
methodologies  
? Identify new (quantitative and qualitative) 
approaches for already evaluated 
technologies:  socio-technical and psycho-
cognitive aspects  
? Identify protocols for new technologies 
and applications  
? Identification of language resources 
relevant for evaluation (to promote the 
development of new linguistic resources 
for those languages and domains where 
they do not exist yet, or only exist in a 
prototype stage, or exist but cannot be 
made available to the interested users); 
 
The object of the CESTA campaign is twofold. 
It is on the one hand to provide an evaluation of 
commercial Machine Translation Systems and on 
the other hand, to work collectively on the setting 
of a new reusable Machine Translation Evaluation 
protocol that is both user oriented and accounts for 
the necessity to use semantic metrics in order to 
make available a high quality reusable machine 
translation protocol to system providers.  
 
1.2 Object of the campaign 
The object of the CESTA campaign is to 
evaluate technologies together with metrics, i.e. to 
contribute to the setting of a state of the art within 
the field of Machine Translation systems 
evaluation.  
1.3 CESTA user oriented protocol 
The campaign will last three years, starting 
from January 2003. A board of European 
experts are members of CESTA Scientific 
committee and have been working together in 
order to determine the protocol to use for the 
campaign. Six systems are being evaluated. 
Five of these systems are commercial MT 
systems and one is a prototype developed at 
the university of Montreal by the RALI 
research centre. Evaluation is carried out on 
text rather than sentences. Text approximate 
width will be 400 words. Two runs will be 
carried out. For industrial reasons, systems 
will be made anonymous. 
 
2 State-of-the-art in the field of Machine 
Translation evaluation 
 
In 1966, the ALPAC report draws light on the 
limits of Machine Translation systems. In 1979, 
the Van Slype report presented a study dedicated to 
Machine Translation metrics.  
 
In 1992, the JEIDA campaign puts the user at the 
center of evaluator?s preoccupation. JEIDA 
proposed to draw human measures on the basis of 
three questionnaires: 
? One destined to users (containing a 
hundred questions) 
? Other questionnaires are destined to 
system Machine translation systems 
editors (three different questionnaires),  
? And a set of other questionnaires reserved 
to Machine Translation systems 
developers.  
 
Scores are worked out on the background of 
fourteen categories of questions. From these 
scores, graphs are produced according to the 
answers obtained. A comparison of different 
graphs for each systems is used as a basis for 
systems classification. 
 
The first DARPA Machine Translation 
evaluation campaign (1992-1994) makes use of 
human judgments. It is a very expensive method 
but interesting however, as regards the reliability 
of the evaluation thus produced. This campaign is 
based on tests carried out from French, Spanish 
and Japanese as source languages and English as a 
target language. The measures used for each of the 
following criteria are:  
? Fidelity ? a proximity distance is worked 
out between a source sentence and a target 
sentence on a 1 to 5 scale. 
? Intelligibility, that corresponds to 
linguistic acceptability of a translation is 
measured on a 1 to 5 evaluation scale. 
? Informativeness: the test is carried out on 
reading of the target text alone. A 
questionnaire on text informative content 
is displayed allowing to work out a 
measure calculated on the basis of the 
percentage of good answers provided in 
system translation.  
 
In 1995, the OVUM report proposes to compare 
commercial Machine Translation systems on the 
basis of ten criteria. 
 
In 1996, the EAGLES report (EAGLES, 1999) 
sets new standards for Natural Language 
Processing software evaluation on the background 
of ISO 9126.  
 
Initiated in 1999, and coordinated by Pr Antonio 
Zampolli, the ISLE project is divided into three 
working groups, one being a Machine Translation 
group.  
 
Starting from ISO 9126 standard (King, 1999b), 
the aim of the project is to produce two taxonomies 
(c.f. section 3 of this article) and : 
? One defining quality subcriteria with the 
aim of refining the six criteria defined by 
ISO 9126 (i.e. functionality, reliability, 
user-friendliness, efficiency, maintenance 
portability) 
? The second one specifying use contexts 
that define the type of task induced the use 
of a by Machine Translation system, the 
types of users and input data. This 
taxonomy uses contextual parameters to 
select and order the quality criteria subject 
to evaluation. This taxonomy can be 
viewed and downloaded on the ISSCO 
website at the following address : 
http://www.issco.unige.ch/projects/isle/fe
mti/  
 
The second DARPA campaign (Papineni, K., S. 
Roukos, T. Ward and Z. Wei-Jing, 2001), making 
use of the IBM BLEU metric is mentioned in the 
CESTA protocol (c.f. section 8.1 of this article). 
 
3 User-oriented evaluations 
An emerging evaluation methodology in NLP 
technology focuses on quality requirements 
analysis. The needs and consequently the 
satisfaction of end-users, and this will depend on 
the tasks and expected results requirement 
domains, which we have identified as diagnostic 
quality dimensions. One of the most suitable 
methods in this type of evaluation is the adequacy 
evaluation that aims at finding out whether a 
system or product is adequate to someone?s needs 
(see Sparck-Jones & Gallier, 1996 and King, 1996 
among many others for a more detailed discussion 
of these issues). This approach encourages 
communication between users and developers. 
 
The definition of the CESTA evaluation 
protocol took into account the Framework for 
MT Evaluation in ISLE (FEMTI), available 
online. FEMTI offers the possibility to define 
evaluation requirements, then to select relevant 
'qualities', and the metrics commonly used to 
score them (cf. ISO/IEC 9126, 14598). The 
CESTA evaluation methodology is founded on 
a black box approach.  
 
CESTA evaluators considered a generic user, 
which is interested in general-purpose, ready-
to-use translations, preferably using an off-the-
shelf system. In addition, CESTA aims at 
producing reusable resources, and providing 
information about the reliability of the metrics 
(validation), while being cost-effective and 
fast.  
 
With these evaluation requirements in mind 
(FEMTI-1), it appears that the relevant 
qualities (FEMTI-2) are 'suitability', 'accuracy' 
and 'well-formedness'. Automated metrics best 
meet the CESTA needs for reusability, among 
which BLEU, X-score and D-score (chosen for 
internal reasons). Their validation requires the 
comparison of their scores with recognised 
human scores for the same qualities (e.g., 
human assessment of fidelity or fluency). 
'Efficiency', measured through post-editing 
time, was also discussed. For the evaluation, 
first a general-purpose dictionary could be 
used, then a domain-specific one. 
 
 
3.1 An approach based on use cases 
 
ISO 14598 directives for evaluators put forth as 
a prequisite for systems development the detailed 
identification of user needs that ought to be 
specified through the use case document. 
Moreover, conducting a full evaluation process 
involves going through the establishment of an 
evaluation requirements document. ISO 14598 
document specifies that quality requirements 
should be identified ?according to user needs, 
application area and experience, software integrity 
and experience, regulations, law, required 
standards, etc.?. 
 
The evaluation specification document is created 
using the Software Requirement Specifications 
(SRS) and the Use-Case document. The CESTA 
protocol relies on a use case that refers to a 
translation need grounded on basic syntactic 
correctness and simple understanding of a text, as 
required by information watch tasks for example, 
and excludes making a direct use of the text for 
post editing purposes.  
4 Two campaigns 
4.1 Specificities of the CESTA campaign 
Two campaigns are being organised : 
The first campaign is organised using a system?s 
default dictionary. After systems terminological 
adaptation a second campaign will be organised. 
Two studies previously carried out and presented 
respectively at the 2001 MT Summit (Mustafa El 
Hadi, Dabbadie, Timimi, 2001) and at the 2002 
LREC conference (Mustafa Mustafa El Hadi, 
Dabbadie, Timimi, 2002) allowed us to realise the 
gap in terms in terms of quality between results 
obtained on target text after terminological 
enrichment.  
 
4.2 First campaign 
The organisation of the campaign implies going 
through several steps : 
? Identification of potential participants 
? Original protocol readjustement, 
? The setting of a specific test tool that is 
currently being be implemented in 
conformity with protocol specifications 
validated by CESTA scientific 
committee. CESTA protocol 
specifications have been 
communicated to participants in 
particular as regards data formatting, 
test schedule, metrics and adaptation 
phase. For cost requirements, CESTA 
will not include a training phase. The 
first run will start during autumn 2004 
 
4.3 Second campaign 
The systems having already been tuned, an 
adaptation phase will not be carried out for the 
second campaign. However terminological 
adaptation will be necessary at this stage. The 
second series of tests being carried out on a 
thematically homogeneous corpus, the thematic 
domain only will be communicated to participants 
for terminological adaptation. For thematic  
adaptation, and in order to avoid system 
optimisation after the first series of tests, a new 
domain specific 200.000 word hiding corpus will 
be used.  
 
The terminological domain on which evaluation 
will be carried out will then have to be defined. 
This terminological domain will be communicated 
to participants but not the corpus used itself. On 
the other hand, participants will be asked to send 
organisers a written agreement by which they will 
commit themselves to provide organisers with any 
relevant information regarding system tuning and 
specific adaptations that have made on each of the 
participating MT systems, in order to allow the 
scientific committee to understand and analyse the 
origin of the potential system ranking changes. The 
second run will start during year 2005. 
 
Organisers have committed themselves not to 
publish the results between the two campaigns. 
 
After the training phase, the second campaign 
will take place. Participants will be given a fifteen 
days delay to send the results. An additional three 
months period will be necessary to carry out result 
analysis and prepare data publication and 
workshop organisation.  
 
CESTA scientific committee also decided in 
parallel with the two campaigns, to evaluate 
systems capacity to process formatted texts 
including images and HTML tags. Participants 
who do not wish to participate to this additional 
test have informed the scientific committee. Most 
of the time the reason is that their system is only 
capable of processing raw text. This is the case 
mainly for academic systems involved in the 
campaign, most of the commercial systems being 
nowadays able to process formatted text. 
 
5 Contrastive evaluation 
One of the particularities of the CESTA protocol 
is to provide a Meta evaluation of the automated 
metrics used for the campaign ? a kind of state of 
the art of evaluation metrics. The robustness of the 
metrics will be tested on minor language pairs 
through a contrastive evaluation against human 
judgement.  
 
The scientific committee has decided to use 
Arabic?French as a minor language pair. 
Evaluation on the minor language pair will be 
carried directly on two of the participating systems 
and using English as a pivotal language on the 
other systems. Translation through a pivotal 
language will then be the following : 
Arabic?English?French.  
 
Organiser are, of course, perfectly aware of the 
potential loss of quality provoked by the use of a 
pivotal language but recall however that, contrarily 
to the major language pair, evaluation carried out 
on the minor language pair through a pivotal 
system will not be used to evaluate these systems 
themselves, but metric robustness. Results of 
metric evaluation and systems evaluation will, of 
course, be obtained and disseminated separately. 
 
During the tests of the first campaign, the 
French?English system obtaining the best ranking 
will be selected to be used as a pivotal system for 
metrics robustness Meta evaluation.  
 
6 Test material 
The required material is a set of corpora as 
detailed in the following section and a test tool that 
will be implemented according to metrics 
requirements and under the responsibility of 
CESTA organisers. 
6.1 Corpus 
 
The evaluation corpus is composed of 50 texts, 
each text length is 400 words to be translated 
twice, considering that a translation already exists 
in the original corpus. The different corpora are 
provided by ELRA. The masking corpus has 
250.000 words and must be thematically 
homogeneous.  
 
 
For each language pair the following corpora 
will be used: 
 
Adaptation 
? This 200.000 ? 250.000 word corpus is a 
bilingual corpus. It is used to validate 
exchanges between organisers and 
participants and for system tuning.  
First Campaign 
? One 20.000 word evaluation corpus will be 
used (50 texts of 400 words each) 
? One 200.000 to 250.000 word masking 
corpus that hides the evaluation corpus. 
Second campaign 
? One new 20.000 word corpus will be used 
but it will have to be thematically 
homogeneous (on a specific domain that 
will be communicated to participants a few 
months before the run takes place) 
? One masking corpus similar to the 
previous one. 
 
Additional requirement 
The BLANC metric requires the use of a 
bilingual aligned corpus at document scale. 
 
Three human translations will be used for each 
of the evaluation source texts. Considering that the 
corpora used, already provide one official 
translations, only two additional human 
translations will be necessary. These translations 
will be carried out under the organisers 
responsibility. Within the framework of CESTA 
use cases, evaluation is not made in order to obtain 
a ready to publish target language translation, but 
rather to provide a foreign user a simple access to 
information within the limits of basic grammatical 
correctness, as already mentioned in this article.  
 
7 The BLEU, BLANC and ROUGE metrics 
 
Three types of metrics will be tested on the 
corpus, the CESTA protocol being the combination 
of a contrastive reference to three different 
protocols:  
 
7.1 The IBM ?BLEU? protocol (Papineni, K., 
S. Roukos, T. Ward and Z. Wei-Jing, 
2001). 
 
The IBM BLEU metric used by the DARPA for 
its 2001 evaluation campaign, uses co-occurrence 
measures based on N-Grams. The translation in 
English of 80 Chinese source documents by six 
different commercial Machine Translation 
systems, was submitted to evaluation. From a 
reference corpus of translations made by experts, 
this metric works out quality measures according 
to a distance calculated between an automatically 
produced translation and the reference translation 
corpus based on shared N-grams (n=1,2,3?). The 
results of this evaluation are then compared to 
human judgments. 
? NIST now offers an online evaluation of 
MT systems performance, i.e.:  
o A program that can be 
downloaded for research aims. 
The user then provides source 
texts and reference translations for 
a determined pair of languages. 
o An e-mail evaluation service, for 
more formal evaluations. Results 
can be obtained in a few minutes. 
 
7.2 The ?BLANC? protocol 
 
It is a metric derived from a study presented at 
the LREC 2002 conference (Hartley A., Rajman 
M., 2002). We only take into account a part of the 
protocol described in the referred paper, i.e. the X 
score, that corresponds to grammatical correctness.  
 
We will not give an exhaustive description of 
this experience and shall only detail the elements 
that are relevant to the CESTA evaluation protocol.  
 
The protocol has been tested on the following 
languages.  
? Source language: French 
? Target language: English 
? Source corpus : 100 texts ? domain : 
newspaper articles 
 
Human judgements for comparison referential: 
? 12 English monolingual students.  
? No human translation reference corpus. 
? Three criteria were tested: Fluency, 
Adequacy, Informativeness  
 
Six systems were submitted to evaluation : 
Candide (CD), Globalink (GL), MetalSystem 
(MS), Reverso (RV), Systran (SY), XS (XS) 
? Each of the systems is due to translate a 
hundred source texts ranging from 250 to 
300 words each. A corpus of 600 
translations is thus produced. 
? For each of the source texts, a corpus of 6 
translations is produced automatically. 
These translations are then regrouped by 
series of six texts.  
? According to the protocol initiated by 
(White & Forner, 2001) these series are 
then ranked by medium adequacy score. 
? Every 5 series, a series is extracted from 
the whole. Packs of twenty series of target 
translations are thus obtained and 
submitted to human evaluators.  
 
7.2.1 Evaluators? tasks 
? Each evaluator reads 10 series of 6 
translations i.e. 60 texts.  
? Each of these series is then read by six 
different evaluators 
? The evaluators must observe a ten minute 
compulsory break every two series.  
? The evaluators do not know that the texts 
have been translated automatically. 
 
The directive given to them is the following: 
? rank these six texts from best to worst. If 
you cannot manage to give a different ranking 
to two texts, regroup them under the same 
parenthesis and give them the same score, as in 
the following example : 4 [1 2] 6 [3 5].? 
The aim of this instruction is to produce 
rankings that are similar to the rankings attributed 
automatically.  
Human judgement that ranks from best to worse 
corresponds in reality to a set of the fluency, 
adequacy and Informativeness criteria that can be 
attributed to the texts translated automatically.  
 
7.2.2 Automatically generated scores 
? X-score : syntactic score 
? D-score : semantic score 
 
Within the framework of the CESTA 
evaluation campaign the scientific committee 
decided to make use of the X-score only, the 
semantic D-score having proved to be unstable 
and that it could be advantageously replaced 
by the a metric based on (Bogdan, B.; Hartley, 
A.; Atwell, 2003), a reformulation of the D-
score developed by (Rajman, M. and T. 
Hartley, 2001), and which we refer to as the 
ROUGE metric in this article. 
 
7.2.3 X-score: definition 
? This score corresponds to a grammaticality 
metric 
? Each of the texts is previously parsed with 
XELDA Xerox parser. 
? 22 types of syntactic dependencies 
identified through the corpus of automatic 
translations. 
? The syntactic profile of each source 
document is computed. This profile is then 
used to derive the X-score for each 
document, making use of the following 
formula: 
? X-score = (#RELSUBJ+#RELSUBJPASS-
#PADJ-#ADVADJ)  
 
 
7.3 The ?ROUGE? protocol  
 
This protocol, developed by Anthony Hartley in 
(Bogdan, B.; Hartley, A.; Atwell, 2003), is a 
semantic score. It is the result of a reformulation of 
the D-Score, the semantic score initiated through 
previous collaboration with Martin Rajman 
(Rajman, M. and T. Hartley, 2001), as explained in 
the previous section.  
 
The original idea on which this protocol is based 
relies on the fact that MT evaluation metrics that 
?are based on comparing the distribution of 
statistically significant words in corpora of MT 
output and in human reference translation 
corpora?.  
 
The method used to measure MT quality is the 
following:  a statistical model for MT output 
corpora and for a parallel corpus of human 
translations, each statistically significant word 
being highlighted in the corpus. On the other hand, 
a statistical significance score is given for each 
highlighted word. Then statistical models for MT 
target texts and human translations are compared, 
special attention being paid to words that are 
automatically marked as significant in MT outputs, 
whereas they do not appear to be marked as 
significant in human translations. These words are 
considered to be ?over generated?. The same 
operation is then carried out on ?under generated 
words?. At this stage, a third operation consists in 
the marking of the words equally marked as 
significant by the MT systems and the human 
translations. The overall difference is then 
calculated for each pair of texts in the corpora. 
Three measures specifying differences in statistical 
models for MT and human translations are then 
implemented : the first one aiming at avoiding 
?over generation?, the second one aiming at 
avoiding ?under generation? and the last one being 
a combination of these two measures. The average 
scores for each of the MT systems are then 
computed.  
 
As detailed in (Bogdan, B.; Hartley, A.; Atwell, 
2003): 
 
?1. The score of statistical significance is 
computed for each word (with absolute frequency 
? 2 in the particular text) for each text in the 
corpus, as follows: ( )
][
][][][
][ ln
corpallword
foundnottxtswordcorprestwordtextword
textword P
NPP
S
?
??? ??=
 
where: 
Sword[text] is the score of statistical significance for 
a particular word in a particular text 
Pword[text] is the relative frequency of the word in 
the text; 
Pword[rest-corp] is the relative frequency of the same 
word in the rest of the corpus, without this text; 
Nword[txt-not-found] is the proportion of texts in the 
corpus, where this word is not found (number of 
texts, where it is not found divided by number of 
texts in the corpus) 
Pword[all-corp] is the relative frequency of the word 
in the whole corpus, including this particular text 
 
2. In the second stage, the lists of statistically 
significant words for corresponding texts together 
with their Sword[text] scores are compared across 
different MT systems. Comparison is done in the 
following way: 
For all words which are present in lists of 
statistically significant words both in the human 
reference translation and in the MT output, we 
compute the sum of changes of their Sword[text] 
scores: ( )? ?= ].[].[. MTtextwordreferencetextworddifftext SSS  
The score Stext.diff is added to the scores of all 
"over-generated" words (words that do not appear 
in the list of statistically significant words for 
human reference translation, but are present in 
such list for MT output). The resulting score 
becomes the general "over-generation" score for 
this particular text: 
? ?? +=
textwords
textgeneratedoverworddifftexttextgenerationover SSS
.
][...
 
The opposite "under-generation" score for 
each text in the corpus is computed by adding 
Stext.dif and all Sword[text]  scores of "under-generated" 
words ? words present in the human reference 
translation, but absent from the MT output. 
?+=?
textwords
textatedundergenerworddifftexttextgenerationunder SSS
.
][...
 
It is more convenient to use inverted scores, 
which increases as the MT system improves. These 
scores, So.text and Su.text, could be interpreted as 
scores for ability to avoid "over-generation" and 
"under-generation" of statistically significant 
words. The combined (o&u) score is computed 
similarly to the F-measure, where Precision and 
Recall are equally important: 
textgenerationover
texto S
S
.
.
1
?
= ; 
 
textgenerationunder
textu S
S
.
.
1
?
= ;
 
textutexto
textutexto
textuo SS
SSS
..
..
.&
2
+=  
The number of statistically significant words 
could be different in each text, so in order to make 
the scores compatible across texts we compute the 
average over-generation and under-generation 
scores per each statistically significant word in a 
given text. For the otext score we divide So.text by the 
number of statistically significant words in the MT 
text, for the utext score we divide Su.text by the 
number of statistically significant words in the 
human (reference) translation: 
rdsInMTstatSignWo
texto
text n
So .= ;
 
rdsInHTstatSignWo
textu
text n
Su .= ; 
 
texttext
texttext
text uo
uoou +=
2&  
The general performance of an MT system for IE 
tasks could be characterised by the average o-
score, u-score and u&o-score for all texts in the 
corpus?. 
8 Time Schedule and result dissemination 
 
The CESTA evaluation campaign started in 
January 2003 after having been labeled by the 
French Ministry of Research. During year 2003 
CESTA scientific committee went through 
protocol detailed redefinition and specification and 
a time schedule was agreed upon.  
 
2004 first semester is being dedicated to corpus 
untagging and the programming of CESTA 
evaluation tool. Reference human translations will 
also have to be produced and the implemented 
evaluation tool submitted to trial and validation.  
 
After this preliminary work, the first run will 
start during autumn 2004. At the end of the first 
campaign, result analysis will be carried out. A 
workshop will then be organized for CESTA 
participants. Then the second campaign will take 
place at the end of Spring 2005, the terminological 
adaptation phase being scheduled on a five month 
scale. 
 
After carrying out result analysis and final report 
redaction, a public workshop will be organized and 
the results disseminated and subject to publication 
at the end of 2005.  
 
9 Conclusion 
 
CESTA is the first European Campaign 
dedicated to MT Evaluation. The results of the 
campaign will be published in a final report and be 
the object of an intermediary workshop between 
the two campaigns and a final workshop at the end 
of the campaign.  
 
It is a noticeable point that the CESTA campaign 
aims at providing a state of the art of automated 
metrics in order to ensure protocol reusability. The 
originality of the CESTA protocol lies in the 
combination and contrastive use of three different 
types of measures carried out in parallel with a 
Meta evaluation of the metrics. 
 
It is also important to note that CESTA aims at 
providing a black box evaluation of available 
Machine Translation technologies, rather than a 
comparison of systems and interfaces, that can be 
tuned to match a particular need. If systems had to 
be compared, the fact that these applications 
should be compared including all software lawyers 
and ergonomic properties, ought to be taken into 
consideration.  
 
Moreover apart from providing a state of the art 
through a Meta evaluation of the metrics used in its 
protocol, thanks to the setting of this original 
protocol that relies on the contrastive use of 
complementary metrics, CESTA aims at protocol 
reusability. One of the outputs of the campaign 
will be the creation of a Machine Translation 
evaluation toolkit that will be put at users and 
system developers? disposal.Acknowledgements 
References  
Besan?on, R. and Rajman, M., (2002). Evaluation 
of aVector Space similarity measure in a 
multilingual framework. Procs. 3rd 
International Conference on Language 
Resources and Evaluation, Las 
Palmas,Spain,.1252 
Bogdan, B.; Hartley, A.; Atwell E.; Statistical 
modelling of MT output corpora for 
Information Extraction Proceedings Corpus 
Linguistics 2003, Lancaster, UK, 28-31 
March 2003, pp. 62-70 
Chaudiron, S. Technolangue. In: 
http://www.apil.asso.fr/metil.htm, mars 2001 
Chaudiron, S. L??valuation des syst?mes de 
traitement de l?information textuelle : vers un 
changement de paradigmes, M?moire pour 
l?habilitation ? diriger des recherches en sciences 
de l?information, pr?sent? devant l?Universit? de 
Paris 10, Paris, novembre 2001 
Dabbadie, M, Mustafa El Hadi, W., Timimi, I. 
(2001). Setting a Methodology for Machine 
Translation Evaluation. In: Machine 
Translation Summit VIII, ISLE/EMTA, 
Santiago de Compestela, Spain, 18-23 
October 2001, pp. 49-54. 
Dabbadie, M., Mustafa El Hadi, W., Timimi, I., 
(2002). Terminological Enrichment for non-
Interactive MT Evaluation. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 6 ? 1878-
1884 
EAGLES-Evaluation-Workgroup. (1996). 
EAGLES evaluation of natural language 
processing systems. Final report, Center for 
Sprogteknologi, Denmark, October 1996. 
EAGLES (1999). EAGLES Reports (Expert 
Advisory Group on Language Engineering 
Standards)http://www.issco.unige.ch/project
s/eagles/ewg99. 
ISLE (2001). MT Evaluation Classification, 
Expanded Classification. 
http://www.isi.edu/natural-
language/mteval/2b-MT-classification.htm. 
ISO/IEC-9126. 1991. ISO/IEC 9126:1991 (E) ? 
Information Technology ? Software 
Product Evaluation ? Quality 
Characteristics and Guidelines for Their Use. 
ISO/IEC, Geneva. 
ISO (1999). Standard ISO/IEC 9126-1 Information 
Technology ? Software Engineering ? 
Quality characteristics and sub-
characteristics. Software Quality 
Characteristics and Metrics - Part 1 
ISO (1999). Standard ISO/IEC 9126-2 Information 
Technology ? Software Engineering ? 
Software products Quality : External Metrics 
- Part 2 
ISO/IEC-14598. 1998-2001. ISO/IEC 14598 ? 
Information technology ? Software product 
evaluation ? Part 1: General overview 
(1999), Part 2: Planning and management 
(2000), Part 3: Process for developers 
(2000), Part 4: Process for acquirers (1999), 
Part 5: Process for evaluators (1998), Part 6: 
Documentation of evaluation modules 
(2001). ISO/IEC, Geneva. 
ISSCO (2001) Machine Translation Evaluation : 
An Invitation to Get Your Hands Dirty!, 
ISSCO, University of Geneva, Workshop 
organised by M. King (ISSCO) & F. Reed, 
(Mitre Corporation), April 19-24 2001. 
King (1999a) EAGLES Evaluation Working 
Group, report,http://www.issco.unige.ch/ 
projects/eagles. 
King, M. (1999b). ?ISO Standards as a Point of 
Departure for EAGLES Work in EELS 
Conference (European Evaluation of 
Language Systems), 12-13 April 1999. 
Mariani, Joseph. ?Language Technologies : 
Technolangue Action ?. Presentation. In: 
LREC'2002 International Strategy Panel17, Las 
Palmas, May 2002. 
Nomura, H. and J. Isahara. (1992). The JEIDA 
report on MT. In Workshop on MT 
Evaluation: Basis for Future Directions, San 
Diego, CA. Association for Machine 
Translation in the Americas (AMTA). 
Popescu-Belis, A. S. Manzi, and M. King. (2001). 
Towards a two-stage taxonomy for MT 
evaluation. In Workshop on MT Evaluation 
?Who did what to whom?? at Mt Summit 
VIII, pages 1?8, Santiago de Compostela, 
Spain. 
Rajman, M. and T. Hartley, (2001). Automatically 
predicting MT systems rankings compatible 
with Fluency, Adequacy or Informativeness 
scores. Procs. 4th ISLE Workshop on MT 
Evaluation, MT Summit VIII, 29-34. 
Rajman, M. and T. Hartley, (2002). Automatic 
ranking of MT systems. In: LREC 2002 
Proceedings ? Las Palmas de Gran Canaria, 
Spain ? 29th ? 31st May 2002 ? vol 4 ? 1247-
1253 
Reeder, F., K. Miller, J. Doyon, and J. White, J. 
(2001). The naming of things and the 
confusion of tongues: an MT metric. Procs. 
4th ISLE Workshop on MT Evaluation, MT 
Summit VIII, 55-59. 
Sparck-Jones K., Gallier, J.R. (1996). Evaluating 
Natural Language Processing Systems: An 
Analysis and Review, Springer, Berlin. 
TREC, NIST Website, last updated, August 1st, 
2000, visited by the authors, 23-03-2003  
Vanni, M. and K. Miller (2001). Scaling the ISLE 
framework: validating tests of machine 
translation quality for multi-dimensional 
measurement. Procs. 4th ISLE Workshop on 
MT Evaluation, MT Summit VIII, 21-27. 
VanSlype., G. (1979). Critical study of methods 
for evaluating the quality of MT. Technical 
Report BR 19142, European Commission / 
Directorate for General Scientific and 
Technical Information Management (DG 
XIII). 
V?ronis, J., Langlais, Ph. (2000). ARCADE: 
?valuation de syst?mes d'alignement de textes 
multilingues. In Chibout, K., Mariani, J., 
Masson, N., Neel, F. ?ds., (2000). Ressources et 
?valuation en ing?nierie de la langue, Duculot, 
Coll. Champs linguistiques, et Collection 
Universit?s Francophones (AUF). 
