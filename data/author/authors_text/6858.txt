Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 161?168
Manchester, August 2008
KnowNet: Building a Large Net of Knowledge from the Web
Montse Cuadros
TALP Research Center, UPC
Barcelona, Spain
cuadros@lsi.upc.edu
German Rigau
IXA NLP Group, UPV/EHU
Donostia, Spain
german.rigau@ehu.es
Abstract
This paper presents a new fully auto-
matic method for building highly dense
and accurate knowledge bases from ex-
isting semantic resources. Basically, the
method uses a wide-coverage and accu-
rate knowledge-based Word Sense Dis-
ambiguation algorithm to assign the most
appropriate senses to large sets of topi-
cally related words acquired from the web.
KnowNet, the resulting knowledge-base
which connects large sets of semantically-
related concepts is a major step towards
the autonomous acquisition of knowledge
from raw corpora. In fact, KnowNet is sev-
eral times larger than any available knowl-
edge resource encoding relations between
synsets, and the knowledge KnowNet con-
tains outperform any other resource when
is empirically evaluated in a common
framework.
1 Introduction
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, of-
ten necessary, practice for most current Natural
Language Processing (NLP) systems. Even now,
building large and rich enough knowledge bases
for broad?coverage semantic processing takes a
great deal of expensive manual effort involving
large research groups during long periods of de-
velopment. In fact, hundreds of person-years have
been invested in the development of wordnets for
various languages (Vossen, 1998). For example, in
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
more than ten years of manual construction (from
1995 to 2006, that is from version 1.5 to 3.0),
WordNet grew from 103,445 to 235,402 semantic
relations
1
. But this data does not seem to be rich
enough to support advanced concept-based NLP
applications directly. It seems that applications
will not scale up to work in open domains without
more detailed and rich general-purpose (and also
domain-specific) semantic knowledge built by au-
tomatic means. Obviously, this fact has severely
hampered the state-of-the-art of advanced NLP ap-
plications.
However, the Princeton WordNet (WN) is by far
the most widely-used knowledge base (Fellbaum,
1998). In fact, WordNet is being used world-wide
for anchoring different types of semantic knowl-
edge including wordnets for languages other than
English (Atserias et al, 2004), domain knowledge
(Magnini and Cavagli`a, 2000) or ontologies like
SUMO (Niles and Pease, 2001) or the EuroWord-
Net Top Concept Ontology (
?
Alvez et al, 2008).
It contains manually coded information about En-
glish nouns, verbs, adjectives and adverbs and is
organized around the notion of a synset. A synset
is a set of words with the same part-of-speech that
can be interchanged in a certain context. For ex-
ample, <party, political party> form a synset be-
cause they can be used to refer to the same concept.
A synset is often further described by a gloss, in
this case: ?an organization to gain political power?
and by explicit semantic relations to other synsets.
Fortunately, during the last years the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
1
Symmetric relations are counted only once.
161
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences
acquired from SemCor (Agirre and Martinez,
2001; Agirre and Martinez, 2002) or acquired from
British National Corpus (BNC) (McCarthy, 2001),
large-scale Topic Signatures for each synset ac-
quired from the web (Agirre and de Lacalle, 2004)
or knowledge about individuals from Wikipedia
(Suchanek et al, 2007). Obviously, all these se-
mantic resources have been acquired using a very
different methods, tools and corpora. As expected,
each semantic resource has different volume and
accuracy figures when evaluated in a common and
controlled framework (Cuadros and Rigau, 2006).
However, not all these large-scale resources en-
code semantic relations between synsets. In some
cases, only relations between synsets and words
have been acquired. This is the case of the Topic
Signatures acquired from the web (Agirre and de
Lacalle, 2004). This is one of the largest seman-
tic resources ever built with around one hundred
million relations between synsets and semantically
related words
2
.
A knowledge net or KnowNet (KN), is an exten-
sible, large and accurate knowledge base, which
has been derived by semantically disambiguating
small portions of the Topic Signatures acquired
from the web. Basically, the method uses a ro-
bust and accurate knowledge-based Word Sense
Disambiguation algorithm to assign the most ap-
propriate senses to the topic words associated to
a particular synset. The resulting knowledge-base
which connects large sets of topically-related con-
cepts is a major step towards the autonomous ac-
quisition of knowledge from raw text.
Table 1 compares the different volumes of se-
mantic relations between synset pairs of avail-
able knowledge bases and the newly created
KnowNets
3
.
Varying from five to twenty the number of pro-
cessed words from each Topic Signature, we cre-
ated automatically four different KnowNet ver-
sions with millions of new semantic relations be-
tween synsets. In fact, KnowNet is several times
larger than WordNet, and when evaluated empir-
ically in a common framework, the knowledge it
contains outperforms any other semantic resource.
After this introduction, section 2 describes the
Topic Signatures acquired from the web. Section
2
Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus
3
These KnowNet versions are available at
http://adimen.si.ehu.es
Source #relations
Princeton WN3.0 235,402
Selectional Preferences from SemCor 203,546
eXtended WN 550,922
Co-occurring relations from SemCor 932,008
New KnowNet-5 231,163
New KnowNet-10 689,610
New KnowNet-15 1,378,286
New KnowNet-20 2,358,927
Table 1: Number of synset relations
3 presents the approach we followed for building
highly dense and accurate knowledge bases from
the Topic Signatures. In section 4, we present the
evaluation framework used in this study. Section 5
describes the results when evaluating different ver-
sions of KnowNet and finally, section 6 presents
some concluding remarks and future work.
2 Topic Signatures
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Sig-
natures are built by retrieving context words of a
target topic from a large corpora. This study con-
siders word senses as topics. Basically, the acqui-
sition of TS consists of:
? acquiring the best possible corpus examples
for a particular word sense (usually character-
izing each word sense as a query and perform-
ing a search on the corpus for those examples
that best match the queries)
? building the TS by selecting the context
words that best represent the word sense from
the selected corpora.
The Topic Signatures acquired from the web
(hereinafter TSWEB) constitutes one of the largest
semantic resource available with around 100 mil-
lion relations (between synsets and words) (Agirre
and de Lacalle, 2004). Inspired by the work of
(Leacock et al, 1998), TSWEB was constructed
using monosemous relatives from WN (synonyms,
hypernyms, direct and indirect hyponyms, and sib-
lings), querying Google and retrieving up to one
thousand snippets per query (that is, a word sense),
extracting the salient words with distinctive fre-
quency using TFIDF. Thus, TSWEB consist of
large ordered lists of words with weights associ-
ated to the polysemous nouns of WN1.6. The
number of constructed topic signatures is 35,250
with an average size per signature of 6,877 words.
162
tammany#n 0.0319
federalist#n 0.0315
whig#n 0.0300
missionary#j 0.0229
Democratic#n 0.0218
nazi#j 0.0202
republican#n 0.0189
constitutional#n 0.0186
conservative#j 0.0148
socialist#n 0.0140
Table 2: TS of party#n#1 (first 10 out of 12,890
total words)
When evaluating TSWEB, we used at maximum
the first 700 words while for building KnowNet we
used at maximum the first 20 words.
For example, table 2 presents the first words
(lemmas and part-of-speech) and weights of the
Topic Signature acquired for party#n#1
4
.
3 Building highly connected and dense
knowledge bases
We acquired by fully automatic means highly
connected and dense knowledge bases by disam-
biguating small portions of the Topic Signatures
obtained from the web, increasing the total num-
ber of semantic relations from less than one mil-
lion (the current number of available relations) to
millions of new and accurate semantic relations
between synsets. We applied a knowledge?based
all?words Word Sense Disambiguation algorithm
to the Topic Signatures for deriving a sense vector
from each word vector.
3.1 SSI-Dijkstra
We have implemented a version of the Struc-
tural Semantic Interconnections algorithm (SSI), a
knowledge-based iterative approach to Word Sense
Disambiguation (Navigli and Velardi, 2005). The
SSI algorithm is very simple and consists of an ini-
tialization step and a set of iterative steps (see al-
gorithm 1).
Given W, an ordered list of words to be dis-
ambiguated, the SSI algorithm performs as fol-
lows. During the initialization step, all monose-
mous words are included into the set I of already
interpreted words, and the polysemous words are
included in P (all of them pending to be disam-
biguated). At each step, the set I is used to disam-
biguate one word of P, selecting the word sense
which is closer to the set I of already disam-
4
This format stands for word#pos#sense.
biguated words. Once a sense is selected, the word
sense is removed from P and included into I. The
algorithm finishes when no more pending words
remain in P.
Algorithm 1 SSI-Dijkstra Algorithm
SSI (T: list of terms)
for each {t ? T} do
I[t] = ?
if t is monosemous then
I[t] := the only sense of t
else
P := P ? {t}
end if
end for
repeat
P
?
:= P
for each {t ? P} do
BestSense := ?
MaxV alue := 0
for each {sense s of t} do
W [s] := 0
N [s] := 0
for each {sense s
?
? I} do
w := DijsktraShortestPath(s, s
?
)
if w > 0 then
W [s] := W [s] + (1/w)
N [s] := N [s] + 1
end if
end for
if N [s] > 0 then
NewV alue := W [s]/N [s]
if NewV alue > MaxV alue then
MaxV alue := NewV alue
BestSense := s
end if
end if
end for
if MaxV alue > 0 then
I[t] := BestSense
P := P \ {t}
end if
end for
until P 6= P
?
return (I, P);
Initially, the list I of interpreted words should in-
clude the senses of the monosemous words in W,
or a fixed set of word senses
5
. However, when dis-
5
If no monosemous words are found or if no initial senses
are provided, the algorithm could make an initial guess based
on the most probable sense of the less ambiguous word of W.
163
ambiguating a TS of a word sense s (for instance
party#n#1), the list I already includes s.
In order to measure the proximity of one synset
to the rest of synsets of I, we use part of the
knowledge already available to build a very large
connected graph with 99,635 nodes (synsets) and
636,077 edges. This graph includes the set of
direct relations between synsets gathered from
WordNet and eXtended WordNet. On that graph,
we used a very efficient graph library, Boost-
Graph
6
to compute the Dijkstra algorithm. The
Dijkstra algorithm is a greedy algorithm for com-
puting the shortest path distance between one node
an the rest of nodes of a graph. In that way, we can
compute very efficiently the shortest distance be-
tween any two given nodes of a graph. We call this
version of the SSI algorithm, SSI-Dijkstra.
SSI-Dijkstra has very interesting properties. For
instance, it always provides the minimum distance
between two synsets. That is, the algorithm always
provides an answer being the minimum distance
close or far. In contrast, the original SSI algorithm
not always provides a path distance because it de-
pends on a predefined grammar of semantic rela-
tions. In fact, the SSI-Dijkstra algorithm compares
the distances between the synsets of a word and all
the synsets already interpreted in I. At each step,
the SSI-Dijkstra algorithm selects the synset which
is closer to I (the set of already interpreted words).
Furthermore, this approach is completely lan-
guage independent. The same graph can be used
for any language having words connected to Word-
Net.
3.2 Building KnowNet
We developed KnowNet (KN), a large-scale and
extensible knowledge base, by applying SSI-
Dijkstra to each topic signature from TSWEB.
We have generated four different versions of
KnowNet applying SSI-Dijkstra to only the first
5, 10, 15 and 20 words for each TS. SSI-Dijkstra
used only the knowledge present in WordNet and
eXtended WordNet which consist of a very large
connected graph with 99,635 nodes (synsets) and
636,077 edges (semantic relations).
We generated each KnowNet by applying the
SSI-Dijkstra algorithm to the whole TSWEB (pro-
cessing the first words of each of the 35,250
topic signatures). For each TS, we obtained the
direct relations from the topic (a word sense)
6
http://www.boost.org
KB WN+XWN #relations #synsets
KN-5 3,1% 231,163 39,864
KN-10 5,0% 689,610 45,817
KN-15 6,9% 1,378,286 48,521
KN-20 8,5% 2,358,927 50,789
Table 3: Size and percentage of overlapping rela-
tions between KnowNet versions and WN+XWN
to the disambiguated word senses of the TS
(for instance, party#n#1?>federalist#n#1), but
also the indirect relations between disambiguated
words from the TS (for instance, federalist#n#1?
>republican#n#1). Finally, we removed symmet-
ric and repeated relations.
Table 3 shows the overlaping percentage be-
tween each KnowNet and the knowledge con-
tained into WordNet and eXtended WordNet, and
the total number of relations and synsets of each
resource. For instance, only 8,5% of the total di-
rect relations included into WN+XWN are also
present in KnowNet-20. This means that the rest
of relations from KnowNet-20 are new. As ex-
pected, each KnowNet is very large, ranging from
hundreds of thousands to millions of new semantic
relations between synsets among increasing sets of
synsets.
4 Evaluation framework
In order to empirically establish the relative qual-
ity of these new semantic resources, we used the
evaluation framework of task 16 of SemEval-2007:
Evaluation of wide coverage knowledge resources
(Cuadros and Rigau, 2007).
In this framework all knowledge resources are
evaluated on a common WSD task. In particu-
lar, we used the noun-sets of the English Lexi-
cal Sample task of Senseval-3 and SemEval-2007
exercises which consists of 20 and 35 nouns re-
spectively. All performances are evaluated on the
test data using the fine-grained scoring system pro-
vided by the organizers.
Furthermore, trying to be as neutral as possible
with respect to the resources studied, we applied
systematically the same disambiguation method to
all of them. Recall that our main goal is to es-
tablish a fair comparison of the knowledge re-
sources rather than providing the best disambigua-
tion technique for a particular knowledge base. All
knowledge bases are evaluated as topic signatures.
That is, word vectors with weights associated to a
particular synset which are obtained by collecting
164
those word senses appearing in the synsets directly
related to the topics. This simple representation
tries to be as neutral as possible with respect to the
resources used.
A common WSD method has been applied to all
knowledge resources. A simple word overlapping
counting is performed between the topic signature
representing a word sense and the test example
7
.
The synset having higher overlapping word counts
is selected. In fact, this is a very simple WSD
method which only considers the topical informa-
tion around the word to be disambiguated. Finally,
we should remark that the results are not skewed
(for instance, for resolving ties) by the most fre-
quent sense in WN or any other statistically pre-
dicted knowledge.
4.1 Baselines
We have designed a number of baselines in order
to establish a complete evaluation framework for
comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method
selects a random sense. This baseline can be con-
sidered as a lower-bound.
SEMCOR-MFS: This baseline selects the most
frequent sense of the target word in SemCor.
WN-MFS: This baseline is obtained by se-
lecting the most frequent sense (the first sense
in WN1.6) of the target word. WordNet word-
senses were ranked using SemCor and other sense-
annotated corpora. Thus, WN-MFS and SemCor-
MFS are similar, but not equal.
TRAIN-MFS: This baseline selects the most
frequent sense in the training corpus of the target
word.
TRAIN: This baseline uses the training corpus
to directly build a Topic Signature using TFIDF
measure for each word sense and selecting at max-
imum the first 450 words. Note that in WSD eval-
uation frameworks, this is a very basic baseline.
However, in our evaluation framework, this ?WSD
baseline? could be considered as an upper-bound.
We do not expect to obtain better topic signatures
for a particular sense than from its own annotated
corpus.
4.2 Other Large-scale Knowledge Resources
In order to measure the relative quality of the new
resources, we include in the evaluation a wide
7
We also consider those multiword terms appearing in
WN.
range of large-scale knowledge resources con-
nected to WordNet.
WN (Fellbaum, 1998): This resource uses the
different direct relations encoded in WN1.6 and
WN2.0. We also tested WN
2
using relations at dis-
tance 1 and 2, WN
3
using relations at distances 1
to 3 and WN
4
using relations at distances 1 to 4.
XWN (Mihalcea and Moldovan, 2001): This
resource uses the direct relations encoded in eX-
tended WN.
spBNC (McCarthy, 2001): This resource con-
tains 707,618 selectional preferences acquired for
subjects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This
resource contains the selectional preferences ac-
quired for subjects and objects from SemCor.
MCR (Atserias et al, 2004): This resource in-
tegrates the direct relations of WN, XWN and
spSemCor.
TSSEM (Cuadros et al, 2007): These Topic
Signatures have been constructed using Sem-
Cor.For each word-sense appearing in SemCor, we
gather all sentences for that word sense, building a
TS using TFIDF for all word-senses co-occurring
in those sentences.
4.3 Integrated Knowledge Resources
We also evaluated the performance of the integra-
tion (removing duplicated relations) of some of
these resources.
WN+XWN: This resource integrates the di-
rect relations of WN and XWN. We also tested
(WN+XWN)
2
(using either WN or XWN rela-
tions at distances 1 and 2).
MCR (Atserias et al, 2004): This resource in-
tegrates the direct relations of WN, XWN and
spSemCor.
WN+XWN+KN-20: This resource integrates
the direct relations of WN, XWN and KnowNet-
20.
5 KnowNet Evaluation
We evaluated KnowNet using the same framework
explained in section 4. That is, the noun part of the
test set from the English Senseval-3 and SemEval-
2007 English lexical sample tasks.
5.1 Senseval-3 evaluation
Table 4 presents ordered by F1 measure, the per-
formance in terms of precision (P), recall (R) and
165
KB P R F1 Av. Size
TRAIN 65.1 65.1 65.1 450
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
TSSEM 52.5 52.4 52.4 103
SEMCOR-MFS 49.0 49.1 49.0
MCR
2
45.1 45.1 45.1 26,429
WN+XWN+KN-20 44.8 44.8 44.8 671
MCR 45.3 43.7 44.5 129
KnowNet-20 44.1 44.1 44.1 610
KnowNet-15 43.9 43.9 43.9 339
spSemCor 43.1 38.7 40.8 56
KnowNet-10 40.1 40.0 40.0 154
(WN+XWN)
2
38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
KnowNet-5 35.0 35.0 35.0 44
WN
3
35.0 34.7 34.8 503
WN
4
33.2 33.1 33.2 2,346
WN
2
33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
RANDOM 19.1 19.1 19.1
Table 4: P, R and F1 fine-grained results for the
resources evaluated at Senseval-3, English Lexical
Sample Task.
F1 measure (F1, harmonic mean of recall and pre-
cision) of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
different KnowNet versions appear marked in bold
and the baselines appear in italics.
As expected, RANDOM obtains the poorest re-
sult. The most frequent senses obtained from Sem-
Cor (SEMCOR-MFS) and WN (WN-MFS) are
both below the most frequent sense of the training
corpus (TRAIN-MFS). However, all of them are
far below to the Topic Signatures acquired using
the training corpus (TRAIN).
The best results are obtained by TSSEM (with
F1 of 52.4). The lowest result is obtained by the
knowledge directly gathered from WN mainly be-
cause of its poor coverage (R of 18.4 and F1 of
26.1). Interestingly, the knowledge integrated in
the MCR although partly derived by automatic
means performs much better in terms of precision,
recall and F1 measures than using them separately
(F1 with 18.4 points higher than WN, 9.1 than
XWN and 3.7 than spSemCor).
Despite its small size, the resources derived
from SemCor obtain better results than its coun-
terparts using much larger corpora (TSSEM vs.
TSWEB and spSemCor vs. spBNC).
Regarding the baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Regarding the expansions and combinations, the
performance of WN is improved using words at
distances up to 2, and up to 3, but it decreases using
distances up to 4. Interestingly, none of these WN
expansions achieve the results of XWN. Finally,
(WN+XWN)
2
performs better than WN+XWN
and MCR
2
slightly better than MCR
8
.
The different versions of KnowNet consistently
obtain better performances as they increase the
window size of processed words of TSWEB. As
expected, KnowNet-5 obtain the lower results.
However, it performs better than WN (and all
its extensions) and spBNC. Interestingly, from
KnowNet-10, all KnowNet versions surpass the
knowledge resources used for their construction
(WN, XWN, TSWEB and WN+XWN). In fact,
KnowNet-10 also outperforms (WN+XWN)
2
with
much more relations per sense. Also interesting
is that KnowNet-10 and KnowNet-20 obtain bet-
ter performance than spSemCor which was derived
from annotated corpora. However, KnowNet-20
only performs slightly better than KnowNet-15
while almost doubling the number of relations.
These initial results seem to be very promis-
ing. If we do not consider the resources derived
from manually sense annotated data (spSemCor,
MCR, TSSEM, etc.), KnowNet-10 performs bet-
ter that any knowledge resource derived by man-
ual or automatic means. In fact, KnowNet-15 and
KnowNet-20 outperforms spSemCor which was
derived from manually annotated corpora. This is
a very interesting result since these KnowNet ver-
sions have been derived only with the knowledge
coming from WN and the web (that is, TSWEB),
and WN and XWN as a knowledge source for SSI-
Dijkstra
9
.
Regarding the integration of resources,
WN+XWN+KN-20 performs better than MCR
and similarly to MCR
2
(having less than 50 times
its size). Also interesting is that WN+XWN+KN-
20 have better performance than their individual
resources, indicating a complementary knowledge.
In fact, WN+XWN+KN-20 performs much better
than the resources from which it derives (WN,
XWN and TSWEB).
8
No further distances have been tested
9
eXtended WordNet only has 17,185 manually labeled
senses.
166
KB P R F1 Av. Size
TRAIN 87.6 87.6 87.6 450
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
WN+XWN+KN-20 53.0 53.0 53.0 627
(WN+XWN)
2
54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
KnowNet-20 49.5 46.1 47.7 561
KnowNet-15 47.0 43.5 45.2 308
XWN 50.1 39.8 44.4 96
KnowNet-10 44.0 39.8 41.8 139
WN+XWN 45.4 36.8 40.7 101
SEMCOR-MFS 42.4 38.4 40.3
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
KnowNet-5 35.5 26.5 30.3 41
MCR
2
32.4 29.5 30.9 24,896
WN
3
29.3 26.3 27.7 584
RANDOM 27.4 27.4 27.4
WN
2
25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN
4
26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
Table 5: P, R and F1 fine-grained results for the re-
sources evaluated at SemEval-2007, English Lexi-
cal Sample Task.
5.2 SemEval-2007 evaluation
Table 5 presents ordered by F1 measure, the per-
formance in terms of precision (P), recall (R) and
F1 measure (F1) of each knowledge resource on
SemEval-2007 and its average size of the TS per
word-sense
10
. Again, the different KnowNet ver-
sions appear marked in bold and the baselines ap-
pear in italics.
As in the previous evaluation, RANDOM ob-
tains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired
using the training corpus (TRAIN).
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
individual results are obtained by TSWEB, while
in this case TSSEM obtains very modest results.
The lowest result is obtained by the knowledge en-
coded in spBNC.
Regarding the baselines, spBNC, WN (and also
WN
2
and WN
4
) and spSemCor do not surpass
RANDOM, and none achieves neither WN-MFS,
TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)
2
obtain better
10
The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
results than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
Regarding other expansions and combinations,
the performance of WN is improved using words
at distances up to 2, and up to 3, but it decreases
using distances up to 4. Again, none of these WN
expansions achieve the results of XWN. Finally,
(WN+XWN)
2
performs better than WN+XWN
and MCR
2
slightly better than MCR
11
.
On SemEval-2007, the different versions of
KnowNet consistently obtain better performances
as they incease the window size of processed
words of TSWEB. As expected, KnowNet-5 ob-
tain the lower results. However, it performs better
than spBNC, WN (and all its extensions), spSem-
Cor and MCR
2
. This time, all KnowNet ver-
sions perform worse than TSWEB. However, as in
the previous evaluation, KnowNet-10 outperforms
WN+XWN, and this time, also TSSEM and the
MCR, with much more relations per sense. Also
interesting is that from KnowNet-10, all KnowNet
versions perform better than the resources derived
from manually sense annotated corpora (spSem-
Cor, MCR, TSSEM, etc.).
Regarding the integration of resources,
WN+XWN+KN-20 performs better than any
knowledge resource derived by manual or auto-
matic means. Again, it is interesting to note that
WN+XWN+KN-20 have better performance than
their individual resources, indicating a comple-
mentary knowledge. In fact, WN+XWN+KN-20
performs much better than the resources from
which it derives (WN, XWN and TSWEB).
5.3 Discussion
When comparing the ranking of the different
knowledge resources, the different versions of
KnowNet seem to be more robust and stable
across corpora changes. For instance, in both
evaluation frameworks (Senseval-3 and SemEval-
2007), KnowNet-20 ranks 5th and 4th, respec-
tively ((WN+XWN)
2
ranks 8th and 2nd, TSSEM
ranks 1st and 10th, MCR ranks 4th and 9th,
TSWEB ranks 11th and 3rd, etc.). In fact,
WN+XWN+KN-20 ranks 3rd and 1st, respec-
tively.
11
No further distances have been tested
167
6 Conclusions and future research
It is our belief, that accurate semantic processing
(such as WSD) would rely not only on sophisti-
cated algorithms but on knowledge intensive ap-
proaches. The results presented in this paper sug-
gests that much more research on acquiring and
using large-scale semantic resources should be ad-
dressed.
The knowledge acquisition bottleneck problem
is particularly acute for open domain (and also
domain specific) semantic processing. The ini-
tial results obtained for the different versions of
KnowNet seem to be a major step towards the au-
tonomous acquisition of knowledge from raw cor-
pora, since they are several times larger than the
available knowledge resources which encode re-
lations between synsets, and the knowledge they
contain outperform any other resource when is em-
pirically evaluated in a common framework.
It remains for future research the evaluation of
these KnowNet versions in combination with other
large-scale semantic resources or in a cross-lingual
setting.
Acknowledgments
We want to thank Aitor Soroa for his technical
support and the anonymous reviewers for their
comments. This work has been supported by
KNOW (TIN2006-15049-C03-01) and KYOTO
(ICT-2007-211423).
References
Agirre, E. and O. Lopez de Lacalle. 2004. Publicly
available topic signatures for all wordnet nominal
senses. In Proceedings of LREC, Lisbon, Portugal.
Agirre, E. and D. Martinez. 2001. Learning class-
to-class selectional preferences. In Proceedings of
CoNLL, Toulouse, France.
Agirre, E. and D. Martinez. 2002. Integrating selec-
tional preferences in wordnet. In Proceedings of
GWC, Mysore, India.
?
Alvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver,
and G. Rigau. 2008. Consistent annotation of eu-
rowordnet with the top concept ontology. In Pro-
ceedings of Fourth International WordNet Confer-
ence (GWC?08).
Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings
of GWC, Brno, Czech Republic.
Cuadros, M. and G. Rigau. 2006. Quality assessment
of large scale knowledge resources. In Proceedings
of the EMNLP.
Cuadros, M. and G. Rigau. 2007. Semeval-2007
task 16: Evaluation of wide coverage knowledge re-
sources. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007).
Cuadros, M., G. Rigau, and M. Castillo. 2007. Eval-
uating large-scale knowledge resources across lan-
guages. In Proceedings of RANLP.
Fellbaum, C., editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
Leacock, C., M. Chodorow, and G. Miller. 1998.
Using Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24(1):147?166.
Lin, C. and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
Proceedings of COLING. Strasbourg, France.
Magnini, B. and G. Cavagli`a. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
McCarthy, D. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Sub-
categorization Frames and Selectional Preferences.
Ph.D. thesis, University of Sussex.
Mihalcea, R. and D. Moldovan. 2001. extended word-
net: Progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh, PA.
Navigli, R. and P. Velardi. 2005. Structural seman-
tic interconnections: a knowledge-based approach to
word sense disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence (PAMI),
27(7):1063?1074.
Niles, I. and A. Pease. 2001. Towards a standard up-
per ontology. In Proceedings of the 2nd Interna-
tional Conference on Formal Ontology in Informa-
tion Systems (FOIS-2001), pages 17?19. Chris Welty
and Barry Smith, eds.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A Core of Semantic Knowl-
edge. In 16th international World Wide Web con-
ference (WWW 2007), New York, NY, USA. ACM
Press.
Vossen, P., editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
168
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389?397,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
An Empirical Study on Class-based Word Sense Disambiguation?
Rube?n Izquierdo & Armando Sua?rez
Deparment of Software and Computing Systems
University of Alicante. Spain
{ruben,armando}@dlsi.ua.es
German Rigau
IXA NLP Group.
EHU. Donostia, Spain
german.rigau@ehu.es
Abstract
As empirically demonstrated by the last
SensEval exercises, assigning the appro-
priate meaning to words in context has re-
sisted all attempts to be successfully ad-
dressed. One possible reason could be the
use of inappropriate set of meanings. In
fact, WordNet has been used as a de-facto
standard repository of meanings. How-
ever, to our knowledge, the meanings rep-
resented by WordNet have been only used
for WSD at a very fine-grained sense level
or at a very coarse-grained class level. We
suspect that selecting the appropriate level
of abstraction could be on between both
levels. We use a very simple method for
deriving a small set of appropriate mean-
ings using basic structural properties of
WordNet. We also empirically demon-
strate that this automatically derived set of
meanings groups senses into an adequate
level of abstraction in order to perform
class-based Word Sense Disambiguation,
allowing accuracy figures over 80%.
1 Introduction
Word Sense Disambiguation (WSD) is an inter-
mediate Natural Language Processing (NLP) task
which consists in assigning the correct semantic
interpretation to ambiguous words in context. One
of the most successful approaches in the last years
is the supervised learning from examples, in which
statistical or Machine Learning classification mod-
els are induced from semantically annotated cor-
pora (Ma`rquez et al, 2006). Generally, super-
vised systems have obtained better results than
the unsupervised ones, as shown by experimental
work and international evaluation exercises such
?This paper has been supported by the European Union
under the projects QALL-ME (FP6 IST-033860) and KY-
OTO (FP7 ICT-211423), and the Spanish Government under
the project Text-Mess (TIN2006-15265-C06-01) and KNOW
(TIN2006-15049-C03-01)
as Senseval1. These annotated corpora are usu-
ally manually tagged by lexicographers with word
senses taken from a particular lexical semantic re-
source ?most commonly WordNet2 (WN) (Fell-
baum, 1998).
WN has been widely criticized for being a sense
repository that often provides too fine?grained
sense distinctions for higher level applications
like Machine Translation or Question & Answer-
ing. In fact, WSD at this level of granularity
has resisted all attempts of inferring robust broad-
coverage models. It seems that many word?sense
distinctions are too subtle to be captured by auto-
matic systems with the current small volumes of
word?sense annotated examples. Possibly, build-
ing class-based classifiers would allow to avoid
the data sparseness problem of the word-based ap-
proach. Recently, using WN as a sense reposi-
tory, the organizers of the English all-words task
at SensEval-3 reported an inter-annotation agree-
ment of 72.5% (Snyder and Palmer, 2004). In-
terestingly, this result is difficult to outperform by
state-of-the-art sense-based WSD systems.
Thus, some research has been focused on deriv-
ing different word-sense groupings to overcome
the fine?grained distinctions of WN (Hearst and
Schu?tze, 1993), (Peters et al, 1998), (Mihalcea
and Moldovan, 2001), (Agirre and LopezDeLa-
Calle, 2003), (Navigli, 2006) and (Snow et al,
2007). That is, they provide methods for grouping
senses of the same word, thus producing coarser
word sense groupings for better disambiguation.
Wikipedia3 has been also recently used to over-
come some problems of automatic learning meth-
ods: excessively fine?grained definition of mean-
ings, lack of annotated data and strong domain de-
pendence of existing annotated corpora. In this
way, Wikipedia provides a new very large source
of annotated data, constantly expanded (Mihalcea,
2007).
1http://www.senseval.org
2http://wordnet.princeton.edu
3http://www.wikipedia.org
389
In contrast, some research have been focused on
using predefined sets of sense-groupings for learn-
ing class-based classifiers for WSD (Segond et al,
1997), (Ciaramita and Johnson, 2003), (Villarejo
et al, 2005), (Curran, 2005) and (Ciaramita and
Altun, 2006). That is, grouping senses of different
words into the same explicit and comprehensive
semantic class.
Most of the later approaches used the origi-
nal Lexicographical Files of WN (more recently
called SuperSenses) as very coarse?grained sense
distinctions. However, not so much attention has
been paid on learning class-based classifiers from
other available sense?groupings such as WordNet
Domains (Magnini and Cavaglia`, 2000), SUMO
labels (Niles and Pease, 2001), EuroWordNet
Base Concepts (Vossen et al, 1998), Top Con-
cept Ontology labels (Alvez et al, 2008) or Ba-
sic Level Concepts (Izquierdo et al, 2007). Obvi-
ously, these resources relate senses at some level
of abstraction using different semantic criteria and
properties that could be of interest for WSD. Pos-
sibly, their combination could improve the overall
results since they offer different semantic perspec-
tives of the data. Furthermore, to our knowledge,
to date no comparative evaluation has been per-
formed on SensEval data exploring different levels
of abstraction. In fact, (Villarejo et al, 2005) stud-
ied the performance of class?based WSD com-
paring only SuperSenses and SUMO by 10?fold
cross?validation on SemCor, but they did not pro-
vide results for SensEval2 nor SensEval3.
This paper empirically explores on the super-
vised WSD task the performance of different
levels of abstraction provided by WordNet Do-
mains (Magnini and Cavaglia`, 2000), SUMO la-
bels (Niles and Pease, 2001) and Basic Level Con-
cepts (Izquierdo et al, 2007). We refer to this ap-
proach as class?based WSD since the classifiers
are created at a class level instead of at a sense
level. Class-based WSD clusters senses of differ-
ent words into the same explicit and comprehen-
sive grouping. Only those cases belonging to the
same semantic class are grouped to train the clas-
sifier. For example, the coarser word grouping ob-
tained in (Snow et al, 2007) only has one remain-
ing sense for ?church?. Using a set of Base Level
Concepts (Izquierdo et al, 2007), the three senses
of ?church? are still represented by faith.n#3,
building.n#1 and religious ceremony.n#1.
The contribution of this work is threefold. We
empirically demonstrate that a) Basic Level Con-
cepts group senses into an adequate level of ab-
straction in order to perform supervised class?
based WSD, b) that these semantic classes can
be successfully used as semantic features to boost
the performance of these classifiers and c) that
the class-based approach to WSD reduces dramat-
ically the required amount of training examples to
obtain competitive classifiers.
After this introduction, section 2 presents the
sense-groupings used in this study. In section 3 the
approach followed to build the class?based system
is explained. Experiments and results are shown in
section 4. Finally some conclusions are drawn in
section 5.
2 Semantic Classes
WordNet (Fellbaum, 1998) synsets are organized
in forty five Lexicographer Files, more recetly
called SuperSenses, based on open syntactic cat-
egories (nouns, verbs, adjectives and adverbs) and
logical groupings, such as person, phenomenon,
feeling, location, etc. There are 26 basic cate-
gories for nouns, 15 for verbs, 3 for adjectives and
1 for adverbs.
WordNet Domains4 (Magnini and Cavaglia`,
2000) is a hierarchy of 165 Domain Labels which
have been used to label all WN synsets. Informa-
tion brought by Domain Labels is complementary
to what is already in WN. First of all a Domain La-
bels may include synsets of different syntactic cat-
egories: for instance MEDICINE groups together
senses from nouns, such as doctor and hospital,
and from Verbs such as to operate. Second, a Do-
main Label may also contain senses from differ-
ent WordNet subhierarchies. For example, SPORT
contains senses such as athlete, deriving from life
form, game equipment, from physical object, sport
from act, and playing field, from location.
SUMO5 (Niles and Pease, 2001) was created as
part of the IEEE Standard Upper Ontology Work-
ing Group. The goal of this Working Group is
to develop a standard upper ontology to promote
data interoperability, information search and re-
trieval, automated inference, and natural language
processing. SUMO consists of a set of concepts,
relations, and axioms that formalize an upper on-
tology. For these experiments, we used the com-
plete WN1.6 mapping with 1,019 SUMO labels.
4http://wndomains.itc.it/
5http://www.ontologyportal.org/
390
Basic Level Concepts6 (BLC) (Izquierdo et al,
2007) are small sets of meanings representing the
whole nominal and verbal part of WN. BLC can
be obtained by a very simple method that uses ba-
sic structural WN properties. In fact, the algorithm
only considers the relative number of relations of
each synset alng the hypernymy chain. The pro-
cess follows a bottom-up approach using the chain
of hypernymy relations. For each synset in WN,
the process selects as its BLC the first local maxi-
mum according to the relative number of relations.
The local maximum is the synset in the hypernymy
chain having more relations than its immediate
hyponym and immediate hypernym. For synsets
having multiple hypernyms, the path having the
local maximum with higher number of relations
is selected. Usually, this process finishes having
a number of preliminary BLC. Obviously, while
ascending through this chain, more synsets are
subsumed by each concept. The process finishes
checking if the number of concepts subsumed by
the preliminary list of BLC is higher than a cer-
tain threshold. For those BLC not representing
enough concepts according to the threshold, the
process selects the next local maximum following
the hypernymy hierarchy. Thus, depending on the
type of relations considered to be counted and the
threshold established, different sets of BLC can be
easily obtained for each WN version.
In this paper, we empirically explore the perfor-
mance of the different levels of abstraction pro-
vided by Basic Level Concepts (BLC) (Izquierdo
et al, 2007).
Table 1 presents the total number of BLC and
its average depth for WN1.6, varying the threshold
and the type of relations considered (all relations
or only hyponymy).
Thres. Rel. PoS #BLC Av. depth.
0
all Noun 3,094 7.09Verb 1,256 3.32
hypo Noun 2,490 7.09Verb 1,041 3.31
20
all Noun 558 5.81Verb 673 1.25
hypo Noun 558 5.80Verb 672 1.21
50
all Noun 253 5.21Verb 633 1.13
hypo Noun 248 5.21Verb 633 1.10
Table 1: BLC for WN1.6 using all or hyponym relations
6http://adimen.si.ehu.es/web/BLC
Classifier Examples # of examples
church.n#2 (sense approach) church.n#2 58
church.n#2 58
building.n#1 48
hotel.n#1 39
building, edifice (class approach) hospital.n#1 20
barn.n#1 17
....... ......
TOTAL= 371 examples
Table 2: Examples and number of them in Semcor, for
sense approach and for class approach
3 Class-based WSD
We followed a supervised machine learning ap-
proach to develop a set of class-based WSD tag-
gers. Our systems use an implementation of a Sup-
port Vector Machine algorithm to train the clas-
sifiers (one per class) on semantic annotated cor-
pora for acquiring positive and negative examples
of each class and on the definition of a set of fea-
tures for representing these examples. The system
decides and selects among the possible semantic
classes defined for a word. In the sense approach,
one classifier is generated for each word sense, and
the classifiers choose between the possible senses
for the word. The examples to train a single clas-
sifier for a concrete word are all the examples of
this word sense. In the semantic?class approach,
one classifier is generated for each semantic class.
So, when we want to label a word, our program
obtains the set of possible semantic classes for
this word, and then launch each of the semantic
classifiers related with these semantic categories.
The most likely category is selected for the word.
In this approach, contrary to the word sense ap-
proach, to train a classifier we can use all examples
of all words belonging to the class represented by
the classifier. In table 2 an example for a sense
of ?church? is shown. We think that this approach
has several advantages. First, semantic classes re-
duce the average polysemy degree of words (some
word senses are grouped together within the same
class). Moreover, the well known problem of ac-
quisition bottleneck in supervised machine learn-
ing algorithms is attenuated, because the number
of examples for each classifier is increased.
3.1 The learning algorithm: SVM
Support Vector Machines (SVM) have been
proven to be robust and very competitive in many
NLP tasks, and in WSD in particular (Ma`rquez et
al., 2006). For these experiments, we used SVM-
Light (Joachims, 1998). SVM are used to learn
an hyperplane that separates the positive from the
391
negative examples with the maximum margin. It
means that the hyperplane is located in an interme-
diate position between positive and negative ex-
amples, trying to keep the maximum distance to
the closest positive example, and to the closest
negative example. In some cases, it is not possi-
ble to get a hyperplane that divides the space lin-
early, or it is better to allow some errors to obtain a
more efficient hyperplane. This is known as ?soft-
margin SVM?, and requires the estimation of a pa-
rameter (C), that represent the trade-off allowed
between training errors and the margin. We have
set this value to 0.01, which has been proved as a
good value for SVM in WSD tasks.
When classifying an example, we obtain the
value of the output function for each SVM clas-
sifier corresponding to each semantic class for the
word example. Our system simply selects the class
with the greater value.
3.2 Corpora
Three semantic annotated corpora have been used
for training and testing. SemCor has been used
for training while the corpora from the English
all-words tasks of SensEval-2 and SensEval-3
has been used for testing. We also consid-
ered SemEval-2007 coarse?grained task corpus
for testing, but this dataset was discarded because
this corpus is also annotated with clusters of word
senses.
SemCor (Miller et al, 1993) is a subset of the
Brown Corpus plus the novel The Red Badge of
Courage, and it has been developed by the same
group that created WordNet. It contains 253 texts
and around 700,000 running words, and more than
200,000 are also lemmatized and sense-tagged ac-
cording to Princeton WordNet 1.6.
SensEval-27 English all-words corpus (here-
inafter SE2) (Palmer et al, 2001) consists on 5,000
words of text from three WSJ articles represent-
ing different domains from the Penn TreeBank II.
The sense inventory used for tagging is WordNet
1.7. Finally, SensEval-38 English all-words cor-
pus (hereinafter SE3) (Snyder and Palmer, 2004),
is made up of 5,000 words, extracted from two
WSJ articles and one excerpt from the Brown Cor-
pus. Sense repository of WordNet 1.7.1 was used
to tag 2,041 words with their proper senses.
7http://www.sle.sharp.co.uk/senseval2
8http://www.senseval.org/senseval3
3.3 Feature types
We have defined a set of features to represent the
examples according to previous works in WSD
and the nature of class-based WSD. Features
widely used in the literature as in (Yarowsky,
1994) have been selected. These features are
pieces of information that occur in the context of
the target word, and can be organized as:
Local features: bigrams and trigrams that
contain the target word, including part-of-speech
(PoS), lemmas or word-forms.
Topical features: word?forms or lemmas ap-
pearing in windows around the target word.
In particular, our systems use the following ba-
sic features:
Word?forms and lemmas in a window of 10
words around the target word
PoS: the concatenation of the preced-
ing/following three/five PoS
Bigrams and trigrams formed by lemmas and
word-forms and obtained in a window of 5 words.
We use of all tokens regardless their PoS to build
bi/trigrams. The target word is replaced by X
in these features to increase the generalization of
them for the semantic classifiers
Moreover, we also defined a set of Semantic
Features to explode different semantic resources
in order to enrich the set of basic features:
Most frequent semantic class calculated over
SemCor, the most frequent semantic class for the
target word.
Monosemous semantic classes semantic
classes of the monosemous words arround the
target word in a window of size 5. Several types
of semantic classes have been considered to create
these features. In particular, two different sets
of BLC (BLC20 and BLC509), SuperSenses,
WordNet Domains (WND) and SUMO.
In order to increase the generalization capabil-
ities of the classifiers we filter out irrelevant fea-
tures. We measure the relevance of a feature10. f
for a class c in terms of the frequency of f. For each
class c, and for each feature f of that class, we cal-
culate the frequency of the feature within the class
(the number of times that it occurs in examples
9We have selected these set since they represent different
levels of abstraction. Remember that 20 and 50 refer to the
threshold of minimum number of synsets that a possible BLC
must subsume to be considered as a proper BLC. These BLC
sets were built using all kind of relations.
10That is, the value of the feature, for example a feature
type can be word-form, and a feature of that type can be
?houses?
392
of the class), and also obtain the total frequency
of the feature, for all the classes. We divide both
values (classFreq / totalFreq) and if the result is
not greater than a certain threshold t, the feature
is removed from the feature list of the class c11.
In this way, we ensure that the features selected
for a class are more frequently related with that
class than with others. We set this threshold t to
0.25, obtained empirically with very preliminary
versions of the classifiers on SensEval3 test.
4 Experiments and Results
To analyze the influence of each feature type in the
class-based WSD, we designed a large set of ex-
periments. An experiment is defined by two sets of
semantic classes. First, the semantic class type for
selecting the examples used to build the classifiers
(determining the abstraction level of the system).
In this case, we tested: sense12, BLC20, BLC50,
WordNet Domains (WND), SUMO and Super-
Sense (SS). Second, the semantic class type used
for building the semantic features. In this case, we
tested: BLC20, BLC50, SuperSense, WND and
SUMO. Combining them, we generated the set of
experiments described later.
Test pos Sense BLC20 BLC50 WND SUMO SS
SE2 N 4.02 3.45 3.34 2.66 3.33 2.73V 9.82 7.11 6.94 2.69 5.94 4.06
SE3 N 4.93 4.08 3.92 3.05 3.94 3.06V 10.95 8.64 8.46 2.49 7.60 4.08
Table 3: Average polysemy on SE2 and SE3
Table 3 presents the average polysemy on SE2
and SE3 of the different semantic classes.
4.1 Baselines
The most frequent classes (MFC) of each word
calculated over SemCor are considered to be the
baselines of our systems. Ties between classes on
a specific word are solved obtaining the global fre-
quency in SemCor of each of these tied classes,
and selecting the more frequent class over the
whole training corpus. When there are no occur-
rences of a word of the test corpus in SemCor (we
are not able to calculate the most frequent class of
the word), we obtain again the global frequency
for each of its possible semantic classes (obtained
11Depending on the experiment, around 30% of the origi-
nal features are removed by this filter.
12We included this evaluation for comparison purposes
since the current system have been designed for class-based
evaluation only.
from WN) over SemCor, and we select the most
frequent.
4.2 Results
Tables 4 and 5 present the F1 measures (harmonic
mean of recall and precision) for nouns and verbs
respectively when training our systems on Sem-
Cor and testing on SE2 and SE3. Those results
showing a statistically significant13 positive dif-
ference when compared with the baseline are in
marked bold. Column labeled as ?Class? refers to
the target set of semantic classes for the classifiers,
that is, the desired semantic level for each exam-
ple. Column labeled as ?Sem. Feat.? indicates
the class of the semantic features used to train the
classifiers. For example, class BLC20 combined
with Semantic Feature BLC20 means that this set
of classes were used both to label the test exam-
ples and to define the semantic features. In order
to compare their contribution we also performed
a ?basicFeat? test without including semantic fea-
tures.
As expected according to most literature in
WSD, the performances of the MFC baselines are
very high. In particular, those corresponding to
nouns (ranging from 70% to 80%). While nom-
inal baselines seem to perform similarly in both
SE2 and SE3, verbal baselines appear to be con-
sistently much lower for SE2 than for SE3. In
SE2, verbal baselines range from 44% to 68%
while in SE3 verbal baselines range from 52% to
79%. An exception is the results for verbs con-
sidering WND: the results are very high due to
the low polysemy for verbs according to WND.
As expected, when increasing the level of abstrac-
tion (from senses to SuperSenses) the results also
increase. Finally, it also seems that SE2 task is
more difficult than SE3 since the MFC baselines
are lower.
As expected, the results of the systems increase
while augmenting the level of abstraction (from
senses to SuperSenses), and almost in every case,
the baseline results are reached or outperformed.
This is very relevant since the baseline results are
very high.
Regarding nouns, a very different behaviour is
observed for SE2 and SE3. While for SE3 none
of the system presents a significant improvement
over the baselines, for SE2 a significant improve-
ment is obtained by using several types of seman-
13Using the McNemar?s test.
393
tic features. In particular, when using WordNet
Domains but also BLC20. In general, BLC20 se-
mantic features seem to be better than BLC50 and
SuperSenses.
Regarding verbs, the system obtains significant
improvements over the baselines using different
types of semantic features both in SE2 and SE3.
In particular, when using again WordNet Domains
as semantic features.
In general, the results obtained by BLC20 are
not so much different to the results of BLC50
(in a few cases, this difference is greater than
2 points). For instance, for nouns, if we con-
sider the number of classes within BLC20 (558
classes), BLC50 (253 classes) and SuperSense (24
classes), BLC classifiers obtain high performance
rates while maintaining much higher expressive
power than SuperSenses. In fact, using Super-
Senses (40 classes for nouns and verbs) we can
obtain a very accurate semantic tagger with per-
formances close to 80%. Even better, we can use
BLC20 for tagging nouns (558 semantic classes
and F1 over 75%) and SuperSenses for verbs (14
semantic classes and F1 around 75%).
Obviously, the classifiers using WordNet Do-
mains as target grouping obtain very high per-
formances due to its reduced average polysemy.
However, when used as semantic features it seems
to improve the results in most of the cases.
In addition, we obtain very competitive classi-
fiers at a sense level.
4.3 Learning curves
We also performed a set of experiments for mea-
suring the behaviour of the class-based WSD sys-
tem when gradually increasing the number of
training examples. These experiments have been
carried for nouns and verbs, but only noun results
are shown since in both cases, the trend is very
similar but more clear for nouns.
The training corpus has been divided in portions
of 5% of the total number of files. That is, com-
plete files are added to the training corpus of each
incremental test. The files were randomly selected
to generate portions of 5%, 10%, 15%, etc. of the
SemCor corpus14. Then, we train the system on
each of the training portions and we test the sys-
tem on SE2 and SE3. Finally, we also compare the
14Each portion contains also the same files than the previ-
ous portion. For example, all files in the 25% portion are also
contained in the 30% portion.
Class Sem. Feat. SensEval2 SensEval3Poly All Poly All
Sense
baseline 59.66 70.02 64.45 72.30
basicFeat 61.13 71.20 65.45 73.15
BLC20 61.93 71.79 65.45 73.15
BLC50 61.79 71.69 65.30 73.04
SS 61.00 71.10 64.86 72.70
WND 61.13 71.20 65.45 73.15
SUMO 61.66 71.59 65.45 73.15
BLC20
baseline 65.92 75.71 67.98 76.29
basicFeat 65.65 75.52 64.64 73.82
BLC20 68.70 77.69 68.29 76.52
BLC50 68.83 77.79 67.22 75.73
SS 65.12 75.14 64.64 73.82
WND 68.97 77.88 65.25 74.24
SUMO 68.57 77.60 64.49 73.71
BLC50
baseline 67.20 76.65 68.01 76.74
basicFeat 64.28 74.57 66.77 75.84
BLC20 69.72 78.45 68.16 76.85
BLC50 67.20 76.65 68.01 76.74
SS 65.60 75.52 65.07 74.61
WND 70.39 78.92 65.38 74.83
SUMO 71.31 79.58 66.31 75.51
WND
baseline 78.97 86.11 76.74 83.8
basicFeat 70.96 80.81 67.85 77.64
BLC20 72.53 81.85 72.37 80.79
BLC50 73.25 82.33 71.41 80.11
SS 74.39 83.08 68.82 78.31
WND 78.83 86.01 76.58 83.71
SUMO 75.11 83.55 73.02 81.24
SUMO
baseline 66.40 76.09 71.96 79.55
basicFeat 68.53 77.60 68.10 76.74
BLC20 65.60 75.52 68.10 76.74
BLC50 65.60 75.52 68.72 77.19
SS 68.39 77.50 68.41 76.97
WND 68.92 77.88 69.03 77.42
SUMO 68.92 77.88 70.88 78.76
SS
baseline 70.48 80.41 72.59 81.50
basicFeat 69.77 79.94 69.60 79.48
BLC20 71.47 81.07 72.43 81.39
BLC50 70.20 80.22 72.92 81.73
SS 70.34 80.32 65.12 76.46
WND 73.59 82.47 70.10 79.82
SUMO 70.62 80.51 71.93 81.05
Table 4: Results for nouns
resulting system with the baseline computed over
the same training portion.
Figures 1 and 2 present the learning curves over
SE2 and SE3, respectively, of a class-based WSD
system based on BLC20 using the basic features
and the semantic features built with WordNet Do-
mains.
Surprisingly, in SE2 the system only improves
the F1 measure around 2% while increasing the
training corpus from 25% to 100% of SemCor.
In SE3, the system again only improves the F1
measure around 3% while increasing the training
corpus from 30% to 100% of SemCor. That is,
most of the knowledge required for the class-based
WSD system seems to be already present on a
small part of SemCor.
Figures 3 and 4 present the learning curves over
SE2 and SE3, respectively, of a class-based WSD
system based on SuperSenses using the basic fea-
tures and the semantic features built with WordNet
Domains.
Again, in SE2 the system only improves the F1
394
Class Sem. Feat. SensEval2 SensEval3Poly All Poly All
Sense
baseline 41.20 44.75 49.78 52.88
basicFeat 42.01 45.53 54.19 57.02
BLC20 41.59 45.14 53.74 56.61
BLC50 42.01 45.53 53.6 56.47
SS 41.80 45.34 53.89 56.75
WND 42.01 45.53 53.89 56.75
SUMO 42.22 45.73 54.19 57.02
BLC20
baseline 50.21 55.13 54.87 58.82
basicFeat 52.36 57.06 57.27 61.10
BLC20 52.15 56.87 56.07 59.92
BLC50 51.07 55.90 56.82 60.60
SS 51.50 56.29 57.57 61.29
WND 54.08 58.61 57.12 60.88
SUMO 52.36 57.06 57.42 61.15
BLC50
baseline 49.78 54.93 55.96 60.06
basicFeat 53.23 58.03 58.07 61.97
BLC20 52.59 57.45 57.32 61.29
BLC50 51.72 56.67 57.01 61.01
SS 52.59 57.45 57.92 61.83
WND 55.17 59.77 58.52 62.38
SUMO 52.16 57.06 57.92 61.83
WND
baseline 84.80 90.33 84.96 92.20
basicFeat 84.50 90.14 78.63 88.92
BLC20 84.50 90.14 81.53 90.42
BLC50 84.50 90.14 81.00 90.15
SS 83.89 89.75 78.36 88.78
WND 85.11 90.52 84.96 92.20
SUMO 85.11 90.52 80.47 89.88
SUMO
baseline 54.24 60.35 59.69 64.71
basicFeat 56.25 62.09 61.41 66.21
BLC20 55.13 61.12 61.25 66.07
BLC50 56.25 62.09 61.72 66.48
SS 53.79 59.96 59.69 64.71
WND 55.58 61.51 61.56 66.35
SUMO 54.69 60.74 60.00 64.98
SS
baseline 62.79 68.47 76.24 79.07
basicFeat 66.89 71.95 75.47 78.39
BLC20 63.70 69.25 74.69 77.70
BLC50 63.70 69.25 74.69 77.70
SS 63.70 69.25 74.84 77.84
WND 66.67 71.76 77.02 79.75
SUMO 64.84 70.21 74.69 77.70
Table 5: Results for verbs
measure around 2% while increasing the training
corpus from 25% to 100% of SemCor. In SE3,
the system again only improves the F1 measure
around 2% while increasing the training corpus
from 30% to 100% of SemCor. That is, with only
25% of the whole corpus, the class-based WSD
system reaches a F1 close to the performance us-
ing all corpus. This evaluation seems to indicate
that the class-based approach to WSD reduces dra-
matically the required amount of training exam-
ples.
In both cases, when using BLC20 or Super-
Senses as semantic classes for tagging, the be-
haviour of the system is similar to MFC baseline.
This is very interesting since the MFC obtains high
results due to the way it is defined, since the MFC
over the total corpus is assigned if there are no oc-
currences of the word in the training corpus. With-
out this definition, there would be a large number
of words in the test set with no occurrences when
using small training portions. In these cases, the
recall of the baselines (and in turn F1) would be
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV2MFC SV2
Figure 1: Learning curve of BLC20 on SE2
 62
 64
 66
 68
 70
 72
 74
 76
 78
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV3MFC SV3
Figure 2: Learning curve of BLC20 on SE3
much lower.
5 Conclusions and discussion
We explored on the WSD task the performance
of different levels of abstraction and sense group-
ings. We empirically demonstrated that Base
Level Concepts are able to group word senses into
an adequate medium level of abstraction to per-
form supervised class?based disambiguation. We
also demonstrated that the semantic classes pro-
vide a rich information about polysemous words
and can be successfully used as semantic fea-
tures. Finally we confirm the fact that the class?
based approach reduces dramatically the required
amount of training examples, opening the way to
solve the well known acquisition bottleneck prob-
lem for supervised machine learning algorithms.
In general, the results obtained by BLC20 are
not very different to the results of BLC50. Thus,
we can select a medium level of abstraction, with-
out having a significant decrease of the perfor-
395
 68
 70
 72
 74
 76
 78
 80
 82
 84
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV2MFC SV2
Figure 3: Learning curve of SuperSense on SE2
 70
 72
 74
 76
 78
 80
 82
 5  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100
F1
% corpus
System SV3MFC SV3
Figure 4: Learning curve of SuperSense on SE3
mance. Considering the number of classes, BLC
classifiers obtain high performance rates while
maintaining much higher expressive power than
SuperSenses. However, using SuperSenses (46
classes) we can obtain a very accurate semantic
tagger with performances around 80%. Even bet-
ter, we can use BLC20 for tagging nouns (558 se-
mantic classes and F1 over 75%) and SuperSenses
for verbs (14 semantic classes and F1 around
75%).
As BLC are defined by a simple and fully au-
tomatic method, they can provide a user?defined
level of abstraction that can be more suitable for
certain NLP tasks.
Moreover, the traditional set of features used for
sense-based classifiers do not seem to be the most
adequate or representative for the class-based ap-
proach. We have enriched the usual set of fea-
tures, by adding semantic information from the
monosemous words of the context and the MFC
of the target word. With this new enriched set of
features, we can generate robust and competitive
class-based classifiers.
To our knowledge, the best results for class?
based WSD are those reported by (Ciaramita and
Altun, 2006). This system performs a sequence
tagging using a perceptron?trained HMM, using
SuperSenses, training on SemCor and testing on
SensEval3. The system achieves an F1?score of
70.54, obtaining a significant improvement from
a baseline system which scores only 64.09. In
this case, the first sense baseline is the SuperSense
of the most frequent synset for a word, according
to the WN sense ranking. Although this result is
achieved for the all words SensEval3 task, includ-
ing adjectives, we can compare both results since
in SE2 and SE3 adjectives obtain very high per-
formance figures. Using SuperSenses, adjectives
only have three classes (WN Lexicographic Files
00, 01 and 44) and more than 80% of them belong
to class 00. This yields to really very high perfor-
mances for adjectives which usually are over 90%.
As we have seen, supervised WSD systems are
very dependent of the corpora used to train and
test the system. We plan to extend our system by
selecting new corpora to train or test. For instance,
by using the sense annotated glosses from Word-
Net.
References
E. Agirre and O. LopezDeLaCalle. 2003. Clustering
wordnet word senses. In Proceedings of RANLP?03,
Borovets, Bulgaria.
J. Alvez, J. Atserias, J. Carrera, S. Climent, E. Laparra,
A. Oliver, and G. Rigau G. 2008. Complete and
consistent annotation of wordnet using the top con-
cept ontology. In 6th International Conference on
Language Resources and Evaluation LREC, Mar-
rakesh, Morroco.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP?06), pages 594?602,
Sydney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in wordnet. In Proceedings
of the Conference on Empirical methods in natural
language processing (EMNLP?03), pages 168?175.
ACL.
J. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL?05), pages 26?33. ACL.
396
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
M. Hearst and H. Schu?tze. 1993. Customizing a lexi-
con to better suit a computational task. In Proceed-
ingns of the ACL SIGLEX Workshop on Lexical Ac-
quisition, Stuttgart, Germany.
R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor-
ing the automatic selection of basic level concepts.
In Galia Angelova et al, editor, International Con-
ference Recent Advances in Natural Language Pro-
cessing, pages 298?302, Borovets, Bulgaria.
T. Joachims. 1998. Text categorization with support
vector machines: learning with many relevant fea-
tures. In Claire Ne?dellec and Ce?line Rouveirol, edi-
tors, Proceedings of ECML-98, 10th European Con-
ference on Machine Learning, number 1398, pages
137?142, Chemnitz, DE. Springer Verlag, Heidel-
berg, DE.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
Ll. Ma`rquez, G. Escudero, D. Mart??nez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In
E. Agirre and P. Edmonds (Eds.) Word Sense Disam-
biguation: Algorithms and applications., volume 33
of Text, Speech and Language Technology. Springer.
R. Mihalcea and D. Moldovan. 2001. Automatic gen-
eration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Cus-
tomizations, Pittsburg, USA.
R. Mihalcea. 2007. Using wikipedia for automatic
word sense disambiguation. In Proceedings of
NAACL HLT 2007.
G. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993.
A Semantic Concordance. In Proceedings of the
ARPA Workshop on Human Language Technology.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In ACL-44: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 105?112, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
I. Niles and A. Pease. 2001. Towards a standard upper
ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001), pages 17?19. Chris Welty and
Barry Smith, eds.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-
words and verb lexical sample. In Proceedings
of the SENSEVAL-2 Workshop. In conjunction with
ACL?2001/EACL?2001, Toulouse, France.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), Granada, Spain.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on
Automatic Information Extraction and Building of
Lexical Semantic Resources for NLP Applications,
pages 78?81. ACL, New Brunswick, New Jersey.
R. Snow, Prakash S., Jurafsky D., and Ng A. 2007.
Learning to merge word senses. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1005?
1014.
B. Snyder and M. Palmer. 2004. The english all-words
task. In Rada Mihalcea and Phil Edmonds, edi-
tors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis
of Text, pages 41?43, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics.
L. Villarejo, L. Ma`rquez, and G. Rigau. 2005. Ex-
ploring the construction of semantic class classi-
fiers for wsd. In Proceedings of the 21th Annual
Meeting of Sociedad Espaola para el Procesamiento
del Lenguaje Natural SEPLN?05, pages 195?202,
Granada, Spain, September. ISSN 1136-5948.
P. Vossen, L. Bloksma, H. Rodriguez, S. Climent,
N. Calzolari, A. Roventini, F. Bertagna, A. Alonge,
and W. Peters. 1998. The eurowordnet base con-
cepts and top ontology. Technical report, Paris,
France, France.
D. Yarowsky. 1994. Decision lists for lexical ambigu-
ity resolution: Application to accent restoration in
spanish and french. In Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics (ACL?94).
397
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
  	
 ffTALP System for the English Lexical Sample Task
Gerard Escudero?, Llu??s Ma`rquez? and German Rigau?
?TALP Research Center. EUETIB. LSI. UPC. escudero@lsi.upc.es
?TALP Research Center. LSI. UPC. lluism@lsi.upc.es
? IXA Group. UPV/EHU. rigau@si.ehu.es
1 Introduction
This paper describes the TALP system on the En-
glish Lexical Sample task of the Senseval-31 event.
The system is fully supervised and relies on a par-
ticular Machine Learning algorithm, namely Sup-
port Vector Machines. It does not use extra exam-
ples than those provided by Senseval-3 organisers,
though it uses external tools and ontologies to ex-
tract part of the representation features.
Three main characteristics have to be pointed out
from the system architecture. The first thing is the
way in which the multiclass classification problem
posed by WSD is addressed using the binary SVM
classifiers. Two different approaches for binarizing
multiclass problems have been tested: one?vs?all
and constraint classification. In a cross-validation
experimental setting the best strategy has been se-
lected at word level. Section 2 is devoted to explain
this issue in detail.
The second characteristic is the rich set of fea-
tures used to represent training and test examples.
Topical and local context features are used as usual,
but also syntactic relations and semantic features in-
dicating the predominant semantic classes in the ex-
ample context are taken into account. A detailed
description of the features is presented in section 3.
And finally, since each word represents a learning
problem with different characteristics, a per?word
feature selection has been applied. This tuning pro-
cess is explained in detail in section 4.
The last two sections discuss the experimental re-
sults (section 5) and present the main conclusions of
the work performed (section 6).
2 Learning Framework
The TALP system belongs to the supervised Ma-
chine Learning family. Its core algorithm is the
Support Vector Machines (SVM) learning algorithm
(Cristianini and Shawe-Taylor, 2000). Given a set
of binary training examples, SVMs find the hy-
perplane that maximizes the margin in a high di-
1http://www.senseval.org
mensional feature space (transformed from the in-
put space through the use of a non-linear function,
and implicitly managed by using the kernel trick),
i.e., the hyperplane that separates with maximal dis-
tance the positive examples from the negatives. This
learning bias has proven to be very effective for pre-
venting overfitting and providing good generalisa-
tion. SVMs have been also widely used in NLP
problems and applications.
One of the problems in using SVM for the WSD
problem is how to binarize the multiclass classifi-
cation problem. The two approximations tested in
the TALP system are the usual one?vs?all and the
recently introduced constraint?classification frame-
work (Har-Peled et al, 2002).
In the one?vs?all approach, the problem is de-
composed into as many binary problems as classes
has the original problem, and one classifier is
trained for each class trying to separate the exam-
ples of that class (positives) from the examples of
all other classes (negatives). This method assumes
the existence of a separator between each class and
the set of all other classes. When classifying a new
example, all binary classifiers predict a class and
the one with highest confidence is selected (winner?
take?all strategy).
2.1 Constraint Classification
Constraint classification (Har-Peled et al, 2002) is
a learning framework that generalises many multi-
class classification and ranking schemes. It consists
of labelling each example with a set of binary con-
straints indicating the relative order between pairs
of classes. For the WSD setting of Senseval-3, we
have one constraint for each correct class (sense)
with each incorrect class, indicating that the clas-
sifier to learn should give highest confidence to the
correct classes than to the negatives. For instance, if
we have 4 possible senses {1, 2, 3, 4} and a training
example with labels 2 and 3, the constraints corre-
sponding to the example are {(2>1), (2>4), (3>1),
and (3>4)}. The aim of the methodology is to learn
a classifier consistent with the partial order defined
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
by the constraints. Note that here we are not as-
suming that perfect separators can be constructed
between each class and the set of all other classes.
Instead, the binary decisions imposed are more con-
servative.
Using Kesler?s construction for multiclass classi-
fication, each training example is expanded into a
set of (longer) binary training examples. Finding
a vector?based separator in this new training set is
equivalent to find a separator for each of the binary
constraints imposed by the problem. The construc-
tion is general, so we can use SVMs directly on the
expanded training set to solve the multiclass prob-
lem. See (Har-Peled et al, 2002) for details.
3 Features
We have divided the features of the system in 4 cat-
egories: local, topical, knowledge-based and syn-
tactic features. First section of table 1 shows the
local features. The basic aim of these features
is to modelize the information of the surrounding
words of the target word. All these features are ex-
tracted from a ?3?word?window centred on the tar-
get word. The features also contain the position of
all its components. To obtain Part?of?Speech and
lemma for each word, we used FreeLing 2. Most
of these features have been doubled for lemma and
word form.
Three types of Topical features are shown in the
second section of table 1. Topical features try to
obtain non?local information from the words of the
context. For each type, two overlapping sets of
redundant topical features are considered: one ex-
tracted from a ?10?word?window and another con-
sidering all the example.
The third section of table 1 presents the
knowledge?based features. These features have
been obtained using the knowledge contained into
the Multilingual Central Repository (MCR) of the
MEANING project3 (Atserias et al, 2004). For each
example, the feature extractor obtains, from each
context, all nouns, all their synsets and their associ-
ated semantic information: Sumo labels, domain la-
bels, WordNet Lexicographic Files, and EuroWord-
Net Top Ontology. We also assign to each label a
weight which depends on the number of labels as-
signed to each noun and their relative frequencies
in the whole WordNet. For each kind of seman-
tic knowledge, summing up all these weights, the
program finally selects those semantic labels with
higher weights.
2http://www.lsi.upc.es/?nlp/freeling
3http://www.lsi.upc.es/?meaning
local feats.
Feat. Description
form form of the target word
locat all part?of?speech / forms / lemmas in
the local context
coll all collocations of two part?of?speech /
forms / lemmas
coll2 all collocations of a form/lemma and a
part?of?speech (and the reverse)
first form/lemma of the first noun / verb /
adjective / adverb to the left/right of the
target word
topical feats.
Feat. Description
topic bag of forms/lemmas
sbig all form/lemma bigrams of the example
comb forms/lemmas of consecutive (or not)
pairs of the open?class?words in the
example
knowledge-based feats.
Feat. Description
f sumo first sumo label
a sumo all sumo labels
f semf first wn semantic file label
a semf all wn semantic file labels
f tonto first ewn top ontology label
a tonto all ewn top ontology labels
f magn first domain label
a magn all domain labels
syntactical feats.
Feat. Description
tgt mnp syntactical relations of the target word
from minipar
rels mnp all syntactical relations from minipar
yar noun NounModifier, ObjectTo, SubjectTo
for nouns
yar verb Object, ObjectToPreposition, Preposi-
tion for verbs
yar adjs DominatingNoun for adjectives
Table 1: Feature Set
Finally, the last section of table 1 describes
the syntactic features which contains features ex-
tracted using two different tools: Dekang Lin?s
Minipar4 and Yarowsky?s dependency pattern ex-
tractor.
It is worth noting that the set of features presented
is highly redundant. Due to this fact, a feature se-
lection process has been applied, which is detailed
in the next section.
4 Experimental Setting
For each binarization approach, we performed a fea-
ture selection process consisting of two consecutive
steps:
4http://www.cs.ualberta.ca/?lindek/minipar.htm
? POS feature selection: Using the Senseval?2
corpus, an exhaustive selection of the best set
of features for each particular Part?of?Speech
was performed. These feature sets were taken
as the initial sets in the feature selection pro-
cess of Senseval-3.
? Word feature selection: We applied a
forward(selection)?backward(deletion) two?
step procedure to obtain the best feature
selection per word. For each word, the process
starts with the best feature set obtained in the
previous step according to its Part?of?Speech.
Now, during selection, we consider those
features not selected during POS feature
selection, adding all features which produce
some improvement. During deletion, we con-
sider only those features selected during POS
feature selection, removing all features which
produces some improvement. Although this
addition?deletion procedure could be iterated
until no further improvement is achieved, we
only performed a unique iteration because
of the computational overhead. One brief
experiment (not reported here) for one?vs?all
achieves an increase of 2.63% in accuracy
for the first iteration and 0.52% for a second
one. First iteration improves the accuracy of
53 words and the second improves only 15.
Comparing the evolution of these 15 words,
the increase in accuracy is of 2.06% for the
first iteration and 1.68% for the second one.
These results may suggest that accuracy could
be increased by this iteration procedure.
The result of this process is the selection of the
best binarization approach and the best feature set
for each individual word.
Considering feature selection, we have inspected
the selected attributes for all the words and we ob-
served that among these attributes there are fea-
tures of all four types. The most selected features
are the local ones, and among them those of ?first
noun/adjective on the left/right?; from topical fea-
tures the most selected ones are the ?comb? and in a
less measure the ?topic?; from the knowledge?based
the most selected feature are those of ?sumo? and
?domains labels?; and from syntactical ones, those
of ?Yarowsky?s patterns?. All the features previ-
ously mentioned where selected at least for 50 of
the 57 Senseval?3 words. Even so, it is useful the
use of all features when a selection procedure is ap-
plied. These general features do not work fine for
all words. Some words make use of the less selected
features; that is, every word is a different problem.
Regarding the implementation details of the sys-
tem, we used SVMlight (Joachims, 2002), a very ro-
bust and complete implementation of Support Vec-
tor Machines learning algorithms, which is freely
available for research purposes5 . A simple lineal
kernel with a regularization C value of 0.1 was
applied. This parameter was empirically decided
on the basis of our previous experiments on the
Senseval?2 corpus. Additionally, previous tests us-
ing non?linear kernels did not provide better results.
The selection of the best feature set and the bi-
narization scheme per word described above, have
been performed using a 5-fold cross validation pro-
cedure on the Senseval-3 training set. The five parti-
tions of the training set were obtained maintaining,
as much as possible, the initial distribution of exam-
ples per sense.
After several experiments considering the ?U? la-
bel as an additional regular class, we found that we
obtained better results by simply ignoring it. Then,
if a training example was tagged only with this la-
bel, it was removed from the training set. If the ex-
ample was tagged with this label and others, the ?U?
label was also removed from the learning example.
In that way, the TALP system do not assigns ?U?
labels to the test examples.
Due to lack of time, the TALP system presented
at the competition time did not include a com-
plete model selection for the constraint classifica-
tion binarization setting. More precisely, 14 words
were processed within the complete model selection
framework, and 43 were adjusted with a fixed one?
vs?all approach but a complete feature selection.
After the competition was closed, we implemented
the constraint classification setting more efficiently
and we reprocessed again the data. Section 5 shows
the results of both variants.
A rough estimation of the complete model selec-
tion time for both approaches is the following. The
training spent about 12 hours (OVA setting) and 5
days (CC setting) to complete6 , suggesting that the
main drawback of these approaches is the computa-
tional overhead. Fortunately, the process time can
be easily reduced: the CC layer could be ported
from Perl to C++ and the model selection could be
easily parallelized (since the treatment of each word
is independent).
5 Results
Table 2 shows the accuracy obtained on the train-
ing set and table 3 the results of our system (SE3,
5http://svmlight.joachims.org
6These figures were calculated using a 800 MHz Pentium
III PC with 320 Mb of memory.
TALP), together with the most frequent sense base-
line (mfs), the recall result of the best system in the
task (best), and the recall median between all par-
ticipant systems (avg). These last three figures were
provided provided by the organizers of the task.
OVA(base) in table 2 stands for the results of the
one?vs?all approach on the starting feature set (5?
fold?cross validation on the training set). CC(base)
refers to the constrain?classification setting on the
starting feature set. OVA(best) and CC(best) mean
one?vs?all and constraint?classification with their
respective feature selection. Finally, SE3 stands for
the system officially presented at competition time7
and TALP stands for the complete architecture.
method accuracy
OVA(base) 72 38%
CC(base) 72.28%
OVA(best) 75.27%
CC(best) 75.70%
SE3 75.62%
TALP 76.02%
Table 2: Overall results of all system variants on the
training set
It can be observed that the feature selection pro-
cess consistently improves the accuracy by around
3 points, both in OVA and CC binarization set-
tings. Constraint?classification is slightly better
than one?vs?all approach when feature selection
is performed, though this improvement is not con-
sistent along all individual words (detailed results
omitted) neither statistically significant (z?test with
0.95 confidence level). Finally, the combined
binarization?feature selection further increases the
accuracy in half a point (again this difference is not
statistically significant).
measure mfs avg best SE3 TALP
fine 55.2 65.1 72.9 71.3 71.6
coarse 64.5 73.7 79.5 78.2 78.2
Table 3: Overall results on the Senseval-3 test set
However, when testing the complete architecture
on the official test set, we obtained an accuracy de-
crease of more than 4 points. It remains to be ana-
lyzed if this difference is due to a possible overfit-
ting to the training corpus during model selection,
or simply is due to the differences between train-
ing and test corpora. Even so, the TALP system
achieves a very good performance, since there is a
7Only 14 words were processed with the full architecture.
difference of only 1.3 points in fine and coarse re-
call respect to the best system of the English lexical
sample task of Senseval?3.
6 Conclusions
Regarding supervised Word Sense Disambiguation,
each word can be considered as a different classi-
fication problem. This implies that each word has
different feature models to describe its senses.
We have proposed and tested a supervised sys-
tem in which the examples are represented through
a very rich and redundant set of features (using the
information content coherently integrated within the
Multilingual Central Repository of the MEANING
project), and which performs a specialized selection
of features and binarization process for each word.
7 Acknowledgments
This research has been partially funded by the Eu-
ropean Commission (Meaning Project, IST-2001-
34460), and by the Spanish Research Department
(Hermes Project: TIC2000-0335-C03-02).
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, P. Vossen 2004. The MEAN-
ING Multilingual Central Repository. In Pro-
ceedings of the Second International WordNet
Conference.
N. Cristianini and J. Shawe-Taylor 2000. An Intro-
duction to Support Vector Machines. Cambridge
University Press.
T. Joachims 2002. Learning to Classify Text Using
Support Vector Machines. Dissertation, Kluwer.
S. Har-Peled and D. Roth and D. Zimak 2002. Con-
straint Classification for Multiclass Classification
and Ranking. In Proceedings of the 15th Work-
shop on Neural Information Processing Systems.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 534?541,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Quality Assessment of Large Scale Knowledge Resources
Montse Cuadros
IXA NLP Group
EHU/UPV
Donostia, Basque Country
mcuadros001@ikasle.ehu.es
German Rigau
IXA NLP Group
EHU/UPV
Donostia, Basque Country
german.rigau@ehu.es
Abstract
This paper presents an empirical eval-
uation of the quality of publicly avail-
able large-scale knowledge resources. The
study includes a wide range of manu-
ally and automatically derived large-scale
knowledge resources. In order to establish
a fair and neutral comparison, the qual-
ity of each knowledge resource is indi-
rectly evaluated using the same method on
a Word Sense Disambiguation task. The
evaluation framework selected has been
the Senseval-3 English Lexical Sample
Task. The study empirically demonstrates
that automatically acquired knowledge re-
sources surpass both in terms of preci-
sion and recall the knowledge resources
derived manually, and that the combina-
tion of the knowledge contained in these
resources is very close to the most frequent
sense classifier. As far as we know, this is
the first time that such a quality assessment
has been performed showing a clear pic-
ture of the current state-of-the-art of pub-
licly available wide coverage semantic re-
sources.
1 Introduction
Using large-scale semantic knowledge bases, such
as WordNet (Fellbaum, 1998), has become a
usual, often necessary, practice for most current
Natural Language Processing systems. Even now,
building large and rich enough knowledge bases
for broad?coverage semantic processing takes a
great deal of expensive manual effort involving
large research groups during long periods of de-
velopment. This fact has severely hampered the
state-of-the-art of current Natural Language Pro-
cessing (NLP) applications. For example, dozens
of person-years have been invested in the develop-
ment of wordnets for various languages (Vossen,
1998), but the data in these resources seems not to
be rich enough to support advanced concept-based
NLP applications directly. It seems that applica-
tions will not scale up to working in open domains
without more detailed and rich general-purpose
(and also domain-specific) linguistic knowledge
built by automatic means.
For instance, in more than eight years of man-
ual construction (from version 1.5 to 2.0), Word-
Net passed from 103,445 semantic relations to
204,074 semantic relations1. That is, around
twelve thousand semantic relations per year. How-
ever, during the last years the research commu-
nity has devised a large set of innovative processes
and tools for large-scale automatic acquisition of
lexical knowledge from structured or unstructured
corpora. Among others we can mention eX-
tended WordNet (Mihalcea and Moldovan, 2001),
large collections of semantic preferences acquired
from SemCor (Agirre and Martinez, 2001; Agirre
and Martinez, 2002) or acquired from British Na-
tional Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired
from the web (Agirre and de la Calle, 2004) or
acquired from the BNC (Cuadros et al, 2005).
Obviously, all these semantic resources have
been acquired using a very different set of meth-
ods, tools and corpora, resulting on a different set
of new semantic relations between synsets. In fact,
each resource has different volume and accuracy
figures. Although isolated evaluations have been
performed by their developers in different experi-
1Symmetric relations are counted only once.
534
mental settings, to date no comparable evaluation
has been carried out in a common and controlled
framework.
This work tries to establish the relative qual-
ity of these semantic resources in a neutral envi-
ronment. The quality of each large-scale knowl-
edge resource is indirectly evaluated on a Word
Sense Disambiguation (WSD) task. In particular,
we use a well defined WSD evaluation benchmark
(Senseval-3 English Lexical Sample task) to eval-
uate the quality of each resource.
Furthermore, this work studies how these re-
sources complement each other. That is, to which
extent each knowledge base provides new knowl-
edge not provided by the others.
This paper is organized as follows: after this
introduction, section 2 describes the large-scale
knowledge resources studied in this work. Section
3 describes the evaluation framework. Section 4
presents the evaluation results of the different se-
mantic resources considered. Section 5 provides a
qualitative assessment of this empirical study and
finally, the conclusions and future work are pre-
sented in section 6.
2 Large Scale Knowledge Resources
This study covers a wide range of large-scale
knowledge resources: WordNet (WN) (Fell-
baum, 1998), eXtended WordNet (Mihalcea and
Moldovan, 2001), large collections of semantic
preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or
acquired from the BNC (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired
from the web (Agirre and de la Calle, 2004) or
acquired from the BNC (Cuadros et al, 2005).
However, although these resources have been
derived using different WN versions, the research
community has the technology for the automatic
alignment of wordnets (Daude? et al, 2003). This
technology provides a mapping among synsets of
different WN versions, maintaining the compati-
bility to all the knowledge resources which use
a particular WN version as a sense repository.
Furthermore, this technology allows to port the
knowledge associated to a particular WN version
to the rest of WN versions already connected.
Using this technology, most of these resources
are integrated into a common resource called Mul-
tilingual Central Repository (MCR) (Atserias et
al., 2004). In particular, all WordNet versions, eX-
tended WordNet, and the semantic preferences ac-
quired from SemCor and BNC.
2.1 Multilingual Central Repository
The Multilingual Central Repository (MCR)2 fol-
lows the model proposed by the EuroWordNet
project. EuroWordNet (Vossen, 1998) is a multi-
lingual lexical database with wordnets for several
European languages, which are structured as the
Princeton WordNet. The Princeton WordNet con-
tains information about nouns, verbs, adjectives
and adverbs in English and is organized around the
notion of a synset. A synset is a set of words with
the same part-of-speech that can be interchanged
in a certain context. For example, <party, po-
litical party> form a synset because they can be
used to refer to the same concept. A synset is
often further described by a gloss, in this case:
?an organization to gain political power?. Finally,
synsets can be related to each other by semantic
relations, such as hyponymy (between specific and
more general concepts), meronymy (between parts
and wholes), cause, etc.
The current version of the MCR (Atserias et al,
2004) is a result of the 5th Framework MEANING
project. The MCR integrates into the same Eu-
roWordNet framework wordnets from five differ-
ent languages (together with four English Word-
Net versions). The MCR also integrates WordNet
Domains (Magnini and Cavaglia`, 2000) and new
versions of the Base Concepts and Top Concept
Ontology. The final version of the MCR contains
1,642,389 semantic relations between synsets,
most of them acquired by automatic means. This
represents almost one order of magnitude larger
than the Princeton WordNet (204,074 unique se-
mantic relations in WordNet 2.0). Table 1 summa-
rizes the main sources for semantic relations inte-
grated into the MCR.
Table 2 shows the number of semantic relations
between synsets pairs in the MCR and its overlap-
pings. Note that, most of the relations in the MCR
between synsets-pairs are unique.
Hereinafter we will refer to each semantic re-
source as follows:
? WN (Fellbaum, 1998): This knowledge re-
source uses the direct relations encoded in
WordNet 1.6 or 2.0. We also tested WN-2
(using relations at distance 1 and 2) and WN-
3 (using relations at distance 1, 2 and 3).
2http://nipadio.lsi.upc.es/?nlp/meaning
535
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
Selectional Preferences from the BNC 707,618
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 1,642,389
Table 1: Main sources of semantic relations
Type of Relations #relations
Total Relations 1,642,389
Different Relations 1,531,380
Unique Relations 1,390,181
Non-unique relations (>1) 70,425
Non-unique relations (>2) 341
Non-unique relations (>3) 8
Table 2: Overlapping relations in the MCR
? XWN (Mihalcea and Moldovan, 2001): This
knowledge resource uses the direct relations
encoded in eXtended WordNet.
? XWN+WN: This knowledge resource uses
the direct relations included in WN and
XWN.
? spBNC (McCarthy, 2001): This knowledge
resource contains the selectional preferences
acquired from the BNC.
? spSemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002): This knowledge
resource contains the selectional preferences
acquired from SemCor.
? spBNC+spSemCor: This knowledge re-
source uses the selectional preferences ac-
quired from the BNC and SemCor.
? MCR (Atserias et al, 2004): This knowledge
resource uses the direct relations included in
MCR.
2.2 Automatically retrieved Topic Signatures
Topic Signatures (TS) are word vectors related to
a particular topic (Lin and Hovy, 2000). Topic
Signatures are built by retrieving context words
of a target topic from large volumes of text. In
our case, we consider word senses as topics. Ba-
sically, the acquisition of TS consists of A) ac-
quiring the best possible corpus examples for a
particular word sense (usually characterizing each
word sense as a query and performing a search on
the corpus for those examples that best match the
queries), and then, B) building the TS by deriv-
ing the context words that best represent the word
sense from the selected corpora.
For this study, we use the large-scale Topic Sig-
natures acquired from the web (Agirre and de la
Calle, 2004) and those acquired from the BNC
(Cuadros et al, 2005).
? TSWEB3: Inspired by the work of (Lea-
cock et al, 1998), these Topic Signatures
were constructed using monosemous rela-
tives from WordNet (synonyms, hypernyms,
direct and indirect hyponyms, and siblings),
querying Google and retrieving up to one
thousand snippets per query (that is, a word
sense). In particular, the method was as fol-
lows:
? Organizing the retrieved examples from
the web in collections, one collection
per word sense.
? Extracting the words and their frequen-
cies for each collection.
? Comparing these frequencies with those
pertaining to other word senses using
TFIDF (see formula 1).
? Gathering in an ordered list, the words
with distinctive frequency for one of the
collections, which constitutes the Topic
Signature for the respective word sense.
This constitutes the largest available seman-
tic resource with around 100 million relations
(between synsets and words).
? TSBNC: These Topic Signatures have been
constructed using ExRetriever4, a flexible
tool to perform sense queries on large cor-
pora.
? This tool characterizes each sense of a
word as a specific query using a declar-
ative language.
? This is automatically done by using a
particular query construction strategy,
defined a priori, and using information
from a knowledge base.
In this study, ExRetriever has been evaluated
using the BNC, WN as a knowledge base and
3http://ixa.si.ehu.es/Ixa/resources/sensecorpus
4http://www.lsi.upc.es/?nlp/meaning/downloads.html
536
TFIDF (as shown in formula 1) (Agirre and
de la Calle, 2004)5.
TFIDF (w,C) = wfwmaxwwfw ? log
N
Cfw (1)
Where w stands for word context, wf for the
word frecuency, C for Collection (all the cor-
pus gathered for a particular word sense), and
Cf stands for the Collection frecuency.
In this study we consider two different query
strategies:
? Monosemous A (queryA): (OR
monosemous-words). That is, the union
set of all synonym, hyponym and hyper-
onym words of a WordNet synset which are
monosemous nouns (these words can have
other senses as verbs, adjectives or adverbs).
? Monosemous W (queryW): (OR
monosemous-words). That is, the union
set of all words appearing as synonyms,
direct hyponyms, hypernyms indirect hy-
ponyms (distance 2 and 3) and siblings. In
this case, the nouns collected are monose-
mous having no other senses as verbs,
adjectives or adverbs.
While TSWEB use the query construction
queryW, ExRetriever use both.
3 Indirect Evaluation on Word Sense
Disambiguation
In order to measure the quality of the knowl-
edge resources described in the previous section,
we performed an indirect evaluation by using all
these resources as Topic Signatures (TS). That is,
word vectors with weights associated to a partic-
ular synset which are obtained by collecting those
word senses appearing in the synsets directly re-
lated to them 6. This simple representation tries to
be as neutral as possible with respect to the evalu-
ation framework.
All knowledge resources are indirectly evalu-
ated on a WSD task. In particular, the noun-set
5Although other measures have been tested, such as Mu-
tual Information or Association Ratio, the best results have
been obtained using TFIDF formula.
6A weight of 1 is given when the resource do not has as-
sociated weight.
of Senseval-3 English Lexical Sample task which
consists of 20 nouns. All performances are evalu-
ated on the test data using the fine-grained scoring
system provided by the organizers.
Furthermore, trying to be as neutral as possi-
ble with respect to the semantic resources studied,
we applied systematically the same disambigua-
tion method to all of them. Recall that our main
goal is to establish a fair comparison of the knowl-
edge resources rather than providing the best dis-
ambiguation technique for a particular semantic
knowledge base.
A common WSD method has been applied to
all knowledge resources. A simple word over-
lapping counting (or weighting) is performed be-
tween the Topic Signature and the test example7.
Thus, the occurrence evaluation measure counts
the amount of overlapped words and the weight
evaluation measure adds up the weights of the
overlapped words. The synset having higher over-
lapping word counts (or weights) is selected for a
particular test example. However, for TSWEB and
TSBNC the better results have been obtained us-
ing occurrences (the weights are only used to or-
der the words of the vector). Finally, we should
remark that the results are not skewed (for in-
stance, for resolving ties) by the most frequent
sense in WN or any other statistically predicted
knowledge.
Figure 3 presents an example of Topic Signa-
ture from TSWEB using queryW and the web and
from TSBNC using queryA and the BNC for the
first sense of the noun party. Although both auto-
matically acquired TS seem to be closely related to
the first sense of the noun party, they do not have
words in common.
As an example, table 4 shows a test example of
Senseval-3 corresponding to the first sense of the
noun party. In bold there are the words that ap-
pear in TSBNC-queryA. There are several impor-
tant words that appear in the text that also appear
in the TS.
4 Evaluating the quality of knowledge
resources
In order to establish a clear picture of the current
state-of-the-art of publicly available wide cover-
age knowledge resources we also consider a num-
ber of basic baselines.
7We also consider multiword terms.
537
democratic 0.0126 socialist 0.0062
tammany 0.0124 organization 0.0060
alinement 0.0122 conservative 0.0059
federalist 0.0115 populist 0.0053
missionary 0.0103 dixiecrats 0.0051
whig 0.0099 know-nothing 0.0049
greenback 0.0089 constitutional 0.0045
anti-masonic 0.0083 pecking 0.0043
nazi 0.0081 democratic-republican 0.0040
republican 0.0074 republicans 0.0039
alcoholics 0.0073 labor 0.0039
bull 0.0070 salvation 0.0038
party 4.9350 trade 1.5295
political 3.7722 parties 1.4083
government 2.4129 politics 1.2703
election 2.2265 campaign 1.2551
policy 2.0795 leadership 1.2277
support 1.8537 movement 1.2156
leader 1.8280 general 1.2034
years 1.7128 public 1.1874
people 1.7044 member 1.1855
local 1.6899 opposition 1.1751
conference 1.6702 unions 1.1563
power 1.6105 national 1.1474
Table 3: Topic Signatures for party#n#1 using TSWEB (24 out of 15881 total words) and TS-
BNC(queryA) with TFIDF (24 out of 9069 total words)
<instance id=?party.n.bnc.00008131? docsrc=?BNC?> <context> Up to the late 1960s , catholic nationalists were split between
two main political groupings . There was the Nationalist Party , a weak organization for which local priests had to provide
some kind of legitimation . As a <head>party</head> , it really only exercised a modicum of power in relation to the Stormont
administration . Then there were the republican parties who focused their attention on Westminster elections . The disorganized
nature of catholic nationalist politics was only turned round with the emergence of the civil rights movement of 1968 and the
subsequent forming of the SDLP in 1970 . </context> </instance>
Table 4: Example of test num. 00008131 for party#n which its correct sense is 1
4.1 Baselines
We have designed several baselines in order to es-
tablish a relative comparison of the performance
of each semantic resource:
? RANDOM: For each target word, this
method selects a random sense. This baseline
can be considered as a lower-bound.
? WordNet MFS (WN-MFS): This method
selects the most frequent sense (the first sense
in WordNet) of the target word.
? TRAIN-MFS: This method selects the most
frequent sense in the training corpus of the
target word.
? Train Topic Signatures (TRAIN): This
baseline uses the training corpus to directly
build a Topic Signature using TFIDF measure
for each word sense. Note that in this case,
this baseline can be considered as an upper-
bound of our evaluation framework.
Table 5 presents the F1 measure (harmonic
mean of recall and precision) of the different base-
lines. In this table, TRAIN has been calculated
with a fixed vector size of 450 words. As ex-
pected, RANDOM baseline obtains the poorest
result while the most frequent sense of Word-
Net (WN-MFS) is very close to the most frequent
sense of the training corpus (TRAIN-MFS), but
Baselines F1
TRAIN 65.1
TRAIN-MFS 54.5
WN-MFS 53.0
RANDOM 19.1
Table 5: Baselines
both are far below to the Topic Signatures acquired
using the training corpus (TRAIN).
4.2 Performance of the knowledge resources
Table 6 presents the performance of each knowl-
edge resource uploaded into the MCR and the av-
erage size of its vectors. In bold appear the best
results for precision, recall and F1 measures. The
lowest result is obtained by the knowledge directly
gathered from WN mainly because of its poor cov-
erage (Recall of 17.6 and F1 of 25.6). Its perfor-
mance is improved using words at distance 1 and
2 (F1 of 33.3), but it decreases using words at dis-
tance 1, 2 and 3 (F1 of 30.4). The best precision is
obtained by WN (46.7), but the best performance
is achieved by the combined knowledge of MCR-
spBNC8 (Recall of 42.9 and F1 of 44.1). This rep-
resents a recall 18.5 points higher than WN. That
is, the knowledge integrated into the MCR (Word-
Net, eXtended WordNet and the selectional prefer-
ences acquired from SemCor) although partly de-
rived by automatic means performs much better
8MCR without Selectional Preferences from BNC
538
KB P R F1 Av. Size
MCR-spBNC 45.4 42.9 44.1 115
MCR 41.8 40.4 41.1 235
spSemCor 43.1 38.7 40.8 56
spBNC+spSemCor 41.4 30.1 40.7 184
WN+XWN 45.5 28.1 34.7 68
WN-2 38.0 29.7 33.3 72
XWN 45.0 25.6 32.6 55
WN-3 31.6 29.3 30.4 297
spBNC 36.3 25.4 29.9 128
WN 46.7 17.6 25.6 13
Table 6: P, R and F1 fine-grained results for the
resources integrated into the MCR.
in terms of recall and F1 measures than using the
knowledge currently present in WN alone (with a
small decrease in precision). It also seems that the
knowledge from spBNC always degrades the per-
formance of their combinations9.
Regarding the baselines, all knowledge re-
sources integrated into the MCR surpass RAN-
DOM, but none achieves neither WN-MFS,
TRAIN-MFS nor TRAIN.
Figure 1 plots F1 results of the fine-grained
evaluation on the nominal part of the English lex-
ical sample of Senseval-3 of the baselines (in-
cluding upper and lower-bounds), the knowledge
bases integrated into the MCR, the best perform-
ing Topic Signatures acquired from the web and
the BNC evaluated individually and in combina-
tion with others. The figure presents F1 (Y-axis)
in terms of the size of the word vectors (X-axis)10.
In order to evaluate more deeply the quality of
each knowledge resource, we also provide some
evaluations of the combined outcomes of several
knowledge resources. The combinations are per-
formed following a very simple voting method:
first, for each knowledge resource, the scoring re-
sults obtained for each word sense are normal-
ized, and then, for each word sense, the normal-
ized scores are added up selecting the word sense
with higher score.
Regarding Topic Signatures, as expected, in
general the knowledge gathered from the web
(TSWEB) is superior to the one acquired from the
BNC either using queryA or queryW (TSBNC-
queryA and TSBNC-queryW). Interestingly, the
performance of TSBNC-queryA when using the
9All selectional preferences acquired from SemCor or the
BNC have been considered including those with very low
confidence score.
10Only varying the size of TS for TSWEB and TSBNC.
first two hundred words of the TS is slightly bet-
ter than using queryW (both using the web or the
BNC).
Although TSBNC-queryA and TSBNC-
queryW perform very similar, both knowledge
resources contain different knowledge. This is
shown when combining the outcomes of these
two different knowledge resources with TSWEB.
While no improvement is obtained when com-
bining the knowledge acquired from the web
and the BNC when using the same acquisition
method (queryW), the combination of TSWEB
and TSBNC-queryA (TSWEB+ExRetA) obtains
better F1 results than TSWEB (TSBNC-queryA
have some knowledge not included into TSWEB).
Surprisingly, the knowledge integrated into the
MCR (MCR-spBNC) surpass the knowledge from
Topic Signatures acquired from the web or the
BNC, using queryA, queryW or their combina-
tions.
Furthermore, the combination of TSWEB and
MCR-spBNC (TSWEB+MCR-spBNC) outper-
forms both resources individually indicating that
both knowledge bases contain complementary in-
formation. The maximum is achieved with TS
vectors of at most 700 words (with 49.3% preci-
sion, 49.2% recall and 49.2% F1). In fact, the
resulting combination is very close to the most
frequent sense baselines. This fact indicates that
the resulting large-scale knowledge base almost
encodes the knowledge necessary to behave as a
most frequent sense tagger.
4.3 Senseval-3 system performances
For sake of comparison, tables 7 and 8 present the
F1 measure of the fine-grained results for nouns
of the Senseval-3 lexical sample task for the best
and worst unsupervised and supervised systems,
respectively. We also include in these tables some
of the baselines and the best performing combina-
tion of knowledge resources (including TSWEB
and MCR-spBNC)11. Regarding the knowledge
resources evaluated in this study, the best com-
bination (including TSWEB and MCR-spBNC)
achieves an F1 measure much better than some su-
pervised and unsupervised systems and it is close
to the most frequent sense of WordNet (WN-MFS)
and to the most frequent sense of the training cor-
pora (TRAIN-MFS).
11Although we maintain the classification of the organiz-
ers, system s3 wsdiit used the train data.
539
Figure 1: Fine-grained evaluation results for the knowledge resources
s3 systems F1
s3 wsdiit 68.0
WN-MFS 53.0
Comb TSWEB MCR-spBNC 49.2
s3 DLSI 17.8
Table 7: Senseval-3 Unsupervised Systems
s3 systems F1
htsa3 U.Bucharest (Grozea) 74.2
TRAIN 65.1
TRAIN-MFS 54.5
DLSI-UA-LS-SU U.Alicante (Vazquez) 41.0
Table 8: Senseval-3 Supervised Systems
We must recall that the main goal of this re-
search is to establish a clear and neutral view of the
relative quality of available knowledge resources,
not to provide the best WSD algorithm using these
resources. Obviously, much more sophisticated
WSD systems using these resources could be de-
vised.
5 Quality Assessment
Summarizing, this study provides empirical evi-
dence for the relative quality of publicly avail-
able large-scale knowledge resources. The rela-
tive quality has been measured indirectly in terms
of precision and recall on a WSD task.
The study empirically demonstrates that auto-
matically acquired knowledge bases clearly sur-
pass both in terms of precision and recall the
knowledge manually encoded from WordNet (us-
ing relations expanded to one, two or three levels).
Surprisingly, the knowledge contained into the
MCR (WordNet, eXtended WordNet, Selectional
Preferences acquired automatically from SemCor)
is of a better quality than the automatically ac-
quired Topic Signatures. In fact, the knowledge
resulting from the combination of all these large-
scale resources outperforms each resource indi-
vidually indicating that these knowledge bases
contain complementary information. Finally, we
should remark that the resulting combination is
very close to the most frequent sense classifiers.
Regarding the automatic acquisition of large-
scale Topic Signatures it seems that those ac-
quired from the web are slightly better than those
acquired from smaller corpora (for instance, the
BNC). It also seems that queryW performs better
than queryA but that both methods (queryA and
540
queryW) also produce complementary knowledge.
Finally, it seems that the weights are not useful for
measuring the strength of a vote (they are only use-
ful for ordering the words in the Topic Signature).
6 Conclusions and future work
During the last years the research community has
derived a large set of semantic resources using a
very different set of methods, tools and corpus, re-
sulting on a different set of new semantic relations
between synsets. In fact, each resource has dif-
ferent volume and accuracy figures. Although iso-
lated evaluations have been performed by their de-
velopers in different experimental settings, to date
no complete evaluation has been carried out in a
common framework.
In order to establish a fair comparison, the qual-
ity of each resource has been indirectly evaluated
in the same way on a WSD task. The evaluation
framework selected has been the Senseval-3 En-
glish Lexical Sample Task. The study empirically
demonstrates that automatically acquired knowl-
edge bases surpass both in terms of precision and
recall to the knowledge bases derived manually,
and that the combination of the knowledge con-
tained in these resources is very close to the most
frequent sense classifier.
Once empirically demonstrated that the knowl-
edge resulting from MCR and Topic Signatures ac-
quired from the web is complementary and close
to the most frequent sense classifier, we plan to
integrate the Topic Signatures acquired from the
web (of about 100 million relations) into the MCR.
This process will be performed by disambiguat-
ing the Topic Signatures. That is, trying to obtain
word sense vectors instead of word vectors. This
will allow to enlarge the existing knowledge bases
in several orders of magnitude by fully automatic
methods. Other evaluation frameworks such as PP
attachment will be also considered.
7 Acknowledgements
This work is being funded by the IXA NLP
group from the Basque Country Univer-
sity, EHU/UPV-CLASS project and Basque
Government-ADIMEN project. We would like
to thank also the three anonymous reviewers for
their valuable comments.
References
E. Agirre and O. Lopez de la Calle. 2004. Publicly
available topic signatures for all wordnet nominal
senses. In Proceedings of LREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-
to-class selectional preferences. In Proceedings of
CoNLL, Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selec-
tional preferences in wordnet. In Proceedings of
GWC, Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and Piek Vossen. 2004. The meaning
multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
M. Cuadros, L. Padro?, and G. Rigau. 2005. Compar-
ing methods for automatic acquisition of topic sig-
natures. In Proceedings of RANLP, Borovets, Bul-
garia.
J. Daude?, L. Padro?, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Pro-
ceedings of RANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for
Sense Identification. Computational Linguistics,
24(1):147?166.
C. Lin and E. Hovy. 2000. The automated acquisition
of topic signatures for text summarization. In Pro-
ceedings of COLING. Strasbourg, France.
B. Magnini and G. Cavaglia`. 2000. Integrating subject
field codes into wordnet. In Proceedings of LREC,
Athens. Greece.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Sub-
categorization Frames and Selectional Preferences.
Ph.D. thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended word-
net: Progress report. In Proceedings of NAACL
Workshop on WordNet and Other Lexical Resources,
Pittsburgh, PA.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
541
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 81?86,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources
Montse Cuadros
TALP Research Center
Universitat Polite?cnica de Catalunya
Barcelona, Spain
cuadros@lsi.upc.edu
German Rigau
IXA NLP Group
Euskal Herriko Unibersitatea
Donostia, Spain
german.rigau@ehu.es
Abstract
This task tries to establish the relative qual-
ity of available semantic resources (derived
by manual or automatic means). The qual-
ity of each large-scale knowledge resource
is indirectly evaluated on a Word Sense Dis-
ambiguation task. In particular, we use
Senseval-3 and SemEval-2007 English Lex-
ical Sample tasks as evaluation bechmarks
to evaluate the relative quality of each re-
source. Furthermore, trying to be as neu-
tral as possible with respect the knowledge
bases studied, we apply systematically the
same disambiguation method to all the re-
sources. A completely different behaviour is
observed on both lexical data sets (Senseval-
3 and SemEval-2007).
1 Introduction
Using large-scale knowledge bases, such as Word-
Net (Fellbaum, 1998), has become a usual, often
necessary, practice for most current Natural Lan-
guage Processing (NLP) systems. Even now, build-
ing large and rich enough knowledge bases for
broad?coverage semantic processing takes a great
deal of expensive manual effort involving large re-
search groups during long periods of development.
In fact, dozens of person-years have been invested in
the development of wordnets for various languages
(Vossen, 1998). For example, in more than ten years
of manual construction (from version 1.5 to 2.1),
WordNet passed from 103,445 semantic relations to
245,509 semantic relations1. That is, around one
thousand new relations per month. But this data
does not seems to be rich enough to support ad-
vanced concept-based NLP applications directly. It
seems that applications will not scale up to work-
ing in open domains without more detailed and rich
general-purpose (and also domain-specific) seman-
tic knowledge built by automatic means.
Fortunately, during the last years, the research
community has devised a large set of innovative
methods and tools for large-scale automatic acqui-
sition of lexical knowledge from structured and un-
structured corpora. Among others we can men-
tion eXtended WordNet (Mihalcea and Moldovan,
2001), large collections of semantic preferences ac-
quired from SemCor (Agirre and Martinez, 2001;
Agirre and Martinez, 2002) or acquired from British
National Corpus (BNC) (McCarthy, 2001), large-
scale Topic Signatures for each synset acquired from
the web (Agirre and de la Calle, 2004) or acquired
from the BNC (Cuadros et al, 2005). Obviously,
these semantic resources have been acquired using a
very different set of methods, tools and corpora, re-
sulting on a different set of new semantic relations
between synsets (or between synsets and words).
Many international research groups are working
on knowledge-based WSD using a wide range of ap-
proaches (Mihalcea, 2006). However, less attention
has been devoted on analysing the quality of each
semantic resource. In fact, each resource presents
different volume and accuracy figures (Cuadros et
al., 2006).
In this paper, we evaluate those resources on the
1Symmetric relations are counted only once.
81
SemEval-2007 English Lexical Sample task. For
comparison purposes, we also include the results of
the same resources on the Senseval-3 English Lex-
ical sample task. In both cases, we used only the
nominal part of both data sets and we also included
some basic baselines.
2 Evaluation Framework
In order to compare the knowledge resources, all the
resources are evaluated as Topic Signatures (TS).
That is, word vectors with weights associated to a
particular synset. Normally, these word vectors are
obtained by collecting from the resource under study
the word senses appearing as direct relatives. This
simple representation tries to be as neutral as possi-
ble with respect to the resources studied.
A common WSD method has been applied to
all knowledge resources on the test examples of
Senseval-3 and SemEval-2007 English lexical sam-
ple tasks. A simple word overlapping counting is
performed between the Topic Signature and the test
example. The synset having higher overlapping
word counts is selected. In fact, this is a very sim-
ple WSD method which only considers the topical
information around the word to be disambiguated.
Finally, we should remark that the results are not
skewed (for instance, for resolving ties) by the most
frequent sense in WN or any other statistically pre-
dicted knowledge.
As an example, table 1 shows a test example of
SemEval-2007 corresponding to the first sense of the
noun capital. In bold there are the words that appear
in its corresponding Topic Signature acquired from
the web.
Note that although there are several important
related words, the WSD process implements ex-
act word form matching (no preprocessing is per-
formed).
2.1 Basic Baselines
We have designed a number of basic baselines in
order to establish a complete evaluation framework
for comparing the performance of each semantic re-
source on the English WSD tasks.
RANDOM: For each target word, this method se-
lects a random sense. This baseline can be consid-
ered as a lower-bound.
Baselines P R F1
TRAIN 65.1 65.1 65.1
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
SEMCOR-MFS 49.0 49.1 49.0
RANDOM 19.1 19.1 19.1
Table 2: P, R and F1 results for English Lexical Sam-
ple Baselines of Senseval-3
SemCor MFS (SEMCOR-MFS): This method
selects the most frequent sense of the target word
in SemCor.
WordNet MFS (WN-MFS): This method selects
the first sense in WN1.6 of the target word.
TRAIN-MFS: This method selects the most fre-
quent sense in the training corpus of the target word.
Train Topic Signatures (TRAIN): This baseline
uses the training corpus to directly build a Topic Sig-
nature using TFIDF measure for each word sense.
Note that this baseline can be considered as an
upper-bound of our evaluation.
Table 2 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of Senseval-3. In this table, TRAIN
has been calculated with a vector size of at maxi-
mum 450 words. As expected, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both below the most frequent sense
of the training corpus (TRAIN-MFS). However, all
of them are far below the Topic Signatures acquired
using the training corpus (TRAIN).
Table 3 presents the precision (P), recall (R) and
F1 measure (harmonic mean of recall and preci-
sion) of the different baselines in the English Lexical
Sample exercise of SemEval-2007. Again, TRAIN
has been calculated with a vector size of at max-
imum 450 words. As before, RANDOM baseline
obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN
(WN-MFS) are both far below the most frequent
sense of the training corpus (TRAIN-MFS), and all
of them are below the Topic Signatures acquired us-
ing the training corpus (TRAIN).
Comparing both lexical sample sets, SemEval-
2007 data appears to be more skewed and simple for
WSD systems than the data set from Senseval-3: less
82
<instance id=?19:0@11@wsj/01/wsj 0128@wsj@en@on? docsrc=?wsj?> <context>
? A sweeping restructuring of the industry is possible . ? Standard & Poor ?s Corp. says First Boston , Shearson
and Drexel Burnham Lambert Inc. , in particular , are likely to have difficulty shoring up their credit standing in
months ahead . What worries credit-rating concerns the most is that Wall Street firms are taking long-term risks
with their own <head> capital </head> via leveraged buy-out and junk bond financings . That ?s a departure from
their traditional practice of transferring almost all financing risks to investors . Whereas conventional securities
financings are structured to be sold quickly , Wall Street ?s new penchant for leveraged buy-outs and junk bonds is
resulting in long-term lending commitments that stretch out for months or years .
</context> </instance>
Table 1: Example of test id for capital#n which its correct sense is 1
Baselines P R F1
TRAIN 87.6 87.6 87.6
TRAIN-MFS 81.2 79.6 80.4
WN-MFS 66.2 59.9 62.9
SEMCOR-MFS 42.4 38.4 40.3
RANDOM 27.4 27.4 27.4
Table 3: P, R and F1 results for English Lexical Sam-
ple Baselines of SemEval-2007
polysemous (as shown by the RANDOM baseline),
less similar than SemCor word sense frequency dis-
tributions (as shown by SemCor-MFS), more simi-
lar to the first sense of WN (as shown by WN-MFS),
much more skewed to the first sense of the training
corpus (as shown by TRAIN-MFS), and much more
easy to be learned (as shown by TRAIN).
3 Large scale knowledge Resources
The evaluation presented here covers a wide range
of large-scale semantic resources: WordNet (WN)
(Fellbaum, 1998), eXtended WordNet (Mihalcea
and Moldovan, 2001), large collections of seman-
tic preferences acquired from SemCor (Agirre and
Martinez, 2001; Agirre and Martinez, 2002) or ac-
quired from the BNC (McCarthy, 2001), large-scale
Topic Signatures for each synset acquired from the
web (Agirre and de la Calle, 2004) or SemCor (Lan-
des et al, 2006).
Although these resources have been derived us-
ing different WN versions, using the technology for
the automatic alignment of wordnets (Daude? et al,
2003), most of these resources have been integrated
into a common resource called Multilingual Cen-
tral Repository (MCR) (Atserias et al, 2004) main-
taining the compatibility among all the knowledge
resources which use a particular WN version as a
sense repository. Furthermore, these mappings al-
low to port the knowledge associated to a particular
WN version to the rest of WN versions.
The current version of the MCR contains 934,771
semantic relations between synsets, most of them
acquired by automatic means. This represents al-
most four times larger than the Princeton WordNet
(245,509 unique semantic relations in WordNet 2.1).
Hereinafter we will refer to each semantic re-
source as follows:
WN (Fellbaum, 1998): This resource uses the
direct relations encoded in WN1.6 or WN2.0 (for
instance, tree#n#1?hyponym?>teak#n#2). We also
tested WN2 (using relations at distances 1 and 2),
WN3 (using relations at distances 1 to 3) and WN4
(using relations at distances 1 to 4).
XWN (Mihalcea and Moldovan, 2001): This re-
source uses the direct relations encoded in eXtended
WN (for instance, teak#n#2?gloss?>wood#n#1).
WN+XWN: This resource uses the direct rela-
tions included in WN and XWN. We also tested
(WN+XWN)2 (using either WN or XWN relations
at distances 1 and 2, for instance, tree#n#1?related?
>wood#n#1).
spBNC (McCarthy, 2001): This resource contains
707,618 selectional preferences acquired for sub-
jects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This re-
source contains the selectional preferences acquired
for subjects and objects from SemCor (for instance,
read#v#1?tobj?>book#n#1).
MCR (Atserias et al, 2004): This resource
uses the direct relations included in MCR but ex-
cluding spBNC because of its poor performance.
Thus, MCR contains the direct relations from
WN (as tree#n#1?hyponym?>teak#n#2), XWN
(as teak#n#2?gloss?>wood#n#1), and spSemCor
(as read#v#1?tobj?>book#n#1) but not the indi-
83
Source #relations
Princeton WN1.6 138,091
Selectional Preferences from SemCor 203,546
New relations from Princeton WN2.0 42,212
Gold relations from eXtended WN 17,185
Silver relations from eXtended WN 239,249
Normal relations from eXtended WN 294,488
Total 934,771
Table 4: Semantic relations uploaded in the MCR
rect relations of (WN+XWN)2 (tree#n#1?related?
>wood#n#1). We also tested MCR2 (using rela-
tions at distances 1 and 2), which also integrates
(WN+XWN)2 relations.
Table 4 shows the number of semantic relations
between synset pairs in the MCR.
3.1 Topic Signatures
Topic Signatures (TS) are word vectors related to a
particular topic (Lin and Hovy, 2000). Topic Signa-
tures are built by retrieving context words of a target
topic from large corpora. In our case, we consider
word senses as topics.
For this study, we use two different large-scale
Topic Signatures. The first constitutes one of the
largest available semantic resource with around 100
million relations (between synsets and words) ac-
quired from the web (Agirre and de la Calle, 2004).
The second has been derived directly from SemCor.
TSWEB2: Inspired by the work of (Leacock et
al., 1998), these Topic Signatures were constructed
using monosemous relatives from WordNet (syn-
onyms, hypernyms, direct and indirect hyponyms,
and siblings), querying Google and retrieving up to
one thousand snippets per query (that is, a word
sense), extracting the words with distinctive fre-
quency using TFIDF. For these experiments, we
used at maximum the first 700 words of each TS.
TSSEM: These Topic Signatures have been con-
structed using the part of SemCor having all words
tagged by PoS, lemmatized and sense tagged ac-
cording to WN1.6 totalizing 192,639 words. For
each word-sense appearing in SemCor, we gather
all sentences for that word sense, building a TS us-
ing TFIDF for all word-senses co-occurring in those
sentences.
2http://ixa.si.ehu.es/Ixa/resources/
sensecorpus
political party#n#1 2.3219
party#n#1 2.3219
election#n#1 1.0926
nominee#n#1 0.4780
candidate#n#1 0.4780
campaigner#n#1 0.4780
regime#n#1 0.3414
identification#n#1 0.3414
government#n#1 0.3414
designation#n#3 0.3414
authorities#n#1 0.3414
Table 5: Topic Signatures for party#n#1 obtained
from Semcor (11 out of 719 total word senses)
.
In table 5, there is an example of the first word-
senses we calculate from party#n#1.
The total number of relations between WN
synsets acquired from SemCor is 932,008.
4 Evaluating each resource
Table 6 presents ordered by F1 measure, the perfor-
mance of each knowledge resource on Senseval-3
and the average size of the TS per word-sense. The
average size of the TS per word-sense is the number
of words associated to a synset on average. Obvi-
ously, the best resources would be those obtaining
better performances with a smaller number of asso-
ciated words per synset. The best results for preci-
sion, recall and F1 measures are shown in bold. We
also mark in italics those resources using non-direct
relations.
Surprisingly, the best results are obtained by
TSSEM (with F1 of 52.4). The lowest result is ob-
tained by the knowledge directly gathered from WN
mainly because of its poor coverage (R of 18.4 and
F1 of 26.1). Also interesting, is that the knowledge
integrated in the MCR although partly derived by
automatic means performs much better in terms of
precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1
than XWN and 3.7 than spSemCor).
Despite its small size, the resources derived from
SemCor obtain better results than its counterparts
using much larger corpora (TSSEM vs. TSWEB and
spSemCor vs. spBNC).
Regarding the basic baselines, all knowledge re-
sources surpass RANDOM, but none achieves nei-
ther WN-MFS, TRAIN-MFS nor TRAIN. Only
84
KB P R F1 Av. Size
TSSEM 52.5 52.4 52.4 103
MCR2 45.1 45.1 45.1 26,429
MCR 45.3 43.7 44.5 129
spSemCor 43.1 38.7 40.8 56
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
Table 6: P, R and F1 fine-grained results for the
resources evaluated individually at Senseval-03 En-
glish Lexical Sample Task.
TSSEM obtains better results than SEMCOR-MFS
and is very close to the most frequent sense of WN
(WN-MFS) and the training (TRAIN-MFS).
Table 7 presents ordered by F1 measure, the per-
formance of each knowledge resource on SemEval-
2007 and its average size of the TS per word-sense3.
The best results for precision, recall and F1 mea-
sures are shown in bold. We also mark in italics
those resources using non-direct relations.
Interestingly, on SemEval-2007, all the knowl-
edge resources behave differently. Now, the best
results are obtained by (WN+XWN)2 (with F1 of
52.9), followed by TSWEB (with F1 of 51.0). The
lowest result is obtained by the knowledge encoded
in spBNC mainly because of its poor precision (P of
24.4 and F1 of 20.8).
Regarding the basic baselines, spBNC, WN (and
also WN2 and WN4) and spSemCor do not sur-
pass RANDOM, and none achieves neither WN-
MFS, TRAIN-MFS nor TRAIN. Now, WN+XWN,
XWN, TSWEB and (WN+XWN)2 obtain better re-
sults than SEMCOR-MFS but far below the most
frequent sense of WN (WN-MFS) and the training
(TRAIN-MFS).
5 Combination of Knowledge Resources
In order to evaluate deeply the contribution of each
knowledge resource, we also provide some results
of the combined outcomes of several resources. The
3The average size is different with respect Senseval-3 be-
cause the words selected for this task are different
KB P R F1 Av. Size
(WN+XWN)2 54.9 51.1 52.9 5,153
TSWEB 54.8 47.8 51.0 700
XWN 50.1 39.8 44.4 96
WN+XWN 45.4 36.8 40.7 101
MCR 40.2 35.5 37.7 149
TSSEM 35.1 32.7 33.9 428
MCR2 32.4 29.5 30.9 24,896
WN3 29.3 26.3 27.7 584
WN2 25.9 27.4 26.6 72
spSemCor 31.4 23.0 26.5 51.0
WN4 26.1 23.9 24.9 2,710
WN 36.8 16.1 22.4 13
spBNC 24.4 18.1 20.8 290
Table 7: P, R and F1 fine-grained results for the
resources evaluated individually at SemEval-2007,
English Lexical Sample Task .
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 55.5
Table 8: F1 fine-grained results for the 4 system-
combinations on Senseval-3
combinations are performed following a very basic
strategy (Brody et al, 2006).
Rank-Based Combination (Rank): Each se-
mantic resource provides a ranking of senses of the
word to be disambiguated. For each sense, its place-
ments according to each of the methods are summed
and the sense with the lowest total placement (clos-
est to first place) is selected.
Table 8 presents the F1 measure result with re-
spect this method when combining four different se-
mantic resources on the Senseval-3 test set.
Regarding the basic baselines, this combination
outperforms the most frequent sense of SemCor
(SEMCOR-MFS with F1 of 49.1), WN (WN-MFS
with F1 of 53.0) and, the training data (TRAIN-MFS
with F1 of 54.5).
Table 9 presents the F1 measure result with re-
spect the rank mthod when combining the same four
different semantic resources on the SemEval-2007
test set.
KB Rank
MCR+(WN+XWN)2+TSWEB+TSSEM 38.9
Table 9: F1 fine-grained results for the 4 system-
combinations on SemEval-2007
85
In this case, the combination of the four resources
obtains much lower result. Regarding the baselines,
this combination performs lower than the most fre-
quent senses from SEMCOR, WN or the training
data. This could be due to the poor individual per-
formance of the knowledge derived from SemCor
(spSemCor, TSSEM and MCR, which integrates
spSemCor). Possibly, in this case, the knowledge
comming from SemCor is counterproductive. Inter-
estingly, the knowledge derived from other sources
(XWN from WN glosses and TSWEB from the
web) seems to be more robust with respect corpus
changes.
6 Conclusions
Although this task had no participants, we provide
the performances of a large set of knowledge re-
sources on two different test sets: Senseval-3 and
SemEval-2007 English Lexical Sample task. We
also provide the results of a system combination of
four large-scale semantic resources. When evalu-
ated on Senseval-3, the combination of knowledge
sources surpass the most-frequent classifiers. How-
ever, a completely different behaviour is observed
on SemEval-2007 data test. In fact, both corpora
present very different characteristics. The results
show that some resources seems to be less depen-
dant than others to corpus changes.
Obviously, these results suggest that much more
research on acquiring, evaluating and using large-
scale semantic resources should be addressed.
7 Acknowledgements
We want to thank the valuable comments of the
anonymous reviewers. This work has been partially
supported by the projects KNOW (TIN2006-15049-
C03-01) and ADIMEN (EHU06/113).
References
E. Agirre and O. Lopez de la Calle. 2004. Publicly avail-
able topic signatures for all wordnet nominal senses.
In Proceedings of LREC, Lisbon, Portugal.
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL,
Toulouse, France.
E. Agirre and D. Martinez. 2002. Integrating selectional
preferences in wordnet. In Proceedings of GWC,
Mysore, India.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and Piek Vossen. 2004. The mean-
ing multilingual central repository. In Proceedings of
GWC, Brno, Czech Republic.
S. Brody, R. Navigli, and M. Lapata. 2006. Ensem-
ble methods for unsupervised wsd. In Proceedings of
COLING-ACL, pages 97?104.
M. Cuadros, L. Padro?, and G. Rigau. 2005. Comparing
methods for automatic acquisition of topic signatures.
In Proceedings of RANLP, Borovets, Bulgaria.
M. Cuadros, L. Padro?, and G. Rigau. 2006. An empirical
study for automatic acquisition of topic signatures. In
Proceedings of GWC, pages 51?59.
J. Daude?, L. Padro?, and G. Rigau. 2003. Validation and
Tuning of Wordnet Mapping Techniques. In Proceed-
ings of RANLP, Borovets, Bulgaria.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
S. Landes, C. Leacock, and R. Tengi. 2006. Build-
ing a semantic concordance of english. In WordNet:
An electronic lexical database and some applications.
MIT Press, Cambridge,MA., 1998, pages 97?104.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24(1):147?
166.
C. Lin and E. Hovy. 2000. The automated acquisition of
topic signatures for text summarization. In Proceed-
ings of COLING. Strasbourg, France.
D. McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, Subcate-
gorization Frames and Selectional Preferences. Ph.D.
thesis, University of Sussex.
R. Mihalcea and D. Moldovan. 2001. extended wordnet:
Progress report. In Proceedings of NAACL Workshop
on WordNet and Other Lexical Resources, Pittsburgh,
PA.
R. Mihalcea. 2006. Knowledge based methods for word
sense disambiguation. In E. Agirre and P. Edmonds
(Eds.) Word Sense Disambiguation: Algorithms and
applications., volume 33 of Text, Speech and Lan-
guage Technology. Springer.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks . Kluwer
Academic Publishers .
86
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 157?160,
Prague, June 2007. c?2007 Association for Computational Linguistics
GPLSI: Word Coarse-grained Disambiguation aided by Basic Level
Concepts?
Rube?n Izquierdo Armando Sua?rez
GPLSI Group, DLSI
University of Alicante
Spain
{ruben, armando}@dlsi.ua.es
German Rigau
IXA NLP Group
EHU/UPV
Donostia, Basque Country
german.rigau@ehu.es
Abstract
We present a corpus-based supervised lear-
ning system for coarse-grained sense disam-
biguation. In addition to usual features for
training in word sense disambiguation, our
system also uses Base Level Concepts au-
tomatically obtained from WordNet. Base
Level Concepts are some synsets that gene-
ralize a hyponymy sub?hierarchy, and pro-
vides an extra level of abstraction as well as
relevant information about the context of a
word to be disambiguated. Our experiments
proved that using this type of features re-
sults on a significant improvement of preci-
sion. Our system has achieved almost 0.8 F1
(fifth place) in the coarse?grained English
all-words task using a very simple set of fea-
tures plus Base Level Concepts annotation.
1 Introduction
The GPLSI system in SemEval?s task 7, coarse?
grained English all-words, consists of a corpus-
based supervised-learning method which uses lo-
cal context information. The system uses Base Le-
vel Concepts (BLC) (Rosch, 1977) as features. In
short, BLC are synsets of WordNet (WN) (Fell-
baum, 1998) that are representative of a certain hy-
ponymy sub?hierarchy. The synsets that are se-
lected to be BLC must accomplish certain condi-
tions that will be explained in next section. BLC
?This paper has been supported by the European Union un-
der the project QALL-ME (FP6 IST-033860) and the Spanish
Government under the project Text-Mess (TIN2006-15265-
C06-01) and KNOW (TIN2006-15049-C03-01)
are slightly different from Base Concepts of Eu-
roWordNet1 (EWN) (Vossen et al, 1998), Balkanet2
or Meaning Project3 because of the selection crite-
ria but also because our method is capable to define
them automatically. This type of features helps our
system to achieve 0.79550 F1 (over the First?Sense
baseline, 0.78889) while only four systems outper-
formed ours being the F1 of the best one 0.83208.
WordNet has been widely criticised for being a
sense repository that often offers too fine?grained
sense distinctions for higher level applications like
Machine Translation or Question & Answering. In
fact, WSD at this level of granularity, has resisted
all attempts of inferring robust broad-coverage mo-
dels. It seems that many word?sense distinctions are
too subtle to be captured by automatic systems with
the current small volumes of word?sense annotated
examples. Possibly, building class-based classifiers
would allow to avoid the data sparseness problem of
the word-based approach.
Thus, some research has been focused on deri-
ving different sense groupings to overcome the fine?
grained distinctions of WN (Hearst and Schu?tze,
1993) (Peters et al, 1998) (Mihalcea and Moldo-
van, 2001) (Agirre et al, 2003) and on using predefi-
ned sets of sense-groupings for learning class-based
classifiers for WSD (Segond et al, 1997) (Ciaramita
and Johnson, 2003) (Villarejo et al, 2005) (Curran,
2005) (Ciaramita and Altun, 2006). However, most
of the later approaches used the original Lexico-
graphical Files of WN (more recently called Super-
1http://www.illc.uva.nl/EuroWordNet/
2http://www.ceid.upatras.gr/Balkanet
3http://www.lsi.upc.es/ nlp/meaning
157
senses) as very coarse?grained sense distinctions.
However, not so much attention has been paid on
learning class-based classifiers from other available
sense?groupings such as WordNet Domains (Mag-
nini and Cavaglia, 2000), SUMO labels (Niles and
Pease, 2001), EuroWordNet Base Concepts or Top
Concept Ontology labels (Atserias et al, 2004). Ob-
viously, these resources relate senses at some level
of abstraction using different semantic criteria and
properties that could be of interest for WSD. Pos-
sibly, their combination could improve the overall
results since they offer different semantic perspecti-
ves of the data. Furthermore, to our knowledge, to
date no comparative evaluation have been performed
exploring different sense?groupings.
This paper is organized as follows. In section 2,
we present a method for deriving fully automatica-
lly a number of Base Level Concepts from any WN
version. Section 3 shows the details of the whole
system and finally, in section 4 some concluding re-
marks are provided.
2 Automatic Selection of Base Level
Concepts
The notion of Base Concepts (hereinafter BC) was
introduced in EWN. The BC are supposed to be the
concepts that play the most important role in the va-
rious wordnets4 (Fellbaum, 1998) of different lan-
guages. This role was measured in terms of two
main criteria:
? A high position in the semantic hierarchy;
? Having many relations to other concepts;
Thus, the BC are the fundamental building blocks
for establishing the relations in a wordnet and give
information about the dominant lexicalization pat-
terns in languages. BC are generalizations of featu-
res or semantic components and thus apply to a ma-
ximum number of concepts. Thus, the Lexicografic
Files (or Supersenses) of WN could be considered
the most basic set of BC.
Basic Level Concepts (Rosch, 1977) should not
be confused with Base Concepts. BLC are the result
of a compromise between two conflicting principles
of characterization:
4http://wordnet.princeton.edu
#rel. synset
18 group 1,grouping 1
19 social group 1
37 organisation 2,organization 1
10 establishment 2,institution 1
12 faith 3,religion 2
5 Christianity 2,church 1,Christian church 1
#rel. synset
14 entity 1,something 1
29 object 1,physical object 1
39 artifact 1,artefact 1
63 construction 3,structure 1
79 building 1,edifice 1
11 place of worship 1, ...
19 church 2,church building 1
#rel. synset
20 act 2,human action 1,human activity 1
69 activity 1
5 ceremony 3
11 religious ceremony 1,religious ritual 1
7 service 3,religious service 1,divine service 1
1 church 3,church service 1
Table 1: Possible Base Level Concepts for the noun
Church
? Represent as many concepts as possible;
? Represent as many features as possible;
As a result of this, Basic Level Concepts typically
occur in the middle of hierarchies and less than the
maximum number of relations. BC mostly involve
the first principle of the Basic Level Concepts only.
Our work focuses on devising simple methods for
selecting automatically an accurate set of Basic Le-
vel Concepts from WN. In particular, our method se-
lects the appropriate BLC of a particular synset con-
sidering the relative number of relations encoded in
WN of their hypernyms.
The process follows a bottom-up approach using
the chain of hypernym relations. For each synset
in WN, the process selects as its Base Level Con-
cept the first local maximum according to the rela-
tive number of relations. For synsets having multi-
ple hypernyms, the path having the local maximum
with higher number of relations is selected. Usually,
this process finishes having a number of ?fake? Base
Level Concepts. That is, synsets having no descen-
dants (or with a very small number) but being the
first local maximum according to the number of re-
lations considered. Thus, the process finishes che-
cking if the number of concepts subsumed by the
158
Senses BLC SuperSenses
Nouns 4.92 4.10 3.01
Verbs 11.00 8.67 1.03
Nouns + Verbs 7.66 6.16 3.47
Table 2: Polysemy degree over SensEval?3
preliminary list of BLC is higher than a certain th-
reshold. For those BLC not representing enough
concepts according to a certain threshold, the pro-
cess selects the next local maximum following the
hypernym hierarchy.
An example is provided in table 1. This table
shows the possible BLC for the noun ?church? using
WN1.6. The table presents the hypernym chain for
each synset together with the number of relations en-
coded in WN for the synset. The local maxima along
the hypernym chain of each synset appears in bold.
Table 2 presents the polysemy degree for nouns
and verbs of the different words when grouping its
senses with respect the different semantic classes on
SensEval?3. Senses stand for the WN senses, BLC
for the Automatic BLC derived using a threshold of
20 and SuperSenses for the Lexicographic Files of
WN.
3 The GPLSI system
The GPLSI system uses a publicly available imple-
mentation of Support Vector Machines, SVMLight5
(Joachims, 2002), and Semcor as learning corpus.
Semcor has been properly mapped and labelled with
both BLC6 and sense-clusters.
Actually, the process of training-classification has
two phases: first, one classifier is trained for each
possible BLC class and then the SemEval test data
is classified and enriched with them, and second, a
classifier for each target word is built using as addi-
tional features the BLC tags in Semcor and SemE-
val?s test.
Then, the features used for training the classifiers
are: lemmas, word forms, PoS tags7, BLC tags, and
first sense class of target word (S1TW). All features
5http://svmlight.joachims.org/
6Because BLC are automatically defined from WN, some tu-
ning must be performed due to the nature of the task 7. We have
not enough room to present the complete study but threshold 20
has been chosen, using SENSEVAL-3 English all-words as test
data. Moreover, our tests showed roughly 5% of improvement
against not using these features.
7TreeTagger (Schmid, 1994) was used
were extracted from a window [?3.. + 3] except for
the last type (S1TW). The reason of using S1TW
features is to assure the learning of the baseline. It is
well known that Semcor presents a higher frequency
on first senses (and it is also the baseline of the task
finally provided by the organizers).
Besides, these are the same features for both first
and second phases (obviously except for S1TW be-
cause of the different target set of classes). Nevert-
heless, the training in both cases are quite different:
the first phase is class-based while the second is
word-based. By word-based we mean that the lear-
ning is performed using just the examples in Semcor
that contains the target word. We obtain one classi-
fier per polysemous word are in the SemEval test
corpus. The output of these classifiers is a sense-
cluster. In class-based learning all the examples in
Semcor are used, tagging those ones belonging to a
specific class (BLC in our case) as positive exam-
ples while the rest are tagged as negatives. We ob-
tain so many binary classifiers as BLC are in Se-
mEval test corpus. The output of these classifiers
is true or false, ?the example belongs to a class?
or not. When dealing with a concrete target word,
only those BLC classifiers that are related to it are
?activated? (i.e, ?animal? classifier will be not used
to classify ?church?), ensuring that the word will be
tagged with coherent labels. In order to avoid statis-
tical bias because of very large set of negative exam-
ples, the features are defined from positive examples
only (although they are obviously used to characte-
rize all the examples).
4 Conclusions and further work
The WSD task seems to have reached its maxi-
mum accuracy figures with the usual framework.
Some of its limitations could come from the sense?
granularity of WN. In particular, SemEval?s coarse-
grained English all-words task represents a solution
in this direction.
Nevertheless, the task still remains oriented to
words rather than classes. Then, other problems
arise like data sparseness just because the lack of
adequate and enough examples. Changing the set of
classes could be a solution to enrich training corpora
with many more examples Another option seems to
be incorporating more semantic information.
159
Base Level Concepts (BLC) are concepts that are
representative for a set of other concepts. A simple
method for automatically selecting BLC from WN
based on the hypernym hierarchy and the number of
stored relationships between synsets have been used
to define features for training a supervised system.
Although in our system BLC play a simple role
aiding to the disambiguation just as additional fea-
tures, the good results achieved with such simple
features confirm us that an appropriate set of BLC
will be a better semantic discriminator than senses
or even sense-clusters.
References
E. Agirre, I. Aldezabal, and E. Pociello. 2003. A pi-
lot study of english selectional preferences and their
cross-lingual compatibility with basque. In Procee-
dings of the International Conference on Text Speech
and Dialogue (TSD?2003), CeskBudojovice, Czech
Republic.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning mul-
tilingual central repository. In Proceedings of Global
WordNet Conference (GWC?04), Brno, Czech Repu-
blic.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction with
a supersense sequence tagger. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?06), pages 594?602, Syd-
ney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tagging
of unknown nouns in wordnet. In Proceedings of the
Conference on Empirical methods in natural language
processing (EMNLP?03), pages 168?175. ACL.
J. Curran. 2005. Supersense tagging of unknown nouns
using semantic similarity. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics (ACL?05), pages 26?33. ACL.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
M. Hearst and H. Schu?tze. 1993. Customizing a lexicon
to better suit a computational task. In Proceedingns
of the ACL SIGLEX Workshop on Lexical Acquisition,
Stuttgart, Germany.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer Academic
Publishers.
B. Magnini and G. Cavaglia. 2000. Integrating subject
fields codes into wordnet. In Proceedings of the Se-
cond International Conference on Language Resour-
ces and Evaluation (LREC?00).
R. Mihalcea and D. Moldovan. 2001. Automatic ge-
neration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lexical
Resources: Applications, Extensions and Customiza-
tions, Pittsburg, USA.
I. Niles and A. Pease. 2001. Towards a standard up-
per ontology. In Proceedings of the 2nd International
Conference on Formal Ontology in Information Sys-
tems (FOIS-2001), pages 17?19. Chris Welty and Ba-
rry Smith, eds.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Internatio-
nal Conference on Language Resources and Evalua-
tion (LREC?98), Granada, Spain.
E. Rosch. 1977. Human categorisation. Studies in
Cross-Cultural Psychology, I(1):1?49.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of NemLap-
94, pages 44?49, Manchester, England.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on Au-
tomatic Information Extraction and Building of Lexi-
cal Semantic Resources for NLP Applications, pages
78?81. ACL, New Brunswick, New Jersey.
L. Villarejo, L. Ma`rquez, and G. Rigau. 2005. Explo-
ring the construction of semantic class classifiers for
wsd. In Proceedings of the 21th Annual Meeting of
Sociedad Espaola para el Procesamiento del Lenguaje
Natural SEPLN?05, pages 195?202, Granada, Spain,
September. ISSN 1136-5948.
P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Cal-
zolari, A. Roventini, F. Bertagna, A. Alonge, and
W. Peters. 1998. The eurowordnet base concepts and
top ontology. Technical report, Paris, France, France.
160
  
	 In: Proceedings of CoNLL-2000 and LLL-2000, pages 31-36, Lisbon, Portugal, 2000. 
A Comparison between Supervised Learning Algorithms for Word 
Sense Disambiguation* 
Gerard  Escudero  and Lluis Mhrquez  and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g.rigau}@Isi.upc.es 
Abst ract  
This paper describes a set of comparative exper- 
iments, including cross-corpus evaluation, be- 
tween five alternative algorithms for supervised 
Word Sense Disambiguation (WSD), namely 
Naive Bayes, Exemplar-based learning, SNOW, 
Decision Lists, and Boosting. Two main conclu- 
sions can be drawn: 1) The LazyBoosting algo- 
rithm outperforms the other four state-of-the- 
art algorithms in terms of accuracy and ability 
to tune to new domains; 2) The domain depen- 
dence of WSD systems eems very strong and 
suggests that some kind of adaptation or tun- 
ing is required for cross-corpus application. 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the prob- 
lem of assigning the appropriate meaning (or 
sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understanding 
applications and their associate tasks (Ide and 
V4ronis, 1998). Besides, WSD is one of the most 
important open problems in NLP. Despite the 
wide range of approaches investigated (Kilgar- 
rift and Rosenzweig, 2000) and the large effort 
devoted to tackling this problem, to date, no 
large-scale broad-coverage and highly accurate 
WSD system has been built. 
One of the most successful current lines of 
research is the corpus-based approach, in which 
statistical or Machine Learning (M L) algorithms 
have been applied to learn statistical models 
or classifiers from corpora in order to per- 
* This research has been partially funded by the Spanish 
Research Department (CICYT's project TIC98-0423- 
C06), by the EU Commission (NAMIC I8T-1999-12392), 
and by the Catalan Research Department (CIRIT's 
consolidated research group 1999SGR-150 and CIRIT's 
grant 1999FI 00773). 
form WSD. Generally, supervised approaches 
(those that learn from previously semantically 
annotated corpora) have obtained better esults 
than unsupervised methods on small sets of se- 
lected ambiguous words, or artificial pseudo- 
words. Many standard M L algorithms for su- 
pervised learning have been applied, such as: 
Decision Lists (Yarowsky, 1994; Agirre and 
Martinez, 2000), Neural Networks (Towell and 
Voorhees, 1998), Bayesian learning (Bruce and 
Wiebe, 1999), Exemplar-based learning (Ng, 
1997), Boosting (Escudero et al, 2000a), etc. 
Further, in (Mooney, 1996) some of the previ- 
ous methods are compared jointly with Decision 
Trees and Rule Induction algorithms, on a very 
restricted omain. 
Although some published works include the 
comparison between some alternative algo- 
rithms (Mooney, 1996; Ng, 1997; Escudero et 
al., 2000a; Escudero et al, 2000b), none of 
them addresses the issue of the portability of 
supervised ML algorithms for WSD, i.e., testing 
whether the accuracy of a system trained on 
a certain corpus can be extrapolated to other 
corpora or not. We think that the study of the 
domain dependence of WSD -- in the style of 
other studies devoted to parsing (Sekine, 1997; 
Ratnaparkhi, 1999)-- is needed to assure the 
validity of the supervised approach, and to de- 
termine to which extent a tuning pre-process i
necessary to make real WSD systems portable. 
In this direction, this work compares five differ- 
ent M L algorithms and explores their portability 
and tuning ability by training and testing them 
on different corpora. 
2 Learn ing  A lgor i thms Tested  
Naive-Bayes (NB). Naive Bayes is intended 
as a simple representative of statistical learning 
methods. It has been used in its most classi- 
31 
cal setting (Duda and Hart, 1973). That is, 
assuming the independence of features, it clas- 
sifies a new example by assigning the class that 
maximizes the conditional probability of the 
class given the observed sequence of features 
of that example. Model probabilities are esti- 
mated during the training process using relative 
frequencies. To avoid the effect of zero counts, a 
very simple smoothing technique has been used, 
which was proposed in (Ng, 1997). 
Despite its simplicity, Naive Bayes is claimed 
to obtain state-of-the-art accuracy on super- 
vised WSD in many papers (Mooney, 1996; Ng, 
1997; Leacock et al, 1998). 
Exemplar -based  Classif ier (EB). In exem- 
plar, instance, or memory-based learning (Aha 
et al, 1991) no generalization of training ex- 
amples is performed. Instead, the examples are 
simply stored in memory and the classification 
of new examples is based on the most similar 
stored exemplars. In our implementation, all
examples are kept in memory and the classifica- 
tion is based on a k-NN (Nearest-Neighbours) 
algorithm using Hamming distance to measure 
closeness. For k's greater than 1, the resulting 
sense is the weighted majority sense of the k 
nearest neighbours --where each example votes 
its sense with a strength proportional to its 
closeness to the test example. 
Exemplar-based learning is said to be the 
best option for WSD (Ng, 1997). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior in 
language learning problems because they do not 
forget exceptions. 
The SNoW Arch i tec ture  (SN). SNoWis a 
Sparse Network of linear separators which uti- 
lizes the Winnow learning algorithm 1. In the 
SNo W architecture there is a winnow node for 
each class, which learns to separate that class 
from all the rest. During training, which is per- 
formed in an on-line fashion, each example is 
considered a positive example for the winnow 
node associated to its class and a negative x- 
ample for all the others. A key point that allows 
a fast learning is that the winnow nodes are not 
connected to all features but only to those that 
1The Winnow algorithm (Littlestone, 1988) consists 
of a linear threshold algorithm with multiplicative weight 
updating for 2-class problems. 
are "relevant" for their class. When classify- 
ing a new example, SNo W is similar to a neural 
network which takes the input features and out- 
puts the class with the highest activation. Our 
implementation f SNo W for WSD is explained 
in (Escudero et al, 2000c). 
SNoW is proven to perform very well in 
high dimensional NLP problems, where both the 
training examples and the target function reside 
very sparsely in the feature space (Roth, 1998), 
e.g: context-sensitive spelling correction, POS 
tagging, PP-attachment disambiguation, etc. 
Decis ion Lists (DL). In this setting, a Deci- 
sion List is a list of features extracted from the 
training examples and sorted by a log-likelihood 
measure. This measure stimates how strong a 
particular feature is as an indicator of a specific 
sense (Yarowsky, 1994). When testing, the deci- 
sion list is checked in order and the feature with 
the highest weight that matches the test exam- 
ple is used to select the winning word sense. 
Thus, only the single most reliable piece of ev- 
idence is used to perform disambiguation. Re- 
garding the details of implementation (smooth- 
ing, pruning of the decision list, etc.) we have 
followed (Agirre and Martinez, 2000). 
Decision Lists were one of the most success- 
ful systems on the 1st Senseval competition for 
WSD (Kilgarriff and Rosenzweig, 2000). 
LazyBoost ing  (LB). The main idea of boost- 
ing algorithms is to combine many simple and 
moderately accurate hypotheses (weak classi- 
fiers) into a single, highly accurate classifier. 
The weak classifiers are trained sequentially 
and, conceptually, each of them is trained on the 
examples which were most difficult to classify by 
the preceding weak classifiers. These weak hy- 
potheses are then linearly combined into a single 
rule called the combined hypothesis. 
Schapire and Singer's real AdaBoost.MH al- 
gorithm for multiclass multi-label classifica- 
tion (Schapire and Singer, 1999) has been used. 
It constructs a combination of very simple 
weak hypotheses that test the value of a single 
boolean predicate and make a real-valued pre- 
diction based on that value. LazyBoosting (Es- 
cudero et al, 2000a) is a simple modification 
of the AdaBoost.MH algorithm, which consists 
in reducing the feature space that is explored 
when learning each weak classifier. This mod- 
ification significantly increases the efficiency of 
32 
the learning process with no loss in accuracy. 
3 Set t ing  
A number of comparative experiments has been 
carried out on a subset of 21 highly ambiguous 
words of the DSO corpus, which is a semanti- 
cally annotated English corpus collected by Ng 
and colleagues (Ng and Lee, 1996). Each word 
is treated as a different classification problem. 
The 21 words comprise 13 nouns (age, art, body, 
car, child, cost, head, interest, line, point, state, 
thing, work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell), which frequently appear 
in the WSD literature. The average number of 
senses per word is close to 10 and the number 
of training examples is around 1,000. 
The DSO corpus contains entences from two 
different corpora, namely Wall Street Journal 
(WSJ) and Brown Corpus (BC). Therefore, it is 
easy to perform experiments about the porta- 
bility of systems by training them on the WSJ 
part (A part, hereinafter) and testing them on 
the BC part (B part, hereinafter), or vice-versa. 
Two kinds of information are used to train 
classifiers: local and topical context. Let 
... " be ~ W-3 W--2 W--1 W W-i_ 1 W+2 W+3. . .  
the context of consecutive words around the 
word w to be disambiguated, and P?i ( -3  < 
i < 3) be the part-of-speech tag of word 
w?i. Attributes referring to local context 
are the following 15: P-3, P-2, P- l ,  P+I, 
P+2, P+3, w- l ,  W-t-1 , (W-2, W-1), (W-i,W+I), 
(W+I ,W+2) ,  (W-3,  W--2, W--1), (W-2, W- i ,W+I ) ,  
(W--l, W+i , W+2), and (W+l, w+2, w+3), where 
the last seven correspond to collocations of two 
and three consecutive words. The topical con- 
text is formed by c l , . . . ,  Cm, which stand for the 
unordered set of open class words appearing in 
the sentence 2. Details about how the different 
algorithms translate this information into fea- 
tures can be found in (Escudero et al, 2000c). 
4 Compar ing  the  five approaches  
The five algorithms, jointly with a naive Most- 
Frequent-sense Classifier (MFC), have been 
tested, by 10-fold cross validation, on 7 different 
combinations of training-test sets 3. Accuracy 
2This set of attributes corresponds to that used in (Ng 
and Lee, 1996), with the exception of the morphology of 
the target word and the verb-object syntactic relation. 
3The combinations of training-test sets are called: 
A+B-A+B, A-I-B-A, A+B-B, A-A, B-B, A-B, and B-A, 
figures, micro-averaged over the 21 words and 
over the ten folds, are reported in table 1. The 
comparison leads to the following conclusions: 
As expected, the five algorithms ignificantly 
outperform the baseline M FC classifier. Among 
them, three groups can be observed: Ni3, DL, 
and SN perform similarly; LB outperforms all 
the other algorithms in all experiments; and EB 
is somewhere in between. The difference be- 
tween \[B and the rest is statistically significant 
in all cases except when comparing \[B to the EB 
approach in the case marked with an asterisk 4. 
Extremely poor results are observed when 
testing the portability of the systems. Restrict- 
ing to LB results, it can be observed that the 
accuracy obtained in A-B is 47.1%, while the 
accuracy in B-B (which can be considered an 
upper bound for LB in B corpus) is 59.0%, that 
is, that there is a difference of 12 points. Fur- 
thermore, 47.1% is only slightly better than the 
most frequent sense in corpus B, 45.5%. 
Apart from accuracy figures, the comparison 
between the predictions made by the five meth- 
ods on the test sets provides interesting infor- 
mation about the relative behaviour of the algo- 
rithms. Table 2 shows the agreement rates and 
the Kappa statistics 5 between all pairs of meth- 
ods in the A+B-A+B experiment. Note that 
'DSO' stands for the annotation of DSO corpus, 
which is taken as the correct one. 
It can be observed that N B obtains the most 
similar results with regard to M FC in agreement 
and Kappa values. The agreement ratio is 74%, 
that is, almost 3 out of 4 times it predicts the 
most frequent sense. On the other extreme, LB 
obtains the most similar results with regard to 
DSO in agreement and Kappa values, and it has 
the least similar with regard to M FC, suggesting 
respectively. In this notation, the training set is placed 
on the left hand side of symbol "-", while the test set 
is on the right hand side. For instance, A-B means that 
the training set is corpus A and the test set is corpus B. 
The symbol "+" stands for set union. 
4Statistical tests of significance applied: McNemar's 
test and 10-fold cross-validation paired Student's t-test 
at a confidence value of 95% (Dietterich, 1998). 
~The Kappa statistic (Cohen, 1960) is a better mea- 
sure of inter-annotator agreement which reduces the ef- 
fect of chance agreement. It has been used for measur- 
ing inter-annotator agreement during the construction 
of semantic annotated corpora (V~ronis, 1998; Ng et al, 
1999). A Kappa value of 1 indicates perfect agreement, 
while 0.8 is considered as indicating ood agreement. 
33 
Accuracy (%) 
LazyBoosting 
A+B-A+B A+B-A A+B-B A-A B-B 
MFC 46.55?o.71 53.90?2.ol 39.21?i.9o 55.94?Mo 45.52?1.27 
Naive Bayes 61.55?1.o4 67.25?1.o7 55.85?1.81 65.86?1.11 56.80?1.12 
Decision Lists 61.58?o.98 67.64?0.94 55.53?1.85 67.57?1.44 56.56?1.59 
SNoW 60.92?1.o9 65.57?1.33 56.28?1.1o 67.12?1.16 56.13?1.23 
Exemplar-based 63.01?o.93 69.08?1.66 56.97?1.22 68.98?1.o6 57.36?1.68 
66.32?1.34 71.79?1.51 60.85~L81 71,26?1.i5 58.96?1.86 
A-B B-A 
36.40 38.71 
41.38 47.66 
43.01 48.83 
44.07 49.76 
45.32 51.13 
47.10 51.99" 
Table 1: Accuracy results (=h standard eviation) of the methods on all training-test combinations 
A+B-A+B 
DSO MFC NB EB SN DL LB 
DSO - -  46.6 61.6 63.0 60.9 61.6 66.3 
MFC -0.19 --  73.9 60.0 55.9 64.9 54.9 
NB 0.24 -0.09 --  76.3 74.5 76.8 71.4 
EB 0.36 -0.15 0.44 - -  69.6 70.7 72.5 
SN 0.36 -0.17 0.44 0.44 - -  67.5 69.0 
DL 0.32 -0.13 0.40 0.41 0.38 --  69.9 
LB 0.44 -0.17 0.37 0.50 0.46 0.42 - -  
Table 2: Kappa statistic (below diagonal) and 
% of agreement (above diagonal) between all 
methods in the A+B-A+B experiment 
that LB is the algorithm that better learns the 
behaviour of the DSO examples. 
In absolute terms, the Kappa values are very 
low. But, as it is suggested in (Vdronis, 1998), 
evaluation measures hould be computed rela- 
tive to the agreement between the human an- 
notators of the corpus and not to a theoreti- 
cal 100%. It seems pointless to expect more 
agreement between the system and the refer- 
ence corpus than between the annotators them- 
selves. Contrary to the intuition that the agree- 
ment between human annotators should be very 
high in the WSD task, some papers report sur- 
prisingly low figures. For instance, (Ng et al, 
1999) reports an accuracy rate of 56.7% and a 
Kappa value of 0.317 when comparing the anno- 
tation of a subset of the DSO corpus performed 
by two independent research groups. From this 
perspective, the Kappa value of 0.44 achieved 
by LB in A+B-A+B could be considered an ex- 
cellent result. Unfortunately, the subset of the 
\[:)SO corpus studied by (Ng et al, 1999) and 
that used in this report are not the same and, 
thus, a direct comparison is not possible. 
4.1 About  the  tun ing  to new domains  
This experiment explores the effect of a sim- 
ple tuning process consisting in adding to the 
original training set A a relatively small sample 
of manually sense-tagged examples of the new 
domain B. The size of this supervised portion 
varies from 10% to 50% of the available corpus 
in steps of 10% (the remaining 50% is kept for 
testing) 6. This experiment will be referred to 
as A+%B-B T. In order to determine to which 
extent the original training set contributes to 
accurately disambiguating in the new domain, 
we also calculate the results for %B-B, that is, 
using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
A+%B-B and %B-B curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds B-B and A+B-B. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between A+%B-B and %B-B 
reveals some interesting facts. In plots (c) and 
(d), the contribution of the original training cor- 
pus is null, while in plots (a) and (b), a degrada- 
tion onthe  accuracy is observed. Summarizing, 
these results suggest hat for NB, DL, SN, and 
EB methods it is not worth keeping the original 
training examples. Instead, a better (but dis- 
appointing) strategy would be simply using the 
tuning corpus. However, this is not the situa- 
tion of LB - -plot  (d) - -  for which a moderate, 
but consistent, improvement of accuracy is ob- 
served when retaining the original training set. 
6Tuning examples can be weighted more highly than 
the training examples to force the learning algorithm to 
adapt more quickly to the new corpus. Some experi- 
ments in this direction revealed that slightly better e- 
sults can be obtained, though the improvement was not 
statistically significant. 
7The converse xperiment B-F%A-A is not reported 
in this paper due to space limitations. Results can be 
found in (Escudero et al, 2000c). 
34 
58 
56 
54 
g 52 
al 
~ 50  
~ 46 
44 
42  
40  
(a) Naive Bayes 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  M~S o 
........................................................... ~ ................. B...B . . . . .  
A+B-B ........ 
A+%B-B ~-  
%B-B . . . . .  .. 
56 
~ 54  
o 52 
~ 48 
46 
44  
58  
56 
54 
16 
44  
42  
(b) Decision Lists 
. . . . . . . . . . . . . . . . . . .  G . . . . . . . . . . . . . . . . . . .  o . . . . . . . . . . . . . . . . . . .  ~ . . . . . . . . . . . .  AYS:B : : i : : :  .... 
A+%B-B 
%B-B . . . . .  ' 
, , , , , 
5 10 15 20 25 30 35 40 45 50 5 10 15 
(d )  SNoW 
58 
E::E::E.~:E.~:E.~:EZ::Z~:E.~:Z.Z..~YE::LL:::Y.:~.'S::47724:L:; 
B-B . . . . .  
Zoi~:~ '?~ 
5 10 15 20 25 30 35 40 45 50 
0 20 25 30 35 40 45 50 
62 
60 
58 
~ 56 
~ 52 
48 
46  
44  
(c) Exemplar Based 
58 
MFS 
56 =::=:==::=Q:~=:::=:==:::a=:====::==~=::=:::=:=:Q-~,~=--~---~ A+B-B ........ 
A+%B-B 
54 %B-B ....... 
52 
50 ..... ~ 
46  / o  , , , 
' ' ' 44  ' ' ' ' ' ' ' ' ' 
5 10 15 20 25 30 35 40 45 50 
(e) Lazygoosting 
.................. o................... ~ ................... ~ ................ MF$- -~- -  
B-B ...... 
...................................................... A~'B-B . . . . . . .  
A+%B-B ~- -  
%B-B . . . . .  
/ / 
i / 
5 10 15 20 25 30 35 40 45 50 
Figure 1: Results of the tuning experiment 
We observed that part of the poor results 
obtained is explained by: 1) corpus A and 
B have a very different distribution of senses, 
and, therefore, different a-priori biases; further- 
more, 2) examples of corpus A and B contain 
different information and, therefore, the learn- 
ing algorithms acquire different (and non inter- 
changeable) classification clues from both cor- 
pora. The study of the rules acquired by Lazy- 
Boosting from WSJ and BC helped understand- 
ing the differences between corpora. On the one 
hand, the type of features used in the rules was 
significantly different between corpora and, ad- 
ditionally, there were very few rules that applied 
to both sets. On the other hand, the sign of the 
prediction of many of these common rules was 
somewhat contradictory between corpora. See 
(Escudero et al, 2000c) for details. 
4.2 About  the tra in ing data qual i ty 
The observation of the rules acquired by Lazy- 
Boosting could also help improving data quality 
in a semi-supervised fashion. It is known that 
mislabelled examples resulting from annotation 
errors tend to be hard examples to classify cor- 
rectly and, therefore, tend to have large weights 
in the final distribution. This observation al- 
lows both to identify the noisy examples and 
use LazyBoosting as a way to improve the train- 
ing corpus. 
A preliminary experiment has been carried 
out in this direction by studying the rules ac- 
quired by LazyBoosting from the training ex- 
amples of the word state. The manual revi- 
sion, by four different people, of the 50 high- 
est scored rules, allowed us to identify 28 noisy 
training examples. 11 of them were clear tag- 
ging errors, and the remaining 17 were not co- 
herently tagged and very difficult to judge, since 
the four annotators achieved systematic dis- 
agreement (probably due to the extremely fine 
grained sense definitions involved in these ex- 
amples). 
5 Conc lus ions  
This work reports a comparative study of five 
ML algorithms for WSD, and provides some re- 
sults on cross corpora evaluation and domain 
re-tuning. 
Regarding portability, it seems that the per- 
formance of supervised sense taggers is not 
guaranteed when moving from one domain to 
another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
35 
These results imply that some kind of adap- 
tation is required for cross-corpus application. 
Consequently, it is our belief that a number of 
issues regarding portability, tuning, knowledge 
acquisition, etc., should be thoroughly studied 
before stating that the supervised M k paradigm 
is able to resolve a realistic WSD problem. 
Regarding the ML algorithms tested, kazy- 
Boosting emerges as the best option, since 
it outperforms the other four state-of-the-art 
methods in all experiments. Furthermore, this 
algorithm shows better properties when tuned 
to new domains. Future work is planned for 
an extensive valuation of kazyBoosting on the 
WSD task. This would include taking into ac- 
count additional/alternative attributes, learn- 
ing curves, testing the algorithm on other cor- 
pora, etc. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists and 
Automatic Word Sense Disambiguation. In Pro- 
ceedings of the COLING Workshop on Semantic 
Annotation and Intelligent Content. 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decomposable 
Modeling in Natural Language Processing. Com- 
putational Linguistics, 25(2):195-207. 
J. Cohen. 1960. A Coefficient of Agreement for 
Nominal Scales. Journal of Educational and Psy- 
chological Measurement, 20:37-46. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Classifi- 
cation and Scene Analysis. Wiley ~: Sons. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disambiguation. 
In Proceedings of the 12th European Conference 
on Machine Learning, ECML. 
G. Escudero, L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches to 
Word Sense Disambiguation Revisited. In Pro- 
ceedings of the 14th European Conference on Ar- 
tificial Intelligence, ECAL 
G. Escudero, L. M~rquez, and G. Rigau. 2000c. On 
the Portability and Tuning of Supervised Word 
Sense Disambiguation Systems. Research Report 
LSI-00-30-R, Software Department (LSI). Techni- 
cal University of Catalonia (UPC). 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguistics, 
24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English SEN- 
SEVAL: Report and Results. In Proceedings of the 
2nd International Conference on Language Re- 
sources and Evaluation, LREC. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computational Linguis- 
tics, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Ir- 
relevant Attributes Abound. Machine Learning, 
2:285-318. 
R. J. Mooney. 1996. Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In Proceed- 
ings of the 1st Conference on Empirical Methods 
in Natural Language Processing, EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Senses: 
An Exemplar-based Approach. In Proceedings of 
the 3~th Annual Meeting of the ACL. 
H. T. Ng, C. Lim, and S. Foo. 1999. A Case Study 
on Inter-Annotator Agreement for Word Sense 
Disambiguation. In Procs. of the ACL SIGLEX 
Workshop: Standardizing Lexical Resources. 
H. T. Ng. 1997. Exemplar-Base Word Sense Disam- 
biguation: Some Recent Improvements. In Procs. 
of the 2nd Conference on Empirical Methods in 
Natural Language Processing, EMNLP. 
A. Ratnaparkhi. 1999. Learning to Parse Natural 
Language with Maximum Entropy Models. Ma- 
chine Learning, 34:151-175. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In Pro- 
ceedings of the National Conference on Artificial 
Intelligence, AAAI  '98. 
R. E. Schapire and Y. Singer. 1999. Improved 
Boosting Algorithms Using Confidence-rated Pre- 
dictions. Machine Learning, 37(3):297-336. 
S. Sekine. 1997. The Domain Dependence of Pars- 
ing. In Proceedings of the 5th Conference on Ap- 
plied Natural Language Processing, ANLP. 
G. Towell and E. M. Voorhees. 1998. Disambiguat- 
ing Highly Ambiguous Words. Computational 
Linguistics, 24(1):125-146. 
J. V@ronis. 1998. A study of polysemy judgements 
and inter-annotator agreement. In Programme 
and advanced papers of the Senseval workshop, 
Herstmonceux Castle, England. 
D. Yarowsky. 1994. Decision Lists for Lexical Ambi- 
guity Resolution: Application to Accent Restora- 
tion in Spanish and French. In Proceedings of the 
32nd Annual Meeting of the ACL. 
36 
An Empirical Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems* 
Gerard  Escudero ,  L lu is  M~trquez~ and German R igau  
TALP Research Center. LSI Department. Universitat Polit~cnica de Catalunya (UPC) 
Jordi Girona Salgado 1-3. E-08034 Barcelona. Catalonia 
{escudero, lluism, g. rigau}@isi, upc. es 
Abst ract  
This paper describes a set of experiments car- 
ried out to explore the domain dependence 
of alternative supervised Word Sense Disam- 
biguation algorithms. The aim of the work is 
threefold: studying the performance of these 
algorithms when tested on a different cor- 
pus from that they were trained on; explor- 
ing their ability to tune to new domains, 
and demonstrating empirically that the Lazy- 
Boosting algorithm outperforms tate-of-the- 
art supervised WSD algorithms in both previ- 
ous situations. 
Keywords:  Cross-corpus evaluation of Ni_P 
systems, Word Sense Disambiguation, Super- 
vised Machine Learning 
1 In t roduct ion  
Word Sense Disambiguation (WSD) is the 
problem of assigning the appropriate meaning 
(sense) to a given word in a text or discourse. 
Resolving the ambiguity of words is a central 
problem for large scale language understand- 
ing applications and their associate tasks (Ide 
and V4ronis, 1998), e.g., machine transla- 
tion, information retrieval, reference resolu- 
tion, parsing, etc. 
WSD is one of the most important open 
problems in NLP. Despite the wide range of 
approaches investigated and the large effort 
devoted to tackle this problem, to date, no 
large-scale broad-coverage and highly accu- 
rate WSD system has been built --see the 
main conclusions of the first edition of Sen- 
sEval (Kilgarriff and Rosenzweig, 2000). 
One of the most successful current lines 
of research is the corpus-based approach in 
" This research has been partially funded by the Span- 
ish Research Department (CICYT's project TIC98- 
0423-C06). by the EU Commission (NAMIC IST- 
1999-12392), and by the Catalan Research Depart- 
ment (CIRIT's consolidated research group 1999SGR- 
150 and CIRIT's grant 1999FI 00773). 
which statistical or Machine Learning (ML) al- 
gorithms are applied to learn statistical mod- 
els or classifiers from corpora in order to per- 
form WSD. Generally, supervised approaches 1 
have obtained better results than unsuper- 
vised methods on small sets of selected am- 
biguous words, or artificial pseudo-words. 
Many standard M L algorithms for supervised 
learning have been applied, such as: Decision 
Lists (?arowsky, 1994; Agirre and Martinez, 
2000), Neural Networks (Towell and Voorhees, 
1998), Bayesian learning (Bruce and Wiebe, 
1999), Exemplar-Based learning (Ng, 1997a; 
Fujii et al, 1998), Boosting (Escudero et al, 
2000a), etc. Unfortunately, there have been 
very few direct comparisons between alterna- 
tive methods for WSD. 
In general, supervised learning presumes 
that the training examples are somehow re- 
flective of the task that will be performed by 
the trainee on other data. Consequently, the 
performance of such systems is commonly es- 
timated by testing the algorithm on a separate 
part of the set of training examples (say 10- 
20% of them), or by N-fold cross-validation, 
in which the set of examples i partitioned into 
N disjoint sets (or folds), and the training- 
test procedure is repeated N times using all 
combinations of N-1  folds for training and 1 
fold for testing. In both cases, test examples 
are different from those used for training, but 
they belong to the same corpus, and, there- 
fore, they are expected to be quite similar. 
Although this methodology could be valid 
for certain NLP problems, such as English 
Part-of-Speech tagging, we think that there 
exists reasonable vidence to say that, in 
WSD, accuracy results cannot be simply ex- 
trapolated to other domains (contrary to the 
opinion of other authors (Ng, 1997b)): On the 
aSupervised approaches, also known as data-driven 
or corpus-dmven, are those that learn from a previ- 
ously semantically annotated corpus. 
172 
one hand, WSD is very dependant to the do- 
main of application (Gale et al, 1992b) --see 
also (Ng and Lee, 1996; Ng, 1997a), in which 
quite different accuracy figures are obtained 
when testing an exemplar-based WSD classi- 
fier on two different corpora. Oi1 the other 
hand, it does not seem reasonable to think 
that the training material is large and repre- 
sentative nough to cover "all" potential types 
of examples. 
To date, a thorough study of the domain 
dependence of WSD - - in  the style of other 
studies devoted to parsing (Sekine, 1997)-- 
has not been carried out. We think that such 
an study is needed to assess the validity of 
the supervised approach, and to determine to 
which extent a tuning process is necessary to 
make real WSD systems portable. In order 
to corroborate the previous hypotheses, this 
paper explores the portability and tuning of 
four different ML algorithms (previously ap- 
plied to WSD) by training and testing them 
on different corpora. 
Additionally, supervised methods suffer 
from the "knowledge acquisition bottle- 
neck" (Gale et al, 1992a). (Ng, 1997b) esti- 
mates that the manual annotation effort nec- 
essary to build a broad coverage semantically 
annotated English corpus is about 16 person- 
years. This overhead for supervision could be 
much greater if a costly tuning procedure is 
required before applying any existing system 
to each new domain. 
Due to this fact, recent works have focused 
on reducing the acquisition cost as well as the 
need for supervision i  corpus-based methods. 
It is our belief that the research by (Leacock et 
al., 1998; Mihalcea and Moldovan, 1999) 2 pro- 
vide enough evidence towards the "opening" 
of the bottleneck in the near future. For that 
reason, it is worth further investigating the 
robustness and portability of existing super- 
vised ML methods to better resolve the WSD 
problem. 
It is important o note that the focus of 
this work will be on the empirical cross- 
corpus evaluation of several M L supervised al- 
gorithms. Other important issues, such as: 
selecting the best attribute set, discussing an 
appropriate definition of senses for the task, 
etc., are not addressed in this paper. 
eIn the line of using lexical resources and search en- 
gunes to automatically collect training examples from 
large text collections or Internet. 
This paper is organized as follows: Section 2 
presents the four ML algorithms compared. 
In section 3 the setting is presented in de- 
tail, including the corpora and the experimen- 
tal methodology used. Section 4 reports the 
experiments carried out and the results ob- 
tained. Finally, section 5 concludes and out- 
lines some lines for further esearch. 
2 Learn ing  A lgor i thms Tested  
2.1 Naive-Bayes (NB) 
Naive Bayes is intended as a simple represen- 
tative of statistical learning methods. It has 
been used in its most classical setting (Duda 
and Hart, 1973). That is, assuming indepen- 
dence of features, it classifies a new example 
by assigning the class that maximizes the con- 
ditional probability of the class given the ob- 
served sequence of features of that example. 
Model probabilities are estimated uring 
training process using relative frequencies. To 
avoid the effect of zero counts when esti- 
mating probabilities, a very simple smooth- 
ing technique has been used, which was pro- 
posed in (Ng, 1997a). Despite its simplicity, 
Naive Bayes is claimed to obtain state-of-the- 
art accuracy on supervised WSD in many pa- 
pers (Mooney, 1996; Ng, 1997a; Leacock et 
al., 1998). 
2.2 Exemplar -based  Classif ier (EB) 
In Exemplar-based learning (Aha et al, 1991) 
no generalization of training examples is per- 
formed. Instead, the examples are stored 
in memory and the classification of new ex- 
amples is based on the classes of the most 
similar stored examples. In our implemen- 
tation, all examples are kept in memory and 
the classification of a new example is based 
on a k-NN (Nearest-Neighbours) algorithm 
using Hamming distance 3 to measure close- 
ness (in doing so, all examples are examined). 
For k's greater than 1, the resulting sense is 
the weighted majority sense of the k near- 
est neighbours --where each example votes its 
sense with a strength proportional to its close- 
ness to the test example. 
In the experiments explained in section 4, 
the EB algorithm is run several times using 
different number of nearest neighbours (1, 3, 
SAlthough the use of MVDM metric (Cost and 
Salzberg, 1993) could lead to better results, current 
implementations have prohivitive computational over- 
heads(Escudero et al, 2000b) 
173 
5, 7, 10, 15, 20 and 25) and the results corre- 
sponding to the best choice are reported 4.
Exemplar-based learning is said to be the 
best option for VSD (Ng, 1997a). Other au- 
thors (Daelemans et al, 1999) point out that 
exemplar-based methods tend to be superior 
in language learning problems because they 
do not forget exceptions. 
2.3 Snow: A Winnow-based  Classif ier 
Snow stands for Sparse Network Of Winnows, 
and it is intended as a representative of on- 
line learning algorithms. 
The basic component is the Winnow al- 
gorithm (Littlestone, 1988). It consists of a 
linear threshold algorithm with multiplicative 
weight updating for 2-class problems, which 
learns very fast in the presence of many bi- 
nary input features. 
In the Snow architecture there is a winnow 
node for each class, which learns to separate 
that class from all the rest. During training, 
each example is considered a positive xample 
for winnow node associated to its class and 
a negative example for all the rest. A key 
point that allows a fast learning is that the 
winnow nodes are not connected to all features 
but only to those that are "relevant" for their 
class. When classifying a new example, Snow 
is similar to a neural network which takes the 
input features and outputs the class with the 
highest activation. 
Snow is proven to perform very well in 
high dimensional domains, where both, the 
training examples and the target function re- 
side very sparsely in the feature space (Roth, 
1998), e.g: text categorization, context- 
sensitive spelling correction, WSD, etc. 
In this paper, our approach to WSD using 
Snow follows that of (Escudero et al, 2000c). 
2.4 LazyBoost ing  (LB) 
The main idea of boosting algorithms is to 
combine many simple and moderately accu- 
rate hypotheses (called weak classifiers) into 
a single, highly accurate classifier. The weak 
classifiers are trained sequentially and, con- 
ceptually, each of them is trained on the ex- 
amples which were most difficult to classify 
by the preceding weak classifiers. These weak 
4In order to construct a real EB-based system for 
WSD, the k parameter should be estimated by cross- 
validation using only the training set (Ng, 1997a), 
however, in our case, this cross-validation i side the 
cross-validation i volved in the testing process would 
generate a prohibitive overhead. 
hypotheses are then linearly combined into a 
single rule called the combined hypothesis. 
More particularly, the Schapire and Singer's 
real AdaBoost.MH algorithm for multi- 
class multi-label classification (Schapire and 
Singer, to appear) has been used. As in that 
paper, very simple weak hypotheses are used. 
They test the value of a boolean predicate and 
make a real-valued prediction based on that 
value. The predicates used, which are the bi- 
narization of the attributes described in sec- 
tion 3.2, are of the form "f = v", where f is a 
feature and v is a value (e.g: "-r v" p e mus_word 
= hosp i ta l " ) .  Each weak rule uses a single 
feature, and, therefore, they can be seen as 
simple decision trees with one internal node 
(testing the value of a binary feature) and two 
leaves corresponding to the yes/no answers to 
that test. 
LazyBoosting (Escudero et al, 2000a), is a 
simple modification of the AdaBoost.MH al- 
gorithm, which consists of reducing the fea- 
ture space that is explored when learning each 
weak classifier. More specifically, a small pro- 
portion p of attributes are randomly selected 
and the best weak rule is selected only among 
them. The idea behind this method is that 
if the proportion p is not too small, probably 
a sufficiently good rule can be found at each 
iteration. Besides, the chance for a good rule 
to appear in the whole learning process is very 
high. Another important characteristic is that 
no attribute needs to be discarded and, thus, 
the risk of eliminating relevant attributes is 
avoided. The method seems to work quite well 
since no important degradation is observed in 
performance for values of p greater or equal 
to 5% (this may indicate that there are many 
irrelevant or highly dependant attributes in 
the WSD domain). Therefore, this modifica- 
tion significantly increases the efficiency of the 
learning process (empirically, up to 7 times 
faster) with no loss in accuracy. 
3 Set t ing  
3.1 The  DSO Corpus  
The DSO corpus is a semantically annotated 
corpus containing 192,800 occurrences of 121 
nouns and 70 verbs, corresponding to the most 
frequent and ambiguous English words. This 
corpus was collected by Ng and colleagues (Ng 
and Lee, 1996) and it is available from the 
Linguistic Data Consortium (LDC) 5. 
5LDC address: http://www. Idc.upeaa. ed~/ 
174 
The D50 corpus contains sentences from 
two different corpora, namely Wall Street 
Journal (WSJ) and Brown Corpus (BC). 
Therefore, it is easy to perform experiments 
about the portability of alternative systems 
by training them on the WSJ part and testing 
them on the BE part, or vice-versa. Here- 
inafter, the WSJ part of DSO will be referred 
to as corpus A, and the BC part to as corpus B. 
At a word level, we force the number of exam- 
ples of corpus A and B be the same 6 in order 
to have symmetry and allow the comparison 
in both directions. 
From these corpora, a group of 21 words 
which frequently appear in the WSD litera- 
ture has been selected to perform the com- 
parative experiments (each word is treated 
as a different classification problem). These 
words are 13 nouns (age, art, body, car, child, 
cost, head, interest, line, point, state, thing, 
work) and 8 verbs (become, fall, grow, lose, 
set, speak, strike, tell). Table 1 contains in- 
formation about the number of examples, the 
number of senses, and the percentage of the 
most frequent sense (MF5) of these reference 
words, grouped by nouns, verbs, and all 21 
words. 
3.2 At t r ibutes  
Two kinds of information are used to perform 
disambiguation: local and topical context. 
Let "... w-3 w-2 w-1 w W+l w+2 w+3..." 
be the context of consecutive words around 
the word w to be disambiguated, and p?, 
( -3  < i _< 3)be  the part-of-speech tag 
of word w?~. Attributes referring to local 
context are the following 15: P-3, P-2, 
P- l ,  P+i, P+2, P+3, w- l ,  W+l, (W-2,W-1), 
(w-i.w+i), (w+l,w+2), 
(w-2, W-l, w+l), (w-i ,  w+l, w+2), and 
(w+l,w+2, w+3), where the last seven cor- 
respond to collocations of two and three 
consecutive words. 
The topical context is formed by Cl,..., Cm, 
which stand for the unordered set of open class 
words appearing in the sentence 7. 
The four methods tested translate this 
information into features in different ways. 
Snow and LB algorithms require binary fea- 
6This is achieved by ramdomly reducing the size of 
the largest corpus to the size of the smallest. 
7The already described set of attributes contains 
those attributes used in (Ng and Lee, 1996), with the 
exception of the morphology of the target word and 
the verb-object syntactic relation. 
tures. Therefore, local context attributes have 
to be binarized in a preprocess, while the top- 
ical context attributes remain as binary tests 
about the presence/absence of a concrete word 
in the sentence. As a result the number of 
attributes is expanded to several thousands 
(from 1,764 to 9,900 depending on the partic- 
ular word). 
The binary representation of attributes is 
not appropriate for NB and EB algorithms. 
Therefore, the 15 local-context attributes are 
taken straightforwardly. Regarding the binary 
topical-context attributes, we have used the 
variants described in (Escudero et al, 2000b). 
For EB, the topical information is codified as 
a single set-valued attribute (containing all 
words appearing in the sentence) and the cal- 
culation of closeness is modified so as to han- 
dle this type of attribute. For NB, the top- 
ical context is conserved as binary features, 
but when classifying new examples only the 
information of words appearing in the exam- 
ple (positive information) is taken into ac- 
count. In that paper, these variants are called 
positive Exemplar-based (PEB) and positive 
Naive Bayes (PNB), respectively. PNB and 
PEB algorithms are empirically proven to per- 
form much better in terms of accuracy and 
efficiency in the WSD task. 
3.3 Exper imenta l  Methodo logy  
The comparison of algorithms has been per- 
formed in series of controlled experiments us- 
ing exactly the same training and test sets. 
There are 7 combinations of training-test sets 
called: A+B-A+B, A+B-A, A+B-B, A-A, B- 
B, A-B, and B-A, respectively. In this nota- 
tion, the training set is placed at the left hand 
side of symbol "-", while the test set is at the 
right hand side. For instance, A-B means that 
the training set is corpus A and the test set 
is corpus B. The symbol "+" stands for set 
union, therefore A+B-B means that the train- 
ing set is A union B and the test set is B. 
When comparing the performance oftwo al- 
gorithms, two different statistical tests of sig- 
nificance have been apphed depending on the 
case. A-B and B-A combinations represent a 
single training-test experiment. In this cases, 
the McNemar's test of significance is used 
(with a confidence value of: X1,0.952 = 3.842), 
which is proven to be more robust than a sim- 
ple test for the difference of tw0_proportions. 
In the other combinations, a 10-fold cross- 
validation was performed in order to prevent 
175 
nouns 
verbs 
AorB  
examples 
rain 
senses 
min max avg 
A 
i MFS (%) 
min min 
senses 
B 
MFS (%! 
min max max avg max avg max avg avg 
122 714 420 2 24 7.7 37.9 90.7 59.8 3 24 8.8 21.0 87.7 45.3 
101 741 369 4 13 8.9120.8 81.6 49.3 4 14 11.4 28.0 71.7 46.3 
101 741 401 2 24 8.1 J20.8 90.7 56.1 3 24 9.8 21.0 87.7 45.6 
Table 1: Information about the set of 21 words of reference. 
testing on the same material used for training. 
In these cases, accuracy/error rate figures re- 
ported in section 4 are averaged over the re- 
sults of the 10 folds. The associated statistical 
tests of significance is a paired Student's t-test 
with a confidence value of: t9,0.975 = 2.262. 
Information about both statistical tests can 
be found at (Dietterich, 1998). 
4 Exper iments  
4.1 F i rs t  Exper iment  
Table 2 shows the accuracy figures of the four 
methods in all combinations of training and 
test sets . Standard deviation numbers are 
supplied in all cases involving cross valida- 
tion. M FC stands for a Most-Frequent-sense 
Classifier, that is, a naive classifier that learns 
the most frequent sense of the training set 
and uses it to classify all examples of the test 
set. Averaged results are presented for nouns. 
verbs, and overall, and the best results for 
each case are printed in boldface. 
The following conclusions can be drawn: 
? LB outperforms all other methods in 
all cases. Additionally, this superiority 
is statistically significant, except when 
comparing LB to the PEB approach in the 
cases marked with an asterisk. 
? Surprisingly, LB in A+B-A (or A+B-B) 
does not achieve substantial improvement 
to the results of A-A (or B-S) w in  fact, 
the first variation is not statistically sig- 
nificant and the second is only slightly 
significant. That is, the addition of extra 
examples from another domain does not 
necessarily contribute to improve the re- 
sults on the original corpus. This effect is 
also observed in the other methods, spe- 
cially in some cases (e.g. Snow in A+B-A 
vs. A-A) in which the joining of both 
training corpora is even counterproduc- 
tive. 
SThe second and third column correspond to the 
train and test sets used by (Ng and Lee, 1996; Ng, 
1997a) 
? Regarding the portability of the systems, 
very disappointing results are obtained. 
Restricting to \[B results, we observe that 
the accuracy obtained in A-B is 47.1% 
while the accuracy in B-B (which can 
be considered an upper bound for LB in 
B corpus) is 59.0%, that is, a drop of 
12 points. Furthermore, 47.1% is only 
slightly better than the most frequent 
sense in corpus B, 45.5%. The compari- 
son in the reverse direction is even worse: 
a drop from 71.3% (A-A) to 52.0% (B- 
A), which is lower than the most frequent 
sense of corpus A, 55.9%. 
4.2 Second Exper iment  
The previous experiment shows that classi- 
tiers trained on the A corpus do not work well 
on the B corpus, and vice-versa. Therefore, 
it seems that some kind of tuning process is 
necessary to adapt supervised systems to each 
new domain. 
This experiment explores the effect of a sim- 
ple tuning process consisting of adding to the 
original training set a relatively small sarn- 
ple of manually sense tagged examples of the 
new domain. The size of this supervised por- 
tion varies from 10% to 50% of the available 
corpus in steps of 10% (the remaining 50% is 
kept for testing). This set of experiments will 
be referred to as A+%B-B, or conversely, to 
B+%A-A. 
In order to determine to which extent the 
original training set contributes to accurately 
disambiguate in the new domain, we also cal- 
culate the results for %A-A (and %B-B), that 
is, using only the tuning corpus for training. 
Figure 1 graphically presents the results ob- 
tained by all methods. Each plot contains the 
X+%Y-Y and %Y-Y curves, and the straight 
lines corresponding to the lower bound MFC, 
and to the upper bounds Y-Y and X+Y-Y. 
As expected, the accuracy of all methods 
grows (towards the upper bound) as more tun- 
ing corpus is added to the training set. How- 
ever, the relation between X+%Y-Y and %Y- 
Y reveals some interesting facts. In plots 2a, 
176 
nouns 
MFC verbs 
total 
nouns 
PNB verbs 
total  
nouns  
PEB verbs 
total 
nouns  
Snow verbs 
total 
nouns 
LB verbs 
total 
A+B-A+B 
46.59?1.08 
46.49?1.37 
46.55?0.71 
62.29?1.25 
60.18?1.64 
61.55?1.04 
62.66?0.87 
63.67?1.94 
63.01?0.93 
61.24?1.14 
60.35?1.57 
60.92?1.09 
66.00?1.47 
66.91?2.25 
66.32?1.34 
A+B-A 
56.68?2.79 
48.74?1.98 
53.90?2.01 
68.89?0.93 
64.21?2.26 
67.25?1.07 
69.45?1 51 
68.39?3.25 
69.08?1.66 
66.36?1 57 
64.11?2.76 
65.57?1.33 
2.09?1.61 
71.23?2.99 
71.79?1.51 
Accuracy (%) 
A+B-B 
36.49?2.41 
44.23?2.67 
39.21?1.90 
55.69?1.94 
56.14?2.79 
55.85?1.81 
56.09?1.12 
58.58?2.40 
56.97?1.22 
56.11?1.45 
56.58?2.45 
56.28?1.10 
59.92?1.93 
62.58?2.93 
60.85?1.81 
A-A 
59.77?1.44 
48.85?2.09 
55.94?1.10 
66.93?1.44 
63.87?1.80 
65.86?1.11 
69.38?1.24 
68.25?2.84 
68.98?1.06 
68.85?1.36 
63.91?1.51 
67.12?1.16 
71.69?1.54 
70.45?2.14" 
71.26?1.15 
B-B \[ A-B B-A 
45.28?1.81 33.97 39.46 
45.96?2.6O 40.91 37.31 
45.52?1.27 36.40 38.71 
56.17?1.60 36.62 45.99 
57.97?2.86 50.20 50.75 
56.80?1.12 41.38 47.66 
56.17?1.80 42.15 50.53 
59.57?2.86 51.19 52.24 
57.36?1.68 45.32 51.13 
56.55?1.31 42.13 49.96 
55.36?3.27 47.66 49.39 
56.13?1.23 44.07 49.76 
58.33?2.26 43.92 51.28" 
60.14?3.43" 52.99 53.29* 
58.96?1.86 47.10 51.99" 
Table 2: Accuracy results (:i: standard eviation) of the methods on all training-test combina- 
tions 
3a, and lb the contribution of the original 
training corpus is null. Furthermore, in plots 
la, 2b, and 3b a degradation on the accuracy 
performance is observed. Summarizing, these 
six plots show that for Naive Bayes, Exemplar 
Based, and Snow methods it is not worth keep- 
ing the original training examples. Instead, a 
better (but disappointing) strategy would be 
simply using the tuning corpus. 
However, this is not the situation of Lazy- 
Boosting (plots 4a and 4b), for which a mod- 
erate (but consistent) improvement of accu- 
racy is observed when retaining the original 
training set. Therefore, Lazy\[3oosting shows 
again a better behaviour than their competi- 
tors when moving from one domain to an- 
other. 
4.3 Th i rd  Exper iment  
The bad results about portability could be ex- 
plained by, at least, two reasons: 1) Corpus 
A and \[3 have a very different distribution of 
senses, and, therefore, different a-priori bi- 
ases; 2) Examples of corpus A and \[3 con- 
tain different information, and, therefore, the 
learning algorithms acquire different (and non 
interchangeable) classification cues from both 
corpora,. 
The first hypothesis confirmed by observ- 
ing the bar plots of figure 2, which contain the 
distribution of the four most frequent senses 
of some sample words in the corpora A and 
B. respectively. In order to check the second 
hypothesis, two new sense-balanced corpora 
have been generated from the DSO corpus, by 
equilibrating the number of examples of each 
sense between A and B parts. In this way, the 
first difficulty is artificially overrided and the 
algorithms hould be portable if examples of 
both parts are quite similar. 
Table 3 shows the results obtained by Lazy- 
Boosting on these new corpora. 
Regarding portability, we observe a signifi- 
cant accuracy decrease of 7 and 5 points from 
A-A to B-A, and from B-B to A-B, respec- 
tively 9. That is, even when the sazne distri- 
bution of senses is conserved between training 
and test examples, the portability of the su- 
pervised WSD systems is not guaranteed. 
These results imply that examples have to 
be largely different from one corpus to an- 
other. By studying the weak rules generated 
by kazyBoosting in both cases, we could cor- 
roborate this fact. On the one hand, the type 
of features used in the rules were significantly 
different between corpora, and, additionally, 
there were very few rules that apply to both 
sets; On the other hand, the sign of the pre- 
diction of many of these common rules was 
somewhat contradictory between corpora. 
9This loss in accuracy is not as important as m the 
first experiment, due to the simplification provided by 
the balancing ofsense distributions. 
177 
Naive Bayes 
Exemplar Based 
Snow 
LazyBoosting 
58 
56 1 
54 
~?52 
o 
50 
44 
4O 
58 
56 
Af~ 
52 
~o 
~ 48 
46  
58 
56 ' 
54 
o 52 
50 
46 
62 
60 ' 
58  
~o 
48 
46  
4.4 
Test on B corpus 
(la) 
. . . . .  -MF'~ . . . .  
o B,-B 
A+B-B o 
A+%B-B 
%B-B . . . . .  
/ . f "  
5 10 15 20 25 30 35 40 45 50 
(2a) 
. . . . . . .  ? ; . . . .  ; . . . . . .  ~ --:-" ~S-- :~. : : , - -  ~ 
B-B --- 
A+B-B o 
A+%B-B * - -  
%B-B - "u ' -  
/+.- 
J 
5 10 15 20 25 30 355 40 45 50 
(3a) 
MFS 
A+B-B o 
A+%B-B ~- -  
%B-B ==- 
5 10 15 20 25 30 35 40 45 50 
(4a) 
B-B  ~ - -  
A+%B-B . . . .  
%B-B - - - -  
/ /+  / / " "  - 
-/ , /  
.+ 
$+* 
, , = , = , i , , 
5 10 15 20 25 30 35 40 45 50 
72 
70 
68 
~66 
~,64 
62 
60 
58 
56 
54 
72 
70 
68 
66 
64 
62. 
60 
58 
56 
54 
72 
7O 
68 ~66 
60 
58 
56 
54 
72 
7O 
68 
o  .64 
~: eo 
58 
56 
54 
Test on A corpus 
(lb) 
MFS 
A-A - - - -  
B+A-A 
B+%A-A ~ - -  
o o . %,~oA -~- 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
/ 
, , , , , . . . .  
5 10 15 20 25 30 35 40 45 50 
(2b) 
MFS 
A-A~- -  
B+A-A . . . . . . . . . . . . . . . . . . .  ~ . . . . . .  = . -  ~. . , .Z~:~. . .  ~ ~.~ 
%A-A . . . .  
_~. - -  
+ 
+ ? , , , , , , , , , 
5 10 15 20 25 30 35 40 45 50 
(3b) 
MF$ 
A-A . . . .  
B+A-A a 
B+%A-A ~-  
%A-A . . . . .  
j." 
/ 
5 10 15 20 25 30 ,35 40 45 50 
(4b) 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  f~tE~ . . . . .  
A-A . . . .  
B+A=A 
B+%A-A . . . .  
/P  +/"  
+" 1" 
/ 
/ 
/ 
/,' 
, , , , , , , , i 
5 10  15  20  25  30  35  40  45  50  
Figure 1: Results of the tuning experiment 
5 Conc lus ions  and  Fur ther  Work  
This work has pointed out some difficulties 
regarding the portability of supervised WSD 
systems, a very important issue that has been 
paid little attention up to the present. 
According to our experiments, it seems that 
the performance of supervised sense taggers is 
not guaranteed when moving from one domain 
to another (e.g. from a balanced corpus, such 
as BC, to an economic domain, such as WSJ). 
These results implies that some_kind of adap- 
tation is required for cross-corpus application. 
178 
. . . . .  il ii\[ 
"~ head ~a I r l temt  ~o fa l l  oo grow 
mL. 
Figure 2: Distribution of the four most frequent senses for two nouns (head, interest) and two 
verbs (line, state). Black bars = A corpus; Grey bars = B corpus 
nouns 
MFC verbs 
total 
nouns 
LB verbs 
total 
Accuracy (%) 
A+B-A+B A+B-B A-A 
48.75?0.91 
48.22?1 68 
48.55?1 6 
62.82?1.43 
66.82?1.53 
64.35?1.16 
A+B-A 
48.90?1.69 
48.22?1.90 
48.64?1.04 
64.26?2.07 
69.33?2.92 
66.20?2.12 
48.61?0.96 
48.22?3 06 
48.46?1.21 
61.38?2.08 
64.32?3.27 
62.50?1.47 
48.87?1 68 
48.22?1.90 
48.62?1.09 
63.19?1.65 
68.51?2.45 
65.22?1.50 
B-B A-B B-A 
48.61?0.96 48.99 48.99 
48.22?3.06 48.22 48.22 
48.46?1.21 48.70 48.70 
60.65?1.01 53.45 55.27 
63.49?2.27 60.44 62.55 
61.74?1.18 56.12 58.05 
Table 3: Accuracy results (5= standard eviation) of LazyBoosting on the sense-balanced corpora 
Furthermore, these results are in contradic- 
tion with the idea of "robust broad-coverage 
WSD" introduced by (Ng, 1997b), in which a 
supervised system trained on a large enough 
corpora (say a thousand examples per word) 
~hould provide accurate disambiguation on 
any corpora (or, at least significantly better 
than MFS). 
Consequently, it is our belief that a number 
of issues regarding portability, tuning, knowl- 
edge acquisition, etc., should be thoroughly 
studied before stating that the supervised ML 
paradigm is able to resolve a realistic WSD 
problem. 
Regarding the M L algorithms tested, the 
contribution of this work consist of empiri- 
cally demonstrating that the LazyBoosting al- 
gorithm outperforms other three state-of-the- 
art supervised ML methods for WSD. Further- 
more. this algorithm is proven to have better 
properties when is applied to new domains. 
Further work is planned to be done in the 
following directions: 
? Extensively evaluate LazyBoosting on the 
WSD task. This would include tak- 
ing into account additional/alternative 
attributes and testing the algorithm in 
other corpora --specially on sense-tagged 
corpora automatically obtained from In- 
ternet or large text collections using non- 
supervised methods (Leazock et al, 1998; 
Mihalcea and Moldovan, 1999). 
? Since most of the knowledge l arned from 
a domain is not useful when changing 
to a new domain, further investigation is 
needed on tuning strategies, pecially on 
those using non-supervised algorithms. 
? It is known that mislabelled examples re- 
sulting from annotation errors tend to be 
hard examples to classify correctly, and, 
therefore, tend to have large weights in 
the final distribution. This observation 
allows both to identify the noisy exam- 
ples and use LazyBoosting as a way to 
improve data quality. Preliminary exper- 
iments have been already carried out in 
this direction on the DSO corpus. 
? Moreover, the inspection of the rules 
learned by kazyBoosting could provide 
evidence about similar behaviours of a- 
priori different senses. This type of 
knowledge could be useful to perform 
clustering of too fine-grained or artificial 
senses. 
Re ferences  
E. Agirre and D. Martinez. 2000. Decision Lists 
and Automatic Word Sense Disambiguation. In
Proceedings o\] the COLING Workshop on Se- 
mantic Annotation and Intelligent Content 
D. Aha, D. Kibler, and M. Albert. 1991. Instance- 
based Learning Algorithms. Machine Learning, 
7:37-66. 
R. F. Bruce and J. M. Wiebe. 1999. Decompos- 
179 
able Modeling in Natural Language Processing. 
Computatwnal Linguistics. 25(2):195-207. 
S. Cost and S. Salzberg. 1993. A weighted nearest 
neighbor algorithm for learning with symbolic 
features. Machine Learning, 10(1), 57-78. 
W. Daelemans, A. van den Bosch, and J. Zavrel. 
1999. Forgetting Exceptions is Harmful in Lan- 
guage Learning. Machine Learning, 34:11-41. 
T. G. Dietterich. 1998. Approximate Statisti- 
cal Tests for Comparing Supervised Classifi- 
cation Learning Algorithms. Neural Computa- 
tion, 10(7). 
R. O. Duda and P. E. Hart. 1973. Pattern Clas- 
sificatwn and Scene Analysis. Wiley. 
G. Escudero, L. M~rquez, and G. Rigau. 2000a. 
Boosting Applied to Word Sense Disam- 
biguation. In Proceedings of the 12th Euro- 
pean Conference on Machine Learning, ECML, 
Barcelona, Spain. 
G. Escudero. L. M~rquez, and G. Rigau. 2000b. 
Naive Bayes and Exemplar-Based Approaches 
to Word Sense Disambiguation Revisited. In 
To appear in Proceedings of the 14th European 
Conference on Artificial Intelligence, ECAI. 
G. Escudero, L. M~quez, and G. Rigau. 2000c. 
On the Portability and Tuning of Super- 
vised Word Sense Disambiguation Systems. Re- 
search Report LSI-00-30-R, Software Depart- 
ment (LSI). Technical University of Catalonia 
(UPC). 
A. Fujii, K. Inui. T. Tokunaga, and H. Tanaka. 
1998. Selective Sampling for Example-based 
W'ord Sense Disambiguation. Computatwnal 
Linguistics, 24(4):573-598. 
W. Gale, K. W. Church, and D. Yarowsky. 1992a. 
A Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26:415-439. 
W. Gale, K. W. Church, and D. Yarowsky. 1992b. 
Estimating Upper and Lower Bounds on the 
Performance of Word Sense Disambiguation. 
In Proceedings of the 30th Annual Meeting of 
the Association for Computational Linguistics. 
ACL. 
N. Ide and J. V@ronis. 1998. Introduction to the 
Special Issue on Word Sense Disambiguation: 
The State of the Art. Computational Linguis- 
tics, 24(1):1-40. 
A. Kilgarriff and J. Rosenzweig. 2000. English 
SENSEVAL: Report and Results. In Proceed- 
ings of the 2nd International Conference on 
Language Resources and Evaluatwn, LREC, 
Athens, Greece. 
C. Leacock, M. Chodorow, and G. A. Miller. 1998. 
Using Corpus Statistics and WordNet Relations 
for Sense Identification. Computatwnal Lin- 
guistwcs, 24(1):147-166. 
N. Littlestone. 1988. Learning Quickly when Irrel- 
evant Attributes Abound. Machine Learning, 
2:285-318. 
R. Mihalcea and I. Moldovan. 1999. An Au- 
tomatic Method for Generating Sense Tagged 
Corpora. In Proceedings of the 16th National 
Conference on Artificial Intelligence. AAAI 
Press. 
R. J. Mooney. 1996. Comparative Experiments 
on Disambiguating Word Senses: An Illustra- 
tion of the Role of Bias in Machine Learning. 
In Proceedings of the 1st Conference on Empir- 
ical Methods m Natural Language Processing, 
EMNLP. 
H. T. Ng and H. B. Lee. 1996. Integrating Multi- 
ple Knowledge Sources to Disambiguate Word 
Sense: An Exemplar-based Approach. In Pro- 
ceedmgs of the 3~th Annual Meeting of the As- 
sociation for Computational Linguistics. ACL. 
H. T. Ng. 1997a. Exemplar-Base Wbrd Sense Dis- 
ambiguation: Some Recent Improvements. In 
Proceedings of the 2nd Conference on Empir- 
zcal Methods in Natural Language Processing, 
EMNLP. 
H. T. Ng. 1997b. Getting Serious about Word 
Sense Disambiguation. In Proceedings of the 
ACL SIGLEX Workshop: Tagging Text with 
Lexical Semantics: Why, what and how?, Wash- 
ington, USA. 
D. Roth. 1998. Learning to Resolve Natural Lan- 
guage Ambiguities: A Unified Approach. In 
Proceedings of the National Conference on Ar- 
tzficial Intelhgence, AAAI 'Y8, July. 
R. E. Schapire and Y. Singer. to appear. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning. Also appearing 
in Proceedings of the 11th Annual Conference on 
Computatzonal Learning Theory, 1998. 
S. Sekine. 1997. The Domain Dependence ofPars- 
ing. In Proceedings o\] the 5th Conference on 
Applied Natural Language Processing, ANLP, 
Washington DC. ACL. 
G. Towell and E. M. Voorhees. 1998. Disam- 
biguating Highly Ambiguous Words. Computa- 
tional Lingu~stzcs. 24(1):125-146. 
D. Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent 
Restoration i  Spanish and French. In Proceed- 
ings of the 32nd Annual Meeting of the Associ- 
ation for Computational Linguistics, pages 88- 
95, Las Cruces, NM. ACL. 
180 
Multilingual Authoring: the NAMIC approach
R. Basili, M.T. Pazienza
F. Zanzotto
Dept. of Computer Science
University of Rome, Tor Vergata
Via di Tor Vergata,
00133 Roma
Italy
basili@info.uniroma2.it
pazienza@info.uniroma2.it
zanzotto@info.uniroma2.it
R. Catizone, A. Setzer
N. Webb, Y. Wilks
Department of Computer Science
University of Sheffield
Regent Court
211 Portobello Street,
Sheffield S1 4DP, UK
R.Catizone@dcs.shef.ac.uk
A.Setzer@dcs.shef.ac.uk
N.Webb@dcs.shef.ac.uk
Y.Wilks@dcs.shef.ac.uk
L. Padro?, G. Rigau
Dept. Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
Centre de Recerca TALP
Jordi Girona Salgado 1-3,
08034 Barcelona
Spain
padro@lsi.upc.es
g.rigau@lsi.upc.es
Abstract
With increasing amounts of elec-
tronic information available, and the
increase in the variety of languages
used to produce documents of the
same type, the problem of how to
manage similar documents in dif-
ferent languages arises. This pa-
per proposes an approach to process-
ing/structuring text so that Multi-
lingual Authoring (creating hyper-
text links) can be effectively car-
ried out. This work, funded by
the European Union, is applied to
the Multilingual Authoring of news
agency text. We have applied meth-
ods from Natural Language Process-
ing, especially Information Extrac-
tion technology, to both monolingual
and Multilingual Authoring.
1 Introduction
Modern Information Technologies are faced
with the problem of selecting, filtering and
managing growing amounts of multilingual
information to which access is usually criti-
cal. Traditional Information Retrieval (IR)
approaches are too general in their selection
of relevant documents where as traditional
Information Extraction (IE) (Gaizauskas and
Wilks, 1998; Pazienza, 1997) approaches are
too specific and inflexible. Automatic Au-
thoring is a good example of how these two
methods can be improved and used to cre-
ate a hypertextual organisation of (multilin-
gual) information. This kind of information
is ?added value? to the information embodied
in the text and is not in contrast with other
retrieval paradigms. Automatic Authoring
is the activity of processing news items in
streams, detecting and extracting relevant in-
formation from them and, accordingly, organ-
ising texts in a non-linear fashion.
While IE systems like the ones participat-
ing in the Message Understanding Conference
(MUC, 1998) are oriented towards specific
phenomena (e.g. joint ventures) in restricted
domains, the scope of Automatic Authoring
is wider. In Automatic Authoring, the hy-
pertextual structure has to provide naviga-
tion guidelines to the final user which can also
refuse the system suggestions.
In this paper an architecture for Automatic
Multilingual Authoring is presented based on
knowledge-intensive and large-scale Informa-
tion Extraction. The general architecture
is presented capitalising robust methods of
Information Extraction (Cunningham et al,
1999) and large-scale multilingual resources
(e.g. EuroWordNet). The system is de-
veloped within a European project in the
Human Language Technologies area, called
NAMIC (News Agencies Multilingual Infor-
mation Categorisation)1. It aims to extract
relevant facts from the news streams of large
European news agencies and newspaper pro-
ducers2, to provide hypertextual structures
within each (monolingual) stream and then
produce cross-lingual links between streams.
2 Authoring
2.1 Automatic Authoring
As Automatic Authoring is the task of au-
tomatically deriving a hypertextual structure
from a set of available news articles (in three
different languages English, Spanish and Ital-
ian in our case), the complexity of the overall
framework requires a suitable decomposition:
Text processing requires at least the de-
tection of morphosyntactic information char-
acterising the source texts: recognition, nor-
malisation, and assignment of roles is required
for the main participants for the different
events/facts described.
Event Matching is then the activity of
selecting the relevant facts of a news arti-
cle, in terms of their general type (e.g. sell-
ing or buying companies, winning a football
match), their participants and their related
roles (e.g. the company sold or the winning
football team).
Authoring is thus the activity of gener-
ating links between news articles according
to relationships established among facts de-
tected in the previous phase.
For instance, a company acquisition can be
referred to in one (or more) news items as:
? Intel, the world?s largest chipmaker,
bought a unit of Danish cable maker NKT
that designs high-speed computer chips ...
1See http://namic.itaca.it.
2EFE and ANSA, the major news agencies in Spain
and Italy respectively, and the Financial Times are all
members of the NAMIC consortium.
? The giant chip maker Intel said it ac-
quired the closely held ICP Vortex Com-
putersysteme, a German maker of sys-
tems ...
? Intel ha acquistato Xircom inc. per 748
milioni di dollari.
The hypothesis underlying Authoring is
that all the above news items deal with facts
in the same area of interest to a potential class
of readers. They should be thus linked and
links should suggest to the user that the un-
derlying motivation (used to decide whether
or not to follow an available link) is that they
all refer to Intel acquisitions.
Notice that a link generation process based
only upon words would fail in the above case
as the common word (that could play the role
of anchor in linking) is the proper noun Intel.
As no other information is available, the re-
sulting set of potential matches can be huge
and the connectivity too high.
In order to get the suitable links the equiv-
alence between the senses of bought and ac-
quired in the first two news items must be
known. Although such a relation can be
drawn by mechanisms like query expansion or
thesauri of synonyms (e.g. WordNet (Miller,
1990)), word polysemy and noise may re-
sult in an inherent proliferation of irrelevant
matches. Contextual information is critical
here. Notice that the senses of ?buy? and ?ac-
quire? are constrained by the role played by
Intel as ?agent ? and NKT or ICP Vortex be-
ing the sold companies. In fact, Intel buys
silicon represents an unwanted sense of the
verb and should be distinguished.
The relevant information concerning Intel
should be thus limited to:
? Intel buys a unit of NKT
? Intel acquires ICP Vortex.
These descriptions provide the core infor-
mation able to establish equivalence among
the underlying events. Whenever base event
descriptions are available the linking process
can be carried out via simpler equivalence in-
ferences. The Authoring problem is thus a
side effect of the overall language-processing
task.
According to the suggested decomposition
all the above steps are mandatory. First text
processing is responsible for morpho-syntactic
recognition. Morphological units and syntac-
tic relations are produced for each sentence at
this stage. However, syntactic relations (e.g.
among subjects and verbs) are not sufficient
for proper event characterisation. In the ex-
ample(s), the subject of the verb acquire is
a pronoun only anaphorically referring to In-
tel. Co-reference resolution is usually applied
to this kind of mismatch at the surface level.
This capability is under the responsibility of
the event matching phase. Moreover, in or-
der to keep track of events over syntactic rep-
resentations, references to a target ontology
are required. In such an ontology, equiva-
lence among facts (e.g. buying companies) is
represented. For instance, the relation among
buy and acquire can be encoded under a more
general notion of financial acquisition. On-
tologies also define the set of relevant facts of
the target domain. A financial acquisition is
a perfect example of what is needed in cor-
porate industrial news but is less important,
for example, in sports news, where hiring of
players seems a more relevant event class.
Conceptual differences among facts (de-
tected during event matching) motivate a se-
lective notion of hyperlinking. These links
can be thus generated during the automatic
authoring phase. They are ontologically jus-
tified as their conceptual representation is al-
ready available at this stage. Types as same
acquisition fact, same person, or company can
be used to distinguish links and make expla-
nations available to the user.
2.2 Multilingual Automatic
Authoring
?From a multilingual perspective, the prob-
lem is to establish links among news in dif-
ferent languages. Full-text approaches can
rely only on language independent phenom-
ena (e.g. proper nouns like Intel) that are
very limited in texts. Most of the above-
mentioned inferences require language neu-
tral information (i.e. conceptual and not lexi-
cal constraints). The inherent overgeneration
related to word polysemy affects the results
of translation-based approaches. Again prin-
cipled representations made available by IE
processes (i.e. templates) provide a viable
solution. The different event realisations (in
the different languages) can be handled dur-
ing the overall event matching. A lexical in-
terface to the ontology is able to factor the
language specific information. As syntactic
differences are handled during text process-
ing, the result is a common domain model for
IE plus independent lexical interfaces. The
unified representation of the set of facts ac-
tivates multilingual linking at a conceptual
level, thus making the Authoring a language
independent process. Some challenges of such
a framework are:
? the size of the ontological resources re-
quired in terms of taxonomic (i.e. IS A
relations) and conceptual information
(i.e. classes of events and implied
participant-event relations)
? the size of the lexical interfaces to the
ontology available for the different lan-
guages
? the amount of task dependent knowledge.
For example the definition of the set of
events useful for the target application is
underspecified.
In the following, we propose a complex ar-
chitecture where the above problems are
approached according to well-assessed tech-
niques presented elsewhere. Robust Informa-
tion Extraction is adopted (Humphreys et al,
1998) as an overall method for text process-
ing and event matching. Target events are
semiautomatically derived from domain texts
and represented in the IE engine ontology. Fi-
nally, multilinguality is realised by assuming a
large-scale multilingual lexical hierarchy as a
reference ontology for nominal concepts. The
resulting architecture for Multilingual Auto-
matic Authoring is presented in Section 3.4.
3 The NAMIC system
3.1 Large scale IE for Automatic
Authoring
Information Extraction is a very good ap-
proach to Automatic Authoring for a num-
ber of reasons. The key components of an IE
system are events and objects - the kind of
components that trigger hyperlinks in an Au-
thoring system. Coreference is a significant
part of Information Extraction and indeed a
necessary component in Authoring. Named
Entities - people, places, and organisations,
etc. - play an important part in Authoring
and again are firmly addressed in Information
Extraction systems.
The role of a world model as a method
for event matching and coreferencing
The world model is an ontological represen-
tation of events and objects for a particular
domain or set of domains. The world model
is made up of a set of event and object types,
with attributes. The event types characterise
a set of events in a particular domain and
are usually represented in a text by verbs.
Object Types on the other hand, are best
thought of as characterising a set of people,
places or things and are usually represented
in a text by nouns (both proper and com-
mon). When used as part of an Information
Extraction system, the instances of each type
are inserted/added to the world model. Once
the instances have been added, a procedure
is carried out to link those instances that re-
fer to the same thing - achieving coreference
resolution.
In NAMIC, the world model is created
using the XI cross-classification hierarchy
(Gaizauskas and Humphreys, 1996). The def-
inition of a XI cross-classification hierarchy is
referred to as an ontology, and this together
with an association of attributes with nodes
in the ontology forms the world model. Pro-
cessing a text acts to populate this initially
bare world model with the various instances
and relations mentioned in the text, convert-
ing it into a discourse model specific to the
particular text.
The attributes associated with nodes in
the ontology are simple attribute:value pairs
where the value may either be fixed, as in
the attribute animate:yes which is associ-
ated with the person node, or where the value
may be dependent on various conditions, the
evaluation of which makes reference to other
information in the model.
3.1.1 The Description of LaSIE
LaSIE is a Large-scale Information Ex-
traction system, developed for MUC (Mes-
sage Understanding Conference) competi-
tions, comprised of a variety of modules, see
(Humphreys et al, 1998; MUC, 1998). Al-
though we are not using the complete LaSIE
system in NAMIC, we are using 2 of the key
modules - the Named Entity Matcher and the
Discourse Processor. Below is a description of
each of these modules.
Named Entity Matcher The Named En-
tity Matcher finds named entities through
a secondary phase of parsing which uses a
named entity grammar and a set of gazetteer
lists. It takes as input parsed text from the
first phase of parsing and the named entity
grammar which contains rules for finding a
predefined set of named entities and a set of
gazetteer lists containing proper nouns. The
Name Entity Matcher returns the text with
the Named Entities marked. The Named En-
tities in NAMIC are PERSONS, ORGANI-
SATIONS, LOCATIONS, and DATES. The
Named Entity grammar contains rules for
coreferring abbreviations as well as different
ways of expressing the same named entity
such as Dr. Smith, John Smith and Mr.
Smith occurring in the same article.
Discourse Processor The Discourse Pro-
cessor module translates the semantic rep-
resentation produced by the parser into a
representation of instances, their ontolog-
ical classes and their attributes, in the
XI knowledge representation language (see
Gaizauskas(1996)). XI allows a straightfor-
ward definition of cross-classification hierar-
chies, the association of arbitrary attributes
with classes or instances, and a simple mech-
anism to inherit attributes from classes or in-
stances higher in the hierarchy.
The semantic representation produced by
the parser for a single sentence is processed
by adding its instances, together with their
attributes, to the discourse model which has
been constructed so far for the text.
Following the addition of the instances
mentioned in the current sentence, together
with any presuppositions that they inherit,
the coreference algorithm is applied to at-
tempt to resolve, or in fact merge, each of
the newly added instances with instances cur-
rently in the discourse model.
The merging of instances involves the re-
moval of the least specific instance (i.e. the
highest in the ontology) and the addition of
all its attributes to the other instance. This
results in a single instance with more than one
realisation attribute, which corresponds to a
single entity mentioned more than once in the
text, i.e. a coreference.
3.2 Ontological Modeling
As we have seen in section 3.1, some critical
issues of the NAMIC project rely on the per-
formance of the lexical and conceptual compo-
nents of all linguistic processors. As NAMIC
faces large-scale coverage of news in several
languages we decided to adopt EuroWordNet
(Vossen, 1998) as a common semantic formal-
ism to support:
? lexical semantic inferences (e.g. general-
isation, disambiguation)
? broad coverage (e.g. lexical and semanti-
cal) and
? a common interlingual platform for link-
ing events from different documents.
The NAMIC ontology consists of 40 prede-
fined object classes and 46 attribute types re-
lated to Name Entity objects and nearly 1000
objects relating to EuroWordNet base con-
cepts.
3.2.1 EuroWordNet as a Multilingual
Lexical Knowledge Base
Since the world model aims to describe the
language used in a given domain via events
and objects, the accuracy and breadth of the
model will impact how well the information
extraction works.
EuroWordNet (Vossen, 1998) is a multilin-
gual lexical knowledge base (LKB) with word-
nets for several European languages (Dutch,
Italian, Spanish, German, French, Czech and
Estonian). The wordnets are structured
in the same way as the American wordnet
for English developed at Princeton (Miller,
1990) containing synsets (sets of synonymous
words) with basic semantic relations between
them.
Each wordnet represents a unique
language-internal system of lexicalisa-
tions. In addition, the wordnets are linked
to an Inter-Lingual-Index (ILI), based on
the Princeton WordNet 1.5. WordNet 1.6 is
also connected to the ILI as another English
WordNet (Daude et al, 2000). Via this
index, the languages are interconnected so
that it is possible to go from the words in
one language to words in any other language
having similar meaning. The index also
gives access to a shared top-ontology and
a subset of 1024 Base Concepts (BC). The
Base Concepts provide a common seman-
tic framework for all the languages, while
language specific properties are maintained
in the individual wordnets. The LKB can
be used, among others, for monolingual and
cross-lingual information retrieval, which
has been demonstrated in other projects
(Gonzalo et al, 1998).
3.3 Multilingual Event description
The traditional limitations of a knowledge-
based information extraction system such as
LaSIE have been the need to hand-code in-
formation for the world model - specifically
relating to the event structure of the domain.
For the NAMIC project, we have decided
to semi-automate the process of adding new
?event descriptions? to the World Model. To
us, event descriptions can be categorised as a
set of regularly occurring verbs within our do-
main, complete with their subcategorisation
information.
These verbs can be extracted with simple
statistical techniques and are, for the moment
subjected to hand pruning. Once a list of
verbs has been extracted, subcategorisation
patterns can be generated automatically using
a Galois lattice (as described in (Basili et al,
2000b)). These frames can then be uploaded
into the event hierarchy of the discourse in-
terpreter world model.
The world model can have a structure
which is essentially language independent in
all but the lowest level - at which stage lexi-
calisations relating to each representative lan-
guage are required. Associated with these lex-
icalisations are language dependent scenario
rules which control the behaviour of instances
of these events with a Discourse Model. These
rules are expected to differ across languages in
the way they control coreference for languages
which are constrained to lesser or greater de-
gree.
The lattice generates patterns which refer
to synsets in the WordNet hierarchy. For
our purposes, we will use patterns referring to
Base Concepts in the EuroWordNet hierarchy
- which allows us to exploit the Inter-Lingual-
Index as described in the previous section.
These Base Concepts serve as a level of mul-
tilingual abstraction for the conceptual con-
straints of our events, and allow us to extend
the number of semantic classes from seven
(the MUC Named Entity classifications) to
1024 - the number of base concepts in EWN.
3.4 The NAMIC Architecture
The complexity of the overall NAMIC sys-
tem required the adoption of a distributed
computing paradigm in the design. The sys-
tem is a distributed object oriented system
where services (like text processing or Multi-
lingual Authoring) are provided by indepen-
dent components and asynchronous communi-
cation is allowed. Independent news streams
for the different languages (English, Spanish,
and Italian) are assumed. Language specific
processors (LPs) are thus responsible for text
processing and event matching in indepen-
dent text units in each stream. LPs com-
pile an objective representation (see Fig. 1)
for each source texts, including the detected
morphosyntactic information, categorisation
in news standards (IPTC classes) and descrip-
tion of the relevant events. Any later Au-
thoring activity is based on this canonical
representation of the news. In particular a
monolingual process is carried out within any
stream by the three monolingual Authoring
Engines (English AE, Spanish AE, and Ital-
ian AE). A second phase is foreseen to take
into account links across streams, i.e. multi-
lingual hyper-linking: a Multilingual Author-
ing Engine (M-AE) is here foreseen. Figure
1 represents the overall flow of information.
The Language Processors are composed of a
morphosyntactic (Eng, Ita and Spa MS) and
an event-matching component (EM). The lex-
ical interfaces (ELI, SLI and ItLI) to the uni-
fied Domain model are also used during event
matching.
The linguistic processors are in charge of
producing the objective representation of in-
coming news. This task is performed during
MS analysis by two main subprocessors:
? a modular and lexicalised shallow
morpho-syntactic parser (Basili et al,
2000c), providing name entity match-
ing and extracting dependency graphs
from source sentences. Ambiguity is
controlled by part-of-speech tagging and
domain verb-subcategorisation frames
that guide the dependency recognition
phase.
? a statistical linear text classifier based
upon some of the derived linguistic fea-
tures (Basili et al, 2000a) (lemmas, POS
tags and proper nouns)
The results are then input to the event
matcher that by means of the discourse in-
terpreter (Humphreys et al, 1998) derive the
objective representation. As discussed in sec-
tion 3.1, coreferencing is a side effect of the
discourse interpretation (Humphreys et al,
1998). It is based on the multilingual domain
model where relevant events are described and
nominal concepts represented.
The overall architecture is highly modular
and open to load balancing activity as well as
to adaptation and porting. The communica-
tion interfaces among the MS and EM com-
ponents as well as among the AEs and the M-
AE processors are specified via XML DTDs.
This allows for user-friendly uploading of a
back-end database with the detected material
as well as the easy design and management of
the front-end databases (available for tempo-
rary tasks, like event matching after MS). All
the servers are objects in a distributed archi-
tecture within a CORBA environment. The
current version includes the linguistic proces-
sors (MS and EM) for all the three languages.
The English and Italian linguistic processors
are fully object oriented modules based on
EnglishMS
SpanishMS
ItalianMS
EnglishAE
SpanishAE
ItalianAE
news ObjectiveRepresentation Monolingual Links
Multilingual Links
EnglishEM
SpanishEM
ItalianEM
DomainModel
ELI
SLI
ItLI
Multi-LingualAuthoringEngine
Language Processors
Figure 1: Namic Architecture
Java. They integrate libraries written in C,
C++, Prolog, and Perl for specific functional-
ities (e.g. parsing) running under a Windows
NT platform. The Spanish linguistic proces-
sor shares the discourse interpreter and the
text classifier with the other modules, while
the morpho syntactic component is currently
a Unix server based on Perl. The use of a dis-
tributed architecture under CORBA allowed
a flexible solution to its integration into the
overall architecture. The servers can be in-
stantiated in multiple copies throughout the
network if the amount of required computa-
tion exceeds the capability of a current con-
figuration. As the workload of a news stream
is not easily predictable, distribution and dy-
namic load balancing is the only realistic ap-
proach.
4 Discussion and Future Work
The above sections have provided the out-
line of a general NLP-based approach to auto-
matic authoring. The emphasis given to tra-
ditional capabilities of Information Extraction
depends on the relevance of news content in
the target Web service scenarios as well as
on their inherent multilinguality. The bet-
ter is the generalisation provided by the IE
component, the higher is the independence
from the text source language. As a result,
IE is here seen as a natural approach to cross-
lingual hypertextual authoring. Other works
in this area make extensive use of traditional
IR techniques (e.g. full text search) or rely
on already traced (i.e. manually coded) hy-
perlinks (e.g. (Chakrabarti et al, 1998; Klein-
berg, 1999)). The suggested NAMIC architec-
ture exploits linguistic capabilities for deriv-
ing entirely original (ex novo) resources, over
dynamic, previously unreleased, streams of in-
formation.
The result is a large-scale multilingual NLP
application capitalising existing methods and
resources within an advanced software engi-
neering process. The use of a distributed
Java/CORBA architecture makes the system
very attractive for its scalability and adaptiv-
ity. It results in a very complex (but realis-
tic) NLP architecture. Its organisation (lexi-
cal interfaces with respect to the multilingual
ontology) makes it very well suited for cus-
tomisation and porting to large domains. Al-
though the current version is a prototype, it
realises the complete set of core functionali-
ties, including the main IE steps and the dis-
tributed Java/CORBA layer.
It is worth noticing that a set of extensions
are made viable within the proposed architec-
ture. A first line is the extension of the avail-
able multilingual lexical knowledge. The Dis-
course Model can be used to better reflect on-
tological relationships within a particular do-
main. These relationships could be examined
to confirm known word sense usage as well
as to postulate/propose novel word sense us-
age. Using the mechanism for the addition of
events (as categorised by verbs) to the world
model, users can specify new events which can
be added to the IE system, to achieve User
Driven IE, and deliver a form of adaptive in-
formation extraction.
The instantiated domain models can be
thus used as a basis for ontological resource
expansion as a form of adaptive process.
For example, the stored instantiations of dis-
course models within a specific domain can be
compared: it may be thus possible to recog-
nise new sets of events or objects which are
not currently utilised within the system.
The evaluation strategy that is made possi-
ble within the NAMIC consortium will make
use of the current users (i.e. news agencies)
expertise. The agreed evaluation methods
will provide evidence about the viability of
the proposed large-scale IE-based approach to
authoring, as a valuable paradigm for infor-
mation access.
Acknowledgements
This research is funded by the European
Union, grant number IST-1999-12392. We
would also like to thank all of the partners
in the NAMIC consortium.
References
R. Basili, A. Moschitti, and M.T. Pazienza. 2000a.
Language sensitive text classification. In In
proceeding of 6th RIAO Conference (RIAO
2000), Content-Based Multimedia Information
Access, Coll ge de France, Paris, France.
R. Basili, M.T. Pazienza, and M. Vindigni. 2000b.
Corpus-driven learning of event recognition
rules. In Proc. of Machine Learning for Infor-
mation Extraction workshop, held jointly with
the ECAI2000, Berlin, Germany.
R. Basili, M.T. Pazienza, and F.M. Zanzotto.
2000c. Customizable modular lexicalized pars-
ing. In Proc. of the 6th International Workshop
on Parsing Technology, IWPT2000, Trento,
Italy.
S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg,
P. Raghavan, and S. Rajagopalan. 1998. Auto-
matic resource compilation by analysing hyper-
link structure and associated text. In Proceed-
ings of the 7th International World Wide Web
Conference, Brisbane, Australia.
C. Cunningham, R. Gaizauskas, K. Humphreys,
and Y. Wilks. 1999. Experience with a lan-
guage engineering architecture: 3 years of gate.
In Proceedings of the AISB?99 Workshop on
Reference Architectures and Data Standards for
NLP, Edinburgh, UK.
J. Daude, L. Padro, and G. Rigau. 2000. Map-
ping wordnets using structural information.
In Proceedings of the 38th Annual Meeting of
the Association for Computational Linguistics
ACL?00, Hong Kong, China.
R. Gaizauskas and K. Humphreys. 1996. Xi:
A simple prolog-based language for cross-
classification and inheritance. In Proceedings of
the 6th International Conference on Artificial
Intelligence: Methodologies, Systems, Applica-
tions (AIMSA96), pages 86?95.
R. Gaizauskas and Y. Wilks. 1998. Information
Extraction: Beyond Document Retrieval. Jour-
nal of Documentation, 54(1):70?105.
J. Gonzalo, F. Verdejo, I. Chugur, and J. Cigar-
ran. 1998. Indexing with wordnet synsets
can improve text retrieval. In Proceedings of
the COLING/ACL?98 Workshop on Usage of
WordNet for NLP, Montreal, Canada.
K. Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield: De-
scription of the lasie-ii system as used for muc-7.
In Proceedings of the Seventh Message Under-
standing Conferences (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
Jon M. Kleinberg. 1999. Authoritative sources
in a hyperlinked environment. Journal of the
ACM, 46(5):604?632.
G. Miller. 1990. Five papers on wordnet. Inter-
national Journal of Lexicography, 4(3).
1998. Proceedings of the Seventh Message Under-
standing Conference (MUC-7). Morgan Kauf-
man. Available at http://www.saic.com.
M.T. Pazienza, editor. 1997. Information Ex-
traction. A Multidisciplinary Approach to an
Emerging Information Technology. Number
1299 in LNAI. Springer-Verlag, Heidelberg,
Germany.
P. Vossen. 1998. EuroWordNet: A Multilin-
gual Database with Lexical Semantic Networks.
Kluwer Academic Publishers, Dordrecht.
Knowledge-Based Multilingual Document Analysis
R. Basili
 
and R. Catizone  and L. Padro  and M.T. Pazienza  
G. Rigau  and A. Setzer  and N. Webb 
F. Zanzotto
 
 
Dept. of Computer Science, Systems and Production
University of Rome, Tor Vergata
Via di Tor Vergata
00133 Roma, Italy
basili, pazienza, zanzotto@info.uniroma2.it
 Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
R.Catizone, A.Setzer, N.Webb@dcs.shef.ac.uk
 Departament de Llenguatges i Sistemes Informatics
Universitat Politecnica de Catalunya
Centre de Recerca TALP
Jordi Girona Salgado 1-3
08034 Barcelona, Spain
l.padro, g.rigau@lsi.upc.es
Abstract
The growing availability of multilingual resources,
like EuroWordnet, has recently inspired the develop-
ment of large scale linguistic technologies, e.g. mul-
tilingual IE and Q&A, that were considered infeasi-
ble until a few years ago. In this paper a system
for categorisation and automatic authoring of news
streams in different languages is presented. In our
system, a knowledge-based approach to Information
Extraction is adopted as a support for hyperlinking.
Authoring across documents in different languages
is triggered by Named Entities and event recogni-
tion. The matching of events in texts is carried out
by discourse processing driven by a large scale world
model. This kind of multilingual analysis relies on a
lexical knowledge base of nouns(i.e. the EuroWord-
net Base Concepts) shared among English, Spanish
and Italian lexicons. The impact of the design choices
on the language independence and the possibilities it
opens for automatic learning of the event hierarchy
will be discussed.
1 Introduction
Modern information technologies are faced with the
problem of selecting, filtering, linking and manag-
ing growing amounts of multilingual information to
which access is usually critical. Our work is moti-
vated by the linking of multilingual information in a
wide range of domains. Although this problem ap-
pears to be directly related to the Information Re-
trieval task, we aimed to link articles, not in the broad
sense of clustering documents related to the same
topic, but rather more specifically linking particular
pieces of information together from different docu-
ments. Furthermore, we found that IE research, al-
though appropriate for our task, was not designed for
the scale/variety of different domains that we needed
to process. In general, creating the world model nec-
essary for the addition of a new domain to an IE sys-
tem is a time-consuming process. As such, we de-
signed an IE system that could be semi-automatically
and easily adapted to new domains - a process we will
refer to as large scale IE. The key to creating new
world models relied on incorporating large amounts
of domain knowledge. As a result we selected Eu-
roWordnet as our base knowledge source. EuroWord-
net has the advantages of 1) providing the foundation
for broad knowledge across many domains and 2) is
multilingual in nature. In this paper, we will explain
how our system works, how the knowledge base was
incorporated and a discussion of other applications
that could make use of the same technology.
2 The Application
In the 5th Framework NAMIC Project (News Agen-
cies Multilingual Information Categorisation), the de-
fined task of the system was to support the automatic
authoring of multilingual news agencies texts where
the chosen languages were English, Italian and Span-
ish. The goal was the Hypertextual linking of related
articles in one language as well as related articles in
the other project languages. One of the intermediate
goals of NAMIC was to categorise incoming news ar-
ticles, in one of the three target languages and use
Natural Language Technology to derive an ?objec-
tive representation? of the events and agents contained
within the news. This representation which is ini-
tially created once using representative news corpora
is stored in a repository and accessed in the authoring
process.
2.1 Automatic Authoring
Automatic Authoring is the task of automatically de-
riving a hypertextual structure from a set of available
news articles (in three different languages English,
Spanish and Italian in our case). This relies on the ac-
tivity of event matching. Event matching is the pro-
cess of selecting the relevant facts in a news article
in terms of their general type (e.g. selling or buying
companies, winning a football match), their partici-
pants and their related roles (e.g. the company sold or
the winning football team) Authoring is the activity
of generating links between news articles according
to relationships established among facts detected in
the previous phase.
For instance, a company acquisition can be referred
to in one (or more) news items as:
 Intel, the world?s largest chipmaker, bought a
unit of Danish cable maker NKT that designs
high-speed computer chips used in products that
direct traffic across the internet and corporate
networks.
 The giant chip maker Intel said it acquired the
closely held ICP Vortex Computersysteme, a
German maker of systems for storing data on
computer networks, to enhance its array of data-
storage products.
 Intel ha acquistato Xircom inc. per 748 milioni
di dollari.
 Le dichiarazioni della Microsoft, infatti, sono
state precedute da un certo fermento, dovuto
all?interesse verso Linux di grandi ditte quali
Corel, Compaq e non ultima Intel (che ha ac-
quistato quote della Red Hat) ...
The hypothesis underlying Authoring is that all the
above news items deal with facts in the same area of
interest to a potential class of readers. They should be
thus linked and links should suggest to the user that
the underlying motivation is that they all refer to Intel
acquisitions.
3 The NAMIC Architecture
The NAMIC system uses a modularised IE architec-
ture whose principal components, used to create the
IE repository, are morpho-syntactic analysis, cate-
gorisation and semantic analysis. During Morpho-
Syntactic analysis, a modular and lexicalised shal-
low morpho-syntactic parser (Basili et al, 2000b),
provides the extraction of dependency graphs from
source sentences. Ambiguity is controlled by part-
of-speech tagging and domain verb-subcategorisation
frames that guide the dependency recognition phase.
It is within the semantic analysis, which relies on the
output of this parser, that objects in the text, and their
relationships to key events are captured. This process
is explained in more detail in 4. In the next two sec-
tions, we will elaborate on the IE engine. For a full
description of the NAMIC Architecture see (Basili et
al., 2001).
3.1 LaSIE
In NAMIC, we have integrated a key part of the Infor-
mation Extraction system called LaSIE (Large-scale
Information Extraction system, (Humphreys et al,
1998)). Specifically, we have taken the Named Entity
Matcher and the Discourse Processor from the over-
all architecture of LaSIE. The roles of each of these
modules is outlined below.
3.1.1 Named Entity Matcher
The Named Entity (NE) Matcher finds named enti-
ties (persons, organisations, locations, and dates, in
our case) through a secondary phase of parsing which
uses a NE grammar and a set of gazetteer lists. It takes
as input parsed text from the first phase of parsing and
the NE grammar which contains rules for finding a
predefined set of named entities and a set of gazetteer
lists containing proper nouns. The NE Matcher re-
turns the text with the Named Entities marked. The
NE grammar contains rules for coreferring abbrevia-
tions as well as different ways of expressing the same
named entity such as Dr. Smith, John Smith and Mr.
Smith occurring in the same article.
3.1.2 Discourse Processor
The Discourse Processor module translates the se-
mantic representation produced by the parser into a
representation of instances, their ontological classes
and their attributes, in the XI knowledge representa-
tion language (Gaizauskas and Humphreys, 1996).
XI allows a straightforward definition of cross-
classification hierarchies, the association of arbitrary
attributes with classes or instances, and a simple
mechanism to inherit attributes from classes or in-
stances higher in the hierarchy.
The semantic representation produced by the
parser for a single sentence is processed by adding
its instances, together with their attributes, to the dis-
course model which has been constructed for a text.
Following the addition of the instances mentioned
in the current sentence, together with any presuppo-
sitions that they inherit, the coreference algorithm is
applied to attempt to resolve, or in fact merge, each
of the newly added instances with instances currently
in the discourse model.
The merging of instances involves the removal of
the least specific instance (i.e. the highest in the on-
tology) and the addition of all its attributes to the other
instance. This results in a single instance with more
than one realisation attribute, which corresponds to a
single entity mentioned more than once in the text,
i.e. a coreference.
The mechanism described here is an extremely
powerful tool for accomplishing the IE task, however,
in common with all knowledge-based approaches,
and as highlighted in the introduction to this paper,
the significant overhead in terms of development and
deployment is in the creation of the world model rep-
resentation.
4 Large-Scale World Model Acquisition
The traditional limitations of a knowledge-based in-
formation extraction system such as LaSIE have been
the need to hand-code information for the world
model - specifically relating to the event structure of
the domain. This is also valid for NAMIC. To aid the
development of the world model, a semi-automatic
boot-strapping process has been developed, which
creates the event type component of the world model.
To us, event descriptions can be categorised as a set
of regularly occurring verbs within our domain, com-
plete with their subcategorisation information.
4.1 Event Hierarchy
The domain verbs can be selected according to sta-
tistical techniques and are, for the moment, subjected
to hand pruning. Once a list of verbs has been ex-
tracted, subcategorisation patterns can be generated
automatically using a combination of weakly super-
vised example-driven machine learning algorithms.
There are mainly three induction steps. First, syn-
tactic properties are derived for each verb, express-
ing the major subcategorisation information under-
lying those verbal senses which are more important
in the domain. Then, in a second phase, verb usage
examples are used to induce the semantic properties
of nouns in argumental positions. This information
relates to selectional constraints, independently as-
signed to each verb subcategorisation pattern. Thus,
different verb senses are derived, able to describe the
main properties of the domain events (e.g. Compa-
nies acquire companies). In a third and final phase
event types are derived by grouping verbs accord-
ing to their syntactic-semantic similarities. Here,
shared properties are used to generalise from the lex-
ical level, and generate verbal groups expressing spe-
cific semantic (and thus conceptual) aspects. These
types are then fed into the event hierarchy as required
for their straightforward application within the target
IE scenario.
4.1.1 Acquisition of Subcategorisation Patterns
Each verb  is separately processed. First, each local
context (extracted from sentences in the source cor-
pus) is mapped into a feature vector describing:
 the verb  of each vector (i.e. the lexical head of
the source clause);
 the different grammatical relationships (e.g.
Subj and Obj for grammatical subject and ob-
jects respectively) as observed in the clause;
 the lexical items, usually nouns, occurring in
specific grammatical positions, e.g. the subject
Named Entity, in the clause.
Then, vectors are clustered according to the set of
shared grammatical (not lexical) properties: Only the
clauses showing the same relationships (e.g. all the
Subj- 
	 -Obj triples) enter in the same subset  .
Each cluster thus expresses a specific grammatical be-
haviour shared by several contexts (i.e. clauses) in the
corpus. The shared properties in  characterise the
cluster, as they are necessary and sufficient member-
ship conditions for the grouped contexts.
As one context can enter in more than one cluster
(as it can share all (or part) of its relations with the
others), the inclusion property establishes a natural
partial order among clusters. A cluster  is included
in another cluster  if its set of properties is larger
(i.e.  ) but it is shown only by a subset of the
contexts of the latter   . The larger the set of mem-
bership constraints is, the smaller the resulting cluster
is. In this way, clusters are naturally organised into
a lattice (called Galois lattice). Complete properties
express for each cluster candidate subcategorisation
patterns for the target verb  .
Finally, the lattice is traversed top-down and the
search stops at the more important clusters (i.e. those
showing a large set of members and characterised
by linguistically appealing properties): they are re-
tained and a lexicon of subcategorisation structures
(i.e. grammatical patterns describing different us-
ages of the same verb) is compiled for the target verb
 . For example, (buy, [Subj:X, Obj:Y]) can
be used to describe the transitive usage of the verb
	 . More details can be found in (Basili et al, 1997).
4.1.2 Corpus-driven Induction of Verb
Selectional Restrictions
The lattice can be further refined to express seman-
tic constraints over the syntactic patterns specified at
the previous stage. A technique proposed in (Basili
et al, 2000a) is adopted by deriving semantic con-
straints via synsets (i.e. synonymy sets) in the Word-
Net 1.6 base concepts (part of EuroWordNet). When
a given lattice node expresses a set of syntactic prop-
erties, then this suggests:
 a set of grammatical relations necessary to ex-
press a given verb meaning, MEANING: a Roadmap to Knowledge Technologies 
 
German Rigau. TALP Research Center. UPC. Barcelona. rigau@lsi.upc.es 
Bernardo Magnini. ITC-IRST. Povo-Trento. magnini@itc.it 
Eneko Agirre. IXA group. EHU. Donostia. eneko@si.ehu.es  
Piek Vossen. Irion Technologies. Delft. Piek.Vossen@irion.nl  
John Carroll. COGS. U. Sussex. Brighton. johnca@cogs.susx.ac.uk 
 
Abstract  
Knowledge Technologies need to extract 
knowledge from existing texts, which 
calls for advanced Human Language 
Technologies (HLT). Progress is being 
made in Natural Language Processing but 
there is still a long way towards Natural 
Language Understanding. An important 
step towards this goal is the development 
of technologies and resources that deal 
with concepts rather than words. The 
MEANING project argues that we need to 
solve two complementary and 
intermediate tasks to enable the next 
generation of intelligent open domain 
HLT application systems: Word Sense 
Disambiguation and large-scale 
enrichment of Lexical Knowledge Bases. 
Innovations in this area will lead to HLT 
with deeper understanding of texts, and 
immediate progress in real applications of 
Knowledge Technologies. 
Introduction 
The field of Information Society Technologies 
(IST) is one of the main thematic priorities of 
the European Commission for the 6th Framework 
programme. In this field, Knowledge 
Technologies (KT) aim to provide meaning to 
the petabytes of information content our 
societies will generate in the near future. 
Information and knowledge management 
systems need to evolve accordingly, to enable 
the next generation of intelligent open domain 
Human Language Technologies (HLT) that will 
deal with the growing potential of the 
knowledge-rich and multilingual society. 
In order to develop a trustable semantic web 
infrastructure and a multilingual ontology 
framework to support knowledge management a 
wide range of techniques are required to 
progressively automate the knowledge lifecycle. 
In particular, this involves extracting high-level 
meaning from the large collections of content 
data and its representation and management in a 
common knowledge base. 
Even now, building large and rich knowledge 
bases takes a great deal of expensive manual 
effort; this has severely hampered Knowledge-
Technologies and HLT application development. 
For example, dozens of person-years have been 
invest into the development of wordnets1 for 
various languages, but the data in these 
resources is still not sufficiently rich to support 
advanced concept-based HLT applications 
directly. Furthermore, resources produced by 
introspection usually fail to register what really 
occurs in texts. Applications will not scale up to 
working in the open domain without more 
detailed and rich general-purpose, which should 
perhaps include domain-specific linguistic 
knowledge.  
The MEANING project identifies two 
complementary intermediate tasks which we 
think are crucial in order to enable the next 
generation of intelligent open domain HLT 
application systems: Word Sense 
Disambiguation (WSD) and large-scale 
enrichment of Lexical Knowledge Bases.  
                                                     
1 A wordnet is a conceptually structured knowledge 
base of word senses. The English WordNet (Miller 
90, Fellbaum 98) has been developed at Princeton 
University over the past 14 years. EuroWordNet 
(Vossen 1998) is a multilingual database with 
wordnets for several European languages (Dutch, 
Italian, Spanish, German, French, Czech and 
Estonian). Balkanet is building wordnets for the 
Balkan languages following the EuroWordNet 
design. 
The advance in these two areas will allow for 
large-scale extractions of shallow meaning from 
texts, in the form of relations among concepts. 
WSD provides the technology to convert 
relations between words into relations between 
concepts. Rich and large-scale Lexical 
Knowledge Bases will have be the repositories 
of extracted relations and other linguistic 
knowledge.  
However, progress is difficult due to the 
following interdependence: 
? In order to achieve accurate WSD, we need 
far more linguistic and semantic knowledge 
than is available in current lexical 
knowledge bases (e.g. current wordnets).  
? In order to enrich Lexical Knowledge Bases 
we need to acquire information from 
corpora, which have been accurately tagged 
with word senses.  
Providing innovative technology to solve this 
problem will be one of the main challenges to 
access KTs.  
Following this introduction section 1 presents 
the major research goals in HLT. Section 2 
presents the MEANING roadmap. Finally, 
section 4 draws the conclusions. 
1 Major research goals in HLT 
In order to extend the state-of-the-art in human 
language technologies (HLT) future research 
must devise: (1) innovative processes and tools 
for automatic acquisition of lexical knowledge 
from large-scale document collections; (2) novel 
techniques for accurately selecting the sense of 
open-class words in a large number of 
languages; (3) ways to enrich existing 
multilingual linguistic knowledge resources with 
new kinds of lexical information by 
automatically mapping information across 
languages. We present each one in turn. 
1.1 Dealing with knowledge acquisition 
The acquisition of linguistic knowledge from 
corpora has been a very successful line of 
research. Research in the acquisition of 
subcategorization information, selectional 
preferences, in thematic role assignments and 
diathesis alternations (Agirre and Mart?nez 
2001, 2002, McCarthy and Korhonen, 1998; 
Korhonen et al, 2000; McCarthy 2001), domain 
information (Magnini and Cavagli? 2000), topic 
signatures (Agirre et al 2001b), lexico-semantic 
relations between words (Agirre et al 2002) etc. 
has obtained encouraging results. The 
acquisition process usually involves large bodies 
of text, which have been previously processed 
with shallow language processors.  
Much of the use of the acquired knowledge 
has been hampered by the fact that the texts are 
not sense-disambiguated, and therefore, only 
knowledge for words can be acquired, that is, 
subcategorization for words, selectional 
preferences for words, etc. It is a well 
established fact that much of the linguistic 
behavior of words can be better explained if it is 
keyed to word senses.  
For instance, the subcategorization frames of 
verbs are highly dependent of the sense of the 
verb. Some senses of a given verb allow for a 
particular combination of complements, while 
others do not (McCarthy, 2001). The same is 
applicable to selectional preferences; traditional 
approaches that learn selectional preferences for 
a verb, tend to mix e.g. all subjects for differents 
senses, even if verbs can have different 
selectional preferences for each word sense 
(Agirre & Martinez, 2002). 
Having texts automatically sense-tagged with 
high accuracy will produce significantly better 
acquired knowledge at a sense level, including 
subcategorization frequencies, domain 
information, topic signatures, selectional 
preferences, specific lexico-semantic relations, 
thematic role assignments and diathesis 
alternations. It will also facilitate the 
investigation on automatic methods for dealing 
with new senses not present in current wordnets 
and clustering of word senses. Furthermore, 
linguistic information keyed to word senses that 
are linked to interlingual concepts (as proposed 
in the EuroWordNet model), can be easily 
integrated in a multilingual Lexical Knowledge 
Base (cf. section 2.3) 
2.2 Dealing with WSD 
Word Sense Disambiguation (WSD) is the task 
of assigning the appropriate meaning (sense) to a 
given word in a text or discourse. Ide and 
Veronis (1998) argue that word sense ambiguity 
is a central problem for many established HLT 
applications (for example Machine Translation, 
Information Extraction and Information 
Retrieval). This is also the case for associated 
sub-tasks (i.e. reference resolution and parsing). 
For this reason many international research 
groups are working on WSD, using a wide range 
of approaches. However, no large-scale broad-
coverage accurate WSD system has been built 
up to date2. With current state-of-the-art 
accuracy in the range 60-70%, WSD is one of 
the most important open problems in Natural 
Language Processing. 
A promising current line of research uses 
semantically annotated corpora to train Machine 
Learning (ML) algorithms to decide which word 
sense to choose in which contexts. The words in 
these annotated corpora are tagged manually 
with semantic classes taken from a particular 
lexical semantic resource (most commonly 
WordNet). Many standard ML techniques have 
been tried, such as Bayesian learning, Exemplar 
based learning, Decision Lists, and recently 
margin-based classifiers like Boosting and 
Support Vector Machines (Escudero et al, 
2000a, 2000b, 2000c, 2000d, 2001; Mart?nez 
and Agirre, 2000). These approaches are termed 
"supervised" because they learn from previously 
sense annotated data and therefore they require a 
large amount of human intervention to annotate 
the training data. 
Supervised WSD systems are data hungry. 
They suffer from the "knowledge acquisition 
bottleneck", it takes them mere seconds to digest 
all of the processed corpus contained in training 
materials that take months to annotate manually. 
So, although Machine Learning classifiers are 
undeniably effective, they are not feasible until 
we can obtain reliable unsupervised training 
data. Ng (1997) estimates that the manual 
annotation effort necessary to build a broad 
coverage word-sense annotated English corpus 
is about 16 person-years; and this effort would 
have to be replicated for each different language. 
Unfortunately, many people think that Ng?s 
estimate might fell short, as the annotated corpus 
thus produced is not guaranteed to enable high 
accuracy WSD.  
Some recent work is focusing on reducing 
the acquisition cost and the need for supervision 
                                                     
2 See the conclusions of the SENSEVAL-2 
competition: http://www.sle.sharp.co.uk/senseval2/ 
in corpus-based methods for WSD. Leacock et 
al. (1998) and Mihalcea and Moldovan (1999) 
automatically generate arbitrarily large corpora 
for unsupervised WSD training, using the 
synonyms or definitions of word senses 
provided in WordNet to formulate search engine 
queries over the Web. In another line of 
research, (Yarowsky, 1995) and (Blum and 
Mitchell, 1998) have shown that it is possible to 
reduce the need for supervision with the help of 
large amounts of unannotated data. Applying 
these ideas, (Agirre and Mart?nez, 2000) has 
developed knowledge-based prototypes for 
obtaining accurate examples from the web for 
specific WordNet synsets, as well as, large 
quantities of unannotated examples. 
But in order to make significant advances in 
WSD system accuracy, systems need to be able 
to use types of lexical knowledge that are not 
currently available in wide-coverage lexical 
knowledge bases: for example subcategorisation 
frequencies for predicates (particularly verbs) 
rely on word senses, selectional preferences of 
predicates for classes of arguments, amongst 
others (Carroll and McCarthy, 2000; McCarthy 
et al, 2001; Agirre and Mart?nez, 2002;).  
2.3 Dealing with multilingualism  
Language diversity is at the same time a 
valuable cultural heritage worth preserving, and 
an obstacle to achieving a more cohesive social 
and economic development. This situation has 
been further stressed as a major challenge in IST 
research lines. Improving language 
communication capabilities is a prerequisite for 
increasing industrial competitiveness, this way 
leading to a sound growth in key economic 
sectors.  
However, this obstacle can be helpful 
because all languages realize the meaning in 
different ways. We can benefit from this fact 
using a novel multilingual mapping process that 
exploits the EuroWordNet architecture. In 
EuroWordNet local wordnets are linked via an 
Inter-Lingual-Index (ILI) allowing the 
connection from words in one language to 
translation equivalent words in any of the other 
languages. In that way, technological advances 
in one language can help the other.  
For instance, for Basque, being an 
agglutinative language with very rich 
morphological-syntactic information, it is 
possible to extract semantic relations that would 
be more difficult to capture in other languages. 
Below we can see an example of the relation 
betwewen silversmith and silver, extracted from 
the Basque words zilargile ? zilar respectively. 
This relation has been disambiguated into the 
?maker_of? lexico-semantic relation (Agirre & 
Lersundi, 2000).  
On the contrary, Basque is not largely present 
in the web as the others. Using this approach it is 
possible to balance both gaps.  
Although the technology to provide 
compatibility across wordnets exits (Daud? et al 
1999, 2000, 2001), new research is needed for 
porting and uploading the various types of 
knowledge across languages, and new ways to 
test the validity of the ported knowledge in the 
target languages.  
3. The MEANING Roadmap 
The improvements mentioned above have been 
explored separately with relative success. In 
fact, no research group in isolation has tried to 
combine all this aforementioned factors. We 
designed the MEANING project3 convinced that 
only a combination of all relevant knowledge 
and resources will be able to produce significant 
advances in this crucial research area.  
MEANING will treat the web as a (huge) 
corpus to learn information from, since even the 
largest conventional corpora available (e.g. the 
Reuters corpus, the British National Corpus) are 
not large enough to be able to acquire reliable 
information in sufficient detail about language 
behaviour. Moreover, most languages do not 
have large or diverse enough corpora available. 
MEANING proposes an innovative 
bootstrapping process to deal with the inter-
dependency between WSD and knowledge 
acquisition: 
1. Train accurate WSD systems and apply 
them to very large corpora by coupling 
knowledge-based techniques on the existing 
EuroWordNet (e.g. to populate it with 
domain labels, to induce automatically 
                                                     
                                                     3 Started in March 2002, MEANING IST-2001-
34460 "Developing Multilingual Web-scale 
Language Technologies" is a three years research 
project funded by the EC. 
training examples) with ML techniques that 
combine very large amounts of labeled and 
unlabeled data. When ready, use also the 
knowledge acquired in 2. 
2. Use the obtained accurate WSD data in 
conjunction with shallow parsing techniques 
and domain tagging to extract new linguistic 
knowledge to incorporate into 
EuroWordNet. 
This method will be able to break this 
interdependency in a series of cycles thanks to 
the fact that the WSD system will be based on 
all domain information, sophisticated linguistic 
knowledge, large numbers of automatically 
tagged examples from the web, and a 
combination of annotated and unannotated data. 
The first WSD system will have weaker 
linguistic knowledge, but the sole combination 
of the rest of the factors will produce significant 
performance gains. Besides, some of the 
required linguistic knowledge can be acquired 
from unnanotated data, and can therefore be 
acquired without using any WSD system. Once 
acceptable WSD is available, the acquired 
knowledge will be of a higher quality, and will 
allow for better WSD performance. 
Multilingualism will be also helpful for 
MEANING. The idiosyncratic way the meaning 
is realised in a particular language will be 
captured and ported to the rest of languages 
involved in the project4 using EuroWordNet as a 
Multilingual Central Repository in three 
consecutive phases (see figure 1). 
For instance, selectional preferences acquired 
for verb senses based on the English corpora, 
can be uploaded into the Multilingual Central 
Repository. As the selectional prefenrece 
relation is keyed to concepts in the repository, 
this knowledge can be ported to the other 
languages. Of course, the ported knowledge 
needs to be checked in order to evaluate the 
validity of this approach.  
Below, we can see the selectional preference 
for the first sense of know from (Agirre & 
martinez, 2002). The first sense of know is 
univocally linked to <know, cognize,
cognise>, which in EuroWordNet is linked to 
4 MEANING will work with three major European 
languages (English, Spanish and Italian) and two 
minority languages (Catalan and Basque).  
w
S
a
B
s
0
0
0
0
0
4
W
s
m
p
m
c
e
Multilingual Central Repository 
EANING is going to constitute 
wledge resource for a number of 
sses that need large amounts of 
to be effective tools (e.g. web 
P tools and software of the next 
l benefit from the MEANING 
Multilingual
Central Repository
Italian
EWN
Basque
EWN
Spanish
EWN
English
EWN
Basque
Web Corpus
Italian
Web Corpus
English
Web Corpus
Catalan
EWN
Spanish
Web Corpus
Catalan
Web Corpus
ACQ
ACQACQ
ACQ
UPLOADUPLOAD
UPLOADUPLOAD
PORT
PORT
PORT
PORT
WSD
WSD
WSD
WSD
 access applications are based on 
NG will open the way for access 
gual web based on concepts, 
lications with capabilities that 
ceed those currently available. 
ill facilitate development of 
pen domain Internet applications 
tion/Answering, Cross Lingual 
etrieval, Summarisation, Text 
Event Tracking, Information 
achine Translation, etc.). 
EANING will supply a common 
cture to Internet documents, thus 
owledge management of web 
ommon conceptual structure is a ord senses conocer_1 and saber_1 in 
panish, con?ixer_1 and saber_1 in Catalan 
nd antzeman_1, jakin_2 and ezagutu_1 in 
asque.  
ense 1: know, cognize -- (be
cognizant or aware of a fact or a
specific piece of information;
possess knowledge or information
about;
,1128 <communication> 
,0615 <measure quantity amount quantum> 
,0535 <attribute> 
,0389 <object physical_object> 
,0307 <cognition knowledge> 
 Conclusions 
here the acquisition of knowledge  from large-
cale document collections will be  one of the 
ajor challenge for the next generation of text 
rocessing applications, MEANING emphasises 
ultilingual  content-based access to web 
ontent. Moreover, it can provide a keystone 
nabling technologies for the semantic web. In 
particular, the 
produced by M
the natural kno
semantic proce
linguistic data 
ontologies). NL
generation wil
outcomes.  
Figure 1: MEANING data flow. 
Current web
words; MEANI
to the multilin
providing app
significantly ex
MEANING w
concept-based o
(such as Ques
Information R
Categorisation, 
Extraction, M
Furthermore, M
conceptual stru
facilitating kn
content. This c
decisive enabling technology for allowing the 
semantic web. 
Acknowledgements 
The MEANING project is funded by the 
European Commission (IST-2001-34460). 
References 
Agirre E. and Lersundi M. Extracci?n de relaciones 
l?xico-sem?nticas a partir de palabras derivadas 
usando patrones de definici?n. Proceedings of the 
Annual SEPLN meeting. Spain, 2000. 
Agirre E., Lersundi M. and Mart?nez D. A 
Multilingual Approach to Disambiguate 
Prepositions and Case Suffixes. Proceeding of the 
Workshop ?Word Sense Disambiguation: Recent 
Successes and Future Directions? organized by 
ACL 2002.  
Agirre E. and Mart?nez D. Exploring automatic word 
sense disambiguation with decision lists and the 
Web.  Proceedings of the Workshop ?Semantic 
Annotation And Intelligent Annotation? organized 
by COLING 2000. Luxembourg. 2000.  
Agirre E. and Martinez D. Learning class-to-class 
selectional preferences. Proceedings of the 
Workshop "Computational Natural Language 
Learning" (CoNLL-2001). In conjunction with 
ACL'2001/EACL'2001. Toulouse. 2001. 
Agirre E., Ansa O., Mart?nez D. and Hovy E. 
Enriching WordNet concepts with topic signatures. 
Proceedings of the NAACL workshop on WordNet 
and Other lexical Resources: Applications, 
Extensions and Customizations. Pittsburg. 2001. 
Agirre E. and Martinez D. Integrating selectional 
preferences in WordNet. Proceedings of the first 
International WordNet Conference. Mysore, India, 
2002. 
Blum A. and Mitchel T. Combining labelled and 
unlabeled data with co-training. In Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory. 1998. 
Carroll, J. and McCarthy, D. Word sense 
disambiguation using automatically acquired 
verbal preferences. Computers and the Humanities. 
Senseval Special Issue, Vol. 34, No 1-2. 2000. 
Daud? J., Padr? L. and Rigau G., Mapping 
Multilingual Hierarchies using Relaxation 
Labelling, Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora (EMNLP/VLC'99). Maryland, 
1999.  
Daud? J., Padr? L. and Rigau G., Mapping WordNets 
Using Structural Information , 38th Anual Meeting 
of the ACL. Hong Kong, 2000.  
Daud? J., Padr? L. and Rigau G., A Complete WN1.5 
to WN1.6 Mapping, Proceedings of NAACL 
Workshop "WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations". 
Pittsburg, PA, 2001. 
Escudero G., M?rquez L. and Rigau G., Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 11th European Conference on 
Machine Learning. Barcelona. 2000.  
Escudero G., M?rquez L. and Rigau G., Naive Bayes 
and Exemplar-Based approaches to Word Sense 
Disambiguation Revisited.  Proceedings of the 14th 
European Conference on Artificial Intelligence, 
Berlin. 2000.  
Escudero G., M?rquez L. and Rigau G., A 
Comparison between Supervised Learning 
Algorithms for Word Sense Disambiguation. 
Proceedings of Fourth Computational Natural 
Language Learning Workshop. Lisbon. 2000.  
Escudero G., M?rquez L. and Rigau G., An Empirical 
Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems. Proceedings 
of Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. Hong Kong. 2000. 
Escudero G., M?rquez L. and Rigau G., Using 
LazyBoosting for Word Sense Disambiguation. 
Proceedings of 2nd International Workshop 
?Evaluating Word Sense Disambiguation 
Systems?, SENSEVAL-2. Toulouse. 2001. 
Fellbaum C. editor. WordNet An Electronic Lexical 
Database. The MIT Press. 1998. 
Ide, N. and V?ronis, J. Introduction to the special 
issue on word sense disambiguation: The state of 
the art. Computational Linguistics, 24 (1), 1998. 
Korhonen A., Gorrell, G. and McCarthy D. Statistical 
Filtering and Subcategorization Frame 
Acquisition. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
Hong Kong. 2000. 
Leacock, C. Chodorow, M. and Miller, G.A. Using 
Corpus Statistics and WordNet Relations for Sense 
Identication, Computational Linguistics, 24(1), 
1998. 
Magnini B. and Cavagli? G., Integrating subject field 
codes into WordNet. In Proceedings of the 2nd 
International Conference on Language Resources 
and Evaluation, Athens. 2000. 
Mart?nez D. and Agirre E. One Sense per Collocation 
and Genre/Topic Variations. Proceedings of the 
Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. Hong Kong, 2000. 
McCarthy, D. and Korhonen, A. Detecting verbal 
participation in diathesis alternations. Proceedings 
of the 17th International Conference on 
Computational Linguistics and 36th Annual 
Meeting of the Association for Computational 
Linguistics COLING-ACL'98. Montreal. 1998.  
McCarthy D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, 
Subcategorization Frames and Selectional 
Preferences. Ph.D. thesis, University of Sussex. 
2001. 
McCarthy D., Carroll J. and Preiss J. Disambiguating 
noun and verb senses using automatically acquired 
selectional preferences. Proceedings of the 
SENSEVAL-2 Workshop at ACL/EACL'01, 
Toulouse. 2001. 
Mihalcea R. and Moldovan D. An automatic method 
for generating sense tagged corpora. In 
Proceedings of American Association for Artificial 
Intelligence. 1999. 
Miller G. Five papers on WordNet, Special Issue of 
International Journal of Lexicogrphy 3(4). 1990. 
Ng. H. T. Getting Serious about Word Sense 
Disambiguation. In Proceedings of Workshop 
?Tagging Text with Lexical Semantics: Why, what 
and how??, Washington, 1997. 
Vossen P. EuroWordNet: A Multilingual Database 
with Lexical Semantic Networks, Kluwer Academic 
Publishers, Dordrecht. 1998. 
Yarowsky D., Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 1995. 
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 88?97,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Simple, Robust and (almost) Unsupervised Generation of Polarity
Lexicons for Multiple Languages
I
?
naki San Vicente, Rodrigo Agerri, German Rigau
IXA NLP Group
University of the Basque Country (UPV/EHU)
Donostia-San Sebasti?an
{inaki.sanvicente,rodrigo.agerri,german.rigau}@ehu.es
Abstract
This paper presents a simple, robust and
(almost) unsupervised dictionary-based
method, qwn-ppv (Q-WordNet as Person-
alized PageRanking Vector) to automati-
cally generate polarity lexicons. We show
that qwn-ppv outperforms other automat-
ically generated lexicons for the four ex-
trinsic evaluations presented here. It also
shows very competitive and robust results
with respect to manually annotated ones.
Results suggest that no single lexicon is
best for every task and dataset and that
the intrinsic evaluation of polarity lexicons
is not a good performance indicator on
a Sentiment Analysis task. The qwn-ppv
method allows to easily create quality po-
larity lexicons whenever no domain-based
annotated corpora are available for a given
language.
1 Introduction
Opinion Mining and Sentiment Analysis are im-
portant for determining opinions about commer-
cial products, on companies reputation manage-
ment, brand monitoring, or to track attitudes by
mining social media, etc. Given the explosion of
information produced and shared via the Internet,
it is not possible to keep up with the constant flow
of new information by manual methods.
Sentiment Analysis often relies on the availabil-
ity of words and phrases annotated according to
the positive or negative connotations they convey.
?Beautiful?, ?wonderful?, and ?amazing? are exam-
ples of positive words whereas ?bad?, ?awful?, and
?poor? are examples of negatives.
The creation of lists of sentiment words has
generally been performed by means of manual-,
dictionary- and corpus-based methods. Manually
collecting such lists of polarity annotated words is
labor intensive and time consuming, and is thus
usually combined with automated approaches as
the final check to correct mistakes. However,
there are well known lexicons which have been
fully (Stone et al., 1966; Taboada et al., 2010) or
at least partially manually created (Hu and Liu,
2004; Riloff and Wiebe, 2003).
Dictionary-based methods rely on some dictio-
nary or lexical knowledge base (LKB) such as
WordNet (Fellbaum and Miller, 1998) that con-
tain synonyms and antonyms for each word. A
simple technique in this approach is to start with
some sentiment words as seeds which are then
used to perform some iterative propagation on the
LKB (Hu and Liu, 2004; Strapparava and Vali-
tutti, 2004; Kim and Hovy, 2004; Takamura et al.,
2005; Turney and Littman, 2003; Mohammad et
al., 2009; Agerri and Garc??a-Serrano, 2010; Bac-
cianella et al., 2010).
Corpus-based methods have usually been ap-
plied to obtain domain-specific polarity lexicons:
they have been created by either starting from a
seed list of known words and trying to find other
related words in a corpus or by attempting to di-
rectly adapt a given lexicon to a new one using
a domain-specific corpus (Hatzivassiloglou and
McKeown, 1997; Turney and Littman, 2003; Ding
et al., 2008; Choi and Cardie, 2009; Mihalcea et
al., 2007). One particular issue arising from cor-
pus methods is that for a given domain the same
word can be positive in one context but negative
in another. This is also a problem shared by man-
ual and dictionary-based methods, and that is why
qwn-ppv also produces synset-based lexicons for
approaches on Sentiment Analysis at sense level.
This paper presents a simple, robust and
(almost) unsupervised dictionary-based method,
QWordNet-PPV (QWordNet by Personalized
PageRank Vector) to automatically generate
polarity lexicons based on propagating some
automatically created seeds using a Personalized
88
PageRank algorithm (Agirre et al., 2014; Agirre
and Soroa, 2009) over a LKB projected into a
graph. We see qwn-ppv as an effective method-
ology to easily create polarity lexicons for any
language for which a WordNet is available.
This paper empirically shows that: (i) qwn-ppv
outperforms other automatically generated lexi-
cons (e.g. SentiWordNet 3.0, MSOL) on the 4
extrinsic evaluations presented here; it also dis-
plays competitive and robust results also with re-
spect to manually annotated lexicons; (ii) no single
polarity lexicon is fit for every Sentiment Analy-
sis task; depending on the text data and the task
itself, one lexicon will perform better than oth-
ers; (iii) if required, qwn-ppv efficently generates
many lexicons on demand, depending on the task
on which they will be used; (iv) intrinsic evalua-
tion is not appropriate to judge whether a polar-
ity lexicon is fit for a given Sentiment Analysis
(SA) task because good correlation with respect to
a gold-standard does not correspond with correla-
tion with respect to a SA task; (v) it is easily ap-
plicable to create qwn-ppv(s) for other languages,
and we demonstrate it here by creating many po-
larity lexicons not only for English but also for
Spanish; (vi) the method works at both word and
sense levels and it only requires the availability
of a LKB or dictionary; finally, (vii) a dictionary-
based method like qwn-ppv allows to easily cre-
ate quality polarity lexicons whenever no domain-
based annotated reviews are available for a given
language. After all, there usually is available a
dictionary for a given language; for example, the
Open Multilingual WordNet site lists WordNets
for up to 57 languages (Bond and Foster, 2013).
Although there has been previous work using
graph methods for obtaining lexicons via propa-
gation, the qwn-ppv method to combine the seed
generation and the Personalized PageRank prop-
agation is novel. Furthermore, it is considerable
simpler and obtains better and easier to reproduce
results than previous automatic approaches (Esuli
and Sebastiani, 2007; Mohammad et al., 2009;
Rao and Ravichandran, 2009).
Next section reviews previous related work, tak-
ing special interest on those that are currently
available for evaluation purposes. Section 3 de-
scribes the qwn-ppv method to automatically gen-
erate lexicons. The resulting lexical resources are
evaluated in section 4. We finish with some con-
cluding remarks and future work in section 5.
2 Related Work
There is a large amount of work on Sentiment
Analysis and Opinion Mining, and good com-
prehensive overviews are already available (Pang
and Lee, 2008; Liu, 2012), so we will review
the most representative and closest to the present
work. This means that we will not be review-
ing corpus-based approaches but rather those con-
structed manually or upon a dictionary or LKB.
We will in turn use the approaches here reviewed
for comparison with qwn-ppv in section 4.
The most popular manually-built polarity lexi-
con is part of the General Inquirer (Stone et al.,
1966), and consists of 1915 words labelled as
?positive? and 2291 as ?negative?. Taboada et al.
(2010) manually created their lexicons annotating
the polarity of 6232 words on a scale of 5 to -5.
Liu et al., starting with Hu and Liu (2004), have
along the years collected a manually corrected po-
larity lexicon which is formed by 4818 negative
and 2041 positive words. Another manually cor-
rected lexicon (Riloff and Wiebe, 2003) is the one
used by the Opinion Finder system (Wilson et al.,
2005) and contains 4903 negatively and 2718 pos-
itively annotated words respectively.
Among the automatically built lexicons, Turney
and Littman (2003) proposed a minimally super-
vised algorithm to calculate the polarity of a word
depending on whether it co-ocurred more with a
previously collected small set of positive words
rather than with a set of negative ones. Agerri and
Garc??a Serrano presented a very simple method
to extract the polarity information starting from
the quality synset in WordNet (Agerri and Garc??a-
Serrano, 2010). Mohammad et al. (2009) de-
veloped a method in which they first identify (by
means of affixes rules) a set of positive/negative
words which act as seeds, then used a Roget-like
thesaurus to mark the synonymous words for each
polarity type and to generalize from the seeds.
They produce several lexicons the best of which,
MSOL(ASL and GI) contains 51K and 76K en-
tries respectively and uses the full General Inquirer
as seeds. They performed both intrinsic and ex-
trinsic evaluations using the MPQA 1.1 corpus.
Finally, there are two approaches that are some-
what closer to us, because they are based on Word-
Net and graph-based methods. SentiWordNet 3.0
(Baccianella et al., 2010) is built in 4 steps: (i)
they select the synsets of 14 paradigmatic pos-
itive and negative words used as seeds (Turney
89
and Littman, 2003). These seeds are then it-
eratively extended following the construction of
WordNet-Affect (Strapparava and Valitutti, 2004).
(ii) They train 7 supervised classifiers with the
synsets? glosses which are used to assign polar-
ity and objectivity scores to WordNet senses. (iii)
In SentiWordNet 3.0 (Esuli and Sebastiani, 2007)
they take the output of the supervised classifiers
as input to applying PageRank to WordNet 3.0?s
graph. (iv) They intrinsically evaluate it with re-
spect to MicroWnOp-3.0 using the p-normalized
Kendall ? distance (Baccianella et al., 2010). Rao
and Ravichandran (2009) apply different semi-
supervised graph algorithms (Mincuts, Random-
ized Mincuts and Label Propagation) to a set of
seeds constructed from the General Inquirer. They
evaluate the generated lexicons intrinsically taking
the General Inquirer as the gold standard for those
words that had a match in the generated lexicons.
In this paper, we describe two methods to au-
tomatically generate seeds either by following
Agerri and Garc??a-Serrano (2010) or using Tur-
ney and Littman?s (2003) seeds. The automati-
cally obtained seeds are then fed into a Person-
alized PageRank algorithm which is applied over
a WordNet projected on a graph. This method is
fully automatic, simple and unsupervised as it only
relies on the availability of a LKB.
3 Generating qwn-ppv
The overall procedure of our approach consists of
two steps: (1) automatically creates a set of seeds
by iterating over a LKB (e.g. a WordNet) rela-
tions; and (2) uses the seeds to initialize contexts
to propagate over the LKB graph using a Personal-
ized Pagerank algorithm. The result is qwn-ppv(s):
Q-WordNets as Personalized PageRanking Vec-
tors.
3.1 Seed Generation
We generate seeds by means of two different auto-
matic procedures.
1. AG: We start at the quality synset of WordNet
and iterate over WordNet relations following
the original Q-WordNet method described in
Agerri and Garc??a Serrano (2010).
2. TL: We take a short manually created list
of 14 positive and negative words (Turney
and Littman, 2003) and iterate over Word-
Net using five relations: antonymy, similarity,
derived-from, pertains-to and also-see.
The AG method starts the propagation from
the attributes of the quality synset in WordNet.
There are five noun quality senses in WordNet,
two of which contain attribute relations (to adjec-
tives). From the quality
1
n
synset the attribute re-
lation takes us to positive
1
a
, negative
1
a
, good
1
a
and
bad
1
a
; quality
2
n
leads to the attributes superior
1
a
and
inferior
2
a
. The following step is to iterate through
every WordNet relation collecting (i.e., annotat-
ing) those synsets that are accessible from the
seeds. Both AG and TL methods to generate seeds
rely on a number of relations to obtain a more bal-
anced POS distribution in the output synsets. The
output of both methods is a list of (assumed to be)
positive and negative synsets. Depending on the
number of iterations performed a different number
of seeds to feed UKB is obtained. Seed numbers
vary from 100 hundred to 10K synsets. Both seed
creation methods can be applied to any WordNet,
not only Princeton WordNet, as we show in sec-
tion 4.
3.2 PPV generation
The second and last step to generate qwn-ppv(s)
consists of propagating over a WordNet graph to
obtain a Personalized PageRanking Vector (PPV),
one for each polarity. This step requires:
1. A LKB projected over a graph.
2. A Personalized PageRanking algorithm
which is applied over the graph.
3. Seeds to create contexts to start the propaga-
tion, either words or synsets.
Several undirected graphs based on WordNet
3.0 as represented by the MCR 3.0 (Agirre et
al., 2012) have been created for the experimenta-
tion, which correspond to 4 main sets: (G1) two
graphs consisting of every synset linked by the
synonymy and antonymy relations; (G2) a graph
with the nodes linked by every relation, includ-
ing glosses; (G3) a graph consisting of the synsets
linked by every relation except those that are
linked by antonymy; finally, (G4) a graph consist-
ing of the nodes related by every relation except
the antonymy and gloss relations.
Using the (G1) graphs, we propagate from the
seeds over each type of graph (synonymy and
antonymy) to obtain two rankings per polarity.
90
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .65 .45 .53 .58 .76 .66 76400 .70 .49 .58 .61 .79 .69
QWN 15508 .69 .53 .60 .62 .76 .68 11693 .64 .53 .58 .60 .70 .65
SWN 27854 .73 .57 .64 .65 .79 .71 38346 .70 .55 .62 .63 .77 .69
QWN-PPV-AG(s03 G1/w01 G1) 2589 .77 .63 .69 .69 .81 .74 5119 .68 .77 .72 .73 .64 .68
QWN-PPV-TL(s04 G1/w01 G1) 5010 .76 .66 .70 .70 .79 .74 4644 .68 .71 .69 .70 .67 .68
(Semi-) Manually created
GI* 2791 .74 .57 .64 .65 .80 .72 3376 .79 .64 .71 .70 .83 .76
OF* 4640 .77 .61 .68 .68 .81 .74 6860 .82 .71 .76 .74 .84 .79
Liu* 4127 .81 .63 .71 .70 .85 .76 6786 .85 .74 .79 .77 .87 .82
SO-CAL* 4212 .75 .57 .64 .65 .81 .72 6226 .82 .70 .76 .74 .85 .79
Table 1: Evaluation of lexicons at document level using Bespalov?s Corpus.
The graphs created in (G2), (G3) and (G4) are
used to obtain two ranks, one for each polarity by
propagating from the seeds. In all four cases the
different polarity rankings have to be combined in
order to obtain a final polarity lexicon: the polar-
ity score pol(s) of a given synset s is computed
by adding its scores in the positive rankings and
subtracting its scores in the negative rankings. If
pol(s) > 0 then s is included in the final lexicon
as positive. If pol(s) < 0 then s is included in the
final lexicon as negative. We assume that synsets
with null polarity scores have no polarity and con-
sequently they are excluded from the final lexicon.
The Personalized PageRanking propagation is
performed starting from both synsets and words
and using both AG and TL styles of seed gen-
eration, as explained in section 3.1. Combin-
ing the various possibilities will produce at least
6 different lexicons for each iteration, depending
on which decisions are taken about which graph,
seeds and word/synset to create the qwn-ppv(s). In
fact, the experiments produced hundreds of lexi-
cons, according to the different iterations for seed
generation
1
, but we will only refer to those that
obtain the best results in the extrinsic evaluations.
With respect to the algorithm to propagate over
the WordNet graph from the automatically created
seeds, we use a Personalized PageRank algorithm
(Agirre et al., 2014; Agirre and Soroa, 2009). The
famous PageRank (Brin and Page, 1998) algo-
rithm is a method to produce a rank from the ver-
tices in a graph according to their relative struc-
tural importance. PageRank has also been viewed
as the result of a Random Walk process, where the
final rank of a given node represents the probabil-
ity of a random walk over the graph which ends on
that same node. Thus, if we take the created Word-
1
The total time to generate the final 352 QWN-PPV prop-
agations amounted to around two hours of processing time in
a standard PC.
Net graph G with N vertices v
1
, . . . , v
n
and d
i
as
being the outdegree of node i, plus a N ?N tran-
sition probability matrix M where M
ji
= 1/d
i
if a link from i to j exists and 0 otherwise, then
calculating the PageRank vector over a graph G
amounts to solve the following equation (1):
Pr = cMPr + (1? c)v (1)
In the traditional PageRank, vector v is a uni-
form normalized vector whose elements values are
all 1/N , which means that all nodes in the graph
are assigned the same probabilities in case of a
random walk. Personalizing the PageRank algo-
rithm in this case means that it is possible to make
vector v non-uniform and assign stronger proba-
bilities to certain nodes, which would make the
algorithm to propagate the initial importance of
those nodes to their vicinity. Following Agirre et
al. (2014), in our approach this translates into ini-
tializing vector v with those senses obtained by the
seed generation methods described above in sec-
tion 3.1. Thus, the initialization of vector v us-
ing the seeds allows the Personalized propagation
to assign greater importance to those synsets in
the graph identified as being positive and negative,
which resuls in a PPV with the weigths skewed to-
wards those nodes initialized/personalized as pos-
itive and negative.
4 Evaluation
Previous approaches have provided intrinsic eval-
uation (Mohammad et al., 2009; Rao and
Ravichandran, 2009; Baccianella et al., 2010) us-
ing manually annotated resources such as the Gen-
eral Inquirer (Stone et al., 1966) as gold stan-
dard. To facilitate comparison, we also provide
such evaluation in section 4.3. Nevertheless, and
as demonstrated by the results of the extrinsic eval-
uations, we believe that polarity lexicons should
91
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .56 .37 .44 .76 .87 .81 76400 .67 .5 .57 .80 .89 .85
QWN 15508 .63 .22 .33 .73 .94 .83 11693 .58 .22 .31 .73 .93 .82
SWN 27854 .57 .33 .42 .75 .89 .81 38346 .55 .55 .55 .80 .8 .80
QWN-PPV-AG (w10 G3/s09 G4) 117485 .60 .63 .62 .83 .82 .83 144883 .65 .50 .57 .80 .88 .84
QWN-PPV-TL (s05 G4) 114698 .61 .58 .59 .82 .83 .83 144883 .66 .53 .59 .81 .88 .84
(Semi-) Manually created
GI* 2791 .70 .32 .44 .76 .94 .84 3376 .71 .56 .62 .82 .90 .86
OF* 4640 .67 .37 .48 .77 .92 .84 6860 .75 .68 .71 .87 .90 .88
Liu* 4127 .67 .33 .44 .76 .93 .83 6786 .78 .45 .57 .79 .94 .86
SO-CAL* 4212 .69 .3 .42 .75 .94 .84 6226 .73 .53 .61 .81 .91 .86
Table 2: Evaluation of lexicons using averaged ratio on the MPQA 1.2
test
Corpus.
in general be evaluated extrinsically. After all,
any polarity lexicon is as good as the results ob-
tained by using it for a particular Sentiment Anal-
ysis task.
Our goal is to evaluate the polarity lexicons
simplifying the evaluation parameters to avoid as
many external influences as possible on the re-
sults. We compare our work with most of the
lexicons reviewed in section 2, both at synset
and word level, both manually and automatically
generated: General Inquirer (GI), Opinion Finder
(OF), Liu, Taboada et al.?s (SO-CAL), Agerri
and Garc??a-Serrano (2010) (QWN), Mohammad
et al?s, (MSOL(ASL-GI)) and SentiWordNet 3.0
(SWN). The results presented in section 4.2 show
that extrinsic evaluation is more meaningful to de-
termine the adequacy of a polarity lexicon for a
specific Sentiment Analysis task.
4.1 Datasets and Evaluation System
Three different corpora were used: Bespalov et
al.?s (2011) and MPQA (Riloff and Wiebe, 2003)
for English, and HOpinion
2
in Spanish. In addi-
tion, we divided the corpus into two subsets (75%
development and 25% test) for applying our ratio
system for the phrase polarity task too. Note that
the development set is only used to set up the po-
larity classification task, and that the generation of
qwn-ppv lexicons is unsupervised.
For Spanish we tried to reproduce the English
settings with Bespalov?s corpus. Thus, both devel-
opment and test sets were created from the HOpin-
ion corpus. As it contains a much higher propor-
tion of positive reviews, we created also subsets
which contain a balanced number of positive and
negative reviews to allow for a more meaningful
comparison than that of table 6. Table 3 shows the
number of documents per polarity for Bespalov?s,
2
http://clic.ub.edu/corpus/hopinion
MPQA 1.2 and HOpinion.
Corpus POS docs NEG docs Total
Bespalov
dev
23,112 23,112 46,227
Bespalov
test
10,557 10,557 21,115
MPQA 1.2
dev
2,315 5,260 7,575
MPQA 1.2
test
771 1,753 2,524
MPQA 1.2
total
3,086 7,013 10,099
HOpinion Balanced
dev
1,582 1,582 3,164
HOpinion Balanced
test
528 528 1,056
HOpinion
dev
9,236 1,582 10,818
HOpinion
test
3,120 528 3,648
Table 3: Number of positive and negative docu-
ments in train and test sets.
We report results of 4 extrinsic evaluations or
tasks, three of them based on a simple ratio av-
erage system, inspired by Turney (2002), and an-
other one based on Mohammad et al. (2009). We
first implemented a simple average ratio classifier
which computes the average ratio of the polarity
words found in document d:
polarity(d) =
?
w?d
pol(w)
|d|
(2)
where, for each polarity, pol(w) is 1 if w is in-
cluded in the polarity lexicon and 0 otherwise.
Documents that reach a certain threshold are clas-
sified as positive, and otherwise as negative. To
setup an evaluation enviroment as fair as possi-
ble for every lexicon, the threshold is optimised by
maximising accuracy over the development data.
Second, we implemented a phrase polarity task
identification as described by Mohammad et al.
(2009). Their method consists of: (i) if any of
the words in the target phrase is contained in the
negative lexicon, then the polarity is negative; (ii)
if none of the words are negative, and at least one
word is in the positive lexicon, then is positive;
(iii) the rest are not tagged.
We chose this very simple polarity estimators
because our aim was to minimize the role other
92
Synset Level Word level
Positives Negatives Positives Negatives
Lexicon size P R F P R F size P R F P R F
Automatically created
MSOL(ASL-GI)* 32706 .52 .48 .50 .85 .62 .71 76400 .68 .56 .62 .82 .86 .84
QWN 15508 .50 .36 .42 .84 .32 .46 11693 .45 .49 .47 .78 .51 .61
SWN 27854 .50 .45 .47 .85 .48 .61 38346 .49 .52 .50 .78 .68 .73
QWN-PPV-AG (s09 G3/w02 G3) 117485 .59 .67 .63 .85 .78 .82 147194 .64 .64 .64 .84 .83 .83
QWN-PPV-TL (w02 G3/s06 G3) 117485 .59 .57 .58 .82 .81 .81 147194 .63 .67 .65 .85 .81 .83
(Semi-) Manually created
GI* 2791 .60 .40 .47 .91 .38 .54 3376 .70 .60 .65 .93 .52 .67
OF* 4640 .63 .42 .50 .93 .46 .62 6860 .75 .71 .73 .95 .66 .78
Liu* 4127 .65 .36 .47 .94 .45 .60 6786 .78 .49 .60 .97 .61 .75
SO-CAL* 4212 .65 .37 .47 .92 .45 .60 6226 .73 .57 .64 .96 .59 .73
Table 4: Evaluation of lexicons at phrase level using Mohammad et al.?s (2009) method on MPQA
1.2
total
Corpus.
aspects play in the evaluation and focus on how,
other things being equal, polarity lexicons perform
in a Sentiment Analysis task. The average ratio
is used to present results of tables 1 and 2 (with
Bespalov corpus), and 5 and 6 (with HOpinion),
whereas Mohammad et al.?s is used to report re-
sults in table 4. Mohammad et al.?s (2009) testset
based on MPQA 1.1 is smaller, but both MPQA
1.1 and 1.2 are hugely skewed towards negative
polarity (30% positive vs. 70% negative).
All datasets were POS tagged and Word
Sense Disambiguated using FreeLing (Padr?o and
Stanilovsky, 2012; Agirre and Soroa, 2009). Hav-
ing word sense annotated datasets gives us the op-
portunity to evaluate the lexicons both at word and
sense levels. For the evaluation of those lexicons
that are synset-based, such as qwn-ppv and Sen-
tiWordNet 3.0, we convert them from senses to
words by taking every word or variant contained
in each of their senses. Moreover, if a lemma ap-
pears as a variant in several synsets the most fre-
quent polarity is assigned to that lemma.
With respect to lexicons at word level, we take
the most frequent sense according to WordNet 3.0
for each of their positive and negative words. Note
that the latter conversion, for synset based evalua-
tion, is mostly done to show that the evaluation at
synset level is harder independently of the quality
of the lexicon evaluated.
4.2 Results
Although tables 1, 2 and 4 also present re-
sults at synset level, it should be noted that the
only polarity lexicons available to us for com-
parison at synset level were Q-WordNet (Agerri
and Garc??a-Serrano, 2010) and SentiWordNet 3.0
(Baccianella et al., 2010). QWN-PPV-AG refers
to the lexicon generated starting from AG?s seeds,
and QWN-PPV-TL using TL?s seeds as described
in section 3.1. Henceforth, we will use qwn-ppv to
refer to the overall method presented in this paper,
regardless of the seeds used.
For every qwn-ppv result reported in this sec-
tion, we have used every graph described in sec-
tion 3.2. The configuration of each qwn-ppv in the
results specifies which seed iteration is used as the
initialization of the Personalized PageRank algo-
rithm, and on which graph. Thus, QWN-PPV-TL
(s05 G4) in table 2 means that the 5th iteration of
synset seeds was used to propagate over graph G4.
If the configuration were (w05 G4) it would have
meant ?the 5th iteration of word seeds were used
to propagate over graph G4?. The simplicity of
our approach allows us to generate many lexicons
simply by projecting a LKB over different graphs.
The lexicons marked with an asterisk denote
those that have been converted from word to
senses using the most frequent sense of WordNet
3.0. We would like to stress again that the purpose
of such word to synset conversion is to show that
SA tasks at synset level are harder than at word
level. In addition, it should also be noted that in
the case of SO-CAL (Taboada et al., 2010), we
have reduced what is a graded lexicon with scores
ranging from 5 to -5 into a binary one.
Table 1 shows that (at least partially) manually
built lexicons obtain the best results on this eval-
uation. It also shows that qwn-ppv clearly out-
performs any other automatically built lexicons.
Moreover, manually built lexicons suffer from the
evaluation at synset level, obtaining most of them
lower scores than qwn-ppv, although Liu?s (Hu
and Liu, 2004) still obtains the best results. In any
case, for an unsupervised procedure, qwn-ppv lex-
icons obtain very competitive results with respect
to manually created lexicons and is the best among
the automatic methods. It should also be noted that
the best results of qwn-ppv are obtained with graph
93
G1 and with very few seed iterations.
Table 2 again sees the manually built lexi-
cons performing better although overall the dif-
ferences are lower with respect to automatically
built lexicons. Among these, qwn-ppv again ob-
tains the best results, both at synset and word
level, although in the latter the differences with
MSOL(ASL-GI) are not large. Finally, table 4
shows that qwn-ppv again outperforms other auto-
matic approaches and is closer to those have been
(partially at least) manually built. In both MPQA
evaluations the best graph overall to propagate the
seeds is G3 because this type of task favours high
recall.
Positives Negatives
Lexicon size P R F P R F
Automatically created
SWN 27854 .87 .99 .93 .70 .16 .27
QWN-PPV-AG
(wrd01 G1)
3306 .86 .00 .92 .67 .01 .02
QWN-PPV-TL
(s04 G1)
5010 .89 .96 .93 .58 .30 .39
Table 5: Evaluation of Spanish lexicons using the
full HOpinion corpus at synset level.
We report results on the Spanish HOpinion cor-
pus in tables 5 and 6. Mihalcea(f) is a manu-
ally revised lexicon based on the automatically
built Mihalcea(m) (P?erez-Rosas et al., 2012). Elh-
Polar (Saralegi and San Vicente, 2013) is semi-
automatically built and manually corrected. SO-
CAL is built manually. SWN and QWN-PPV have
been built via the MCR 3.0?s ILI by applying the
synset to word conversion previously described on
the Spanish dictionary of the MCR. The results for
Spanish at word level in table 6 show the same
trend as for English: qwn-ppv is the best of the
automatic approaches and it obtains competitive
although not as good as the best of the manually
created lexicons (ElhPolar). Due to the dispro-
portionate number of positive reviews, the results
for the negative polarity are not useful to draw any
meaningful conclusions. Thus, we also performed
an evaluation with HOpinion Balanced set as listed
in table 3.
The results with a balanced HOpinion, not
shown due to lack of space, also confirm the pre-
vious trend: qwn-ppv outperforms other automatic
approaches but is still worse than the best of the
manually created ones (ElhPolar).
Positives Negatives
Lexicon size P R F P R F
Automatically created
Mihalcea(m) 2496 .86 .00 .92 .00 .00 .00
SWN 9712 .88 .97 .92 .55 .19 .28
QWN-PPV-AG
(s11 G1)
1926 .89 .97 .93 .59 .26 .36
QWN-PPV-TL
(s03 G1)
939 .89 .98 .93 .71 .26 .38
(Semi-) Manually created
ElhPolar 4673 .94 .94 .94 .64 .64 .64
Mihalcea(f) 1347 .91 .96 .93 .61 .41 .49
SO-CAL 4664 .92 .96 .94 .70 .51 .59
Table 6: Evaluation of Spanish lexicons using the
full HOpinion corpus at word level.
4.3 Intrinsic evaluation
To facilitate intrinsic comparison with previous
approaches, we evaluate our automatically gener-
ated lexicons against GI. For each qwn-ppv lex-
icon shown in previous extrinsic evaluations, we
compute the intersection between the lexicon and
GI, and evaluate the words in that intersection. Ta-
ble 7 shows results for the best-performing QWN-
PPV lexicons (both using AG and TL seeds) in
the extrinsic evaluations at word level of tables 1
(first two rows), 2 (rows 3 and 4) and 4 (rows 5
and 6). We can see that QWN-PPV lexicons sys-
tematically outperform SWN in number of correct
entries. QWN-PPV-TL lexicons obtain 75.04%
of correctness on average. The best performing
lexicon contains up to 81.07% of correct entries.
Note that we did not compare the results with
MSOL(ASL-GI) because it contains the GI.
Lexicon ? wrt. GI Acc. Pos Neg
SWN 2,755 .74 .76 .73
QWN-PPV-AG (w01 G1) 849 .71 .68 .75
QWN-PPV-TL (w01 G1) 713 .78 .80 .76
QWN-PPV-AG (s09 G4) 3,328 .75 .75 .77
QWN-PPV-TL (s05 G4) 3,333 .80 .84 .77
QWN-PPV-AG (w02 G3) 3,340 .74 .71 .77
QWN-PPV-TL (s06 G3) 3,340 .77 .79 .77
Table 7: Accuracy QWN-PPV lexicons and SWN
with respect to the GI lexicon.
4.4 Discussion
QWN-PPV lexicons obtain the best results among
the evaluations for English and Spanish. Further-
more, across tasks and datasets qwn-ppv provides
a more consistent and robust behaviour than most
of the manually-built lexicons apart from OF. The
results also show that for a task requiring high
94
recall the larger graphs, e.g. G3, are preferable,
whereas for a more balanced dataset and document
level task smaller G1 graphs perform better.
These are good results considering that our
method to generate qwn-ppv is simpler, more ro-
bust and adaptable than previous automatic ap-
proaches. Furthermore, although also based on
a Personalized PageRank application, it is much
simpler than SentiWordNet 3.0, consistently out-
performed by qwn-ppv on every evaluation and
dataset. The main differences with respect to Sen-
tiWordNet?s approach are the following: (i) the
seed generation and training of 7 supervised clas-
sifiers corresponds in qwn-ppv to only one simple
step, namely, the automatic generation of seeds
as explained in section 3.1; (ii) the generation
of qwn-ppv only requires a LKB?s graph for the
Personalized PageRank propagation, no disam-
biguated glosses; (iii) the graph they use to do
the propagation also depends on disambiguated
glosses, not readily available for any language.
The fact that qwn-ppv is based on already
available WordNets projected onto simple graphs
is crucial for the robustness and adaptability of
the qwn-ppv method across evaluation tasks and
datasets: Our method can quickly create, over dif-
ferent graphs, many lexicons of diffent sizes which
can then be evaluated on a particular polarity clas-
sification task and dataset. Hence the different
configurations of the qwn-ppv lexicons, because
for some tasks a G3 graph with more AG/TL seed
iterations will obtain better recall and viceversa.
This is confirmed by the results: the tasks using
MPQA seem to clearly benefit from high recall
whereas the Bespalov?s corpus has overall, more
balanced scores. This could also be due to the size
of Bespalov?s corpus, almost 10 times larger than
MPQA 1.2.
The experiments to generate Spanish lexicons
confirm the trend showed by the English evalua-
tions: Lexicons generated by qwn-ppv consistenly
outperform other automatic approaches, although
some manual lexicon is better on a given task and
dataset (usually a different one). Nonetheless the
Spanish evaluation shows that our method is also
robust across languages as it gets quite close to
the manually corrected lexicon of Mihalcea(full)
(P?erez-Rosas et al., 2012).
The results also confirm that no single lexicon is
the most appropriate for any SA task or dataset and
domain. In this sense, the adaptability of qwn-ppv
is a desirable feature for lexicons to be employed
in SA tasks: the unsupervised qwn-ppv method
only relies on the availability of a LKB to build
hundreds of polarity lexicons which can then be
evaluated on a given task and dataset to choose the
best fit. If not annotated evaluation set is avail-
able, G3-based propagations provide the best re-
call whereas the G1-based lexicons are less noisy.
Finally, we believe that the results reported here
point out to the fact that intrinsic evaluations are
not meaningful to judge the adequacy a polarity
lexicon for a specific SA task.
5 Concluding Remarks
This paper presents an unsupervised dictionary-
based method qwn-ppv to automatically generate
polarity lexicons. Although simpler than similar
automatic approaches, it still obtains better results
on the four extrinsic evaluations presented. Be-
cause it only depends on the availability of a LKB,
we believe that this method can be valuable to gen-
erate on-demand polarity lexicons for a given lan-
guage when not sufficient annotated data is avail-
able. We demonstrate the adaptability of our ap-
proach by producing good performance polarity
lexicons for different evaluation scenarios and for
more than one language.
Further work includes investigating different
graph projections of WordNet relations to do the
propagation as well as exploiting synset weights.
We also plan to investigate the use of annotated
corpora to generate lexicons at word level to try
and close the gap with those that have been (at
least partially) manually annotated.
The qwn-ppv lexicons and graphs used in this
paper are publicly available (under CC-BY li-
cense): http://adimen.si.ehu.es/web/qwn-ppv. The
qwn-ppv tool to automatically generate polarity
lexicons given a WordNet in any language will
soon be available in the aforementioned URL.
Acknowledgements
This work has been supported by the OpeNER FP7
project under Grant No. 296451, the FP7 News-
Reader project, Grant No. 316404 and by the
Spanish MICINN project SKATER under Grant
No. TIN2012-38584-C06-01.
95
References
R. Agerri and A. Garc??a-Serrano. 2010. Q-WordNet:
extracting polarity from WordNet senses. In Seventh
Conference on International Language Resources
and Evaluation, Malta. Retrieved May, volume 25,
page 2010.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
Aitor Gonz?alez Agirre, Egoitz Laparra, German Rigau,
and Basque Country Donostia. 2012. Multilin-
gual central repository version 3.0: upgrading a very
large lexical knowledge base. In GWC 2012 6th In-
ternational Global Wordnet Conference, page 118.
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
(Early Access).
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Seventh
conference on International Language Resources
and Evaluation (LREC-2010), Malta., volume 25.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings of
the 20th ACM international conference on Informa-
tion and knowledge management, pages 375?382.
Francis Bond and Ryan Foster. 2013. Linking and ex-
tending an open multilingual wordnet. In 51st An-
nual Meeting of the Association for Computational
Linguistics: ACL-2013.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer networks and ISDN systems,
30(1):107117.
Y. Choi and C. Cardie. 2009. Adapting a polarity lexi-
con using integer linear programming for domain-
specific sentiment classification. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 590?598.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings
of the international conference on Web search and
web data mining, pages 231?240.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 424?431, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
C. Fellbaum and G. Miller, editors. 1998. Wordnet:
An Electronic Lexical Database. MIT Press, Cam-
bridge (MA).
V. Hatzivassiloglou and K. R McKeown. 1997. Pre-
dicting the semantic orientation of adjectives. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, pages 174?181.
M. Hu and B. Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004, pages 1367?1373, Geneva, Switzerland, Aug
23?Aug 27. COLING.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual
projections. In Annual Meeting of the Associa-
tion for Computational Linguistics, volume 45, page
976.
S. Mohammad, C. Dunne, and B. Dorr. 2009. Gen-
erating high-coverage semantic orientation lexicons
from overtly marked words and a thesaurus. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 599?608.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1?135.
Ver?onica P?erez-Rosas, Carmen Banea, and Rada Mi-
halcea. 2012. Learning sentiment lexicons in span-
ish. In LREC, pages 3077?3081.
D. Rao and D. Ravichandran. 2009. Semi-supervised
polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 675?
682.
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the International Conference on Empirical Methods
in Natural Language Processing (EMNLP?03).
Xabier Saralegi and I?naki San Vicente. 2013. Elhuyar
at TASS2013. In XXIX Congreso de la Sociedad Es-
paola de Procesamiento de lenguaje natural, Work-
shop on Sentiment Analysis at SEPLN (TASS2013),
pages 143?150, Madrid.
96
P. Stone, D. Dunphy, M. Smith, and D. Ogilvie. 1966.
The General Inquirer: A Computer Approach to
Content Analysis. Cambridge (MA): MIT Press.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet.
In Proceedings of the 4th International Conference
on Languages Resources and Evaluation (LREC
2004), pages 1083?1086, Lisbon, May.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2010. Lexicon-based methods for sen-
timent analysis. Computational Linguistics, (Early
Access):141.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), page 133140, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
P. Turney and M. Littman. 2003. Measuring praise and
criticism: Inference of semantic oreintation from as-
sociation. ACM Transaction on Information Sys-
tems, 21(4):315?346.
P.D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, page 417424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
page 347354.
97
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 5?8,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multilingual, Efficient and Easy NLP Processing with IXA Pipeline
Rodrigo Agerri
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
Donostia San-Sebasti?an
rodrigo.agerri@ehu.es
Josu Bermudez
Deusto Institute of Technology
Deustotech
Univ. of Deusto
Bilbao
josu.bermudez@deusto.es
German Rigau
IXA NLP Group
Univ. of the Basque Country
UPV/EHU
Donostia-San Sebasti?an
german.rigau@ehu.es
Abstract
IXA pipeline is a modular set of Natural Lan-
guage Processing tools (or pipes) which pro-
vide easy access to NLP technology. It aims at
lowering the barriers of using NLP technology
both for research purposes and for small indus-
trial developers and SMEs by offering robust
and efficient linguistic annotation to both re-
searchers and non-NLP experts. IXA pipeline
can be used ?as is? or exploit its modularity
to pick and change different components. This
paper describes the general data-centric archi-
tecture of IXA pipeline and presents competi-
tive results in several NLP annotations for En-
glish and Spanish.
1 Introduction
Many Natural Language Processing (NLP) applica-
tions demand some basic linguistic processing (Tok-
enization, Part of Speech (POS) tagging, Named Entity
Recognition and Classification (NER), Syntactic Pars-
ing, Coreference Resolution, etc.) to be able to further
undertake more complex tasks. Generally, NLP anno-
tation is required to be as accurate and efficient as pos-
sible and existing tools, quite righly, have mostly fo-
cused on performance. However, this generally means
that NLP suites and tools usually require researchers
to do complex compilation/installation/configuration in
order to use such tools. At the same time, in the indus-
try, there are currently many Small and Medium Enter-
prises (SMEs) offering services that one way or another
depend on NLP annotations.
In both cases, in research and industry, acquiring, de-
ploying or developing such base qualifying technolo-
gies is an expensive undertaking that redirects their
original central focus: In research, much time is spent
in the preliminaries of a particular research experiment
trying to obtain the required basic linguistic annota-
tion, whereas in an industrial environment SMEs see
their already limited resources taken away from of-
fering products and services that the market demands.
IXA pipeline provides ready to use modules to per-
form efficient and accurate linguistic annotation to al-
low users to focus on their original, central task. When
designing the architecture, we took several decisions
with respect to what IXA pipeline had to be:
Simple and ready to use: Every module of the IXA
pipeline can be up an running after two simple steps.
Portable: The modules come with ?all batteries in-
cluded? which means that no classpath configurations
or installing of any third-party dependencies is re-
quired. The modules will will run on any platform as
long as a JVM 1.7+ and/or Python 2.7 are available.
Modular: Unlike other NLP toolkits, which often are
built in a monolithic architecture, IXA pipeline is built
in a data centric architecture so that modules can be
picked and changed (even from other NLP toolkits).
The modules behave like Unix pipes, they all take stan-
dard input, do some annotation, and produce standard
output which in turn is the input for the next module.
The data-centric architecture of IXA pipeline means
that any module is highly independent and can there-
fore be used with other tools from other toolkits if re-
quired.
Efficient: Piping the tokenizer (250K words per sec-
ond) POS tagger and lemmatizer all in one process
annotates over 5K words/second. The NERC mod-
ule annotates over 5K words/second. In a multi-core
machine, these times are dramatically reduced due to
multi-threading.
Multilingual: Currently we offer NLP annotations for
both English and Spanish, but other languages are be-
ing included in the pipeline. Tokenization already
works for several languages, including Dutch, French,
Italian, German, Spanish and English.
Accurate: For example, POS tagging and NERC for
English and Spanish are comparable with other state
of the art systems, as it is the coreference resolution
module for English.
Apache License 2.0: IXA Pipeline is licensed under
the Apache License 2.0, an open-source license that fa-
cilitates source code use, distribution and integration,
also for commercial purposes.
1
Next section describes the IXA pipeline architecture,
section 3 the modules so far developed. Whenever
available, we also present empirical evaluation. Sec-
tion 4 describes the various ways of using the tools.
Finally, section 5 discusses some concluding remarks.
1
http://www.apache.org/licenses/LICENSE-2.0.html
5
2 Architecture
IXA pipeline is primarily conceived as a set of
ready to use tools that can provide efficient and
accurate linguistic annotation without any installa-
tion/configuration/compilation effort. As in Unix-like
operative systems, IXA pipeline consists of a set of pro-
cesses chained by their standard streams, in a way that
the output of each process feeds directly as input to the
next one. The Unix pipeline metaphor has been ap-
plied for NLP tools by adopting a very simple and well
known data centric architecture, in which every mod-
ule/pipe is interchangeable for another one as long as it
takes and produces the required data format.
The data format in which both the input and output of
the modules needs to be formatted to represent and fil-
ter linguistic annotations is KAF (Bosma et al., 2009).
KAF is a language neutral annotation format represent-
ing both morpho-syntactic and semantic annotation in a
structured format. KAF was originally designed in the
Kyoto European project
2
, but it has since been in con-
tinuous development
3
. Our Java modules all use kaflib
4
library for easy integration.
Every module in the IXA pipeline, except the coref-
erence resolution, is implemented in Java, and re-
quires Java JDK1.7+ to compile. The integration of
the Java modules in the IXA pipeline is performed us-
ing Maven
5
. Maven is used to take care of classpaths
configurations and third-party tool dependencies. This
means that the binaries produced and distributed will
work off-the-self. The coreference module uses pip
6
to provide an easy, one step installation. If the source
code of an ixa-pipe-$module is cloned from the remote
repository, one command to compile and have ready the
tools will suffice.
Some modules in IXA pipeline provide linguistic an-
notation based on probabilistic supervised approaches
such as POS tagging, NER and Syntactic Parsing. IXA
pipeline uses two well known machine learning algo-
rithms, namely, Maximum Entropy and the Percep-
tron. Both Perceptron (Collins, 2002; Collins, 2003)
and Maximum Entropy models (Ratnaparkhi, 1999) are
adaptable algorithms which have been successfully ap-
plied to NLP tasks such as POS tagging, NER and
Parsing with state of the art results. To avoid dupli-
cation of efforts, IXA pipeline uses the already avail-
able open-source Apache OpenNLP API
7
to train POS,
NER and parsing probabilistic models using these two
approaches.
2
http://kyoto-project.eu
3
http://www.opener-project.org/kaf/
4
https://github.com/ixa-ehu/kaflib
5
http://maven.apache.org/
6
https://pypi.python.org/pypi/pip
7
http://opennlp.apache.org
3 Pipes
IXA pipeline currently provides the following linguis-
tic annotations: Sentence segmentation, tokenization,
Part of Speech (POS) tagging, Lemmatization, Named
Entity Recognition and Classification (NER), Con-
stituent Parsing and Coreference Resolution. Every
module works for English and Spanish and is imple-
mented in Java/Maven as described above. The only
exception is the coreference resolution module, which
currently is available in Python 2.7 and for English only
(Spanish version will comme soon). We will now de-
scribe which annotation services are provided by each
module of the pipeline.
3.1 ixa-pipe-tok
This module provides rule-based Sentence Segmenta-
tion and Tokenization for French, Dutch, English, Ital-
ian and Spanish. It produces tokenized and segmented
text in KAF, running text and CoNLL formats. The
rules are originally based on the Stanford English To-
kenizer
8
, but with substantial modifications and addi-
tions. These include tokenization for other languages
such as French and Italian, normalization according
the Spanish Ancora Corpus (Taul?e et al., 2008), para-
graph treatment, and more comprehensive gazeteers
of non breaking prefixes. The tokenizer depends on
a JFlex
9
specification file which compiles in seconds
and performs at a very reasonable speed (around 250K
word/second, and much quicker with Java multithread-
ing).
3.2 ixa-pipe-pos
ixa-pipe-pos provides POS tagging and lemmatization
for English and Spanish. We have obtained the best
results so far with the same featureset as in Collins?s
(2002) paper. Perceptron models for English have been
trained and evaluated on the WSJ treebank using the
usual partitions (e.g., as explained in Toutanova et al.
(2003). We currently obtain a performance of 97.07%
vs 97.24% obtained by Toutanova et al., (2003)). For
Spanish, Maximum Entropy models have been trained
and evaluated using the Ancora corpus; it was ran-
domly divided in 90% for training and 10% for test-
ing. This corresponds to 440K words used for train-
ing and 70K words for testing. We obtain a perfor-
mance of 98.88% (the corpus partitions are available
for reproducibility). Gim?enez and Marquez (2004) re-
port 98.86%, although they train and test on a different
subset of the Ancora corpus.
Lemmatization is currently performed via 3 different
dictionary lookup methods: (i) Simple Lemmatizer: It
is based on HashMap lookups on a plain text dictionary.
Currently we use dictionaries from the LanguageTool
project
10
under their distribution licenses. The English
8
http://www-nlp.stanford.edu/software/tokenizer.shtml
9
http://jflex.de/
10
http://languagetool.org/
6
dictionary contains 300K lemmas whereas the Spanish
provides over 600K; (ii) Morfologik-stemming
11
: The
Morfologik library provides routines to produce binary
dictionaries, from dictionaries such as the one used by
the Simple Lemmatizer above, as finite state automata.
This method is convenient whenever lookups on very
large dictionaries are required because it reduces the
memory footprint to 10% of the memory required for
the equivalent plain text dictionary; and (iii) We also
provide lemmatization by lookup in WordNet-3.0 (Fell-
baum and Miller, 1998) via the JWNL API
12
. Note that
this method is only available for English.
3.3 ixa-pipe-nerc
Most of the NER systems nowdays consist of language
independent systems (sometimes enriched with gaze-
teers) based on automatic learning of statistical mod-
els. ixa-pipe-nerc provides Named Entity Recogni-
tion (NER) for English and Spanish. The named en-
tity types are based on the CONLL 2002
13
and 2003
14
tasks which were focused on language-independent su-
pervised named entity recognition (NER) for four types
of named entities: persons, locations, organizations and
names of miscellaneous entities that do not belong to
the previous three groups. We currently provide two
very fast language independent models using a rather
simple baseline featureset (e.g., similar to that of Cur-
ran and Clark (2003), except POS tag features).
For English, perceptron models have been trained
using CoNLL 2003 dataset. We currenly obtain 84.80
F1 which is coherent with other results reported with
these features (Clark and Curran, 2003; Ratinov and
Roth, 2009). The best Stanford NER model reported
on this dataset achieves 86.86 F1 (Finkel et al., 2005),
whereas the best system on this dataset achieves 90.80
F1 (Ratinov and Roth, 2009), using non local features
and substantial external knowledge.
For Spanish we currently obtain best results train-
ing Maximum Entropy models on the CoNLL 2002
dataset. Our best model obtains 79.92 F1 vs 81.39
F1 (Carreras et al., 2002), the best result so far on this
dataset. Their result uses external knowledge and with-
out it, their system obtains 79.28 F1.
3.4 ixa-pipe-parse
ixa-pipe-parse provides statistical constituent parsing
for English and Spanish. Maximum Entropy models
are trained to build shift reduce bottom up parsers (Rat-
naparkhi, 1999) as provided by the Apache OpenNLP
API. Parsing models for English have been trained us-
ing the Penn treebank and for Spanish using the Ancora
corpus (Taul?e et al., 2008).
Furthermore, ixa-pipe-parse provides two methods
of HeadWord finders: one based on Collins? head rules
11
https://github.com/morfologik/morfologik-stemming
12
http://jwordnet.sourceforge.net/
13
http://www.clips.ua.ac.be/conll2002/ner/
14
http://www.clips.ua.ac.be/conll2003/ner/
as defined in his PhD thesis (1999), and another one
based on Stanford?s parser Semantic Head Rules
15
.
The latter are a modification of Collins? head rules ac-
cording to lexical and semantic criteria. These head
rules are particularly useful for the Coreference reso-
lution module and for projecting the constituents into
dependency graphs.
As far as we know, and although previous ap-
proaches exist (Cowan and Collins, 2005), ixa-pipe-
parse provides the first publicly available statistical
parser for Spanish.
3.5 Coreference Resolution
The module of coreference resolution included in the
IXA pipeline is loosely based on the Stanford Multi
Sieve Pass system (Lee et al., 2013). The module takes
every linguistic information it requires from the KAF
layers annotated by all the previously described mod-
ules. The system consists of a number of rule-based
sieves. Each sieve pass is applied in a deterministic
manner, reusing the information generated by the pre-
vious sieve and the mention processing. The order in
which the sieves are applied favours a highest precision
approach and aims at improving the recall with the sub-
sequent application of each of the sieve passes. This
is illustrated by the evaluation results of the CoNLL
2011 Coreference Evaluation task (Lee et al., 2013), in
which the Stanford?s system obtained the best results.
So far we have evaluated our module on the CoNLL
2011 testset and we are a 5% behind the Stanford?s sys-
tem (52.8 vs 57.6 CoNLL F1), the best on that task (Lee
et al., 2013). It is interesting that in our current imple-
mentation, mention-based metrics are favoured (CEAF
and B
3
). Still, note that these results are comparable
with the results obtained by the best CoNLL 2011 par-
ticipants. Currently the module performs coreference
resolution only for English, although a Spanish version
will be coming soon.
4 Related Work
Other NLP toolkits exist providing similar or more ex-
tensive functionalities than the IXA pipeline tools, al-
though not many of them provide multilingual support.
GATE (Cunningham, 2002) is an extensive framework
supporting annotation of text. GATE has some capacity
for wrapping Apache UIMA components
16
, so should
be able to manage distributed NLP components. How-
ever, GATE is a very large and complex system, with a
corresponding steep learning curve.
Freeling (Padr?o and Stanilovsky, 2012) provides
multilingual processing for a number of languages,
incluing Spanish and English. As opposed to IXA
pipeline, Freeling is a monolithic toolkit written in C++
which needs to be compiled natively. The Stanford
15
http://www-nlp.stanford.edu/software/lex-parser.shtml
16
http://uima.apache.org/
7
CoreNLP
17
is a monolithic suite, which makes it dif-
ficult to integrate other tools in its chain.
IXA pipeline tools can easily be used piping the in-
put with the output of another too, and it is also pos-
sible to easily replace or extend the toolchain with a
third-party tool. IXA pipeline is already being used to
do extensive parallel processing in the FP7 European
projects OpeNER
18
and NewsReader
19
.
5 Conclusion and Future Work
IXA pipeline provides a simple, efficient, accurate and
ready to use set of NLP tools. Its modularity and data
centric architecture makes it flexible to pick and change
or integrate new linguistic annotators. Currently we of-
fer linguistic annotation for English and Spanish, but
more languages are being integrated. Furthermore,
other annotations such as Semantic Role Labelling and
Named Entity Disambiguation are being included in
the pipeline following the same principles.
Additionally, current integrated modules are be-
ing improved: both on the quality and variety of
the probabilistic models, and on specific issues such
as lemmatization, and treatment of time expressions.
Finally, we are adding server-mode execution into
the pipeline to provide faster processing. IXA
pipeline is publicly available under Apache 2.0 license:
http://adimen.si.ehu.es/web/ixa-pipes.
Acknowledgements
TThis work has been supported by the OpeNER FP7
project under Grant No. 296451, the FP7 NewsReader
project, Grant No. 316404, and by the SKATER Span-
ish MICINN project No TIN2012-38584-C06-01. The
work of Josu Bermudez on coreference resolution is
supported by a PhD Grant of the University of Deusto
(http://www.deusto.es).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. Kaf: a
generic semantic annotation format. In Proceedings
of the GL2009 Workshop on Semantic Annotation.
X. Carreras, L. Marquez, and L. Padro. 2002. Named
entity extraction using AdaBoost. In proceedings
of the 6th conference on Natural language learning-
Volume 20, pages 1?4.
Stephen Clark and James Curran. 2003. Language In-
dependent NER using a Maximum Entropy Tagger.
In Proceedings of the Seventh Conference on Nat-
ural Language Learning (CoNLL-03), pages 164?
167, Edmonton, Canada.
17
http://nlp.stanford.edu/software/corenlp.shtml
18
http://www.opener-project.org
19
http://www.newsreader-project.eu
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Brooke Cowan and Michael Collins. 2005. Mor-
phology and reranking for the statistical parsing of
spanish. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 795?802.
Association for Computational Linguistics.
Hamish Cunningham. 2002. Gate, a general architec-
ture for text engineering. Computers and the Hu-
manities, 36(2):223?254.
C. Fellbaum and G. Miller, editors. 1998. Wordnet: An
Electronic Lexical Database. MIT Press, Cambridge
(MA).
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, pages 363?370.
Jes?us Gim?enez and Lluis Marquez. 2004. Svmtool: A
general pos tagger generator based on support vector
machines. In In Proceedings of the 4th International
Conference on Language Resources and Evaluation.
Citeseer.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, pages 1?54, January.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, page 147155.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
learning, 34(1-3):151?175.
Mariona Taul?e, Maria Ant`onia Mart??, and Marta Re-
casens. 2008. Ancora: Multilevel annotated corpora
for catalan and spanish. In LREC.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In Proceedings of HLT-NAACL, pages 252?259.
8
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180?1189,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling
Egoitz Laparra
IXA Group
University of the Basque Country
San Sebastian, Spain
egoitz.laparra@ehu.es
German Rigau
IXA Group
University of the Basque Country
San Sebastian, Spain
german.rigau@ehu.es
Abstract
This paper presents a novel deterministic
algorithm for implicit Semantic Role La-
beling. The system exploits a very sim-
ple but relevant discursive property, the ar-
gument coherence over different instances
of a predicate. The algorithm solves the
implicit arguments sequentially, exploit-
ing not only explicit but also the implicit
arguments previously solved. In addition,
we empirically demonstrate that the algo-
rithm obtains very competitive and robust
performances with respect to supervised
approaches that require large amounts of
costly training data.
1 Introduction
Traditionally, Semantic Role Labeling (SRL) sys-
tems have focused in searching the fillers of those
explicit roles appearing within sentence bound-
aries (Gildea and Jurafsky, 2000, 2002; Carreras
and Ma`rquez, 2005; Surdeanu et al, 2008; Hajic?
et al, 2009). These systems limited their search-
space to the elements that share a syntactical re-
lation with the predicate. However, when the par-
ticipants of a predicate are implicit this approach
obtains incomplete predicative structures with null
arguments. The following example includes the
gold-standard annotations for a traditional SRL
process:
(1) [arg0 The network] had been expected to have [np
losses] [arg1 of as much as $20 million] [arg3 on base-
ball this year]. It isn?t clear how much those [np losses]
may widen because of the short Series.
The previous analysis includes annotations for
the nominal predicate loss based on the NomBank
structure (Meyers et al, 2004). In this case the
annotator identifies, in the first sentence, the argu-
ments arg0, the entity losing something, arg1, the
thing lost, and arg3, the source of that loss. How-
ever, in the second sentence there is another in-
stance of the same predicate, loss, but in this case
no argument has been associated with it. Tradi-
tional SRL systems facing this type of examples
are not able to fill the arguments of a predicate
because their fillers are not in the same sentence
of the predicate. Moreover, these systems also let
unfilled arguments occurring in the same sentence,
like in the following example:
(2) Quest Medical Inc said it adopted [arg1 a sharehold-
ers? rights] [np plan] in which rights to purchase shares
of common stock will be distributed as a dividend to
shareholders of record as of Oct 23.
For the predicate plan in the previous sentence,
a traditional SRL process only returns the filler for
the argument arg1, the theme of the plan.
However, in both examples, a reader could eas-
ily infer the missing arguments from the surround-
ing context of the predicate, and determine that
in (1) both instances of the predicate share the
same arguments and in (2) the missing argument
corresponds to the subject of the verb that domi-
nates the predicate, Quest Medical Inc. Obviously,
this additional annotations could contribute posi-
tively to its semantic analysis. In fact, Gerber and
Chai (2010) pointed out that implicit arguments
can increase the coverage of argument structures
in NomBank by 71%. However, current automatic
systems require large amounts of manually anno-
tated training data for each predicate. The effort
required for this manual annotation explains the
absence of generally applicable tools. This prob-
lem has become a main concern for many NLP
tasks. This fact explains a new trend to develop
accurate unsupervised systems that exploit sim-
ple but robust linguistic principles (Raghunathan
et al, 2010).
In this work, we study the coherence of the
predicate and argument realization in discourse. In
particular, we have followed a similar approach to
1180
the one proposed by Dahl et al (1987) who filled
the arguments of anaphoric mentions of nominal
predicates using previous mentions of the same
predicate. We present an extension of this idea
assuming that in a coherent document the differ-
ent ocurrences of a predicate, including both ver-
bal and nominal forms, tend to be mentions of
the same event, and thus, they share the same
argument fillers. Following this approach, we
have developed a deterministic algorithm that ob-
tains competitive results with respect to supervised
methods. That is, our system can be applied to any
predicate without training data.
The main contributions of this work are the fol-
lowing:
? We empirically prove that there exists a
strong discourse relationship between the im-
plicit and explicit argument fillers of the same
predicates.
? We propose a deterministic approach that ex-
ploits this discoursive property in order to ob-
tain the fillers of implicit arguments.
? We adapt to the implicit SRL problem a clas-
sic algorithm for pronoun resolution.
? We develop a robust algorithm, ImpAr, that
obtains very competitive results with respect
to existing supervised systems. We release
an open source prototype implementing this
algorithm1.
The paper is structured as follows. Section 2
discusses the related work. Section 3 presents in
detail the data used in our experiments. Section
4 describes our algorithm for implicit argument
resolution. Section 5 presents some experiments
we have carried out to test the algorithm. Section
6 discusses the results obtained. Finally, section
7 offers some concluding remarks and presents
some future research lines.
2 Related Work
The first attempt for the automatic annotation of
implicit semantic roles was proposed by Palmer
et al (1986). This work applied selectional restric-
tions together with coreference chains, in a very
specific domain. In a similar approach, Whitte-
more et al (1991) also attempted to solve implicit
1http://adimen.si.ehu.es/web/ImpAr
arguments using some manually described seman-
tic constraints for each thematic role they tried to
cover. Another early approach was presented by
Tetreault (2002). Studying another specific do-
main, they obtained some probabilistic relations
between some roles. These early works agree that
the problem is, in fact, a special case of anaphora
or coreference resolution.
Recently, the task has been taken up again
around two different proposals. On the one
hand, Ruppenhofer et al (2010) presented a task
in SemEval-2010 that included an implicit argu-
ment identification challenge based on FrameNet
(Baker et al, 1998). The corpus for this task
consisted in some novel chapters. They covered
a wide variety of nominal and verbal predicates,
each one having only a small number of instances.
Only two systems were presented for this sub-
task obtaining quite poor results (F1 below 0,02).
VENSES++ (Tonelli and Delmonte, 2010) applied
a rule based anaphora resolution procedure and se-
mantic similarity between candidates and thematic
roles using WordNet (Fellbaum, 1998). The sys-
tem was tuned in (Tonelli and Delmonte, 2011)
improving slightly its performance. SEMAFOR
(Chen et al, 2010) is a supervised system that
extended an existing semantic role labeler to en-
large the search window to other sentences, replac-
ing the features defined for regular arguments with
two new semantic features. Although this system
obtained the best performance in the task, data
sparseness strongly affected the results. Besides
the two systems presented to the task, some other
systems have used the same dataset and evaluation
metrics. Ruppenhofer et al (2011), Laparra and
Rigau (2012), Gorinski et al (2013) and Laparra
and Rigau (2013) explore alternative linguistic and
semantic strategies. These works obtained signifi-
cant gains over previous approaches. Silberer and
Frank (2012) adapted an entity-based coreference
resolution model to extend automatically the train-
ing corpus. Exploiting this additional data, their
system was able to improve previous results. Fol-
lowing this approach Moor et al (2013) present a
corpus of predicate-specific annotations for verbs
in the FrameNet paradigm that are aligned with
PropBank and VerbNet.
On the other hand, Gerber and Chai (2010,
2012) studied the implicit argument resolution on
NomBank. They uses a set of syntactic, semantic
and coreferential features to train a logistic regres-
1181
sion classifier. Unlike the dataset from SemEval-
2010 (Ruppenhofer et al, 2010), in this work the
authors focused on a small set of ten predicates.
But for those predicates, they annotated a large
amount of instances in the documents from the
Wall Street Journal that were already annotated
for PropBank (Palmer et al, 2005) and NomBank.
This allowed them to avoid the sparseness prob-
lems and generalize properly from the training
set. The results of this system were far better
than those obtained by the systems that faced the
SemEval-2010 dataset. This works represent the
deepest study so far of the features that charac-
terizes the implicit arguments 2. However, many
of the most important features are lexically depen-
dent on the predicate and cannot been generalized.
Thus, specific annotations are required for each
new predicate to be analyzed.
All the works presented in this section agree that
implicit arguments must be modeled as a particu-
lar case of coreference together with features that
include lexical-semantic information, to build se-
lectional preferences. Another common point is
the fact that these works try to solve each instance
of the implicit arguments independently, without
taking into account the previous realizations of
the same implicit argument in the document. We
propose that these realizations, together with the
explicit ones, must maintain a certain coherence
along the document and, in consequence, the filler
of an argument remains the same along the fol-
lowing instances of that argument until a stronger
evidence indicates a change. We also propose that
this feature can be exploited independently from
the predicate.
3 Datasets
In our experiments, we have focused on the dataset
developed in Gerber and Chai (2010, 2012). This
dataset (hereinafter BNB which stands for ?Be-
yond NomBank?) extends existing predicate an-
notations for NomBank and ProbBank.
BNB presented the first annotation work of im-
plicit arguments based on PropBank and Nom-
Bank frames. This annotation was an extension
of the standard training, development and testing
sections of Penn TreeBank that have been typi-
cally used for SRL evaluation and were already
annotated with PropBank and NomBank predicate
2Gerber and Chai (2012) includes a set of 81 different fea-
tures.
structures. The authors selected a limited set of
predicates. These predicates are all nominaliza-
tions of other verbal predicates, without sense am-
biguity, that appear frequently in the corpus and
tend to have implicit arguments associated with
their instances. These constraints allowed them to
model enough occurrences of each implicit argu-
ment in order to cover adequately all the possible
cases appearing in a test document. For each miss-
ing argument position they went over all the pre-
ceding sentences and annotated all mentions of the
filler of that argument. In tables 3 and 4 we show
the list of predicates and the resulting figures of
this annotation.
In this work we also use the corpus provided
for the CoNLL-2008 task. These corpora cover
the same BNB documents and include annotated
predictions for syntactic dependencies and Super-
Sense labels as semantic tags. Unlike Gerber and
Chai (2010, 2012) we do not use the constituent
analysis from the Penn TreeBank.
4 ImpAr algorithm
4.1 Discoursive coherence of predicates
Exploring the training dataset of BNB, we ob-
served a very strong discourse effect on the im-
plicit and explicit argument fillers of the predi-
cates. That is, if several instances of the same
predicate appear in a well-written discourse, it is
very likely that they maintain the same argument
fillers. This property holds when joining the dif-
ferent parts-of-speech of the predicates (nominal
or verbal) and the explicit or implicit realizations
of the argument fillers. For instance, we observed
that 46% of all implicit arguments share the same
filler with the previous instance of the same predi-
cate while only 14% of them have a different filler.
The remaining 40% of all implicit arguments cor-
respond to first occurrences of their predicates.
That is, these fillers can not be recovered from pre-
vious instances of their predicates.
The rationale behind this phenomena seems to
be simple. When referring to different aspects of
the same event, the writer of a coherent document
does not repeat redundant information. They re-
fer to previous predicate instances assuming that
the reader already recalls the involved participants.
That is, the filler of the different instances of a
predicate argument maintain a certain discourse
coherence. For instance, in example (1), all the ar-
gument positions of the second occurrence of the
1182
predicate loss are missing, but they can be easily
inferred from the previous instance of the same
predicate.
(1) [arg0 The network] had been expected to have [np
losses] [arg1 of as much as $20 million] [arg3 on base-
ball this year]. It isn?t clear how much those [np losses]
may widen because of the short Series.
Therefore, we propose to exploit this property
in order to capture correctly how the fillers of all
predicate arguments evolve through a document.
Our algorithm, ImpAr, processes the docu-
ments sentence by sentence, assuming that se-
quences of the same predicate (in its nominal or
verbal form) share the same argument fillers (ex-
plicit or implicit)3. Thus, for every core argument
argn of a predicate, ImpAr stores its previous
known filler as a default value. If the arguments
of a predicate are explicit, they always replace de-
fault fillers previously captured. When there is no
antecedent for a particular implicit argument argn,
the algorithm tries to find in the surrounding con-
text which participant is the most likely to be the
filler according to some salience factors (see Sec-
tion 4.2). For the following instances, without an
explicit filler for a particular argument position,
the algorithm repeats the same selection process
and compares the new implicit candidate with the
default one. That is, the default implicit argument
of a predicate with no antecedent can change ev-
ery time the algorithm finds a filler with a greater
salience. A damping factor is applied to reduce the
salience of distant predicates.
4.2 Filling arguments without explicit
antecedents
Filling the implicit arguments of a predicate has
been identified as a particular case of corefer-
ence, very close to pronoun resolution (Silberer
and Frank, 2012). Consequently, for those implicit
arguments that have not explicit antecedents, we
propose an adaptation of a classic algorithm for
deterministic pronoun resolution. This component
of our algorithm follows the RAP approach (Lap-
pin and Leass, 1994). When our algorithm needs
to fill an implicit predicate argument without an
explicit antecedent it considers a set of candidates
within a window formed by the sentence of the
predicate and the two previous sentences. Then,
the algorithm performs the following steps:
3Note that the algorithm could also consider sequences of
closely related predicates.
1. Apply two constraints to the candidate list:
(a) All candidates that are already explicit arguments
of the predicate are ruled out.
(b) All candidates commanded by the predicate in
the dependency tree are ruled out.
2. Select those candidates that are semantically consistent
with the semantic category of the implicit argument.
3. Assign a salience score to each candidate.
4. Sort the candidates by their proximity to the predicate
of the implicit argument.
5. Select the candidate with the highest salience value.
As a result, the candidate with the highest
salience value is selected as the filler of the im-
plicit argument. Thus, this filler with its corre-
sponding salience weight will be also considered
in subsequent instances of the same predicate.
Now, we explain each step in more detail using
example (2). In this example, arg0 is missing for
the predicate plan:
(2) Quest Medical Inc said it adopted [arg1 a sharehold-
ers? rights] [np plan] in which rights to purchase shares
of common stock will be distributed as a dividend to
shareholders of record as of Oct 23.
Filtering. In the first step, the algorithm fil-
ters out the candidates that are actual explicit argu-
ments of the predicate or have a syntactic depen-
dency with the predicate, and therefore, they are in
the search space of a traditional SRL system.
In our example, the filtering process would re-
move [a shareholders? rights] because it is already
the explicit argument arg1, and [in which rights
to purchase shares of common stock will be dis-
tributed as a dividend to shareholders of record as
of Oct 23] because it is syntactically commanded
by the predicate plan.
Semantic consistency. To determine the se-
mantic coherence between the potential candidates
and a predicate argument argn, we have exploited
the selectional preferences in the same way as
in previous SRL and implicit argument resolution
works. First, we have designed a list of very
general semantic categories. Second, we have
semi-automatically assigned one of them to every
predicate argument argn in PropBank and Nom-
Bank. For this, we have used the semantic an-
notation provided by the training documents of
the CoNLL-2008 dataset. This annotation was
performed automatically using the SuperSense-
Tagger (Ciaramita and Altun, 2006) and includes
1183
named-entities and WordNet Super-Senses4. We
have also defined a mapping between the semantic
classes provided by the SuperSenseTagger and our
seven semantic categories (see Table 1 for more
details). Then, we have acquired the most com-
mon categories of each predicate argument argn.
ImpAr algorithm also uses the SuperSenseTagger
over the documents to be processed from BNB
to check if the candidate belongs to the expected
semantic category of the implicit argument to be
filled.
Following the example above, [Quest Medi-
cal Inc] is tagged as an ORGANIZATION by the
SuperSenseTagger. Therefore, it belongs to our
semantic category COGNITIVE. As the seman-
tic category for the implicit argument arg0 for
the predicate plan has been recognized to be also
COGNITIVE, [Quest Medical Inc] remains in the
list of candidates as a possible filler.
Semantic category Name-entities Super-Senses
COGNITIVE
PERSON noun.personORGANIZATION noun.groupANIMAL noun.animal... ...
TANGIBLE PRODUCT noun.artifactSUBSTANCE noun.object... ...
EVENTIVE GAME noun.actDISEASE noun.communication... ...
RELATIVE noun.shapenoun.attribute...LOCATIVE LOCATION noun.locationTIME DATE noun.time
MESURABLE QUANTITY noun.quantityPERCENT...
Table 1: Links between the semantic categories and some
name-entities and super-senses.
Salience weighting. In this process, the algo-
rithm assigns to each candidate a set of salience
factors that scores its prominence. The sentence
recency factor prioritizes the candidates that oc-
cur close to the same sentence of the predicate.
The subject, direct object, indirect object and non-
adverbial factors weight the salience of the candi-
date depending on the syntactic role they belong
to. Additionally, the head of these syntactic roles
are prioritized by the head factor. We have used
the same weights, listed in table 2, proposed by
Lappin and Leass (1994).
In the example, candidate [Quest Medical Inc]
is in the same sentence as the predicate plan, it
4Lexicographic files according to WordNet terminology.
Factor type weight
Sentence recency 100
Subject 80
Direct object 50
Indirect object 40
Head 80
Non-adverbial 50
Table 2: Weights assigned to each salience factor.
belongs to a subject, and, indeed, it is the head
of that subject. Hence, the salience score for this
candidate is: 100 + 80 + 80 = 260.
4.3 Damping the salience of the default
candidate
As the algorithm maintains the default candidate
until an explicit filler appears, potential errors pro-
duced in the automatic selection process explained
above can spread to distant implicit instances, spe-
cially when the salience score of the default can-
didate is high. In order to reduce the impact of
these errors we have included a damping factor
that is applied sentence by sentence to the salience
value of the default candidate. ImpAr applies that
damping factor, r, as follows. It assumes that, in-
dependently of the initial salience assigned, 100
points of the salience score came from the sen-
tence recency factor. Then, the algorithm changes
this value multiplying it by r. So, given a salience
score s, the value of the score in a following sen-
tence, s?, is:
s? = s? 100 + 100 ? r
Obviously, the value of r must be defined with-
out harming excessively those cases where the de-
fault candidate has been correctly identified. For
this, we studied in the training dataset the cases
of implicit arguments filled with the default can-
didate. Figure 1 shows that the influence of the
default filler is much higher in near sentences that
in more distance ones.
We tried to mimic a damping factor following
this distribution. That is, to maintain high score
salience for the near sentences while strongly de-
creasing them in the subsequent ones. In this way,
if the filler of the implicit argument is wrongly
identified, the error only spreads to the nearest in-
stances. If the identification is correct, a lower
score for more distance sentences is not too harm-
ful. The distribution shown in figure 1 follows
an exponential decay, therefore we have described
the damping factor as a curve like the following,
where ? must be a value within 0 and 1:
1184
Figure 1: Distances between the implicit argument and the
default candidate. The y axis indicate the percentage of cases
occurring in each sentence distance, expressed in x
r = ?d
In this function, d stands for the sentence dis-
tance and r for the damping factor to apply in that
sentence. In this paper, we have decided to set the
value of ? to 0.5.
r = 0.5d
This value maintains the influence of the default
fillers with high salience in near sentences. But it
decreases that influence strongly in the following.
In order to illustrate the whole process we will
use the previous example. In that case, [Quest
Medical Inc] is selected as the arg0 of plan with
a salience score of 260. Therefore [Quest Medi-
cal Inc] becomes the default arg0 of plan. In the
following sentence the damping factor is:
0.5 = 0.51
Therefore, its salience score changes to 260 ?
100+100?0.5 = 210. Then, the algorithm changes
the default filler for arg0 only if it finds a candi-
date that scores higher in their current context. At
two sentence distance, the resulting score for the
default filler is 260 ? 100 + 100 ? 0.25 = 185. In
this way, at more distance sentences, the influence
of the default filler of arg0 becomes smaller.
5 Evaluation
In order to evaluate the performance of the Im-
pAr algorithm, we have followed the evaluation
method presented by Gerber and Chai (2010,
2012). For every argument position in the gold-
standard the scorer expects a single predicted con-
stituent to fill in. In order to evaluate the correct
span of a constituent, a prediction is scored using
the Dice coefficient:
2|Predicted ? True|
|Predicted| + |True|
The function above relates the set of tokens that
form a predicted constituent, Predicted, and the
set of tokens that are part of an annotated con-
stituent in the gold-standard, True. For each
missing argument, the gold-standard includes the
whole coreference chain of the filler. Therefore,
the scorer selects from all coreferent mentions the
highest Dice value. If the predicted span does not
cover the head of the annotated filler, the scorer re-
turns zero. Then, Precision is calculated by the
sum of all prediction scores divided by the number
of attempts carried out by the system. Recall is
equal to the sum of the prediction scores divided
by the number of actual annotations in the gold-
standard. F-measure is calculated as the harmonic
mean of recall and precision.
Traditionally, there have been two approaches
to develop SRL systems, one based on constituent
trees and the other one based on syntactic depen-
dencies. Additionally, the evaluation of both types
of systems has been performed differently. For
constituent based SRL systems the scorers eval-
uate the correct span of the filler, while for depen-
dency based systems the scorer just check if the
systems are able to capture the head token of the
filler. As shown above, previous works in implicit
argument resolution proposed a metric that in-
volves the correct identification of the whole span
of the filler. ImpAr algorithm works with syntac-
tic dependencies and therefore it only returns the
head token of the filler. In order to compare our
results with previous works, we had to apply some
simple heuristics to guess the correct span of the
filler. Obviously, this process inserts some noise
in the final evaluation.
We have performed a first evaluation over the
test set used in (Gerber and Chai, 2010). This
dataset contains 437 predicate instances but just
246 argument positions are implicitly filled. Table
3 includes the results obtained by ImpAr, the re-
sults of the system presented by Gerber and Chai
(2010) and the baseline proposed for the task. Best
results are marked in bold5. For all predicates,
ImpAr improves over the baseline (19.3 points
higher in the overall F1). Our system also out-
performs the one presented by Gerber and Chai
(2010). Interestingly, both systems present very
different performances predicate by predicate. For
5No proper significance test can be carried out without the
the full predictions of all systems involved.
1185
Baseline Gerber & Chai ImpAr
#Inst. #Imp. F1 P R F1 P R F1
sale 64 65 36.2 47.2 41.7 44.2 41.2 39.4 40.3
price 121 53 15.4 36.0 32.6 34.2 53.3 53.3 53.3
investor 78 35 9.8 36.8 40.0 38.4 43.0 39.5 41.2
bid 19 26 32.3 23.8 19.2 21.3 52.9 51.0 52.0
plan 25 20 38.5 78.6 55.0 64.7 40.7 40.7 40.7
cost 25 17 34.8 61.1 64.7 62.9 56.1 50.2 53.0
loss 30 12 52.6 83.3 83.3 83.3 68.4 63.5 65.8
loan 11 9 18.2 42.9 33.3 37.5 25.0 20.0 22.2
investment 21 8 0.0 40.0 25.0 30.8 47.6 35.7 40.8
fund 43 6 0.0 14.3 16.7 15.4 66.7 33.3 44.4
Overall 437 246 26.5 44.5 40.4 42.3 47.9 43.8 45.8
Table 3: Evaluation with the test. The results from (Gerber and Chai, 2010) are included.
Baseline Gerber & Chai ImpAr
#Inst. #Imp. F1 P R F1 P R F1
sale 184 181 37.3 59.2 44.8 51.0 44.3 43.3 43.8
price 216 138 34.6 56.0 48.7 52.1 55.0 54.5 54.7
investor 160 108 5.1 46.7 39.8 43.0 28.2 27.0 27.6
bid 88 124 23.8 60.0 36.3 45.2 48.4 41.8 45.0
plan 100 77 32.3 59.6 44.1 50.7 47.0 47.0 47.0
cost 101 86 17.8 62.5 50.9 56.1 49.2 43.7 46.2
loss 104 62 54.7 72.5 59.7 65.5 63.0 58.2 60.5
loan 84 82 31.2 67.2 50.0 57.3 56.4 45.6 50.6
investment 102 52 15.5 32.9 34.2 33.6 41.2 30.9 35.4
fund 108 56 15.5 80.0 35.7 49.4 55.6 44.6 49.5
Overall 1,247 966 28.9 57.9 44.5 50.3 47.7 43.0 45.3
Table 4: Evaluation with the full dataset. The results from (Gerber and Chai, 2012) are included.
instance, our system obtains much higher results
for the predicates bid and fund, while much lower
for loss and loan. In general, ImpAr seems to be
more robust since it obtains similar performances
for all predicates. In fact, the standard deviation,
? , of F1 measure is 10.98 for ImpAr while this
value for the (Gerber and Chai, 2010) system is
20.00.
In a more recent work, Gerber and Chai (2012)
presented some improvements of their previous
results. In this work, they extended the evalua-
tion of their model using the whole dataset and
not just the testing documents. Applying a cross-
validated approach they tried to solve some prob-
lems that they found in the previous evaluation,
like the small size of the testing set. For this work,
they also studied a wider set of features, specially,
they experimented with some statistics learnt from
parts of GigaWord automatically annotated. Table
4 shows that the improvement over their previous
system was remarkable. The system also seems
to be more stable across predicates. For compar-
ison purposes, we also included the performance
of ImpAr applied over the whole dataset.
The results in table 4 show that, although ImpAr
still achieves the best results in some cases, this
time, it cannot beat the overall results obtained by
the supervised model. In fact, both systems obtain
a very similar recall, but the system from (Gerber
and Chai, 2012) obtains much higher precision.
In both cases, the ? value of F1 is reduced, 8.81
for ImpAr and 8.21 for (Gerber and Chai, 2012).
However, ImpAr obtains very similar performance
independently of the testing dataset what proves
the robustness of the algorithm. This suggests
that our algorithm can obtain strong results also
for other corpus and predicates. Instead, the su-
pervised approach would need a large amount of
manual annotations for every predicate to be pro-
cessed.
6 Discussion
6.1 Component Analysis
In order to assess the contribution of each sys-
tem component, we also tested the performance
of ImpAr algorithm when disabling only one of
its components. With this evaluations we pretend
to sight the particular contribution of each compo-
nent. In table 5 we present the results obtained in
the following experiments for the two testing sets
explained in section 5:
? Exp1: The damping factor is disabled. All se-
lected fillers maintain the same salience over
1186
all sentences.
? Exp2: Only explicit fillers are considered as
candidates6.
? Exp3: No default fillers are considered as
candidates.
As expected, we observe a very similar perfor-
mances in both datasets. Additionally, the high-
est loss appears when the default fillers are ruled
out (Exp3). In particular, it also seems that the
explicit information from previous predicates pro-
vides the most correct evidence (Exp2). Also note
that for Exp2, the system obtains the highest preci-
sion. This means that the most accurate cases are
obtained by previous explicit antecedents.
test full
P R F1 P R F1
full 47.9 43.8 45.8 47.7 43.0 45.3
Exp1 45.7 41.8 43.6 47.1 42.5 44.8
Exp2 51.2 24.6 33.2 55.3 25.5 34.9
Exp3 34.6 29.7 31.9 34.8 28.9 31.5
Exp4 42.6 37.9 40.1 37.5 31.2 34.1
Exp5 38.8 34.5 36.5 35.7 29.7 32.4
Exp6 53.3 48.7 50.9 52.4 47.2 49.6
Table 5: Exp1, Exp2 and Exp3 correspond to ablations of the
components. Exp3 and Exp4 are experiments over the cases
that are not solved by explicit antecedents. Exp6 evaluates
the system capturing just the head tokens of the constituents.
As Exp1 also includes instances with explicit
antecedents, and for these cases the damping fac-
tor component has no effect, we have designed two
additional experiments:
? Exp4: Full system for the cases not solved by
explicit antecedents.
? Exp5: As in Exp4 but with the damping fac-
tor disabled.
As expected, now the contribution of the dump-
ing factor seems to be more relevant, in particular,
for the test dataset.
6.2 Correct span of the fillers
As explained in Section 5, our algorithm works
with syntactic dependencies and its predictions
only return the head token of the filler. Obtaining
the correct constituents from syntactic dependen-
cies is not trivial. In this work we have applied
a simple heuristic that returns all the descendant
6That is, implicit arguments without explicit antecedents
are not filled.
tokens of the predicted head token. This naive
process inserts some noise to the evaluation of the
system. For example, from the following sentence
our system gives the following prediction for an
implicit arg1 of an instance of the predicate sale:
Ports of Call Inc. reached agreements to sell its re-
maining seven aircraft [arg1 to buyers] that weren?t
disclosed.
But the actual gold-standard annotation is:
[arg1 buyers that weren?t disclosed]. Although the
head of the constituent, buyers, is correctly cap-
tured by ImpAr, the final prediction is heavily pe-
nalized by the scoring method. Table 5 presents
the results of ImpAr when evaluating the head to-
kens of the constituents only (Exp6). These results
show that the current performance of our system
can be easily improved applying a more accurate
process for capturing the correct span.
7 Conclusions and Future Work
In this work we have presented a robust determin-
istic approach for implicit Semantic Role Label-
ing. The method exploits a very simple but rel-
evant discoursive coherence property that holds
over explicit and implicit arguments of closely re-
lated nominal and verbal predicates. This prop-
erty states that if several instances of the same
predicate appear in a well-written discourse, it is
very likely that they maintain the same argument
fillers. We have shown the importance of this phe-
nomenon for recovering the implicit information
about semantic roles. To our knowledge, this is the
first empirical study that proves this phenomenon.
Based on these observations, we have devel-
oped a new deterministic algorithm, ImpAr, that
obtains very competitive and robust performances
with respect to supervised approaches. That is, it
can be applied where there is no available manual
annotations to train. The code of this algorithm is
publicly available and can be applied to any docu-
ment. As input it only needs the document with
explicit semantic role labeling and Super-Sense
annotations. These annotations can be easily ob-
tained from plain text using available tools7, what
makes this algorithm the first effective tool avail-
able for implicit SRL.
As it can be easily seen, ImpAr has a large
margin for improvement. For instance, providing
more accurate spans for the fillers. We also plan
7We recommend mate-tools (Bjo?rkelund et al, 2009) and
SuperSenseTagger (Ciaramita and Altun, 2006).
1187
to test alternative approaches to solve the argu-
ments without explicit antecedents. For instance,
our system can also profit from additional annota-
tions like coreference, that has proved its utility in
previous works. Finally, we also plan to study our
approach on different languages and datasets (for
instance, the SemEval-2010 dataset).
8 Acknowledgment
We are grateful to the anonymous reviewers
for their insightful comments. This work has
been partially funded by SKaTer (TIN2012-
38584-C06-02), OpeNER (FP7-ICT-2011-SME-
DCL-296451) and NewsReader (FP7-ICT-2011-
8-316404), as well as the READERS project
with the financial support of MINECO, ANR
(convention ANR-12-CHRI-0004-03) and EPSRC
(EP/K017845/1) in the framework of ERA-NET
CHIST-ERA (UE FP7/2007-2013).
References
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998).
The berkeley framenet project. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th Inter-
national Conference on Computational Linguis-
tics, ACL ?98, Montreal, Quebec, Canada, pp.
86?90.
Bjo?rkelund, A., L. Hafdell, and P. Nugues (2009).
Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, CoNLL ?09, Boulder, Colorado, USA, pp.
43?48.
Carreras, X. and L. Ma`rquez (2005). Introduction
to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of the 9th Conference
on Computational Natural Language Learning,
CoNLL ?05, Ann Arbor, Michigan, USA, pp.
152?164.
Chen, D., N. Schneider, D. Das, and N. A. Smith
(2010). Semafor: Frame argument resolution
with log-linear models. In Proceedings of the
5th International Workshop on Semantic Eval-
uation, SemEval ?10, Los Angeles, California,
USA, pp. 264?267.
Ciaramita, M. and Y. Altun (2006). Broad-
coverage sense disambiguation and information
extraction with a supersense sequence tagger. In
Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP ?06, Sydney, Australia, pp. 594?602.
Dahl, D. A., M. S. Palmer, and R. J. Passonneau
(1987). Nominalizations in pundit. In In Pro-
ceedings of the 25th Annual Meeting of the As-
sociation for Computational Linguistics, ACL
?87, Stanford, California, USA, pp. 131?139.
Fellbaum, C. (1998). WordNet: an electronic lexi-
cal database. MIT Press.
Gerber, M. and J. Chai (2012, December). Se-
mantic role labeling of implicit arguments for
nominal predicates. Computational Linguis-
tics 38(4), 755?798.
Gerber, M. and J. Y. Chai (2010). Beyond nom-
bank: a study of implicit arguments for nomi-
nal predicates. In Proceedings of the 48th An-
nual Meeting of the Association for Computa-
tional Linguistics, ACL ?10, Uppsala, Sweden,
pp. 1583?1592.
Gildea, D. and D. Jurafsky (2000). Automatic la-
beling of semantic roles. In Proceedings of the
38th Annual Meeting on Association for Com-
putational Linguistics, ACL ?00, Hong Kong,
pp. 512?520.
Gildea, D. and D. Jurafsky (2002, September).
Automatic labeling of semantic roles. Compu-
tational Linguistics 28(3), 245?288.
Gorinski, P., J. Ruppenhofer, and C. Sporleder
(2013). Towards weakly supervised resolution
of null instantiations. In Proceedings of the 10th
International Conference on Computational Se-
mantics, IWCS ?13, Potsdam, Germany, pp.
119?130.
Hajic?, J., M. Ciaramita, R. Johansson, D. Kawa-
hara, M. A. Mart??, L. Ma`rquez, A. Meyers,
J. Nivre, S. Pado?, J. S?te?pa?nek, P. Stran?a?k,
M. Surdeanu, N. Xue, and Y. Zhang (2009).
The CoNLL-2009 shared task: Syntactic and
semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference
on Computational Natural Language Learning:
Shared Task, CoNLL ?09, Boulder, Colorado,
USA, pp. 1?18.
Laparra, E. and G. Rigau (2012). Exploiting ex-
plicit annotations and semantic types for im-
plicit argument resolution. In 6th IEEE Inter-
national Conference on Semantic Computing,
ICSC ?12, Palermo, Italy, pp. 75?78.
1188
Laparra, E. and G. Rigau (2013). Sources of evi-
dence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference
on Computational Semantics, IWCS ?13, Pots-
dam, Germany, pp. 155?166.
Lappin, S. and H. J. Leass (1994, December). An
algorithm for pronominal anaphora resolution.
Computational Linguistics 20(4), 535?561.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman
(2004). The nombank project: An interim re-
port. In In Proceedings of the NAACL/HLT
Workshop on Frontiers in Corpus Annota-
tion, HLT-NAACL ?04, Boston, Massachusetts,
USA, pp. 24?31.
Moor, T., M. Roth, and A. Frank (2013).
Predicate-specific annotations for implicit role
binding: Corpus annotation, data analysis and
evaluation experiments. In Proceedings of
the 10th International Conference on Compu-
tational Semantics, IWCS ?13, Potsdam, Ger-
many, pp. 369?375.
Palmer, M., D. Gildea, and P. Kingsbury (2005,
March). The proposition bank: An annotated
corpus of semantic roles. Computational Lin-
guistics 31(1), 71?106.
Palmer, M. S., D. A. Dahl, R. J. Schiffman,
L. Hirschman, M. Linebarger, and J. Dowding
(1986). Recovering implicit information. In
Proceedings of the 24th annual meeting on As-
sociation for Computational Linguistics, ACL
?86, New York, New York, USA, pp. 10?19.
Raghunathan, K., H. Lee, S. Rangarajan,
N. Chambers, M. Surdeanu, D. Jurafsky, and
C. Manning (2010). A multi-pass sieve for
coreference resolution. In Proceedings of the
2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ?10, Cam-
bridge, Massachusetts, USA, pp. 492?501.
Ruppenhofer, J., P. Gorinski, and C. Sporleder
(2011). In search of missing arguments: A lin-
guistic approach. In Proceedings of the Inter-
national Conference Recent Advances in Nat-
ural Language Processing 2011, RANLP ?11,
Hissar, Bulgaria, pp. 331?338.
Ruppenhofer, J., C. Sporleder, R. Morante,
C. Baker, and M. Palmer (2010). Semeval-2010
task 10: Linking events and their participants
in discourse. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, Se-
mEval ?10, Los Angeles, California, USA, pp.
45?50.
Silberer, C. and A. Frank (2012). Casting implicit
role linking as an anaphora resolution task. In
Proceedings of the First Joint Conference on
Lexical and Computational Semantics, *SEM
?12, Montre?al, Canada, pp. 1?10.
Surdeanu, M., R. Johansson, A. Meyers,
L. Ma`rquez, and J. Nivre (2008). The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
the Twelfth Conference on Natural Language
Learning, CoNLL ?08, Manchester, United
Kingdom, pp. 159?177.
Tetreault, J. R. (2002). Implicit role reference.
In International Symposium on Reference Res-
olution for Natural Language Processing, Ali-
cante, Spain, pp. 109?115.
Tonelli, S. and R. Delmonte (2010). Venses++:
Adapting a deep semantic processing system to
the identification of null instantiations. In Pro-
ceedings of the 5th International Workshop on
Semantic Evaluation, SemEval ?10, Los Ange-
les, California, USA, pp. 296?299.
Tonelli, S. and R. Delmonte (2011). Desperately
seeking implicit arguments in text. In Proceed-
ings of the ACL 2011 Workshop on Relational
Models of Semantics, RELMS ?11, Portland,
Oregon, USA, pp. 54?62.
Whittemore, G., M. Macpherson, and G. Carlson
(1991). Event-building through role-filling and
anaphora resolution. In Proceedings of the 29th
annual meeting on Association for Computa-
tional Linguistics, ACL ?91, Berkeley, Califor-
nia, USA, pp. 17?24.
1189
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 402?406,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
GPLSI-IXA: Using Semantic Classes to Acquire Monosemous Training
Examples from Domain Texts
Rub
?
en Izquierdo & Armando Su
?
arez
GPLSI Group
University of Alicante. Spain
{ruben,armando}@dlsi.ua.es
German Rigau
IXA NLP Group.
EHU. Donostia, Spain
german.rigau@ehu.es
Abstract
This paper summarizes our participation
in task #17 of SemEval?2 (All?words
WSD on a specific domain) using a su-
pervised class-based Word Sense Disam-
biguation system. Basically, we use Sup-
port Vector Machines (SVM) as learning
algorithm and a set of simple features to
build three different models. Each model
considers a different training corpus: Sem-
Cor (SC), examples from monosemous
words extracted automatically from back-
ground data (BG), and both SC and
BG (SCBG). Our system explodes the
monosemous words appearing as mem-
bers of a particular WordNet semantic
class to automatically acquire class-based
annotated examples from the domain text.
We use the class-based examples gathered
from the domain corpus to adapt our tra-
ditional system trained on SemCor. The
evaluation reveal that the best results are
achieved training with SemCor and the
background examples from monosemous
words, obtaining results above the first
sense baseline and the fifth best position
in the competition rank.
1 Introduction
As empirically demonstrated by the last SensEval
and SemEval exercises, assigning the appropriate
meaning to words in context has resisted all at-
tempts to be successfully addressed. In fact, super-
vised word-based WSD systems are very depen-
dent of the corpora used for training and testing
the system (Escudero et al, 2000). One possible
reason could be the use of inappropriate level of
abstraction.
Most supervised systems simply model each
polysemous word as a classification problem
where each class corresponds to a particular synset
of the word. But, WordNet (WN) has been widely
criticized for being a sense repository that often
provides too fine?grained sense distinctions for
higher level applications like Machine Translation
or Question & Answering. In fact, WSD at this
level of granularity has resisted all attempts of in-
ferring robust broad-coverage models. It seems
that many word?sense distinctions are too subtle
to be captured by automatic systems with the cur-
rent small volumes of word?sense annotated ex-
amples.
Thus, some research has been focused on deriv-
ing different word-sense groupings to overcome
the fine?grained distinctions of WN (Hearst and
Sch?utze, 1993), (Peters et al, 1998), (Mihalcea
and Moldovan, 2001), (Agirre and LopezDeLa-
Calle, 2003), (Navigli, 2006) and (Snow et al,
2007). That is, they provide methods for grouping
senses of the same word, thus producing coarser
word sense groupings for better disambiguation.
In contrast, some research have been focused on
using predefined sets of sense-groupings for learn-
ing class-based classifiers for WSD (Segond et al,
1997), (Ciaramita and Johnson, 2003), (Villarejo
et al, 2005), (Curran, 2005), (Kohomban and Lee,
2005) and (Ciaramita and Altun, 2006). That is,
grouping senses of different words into the same
explicit and comprehensive semantic class. Most
of the later approaches used the original Lexico-
graphical Files of WN (more recently called Su-
perSenses) as very coarse?grained sense distinc-
tions.
We suspect that selecting the appropriate level
of abstraction could be on between both levels.
Thus, we use the semantic classes modeled by the
Basic Level Concepts
1
(BLC) (Izquierdo et al,
2007). Our previous research using BLC empiri-
cally demonstrated that this automatically derived
1
http://adimen.si.ehu.es/web/BLC
402
set of meanings groups senses into an adequate
level of abstraction in order to perform class-based
Word Sense Disambiguation (WSD) (Izquierdo et
al., 2009). Now, we also show that class-based
WSD allows to successfully incorporate monose-
mous examples from the domain text. In fact,
the robustness of our class-based WSD approach
is shown by our system that just uses the Sem-
Cor examples (SC). It performs without any kind
of domain adaptation as the Most Frequent Sense
(MFS) baseline.
This paper describes our participation in
SemEval-2010 Task 17 (Agirre et al, 2010). In
section 2 semantic classes used and selection al-
gorithm used to obtain them automatically from
WordNet are described. In section 3 the technique
employed to extract monosemous examples from
background data is described. Section 4 explains
the general approach of our system, and the ex-
periments designed, and finally, in section 5, the
results and some analysis are shown.
2 Semantic Classes
The set of semantic classes used in this work are
the Basic Level Concepts
2
(BLC) (Izquierdo et
al., 2007). These concepts are small sets of mean-
ings representing the whole nominal and verbal
part of WN. BLC can be obtained by a very simple
method that uses basic structural WordNet proper-
ties. In fact, the algorithm only considers the rel-
ative number of relations of each synset alng the
hypernymy chain. The process follows a bottom-
up approach using the chain of hypernymy rela-
tions. For each synset in WN, the process selects
as its BLC the first local maximum according to
the relative number of relations. The local maxi-
mum is the synset in the hypernymy chain having
more relations than its immediate hyponym and
immediate hypernym. For synsets having multi-
ple hypernyms, the path having the local maxi-
mum with higher number of relations is selected.
Usually, this process finishes having a number of
preliminary BLC. Figure 1 shows an example of
selection of a BLC. The figure represents the hy-
pernymy hierarchy of WordNet, with circles rep-
resenting synsets, and links between them repre-
senting hypernym relations. The algorithm selects
the D synset as BLC for J, due to D is the first
maximum in the hypernymy chain, according to
the number of relations (F has 2 hyponyms, D has
2
http://adimen.si.ehu.es/web/BLC
3, and A has 2, so D is the first maximum).
I
D
G H
C
A
J
F
2
3
2
B
E
2
B L C
S y n s e t
Figure 1: Example of BLC selection
Obviously, while ascending through this chain,
more synsets are subsumed by each concept. The
process finishes checking if the number of con-
cepts subsumed by the preliminary list of BLC is
higher than a certain threshold. For those BLC
not representing enough concepts according to the
threshold, the process selects the next local max-
imum following the hypernymy hierarchy. Thus,
depending on the type of relations considered to
be counted and the threshold established, different
sets of BLC can be easily obtained for each WN
version.
We have selected the set which considers WN
version 3.0, the total number of relations per
synset, and a minimum threshold of 20 concepts to
filter out not representative BLC (BLC?20). This
set has shown to reach good performance on previ-
ous SensEval and SemEval exercices (Izquierdo et
al., 2009). There are 649 different BLC for nouns
on WordNet 3.0, and 616 for verbs. Table 2 shows
the three most frequent BLC per POS, with the
number of synsets subsumed by each concept, and
its WordNet gloss.
3 Using Monosemous Examples from the
Domain
We did not applied any kind of specific domain
adaptation technique to our class-based supervised
system. In order to adapt our supervised system to
the environmental domain we only increased the
training data with new examples of the domain. To
acquire these examples, we used the environmen-
tal domain background documents provided by the
organizers. Specifically, we used the 122 back-
403
PoS Num. BLC Gloss
Nouns
4.792 person.n.01 a human being
1.935 activity.n.01 any specific behavior
1.846 act.n.02 something that people do or cause to happen
Verbs
1.541 change.v.01 cause to change; make different; cause a transformation
1.085 change.v.02 undergo a change; become different in essence; losing one?s or its original na-
ture
519 move.v.02 cause to move or shift into a new position or place, both in a concrete and in an
abstract sense
Table 1: Most frequent BLC?20 semantic classes on WordNet 3.0
ground documents
3
. TreeTagger has been used
to preprocess the documents, performing PoS tag-
ging and lemmatization. Since the background
documents are not semantically annotated, and our
supervised system needs labeled data, we have se-
lected only the monosemous words occurring in
the documents. In this way, we have obtained au-
tomatically a large set of examples annotated with
BLC. Table 3 presents the total number of training
examples extracted from SemCor (SC) and from
the background documents (BG). As expected, by
this method a large number of monosemous ex-
amples can be obtained for nouns and verbs. Also
as expected, verbs are much less productive than
nouns. However, all these background examples
correspond to a reduced set of 7,646 monosemous
words.
Nouns Verbs N+V
SC 87.978 48.267 136.245
BG 193.536 10.821 204.357
Total 281.514 59.088 340.602
Table 2: Number of training examples
Table 3 lists the ten most frequent monosemous
nouns and verbs occurring in the background doc-
uments. Note that all these examples are monose-
mous according to BLC?20 semantic classes.
Nouns Verbs
Lemma # ex. Lemma # ex.
1 biodiversity 7.476 monitor 788
2 habitat 7.206 achieve 784
3 specie 7.067 target 484
4 climate 3.539 select 345
5 european 2.818 enable 334
6 ecosystem 2.669 seem 287
7 river 2.420 pine 281
8 grassland 2.303 evaluate 246
9 datum 2.276 explore 200
10 directive 2.197 believe 172
Table 3: Most frequent monosemic words in BG
3
We used the documents contained on the trial data and
the background.
4 System Overview
Our system applies a supervised machine learn-
ing approach. We apply a feature extractor to
represent the training examples of the examples
acquired from SemCor and the background doc-
uments. Then, a machine learning engine uses
the annotated examples to train a set of classi-
fiers. Support Vector Machines (SVM) have been
proven to be robust and very competitive in many
NLP tasks, and in WSD in particular (M`arquez et
al., 2006). We used the SVM-Light implementa-
tion
4
(Joachims, 1998).
We create a classifier for each semantic class.
This approach has several advantages compared to
word?based approach. The training data per clas-
sifier is increased (we can use examples of dif-
ferent target words for a single classifier, when-
ever all examples belong to the same semantic
class), the polysemy is reduced (some different
word senses can be collapsed into the same se-
mantic class), and, finally, semantic classes pro-
vide higher levels of abstraction.
For each polysemous word occurring in the test
corpus, we obtain its potential BLC?20 classes.
Then, we only apply the classifiers corresponding
to the BLC-20 classes of the polysemous word. Fi-
nally, our system simply selects the BLC?20 class
with the greater prediction.
In order to obtain the correct WordNet 3.0
synset required by the task, we apply a simple
heuristic that has shown to be robust and accurate
(Kohomban and Lee, 2005). Our classifiers ob-
tain first the semantic class, and then, the synset of
the first WordNet sense that fits with the semantic
class is assigned to the word.
We selected a simple feature set widely used in
many WSD systems. In particular, we use a win-
dow of five tokens around the target word to ex-
tract word forms, lemmas; bigrams and trigrams
of word forms and lemmas; trigrams of PoS tags,
4
http://svmlight.joachims.org
404
and also the most frequent BLC?20 semantic class
of the target word in the training corpus.
Our system is fully described in (Izquierdo et
al., 2009). The novelty introduced here is the use
of semantic classes to obtain monosemous exam-
ples from the domain corpus.
Following the same framework (BLC?20 se-
mantic architecture and basic set of features) we
designed three runs, each one using a different
training corpus.
? SC: only training examples extracted from
SemCor
? BG: only monosemous examples extracted
from the background data
? SCBG: training examples extracted from
SemCor and monosemous background data
The first run shows the behavior of a supervised
system trained on a general corpus, and tested in a
specific domain. The second one analyzes the con-
tribution of the monosemous examples extracted
from the background data. Finally, the third run
studies the robustness of the approach when com-
bining the training examples from SemCor and
from the background.
5 Results and Discussion
A total of 29 runs has been submitted for the En-
glish All?words WSD on a Specific Domain. Ta-
ble 5 shows the ranking results of our three runs
with respect to the other participants. The figures
for the first sense (1sense) and random sense (Ran-
dom) baselines are included.
In general, the results obtained are not very
high. The best system only achieves a precision of
0.570, and the first sense baseline reaches a preci-
sion of 0.505. This shows that the task is hard to
solve, and the domain adaptation of WSD systems
is not an easy task.
Interestingly, our worst result is obtained by the
system using only the monosemous background
examples (BG). This system ranks 23th with a Pre-
cision and Recall of 0.380 (0.385 for nouns and
0.366 for verbs). The system using only SemCor
(SC) ranks 6th with Precision and Recall of 0.505
(0.527 for nouns and 0.443 for verbs). This is also
the performance of the first sense baseline. As ex-
pected, the best result of our three runs is obtained
when combining the examples from SemCor and
the background (SCBG). This supervised system
obtains the 5th position with a Precision and Re-
call of 0.513 (0.534 for nouns, 0.454 for verbs)
which is slightly above the baseline.
Rank Precision Recall
1 0.570 0.555
2 0.554 0.540
3 0.534 0.528
4 0.522 0.516
(SCBG) 5 0.513 0.513
1sense 0.505 0.505
(SC) 6 0.505 0.505
7 0.512 0.495
8 0.506 0.493
9 0.504 0.491
10 0.481 0.481
11 0.492 0.479
12 0.461 0.460
13 0.447 0.441
14 0.436 0.435
15 0.440 0.434
16 0.496 0.433
17 0.498 0.432
18 0.433 0.431
19 0.426 0.425
20 0.424 0.422
21 0.437 0.392
22 0.384 0.384
(BG) 23 0.380 0.380
24 0.381 0.356
25 0.351 0.350
26 0.370 0.345
27 0.328 0.322
28 0.321 0.315
29 0.312 0.303
Random 0.230 0.230
Table 4: Results of task#17
Possibly, the reason of low performance of the
BG system is the high correlation between the fea-
tures of the target word and its semantic class. In
this case, these features correspond to the monose-
mous word while when testing corresponds to the
target word. However, it also seems that class-
based systems are robust enough to incorporate
large sets of monosemous examples from the do-
main text. In fact, to our knowledge, this is the first
time that a supervised WSD algorithm have been
successfully adapted to an specific domain. Fur-
thermore, our system trained only on SemCor also
achieves a good performance, reaching the first
sense baseline, showing that class-based WSD ap-
proaches seem to be robust to domain variations.
Acknowledgments
This paper has been supported by the Euro-
pean Union under the project KYOTO (FP7 ICT-
211423), the Valencian Region Government un-
der PROMETEO project for excellence groups
and the Spanish Government under the projects
405
KNOW2 (TIN2009-14715-C04-04) and TEXT-
MESS-2 (TIN2009-13391-C04-04).
References
E. Agirre and O. LopezDeLaCalle. 2003. Clustering
wordnet word senses. In Proceedings of RANLP?03,
Borovets, Bulgaria.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP?06), pages 594?602,
Sydney, Australia. ACL.
M. Ciaramita and M. Johnson. 2003. Supersense tag-
ging of unknown nouns in wordnet. In Proceedings
of the Conference on Empirical methods in natural
language processing (EMNLP?03), pages 168?175.
ACL.
J. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL?05), pages 26?33. ACL.
G. Escudero, L. M`arquez, and G. Rigau. 2000. An
Empirical Study of the Domain Dependence of Su-
pervised Word Sense Disambiguation Systems. In
Proceedings of the joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, EMNLP/VLC, Hong Kong,
China.
M. Hearst and H. Sch?utze. 1993. Customizing a lexi-
con to better suit a computational task. In Proceed-
ingns of the ACL SIGLEX Workshop on Lexical Ac-
quisition, Stuttgart, Germany.
R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor-
ing the automatic selection of basic level concepts.
In Galia Angelova et al, editor, International Con-
ference Recent Advances in Natural Language Pro-
cessing, pages 298?302, Borovets, Bulgaria.
Rub?en Izquierdo, Armando Su?arez, and German Rigau.
2009. An empirical study on class-based word sense
disambiguation. In Proceedings of the 12th Con-
ference of the European Chapter of the ACL (EACL
2009), pages 389?397, Athens, Greece, March. As-
sociation for Computational Linguistics.
T. Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant
features. In Claire N?edellec and C?eline Rouveirol,
editors, Proceedings of ECML-98, 10th European
Conference on Machine Learning, pages 137?142,
Chemnitz, DE. Springer Verlag, Heidelberg, DE.
Upali S. Kohomban and Wee Sun Lee. 2005. Learning
semantic classes for word sense disambiguation. In
ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
34?41, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Ll. M`arquez, G. Escudero, D. Mart??nez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In
E. Agirre and P. Edmonds (Eds.) Word Sense Disam-
biguation: Algorithms and applications., volume 33
of Text, Speech and Language Technology. Springer.
R. Mihalcea and D. Moldovan. 2001. Automatic gen-
eration of coarse grained wordnet. In Proceding of
the NAACL workshop on WordNet and Other Lex-
ical Resources: Applications, Extensions and Cus-
tomizations, Pittsburg, USA.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In ACL-44: Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 105?112, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
W. Peters, I. Peters, and P. Vossen. 1998. Automatic
sense clustering in eurowordnet. In First Interna-
tional Conference on Language Resources and Eval-
uation (LREC?98), Granada, Spain.
F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid-
den markov model tagging. In ACL Workshop on
Automatic Information Extraction and Building of
Lexical Semantic Resources for NLP Applications,
pages 78?81. ACL, New Brunswick, New Jersey.
R. Snow, Prakash S., Jurafsky D., and Ng A. 2007.
Learning to merge word senses. In Proceedings of
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1005?
1014.
L. Villarejo, L. M`arquez, and G. Rigau. 2005. Ex-
ploring the construction of semantic class classi-
fiers for wsd. In Proceedings of the 21th Annual
Meeting of Sociedad Espaola para el Procesamiento
del Lenguaje Natural SEPLN?05, pages 195?202,
Granada, Spain, September. ISSN 1136-5948.
406
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 132?137, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UBC UOS-TYPED: Regression for Typed-similarity
Eneko Agirre
University of the Basque Country
Donostia, 20018, Basque Country
e.agirre@ehu.es
Nikolaos Aletras
University of Sheffield
Sheffield, S1 4DP, UK
n.aletras@dcs.shef.ac.uk
Aitor Gonzalez-Agirre
University of the Basque Country
Donostia, 20018, Basque Country
agonzalez278@ikasle.ehu.es
German Rigau
University of the Basque Country
Donostia, 20018, Basque Country
german.rigau@ehu.es
Mark Stevenson
University of Sheffield
Sheffield, S1 4DP, UK
m.stevenson@dcs.shef.ac.uk
Abstract
We approach the typed-similarity task using
a range of heuristics that rely on information
from the appropriate metadata fields for each
type of similarity. In addition we train a linear
regressor for each type of similarity. The re-
sults indicate that the linear regression is key
for good performance. Our best system was
ranked third in the task.
1 Introduction
The typed-similarity dataset comprises pairs of Cul-
tural Heritage items from Europeana1, a single ac-
cess point to digitised versions of books, paintings,
films, museum objects and archival records from in-
stitutions throughout Europe. Typically, the items
comprise meta-data describing a cultural heritage
item and, sometimes, a thumbnail of the item itself.
Participating systems need to compute the similarity
between items using the textual meta-data. In addi-
tion to general similarity, the dataset includes spe-
cific kinds of similarity, like similar author, similar
time period, etc.
We approach the problem using a range of sim-
ilarity techniques for each similarity types, these
make use of information contained in the relevant
meta-data fields.In addition, we train a linear regres-
sor for each type of similarity, using the training data
provided by the organisers with the previously de-
fined similarity measures as features.
We begin by describing our basic system in Sec-
tion 2, followed by the machine learning system in
1http://www.europeana.eu/
Section 3. The submissions are explained in Section
4. Section 5 presents our results. Finally, we draw
our conclusions in Section 6.
2 Basic system
The items in this task are taken from Europeana.
They cannot be redistributed, so we used the urls
and scripts provided by the organizers to extract the
corresponding metadata. We analysed the text in the
metadata, performing lemmatization, PoS tagging,
named entity recognition and classification (NERC)
and date detection using Stanford CoreNLP (Finkel
et al, 2005; Toutanova et al, 2003). A preliminary
score for each similarity type was then calculated as
follows:
? General: cosine similarity of TF.IDF vectors of
tokens, taken from all fields.
? Author: cosine similarity of TF.IDF vectors of
dc:Creator field.
? People involved, time period and location:
cosine similarity of TF.IDF vectors of loca-
tion/date/people entities recognized by NERC
in all fields.
? Events: cosine similarity of TF.IDF vectors of
event verbs and nouns. A list of verbs and
nouns possibly denoting events was derived us-
ing the WordNet Morphosemantic Database2.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields.
IDF values were calculated using a subset of Eu-
ropeana items (the Culture Grid collection), avail-
able internally. These preliminary scores were im-
2urlhttp://wordnetcode.princeton.edu/standoff-
files/morphosemantic-links.xls
132
proved using TF.IDF based on Wikipedia, UKB
(Agirre and Soroa, 2009) and a more informed time
similarity measure. We describe each of these pro-
cesses in turn.
2.1 TF.IDF
A common approach to computing document sim-
ilarity is to represent documents as Bag-Of-Words
(BOW). Each BOW is a vector consisting of the
words contained in the document, where each di-
mension corresponds to a word, and the weight is
the frequency in the corresponding document. The
similarity between two documents can be computed
as the cosine of the angle between their vectors. This
is the approached use above.
This approach can be improved giving more
weight to words which occur in only a few docu-
ments, and less weight to words occurring in many
documents (Baeza-Yates and Ribeiro-Neto, 1999).
In our system, we count document frequencies of
words using Wikipedia as a reference corpus since
the training data consists of only 750 items associ-
ated with short textual information and might not be
sufficient for reliable estimations. The TF.IDF sim-
ilarity between items a and b is defined as:
simtf.idf(a, b) =
?
w?a,b tfw,a ? tfw,b ? idf
2
w
??
w?a(tfw,a ? idfw)
2 ?
??
w?b(tfw,b ? idfw)
2
where tfw,x is the frequency of the term w in x ?
{a, b} and idfw is the inverted document frequency
of the word w measured in Wikipedia. We substi-
tuted the preliminary general similarity score by the
obtained using the TF.IDF presented in this section.
2.2 UKB
The semantic disambiguation UKB3 algorithm
(Agirre and Soroa, 2009) applies personalized
PageRank on a graph generated from the English
WordNet (Fellbaum, 1998), or alternatively, from
Wikipedia. This algorithm has proven to be very
competitive in word similarity tasks (Agirre et al,
2010).
To compute similarity using UKB we represent
WordNet as a graph G = (V,E) as follows: graph
nodes represent WordNet concepts (synsets) and
3http://ixa2.si.ehu.es/ukb/
dictionary words; relations among synsets are rep-
resented by undirected edges; and dictionary words
are linked to the synsets associated to them by di-
rected edges.
Our method is provided with a pair of vectors of
words and a graph-based representation of WordNet.
We first compute the personalized PageRank over
WordNet separately for each of the vector of words,
producing a probability distribution over WordNet
synsets. We then compute the similarity between
these two probability distributions by encoding them
as vectors and computing the cosine between the
vectors. We present each step in turn.
Once personalized PageRank is computed, it
returns a probability distribution over WordNet
synsets. The similarity between two vectors of
words can thus be implemented as the similarity be-
tween the probability distributions, as given by the
cosine between the vectors.
We used random walks to compute improved sim-
ilarity values for author, people involved, location
and event similarity:
? Author: UKB over Wikipedia using person en-
tities recognized by NERC in the dc:Creator
field.
? People involved and location: UKB over
Wikipedia using people/location entities recog-
nized by NERC in all fields.
? Events: UKB over WordNet using event nouns
and verbs recognized in all fields.
Results on the training data showed that perfor-
mance using this approach was quite low (with the
exception of events). This was caused by the large
number of cases where the Stanford parser did not
find entities which were in Wikipedia. With those
cases on mind, we combined the scores returned by
UKB with the similarity scores presented in Section
2 as follows: if UKB similarity returns a score, we
multiply both, otherwise we return the square of the
other similarity score. Using the multiplication of
the two scores, the results on the training data im-
proved.
2.3 Time similarity measure
In order to measure the time similarity between a
pair of items, we need to recognize time expres-
sions in both items. We assume that the year of
133
creation or the year denoting when the event took
place in an artefact are good indicators for time sim-
ilarity. Therefore, information about years is ex-
tracted from each item using the following pattern:
[1|2][0 ? 9]{3}. Using this approach, each item is
represented as a set of numbers denoting the years
mentioned in the meta-data.
Time similarity between two items is computed
based on the similarity between their associated
years. Similarity between two years is defined as:
simyear(y1, y2) = max{0, 1? |y1? y2| ? k}
k is a parameter to weight the difference between
two years, e.g. for k = 0.1 all items that have differ-
ence of 10 years or more assigned a score of 0. We
obtained best results for k = 0.1.
Finally, time similarity between items a and b is
computed as the maximum of the pairwise similarity
between their associated years:
simtime(a, b) = max?i?a
?j?b
{0, simyear(ai, bj)}
We substituted the preliminary time similarity
score by the measure obtained using the method pre-
sented in this section.
3 Applying Machine Learning
The above heuristics can be good indicators for the
respective kind of similarity, and can be thus applied
directly to the task. In this section, we take those
indicators as features, and use linear regression (as
made available by Weka (Hall et al, 2009)) to learn
models that fit the features to the training data.
We generated further similarity scores for gen-
eral similarity, including Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), UKB and Wikipedia Link
Vector Model (WLVM)(Milne, 2007) using infor-
mation taken from all fields, as explained below.
3.1 LDA
LDA (Blei et al, 2003) is a statistical method that
learns a set of latent variables called topics from a
training corpus. Given a topic model, documents
can be inferred as probability distributions over top-
ics, ?. The distribution for a document i is denoted
as ?i. An LDA model is trained using the train-
ing set consisting of 100 topics using the gensim
package4. The hyperparameters (?, ?) were set to
1
num of topics . Therefore, each item in the test set is
represented as a topic distribution.
The similarity between a pair of items is estimated
by comparing their topic distributions following the
method proposed in Aletras et al (2012; Aletras and
Stevenson (2012). This is achieved by considering
each distribution as a vector (consisting of the topics
corresponding to an item and its probability) then
computing the cosine of the angle between them, i.e.
simLDA(a, b) =
~?a ? ~?b
|~?a| ? | ~?b|
where ~?a is the vector created from the probability
distribution generated by LDA for item a.
3.2 Pairwise UKB
We run UKB (Section 2.2) to generate a probabil-
ity distribution over WordNet synsets for all of the
words of all items. Similarity between two words
is computed by creating vectors from these distri-
butions and comparing them using the cosine of the
angle between the two vectors. If a words does not
appear in WordNet its similarity value to every other
word is set to 0. We refer to that similarity metric as
UKB here.
Similarity between two items is computed by per-
forming pairwise comparison between their words,
for each, selecting the highest similarity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?b UKB(w1, w2)
|a|
+
?
w2?b
argmaxw1?a UKB(w2, w1)
|b|
)
where a and b are two items, |a| the number of
tokens in a and UKB(w1, w2) is the similarity be-
tween words w1 and w2.
3.3 WLVM
An algorithm described by Milne and Witten (2008)
associates Wikipedia articles which are likely to be
relevant to a given text snippet using machine learn-
ing techniques. We make use of that method to rep-
resent each item as a set of likely relevant Wikipedia
4http://pypi.python.org/pypi/gensim
134
articles. Then, similarity between Wikipedia arti-
cles is measured using the Wikipedia Link Vector
Model (WLVM) (Milne, 2007). WLVM uses both
the link structure and the article titles of Wikipedia
to measure similarity between two Wikipedia arti-
cles. Each link is weighted by the probability of it
occurring. Thus, the value of the weight w for a link
x? y between articles x and y is:
w(x? y) = |x? y| ? log
(
t?
z=1
t
z ? y
)
where t is the total number of articles in Wikipedia.
The similarity of articles is compared by forming
vectors of the articles which are linked from them
and computing the cosine of their angle. For exam-
ple the vectors of two articles x and y are:
x = (w(x? l1), w(x? l2), ..., w(x? ln))
y = (w(y ? l1), w(y ? l2), ..., w(y ? ln))
where x and y are two Wikipedia articles and x? li
is a link from article x to article li.
Since the items have been mapped to Wikipedia
articles, similarity between two items is computed
by performing pairwise comparison between articles
using WLVM, for each, selecting the highest simi-
larity score:
sim(a, b) =
1
2
(?
w1?a
argmaxw2?bWLVM(w1, w2)
|a|
+
?
w2?b
argmaxw1?aWLVM(w2, w1)
|b|
)
where a and b are two items, |a| the number of
Wikipedia articles in a and WLVM(w1, w2) is the
similarity between concepts w1 and w2.
4 Submissions
We selected three systems for submission. The first
run uses the similarity scores of the basic system
(Section 2) for each similarity types as follows:
? General: cosine similarity of TF.IDF vectors,
IDF based on Wikipedia (as shown in Section
2.1).
? Author: product of the scores obtained ob-
tained using TF.IDF vectors and UKB (as
shown in Section 2.2) using only the data ex-
tracted from dc:Creator field.
? People involved and location: product of co-
sine similarity of TF.IDF vectors and UKB (as
shown in Section 2.2) using the data extracted
from all fields.
? Time period: time similarity measure (as
shown in Section 2.3).
? Events: product of cosine similarity of TF.IDF
vectors and UKB (as shown in Section 2.2) of
event nouns and verbs recognized in all fields.
? Subject and description: cosine similarity of
TF.IDF vectors of respective fields (as shown
in Section 2).
For the second run we trained a ML model for
each of the similarity types, using the following fea-
tures:
? Cosine similarity of TF.IDF vectors as shown
in Section 2 for the eight similarity types.
? Four new values for general similarity: TF.IDF
(Section 2.1), LDA (Section 3.1), UKB and
WLVM (Section 3.3).
? Time similarity as shown in Section 2.3.
? Events similarity computed using UKB initial-
ized with the event nouns and verbs in all fields.
We decided not to use the product of TF.IDF
and UKB presented in Section 2.2 in this system
because our intention was to measure the power of
the linear regression ML algorithm to learn on the
given raw data.
The third run is similar, but includes all available
features (21). In addition to the above, we included:
? Author, people involved and location similar-
ity computed using UKB initialized with peo-
ple/location recognized by NERC in dc:Creator
field for author, and in all fields for people in-
volved and location.
? Author, people involved, location and event
similarity scores computed by the product of
TF.IDF vectors and UKB values as shown in
Section 2.2.
5 Results
Evaluation was carried out using the official scorer
provided by the organizers, which computes the
Pearson Correlation score for each of the eight sim-
ilarity types plus an additional mean correlation.
135
Team and run General Author People involved Time Location Event Subject Description Mean
UBC UOS-RUN1 0.7269 0.4474 0.4648 0.5884 0.4801 0.2522 0.4976 0.5389 0.5033
UBC UOS-RUN2 0.7777 0.6680 0.6767 0.7609 0.7329 0.6412 0.7516 0.8024 0.7264
UBC UOS-RUN3 0.7866 0.6941 0.6965 0.7654 0.7492 0.6551 0.7586 0.8067 0.7390
Table 1: Results of our systems on the training data, using cross-validation when necessary.
Team and run General Author People involved Time Location Event Subject Description Mean Rank
UBC UOS-RUN1 0.7256 0.4568 0.4467 0.5762 0.4858 0.3090 0.5015 0.5810 0.5103 6
UBC UOS-RUN2 0.7457 0.6618 0.6518 0.7466 0.7244 0.6533 0.7404 0.7751 0.7124 4
UBC UOS-RUN3 0.7461 0.6656 0.6544 0.7411 0.7257 0.6545 0.7417 0.7763 0.7132 3
Table 2: Results of our submitted systems.
5.1 Development
The three runs mentioned above were developed us-
ing the training data made available by the organiz-
ers. In order to avoid overfitting we did not change
the default parameters of the linear regressor, and
10-fold cross-validation was used for evaluating the
models on the training data. The results of our sys-
tems on the training data are shown on Table 1. The
table shows that the heuristics (RUN1) obtain low
results, and that linear regression improves results
considerably in all types. Using the full set of fea-
tures, RUN3 improves slightly over RUN2, but the
improvement is consistent across all types.
5.2 Test
The test dataset was composed of 750 pairs of items.
Table 2 illustrates the results of our systems in the
test dataset. The results of the runs are very similar
to those obtained on the training data, but the dif-
ference between RUN2 and RUN3 is even smaller.
Our systems were ranked #3 (RUN 3), #4 (RUN
2) and #6 (RUN 1) among 14 systems submitted
by 6 teams. Our systems achieved good correlation
scores for almost all similarity types, with the excep-
tion of author similarity, which is the worst ranked
in comparison with the rest of the systems.
6 Conclusions and Future Work
In this paper, we presented the systems submitted
to the *SEM 2013 shared task on Semantic Tex-
tual Similarity. We combined some simple heuris-
tics for each type of similarity, based on the appro-
priate metadata fields. The use of lineal regression
improved the results considerably across all types.
Our system fared well in the competition. We sub-
mitted three systems and the highest-ranked of these
achieved the third best results overall.
Acknowledgements
This work is partially funded by the PATHS
project (http://paths-project.eu) funded by the Eu-
ropean Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
no. 270082. Aitor Gonzalez-Agirre is supported by
a PhD grant from the Spanish Ministry of Education,
Culture and Sport (grant FPU12/06243).
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Proceed-
ings of the 12th conference of the European chapter of
the Association for Computational Linguistics (EACL-
2009), Athens, Greece.
Eneko Agirre, Montse Cuadros, German Rigau, and Aitor
Soroa. 2010. Exploring knowledge bases for sim-
ilarity. In Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC10). European Language Resources Associa-
tion (ELRA). ISBN: 2-9517408-6-7. Pages 373?377.?.
Nikolaos Aletras and Mark Stevenson. 2012. Computing
similarity between cultural heritage items using multi-
modal features. In Proceedings of the 6th Workshop
on Language Technology for Cultural Heritage, So-
cial Sciences, and Humanities, pages 85?93, Avignon,
France.
Nikolaos Aletras, Mark Stevenson, and Paul Clough.
2012. Computing similarity between items in a digi-
tal library of cultural heritage. J. Comput. Cult. Herit.,
5(4):16:1?16:19, December.
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern In-
formation Retrieval. Addison Wesley Longman Lim-
ited, Essex.
136
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022, March.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 363?370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
D. Milne and I. Witten. 2008. Learning to Link
with Wikipedia. In Proceedings of the ACM Con-
ference on Information and Knowledge Management
(CIKM?2008), Napa Valley, California.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia?s link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 173?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
137
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81?91,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 10: Multilingual Semantic Textual Similarity
Eneko Agirre
a
, Carmen Banea
b?
, Claire Cardie
c
, Daniel Cer
d
, Mona Diab
e?
Aitor Gonzalez-Agirre
a
, Weiwei Guo
f
, Rada Mihalcea
b
, German Rigau
a
, Janyce Wiebe
g
a
University of the Basque Country
Basque Country, Spain
b
University of Michigan
Ann Arbor, MI
c
Cornell University
Ithaca, NY
d
Google Inc.
Mountain View, CA
e
George Washington University
Washington, DC
f
Columbia University
New York, NY
g
University of Pittsburgh
Pittsburgh, PA
Abstract
In Semantic Textual Similarity, systems
rate the degree of semantic equivalence
between two text snippets. This year,
the participants were challenged with new
data sets for English, as well as the in-
troduction of Spanish, as a new language
in which to assess semantic similarity.
For the English subtask, we exposed the
systems to a diversity of testing scenar-
ios, by preparing additional OntoNotes-
WordNet sense mappings and news head-
lines, as well as introducing new gen-
res, including image descriptions, DEFT
discussion forums, DEFT newswire, and
tweet-newswire headline mappings. For
Spanish, since, to our knowledge, this is
the first time that official evaluations are
conducted, we used well-formed text, by
featuring sentences extracted from ency-
clopedic content and newswire. The an-
notations for both tasks leveraged crowd-
sourcing. The Spanish subtask engaged 9
teams participating with 22 system runs,
and the English subtask attracted 15 teams
with 38 system runs.
1 Introduction and motivation
Given two snippets of text, Semantic Textual Sim-
ilarity (STS) captures the notion that some texts
are more similar than others, measuring their de-
gree of semantic equivalence. Textual similar-
ity can range from complete unrelatedness to ex-
act semantic equivalence, and a graded similar-
ity intuitively captures the notion of intermediate
?
carmennb@umich.edu, mtdiab@gwu.edu
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shades of similarity, as pairs of text may differ
from some minor nuanced aspects of meaning, to
relatively important semantic differences, to shar-
ing only some details, or to simply being related
to the same topic (cf. Section 2).
One of the goals of the STS task is to create a
unified framework for combining several seman-
tic components that otherwise have historically
tended to be evaluated independently and with-
out characterization of impact on NLP applica-
tions. By providing such a framework, STS al-
lows for an extrinsic evaluation of these modules.
Moreover, such an STS framework itself could in
turn be evaluated intrinsically and extrinsically as
a grey/black box within various NLP applications
such as Machine Translation (MT), Summariza-
tion, Generation, Question Answering (QA), etc.
STS is related to both Textual Entailment (TE)
and Paraphrasing, but differs in a number of ways
and it is more directly applicable to a number of
NLP tasks. STS is different from TE inasmuch
as it assumes bidirectional graded equivalence be-
tween the pair of textual snippets. In the case of
TE the equivalence is directional, e.g. a car is a
vehicle, but a vehicle is not necessarily a car. STS
also differs from both TE and Paraphrasing (in as
far as both tasks have been defined to date in the
literature) in that, rather than being a binary yes/no
decision (e.g. a vehicle is not a car), we define
STS to be a graded similarity notion (e.g. a ve-
hicle and a car are more similar than a wave and
a car). A quantifiable graded bidirectional notion
of textual similarity is useful for a myriad of NLP
tasks such as MT evaluation, information extrac-
tion, question answering, summarization, etc.
In 2012 we held the first pilot task at SemEval
2012, as part of the *SEM 2012 conference, with
great success: 35 teams participated with 88 sys-
tem runs (Agirre et al., 2012). In addition, we held
81
year dataset pairs source
2012 MSRpar 1500 newswire
2012 MSRvid 1500 videos
2012 OnWN 750 glosses
2012 SMTnews 750 MT eval.
2012 SMTeuroparl 750 MT eval.
2013 HDL 750 newswire
2013 FNWN 189 glosses
2013 OnWN 561 glosses
2013 SMT 750 MT eval.
2014 HDL 750 newswire headlines
2014 OnWN 750 glosses
2014 Deft-forum 450 forum posts
2014 Deft-news 300 news summary
2014 Images 750 image descriptions
2014 Tweet-news 750 tweet-news pairs
Table 2: English subtask: Summary of train (2012
and 2013) and test (2014) datasets.
a DARPA sponsored workshop at Columbia Uni-
versity.
1
In 2013, STS was selected as the offi-
cial Shared Task of the *SEM 2013 conference,
with two subtasks: The Core task, which is sim-
ilar to the 2012 task; and a Pilot task on Typed-
similarity between semi-structured records. The
Core task attracted 34 participants with 89 runs,
and the Typed-similarity task attracted 6 teams
with 14 runs.
For STS 2014 we defined two subtasks: En-
glish and Spanish. For the English subtask we pro-
vided five test datasets: two datasets that extend
already released genres (the OntoNotes-WordNet
sense mappings and news headlines) and three
new genres: image descriptions, DEFT discus-
sion forum data and newswire, as well as tweet-
newswire headline mappings. Participants could
use all datasets released in 2012 and 2013 as train-
ing data. The Spanish subtask introduced two di-
verse datasets on different genres, namely ency-
clopedic descriptions extracted from the Spanish
Wikipedia and contemporary Spanish newswire.
For the Spanish subtask, the participants had ac-
cess to a limited amount of labeled data, consist-
ing of 65 sentence pairs, which they could use for
training.
2 Task Description
2.1 English Subtask
The English dataset comprises pairs of news head-
lines (HDL), pairs of glosses (OnWN), image de-
scriptions (Images), DEFT-related discussion fo-
rums (Deft-forum) and news (Deft-news), and
1
http://www.cs.columbia.edu/
?
weiwei/
workshop/
tweet comments and newswire headline mappings
(Tweets).
For HDL, we used naturally occurring news
headlines gathered by the Europe Media Moni-
tor (EMM) engine (Best et al., 2005) from sev-
eral different news sources. EMM clusters to-
gether related news. Our goal was to generate
a balanced data set across the different similar-
ity ranges, hence we built two sets of headline
pairs: (i) a set where the pairs come from the same
EMM cluster, (ii) and another set where the head-
lines come from a different EMM cluster, then
we computed the string similarity between those
pairs. Accordingly, we sampled 375 headline pairs
of headlines that occur in the same EMM cluster,
aiming for pairs equally distributed between min-
imal and maximal similarity using simple string
similarity. We sampled other 375 pairs from the
different EMM cluster in the same manner.
For OnWN, we used the sense definition pairs
of OntoNotes (Hovy et al., 2006) and WordNet
(Fellbaum, 1998). Different from previous tasks,
the two definition sentences in a pair belong to dif-
ferent senses. We sampled 750 pairs based on a
string similarity ranging from 0.5 to 1.
The Images data set is a subset of the PAS-
CAL VOC-2008 data set (Rashtchian et al., 2010),
which consists of 1,000 images and has been used
by a number of image description systems. It was
also sampled from string similarity values between
0.6 and 1.
Deft-forum and Deft-news are from DEFT
data.
2
Deft-forum contains the forum post sen-
tences, and Deft-news are news summaries. We
selected 450 pairs for Deft-forum and 300 pairs for
Deft-news. They are sampled evenly from string
similarities falling in the interval 0.6 to 1.
The Tweets data set contains tweet-news pairs
selected from the corpus released in (Guo et al.,
2013), where each pair contains a sentence that
pertains to the news title, while the other one rep-
resents a Twitter comment on that particular news.
They are evenly sampled from string similarity
values between 0.5 and 1.
Table 1 shows the explanations and values as-
sociated with each score between 5 and 0. As
in prior years, we used Amazon Mechanical Turk
(AMT)
3
to crowdsource the annotation of the En-
glish pairs.
4
Annotators are presented with the
2
LDC2013E19, LDC2012E54
3
www.mturk.com
4
For STS 2013, we used CrowdFlower as a front-end to
82
Score English Spanish
5/4 The two sentences are completely equivalent, as they mean the same thing.
The bird is bathing in the sink.
Birdie is washing itself in the water basin.
El p?ajaro se esta ba?nando en el lavabo.
El p?ajaro se est?a lavando en el aguamanil.
4 The two sentences are mostly equivalent, but some unimportant details differ.
In May 2010, the troops attempted to invade
Kabul.
The US army invaded Kabul on May 7th last
year, 2010.
3 The two sentences are roughly equivalent, but some important information differs/missing.
John said he is considered a witness but not a
suspect.
?He is not a suspect anymore.? John said.
John dijo que ?el es considerado como testigo, y
no como sospechoso.
?
?
El ya no es un sospechoso,? John dijo.
2 The two sentences are not equivalent, but share some details.
They flew out of the nest in groups.
They flew into the nest together.
Ellos volaron del nido en grupos.
Volaron hacia el nido juntos.
1 The two sentences are not equivalent, but are on the same topic.
The woman is playing the violin.
The young lady enjoys listening to the guitar.
La mujer est?a tocando el viol??n.
La joven disfruta escuchar la guitarra.
0 The two sentences are completely dissimilar.
John went horse back riding at dawn with a
whole group of friends.
Sunrise at dawn is a magnificent view to take
in if you wake up early enough for it.
Al amanecer, Juan se fue a montar a caballo
con un grupo de amigos.
La salida del sol al amanecer es una magn??fica
vista que puede presenciar si usted se despierta
lo suficientemente temprano para verla.
Table 1: Similarity scores with explanations and examples for the English and Spanish subtasks, where
the sentences in Spanish are translations of the English ones.
A similarity score of 5 in English is mirrored by a maximum score of 4 in Spanish; the definitions pertaining to scores 3 and 4
in English were collapsed under a score of 3 in Spanish, with the definition ?The two sentences are mostly equivalent, but some
details differ.?
detailed instructions provided in Figure 1, and
are asked to label each STS sentence pair on our
six point scale, selecting from a dropdown box.
Five sentence pairs are presented to each annota-
tor at once, per human intelligence task (HIT), at
a payrate of $0.20; we collect five separate anno-
tations per sentence pair. Annotators were only el-
igible to work on the task if they had the Mechan-
ical Turk Master Qualification. This is a special
Amazon Mechanical Turk, since it provides numerous useful
tools to assist in running a successful annotation project using
crowdsourcing, such as support for hidden ?golden? questions
that can be used both to train annotators and to automatically
stop people who repeatedly make mistakes from contribut-
ing to the task. However, in 2013, CrowdFlower dropped
Amazon Mechanical Turk as an annotation source. When we
tried running pairs for STS 2014 on CrowdFlower using the
same templates that were successfully used for the 2013 task,
we found that we obtained significantly degraded annotation
quality, with an average Pearson (AMT provider vs. rest of
AMT providers) of only 22.8%. In contrast, when we ran the
task for 2014 on AMT, we obtained a one-vs-rest annotation
of 73.6%.
qualification conferred by AMT (using a priority
statistical model) to annotators who consistently
maintain a very high level of quality across a vari-
ety of tasks from numerous requesters). Access to
these skilled workers entails a 20% surcharge.
To monitor the quality of the annotations, we
use the gold dataset of 105 pairs that were manu-
ally annotated by the task organizers during STS
2013. We include one of these gold pairs in each
set of five sentence pairs, where the gold pairs are
indistinguishable from the rest. Unlike when we
ran on CrowdFlower for STS 2013, the gold pairs
are not used for training purposes, nor are workers
automatically banned from the task if they make
too many mistakes on annotating them. Rather, the
gold pairs are only used to help in identifying and
removing the data associated with poorly perform-
ing annotators. With few exceptions, 90% of the
answers from each individual annotator fall within
+/-1 of the answers selected by the organizers for
83
Figure 1: Annotation instructions for English subtask.
the gold dataset.
The distribution of scores obtained from the
AMT providers in the Deft-forum, Deft-news,
OnWN and tweet-news datasets is roughly uni-
form across the different grades of similarity, al-
though the scores are slightly higher for tweet-
news. Compared to the other data sets, the scores
for OnWN, were more bimodal, ranging between
4.6 to 5 and 0 to 0.4, when compared to middle
values (2.6-3.4).
In order to assess the annotation quality, we
measure the correlation of each annotator with the
average of the rest of the annotators, and then aver-
age the results. This approach to estimate the qual-
ity is identical to the method used for evaluations
(see Section 3), and it can thus be considered as
the upper bound of the systems. The inter-tagger
correlation for each English dataset is as follows:
? HDL: 79.4%
? OnWN: 67.2%
? Deft-forum: 58.6%
? Deft-news: 70.7%
? Images: 83.6%
? Tweets-news: 74.4%
The correlation figures are generally high (over
70%), with the exception of the OnWN and Deft
datasets, which score 67.2% and 58.6%, respec-
tively. The reason for the low inter-tagger correla-
tion on OnWN compared to the higher correlations
in previous years is that we only used unmapped
sense definitions, i.e., the two sentences in a pair
belong to two different senses. For the Deft-forum
dataset, we found that similarity values tend to be
lower than in the other datasets, and more annota-
tion disagreements happen in these low similarity
values.
2.2 Spanish Subtask
The Spanish subtask follows a setup similar to the
English subtask, except that the similarity scores
were adapted to fit a range from 0 to 4 (see Table
1). We thought that the distinction between a score
of 3 and 4 for the English task will pose more dif-
ficulty for us in conveying into Spanish, as the sole
difference between the two lies in how the annota-
tors perceive the importance of additional details
or missing information with respect to the core se-
mantic interpretation of the pair. As this aspect en-
tails a subjective judgement, and since it is the first
time that a Spanish STS evaluation is organized,
we casted the annotation guidelines into straight-
forward and unambiguous instructions, and thus
opted to use a similarity range from 0 to 4.
Prior to the evaluation window, we released 65
Spanish sentence pairs for trial / training. In or-
der to evaluate system performance under differ-
84
ent scenarios, we developed two test datasets, one
extracted from the Spanish Wikipedia
5
(December
2013 dump) and one from contemporary news ar-
ticles collected from media in Spanish (February
2014).
2.2.1 Spanish Wikipedia
The Wikipedia dump was processed using the
Parse::MediaWikiDump Perl library. We removed
all titles, html tags, wiki tags and hyperlinks
(keeping only the surface forms). Each article was
split into paragraphs, where the first paragraph
was considered to be the article?s abstract, while
the remaining ones were deemed to be its content.
Each of these were split into sentences using the
Perl library Uplug::PreProcess::SentDetect, and
only the sentences longer than eight words were
used. We iteratively computed the lexical simi-
larity
6
between every sentence in the abstract and
every sentence in the content, and retained those
pairs whose sentence length ratio was higher than
0.5, and their similarity scored over 0.35.
The final set of sentence pairs was split into five
bins, and their scores normalized to range from
0 to 1. The more interesting and difficult pairs
were found, perhaps not surprisingly, in bins 0 and
1, where synonyms/short paraphrases where more
frequent. An example extracted from those bins,
where the text in italics highlights the differences
between the two sentences:
? ?America? es el segundo continente m?as
grande del planeta, despu?es de Asia.
?America? is the second largest continent in the world,
following Asia.
? America corresponde a la segunda masa de
tierra m?as grande del planeta, luego de Asia.
America is the second largest land mass on the planet,
after Asia.
The Spanish verb ?Es? maps to (En:
7
is), ?cor-
responde a? (En: corresponds to), the phrase ?el
segundo continente? (En: the second continent) is
equivalent to ?la segunda masa de tierra? (the sec-
ond land mass), and ?despues? (En: following) to
?luego? (En: after). Despite the difference in vo-
cabulary choice, the two sentences are paraphrases
of each other.
From the candidate pairs, we manually selected
324 sentence pairs, in order to ensure a diverse
5
es.wikipedia.org
6
Algorithm based on the Linux diff command (Algo-
rithm::Diff Perl module).
7
?En? stands for English.
and challenging set. This set was annotated in two
ways, first by two graduate students in Computer
Science who are native speakers of Spanish, and
second by using AMT.
The AMT framework was set up to contain
seven sentence pairs per HIT, where six of them
were part of the test dataset, while one was used
for control. AMT providers were eligible to com-
plete a task if they had more than 500 accepted
HITs, with 90%+ acceptance rate.
8
We paid $0.30
per HIT, and each HIT was annotated by five AMT
providers. We sought to ensure that only Spanish
speaking annotators would complete the HITs by
providing all the information related to the task (its
title, abstract, description, guidelines and exam-
ples), as well as the control pair in Spanish only.
The participants were instructed to label the pairs
on a scale from 0 to 4 (see Table 1). Each sentence
pair was followed by a comment text box, which
the AMT providers used to provide the topic of the
sentences, corrections, etc.
The two students achieved a Pearson correla-
tion of 0.6974 on the Wikipedia dataset. To see
how their judgement compares to the crowd wis-
dom, we averaged the AMT scores for each pair,
and computed their correlation with our annota-
tors, obtaining 0.824 and 0.742, respectively. Sur-
prisingly enough, both these correlation values are
higher than the correlation among the annotators
themselves. When averaging the annotator scores
and comparing them with the AMT providers?
average score per pair, the correlation becomes
0.8546, indicating that the task is well defined,
and that the annotations contributed by the AMT
providers are of satisfactory quality. Given these
scores, the gold standard was annotated using the
average AMT provider judgement per pair.
2.2.2 Spanish News
The second Spanish dataset was extracted from
news articles published in Spanish language me-
dia from around the world in February 2014. The
hyperlinks to the articles were obtained by pars-
ing the ?International? page of Spanish Google
News,
9
which aggregates or clusters in real time
articles describing a particular event from a di-
verse pool of news sites, where each grouping
8
Initially, Amazon had automatically upgraded our anno-
tation task to require Master level providers (as those partici-
pating in the English annotations), yet after approximately 4
days, no HIT had been completed.
9
news.google.es
85
is labeled with the title of one of the predomi-
nant articles. By leveraging these clusters of links
pointing to the sites where the articles were orig-
inally published, we are able to gather raw text
that has a high probability of containing seman-
tically similar sentences. We encountered several
difficulties while mining the articles, ranging from
each article having its own formatting depend-
ing on the source site, to advertisements, cookie
requirements, to encoding for Spanish diacritics.
We used the lynx text-based browser,
10
which was
able to standardize the raw articles to a degree.
The output of the browser was processed using a
rule based approach taking into account continu-
ous text span length, ratio of symbols and num-
bers to the text, etc., in order to determine when
a paragraph is part of the article content. After
that, a second pass over the predictions corrected
mislabeled paragraphs if they were preceded and
followed by paragraphs identified as content. All
the content pertaining to articles on the same event
was joined, sentence split, and diff pairwise simi-
larities were computed. The set of candidate sen-
tences followed the same requirements as for the
Wikipedia dataset, namely length ratio higher than
0.5 and similarity score over 0.35. From these, we
manually extracted 480 sentence pairs which were
deemed to pose a challenge to an automated sys-
tem.
Due to the high correlations obtained between
the AMT providers? scores and the annotators?
scores on Wikipedia, the news dataset was only
annotated using AMT, following exactly the same
task setup as for Wikipedia.
3 Evaluation
Evaluation of STS is still an open issue.
STS experiments have traditionally used Pearson
product-moment correlation between the system
scores and the GS scores, or, alternatively, Spear-
man rank order correlation. In addition, we also
need a method to aggregate the results from each
dataset into an overall score. The analysis per-
formed in (Agirre and Amig?o, In prep) shows that
Pearson and averaging across datasets are the best
suited combination in general. In particular, Pear-
son is more informative than Spearman, in that
Spearman only takes the rank differences into ac-
count, while Pearson does account for value dif-
ferences as well. The study also showed that other
10
lynx.browser.org
alternatives need to be considered, depending on
the requirements of the target application.
We leave application-dependent evaluations for
future work, and focus on average Pearson correla-
tion. When averaging, we weight each individual
correlation by the size of the dataset. In order to
compute statistical significance among system re-
sults, we use a one-tailed parametric test based on
Fisher?s z-transformation (Press et al., 2002, equa-
tion 14.5.10). In addition, English subtask partic-
ipants could provide an optional confidence mea-
sure between 0 and 100 for each of their predic-
tions. Team RTM-DCU is the only one who has
provided these, and the evaluation of their runs us-
ing weighted Pearson (Pozzi et al., 2012) is listed
at the end of Table 3.
Participants
11
could take part in the shared task
with a maximum of 3 system runs per subtask.
3.1 English Subtask
In order to provide a simple word overlap baseline
(Baseline-tokencos), we tokenize the input sen-
tences splitting on white spaces, and then repre-
sent each sentence as a vector in the multidimen-
sional token space. Each dimension has 1 if the to-
ken is present in the sentence, 0 otherwise. Vector
similarity is computed using the cosine similarity
metric.
We also run the freely available system, Take-
Lab (
?
Sari?c et al., 2012), which yielded state of the
art performance in STS 2012 and strong results
out-of-the-box in 2013.
12
15 teams participated in the English subtask,
submitting 38 system runs. One team submitted
the results past the deadline, as explicitly marked
in Table 3. After the submission deadline expired,
the organizers published the gold standard and par-
ticipant submissions on the task website, in order
to ensure a transparent evaluation process.
Table 3 shows the results of the English sub-
task, with runs listed in alphabetical order. The
correlation in each dataset is given, followed
11
Participating teams: Bielefeld SC (McCrae et al.,
2013), BUAP (Vilari?no et al., 2014), DLS@CU (Sultan et
al., 2014b), FBK-TR (Vo et al., 2014), IBM EG (no in-
formation), LIPN (Buscaldi et al., 2014), Meerkat Mafia
(Kashyap et al., 2014), NTNU (Lynum et al., 2014), RTM-
DCU (Bic?ici and Way, 2014), SemantiKLUE (Proisi et al.,
2014), StanfordNLP (Socher et al., 2014), TeamZ (Gupta,
2014), UMCC DLSI SemSim (Chavez et al., 2014), UNAL-
NLP (Jimenez et al., 2014), UNED (Martinez-Romo et al.,
2011), UoW (Rios, 2014).
12
Code is available at http://ixa2.si.ehu.es/
stswiki
86
Run Name deft deft Headl images OnWN tweet Weighted mean Rank
forum news news
Baseline-tokencos 0.353 0.596 0.510 0.513 0.406 0.654 0.507 -
TakeLab 0.333 0.716 0.720 0.742 0.793 0.650 0.678 -
Bielefeld SC-run1 0.211 0.432 0.321 0.368 0.367 0.415 0.354 32
Bielefeld SC-run2 0.211 0.431 0.311 0.356 0.361 0.409 0.347 33
BUAP-EN-run1 0.456 0.686 0.689 0.697 0.654 0.771 0.671 19
DLS@CU-run1 0.483 0.766 0.765 0.821 0.723 0.764 0.734 7
DLS@CU-run2 0.483 0.766 0.765 0.821 0.859 0.764 0.761 1
FBK-TR-run1 0.322 0.523 0.547 0.601 0.661 0.462 0.535 25
FBK-TR-run2 0.167 0.421 0.485 0.521 0.572 0.359 0.441 28
FBK-TR-run3 0.305 0.405 0.471 0.489 0.551 0.438 0.459 27
IBM EG-run1 0.474 0.743 0.737 0.801 0.760 0.730 0.722 8
IBM EG-run2 0.464 0.641 0.710 0.747 0.732 0.696 0.684 15
LIPN-run1 0.454 0.640 0.653 0.809 - 0.551 0.508 26
LIPN-run2 0.084 - - - - - 0.010 35
Meerkat Mafia-Hulk 0.449 0.785 0.757 0.790 0.787 0.757 0.735 6
Meerkat Mafia-pairingWords 0.471 0.763 0.760 0.801 0.875 0.779 0.761 2
Meerkat Mafia-SuperSaiyan 0.492 0.771 0.767 0.768 0.802 0.765 0.741 5
NTNU-run1 0.437 0.714 0.722 0.800 0.835 0.411 0.663 20
NTNU-run2 0.508 0.766 0.753 0.813 0.777 0.792 0.749 4
NTNU-run3 0.531 0.781 0.784 0.834 0.850 0.675 0.755 3
SemantiKLUE-run1 0.337 0.608 0.728 0.783 0.848 0.632 0.687 14
SemantiKLUE-run2 0.349 0.643 0.733 0.773 0.855 0.640 0.694 13
StanfordNLP-run1 0.319 0.635 0.636 0.758 0.627 0.669 0.627 22
StanfordNLP-run2 0.304 0.679 0.621 0.715 0.625 0.636 0.610 24
StanfordNLP-run3 0.342 0.650 0.602 0.754 0.609 0.638 0.614 23
UMCC DLSI SemSim-run1 0.475 0.662 0.632 0.742 0.813 0.675 0.682 16
UMCC DLSI SemSim-run2 0.469 0.662 0.625 0.739 0.814 0.654 0.676 18
UMCC DLSI SemSim-run3 0.283 0.385 0.267 0.436 0.603 0.278 0.381 30
UNAL-NLP-run1 0.504 0.721 0.762 0.807 0.782 0.614 0.711 12
UNAL-NLP-run2 0.383 0.730 0.765 0.771 0.827 0.403 0.657 21
UNAL-NLP-run3 0.461 0.722 0.761 0.778 0.843 0.658 0.721 9
UNED-run22 p np 0.104 0.315 0.037 0.324 0.509 0.490 0.310 34
UNED-runS5K 10 np 0.118 0.506 0.057 0.498 0.488 0.579 0.379 31
UNED-runS5K 3 np 0.094 0.564 0.018 0.607 0.577 0.670 0.431 29
UoW-run1 0.342 0.751 0.754 0.776 0.799 0.737 0.714 11
UoW-run2 0.342 0.587 0.754 0.788 0.799 0.628 0.682 17
UoW-run3 0.342 0.763 0.754 0.788 0.799 0.753 0.721 10
?RTM-DCU-run1 0.434 0.697 0.620 0.699 0.806 0.688 0.671
?RTM-DCU-run2 0.397 0.681 0.613 0.666 0.799 0.669 0.651
?RTM-DCU-run3 0.308 0.556 0.630 0.647 0.800 0.553 0.608
?RTM-DCU-run1 0.418 0.685 0.622 0.698 0.833 0.687 0.673
?RTM-DCU-run2 0.383 0.674 0.609 0.663 0.826 0.669 0.653
?RTM-DCU-run3 0.273 0.553 0.633 0.644 0.825 0.568 0.611
Table 3: English evaluation results. Results at the top correspond to out-of-the-box systems. Results at
the bottom correspond to results using the confidence score.
Notes: ?-? for not submitted, ??? for post-deadline submission.
87
by the mean correlation (the official measure),
and the rank of the run. The highest correla-
tions are for OnWN (87.5%, by Meerkat Mafia)
and images (83.4%, by NTNU), followed by
Tweets (79.2%, by NTNU), HEADL (78.4%, by
NTNU) and deft news and forums (78.1% and
53.1%, respectively, by NTNU). Compared to the
inter-annotator agreement correlation, the ranking
among datasets is very similar, with the exception
of OnWN, as it gets the best score but has very low
agreement. One possible reason is that the partic-
ipants used previously available data. The results
of the best 4 top system runs are significantly dif-
ferent (p-value < 0.05) from the 5th top scoring
system run and below. The top 4 systems did not
show statistical significant variation among them.
Only three runs (cf. lower rows in Table 3) in-
cluded non-uniform confidence scores, barely af-
fecting their ranking.
Interestingly, the two top performing systems
on the English STS sub-task are both unsuper-
vised. DLS@CU (Sultan et al., 2014b) presents
an unsupervised algorithm which predicts the STS
score based on the proportion of word alignments
in the two sentences. Two related words are
aligned depending on how similar the two words
are, and also on how similar the contexts of the
words are in the respective sentences (Sultan et al.,
2014a). Meerkat Mafia pairingWords (Kashyap
et al., 2014) also follows a fully unsupervised ap-
proach. The authors train LSA on an English cor-
pus of three billion words using a sliding window
approach, resulting in a vocabulary size of 29,000
words associated with 300 dimensions. They ac-
count for named entities and out-of-vocabulary
words by leveraging external resources such as
DBpedia
13
and Wordnik.
14
In Spanish, the sys-
tem equivalent to this run ranked second following
a cross-lingual approach, by applying the English
system to the translated version of the dataset (see
3.2).
The Table also shows the results of TakeLab,
which was trained with all datasets from previ-
ous years. TakeLab would rank 18th, ten absolute
points below the best system, a smaller difference
than in 2013.
13
dbpedia.org
14
wordnick.com
3.2 Spanish Subtask
The Spanish subtask attracted 9 teams with 22
participating systems, out of which 16 were su-
pervised and 6 unsupervised. The participants
were from both Spanish (Colombia, Cuba, Mex-
ico, Spain), and non-Spanish speaking countries
(two teams from France, Germany, Ireland, UK,
US). The evaluation results appear in Table 4.
The top ranking system is the 2nd run of
UMCC DLSI SemSim (Chavez et al., 2014),
which achieves a weighted correlation of 0.807. It
entails a cross-lingual approach, as it leverages a
SVM-based English framework, by mapping the
Spanish words to their English equivalent using
the most common sense in WordNet 3.0. The clas-
sifier uses a combination of features, such as those
derived from traditional knowledge-based ((Lea-
cock and Chodorow, 1998; Wu and Palmer, 1994;
Lin, 1998), and others) and corpus-based metrics
(LSA (Landauer et al., 1997)), paired with lexi-
cal features (such as Dice-Similarity, Euclidean-
distance, etc.). It is trained on a cumulative En-
glish STS dataset comprising train and test data
released as part of tasks in SemEval2012 (Agirre
et al., 2012) and *Sem 2013 (Agirre et al., 2013),
as well as training data available from tasks 1 and
10 in SemEval 2014. Interestingly enough, run 2
of the system performs better than run 1, despite
the fact that it uses half the features, and focuses
on string based similarity measures only. This dif-
ference between runs is noticed on the Wikipedia
dataset only, and it amounts to 4% Pearson corre-
lation. While the system had a robust performance
on the Spanish subtask, for English, its overall
rank was 16, 18, and 33, respectively.
Coming in close at only 0.3% difference, is
Meerkat-Mafia PairingAvg (run 2) (Kashyap et
al., 2014), which also follows a cross-lingual ap-
proach, by applying the system the team devel-
oped for the English subtask to the translated ver-
sion of the datasets (see 3.1). The interesting as-
pect of their work is that in their first submission
(run 1), they only consider the similarity result-
ing from the sentence pair translation through the
Google Translate service.
15
In the second run,
they expand each sentence to 20 possible combi-
nations by accounting for the multiple translation
meanings of a given word, and considering the av-
erage similarity of all resulting pairs. While the
first run achieves a weighted correlation of 73.8%,
15
translate.google.com
88
Run Name System type Wikipedia News Weighted mean Rank
Bielefeld-SC-run1 unsupervised* 0.263 0.554 0.437 22
Bielefeld-SC-run2 unsupervised* 0.265 0.555 0.438 21
BUAP-run1 supervised 0.550 0.679 0.627 17
BUAP-run2 unsupervised 0.640 0.764 0.714 14
RTM-DCU-run1 supervised 0.422 0.700 0.588 18
RTM-DCU-run2 supervised 0.369 0.625 0.522 20
RTM-DCU-run3 supervised 0.424 0.641 0.554 19
LIPN-run1 supervised 0.652 0.826 0.756 11
LIPN-run2 supervised 0.716 0.832 0.785 6
LIPN-run3 supervised 0.716 0.809 0.771 10
Meerkat-Mafia-run1 unsupervised 0.668 0.785 0.738 13
Meerkat-Mafia-run2 unsupervised 0.743 0.845 0.804 2
Meerkat-Mafia-run3 supervised 0.738 0.822 0.788 5
TeamZ-run1 supervised 0.610 0.717 0.674 15
TeamZ-run2 supervised 0.604 0.710 0.667 16
UMCC-DLSI-run1 supervised 0.741 0.825 0.791 4
UMCC-DLSI-run2 supervised 0.7802 0.825 0.807 1
UNAL-NLP-run1 weakly supervised 0.7803 0.815 0.801 3
UNAL-NLP-run2 supervised 0.757 0.783 0.772 9
UNAL-NLP-run3 supervised 0.689 0.796 0.753 12
UoW-run1 supervised 0.748 0.800 0.779 7
UoW-run2 supervised 0.748 0.800 0.779 8
Table 4: Spanish evaluation results in terms of Pearson correlation.
the second one performs significantly better at
80.4%, indicating that the additional context may
also include multiple instances of accurate trans-
lations, hence significantly impacting the overall
similarity score. In English, the system equiva-
lent to run 2 in Spanish, namely Meerkat Mafia-
pairingWords, achieves a competitive ranked per-
formance across all six datasets, ranking second,
at an order of 10
?4
distance from the top sys-
tem. This supports the claim that, despite its unsu-
pervised nature, the system is quite versatile and
highly competitive with the top performing super-
vised frameworks, and that it may achieve an even
higher performance in Spanish if accurate sen-
tence translations were provided.
Overall, most systems were cross-lingual, rely-
ing on different translation approaches, such as 1)
translating the test data into English (as the two
systems above), and then exporting the score ob-
tained for the English sentences back to Spanish,
or 2) performing automatic translation of the En-
glish training data, and learning a classifier di-
rectly in Spanish. (Buscaldi et al., 2014) supple-
mented their training dataset with human annota-
tions conducted in Spanish, using definition pairs
extracted from a Spanish dictionary. A different
angle was explored by (Rios, 2014), who proposed
a multilingual framework using transfer learning
across English and Spanish by training on tradi-
tional lexical, knowledge-based and corpus-based
features. The semantic similarity task was ap-
proached from a monolingual perspective as well
(Gupta, 2014), by focusing on Spanish resources,
such as the trial data we released as part of the
subtask, and the Spanish WordNet;
16
these were
leveraged using meta-learning over variations of
overlap-based metrics. Following the same line,
(Bic?ici and Way, 2014) pursued language inde-
pendent methods, who avoided relying on task or
domain specific information through the usage of
referential translation machines. This approach
models textual semantic similarity as a decision in
terms of translation quality between two datasets
(in our case Spanish STS trial and test data) given
relevant examples from an in-language reference
corpus.
In comparison to the correlations obtained in the
English subtask, where the highest weighted mean
was 76.1%, for Spanish, we obtained 80.7%, prob-
ably due to the more formal nature of the datasets,
since Wikipedia and news articles employ mostly
well formed and grammatically correct sentences,
and we selected all snippets to be longer than 8
words. The overall correlation scores obtained for
English were hurt by the deft-forum data, which
scored significantly lower (at a maximum corre-
lation of 50.8%), when compared to all the other
datasets whose correlation was higher than 70%.
The OnWN data was most similar to our test sets,
and it attained a maximum of 85.9%.
16
grial.uab.es/descarregues.php
89
4 Conclusion
This year?s STS task comprised a multilingual
flair, by introducing Spanish datasets alongside the
English ones. In English, the datasets sought to ex-
pose the participating teams to more diverse sce-
narios compared to the previous years, by intro-
ducing image descriptions, forum and newswire
genre, and tweet-newswire headline mappings.
For Spanish, two datasets were developed consist-
ing of encyclopedic and newswire text acquired
from Spanish sources. Overall, the English sub-
task attracted 15 teams (with 38 system varia-
tions), while the Spanish subtask had 9 teams
(with 22 system runs). Most teams from the Span-
ish subtask have also submitted runs for the En-
glish evaluations.
Acknowledgments
The authors are grateful to Ver?onica P?erez-Rosas
and Vanessa Loza for their help with the anno-
tations for the Spanish subtask. This material is
based in part upon work supported by National
Science Foundation CAREER award #1361274
and IIS award #1018613, by DARPA-BAA-12-
47 DEFT grant #12475008, and by MINECO
CHIST-ERA READERS and SKATER projects
(PCIN-2013-002-C02-01, TIN2012-38584-C06-
02). Aitor Gonzalez Agirre is supported by a doc-
toral grant from MINECO. Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation or the Defense Advanced Re-
search Projects Agency.
References
Eneko Agirre and Enrique Amig?o. In prep. Exploring
evaluation measures for semantic textual similarity.
In Unpublished manuscript.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 385?
393, Montr?eal, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic textual similarity, including a pi-
lot on typed-similarity. In The Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM 2013), pages 32?43.
Clive Best, Erik van der Goot, Ken Blackler, Tefilo
Garcia, and David Horby. 2005. Europe me-
dia monitor - system description. In EUR Report
22173-En, Ispra, Italy.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le
Roux, Nadi Tomeh, and Belem Priego Sanchez.
2014. LIPN: Introducing a new geographical con-
text similarity measure and a statistical similarity
measure based on the Bhattacharyya coefficient. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Alexander Chavez, Hector Davila, Yoan Gutierrez,
Antonio Fernandez-Orquin, Andr?es Montoyo, and
Rafael Munoz. 2014. UMCC DLSI SemSim: Mul-
tilingual system for measuring semantic textual sim-
ilarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Christiane Fellbaum. 1998. WordNet - An electronic
lexical database. MIT Press.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013.
Linking tweets to news: A framework to enrich on-
line short text data in social media. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 239?249.
Anubhav Gupta. 2014. TeamZ: Measuring semantic
textual similarity for Spanish using an overlap-based
approach. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, pages 57?60.
Sergio Jimenez, George Due?nas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
cross-level semantic textual similarity systems. In
Proceedings of the 8th International Workshop on
90
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Thomas K. Landauer, Darrell Laham, Bob Rehder, and
M. E. Schreiner. 1997. How well can passage mean-
ing be derived without using word order? A compar-
ison of latent semantic analysis and humans. Cogni-
tive Science.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In WordNet: An Elec-
tronic Lexical Database, pages 305?332.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, Madison, Wisconsin.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinality. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Juan Martinez-Romo, Lourdes Araujo, Javier Borge-
Holthoefer, Alex Arenas, Jos?e A. Capit?an, and
Jos?e A. Cuesta. 2011. Disentangling categori-
cal relationships through a graph of co-occurrences.
Phys. Rev. E, 84:046108, Oct.
John P. McCrae, Philipp Cimiano, and Roman Klinger.
2013. Orthonormal explicit topic analysis for cross-
lingual document matching. In Proceedings of the
2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1732?1740, Seattle,
Washington, USA.
Francesco Pozzi, Tiziana Di Matteo, and Tomaso Aste.
2012. Exponential smoothing weighted correla-
tions. The European Physical Journal B, 85(6).
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 2002. Numerical
recipes: The art of scientific computing V 2.10 with
Linux or single-screen license. Cambridge Univer-
sity Press.
Thomas Proisi, Stefan Evert, Paul Greiner, and Besim
Kabashi. 2014. SemantiKLUE: Robust semantic
similarity at multiple levels using maximum weight
matching. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using Amazon?s Mechanical Turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, CSLDAMT ?10, pages 139?147, Strouds-
burg, PA, USA.
Miguel Rios. 2014. UoW: Multi-task learning Gaus-
sian process for semantic textual similarity. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
pages 207?218.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014a. Back to basics for monolingual align-
ment: Exploiting word similarity and contextual ev-
idence. Transactions of the Association for Compu-
tational Linguistics, 2:219?230.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014b. DLS@CU: Sentence similarity from
word aligment. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Darnes Vilari?no, David Pinto, Sa?ul Le?on, Mireya To-
var, and Beatriz Beltr?an. 2014. BUAP: Evaluating
features for multilingual and cross-level semantic
textual similarity. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Ngoc Phuoc An Vo, Tommaso Caselli, and Octavian
Popescu. 2014. FBK-TR: Applying SVM with
multiple linguistic features for cross-level semantic
similarity. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Frane
?
Sari?c, Goran Glava?s, Mladen Karan, Jan
?
Snajder,
and Bojana Dalbelo Ba?si?c. 2012. Takelab: Sys-
tems for measuring semantic text similarity. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441?448,
Montr?eal, Canada, 7-8 June.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico.
91
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 833?837,
Dublin, Ireland, August 23-24, 2014.
V3: Unsupervised Generation of Domain Aspect Terms for
Aspect Based Sentiment Analysis
Aitor Garc??a-Pablos,
Montse Cuadros, Se
?
an Gaines
Vicomtech-IK4 research centre
Mikeletegi 57, San Sebastian, Spain
{agarciap,mcuadros}@vicomtech.org
German Rigau
IXA Group
Euskal Herriko Unibertsitatea,
San Sebastian, Spain
german.rigau@ehu.es
Abstract
This paper presents V3, an unsupervised
system for aspect-based Sentiment Analy-
sis when evaluated on the SemEval 2014
Task 4. V3 focuses on generating a list
of aspect terms for a new domain using a
collection of raw texts from the domain.
We also implement a very basic approach
to classify the aspect terms into categories
and assign polarities to them.
1 Introduction
The automatic analysis of opinions, within the
framework of opinion mining or sentiment anal-
ysis, has gained a huge importance during the last
decade due to the amount of review web sites,
blogs and social networks producing everyday a
massive amount of new content (Pang and Lee,
2008; Liu, 2012; Zhang and Liu, 2014). This con-
tent usually contains opinions about different enti-
ties, products or services. Trying to cope with this
large amounts of textual data is unfeasible with-
out the help of automatic Opinion Mining tools
which try to detect, identify, classify, aggregate
and summarize the opinions expressed about dif-
ferent topics (Hu and Liu, 2004) (Popescu and Et-
zioni, 2005) (Wu et al., 2009) (Zhang et al., 2010).
In this framework, aspect based opinion mining
systems aim to detect the sentiment at ?aspect?
level (i.e. the precise feature being opinionated in
a clause or sentence).
In this paper we describe our system presented
in the SemEval 2014 task 4
1
Aspect Based Senti-
ment Analysis (Pontiki et al., 2014), which focuses
on detecting opinionated aspect terms (e.g. wine
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
1
http://alt.qcri.org/semeval2014/
task4/
list and menu in restaurant domain, and hard disk
and battery life in laptop domain), their categories
and polarities in customer review sentences.
The task provides two training datasets, one of
restaurant reviews and other of laptop reviews.
The restaurant review dataset consists of over
3,000 English sentences from restaurant reviews
borrowed from (Ganu et al., 2009). The laptop
review dataset consists of over 3,000 English sen-
tences extracted from customer reviews. The task
is divided in four different subtasks: subtask 1 as-
pect term extraction, subtask 2 aspect term polar-
ity detection, subtask 3 aspect category detection,
subtask 4 aspect category polarity detection. Our
system mainly focused on subtask 1, but we have
also participated in the other subtasks.
The paper is organized as follows: section 2
presents our approach, section 3 details the im-
provement methods used for the aspects term se-
lection and section 4 focus on category and polar-
ity tagging. Finally section 5 presents the results
obtained and section 6 draws the conclusions and
future work.
2 Our approach
We have adapted the double-propagation tech-
nique described in (Qiu et al., 2009; Qiu et al.,
2011). This method consists of using a minimal
seed list of aspect terms and opinion words and
propagate them through an unlabelled domain-
related corpus using a set of propagation rules.
The goal is to obtain an extended aspect term and
opinion word lists. (Qiu et al., 2009) define opin-
ion words as words that convey some positive or
negative sentiment polarities. They only extract
nouns as aspect terms and adjectives as opinion
words, and we assume the same restriction.
The propagation rules have the form of depen-
dency relations and some part-of-speech restric-
tions. Some rules extract new aspect terms, and
others extract new opinion words. Table 1 shows
833
the rules used in our approach, similar to those de-
tailed in (Qiu et al., 2011) with some modifica-
tions. In this table, T stands for aspect term (i.e.
a word already in the aspect terms set) and O for
opinion word (i.e. a word already in the opinion
words set). W means any word. The dependency
types used are amod, dobj, subj and conj, which
stand for adjectival modifier, direct object, subject
and conjunction respectively. Additional restric-
tions on the Part-Of-Speech (POS) of the words
present in the rule are shown in the third column
of the table. The last column indicates to which
set (aspect terms or opinion words) the new word
is added.
To obtain the dependency trees and word lem-
mas and POS tags, we use the Stanford NLP tools
2
(De Marneffe et al., 2006). Our initial seed words
are just the adjectives good and bad, which are
added to the initial opinion words set. The ini-
tial aspect terms set starts empty. Each sentence
in the dataset is analysed to obtain its dependency
tree and the rules are checked sequentially. If rule
is triggered, then the word indicated by that rule
is added to the corresponding set (aspect terms
or opinion words, depending on the rule). These
new words can be then used to trigger the propa-
gation rules later. After the last sentence the pro-
cess starts from the beginning to check the rules
with the newly added words. The process stops
when no more words have been added during a
full dataset iteration.
3 Selecting aspect term candidates
The double-propagation process populates both
sets of domain aspect terms and domain opinion
words, but we focus our attention in the aspect
terms set. Due to the nature of the process it tends
to generate hundreds of different potential aspect
terms, many of them being incorrect. We apply
some additional processes to improve the list.
3.1 Ranking the aspect terms
One way to reduce the undesired terms is to rank
them, pushing the incorrect aspect terms to the
bottom of the list and using only a certain subset
of top ranked terms. In order to rank this list we
have modelled the double-propagation process as
a undirected graph population process. Each new
aspect term or opinion word discovered by apply-
2
http://nlp.stanford.edu/software/
lex-parser.shtml
ing a propagation rule is added as a vertex to the
graph. The rule used to extract the new word is
added as an edge to the graph, connecting the orig-
inal word and the newly discovered word.
We have applied the well-known PageRank al-
gorithm (Brin and Page, 1998) to score the vertices
of the graph. To calculate the PageRank scores
we have used the JUNG framework
3
(OMadad-
hain et al., 2005), a set of Java libraries to work
with graphs. The value of the alpha parameter that
represents the probability of a random jump to any
node of the graph has been left at 0.15 (in the lit-
erature it is recommended an alpha value between
0.1 and 0.2). The aspect terms are then ordered us-
ing their associated score, being the most relevant
aspect term, the one with the highest score. Then
the list can be trimmed to a certain amount of top
ranked terms, trying to balance the precision and
recall of the resulting list.
3.2 Filtering undesired words
The double-propagation method always intro-
duces many undesired words. Some of these un-
desired words appear very frequently and are com-
bined with a large number of words. So, they tend
to also appear in high positions in the ranking.
Many of these words are easy to identify, and they
are not likely to be useful aspect terms in any do-
main. Examples of these words are: nothing, ev-
erything, thing, anyone, someone, somebody, etc.
In this work we use a domain agnostic stop word
list to deal with this kind of words. The authors
of the original double-propagation approach use
some clause and frequency based heuristics that
we do not employ here.
3.3 Detecting multiword terms
Many aspect terms are not just single words, but
compounds and multiword terms (e.g. wine list,
hard disk drive, battery life, etc.). In the origi-
nal double-propagation paper, the authors consider
adjacent nouns to a given aspect term as multiword
terms and perform an a posteriori pruning based
on the frequency of the combination. We have
tried to add multiword terms without increasing
the amount of noise in the resulting list. One of the
approaches included in the system exploits Word-
Net
4
(Fellbaum, 1999), and the following simple
rules:
3
http://jung.sourceforge.net
4
http://wordnet.princeton.edu/
834
Rule Observations Constraints Action
R11 O? amod?W W is a noun W?T
R12 O?dobj?W1?subj?W2 W2 is a noun W2?T
R21 T? amod?W W is an adjective W?O
R22 T? subj?W1? dobj?W2 W2 is an adjective W2? O
R31 T? conj?W W is a noun W? T
R32 T? subj?W1? dobj?W2 W2 is a noun W? T
R41 O? conj?W W is an adjective W? O
R42 O? Dep1?W1? Dep2?W2 Dep1==Dep2, W2 is an adjective W2? O
Table 1: Propagation rules.
? If word N and word N+1 are nouns, and the
combination is an entry in WordNet (or in
Wikipedia, see below). E.g.: battery life
? If word N is an adjective and word N+1 is
a noun, and the combination is an entry in
WordNet. E.g.: hot dog, happy hour
? If word N is an adjective, word N+1 is a noun,
and word n is a relational adjective in Word-
Net (lexical file 01). E.g.: Thai food
In order to improve the coverage of the Word-
Net approach, we also check if a combination of
two consecutive nouns appears as a Wikipedia ar-
ticle title. Wikipedia articles refer to real word
concepts and entities, so if a combination of words
is a title of a Wikipedia article it is very likely
that this word combination is also meaningful (e.g.
DVD player, USB port, goat cheese, pepperoni
pizza). We limit the lookup in Wikipedia titles just
to combination of nouns to avoid the inclusion of
incorrect aspect terms.
4 Assigning categories and polarities
Despite we have focused our attention on acquir-
ing aspect terms from a domain, we have also par-
ticipated in the rest of subtasks: grouping aspect
terms into a fixed set of categories, and assigning
polarities to both aspect terms and categories.
To group the aspect terms into categories, we
have employed WordNet similarities. The idea
is to compare the detected aspect terms against a
term or group of terms representative of the tar-
get categories. In this case the categories (only
for restaurants) were food, service, price, ambi-
ence and anecdotes/miscellaneous.
Initially, the representative word for each cate-
gory (except for the anecdotes/miscellaneous) was
the name of the category itself. We use the similar-
ity measure described by (Wu and Palmer, 1994).
Detected aspect terms are compared to the set of
representative words on each category, and they
are assigned to the category with a higher similar-
ity result. For example using this approach, the
similarity between food and cheese is 0.8, while
similarity between service and cheese is 0.25, and
between price and cheese is 0.266. Thus, in this
case cheese is assigned to the category food.
If the similarity does not surpass a given min-
imum threshold (manually set to 0.7), the current
aspect term is not assigned to the category to avoid
assigning a wrong category just because the other
were even less similar. After classifying the as-
pect terms of a given sentence into categories, we
assign those categories to the sentence. If no cat-
egory has been assigned, then we use the anec-
dotes/miscellaneous category as the default one.
This approach is quite naive and it has many
limitations. It works quite well for the category
food, classifying ingredients and meals, but it fails
when the category or the aspect terms are more
vague or abstract. In addition, we do not perform
any kind of word sense disambiguation or sense
pruning, which probably would discard unrelated
senses.
For detecting the polarity we have used the
SentiWords (Guerini et al., 2013; Warriner et al.,
2013) as a polarity lexicon. Using direct depen-
dency relations between aspect terms and polarity
bearing words we assign the polarity value from
the lexicon to the aspect term. We make a simple
count of the polarities of the aspect terms classi-
fied under a certain category to assign the polarity
of that category in a particular sentence.
5 Evaluation
The run submitted to the SemEval task 4 compe-
tition was based on 25k unlabelled sentences ex-
tracted from domain related reviews (for restau-
rants and laptops) obtained by scraping different
websites. We used these unlabelled sentences to
execute our unsupervised system to generate and
835
Restaur. aspect terms Precision Recall F-score
SemEval Baseline 0.525 0.427 0.471
V3 (S) 0.656 0.562 0.605
V3 (W) 0.571 0.641 0.604
V3 (W+S) 0.575 0.645 0.608
Table 2: Results on the restaurant review test set.
Laptops aspect terms Precision Recall F-score
SemEval Baseline 0.443 0.298 0.356
V3 (S) 0.265 0.276 0.271
V3 (W) 0.321 0.425 0.366
V3 (W+S) 0.279 0.444 0.343
Table 3: Results on the laptop review test set.
rank the aspect term lists. Then we used those
aspect term lists to annotate the sentences using
a simple lemma matching approach between the
words. The generated aspect term lists were lim-
ited to the first ranked 550 items after some initial
experiments with the SemEval training sets.
The SemEval test datasets (restaurants and lap-
tops) contain about 800 sentences each. The
restaurant dataset contains 1,134 labelled gold as-
pect term spans, and the laptop dataset contains
634 labelled gold aspect term spans. We compare
the results against the SemEval baseline which is
calculated using the scripts provided by the Se-
mEval organizers. This baseline splits the dataset
into train and test subsets, and uses all the labelled
aspect terms in the train subset to build a dictio-
nary of aspect terms. Then it simply uses that dic-
tionary to label the test subset for evaluation.
Tables 2 and 3 show the performance of our sys-
tem with respect to the baselines in both datasets.
?V3 (S)? stands for our system only using the Se-
mEval test data (as our approach is unsupervised
it learns from the available texts for the task). (W)
refers to the results using our own dataset scraped
from the Web. Finally (W+S) refers to the results
using both SemEval and our Web dataset mixed
together. The best results are highlighted in bold.
For subtask 1, although our system outperforms
the baseline in terms of F-score in both datasets, in
the competition our system obtained quite modest
results ranking 24th and 26th out of 29 participants
Restaur. categories Precision Recall F-score
SemEval Baseline 0.671 0.602 0.638
V3 0.638 0.569 0.602
Table 4: Results on restaurant category detection
using the test set.
Polarity detection accuracy Baseline V3
Restaur. aspect terms 0.642 0.597
Restaur. categories 0.656 0.472
Laptop aspect terms 0.510 0.538
Table 5: Results for the polarity classification sub-
tasks (subtasks 2 and 4).
for restaurants and laptops respectively.
One of the most important source of errors are
the multiword aspect term detection. In the Se-
mEval datasets, about the 25% of the gold aspect
terms are multiword terms. In both datasets we
find a large number of names of recipes and meals,
composed by two, three or even more words,
which cannot appear in our aspect term lists be-
cause we limit the multiword length up to two
words.
As mentioned in the introduction our approach
focuses mainly in the aspects so the approach for
detecting categories and polarities needs more at-
tention. Table 4 presents our results on category
detection and table 5 our results on polarities. The
results are quite poor so we do not comment on
them here. We will address these subtasks in fu-
ture work.
6 Conclusions and future work
In this paper we propose a simple and unsuper-
vised system able to bootstrap and rank a list
of domain aspect terms from a set of unlabelled
domain texts. We use a double-propagation ap-
proach, and we model the obtained terms and their
relations as a graph. Then, we apply the PageRank
algorithm to score the obtained terms. Despite the
modest results, our unsupervised system for de-
tecting aspect terms performs better than the su-
pervised baseline. In our future work we will try
to improve the way we deal with multiword terms
to reduce the amount of incorrect aspect terms and
generate a better ranking. We also plan to try
different methods for the category grouping, and
explore knowledge-based word sense disambigua-
tion methods for improving the current system.
Acknowledgements
This work has been partially funded by SKaTer
(TIN2012-38584-C06-02) and OpeNER (FP7-
ICT-2011-SME- DCL-296451).
836
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Christiane Fellbaum. 1999. WordNet. Wiley Online
Library.
Gayatree Ganu, N Elhadad, and A Marian. 2009. Be-
yond the Stars: Improving Rating Predictions using
Review Text Content. WebDB, (WebDB):1?6.
Marco Guerini, Lorenzo Gatti, and Marco Turchi.
2013. Sentiment analysis: How to derive prior
polarities from sentiwordnet. arXiv preprint
arXiv:1309.5843.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. AAAI.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Joshua OMadadhain, Danyel Fisher, Padhraic Smyth,
Scott White, and Yan-Biao Boey. 2005. Analysis
and visualization of network data using jung. Jour-
nal of Statistical Software, 10(2):1?35.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
AM Popescu and Oren Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. Natural lan-
guage processing and text mining, (October):339?
346.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding Domain Sentiment Lexicon
through Double Propagation. IJCAI.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics, (July 2010).
Amy Beth Warriner, Victor Kuperman, and Marc Brys-
baert. 2013. Norms of valence, arousal, and dom-
inance for 13,915 english lemmas. Behavior re-
search methods, 45(4):1191?1207.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing:
Volume 3-Volume 3, pages 1533?1541. Association
for Computational Linguistics.
Lei Zhang and Bing Liu. 2014. Aspect and Entity
Extraction for Opinion Mining. Data Mining and
Knowledge Discovery for Big Data.
L Zhang, Bing Liu, SH Lim, and E O?Brien-Strain.
2010. Extracting and ranking product features in
opinion documents. Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
(August):1462?1470.
837
KnowNet:
A Proposal for Building Highly
Connected and Dense
Knowledge Bases from the Web
Montse Cuadros
TALP Research Center, UPC, Barcelona (Spain)
email: cuadros@lsi.upc.edu
German Rigau
IXA NLP Group, UPV/EHU, Donostia (Spain)
email: german.rigau@ehu.es
Abstract
This paper presents a new fully automatic method for building highly
dense and accurate knowledge bases from existing semantic resources.
Basically, the method uses a wide-coverage and accurate knowledge-
based Word Sense Disambiguation algorithm to assign the most appro-
priate senses to large sets of topically related words acquired from the
web. KnowNet, the resulting knowledge-base which connects large sets
of semantically-related concepts is a major step towards the autonomous
acquisition of knowledge from raw corpora. In fact, KnowNet is several
times larger than any available knowledge resource encoding relations
between synsets, and the knowledge that KnowNet contains outperform
any other resource when empirically evaluated in a common multilingual
framework.
71
72 Cuadros and Rigau
1 Introduction
Using large-scale knowledge bases, such as WordNet (Fellbaum, 1998), has become a
usual, often necessary, practice for most current Natural Language Processing (NLP)
systems. Even now, building large and rich enough knowledge bases for broad?
coverage semantic processing takes a great deal of expensive manual effort involv-
ing large research groups during long periods of development. In fact, hundreds
of person-years have been invested in the development of wordnets for various lan-
guages (Vossen, 1998). For example, in more than ten years of manual construction
(from 1995 to 2006, that is from version 1.5 to 3.0), WordNet passed from 103,445 to
235,402 semantic relations1. But this data does not seems to be rich enough to support
advanced concept-based NLP applications directly. It seems that applications will not
scale up to working in open domains without more detailed and rich general-purpose
(and also domain-specific) semantic knowledge built by automatic means. Obviously,
this fact has severely hampered the state-of-the-art of advanced NLP applications.
However, the Princeton WordNet is by far the most widely-used knowledge base
(Fellbaum, 1998). In fact, WordNet is being used world-wide for anchoring differ-
ent types of semantic knowledge including wordnets for languages other than English
(Atserias et al, 2004), domain knowledge (Magnini and Cavagli?, 2000) or ontolo-
gies like SUMO (Niles and Pease, 2001) or the EuroWordNet Top Concept Ontology
(?lvez et al, 2008). It contains manually coded information about nouns, verbs, ad-
jectives and adverbs in English and is organised around the notion of a synset. A
synset is a set of words with the same part-of-speech that can be interchanged in a cer-
tain context. For example, <party, political_party> form a synset because they can
be used to refer to the same concept. A synset is often further described by a gloss, in
this case: "an organisation to gain political power" and by explicit semantic relations
to other synsets.
Fortunately, during the last years the research community has devised a large set of
innovative methods and tools for large-scale automatic acquisition of lexical knowl-
edge from structured and unstructured corpora. Among others we can mention eX-
tended WordNet (Mihalcea and Moldovan, 2001), large collections of semantic pref-
erences acquired from SemCor (Agirre and Martinez, 2001, 2002) or acquired from
British National Corpus (BNC) (McCarthy, 2001), large-scale Topic Signatures for
each synset acquired from the web (Agirre and de la Calle, 2004) or knowledge about
individuals from Wikipedia (Suchanek et al, 2007). Obviously, all these semantic re-
sources have been acquired using a very different set of processes (Snow et al, 2006),
tools and corpora. In fact, each semantic resource has different volume and accuracy
figures when evaluated in a common and controlled framework (Cuadros and Rigau,
2006).
However, not all available large-scale resources encode semantic relations between
synsets. In some cases, only relations between synsets and words have been acquired.
This is the case of the Topic Signatures (Agirre et al, 2000) acquired from the web
(Agirre and de la Calle, 2004). This is one of the largest semantic resources ever built
with around one hundred million relations between synsets and semantically related
1Symmetric relations are counted only once.
KnowNet: A Proposal for Building Knowledge Bases from the Web 73
words.2
A knowledge net or KnowNet, is an extensible, large and accurate knowledge
base, which has been derived by semantically disambiguating the Topic Signatures
acquired from the web. Basically, the method uses a robust and accurate knowledge-
based Word Sense Disambiguation algorithm to assign the most appropriate senses
to the topic words associated to a particular synset. The resulting knowledge-base
which connects large sets of topically-related concepts is a major step towards the au-
tonomous acquisition of knowledge from raw text. In fact, KnowNet is several times
larger than WordNet and the knowledge contained in KnowNet outperformsWordNet
when empirically evaluated in a common framework.
Table 1 compares the different volumes of semantic relations between synset pairs
of available knowledge bases and the newly created KnowNets3.
Table 1: Number of synset relations
Source #relations
Princeton WN3.0 235,402
Selectional Preferences from SemCor 203,546
eXtended WN 550,922
Co-occurring relations from SemCor 932,008
New KnowNet-5 231,163
New KnowNet-10 689,610
New KnowNet-15 1,378,286
New KnowNet-20 2,358,927
Varying from five to twenty the number of processed words from each Topic Signa-
ture, we created automatically four different KnowNets with millions of new semantic
relations between synsets.
After this introduction, Section 2 describes the Topic Signatures acquired from the
web. Section 3 presents the approach we plan to follow for building highly dense and
accurate knowledge bases. Section 4 describes the methods we followed for building
KnowNet. In Section 5, we present the evaluation framework used in this study. Sec-
tion 6 describes the results when evaluating different versions of KnowNet and finally,
Section 7 presents some concluding remarks and future work.
2 Topic Signatures
Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy,
2000). Topic Signatures are built by retrieving context words of a target topic from
large corpora. In our case, we consider word senses as topics. Basically, the acquisi-
tion of TS consists of:
? acquiring the best possible corpus examples for a particular word sense (usually
characterising each word sense as a query and performing a search on the corpus
2Available at http://ixa.si.ehu.es/Ixa/resources/sensecorpus
3These KnowNet versions can be downloaded from http://adimen.si.ehu.es
74 Cuadros and Rigau
Table 2: TS of party#n#1 (first 10 out of 12,890 total words)
tammany#n 0.0319
alinement#n 0.0316
federalist#n 0.0315
whig#n 0.0300
missionary#j 0.0229
Democratic#n 0.0218
nazi#j 0.0202
republican#n 0.0189
constitutional#n 0.0186
organization#n 0.0163
for those examples that best match the queries)
? building the TS by deriving the context words that best represent the word sense
from the selected corpora.
The Topic Signatures acquired from the web (hereinafter TSWEB) constitutes one
of the largest available semantic resources with around 100 million relations (between
synsets and words) (Agirre and de la Calle, 2004). Inspired by the work of Leacock
et al (1998), TSWEB was constructed using monosemous relatives from WN (syn-
onyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and
retrieving up to one thousand snippets per query (that is, a word sense), extracting
the salient words with distinctive frequency using TFIDF. Thus, TSWEB consist of a
large ordered list of words with weights associated to each of the senses of the poly-
semous nouns of WordNet 1.6. The number of constructed topic signatures is 35,250
with an average size per signature of 6,877 words. When evaluating TSWEB, we used
at maximum the first 700 words while for building KnowNet we used at maximum the
first 20 words.
For example, Table 2 present the first words (lemmas and part-of-speech) and
weights of the Topic Signature acquired for party#n#1.
3 Building highly connected and dense knowledge bases
It is our belief, that accurate semantic processing (such as WSD) would rely not
only on sophisticated algorithms but on knowledge intensive approaches. In fact,
the cycling arquitecture of the MEANING4 project demonstrated that acquiring bet-
ter knowledge allow to perform better Word Sense Disambiguation (WSD) and that
having improvedWSD systems we are able to acquire better knowledge (Rigau et al,
2002).
Thus, we plan to acquire by fully automatic means highly connected and dense
knowledge bases from large corpora or the web by using the knowledge already avail-
able, increasing the total number of relations from less than one million (the current
number of available relations) to millions.
4http://www.lsi.upc.edu/~nlp/meaning
KnowNet: A Proposal for Building Knowledge Bases from the Web 75
The current proposal consist of:
? to follow Cuadros et al (2005) and Cuadros and Rigau (2006) for acquiring
highly accurate Topic Signatures for all monosemous words in WordNet (for
instance, using InfoMap (Dorow and Widdows, 2003)). That is, to acquire
word vectors closely related to a particular monosemousword (for instance, air-
port#n#1) from BNC or other large text collections like GigaWord, Wikipedia
or the web.
? to apply a very accurate knowledge?based all?words disambiguation algorithm
to the Topic Signatures in order to obtain sense vectors instead of word vectors
(for instance, using a version of Structural Semantic Interconnections algorithm
(SSI) (Navigli and Velardi, 2005)).
For instance, consider the first ten weighted words (with Part-of-Speech) appear-
ing in the Topic Signature (TS) of the word sense airport#n#1 corresponding to the
monosemous word airport, as shown in Table 3. This TS has been obtained from
BNC using InfoMap. From the ten words appearing in the TS, two of them do not
appear in WN (corresponding to the proper names heathrow#n and gatwick#n), four
words are monosemous (airport#n, airfield#n, travelling#n and passenger#n) and four
other are polysemous (flight#n, train#n, station#n and ferry#n).
Table 3: First ten words with weigths and number of senses in WN of the Topic
Signature for airport#n#1 obtained from BNC using InfoMap
word+pos weight #senses
airport#n 1.000000 1
heathrow#n 0.843162 0
gatwick#n 0.768215 0
flight#n 0.765804 9
airfield#n 0.740861 1
train#n 0.739805 6
travelling#n 0.732794 1
passenger#n 0.722912 1
station#n 0.722364 4
ferry#n 0.717653 2
SSI-Dijkstra
We have implemented a version of the Structural Semantic Interconnections algorithm
(SSI), a knowledge-based iterative approach to Word Sense Disambiguation (Navigli
and Velardi, 2005). The SSI algorithm is very simple and consists of an initialisation
step and a set of iterative steps. GivenW, an ordered list of words to be disambiguated,
the SSI algorithm performs as follows. During the initialisation step, all monosemous
words are included into the set I of already interpreted words, and the polysemous
words are included in P (all of them pending to be disambiguated). At each step, the
76 Cuadros and Rigau
Table 4: Minimum distances from airport#n#1
Synsets Distance
4 6
4530 5
64713 4
29767 3
597 2
20 1
1 0
set I is used to disambiguate one word of P, selecting the word sense which is closer
to the set I of already disambiguated words. Once a sense is disambiguated, the word
sense is removed from P and included into I. The algorithm finishes when no more
pending words remain in P.
Initially, the list I of interpretedwords should include the senses of the monosemous
words inW, or a fixed set of word senses5. However, in this case, when disambiguating
a TS derived from a monosemous word m, the list I includes since the beginning at
least the sense of the monosemous word m (in our example, airport#n#1).
In order to measure the proximity of one synset (of the word to be disambiguated at
each step) to a set of synsets (those word senses already interpreted in I), the original
SSI uses an in-house knowledge base derived semi-automatically which integrates a
variety of online resources (Navigli, 2005). This very rich knowledge-base is used to
calculate graph distances between synsets. In order to avoid the exponential explosion
of possibilities, not all paths are considered. They used a context-free grammar of
relations trained on SemCor to filter-out inappropriate paths and to provide weights to
the appropriate paths.
Instead, we use part of the knowledge already available to build a very large con-
nected graph with 99,635 nodes (synsets) and 636,077 edges (the set of direct relations
between synsets gathered from WordNet and eXtended WordNet). On that graph, we
used a very efficient graph library to compute the Dijkstra algorithm.6 The Dijkstra
algorithm is a greedy algorithm that computes the shortest path distance between one
node an the rest of nodes of a graph. In that way, we can compute very efficiently
the shortest distance between any two given nodes of a graph. This version of the SSI
algorithm is called SSI-Dijkstra.
For instance, Table 4 shows the volumes of the minimumdistances from airport#n#1
to the rest of the synsets of the graph. Interestingly, from airport#n#1 all synsets of
the graph are accessible following paths of at maximum six edges. While there is
only one synset at distance zero (airport#n#1) and twenty synsets directly connected
to airport#n#1, 95% of the total graph is accessible at distance four or less.
SSI-Dijkstra has very interesting properties. For instance, SSI-Dijkstra always pro-
5If no monosemous words are found or if no initial senses are provided, the algorithm could make an
initial guess based on the most probable sense of the less ambiguous word of W.
6See http://www.boost.org
KnowNet: A Proposal for Building Knowledge Bases from the Web 77
vides an answer when comparing the distances between the synsets of a word and all
the synsets already interpreted in I. That is, the Dijkstra algorithm always provides
an answer being the minimum distance close or far7. At each step, the SSI-Dijkstra
algorithm selects the synset which is closer to I (the set of already interpreted words).
Table 5 presents the result of the word?sense disambiguation process with the SSI-
Dijkstra algorithm on the TS presented in Table 38. Now, part of the TS obtained
from BNC using InfoMap have been disambiguated at a synset level resulting on a
word?sense disambiguated TS. Those words not present in WN1.6 have been ignored
(heathrow and gatwick). Some others, being monosemous in WordNet were consid-
ered already disambiguated (travelling, passenger, airport and airfield). But the rest,
have been correctly disambiguated (flight with nine senses, train with six senses, sta-
tion with four and ferry with two).
Table 5: Sense disambiguated TS for airport#n#1 obtained from BNC using InfoMap
and SSI-Dijkstra
.
Word Offset-WN Weight Gloss
flight#n 00195002n 0.017 a scheduled trip by plane between designated
airports
travelling#n 00191846n 0 the act of going from one place to another
train#n 03528724n 0.012 a line of railway cars coupled together and drawn
by a locomotive
passenger#n 07460409n 0 a person travelling in a vehicle (a boat or bus or
car or plane or train etc) who is not operating it
station#n 03404271n 0.019 a building equipped with special equipment and
personnel for a particular purpose
airport#n 02175180n 0 an airfield equipped with control tower and hangers
as well as accommodations for passengers and cargo
ferry#n 02671945n 0.010 a boat that transports people or vehicles across a
body of water and operates on a regular schedule
airfield#n 02171984n 0 a place where planes take off and land
This sense disambiguated TS represents seven direct new semantic relations be-
tween airport#n#1 and the first words of the TS. It could be directly integrated into a
new knowledge base (for instance, airport#n#1 ?related?> flight#n#9), but also all the
indirect relations of the disambiguated TS (for instance, flight#n#9 ?related?> trav-
elling#n#1). In that way, having n disambiguated word senses, a total of (n2 ? n)/2
relations could be created. That is, for the ten initial words of the TS of airport#n#1,
twenty-eight new direct relations between synsets could be created.
This process could be repeated for all monosemous words of WordNet appearing
in the selected corpus. The total number of monosemous words in WN1.6 is 98,953.
Obviously, not all these monosemous words are expected to appear in the corpus.
However, we expect to obtain in that way several millions of new semantic relations
between synsets. This method will allow to derive by fully automatic means a huge
knowledge base with millions of new semantic relations.
7In contrast, the original SSI algorithm not always provides a path distance because it depends on the
grammar.
8It took 4.6 seconds to disambiguate the TS on a modern personal computer.
78 Cuadros and Rigau
Furthermore, this approach is completely language independent. It could be re-
peated for any language having words connected to WordNet.
It remains for further study and research, how to convert the relations created in
that way to more specific and informed relations.
4 Building KnowNet
As a proof of concept, we developed KnowNet (KN), a large-scale and extensible
knowledge base obtained by applying the SSI-Dijkstra algorithm to each topic signa-
ture from TSWEB. That is, instead of using InfoMap and a large corpora for acquiring
new Topic Signatures for all the monosemous terms in WN, we used the already avail-
able TSWEB. We have generated four different versions of KonwNet applying SSI-
Dijkstra to the first 5, 10, 15 and 20 words for each TS. SSI-Dijkstra used only the
knowledge present in WordNet and eXtended WordNet which consist of a very large
connected graph with 99,635 nodes (synsets) and 636,077 edges (semantic relations).
We generated each KnowNet by applying the SSI-Dijkstra algorithm to the whole
TSWEB (processing the first words of each of the 35,250 topic signatures). For each
TS, we obtained the direct and indirect relations from the topic (a word sense) to the
disambiguated word senses of the TS. Then, as explained in Section 3, we also gen-
erated the indirect relations for each TS. Finally, we removed symmetric and repeated
relations.
Table 6 shows the percentage of the overlapping between each KnowNet with re-
spect the knowledge contained intoWordNet and eXtendedWordNet, the total number
of relations and synsets of each resource. For instance, only an 8,6% of the total rela-
tions included into WN+XWN are also present in KN-20. This means that the rest of
relations from KN-20 are new. This table also shows the different KnowNet volumes.
As expected, each KnowNet is very large, ranging from hundreds of thousands to
millions of new semantic relations between synsets among increasing sets of synsets.
Surprisingly, the overlapping between the semantic relations of KnowNet and the
knowledge bases used for building the SSI-Dijkstra graph (WordNet and eXtended
WordNet) is very small, possibly indicating disjunct types of knowledge.
Table 6: Size and percentage of overlapping relations between KnowNet versions and
WN+XWN
KB WN+XWN #relations #synsets
KN-5 3.2% 231,164 39,837
KN-10 5.4% 689,610 45,770
KN-15 7.0% 1,378,286 48,461
KN-20 8.6% 2,358,927 50,705
Table 7 presents the percentage of overlapping relations between KnowNet ver-
sions. The upper triangular part of the matrix presents the overlapping percentage
covered by larger KnowNet versions.That is, most of the knowledge from KN-5 is
also contained in larger versions of KnowNet. Interestingly, the knowledge contained
into KN-10 is only partially covered by KN-15 and KN-20. The lower triangular
KnowNet: A Proposal for Building Knowledge Bases from the Web 79
part of the matrix presents the overlapping percentage covered by smaller KnowNet
versions.
Table 7: Percentage of overlapping relations between KnowNet versions
overlapping KN-5 KN-10 KN-15 KN-20
KN-5 100 93,3 97,7 97,2
KN-10 31,2 100 88,5 88,9
KN-15 16,4 44,4 100 97.14
KN-20 9,5 26,0 56,7 100
5 Evaluation framework
In order to empirically establish the relative quality of these KnowNet versions with
respect already available semantic resources, we used the noun-set of Senseval-3 En-
glish Lexical Sample task which consists of 20 nouns.
Trying to be as neutral as possible with respect to the resources studied, we applied
systematically the same disambiguation method to all of them. Recall that our main
goal is to establish a fair comparison of the knowledge resources rather than providing
the best disambiguation technique for a particular resource. Thus, all the semantic re-
sources studied are evaluated as Topic Signatures. That is, word vectors with weights
associated to a particular synset (topic) which are obtained by collecting those word
senses appearing in the synsets directly related to the topics.
A common WSD method has been applied to all knowledge resources. A simple
word overlapping counting is performed between the Topic Signature and the test
example9. The synset having higher overlapping word counts is selected. In fact, this
is a very simple WSD method which only considers the topical information around
the word to be disambiguated. All performances are evaluated on the test data using
the fine-grained scoring system provided by the organisers. Finally, we should remark
that the results are not skewed (for instance, for resolving ties) by the most frequent
sense in WN or any other statistically predicted knowledge.
5.1 Baselines
We have designed a number of baselines in order to establish a complete evaluation
framework for comparing the performance of each semantic resource on the English
WSD task.
RANDOM: For each target word, this method selects a random sense. This base-
line can be considered as a lower-bound.
SEMCOR-MFS: This baseline selects the most frequent sense of the target word
in SemCor.
WN-MFS: This baseline is obtained by selecting the most frequent sense (the first
sense in WN1.6) of the target word. WordNet word-senses were ranked using SemCor
and other sense-annotated corpora. Thus, WN-MFS and SemCor-MFS are similar, but
not equal.
9We also consider the multiword terms.
80 Cuadros and Rigau
TRAIN-MFS: This baseline selects the most frequent sense in the training corpus
of the target word.
TRAIN: This baseline uses the training corpus to directly build a Topic Signature
using TFIDF measure for each word sense. Note that in WSD evaluation frameworks,
this is a very basic baseline. However, in our evaluation framework, this "WSD base-
line" could be considered as an upper-bound. We do not expect to obtain better topic
signatures for a particular sense than from its own annotated corpus.
5.2 Large-scale Knowledge Resources
In order to measure the relative quality of the new resources, we include in the evalu-
ation a wide range of large-scale knowledge resources connected to WordNet.
WN (Fellbaum, 1998): This resource uses the different direct relations encoded in
WN1.6 and WN2.0. We also tested WN2 using relations at distance 1 and 2, WN3
using relations at distances 1 to 3 and WN4 using relations at distances 1 to 4.
XWN (Mihalcea and Moldovan, 2001): This resource uses the direct relations en-
coded in eXtended WN.
WN+XWN: This resource uses the direct relations included in WN and XWN. We
also tested (WN+XWN)2 (using either WN or XWN relations at distances 1 and 2).
spBNC (McCarthy, 2001): This resource contains 707,618 selectional preferences
acquired for subjects and objects from BNC.
spSemCor (Agirre and Martinez, 2002): This resource contains the selectional
preferences acquired for subjects and objects from SemCor.
MCR (Atserias et al, 2004): This resource uses the direct relations of WN, XWN
and spSemCor (we excluded spBNC because of its poor performance).
TSSEM (Cuadros et al, 2007): These Topic Signatures have been constructed
using the part of SemCor having all words tagged by PoS, lemmatized and sense
tagged according to WN1.6 totalizing 192,639 words. For each word-sense appearing
in SemCor, we gather all sentences for that word sense, building a TS using TFIDF
for all word-senses co-occurring in those sentences.
6 KnowNet Evaluation
We evaluated KnowNet using the framework of Section 5, that is, the noun part of the
test set from the Senseval-3 English lexical sample task.
Table 8 presents ordered by F1 measure, the performance in terms of precision
(P), recall (R) and F1 measure (F1, harmonic mean of recall and precision) of each
knowledge resource on Senseval-3 and its average size of the TS per word-sense. The
different KnowNet versions appear marked in bold and the baselines appear in italics.
In this table, TRAIN has been calculated with a vector size of at maximum 450 words.
As expected, RANDOM baseline obtains the poorest result. The most frequent senses
obtained from SemCor (SEMCOR-MFS) and WN (WN-MFS) are both below the
most frequent sense of the training corpus (TRAIN-MFS). However, all of them are
far below to the Topic Signatures acquired using the training corpus (TRAIN).
The best resources would be those obtaining better performances with a smaller
number of related words per synset. The best results are obtained by TSSEM (with
F1 of 52.4). The lowest result is obtained by the knowledge directly gathered from
WN mainly because of its poor coverage (R of 18.4 and F1 of 26.1). Interestingly,
KnowNet: A Proposal for Building Knowledge Bases from the Web 81
the knowledge integrated in the MCR although partly derived by automatic means
performs much better in terms of precision, recall and F1 measures than using them
separately (F1 with 18.4 points higher than WN, 9.1 than XWN and 3.7 than spSem-
Cor).
Despite its small size, the resources derived from SemCor obtain better results than
its counterparts using much larger corpora (TSSEM vs. TSWEB and spSemCor vs.
spBNC).
Regarding the baselines, all knowledge resources surpass RANDOM, but none
achieves neither WN-MFS, TRAIN-MFS nor TRAIN. Only TSSEM obtains better
results than SEMCOR-MFS and is very close to the most frequent sense of WN (WN-
MFS) and the training (TRAIN-MFS).
The different versions of KnowNet consistently obtain better performances as they
increase the window size of processed words of TSWEB. As expected, KnowNet-
5 obtain the lower results. However, it performs better than WN (and all its ex-
tensions) and spBNC. Interestingly, from KnowNet-10, all KnowNet versions sur-
pass the knowledge resources used for their construction (WN, XWN, TSWEB and
WN+XWN).
Furthermore, the integration of WN+XWN+KN?20 performs better than MCR
and similarly to MCR2 (having less than 50 times its size). It is also interesting to note
that WN+XWN+KN?20 has a better performance than their individual resources,
indicating a complementary knowledge. In fact, WN+XWN+KN?20 performs much
better than the resources from which it derives (WN, XWN and TSWEB).
These initial results seem to be very promising. If we do not consider the re-
sources derived frommanually sense annotated data (spSemCor, MCR, TSSEM, etc.),
KnowNet-10 performs better that any knowledge resource derived by manual or au-
tomatic means. In fact, KnowNet-15 and KnowNet-20 outperforms spSemCor which
was derived from manually annotated corpora. This is a very interesting result since
these KnowNet versions have been derived only with the knowledge coming from
WN and the web (that is, TSWEB), and WN and XWN as a knowledge source for
SSI-Dijkstra (eXtended WordNet only has 17,185 manually labelled senses).
7 Conclusions and future research
The initial results obtained for the different versions of KnowNet seem to be very
promising, since they seem to be of a better quality than other available knowledge
resources encoding relations between synsets derived from non-annotated sense cor-
pora.
We tested all these resources and the different versions of KnowNet on SemEval-
2007 English Lexical Sample Task (Cuadros and Rigau, 2008a). When comparing
the ranking of the different knowledge resources, the different versions of KnowNet
seem to be more robust and stable across corpora changes than the rest of resources.
Furthermore, we also tested the performance of KnowNet when ported to Spanish (as
the Spanish WordNet is also integrated into the MCR). Starting from KnowNet-10,
all KnowNet versions perform better than any other knowledge resource on Spanish
derived by manual or automatic means (including the MCR) (Cuadros and Rigau,
2008b).
82 Cuadros and Rigau
Table 8: P, R and F1 fine-grained results for the resources evaluated at Senseval-3,
English Lexical Sample Task
KB P R F1 Av. Size
TRAIN 65.1 65.1 65.1 450
TRAIN-MFS 54.5 54.5 54.5
WN-MFS 53.0 53.0 53.0
TSSEM 52.5 52.4 52.4 103
SEMCOR-MFS 49.0 49.1 49.0
MCR2 45.1 45.1 45.1 26,429
WN+XWN+KN-20 44.8 44.8 44.8 671
MCR 45.3 43.7 44.5 129
KnowNet-20 44.1 44.1 44.1 610
KnowNet-15 43.9 43.9 43.9 339
spSemCor 43.1 38.7 40.8 56
KnowNet-10 40.1 40.0 40.0 154
(WN+XWN)2 38.5 38.0 38.3 5,730
WN+XWN 40.0 34.2 36.8 74
TSWEB 36.1 35.9 36.0 1,721
XWN 38.8 32.5 35.4 69
KnowNet-5 35.0 35.0 35.0 44
WN3 35.0 34.7 34.8 503
WN4 33.2 33.1 33.2 2,346
WN2 33.1 27.5 30.0 105
spBNC 36.3 25.4 29.9 128
WN 44.9 18.4 26.1 14
RANDOM 19.1 19.1 19.1
In sum, this is a preliminary step towards improved KnowNets we plan to obtain
exploiting the Topic Signatures derived from monosemous words as explained in Sec-
tion 3.
Acknowledgments
We want to thank Aitor Soroa for his technical support and the anonymous reviewers
for their comments. This work has been supported by KNOW (TIN2006-15049-C03-
01) and KYOTO (ICT-2007-211423).
References
Agirre, E., O. Ansa, D. Martinez, and E. Hovy (2000). Enriching very large ontologies
with topic signatures. In Proceedings of ECAI?00 workshop on Ontology Learning,
Berlin, Germany.
Agirre, E. and O. L. de la Calle (2004). Publicly available topic signatures for all
wordnet nominal senses. In Proceedings of LREC, Lisbon, Portugal.
KnowNet: A Proposal for Building Knowledge Bases from the Web 83
Agirre, E. and D. Martinez (2001). Learning class-to-class selectional preferences. In
Proceedings of CoNLL, Toulouse, France.
Agirre, E. and D. Martinez (2002). Integrating selectional preferences in wordnet. In
Proceedings of GWC, Mysore, India.
?lvez, J., J. Atserias, J. Carrera, S. Climent, A. Oliver, and G. Rigau (2008). Consis-
tent annotation of eurowordnet with the top concept ontology. In Proceedings of
Fourth International WordNet Conference (GWC?08).
Atserias, J., L. Villarejo, G. Rigau, E. Agirre, J. Carroll, B. Magnini, and P. Vossen
(2004). The meaning multilingual central repository. In Proceedings of GWC,
Brno, Czech Republic.
Cuadros, M., L. Padr?, and G. Rigau (2005). Comparing methods for automatic ac-
quisition of topic signatures. In Proceedings of RANLP, Borovets, Bulgaria.
Cuadros, M. and G. Rigau (2006). Quality assessment of large scale knowledge re-
sources. In Proceedings of the EMNLP.
Cuadros, M. and G. Rigau (2008a). KnowNet: Building a Ln?arge Net of Knowledge
from the Web. In Proceedings of COLING.
Cuadros, M. and G. Rigau (2008b). Multilingual Evaluation of KnowNet. In Pro-
ceedings of SEPLN.
Cuadros, M., G. Rigau, and M. Castillo (2007). Evaluating large-scale knowledge
resources across languages. In Proceedings of RANLP.
Dorow, B. and D. Widdows (2003). Discovering corpus-specific word senses. In
EACL, Budapest.
Fellbaum, C. (1998). WordNet. An Electronic Lexical Database. The MIT Press.
Leacock, C., M. Chodorow, and G. Miller (1998). Using Corpus Statistics and Word-
Net Relations for Sense Identification. Computational Linguistics 24(1), 147?166.
Lin, C. and E. Hovy (2000). The automated acquisition of topic signatures for text
summarization. In Proceedings of COLING. Strasbourg, France.
Magnini, B. and G. Cavagli? (2000). Integrating subject field codes into wordnet. In
Proceedings of LREC, Athens. Greece.
McCarthy, D. (2001). Lexical Acquisition at the Syntax-Semantics Interface: Diathe-
sis Aternations, Subcategorization Frames and Selectional Preferences. Ph. D.
thesis, University of Sussex.
Mihalcea, R. and D. Moldovan (2001). extended wordnet: Progress report. In Pro-
ceedings of NAACL Workshop on WordNet and Other Lexical Resources, Pitts-
burgh, PA.
84 Cuadros and Rigau
Navigli, R. (2005). Semi-automatic extension of large-scale linguistic knowledge
bases. In Proc. of 18th FLAIRS International Conference (FLAIRS), Clearwater
Beach, Florida.
Navigli, R. and P. Velardi (2005). Structural semantic interconnections: a knowledge-
based approach to word sense disambiguation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence (PAMI) 27(7), 1063?1074.
Niles, I. and A. Pease (2001). Towards a standard upper ontology. In C. Welty and
B. Smith (Eds.), Proc. of the 2nd International Conference on Formal Ontology in
Information Systems (FOIS-2001), pp. 17?19.
Rigau, G., B. Magnini, E. Agirre, P. Vossen, and J. Carroll (2002). Meaning: A
roadmap to knowledge technologies. In Proceedings of COLING?2002 Workshop
on A Roadmap for Computational Linguistics, Taipei, Taiwan.
Snow, R., D. Jurafsky, and A. Y. Ng (2006). Semantic taxonomy induction from
heterogenous evidence. In Proceedings of COLING-ACL.
Suchanek, F. M., G. Kasneci, and G. Weikum (2007). Yago: A Core of Semantic
Knowledge. In 16th international World Wide Web conference (WWW 2007), New
York, NY, USA. ACM Press.
Vossen, P. (1998). EuroWordNet: A Multilingual Database with Lexical Semantic
Networks. Kluwer Academic Publishers.
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 1?10,
Beijing, August 2010
KYOTO: an open platform for mining facts
Piek Vossen
VU University Amsterdam
p.vossen@let.vu.nl
German Rigau
Eneko Agirre
Aitor Soroa
University of the Basque 
Country
german.rigau/e.a-
girre/a.soroa@ehu.es
Monica Monachini
Roberto Bartolini
Istituto di Linguistica 
Computazionale, CNR
monica.monachini/r
oberto.bartolin-
i@ilc.cnr.it
Abstract
This  document  describes  an  open 
text-mining  system  that  was  developed 
for the Asian-European project KYOTO. 
The  KYOTO system uses  an  open text 
representation format and a central onto-
logy to  enable  extraction  of  knowledge 
and facts  from large volumes of text  in 
many different languages. We implemen-
ted a semantic tagging approach that per-
forms off-line reasoning. Mining of facts 
and  knowledge  is  achieved  through  a 
flexible pattern matching module that can 
work in much the same way for different 
languages,  can  handle  efficiently  large 
volumes of documents and is not restric-
ted to a specific domain. We applied the 
system to an English database on estuar-
ies.
1 Introduction
Traditionally, Information Extraction (IE) is the 
task of filling template information from previ-
ously unseen text which belongs to a predefined 
domain (Peshkin & Pfeffer 2003). Most systems 
in  the  Message  Understanding  Conferences 
(MUC,  1987-1998)  and the  Automatic  Content 
Extraction  program  (ACE)1 use  a  pipeline  of 
tools to achieve this, ranging from sophisticated 
NLP tools (like deep parsing) to shallower text-
processing (e.g. FASTUS (Appelt 1995)).
Standard  IE  systems  are  based  on  lan-
guage-specific  pattern  matching  (Kaiser  & 
1http://www.itl.nist.gov/iad/mig//tests/ace  
Miksch 2005), where each pattern consists of a 
regular  expression  and  an  associated  mapping 
from syntactic to logical form. In general, the ap-
proaches can be categorized into two groups: (1) 
the Knowledge Engineering approach (Appelt et 
al.1995), and (2) the learning approach, such as 
AutoSlog  (Appelt  et  al.  1993),  SRV  (Freitag 
1998), or RAPIER (Califf & R. Mooney 1999). 
Another  important  system  is  GATE (Cunning-
ham et al2002), which is a platform for creating 
IE systems. It uses regular expressions, but it can 
also  use  ontologies  to  perform semantic  infer-
ences  to  constrain  linguistic  patterns  semantic-
ally. The use of ontologies in IE is an emerging 
field (Bontcheva & Wilks 2004): linking text in-
stances with elements belonging to the ontology, 
instead of consulting flat gazetteers.
The major disadvantage of traditional IE sys-
tems is that they focus on satisfying precise, nar-
row, pre-specified requests from small homogen-
eous corpora (e.g., extract information about ter-
rorist events). Likewise, they are not flexible, are 
limited to specific types of knowledge and need 
to be built by knowledge engineers for each spe-
cific application and language. In fact most text 
mining  systems are  developed for  a  single  do-
main and a single language, and are not able to 
handle  knowledge  expressed  in  different  lan-
guages  or  expressed  and conceptualized  differ-
ently across cultures.
In this paper we describe an open platform for 
text-mining  or  IE that  can  be applied  to many 
different  languages  in  the  same  way  using  an 
open text representation system and a central on-
1
tology that  is  shared across  languages.  Ontolo-
gical implications are inserted in the text through 
off-line  reasoning and ontological  tagging.  The 
events and facts are extracted from large amounts 
of text using a flexible pattern-matching module, 
as specified by profiles  which comprise  ontolo-
gical and shallow linguistic patterns. The system 
is  developed  in  the  Asian-European  project 
KYOTO2.
In the next section,  we describe the general 
architecture of the KYOTO system. In section 3, 
we specify the knowledge structure that is used. 
Section  4,  describes  the  off-line  reasoning  and 
ontological tagging. In section 5, we describe the 
module for mining knowledge from the text that 
is enriched with ontological  statements.  Finally 
in section 6, we describe the first results of ap-
plying the system to databases on Estuaries.
2 KYOTO overview
The  KYOTO  project  allows  communities  to 
model terms and concepts in their domain and to 
use this knowledge to apply text mining on docu-
ments. The knowledge cycle in the KYOTO sys-
tem starts  with a set  of  source  documents pro-
duced by the community, such as PDFs and web-
sites.  Linguistic  processors  apply  tokenization, 
segmentation, morpho-syntactic analysis and  se-
mantic  processing  to  the  text  in  different  lan-
guages. The semantic processing involves the de-
tection of named-entities (persons, organizations, 
places,  time-expressions)  and  determining  the 
meaning of  words  in  the  text  according to  the 
given wordnet.  
The  output  of  the  linguistic  processors is 
stored in an XML annotation format that  is the 
same for  all  the languages,  called  the KYOTO 
Annotation  Format  (KAF,  Bosma  et  al  2009). 
This format incorporates standardized proposals 
for the linguistic annotation of text and represents 
them in an easy-to-use layered structure, which is 
compatible with the Linguistic Annotation Frame-
work  (LAF,  Ide  and  Romary  2003).  In  KAF, 
words, terms, constituents and syntactic depend-
encies  are  stored  in  separate  layers  with  refer-
ences across the structures. This makes it easier 
to harmonize the output of  linguistic processors 
2 Http://www.kyoto-project.eu
for different languages and to add new semantic 
layers to the basic output, when needed (Bosma 
et al 2009, Vossen et al 2010). All modules in 
KYOTO draw their input from these structures. 
In fact, the word-sense disambiguation process is 
carried out to the same KAF annotation in differ-
ent languages and is therefore the same for all the 
languages (Agirre et al 2009). In the current sys-
tem,  there  are  processors  for  English,  Dutch, 
Italian, Spanish, Basque, Chinese and Japanese.
The KYOTO system proceeds in 2 cycles (see 
Figure 1). In the 1st cycle, the Tybot (Term Yield-
ing Robot) extracts the most relevant terms from 
the documents. The Tybot is another generic pro-
gram that  can  do  this  for  all  the  different  lan-
guages in much the same way. The terms are or-
ganized as a structured hierarchy and, wherever 
possible,  related  to  generic  semantic  databases, 
i.e. wordnets for each language. In the left part of 
Figure 1, we show those terms in the input docu-
ment and their classification in wordnet. Terms in 
italics are present in the original wordnet, while 
underlined terms correspond to terms which were 
not in the original wordnet but were automatic-
ally discovered and linked to wordnet by Tybots. 
Straight  terms  correspond  to hyperonyms  in 
wordnet that do not necessarily occur in the text 
but are linked to ontological classes. The result of 
this  1st cycle  is a domain wordnet  for the target 
language.
The 2nd cycle of the system involves the actu-
al extraction of factual knowledge from the docu-
ments by the Kybots  (Knowledge Yielding Ro-
bots). Kybots use a collection of profiles that rep-
resent patterns of information of interest. In the 
profile, conceptual relations are expressed using 
ontological  and morpho-syntactic linguistic pat-
terns. Since the semantics is defined through the 
ontology,  it  is  possible  to  detect  similar  data 
across documents in different languages, even if 
expressed differently. In Figure 1, we give an ex-
ample of a conceptual pattern that relates organ-
isms that live in habitats. The Kybot can combine 
morpho-syntactic and semantic patterns. When a 
match is detected, the instantiation of the pattern 
is saved in a formal representation, either in KAF 
or in RDF. Since the wordnets in different lan-
guages are mapped to the same ontology and the 
text in these languages is represented in the same 
KAF,  similar  patterns  can  easily  be  applied  to 
multiple languages.
2
3 Ontological  and  lexical  background 
knowledge
As a semantic background model, we defined a 
3-layered  knowledge  architecture  following the 
principle  of  the  division  of  labour  (Putnam 
1975). In this model, the ontology does not need 
to be the central hub for all terms in a domain in 
all  languages.  Following the division  of labour 
principle, we can state that a computer does not 
need  to  distinguish  between  instances  of  a 
European Tree Frog and a Glass Tree frog. We 
assume  that  rigid  concepts  (as  defined  by 
Guarino and Welty 2002) are known to the do-
main experts and do not need to be defined form-
ally in the ontology but can remain in the avail-
able  background  resources,  such  as   databases 
with millions of species.  Terms in the documents 
are mostly non-rigid, e.g.  endangered frogs,  in-
vasive  frogs.  Such  non-rigid  terms  refer  to  in-
stances  of  species  in  contextual  circumstances. 
The processes and states are the important pieces 
of  information  that  matter  to the users  and are 
useful for mining text. The model therefore dis-
tinguishes between background vocabularies, do-
main terms,  wordnets and the central  ontology. 
The  background  vocabularies  are  automatically 
aligned  to  wordnet,  where  we  assume  that 
hyponymy relations to rigid synsets in wordnet 
declare those subconcepts as rigid subtypes too, 
without the necessity to include them in the onto-
logy.  For  non-rigid  terms,  we  defined  a  set  of 
mapping relations to the ontology through which 
we express their non-rigid involvement in these 
processes and states. Likewise, the ontology has 
been extended with processes and states for the 
domain  and  verbs  and  adjectives  have  been 
mapped to be able to detect expressions in text.
The  3-layered  knowledge  model  combines  the 
efforts from 3 different communities:
1.Domain  experts  in  social  communities  that 
continuously build background vocabularies;
2.Wordnet  specialists  that  define  the  basic  se-
mantic model for general concepts for a lan-
guage
3.Semantic Web specialists that define top-level 
and domain-specific ontologies that capture 
formal definitions of concepts;
We formalized the relations between these repos-
itories so that they can developed separately but 
combined within KYOTO to form a coherent and 
formal model.
3.1 Ontology
The KYOTO ontology currently consists of 1149 
classes divided over three layers. The top layer is 
based  on  DOLCE  (DOLCE-Lite-Plus  version 
3.9.7,  Masolo  et  al  2003)  and  OntoWordNet. 
This layer of the ontology has been modified for 
our purposes (Herold et. al. 2009).  The second 
layer consists of so-called Base Concepts (BCs) 
derived  from various  wordnets  (Vossen  1998, 
Izquierdo  et  al. 2007).  Examples  of  BCs  are: 
building,  vehicle,  animal,  plant,  change,  move,  
size, weight. The BCs are those synsets in Word-
Net 3.0 that have the most relations with other 
synsets in the wordnet hierarchies and are selec-
ted in a way that ensures complete coverage of 
the nominal and verbal part of WordNet. This has 
been  completed  for  the  nouns  (about  500 
synsets).  The ontology has also been adapted to 
include important concepts in the domain. Spe-
cial attention has been paid to represents the pro-
cesses  (perdurants)  in  which  objects  (endur-
ants)  of  the domain are  involved and qualities 
they may have. This is typically the information 
that is found in documents on the environment. 
We thus added 40 new event classes for repres-
enting  important  verbs  (e.g.  pollute, absorb, 
damage, drain) and 115 new qualities and qual-
ity-regions for representing important adjectives 
(e.g. airborne, acid, (un)healthy, clear). The full 
Figure 1: Two Cycles of processing in KYOTO
3
ontology can be downloaded from the KYOTO 
website, free for use. A considerable set of gener-
al verbs and adjectives (relevant for for the do-
main)  have  then  been  mapped  to  ontological 
classes: 189  verbal  synsets  and  222  adjectival 
synsets.
The  500  nominal  BCs  are  connected  to  the 
complete  WordNet  hierarchy,  whereas  the  189 
verbs represent 5,978 more specific verbal syn-
sets and the 222 adjectives represent  1,081 ad-
jectival synsets through the wordnet relations.
This basic ontology and the mapping to Word-
Net  are  used  to  model  the  shared  and  lan-
guage-neutral  concepts  and  relations  in the do-
main. Instances are excluded from the ontology. 
Instances will be detected in the documents and 
will be mapped to the ontology through instance 
to ontology relations (see below).  Likewise, we 
make a clear separation between the ontological 
model and the instantiation of the model as de-
scribed in the text.
3.2 Wordnet to ontology mappings
In addition to the ontology, we have wordnets for 
each language in the domain. In addition to the 
regular synset to synset relations in the wordnet, 
we will have a specific set of relations for map-
ping the synsets  to the ontology,  which are  all 
prefixed with sc_ standing for synset-to-concept. 
We differentiate between rigid and non-rigid con-
cepts in the wordnets through the mapping rela-
tions:
? sc_equivalenceOf: the synset is fully equi-
valent to the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_  subclassOf: the synset is a proper sub-
class of the ontology Type & inherits all proper-
ties; the synset is Rigid
? sc_domainOf: the synset is not a proper sub-
class  of  the  ontology  Type  &  is  not  disjoint 
(therefore orthogonal) with other synsets that are 
mapped to the same Type either through sc_sub-
classOf or sc_domainOf; the synset is non-Rigid 
but still inherits all properties of the target onto-
logy Type;  the synset  is  also related to a Role 
with a sc_playRole relation
? sc_playRole:  the  synset  denotes  instances 
for  which  the  context  of  the  Role  applies  for 
some period of time but this is not essential for 
the existence of the instances, i.e. if the context 
ceases to exist then the instances may still exist 
(Mizoguchi et al 2007).3
? sc_participantOf:  instances of the concept 
(denoted by the synset) participate in some en-
durant, where the specific role relation is indic-
ated by the playRole mapping. 
? sc_hasState: instances of the concept are in 
a particular state which is not essential and can 
be changed. There is no need to represent the role 
for a stative perdurant.
This model  extends  existing  WordNet  to  onto-
logy mappings.  For  instance,  in  the  SUMO to 
Wordnet mapping (Niles and Pease 2003), only 
the  sc_equivalenceOf and  sc_subclassOf rela-
tions  are  used,  represented  by  the symbols  ?=? 
and ?+? respectively. The SUMO-Wordnet map-
ping likewise does not systematically distinguish 
rigid from non-rigid  synsets.  In our  model,  we 
separate the linguistically and culturally specific 
vocabularies from the shared ontology while us-
ing the ontology  to interface  the concepts used 
by the various communities.
Using these mapping relations, we can express 
that the synset for  duck (which has a hypernym 
relation to the synset  bird, which, in its turn, has 
an  equivalence  relation  to  the  ontology  class 
bird) is  thus  a  proper  subclassOf  the  ontology 
class bird:
wn:duck hypernym wn:bird
wn:bird  sc_equivalenceOf ont:bird
For a concept such as migratory bird, which is 
also  a  hyponym of  bird in  wordnet  but  not  a 
proper subclass as a non-rigid concept, we thus 
create the following mapping:
wn:migratory bird 
? sc_domainOf ont:bird
? sc_playRole ont:done-by
? sc_participantOf ont:migration
This mapping indicates that the synset is used to 
refer to instances of endurants (not subclasses!), 
where the domain is restricted to birds. Further-
more, these instances participate in the process of 
3 Some terms involve more than one role,  e.g.  gas-
powered-vehicle.  Secondary  participants  are  related 
through  sc_hasCoParticipant and sc_playCoRole 
mappings.
4
migration in the role of  done-by. The properties 
of  the  process  migration are  further  defined  in 
the  ontology,  which  indicates  that  it  is  a  act-
ive-change-of-location  done-by  some  endurant, 
going from a source, via a path to some destina-
tion. The mapping relations from the wordnet to 
the ontology, need to satisfy the constraints of the 
ontology, i.e. only roles can be expressed that are 
compatible with the role-schema of the process 
in which they participate.
For  implied  non-essential  states,  we  use  the 
sc_hasState relation to express that a synset such 
as wild dog refers to instances of dogs that life in 
the wild but can stop being wild:
wn:wild dog ? sc_domainOf ont:dog
wn:wild dog ? sc_hasState ont:wild
Ideally, all processes and states that can be ap-
plied to endurants should be defined in the onto-
logy. This may hold for most verbs and adject-
ives in languages, which do not tend to extend in 
specific  domains  and  are  part  of  the  general 
vocabulary  (e.g.  to  pollute,  to  reduce,  wild). 
However, domain specific text contain many new 
nominal terms that refer to domain-specific pro-
cesses and states, e.g. air pollution, nitrogen pol-
lution,  nitrogen  reduction.  These  terms  are 
equally relevant as their counter-parts that refer 
to endurants involved in similar  processes, e.g. 
polluted air, polluting nitrogen or reduced nitro-
gen. We therefore use the reverse participant and 
role mappings to be able to define such terms for 
processes  as  subclasses  of  more  general  pro-
cesses  involving  specific  participants  in  a  spe-
cified role:
wn:air pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:air
? sc_hasRole ont:patient
wn:nitrogen pollution
? sc_subcassOf ont:pollution (perdurant)
? sc_hasParticipant ont:nitrogen
? sc_hasRole ont:done-by
 
Further  mapping  relations  are  described  in  the 
documentation on the KYOTO website. Through 
the mapping relations, we can keep the ontology 
relatively small and compact whereas we can still 
define  the  richness  of  the  vocabularies  of  lan-
guages in a precise way. The classes in the onto-
logy can be defined using rich axioms that model 
precise implications for inferencing. The wordnet 
to synset mappings can be used to define rather 
basic relations relative to the given ontology that 
still  captures  the  semantics  of  the  terms. The 
term definitions capture both relevance and per-
spective  (those  relations  that  matter  from  the 
point of the view of the term), on the one hand, 
and some semantics with respect to the concepts 
that are involved and their (role) relation on the 
other  hand.  Likewise,  the  KYOTO  system can 
model the linguistic and cultural diversity of lan-
guages in a domain but at the same time keep a 
firm anchoring to a basic and compact ontology.
3.3 Domain wordnet
We selected 3 representative documents on estu-
aries to extract relevant terms for the domain us-
ing the Tybot module. The terms have been re-
lated  through  structural  relations,  e.g.  nitrogen 
pollution is a hyponym of pollution, and through 
WordNet synsets that are assigned through WSD 
of the text.  We extracted 3950 candidate  terms 
form the KAF representations of the documents. 
Most of these are nouns (2818 terms). The nom-
inal  terms matched for 40% with wordnet syn-
sets, the verbs and adjectives for 98% and 85% 
respectively. For the domain wordnet, we restric-
ted ourselves to the nouns. From the new nomin-
al  terms,  environmentalists selected  390  terms 
that they deem to be important. These terms are 
connected to parent terms, which ultimately are 
connected to wordnet synsets.  The final domain 
wordnet contains 659 synsets: 197 synsets from 
the generic wordnet and 462 new synsets connec-
ted to the former.  The domain wordnet synsets 
got 990 mappings to the ontology, using the rela-
tions described in the previous section. There are 
86 synsets that have a sc_domainOf mapping, in-
dicating  that  they  are  non-rigid.  Note  that 
hyponyms of these synsets are also non-rigid by 
definition. These non-rigid synsets have complex 
mappings to processes and states in which  they 
are involved. The domain wordnet can be down-
loaded from the KYOTO website, free for use.
5
4 Off-line reasoning and ontological tag-
ging 
The ontological tagging represents the last phase 
in the KYOTO Linguistic  Processor  annotation 
pipeline.  It  consists  of  a three-step module  de-
vised to enrich the KAF documents with know-
ledge derived from the ontology. For each synset 
connected to a term, the first step   adds the Base 
Concepts to which the synset is related through 
the wordnet taxonomical relations. Then, through 
the synset to ontology mapping, it  adds the cor-
responding ontology type with appropriate rela-
tions. Once each synset is specified as to its onto-
logy type,  the  last  ontotagging  step  inserts  the 
full  set  of  ontological  implications  that  follow 
from the explicit ontology. The explicit ontology 
is a new data  structure consisting of a table with 
all  ontology nodes and all  ontological  implica-
tions expressed. The main purpose is to optimize 
<term lemma="pollution" pos="N" tid="t13444" type="open">
  <externalReferences>
   <externalRef reference="eng-30-00191142-n" reftype="baseConcept" resource="wn30g"/>
   <externalRef reference="Kyoto#change-eng-3.0-00191142-n" reftype="sc_subClassOf" resource="ontology">
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#contamination_pollution"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#accomplishment" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#event" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#part" reference="DOLCE-Lite.owl#perdurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#specific-constant-constituent" reference="DOLCE-Lite.owl#perdurant" 
status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-quality" status="implied"/>
      <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#spatio-temporal-particular" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#participant" reference="DOLCE-Lite.owl#endurant" status="implied"/>
      <externalRef reftype="DOLCE-Lite.owl#has-quality" reference="DOLCE-Lite.owl#temporal-location_q" status="im-
plied"/>
    <externalRef reftype="SubClassOf" reference="DOLCE-Lite.owl#particular" status="implied"/>
    </externalRef>
  </externalReferences>
</term>
Figure 2: An example of an OntoTagged output
<kprofile>
 <variables>
<var name="x" type="term" pos="N"/>
  <var name="y" type="term" 
       lemma="produce | generate | release | ! create"/>
  <var name="z" type="term"
       reference="DOLCE-Lite.owl#contamination_pollution"
       reftype="SubClassOf"/>
 </variables>
 <relations>
  <root span="y"/>
  <rel span="x" pivot="y" direction="preceding"/>
  <rel span="z" pivot="y" direction="following"/>
 </relations>
 <events>
  <event target="$y/@tid" lemma="$y/@lemma" pos="$y/@pos"/>
  <role target="$x/@tid" rtype="agent" lemma="$x/@lemma"/>
  <role target="$z/@tid" rtype="patient"lemma="$z/@lemma"/>$
 </events>
</kprofile>
Figure 3: An example of a Kybot profile
<kybotOut>
 <doc name="11767.mw.wsd.ne.onto.kaf">
  <event eid="e1" lemma="generate" pos="V" target="t3504"/>
  <role rid="r1" lemma="industry" rtype="agent" target="t3493" pos="N" event="e1"/>
  <role rid="r2" lemma="pollution" rtype="patient" target="t3495" pos="N" event="e1"/>
 </doc>
 <doc name="16266.mw.wsd.ne.onto.kaf">
  <event eid="e2" lemma="release" pos="V" target="t97"/>
  <role rid="r3" lemma="fuel" rtype="agent" target="t96" pos="N" event="e2"/>
  <role rid="r4" lemma="exhaust_gas" rtype="patient" target="t101" pos="V" event="e2"/>
 </doc>
</kybotOut>
Figure 4: An example of a Kybot output
6
the performance of the mining module over large 
quantities of documents. The advantage for Ky-
bots from ontotagging are many. First of all, they 
are  able  to  run  and  apply  pattern-matching  to 
Base  Concepts  and  ontological  classes  rather 
than just to words or synsets. Moreover, by mak-
ing explicit  the  implicit  ontological  statements, 
Kybots are able to find the same relations hidden 
in  different  expressions  with  different  surface 
realizations:  fish migration,  migratory  fish,  mi-
gration of fish, fishes that migrate, that directly 
or indirectly express the same relations. With on-
totagging,  they  share  the  same ontological  im-
plications which will allow Kybots to apply the 
same patterns and perform the extraction of facts. 
The implications will be represented in the same 
way across different languages, thus facilitating 
cross-lingual extraction of facts. Lastly, ontotag-
ging is a kind of off-line ontological reasoning: 
without  doing reasoning over concepts,  Kybots 
substantially  improve their  performance.  Figure 
2 shows the result of onto-tagging for the term 
pollution.
5 Event and fact extraction
Kybots (Knowledge Yielding Robots) are  com-
puter  programs  that  use  the  mined 
concepts and the generic  concepts  already con-
nected to the language wordnets and the KYOTO 
ontology to extract actual concept instances and 
relations in KAF documents. Kybots incorporate 
technology  for  the  extraction  of  relationships, 
either eventual or not, relative to the general or 
domain concepts already captured by the Tybots. 
That is, the extraction of factual knowledge is be-
ing carried out by the Kybot server by processing 
Kybot profiles on the linguistically enriched doc-
uments.
Kybots  are  defined  following  a  declarative 
format,  the  so  called  Kybot  
profiles, which describe general morpho-syntact-
ic  and  semantic  conditions  on  sequences  of 
terms. Profiles are compiled to generate the Ky-
bots, which scan over KAF documents searching 
for the patterns and extract the relevant informa-
tion from each matching.
Linguistic  patterns  include morphologic  con-
straints and also semantic conditions the matched 
terms must hold.  Kybot are thus able to search 
for term lemmas or part-of-speech tags but also 
for terms linked to ontological process and states 
using  the  mappings  described  in  Section  3.2. 
Thus, it is possible to detect similar eventual in-
formation  across  documents  in  different  lan-
guages, even if expressed differently.
5.1 Example of a Kybot Profile
Kybot Profiles are described using XML syn-
tax.  Figure 3 presents an example of a profile. 
Kybot profiles consist of three main parts: 
?Variable  declaration (<variables> element): 
In this section the search entities are defined. The 
example  defines  three  variables:  x (denoting 
terms  whose  part-of-speech is  noun),  y (which 
are  terms whose lemma is ?release?, ?produce? 
or  ?generate?  but   not  ?create?)  and  z (terms 
linked to  the  ontological  endurant  ?DOLCE-L-
ite.owl#contamination_pollution?, meaning ``be-
ing contaminated with harmful  substances''). 
?Declarations  of  the  relations  among  variables 
(<rel> element): specify the relations among the 
previously  defined variables.  The example pro-
file specifies y  as the main pivot, and states that 
variable  x must  be  preceding  variable  y in  the 
same sentence, and that variable  z must be fol-
lowing variable  y.  Thus,  the Kybot will  search 
for patterns like 'x ? y ? z' in a sentence.
?Output template (<events> element): describes 
the output to be produced on every matching. In 
the example, each match generates a new event 
targeting term  y,  which becomes the main term 
of the event. It also fills two roles of the event, 
the 'agent' role filled by term x and 'patient' role, 
filled by z. 
Figure  4  presents  the  output  of  the  Kybot 
when applied against the benchmark documents.
The Kybot output follows the stand-off architec-
ture when producing new information, and it thus 
forms  a  new KAF layer  on  the  original  docu-
ments.
6 Experimental results
We applied the KYOTO system and resources to 
English documents on estuaries. We collected 50 
URLs for two English estuaries: the Humber Es-
tuary in Hull (UK) and the Chesapeake Bay estu-
ary in the US and for background documents on 
bird  migration,  sedimentation,  habitat  destruc-
tion,  and  climate  change.  In  addition  to  the 
webpages, we extracted 815 PDF files from the 
sites. In total, 4625 files have been extracted. All 
7
the documents have been processed by the lin-
guistic  processor  for  English,  which  generated 
KAF representations for all the documents. From 
this  database,  3  documents  were  selected  for 
benchmarking.
The  documents  were  processed  by  applying 
multiword  tagging,  word-sense-disambiguation, 
named-entity-recognition  and  the  ontological 
tagging to the 3 documents and to the complete 
database; This was done twice: once without the 
domain model and once with the domain model. 
We thus created 4 datasets:  3 benchmark docu-
ments  processed  with  and  without  the  domain 
model; the complete database processed with and 
without the domain model.
Furthermore, we created Kybot profiles based 
on the type of information represented in the do-
main model. We applied the Kybots to all 4 data 
sets. We generate the following data files through 
an WN-LMF export of the domain wordnet:
1. a set of domain multiwords for the multi-
word tagger
2. an extension of the lexicon and the graph 
of  concepts  that  is  used  by  the  WSD 
module
3. an extension of the wordnet-to-ontology 
mappings for the ontotagger
In addition, we constructed mapping lists for all 
WordNet 3.0 synsets to Base Concepts and to ad-
jective and verbs that are matched to the onto-
logy.  These mappings provide the generic  con-
ceptual model based on wordnet and on the onto-
logy. 
Table 1 shows the effects of using the domain 
model for the first 3 modules. We can see that the 
domain  model  has  a  clear  effect  on  the multi-
word  detection  in  the  3  evaluation  documents. 
Using the domain model,  600 multiwords have 
been detected, against 145 with just the generic 
wordnet. This is obvious since the terms are ex-
tracted  from  the  same  documents.  However, 
when applying it  to the complete  database,  we 
see that  still  over 2,300 more multiwords have 
been  detected  using  the domain wordnet.  Note 
that the domain wordnet has only 97 multiwords 
and the generic wordnet has 19,126 multiwords. 
So 0.5% of the multiwords in the domain word-
net add 1.5 times more multiword tokens in the 
database. The third row specifies the number of 
synsets that have been assigned. We can see that 
for the domain model almost 400 more synsets 
have been detected. In the case of the full estuary 
database, we see that relatively few more have 
been detected, almost 1,500 while the database is 
80 times as big. If we look more closely at the 
numbers of actual  domain synsets detected,  we 
see the following results. In the benchmark docu-
ments  637 (or 5%) of  the synsets  is  a  domain 
wordnet  synset,  whereas  5,353 synsets  are  do-
main synsets in the full estuary database, which 
is only 0.52%. Note that in KAF multiwords are 
represented both as a single terms and in terms of 
their elements. The WSD module assigns synsets 
to  both.  The  domain  model  can  thus  only  add 
synsets compared to the processing without the 
domain. 
Finally, if we look at the named-entity-recogni-
tion module, we see a slight negative effect for 
the detection of named-entities due to the domain 
model.  The  named-entity-recognition  module 
does not consider the elements of multiwords but 
just  the multiword terms as a whole. Grouping 
terms  as  multiwords  thus  leads  to  less  named-
entities being detected. This is not necessarily a 
bad things, since the detection heavily over-gen-
erates and could have now more precision.
Table 1: Statistics on processing the estuary documents with and without domain model
bench mark documents (3) estuary documents (4742)
No Domain Domain No Domain Domain
terms 22,204 22,204 2,419,839 2,419,839
multiwords 145 600 4,389 6,671
12,526 12,910 1,021,598 1,023,017
158 126 41,681 40,714
67 66 10,288 10,233
synsets
ne location
ne date
8
Table 2 shows the effect of inserting ontologic-
al  implications  into  the  text  representation.For 
the benchmark documents, we see that more than 
half a million ontological implications have been 
inserted.  Of  these, 82% are implied references, 
that are extracted from the explicit ontology on 
the  basis  of  a  direct  mapping to  the  ontology. 
About  8% of  the  mappings  are  synset-to-onto-
logy mappings (sc) and 9.5% are mappings rep-
resenting the subclass hierarchy. The differences 
between using the domain model and not-using 
the domain model are minimal. For the complete 
database, the implications are 80 times as much 
but the proportions are similar.
Table 3 shows the type of sc-relations that oc-
cur.  Obviously,  sc_subClassOf  and  sc_equival-
entOf  are  the  most  frequent.  Nevertheless,  we 
still  find  about  500  mappings  that  present  the 
participation in a process or state. 
 
     30  reftype="sc_playCoRole"
     32  reftype="sc_hasCoParticipant"
     42  reftype="sc_partOf"
     59  reftype="sc_stateOf"
     92  reftype="sc_playRole"
     94  reftype="sc_hasRole"
     97  reftype="sc_participantOf"
   105  reftype="sc_hasParticipant"
   128  reftype="sc_domainOf"
   169  reftype="sc_hasState"
   312  reftype="sc_hasPart"
 3637  reftype="sc_equivalentOf"
42048  reftype="sc_subClassOf"
Table 3: Type of relations for the wordnet to ontology  
mappings using the domain model
The table clearly shows the impact of role rela-
tions  that  are  encoded  in  the  domain  wordnet. 
When  we  extract  the  mappings  for  the  files 
without the domain model (ony using the map-
pings to the generic wordnet), we get only equi-
valence and subclass mappings.
Finally to complete the knowledge cycle, we cre-
ated a few Kybot profiles for extracting events 
from the  onto-tagged  documents.  As  an  initial 
test, 3 profiles have been created:
1. events of destruction
2. destructions of locations
3. destruction of objects
Using  these  profiles,  we  extracted  211  events 
from the 3 benchmark documents with 396 roles. 
The profiles are created to run over the ontolo-
gical  types  inserted  by  the  ontotagger,  e.g.  re-
stricted to events and change_of_integrity.  Des-
pite the generality of the profiles, we still see a 
clear signature of the domain in the output. This 
is a good indication that we will be able to ex-
tract valuable events from the data, even though 
the  ontotagger  generates  a  massive  amount  of 
implications.  Especially  events  that  combine 
multiple  roles  appear  to  give  rich  information. 
For example, the following sentence:
"One of the greatest challenges to restoration is con-
tinued population growth and development, which 
destroys forests, wetlands and other natural areas"
yielded the following output:
   <event target="t1471" lemma="destroy" pos="V" 
eid="e74"/>
   <role target="t1477" rtype="patient" lemma="area" 
pos="N" event="e74" rid="r138"/>
   <role target="t1472" rtype="patient" 
lemma="forest" pos="N" event="e74" rid="r151"/>
   <role target="t1469" rtype="actor" lemma="devel-
opment" pos="N" event="e74" rid="r180"/>
Running the full set of profiles on the complete data-
base with almost 60 million ontological statements 
took about 2 hours. This shows that our approach is 
scalable and efficient.
Table 2: Ontological implications for the four data sets
bench mark documents (3) estuary documents (4272
No Domain Domain Domain
ontology references 555,677 576,432 48,708,300
implied ontology references 457,332 82.30% 474,916 82.39% 40,523,452 83.20%
direct ontology references 53,178 9.57% 54,769 9.50% 4,377,814 8.99%
45,167 8.13% 46,747 8.11% 3,807,034 7.82%domain synset to ontology mappings
9
7 Conclusions
In this paper, we described an open platform for 
text-mining  using wordnets  and a central  onto-
logy.  The  system  can  be  used  across  different 
languages and can be tailored to mine any type of 
conceptual relations. It can handle semantic im-
plications that are expressed in very different lin-
guistic expressions and yield systematic output. 
As future work, we will carry out benchmarking 
and testing of the mining of events, both for Eng-
lish and for the other languages in the KYOTO 
project.
Acknowledgements
The KYOTO project is co-funded by EU - FP7 
ICT Work Programme 2007 under Challenge 4 - 
Digital  libraries  and  Content,  Objective  ICT-
2007.4.2  (ICT-2007.4.4):  Intelligent  Contsent 
and Semantics  (challenge 4.2).  The Asian part-
ners from Tapei and Kyoto are funded from na-
tional funds. This work has been also supported 
by  Spanish  project  KNOW-2 (TIN2009-14715-
C04-01).
References
Agirre, E., & Soroa, A. (2009) Personalizing PageR-
ank for Word Sense Disambiguation. Proceedings 
of the 12th EACL, 2009. Athens, Greece. 
Agirre, E., Lopez de Lacalle, O., & Soroa, A. (2009) 
Knowledge-based WSD and specific domains: per-
forming over supervised WSD. Proceedings of IJ-
CAI. Pasadena, USA. http://ixa.si.ehu.es/ukb
?lvez J., Atserias J., Carrera J., Climent S., Laparra 
E., Oliver A. and Rigau G. (2008) Complete and 
Consistent  Annotation of  WordNet  using the Top 
Concept Ontology. Proceedings of LREC'08, Mar-
rakesh, Morroco. 2008.
Appelt Douglas E., Jerry R. Hobbs, John Bear, David 
Israel, Megumi Kameyama, Andrew Kehler, David 
Martin,  Karen Myers and Mabry Tyson. Descrip-
tion of the FASTUS System Used for MUC-6. In 
Proceedings  of  MUC-6,  pages  237?248.  San 
Mateo, Morgan Kaufmann, 1995.
Auer A., C. Bizer, G. Kobilarov, J. Lehmann, R. Cy-
ganiak and Z. Ives. DBpedia: A Nucleus for a Web 
of  Open  Data.  In  Proceedings  of  the
International  Semantic  Web  Conference  (ISWC), 
volume 4825 of  Lecture Notes  in Computer Sci-
ence, pages 722-735. 2007.
Bosma, W., Vossen, P., Soroa, A. , Rigau, G., Tesconi, 
M., Marchetti, A., Monachini, M., & Apiprandi, C. 
(2009) KAF: a generic semantic annotation format. 
In Proceedings of the 5th International Conference 
on Generative Approaches to the Lexicon Sept 17-
19, 2009, Pisa, Italy.
Fellbaum,  C.  (Ed.)  (1998)  WordNet:  An  Electronic 
Lexical Database. Cambridge, MA: MIT Press.
Freitag, D. (1998) Information extraction from html: 
Application  of  a  general  machine  learning  ap-
proach.  In  Proceedings  of  the  Fifteenth  National 
Conference on Artificial Intelligence, 1998.
Gangemi  A.,  Guarino  N.,  Masolo  C.,  Oltramari  A., 
Schneider  L.  (2002)  Sweetening  Ontologies  with 
DOLCE. Proceedings of EKAW. 2002
Ide, N. and L. Romary. 2003. Outline of the inter- na-
tional standard Linguistic Annotation Framework. 
In Proceedings of ACL 2003 Workshop on Lin-
guistic Annotation: Getting the Model Right, pages 
1?5.
Izquierdo R., Su?rez A. & Rigau G. Exploring the 
Automatic Selection of Basic Level Concepts. Pro-
ceedings of RANLP'07, Borovetz, Bulgaria. 
September, 2007.
Masolo, C., Borgo, S., Gangemi, A.,  Guarino, N. & 
Oltramari, A. (2003) WonderWeb Deliverable D18: 
Ontology Library, ISTC-CNR, Trento, Italy.
Mizoguchi R., Sunagawa E., Kozaki K. & Kitamura 
Y. (2007 A Model of Roles within an Ontology De-
velopment  Tool:  Hozo.  Journal  of  Applied  Onto-
logy, Vol.2, No.2, 159-179.
Niles, I. & Pease, A. (2001) Formal Ontology in In-
formation Systems. Proceedings of the internation-
al Conference on Formal Ontology in Information 
Systems ? Vol. 2001 Ogunquit, Maine,  USA
Niles, I. and A. Pease. Linking lexicons and ontolo-
gies:  Mapping  WordNet  to  the  Suggested  Upper 
Merged Ontology. In Proc. IEEE IKE, pages 412?
416, 2003.
Vossen, P. (Ed.) (1998) EuroWordNet: a multilingual 
database  with  lexical  semantic  networks  for 
European Languages. Kluwer, Dordrecht.
Vossen P., W. Bosma, E. Agirre, G. Rigau, A. Soroa 
(2010) A full Knowledge Cycle for Semantic Inter-
operability.  Proceedings  of  the  5th  Joint  ISO-
ACL/SIGSEM Workshop on Interoperable Semant-
ic Annotation, (ICGL 2010) Hong Kong, 2010.
10
Proceedings of BioNLP Shared Task 2011 Workshop, pages 138?142,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Using Kybots for Extracting Events in Biomedical Texts
Arantza Casillas (*)
arantza.casillas@ehu.es
Arantza D??az de Ilarraza (?)
a.diazdeilarraza@ehu.es
Koldo Gojenola (?)
koldo.gojenola@ehu.es
Maite Oronoz (?)
maite.oronoz@ehu.es
German Rigau (?)
german.rigau@ehu.es
IXA Taldea UPV/EHU
(*) Department of Electricity and Electronics
(?) Department of Computer Languages and Systems
Abstract
In this paper we describe a rule-based sys-
tem developed for the BioNLP 2011 GENIA
event detection task. The system applies Ky-
bots (Knowledge Yielding Robots) on anno-
tated texts to extract bio-events involving pro-
teins or genes. The main goal of this work is to
verify the usefulness and portability of the Ky-
bot technology to the domain of biomedicine.
1 Introduction
The aim of the BioNLP?11 Genia Shared Task (Kim
et al, 2011b) concerns the detection of molecular
biology events in biomedical texts using NLP tools
and methods. It requires the identification of events
together with their gene or protein arguments. Nine
event types are considered: localization, binding,
gene expression, transcription, protein catabolism,
phosphorylation, regulation, positive regulation and
negative regulation.
When identifying the events related to the given
proteins, it is mandatory to detect also the event
triggers, together with its associated event-type, and
recognize their primary arguments. There are ?sim-
ple? events, concerning an event together with its
arguments (Theme, Site, ...) and also ?complex?
events, or events that have other events as secundary
arguments. Our system did not participate in the op-
tional tasks of recognizing negation and speculation.
The training dataset contained 909 texts together
with a development dataset of 259 texts. 347 texts
were used for testing the system.
The main objective of the present work was to ver-
ify the applicability of a new Information Extraction
(IE) technology developed in the KYOTO project1
(Vossen et al, 2008), to a new specific domain. The
KYOTO system comprises a general and extensible
multilingual architecture for the extraction of con-
ceptual and factual knowledge from texts, which has
already been applied to the environmental domain.
Currently, our system follows a rule-based ap-
proach (i.e. (Kim et al, 2009), (Kim et al, 2011a),
(Cohen et al, 2011) or (Vlachos, 2009)), using a set
of manually developed rules.
2 System Description
Our system proceeds in two phases. Firstly, text doc-
uments are tokenized and structured using an XML
layered structure called KYOTO Annotation Format
(KAF) (Bosma et al, 2009). Secondly, a set of Ky-
bots (Knowledge Yielding Robots) are applied to de-
tect the biological events of interest occurring in the
KAF documents. Kybots form a collection of gen-
eral morpho-syntactic and semantic patterns on se-
quences of KAF terms. These patterns are defined
in a declarative format using Kybot profiles.
2.1 KAF
Firstly, basic linguistic processors apply segmenta-
tion and tokenization to the text. Additionally, the
offset positions of the proteins given by the task or-
ganizers are also considered. The output of this ba-
sic processing is stored in KAF, where words, terms,
syntactic and semantic information can be stored in
separate layers with references across them.
Currently, our system only considers a minimal
amount of linguistic information. We are only using
1http://www.kyoto-project.eu/
138
the word form and term layers. Figure 1 shows an
example of a KAF document where proteins have
been annotated using a special POS tag (PRT). Note
that our approach did not use any external resource
apart of the basic linguistic processing.
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<KAF xml:lang="en">
<text>
...
<wf wid="w210" sent="10">phosphorylation</wf>
<wf wid="w211" sent="10">of</wf>
<wf wid="w212" sent="10">I</wf>
<wf wid="w213" sent="10">kappaB</wf>
<wf wid="w214" sent="10">alpha<...
</text>
<term tid="t210" type="open" lemma="phosphorylation"
start="1195" end="1210" pos="W">
<span><target id="w210"/></span>
</term>
<term tid="t211" type="open" lemma="of"
start="1211" end="1213" pos="W">
<span><target id="w211"/></span>
</term>
<term tid="T5" type="open" lemma="I kappaB alpha"
start="1214" end="1228" pos="PRT">
<span><target id="w212"/></span>
<target id="w213"/>
<target id="w214"/></span>
</term>...
</terms>
</KAF>
Figure 1: Example of a document in KAF format.
2.2 Kybots
Kybots (Knowledge Yielding Robots) are abstract
patterns that detect actual concept instances and re-
lations in KAF. The extraction of factual knowledge
by the mining module is done by processing these
abstract patterns on the KAF documents. These pat-
terns are defined in a declarative format using Kybot
profiles, which describe general morpho-syntactic
and semantic conditions on sequences of terms. Ky-
bot profiles are compiled to XQueries to efficiently
scan over KAF documents uploaded into an XML
database. These patterns extract and rank the rele-
vant information from each match.
Kybot profiles are described using XML syn-
tax and each one consists of three main declarative
parts:
? Variables: In this part, the entities and its prop-
erties are defined
? Relations: This part specifies the positional re-
lations among the previously defined variables
? Events: describes the output to be produced for
every matching
Variables (see the Kybot section variables in fig-
ure 2) describe the term variables used by the Kybot.
They have been designed with the aim of being flex-
ible enough to deal with many different information
associated with the KAF terms including semantic
and ontological statements.
Relations (see the Kybot section relations in fig-
ure 2) define the sequence of variables the Kybot
is looking for. For example, in the Kybot in fig-
ure 2, the variable named Phosphorylation
is the main pivot, the variable Of must follow
Phosphorylation (immediate is true indi-
cating that it must be the next term in the sequence),
and a variable representing a Proteinmust follow
Of. Proteins and genes are identified with the PRT
tag.
Events (expressions marked as events in figure 2)
describes the output template of the Kybot. For ev-
ery matched pattern, the kybot produces a new event
filling the template structure with the selected pieces
of information. For example, the Kybot in figure 2
selects some features of the event represented with
the variable called Phosphorylation: its term-
identification (@tid), its lemma, part of speech and
offset. The expression also describes that the vari-
able Protein plays the role of being the ?Theme?
of the event. The output obtained when aplying the
Kybot in figure 2 is shown in figure 3. Comparing
the examples in table 1 and in figure 3 we observe
that all the features needed for generating the files
for describing the results are also produced by the
Kybot.
<doc shortname="PMID-9032271.kaf">
<event eid="e1" target="t210" kybot="phosphorylation of P"
type="Phosphorylation"
lemma="phosphorylation" start="1195" end="1210" />
<role target="T5" rtype="Theme"
lemma="I kappaB alpha" start="1214" end="1228" />
</doc>
Figure 3: Output obtained after the application of the Ky-
bot in figure 2.
3 GENIA Event Extraction Task and
Results
We developed a set of basic auxiliary pro-
grams to extract event patterns from the train-
ing corpus. These programs obtain the struc-
139
<?xml version="1.0" encoding="utf-8"?>
<!-- Sentence: phosphorylation of Protein
Event1: phosphorylation
Role: Theme Protein -->
<Kybot id="bionlp">
<variables>
<var name="Phosphorylation" type="term" lemma="phosphorylat*>
<var name="Of" type="term" lemma="of"/>
<var name="Protein" type="term" pos="PRT"/>
</variables>
<relations>
<root span="Phosphorylation"/>
<rel span="Of" pivot="Phosphorylation" direction="following" immediate="true"/>
<rel span="Protein" pivot="Of" direction="following" immediate="true"/>
</relations>
<events>
<event eid="" target="$Phosphorylation/@tid" kybot="phosphorylation of P"
type="Phosphorylation" lemma="$Phosphorylation/@lemma"
pos="$Phosphorylation/@pos" start="$Phosphorylation/@start" end="$Phosphorylation/@end"/>
<role target="$Protein/@tid" rtype="Theme" lemma="$Protein/@lemma" start="$Protein/@start"
end="$Protein/@end"/>
</events>
</Kybot>
Figure 2: Example of a Kybot for the pattern Event of Protein.
.a1 file
T5 Protein 1214 1228 I kappaB alpha
.a2 file
T20 Phosphorylation 1195 1210 phosphorylation
E7 Phosphorylation:T20 Theme:T5
Table 1: Results in the format required in the GENIA
shared task.
ture of the events, their associated trigger words
and their frequency. For example, in the
training corpus, a pattern of the type Event
of Protein appears 35 times, where the
Event is further described as phosporylation,
phosphorylated.... Taking the most fre-
quently occurring patterns in the training data into
account, we manually developed the set of Kybots
used to extract the events from the development and
test corpora. For example, in this way we wrote the
Kybot in figure 2 that fulfils the conditions of the
pattern of interest.
The two phases mentioned in section 2, corre-
sponding to the generation of the KAF documents
and the application of Kybots, have different input
files depending on the type of event we want to
detect: simple or complex events. When extract-
ing simple events (see figure 4), we used the in-
put text and the files containing protein annotations
(?.a1? files in the task) to generate the KAF docu-
ments. These KAF documents and Kybots for sim-
ple events are provided to the mining module. In
the case of complex events (events that have other
KAF generator
.txt .a1
.kaf
Kybot processor
Kybots
(Simple)
.a2
Figure 4: Application of Kybots. Simple events.
events as arguments), the identifiers of the detected
simple events are added to the KAF document in the
first phase. A new set of Kybots describing complex
events and KAF (now with annotations of the simple
events) are used to obtain the final result (see figure
5).
For the evaluation, we also developed some pro-
grams for adapting the output of the Kybots (see fig-
ure 3) to the required format (see table 1).
We used the development corpus to improve the
Kybot performance. We developed 65 Kybots for
detecting simple events. Table 2 shows the number
of Kybots for each event type. Complex events rela-
tive to regulation (also including negative and posi-
tive regulations) were detected using a set of 24 Ky-
bots.
The evaluation of the task was based on the output
140
KAF generator
.a2 .kaf
.kaf
(with simple events)
Kybot processor
Kybots
(Complex)
.a2
Figure 5: Application of Kybots. Complex events.
Event Class Simple Kyb. Complex Kyb.
Transcription 10
Protein Catabolism 5
Binding 5
Regulation 3
Negative Regulation 5 4
Positive Regulation 3 17
Localization 7
Phosphorylation 18
Gene Exrpesion 12
Total 65 24
Table 2: Number of Kybots generated for each event.
of the system when applied to the test dataset of 347
previously unseen texts. Table 3 shows in the Gold
column the number of instances for each event-type
in the test corpus. R, P and F-score columns stand
for the recall, precision and f-score the system ob-
tained for each type of event. As a consequence of
the characteristics of our system, precision is primed
over recall. For example, the system obtains 95%
and 97% precision on Phosphorylation an Localiza-
tion events, respectively, although its recall is con-
siderably lower (41% and 19%).
4 Conclusions and Future work
This work presents the first results of the applica-
tion of the KYOTO text mining system for extracting
events when ported to the biomedical domain. The
KYOTO technology and data formats have shown to
be flexible enough to be easily adapted to a new task
and domain. Although the results are far from satis-
factory, we must take into account the limited effort
we dedicated to adapting the system and designing
the kybots, which can be roughly estimated in two
Event Class Gold R P F-score
Localization 191 19.90 97.44 33.04
Binding 491 5.30 50.00 9.58
Gene Expression 1002 54.19 42.22 47.47
Transcription 174 13.22 62.16 21.80
Protein catabolism 15 26.67 44.44 33.33
Phosphorylation 185 41.62 95.06 57.89
Non-reg total 2058 34.55 47.27 39.92
Regulation 385 7.53 9.63 8.45
Positive regulation 1443 6.38 62.16 11.57
Negative regulation 571 3.15 26.87 5.64
Regulatory total 2399 5.79 26.94 9.54
All total 4457 19.07 42.08 26.25
Table 3: Performance analysis on the test dataset.
person/months.
After the final evaluation, our system obtained the
thirteenth position out of 15 participating systems
in the main task (processing PubMed abstracts and
full paper articles), obtaining 19.07%, 42.08% and
26.25 recall, precision an f-score, respectively, far
from the best competing system (49.41%, 64.75%
and 56.04%). Although they are far from satisfac-
tory, we must take into account the limited time we
dedicated to adapting the system and designing the
kybots. Apart from that, due to time restrictions,
our system did not make use of the ample set of
resources available, such as named entities, corefer-
ence resolution or syntactic parsing of the sentences.
On the other hand, the system, based on manually
developed rules, obtains reasonable accuracy in the
task of processing full paper articles, obtaining 45%
precision and 21% recall, compared to 59% and 47%
for the best system, which means that the rule-based
approach performs more robustly when dealing with
long texts (5 full texts correspond to approximately
150 abstracts). As we have said before, our main
objective was to evaluate the capabilities of the KY-
OTO technology without adding any additional in-
formation. The use of more linguistic information
will probably facilitate our work and will benefit the
system results. In the near future we will study the
application of machine learning techniques for the
automatic generation of Kybots from the training
data. We also plan to include additional linguistic
and semantic processing in the event extraction pro-
cess to exploit the current semantic and ontological
capabilities of the KYOTO technology.
141
Acknowledgments
This research was supported by the the KYOTO
project (STREP European Community ICT-2007-
211423) and the Basque Government (IT344-10).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini and Carlo Aliprandi. KAF: a generic semantic
annotation format Proceedings of the 5th International
Conference on Generative Approaches to the Lexicon
GL 2009 Pisa, Italy, September 17-19, 2009
Kevin Bretonnel Cohen, Karin Verspoor, Helen L. John-
son, Chris Roeder, Philip V. Ogren, Willian A. Baum-
gartner, Elizabeth White, Hannah Tipney, and Lawer-
ence Hunter. High-precision biological event extrac-
tion: Effects of system and data. Computational Intel-
ligence, to appear, 2011.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. Overview of
BioNLP?09 Shared Task on Event Extraction. Pro-
ceedings of the BioNLP 2009 Workshop. Association
for Computational Linguistics. Boulder, Colorado, pp.
89?96., 2011
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. Proceedings of the
BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Andreas Vlachos. Two Strong Baselines for the BioNLP
2009 Event Extraction Task. Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing. Association for Computational Linguistics Upp-
sala, Sweden, pp. 1?9., 2010
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Chris-
tiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hi-
toshi Isahara, Kyoko Kanzaki, Andrea Marchetti,
Monica Monachini, Federico Neri, Remo Raffaelli,
German Rigau, Maurizio Tescon, Joop VanGent. KY-
OTO: A System for Mining, Structuring, and Dis-
tributing Knowledge Across Languages and Cultures.
Proceedings of LREC 2008. Marrakech, Morocco,
2008.
142
