Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
FBK-IRST: Kernel Methods for Semantic Relation Extraction
Claudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza Romano
FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica
I-38050, Povo (TN), ITALY
{giuliano,lavelli,pighin,romano}@itc.it
Abstract
We present an approach for semantic rela-
tion extraction between nominals that com-
bines shallow and deep syntactic processing
and semantic information using kernel meth-
ods. Two information sources are consid-
ered: (i) the whole sentence where the re-
lation appears, and (ii) WordNet synsets and
hypernymy relations of the candidate nom-
inals. Each source of information is rep-
resented by kernel functions. In particu-
lar, five basic kernel functions are linearly
combined and weighted under different con-
ditions. The experiments were carried out
using support vector machines as classifier.
The system achieves an overall F1 of 71.8%
on the Classification of Semantic Relations
between Nominals task at SemEval-2007.
1 Introduction
The starting point of our research is an approach
for identifying relations between named entities ex-
ploiting only shallow linguistic information, such as
tokenization, sentence splitting, part-of-speech tag-
ging and lemmatization (Giuliano et al, 2006). A
combination of kernel functions is used to represent
two distinct information sources: (i) the global con-
text where entities appear and (ii) their local con-
texts. The whole sentence where the entities appear
(global context) is used to discover the presence of
a relation between two entities. Windows of limited
size around the entities (local contexts) provide use-
ful clues to identify the roles played by the entities
within a relation (e.g., agent and target of a gene in-
teraction). In the task of detecting protein-protein
interactions, we obtained state-of-the-art results on
two biomedical data sets. In addition, promising re-
sults have been recently obtained for relations such
as work for and org based in in the news domain1.
In this paper, we investigate the use of the above
approach to discover semantic relations between
nominals. In addition to the original feature rep-
resentation, we have integrated deep syntactic pro-
cessing of the global context and semantic informa-
tion for each candidate nominals using WordNet as
external knowledge source. Each source of informa-
tion is represented by kernel functions. A tree kernel
(Moschitti, 2004) is used to exploit the deep syn-
tactic processing obtained using the Charniak parser
(Charniak, 2000). On the other hand, bag of syn-
onyms and hypernyms is used to enhance the repre-
sentation of the candidate nominals. The final sys-
tem is based on five basic kernel functions (bag-of-
words kernel, global context kernel, tree kernel, su-
persense kernel, bag of synonyms and hypernyms
kernel) linearly combined and weighted under dif-
ferent conditions. The experiments were carried out
using support vector machines (Vapnik, 1998) as
classifier.
We present results on the Classification of Seman-
tic Relations between Nominals task at SemEval-
2007, in which sentences containing ordered pairs
of marked nominals, possibly semantically related,
have to be classified. On this task, we achieve an
overall F1 of 71.8% (B category evaluation), largely
outperforming all the baselines.
1These results appear in a paper currently under revision.
141
2 Kernel Methods for Relation Extraction
In order to implement the approach based on syntac-
tic and semantic information, we employed a linear
weighted combination of kernels, using support vec-
tor machines as classifier. We designed two families
of basic kernels: syntactic kernels and semantic ker-
nels. These basic kernels are combined by exploit-
ing the closure properties of kernels. We define our
composite kernel KC(x1, x2) as follows
n
?
i=1
wi
Ki(x1, x2)
?
Ki(x1, x1)Ki(x2, x2)
, (1)
where each basic kernel Ki is normalized and wi ?
{0, 1} is the kernel weight. The normalization factor
plays an important role in allowing us to integrate in-
formation from heterogeneous knowledge sources.
All basic kernels, but the tree kernel (see Section
2.1.3), are explicitly calculated as follows
Ki(x1, x2) = ??(x1), ?(x2)?, (2)
where ?(?) is the embedding vector. Even though
the resulting feature space has high dimensionality,
an efficient computation of Equation 2 can be carried
out explicitly since the input representations defined
below are extremely sparse.
2.1 Syntactic Kernels
Syntactic kernels are defined over the whole sen-
tence where the candidate nominals appear.
2.1.1 Global Context Kernel
Bunescu and Mooney (2005) and Giuliano et al
(2006) successfully exploited the fact that relations
between named entities are generally expressed us-
ing only words that appear simultaneously in one of
the following three contexts.
Fore-Between Tokens before and between the two
entities, e.g. ?the head of [ORG], Dr. [PER]?.
Between Only tokens between the two entities, e.g.
?[ORG] spokesman [PER]?.
Between-After Tokens between and after the two
entities, e.g. ?[PER], a [ORG] professor?.
Here, we investigate whether this assumption is
also correct for semantic relations between nomi-
nals. Our global context kernel operates on the con-
texts defined above, where each context is repre-
sented using a bag-of-words. More formally, given
a) S1
S
NP
PRP
I
VP
VBD
found
NP
DT
some
NN
candy
PP
IN
in
NP
PRP$
my
NN
underwear
.
.
b) S
VP
VBD
found
NP
NNS
agent
PP
IN
in
NP
NN
target
Figure 1: A content-container relation test sentence
parse tree (a) and the corresponding RT structure (b).
a relation example R, we represent a context C as a
row vector
?C(R) = (tf(t1, C), tf(t2, C), . . . , tf(tl, C)) ? Rl, (3)
where the function tf(ti, C) records how many
times a particular token ti is used in C . Note that
this approach differs from the standard bag-of-words
as punctuation and stop words are included in ?C ,
while the nominals are not. To improve the classi-
fication performance, we have further extended ?C
to embed n-grams of (contiguous) tokens (up to n =
3). By substituting ?C into Equation 2, we obtain
the n-gram kernel Kn, which counts uni-grams, bi-
grams, . . . , n-grams that two patterns have in com-
mon2. The Global Context kernel KGC(R1, R2) is
then defined as
KF B(R1, R2) +KB(R1, R2) +KBA(R1, R2), (4)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
2.1.2 Bag-of-Words Kernel
The bag-of-words kernel is defined as the previ-
ous kernel but it operates on the whole sentence.
2.1.3 Tree Kernel
Tree kernels can trigger automatic feature selec-
tion and represent a viable alternative to the man-
2In the literature, it is also called n-spectrum kernel.
142
ual design of attribute-value syntactic features (Mos-
chitti, 2004). A tree kernel KT (t1, t2) evaluates
the similarity between two trees t1 and t2 in terms
of the number of fragments they have in common.
Let Nt be the set of nodes of a tree t and F =
{f1, f2, . . . , f|F|} be the fragment space of t1 and
t2. Then
KT (t1, t2) =
P
ni?Nt1
P
nj?Nt2
?(ni, nj) , (5)
where ?(ni, nj) =
?|F|
k=1 Ik(ni) ? IK(nj) and
Ik(n) = 1 if k is rooted in n, 0 otherwise.
For this task, we defined an ad-hoc class of struc-
tured features (Moschitti et al, 2006), the Reduced
Tree (RT), which can be derived from a sentence
parse tree t by the following steps: (1) remove all the
terminal nodes but those labeled as relation entities
and those POS tagged as verbs, auxiliaries, prepo-
sitions, modals or adverbs; (2) remove all the in-
ternal nodes not covering any remaining terminal;
(3) replace the entity words with placeholders that
indicate the direction in which the relation should
hold. Figure 1 shows a parse tree and the resulting
RT structure.
2.2 Semantic Kernels
In (Giuliano et al, 2006), we used the local context
kernel to infer semantic information on the candi-
date entities (i.e., roles played by the entities). As
the task organizers provide the WordNet sense and
role for each nominal, we directly use this informa-
tion to enrich the feature space and do not include
the local context kernel in the combination.
2.2.1 Bag of Synonyms and Hypernyms Kernel
By using the WordNet sense key provided, each
nominal is represented by the bag of its synonyms
and hypernyms (direct and inherited hypernyms).
Formally, given a relation example R, each nominal
N is represented as a row vector
?N(R) = (f(t1, N), f(t2, N), . . . , f(tl, N)) ? Rl, (6)
where the binary function f(ti, N) records if a par-
ticular lemma ti is contained into the bag of syn-
onyms and hypernyms of N. The bag of synonyms
and hypernyms kernel KS&H(R1, R2) is defined as
Ktarget(R1, R2) +Kagent(R1, R2), (7)
where Ktarget and Kagent are defined by substitut-
ing the embedding of the target and agent nominals
into Equation 2 respectively.
2.2.2 Supersense Kernel
WordNet synsets are organized into 45 lexicogra-
pher files, based on syntactic category and logical
groupings. E.g., noun.artifact is for nouns denoting
man-made objects, noun.attribute for nouns denot-
ing attributes for people and objects etc. The super-
sense kernel KSS(R1, R2) is a variant of the previ-
ous kernel that uses the names of the lexicographer
files (i.e., the supersense) to index the feature space.
3 Experimental Setup and Results
Sentences have been tokenized, lemmatized, and
POS tagged with TextPro3. We considered each re-
lation as a different binary classification task, and
each sentence in the data set is a positive or negative
example for the relation. The direction of the rela-
tion is considered labelling the first argument of the
relation as agent and the second as target.
All the experiments were performed using the
SVM package SVMLight-TK4, customized to em-
bed our own kernels. We optimized the linear com-
bination weights wi and regularization parameter c
using 10-fold cross-validation on the training set.
We set the cost-factor j to be the ratio between the
number of negative and positive examples.
Table 1 shows the performance on the test set. We
achieve an overall F1 of 71.8% (B category evalua-
tion), largely outperforming all the baselines, rang-
ing from 48.5% to 57.0%. The average training plus
test running time for a relation is about 10 seconds
on a Intel Pentium M755 2.0 GHz. Figure 2 shows
the learning curves on the test set. For all relations
but theme-tool, accurate classifiers can be learned
using a small fraction of training.
4 Discussion and Conclusion
Experimental results show that our kernel-based ap-
proach is appropriate also to detect semantic rela-
tions between nominals. However, differently from
relation extraction between named entities, there is
not a common kernel setup for all relations. E.g.,
3http://tcc.itc.it/projects/textpro/
4http://ai-nlp.info.uniroma2.it/moschitti/
143
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 30  40  50  60  70  80  90  100
F 1
Percentage of Training
Learning Curve
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container
Figure 2: Learning curves on the test set.
Relation P R F1 Acc
Cause-Effect 67.3 90.2 77.1 72.5
Instrument-Agency 76.9 78.9 77.9 78.2
Product-Producer 76.2 77.4 76.8 68.8
Origin-Entity 62.2 63.9 63.0 66.7
Theme-Tool 69.2 62.1 65.5 73.2
Part-Whole 65.5 73.1 69.1 76.4
Content-Container 78.8 68.4 73.2 74.3
Avg 70.9 73.4 71.8 72.9
Table 1: Results on the test set.
for content-container we obtain the best perfor-
mance combining the tree kernel and the bag of syn-
onyms and hypernyms kernel; on the other hand, for
instrument-agency the best performance is obtained
by combining the global kernel and the supersense
kernel. Surprisingly, the supersense kernel alone
works quite well and obtains results comparable to
the bag of synonyms and hypernyms kernel. This
result is particularly interesting as a supersense tag-
ger can easily provide a satisfactory accuracy (Cia-
ramita and Altun, 2006). On the other hand, ob-
taining an acceptable accuracy in word sense disam-
biguation (required for a realistic application of the
bag of synonyms and hypernyms kernel) is imprac-
tical as a sufficient amount of training for at least all
nouns is currently not available. Hence, the super-
sense could play a crucial role to improve the perfor-
mance when approaching this task without the nomi-
nals disambiguated. To model the global context us-
ing the Fore-Between, Between and Between-After
contexts did not produce a significant improvement
with respect to the bag-of-words model. This is
mainly due to the fact that examples have been col-
lected from the Web using heuristic patterns/queries,
most of which implying Between patterns/contexts
(e.g., for the cause-effect relation ?* comes from *?,
?* out of *? etc.).
5 Acknowledgements
Claudio Giuliano, Alberto Lavelli and Lorenza Ro-
mano are supported by the X-Media project (http:
//www.x-media-project.org), sponsored
by the European Commission as part of the Infor-
mation Society Technologies (IST) programme un-
der EC grant number IST-FP6-026978.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings
of the 19th Conference on Neural Information Pro-
cessing Systems, Vancouver, British Columbia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132?139, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594?602,
Sydney, Australia, July.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of the Eleventh Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-2006), Trento, Italy, 5-7 April.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
CoNLL-X.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York, NY.
144
Exploiting Shallow Linguistic Information for
Relation Extraction from Biomedical Literature
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
We propose an approach for extracting re-
lations between entities from biomedical
literature based solely on shallow linguis-
tic information. We use a combination of
kernel functions to integrate two different
information sources: (i) the whole sen-
tence where the relation appears, and (ii)
the local contexts around the interacting
entities. We performed experiments on ex-
tracting gene and protein interactions from
two different data sets. The results show
that our approach outperforms most of the
previous methods based on syntactic and
semantic information.
1 Introduction
Information Extraction (IE) is the process of find-
ing relevant entities and their relationships within
textual documents. Applications of IE range from
Semantic Web to Bioinformatics. For example,
there is an increasing interest in automatically
extracting relevant information from biomedi-
cal literature. Recent evaluation campaigns on
bio-entity recognition, such as BioCreAtIvE and
JNLPBA 2004 shared task, have shown that sev-
eral systems are able to achieve good performance
(even if it is a bit worse than that reported on news
articles). However, relation identification is more
useful from an applicative perspective but it is still
a considerable challenge for automatic tools.
In this work, we propose a supervised machine
learning approach to relation extraction which is
applicable even when (deep) linguistic process-
ing is not available or reliable. In particular, we
explore a kernel-based approach based solely on
shallow linguistic processing, such as tokeniza-
tion, sentence splitting, Part-of-Speech (PoS) tag-
ging and lemmatization.
Kernel methods (Shawe-Taylor and Cristianini,
2004) show their full potential when an explicit
computation of the feature map becomes compu-
tationally infeasible, due to the high or even infi-
nite dimension of the feature space. For this rea-
son, kernels have been recently used to develop
innovative approaches to relation extraction based
on syntactic information, in which the examples
preserve their original representations (i.e. parse
trees) and are compared by the kernel function
(Zelenko et al, 2003; Culotta and Sorensen, 2004;
Zhao and Grishman, 2005).
Despite the positive results obtained exploiting
syntactic information, we claim that there is still
room for improvement relying exclusively on shal-
low linguistic information for two main reasons.
First of all, previous comparative evaluations put
more stress on the deep linguistic approaches and
did not put as much effort on developing effec-
tive methods based on shallow linguistic informa-
tion. A second reason concerns the fact that syn-
tactic parsing is not always robust enough to deal
with real-world sentences. This may prevent ap-
proaches based on syntactic features from produc-
ing any result. Another related issue concerns the
fact that parsers are available only for few lan-
guages and may not produce reliable results when
used on domain specific texts (as is the case of
the biomedical literature). For example, most of
the participants at the Learning Language in Logic
(LLL) challenge on Genic Interaction Extraction
(see Section 4.2) were unable to successfully ex-
ploit linguistic information provided by parsers. It
is still an open issue whether the use of domain-
specific treebanks (such as the Genia treebank1)
1http://www-tsujii.is.s.u-tokyo.ac.jp/
401
can be successfully exploited to overcome this
problem. Therefore it is essential to better investi-
gate the potential of approaches based exclusively
on simple linguistic features.
In our approach we use a combination of ker-
nel functions to represent two distinct informa-
tion sources: the global context where entities ap-
pear and their local contexts. The whole sentence
where the entities appear (global context) is used
to discover the presence of a relation between two
entities, similarly to what was done by Bunescu
and Mooney (2005b). Windows of limited size
around the entities (local contexts) provide use-
ful clues to identify the roles of the entities within
a relation. The approach has some resemblance
with what was proposed by Roth and Yih (2002).
The main difference is that we perform the extrac-
tion task in a single step via a combined kernel,
while they used two separate classifiers to identify
entities and relations and their output is later com-
bined with a probabilistic global inference.
We evaluated our relation extraction algorithm
on two biomedical data sets (i.e. the AImed cor-
pus and the LLL challenge data set; see Section
4). The motivations for using these benchmarks
derive from the increasing applicative interest in
tools able to extract relations between relevant en-
tities in biomedical texts and, consequently, from
the growing availability of annotated data sets.
The experiments show clearly that our approach
consistently improves previous results. Surpris-
ingly, it outperforms most of the systems based on
syntactic or semantic information, even when this
information is manually annotated (i.e. the LLL
challenge).
2 Problem Formalization
The problem considered here is that of iden-
tifying interactions between genes and proteins
from biomedical literature. More specifically, we
performed experiments on two slightly different
benchmark data sets (see Section 4 for a detailed
description). In the former (AImed) gene/protein
interactions are annotated without distinguishing
the type and roles of the two interacting entities.
The latter (LLL challenge) is more realistic (and
complex) because it also aims at identifying the
roles played by the interacting entities (agent and
target). For example, in Figure 1 three entities
are mentioned and two of the six ordered pairs of
GENIA/topics/Corpus/GTB.html
entities actually interact: (sigma(K), cwlH) and
(gerE, cwlH).
Figure 1: A sentence with two relations, R12 and
R32, between three entities, E1, E2 and E3.
In our approach we cast relation extraction as a
classification problem, in which examples are gen-
erated from sentences as follows.
First of all, we describe the complex case,
namely the protein/gene interactions (LLL chal-
lenge). For this data set entity recognition is per-
formed using a dictionary of protein and gene
names in which the type of the entities is unknown.
We generate examples for all the sentences con-
taining at least two entities. Thus the number of
examples generated for each sentence is given by
the combinations of distinct entities (N ) selected
two at a time, i.e. NC2. For example, as the sen-
tence shown in Figure 1 contains three entities, the
total number of examples generated is 3C2 = 3. In
each example we assign the attribute CANDIDATE
to each of the candidate interacting entities, while
the other entities in the example are assigned the
attribute OTHER, meaning that they do not partici-
pate in the relation. If a relation holds between the
two candidate interacting entities the example is
labeled 1 or 2 (according to the roles of the inter-
acting entities, agent and target, i.e. to the direc-
tion of the relation); 0 otherwise. Figure 2 shows
the examples generated from the sentence in Fig-
ure 1.
Figure 2: The three protein-gene examples gener-
ated from the sentence in Figure 1.
Note that in generating the examples from the
sentence in Figure 1 we did not create three neg-
402
ative examples (there are six potential ordered re-
lations between three entities), thereby implicitly
under-sampling the data set. This allows us to
make the classification task simpler without loos-
ing information. As a matter of fact, generating
examples for each ordered pair of entities would
produce two subsets of the same size containing
similar examples (differing only for the attributes
CANDIDATE and OTHER), but with different clas-
sification labels. Furthermore, under-sampling al-
lows us to halve the data set size and reduce the
data skewness.
For the protein-protein interaction task (AImed)
we use the correct entities provided by the manual
annotation. As said at the beginning of this sec-
tion, this task is simpler than the LLL challenge
because there is no distinction between types (all
entities are proteins) and roles (the relation is sym-
metric). As a consequence, the examples are gen-
erated as described above with the following dif-
ference: an example is labeled 1 if a relation holds
between the two candidate interacting entities; 0
otherwise.
3 Kernel Methods for Relation
Extraction
The basic idea behind kernel methods is to embed
the input data into a suitable feature space F via
a mapping function ? : X ? F , and then use
a linear algorithm for discovering nonlinear pat-
terns. Instead of using the explicit mapping ?, we
can use a kernel function K : X ? X ? R, that
corresponds to the inner product in a feature space
which is, in general, different from the input space.
Kernel methods allow us to design a modular
system, in which the kernel function acts as an
interface between the data and the learning algo-
rithm. Thus the kernel function is the only domain
specific module of the system, while the learning
algorithm is a general purpose component. Po-
tentially any kernel function can work with any
kernel-based algorithm. In our approach we use
Support Vector Machines (Vapnik, 1998).
In order to implement the approach based on
shallow linguistic information we employed a
linear combination of kernels. Different works
(Gliozzo et al, 2005; Zhao and Grishman, 2005;
Culotta and Sorensen, 2004) empirically demon-
strate the effectiveness of combining kernels in
this way, showing that the combined kernel always
improves the performance of the individual ones.
In addition, this formulation allows us to evalu-
ate the individual contribution of each informa-
tion source. We designed two families of kernels:
Global Context kernels and Local Context kernels,
in which each single kernel is explicitly calculated
as follows
K(x1, x2) =
??(x1), ?(x2)?
??(x1)???(x2)?
, (1)
where ?(?) is the embedding vector and ? ? ? is the
2-norm. The kernel is normalized (divided) by the
product of the norms of embedding vectors. The
normalization factor plays an important role in al-
lowing us to integrate information from heteroge-
neous feature spaces. Even though the resulting
feature space has high dimensionality, an efficient
computation of Equation 1 can be carried out ex-
plicitly since the input representations defined be-
low are extremely sparse.
3.1 Global Context Kernel
In (Bunescu and Mooney, 2005b), the authors ob-
served that a relation between two entities is gen-
erally expressed using only words that appear si-
multaneously in one of the following three pat-
terns:
Fore-Between: tokens before and between the
two candidate interacting entities. For in-
stance: binding of [P1] to [P2], interaction in-
volving [P1] and [P2], association of [P1] by
[P2].
Between: only tokens between the two candidate
interacting entities. For instance: [P1] asso-
ciates with [P2], [P1] binding to [P2], [P1],
inhibitor of [P2].
Between-After: tokens between and after the two
candidate interacting entities. For instance:
[P1] - [P2] association, [P1] and [P2] interact,
[P1] has influence on [P2] binding.
Our global context kernels operate on the patterns
above, where each pattern is represented using a
bag-of-words instead of sparse subsequences of
words, PoS tags, entity and chunk types, or Word-
Net synsets as in (Bunescu and Mooney, 2005b).
More formally, given a relation example R, we
represent a pattern P as a row vector
?P (R) = (tf(t1, P ), tf(t2, P ), . . . , tf(tl, P )) ? Rl, (2)
where the function tf(ti, P ) records how many
times a particular token ti is used in P . Note that,
403
this approach differs from the standard bag-of-
words as punctuation and stop words are included
in ?P , while the entities (with attribute CANDI-
DATE and OTHER) are not. To improve the clas-
sification performance, we have further extended
?P to embed n-grams of (contiguous) tokens (up
to n = 3). By substituting ?P into Equation 1, we
obtain the n-gram kernel Kn, which counts com-
mon uni-grams, bi-grams, . . . , n-grams that two
patterns have in common2. The Global Context
kernel KGC(R1, R2) is then defined as
KFB(R1, R2) +KB(R1, R2) +KBA(R1, R2), (3)
where KFB , KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
3.2 Local Context Kernel
The type of the candidate interacting entities can
provide useful clues for detecting the agent and
target of the relation, as well as the presence of the
relation itself. As the type is not known, we use
the information provided by the two local contexts
of the candidate interacting entities, called left and
right local context respectively. As typically done
in entity recognition, we represent each local con-
text by using the following basic features:
Token The token itself.
Lemma The lemma of the token.
PoS The PoS tag of the token.
Orthographic This feature maps each token into
equivalence classes that encode attributes
such as capitalization, punctuation, numerals
and so on.
Formally, given a relation example R, a local con-
text L = t?w, . . . , t?1, t0, t+1, . . . , t+w is repre-
sented as a row vector
?L(R) = (f1(L), f2(L), . . . , fm(L)) ? {0, 1}m, (4)
where fi is a feature function that returns 1 if it is
active in the specified position of L, 0 otherwise3.
The Local Context kernel KLC(R1, R2) is defined
as
Kleft(R1, R2) +Kright(R1, R2), (5)
whereKleft andKright are defined by substituting
the embedding of the left and right local context
into Equation 1 respectively.
2In the literature, it is also called n-spectrum kernel.
3In the reported experiments, we used a context window
of ?2 tokens around the candidate entity.
Notice that KLC differs substantially from
KGC as it considers the ordering of the tokens and
the feature space is enriched with PoS, lemma and
orthographic features.
3.3 Shallow Linguistic Kernel
Finally, the Shallow Linguistic kernel
KSL(R1, R2) is defined as
KGC(R1, R2) +KLC(R1, R2). (6)
It follows directly from the explicit construction
of the feature space and from closure properties of
kernels that KSL is a valid kernel.
4 Data sets
The two data sets used for the experiments concern
the same domain (i.e. gene/protein interactions).
However, they present a crucial difference which
makes it worthwhile to show the experimental re-
sults on both of them. In one case (AImed) in-
teractions are considered symmetric, while in the
other (LLL challenge) agents and targets of genic
interactions have to be identified.
4.1 AImed corpus
The first data set used in the experiments is the
AImed corpus4, previously used for training pro-
tein interaction extraction systems in (Bunescu et
al., 2005; Bunescu and Mooney, 2005b). It con-
sists of 225 Medline abstracts: 200 are known
to describe interactions between human proteins,
while the other 25 do not refer to any interaction.
There are 4,084 protein references and around
1,000 tagged interactions in this data set. In this
data set there is no distinction between genes and
proteins and the relations are symmetric.
4.2 LLL Challenge
This data set was used in the Learning Language
in Logic (LLL) challenge on Genic Interaction
extraction5 (Nede?llec, 2005). The objective of
the challenge was to evaluate the performance of
systems based on machine learning techniques to
identify gene/protein interactions and their roles,
agent or target. The data set was collected by
querying Medline on Bacillus subtilis transcrip-
tion and sporulation. It is divided in a training set
(80 sentences describing 271 interactions) and a
4ftp://ftp.cs.utexas.edu/pub/mooney/
bio-data/interactions.tar.gz
5http://genome.jouy.inra.fr/texte/
LLLchallenge/
404
test set (87 sentences describing 106 interactions).
Differently from the training set, the test set con-
tains sentences without interactions. The data set
is decomposed in two subsets of increasing diffi-
culty. The first subset does not include corefer-
ences, while the second one includes simple cases
of coreference, mainly appositions. Both subsets
are available with different kinds of annotation:
basic and enriched. The former includes word and
sentence segmentation. The latter also includes
manually checked information, such as lemma and
syntactic dependencies. A dictionary of named
entities (including typographical variants and syn-
onyms) is associated to the data set.
5 Experiments
Before describing the results of the experiments,
a note concerning the evaluation methodology.
There are different ways of evaluating perfor-
mance in extracting information, as noted in
(Lavelli et al, 2004) for the extraction of slot
fillers in the Seminar Announcement and the Job
Posting data sets. Adapting the proposed classi-
fication to relation extraction, the following two
cases can be identified:
? One Answer per Occurrence in the Document
? OAOD (each individual occurrence of a
protein interaction has to be extracted from
the document);
? One Answer per Relation in a given Docu-
ment ? OARD (where two occurrences of the
same protein interaction are considered one
correct answer).
Figure 3 shows a fragment of tagged text drawn
from the AImed corpus. It contains three different
interactions between pairs of proteins, for a total
of seven occurrences of interactions. For example,
there are three occurrences of the interaction be-
tween IGF-IR and p52Shc (i.e. number 1, 3 and
7). If we adopt the OAOD methodology, all the
seven occurrences have to be extracted to achieve
the maximum score. On the other hand, if we use
the OARD methodology, only one occurrence for
each interaction has to be extracted to maximize
the score.
On the AImed data set both evaluations were
performed, while on the LLL challenge only the
OAOD evaluation methodology was performed
because this is the only one provided by the eval-
uation server of the challenge.
Figure 3: Fragment of the AImed corpus with all
proteins and their interactions tagged. The pro-
tein names have been highlighted in bold face and
their same subscript numbers indicate interaction
between the proteins.
5.1 Implementation Details
All the experiments were performed using the
SVM package LIBSVM6 customized to embed our
own kernel. For the LLL challenge submission,
we optimized the regularization parameter C by
10-fold cross validation; while we used its default
value for the AImed experiment. In both exper-
iments, we set the cost-factor Wi to be the ratio
between the number of negative and positive ex-
amples.
5.2 Results on AImed
KSL performance was first evaluated on the
AImed data set (Section 4.1). We first give an
evaluation of the kernel combination and then we
compare our results with the Subsequence Ker-
nel for Relation Extraction (ERK) described in
(Bunescu and Mooney, 2005b). All experiments
are conducted using 10-fold cross validation on
the same data splitting used in (Bunescu et al,
2005; Bunescu and Mooney, 2005b).
Table 1 shows the performance of the three ker-
nels defined in Section 3 for protein-protein in-
teractions using the two evaluation methodologies
described above.
We report in Figure 4 the precision-recall curves
of ERK andKSL using OARD evaluation method-
ology (the evaluation performed by Bunescu and
Mooney (2005b)). As in (Bunescu et al, 2005;
Bunescu andMooney, 2005b), the graph points are
obtained by varying the threshold on the classifi-
6http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
405
OAOD
Kernel Precision Recall F1
KGC 57.7 60.1 58.9
KLC 37.3 56.3 44.9
KSL 60.9 57.2 59.0
OARD
Kernel Precision Recall F1
KGC 58.9 66.2 62.2
KLC 44.8 67.8 54.0
KSL 64.5 63.2 63.9
ERK 65.0 46.4 54.2
Table 1: Performance on the AImed data set us-
ing the two evaluation methodologies, OAOD and
OARD.
cation confidence7. The results clearly show that
KSL outperforms ERK, especially in term of re-
call (see Table 1).
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Pr
ec
is
io
n
Recall
KSL vs. ERK
ERK
KSL
Figure 4: Precision-recall curves on the AImed
data set using OARD evaluation methodology.
Finally, Figure 5 shows the learning curve of the
combined kernel KSL using the OARD evaluation
methodology. The curve reaches a plateau with
around 100 Medline abstracts.
5.3 Results on LLL challenge
The system was evaluated on the ?basic? version
of the LLL challenge data set (Section 4.2).
Table 2 shows the results of KSL returned by
the scoring service8 for the three subsets of the
training set (with and without coreferences, and
with their union). Table 3 shows the best results
obtained at the official competition performed in
April 2005. Comparing the results we see that
KSL trained on each subset outperforms the best
7For this purpose the probability estimate output of LIB-
SVM is used.
8http://genome.jouy.inra.fr/texte/
LLLchallenge/scoringService.php
0
0.2
0.4
0.6
0.8
1
0 50 100 150 200
F 1
Number of documents
Figure 5: KSL learning curve on the AImed data
set using OARD evaluation methodology.
Coref. Precision Recall F1
all 56.0 61.4 58.6
with 29.0 31.0 30.0
without 54.8 62.9 58.6
Table 2: KSL performance on the LLL challenge
test set using only the basic linguistic information.
systems of the LLL challenge9. Notice that the
best results at the challenge were obtained by dif-
ferent groups and exploiting the linguistic ?en-
riched? version of the data set. As observed in
(Nede?llec, 2005), the scores obtained using the
training set without coreferences and the whole
training set are similar.
We also report in Table 4 an analysis of the ker-
nel combination. Given that we are interested here
in the contribution of each kernel, we evaluated
the experiments by 10-fold cross-validation on the
whole training set avoiding the submission pro-
cess.
5.4 Discussion of Results
The experimental results show that the combined
kernel KSL outperforms the basic kernels KGC
andKLC on both data sets. In particular, precision
significantly increases at the expense of a lower re-
call. High precision is particularly advantageous
when extracting knowledge from large corpora,
because it avoids overloading end users with too
many false positives.
Although the basic kernels were designed to
model complementary aspects of the task (i.e.
9After the challenge deadline, Reidel and Klein (2005)
achieved a significant improvement, F1 = 68.4% (without
coreferences) and F1 = 64.7% (with and without corefer-
ences).
406
Test set Coref. Precision Recall F1
Enriched all 55.6 53.0 54.3
with 29.0 31.0 24.4
without 60.9 46.2 52.6
Basic all n/a n/a n/a
with 14.0 82.7 24.0
without 50.0 53.8 51.8
Table 3: Best performance on basic and enriched
test sets obtained by participants in the official
competition at the LLL challenge.
Kernel Precision Recall F1
KGC 55.1 66.3 60.2
KLC 44.8 60.1 53.8
KSL 62.1 61.3 61.7
Table 4: Comparison of the performance of kernel
combination on the LLL challenge using 10-fold
cross validation.
presence of the relation and roles of the interact-
ing entities), they perform reasonably well even
when considered separately. In particular, KGC
achieved good performance on both data sets. This
result was not expected on the LLL challenge be-
cause this task requires not only to recognize the
presence of relationships between entities but also
to identify their roles. On the other hand, the out-
comes of KLC on the AImed data set show that
such kernel helps to identify the presence of rela-
tionships as well.
At first glance, it may seem strange that KGC
outperforms ERK on AImed, as the latter ap-
proach exploits a richer representation: sparse
sub-sequences of words, PoS tags, entity and
chunk types, or WordNet synsets. However, an
approach based on n-grams is sufficient to identify
the presence of a relationship. This result sounds
less surprising, if we recall that both approaches
cast the relation extraction problem as a text cate-
gorization task. Approaches to text categorization
based on rich linguistic information have obtained
less accuracy than the traditional bag-of-words ap-
proach (e.g. (Koster and Seutter, 2003)). Shallow
linguistics information seems to be more effective
to model the local context of the entities.
Finally, we obtained worse results performing
dimensionality reduction either based on generic
linguistic assumptions (e.g. by removing words
from stop lists or with certain PoS tags) or using
statistical methods (e.g. tf.idf weighting schema).
This may be explained by the fact that, in tasks like
entity recognition and relation extraction, useful
clues are also provided by high frequency tokens,
such as stop words or punctuation marks, and by
the relative positions in which they appear.
6 Related Work
First of all, the obvious references for our work
are the approaches evaluated on AImed and LLL
challenge data sets.
In (Bunescu and Mooney, 2005b), the authors
present a generalized subsequence kernel that
works with sparse sequences containing combina-
tions of words and PoS tags.
The best results on the LLL challenge were ob-
tained by the group from the University of Ed-
inburgh (Reidel and Klein, 2005), which used
Markov Logic, a framework that combines log-
linear models and First Order Logic, to create a
set of weighted clauses which can classify pairs of
gene named entities as genic interactions. These
clauses are based on chains of syntactic and se-
mantic relations in the parse or Discourse Repre-
sentation Structure (DRS) of a sentence, respec-
tively.
Other relevant approaches include those that
adopt kernel methods to perform relation extrac-
tion. Zelenko et al (2003) describe a relation ex-
traction algorithm that uses a tree kernel defined
over a shallow parse tree representation of sen-
tences. The approach is vulnerable to unrecover-
able parsing errors. Culotta and Sorensen (2004)
describe a slightly generalized version of this ker-
nel based on dependency trees, in which a bag-of-
words kernel is used to compensate for errors in
syntactic analysis. A further extension is proposed
by Zhao and Grishman (2005). They use compos-
ite kernels to integrate information from different
syntactic sources (tokenization, sentence parsing,
and deep dependency analysis) so that process-
ing errors occurring at one level may be overcome
by information from other levels. Bunescu and
Mooney (2005a) present an alternative approach
which uses information concentrated in the short-
est path in the dependency tree between the two
entities.
As mentioned in Section 1, another relevant ap-
proach is presented in (Roth and Yih, 2002). Clas-
sifiers that identify entities and relations among
them are first learned from local information in
the sentence. This information, along with con-
straints induced among entity types and relations,
is used to perform global probabilistic inference
407
that accounts for the mutual dependencies among
the entities.
All the previous approaches have been evalu-
ated on different data sets so that it is not possi-
ble to have a clear idea of which approach is better
than the other.
7 Conclusions and Future Work
The good results obtained using only shallow lin-
guistic features provide a higher baseline against
which it is possible to measure improvements ob-
tained using methods based on deep linguistic pro-
cessing. In the near future, we plan to extend our
work in several ways.
First, we would like to evaluate the contribu-
tion of syntactic information to relation extraction
from biomedical literature. With this aim, we will
integrate the output of a parser (possibly trained on
a domain-specific resource such the Genia Tree-
bank). Second, we plan to test the portability of
our model on ACE and MUC data sets. Third,
we would like to use a named entity recognizer
instead of assuming that entities are already ex-
tracted or given by a dictionary. Our long term
goal is to populate databases and ontologies by
extracting information from large text collections
such as Medline.
8 Acknowledgements
We would like to thank Razvan Bunescu for pro-
viding detailed information about the AImed data
set and the settings of the experiments. Clau-
dio Giuliano and Lorenza Romano have been sup-
ported by the ONTOTEXT project, funded by the
Autonomous Province of Trento under the FUP-
2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005a.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing, Van-
couver, B.C, October.
Razvan Bunescu and Raymond J. Mooney. 2005b.
Subsequence kernels for relation extraction. In
Proceedings of the 19th Conference on Neural In-
formation Processing Systems, Vancouver, British
Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), Barcelona,
Spain.
Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar-
ava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
Cornelis H. A. Koster and Mark Seutter. 2003. Taming
wild phrases. In Advances in Information Retrieval,
25th European Conference on IR Research (ECIR
2003), pages 161?176, Pisa, Italy.
Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna,
Dayne Freitag, Claudio Giuliano, Nicholas Kushm-
erick, and Lorenza Romano. 2004. IE evaluation:
Criticisms and recommendations. In Proceedings of
the AAAI 2004 Workshop on Adaptive Text Extrac-
tion and Mining (ATEM 2004), San Jose, California.
Claire Nede?llec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the ICML-2005 Workshop on Learning Lan-
guage in Logic (LLL05), pages 31?37, Bonn, Ger-
many, August.
Sebastian Reidel and Ewan Klein. 2005. Genic
interaction extraction with semantic and syntactic
chains. In Proceedings of the ICML-2005 Workshop
on Learning Language in Logic (LLL05), pages 69?
74, Bonn, Germany, August.
D. Roth and W. Yih. 2002. Probabilistic reasoning
for entity & relation recognition. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02), Taipei, Taiwan.
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press, New York, NY, USA.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for information
extraction. Journal of Machine Learning Research,
3:1083?1106.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor, Michigan, June.
408
Investigating a Generic Paraphrase-based Approach
for Relation Extraction
Lorenza Romano
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
romano@itc.it
Milen Kouylekov
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
kouylekov@itc.it
Idan Szpektor
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
szpekti@cs.biu.ac.il
Ido Dagan
Department of Computer Science
Bar Ilan University
Ramat Gan, 52900, Israel
dagan@cs.biu.ac.il
Alberto Lavelli
ITC-irst
via Sommarive, 18
38050 Povo (TN), Italy
lavelli@itc.it
Abstract
Unsupervised paraphrase acquisition has
been an active research field in recent
years, but its effective coverage and per-
formance have rarely been evaluated. We
propose a generic paraphrase-based ap-
proach for Relation Extraction (RE), aim-
ing at a dual goal: obtaining an applicative
evaluation scheme for paraphrase acquisi-
tion and obtaining a generic and largely
unsupervised configuration for RE.We an-
alyze the potential of our approach and
evaluate an implemented prototype of it
using an RE dataset. Our findings reveal a
high potential for unsupervised paraphrase
acquisition. We also identify the need for
novel robust models for matching para-
phrases in texts, which should address syn-
tactic complexity and variability.
1 Introduction
A crucial challenge for semantic NLP applica-
tions is recognizing the many different ways for
expressing the same information. This seman-
tic variability phenomenon was addressed within
specific applications, such as question answering,
information extraction and information retrieval.
Recently, the problem was investigated within
generic application-independent paradigms, such
as paraphrasing and textual entailment.
Eventually, it would be most appealing to apply
generic models for semantic variability to concrete
applications. This paper investigates the applica-
bility of a generic ?paraphrase-based? approach to
the Relation Extraction (RE) task, using an avail-
able RE dataset of protein interactions. RE is
highly suitable for such investigation since its goal
is to exactly identify all the different variations in
which a target semantic relation can be expressed.
Taking this route sets up a dual goal: (a) from
the generic paraphrasing perspective - an objective
evaluation of paraphrase acquisition performance
on a concrete application dataset, as well as identi-
fying the additional mechanisms needed to match
paraphrases in texts; (b) from the RE perspective -
investigating the feasibility and performance of a
generic paraphrase-based approach for RE.
Our configuration assumes a set of entailing
templates (non-symmetric ?paraphrases?) for the
target relation. For example, for the target rela-
tion ?X interact with Y? we would assume a set of
entailing templates as in Tables 3 and 7. In addi-
tion, we require a syntactic matching module that
identifies template instances in text.
First, we manually analyzed the protein-
interaction dataset and identified all cases in which
protein interaction is expressed by an entailing
template. This set a very high idealized upper
bound for the recall of the paraphrase-based ap-
proach for this dataset. Yet, obtaining high cover-
age in practice would require effective paraphrase
acquisition and lexical-syntactic template match-
ing. Next, we implemented a prototype that uti-
lizes a state-of-the-art method for learning en-
tailment relations from the web (Szpektor et al,
2004), the Minipar dependency parser (Lin, 1998)
and a syntactic matching module. As expected,
the performance of the implemented system was
much lower than the ideal upper bound, yet ob-
taining quite reasonable practical results given its
unsupervised nature.
The contributions of our investigation follow
409
the dual goal set above. To the best of our knowl-
edge, this is the first comprehensive evaluation
that measures directly the performance of unsuper-
vised paraphrase acquisition relative to a standard
application dataset. It is also the first evaluation of
a generic paraphrase-based approach for the stan-
dard RE setting. Our findings are encouraging for
both goals, particularly relative to their early ma-
turity level, and reveal constructive evidence for
the remaining room for improvement.
2 Background
2.1 Unsupervised Information Extraction
Information Extraction (IE) and its subfield Rela-
tion Extraction (RE) are traditionally performed
in a supervised manner, identifying the different
ways to express a specific information or relation.
Given that annotated data is expensive to produce,
unsupervised or weakly supervised methods have
been proposed for IE and RE.
Yangarber et al (2000) and Stevenson and
Greenwood (2005) define methods for automatic
acquisition of predicate-argument structures that
are similar to a set of seed relations, which rep-
resent a specific scenario. Yangarber et al (2000)
approach was evaluated in two ways: (1) manually
mapping the discovered patterns into an IE system
and running a full MUC-style evaluation; (2) using
the learned patterns to perform document filtering
at the scenario level. Stevenson and Greenwood
(2005) evaluated their method through document
and sentence filtering at the scenario level.
Sudo et al (2003) extract dependency subtrees
within relevant documents as IE patterns. The goal
of the algorithm is event extraction, though perfor-
mance is measured by counting argument entities
rather than counting events directly.
Hasegawa et al (2004) performs unsupervised
hierarchical clustering over a simple set of fea-
tures. The algorithm does not extract entity pairs
for a given relation from a set of documents but
rather classifies all relations in a large corpus. This
approach is more similar to text mining tasks than
to classic IE problems.
To conclude, several unsupervised approaches
learn relevant IE templates for a complete sce-
nario, but without identifying their relevance to
each specific relation within the scenario. Accord-
ingly, the evaluations of these works either did not
address the direct applicability for RE or evaluated
it only after further manual postprocessing.
2.2 Paraphrases and Entailment Rules
A generic model for language variability is us-
ing paraphrases, text expressions that roughly con-
vey the same meaning. Various methods for auto-
matic paraphrase acquisition have been suggested
recently, ranging from finding equivalent lexical
elements to learning rather complex paraphrases
at the sentence level1.
More relevant for RE are ?atomic? paraphrases
between templates, text fragments containing vari-
ables, e.g. ?X buy Y ? X purchase Y?. Under a
syntactic representation, a template is a parsed text
fragment, e.g. ?X subj? interact mod? with pcomp?n? Y?
(based on the syntactic dependency relations of
the Minipar parser). The parses include part-of-
speech tags, which we omit for clarity.
Dagan and Glickman (2004) suggested that a
somewhat more general notion than paraphrasing
is that of entailment relations. These are direc-
tional relations between two templates, where the
meaning of one can be entailed from the meaning
of the other, e.g. ?X bind to Y? X interact with Y?.
For RE, when searching for a target relation, it is
sufficient to identify an entailing template since it
implies that the target relation holds as well. Un-
der this notion, paraphrases are bidirectional en-
tailment relations.
Several methods extract atomic paraphrases by
exhaustively processing local corpora (Lin and
Pantel, 2001; Shinyama et al, 2002). Learn-
ing from a local corpus is bounded by the cor-
pus scope, which is usually domain specific (both
works above processed news domain corpora). To
cover a broader range of domains several works
utilized the Web, while requiring several manu-
ally provided examples for each input relation,
e.g. (Ravichandran and Hovy, 2002). Taking a
step further, the TEASE algorithm (Szpektor et al,
2004) provides a completely unsupervised method
for acquiring entailment relations from the Web
for a given input relation (see Section 5.1).
Most of these works did not evaluate their re-
sults in terms of application coverage. Lin and
Pantel (2001) compared their results to human-
generated paraphrases. Shinyama et al (2002)
measured the coverage of their learning algorithm
relative to the paraphrases present in a given cor-
pus. Szpektor et al (2004) measured ?yield?, the
number of correct rules learned for an input re-
1See the 3rd IWP workshop for a sample of recent works
on paraphrasing (http://nlp.nagaokaut.ac.jp/IWP2005/).
410
lation. Ravichandran and Hovy (2002) evaluated
the performance of a QA system that is based
solely on paraphrases, an approach resembling
ours. However, they measured performance using
Mean Reciprocal Rank, which does not reveal the
actual coverage of the learned paraphrases.
3 Assumed Configuration for RE
Phenomenon Example
Passive form ?Y is activated by X?
Apposition ?X activates its companion, Y?
Conjunction ?X activates prot3 and Y?
Set ?X activates two proteins, Y and Z?
Relative clause ?X, which activates Y?
Coordination ?X binds and activates Y?
Transparent head ?X activates a fragment of Y?
Co-reference ?X is a kinase, though it activates Y?
Table 1: Syntactic variability phenomena, demon-
strated for the normalized template ?X activate Y?.
The general configuration assumed in this pa-
per for RE is based on two main elements: a list
of lexical-syntactic templates which entail the re-
lation of interest and a syntactic matcher which
identifies the template occurrences in sentences.
The set of entailing templates may be collected ei-
ther manually or automatically. We propose this
configuration both as an algorithm for RE and as
an evaluation scheme for paraphrase acquisition.
The role of the syntactic matcher is to iden-
tify the different syntactic variations in which tem-
plates occur in sentences. Table 1 presents a list
of generic syntactic phenomena that are known in
the literature to relate to linguistic variability. A
phenomenon which deserves a few words of ex-
planation is the ?transparent head noun? (Grish-
man et al, 1986; Fillmore et al, 2002). A trans-
parent noun N1 typically occurs in constructs of
the form ?N1 preposition N2? for which the syn-
tactic relation involving N1, which is the head of
the NP, applies to N2, the modifier. In the example
in Table 1, ?fragment? is the transparent head noun
while the relation ?activate? applies to Y as object.
4 Manual Data Analysis
4.1 Protein Interaction Dataset
Bunescu et al (2005) proposed a set of tasks re-
garding protein name and protein interaction ex-
traction, for which they manually tagged about
200 Medline abstracts previously known to con-
tain human protein interactions (a binary symmet-
ric relation). Here we consider their RE task of
extracting interacting protein pairs, given that the
correct protein names have already been identi-
fied. All protein names are annotated in the given
gold standard dataset, which includes 1147 anno-
tated interacting protein pairs. Protein names are
rather complex, and according to the annotation
adopted by Bunescu et al (2005) can be substrings
of other protein names (e.g., <prot> <prot>
GITR </prot> ligand </prot>). In such
cases, we considered only the longest names and
protein pairs involving them. We also ignored all
reflexive pairs, in which one protein is marked
as interacting with itself. Altogether, 1052 inter-
actions remained. All protein names were trans-
formed into symbols of the type ProtN , where N
is a number, which facilitates parsing.
For development purposes, we randomly split
the abstracts into a 60% development set (575 in-
teractions) and a 40% test set (477 interactions).
4.2 Dataset analysis
In order to analyze the potential of our approach,
two of the authors manually annotated the 575 in-
teracting protein pairs in the development set. For
each pair the annotators annotated whether it can
be identified using only template-based matching,
assuming an ideal implementation of the configu-
ration of Section 3. If it can, the normalized form
of the template connecting the two proteins was
annotated as well. The normalized template form
is based on the active form of the verb, stripped
of the syntactic phenomena listed in Table 1. Ad-
ditionally, the relevant syntactic phenomena from
Table 1 were annotated for each template instance.
Table 2 provides several example annotations.
A Kappa value of 0.85 (nearly perfect agree-
ment) was measured for the agreement between
the two annotators, regarding whether a protein
pair can be identified using the template-based
method. Additionally, the annotators agreed on
96% of the normalized templates that should be
used for the matching. Finally, the annotators
agreed on at least 96% of the cases for each syn-
tactic phenomenon except transparent heads, for
which they agreed on 91% of the cases. This high
level of agreement indicates both that template-
based matching is a well defined task and that nor-
malized template form and its syntactic variations
are well defined notions.
Several interesting statistics arise from the an-
411
Sentence Annotation
We have crystallized a complex between human FGF1 and
a two-domain extracellular fragment of human FGFR2.
? template: ?complex between X and Y?
? transparent head: ?fragment of X?
CD30 and its counter-receptor CD30 ligand (CD30L) are
members of the TNF-receptor / TNFalpha superfamily and
function to regulate lymphocyte survival and differentiation.
? template: ?X?s counter-receptor Y?
? apposition
? co-reference
iCdi1, a human G1 and S phase protein phosphatase that
associates with Cdk2.
? template: ?X associate with Y?
? relative clause
Table 2: Examples of annotations of interacting protein pairs. The annotation describes the normalized
template and the different syntactic phenomena identified.
Template f Template f Template f
X interact with Y 28 interaction of X with Y 12 X Y interaction 5
X bind to Y 22 X associate with Y 11 X interaction with Y 4
X Y complex 17 X activate Y 6 association of X with Y 4
interaction between X and Y 16 binding of X to Y 5 X?s association with Y 3
X bind Y 14 X form complex with Y 5 X be agonist for Y 3
Table 3: The 15 most frequent templates and their instance count (f ) in the development set.
notation. First, 93% of the interacting protein pairs
(537/575) can be potentially identified using the
template-based approach, if the relevant templates
are provided. This is a very promising finding,
suggesting that the template-based approach may
provide most of the requested information. We
term these 537 pairs as template-based pairs. The
remaining pairs are usually expressed by complex
inference or at a discourse level.
Phenomenon % Phenomenon %
transparent head 34 relative clause 8
apposition 24 co-reference 7
conjunction 24 coordination 7
set 13 passive form 2
Table 4: Occurrence percentage of each syntactic
phenomenon within template-based pairs (537).
Second, for 66% of the template-based pairs
at least one syntactic phenomenon was annotated.
Table 4 contains the occurrence percentage of each
phenomenon in the development set. These results
show the need for a powerful syntactic matcher on
top of high performance template acquisition, in
order to correctly match a template in a sentence.
Third, 175 different normalized templates were
identified. For each template we counted its tem-
plate instances, the number of times the tem-
plate occurred, counting only occurrences that ex-
press an interaction of a protein pair. In total,
we counted 341 template instances for all 175
templates. Interestingly, 50% of the template in-
stances (184/341) are instances of the 21 most fre-
quent templates. This shows that, though protein
interaction can be expressed in many ways, writ-
ers tend to choose from among just a few common
expressions. Table 3 presents the most frequent
templates. Table 5 presents the minimal number
of templates required to obtain the range of differ-
ent recall levels.
Furthermore, we grouped template variants
that are based on morphological derivations (e.g.
?X interact with Y? and ?X Y interaction?)
and found that 4 groups, ?X interact with Y?,
?X bind to Y?, ?X associate with Y? and ?X com-
plex with Y?, together with their morphological
derivations, cover 45% of the template instances.
This shows the need to handle generic lexical-
syntactic phenomena, and particularly morpholog-
ical based variations, separately from the acquisi-
tion of normalized lexical syntactic templates.
To conclude, this analysis indicates that the
template-based approach provides very high cov-
erage for this RE dataset, and a small number of
normalized templates already provides significant
recall. However, it is important to (a) develop
a model for morphological-based template vari-
ations (e.g. as encoded in Nomlex (Macleod et
al., )), and (b) apply accurate parsing and develop
syntactic matching models to recognize the rather
412
complex variations of template instantiations in
text. Finally, we note that our particular figures
are specific to this dataset and the biological ab-
stracts domain. However, the annotation and anal-
ysis methodologies are general and are suggested
as highly effective tools for further research.
R(%) # templates R(%) # templates
10 2 60 39
20 4 70 73
30 6 80 107
40 11 90 141
50 21 100 175
Table 5: The number of most frequent templates
necessary to reach different recall levels within the
341 template instances.
5 Implemented Prototype
This section describes our initial implementation
of the approach in Section 3.
5.1 TEASE
The TEASE algorithm (Szpektor et al, 2004) is
an unsupervised method for acquiring entailment
relations from the Web for a given input template.
In this paper we use TEASE for entailment rela-
tion acquisition since it processes an input tem-
plate in a completely unsupervised manner and
due to its broad domain coverage obtained from
the Web. The reported percentage of correct out-
put templates for TEASE is 44%.
The TEASE algorithm consists of 3 steps,
demonstrated in Table 6. TEASE first retrieves
from the Web sentences containing the input tem-
plate. From these sentences it extracts variable in-
stantiations, termed anchor-sets, which are identi-
fied as being characteristic for the input template
based on statistical criteria (first column in Ta-
ble 6). Characteristic anchor-sets are assumed to
uniquely identify a specific event or fact. Thus,
any template that appears with such an anchor-set
is assumed to have an entailment relationship with
the input template. Next, TEASE retrieves from
the Web a corpus S of sentences that contain the
characteristic anchor-sets (second column), hop-
ing to find occurrences of these anchor-sets within
templates other than the original input template.
Finally, TEASE parses S and extracts templates
that are assumed to entail or be entailed by the
input template. Such templates are identified as
maximal most general sub-graphs that contain the
anchor sets? positions (third column in Table 6).
Each learned template is ranked by number of oc-
currences in S.
5.2 Transformation-based Graph Matcher
In order to identify instances of entailing templates
in sentences we developed a syntactic matcher that
is based on transformations rules. The matcher
processes a sentence in 3 steps: 1) parsing the sen-
tence with the Minipar parser, obtaining a depen-
dency graph2; 2) matching each template against
the sentence dependency graph; 3) extracting can-
didate term pairs that match the template variables.
A template is considered directly matched in a
sentence if it appears as a sub-graph in the sen-
tence dependency graph, with its variables instan-
tiated. To further address the syntactic phenomena
listed in Table 1 we created a set of hand-crafted
parser-dependent transformation rules, which ac-
count for the different ways in which syntactic
relationships may be realized in a sentence. A
transformation rule maps the left hand side of the
rule, which strictly matches a sub-graph of the
given template, to the right hand side of the rule,
which strictly matches a sub-graph of the sentence
graph. If a rule matches, the template sub-graph is
mapped accordingly into the sentence graph.
For example, to match the syntactic tem-
plate ?X(N) subj? activate(V) obj? Y(N)? (POS
tags are in parentheses) in the sentence ?Prot1
detected and activated Prot2? (see Figure 1) we
should handle the coordination phenomenon.
The matcher uses the transformation rule
?Var1(V) ? and(U)mod? Word(V) conj? Var1(V)?
to overcome the syntactic differences. In this
example Var1 matches the verb ?activate?, Word
matches the verb ?detect? and the syntactic rela-
tions for Word are mapped to the ones for Var1.
Thus, we can infer that the subject and object
relations of ?detect? are also related to ?activate?.
6 Experiments
6.1 Experimental Settings
To acquire a set of entailing templates we first ex-
ecuted TEASE on the input template ?X subj? in-
teract mod? with pcomp?n? Y?, which corresponds to
the ?default? expression of the protein interaction
2We chose a dependency parser as it captures directly the
relations between words; we use Minipar due to its speed.
413
Extracted Anchor-set Sentence containing Anchor-set Learned Template
X=?chemokines?,
Y=?specific receptors?
Chemokines bind to specific receptors on the target
cells
X subj? bind mod?
to
pcomp?n
? Y
X=?Smad3?, Y=?Smad4? Smad3 / Smad4 complexes translocate to the nucleus X Y nn? complex
Table 6: TEASE output at different steps of the algorithm for ?X subj? interact mod? with pcomp?n? Y?.
1. X bind to Y 7. X Y complex 13. X interaction with Y
2. X activate Y 8. X recognize Y 14. X trap Y
3. X stimulate Y 9. X block Y 15. X recruit Y
4. X couple to Y 10. X binding to Y 16. X associate with Y
5. interaction between X and Y 11. X Y interaction 17. X be linked to Y
6. X become trapped in Y 12. X attach to Y 18. X target Y
Table 7: The top 18 correct templates learned by TEASE for ?X interact with Y?.
detect(V )
subjwwppp
pp
pp
pp
pp
conj

mod
''NN
NN
NN
NN
NN
N
obj // Prot2(N)
Prot1(N) activate(V ) and(U)
Figure 1: The dependency parse graph of the sen-
tence ?Prot1 detected and activated Prot2?.
relation. TEASE learned 118 templates for this
relation. Table 7 lists the top 18 learned templates
that we considered as correct (out of the top 30
templates in TEASE output). We then extracted
interacting protein pair candidates by applying the
syntactic matcher to the 119 templates (the 118
learned plus the input template). Candidate pairs
that do not consist of two proteins, as tagged in the
input dataset, were filtered out (see Section 4.1;
recall that our experiments were applied to the
dataset of protein interactions, which isolates the
RE task from the protein name recognition task).
In a subsequent experiment we iteratively ex-
ecuted TEASE on the 5 top-ranked learned tem-
plates to acquire additional relevant templates. In
total, we obtained 1233 templates that were likely
to imply the original input relation. The syntactic
matcher was then reapplied to extract candidate in-
teracting protein pairs using all 1233 templates.
We used the development set to tune a small
set of 10 generic hand-crafted transformation rules
that handle different syntactic variations. To han-
dle transparent head nouns, which is the only phe-
nomenon that demonstrates domain dependence,
we extracted a set of the 5 most frequent trans-
parent head patterns in the development set, e.g.
?fragment of X?.
In order to compare (roughly) our performance
with supervised methods applied to this dataset, as
summarized in (Bunescu et al, 2005), we adopted
their recall and precision measurement. Their
scheme counts over distinct protein pairs per ab-
stract, which yields 283 interacting pairs in our test
set and 418 in the development set.
6.2 Manual Analysis of TEASE Recall
experiment pairs instances
input 39% 37%
input + iterative 49% 48%
input + iterative + morph 63% 62%
Table 8: The potential recall of TEASE in terms of
distinct pairs (out of 418) and coverage of template
instances (out of 341) in the development set.
Before evaluating the system as a whole we
wanted to manually assess in isolation the cover-
age of TEASE output relative to all template in-
stances that were manually annotated in the devel-
opment set. We considered a template as covered
if there is a TEASE output template that is equal
to the manually annotated template or differs from
it only by the syntactic phenomena described in
Section 3 or due to some parsing errors. Count-
ing these matches, we calculated the number of
template instances and distinct interacting protein
pairs that are covered by TEASE output.
Table 8 presents the results of our analysis. The
414
1st line shows the coverage of the 119 templates
learned by TEASE for the input template ?X inter-
act with Y?. It is interesting to note that, though we
aim to learn relevant templates for the specific do-
main, TEASE learned relevant templates also by
finding anchor-sets of different domains that use
the same jargon, such as particle physics.
We next analyzed the contribution of the itera-
tive learning for the additional 5 templates to recall
(2nd line in Table 8). With the additional learned
templates, recall increased by about 25%, showing
the importance of using the iterative steps.
Finally, when allowing matching between a
TEASE template and a manually annotated tem-
plate, even if one is based on a morphologi-
cal derivation of the other (3rd line in Table 8),
TEASE recall increased further by about 30%.
We conclude that the potential recall of the cur-
rent version of TEASE on the protein interaction
dataset is about 60%. This indicates that signif-
icant coverage can be obtained using completely
unsupervised learning from the web, as performed
by TEASE. However, the upper bound for our cur-
rent implemented system is only about 50% be-
cause our syntactic matching does not handle mor-
phological derivations.
6.3 System Results
experiment recall precision F1
input 0.18 0.62 0.28
input + iterative 0.29 0.42 0.34
Table 9: System results on the test set.
Table 9 presents our system results for the test
set, corresponding to the first two experiments in
Table 8. The recall achieved by our current imple-
mentation is notably worse than the upper bound
of the manual analysis because of two general set-
backs of the current syntactic matcher: 1) parsing
errors; 2) limited transformation rule coverage.
First, the texts from the biology domain pre-
sented quite a challenge for the Minipar parser.
For example, in the sentences containing the
phrase ?X bind specifically to Y? the parser consis-
tently attaches the PP ?to? to ?specifically? instead
of to ?bind?. Thus, the template ?X bind to Y? can-
not be directly matched.
Second, we manually created a small number of
transformation rules that handle various syntactic
phenomena, since we aimed at generic domain in-
dependent rules. The most difficult phenomenon
to model with transformation rules is transparent
heads. For example, in ?the dimerization of Prot1
interacts with Prot2?, the transparent head ?dimer-
ization of X? is domain dependent. Transforma-
tion rules that handle such examples are difficult
to acquire, unless a domain specific learning ap-
proach (either supervised or unsupervised) is used.
Finally, we did not handle co-reference resolution
in the current implementation.
Bunescu et al (2005) and Bunescu and Mooney
(2005) approached the protein interaction RE task
using both handcrafted rules and several super-
vised Machine Learning techniques, which uti-
lize about 180 manually annotated abstracts for
training. Our results are not directly comparable
with theirs because they adopted 10-fold cross-
validation, while we had to divide the dataset into
a development and a test set, but a rough compari-
son is possible. For the same 30% recall, the rule-
based method achieved precision of 62% and the
best supervised learning algorithm achieved preci-
sion of 73%. Comparing to these supervised and
domain-specific rule-based approaches our system
is noticeably weaker, yet provides useful results
given that we supply very little domain specific in-
formation and acquire the paraphrasing templates
in a fully unsupervised manner. Still, the match-
ing models need considerable additional research
in order to achieve the potential performance sug-
gested by TEASE.
7 Conclusions and Future Work
We have presented a paraphrase-based approach
for relation extraction (RE), and an implemented
system, that rely solely on unsupervised para-
phrase acquisition and generic syntactic template
matching. Two targets were investigated: (a) a
mostly unsupervised, domain independent, con-
figuration for RE, and (b) an evaluation scheme
for paraphrase acquisition, providing a first evalu-
ation of its realistic coverage. Our approach differs
from previous unsupervised IE methods in that we
identify instances of a specific relation while prior
methods identified template relevance only at the
general scenario level.
We manually analyzed the potential of our ap-
proach on a dataset annotated with protein in-
teractions. The analysis shows that 93% of the
interacting protein pairs can be potentially iden-
tified with the template-based approach. Addi-
415
tionally, we manually assessed the coverage of
the TEASE acquisition algorithm and found that
63% of the distinct pairs can be potentially rec-
ognized with the learned templates, assuming an
ideal matcher, indicating a significant potential re-
call for completely unsupervised paraphrase ac-
quisition. Finally, we evaluated our current system
performance and found it weaker than supervised
RE methods, being far from fulfilling the poten-
tial indicated in our manual analyses due to insuf-
ficient syntactic matching. But, even our current
performance may be considered useful given the
very small amount of domain-specific information
used by the system.
Most importantly, we believe that our analysis
and evaluation methodologies for an RE dataset
provide an excellent benchmark for unsupervised
learning of paraphrases and entailment rules. In
the long run, we plan to develop and improve our
acquisition and matching algorithms, in order to
realize the observed potential of the paraphrase-
based approach. Notably, our findings point to the
need to learn generic morphological and syntactic
variations in template matching, an area which has
rarely been addressed till now.
Acknowledgements
This work was developed under the collaboration
ITC-irst/University of Haifa. Lorenza Romano
has been supported by the ONTOTEXT project,
funded by the Autonomous Province of Trento un-
der the FUP-2004 research program.
References
Razvan Bunescu and Raymond J. Mooney. 2005. Sub-
sequence kernels for relation extraction. In Proceed-
ings of the 19th Conference on Neural Information
Processing Systems, Vancouver, British Columbia.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155. Special Issue on Sum-
marization and Information Extraction from Medi-
cal Documents.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop on Learning Methods for Text Un-
derstanding and Mining, Grenoble, France.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC 2002), pages 787?791, Las Palmas, Spain.
Ralph Grishman, Lynette Hirschman, and Ngo Thanh
Nhan. 1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments. Computa-
tional Linguistics, 12(3).
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discoverying relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), Barcelona, Spain.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation on
MINIPAR. In Proceedings of LREC-98 Workshop
on Evaluation of Parsing Systems, Granada, Spain.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. Nomlex: A lexi-
con of nominalizations. In Proceedings of the 8th
International Congress of the European Association
for Lexicography, Liege, Belgium.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a Question Answering
system. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Philadelphia, PA.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase ac-
quisition from news articles. In Proceedings of
the Human Language Technology Conference (HLT
2002), San Diego, CA.
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005), Ann
Arbor, Michigan.
K. Sudo, S. Sekine, and R. Grishman. 2003. An im-
proved extraction pattern representation model for
automatic IE pattern acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2003), Sapporo, Japan.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisi-
tion of entailment relations. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2004), Barcelona,
Spain.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
Proceedings of the 18th International Conference on
Computational Linguistics, Saarbruecken, Germany.
416
SiSSA - An Infrastructure for NLP Application Development
A. Lavelli and F. Pianesi
ITC-irst
via Sommarive 18
38050 Povo (TN) ITALY
lavelli@itc.it
pianesi@itc.it
E. Maci and I. Prodanof
Ist. di Ling. Comput.
Area di Ricerca CNR
via Alfieri 1, San Cataldo
56010 Pisa ITALY
maci@ilc.pi.cnr.it
irina@ilc.pi.cnr.it
L. Dini and G. Mazzini
CELI
corso Moncalieri 21
10131 Torino ITALY
dini@celi.it
mazzini@celi.it
Abstract
Recently there has been a growing in-
terest in infrastructures for sharing NLP
tools and resources. This paper presents
SiSSA, a project that aims at devel-
oping an infrastructure for prototyping,
editing and validation of NLP applica-
tion architectures. The system will pro-
vide the user with a graphical environ-
ment for (1) selecting the NLP activities
relevant for the particular NLP task and
the associated linguistic processors that
execute them; (2) connecting new lin-
guistic processors to SiSSA; (3) check-
ing that the chosen architectural hy-
pothesis corresponds to the functional
specifications of the given application.
1 Introduction
In recent years there has been a growing inter-
est in the commercial deployment of NLP tech-
nologies and in infrastructures for sharing NLP
tools and resources. Such interest makes more
and more urgent the availability of toolsets that
allow an easy and quick integration of linguistic
resources and modules and the rapid prototyping
of NLP applications. An example of the efforts in
such a direction is GATE (a General Architecture
for Text Engineering, (Cunningham et al, 1997)),
which provides a software infrastructure on top
of which heterogeneous NLP processing modules
may be evaluated and refined individually, or may
be combined into larger application systems.
This paper presents SiSSA (Sistema integrato
di Supporto allo Sviluppo di Applicazioni - Inte-
grated System of Support to Application Devel-
opment), a project with a twofold aim:
  the definition of a common metaformalism
(called FIST) for the unification of different
formalisms for grammar description, and the
implementation of a Grammar Repository
for storing grammars written using FIST;
  the implementation of an infrastructure for
the rapid prototyping and testing of architec-
tures for NLP systems, starting from linguis-
tic processors made available by SiSSA it-
self.
In this paper we concentrate on the latter as-
pect, i.e. the infrastructure for designing NLP ar-
chitectures. To this end, SiSSA provides the user
with a graphical environment for (1) selecting the
linguistic activities which are relevant to the par-
ticular application at hand, along with the linguis-
tic processors that execute them; (2) checking that
the chosen architectural hypothesis corresponds
to the functional specifications of the application;
(3) connecting to SiSSA new linguistic proces-
sors, this way making them available for the pro-
totyping/design activities.
Thus, the design of the architecture of an NLP
system amounts to a) identifying a sequence of
linguistic activities to be performed; b) connect-
ing them in a specific processing chain; and c)
associating each linguistic activity to a suitable
processor, selected among those made available
by SiSSA. The term project is used to refer to the
product of the user?s activity, namely, the archi-
tecture of the NLP application the user is building.
A project encodes processing flows among basic
units, each consisting of a linguistic task that is
executed by a linguistic processor.
Projects have two uses. First, they store the sta-
tus of a user session. Second, they are the main
units of runtime modules: the SiSSA Manager
(see below) interprets projects by executing the
procedures chosen by the user and applying them
to the document selected for execution.
SiSSA makes crucial use of state-of-the-art
software technologies (CORBA, XML) in order
to integrate the various modules in an effective
way. The core of SiSSA has been developed in
Java; hence it can run both on Windows and on
Unix platforms.
SiSSA consists of two parts: an autonomous
application (called SiSSA Manager) and a set
of executable modules, henceforth called proces-
sors. The SiSSA Manager provides an infrastruc-
ture for architecture composition and processor
integration. That is, it provides all the necessary
support to allow the user to select linguistic activ-
ities, connect them in an overall processing chain,
and associate each activity to a linguistic proces-
sor. Moreover, it takes care of executing the pro-
cessing flow encoded in the project, reporting re-
sults to the user, etc.
A major goal while designing SiSSA was to
allow the system to reuse existing processors as
much as possible. This was meant to extend to
processors located in other sites than the user?s.
Already existing processors are written in differ-
ent programming languages and run on different
hardware and software platforms; so this objec-
tive required the adoption of a distributed archi-
tecture, providing:
  flexibility (processors can be developed and
updated independently);
  expandibility (new processors can be added);
  independence from the programming lan-
guages employed to implement the proces-
sors;
  distribution of execution on different hard-
ware platforms.
As a result, the user can exploit for his/her
needs processors that are located anywhere, pro-
vided that they have been notified to SiSSA, and
enclosed in a wrapper so as to comply with the
SiSSA interface (see Section 2.2 and 2.3).
In the following sections, we first present a de-
tailed description of SiSSA. Then some consider-
ations on the pratical use of the system are intro-
duced. Finally some details about the current sta-
tus of the SiSSA implementation and the future
work follow.
2 SiSSA
The SiSSA system consists of:
  the SiSSA Manager;
  the processors;
  the grammars contained in the Grammar
Repository;1
  formal specifications of the interfaces each
processor has to provide in order to be ?in-
tegrable? in SiSSA (this part uses CORBA,
Common Object Request Broker Architec-
ture - http://www.corba.org,);
  protocols for communication and formats for
representation and exchange of information
(achieved using XML (Bray et al, 2000)).
The difference between the third and the fourth
element above is that the CORBA part specifies
the details of the communication process with-
out any reference to the linguistic characteris-
tics of the integrable processors (this part could
be largely reused in other non linguistic projects
involving a distributed architecture); the specifi-
cally linguistic details are embedded in the XML
documents passed between the processors.
2.1 SiSSA Architecture
The central element in the SiSSA architecture is
an autonomous application, called SiSSA Man-
ager. It is autonomous since it takes the initiative
in the management of the processing flow of the
SiSSA system, where it mainly plays the role of
client. Its main tasks are the following:
1This part is not dealt with in the paper.
  to interact with the Processor Repository
(the place where information about proces-
sors known to SiSSA is stored) to take a cen-
sus, activate and connect the processors no-
tified to the system;
  to present the system functionalities to the
user by means of a web-based graphical in-
terface. To this end, the SiSSA Manager
acts as a server with respect to the processors
towards which it mediates the ?centralized?
GUI. Through the latter, the SiSSA Manager
not only interprets the user?s actions but also
gives her/him a report on the ongoing pro-
cessing, storing and presenting logs and sta-
tus messages coming from the active proces-
sors;
  to manage and interpret the projects built by
the user.
The Processor Repository classifies the proces-
sors, by associating each of them to the appro-
priate class of linguistic processors (e.g., mor-
phological analyzers, PoS taggers, etc.).2 The
Processor Repository also provides functionali-
ties for permanently storing the properties asso-
ciated with the processors registered in the repos-
itory. Among them, the properties that specify the
methods for activating a processor are crucial. As
a matter of fact, the single processors must be ac-
tive in order to be available for use by SiSSA. The
activation of a processor takes place by means of
an Activation Server3 reachable via CORBA at
the URL stored in the Processor Repository and
specifying the corresponding activation string.
The information is stored in the repository us-
ing RDF4 and RDFS5 (Resource Description
2Currently the following classes of processors are de-
fined in SiSSA: documentProc, preprocessorProc, textZon-
erProc, nERecognizerProc, morphologyProc, poSTagger-
Proc, syntaxProc, semanticsProc, DiscourseProc, XSLProc.
3In case the processor resides on a computer directly ac-
cessible to the SiSSA Manager, it can be activated by means
of a shell command. In the following we always consider the
case in which the activation server is needed.
4RDF is a W3C Recommendation of 22 February 1999
(Lassila and Swick, 1999) that specifies a declarative lan-
guage (based on XML) formally equivalent to propositional
logics. RDF is usually employed to describe resources on
the web.
5RDF Schema Specification 1.0, published as a W3C
Candidate Recommendation in March 2000 (Brickley and
Guha, 2000).
Framework and RDF Schema). RDF Schema
makes available tools to check that the descrip-
tions of the processors? characteristics comply
with SiSSA Manager?s constraints. The RDF
specification of the processors made available in
SiSSA is usually built using a graphical inter-
face. The adoption of RDF and RDF Schema en-
hances the generality of SiSSA, by avoiding ad
hoc languages for resource description, and ad
hoc schemas for the validation of documents de-
scribing the processors.
Turning to the processors stored in the repos-
itory, they mainly play the role of servers which
are activated upon request by the SiSSA Manager.
The goal of making available distributed ar-
chitectures for projects is pursued through the
adoption of CORBA (Common Object Request
Broker Architecture - http://www.corba.
org, developed by the OMG industry consor-
tium), which acts as the glue keeping together
the executable parts of SiSSA.6 To be available
to SiSSA, processors must be registered in the
Processor Repository. To this end, they must ex-
hibit interfaces that comply with a set of specifi-
cations defined using the CORBA Interface Defi-
nition Language (IDL). Thus, providing the com-
pliant interfaces is a necessary step towards inte-
grating new processors within SiSSA.
As to communication formats, the overall goals
of SiSSA made the adoption of XML (Bray et al,
2000) a natural choice. Thus messages are ex-
changed in the form of XML documents of type
process-data (see Section 2.3). These doc-
uments incorporate in a single structure: the ob-
ject to be processed, and information relevant for
the processing itself (metadata). The generality of
such a format permits its use both for the commu-
nication between the SiSSA Manager and the pro-
cessors, and for those directly taking place among
the processors.
2.2 Communication Protocols
As said, SiSSA provides a set of formal IDL spec-
ifications which the interfaces of processors aim-
ing at being integrated in the environment must
adhere to. Such specifications model the interac-
tion between the SiSSA Manager and the proces-
6The SiSSA Manager uses ORBacus 3.3.2, http://
www.ooc.com/ob/.
Figure 1: The UML diagram of the SiSSA IDLs.
sors, as mediated by CORBA. They can be seen as
a contract that the SiSSA Manager and the proces-
sors have to comply for their mutual integration to
be successful. A UML diagram of the SiSSA IDL
specifications is shown in Figure 1.
The scenario of the cooperation between the
SiSSA Manager and a generic processor can be
described as follows:
  the processor?s activation server starts
and connects on the CORBA bus as a
named server at a specified URL (i.e, the
corbaloc: URL stored in the Processor
Repository);
  the SiSSA Manager, in its turn connected to
the CORBA bus, can contact the activation
server using the corbaloc: URL speci-
fied in the Processor Repository; using the
processor?s activation string it can ask the
server to activate the corresponding proces-
sor;
  from now on, the interaction takes place di-
rectly between the SiSSA Manager and the
processors whose interface it obtained;
  the SiSSA Manager can in this way act as a
true manager, establishing and removing the
connections between the processors accord-
ing to the design of the processing flow de-
cided by the user.
In SiSSA, the communication is asynchronous,
and is implemented by means of a flow of XML
documents. The processors and the SiSSA Man-
ager can be both the source and the target of com-
munication. Moreover, each communication can
have more than one target.
Being a possible target of communications,
each registrable processor provides the function-
alities of the interface IObserver. The SiSSA
Manager?s way to establish/remove the relation-
ships between processors according to the user
requirements amounts to inserting/deleting ob-
servers into a processor?s list of observers.
Besides the communication related to the lin-
guistic processing, other relevant communica-
tion flows concern error messages, and infor-
mation tracing. Logs and messages directed
to the user are managed through the inter-
face IMsgMonitor. Finally, the interface
IStateMonitor (provided by the SiSSA Man-
ager) allows each processor to signal its callers
the status of its own processing (an example of
its use is shown in the bottom bar of the window
shown in Figure 3).
An important service provided by the SiSSA
Manager is the XSL7 processing of XML docu-
ments. To this end, the SiSSA Manager provides
the interface XSLProcServer, through which
XSLProcessor (a processor specialized in XSL
transformations) is made available.8 This feature
allows the insertion of XSL transformations be-
tween any pair of processors, this way providing
the possibility of adapting one processor?s output
to the requirement of the following one(s). This
feature is of the utmost importance for augment-
ing SiSSA?s capabilities of integrating and suc-
cessfully making available a wide range of pro-
cessors.
2.3 Communication and Representation
Formats
Communications take place using a ?data contain-
er? modeled by the interface IDataStream. An
object that implements such interface is sent by a
processor to each of its observers on completion
of its processing.
IDataStream is designed as a container
rather than as a structured model of the data
exchanged. The definition of structured mod-
els for data is completely independent from
IDataStream, and is obtained through differ-
ent means. Indeed, given that the contents of data
streams are XML documents, their structure is
made explicit by means of Document Type Defi-
nitions (DTDs).
SiSSA is a development environment, meant
to be open to the integration of new components,
whereby the latter can differ among them along a
number of dimension, including the input/output
formats. At the same time, SiSSA should allow
the user an adequate level of control over the in-
termediate results produced during the computa-
tion (i.e., the output of each processor). XML al-
lows a representation of data which is transparent
7The Extensible Style Language (XSL (Adler et al,
2000)) is a language that allows to transform data from one
XML representation to another.
8The SiSSA Manager uses the Xalan XSLT processor,
http://xml.apache.org.
and accessible to the developer, without the need
for her/him to know the details of the implementa-
tion of the single components. At the same time,
it does not increase the complexity of the CORBA
interfaces that encapsulate such data.
The data defined in XML are associated to a
document of type process-data. Each doc-
ument of type process-data necessarily in-
cludes two parts:
  linguistic data, usually corresponding to the
result of the computation done by the source
processor;
  metadata. Their role is to specify: the level
of analysis accomplished by the source pro-
cessor (e.g., tokenisation, parsing, etc.); the
unique identifier of the processor originat-
ing the data; further useful information about
processing (time of execution, rules applied,
etc.). Moreover, metadata make available
a unique identifier for the process-data
document. This is useful so to associate the
input with the different output structures pro-
duced by the different processing steps.
The linguistic data have to comply with the
definitions specified for the different classes of
processors. Such classes are identified by the
attribute level-of-analysis present in the
metadata (e.g., morphological analyzer, PoS tag-
ger, chunk parser, etc.) and should take into ac-
count (at least to a certain extent) idiosyncrasies
of specific processors. For instance, a morpho-
logical analyzer can adopt a set of category labels
not entirely coincident with that of another mor-
phological analyzer.
Obviously, a structure that aims to carry lin-
guistic data of different nature, and so differently
represented, can become quite complex when the
levels of analysis taken into consideration in-
crease. Moreover, during the development phase,
the problem arises of the integration of data struc-
tures relative to levels of analysis previously not
taken into consideration, as well as of data struc-
tures idiosyncratic to processors belonging to
some classes. The modular nature of the DTDs
for XML allows a neat distinction among meta-
data, and data relative to classes of processors (id-
iosyncratic data). The former are described in a
single DTD, defined as part of the resources inter-
nal to SiSSA, while the latter can be conveyed by
various DTDs, possibly made available in SiSSA
along with each processor.
As said, each processor at the end of its
processing makes available a document of type
process-data, which contains exclusively the
output data of the specific processor ? and obvi-
ously the corresponding metadata. Such a doc-
ument is a representation of the output of the
processor that generated it, and does not contain
any representation relative to previous levels of
analysis, the input text or the history of the pro-
cessing done so far. Thus, for efficiency reasons
process-data are not incremental collection
of all the data produced by the various processors.
At the same time, the need to keep a link be-
tween the input test and the output produced by
the system cannot be ignored. It is also reason-
able that in certain situations (e.g., during testing
and debugging) the structures produced by the in-
termediate processors, as well as the metadata of
the various processors, are needed to show or save
tracing information. In the proposed architecture,
this task is accomplished by the SiSSA Manager,
that can register itself as an observer of any pro-
cessor; in this way it can access the processor out-
put and show it to the user or build a tracing struc-
ture.9
3 SiSSA at Work
There are two main activities regarding the char-
acteristics of SiSSA described in this paper: the
development of projects and the integration of
processors.
3.1 Projects
The creation and editing of projects takes place
exclusively via the SiSSA graphical interface.
First the user decides which linguistic activities
are relevant to her/his project. Then s/he can
browse the Processor Repository, searching for
those which are suitable to realize each linguis-
tic activity.10 Finally, s/he composes them into a
project.
9The SiSSA Manager uses the Xerces XML parser,
http://xml.apache.org.
10The processors currently available within SiSSA are
some processors developed by the partners of the project: the
morphological analyzer and the parser of NLGRADE (ILC,
When it is necessary to test a given project
on a text the SiSSA Manager prepares a suitable
stream (IDataStream) and sends it to the pro-
cessor selected as the first in the analysis chain.
The processor interprets the metadata, executes
the specified operation on the linguistic data and
finally sends its output to all its observers; some
of them can be required to perform further pro-
cessing on the linguistic data. The output pro-
duced by a processor is sent to the SiSSA Man-
ager as well, so that it can be shown to the user in
a suitable form.
In Figure 2 the starting page of the SiSSA sys-
tem is shown. In the upper part of the window
there are a few buttons that are present in all the
pages of SiSSA. From left to right:
  Home: a link to the starting page of SiSSA;
  SiSSA Manager: a link to the page of the
SiSSA Manager;
  Progetti (projects): a link to the page that
allows to create, edit, and activate the user?s
projects;
  Repository: a link to the page for inter-
acting with the Processor Repository;
  Help: an online help.
Figure 3 shows the applet that interactively
monitors the status of the project currently active
and displays it to the user. In the upper part of
the window the details of the active project are
shown: the processors (left), the connections be-
tween processors (middle), and the XSL filters
(right). In the lower part of the window the mes-
sages coming from the processors are shown. The
bottom bar shows which of the processors/filters
is currently active (using the IStateMonitor
interface described in Section 2.2).
3.2 Integration of processors
Differently from the activity of creation and edit-
ing of projects, only the final part of the work in-
volved in the integration of processors is accom-
plished via the SISSA graphical interface (more
written in C and running under Windows: (Prodanof et al,
1998; Prodanof et al, 2000)) and the preprocessor and the
parser of GEPPETTO (ITC-irst, written in Common Lisp and
running under Solaris: (Ciravegna et al, 1997; Ciravegna et
al., 1998)).
Figure 2: The starting page of SiSSA.
precisely, the registration in the Processor Repos-
itory of the availability of the processors).
In order to make a processor SiSSA-compliant,
the following steps are necessary:
  to provide it with a wrapper so that it com-
municates via the CORBA IDLs of SISSA;
  to make a translation between the proces-
sor?s native input/output and the correspond-
ing linguistic representation specified by
process-data;
  to register the processor in the Processor
Repository using the SiSSA graphical inter-
face; during this step the class of the pro-
cessor, its corbaloc: URL and activation
string have to be specified.
4 Conclusions
The release 1.2 of the SiSSA Manager has been
completed and is currently under use at the sites
involved in the SiSSA project.
Given the emphasis on rapid prototyping,
SiSSA has been developed with flexibility during
the development phase as a primary goal. Obvi-
ously this flexibility is no longer needed when an
application is delivered (on the contrary flexibility
can considerably reduce the performances of the
system). We are currently studying approaches to
allow the delivery of efficient runtime processors.
Currently the SiSSA Manager is a single-user
application. An extension is the possibility of
having more than one user that uses it at the same
time.
The SiSSA system has been developed as part
of the TAL project. TAL is a project partially
funded by the Italian Ministry for University and
Scientific Research.
References
Sharon Adler, Anders Berglund, Jeff Caruso, Stephen
Deach, Paul Grosso, Eduardo Gutentag, Alex
Milowski, Scott Parnell, Jeremy Richman, and
Steve Zilles. 2000. Extensible Stylesheet Lan-
guage (XSL) Version 1.0. W3C Candidate Recom-
mendation 21 November 2000. http://www.
w3.org/TR/xsl/.
Tim Bray, Jean Paoli, C. M. Sperberg-McQueen, and
Eve Maler. 2000. Extensible Markup Language
(XML) 1.0 (Second Edition). W3C Recommenda-
tion 6 October 2000. http://www.w3.org/
TR/REC-xml/.
Dan Brickley and R.V. Guha. 2000. Resource
Description Framework (RDF) Schema Speci-
fication 1.0. W3C Candidate Recommendation
Figure 3: The applet that shows the connections between processors.
27 March 2000. http://www.w3.org/TR/
2000/CR-rdf-schema-20000327/.
Fabio Ciravegna, Alberto Lavelli, Daniela Petrelli, and
Fabio Pianesi. 1997. Participatory Design for Lin-
guistic Engineering: the case of the GEPPETTO
Development Environment. In Proceedings of the
ACL/EACL?97 Workshop on Computational Envi-
ronments for Grammar Development and Linguistic
Engineering, Madrid, Spain, July.
Fabio Ciravegna, Alberto Lavelli, Daniela Petrelli,
and Fabio Pianesi. 1998. Developing language
resources and applications with GEPPETTO. In
Proceedings of the First International Conference
on Language Resources & Evaluation, Granada,
Spain.
Hamish Cunningham, K. Humphreys, Robert
Gaizauskas, and Yorick Wilks. 1997. Software
infrastructure for natural language processing. In
Proceedings of the Fifth Conference on Applied
Natural Language Processing (ANLP-97).
Ora Lassila and Ralph R. Swick. 1999. Re-
source Description Framework (RDF) Model and
Syntax Specification. W3C Recommendation 22
February 1999. http://www.w3.org/TR/
REC-rdf-syntax/.
I. Prodanof, A. Cappelli, L. Moretti, M. Carenini,
P. Moreschini, and M. Vanocchi. 1998. A gram-
mar development environment for reusable and eas-
ily customizable NL applications. In Proceedings
of the First International Conference on Language
Resources & Evaluation, Granada, Spain.
I. Prodanof, A. Cappelli, and L. Moretti. 2000.
Reusability as easy adaptability: A substantial ad-
vance in NL technology. In Proceedings of the
Second International Conference on Language Re-
sources & Evaluation, Athens, Greece.
Simple Information Extraction (SIE):
A Portable and Effective IE System
Claudio Giuliano and Alberto Lavelli and Lorenza Romano
ITC-irst
Via Sommarive, 18
38050, Povo (TN)
Italy
{giuliano,lavelli,romano}@itc.it
Abstract
This paper describes SIE (Simple Infor-
mation Extraction), a modular information
extraction system designed with the goal
of being easily and quickly portable across
tasks and domains. SIE is composed by
a general purpose machine learning algo-
rithm (SVM) combined with several cus-
tomizable modules. A crucial role in the
architecture is played by Instance Filter-
ing, which allows to increase efficiency
without reducing effectiveness. The re-
sults obtained by SIE on several standard
data sets, representative of different tasks
and domains, are reported. The experi-
ments show that SIE achieves performance
close to the best systems in all tasks, with-
out using domain-specific knowledge.
1 Introduction
In designing Information Extraction (IE) systems
based on supervised machine learning techniques,
there is usually a tradeoff between carefully tun-
ing the system to specific tasks and domains and
having a ?generic? IE system able to obtain good
(even if not the topmost) performance when ap-
plied to different tasks and domains (requiring a
very reduced porting time). Usually, the former
alternative is chosen and system performance is
often shown only for a very limited number of
tasks (sometimes even only for a single task), af-
ter a careful tuning. For example, in the Bio-entity
Recognition Shared Task at JNLPBA 2004 (Kim
et al, 2004) the best performing system obtained
a considerable performance improvement adopt-
ing domain specific hacks.
A second important issue in designing IE sys-
tems concerns the fact that usually IE data sets are
highly unbalanced (i.e., the number of positive ex-
amples constitutes only a small fraction with re-
spect to the number of negative examples). This
fact has important consequences. In some ma-
chine learning algorithms the unbalanced distri-
bution of examples can yield a significant loss in
classification accuracy. Moreover, very large data
sets can be problematic to process due to the com-
plexity of many supervised learning techniques.
For example, using kernel methods, such as word
sequence and tree kernels, can become prohibitive
due to the difficulty of kernel based algorithms,
such as Support Vector Machines (SVM) (Cortes
and Vapnik, 1995), to scale to large data sets. As
a consequence, reducing the number of instances
without degrading the prediction accuracy is a cru-
cial issue for applying advanced machine learning
techniques in IE, especially in the case of highly
unbalanced data sets.
In this paper, we present SIE (Simple Informa-
tion Extraction), an information extraction system
based on a supervised machine learning approach
for extracting domain-specific entities from docu-
ments. In particular, IE is cast as a classification
problem by applying SVM to train a set of classi-
fiers, based on a simple and general-purpose fea-
ture representation, for detecting the boundaries of
the entities to be extracted.
SIE was designed with the goal of being easily
and quickly portable across tasks and domains. To
support this claim, we conducted a set of exper-
iments on several tasks in different domains and
languages. The results show that SIE is competi-
tive with the state-of-the-art systems, and it often
outperforms systems customized to a specific do-
main.
SIE resembles the ?Level One? of the ELIE
algorithm (Finn and Kushmerick, 2004). How-
9
ever, a key difference between the two algorithms
is the capability of SIE to drastically reduce the
computation time by exploiting Instance Filtering
(Gliozzo et al, 2005a). This characteristic allows
scaling from toy problems to real-world data sets
making SIE attractive in applicative fields, such as
bioinformatics, where very large amounts of data
have to be analyzed.
2 A Simple IE system
SIE has a modular system architecture. It is com-
posed by a general purpose machine learning algo-
rithm combined with several customizable com-
ponents. The system components are combined
in a pipeline, where each module constrains the
data structures provided by the previous ones.
This modular specification brings significant ad-
vantages. Firstly, a modular architecture is sim-
pler to implement. Secondly, it allows to easily
integrate different machine learning algorithms.
Finally, it allows, if necessary, a fine tuning to
a specific task by simply specializing few mod-
ules. Furthermore, it is worth noting that we tested
SIE across different domains using the same basic
configuration without exploiting any domain spe-
cific knowledge, such as gazetteers, and ad-hoc
pre/post-processing.
Instance
Filtering
Feature
Extraction
Learning
Algorithm
Tag
Matcher
Classification
Algorithm
Instance
Filtering
Feature
Extraction
Lexicon
Training Corpus New Documents
Data Model
Tagged
Documents
Filter Model
Extraction
Script
Extraction
Script
Figure 1: The SIE Architecture.
The architecture of the system is shown in Fig-
ure 1. The information extraction task is per-
formed in two phases. SIE learns off-line a set of
data models from a specified labeled corpus, then
the models are applied to tag new documents.
In both phases, the Instance Filtering module
(Section 3) removes certain tokens from the data
set in order to speed-up the whole process, while
Feature Extraction module (Section 4) is used to
extract a pre-defined set of features from the to-
kens. In the training phase, the Learning Mod-
ule (Section 5) learns two distinct models for each
entity, one for the beginning boundary and an-
other for the end boundary (Ciravegna, 2000; Fre-
itag and Kushmerick, 2000). In the recognition
phase, as a consequence, the Classification mod-
ule (Section 5) identifies the entity boundaries as
distinct token classifications. A Tag Matcher mod-
ule (Section 6) is used to match the boundary pre-
dictions made by the Classification module. Tasks
with multiple entities are considered as multiple
independent single-entity extraction tasks (i.e. SIE
only extracts one entity at a time).
3 Instance Filtering
The purpose of the Instance Filtering (IF) mod-
ule is to reduce the data set size and skewness
by discarding harmful and superfluous instances
without degrading the prediction accuracy. This
is a generic module that can be exploited by any
supervised system that casts IE as a classification
problem.
Instance Filtering (Gliozzo et al, 2005a) is
based on the assumption that uninformative words
are not likely to belong to entities to recognize,
being their information content very low. A naive
implementation of this assumption consists in fil-
tering out very frequent words in corpora because
they are less likely to be relevant than rare words.
However, in IE relevant entities can be composed
by more than one token and in some domains a few
of such tokens can be very frequent in the corpus.
For example, in the field of bioinformatics, protein
names often contain parentheses, whose frequency
in the corpus is very high.
To deal with this problem, we exploit a set of In-
stance Filters (called Stop Word Filters), included
in a Java tool called jInFil1. These filters per-
form a ?shallow? supervision to identify frequent
words that are often marked as positive examples.
The resulting filtering algorithm consists of two
stages. First, the set of uninformative tokens is
identified by training the term filtering algorithm
on the training corpus. Second, instances describ-
ing ?uninformative? tokens are removed from both
the training and the test sets. Note that instances
are not really removed from the data set, but just
1http://tcc.itc.it/research/textec/
tools-resources/jinfil/
10
marked as uninformative. In this way the learning
algorithm will not learn from these instances, but
they will still appear in the feature description of
the remaining instances.
A Stop Word Filter is fully specified by a list of
stop words. To identify such a list, different fea-
ture selection methods taken from the text catego-
rization literature can be exploited. In text catego-
rization, feature selection is used to remove non-
informative terms from representations of texts. In
this sense, IF is closely related to feature selection:
in the former non-informative words are removed
from the instance set, while in the latter they are
removed from the feature set. Below, we describe
the different metrics used to collect a stop word
list from the training corpora.
Information Content (IC) The most commonly
used feature selection metric in text categoriza-
tion is based on document frequency (i.e, the num-
ber of documents in which a term occurs). The
basic assumption is that very frequent terms are
non-informative for document indexing. The fre-
quency of a term in the corpus is a good indica-
tor of its generality, rather than of its information
content. From this point of view, IF consists of
removing all tokens with a very low information
content2.
Correlation Coefficient (CC) In text catego-
rization the ?2 statistic is used to measure the lack
of independence between a term and a category
(Yang and Pedersen, 1997). The correlation coef-
ficient CC2 = ?2 of a term with the negative class
can be used to find those terms that are less likely
to express relevant information in texts.
Odds Ratio (OR) Odds ratio measures the ra-
tio between the odds of a term occurring in the
positive class, and the odds of a term occurring in
the negative class. In text categorization the idea
is that the distribution of the features on the rel-
evant documents is different from the distribution
on non-relevant documents (Raskutti and Kowal-
czyk, 2004). Following this assumption, a term
is non-informative when its probability of being a
negative example is sensibly higher than its prob-
ability of being a positive example (Gliozzo et al,
2005b).
2The information content of a word w can be measured
by estimating its probability from a corpus by the equation
I(w) = ?p(w) log p(w).
An Instance Filter is evaluated by using two
metrics: the Filtering Rate (?), the total percent-
age of filtered tokens in the data set, and the Pos-
itive Filtering Rate (?+), the percentage of pos-
itive tokens (wrongly) removed. A filter is opti-
mized by maximizing ? and minimizing ?+; this
allows us to reduce as much as possible the data
set size preserving most of the positive instances.
We fixed the accepted level of tolerance () on ?+
and found the maximum ? by performing 5-fold
cross-validation on the training set.
4 Feature Extraction
The Feature Extraction module is used to extract
for each input token a pre-defined set of features.
As said above, we consider each token an instance
to be classified as a specific entity boundary or
not. To perform Feature Extraction an applica-
tion called jFex3 was implemented. jFex gener-
ates the features specified by a feature extraction
script, indexes them, and returns the example set,
as well as the mapping between the features and
their indices (lexicon). If specified, it only ex-
tracts features for the instances not marked as ?un-
informative? by instance filtering. jFex is strongly
inspired by FEX (Cumby and Yih, 2003), but it
introduces several improvements. First of all, it
provides an enriched feature extraction language.
Secondly, it makes possible to further extend this
language through a Java API, providing a flexi-
ble tool to define task specific features. Finally,
jFex can output the example set in formats di-
rectly usable by LIBSVM (Chang and Lin, 2001),
SVMlight (Joachims, 1998) and SNoW (Carlson
et al, 1999).
4.1 Corpus Format
The corpus must be prepared in IOBE notation, a
extension of the IOB notation. Both notations do
not allow nested and overlapping entities. Tokens
outside entities are tagged with O, while the first
token of an entity is tagged with B-entity-type, the
last token is tagged E-entity-type, and all the to-
kens inside the entity boundaries are tagged with
I-entity-type, where entity-type is the type of the
marked entity (e.g. protein, person).
Beside the tokens and their types, the nota-
tion allows to represent general purpose and task-
specific annotations defining new columns. Blank
3http://tcc.itc.it/research/textec/
tools-resources/jfex.html.
11
lines can be used to specify sentence or document
boundaries. Table 1 shows an example of a pre-
pared corpus. The columns are: the entity-type,
the PoS tag, the actual token, the token index, and
the output of the instance filter (the ?uninforma-
tive? tokens are marked with 0) respectively.
O TO To 2.12 0
O VB investigate 2.13 0
O IN whether 2.14 0
O DT the 2.15 0
B-cell type NN tumor 2.16 1
O NN expression 2.17 1
O IN of 2.18 0
B-protein NN Beta-2-Microglobulin 2.19 1
O ( ( 2.20 1
B-protein NN Beta 2.21 1
I-protein NN 2 2.22 1
I-protein NN - 2.22 1
E-protein NN M 2.22 1
O ) ) 2.23 1
Table 1: A corpus fragment represented in IOBE
notation.
4.2 Extraction Language
As input to the begin and end classifiers, we use
a bit-vector representation. Each instance is rep-
resented encoding all the following basic features
for the actual token and for all the tokens in a con-
text window of fixed size (in the reported experi-
ments, 3 words before and 3 words after the actual
token):
Token The actual token.
POS The Part of Speech (PoS) of the token.
Token Shapes This feature maps each token into
equivalence classes that encode attributes
such as capitalization, numerals, single char-
acter, and so on.
Bigrams of tokens and PoS tags.
The Feature Extraction language allows to
formally encode the above problem description
through a script. Table 2 provides the extraction
script used in all the tasks4. More details about the
Extraction Language are provided in (Cumby and
Yih, 2003; Giuliano et al, 2005).
4In JNLPBA shared task we added some orthographic fea-
tures borrowed from the bioinformatics literature.
-1 inc loc: w [-3, 3]
-1 inc loc: coloc(w,w) [-3, 3]
-1 inc loc: t [-3, 3]
-1 inc loc: coloc(t,t) [-3, 3]
-1 inc loc: sh [-3, 3]
Table 2: The extraction script used in all tasks.
5 Learning and Classification Modules
As already said, we approach IE as a classifica-
tion problem, assigning an appropriate classifica-
tion label to each token in the data set except for
the tokens marked as irrelevant by the instance fil-
ter. As learning algorithm we use SVM-light5. In
particular, we identify the boundaries that indi-
cate the beginning and the end of each entity as
two distinct classification tasks, following the ap-
proach adopted in (Ciravegna, 2000; Freitag and
Kushmerick, 2000). All tokens that begin(end) an
entity are considered positive instances for the be-
gin(end) classifier, while all the remaining tokens
are negative instances. In this way, two distinct
models are learned, one for the beginning bound-
ary and another for the end boundary. All the pre-
dictions produced by the begin and end classifiers
are then paired by the Tag Matcher module.
When we have to deal with more than one en-
tity (i.e., with a multi-class problem) we train 2n
binary classifiers (where n is the number of entity-
types for the task). Again, all the predictions are
paired by the Tag Matcher module.
6 Tag Matcher
All the positive predictions produced by the begin
and end classifiers are paired by the Tag Matcher
module. If nested or overlapping entities occur,
even if they are of different types, the entity with
the highest score is selected. The score of each
entity is proportional to the entity length probabil-
ity (i.e., the probability that an entity has a certain
length) and the scores assigned by the classifiers to
the boundary predictions. Normalizing the scores
makes it possible to consider the score function as
a probability distribution. The entity length distri-
bution is estimated from the training set.
For example, in the corpus fragment of Table 3
the begin and end classifiers have identified four
possible entity boundaries for the speaker of a
seminar. In the table, the left column shows the
5http://svmlight.joachims.org/
12
Table 3: A corpus fragment with multiple predic-
tions.
O The
O speaker
O will
O be
B-speaker Mr. B-speaker (0.23)
I-speaker John B-speaker (0.1), E-speaker (0.12)
E-speaker Smith E-speaker (0.34)
O .
Table 4: The length distribution for the entity
speaker.
entity len 1 2 3 4 5 ...
P(entity len) 0.10 0.33 0.28 0.02 0.01 ...
actual label, while the right column shows the pre-
dictions and their normalized scores. The match-
ing algorithm has to choose among three mutu-
ally exclusive candidates: ?Mr. John?, ?Mr. John
Smith? and ?John Smith?, with scores 0.23 ?
0.12 ? 0.33 = 0.009108, 0.23 ? 0.34 ? 0.28 =
0.021896 and 0.1 ? 0.34 ? 0.33 = 0.01122, re-
spectively. The length distribution for the entity
speaker is shown in Table 4. In this example, the
matcher, choosing the candidate that maximizes
the score function, namely the second one, extracts
the actual entity.
7 Evaluation
In order to demonstrate that SIE is domain and
language independent we tested it on several tasks
using exactly the same configuration. The tasks
and the experimental settings are described in Sec-
tion 7.1. The results (Section 7.2) show that the
adopted filtering technique decreases drastically
the computation time while preserving (and some-
times improving) the overall accuracy of the sys-
tem.
7.1 The Tasks
SIE was tested on the following IE benchmarks:
JNLPBA Shared Task This shared task (Kim
et al, 2004) is an open challenge task proposed
at the ?International Joint Workshop on Natural
Language Processing in Biomedicine and its Ap-
plications?6. The data set consists of 2, 404 MED-
LINE abstracts from the GENIA project (Kim et
6http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm.
al., 2003), annotated with five entity types: DNA,
RNA, protein, cell-line, and cell-type. The GE-
NIA corpus is split into two partitions: training
(492,551 tokens), and test (101,039 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 0.2% to 6%.
CoNLL 2002 & 2003 Shared Tasks These
shared tasks (Tjong Kim Sang, 2002; Tjong
Kim Sang and De Meulder, 2003)7 concern
language-independent named entity recognition.
Four types of named entities are considered:
persons (PER), locations (LOC), organizations
(ORG) and names of miscellaneous (MISC) en-
tities that do not belong to the previous three
groups. SIE was applied to the Dutch and English
data sets. The Dutch corpus is divided into three
partitions: training and validation (on the whole
258, 214 tokens), and test (73, 866 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set varies
from 1.1% to 2%. The English corpus is divided
into three partitions: training and validation (on
the whole 274, 585 tokens), and test (50, 425 to-
kens). The fraction of positive examples with re-
spect to the total number of tokens in the training
set varies from 1.6% to 3.3%.
TERN 2004 The TERN (Time Expression
Recognition and Normalization) 2004 Evaluation8
requires systems to detect and normalize temporal
expressions occurring in English text (SIE did not
address the normalization part of the task). The
TERN corpus is divided into two partitions: train-
ing (249,295 tokens) and test (72,667 tokens). The
fraction of positive examples with respect to the
total number of tokens in the training set is about
2.1%.
Seminar Announcements The Seminar An-
nouncements (SA) collection (Freitag, 1998) con-
sists of 485 electronic bulletin board postings. The
purpose of each document in the collection is to
announce or relate details of an upcoming talk or
seminar. The documents were annotated for four
entities: speaker, location, stime, and etime. The
corpus is composed by 156, 540 tokens. The frac-
tion of positive examples varies from about 1% to
7http://www.cnts.ua.ac.be/conll2002/
ner/, http://www.cnts.ua.ac.be/conll2003/
ner/.
8http://timex2.mitre.org/tern.html.
13
Metric  ?train/test R P F1 T
0 66.4 67.0 66.7 615
CC 1 64.1/62.3 67.5 67.3 67.4 420
2.5 80.1/78.0 66.6 69.1 67.8 226
5 88.9/86.4 64.8 68.1 66.4 109
OR 1 70.7/68.9 68.3 67.3 67.8 308
2.5 81.0/79.1 67.5 68.3 67.9 193
5 87.8/85.6 65.4 68.2 66.8 114
IC 1 37.3/36.9 58.5 65.7 61.9 570
2.5 38.4/38.0 56.9 65.4 60.9 558
5 39.5/38.9 55.6 65.5 60.1 552
Zhou and Su (2004) 76.0 69.4 72.6
baseline 52.6 43.6 47.7
Table 5: Filtering Rate, Micro-averaged Recall,
Precision, F1 and Time for JNLPBA.
Metric  ?train/test R P F1 T
0 73.6 78.7 76.1 134
CC 1 64.4/64.4 71.6 79.9 75.5 70
2.5 75.1/73.3 72.8 80.3 76.4 50
5 88.6/84.2 66.6 64.7 65.6 24
OR 1 71.5/71.6 72.0 78.3 75.0 61
2.5 82.1/80.7 73.6 78.9 76.2 39
5 90.5/86.1 66.8 64.5 65.6 19
IC 1 47.3/47.5 67.0 79.2 72.6 101
2.5 51.3/51.5 65.9 79.3 72.0 95
5 55.7/56.0 63.8 78.9 70.5 89
Carreras et al (2002) 76.3 77.8 77.1
baseline 45.4 81.3 58.3
Table 6: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2002 (Dutch).
about 2%. The entire document collection is ran-
domly partitioned five times into two sets of equal
size, training and test (Lavelli et al, 2004). For
each partition, learning is performed on the train-
ing set and performance is measured on the corre-
sponding test set. The resulting figures are aver-
aged over the five test partitions.
7.2 Results
The experimental results in terms of filtering rate,
recall, precision, F1, and computation time for
JNLPBA, CoNLL-2002, CoNLL-2003, TERN and
SA are given in Tables 5, 6, 7, 8 and 9 respectively.
To show the differences among filtering strategies
for JNLPBA, CoNLL-2002, TERN 2004 we used
CC, OR and IC filters, while the results for SA
and CoNLL-2003 are reported only for OR filter
(which usually produces the best performance).
For all filters we report results obtained by set-
ting four different values for parameter , the max-
imum value allowed for the Filtering Rate of pos-
itive examples.  = 0 means that no filter is used.
Metric  ?train/test R P F1 T
0 76.7 90.5 83.1 228
OR 1 70.4/83.9 78.2 88.1 82.8 74
2.5 83.6/95.6 76.4 62.6 68.8 33
5 90.5/97.2 75.3 66.5 70.7 14
Florian et al (2003) 88.5 89.0 88.8
baseline 50.9 71.9 59.6
Table 7: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
CoNLL-2003 (English).
Metric  ?train/test R P F1 T
0 77.9 89.8 83.4 82
CC 1 41.8/41.2 76.6 90.7 83.1 57
2.5 64.5/62.8 60.3 88.6 71.7 41
5 86.9/81.7 59.7 76.0 66.9 14
OR 1 56.4/54.6 77.5 91.1 83.8 48
2.5 69.4/66.7 59.8 88.1 71.2 36
5 82.9/79.0 59.5 88.6 71.2 20
IC 1 17.8/17.4 74.9 91.2 82.3 48
2.5 24.0/23.3 74.8 91.5 82.3 36
5 27.6/27.1 75.0 91.5 82.5 20
Table 8: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for
TERN.
The results indicate that both CC and OR do ex-
hibit good performance and are far better than IC
in all the tasks. For example, in the JNLPBA data
set, OR allows to remove more than 70% of the in-
stances, losing less than 1% of the positive exam-
ples. These results pinpoint the importance of us-
ing a supervised metric to collect stop words. The
results also highlight that both CC and OR are ro-
bust against overfitting, because the difference be-
tween the filtering rates in the training and test sets
is minimal. We also report a significant reduction
of the data skewness. Table 10 shows that all the IF
techniques reduce sensibly the skewness ratio, the
ratio between the number of negative and positive
examples, on the JNLPBA data set9. As expected,
both CC and OR consistently outperform IC.
The computation time10 reported includes the
time to perform the overall process of training and
testing the boundary classifiers for each entity11.
The results indicate that both CC and OR are far
superior to IC, allowing a drastic reduction of the
time. Supervised IF techniques are then particu-
9We only report results for this data set as it exhibits the
highest skewness ratios.
10All the experiments have been performed using a dual
1.66 GHz Power Mac G5.
11Execution time for filter optimization is not reported be-
cause it is negligible.
14
Metric  ?train/test R P F1 T
0 81.3 92.5 86.6 179
OR 1 53.6/86.2 81.5 92.1 86.5 91
2.5 69.1/90.8 81.6 90.5 85.9 44
5 74.7/90.8 81.0 85.0 83.0 31
Table 9: Filtering Rate, Micro-averaged Recall,
Precision, F1 and total computation time for SA.
entity  CC OR IC
protein 0 17.1 17.1 17.1
1 7.5 3.8 9.6
2.5 3.0 2.5 9.0
5 1.5 1.4 8.8
DNA 0 59.3 59.3 59.3
1 26.4 18.5 33.2
2.5 14.7 12.6 31.7
5 8.3 8.6 32.4
RNA 0 596.2 596.2 596.2
1 250.7 253.1 288.4
2.5 170.4 170.1 274.5
5 92.4 111.1 280.7
cell type 0 72.9 72.9 72.9
1 13.8 13.4 43.2
2.5 6.3 6.5 43.9
5 3.4 4.4 44.5
cell line 0 146.4 146.4 146.4
1 40.4 41.6 87.7
2.5 24.2 25.9 87.5
5 13.6 14.6 89.6
Table 10: Skewness ratio of each entity for
JNLPBA.
larly convenient when dealing with large data sets.
For example, using the CC metric the time re-
quired by SIE to perform the JNLPBA task is re-
duced from 615 to 109 minutes (see Table 5).
Both OR and CC allow to drastically reduce
the computation time and maintain the prediction
accuracy12 with small values of . Using OR,
for example, with  = 2.5% on JNLPBA, F1 in-
creases from 66.7% to 67.9%. On the contrary,
for CoNLL-2002 and TERN, for  > 2.5% and
 > 1% respectively, the performance of all the
filters rapidly declines. The explanation for this
behavior is that, for the last two tasks, the differ-
ence between the filtering rates on the training and
test sets becomes much larger for  > 2.5% and
 > 1%, respectively. That is, the data skewness
changes significantly from the training to the test
set. It is not surprising that an extremely aggres-
sive filtering step reduces too much the informa-
tion available to the classifiers, leading the overall
12For JNLPBA, CoNLL 2002 & 2003 and Tern 2004, re-
sults are obtained using the official evaluation software made
available by the organizers of the tasks.
performance to decrease.
SIE achieves results close to the best systems in
all tasks13. It is worth noting that state-of-the-art
IE systems often exploit external, domain-specific
information (e.g. gazetteers (Carreras et al, 2002)
and lexical resources (Zhou and Su, 2004)) while
SIE adopts exactly the same feature set and does
not use any external or task dependent knowledge
source.
8 Conclusion and Future Work
The portability, the language independence and
the efficiency of SIE suggest its applicability in
practical problems (e.g. semantic web, infor-
mation extraction from biological data) in which
huge collections of texts have to be processed ef-
ficiently. In this perspective we are pursuing the
recognition of bio-entities from several thousands
of MEDLINE abstracts. In addition, the effective-
ness of instance filtering will allow us to experi-
ment with complex kernel methods. For the fu-
ture, we plan to implement more aggressive in-
stance filtering schemata for Entity Recognition,
by performing a deeper semantic analysis of the
texts.
Acknowledgments
SIE was developed in the context of the IST-
Dot.Kom project (http://www.dot-kom.
org), sponsored by the European Commission as
part of the Framework V (grant IST-2001-34038).
Claudio Giuliano and Lorenza Romano have been
supported by the ONTOTEXT project, funded by
the Autonomous Province of Trento under the
FUP-2004 research program.
References
Andrew J. Carlson, ChadM. Cumby, Jeff L. Rosen, and
Dan Roth. 1999. SNoW user?s guide. Technical
Report UIUCDCS-DCS-R-99-210, Department of
Computer Science, University of Illinois at Urbana-
Champaign, April.
Xavier Carreras, Llu??s Ma?rques, and Llu??s Padro?.
2002. Named entity extraction using adaboost. In
Proceedings of CoNLL-2002, Taipei, Taiwan.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines.
13Note that the TERN results cannot be disclosed, so no di-
rect comparison can be provided. For the reasons mentioned
in (Lavelli et al, 2004), direct comparison cannot be provided
for Seminar Announcements as well.
15
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Fabio Ciravegna. 2000. Learning to tag for infor-
mation extraction. In F. Ciravegna, R. Basili, and
R. Gaizauskas, editors, Proceedings of the ECAI
workshop on Machine Learning for Information Ex-
traction, Berlin.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297.
Chad Cumby and W. Yih. 2003. FEX user guide.
Technical report, Department of Computer Science,
University of Illinois at Urbana-Champaign, April.
Aidan Finn and Nicholas Kushmerick. 2004. Multi-
level boundary classification for information extrac-
tion. In Proceedings of the 15th European Confer-
ence on Machine Learning, Pisa, Italy.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named entity recognition
through classifier combination. In Walter Daele-
mans and Miles Osborne, editors, Proceedings of
CoNLL-2003, pages 168?171. Edmonton, Canada.
Dayne Freitag and Nicholas Kushmerick. 2000.
Boosted wrapper induction. In Proceedings of the
17th National Conference on Artificial Intelligence
(AAAI 2000), pages 577?583.
Dayne Freitag. 1998. Machine Learning for Informa-
tion Extraction in Informal Domains. Ph.D. thesis,
Carnegie Mellon University.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2005. Simple information extraction (SIE).
Technical report, ITC-irst.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005a. Instance filtering for en-
tity recognition. SIGKDD Explorations (special is-
sue on Text Mining and Natural Language Process-
ing), 7(1):11?18, June.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and
Raffaella Rinaldi. 2005b. Instance pruning by fil-
tering uninformative words: an Information Extrac-
tion case study. In Proceedings of the Sixth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005), Mexico
City, Mexico, 13-19 February.
T. Joachims. 1998. Making large-scale support
vector machine learning practical. In A. Smola
B. Scho?lkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
J. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl.1):180?182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recog-
nition task at JNLPBA. In N. Collier, P. Ruch,
and A. Nazarenko, editors, Proceedings of the In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications
(JNLPBA-2004), pages 70?75, Geneva, Switzer-
land, August 28?29.
A. Lavelli, M. Califf, F. Ciravegna, D. Freitag, C. Giu-
liano, N. Kushmerick, and L. Romano. 2004. IE
evaluation: Criticisms and recommendations. In
AAAI-04 Workshop on Adaptive Text Extraction and
Mining (ATEM-2004), San Jose, California.
Bhavani Raskutti and Adam Kowalczyk. 2004.
Extreme re-balancing for SVMs: a case study.
SIGKDD Explor. Newsl., 6(1):60?69.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, pages 142?147. Edmon-
ton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 shared task: Language-independent
named entity recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
Yiming Yang and Jan O. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Douglas H. Fisher, editor, Proceedings of the
14th International Conference on Machine Learning
(ICML-97), pages 412?420, Nashville, US. Morgan
Kaufmann Publishers, San Francisco, US.
Guo Dong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recogni-
tion. In Proceedings of 2004 Joint Workshop on Nat-
ural Processing in Biomedicine and its Applications,
Geneva, Switzerland.
16
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 420?429,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Combining Tree Structures, Flat Features and Patterns
for Biomedical Relation Extraction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
Kernel based methods dominate the current
trend for various relation extraction tasks
including protein-protein interaction (PPI)
extraction. PPI information is critical in un-
derstanding biological processes. Despite
considerable efforts, previously reported
PPI extraction results show that none of the
approaches already known in the literature
is consistently better than other approaches
when evaluated on different benchmark PPI
corpora. In this paper, we propose a
novel hybrid kernel that combines (auto-
matically collected) dependency patterns,
trigger words, negative cues, walk fea-
tures and regular expression patterns along
with tree kernel and shallow linguistic ker-
nel. The proposed kernel outperforms the
exiting state-of-the-art approaches on the
BioInfer corpus, the largest PPI benchmark
corpus available. On the other four smaller
benchmark corpora, it performs either bet-
ter or almost as good as the existing ap-
proaches. Moreover, empirical results show
that the proposed hybrid kernel attains con-
siderably higher precision than the existing
approaches, which indicates its capability
of learning more accurate models. This also
demonstrates that the different types of in-
formation that we use are able to comple-
ment each other for relation extraction.
1 Introduction
Kernel methods are considered the most effective
techniques for various relation extraction (RE)
tasks on both general (e.g. newspaper text) and
specialized (e.g. biomedical text) domains. In
particular, as the importance of syntactic struc-
tures for deriving the relationships between en-
tities in text has been growing, several graph
and tree kernels have been designed and experi-
mented.
Early RE approaches more or less fall in one of
the following categories: (i) exploitation of statis-
tics about co-occurrences of entities, (ii) usage of
patterns and rules, and (iii) usage of flat features
to train machine learning (ML) classifiers. These
approaches have been studied for a long period
and have their own pros and cons. Exploitation
of co-occurrence statistics results in high recall
but low precision, while rule or pattern based ap-
proaches can increase precision but suffer from
low recall. Flat feature based ML approaches em-
ploy various kinds of linguistic, syntactic or con-
textual information and integrate them into the
feature space. They obtain relatively good results
but are hindered by drawbacks of limited feature
space and excessive feature engineering. Kernel
based approaches have become an attractive alter-
native solution, as they can exploit huge amount
of features without an explicit representation.
In this paper, we propose a new hybrid kernel
for RE. We apply the kernel to Protein?protein
interaction (PPI) extraction, the most widely re-
searched topic in biomedical relation extraction.
PPI1 information is very critical in understanding
biological processes. Considerable progress has
been made for this task. Nevertheless, empirical
results of previous studies show that none of the
approaches already known in the literature is con-
sistently better than other approaches when evalu-
ated on different benchmark PPI corpora (see Ta-
ble 4). This demands further study and innovation
1PPIs occur when two or more proteins bind together,
and are integral to virtually all cellular processes, such as
metabolism, signalling, regulation, and proliferation (Tikk
et al 2010).
420
of new approaches that are sensitive to the varia-
tions of complex linguistic constructions.
The proposed hybrid kernel is the composition
of one tree kernel and two feature based kernels
(one of them is already known in the literature
and the other is proposed in this paper for the first
time). The novelty of the newly proposed feature
based kernel is that it envisages to accommodate
the advantages of pattern based approaches. More
precisely:
1. We propose a new feature based kernel (de-
tails in Section 4.1) by using syntactic de-
pendency patterns, trigger words, negative
cues, regular expression (henceforth, regex)
patterns and walk features (i.e. e-walks and
v-walks)2.
2. The syntactic dependency patterns are au-
tomatically collected from a type of depen-
dency subgraph (we call it reduced graph,
more details in Section 4.1.1) during run-
time.
3. We only use the regex patterns, trigger words
and negative cues mentioned in the literature
(Ono et al 2001; Fundel et al 2007; Bui et
al., 2010). The objective is to verify whether
we can exploit knowledge which is already
known and used.
4. We propose a hybrid kernel by combin-
ing the proposed feature based kernel (out-
lined above) with the Shallow Linguistic
(SL) kernel (Giuliano et al 2006) and the
Path-enclosed Tree (PET) kernel (Moschitti,
2004).
The aim of our work is to take advantage of
different types of information (i.e., dependency
patterns, regex patterns, trigger words, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) which can complement each other to learn
more accurate models.
2The syntactic dependencies of the words of a sentence
create a dependency graph. A v-walk feature consists of
(wordi ? dependency typei,i+1 ? wordi+1), and an e-
walk feature is composed of (dependency typei?1,i ?
wordi ? dependency typei,i+1). Note that, in a depen-
dency graph, the words are nodes while the dependency
types are edges.
The remainder of the paper is organized as fol-
lows. In Section 2, we briefly review previous
work. Section 3 lists the datasets. Then, in Sec-
tion 4, we define our proposed hybrid kernel and
describe its individual component kernels. Sec-
tion 5 outlines the experimental settings. Follow-
ing that, empirical results are discussed in Section
6. Finally, we conclude with a summary of our
study as well as suggestions for further improve-
ment of our approach.
2 Related Work
In this section, we briefly discuss some of the
recent work on PPI extraction. Several RE ap-
proaches have been reported to date for the PPI
task, most of which are kernel based methods.
Tikk et al(2010) reported a benchmark evalu-
ation of various kernels on PPI extraction. An
interesting finding is that the Shallow Linguis-
tic (SL) kernel (Giuliano et al 2006) (to be dis-
cussed in Section 4.2), despite its simplicity, is on
par with the best kernels in most of the evaluation
settings.
Kim et al(2010) proposed walk-weighted sub-
sequence kernel using e-walks, partial matches,
non-contiguous paths, and different weights for
different sub-structures (which are used to capture
structural similarities during kernel computation).
Miwa et al(2009a) proposed a hybrid kernel,
which combines the all-paths graph (APG) kernel
(Airola et al 2008), the bag-of-words kernel, and
the subset tree kernel (Moschitti, 2006) (applied
on the shortest dependency paths between target
protein pairs). They used multiple parser inputs.
The system is regarded as the current state-of-the-
art PPI extraction system because of its high re-
sults on different PPI corpora (see the results in
Table 4).
As an extension of their work, they boosted sys-
tem performance by training on multiple PPI cor-
pora instead of on a single corpus and adopting
a corpus weighting concept with support vector
machine (SVM) which they call SVM-CW (Miwa
et al 2009b). Since most of their results are re-
ported by training on the combination of multi-
ple corpora, it is not possible to compare them
directly with the results published in the other re-
lated works (that usually adopt 10-fold cross vali-
dation on a single PPI corpus). To be comparable
with the vast majority of the existing work, we
also report results using 10-fold cross validation
421
Corpus Sentences Positive pairs Negative pairs
BioInfer 1,100 2,534 7,132
AIMed 1,955 1,000 4,834
IEPA 486 335 482
HPRD50 145 163 270
LLL 77 164 166
Table 1: Basic statistics of the 5 benchmark PPI cor-
pora.
on single corpora.
Apart from the approaches described above,
there also exist other studies that used kernels for
PPI extraction (e.g. subsequence kernel (Bunescu
and Mooney, 2006)).
A notable exception is the work published by
Bui et al(2010). They proposed an approach that
consists of two phases. In the first phase, their
system categorizes the data into different groups
(i.e. subsets) based on various properties and pat-
terns. Later they classify candidate PPI pairs in-
side each of the groups using SVM trained with
features specific for the corresponding group.
3 Data
There are 5 benchmark corpora for the PPI task
that are frequently used: HPRD50 (Fundel et al
2007), IEPA (Ding et al 2002), LLL (Ne?dellec,
2005), BioInfer (Pyysalo et al 2007) and AIMed
(Bunescu et al 2005). These corpora adopt dif-
ferent PPI annotation formats. For a comparative
evaluation Pyysalo et al(2008) put all of them
in a common format which has become the stan-
dard evaluation format for the PPI task. In our
experiments, we use the versions of the corpora
converted to such format.
Table 1 shows various statistics regarding the 5
(converted) corpora.
4 Proposed Hybrid Kernel
The hybrid kernel that we propose is as follows:
KHybrid (R1, R2) = KTPWF (R1, R2)
+ KSL (R1, R2) + w * KPET (R1, R2)
where KTPWF stands for the new feature
based kernel (henceforth, TPWF kernel) com-
puted using flat features collected by exploiting
patterns, trigger words, negative cues and walk
features. KSL and KPET stand for the Shallow
Linguistic (SL) kernel and the Path-enclosed Tree
(PET) kernel respectively. w is a multiplicative
constant used for the PET kernel. It allows the
hybrid kernel to assign more (or less) weight to
the information obtained using tree structures de-
pending on the corpus. The proposed hybrid ker-
nel is valid according to the closure properties of
kernels.
Both the TPWF and SL kernels are linear ker-
nels, while PET kernel is computed using Unlex-
icalized Partial Tree (uPT) kernel (Severyn and
Moschitti, 2010). The following subsections ex-
plain each of the individual kernels in more detail.
4.1 Proposed TPWF Kernel
4.1.1 Reduced graph, trigger words,
negative cues and dependency patterns
For each of the candidate entity pairs, we
construct a type of subgraph from the depen-
dency graph formed by the syntactic dependen-
cies among the words of a sentence. We call it
?reduced graph? and define it in the follow-
ing way:
A reduced graph is a subgraph
of the dependency graph of a sentence
which includes:
? the two candidate entities and their
governor nodes up to their least
common governor (if exists).
? dependent nodes (if exist) of all the
nodes added in the previous step.
? the immediate governor(s) (if ex-
ists) of the least common governor.
Figure 1 shows an example of a reduced graph.
A reduced graph is an extension of the smallest
common subgraph of the dependency graph that
aims at overcoming its limitations. It is a known
issue that the smallest common subgraph (or sub-
tree) sometimes does not contain cue words. Pre-
viously, Chowdhury et al(2011a) proposed a lin-
guistically motivated extension of the minimal
(i.e. smallest) common subtree (which includes
the candidate entity pairs), known as Mildly Ex-
tended Dependency Tree (MEDT). However, the
rules used for MEDT are too constrained. Our ob-
jective in constructing the reduced graph is to in-
clude any potential modifier(s) or cue word(s) that
describes the relation between the given pair of
entities. Sometimes such modifiers or cue words
are not directly dependent (syntactically) on any
422
BioInfer AIMed IEPA HPRD50 LLL
P R F P R F P R F P R F P R F
Only walk features 51.8 71.2 60.0 48.7 63.2 55.0 61.0 75.2 67.4 60.2 65.0 62.5 64.6 87.8 74.4
Features: dep. patterns, 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
trigger, neg. cues, walks
Features: dep. patterns, 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
trigger, neg. cues, walks,
regex patterns
Table 2: Results of the proposed TPWF feature based kernel on 5 benchmark PPI corpora before and after adding
features collected using dependency patterns, regex patterns, trigger words and negative cues to the walk features.
The TPWF kernel is a component of the new hybrid kernel.
Figure 1: Dependency graph for the sentence ?A pVHL mutant containing a P154L substitution does not promote
degradation of HIF1-Alpha? generated by the Stanford parser. The edges with blue dots form the smallest
common subgraph for the candidate entity pair pVHL and HIF1-Alpha, while the edges with red dots form the
reduced graph for the pair.
of the entities (of the candidate pair). Rather they
are dependent on some other word(s) which is de-
pendent on one (or both) of the entities. The word
?not? in Figure 1 is one such example. The re-
duced graph aims to preserve these cue words.
The following types of features are collected
from the reduced graph of a candidate pair:
1. HasTriggerWord: whether the least common
governor(s) of the target entity pairs inside
the reduced graph matches any trigger word.
2. Trigger-X: whether the least common gov-
ernor(s) of the target entity pairs inside the
reduced graph matches the trigger word ?X?.
3. HasNegWord: whether the reduced graph
contains any negative word.
4. DepPattern-i: whether the reduced graph
contains all the syntactic dependencies of the
i-th pattern of dependency pattern list.
The dependency pattern list is automatically
constructed from the training data during the
learning phase. Each pattern is a set of syntactic
dependencies of the corresponding reduced graph
of a (positive or negative) entity pair in the train-
ing data. For example, the dependency pattern for
the reduced graph in Figure 1 is {det, amod, part-
mod, nsubj, aux, neg, dobj, prep of}. The same
dependency pattern might be constructed for mul-
tiple (positive or negative) entity pairs. However,
if it is constructed for both positive and negative
pairs, it has to be discarded from the pattern list.
The dependency patterns allow some kind of
underspecification as they do not contain the lex-
ical items (i.e. words) but contain the likely com-
bination of syntactic dependencies that a given re-
lated pair of entities would pose inside their re-
duced graph.
The list of trigger words contains 144 words
previously used by Bui et al(2010) and Fundel
et al(2007). The list of negative cues contain 18
words, most of which are mentioned in Fundel et
al. (2007).
4.1.2 Walk features
We extract e-walk and v-walk features from
the Mildly Extended Dependency Tree (MEDT)
(Chowdhury et al 2011a) of each candidate pair.
Reduced graphs sometimes include some unin-
423
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
Proposed TPWF kernel 53.8 68.8 60.4 50.6 63.9 56.5 63.9 74.6 68.9 65.0 71.8 68.2 66.5 89.6 76.4
(without regex)
Proposed TPWF kernel 53.5 68.6 60.1 52.5 62.9 57.2 63.8 74.6 68.8 65.1 69.9 67.5 67.4 88.4 76.5
(with regex)
SL kernel 60.8 65.8 63.2 56.2 64.4 60.0 73.3 71.9 72.6 62.0 65.0 63.5 74.9 85.4 79.8
PET kernel 72.8 74.9 73.9 44.8 72.8 55.5 70.7 77.9 74.2 65.0 73.0 68.8 72.1 89.6 79.9
Proposed hybrid kernel 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
(PET + SL + TPWF
(without regex))
Proposed hybrid kernel 80.1 72.0 75.9 64.4 58.3 61.2 79.3 69.6 74.1 71.9 61.4 66.2 70.6 95.1 81.0
(PET + SL + TPWF
(with regex))
Table 3: Results of the proposed hybrid kernel and its individual components. Pos. and Neg. refer to number
positive and negative relations respectively. PET refers to the path-enclosed tree kernel, SL refers to the shallow
linguistic kernel, and TPWF refers to the kernel computed using trigger, pattern, negative cue and walk features.
formative words which produce uninformative
walk features. Hence, they are not suitable for
walk feature generation. MEDT suits better for
this purpose. The walk features extracted from
MEDTs have the following properties:
? The directionality of the edges (or nodes) in
an e-walk (or v-walk) is not considered. In
other words, e.g., pos(stimulatory)?amod?
pos(effects) and pos(effects) ? amod ?
pos(stimulatory) are treated as the same fea-
ture.
? The v-walk features are of the form (posi ?
dependency typei,i+1?posi+1). Here, posi is
the POS tag of wordi, i is the governor node
and i + 1 is the dependent node.
? The e-walk features are of the form
(dep. typei?1,i ? posi ? dep. typei,i+1) and
(dep. typei?1,i ? lemmai ? dep. typei,i+1).
Here, lemmai is the lemmatized form of
wordi.
? Usually, the e-walk features are con-
structed using dependency types be-
tween {governor of X, node X} and
{node X, dependent of X}. However,
we also extract e-walk features from
the dependency types between any two
dependents and their common governor
(i.e. {node X, dependent 1 of X} and
{node X, dependent 2 of X}).
Apart from the above types of features, we also
add features for lemmas of the immediate preced-
ing and following words of the candidate entities.
These feature names are augmented with -1 or +1
depending on whether the corresponding words
are preceded or followed by a candidate entity.
4.1.3 Regular expression patterns
We use a set of 22 regex patterns as binary
features. These patterns were previously used
by Ono et al(2001) and Bui et al(2010).
If there is a match for a pattern (e.g. ?En-
tity 1.*activates.*Entity 2? where Entity 1 and
Entity 2 form the candidate entity pair) in a given
sentence, value 1 is added for the feature (i.e., pat-
tern) inside the feature vector.
4.2 Shallow Linguistic (SL) Kernel
The Shallow Linguistic (SL) kernel was proposed
by Giuliano et al(2006). It is one of the best
performing kernels applied on different biomedi-
cal RE tasks such as PPI and DDI (drug-drug in-
teraction) extraction (Tikk et al 2010; Segura-
Bedmar et al 2011; Chowdhury and Lavelli,
2011b; Chowdhury et al 2011c). It is defined
as follows:
KSL (R1, R2) = KLC (R1, R2) + KGC
(R1, R2)
424
BioInfer AIMed IEPA HPRD50 LLL
Pos. / Neg. 2,534 / 7,132 1,000 / 4,834 335 / 482 163 / 270 164 / 166
P R F P R F P R F P R F P R F
SL kernel ? ? ? 60.9 57.2 59.0 ? ? ? ? ? ? ? ? ?
(Giuliano et al 2006)
APG kernel 56.7 67.2 61.3 52.9 61.8 56.4 69.6 82.7 75.1 64.3 65.8 63.4 72.5 87.2 76.8
(Airola et al 2008)
Hybrid kernel and 65.7 71.1 68.1 55.0 68.8 60.8 67.5 78.6 71.7 68.5 76.1 70.9 77.6 86.0 80.1
multiple parser input
(Miwa et al 2009a)
SVM-CW, multiple ? ? 67.6 ? ? 64.2 ? ? 74.4 ? ? 69.7 ? ? 80.5
parser input and graph,
walk and BOW features
(Miwa et al 2009b)
kBSPS kernel 49.9 61.8 55.1 50.1 41.4 44.6 58.8 89.7 70.5 62.2 87.1 71.0 69.3 93.2 78.1
(Tikk et al 2010)
Walk weighted 61.8 54.2 57.6 61.4 53.3 56.6 73.8 71.8 72.9 66.7 69.2 67.8 76.9 91.2 82.4
subsequence kernel
(Kim et al 2010)
2 phase extraction 61.7 57.5 60.0 55.3 68.5 61.2 ? ? ? ? ? ? ? ? ?
(Bui et al 2010)
Our proposed hybrid 80.0 71.4 75.5 64.2 58.2 61.1 81.1 69.3 74.7 72.9 59.5 65.5 70.4 95.7 81.1
kernel (PET + SL +
TPWF without regex)
Table 4: Comparison of the results on the 5 benchmark PPI corpora. Pos. and Neg. refer to number positive and
negative relations respectively. The underlined numbers indicate the best results for the corresponding corpus
reported by any of the existing state-of-the-art approaches. The results of Bui et al(2010) on LLL, HPRD50,
and IEPA are not reported since thy did not use all the positive and negative examples during cross validation.
Miwa et al(2009b) showed that better results can be obtained using multiple corpora for training. However,
we consider only those results of their experiments where they used single training corpus as it is the standard
evaluation approach adopted by all the other studies on PPI extraction for comparing results. All the results of
the previous approaches reported in this table are directly quoted from their respective original papers.
where KSL, KGC and KLC correspond to SL,
global context (GC) and local context (LC) ker-
nels respectively. The GC kernel exploits contex-
tual information of the words occurring before,
between and after the pair of entities (to be in-
vestigated for RE) in the corresponding sentence;
while the LC kernel exploits contextual informa-
tion surrounding individual entities.
4.3 Path-enclosed tree (PET) Kernel
The path-enclosed tree (PET) kernel3 was first
proposed by Moschitti (2004) for semantic role
labeling. It was later successfully adapted by
Zhang et al(2005) and other works for relation
extraction on general texts (such as newspaper do-
3Also known as shortest path-enclosed tree (SPT) kernel.
main). A PET is the smallest common subtree of a
phrase structure tree that includes the two entities
involved in a relation.
A tree kernel calculates the similarity between
two input trees by counting the number of com-
mon sub-structures. Different techniques have
been proposed to measure such similarity. We use
the Unlexicalized Partial Tree (uPT) kernel (Sev-
eryn and Moschitti, 2010) for the computation of
the PET kernel since a comparative evaluation by
Chowdhury et al(2011a) reported that uPT ker-
nels achieve better results for PPI extraction than
the other techniques used for tree kernel compu-
tation.
425
5 Experimental Settings
We have followed the same criteria commonly
used for the PPI extraction tasks, i.e. abstract-
wise 10-fold cross validation on individual corpus
and one-answer-per-occurrence criterion. In fact,
we have used exactly the same (abstract-wise)
fold splitting of the 5 benchmark (converted) cor-
pora used by Tikk et al(2010) for benchmarking
various kernel methods4.
The Charniak-Johnson reranking parser (Char-
niak and Johnson, 2005), along with a self-trained
biomedical parsing model (McClosky, 2010), has
been used for tokenization, POS-tagging and
parsing of the sentences. Before parsing the sen-
tences, all the entities are blinded by assigning
names as EntityX where X is the entity index.
In each example, the POS tags of the two can-
didate entities are changed to EntityX . The
parse trees produced by the Charniak-Johnson
reranking parser are then processed by the Stan-
ford parser5 (Klein and Manning, 2003) to obtain
syntactic dependencies according to the Stanford
Typed Dependency format.
The Stanford parser often skips some syntactic
dependencies in output. We use the following two
rules to add some of such dependencies:
? If there is a ?conj and? or ?conj or? depen-
dency between two words X and Y, then X
should be dependent on any word Z on which
Y is dependent and vice versa.
? If there are two verbs X and Y such that in-
side the corresponding sentence they have
only the word ?and? or ?or? between them,
then any word Z dependent on X should be
also dependent on Y and vice versa.
Our system exploits SVM-LIGHT-TK6 (Mos-
chitti, 2006; Joachims, 1999). We made minor
changes in the toolkit to compute the proposed
hybrid kernel. The ratio of negative and positive
examples has been used as the value of the cost-
ratio-factor parameter. We have done parameter
tuning following the approach described by Hsu
et al(2003).
4Downloaded from http://informatik.hu-
berlin.de/forschung /gebiete/wbi/ppi-benchmark .
5http://nlp.stanford.edu/software/lex-parser.shtml
6http://disi.unitn.it/moschitti/Tree-Kernel.htm
6 Results and Discussion
To measure the contribution of the features col-
lected from the reduced graphs (using dependency
patterns, trigger words and negative cues) and
regex patterns, we have applied the new TPWF
kernel on the 5 PPI corpora before and after using
these features. Results shown in Table 2 clearly
indicate that usage of these features improve the
performance. The improvement of performance
is primarily due to the usage of dependency pat-
terns which resulted in higher precision for all the
corpora.
We have tried to measure the contribution of
the regex patterns. However, from the empirical
results a clear trend does not emerge (see Table
2).
Table 3 shows a comparison among the re-
sults of the proposed hybrid kernel and its indi-
vidual components. As we can see, the overall
results of the hybrid kernel (with and without us-
ing regex pattern features) are better than those
by any of its individual component kernels. Inter-
estingly, precision achieved on the 4 benchmark
corpora (other than the smallest corpus LLL) is
much higher for the hybrid kernel than for the in-
dividual components. This strongly indicates that
these different types of information (i.e. depen-
dency patterns, regex patterns, triggers, negative
cues, syntactic dependencies among words and
constituent parse trees) and their different repre-
sentations (i.e. flat features, tree structures and
graphs) can complement each other to learn more
accurate models.
Table 4 shows a comparison of the PPI extrac-
tion results of our proposed hybrid kernel with
those of other state-of-the-art approaches. Since
the contribution of regex patterns in the perfor-
mance of the hybrid kernel was not relevant (as
Tables 2 and 3 show), we used the results of pro-
posed hybrid kernel without regex for the compar-
ison. As we can see, the proposed kernel achieves
significantly higher results on the BioInfer corpus,
the largest benchmark PPI corpus (2,534 positive
PPI pair annotations) available, than any of the
existing approaches. Moreover, the results of the
proposed hybrid kernel are on par with the state-
of-the-art results on the other smaller corpora.
Furthermore, empirical results show that the
proposed hybrid kernel attains considerably
higher precision than the existing approaches.
426
Since a dependency pattern, by construction,
contains all the syntactic dependencies inside the
corresponding reduced graph, it may happen that
some of the dependencies (e.g. det or determiner)
are not informative for classifying the label of the
corresponding class label (i.e., positive or nega-
tive relation) of the pattern. Their presence in-
side a pattern might make it unnecessarily rigid
and less general. So, we tried to identify and dis-
card such non informative dependencies by mea-
suring probabilities of the dependencies with re-
spect to the class label and then removing any of
them which has probability lower than a threshold
(we tried with different threshold values). But do-
ing so decreased the performance. This suggests
that the syntactic dependencies of a dependency
pattern are not independent of each other even if
some of them might have low probability (with
respect to the class label) individually. We plan to
further investigate whether there could be differ-
ent criteria for identifying non informative depen-
dencies. For the work reported in this paper, we
used the dependency patterns as they are initially
constructed.
We also did experiments to see whether collect-
ing features for trigger words from the whole re-
duced graph would help. But that also decreased
performance. This suggests that trigger words are
more likely to appear in the least common gover-
nors.
7 Conclusion
In this paper, we have proposed a new hybrid
kernel for RE that combines two vector based
kernels and a tree kernel. The proposed kernel
outperforms any of the exiting approaches by a
wide margin on the BioInfer corpus, the largest
PPI benchmark corpus available. On the other
four smaller benchmark corpora, it performs ei-
ther better or almost as good as the existing state-
of-the art approaches.
We have also proposed a novel feature based
kernel, called TPWF kernel, using (automatically
collected) dependency patterns, trigger words,
negative cues, walk features and regular expres-
sion patterns. The TPWF kernel is used as a com-
ponent of the new hybrid kernel.
Empirical results show that the proposed hy-
brid kernel achieves considerably higher precision
than the existing approaches, which indicates its
capability of learning more accurate models. This
also demonstrates that the different types of infor-
mation that we use are able to complement each
other for relation extraction.
We believe there are at least three ways to
further improve the proposed approach. First
of all, the 22 regular expression patterns (col-
lected from Ono et al(2001) and Bui et al
(2010)) are applied at the level of the sen-
tences and this sometimes produces unwanted
matches. For example, consider the sentence
?X activates Y and inhibits Z? where X, Y,
and Z are entities. The pattern ?Entity1. ?
activates. ?Entity2? matches both the X?Y and
X?Z pairs in the sentence. But only the X?Y pair
should be considered. So, the patterns should
be constrained to reduce the number of unwanted
matches. For example, they could be applied on
smaller linguistic units than full sentences. Sec-
ondly, different techniques could be used to iden-
tify less-informative syntactic dependencies in-
side dependency patterns to make them more ac-
curate and effective. Thirdly, usage of automati-
cally collected paraphrases of regular expression
patterns instead of the patterns directly could be
also helpful. Weakly supervised collection of
paraphrases for RE has been already investigated
(e.g. Romano et al(2006)) and, hence, can be
tried for improving the TPWF kernel (which is a
component of the proposed hybrid kernel).
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management
in cancer care?. The authors are grateful to Alessan-
dro Moschitti for his help in the use of SVM-LIGHT-
TK. We also thank the anonymous reviewers for help-
ful suggestions.
References
Antti Airola, Sampo Pyysalo, Jari Bjorne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein inter-
action extraction with evaluation of cross-corpus
learning. BMC Bioinformatics, 9(Suppl 11):S2.
Quoc-Chinh Bui, Sophia Katrenko, and Peter M.A.
Sloot. 2010. A hybrid approach to extract protein-
protein interactions. Bioinformatics.
Razvan Bunescu and Raymond J. Mooney. 2006.
Subsequence kernels for relation extraction. In Pro-
ceedings of NIPS 2006, pages 171?178.
427
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL 2005.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2011b. Drug-drug interaction extraction using com-
posite kernels. In Proceedings of DDIExtrac-
tion2011: First Challenge Task: Drug-Drug In-
teraction Extraction, pages 27?33, Huelva, Spain,
September.
Md. Faisal Mahbub Chowdhury, Alberto Lavelli, and
Alessandro Moschitti. 2011a. A study on de-
pendency tree kernels for automatic extraction of
protein-protein interaction. In Proceedings of
BioNLP 2011 Workshop, pages 124?133, Portland,
Oregon, USA, June.
Md. Faisal Mahbub Chowdhury, Asma Ben Abacha,
Alberto Lavelli, and Pierre Zweigenbaum. 2011c.
Two dierent machine learning techniques for drug-
drug interaction extraction. In Proceedings of
DDIExtraction2011: First Challenge Task: Drug-
Drug Interaction Extraction, pages 19?26, Huelva,
Spain, September.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining MEDLINE: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2007. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-
mano. 2006. Exploiting shallow linguistic infor-
mation for relation extraction from biomedical lit-
erature. In Proceedings of EACL 2006, pages 401?
408.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical
guide to support vector classification. Department
of Computer Science and Information Engineering,
National Taiwan University, Taipei, Taiwan.
Thorsten Joachims. 1999. Making large-scale sup-
port vector machine learning practical. In Advances
in kernel methods: support vector learning, pages
169?184. MIT Press, Cambridge, MA, USA.
Seonho Kim, Juntae Yoon, Jihoon Yang, and Seog
Park. 2010. Walk-weighted subsequence kernels
for protein-protein interaction extraction. BMC
Bioinformatics, 11(1).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL
2003, pages 423?430, Sapporo, Japan.
David McClosky. 2010. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Department of Computer
Science, Brown University.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009a. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, 78.
Makoto Miwa, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009b. A rich feature vector for
protein-protein interaction extraction from multiple
corpora. In Proceedings of EMNLP 2009, pages
121?130, Singapore.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceed-
ings of ACL 2004, Barcelona, Spain.
Alessandro Moschitti. 2006. Making Tree Kernels
Practical for Natural Language Learning. In Pro-
ceedings of EACL 2006, Trento, Italy.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. Proceedings
of the ICML 2005 workshop: Learning Language in
Logic (LLL05), pages 31?37.
Toshihide Ono, Haretsugu Hishigaki, Akira Tanigami,
and Toshihisa Takagi. 2001. Automated ex-
traction of information on protein?protein interac-
tions from the biological literature. Bioinformatics,
17(2):155?161.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: a corpus for information
extraction in the biomedical domain. BMC Bioin-
formatics, 8(1):50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein in-
teraction corpora. BMC Bioinformatics, 9(Suppl
3):S6.
Lorenza Romano, Milen Kouylekov, Idan Szpektor,
Ido Dagan, and Alberto Lavelli. 2006. Investi-
gating a generic paraphrase?based approach for re-
lation extraction. In Proceedings of EACL 2006,
pages 409?416.
Isabel Segura-Bedmar, Paloma Mart??nez, and Cesar de
Pablo-Sa?nchez. 2011. Using a shallow linguistic
kernel for drug-drug interaction extraction. Jour-
nal of Biomedical Informatics, In Press, Corrected
Proof, Available online, 24 April.
Aliaksei Severyn and Alessandro Moschitti. 2010.
Fast cutting plane training for structural kernels. In
Proceedings of ECML-PKDD 2010.
Domonkos Tikk, Philippe Thomas, Peter Palaga,
Jo?rg Hakenberg, and Ulf Leser. 2010. A Compre-
hensive Benchmark of Kernel Methods to Extract
Protein-Protein Interactions from Literature. PLoS
Computational Biology, 6(7), July.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
428
between named entities from a large raw corpus us-
ing tree similarity-based clustering. In Natural Lan-
guage Processing ? IJCNLP 2005, volume 3651 of
Lecture Notes in Computer Science, pages 378?389.
Springer Berlin / Heidelberg.
429
Proceedings of NAACL-HLT 2013, pages 765?771,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Exploiting the Scope of Negations and Heterogeneous Features
for Relation Extraction: A Case Study for Drug-Drug Interaction Extraction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
fmchowdhury@gmail.com, lavelli@fbk.eu
Abstract
This paper presents an approach that exploits
the scope of negation cues for relation extrac-
tion (RE) without the need of using any specif-
ically annotated dataset for building a separate
negation scope detection classifier. New fea-
tures are proposed which are used in two dif-
ferent stages. These also include non-target
entity specific features. The proposed RE ap-
proach outperforms the previous state of the
art for drug-drug interaction (DDI) extraction.
1 Introduction
Negation is a linguistic phenomenon where a nega-
tion cue (e.g. not) can alter the meaning of a partic-
ular text segment or of a fact. This text segment (or
fact) is said to be inside the scope of that negation
(cue). In the context of RE, there is not much work
that aims to exploit the scope of negations.1 The
only work on RE that we are aware of is Sanchez-
Graillet and Poesio (2007) where they used various
heuristics to extract negative protein interaction.
Despite the recent interest on automatically de-
tecting the scope of negation2 till now there seems
to be no empirical evidence supporting its exploita-
tion for the purpose of RE. Even if we could man-
age to obtain highly accurate automatically detected
1In the context of event extraction (a closely related task of
RE), there have been efforts in BioNLP shared tasks of 2009 and
2011 for (non-mandatory sub-task of) event negation detection
(3 participants in 2009; 2 in 2011) (Kim et al, 2009; Kim et al,
2011). The participants approached the sub-task using either
pre-defined patterns or some heuristics.
2This task is popularized by various recently held shared
tasks (Farkas et al, 2010; Morante and Blanco, 2012).
negation scopes, it is not clear how to feed this infor-
mation inside the RE approach. Simply considering
whether a pair of candidate mentions falls under the
scope of a negation cue might not be helpful.
In this paper, we propose that the scope of nega-
tions can be exploited at two different levels. Firstly,
the system would check whether all the target en-
tity3 mentions inside a sentence along with possible
relation clues (or trigger words), if any, fall (directly
or indirectly) under the scope of a negation cue. If
such a sentence is found, then it should be discarded
(i.e. candidate mention pairs4 inside that sentence
would not be considered). Secondly, for each of the
remaining pairs of candidate mentions, the system
should exploit features related to the scope of nega-
tion (rather than simply adding a feature for negation
cue, approach adopted in various RE systems) that
can provide indication (if any such evidence exists)
that the corresponding relation of interest actually
does not hold in that particular context.
In the subsequent sections, we describe our ap-
proach. The RE task considered is drug-drug in-
teraction (DDI) extraction. The task has signifi-
cant importance for public health safety.5 We used
3The target entities, for example, for DDI extraction and for
EMP-ORG relation extraction would be {DRUG} and {PER,
GPE, ORG} respectively. Any entity other than the target enti-
ties (w.r.t. the particular RE task) belongs to non-target entities.
4Candidate mention pairs for RE are taken from target entity
mentions.
5After the death of pop star Michael Jackson, allegedly due
to DDI, it was reported that about 2.2 million people in USA,
age 57 to 85, were taking potentially dangerous combinations of
drugs (Landau, 2009). An earlier report mentioned that deaths
from accidental drug interactions rose 68 percent between 1999
and 2004 (Payne, 2007).
765
the DDIExtraction-2011 challenge corpus (Segura-
Bedmar et al, 2011). The official training and test
data of the corpus contain 4,267 and 1,539 sen-
tences, and 2,402 and 755 DDI annotations respec-
tively.
2 Proposed Approach
2.1 Stage 1: Exploiting scope of negation to
filter out sentences
We propose a two stage RE approach. In the first
stage, our goal is to exploit the scope of negations
to reduce the number of candidate mention pairs by
discarding sentences. For this purpose, we propose
the following features to train a binary classifier:
? has2TM: If the sentence has exactly 2 target entity
mentions (i.e. drug mentions for DDI extraction).
? has3OrMoreTM: Whether the sentence has more
than 2 target entity mentions.
? allTMonRight: Whether all target entity mentions
inside the sentence appear after the negation cue.
? neitherAllTMonLeftOrRight: Whether some but not
all target entity mentions appear after the negation
cue.
? negCue: The negation cue itself.
? immediateGovernor: The word on which the cue is
directly syntactically dependent.
? nearestVerbGovernor: The nearest verb in the de-
pendency graph on which the cue is syntactically
dependent.
? isVerbGovernorRoot: Whether the nearestVerb-
Governor is root of the dependency graph of the
sentence.
? allTMdependentOnNVG: Whether all target en-
tity mentions are syntactically dependent (di-
rectly/indirectly) on the nearestVerbGovernor.
? allButOneTMdependentOnNVG: Whether all but
one target entity mentions are syntactically depen-
dent on the nearestVerbGovernor.
? although*PrecedeCue: Whether the syntactic
clause containing the negation cue begins with ?al-
though / though / despite / in spite?.
? commaBeforeNextTM: Whether there is a comma in
the text between the negation cue and the next target
entity mention after the cue.
? commaAfterPrevTM: Whether there is a comma in
the text between the previous target entity mention
before the negation cue and the cue itself.
? sentHasBut: Whether the sentence contains the
word ?but?.
The objective of the classifier is to decide whether
all of the target entity mentions (i.e. drugs) as well as
any possible evidence of the relation of interest (for
which we assume the immediate and the nearest verb
governors of the negation cue would be good candi-
dates) inside the corresponding sentence fall under
the scope of a negation cue in such a way that the
sentence is unlikely to contain a DDI.
At present, we limit our focus only on the first
occurrence of the following negation cues: ?no?,
?n?t? or ?not?.6 In the Stage 1, any sentence that
contains at least one DDI is considered by the clas-
sifier as a positive (training/test) instance. Other sen-
tences are considered as negative instances. We rule
out any sentence (i.e. we do not consider as train-
ing/test instance for the classifier that filters less in-
formative sentences) during both training and testing
if any of the following conditions holds:
? The sentence contains less than two target entity
mentions (such sentence would not contain the re-
lation of interest anyway).
? It has any of the following phrases ? ?not recom-
mended?, ?should not be? or ?must not be?.7
? There is no ?no?, ?n?t? or ?not? in the sentence.
? No target entity mention appears in the sentence af-
ter ?no?, ?n?t? or ?not?.
To assess the effectiveness of the proposed Stage
1 classifier, we defined a baseline classifier that fil-
ters any sentence that contains ?no?, ?n?t? or ?not?.
2.2 Stage 2
Once the sentences which are likely to have no DDI
are identified and removed, the next step is to ap-
ply a state-of-the-art RE approach on the remaining
sentences. In this section, we propose a new hybrid
kernel, KHybrid, for this purpose. It is defined as
follows:
KHybrid (R1, R2) = KHF (R1, R2) + KSL
(R1, R2) + w * KPET (R1, R2)
6These cues usually occur more frequently and generally
have larger negation scope than other negation cues.
7These expressions often provide clues that one of the bio-
entity mentions negatively influences the level of activity of the
other.
766
Here, KHF stands for a new feature based kernel
(proposed in this paper) that uses a heterogeneous
set of features. KSL stands for the Shallow Linguis-
tic (SL) kernel proposed by Giuliano et al (2006).
KPET stands for the Path-enclosed Tree (PET) ker-
nel (Moschitti, 2004). w is a multiplicative constant
used for the PET kernel. It allows the hybrid kernel
to assign more (or less) weight to the information
obtained using tree structures depending on the cor-
pus.
The proposed kernel composition is valid accord-
ing to the closure properties of kernels. We ex-
ploit the SVM-Light-TK toolkit (Moschitti, 2006;
Joachims, 1999) for kernel computation. In Stage
2, each candidate drug mention pair represents an
instance.
2.2.1 Proposed KHF kernel
As mentioned earlier, this proposed kernel uses
heterogeneous features. The first version of the het-
erogeneous feature set (henceforth, HF v1) com-
bines features proposed by two previous RE works.
The former is Zhou et al (2005), which uses 51 dif-
ferent features. We select the following 27 of their
features for our feature set:
WBNULL, WBFL, WBF, WBL, WBO,
BM1F, BM1L, AM2F, AM2L, #MB, #WB,
CPHBNULL, CPHBFL, CPHBF, CPHBL,
CPHBO, CPHBM1F, CPHBM1L, CPHAM2F,
CPHAM2F, CPP, CPPH, ET12SameNP,
ET12SamePP, ET12SameVP, PTP, PTPH
The latter is the TPWF kernel (Chowdhury and
Lavelli, 2012a) from which we use following fea-
tures:
HasTriggerWord, Trigger-X, DepPattern-i, e-
walk, v-walk
The TPWF kernel extracts the HasTriggerWord,
Trigger-X and DepPattern-i features from a sub-
graph called reduced graph. We also follow this ap-
proach with one minor difference. Unlike Chowd-
hury and Lavelli (2012a), we look for trigger words
in the whole reduced graph instead of using only the
root of the sub-graph.
Due to space limitation we refer the readers to
the corresponding papers for the description of the
above mentioned features and the definition of re-
duced graph.
In addition, HF v1 also includes surrounding to-
kens within the window of {-2,+2} for each candi-
date mention. We are unaware of any available list
of trigger words for drug-drug interaction. So, we
created such a list.8
We extend the heterogeneous feature set by
adding features related to the scope of negation
(henceforth, HF v2). We use a list of 13 negation
cues9 to search inside the reduced graph of a candi-
date pair. If the reduced graph contains any of the
negation cues or their morphological variants then
we add the following features:
? negCue: The corresponding negation cue.
? immediateNegatedWord: If the word following the
negation cue is neither a preposition nor a ?be verb?,
then that word, otherwise the word after the next
word.10
Furthermore, if the corresponding matched nega-
tion cue is either ?no?, ?n?t? or ?not?, then we add
additional features related to negation scope:
? bothEntDependOnImmediateGovernor: Whether
the immediate governor (if any) of the negation cue
is also governor of a dependency sub-tree (of the de-
pendency graph of the corresponding sentence) that
includes both of the candidate mentions.
? immediateGovernorIsVerbGovernor: Whether the
immediate governor of the negation cue is a verb.
? nearestVerbGovernor: The closest verb governor
(i.e. parent or grandparent inside the dependency
graph), if any, of the negation cue.
We further extend the heterogeneous feature set
by adding features related to relevant non-target en-
tities (with respect to the relation of interest; hence-
forth, HF v3). For the purpose of DDI extrac-
tion, we deem the presence of DISEASE mentions
(which might result as a consequence of a DDI)
can provide some clues. So, we use a publicly
available state-of-the-art disease NER system called
BioEnEx (Chowdhury and Lavelli, 2010) to anno-
tate the DDIExtraction-2011 challenge corpus. For
8The RE system developed for this work and the cre-
ated list of trigger words for DDI can be downloaded from
https://github.com/fmchowdhury/HyREX .
9No, not, neither, without, lack, fail, unable, abrogate, ab-
sence, prevent, unlikely, unchanged, rarely.
10For example, ?interested? from ?... not interested ...?, and
?confused? from ?... not to be confused ...?.
767
each candidate (drug) mention pair, we add the fol-
lowing features in HF v3:
? NTEMinsideSentence: Whether the corresponding
sentence contains important non-target entity men-
tion(s) (e.g. disease for DDI).
? immediateGovernorIsVerbGovernorOfNTEM: The
immediate governor (if any) of the non-target entity
mention, only if such governor is also governing a
dependency sub-tree that includes both of the target
candidate entity mentions.
? nearestVerbGovernorOfNTEM: The closest verb
governor (if any) of the non-target entity mention,
only if it also governs the candidate entity mentions.
? immediateGovernorIsVerbGovernorOfNTEM:
Whether the immediate governor is a verb.
3 Results and Discussion
We train a linear SVM classifier in Stage 1 and
tune the hyper-parameters (by doing 5-fold cross-
validation) for obtaining maximum possible recall.
In this way we minimize the number of false neg-
atives (i.e. sentences that contain DDIs but are
wrongly identified as not having any).
During the cross-validation experiments on the
training data, 334 sentences (7.83% of the total sen-
tences) containing at least 2 drug mentions were
identified by our proposed classifier (in Section 2.1)
as unlikely to have any DDI and hence are candi-
dates for discarding. Only 19 of these sentences
were incorrectly identified. When we trained on
the training data and tested on the official test data
of DDIExtraction-2011 challenge corpus, 121 sen-
tences (7.86% of the total test sentences) were iden-
tified by the classifier as candidates for discarding.
Only 5 of them were incorrectly identified.
Unlike Stage 1, in Stage 2 where we train the hy-
brid kernel based RE classifier and use it for RE (i.e.
DDI extraction) from the test data, sentences are not
the RE training/test instances. Instead, a RE instance
corresponds to a candidate mention pair.
All the DDIs (i.e. positive RE instances) of the
incorrectly identified sentences in Stage 1 (i.e. the
sentences which are incorrectly labelled as not hav-
ing any DDI and filtered) are automatically consid-
ered as false negatives during the calculation of DDI
extraction results in Stage 2.
To verify whether our proposed hybrid kernel
achieves state-of-the-art results without taking ben-
efits of the output of Stage 1, we did some experi-
ments without discarding any sentence. These ex-
periments are done using Zhou et al (2005), TPWF
kernel, SL kernel, different versions of proposed
KHF kernel and KHybrid kernel. Table 1 shows
the results of 5-fold cross-validation experiments
(hyper-parameters are tuned for obtaining maximum
F-score). As the results show, there is a gain +0.9
points in F-score (mainly due to the boost in re-
call) after the addition of features related to negation
scope. There is also some minor improvement due
to the proposed non-target entity specific features.
We also performed (5-fold cross validation) ex-
periments by combining the Stage 1 classifier with
each of the Zhou et al (2005), TPWF kernel, SL
kernel, PET kernel, KHF kernel and KHybrid kernel
separately (only the results of KHybrid are reported
in Table 1 due to space limitation). In each case,
there were improvements in precision, recall and F-
score. The gain in F-score ranged from 1.0 to 1.4
points.
P / R / F-score
Using SL kernel (Giuliano et al, 2006) 51.3 / 64.7 / 57.3
Using (Zhou et al, 2005) 58.7 / 37.1 / 45.5
Using PET kernel (Moschitti, 2004) 46.8 / 602 / 52.7
TPWF (Chowdhury and Lavelli, 2012a) 43.7 / 60.7 / 50.8
Proposed approaches
Proposed KHF v1 53.4 / 51.5 / 52.4
KHF v2 (i.e. + neg scope feat.) 53.9 / 52.6 / 53.3 (+0.9)
KHF v3 (i.e. + non-target entity feat.) 53.6 / 53.5 / 53.6 (+0.3)
Proposed KHybrid 56.3 / 68.5 / 61.8
Proposed KHybrid with Stage 1 57.3 / 69.4 / 62.8 (+1.0)
Table 1: 5-fold cross-validation results on training data.
Table 2 reports the results of the previously pub-
lished studies that used the same corpus. Our pro-
posed KHybrid kernel obtains an F-score that is
higher than that of the previous state of the art.
When the Stage 1 classifier (based on negation
scope features) is exploited before using the KHybrid
kernel, the F-score reaches up to 67.4. This is
+1.0 points higher than without exploiting the Stage
1 classifier and +1.7 higher than previous state of
768
the art. We did separate experiments (also reported
in Table 2) to assess the performance improvement
when the output of Stage 1 is used to filter sentences
from either training or test data only. The results
remain the same when only training sentences are
filtered; while there are some improvements when
only test sentences are filtered. Filtering both train-
ing and test sentences provides the larger gain which
is statistically significant.
Usually, the number of negative instances in a
corpus is much higher than that of the positive in-
stances. In a recent work, Chowdhury and Lavelli
(2012b) showed that by removing less informative
(negative) instances (henceforth, LIIs), not only the
skewness in instance distribution could be reduced
but it also leads to a better result. The proposed
Stage 1 classifier, presented in this work, also re-
duces skewness in instance distribution. This is be-
cause we are only removing those sentences that are
unlikely to contain any positive instance. So, in prin-
ciple, the Stage 1 classifier is focused on removing
only negative instances (although the classifier mis-
takenly discards few positive instances, too).
We wanted to study how the Stage 1 classifier
would contribute if we use it on top of the tech-
niques that were proposed in Chowdhury and Lavelli
(2012b) to remove LIIs. As Table 2 shows, by using
the Stage 1 classifier along with LLI filtering, we
could further improve the results (+3.2 points differ-
ence in F-score with the previous state of the art).
4 Conclusion
A major flexibility in the proposed approach is that
it does not require a separate dataset (which needs
to match the genre of the text to be used for RE)
annotated with negation scopes. Instead, the pro-
posed Stage 1 classifier uses the RE training data
(which do not have negation scope annotations) to
self-supervise itself. Various new features have been
exploited (both in stages 1 and 2) that can provide
strong indications of the scope of negation cues with
respect to the relation to be extracted. The only thing
needed is the list of possible negation cues (Morante
(2010) includes such a comprehensive list).
Our proposed kernel, which has a component that
exploits a heterogeneous set of features including
negation scope and presence of non-target entities,
already obtains better results than previous studies.
P R F-score
(Thomas et al, 2011) 60.5 71.9 65.7
(Chowdhury et al, 2011) 58.6 70.5 64.0
(Chowdhury and Lavelli, 2011) 58.4 70.1 63.7
(Bjorne et al, 2011) 58.0 68.9 63.0
Proposed KHybrid 60.0 74.3 66.4
KHybrid + Stage 1 baseline 61.8 68.9 65.1
KHybrid + proposed Stage 1 60.0 74.2 66.4
(only training sentences are filtered)
KHybrid + proposed Stage 1 61.4 73.8 67.0
(only test sentences are filtered)
KHybrid + proposed Stage 1 62.1 73.8 67.4 stat. sig.
(both training and test sentences are filtered)
Proposed KHybrid + LII filtering 61.1 75.1 67.4 stat. sig.
Proposed KHybrid + LII filtering 63.5 75.2 68.9 stat. sig.
+ proposed Stage 1
Table 2: Results obtained on the official test set of the
2011 DDI Extraction challenge. LII filtering refers to the
techniques proposed in Chowdhury and Lavelli (2012b)
for reducing skewness in RE data distribution. stat. sig. in-
dicates that the improvement of F-score, due to usage of
Stage 1 classifier, is statistically significant (verified using
Approximate Randomization Procedure (Noreen, 1989);
number of iterations = 1,000, confidence level = 0.01).
The results considerably improve when possible ir-
relevant sentences from both training and test data
are filtered by exploiting features related to the scope
of negations.
In future, we would like to exploit the scope of
more negation cues, apart from the three cues that
are used in this study. We believe our approach
would help to improve RE in other genres of text
(such as newspaper) as well.
Acknowledgement
This work was carried out in the context of the
project ?eOnco - Pervasive knowledge and data
management in cancer care?.
References
J Bjorne, A Airola, T Pahikkala, and T Salakoski. 2011.
Drug-drug interaction extraction with RLS and SVM
classifiers. In Proceedings of the 1st Challenge task
on Drug-Drug Interaction Extraction (DDIExtraction
2011), pages 35?42, Huelva, Spain, September.
769
MFM Chowdhury and A Lavelli. 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
MFM Chowdhury and A Lavelli. 2011. Drug-drug inter-
action extraction using composite kernels. In Proceed-
ings of the 1st Challenge task on Drug-Drug Interac-
tion Extraction (DDIExtraction 2011), pages 27?33,
Huelva, Spain, September.
MFM Chowdhury and A Lavelli. 2012a. Combining tree
structures, flat features and patterns for biomedical re-
lation extraction. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 420?
429, Avignon, France, April. Association for Compu-
tational Linguistics.
MFM Chowdhury and A Lavelli. 2012b. Impact of Less
Skewed Distributions on Efficiency and Effectiveness
of Biomedical Relation Extraction. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2012) : Posters, pages 205?216,
Mumbai, India, December.
MFM Chowdhury, AB Abacha, A Lavelli, and
P Zweigenbaum. 2011. Two different machine learn-
ing techniques for drug-drug interaction extraction. In
Proceedings of the 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
19?26, Huelva, Spain, September.
R Farkas, V Vincze, G Mo?ra, J Csirik, and G Szarvas.
2010. The CoNLL-2010 shared task: Learning to de-
tect hedges and their scope in natural language text.
In Proceedings of the Fourteenth Conference on Com-
putational Natural Language Learning, pages 1?12,
Uppsala, Sweden, July. Association for Computational
Linguistics.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL 2006),
pages 401?408.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
JD Kim, T Ohta, S Pyysalo, Y Kano, and J Tsujii. 2009.
Overview of BioNLP?09 shared task on event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task, pages 1?9, Boul-
der, Colorado, June. Association for Computational
Linguistics.
JD Kim, Y Wang, T Takagi, and A Yonezawa. 2011.
Overview of Genia event task in BioNLP shared task
2011. In Proceedings of BioNLP Shared Task 2011
Workshop, pages 7?15, Portland, Oregon, USA, June.
Association for Computational Linguistics.
E Landau. 2009. Jackson?s death raises questions about
drug interactions [Published in CNN; June 26, 2009].
http://articles.cnn.com/2009-06-
26/health/jackson.drug.interaction.
caution_1_drug-interactions-heart-
rhythms-antidepressants?_s=PM:
HEALTH.
R Morante and E Blanco. 2012. *SEM 2012 shared task:
Resolving the scope and focus of negation. In *SEM
2012: The First Joint Conference on Lexical and Com-
putational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 265?274,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
R Morante. 2010. Descriptive Analysis of Negation
Cue in Biomedical Texts. In Proceedings of the 7th
International Conference on Language Resources and
Evaluation (LREC 2010), Malta.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics (ACL 2004), Barcelona, Spain.
A Moschitti. 2006. Making Tree Kernels Practical
for Natural Language Learning. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for computational Linguistics (EACL 2006),
Trento, Italy.
EW Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses : An Introduction. Wiley-
Interscience, April.
JW Payne. 2007. A Dangerous Mix [Published
in The Washington Post; February 27, 2007].
http://www.washingtonpost.com/wp-
dyn/content/article/2007/02/23/
AR2007022301780.html.
O Sanchez-Graillet and M Poesio. 2007. Negation of
protein-protein interactions: analysis and extraction.
Bioinformatics, 23(13):i424?i432.
I Segura-Bedmar, P Mart??nez, and CD Pablo-Sa?nchez.
2011. The 1st DDIExtraction-2011 challenge task:
Extraction of Drug-Drug Interactions from biomedi-
cal texts. In Proceedings of the 1st Challenge task
on Drug-Drug Interaction Extraction (DDIExtraction
2011), pages 1?9, Huelva, Spain, September.
P Thomas, M Neves, I Solt, D Tikk, and U Leser.
2011. Relation extraction for drug-drug interactions
using ensemble learning. In Proceedings of the 1st
Challenge task on Drug-Drug Interaction Extraction
770
(DDIExtraction 2011), pages 11?18, Huelva, Spain,
September.
GD Zhou, J Su, J Zhang, and M Zhang. 2005. Ex-
ploring various knowledge in relation extraction. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics (ACL 2005), pages
427?434, Ann Arbor, Michigan, USA.
771
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 351?355, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK-irst : A Multi-Phase Kernel Based Approach for Drug-Drug
Interaction Detection and Classification that Exploits Linguistic Information
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Fondazione Bruno Kessler (FBK-irst), Italy
? University of Trento, Italy
fmchowdhury@gmail.com, lavelli@fbk.eu
Abstract
This paper presents the multi-phase relation
extraction (RE) approach which was used for
the DDI Extraction task of SemEval 2013. As
a preliminary step, the proposed approach in-
directly (and automatically) exploits the scope
of negation cues and the semantic roles of in-
volved entities for reducing the skewness in
the training data as well as discarding possible
negative instances from the test data. Then, a
state-of-the-art hybrid kernel is used to train
a classifier which is later applied on the in-
stances of the test data not filtered out by the
previous step. The official results of the task
show that our approach yields an F-score of
0.80 for DDI detection and an F-score of 0.65
for DDI detection and classification. Our sys-
tem obtained significantly higher results than
all the other participating teams in this shared
task and has been ranked 1st.
1 Introduction
Drug-drug interaction (DDI) is a condition when one
drug influences the level or activity of another. The
extraction of DDIs has significant importance for
public health safety. It was reported that about 2.2
million people in USA, age 57 to 85, were taking
potentially dangerous combinations of drugs (Lan-
dau, 2009). Another report mentioned that deaths
from accidental drug interactions rose by 68 percent
between 1999 and 2004 (Payne, 2007). The DDIEx-
traction 2011 and DDIExtraction 2013 shared tasks
underline the importance of DDI extraction.
The DDIExtraction 2013 task concerns the recog-
nition of drugs and the extraction of drug-drug in-
teractions from biomedical literature. The dataset of
the shared task is composed by texts from the Drug-
Bank database as well as MedLine abstracts in or-
der to deal with different type of texts and language
styles. Participants were asked to not only extract
DDIs but also classify them into one of four pre-
defined classes: advise, effect, mechanism and int.
A detailed description of the task settings and data
can be found in Segura-Bedmar et al (2013).
The system that we used in this shared task
combines various techniques proposed in our re-
cent research activities for relation extraction (RE)
(Chowdhury and Lavelli, 2012a; Chowdhury and
Lavelli, 2012b; Chowdhury and Lavelli, 2013).1
2 DDI Detection
Our system performs DDI detection and classifica-
tion in two separate steps. In this section, we explain
how DDI detection (i.e. whether two drug mentions
participate in a DDI) is accomplished. DDI classifi-
cation will be described in Section 3.
There are three phases for DDI detection: (i) dis-
card less informative sentences, (ii) discard less in-
formative instances, and (iii) train the system (a sin-
gle model regardless of DDI types) on the remaining
training instances and identify possible DDIs from
the remaining test instances. These phases are de-
scribed below.
2.1 Exploiting the scope of negations for
sentence filtering
Negation is a linguistic phenomenon where a nega-
tion cue (e.g. not) can alter the meaning of a partic-
1Available in https://github.com/fmchowdhury/HyREX.
351
ular text segment or of a fact. This text segment (or
fact) is said to be inside the scope of such negation
(cue). In one of our recent papers (Chowdhury and
Lavelli, 2013), we proposed how to exploit the scope
of negations for RE. We hypothesize that a classi-
fier trained solely on features related to the scope of
negations can be used to pro-actively filter groups
of instances which are less informative and mostly
negative.
To be more precise, we propose to train a classi-
fier (which will be applied before using the kernel
based RE classifier mentioned in Section 2.3) that
would check whether all the target entity mentions
inside a sentence along with possible relation clues
(or trigger words), if any, fall (directly or indirectly)
under the scope of a negation cue. If such a sentence
is found, then it would be identified as less informa-
tive and discarded (i.e. the candidate mention pairs
inside such sentence would not be considered). Dur-
ing training (and testing), we group the instances by
sentences. Any sentence that contains at least one
relation of interest is considered by the less infor-
mative sentence (LIS) classifier as a positive (train-
ing/test) instance. The remaining sentences are con-
sidered as negative instances.
We use a number of features related to negation
scopes to train a binary SVM classifier that filters out
less informative sentences. These features are basi-
cally contextual and shallow linguistic features. Due
to space limitation, we do not report these features
here. Interested readers are referred to Chowdhury
and Lavelli (2013).
The objective of the classifier is to decide whether
all target entity mentions as well as any possible ev-
idence inside the corresponding sentence fall under
the scope of a negation cue in such a way that the
sentence is unlikely to contain the relation of in-
terest (e.g. DDI). If the classifier finds such a sen-
tence, then it is assigned the negative class label. At
present, we focus only on the first occurrence of the
negation cues ?no?, ?n?t? or ?not?. These cues usu-
ally occur more frequently and generally have larger
negation scope than other negation cues.
The LIS classifier is trained using a linear SVM
classifier. Its hyper-parameters are tuned during
training for obtaining maximum recall. In this way
we minimize the number of false negatives (i.e. sen-
tences that contain relations but are wrongly filtered
out). Once the classifier is trained using the training
data, we apply it on both the training and test data.
However, if the recall of the LIS classifier is found
to be below a threshold value (we set it to 70.0) dur-
ing cross validation on the training data of a corpus,
it is not used for sentence filtering on such corpus.
Any (training/test) sentence that is classified as
negative is considered as a less informative sentence
and is filtered out. In other words, such a sentence is
not considered for RE. However, it should be noted
that, if such a sentence is a test sentence and it con-
tains positive RE instances, then all these filtered
positive RE instances are automatically considered
as false negatives during the calculation of RE per-
formance.
We rule out sentences (i.e. we consider them nei-
ther positive nor negative instances for training the
classifier that filters less informative sentences) dur-
ing both training and testing if any of the following
conditions holds:
? The sentence contains less than two target en-
tity mentions (such sentence would not contain
the relation of interest anyway).
? It has any of the following phrases ? ?not
recommended?, ?should not be? or ?must not
be?.2
? There is no ?no?, ?n?t? or ?not? in the sentence.
? No target entity mention appears in the sen-
tence after ?no?, ?n?t? or ?not?.
2.2 Discarding instances using semantic roles
and contextual evidence
For identifying less informative negative instances,
we exploit static (i.e. already known, heuristically
motivated) and dynamic (i.e. automatically col-
lected from the data) knowledge which has been
proposed in Chowdhury and Lavelli (2012b). This
knowledge is described by the following criteria:
? C1: If each of the two entity mentions (of a
candidate pair) has anti-positive governors (see
Section 2.2.1) with respect to the type of the
relation, then they are not likely to be in a given
relation.
2These expressions often provide clues that one of the drug
entity mentions negatively influences the level of activity of the
other.
352
? C2: If two entity mentions in a sentence refer
to the same entity, then it is unlikely that they
would have a relation between themselves.
? C3: If a mention is the abbreviation of another
mention (i.e. they refer to the same entity), then
they are unlikely to be in a relation.
Criteria C2 and C3 (static knowledge) are quite
intuitive. For criterion C1, we construct on the fly a
list of anti-positive governors (dynamic knowledge)
taken from the training data and use them for de-
tecting pairs that are unlikely to be in relation. As
for criterion C2, we simply check whether two men-
tions have the same name and there is more than one
character between them. For criterion C3, we look
for any expression of the form ?Entity1 (Entity2)?
and consider ?Entity2? as an abbreviation or alias of
?Entity1?.
The above criteria are used to filter instances from
both training and test data. Any positive test instance
filtered out by these criteria is automatically consid-
ered as a false negative during the calculation of RE
performance.
2.2.1 Anti-positive governors
The semantic roles of the entity mentions may in-
directly contribute either to relate or not to relate
them in a particular relation type (e.g. PPI) in the
corresponding context. To put it differently, the se-
mantic roles of two mentions in the same context
could provide an indication whether the relation of
interest does not hold between them. Interestingly,
the word on which a certain entity mention is (syn-
tactically) dependent (along with the dependency
type) could often provide a clue of the semantic role
of such mention in the corresponding sentence.
Our goal is to automatically identify the words
(if any) that tend to prevent mentions, which are di-
rectly dependent on those words, from participating
in a certain relation of interest with any other men-
tion in the same sentence. We call such words anti-
positive governors and assume that they could be ex-
ploited to identify negative instances (i.e. negative
entity mention pairs) in advance. Interested readers
are referred to Chowdhury and Lavelli (2012b) for
example and description of how anti-positive gov-
ernors are automatically collected from the training
data.
2.3 Hybrid Kernel based RE Classifier
As RE classifier we use the following hybrid kernel
that has been proposed in Chowdhury and Lavelli
(2013). It is defined as follows:
KHybrid (R1, R2) = KHF (R1, R2) + KSL
(R1, R2) + w * KPET (R1, R2)
where KHF is a feature based kernel (Chowdhury
and Lavelli, 2013) that uses a heterogeneous set
of features, KSL is the Shallow Linguistic (SL)
kernel proposed by Giuliano et al (2006), and
KPET stands for the Path-enclosed Tree (PET) ker-
nel (Moschitti, 2004). w is a multiplicative constant
that allows the hybrid kernel to assign more (or less)
weight to the information obtained using tree struc-
tures depending on the corpus. We exploit the SVM-
Light-TK toolkit (Moschitti, 2006; Joachims, 1999)
for kernel computation. The parameters are tuned
by doing 5-fold cross validation on the training data.
3 DDI Type Classification
The next step is to classify the extracted DDIs into
different categories. We train 4 separate models for
each of the DDI types (one Vs all) to predict the
class label of the extracted DDIs. During this train-
ing, all the negative instances from the training data
are removed. The filtering techniques described in
Sections 2.1 and 2.2 are not used in this stage.
The extracted DDIs are assigned a default DDI
class label. Once the above models are trained, they
are applied on the extracted DDIs from the test data.
The class label of the model which has the highest
confidence score for an extracted DDI instance is as-
signed to such instance.
4 Data Pre-processing and Experimental
Settings
The Charniak-Johnson reranking parser (Charniak
and Johnson, 2005), along with a self-trained
biomedical parsing model (McClosky, 2010), has
been used for tokenization, POS-tagging and pars-
ing of the sentences. Then the parse trees are pro-
cessed by the Stanford parser (Klein and Manning,
2003) to obtain syntactic dependencies. The Stan-
ford parser often skips some syntactic dependencies
in output. We use the rules proposed in Chowdhury
353
and Lavelli (2012a) to recover some of such depen-
dencies. We use the same techniques for unknown
characters (if any) as described in Chowdhury and
Lavelli (2011).
Our system uses the SVM-Light-TK toolkit3
(Moschitti, 2006; Joachims, 1999) for computation
of the hybrid kernels. The ratio of negative and posi-
tive examples has been used as the value of the cost-
ratio-factor parameter. The SL kernel is computed
using the jSRE tool4.
The KHF kernel can exploit non-target entities
to extract important clues (Chowdhury and Lavelli,
2013). So, we use a publicly available state-of-the-
art NER system called BioEnEx (Chowdhury and
Lavelli, 2010) to automatically annotate both the
training and the test data with disease mentions.
The DDIExtraction 2013 shared task data include
two types of texts: texts taken from the DrugBank
database and texts taken from MedLine abstracts.
During training we used both types together.
5 Experimental Results
Table 1 shows the results of 5-fold cross validation
for DDI detection on the training data. As we can
see, the usage of the LIS and LII filtering techniques
improves both precision and recall.
We submitted three runs for the DDIExtraction
2013 shared task. The only difference between the
three runs concerns the default class label (i.e. the
class chosen when none of the separate models as-
signs a class label to a predicted DDI). Such default
class label is ?int?, ?effect? and ?mechanism? for
run 1, 2 and 3 respectively. According to the offi-
cial results provided by the task organisers, our best
result was obtained by run 2 (shown in Table 2).
According to the official results, the performance
for ?advise? is very low (F1 0.29) in MedLine texts,
while the performance for ?int? is comparatively
much higher (F1 0.57) with respect to the one of the
other DDI types. In comparison, the performance
for ?int? is much lower (F1 0.55) in DrugBank texts
with respect to the one of the other DDI types.
In MedLine test data, the number of ?effect? (62)
and ?mechanism? (24) DDIs is much higher than
that of ?advise? (7) and ?int? (2). On the other
3http://disi.unitn.it/moschitti/Tree-Kernel.htm
4http://hlt.fbk.eu/en/technology/jSRE
P R F1
KHybrid 0.66 0.80 0.72
LIS filtering + KHybrid 0.67 0.80 0.73
LIS filtering + LII filtering 0.68 0.82 0.74
+ KHybrid
Table 1: Comparison of results for DDI detection on the
training data using 5-fold cross validation. Parameter tun-
ing is not done during these experiments.
P R F1
All text
DDI detection only 0.79 0.81 0.80
Detection and Classification 0.65 0.66 0.65
DrugBank text
DDI detection only 0.82 0.84 0.83
Detection and Classification 0.67 0.69 0.68
MedLine text
DDI detection only 0.56 0.51 0.53
Detection and Classification 0.42 0.38 0.40
Table 2: Official results of the best run (run 2) of our
system in the DDIExtraction 2013 shared task.
hand, in DrugBank test data, the different DDIs are
more evenly distributed ? ?effect? (298), ?mecha-
nism? (278), ?advise? (214) and ?int? (94).
Initially, it was not clear to us why our system (as
well as other participants) achieves so much higher
results on the DrugBank sentences in comparison to
MedLine sentences. Statistics of the average num-
ber of words show that the length of the two types
of training sentences are substantially similar (Drug-
Bank : 21.2, MedLine : 22.3). It is true that the num-
ber of the training sentences for the former is almost
5.3 times higher than the latter. But it could not be
the main reason for such high discrepancies.
So, we turned our attention to the presence of the
cue words. In the 4,683 sentences of the DrugBank
training set (which have at least one drug mention),
we found that the words ?increase? and ?decrease?
are present in 721 and 319 sentences respectively.
While in the 877 sentences of the MedLine train-
ing set (which have at least one drug mention), we
found that the same words are present in only 67
and 40 sentences respectively. In other words, the
presence of these two important cue words in the
354
DrugBank sentences is twice more likely than that
in the MedLine sentences. We assume similar obser-
vations might be also possible for other cue words.
Hence, this is probably the main reason why the re-
sults are so much better on the DrugBank sentences.
6 Conclusion
In this paper, we have described a novel multi-phase
RE approach that outperformed all the other partic-
ipating teams in the DDI Detection and Classifica-
tion task at SemEval 2013. The central component
of the proposed approach is a state-of-the-art hybrid
kernel. Our approach also indirectly (and automat-
ically) exploits the scope of negation cues and the
semantic roles of the involved entities.
Acknowledgments
This work is supported by the project ?eOnco - Pervasive
knowledge and data management in cancer care?. The
authors would like to thank Alessandro Moschitti for his
help in the use of SVM-Light-TK.
References
E Charniak and M Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2005).
MFM Chowdhury and A Lavelli. 2010. Disease mention
recognition with specific features. In Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing, pages 83?90, Uppsala, Sweden, July.
MFM Chowdhury and A Lavelli. 2011. Drug-drug inter-
action extraction using composite kernels. In Proceed-
ings of the 1st Challenge task on Drug-Drug Interac-
tion Extraction (DDIExtraction 2011), pages 27?33,
Huelva, Spain, September.
MFM Chowdhury and A Lavelli. 2012a. Combining tree
structures, flat features and patterns for biomedical re-
lation extraction. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 420?
429, Avignon, France, April.
MFM Chowdhury and A Lavelli. 2012b. Impact of Less
Skewed Distributions on Efficiency and Effectiveness
of Biomedical Relation Extraction. In Proceedings of
the 24th International Conference on Computational
Linguistics (COLING 2012), Mumbai, India, Decem-
ber.
MFM Chowdhury and A Lavelli. 2013. Exploiting the
Scope of Negations and Heterogeneous Features for
Relation Extraction: A Case Study for Drug-Drug In-
teraction Extraction. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technology (NAACL 2013), Atlanta, USA, June.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL 2006),
pages 401?408.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
D Klein and C Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003), pages 423?430, Sapporo, Japan.
E Landau. 2009. Jackson?s death raises ques-
tions about drug interactions [Published in CNN;
June 26, 2009]. http://edition.cnn.
com/2009/HEALTH/06/26/jackson.drug.
interaction.caution/index.html.
D McClosky. 2010. Any Domain Parsing: Automatic
Domain Adaptation for Natural Language Parsing.
Ph.D. thesis, Department of Computer Science, Brown
University.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, ACL ?04, Barcelona, Spain.
A Moschitti. 2006. Making tree kernels practical for nat-
ural language learning. In Proceedings of 11th Confer-
ence of the European Chapter of the Association for
computational Linguistics (EACL 2006), pages 113?
120, Trento, Italy.
JW Payne. 2007. A Dangerous Mix [Published
in The Washington Post; February 27, 2007].
http://www.washingtonpost.com/
wp-dyn/content/article/2007/02/23/
AR2007022301780.html.
I Segura-Bedmar, P Mart??nez, and M Herrero-Zazo.
2013. SemEval-2013 task 9: Extraction of drug-drug
interactions from biomedical texts. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), Atlanta, USA, June.
355
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 466?470, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
FBK: Sentiment Analysis in Twitter with Tweetsted
Md. Faisal Mahbub Chowdhury
FBK and University of Trento, Italy
fmchowdhury@gmail.com
Marco Guerini
Trento RISE, Italy
marco.guerini@trentorise.eu
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Alberto Lavelli
FBK, Trento, Italy
lavelli@fbk.eu
Abstract
This paper presents the Tweetsted system im-
plemented for the SemEval 2013 task on Sen-
timent Analysis in Twitter. In particular, we
participated in Task B on Message Polar-
ity Classification in the Constrained setting.
The approach is based on the exploitation of
various resources such as SentiWordNet and
LIWC. Official results show that our approach
yields a F-score of 0.5976 for Twitter mes-
sages (11th out of 35) and a F-score of 0.5487
for SMS messages (8th out of 28 participants).
1 Introduction
Microblogging is currently a very popular commu-
nication tool where millions of users share opinions
on different aspects of life. For this reason it is a
valuable source of data for opinion mining and sen-
timent analysis.
Working with such type of texts presents chal-
lenges for NLP beyond those typically encountered
when dealing with more traditional texts, such as
newswire data. Tweets are short, the language used
is very informal, with creative spelling and punctua-
tion, misspellings, slang, new words, URLs, genre-
specific terminology and abbreviations, and #hash-
tags. These characteristics need to be handled with
specific approaches.
This paper presents the approach adopted for the
SemEval 2013 task on Sentiment Analysis in Twit-
ter, in particular Task B on Message Polarity Clas-
sification in the Constrained setting (i.e., using the
provided training data only).
The goal of Task B on Message Polarity Classi-
fication is the following: given a message, decide
whether it expresses a positive, negative, or neutral
sentiment. For messages conveying both a positive
and a negative sentiment, whichever is the stronger
sentiment should be chosen.
Two modalities are possible: (1) Constrained (us-
ing the provided training data only; other resources,
such as lexica, are allowed; however, it is not al-
lowed to use additional tweets/SMS messages or ad-
ditional sentences with sentiment annotations); and
(2) Unconstrained (using additional data for train-
ing, e.g., additional tweets/SMS messages or addi-
tional sentences annotated for sentiment). We par-
ticipated in the Constrained modality.
We adopted a supervised machine learning (ML)
approach based on various contextual and seman-
tic features. In particular, we exploited resources
such as SentiWordNet (Esuli and Sebastiani, 2006),
LIWC (Pennebaker and Francis, 2001), and the lex-
icons described in Mohammad et al (2009).
Critical features include: whether the mes-
sage contains intensifiers, adjectives, interjections,
presence of positive or negative emoticons, pos-
sible message polarity based on SentiWordNet
scores (Esuli and Sebastiani, 2006; Gatti and
Guerini, 2012), scores based on LIWC cate-
gories (Pennebaker and Francis, 2001), negated
words, etc.
2 System Description
Our supervised ML-based approach relies on Sup-
port Vector Machines (SVMs). The SVM imple-
mentation used in the system is LIBSVM (Chang
466
and Lin, 2001) for training SVM models and test-
ing. Moreover, in the preprocessing phase we used
TweetNLP (Owoputi et al, 2013), a POS tagger ex-
plicitly tailored for working on tweets.
We adopted a 2 stage approach: (1) during stage
1, we performed a binary classification of messages
according to the classes neutral vs subjective; (2)
in stage 2, we performed a binary classification of
subjective messages according to the classes positive
vs negative. We performed various experiments on
the training and development sets exploring the use
of different features (see Section 2.1) to find the best
configurations for the official submission.
2.1 Feature list
We implement several features divided into three
groups: contextual features, semantic features from
context and semantic features from external re-
sources. The complete list is reported in Table 1.
Contextual features are features computed by
considering only the tokens in the tweets/SMS and
the associated part of speech.
Semantic Features from Context are features
based on words polarity. Emoticons were recog-
nized through a list of emoticons extracted from
Wikipedia1 and then manually labeled as positive or
negative. Negated words (feature n. 18) are any to-
ken occurring between n?t, not, no and a comma, ex-
cluding those tagged as function words. Feature n.
19 captures tokens (or sequences of tokens) labeled
with a positive or negative polarity in the resource
described in Mohammad et al (2009). The intensi-
fiers considered for Feature n. 20 have been identi-
fied by implementing a simple algorithm that detects
tokens containing anomalously repeated characters
(e.g. happyyyyy). Feature n. 21 was computed by
training the system on the training data and predict-
ing labels for the test data, and then using these la-
bels as new features to train the system again.
Semantic Features from external resources in-
clude word classes from the Linguistic Inquiry
and Word Count (LIWC), a tool that calculates
the degree to which people use different cate-
gories of words related to psycholinguistic pro-
cesses (Pennebaker and Francis, 2001). LIWC in-
1http://en.wikipedia.org/wiki/List_of_
emoticons
cludes about 2,200 words and stems grouped into 70
broad categories relevant to psychological processes
(e.g., EMOTION, COGNITION). Sample words are
shown in Table 2.
For each non-zero valued LIWC category of a cor-
responding tweet/SMS, we added a feature for that
category and used the category score as the value
of that feature. We call this LWIC string feature.
Alternatively, we also added a separate feature for
each non-zero valued LIWC category and set 1 as
the value of that feature. This feature is called LWIC
boolean.
We also used words prior polarity - i.e. if a word
out of context evokes something positive or nega-
tive. For this, we relied on SentiWordNet, a broad-
coverage resource that provides polarities for (al-
most) every word. Since words can have multi-
ple senses, we compute the prior polarity of a word
starting from the polarity of each sense and returning
its polarity strength as an index between -1 and 1.
We tested 14 formulae that combine posterior polar-
ities in different ways to obtain a word prior polarity,
as reported in (Gatti and Guerini, 2012).
For the SWNscoresMaximum feature, we select
the prior polarity of the word in a tweet/SMS hav-
ing the maximum absolute score among all words
(of that tweet/SMS). For SWNscoresPolarityCount,
we select the polarity (positive, negative or neutral)
that is assigned to the majority of the words. As
for SWNscoresSum, it corresponds to the sum of
the prior polarities associated with all words in the
tweet/SMS.
3 Experimental Setup
In order to select the best performing feature set,
we carried out several 5-fold cross validation ex-
periments on the training data. We report in Table
3 the best performing feature set. In particular, we
adopted a 2 stage approach:
1. during the first stage we performed a binary
classification of messages according to the
classes neutral vs subjective;
2. in the second stage, we performed a binary
classification of subjective messages according
to the classes positive vs negative.
We opted for a two stage binary classification ap-
proach, since we observed that it produces slightly
467
Contextual Features
1. noOfAdjectives num
2. adjective list string
3. interjection list string
4. firstInterj string
5. lastInterj string
6. bigramList string
7. beginsWithRT boolean
8. hasRTinMiddle boolean
9. endsWithLink boolean
10. endsWithHashtag boolean
11. hasQuestion boolean
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
14. beginsWithPosEmoticon boolean
15. beginsWithNegEmoticon boolean
16. endsWithPosEmoticon boolean
17. endsWithNegEmoticon boolean
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
21. labelPredictedBySystem pos./neg./neut.
Semantic Features from External Resources
22. LIWC string string
23. LIWC boolean string
24. SWNscoresMaximum pos./neg./neut.
25. SWNscoresPolarityCount pos./neg./neut.
26. SWNscoresSum pos./neg./neut.
Table 1: Complete feature list.
LABEL Sample words
CERTAIN all, very, fact*, exact*, certain*, completely
DISCREP but, if, expect*, should
TENTAT or, some, may, possib*, probab*
SENSES observ*, discuss*, shows, appears
SELF we, our, I, us
SOCIAL discuss*, interact*, suggest*, argu*
OPTIM best, easy*, enthus*, hope, pride
ANGER hate, kill, annoyed
INHIB block, constrain, stop
Table 2: Word categories along with sample words
better results than a single stage multi-class ap-
proach (i.e. neutral vs positive vs negative).2 Dif-
ferent combinations of classifiers were explored ob-
taining comparable results. Here we will report only
2The average F-scores (pos and neg) for two stage and single
stage approaches obtained using the official scorer, by training
on the training data and testing on the development data, are
0.5682 and 0.5611 respectively.
the best results.
STAGE 1. The best result for stage (1), neutral vs
subjective, obtained with 5-fold cross validation on
training set only, accounts for an accuracy of 69.6%.
Instead, the best result for stage (1), obtained with
training on training data and testing on development
data, accounts for an accuracy of 72.67%.
The list of best features is reported in Table 3.
Feature selection was performed by starting from a
small set of basic features, and then by adding the
remaining features incrementally.
Contextual Features
2. adjective list string
3. interjection list string
5. lastInterj string
Semantic Features from Context
12. noOfPositiveEmoticons num
13. noOfNegativeEmoticons num
18. negatedWords string
19. indexOfChunksWithPolarity string
20. containsIntensifier boolean
Semantic Features from external resources
23. LIWC boolean string
24. SWNscoresMaximum posi./neg./neut.
Table 3: Best performing feature set.
STAGE 2. In stage (2), positive vs negative, we
started from the best feature set obtained from stage
(1) and added the remaining features one by one in-
crementally. In this case, we kept SWNscoresMaxi-
mum without testing again other formulae; in partic-
ular, to compute words prior polarity, we also kept
the first sense approach, that assigns to every word
the SWN score of its most frequent sense and proved
to be the most discriminative in the first stage neutral
vs. subjective. We found that none of the feature sets
produced better results than that obtained using the
best feature set selected from stage (1). So, the best
feature set for stage (2) is unchanged. We trained
the system on the training data and tested it on the
development data, achieving an accuracy of 80.67%.
4 Evaluation
The SemEval task organizers (Wilson et al, 2013)
provided two test sets on which the systems were
to be evaluated: one included Twitter messages, i.e.
the same type of texts included in the training set,
468
while the other comprised SMS messages, i.e. texts
having more or less the same length as the Twitter
data but (supposedly) a different style. We applied
the same model, trained both on the training and the
development set, on the two types of data, without
any specific adaptation.
The Twitter test set was composed of 3,813
tweets. Official results show that our approach
yields an F-score of 0.5976 for Twitter messages
(11th out of 35), while the best performing system
obtained an F-score of 0.6902. The confusion ma-
trix is reported in Table 4, while the score details
in Table 5. The latter table shows that our system
achieves the lowest results on negative tweets, both
in terms of precision and of recall.
gs/pred positive negative neutral
positive 946 101 525
negative 90 274 237
neutral 210 70 1360
Table 4: Confusion matrix for Twitter task
class prec recall F-score
positive 0.7592 0.6018 0.6714
negative 0.6157 0.4559 0.5239
neutral 0.6409 0.8293 0.7230
average(pos and neg) 0.5976
Table 5: Detailed results for Twitter task
The SMS test set for the competition was com-
posed of 2,094 SMS. Official results provided by the
task organizers show that our approach yields an F-
score of 0.5487 for SMS messages (8th out of 28
participants), while the best performing system ob-
tained an F-score of 0.6846. The confusion matrix
is reported in Table 6, while the score details in Ta-
ble 7. Also in this case the recognition of negative
messages achieves by far the poorest performance.
A comparison of the results on the two test sets
shows that, as expected, our system performs bet-
ter on tweets than on SMS. However, precision
achieved by the system on neutral SMS is 0.12
points better on text messages than on tweets.
Interestingly, it appears from the results in Ta-
bles 5 and 7 (and from the distribution of the classes
in the data sets) that there may be a correlation be-
tween the number of tweets/SMS for a particular
class and the performance obtained for such class.
We plan to further investigate this issue.
gs/pred positive negative neutral
positive 320 44 128
negative 66 171 157
neutral 208 64 936
Table 6: Confusion matrix for SMS task
class prec recall F-score
positive 0.5387 0.6504 0.5893
negative 0.6129 0.4340 0.5082
neutral 0.7666 0.7748 0.7707
average(pos and neg) 0.5487
Table 7: Detailed results for SMS task
5 Conclusions
In this paper, we presented Tweetsted, the system de-
veloped by FBK for the SemEval 2013 task on Sen-
timent Analysis. We trained a classifier performing
a two-step binary classification, i.e. first neutral vs.
subjective data, and then positive vs. negative ones.
We implemented a set of features including contex-
tual and semantic ones. We also integrated in our
feature representation external knowledge from Sen-
tiWordNet, LIWC and the resource by Mohammad
et al (2009). On both test sets (i.e., Twitter mes-
sages and SMS) of the constrained modality of the
challenge, we achieved a good performance, being
among the top 30% of the competing systems. In
the near future, we plan to perform an error analysis
of the wrongly classified data to investigate possible
classification issues, in particular the lower perfor-
mance on negative tweets and SMS.
Acknowledgments
This work is supported by ?eOnco - Pervasive knowledge
and data management in cancer care? and ?Trento RISE
PerTe? projects.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
469
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation (LREC
2006), Genoa, Italy.
Lorenzo Gatti and Marco Guerini. 2012. Assessing sen-
timent strength in words prior polarities. In Proceed-
ings of COLING 2012: Posters, pages 361?370, Mum-
bai, India, December. The COLING 2012 Organizing
Committee.
Saif Mohammad, Bonnie Dorr, and Cody Dunne. 2009.
Generating High-Coverage Semantic Orientation Lex-
icons From Overtly Marked Words and a Thesaurus.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL 2013, Atlanta, Georgia, June.
J. Pennebaker and M. Francis. 2001. Linguistic inquiry
and word count: LIWC. Erlbaum Publishers.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
470
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 83?90,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Disease Mention Recognition with Specific Features
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
?Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
? ICT Doctoral School, University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
Despite an increasing amount of research
on biomedical named entity recognition,
there has been not enough work done on
disease mention recognition. Difficulty of
obtaining adequate corpora is one of the
key reasons which hindered this particu-
lar research. Previous studies argue that
correct identification of disease mentions
is the key issue for further improvement
of the disease-centric knowledge extrac-
tion tasks. In this paper, we present a ma-
chine learning based approach that uses
a feature set tailored for disease mention
recognition and outperforms the state-of-
the-art results. The paper also discusses
why a feature set for the well studied
gene/protein mention recognition task is
not necessarily equally effective for other
biomedical semantic types such as dis-
eases.
1 Introduction
The massive growth of biomedical literature vol-
ume has made the development of biomedical text
mining solutions indispensable. One of the essen-
tial requirements for a text mining application is
the ability to identify relevant entities, i.e. named
entity recognition. Previous work on biomedi-
cal named entity recognition (BNER) has been
mostly focused on gene/protein mention recogni-
tion. Machine learning (ML) based approaches
for gene/protein mention recognition have already
achieved a sufficient level of maturity (Torii et
al., 2009). However, the lack of availability of
adequately annotated corpora has hindered the
progress of BNER research for other semantic
types such as diseases (Jimeno et al, 2008; Lea-
man et al, 2009).
Correct identification of diseases is crucial for
various disease-centric knowledge extraction tasks
(e.g. drug discovery (Agarwal and Searls, 2008)).
Previous studies argue that the most promising
candidate for the improvement of disease related
relation extraction (e.g. disease-gene) is the cor-
rect identification of concept mentions including
diseases (Bundschus et al, 2008).
In this paper, we present a BNER system which
uses a feature set specifically tailored for disease
mention recognition. The system1 outperforms
other approaches evaluated on the Arizona Dis-
ease Corpus (AZDC) (more details in Section 5.1).
One of the key differences between our approach
and previous approaches is that we put more em-
phasis on the contextual features. We exploit syn-
tactic dependency relations as well. Apart from
the experimental results, we also discuss why the
choice of effective features for recognition of dis-
ease mentions is different from that for the well
studied gene/protein mentions.
The remaining of the paper is organized as fol-
lows. Section 2 presents a brief description of pre-
vious work on BNER for disease mention recog-
nition. Then, Section 3 describes our system and
Section 4 the feature set of the system. After that,
Section 5 explains the experimental data, results
and analyses. Section 6 describes the differences
for the choice of feature set between diseases and
genes/proteins. Finally, Section 7 concludes the
paper with an outline of our future research.
2 Related Work
Named entity recognition (NER) is the task of lo-
cating boundaries of the entity mentions in a text
and tagging them with their corresponding seman-
tic types (e.g. person, location, gene and so on).
Although several disease annotated corpora have
been released in the last few years, they have been
annotated primarily to serve the purpose of re-
lation extraction and, for different reasons, they
1The source code of our system is available for download
at http://hlt.fbk.eu/people/chowdhury/research
83
are not suitable for the development of ML based
disease mention recognition systems (Leaman et
al., 2009). For example, the BioText (Rosario
and Hearst, 2004) corpus has no specific anno-
tation guideline and contains several inconsisten-
cies, while PennBioIE (Kulick et al, 2004) is very
specific to a particular sub-domain of diseases.
Among other disease annotated corpora, EBI dis-
ease corpus (Jimeno et al, 2008) is not annotated
with disease mention boundaries which makes it
unsuitable for BNER evaluation for diseases. Re-
cently, an annotated corpus, named as Arizona
Disease Corpus (AZDC) (Leaman et al, 2009),
has been released which has adequate and suitable
annotation of disease mentions following specific
annotation guidelines.
There has been some work on identifying dis-
eases in clinical texts, especially in the context
of CMC Medical NLP Challenge2 and i2b2 Chal-
lenge3. However, as noted by Meystre et al
(2008), there are a number of reasons that make
clinical texts different from texts of biomedical
literature, e.g. composition of short, telegraphic
phrases, use of implicit templates and pseudo-
tables and so on. Hence, the strategies adopted
for NER on clinical texts are not the same as the
ones practiced for NER on biomedical literature.
As mentioned before, most of the work to
date on BNER is focused on gene/protein men-
tion recognition. State-of-the-art BNER systems
are based on ML techniques such as conditional
random fields (CRFs), support vector machines
(SVMs) etc (Dai et al, 2009). These systems use
either gene/protein specific features (e.g. Greek
alphabet matching) or post-processing rules (e.g.
extension of the identified mention boundaries to
the left when a single letter with a hyphen precedes
them (Torii et al, 2009)) which might not be as
effective for other semantic type identification as
they are for genes/proteins. There is a substantial
agreement in the feature set that these systems use
(most of which are actually various orthographical
and morphological features).
Bundschus et al (2008) have used a CRF
based approach that uses typical features for
gene/protein mention recognition (i.e. no feature
tailoring for disease recognition) for disease, gene
and treatement recognition. The work has been
evaluated on two corpora which have been anno-
2http://www.computationalmedicine.org/challenge/index.php
3https://www.i2b2.org/NLP/Relations/Main.php
tated with those entities that participate in disease-
gene and disease-treatment relations. The reported
results show F-measure for recognition of all the
entities that participate in the relations and do
not indicate which F-measure has been achieved
specifically for disease recognition. Hence, the re-
ported results are not applicable for comparison.
To the best of our knowledge, the only sys-
tematic experimental results reported for disease
mention recognition in biomedical literature using
ML based approaches are published by Leaman
and Gonzalez (2008) and Leaman et al (2009).4
They have used a CRF based BNER system named
BANNER which basically uses a set of ortho-
graphic, morphological and shallow syntactic fea-
tures (Leaman and Gonzalez, 2008). The system
achieves an F-score of 86.43 on the BioCreative
II GM corpus5 which is one of the best results for
gene mention recognition task on that corpus.
BANNER achieves an F-score of 54.84 for dis-
ease mention recognition on the BioText corpus
(Leaman and Gonzalez, 2008). However, as said
above, the BioText corpus contains annotation in-
consistencies6. So, the corpus is not ideal for com-
paring system performances. The AZDC corpus
is much more suitable as it is annotated specifi-
cally for benchmarking of disease mention recog-
nition systems. An improved version of BAN-
NER achieves an F-score of 77.9 on AZDC cor-
pus, which is the state of the art on ML based dis-
ease mention recognition in biomedical literature
(Leaman et al, 2009).
3 Description of Our System
There are basically three stages in our approach ?
pre-processing, feature extraction and model train-
ing, and post-processing.
3.1 Pre-processing
At first, the system uses GeniaTagger7 to tokenize
texts and provide PoS tagging. After that, it cor-
rects some common inconsistencies introduced by
GeniaTagger inside the tokenized data (e.g. Ge-
niaTagger replaces double inverted commas with
4However, there are some work on disease recognition in
biomedical literature using other techniques such as morpho-
syntactic heuristic based approach (e.g. MetaMap (Aronson,
2001)), dictionary look-up method and statistical approach
(Ne?ve?ol et al, 2009; Jimeno et al, 2008; Leaman et al,
2009).
5As mentioned in http://banner.sourceforge.net/
6http://biotext.berkeley.edu/data/dis treat data.html
7http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
84
two single inverted commas). These PoS tagged
tokized data are parsed using Stanford parser8.
The dependency relations provided as output by
the parser are used later as features. The tokens
are further processed using the following general-
ization and normalization steps:
? each number (both integer and real) inside a
token is replaced with ?9?
? each token is further tokenized if it contains
either punctuation characters or both digits
and alphabetic characters
? all letters are changed to lower case
? all Greek letters (e.g. alpha) are replaced with
G and Roman numbers (e.g. iv) with R
? each token is normalized using SPECIALIST
lexicon tool9 to avoid spelling variations
3.2 Feature extraction and model training
The features used by our system can be catego-
rized into the following groups:
? general linguistic features (Table 1)
? orthographic features (Table 2)
? contextual features (Table 3)
? syntactic dependency features (Table 4)
? dictionary lookup features (see Section 4)
During dictionary lookup feature extraction, we
ignored punctuation characters while matching
dictionary entries inside sentences. If a sequence
of tokens in a sentence matches an entry in the dic-
tionary, the leftmost token of that sequence is la-
beled with B-DB and the remaining tokens of the
sequence are labeled with I-DB. The label B-DB
indicates the beginning of a dictionary match. If a
token belongs to several dictionary matches, then
all the other dictionary matches except the longest
one are discarded.
The syntactic dependency features are extracted
from the output of the parser while the general lin-
guistic features are extracted directly from the pre-
processed tokens. To collect the orthographic fea-
tures, the original tokens inside the corresponding
sentences are considered. The contextual features
8http://nlp.stanford.edu/software/lex-parser.shtml
9http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html
are derived using other extracted features and the
original tokens.
Tokens are labeled with the corresponding dis-
ease annotations according to the IOB2 format.
Our system uses Mallet (McCallum, 2002) to train
a first-order CRF model. CRF is a state-of-the-
art ML technique applied to a variety of text
processing tasks including named entity recogni-
tion (Klinger and Tomanek, 2007) and has been
successfully used by many other BNER systems
(Smith et al, 2008).
3.3 Post-processing
Once the disease mentions are identified using
the learned model, the following post-processing
techniques are applied to reduce the number of
wrong identifications:
? Bracket mismatch correction: If there is a
mismatch of brackets in the identified men-
tion, then the immediate following (or pre-
ceding) character of the corresponding men-
tion is checked and included inside the men-
tion if that character is the missing bracket.
Otherwise, all the characters from the index
where the mismatched bracket exists inside
the identified mention are discarded from the
corresponding mention.
? One sense per discourse: If any instance of
a character sequence is identified as a disease
mention, then all the other instances of that
character sequence inside the same sentence
are also annotated as disease mentions.
? Short/long form annotation: Using the algo-
rithm of Schwartz and Hearst (2003), ?long
form (short form)? instances are detected in-
side sentences. If the short form is annotated
as disease mention, then the long form is also
annotated and vice versa.
? Ungrammatical conjunction structure cor-
rection: If an annotated mention contains
comma (,) but there is no ?and? in the fol-
lowing character sequence (from the charac-
ter index of that comma) of that mention, then
the annotation is splitted into two parts (at the
index of the comma). Annotation of the origi-
nal mention is removed and the splitted parts
are annotated as two separate disease men-
tions.
85
? Short and long form separation: If both short
and long forms are annotated in the same
mention, then the original mention is dis-
carded and the corresponding short and long
forms are annotated separately.
4 Features for Disease Recognition
There are compelling reasons to believe that vari-
ous issues regarding the well studied gene/protein
mention recognition would not apply to the other
semantic types. For example, Jimeno et al (2008)
argue that the use of disease terms in biomedical
literature is well standardized, which is quite op-
posite for the gene terms (Smith et al, 2008).
After a thorough study and extensive experi-
ments on various features and their possible com-
binations, we have selected a feature set specific
to the disease mention identification which com-
prises features shown in Tables 1, 2, 4 and 3, and
dictionary lookup features.
Feature name Description
PoS Part-of-speech tag
NormWord Normalized token
(see Section 3.1)
Lemma Lemmatized form
charNgram 3 and 4 character n-grams
Suffix 2-4 character suffixes
Prefix 2-4 character prefixes
Table 1: General linguistic features for tokeni
Feature name Description
InitCap Is initial letter capital
AllCap Are all letters capital
MixCase Does contain mixed case letters
SingLow Is a single lower case letter
SingUp Is a single upper case letter
Num Is a number
PuncChar Punctuation character
(if tokeni is
a punctuation character)
PrevCharAN Is previous character
alphanumeric
Table 2: Orthographic features for tokeni
Like Leaman et al (2009), we have created
a dictionary with the instances of the following
nine of the twelve UMLS semantic types from
Feature name Description
Bi-gramk,k+1 Bi-grams of
for i? 2 ? k < i + 2 normalized tokens
Tri-gramk,k+1,k+2 Tri-grams of
for i? 2 ? k < i + 2 normalized tokens
CtxPoSk,k+1 Bi-grams of
for i ? k < i + 2 token PoS
CtxLemmak,k+1 Bi-grams of
for i ? k < i + 2 lemmatized tokens
CtxWordk,k+1 Bi-grams of
for i? 2 ? k < i + 2 original tokens
Offset conjunctions Extracted by Mallet
from features
in the range from
tokeni?1 to tokeni+1
Table 3: Contextual features for tokeni
Feature name Description
dobj Target token(s) to which tokeni
is a direct object
iobj Target token(s) to which tokeni
is an indirect object
nsubj Target token(s) to which tokeni
is an active nominal subject
nsubjpass Target token(s) to which tokeni
is a passive nominal subject
nn Target token(s) to which tokeni
is a noun compound modifier
Table 4: Syntactic dependency features for tokeni.
For example, in the sentence ?Clinton defeated
Dole?, ?Clinton? is the nsubj of the target token
?defeated?.
the semantic group ?DISORDER?10 from UMLS
Metathesaurus (Bodenreider, 2004): (i) disease or
syndrome, (ii) neoplastic process, (iii) congenital
abnormality, (iv) acquired abnormality, (v) exper-
imental model of disease, (vi) injury or poison-
ing, (vii) mental or behavioral dysfunction, (viii)
pathological function and (ix) sign or symptom.
We have not considered the other three semantic
types (findings, anatomical abnormality and cell
or molecular Dysfunction) since these three types
have not been used during the annotation of Ari-
zona Disease Corpus (AZDC) which we have used
in our experiments.
Previous studies have shown that dictionary
lookup features, i.e. name matching against a
10http://semanticnetwork.nlm.nih.gov/SemGroups/
86
dictionary of terms, often increase recall (Torii
et al, 2009; Leaman et al, 2009). However,
an unprocessed dictionary usually does not boost
overall performance (Zweigenbaum et al, 2007).
So, to reduce uninformative lexical differences or
spelling variations, we generalize and normalize
the dictionary entries using exactly the same steps
followed for the pre-processing of sentences (see
Section 3.1).
To reduce chances of false and unlikely
matches, any entry inside the dictionary having
less than 3 characters or more than 10 tokens is
discarded.
5 Experiments
5.1 Data
We have done experiments on the recently re-
leased Arizona Disease Corpus (AZDC)11 (Lea-
man et al, 2009). The corpus has detailed annota-
tions of diseases including UMLS codes, UMLS
concept names, possible alternative codes, and
start and end points of disease mentions inside
the corresponding sentences. These detailed an-
notations make this corpus a valuable resource
for evaluating and benchmarking text mining so-
lutions for disease recognition. Table 5 shows var-
ious characteristics of the corpus.
Item name Total count
Abstracts 793
Sentences 2,783
Total disease mentions 3,455
Disease mentions without overlaps 3,093
Disease mentions with overlaps 362
Table 5: Various characteristics of AZDC.
For the overlapping annotations, (e.g. ?endome-
trial and ovarian cancers? and ?ovarian cancers?)
we have considered only the larger annotations
in our experiments. There remain 3,224 disease
mentions after resolving overlaps according to the
aforementioned criterion. We have observed mi-
nor differences in some statistics of the AZDC re-
ported by Leaman et al (2009) with the statistics
of the downloadable version12 (Table 5). How-
11Downloaded from http://diego.asu.edu/downloads/AZDC/
at 5-Feb-2009
12Note that ?Disease mentions (total)? in the paper of Lea-
man et al (2009) actually refers to the total disease mentions
after overlap resolving (Robert Leaman, personal communi-
cation). One other thing is, Leaman et al (2009) mention 794
ever, these differences can be considered negligi-
ble.
5.2 Results
We follow an experimental setting similar to the
one in Leaman et al (2009) so that we can com-
pare our results with that of the BANNER system.
We performed 10-fold cross validation on AZDC
in such a way that all sentences of the same ab-
stract are included in the same fold. The results of
all folds are averaged to obtain the final outcome.
Table 6 shows the results of the experiments with
different features using the exact matching crite-
rion.
As we can see, our approach achieves signif-
icantly higher result than that of BANNER. Ini-
tially, with only the general linguistic and or-
thographic features the performance is not high.
However, once the contextual features are used,
there is a substantial improvement in the result.
Note that BANNER does not use contextual fea-
tures. In fact, the use of contextual features is also
quite limited in other BNER systems that achieve
high performance for gene/protein identification
(Smith et al, 2008).
Dictionary lookup features provide a very good
contribution in the outcome. This supports the ar-
gument of Jimeno et al (2008) that the use of dis-
ease terms in biomedical literature is well stan-
dardized. Post-processing and syntactic depen-
dency features also increase some performance.
We have done statistical significance tests for
the last four experimental results shown in Table 6.
For each of such four experiments, the immediate
previous experiment is considered as the baseline.
The tests have been performed using the approx-
imate randomization procedure (Noreen, 1989).
We have set the number of iterations to 1,000 and
the confidence level to 0.01. According to the
tests, the contributions of contextual features and
dictionary lookup features are statistically signif-
icant. However, we have found that the contri-
butions of post-processing rules and syntactic de-
pendency features are statistically significant only
when the confidence level is 0.2 or more. Since
AZDC consists of only 2,783 sentences, we can
assume that the impact of post-processing rules
abstracts, 2,784 sentences and 3,228 (overlap resolved) dis-
ease mentions in the AZDC. But in our downloaded version
of AZDC, there is 1 abstract missing (i.e. total 793 abstracts
instead of 794). As a result, there is 1 less sentence and 4
less (overlap resolved) disease mentions than the originally
reported numbers.
87
and syntactic dependency features has been not so
significant despite of some performance improve-
ment.
5.3 Error analysis
One of the sources of errors is the annotations
having conjunction structures. There are 94 dis-
ease mentions in the data which contain the word
?and?. The boundaries of 11 of them have been
wrongly identified during experiments, while 39
of them have been totally missed out by our sys-
tem. Our system also has not performed well
for disease annotations that have some specific
types of prepositional phrase structures. For ex-
ample, there are 80 disease annotations having the
word ?of? (e.g. ?deficient activity of acid beta-
glucosidase GBA?). Only 28 of them are correctly
annotated by our system. The major source of er-
rors, however, concerns abbreviated disease names
(e.g. ?PNH?). We believe one way to reduce this
specific error type is to generate a list of possi-
ble abbreviated disease names from the long forms
of disease names available in databases such as
UMLS Metathesaurus.
6 Why Features for Diseases and
Genes/Proteins are not the Same
Many of the existing BNER systems, which are
mainly tuned for gene/protein identification, use
features such as token shape (also known as word
class and brief word class (Settles, 2004)), Greek
alphabet matching, Roman number matching and
so forth. As mentioned earlier, we have done ex-
tensive experiments with various feature combina-
tions for the selection of disease specific features.
We have observed that many of the features used
for gene/protein identification are not equally ef-
fective for disease identification. Table 7 shows
some of the results of those experiments.
This observation is reasonable because
gene/protein names are much more complex than
entities such as diseases. For example, they often
contain punctuation characters (such as paren-
theses or hyphen), Greek alphabets and digits
which are unlikely in disease names. Ideally,
the ML algorithm itself should be able to utilize
information from only the useful features and
ignore the others in the feature set. But practically,
having non-informative features often mislead
the model learning. In fact, several surveys have
argued that the choice of features matter at least
as much as the choice of the algorithm if not more
(Nadeau and Sekine, 2007; Zweigenbaum et al,
2007).
One of the interesting trends in gene/protein
mention identification is to not utilize syntactic
dependency relations (with the exception of Vla-
chos (2007)). Gene/protein names in biomedi-
cal literature are often combined (i.e. without
being separated by space characters) with other
characters which do not belong to the correspond-
ing mentions (e.g. p53-mediated). Moreover,
as mentioned before, gene/protein mentions com-
monly have very complex structures (e.g. PKR(1-
551)K64E/K296R or RXRalphaF318A). So, it is a
common practice to tokenize gene/proten names
adopting an approach that split tokens as much as
possible to extract effective features (Torii et al,
2009; Smith et al, 2008). But while the extensive
tokenization boosts performance, it is often diffi-
cult to correctly detect dependency relations for
the tokens of the gene/protein names in the sen-
tences where they appear. As a result, use of the
syntactic dependency relations is not beneficial in
such approaches.13 In comparison, disease men-
tions are less complex. So, the identified depen-
dencies for disease mentions are more reliable and
hence may be usable as potential features (refer to
our experimental results in Table 6).
The above mentioned issues are some of the
reasons why a feature set for the well studied
gene/protein focused BNER approaches is not
necessarily suitable for other biomedical semantic
types such as diseases.
7 Conclusion
In this paper, we have presented a single CRF clas-
sifier based BNER approach for disease mention
identification. The feature set is constructed us-
ing disease-specific contextual, orthographic, gen-
eral linguistic, syntactic dependency and dictio-
nary lookup features. We have evaluated our ap-
proach on AZDC corpus. Our approach achieves
significantly higher result than BANNER which is
the current state-of-the-art ML based approach for
disease mention recognition. We have also ex-
plained why the choice of features for the well
studied gene/protein does not apply for other se-
mantic types such as diseases.
13We have done some experiments on Biocreative II GM
corpus with syntactic dependency relations of the tokens,
which are not reported in this paper, and the results support
our argument.
88
System Note Precision Recall F-score
BANNER (Leaman et al, 2009) 80.9 75.1 77.9
Our system Using general linguistic and orthographic features 74.90 71.01 72.90
Our system After adding contextual features 82.15 75.81 78.85
Our system After adding post-processing 81.57 76.61 79.01
Our system After adding syntactic dependency features 82.07 76.66 79.27
Our system After adding dictionary lookup features 83.21 79.06 81.08
Table 6: 10-fold cross validation results using exact matching criteria on AZDC.
Experiment Note Precision Recall F-score
(i) Using general linguistic, orthographic 82.15 75.81 78.85
and contextual features
(ii) After adding WC and BWC features in (i) 82.08 75.57 78.69
(iii) After adding IsGreekAlphabet, HasGreekAlphabet 82.10 75.69 78.76
and IsRomanNumber features in (i)
Table 7: Experimental results of our system after using some of the gene/protein specific features for
disease mention recognition on AZDC. Here, WC and BWC refer to the ?word class? and ?brief word
class? respectively.
Future work includes implementation of disease
mention normalization (i.e. associating a unique
identifier for each disease mention). We also
plan to improve our current approach by includ-
ing more contextual features and post-processing
rules.
Acknowledgments
This work was carried out in the context of the
project ?eOnco - Pervasive knowledge and data
management in cancer care?. The authors would
like to thank Robert Leaman for sharing the set-
tings of his experiments on AZDC.
References
Agarwal, P., Searls, D. 2008. Literature mining in sup-
port of drug discovery. Brief Bioinform, 9(6):479?
492.
Aronson, A. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings AMIA Symposium, pages 17?
21.
Bodenreider, O. 2004. The Unified Medical Language
System (UMLS): integrating biomedical terminol-
ogy. Nucleic Acids Research, 32(suppl 1):D267?
270, January.
Bundschus, M., Dejori, M., Stetter, M., Tresp, V.,
Kriegel, H. 2008. Extraction of semantic biomed-
ical relations from text using conditional random
fields. BMC Bioinformatics, 9:207.
Dai, H., Chang, Y., Tsai, R., Hsu, W. 2009. New
challenges for biological text-mining in the next
decade. Journal of Computer Science and Technol-
ogy, 25(1):169?179.
Jimeno, A., Jimnez-Ruiz, E., Lee, V., Gaudan, S.,
Berlanga, R., Rebholz-Schuhmann, D. 2008. As-
sessment of disease named entity recognition on a
corpus of annotated sentences. BMC Bioinformat-
ics, 9(S-3).
Klinger, R., Tomanek, K. 2007. Classical Probabilistic
Models and Conditional Random Fields. Technical
Report TR07-2-013, Department of Computer Sci-
ence, Dortmund University of Technology, Decem-
ber.
Kulick, S., Bies, A., Liberman, M., Mandel, M., Mc-
Donald, R., Palmer, M., Schein, A., Ungar, L. 2004.
Integrated annotation for biomedical information ex-
traction. In Proceedings of HLT/NAACL 2004 Bi-
oLink Workshop, pages 61?68.
Leaman, R., Gonzalez, G. 2008. Banner: An exe-
cutable survey of advances in biomedical named en-
tity recognition. In Proceedings of Pacific Sympo-
sium on Biocomputing, volume 13, pages 652?663.
Leaman, R., Miller, C., Gonzalez, G. 2009. Enabling
recognition of diseases in biomedical text with ma-
chine learning: Corpus and benchmark. In Proceed-
ings of the 3rd International Symposium on Lan-
guages in Biology and Medicine, pages 82?89.
McCallum, A. 2002. Mallet: A machine learning for
language toolkit. http://mallet.cs.umass.edu,.
Meystre, S., Savova, G., Kipper-Schuler, K., Hurdle,
J. 2008. Extracting information from textual doc-
uments in the electronic health record: a review of
89
recent research. IMIA Yearbook of Medical Infor-
matics, pages 128?44.
Ne?ve?ol, A., Kim, W., Wilbur, W., Lu, Z. 2009. Explor-
ing two biomedical text genres for disease recogni-
tion. In Proceedings of the BioNLP 2009 Workshop,
pages 144?152, June.
Nadeau, D., Sekine, S. 2007. A survey of named entity
recognition and classification. Linguisticae Investi-
gationes, 30(1):3?26.
Noreen, E.W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
Rosario, B., Hearst, M. 2004. Classifying semantic
relations in bioscience texts. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL?04).
Schwartz, A., Hearst, M. 2003. A simple algorithm
for identifying abbreviation definitions in biomedi-
cal text. In Proceedings of Pacific Symposium on
Biocomputing, pages 451?62.
Settles, B. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop
on Natural Language Processing in Biomedicine
and its Applications, pages 104?107.
Smith, L., Tanabe, L., Ando, R., Kuo, C., et al 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biology, 9(Suppl 2).
Torii, M., Hu, Z., Wu, C., Liu, H. 2009. Biotagger-
GM: a gene/protein name recognition system. Jour-
nal of the American Medical Informatics Associa-
tion : JAMIA, 16:247?255.
Vlachos, A. 2007. Tackling the BioCreative2 gene
mention task with conditional random fields and
syntactic parsing. In Proceedings of the 2nd BioCre-
ative Challenge Evaluation Workshop, pages 85?87.
Zweigenbaum, P., Demner-Fushman, D., Yu, H., Co-
hen, K. 2007. Frontiers of biomedical text mining:
current progress. Brief Bioinform, 8(5):358?375.
90
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 124?133,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
A Study on Dependency Tree Kernels
for Automatic Extraction of Protein-Protein Interaction
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ? and Alessandro Moschitti ?
? Department of Information Engineering and Computer Science, University of Trento, Italy
? Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
{chowdhury,lavelli}@fbk.eu, moschitti@disi.unitn.it
Abstract
Kernel methods are considered the most ef-
fective techniques for various relation extrac-
tion (RE) tasks as they provide higher accu-
racy than other approaches. In this paper,
we introduce new dependency tree (DT) ker-
nels for RE by improving on previously pro-
posed dependency tree structures. These are
further enhanced to design more effective ap-
proaches that we call mildly extended depen-
dency tree (MEDT) kernels. The empirical re-
sults on the protein-protein interaction (PPI)
extraction task on the AIMed corpus show that
tree kernels based on our proposed DT struc-
tures achieve higher accuracy than previously
proposed DT and phrase structure tree (PST)
kernels.
1 Introduction
Relation extraction (RE) aims at identifying in-
stances of pre-defined relation types in text as for
example the extraction of protein-protein interaction
(PPI) from the following sentence:
?Native C8 also formed a heterodimer
with C5, and low concentrations of
polyionic ligands such as protamine and
suramin inhibited the interaction.?
After identification of the relevant named entities
(NE, in this case proteins) C8 and C5, the RE task
determines whether there is a PPI relationship be-
tween the entities above (which is true in the exam-
ple).
Kernel based approaches for RE have drawn a lot
of interest in recent years since they can exploit a
huge amount of features without an explicit repre-
sentation. Some of these approaches are structure
kernels (e.g. tree kernels), which carry out struc-
tural similarities between instances of relations, rep-
resented as phrase structures or dependency trees,
in terms of common substructures. Other kernels
simply use techniques such as bag-of-words, subse-
quences, etc. to map the syntactic and contextual
information to flat features, and later compute simi-
larity.
One variation of tree kernels is the dependency
tree (DT) kernel (Culotta and Sorensen, 2004;
Nguyen et al, 2009). A DT kernel (DTK) is a
tree kernel that is computed on a dependency tree
(or subtree). A dependency tree encodes grammati-
cal relations between words in a sentence where the
words are nodes, and dependency types (i.e. gram-
matical functions of children nodes with respect to
their parents) are edges. The main advantage of a
DT in comparison with phrase structure tree (PST)
is that the former allows for relating two words di-
rectly (and in more compact substructures than PST)
even if they are far apart in the corresponding sen-
tence according to their lexical word order.
Several kernel approaches exploit syntactic de-
pendencies among words for PPI extraction from
biomedical text in the form of dependency graphs or
dependency paths (e.g. Kim et al (2010) or Airola
et al (2008)). However, to the best of our knowl-
edge, there are only few works on the use of DT
kernels for this task. Therefore, exploring the po-
tential of DTKs applied to different structures is a
worthwhile research direction. A DTK, pioneered
by Culotta and Sorensen (2004), is typically applied
to the minimal or smallest common subtree that in-
cludes a target pair of entities. Such subtree reduces
124
Figure 1: Part of the DT for the sentence ?The binding
epitopes of BMP-2 for BMPR-IA was characterized using
BMP-2 mutant proteins?. The dotted area indicates the
minimal subtree.
unnecessary information by placing word(s) closer
to its dependent(s) inside the tree and emphasizes
local features of relations. Nevertheless, there are
cases where a minimal subtree might not contain im-
portant cue words or predicates. For example, con-
sider the following sentence where a PPI relation
holds between BMP-2 and BMPR-IA, but the mini-
mal subtree does not contain the cue word ?binding?
as shown in Figure 1:
The binding epitopes of BMP-2 for
BMPR-IA was characterized using BMP-
2 mutant proteins.
In this paper we investigate two assumptions. The
first is that a DTK based on a mild extension of
minimal subtrees would produce better results than
the DTK on minimal subtrees. The second is that
previously proposed DT structures can be further
improved by introducing simplified representation
of the entities as well as augmenting nodes in the
DT tree structure with relevant features. This paper
presents an evaluation of the above assumptions.
More specifically, the contributions of this paper
are the following:
? We propose the use of new DT structures,
which are improvement on the structures de-
fined in Nguyen et al (2009) with the most gen-
eral (in terms of substructures) DTK, i.e. Par-
tial Tree Kernel (PTK) (Moschitti, 2006).
? We firstly propose the use of the Unlexicalized
PTK (Severyn and Moschitti, 2010) with our
dependency structures, which significantly im-
proves PTK.
? We compare the performance of the proposed
DTKs on PPI with the one of PST kernels and
show that, on biomedical text, DT kernels per-
form better.
? Finally, we introduce a novel approach (called
mildly extended dependency tree (MEDT) ker-
nel1, which achieves the best performance
among various (both DT and PST) tree kernels.
The remainder of the paper is organized as fol-
lows. In Section 2, we introduce tree kernels and re-
lation extraction and we also review previous work.
Section 3 describes the unlexicalized PTK (uPTK).
Then, in Section 4, we define our proposed DT struc-
tures including MEDT. Section 5 describes the ex-
perimental results on the AIMed corpus (Bunescu et
al., 2005) and discusses their outcomes. Finally, we
conclude with a summary of our study as well as
plans for future work.
2 Background and Related Work
The main stream work for Relation Extraction uses
kernel methods. In particular, as the syntactic struc-
ture is very important to derive the relationships be-
tween entities in text, several tree kernels have been
designed and experimented. In this section, we in-
troduce such kernels, the problem of relation extrac-
tion and we also focus on the biomedical domain.
2.1 Tree Kernel types
The objective behind the use of tree kernels is
to compute the similarity between two instances
through counting similarities of their sub-structures.
Among the different proposed methods, two of the
most effective approaches are Subset Tree (SST)
kernel (Collins and Duffy, 2001) and Partial Tree
Kernel (PTK) (Moschitti, 2006).
The SST kernel generalizes the subtree ker-
nel (Vishwanathan and Smola, 2002), which consid-
ers all common subtrees in the tree representation of
two compared sentences. In other words, two sub-
trees are identical if the node labels and order of chil-
dren are identical for all nodes. The SST kernel re-
laxes the constraint that requires leaves to be always
included in the sub-structures. In SST, for a given
node, either none or all of its children have to be in-
cluded in the resulting subset tree. An extension of
1We defined new structures, which as it is well known it
corresponds to define a new kernel.
125
the SST kernel is the SST+bow (bag-of-words) ker-
nel (Zhang and Lee, 2003; Moschitti, 2006a), which
considers individual leaves as sub-structures as well.
The PT kernel (Moschitti, 2006) is more flexi-
ble than SST by virtually allowing any tree sub-
structure; the only constraint is that the order of child
nodes must be identical. Both SST and PT kernels
are convolution tree kernels2.
The PT kernel is the most complete in terms of
structures. However, the massive presence of child
node subsequences and single child nodes, which in
a DT often correspond to words, may cause overfit-
ting. Thus we propose the use of the unlexicalized
(i.e. PT kernel without leaves) tree kernel (uPTK)
(Severyn and Moschitti, 2010), in which structures
composed by only one lexical element, i.e. single
nodes, are removed from the feature space (see Sec-
tion 3).
2.2 Relation Extraction using Tree Kernels
A first version of dependency tree kernels (DTKs)
was proposed by Culotta and Sorensen (2004). In
their approach, they find the smallest common sub-
tree in the DT that includes a given pair of enti-
ties. Then, each node of the subtree is represented
as a feature vector. Finally, these vectors are used
to compute similarity. However, the tree kernel they
defined is not a convolution kernel, and hence it gen-
erates a much lower number of sub-structures result-
ing in lower performance.
For any two entities e1 and e2 in a DT, Nguyen
et al (2009) defined the following three dependency
structures to be exploited by convolution tree ker-
nels:
? Dependency Words (DW) tree: a DW tree is
the minimal subtree of a DT, which includes e1
and e2. An extra node is inserted as parent of
the corresponding NE, labeled with the NE cat-
egory. Only words are considered in this tree.
? Grammatical Relation (GR) tree: a GR tree
is similar to a DW tree except that words are
replaced by their grammatical functions, e.g.
prep, nsubj, etc.
2Convolution kernels aim to capture structural information
in term of sub-structures, providing a viable alternative to flat
features (Moschitti, 2004).
? Grammatical Relation and Words (GRW) tree:
a GRW tree is the minimal subtree that uses
both words and grammatical functions, where
the latter are inserted as parent nodes of the for-
mer.
Using PTK for the above dependency tree struc-
tures, the authors achieved an F-measure of 56.3 (for
DW), 60.2 (for GR) and 58.5 (for GRW) on the ACE
2004 corpus3.
Moschitti (2004) proposed the so called path-
enclosed tree (PET)4 of a PST for Semantic Role
Labeling. This was later adapted by Zhang et al
(2005) for relation extraction. A PET is the smallest
common subtree of a PST, which includes the two
entities involved in a relation.
Zhou et al (2007) proposed the so called context-
sensitive tree kernel approach based on PST, which
expands PET to include necessary contextual in-
formation. The expansion is carried out by some
heuristics tuned on the target RE task.
Nguyen et al (2009) improved the PET represen-
tation by inserting extra nodes for denoting the NE
category of the entities inside the subtree. They also
used sequence kernels from tree paths, which pro-
vided higher accuracy.
2.3 Relation Extraction in the biomedical
domain
There are several benchmarks for the PPI task,
which adopt different PPI annotations. Conse-
quently the experimental results obtained by dif-
ferent approaches are often difficult to compare.
Pyysalo et al (2008) put together these corpora (in-
cluding the AIMed corpus used in this paper) in a
common format for comparative evaluation. Each
of these corpora is known as converted corpus of the
corresponding original corpus.
Several kernel-based RE approaches have been
reported to date for the PPI task. These are based on
various methods such as subsequence kernel (Lodhi
et al, 2002; Bunescu and Mooney, 2006), depen-
dency graph kernel (Bunescu and Mooney, 2005),
etc. Different work exploited dependency analy-
ses with different kernel approaches such as bag-of-
3http://projects.ldc.upenn.edu/ace/
4Also known as shortest path-enclosed tree or SPT (Zhou et
al., 2007).
126
words kernel (e.g. Miwa et al (2009)), graph based
kernel (e.g. Kim et al (2010)), etc. However, there
are only few researches that attempted the exploita-
tion of tree kernels on dependency tree structures.
S?tre et al (2007) used DT kernels on AIMed
corpus and achieved an F-score of 37.1. The re-
sults were far better when they combined the out-
put of the dependency parser with that of a Head-
driven Phrase Structure Grammar (HPSG) parser,
and applied tree kernel on it. Miwa et al (2009) also
proposed a hybrid kernel 5, which is a composition
of all-dependency-paths kernel (Airola et al, 2008),
bag-of-words kernel and SST kernel. They used
multiple parser inputs. Their system is the current
state-of-the-art for PPI extraction on several bench-
marks. Interestingly, they applied SST kernel on the
shortest dependency paths between pairs of proteins
and achieved a relatively high F-score of 55.1. How-
ever, the trees they constructed from the shortest de-
pendency paths are actually not dependency trees. In
a dependency tree, there is only one node for each
individual word whereas in their constructed trees
(please refer to Fig. 6 of Miwa et al (2009)), a word
(that belongs to the shortest path) has as many node
representations as the number of dependency rela-
tions with other words (those belonging to the short-
est path). Perhaps, this redundancy of information
might be the reason their approach achieved higher
result. In addition to work on PPI pair extraction,
there has been some approaches that exploited de-
pendency parse analyses along with kernel methods
for identifying sentences that might contain PPI pairs
(e.g. Erkan et al (2007)).
In this paper, we focus on finding the best repre-
sentation based on a single structure. We speculate
that this can be helpful to improve the state-of-the-
art using several combinations of structures and fea-
tures. As a first step, we decided to use uPTK, which
is more robust to overfitting as the description in the
next section unveil.
5The term ?hybrid kernel? is identical to ?combined kernel?.
It refers to those kernels that combine multiple types of kernels
(e.g., tree kernels, graph kernels, etc)
3 Unlexicalized Partial Tree Kernel
(uPTK)
The uPTK was firstly proposed in (Severyn and
Moschitti, 2010) and experimented with semantic
role labeling (SRL). The results showed no improve-
ment for such task but it is well known that in SRL
lexical information is essential (so in that case it
could have been inappropriate). The uPTK defini-
tion follows the general setting of tree kernels.
A tree kernel function over two trees, T1 and T2,
is defined as
TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2),
where NT1 and NT2 are the sets of nodes in T1 and
T2, respectively, and
?(n1, n2) =
|F|?
i=1
?i(n1)?i(n2).
The ? function is equal to the number of common
fragments rooted in nodes n1 and n2 and thus de-
pends on the fragment type.
The algorithm for the uPTK computation straight-
forwardly follows from the definition of the ? func-
tion of PTK provided in (Moschitti, 2006). Given
two nodes n1 and n2 in the corresponding two trees
T1 and T2, ? is evaluated as follows:
1. if the node labels of n1 and n2 are different then
?(n1, n2) = 0;
2. else ?(n1, n2) = ?
(
?2 +
?
~I1,~I2,l(~I1)=l(~I2)
?d(
~I1)+d(~I2)
l(~I1)?
j=1
?(cn1(~I1j), cn2(~I2j))
)
,
where:
1. ~I1 = ?h1, h2, h3, ..? and ~I2 = ?k1, k2, k3, ..?
are index sequences associated with the ordered
child sequences cn1 of n1 and cn2 of n2, respec-
tively;
2. ~I1j and ~I2j point to the j-th child in the corre-
sponding sequence;
3. l(?) returns the sequence length, i.e. the number
of children;
127
4. d(~I1) = ~I1l(~I1)?
~I11 + 1 and d(~I2) = ~I2l(~I2)?
~I21 + 1; and
5. ? and ? are two decay factors for the size of
the tree and for the length of the child subse-
quences with respect to the original sequence,
i.e. we account for gaps.
The uPTK can be obtained by removing ?2 from
the equation in step 2. An efficient algorithm for the
computation of PTK is given in (Moschitti, 2006).
This evaluates ? by summing the contribution of
tree structures coming from different types of se-
quences, e.g. those composed by p children such
as:
?(n1, n2) = ?
(
?2 +
?lm
p=1 ?p(cn1 , cn2)
)
, (1)
where ?p evaluates the number of common subtrees
rooted in subsequences of exactly p children (of n1
and n2) and lm = min{l(cn1), l(cn2)}. It is easy to
verify that we can use the recursive computation of
?p by simply removing ?2 from Eq. 1.
4 Proposed dependency structures and
MEDT kernel
Our objective is twofold: (a) the definition of im-
proved DT structures and (b) the design of new DT
kernels to include important words residing outside
of the shortest dependency tree, which are neglected
in current approaches. For achieving point (a), we
modify the DW, GR and GRW structures, previously
proposed by Nguyen et al (2009). The new pro-
posed structures are the following:
? Grammatical Relation and lemma (GRL) tree:
A GRL tree is similar to a GRW tree except
that words are replaced by their corresponding
lemmas.
? Grammatical Relation, PoS and lemma
(GRPL) tree: A GRPL tree is an extension of a
GRL tree, where the part-of-speech (PoS) tag
of each of the corresponding words is inserted
as a new node between its grammatical func-
tion and its lemma, i.e. the new node becomes
the parent node of the node containing the
lemma.
Figure 2: Part of the DT for the sentence ?Interaction
was identified between BMP-2 and BMPR-IA?. The dot-
ted area indicates the minimal subtree.
Figure 3: Part of the DT for the sentence ?Phe93 forms
extensive contacts with a peptide ligand in the crystal
structure of the EBP bound to an EMP1?. The dotted
area indicates the minimal subtree.
? Ordered GRL (OGRL) or ordered GRW
(OGRW) tree: in a GRW (or GRL) tree, the
node containing the grammatical function of
a word is inserted as the parent node of such
word. So, if the word has a parent node con-
taining its NE category, the newly inserted node
with grammatical function becomes the child
node of the node containing NE category, i.e.
the order of the nodes is the following ? ?NE
category ? grammatical relation ? word (or
lemma)?. However, in OGRW (or OGRL), this
ordering is modified as follows ? ?grammatical
relation? NE category? word (or lemma)?.
? Ordered GRPL (OGRPL) tree: this is similar
to the OGRL tree except for the order of the
nodes, which is the following ? ?grammatical
relation? NE category? PoS? lemma?.
? Simplified (S) tree: any tree structure would
become an S tree if it contains simplified repre-
sentations of the entity types, where all its parts
except the head word of a multi-word entity are
not considered in the minimal subtree.
The second objective is to extend DTKs to include
important cue words or predicates that are missing
128
in the minimal subtree. We do so by mildly expand-
ing the minimal subtree, i.e. we define the mildly
extended DT (MEDT) kernel. We propose three dif-
ferent expansion rules for three versions of MEDT
as follows:
? Expansion rule for MEDT-1 kernel: If the root
of the minimal subtree is not a modifier (e.g.
adjective) or a verb, then look for such node in
its children or in its parent (in the original DT
tree) to extend the subtree.
The following example shows a sentence where
this rule would be applicable:
The binding epitopes of BMP-2
for BMPR-IA was characterized us-
ing BMP-2 mutant proteins.
Here, the cue word is ?binding?, the root of the
minimal subtree is ?epitopes? and the target en-
tities are BMP-2 and BMPR-IA. However, as
shown in Figure 1, the minimal subtree does
not contain the cue word.
? Expansion rule for MEDT-2 kernel: If the root
of the minimal subtree is a verb and its subject
(or passive subject) in the original DT tree is
not included in the subtree, then include it.
Consider the following sentence:
Interaction was identified be-
tween BMP-2 and BMPR-IA.
Here, the cue word is ?Interaction?, the root
is ?identified? and the entities are BMP-2 and
BMPR-IA. The passive subject ?Interaction?
does not belong to the minimal subtree (see
Figure 2).
? Expansion rule for MEDT-3 kernel: If the root
of the minimal subtree is the head word of one
of the interacting entities, then add the parent
node (in the original DT tree) of the root node
as the new root of the subtree.
This is an example sentence where this rule is
applicable (see Figure 3):
Phe93 forms extensive contacts
with a peptide ligand in the crystal
structure of the EBP bound to an
EMP1.
5 Experiments and results
We carried out several experiments with different
dependency structures and tree kernels. Most im-
portantly, we tested tree kernels on PST and our im-
proved representations for DT.
5.1 Data and experimental setup
We used the AIMed corpus (Bunescu et al, 2005)
converted using the software provided by Pyysalo et
al. (2008). AIMed is the largest benchmark corpus
(in terms of number of sentences) for the PPI task.
It contains 1,955 sentences, in which are annotated
1,000 positive PPI and 4,834 negative pairs.
We use the Stanford parser6 for parsing the data.7
The SPECIALIST lexicon tool8 is used to normalize
words to avoid spelling variations and also to pro-
vide lemmas. For training and evaluating tree ker-
nels, we use the SVM-LIGHT-TK toolkit9 (Mos-
chitti, 2006; Joachims, 1999). We tuned the param-
eters ?, ? and c following the approach described by
Hsu et al (2003), and used biased hyperplane.10 All
the other parameters are left as their default values.
Our experiments are evaluated with 10-fold cross
validation using the same split of the AIMed corpus
used by Bunescu et al (2005).
5.2 Results and Discussion
The results of different tree kernels applied to dif-
ferent structures are shown in Tables 1 and 2. All
the tree structures are tested with four different tree
kernel types: SST, SST+bow, PTK and uPTK.
According to the empirical outcome, our new DT
structures perform better than the existing tree struc-
tures. The highest result (F: 46.26) is obtained by
applying uPTK to MEDT-3 (SOGRL). This is 6.68
higher than the best F-measure obtained by previous
DT structures proposed in Nguyen et al (2009), and
0.36 higher than the best F-measure obtained using
PST (PET).
6http://nlp.stanford.edu/software/lex-parser.shtml
7For some of the positive PPI pairs, the connecting depen-
dency tree could not be constructed due to parsing errors for
the corresponding sentences. Such pairs are considered as false
negative (FN) during precision and recall measurements.
8http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10Please refer to http://svmlight.joachims.org/ and
http://disi.unitn.it/moschitti/Tree-Kernel.htm for details
about parameters of the respective tools
129
DT DT DT DT DT DT DT DT DT
(GR) (SGR) (DW) (SDW) (GRW) (SGRW) (SGRL) (SGRPL) (OGRPL)
SST P: 55.29 P: 54.22 P: 31.87 P: 30.74 P: 52.76 P: 52.47 P: 56.09 P: 56.03 P: 57.85
R: 23.5 R: 24.4 R: 27.5 R: 27.3 R: 33.4 R: 30.8 R: 33.6 R: 33.0 R: 31.7
F: 32.98 F: 33.66 F: 29.52 F: 28.92 F: 40.9 F: 38.82 F: 42.03 F: 41.54 F: 40.96
SST P: 57.87 P: 54.91 P: 30.71 P: 29.98 P: 52.98 P: 51.06 P: 51.99 P: 56.8 P: 61.73
+ R: 21.7 R: 23.5 R: 26.9 R: 25.9 R: 32.0 R: 31.3 R: 31.4 R: 28.8 R: 29.2
bow F: 31.56 F: 32.91 F: 28.68 F: 27.79 F: 39.9 F: 38.81 F: 39.15 F: 38.22 F: 39.65
PT P: 60.0 P: 57.84 P: 40.44 P: 42.2 P: 53.35 P: 53.41 P: 51.29 P: 52.88 P: 53.55
R: 15.9 R: 16.6 R: 23.9 R: 26.5 R: 34.2 R: 36.0 R: 37.9 R: 33.0 R: 33.2
F: 25.14 F: 25.8 F: 30.04 F: 32.56 F: 41.68 F: 43.01 F: 43.59 F: 40.64 F: 40.99
uPT P: 58.77 P: 59.5 P: 29.21 P: 29.52 P: 51.86 P: 52.17 P: 52.1 P: 54.64 P: 56.43
R: 23.8 R: 26.0 R: 30.2 R: 31.5 R: 32.0 R: 33.7 R: 36.0 R: 31.2 R: 30.7
F: 33.88 F: 36.19 F: 29.7 F: 30.48 F: 39.58 F: 40.95 F: 42.58 F: 39.72 F: 39.77
Table 1: Performance of DT (GR), DT (DW) and DT (GRW) (proposed by (Nguyen et al, 2009)) and their modified
and improved versions on the converted AIMed corpus.
RE experiments carried out on newspaper text
corpora (such as ACE 2004) have indicated that ker-
nels based on PST obtain better results than kernels
based on DT. Interestingly, our experiments on a
biomedical text corpus indicate an opposite trend.
Intuitively, this might be due to the different na-
ture of the PPI task. PPI can be often identified by
spotting cue words such as interaction, binding, etc,
since the interacting entities (i.e. proteins) usually
have direct syntactic dependency relation on such
cue words. This might have allowed kernels based
on DT to be more accurate.
Although tree kernels applied on DT and PST
structures have produced high performance on cor-
pora of news text (Zhou et al, 2007; Nguyen et al,
2009), in case of biomedical text the results that we
obtained are relatively low. This may be due to the
fact that biomedical texts are different from newspa-
per texts: more variation in vocabulary, more com-
plex naming of (bio) entities, more diversity of the
valency of verbs and so on.
One important finding of our experiments is the
effectiveness of the mild extension of DT struc-
tures. MEDT-3 achieves the best result for all ker-
nels (SST, SST+bow, PTK and uPTK). However, the
other two versions of MEDT appear to be less effec-
tive.
In general, the empirical outcome suggests that
uPTK can better exploit our proposed DT structures
as well as PST. The superiority of uPTK on PTK
demonstrates that single lexical features (i.e. fea-
tures with flat structure) tend to overfit.
Finally, we have performed statistical tests to as-
sess the significance of our results. For each kernel
(i.e. SST, SST+bow, PTK, uPTK), the PPI predic-
tions using the best structure (i.e. MEDT-3 applied
to SOGRL) are compared against the predictions of
the other structures. The tests were performed using
the approximate randomization procedure (Noreen,
1989). We set the number of iterations to 1,000 and
the confidence level to 0.01. According to the tests,
for each kernel, our best structure produces signifi-
cantly better results.
5.3 Comparison with previous work
To the best of our knowledge, the only work on tree
kernel applied on dependency trees that we can di-
rectly compare to ours is reported by S?tre et al
(2007). Their DT kernel achieved an F-score of
37.1 on AIMed corpus which is lower than our best
results. As discussed earlier, Miwa et al (2009))
also used tree kernel on dependency analyses and
achieved a much higher result. However, the tree
structure they used contains multiple nodes for a sin-
gle word and this does not comply with the con-
straints usually applied to dependency tree structures
(refer to Section 2.3). It would be interesting to ex-
amine why such type of tree representation leads to
130
DT DT DT DT MEDT-1 MEDT-2 MEDT-3 PST
(SOGRPL) (OGRL) (SOGRW) (SOGRL) (SOGRL) (SOGRL) (SOGRPL) (PET)
SST P: 57.59 P: 54.38 P: 51.49 P: 54.08 P: 58.15 P: 54.46 P: 59.55 P: 52.72
R: 33.0 R: 33.5 R: 31.2 R: 33.8 R: 34.6 R: 33.6 R: 37.1 R: 35.9
F: 41.96 F: 41.46 F: 38.86 F: 41.6 F: 43.39 F: 41.56 F: 45.72 F: 42.71
SST P: 60.31 P: 53.22 P: 50.08 P: 53.26 P: 58.84 P: 52.87 P: 59.35 P: 52.88
+ R: 30.7 R: 33.1 R: 30.9 R: 32.7 R: 32.6 R: 32.2 R: 34.9 R: 37.7
bow F: 40.69 F: 40.82 F: 38.22 F: 40.52 F: 41.96 F: 40.02 F: 43.95 F: 44.02
PT P: 55.45 P: 49.78 P: 51.05 P: 51.61 P: 52.94 P: 50.89 P: 54.1 P: 58.39
R: 34.6 R: 34.6 R: 34.1 R: 36.9 R: 36.0 R: 37.0 R: 38.9 R: 36.9
F: 42.61 F: 40.82 F: 40.89 F: 43.03 F: 42.86 F: 42.85 F: 45.26 F: 45.22
uPT P: 56.2 P: 50.87 P: 50.0 P: 52.74 P: 55.0 P: 52.17 P: 56.85 P: 56.6
R: 32.2 R: 35.0 R: 33.0 R: 35.6 R: 34.1 R: 34.8 R: 39.0 R: 38.6
F: 40.94 F: 41.47 F: 39.76 F: 42.51 F: 42.1 F: 41.75 F: 46.26 F: 45.9
Table 2: Performance of the other improved versions of DT kernel structures (including MEDT kernels) as well as
PST (PET) kernel (Moschitti, 2004; Nguyen et al, 2009) on the converted AIMed corpus.
a better result.
In this work, we compare the performance of tree
kernels applied of DT with that of PST. Previously,
Tikk et al (2010) applied similar kernels on PST for
exactly the same task and data set. They reported
that SST and PTK (on PST) achieved F-scores of
26.2 and 34.6, respectively on the converted AIMed
corpus (refer to Table 2 in their paper). Such results
do not match our figures obtained with the same
kernels on PST. We obtain much higher results for
those kernels. It is difficult to understand the rea-
son for such differences between our and their re-
sults. A possible explanation could be related to pa-
rameter settings. Another source of uncertainty is
given by the tool for tree kernel computation, which
in their case is not mentioned. Moreover, their de-
scription of PT and SST (in Figure 1 of their paper)
appears to be imprecise: for example, in (partial or
complete) phrase structure trees, words can only ap-
pear as leaves but in their figure they appear as non-
terminal nodes.
The comparison with other kernel approaches (i.e.
not necessarily tree kernels on DT or PST) shows
that there are model achieving higher results (e.g.
Giuliano et al (2006), Kim et al (2010), Airola et
al. (2008), etc). State-of-the-art results on most of
the PPI data sets are obtained by the hybrid kernel
presented in Miwa et al (2009). As noted earlier,
our work focuses on the design of an effective DTK
for PPI that can be combined with others and that
can hopefully be used to design state-of-the-art hy-
brid kernels.
6 Conclusion
In this paper, we have proposed a study of PPI ex-
traction from specific biomedical data based on tree
kernels. We have modeled and experimented with
new kernels and DT structures, which can be ex-
ploited for RE tasks in other domains too.
More specifically, we applied four different tree
kernels on existing and newly proposed DT and PST
structures. We have introduced some extensions of
DT kernel structures which are linguistically moti-
vated. We call these as mildly extended DT kernels.
We have also shown that in PPI extraction lexical
information can lead to overfitting as uPTK outper-
forms PTK. In general, the empirical results show
that our DT structures perform better than the previ-
ously proposed PST and DT structures.
The ultimate objective of our work is to improve
tree kernels applied to DT and then combine them
with other types of kernels and data to produce more
accurate models.
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management in
cancer care?. The authors would like to thank the anony-
mous reviewers for providing excellent feedback.
131
References
A Airola, S Pyysalo, J Bj?orne, T Pahikkala, F Gin-
ter, and T Salakoski. 2008. A graph kernel for
protein-protein interaction extraction. In Proceedings
of BioNLP 2008, pages 1?9, Columbus, USA.
R Bunescu and R Mooney. 2005. A shortest path depen-
dency kernel for relation extraction. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 724?731, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
R Bunescu and RJ Mooney. 2006. Subsequence ker-
nels for relation extraction. In Proceedings of the
19th Conference on Neural Information Processing
Systems, pages 171?178.
R Bunescu, R Ge, RJ Kate, EM Marcotte, RJ Mooney,
AK Ramani, and YW Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine (Special Issue on Summarization
and Information Extraction from Medical Documents),
33(2):139?155.
M Collins and N Duffy. 2001. Convolution kernels for
natural language. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2001).
A Culotta and J Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain.
G Erkan, A Ozgur, and DR Radev. 2007. Semi-
Supervised Classification for Extracting Protein Inter-
action Sentences using Dependency Parsing. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 228?237.
C Giuliano, A Lavelli, and L Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL?2006),
pages 401?408, Trento, Italy.
CW Hsu, CC Chang, and CJ Lin, 2003. A practical guide
to support vector classification. Department of Com-
puter Science and Information Engineering, National
Taiwan University, Taipei, Taiwan.
T Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in ker-
nel methods: support vector learning, pages 169?184.
MIT Press, Cambridge, MA, USA.
S Kim, J Yoon, J Yang, and S Park. 2010. Walk-weighted
subsequence kernels for protein-protein interaction ex-
traction. BMC Bioinformatics, 11(1).
H Lodhi, C Saunders, J Shawe-Taylor, N Cristianini, and
C Watkins. 2002. Text classification using string ker-
nels. Journal of Machine Learning Research, 2:419?
444, March.
M Miwa, R S?tre, Y Miyao, T Ohta, and J Tsujii. 2009.
Protein-protein interaction extraction by leveraging
multiple kernels and parsers. International Journal of
Medical Informatics, 78.
A Moschitti. 2004. A study on convolution kernels for
shallow semantic parsing. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, ACL ?04, Barcelona, Spain.
A Moschitti. 2006. Efficient convolution kernels for de-
pendency and constituent syntactic trees. In Johannes
Fu?rnkranz, Tobias Scheffer, and Myra Spiliopoulou,
editors, Machine Learning: ECML 2006, volume 4212
of Lecture Notes in Computer Science, pages 318?329.
Springer Berlin / Heidelberg.
A Moschitti. 2006a. Making Tree Kernels Practical for
Natural Language Learning. In Proceedings of the
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics, Trento, Italy.
TT Nguyen, A Moschitti, and G Riccardi. 2009. Con-
volution kernels on constituent, dependency and se-
quential structures for relation extraction. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP?2009), pages
1378?1387, Singapore, August.
EW Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
S Pyysalo, A Airola, J Heimonen, J Bjo?rne, F Ginter,
and T Salakoski. 2008. Comparative analysis of five
protein-protein interaction corpora. BMC Bioinfor-
matics, 9(Suppl 3):S6.
R S?tre, K Sagae, and J Tsujii. 2007. Syntactic features
for protein-protein interaction extraction. In Proceed-
ings of the Second International Symposium on Lan-
guages in Biology and Medicine (LBM 2007), pages
6.1?6.14, Singapore.
A Severyn and A Moschitti. 2010. Fast cutting plane
training for structural kernels. In Proceedings of
ECML-PKDD.
D Tikk, P Thomas, P Palaga, J Hakenberg, and U Leser.
2010. A Comprehensive Benchmark of Kernel Meth-
ods to Extract Protein-Protein Interactions from Liter-
ature. PLoS Computational Biology, 6(7), July.
SVN Vishwanathan and AJ Smola. 2002. Fast kernels on
strings and trees. In Proceedings of Neural Informa-
tion Processing Systems (NIPS?2002), pages 569?576,
Vancouver, British Columbia, Canada.
D Zhang and WS Lee. 2003. Question classification us-
ing support vector machines. In Proceedings of the
132
26th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ?03, pages 26?32, Toronto, Canada.
M Zhang, J Su, D Wang, G Zhou, and CL Tan. 2005.
Discovering relations between named entities from a
large raw corpus using tree similarity-based clustering.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 378?389. Springer Berlin / Heidelberg.
GD Zhou, M Zhang, DH Ji, and QM Zhu. 2007. Tree
kernel-based relation extraction with context-sensitive
structured parse tree information. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
728?736, June.
133
Proceedings of the Fifth Law Workshop (LAW V), pages 101?109,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Assessing the Practical Usability
of an Automatically Annotated Corpus
Md. Faisal Mahbub Chowdhury ? ? and Alberto Lavelli ?
? Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy
? Department of Information Engineering and Computer Science, University of Trento, Italy
{chowdhury,lavelli}@fbk.eu
Abstract
The creation of a gold standard corpus (GSC)
is a very laborious and costly process. Silver
standard corpus (SSC) annotation is a very re-
cent direction of corpus development which
relies on multiple systems instead of human
annotators. In this paper, we investigate the
practical usability of an SSC when a machine
learning system is trained on it and tested on
an unseen benchmark GSC. The main focus of
this paper is how an SSC can be maximally ex-
ploited. In this process, we inspect several hy-
potheses which might have influenced the idea
of SSC creation. Empirical results suggest that
some of the hypotheses (e.g. a positive impact
of a large SSC despite of having wrong and
missing annotations) are not fully correct. We
show that it is possible to automatically im-
prove the quality and the quantity of the SSC
annotations. We also observe that considering
only those sentences of SSC which contain an-
notations rather than the full SSC results in a
performance boost.
1 Introduction
The creation of a gold standard corpus (GSC) is
not only a very laborious task due to the manual ef-
fort involved but also a costly and time consuming
process. However, the importance of the GSC to ef-
fectively train machine learning (ML) systems can-
not be underestimated. Researchers have been trying
for years to find alternatives or at least some com-
promise. As a result, self-training, co-training and
unsupervised approaches targeted for specific tasks
(such as word sense disambiguation, syntactic pars-
ing, etc) have emerged. In the process of these re-
searches, it became clear that the size of the (manu-
ally annotated) training corpus has an impact on the
final outcome.
Recently an initiative is ongoing in the context of
the European project CALBC1 which aims to create
a large, so called silver standard corpus (SSC) us-
ing harmonized annotations automatically produced
by multiple systems (Rebholz-Schuhmann et al,
2010; Rebholz-Schuhmann et al, 2010a; Rebholz-
Schuhmann et al, 2010b). The basic idea is that
independent biomedical named entity recognition
(BNER) systems annotate a large corpus of biomed-
ical articles without any restriction on the methodol-
ogy or external resources to be exploited. The differ-
ent annotations are automatically harmonized using
some criteria (e.g. minimum number of systems to
agree on a certain annotation) to yield a consensus
based corpus. This consensus based corpus is called
silver standard corpus because, differently from a
GSC, it is not created exclusively by human anno-
tators. Several factors can influence the quantity and
quality of the annotations during SSC development.
These include varying performance, methodology,
annotation guidelines and resources of the SSC an-
notation systems (henceforth annotation systems).
The annotation of SSC in the framework of the
CALBC project is focused on (bio) entity mentions
(a specific application of the named entity recogni-
tion (NER)2 task). However, the idea of SSC cre-
ation might also be applied to other types of anno-
tations, e.g. annotation of relations among entities,
annotation of treebanks and so on. Hence, if it can be
1http://www.ebi.ac.uk/Rebholz-srv/CALBC/project.html
2Named entity recognition is the task of locating boundaries
of the entity mentions in a text and tagging them with their cor-
responding semantic types (e.g. person, location, disease and
so on).
101
shown that an SSC is a useful resource for the NER
task, similar resources can be developed for anno-
tation of information other than entities and utilized
for the relevant natural language processing (NLP)
tasks.
The primary objective of SSC annotation is to
compensate the cost, time and manual effort re-
quired for a GSC. The procedure of SSC develop-
ment is inexpensive, fast and yet capable of yielding
huge amount of annotated data. These advantages
trigger several hypotheses. For example:
? The size of annotated training corpus always
plays a crucial role in the performance of ML
systems. If the annotation systems have very
high precision and somewhat moderate recall,
they would be also able to annotate automat-
ically a huge SSC which would have a good
quality of annotations. So, one might assume
that, even if such an SSC may contain wrong
and missing annotations, a relatively 15 or 20
times bigger SSC than a smaller GSC should
allow an ML based system to ameliorate the ad-
verse effects of the erroneous annotations.
? Rebholz-Schuhmann et al (2010) hypothesized
that an SSC might serve as an approximation of
a GSC.
? In the absence of a GSC, it is expected that
ML systems would be able to exploit the har-
monised annotations of an SSC to annotate un-
seen text with reasonable accuracy.
? An SSC could be used to semi-automate the an-
notations of a GSC. However, in that case, it
is expected that the annotation systems would
have very high recall. One can assume that
converting an SSC into a GSC would be less
time consuming and less costly than develop-
ing a GSC from scratch.
All these hypotheses are yet to be verified. Nev-
ertheless, once we have an SSC annotated with cer-
tain type of information, the main question would be
how this corpus can be maximally exploited given
the fact that it might be created by annotation sys-
tems that used different resources and possibly not
the same annotation guidelines. This question is di-
rectly related to the practical usability of an SSC,
which is the focus of this paper.
Taking the aforementioned hypotheses into ac-
count, our goal is to investigate the following re-
search questions which are fundamental to the max-
imum exploitation of an SSC:
1. How can the annotation quality of an SSC be
improved automatically?
2. How would a system trained on an SSC per-
form if tested on an unseen benchmark GSC?
3. Can an SSC combined with a GSC produce a
better trained system?
4. What would be the impact on system perfor-
mance if unannotated sentences3 are removed
from an SSC?
5. What would be the effects of the variation in
the size of an SSC on precision and recall?
Our goal is not to judge the procedure of SSC cre-
ation, rather our objective is to examine how an SSC
can be exploited automatically and maximally for a
specific task. Perhaps this would provide useful in-
sights to re-evaluate the approach of SSC creation.
For our experiments, we use a benchmark GSC
called the BioCreAtIvE II GM corpus (Smith et
al., 2008) and the CALBC SSC-I corpus (Rebholz-
Schuhmann et al, 2010a). Both of these corpora
are annotated with genes. Our motivation behind the
choice of a gene annotated GSC for the SSC evalu-
ation is that ML based BNER for genes has already
achieved a sufficient level of maturity. This is not
the case for other important bio-entity types, primar-
ily due to the absence of training GSC of adequate
size. In fact, for many bio-entity types there exist no
GSC. If we can achieve a reasonably good baseline
for gene mention identification by maximizing the
exploitation of SSC, we might be able to apply al-
most similar strategies to exploit SSC for other bio-
entity types, too.
The remaining of this paper is organised as fol-
lows. Section 2 includes brief discussion of the re-
lated work. Apart from mentioning the related liter-
ature, this section also underlines the difference of
3For the specific SSC that we use in this work, unannotated
sentences correspond to those sentences that contain no gene
annotation.
102
SSC development with respect to approaches such
as self-training and co-training. Then in Section 3,
we describe the data used in our experiments and the
experimental settings. Following that, in Section 4,
empirical results are presented and discussed. Fi-
nally, we conclude with a description of what we
learned from this work in Section 5.
2 Related Work
As mentioned, the concept of SSC has been initi-
ated by the CALBC project (Rebholz-Schuhmann et
al., 2010a; Rebholz-Schuhmann et al, 2010). So far,
two versions of SSC have been released as part of the
project. The CALBC SSC-I has been harmonised
from the annotations of the systems provided by
the four project partners. Three of them are dictio-
nary based systems while the other is an ML based
system. The systems utilized different types of re-
sources such as GENIA corpus (Kim et al, 2003),
Entrez Genes4, Uniprot5, etc. The CALBC SSC-
II corpus has been harmonised from the annotations
done by the 11 participants of the first CALBC chal-
lenge and the project partners.6 Some of the par-
ticipants have used the CALBC SSC-I versions for
training while others used various gene databases or
benchmark GSCs such as the BioCreAtIvE II GM
corpus.
One of the key questions regarding an SSC would
be how close its annotation quality is to a corre-
sponding GSC. On the one hand, every GSC con-
tains its special view of the correct annotation of a
given corpus. On the other hand, an SSC is created
by systems that might be trained with resources hav-
ing different annotation standards. So, it is possible
that the annotations of an SSC significantly differ
with respect to a manually annotated (i.e., gold stan-
dard) version of the same corpus. This is because
human experts are asked to follow specific annota-
tion guidelines.
Rebholz-Schuhmann and Hahn (2010c) did an in-
trinsic evaluation of the SSC where they created an
4http://jura.wi.mit.edu/entrez gene/
5http://www.uniprot.org/
6See proceedings of the 1st CALBC Work-
shop, 2010, Editors: Dietrich Rebholz-Schuhmann
and Udo Hahn (http://www.ebi.ac.uk/Rebholz-
srv/CALBC/docs/FirstProceedings.pdf) for details.
SSC and a GSC on a dataset of 3,236 Medline7 ab-
stracts. They were not able to make any specific con-
clusion whether the SSC is approaching to the GSC.
They were of the opinion that SSC annotations are
more similar to terminological resources.
Hahn et al (2010) proposed a policy where sil-
ver standards can be dynamically optimized and cus-
tomized on demand (given a specific goal function)
using a gold standard as an oracle. The gold stan-
dard is used for optimization only, not for training
for the purpose of SSC annotation. They argued that
the nature of diverging tasks to be solved, the lev-
els of specificity to be reached, the sort of guide-
lines being preferred, etc should allow prospective
users of an SSC to customize one on their own and
not stick to something that is already prefabricated
without concrete application in mind.
Self-training and co-training are two of the exist-
ing approaches that have been used for compensat-
ing the lack of a training GSC with adequate size
in several different tasks such as word sense disam-
biguation, semantic role labelling, parsing, etc (Ng
and Cardie, 2003; Pierce and Cardie, 2004; Mc-
Closky et al, 2006; He and Gildea, 2006). Accord-
ing to Ng and Cardie (2003), self-training is the pro-
cedure where a committee of classifiers are trained
on the (gold) annotated examples to tag unannotated
examples independently. Only those new annota-
tions to which all the classifiers agree are added to
the training set and classifiers are retrained. This
procedure repeats until a stop condition is met. Ac-
cording to Clark et al (2003), self-training is a pro-
cedure in which ?a tagger is retrained on its own la-
beled cache at each round?. In other words, a sin-
gle classifier is trained on the initially (gold) anno-
tated data and then applied on a set of unannotated
data. Those examples meeting a selection criterion
are added to the annotated dataset and the classifier
is retrained on this new data set. This procedure can
continue for several rounds as required.
Co-training is another weakly supervised ap-
proach (Blum and Mitchell, 1998). It applies for
those tasks where each of the two (or more) sets of
features from the initially (gold) annotated training
data is sufficient to classify/annotate the unannotated
data (Pierce and Cardie, 2001; Pierce and Cardie,
7http://www.nlm.nih.gov/databases/databases medline.html
103
2004; He and Gildea, 2006). As with SSC annota-
tion and self-training, it also attempts to increase the
amount of annotated data by making use of unanno-
tated data. The main idea of co-training is to repre-
sent the initially annotated data using two (or more)
separate feature sets, each called a ?view?. Then,
two (or more) classifiers are trained on those views
of the data which are then used to tag new unanno-
tated data. From this newly annotated data, the most
confident predictions are added to the previously an-
notated data. This whole process may continue for
several iterations. It should be noted that, by limit-
ing the number of views to one, co-training becomes
self-training.
Like the SSC, the multiple classifier approach
of self-training and co-training, as described above,
adopts the same vision of utilizing automatic sys-
tems for producing the annotation. Apart from that,
SSC annotation is completely different from both
self-training and co-training. For example, classi-
fiers in self-training and co-training utilizes the same
(manually annotated) resource for their initial train-
ing. But SSC annotation systems do not necessar-
ily use the same resource. Both self-training and
co-training are weakly supervised approaches where
the classifiers are based on supervised ML tech-
niques. In the case of SSC annotation, the annota-
tion systems can be dictionary based or rule based.
This attractive flexibility allows SSC annotation to
be a completely unsupervised approach since the
annotation systems do not necessarily need to be
trained.
3 Experimental settings
We use the BioCreAtIvE II GM corpus (henceforth,
only the GSC) for evaluation of an SSC. The training
corpus in the GSC has in total 18,265 gene annota-
tions in 15,000 sentences. The GSC test data has
6,331 annotations in 5,000 sentences.
Some of the CALBC challenge participants have
used the BioCreAtIvE II GM corpus for training to
annotate gene/protein in the CALBC SSC-II corpus.
We wanted our benchmark corpus and benchmark
corpus annotation to be totally unseen by the sys-
tems that annotated the SSC to be used in our experi-
ments so that there is no bias in our empirical results.
SSC-I satisfies this criteria. So, we use the SSC-I
(henceforth, we would refer the CALBC SSC-I as
simply the SSC) in our experiments despite the fact
that it is almost 3 times smaller than the SSC-II.
The SSC has in total 137,610 gene annotations in
316,869 sentences of 50,000 abstracts.
Generally, using a customized dictionary of en-
tity names along with annotated corpus boosts NER
performance. However, since our objective is to ob-
serve to what extent a ML system can learn from
SSC, we avoid the use of any dictionary. We use
an open source ML based BNER system named
BioEnEx8 (Chowdhury and Lavelli, 2010). The
system uses conditional random fields (CRFs), and
achieves comparable results (F1 score of 86.22% on
the BioCreAtIvE II GM test corpus) to that of the
other state-of-the-art systems without using any dic-
tionary or lexicon.
One of the complex issues in NER is to come to an
agreement regarding the boundaries of entity men-
tions. Different annotation guidelines have different
preferences. There may be tasks where a longer en-
tity mention such as ?human IL-7 protein? may be
appropriate, while for another task a short one such
as ?IL-7? is adequate (Hahn et al, 2010).
However, usually evaluation on BNER corpora
(e.g., the BioCreAtIvE II GM corpus) is performed
adopting exact boundary match. Given that we have
used the official evaluation script of the BioCre-
AtIvE II GM corpus, we have been forced to
adopt exact boundary match. Considering a relaxed
boundary matching (i.e. the annotations might dif-
fer in uninformative terms such as the, a, acute, etc.)
rather than exact boundary matching might provide
a slightly different picture of the effectiveness of the
SSC usage.
4 Results and analyses
4.1 Automatically improving SSC quality
The CALBC SSC-I corpus has a negligible num-
ber of overlapping gene annotations (in fact, only 6).
For those overlapping annotations, we kept only the
longest ones. Our hypothesis is that a certain token
in the same context can refer to (or be part of) only
one concept name (i.e. annotation) of a certain se-
mantic group (i.e. entity type). After removing these
few overlaps, the SSC has 137,604 annotations. We
8Freely available at http://hlt.fbk.eu/en/people/chowdhury/research
104
will refer to this version of the SSC as the initial
SSC (ISSC).
We construct a list9 using the lemmatized form
of 132 frequently used words that appear in gene
names. These words cannot constitute a gene name
themselves. If (the lemmatized form of) all the
words in a gene name belong to this list then that
gene annotation should be discarded. We use this list
to remove erroneous annotations in the ISSC. After
this purification step, the total number of annotations
is reduced to 133,707. We would refer to this version
as the filtered SSC (FSSC).
Then, we use the post-processing module of
BioEnEx, first to further filter out possible wrong
gene annotations in the FSSC and then to automati-
cally include potential gene mentions which are not
annotated. It has been observed that some of the
annotated mentions in the SSC-I span only part of
the corresponding token10. For example, in the to-
ken ?IL-2R?, only ?IL-? is annotated. We extend
the post-processing module of BioEnEx to automat-
ically identify all such types of annotations and ex-
pand their boundaries when their neighbouring char-
acters are alphanumeric.
Following that, the extended post-processing
module of BioEnEx is used to check in every sen-
tence whether there exist any potential unannotated
mentions11 which differ from any of the annotated
mentions (in the same sentence) by a single charac-
ter (e.g. ?IL-2L? and ?IL-2R?), number (e.g. ?IL-
2R? and ?IL-345R?) or Greek letter (e.g. ?IFN-
alpha? and ?IFN-beta?). After this step, the total
number of gene annotations is 144,375. This means
that we were able to remove/correct some specific
types of errors and then further expand the total
number of annotations (by including entities not an-
notated in the original SSC) up to 4.92% with re-
spect to the ISSC. We will refer to this expanded
version of the SSC as the processed SSC (PSSC).
When BioEnEx is trained on the above versions
9The words are collected from
http://pir.georgetown.edu/pirwww/iprolink/general name
and the annotation guideline of GENETAG (Tanabe et al,
2005).
10By token we mean a sequence of consecutive non-
whitespace characters.
11Any token or sequence of tokens is considered to verify
whether it should be annotated or not, if its length is more than
2 characters excluding digits and Greek letters.
TP FP FN P R F1
ISSC 2,396 594 3,935 80.13 37.85 51.41
FSSC 2,518 557 3,813 81.89 39.77 53.54
PSSC 2,606 631 3,725 80.51 41.16 54.47
Table 1: The results of experiments when trained with
different versions of the SSC and tested on the GSC test
data.
of the SSC and tested on the GSC test data, we ob-
served an increase of more than 3% of F1 score be-
cause of the filtering and expansion (see Table 1).
One noticeable characteristic in the results is that the
number of annotations obtained (i.e. TP+FP12) by
training on any of the versions of the SSC is almost
half of the actual number annotations of the GSC test
data. This has resulted in a low recall. There could
be mainly two reasons behind this outcome:
? First of all, it might be the case that a consid-
erable number of gene names are not annotated
inside the SSC versions. As a result, the fea-
tures shared by the annotated gene names (i.e.
TP) and unnannotated gene names (i.e. FN)
might not have enough influence.
? There might be a considerable number of
wrong annotations which are actually not genes
(i.e. FP). Consequently, a number of bad fea-
tures might be collected from those wrong an-
notations which are misleading the training
process.
To verify the above conditions, it would be re-
quired to annotate the huge CALBC SSC manually.
This would be not feasible because of the cost of
human labour and time. Nevertheless, we can try to
measure the state of the above conditions roughly by
using only annotated sentences (i.e. sentences con-
taining at least one annotation) and varying the size
of the corpus, which are the subjects of our next ex-
periments.
12TP (true positive) = corresponding annotation done by the
system is correct, FP (false positive) = corresponding anno-
tation done by the system is incorrect, FN (false negative) =
corresponding annotation is correct but it is not annotated by
the system.
105
Figure 1: Graphical representation of the experimental
results with varying size of the CSSC.
4.2 Impact of annotated sentences and
different sizes of the SSC
We observe that only 77,117 out of the 316,869
sentences in the PSSC contain gene annotations.
We will refer to the sentences having at least one
gene annotation collectively as the condensed SSC
(CSSC). Table 2 and Figure 1 show the results when
we used different portions of the CSSC for training.
There are four immediate observations on the
above results:
? Using the full PSSC, we obtain total (i.e.
TP+FP) 3,237 annotations on the GSC test
data. But when we use only annotated sen-
tences of the PSSC (i.e. the CSSC), the total
number of annotations is 4,562, i.e. there is an
increment of 40.93%.
? Although we have a boost in F1 score due to the
increase in recall using the CSSC in place of the
PSSC, there is a considerable drop in precision.
? The number of FP is almost the same for the
usage of 10-75% of the CSSC.
? The number of FN kept decreasing (and TP
kept increasing) for 10-75% of the CSSC.
These observations can be interpreted as follows:
? Unannotated sentences inside the SSC in real-
ity contain many gene annotations; so the in-
clusion of such sentences misleads the training
process of the ML system.
? Some of the unannotated sentences actually
do not contain any gene names, while others
would contain such names but the automatic
annotations missed them. As a consequence,
the former sentences contain true negative ex-
amples which could provide useful features that
can be exploited during training so that less FPs
are produced (with a precision drop using the
CSSC). So, instead of simply discarding all the
unannotated sentences, we could adopt a filter-
ing strategy that tries to distinguish between the
two classes of sentences above.
? The experimental results with the increasing
size of the CSSC show a decrease in both pre-
cision (74.55 vs 76.17) and recall (53.72 vs
54.04). We plan to run again these experiments
with different randomized splits to better assess
the performance.
? Even using only 10% of the whole CSSC does
not produce a drastic difference with the results
when the full CSSC is used. This indicates that
perhaps the more CSSC data is fed, the more
the system tends to overfit.
? It is evident that the more the size of the CSSC
increases, the lower the improvement of F1
score, if the total number of annotations in
the newly added sentences and the accuracy of
the annotations are not considerably higher. It
might be not surprising if, after the addition of
more sentences in the CSSC, the F1 score drops
further rather than increasing. The assumption
that having a huge SSC would be beneficiary
might not be completely correct. There might
be some optimal limit of the SSC (depending
on the task) that can provide maximum bene-
fits.
4.3 Training with the GSC and the SSC
together
Our final experiments were focused on whether it is
possible to improve performance by simply merg-
ing the GSC training data with the PSSC and the
CSSC. The PSSC has almost 24 times the num-
ber of sentences and almost 8 times the number of
gene annotations than the GSC. There is a possibility
that, when we do a simple merge, the weight of the
106
Total tokens in the corpus No of annotated genes TP FP FN P R F1
PSSC 6,955,662 144,375 2,606 631 3,725 80.51 41.16 54.47
100% of CSSC 1,983,113 144,375 3,401 1,161 2,930 74.55 53.72 62.44
75% of CSSC 1,487,823 108,213 3,421 1,070 2,910 76.17 54.04 63.22
50% of CSSC 992,392 72,316 3,265 1,095 3,066 74.89 51.57 61.08
25% of CSSC 494,249 35,984 3,179 1,048 3,152 75.21 50.21 60.22
10% of CSSC 196,522 14,189 2,988 1,097 3,343 73.15 47.20 57.37
Table 2: The results of SSC experiments with varying size of the CSSC = condensed SSC (i.e. sentences containing
at least one annotation). SSC size = 316,869 sentences. CSSC size = 77,117.
TP FP FN P R F1
GSC 5,373 759 958 87.62 84.87 86.22
PSSC +
GSC 3,745 634 2,586 85.52 59.15 69.93
PSSC +
GSC * 8 4,163 606 2,168 87.29 65.76 75.01
CSSC +
GSC * 8 4,507 814 1,824 84.70 71.19 77.36
Table 3: The results of experiments by training on the
GSC training data merged with the PSSC and the CSSC.
gold annotations would be underestimated. So, apart
from doing a simple merge, we also try to balance
the annotations of the two corpora. There are two
options to do this ? (i) by duplicating the GSC train-
ing corpus 8 times to make its total number of anno-
tations equal to that of the PSSC, or (ii) by choos-
ing randomly a portion of the PSSC that would have
almost similar amount of annotations as that of the
GSC. We choose the 1st option.
Unfortunately, when an SSC (i.e. the PSSC or the
CSSC) is combined with the GSC, the result is far
below than that of using the GSC only (see Table 3).
Again, low recall is the main issue partly due to the
lower number of annotations (i.e. TP+FP) done by
the system trained on an SSC and the GSC instead of
the GSC only. As we know, a GSC is manually an-
notated following precise guidelines, while an SSC
is annotated with automatic systems that do not nec-
essarily follow the same guidelines as a GSC. So,
it would not have been surprising if the number of
annotations were high (since we have much bigger
training corpus due to SSC) but precision were low.
But in practice, precision obtained by combining an
SSC and the GSC is almost as high as the precision
achieved using the GSC.
One reason for the lower number of annotations
might be the errors that have been propagated in-
side the SSC. Some of the systems that have been
used for the annotation of the SSC might have low
recall. As a result, during harmonization of their an-
notations several valid gene mentions might not have
been included13.
One other possible reason could be the difference
in the entity name boundaries in the GSC and an
SSC. We have checked some of the SSC annotations
randomly. It appears that in those annotated entity
names some relevant (neighbouring) words (in the
corresponding sentences) are not included. It is most
likely that the SSC annotation systems had disagree-
ments on those words.
When the annotations of the GSC were given
higher preference (by duplicating), there is a sub-
stantial improvement in the F1 score, although still
lower than the result with the GSC only.
5 Conclusions
The idea of SSC development is simple and yet at-
tractive. Obtaining better results on a test dataset
by combining output of multiple (accurate and di-
verse14) systems is not new (Torii et al, 2009; Smith
et al, 2008). But adopting this strategy for cor-
13There can be two reasons for this ? (i) when a certain valid
gene name is not annotated by any of the annotation systems,
and (ii) when only a few of those systems have annotated the
valid name but the total number of such systems is below than
the minimum required number of agreements, and hence the
gene name is not considered as an SSC annotation.
14A system is said to be accurate if its classification perfor-
mance is better than a random classification. Two systems are
considered diverse if they do not make the same classification
mistakes. (Torii et al, 2009)
107
pus development is a novel and unconventional ap-
proach. Some natural language processing tasks (es-
pecially the new ones) lack adequate GSCs to be
used for the training of ML based systems. For such
tasks, domain experts can provide patterns or rules
to build systems that can be used to annotate an ini-
tial version of SSC. Such systems might lack high
recall but are expected to have high precision. Al-
ready available task specific lexicons or dictionaries
can also be utilized for SSC annotation. Such an
initial version of SSC can be later enriched using
automatic process which would utilize existing an-
notations in the SSC.
With this vision in mind, we pose ourselves sev-
eral questions (see Section 1) regarding the practi-
cal usability and exploitation of an SSC. Our experi-
ments are conducted on a publicly available biomed-
ical SSC developed for the training of biomedical
NER systems. For the evaluation of a state-of-the-
art ML system trained on such an SSC, we use a
widely used benchmark biomedical GSC.
In the search of answers for our questions, we ac-
cumulate several important empirical observations.
We have been able to automatically reduce the num-
ber of erroneous annotations from the SSC and in-
clude unannotated potential entity mentions simply
using the annotations that the SSC already provides.
Our techniques have been effective for improving
the annotation quality as there is a considerable in-
crement of F1 score (almost 11% higher when we
use CSSC instead of using ISSC; see Table 1 and 2).
We also observe that it is possible to obtain more
than 80% of precision using the SSC. But recall re-
mains quite low, partly due to the low number of
annotations provided by the system trained with the
SSC. Perhaps, the entity names in the SSC that are
missed by the annotation systems is one of the rea-
sons for that.
Perhaps, the most interesting outcome of this
study is that, if only annotated sentences (which
we call condensed corpus) are considered, then the
number of annotations as well as the performance
increases significantly. This indicates that many
unannotated sentences contain annotations missed
by the automatic annotation systems. However, it
appears that correctly unannotated sentences influ-
ence the achievement of high precision. Maybe a
more sophisticated approach should be adopted in-
stead of completely discarding the unannotated sen-
tences, e.g. devising a filter able to distinguish
between relevant unannotated sentences (i.e., those
that should contain annotations) from non-relevant
ones (i.e., those that correctly do not contain any an-
notation). Measuring lexical similarity between an-
notated and unannotated sentences might help in this
case.
We notice the size of an SSC affects performance,
but increasing it above a certain limit does not
always guarantee an improvement of performance
(see Figure 1). This rejects the hypothesis that hav-
ing a much larger SSC should allow an ML based
system to ameliorate the effect of having erroneous
annotations inside the SSC.
Our empirical results show that combining GSC
and SSC do not improve results for the particular
task of NER, even if GSC annotations are given
higher weights (through duplication). We assume
that this is partly due to the variations in the guide-
lines of entity name boundaries15. These impact the
learning of the ML algorithm. For other NLP tasks
where the possible outcome is boolean (e.g. relation
extraction, i.e. whether a particular relation holds
between two entities or not), we speculate the results
of such combination might be better.
We use a CRF based ML system for our exper-
iments. It would be interesting to see whether the
observations are similar if a system with a different
ML algorithm is used.
To conclude, this study suggests that an automat-
ically pre-processed SSC might already contain an-
notations with reasonable quality and quantity, since
using it we are able to reach more than 62% of F1
score. This is encouraging since in the absence of
a GSC, an ML system would be able to exploit an
SSC to annotate unseen text with a moderate (if not
high) accuracy. Hence, SSC development might be
a good option to semi-automate the annotation of a
GSC.
Acknowledgments
This work was carried out in the context of the project
?eOnco - Pervasive knowledge and data management in
cancer care?. The authors would like to thank Pierre
Zweigenbaum for useful discussion, and the anonymous
reviewers for valuable feedback.
15For example, ?human IL-7 protein? vs ?IL-7?.
108
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
learning theory (COLT?98), pages 92?100.
Md. Faisal Mahbub Chowdhury and Alberto Lavelli.
2010. Disease mention recognition with specific fea-
tures. In Proceedings of the Workshop on Biomedical
Natural Language Processing (BioNLP 2010), 48th
Annual Meeting of the Association for Computational
Linguistics, pages 83?90, Uppsala, Sweden, July.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the 7th Conference on Natural
Language Learning (CoNLL-2003), pages 49?55.
Udo Hahn, Katrin Tomanek, Elena Beisswanger, and Erik
Faessler. 2010. A proposal for a configurable silver
standard. In Proceedings of the 4th Linguistic Anno-
tation Workshop, 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 235?242,
Uppsala, Sweden, July.
Shan He and Daniel Gildea. 2006. Self-training and
co-training for semantic role labeling: Primary report.
Technical report, University of Rochester.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. Genia corpus - semantically annotated
corpus for bio-textmining. Bioinformatics, 19(Suppl
1):i180?182.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the 21st International Con-
ference on Computational Linguistics, pages 337?344,
Sydney, Australia.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views.
In Proceedings of the 2003 Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL-2003), pages 173?180.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2001), pages 1?9.
David Pierce and Claire Cardie. 2004. Co-training and
self-training for word sense disambiguation. In Pro-
ceedings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL-2004), pages 33?40.
Dietrich Rebholz-Schuhmann and Udo Hahn. 2010c.
Silver standard corpus vs. gold standard corpus. In
Proceedings of the 1st CALBC Workshop, Cambridge,
U.K., June.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen Li,
Senay Kafkas, Ian Lewin, Ning Kang, Peter Corbett,
David Milward, Ekaterina Buyko, Elena Beisswanger,
Kerstin Hornbostel, Alexandre Kouznetsov, Rene
Witte, Jonas B Laurila, Christopher JO Baker, Chen-Ju
Kuo, Simon Clematide, Fabio Rinaldi, Richrd Farkas,
Gyrgy Mra, Kazuo Hara, Laura Furlong, Michael
Rautschka, Mariana Lara Neves, Alberto Pascual-
Montano, Qi Wei, Nigel Collier, Md. Faisal Mah-
bub Chowdhury, Alberto Lavelli, Rafael Berlanga,
Roser Morante, Vincent Van Asch, Walter Daele-
mans, Jose? Lu??s Marina, Erik van Mulligen, Jan Kors,
and Udo Hahn. 2010. Assessment of NER solu-
tions against the first and second CALBC silver stan-
dard corpus. In Proceedings of the fourth Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?2010), October.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno-
Yepes, Erik van Mulligen, Ning Kang, Jan Kors, David
Milward, Peter Corbett, Ekaterina Buyko, Elena Beis-
swanger, and Udo Hahn. 2010a. CALBC silver stan-
dard corpus. Journal of Bioinformatics and Computa-
tional Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno-
Yepes, Erik van Mulligen, Ning Kang, Jan Kors, David
Milward, Peter Corbett, Ekaterina Buyko, Katrin
Tomanek, Elena Beisswanger, and Udo Hahn. 2010b.
The CALBC silver standard corpus for biomedical
named entities ? a study in harmonizing the contri-
butions from four independent named entity taggers.
In Proceedings of the 7th International conference on
Language Resources and Evaluation (LREC?10), Val-
letta, Malta, May.
Larry Smith, Lorraine Tanabe, Rie Ando, Cheng-
Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-Shi
Lin, Roman Klinger, Christoph Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Lawrence Hunter, Bob
Carpenter, Richard Tsai, Hong-Jie Dai, Feng Liu,
Yifei Chen, Chengjie Sun, Sophia Katrenko, Pieter
Adriaans, Christian Blaschke, Rafael Torres, Mariana
Neves, Preslav Nakov, Anna Divoli, Manuel Mana-
Lopez, Jacinto Mata, and W John Wilbur. 2008.
Overview of BioCreAtIvE II gene mention recogni-
tion. Genome Biology, 9(Suppl 2):S2.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and W John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl 1):S3.
Manabu Torii, Zhangzhi Hu, Cathy H Wu, and Hong-
fang Liu. 2009. Biotagger-GM: a gene/protein name
recognition system. Journal of the American Medical
Informatics Association : JAMIA, 16:247?255.
109
