Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 33?40
Manchester, August 2008
Improving Alignments for Better Confusion Networks
for Combining Machine Translation Systems
Necip Fazil Ayan and Jing Zheng and Wen Wang
SRI International
Speech Technology and Research Laboratory (STAR)
333 Ravenswood Avenue
Menlo Park, CA 94025
{nfa,zj,wwang}@speech.sri.com
Abstract
The state-of-the-art system combination
method for machine translation (MT) is
the word-based combination using confu-
sion networks. One of the crucial steps in
confusion network decoding is the align-
ment of different hypotheses to each other
when building a network. In this paper, we
present new methods to improve alignment
of hypotheses using word synonyms and a
two-pass alignment strategy. We demon-
strate that combination with the new align-
ment technique yields up to 2.9 BLEU
point improvement over the best input sys-
tem and up to 1.3 BLEU point improve-
ment over a state-of-the-art combination
method on two different language pairs.
1 Introduction
Combining outputs of multiple systems perform-
ing the same task has been widely explored in
various fields such as speech recognition, word
sense disambiguation, and word alignments, and it
had been shown that the combination approaches
yielded significantly better outputs than the in-
dividual systems. System combination has also
been explored in the MT field, especially with
the emergence of various structurally different MT
systems. Various techniques include hypothesis
selection from different systems using sentence-
level scores, re-decoding source sentences using
phrases that are used by individual systems (Rosti
et al, 2007a; Huang and Papineni, 2007) and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
word-based combination techniques using confu-
sion networks (Matusov et al, 2006; Sim et al,
2007; Rosti et al, 2007b). Among these, confu-
sion network decoding of the system outputs has
been shown to be more effective than the others in
terms of the overall translation quality.
One of the crucial steps in confusion network
decoding is the alignment of hypotheses to each
other because the same meaning can be expressed
with synonymous words and/or with a different
word ordering in different hypotheses. Unfortu-
nately, all the alignment algorithms used in confu-
sion network decoding are insensitive to synonyms
of words when aligning two hypotheses to each
other. This paper extends the previous alignment
approaches to handle word synonyms more effec-
tively to improve alignment of different hypothe-
ses. We also present a two-pass alignment strategy
for a better alignment of hypotheses with similar
words but with a different word ordering.
We evaluate our system combination approach
using variants of an in-house hierarchical MT sys-
tem as input systems on two different language
pairs: Arabic-English and Chinese-English. Even
with very similar MT systems as inputs, we show
that the improved alignments yield up to an abso-
lute 2.9 BLEU point improvement over the best
input system and up to an absolute 1.3 BLEU
point improvement over the old alignments in a
confusion-network-based combination.
The rest of this paper is organized as follows.
Section 2 presents an overview of previous sys-
tem combination techniques for MT. Section 3 dis-
cusses the confusion-network-based system com-
bination. In Section 4, we present the new hy-
pothesis alignment techniques. Finally, Section 5
presents our experiments and results on two lan-
guage pairs.
33
2 Related Work
System combination for machine translation can
be done at three levels: Sentence-level, phrase-
level or word-level.
Sentence-level combination is done by choosing
one hypothesis amongmultipleMT system outputs
(and possibly among n-best lists). The selection
criterion can be a combination of translation model
and language model scores with multiple compar-
ison tests (Akiba et al, 2002), or statistical confi-
dence models (Nomoto, 2004).
Phrase-level combination systems assume that
the input systems provide some internal informa-
tion about the system, such as phrases used by the
system, and the task is to re-decode the source sen-
tence using this additional information. The first
example of this approach was the multi-engine MT
system (Frederking and Nirenburg, 1994), which
builds a chart using the translation units inside
each input system and then uses a chart walk algo-
rithm to find the best cover of the source sentence.
Rosti et al (2007a) collect source-to-target corre-
spondences from the input systems, create a new
translation option table using only these phrases,
and re-decode the source sentence to generate bet-
ter translations. In a similar work, it has been
demonstrated that pruning the original phrase ta-
ble according to reliable MT hypotheses and en-
forcing the decoder to obey the word orderings in
the original system outputs improves the perfor-
mance of the phrase-based combination systems
(Huang and Papineni, 2007). In the absence of
source-to-target phrase alignments, the sentences
can be split into simple chunks using a recursive
decomposition as input to MT systems (Mellebeek
et al, 2006). With this approach, the final output
is a combination of the best chunk translations that
are selected by majority voting, system confidence
scores and language model scores.
The word-level combination chooses the best
translation units from different translations and
combine them. The most popular method for
word-based combination follows the idea behind
the ROVER approach for combining speech recog-
nition outputs (Fiscus, 1997). After reordering
hypotheses and aligning to each other, the com-
bination system builds a confusion network and
chooses the path with the highest score. The fol-
lowing section describes confusion-network-based
system combination in detail.
? 2005 SRI International
Confusion Network ExampleHypothesis 1: she went homeHypothesis 2: she was at schoolHypothesis 3: at home was sheshe school#eps# homeatwentwasshe homewas school#eps#atwent she#eps#was#eps# wentat#eps# home #eps#school<s><s><s> </s></s></s>
Figure 1: Alignment of three hypotheses to each
other using different hypotheses as skeletons.
3 System Combination with Confusion
Networks
The general architecture of a confusion-network-
based system combination is as follows:
1. Extract n-best lists from MT systems.
2. Pick a skeleton translation for each segment.
3. Reorder all the other hypotheses by aligning
them to the skeleton translation.
4. Build a confusion network from the re-
ordered translations for each segment.
5. Decode the confusion network using vari-
ous arc features and sentence-level scores
such as LM score and word penalty.
6. Optimize feature weights on a held-out test
set and re-decode.
In this framework, the success of confusion net-
work decoding for system combination depends on
two important choices: Selection of the skeleton
hypothesis and alignment of other hypotheses to
the skeleton.
For selecting the best skeleton, two common
methods are choosing the hypothesis with the Min-
imum Bayes Risk with translation error rate (TER)
(Snover et al, 2006) (i.e., the hypothesis with the
minimum TER score when it is used as the ref-
erence against the other hypotheses) (Sim et al,
2007) or choosing the best hypotheses from each
system and using each of those as a skeleton in
multiple confusion networks (Rosti et al, 2007b).
In this paper, we use the latter since it performs
slightly better than the first method in our exper-
iments. An example confusion network on three
translations is presented in Figure 1.
1
The major difficulty when using confusion net-
works for system combination for MT is aligning
different hypotheses to the skeleton since the word
1
In this paper, we use multiple confusion networks that are
attached to the same start and end node. Throughout the rest
of the paper, the term confusion network refers to one network
among multiple networks used for system combination.
34
order might be different in different hypotheses
and it is hard to align words that are shifted from
one hypothesis to another. Four popular methods
to align hypotheses to each other are as follows:
1. Multiple string-matching algorithm based
on Levenshtein edit distance (Bangalore et
al., 2001)
2. A heuristic-based matching algorithm (Ja-
yaraman and Lavie, 2005)
3. Using GIZA++ (Och and Ney, 2000) with
possibly additional training data (Matusov
et al, 2006)
4. Using TER (Snover et al, 2006) between
the skeleton and a given hypothesis (Sim et
al., 2007; Rosti et al, 2007b)
None of these methods takes word synonyms
into account during alignment of hypotheses.
2
In
this work, we extend the TER-based alignment
to use word stems and synonyms using the pub-
licly available WordNet resource (Fellbaum, 1998)
when aligning hypotheses to each other and show
that this additional information improves the align-
ment and the overall translation significantly.
4 Confusion Networks with Word
Synonyms and Two-pass Alignment
When building a confusion network, the goal is to
put the same words on the same arcs as much as
possible. Matching similar words between two hy-
potheses is necessary to achieve this goal.
When we align two different hypotheses using
TER, it is necessary that two words have the iden-
tical spelling to be considered a match. However,
in natural languages, it is possible to represent the
same meaning using synonyms of words in pos-
sibly different positions. For example, in the fol-
lowing sentences, ?at the same time? and ?in the
meantime?, ?waiting for? and ?expect?, and ?set?
and ?established? correspond to each other, re-
spectively:
Skeleton: at the same time expect israel
to abide by the deadlines set by .
Hypothesis: in the meantime , we are
waiting for israel to abide by the
established deadlines .
Using TER, synonymous words might be
aligned to each other if they appear in the same po-
2
Note that the approach by Matusov et al (2006) at-
tempts to align synonyms and different morphological forms
of words to each other but this is done implicitly, relying on
the parallel text to learn word alignments.
sition in two hypotheses but this is less likely when
two words appear in different positions. With-
out knowing that two words are synonyms of each
other, they are considered two separate words dur-
ing TER alignment.
Our goal is to create equivalence classes for
each word in the given translations and modify the
alignment algorithm to give priority to the match-
ing of words that are in the same equivalence class.
In this paper, the equivalence classes are generated
using WordNet by extracting synonyms of each
word in the translations.
To incorporate matching of word synonyms into
the alignment, we followed three steps:
1. Use WordNet to extract synonyms of the
words that appear in all hypotheses.
2. Augment each skeleton word with all syn-
onymous words that appear in all the hy-
potheses.
3. Modify TER script to handle words with
alternatives using an additional synonym
matching operation.
In the following subsections, we describe how
each of these tasks is performed.
4.1 Extracting Synonyms from WordNet
The first step is to use WordNet to extract syn-
onyms of each word that appears in all hypotheses.
This is simply done using the publicly available
WordNet processing tools to extract all synonyms
of the given word. To allow matching words that
have the same stem or variations of the same word
with different part-of-the-speech (POS) tags, we
extract all synonyms of the given word regardless
of their POS tag in the given translation.
3
In the example above, it is clear that the verbs
wait and expect have the same meaning but TER
is unable to align these two words to each other
because of different word positions. Using Word-
Net, we get the following two entries for the words
waiting and expect, indicating that two words can
be considered as synonyms of each other:
expect: Sense 1: expect, anticipate ? evalu-
ate, pass judgment, judge. Sense 2: ask, require,
expect ? demand. Sense 3: expect, look, await,
wait. Sense 4: expect ? see, consider, reckon,
3
Our goal is to add as many synonyms as possible to in-
crease the chances of a word aligning to one of its possible
synonyms rather than to any other word. Therefore, we do
not distinguish between the synonyms of the same word ac-
cording to their confidence value or their POS tag.
35
ONE-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives the head of the italian chamber of deputies
khaddam will meet with the president of the italian chamber of deputies
TWO-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives of the head italian chamber of the deputies
khaddam will meet with president of the italian chamber of the deputies
Figure 2: Example alignment of three hypotheses with one-pass and two-pass alignment
view, regard. Sense 5: expect ? expect, look,
await, wait. Sense 6: have a bun in the oven, bear,
carry, gestate, expect ? give birth, deliver, bear,
birth, have.
waiting: Sense 1: wait. Sense 2: wait, hold
off, hold back ? act, move. Sense 3: wait, expect,
look, await. Sense 4: wait, waitress ? work.
We should note that WordNet contains only
open-class words, i.e., nouns, verbs, adjectives,
and adverbs. There are no entries for determin-
ers, prepositions, pronouns, conjunctions, and par-
ticles. For better matching of these additional POS
tags, we manually created a different equivalence
class for each POS tag that is not included in the
WordNet so that words with the same POS tag can
be considered synonymous.
4.2 Augmenting References
After extracting the synonyms of each word in the
given translations, the next step is to augment each
reference word with its synonyms. To avoid over-
generation of synonyms, we make the assumption
that words w
i
and w
j
are synonyms of each other
only if w
i
appears in the synonym list of w
j
, and
w
j
appears in the synonym list of w
i
. To make
the alignment task more efficient and faster, we re-
strict the synonym list to only words that appear
in the given translations. In our running exam-
ple, the augmented (extended) skeleton according
to the second hypothesis is as follows:
Extended skeleton: at the same time meantime
expect waiting israel to abide by the
deadlines set established by .
4.3 Modifications to TER Script
The final step is to modify TER script to favor
matching of a word to its synonyms rather than to
any other word. To achieve this goal, we modi-
fied the publicly available TER script, TERCOM
(Snover et al, 2006), to match words in the same
equivalence class at an additional synonym cost.
In its original implementation, TERCOM builds a
hash table for the n-grams that appear in both the
reference and the hypothesis translation to deter-
mine possible shifts of words. To allow synony-
mous words to be shifted and aligned to each other,
we extend the hash table for all possible synonyms
of words in the skeleton. Formally, if the skeleton
includes two consecutive words w
i
s
i
and w
j
s
j
,
where s
i
(s
j
) is a synonym of w
i
(w
j
), we put
all four possible combinations to the hash table:
w
i
w
j
, w
i
s
j
, s
i
w
j
, and s
i
s
j
.
4
To give higher priority to the exact matching
of words (which has zero cost during edit dis-
tance computation), we used a slightly higher cost
for synonym matching, a cost of 0.1.
5
All the
other operations (i.e., insertion, deletion, substitu-
tion and shifting of words) have a cost of 1.0.
4.4 Two-pass Alignment Strategy
When building a confusion network, the usual
strategy is first to align each hypothesis to the
skeleton separately and reorder them so that the
word ordering in the given hypothesis matches the
word ordering in the skeleton translation. Next a
confusion network is built between all these re-
ordered hypotheses.
One of the major problems with this process oc-
curs when the hypotheses include additional words
that do not appear in the skeleton translation, as
depicted in Figure 2. Since the alignments of two
different hypotheses are done independently, two
hypotheses other than the skeleton may not align
perfectly, especially when the additional words ap-
pear in different positions.
To overcome this issue, we employ a two-pass
alignment strategy. In the first pass, we align all
hypotheses to the skeleton independently and build
a confusion network. Next an intermediate refer-
ence sentence is created from the confusion net-
work generated in the first pass. To create this in-
termediate reference, we find the best position for
each word that appears in the confusion network
4
Note that the hash table is built in an iterative fashion.
We consider adding a new n-gram only if the previous n? 1
words appear in the hypothesis as well.
5
Synonym matching cost was determined empirically, try-
ing different costs from 0 to 0.5.
36
WITHOUT SYNONYM MATCHING and ONE-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
WITH SYNONYM MATCHING and TWO-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
Figure 3: Example alignment via confusion networks with and without synonym matching and two-pass
alignment (using the first sentence as the skeleton)
using majority voting. The second pass uses this
intermediate reference as the skeleton translation
to generate the final confusion network.
When we create the intermediate reference, the
number of positions for a given word is bounded
by the maximum number of occurrences of the
same word in any hypothesis. It is possible that
two different words are mapped to the same po-
sition in the intermediate reference. If this is the
case, these words are treated as synonyms when
building the second confusion network, and the in-
termediate reference looks like the extended refer-
ence in Section 4.2.
Finally, Figure 3 presents our running example
with the old alignments versus the alignments with
synonym matching and two-pass alignment.
4.5 Features
Each word in the confusion network is represented
by system-specific word scores. For computing
scores, each hypothesis is assigned a score based
on three different methods:
1. Uniform weighting: Each hypothesis in the
n-best list has the same score of 1/n.
2. Rank-based weighting: Each hypothesis is
assigned a score of 1/(1+r), where r is the
rank of the hypothesis.
3. TM-based weighting: Each hypothesis is
weighted by the score that is assigned to the
hypothesis by the translation model.
The total score of an arc with word w for a given
system S is the sum of all the scores of the hy-
potheses in system S that contain the word w in
the given position. The score for a specific arc be-
tween nodes n
i
and n
j
is normalized by the sum of
the scores for all the arcs between n
i
and n
j
.
Our experiments demonstrated that rank-based
weighting performs the best among all three
weighting methods although the differences are
small. In the rest of the paper, we report only the
results with rank-based weighting.
Besides the arc scores, we employ the following
features during decoding:
1. Skeleton selection features for each system,
2. NULL-word (or epsilon) insertion score,
3. Word penalty, and
4. Language model score.
Skeleton selection feature is intended to help
choose the best skeleton among the input systems.
NULL-word feature controls the number of ep-
silon arcs used in the chosen translation during
the decoding and word penalty feature controls
the length of the translation. For language model
scores, we used a 4-gram LM that we used to train
the input systems.
5 Evaluation and Results
In this section, we describe how we train the input
systems and how we evaluate the proposed system
combination method.
5.1 Systems and Data
To evaluate the impact of the new alignments,
we tested our system combination approach using
the old alignments and improved alignments on
two language pairs: Arabic-English and Chinese-
English. We ran the system combination on three
system outputs that were generated by an in-house
hierarchical phrase-based decoder, as in (Chiang,
2007). The major difference between the three sys-
tems is that they were trained on different subsets
of the available training data using different word
alignments.
For generating the system outputs, first a hier-
archical phrase-based decoder was used to gener-
37
Data for Training/Tuning/Testing
Arabic-English Chinese-English
# of segments # of tokens # of segments # of tokens
Training Data (System1) 14.8M 170M 9.1M 207M
Training Data (System2) 618K 8.1M 13.4M 199M
Training Data (System3) 2.4M 27.5M 13.9M 208M
Tuning Set (Input Systems) 1800 51K 1800 51K
Tuning Set (System Combination) 1259 37K 1785 55K
Test Set - NIST MTEval?05 1056 32K 1082 32K
Test Set - NIST MTEval?06 1797 45K 1664 41K
Test Set - NIST MTEval?08 1360 43K 1357 34K
Table 1: Number of segments and source-side words in the training and test data.
ate three sets of unique 3000-best lists. Nine fea-
tures were used in the hierarchical phrase-based
systems under the log-linear model framework: a
4-gram language model (LM) score (trained on
nearly 3.6 billion words using the SRILM toolkit
(Stolcke, 2002)), conditional rule/phrase probabil-
ities and lexical weights (in both directions), rule
penalty, phrase penalty, and word penalty. Rules
and phrases were extracted in a similar manner
as described in (Chiang, 2007) from the training
data with word alignments generated by GIZA++.
The n-best lists were then re-scored with three ad-
ditional LMs: a count-based LM built from the
Google Tera word corpus, an almost parsing LM
based on super-ARV tagging, and an approximated
full-parser LM (Wang et al, 2007).
For Arabic-English, the first system was trained
on all available training data (see Table 1 for de-
tails), with long sentences segmented into multiple
segments based on IBM model 1 probabilities (Xu
et al, 2005). The second system was trained on a
small subset of the training data, which is mostly
newswire. The third system was trained on an au-
tomatically extracted subset of the training data ac-
cording to n-gram overlap in the test sets.
For Chinese-English, the first system used all
the training data without any sentence segmenta-
tion. The second system used all training data af-
ter IBM-1 based sentence segmentation, with dif-
ferent weightings on different corpora. The third
system is the same as the second system except
that it used different word alignment symmetriza-
tion heuristics (grow-diag-final-and vs. grow-diag-
final (Koehn et al, 2003)).
5.2 Empirical Results
All input systems were optimized on a ran-
domly selected subset of the NIST MTEval?02,
MTEval?03, and MTEval?04 test sets using min-
System MT?05 MT?06 MT?08
System 1 53.4 43.8 43.2
System 2 53.9 46.0 42.8
System 3 56.1 45.3 43.3
No Syns, 1-pass 56.7 47.5 44.9
w/Syns, 2-pass 57.9 48.4 46.2
Table 2: Lowercase BLEU scores (in percentages)
on Arabic NIST MTEval test sets.
imum error rate training (MERT) (Och, 2003)
to maximize BLEU score (Papineni et al, 2002).
System combination was optimized on the rest of
this data using MERT to maximize BLEU score.
As inputs to the system combination, we used 10-
best hypotheses from each of the re-ranked n-best
lists. To optimize system combination, we gener-
ated unique 1000-best lists from a lattice we cre-
ated from the input hypotheses, and used MERT in
a similar way to MT system optimization.
We evaluated system combination with im-
proved alignments on three different NIST
MTEval test sets (MTEval?05, MTEval?06 NIST
portion, and MTEval?08). The final MT outputs
were evaluated using lowercased BLEU scores.
6
Tables 2 and 3 present the BLEU scores (in per-
centages) for the input systems and for different
combination strategies on three test sets in Arabic-
English and Chinese-English, respectively.
On Arabic-English, the combination with syn-
onym matching and two-pass alignment yields ab-
solute improvements of 1.8 to 2.9 BLEU point on
three test sets over the best input system. When
compared to the combination algorithm with the
old alignments (i.e., 1-pass alignment with no syn-
onymmatching), the improved alignments yield an
additional improvement of 0.9 to 1.3 BLEU point
6
We used the NIST script (version 11b) for BLEU with its
default settings: case-insensitive matching of up to 4-grams,
and the shortest reference sentence for the brevity penalty.
38
System MT?05 MT?06 MT?08
System 1 35.8 34.3 27.6
System 2 35.9 34.2 27.8
System 3 36.0 34.3 27.8
No Syns, 1-pass 38.1 36.5 27.9
w/Syns, 2-pass 38.6 37.0 28.3
No Syns, 1-pass, tuning set w/webtext 28.4
w/Syns, 2-pass, tuning set w/webtext 29.3
Table 3: Lowercase BLEU scores (in percentages)
on Chinese NIST MTEval test sets.
on the three test sets.
For Chinese-English, the improvements over the
previous combination algorithm are smaller. The
new combination system yields up to an absolute
2.6 BLEU point improvement over the best input
system and up to 0.5 BLEU point improvement
over the previous combination algorithm on three
different test sets. Note that for Arabic-English,
the individual systems show a high variance in
translation quality when compared to Chinese-
English systems. This might explain why the im-
provements on Chinese-English are modest when
compared to Arabic-English results.
We also noticed that system combination
yielded much smaller improvement on Chinese
MTEval?08 data when compared to other test
sets, regardless of the alignment method (only 0.5
BLEU point over the best input system). We
suspected that this might happen because of a
mismatch between the genres of the test set and
the tuning set (the amount of web text data in
MTEval?08 test set is high although the tuning set
does not include any web text data). To test this
hypothesis, we created a new tuning set for system
combination, which consists of 2000 randomly se-
lected sentences from the previous MTEval test
sets and includes web text data. Using this new
tuning set, combination with the improved align-
ments yields a BLEU score of 29.3 on MTEval?08
data (an absolute improvement of 1.5 BLEU point
over the best input system, and 0.9 BLEU point
improvement over the combination with the old
alignments). These new results again validate the
usefulness of the improved alignments when the
tuning set matches the genre of the test set.
5.3 A Comparison of the Impact of Synonym
Matching and Two-pass Alignment
One last evaluation investigated the impact of each
component on the overall improvement. For this
Synon. 2-pass MT?05 MT?06 MT?08
No No 56.7 47.5 44.9
Yes No 57.3 47.8 45.2
No Yes 57.7 48.0 45.9
Yes Yes 57.9 48.4 46.2
Table 4: Comparison of Synonym Matching and
Two-pass Alignment on Arabic-English
purpose, we ran system combination by turning on
and off each component. Table 4 presents the sys-
tem combination results in terms of BLEU scores
on Arabic-English test sets when each component
is used on its own or when they are used together.
The results indicate that synonym matching on
its own yields improvements of 0.3-0.6 BLEU
points over not using synonym matching. Two-
pass alignment turns out to be more useful than
synonym matching, yielding an absolute improve-
ment of up to 1 BLEU point over one-pass align-
ment.
6 Conclusions
We presented an extension to the previous align-
ment approaches to handle word synonyms more
effectively in an attempt to improve the align-
ments between different hypotheses during confu-
sion network decoding. We also presented a two-
pass alignment strategy for a better alignment of
hypotheses with similar words but with a different
word ordering.
We evaluated our system combination ap-
proach on two language pairs: Arabic-English
and Chinese-English. Combination with improved
alignments yielded up to an absolute 2.9 BLEU
point improvement over the best input system and
up to an absolute 1.3 BLEU point improvement
over combination with the old alignments. It is
worth noting that these improvements are obtained
using very similar input systems. We expect that
the improvements will be higher when we use
structurally different MT systems as inputs to the
combiner.
Our future work includes a more effective use
of existing linguistic resources to handle alignment
of one word to multiple words (e.g., al-nahayan
vs. al nahyan, and threaten vs. pose threat)
and matching of similar (but not necessarily syn-
onymous) words (polls vs. elections). We are
also planning to extend word lattices to include
phrases from the individual systems (i.e., not just
the words) for more grammatical outputs.
39
Acknowledgments This material is based upon work
supported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
References
Akiba, Yasuhiro, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to se-
lect the best among outputs from multiple MT sys-
tems. In Proc. of the 19th Intl. Conf. on Computa-
tional Linguistics (COLING?2002).
Bangalore, Srinivas, German Bordel, and Giuseppe
Riccardi. 2001. Computing consensus translation
from multiple machine translation systems. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU?2001).
Chiang, David. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Fellbaum, Christiane. 1998. WordNet: An Electronic
Lexical Database. Bradford Books, March. Avail-
able at http://wordnet.princeton.edu.
Fiscus, Jonathan G. 1997. A post-processing system
to yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. of IEEE
Automatic Speech Recognition and Understanding
Workshop (ASRU?1997).
Frederking, Robert and Sergei Nirenburg. 1994.
Three heads are better than one. In Proc. of the
4th Conf. on Applied Natural Language Processing
(ANLP?1994).
Huang, Fei and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP?2007).
Jayaraman, Shyamsundar and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
Human Language Technology and the Meeting of the
North American Chapter of the Association for Com-
putational Linguistics Conf. (HLT/NAACL?2003).
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proc. of the 11th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?2006).
Mellebeek, Bart, Karolina Owczarzak, Josef Van Gen-
abith, and Andy Way. 2006. Multi-engine machine
translation by recursive sentence decomposition. In
Proc. of the 7th Conf. of the Association for Machine
Translation in the Americas (AMTA?2006).
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proc. of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04).
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics (ACL?2000).
Och, Franz J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL?2003).
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2002).
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In Proc. of the Human
Language Technology and the Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics Conf. (HLT/NAACL?2007).
Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2007).
Sim, Khe Chai, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007.
Consensus network decoding for statistical machine
translation system combination. In Proc. of the 32nd
Intl. Conf. on Acoustics, Speech, and Signal Process-
ing (ICASSP?2007).
Snover, Matthew, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of the 7th Conf. of the Association for Ma-
chine Translation in the Americas (AMTA?2006).
Stolcke, Andreas. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Intl. Conf. on
Spoken Language Processing (ICSLP?2002).
Wang, Wen, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In Proc.
of the IEEE Automatic Speech Recognition and Un-
derstanding Workshop (ASRU?2007).
Xu, Jia, Richard Zens, and Hermann Ney. 2005.
Sentence segmentation using IBM word alignment
model 1. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
40
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1093?1102, Prague, June 2007. c?2007 Association for Computational Linguistics
Mandarin Part-of-Speech Tagging and Discriminative Reranking
Zhongqiang Huang1
1Purdue University
West Lafayette, IN 47907
zqhuang@purdue.edu
Mary P. Harper1,2
2University of Maryland
College Park, MD 20742
mharper@casl.umd.edu
Wen Wang
SRI International
Menlo Park, CA 94025
wwang@speech.sri.com
Abstract
We present in this paper methods to improve
HMM-based part-of-speech (POS) tagging
of Mandarin. We model the emission prob-
ability of an unknown word using all the
characters in the word, and enrich the stan-
dard left-to-right trigram estimation of word
emission probabilities with a right-to-left
prediction of the word by making use of the
current and next tags. In addition, we utilize
the RankBoost-based reranking algorithm
to rerank the N-best outputs of the HMM-
based tagger using various n-gram, mor-
phological, and dependency features. Two
methods are proposed to improve the gen-
eralization performance of the reranking al-
gorithm. Our reranking model achieves an
accuracy of 94.68% using n-gram and mor-
phological features on the Penn Chinese
Treebank 5.2, and is able to further improve
the accuracy to 95.11% with the addition of
dependency features.
1 Introduction
Part-of-speech (POS) tagging is potentially help-
ful for many advanced natural language processing
tasks, for example, named entity recognition, pars-
ing, and sentence boundary detection. Much re-
search has been done to improve tagging perfor-
mance for a variety of languages. The state-of-the-
art systems have achieved an accuracy of 97% for
English on the Wall Street Journal (WSJ) corpus
(which contains 4.5M words) using various mod-
els (Brants, 2000; Ratnaparkhi, 1996; Thede and
Harper, 1999). Lower accuracies have been reported
in the literature for Mandarin POS tagging (Tseng et
al., 2005; Xue et al, 2002). This is, in part, due to
the relatively small size and the different annotation
guidelines (e.g., granularity of the tag set) for the an-
notated corpus of Mandarin. Xue at el. (2002) and
Tseng at el. (2005) reported accuracies of 93% and
93.74% on CTB-I (Xue et al, 2002) (100K words)
and CTB 5.0 (500K words), respectively, each us-
ing a Maximum Entropy approach. The character-
istics of Mandarin make it harder to tag than En-
glish. Chinese words tend to have greater POS tag
ambiguity than English. Tseng at el. (2005) reported
that 29.9% of the words in CTB have more than one
POS assignment compared to 19.8% of the English
words in WSJ. Moreover, the morphological prop-
erties of Chinese words complicate the prediction of
POS type for unknown words.
These challenges for Mandarin POS tagging
suggest the need to develop more sophisticated
methods. In this paper, we investigate the use
of a discriminative reranking approach to in-
crease Mandarin tagging accuracy. Reranking ap-
proaches (Charniak and Johnson, 2005; Chen et al,
2002; Collins and Koo, 2005; Ji et al, 2006; Roark
et al, 2006) have been successfully applied to many
NLP applications, including parsing, named entity
recognition, sentence boundary detection, etc. To
the best of our knowledge, reranking approaches
have not been used for POS tagging, possibly due
to the already high levels of accuracy for English,
which leave little room for further improvement.
However, the relatively poorer performance of ex-
isting methods on Mandarin POS tagging makes
reranking a much more compelling technique to
evaluate. In this paper, we use reranking to improve
tagging performance of an HMM tagger adapted to
1093
Mandarin. Hidden Markov models are simple and
effective, but unlike discriminative models, such as
Maximum Entropy models (Ratnaparkhi, 1996) and
Conditional Random Fields (John Lafferty, 2001),
they have more difficulty utilizing a rich set of con-
ditionally dependent features. This limitation can be
overcome by utilizing reranking approaches, which
are able to make use of the features extracted from
the tagging hypotheses produced by the HMM tag-
ger. Reranking also has advantages over MaxEnt
and CRF models. It is able to use any features
extracted from entire labeled sentences, including
those that cannot be incorporated into MaxEnt and
CRF models due to inference difficulties. In addi-
tion, reranking methods are able to utilize the infor-
mation provided by N-best lists. Finally, the decod-
ing phase of reranking is much simpler.
The rest of the paper is organized as follows. We
describe the HMM tagger in Section 2. We discuss
the modifications to better handle unknown words in
Mandarin and to enrich the word emission probabil-
ities through the combination of bi-directional esti-
mations. In Section 3, we first describe the reranking
algorithm and then propose two methods to improve
its performance. We also describe the features that
will be used for Mandarin POS reranking in Sec-
tion 3. Experimental results are given in Section 4.
Conclusions and future work appear in Section 5.
2 The HMM Model
2.1 Porting English Tagger to Mandarin
The HMM tagger used in this work is a second-
order HMM tagger initially developed for English
by Thede and Harper (1999). This state-of-the-art
second-order HMM tagger uses trigram transition
probability estimations, P (ti|ti?2ti?1), and trigram
emission probability estimations, P (wi|ti?1ti). Let
ti1 denote the tag sequence t1, ? ? ? , ti, and w
i
1 denote
the word sequencew1, ? ? ? , wi. The tagging problem
can be formally defined as finding the best tag se-
quence ?(wN1 ) for the word sequence w
N
1 of length
N as follows1:
?(wN1 ) = arg max
tN1
P (tN1 |w
N
1 ) = arg max
tN1
P (tN1 w
N
1 )
P (wN1 )
= arg max
tN1
P (tN1 w
N
1 ) (1)
= arg max
tN1
?
i
P (ti|t
i?1
1 w
i?1
1 )P (wi|t
i
1w
i?1
1 )
1We assume that symbols exist implicitly for boundary con-
ditions.
? arg max
tN1
?
i
P (ti|ti?2ti?1)P (wi|ti?1ti) (2)
The best tag sequence ?(wN1 ) can be determined ef-
ficiently using the Viterbi algorithm.
For estimating emission probabilities of unknown
words (i.e., words that do not appear in the train-
ing data) in English (and similarly for other inflected
languages), a weighted sum of P (ski |ti?1ti) (with
k up to four) was used as an approximation, where
ski is the suffix of length k of word wi (e.g., s
1
i is
the last character of word wi). The suffix informa-
tion and three binary features (i.e., whether the word
is capitalized, whether the word is hyphenated, and
whether the word contains numbers) are combined
to estimate the emission probabilities of unknown
words.
The interpolation weights for smoothing tran-
sition, emission, and suffix probabilities were
estimated using the log-based Thede smoothing
method (Thede and Harper, 1999) as follows:
PThede(n-gram)
= ?(n-gram)PML(n-gram) +
(1? ?(n-gram))PThede((n-1)-gram)
where:
PML(n-gram) = the ML estimation
?(n-gram) = f(n-gram count)
f(x) =
loga(x+ 1) + b
loga(x+ 1) + (b+ 1)
While porting the HMM-based English POS tag-
ger to Mandarin is fairly straightforward for words
seen in the training data, some thought is required to
handle unknown words due to the morphology dif-
ferences between the two languages. First, in Man-
darin, there is no capitalization and no hyphenation.
Second, although Chinese has morphology, it is not
the same as in English; words tend to contain far
fewer characters than inflected words in English, so
word endings will tend to be short, say one or two
characters long. Hence, in our baseline model (de-
noted HMM baseline), we simply utilize word end-
ings of up to two characters in length along with a
binary feature of whether the word contains num-
bers or not. In the next two subsections, we describe
two ways in which we enhance this simple HMM
baseline model.
1094
2.2 Improving the Mandarin Unknown Word
Model
Chinese words are quite different from English
words, and the word formation process for Chinese
words can be quite complex (Packard, 2000). In-
deed, the last characters in a Chinese word are, in
some cases, most informative of the POS type, while
for others, it is the characters at the beginning. Fur-
thermore, it is not uncommon for a character in the
middle of a word to provide some evidence for the
POS type of the word. Hence, we chose to employ
a rather simple but effective method to estimate the
emission probability, P (wi|ti?1, ti), of an unknown
word, wi. We use the geometric average2 of the
emission probability of the characters in the word,
i.e., P (ck|ti?1, ti) with ck being the k-th character
in the word. Since some of the characters in wi may
not have appeared in any word tagged as ti in that
context in the training data, only characters that are
observed in this context are used in the computation
of the geometric average, as shown below:
P (wi|ti?1, ti) = n
? ?
ck?wi,P (ck|ti?1,ti)6=0
P (ck|ti?1, ti) (3)
where
n = |{ck ? wi|P (ck|ti?1, ti) 6= 0}|
2.3 Bi-directional Word Probability Estimation
In Equation 2, the word emission probability
P (wi|ti?1ti) is a left-to-right prediction that de-
pends on the current tag ti associated with wi, as
well as its previous tag ti?1. Although the interac-
tion between wi and the next tag ti+1 is captured to
some extent when ti+1 is generated by the model,
this implicit interaction may not be as effective as
adding the information more directly to the model.
Hence, we chose to apply the constraint explicitly in
our HMM framework by replacing P (wi|ti?1ti) in
Equation 2 with P ?(wi|ti?1ti)P 1??(wi|titi+1) for
both known and unknown words, with ?(wN1 ) deter-
mined by:
?(wN1 ) = arg max
tN1
?
i
(P (ti|ti?2ti?1)?
P?(wi|ti?1ti)P
1??(wi|titi+1)) (4)
2Based on preliminary testing, the geometric average pro-
vided greater tag accuracy than the arithmetic average.
This corresponds to a mixture model of two genera-
tion paths, one from the left and one from the right,
to approximate ?(wN1 ) in Equation 1 in a different
way.
?(wN1 ) = arg max
tN1
P (tN1 w
N
1 )
= arg max
tN1
P (tN1 )P (w
N
1 |t
N
1 )
P (tN1 ) ?
?
i
P (ti|ti?1ti?2)
P (wN1 |t
N
1 ) = P
?(wN1 |t
N
1 )P
1??(wN1 |t
N
1 )
?
?
i
P?(wi|ti?1ti)P
1??(wi|titi+1)
In this case, the decoding process involves the
computation of three local probabilities, i.e.,
P (ti|ti?2ti?1), P (wi|ti?1ti), and P (wi|titi+1).
By using a simple manipulation that shifts the
time index of P (wi|titi+1) in Equation 4 by two
time slices3 (i.e., by replacing P (wi|titi+1) with
P (wi?2|ti?2ti?1)), we are able to compute ?(wN1 )
in Equation 4 with the same asymptotic time com-
plexity of decoding as in Equation 2.
3 Discriminative Reranking
In this section, we describe our use of the
RankBoost-based (Freund and Schapire, 1997; Fre-
und et al, 1998) discriminative reranking approach
that was originally developed by Collins and Koo
(2005) for parsing. It provides an additional avenue
for improving tagging accuracy, and also allows us
to investigate the impact of various features on Man-
darin tagging performance. The reranking algorithm
takes as input a list of candidates produced by some
probabilistic model, in our case the HMM tagger,
and reranks these candidates based on a set of fea-
tures. We first introduce Collins? reranking algo-
rithm in Subsection 3.1, and then describe two mod-
ifications in Subsections 3.2 and 3.3 that were de-
signed to improve the generalization performance of
the reranking algorithm for our POS tagging task.
The reranking features that are used for POS tagging
are then described in Subsection 3.4.
3.1 Collins? Reranking Algorithm
For training the reranker for the POS tagging task,
there are n sentences {si : i = 1, ? ? ? , n} each with
ni candidates {xi,j : j = 1, ? ? ? , ni} along with
3Replacing P (wi|titi+1) with P (wi?1|ti?1ti) also gives
the same solution.
1095
the log-probability L(xi,j) produced by the HMM
tagger. Each tagging candidate xi,j in the training
data has a ?goodness? score Score(xi,j) that mea-
sures the similarity between the candidate and the
gold reference. For tagging, we use tag accuracy
as the similarity measure. Without loss of general-
ity, we assume that xi,1 has the highest score, i.e.,
Score(xi,1) ? Score(xi,j) for j = 2, ? ? ? , ni. To
summarize, the training data consists of a set of ex-
amples {xi,j : i = 1, ? ? ? , n; j = 1, ? ? ? , ni}, each
along with a ?goodness? score Score(xi,j) and a
log-probability L(xi,j).
A set of indicator functions {hk : k = 1, ? ? ? ,m}
are used to extract binary features {hk(xi,j) : k =
1, ? ? ? ,m} on each example xi,j . An example of an
indicator function for POS tagging is given below:
h2143(x) = 1 ifx contains n-gram ?go/VV to?
0 otherwise
Each indicator function hk is associated with a
weight parameter ?k which is real valued. In ad-
dition, a weight parameter ?0 is associated with
the log-probability L(xi,j). The ranking func-
tion of candidate xi,j is defined as ?0L(xi,j) +
m?
k=1
?khk(xi,j).
The objective of the training process is to set the
parameters ?? = {?0, ?1, ? ? ? , ?m} to minimize the
following loss function Loss(??) (which is an upper
bound on the training error):
Loss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??)
where Si,j is the weight function that gives the im-
portance of each example, and Mi,j(??) is the mar-
gin:
Si,j = Score(xi,1)? Score(xi,j)
Mi,j(??) = ?0(L(xi,1)? L(xi,j)) +
m?
k=1
?k(hk(xi,1)? hk(xi,j))
All of the ?i?s are initially set to zero. The value
of ?0 is determined first to minimize the loss func-
tion and is kept fixed afterwards. Then a greedy se-
quential 4 optimization method is used in each itera-
tion (i.e., a boosting round) to select the feature that
4Parallel optimization algorithms exist and have comparable
performance according to (Collins et al, 2002).
has the most impact on reducing the loss function
and then update its weight parameter accordingly.
For each k ? {1, ? ? ? ,m}, (hk(xi,1)? hk(xi,j)) can
only take one of the three values: +1, -1, or 0. Thus
the training examples can be divided into three sub-
sets with respect to k:
A+k = {(i, j) : (hk(xi,1)? hk(xi,j)) = +1}
A?k = {(i, j) : (hk(xi,1)? hk(xi,j)) = ?1}
A0k = {(i, j) : (hk(xi,1)? hk(xi,j)) = 0}
The new loss after adding the update parameter ?
to the parameter ?k is shown below:
Loss(??, k, ?) =
?
(i,j)?A+
k
Si,je
?Mi,j(??)?? +
?
(i,j)?A?
k
Si,je
?Mi,j(??)+? +
?
(i,j)?A0
k
Si,je
?Mi,j(??)
= e??W+k + e
?W?k +W
0
k
The best feature/update pair (k?, ??) that minimizes
Loss(??, k, ?) is determined using the following for-
mulas:
k? = arg max
k
?
?
?
?
?
W+k ?
?
W?k
?
?
?
? (5)
?? =
1
2
log
W+k?
W?k?
(6)
The update formula in Equation 6 is problematic
when either W+k? or W
?
k? is zero. W
+
k is zero if hk
never takes on a value 1 for any xi,1 with value 0 on
a corresponding xi,j for j = 2, ? ? ? , ni (and similarly
for W?k ). Collins introduced a smoothing parameter
 to address this problem, resulting in a slight modi-
fication to the update formula:
?? =
1
2
log
W+k? + Z
W?k? + Z
(7)
The value of  plays an important role in this for-
mula. If  is set too small, the smoothing factor Z
would not prevent setting ?? to a potentially overly
large absolute value, resulting in over-fitting. If  is
set too large, then the opposite condition of under-
training could result. The value of  is determined
based on a development set.
1096
3.2 Update Once
Collins? method allows multiple updates to the
weight of a feature based on Equations 5 and 7. We
found that for those features for which either W+k or
W?k equals zero, the update formula in Equation 7
can only increase their weight (in absolute value) in
one direction. Although these features are strong
and useful, setting their weights too large can be un-
desirable in that it limits the use of other features for
reducing the loss.
Based on this analysis, we have developed and
evaluated an update-once method, in which we use
the update formula in Equation 7 but limit weight
updates so that once a feature is selected on a cer-
tain iteration and its weight parameter is updated,
it cannot be updated again. Using this method, the
weights of the strong features are not allowed to pre-
vent additional features from being considered dur-
ing the training phase.
3.3 Regularized Reranking
Although the update-once method may attenuate
over-fitting to some extent, it also prevents adjust-
ing the value of any weight parameter that is initially
set too high or too low in an earlier boosting round.
In order to design a more sophisticated weight up-
date method that allows multiple updates in both di-
rections while penalizing overly large weights, we
have also investigated the addition of a regulariza-
tion term R(??), an exponential function of ??, to the
loss function:
RegLoss(??) =
?
i
ni?
j=2
Si,je
?Mi,j(??) +R(??)
R(??) =
m?
k=1
pk ? (e
??k + e?k ? 2)
where pk is the penalty weight of parameter ?k. The
reason that we chose this form of regularization is
that (e??k +e?k?2) is a symmetric, monotonically
decreasing function of |?k|, and more importantly it
provides a closed analytical expression of the weight
update formula similar to Equations 5 and 6. Hence,
the best feature/update pair for the regularized loss
function is defined as follows:
k? = arg max
k
?
?
?
?
?
W+k + pke
??k ?
?
W?k + pke
+?k
?
?
?
?
?? =
1
2
log
W+k? + pk?e
??k?
W?k? + pk?e
+?k?
There are many ways of choosing pk, the penalty
weight of ?k. In this paper, we use the values of
? ?(W+k +W
?
k ) at the beginning of the first iteration
(after ?0 is determined) for pk, where ? is a weight-
ing parameter to be tuned on the development set.
The regularized weight update formula has many ad-
vantages. It is always well defined no matter what
value W+k and W
?
k take, in contrast to Equation 6.
For all features, even in the case when either W+k or
W?k equals zero, the regularized update formula al-
lows weight updates in two directions. If the weight
is small, W+k and W
?
k have more impact on deter-
mining the weight update direction, however, when
the weight becomes large, the regularization factors
pke?? and pke+? favor reducing the weight.
3.4 Reranking Features
A reranking model has the flexibility of incorporat-
ing any type of feature extracted from N-best can-
didates. For the work presented in this paper, we
examine three types of features. For each window
of three word/tag pairs, we extract all the n-grams,
except those that are comprised of only one word/tag
pair, or only tags, or only words, or do not include
either the word or tag in the center word/tag pair.
These constitute the n-gram feature set.
In order to better handle unknown words, we also
extract the two most important types of morpho-
logical features5 that were utilized in (Tseng et al,
2005) for those words that appear no more than
seven times (following their convention) in the train-
ing set:
Affixation features: we use character n-gram pre-
fixes and suffixes for n up to 4. For example,
for word/tag pair D??/NN (Information-
Bag, i.e., folder), we add the following fea-
tures: (prefix1, D, NN), (prefix2, D?, NN),
(prefix3, D??, NN), (suffix1, ?, NN), (suf-
fix2,??, NN), (suffix3,D??, NN).
AffixPOS features6: we used the training set to
build a prefix/POS and suffix/POS dictionary
associating possible tags with each prefix and
5Tseng at el. also used other morphological features that
require additional resources to which we do not have access.
6AffixPOS features are somewhat different from the CTB-
Morph features used in (Tseng et al, 2005), where a mor-
pheme/POS dictionary with the possible tags for all morphemes
in the training set was used instead of two separate dictionaries
for prefix and suffix. AffixPOS features perform slightly better
in our task than the CTB-morph features.
1097
suffix in the training set. The AffixPOS fea-
tures indicate the set of tags a given affix could
have. For the same example D??/NN, D
occurred as prefix in both NN and VV words in
the training data. So we add the following fea-
tures based on the prefix D: (prefix, D, NN,
1, NN), (prefix, D, VV, 1, NN), and (prefix,
D, X, 0, NN) for every tag X not in {NN, VV},
where 1 and 0 are indicator values. Features are
extracted in the similar way for the suffix?.
The n-gram and morphological features are easy
to compute, however, they have difficulty in captur-
ing the long distance information related to syntac-
tic relationships that might help POS tagging ac-
curacy. In order to examine the effectiveness of
utilizing syntactic information in tagging, we have
also experimented with dependency features that are
extracted based on automatic parse trees. First a
bracketing parser (the Charniak parser (Charniak,
2000) in our case) is used to generate the parse
tree of a sentence, then the const2dep tool devel-
oped by Hwa was utilized to convert the bracket-
ing tree to a dependency tree based on the head
percolation table developed by the second author.
The dependency tree is comprised of a set of de-
pendency relations among word pairs. A depen-
dency relation is a triple ?word-a, word-b, relation?,
in which word-a is governed by word-b with gram-
matical relation denoted as relation. For example,
in the sentence ??(Tibet) ?N(economy) ?
?(construction) ??(achieves) >W(significant)
?(accomplishments)?, one example dependency
relation is ???, ?, mod?. Given these depen-
dency relations, we then extract dependency features
(in total 36 features for each relation) by examining
the POS tags of the words for each tagging candi-
date of a sentence. The relative positions of the word
pairs are also taken into account for some features.
For example, if?? and? in the above sentence
are tagged as VV and NN respectively in one can-
didate, then two example dependency features are
(dep-1, ??, VV, ?, NN, mod), (dep-14, ??,
VV, NN, right, mod), in which dep-1 and dep-14 are
feature types and right indicates that word-b (??)
is to the right of word-a (?).
4 Experiments
4.1 Data
The most recently released Penn Chinese Treebank
5.2 (denoted CTB, released by LDC) is used in our
experiments. It contains 500K words, 800K char-
acters, 18K sentences, and 900 data files, includ-
ing articles from the Xinhua news agency (China-
Mainland), Information Services Department of
HKSAR (Hongkong), and Sinorama magazine (Tai-
wan). Its format is similar to the English WSJ Penn
Treebank, and it was carefully annotated. There are
33 POS tags used, to which we add tags to discrim-
inate among punctuation types. The original POS
tag for punctuation was PU; we created new POS
tags for each distinct punctuation type (e.g., PU-?).
The CTB corpus was collected during different
time periods from different sources with a diversity
of articles. In order to obtain a representative split
of training, development, and test sets, we divide
the whole corpus into blocks of 10 files by sorted
order. For each block, the first file is used for de-
velopment, the second file is used for test, and the
remaining 8 files are used for training. Table 1 gives
the basic statistics on the data. The development
set is used to determine the parameter ? in Equa-
tion 4, the smoothing parameter  in Equation 7, the
weight parameter ? described in Section 3.3, and the
number of boosting rounds in the reranking model.
In order to train the reranking model, the method
in (Collins and Koo, 2005) is used to prepare the
N-best training examples. We divided the training
set into 20 chunks, with each chunk N-best tagged
by the HMM model trained on the combination of
the other 19 chunks. The development set is N-best
tagged by the HMM model trained on the training
set, and the test set is N-best tagged by the HMM
model trained on the combination of the training set
and the development set.
Train Dev Test
#Sentences 14925 1904 1975
#Words 404844 51243 52900
Table 1: The basic statistics on the data.
In the following subsections, we will first exam-
ine the HMM models alone to determine the best
HMM configuration to use to generate the N-best
candidates, and then evaluate the reranking mod-
els. Finally, we compare our performance with pre-
vious work. In this paper, we use the sign test
with p ? 0.01 to evaluate the statistical significance
of the difference between the performances of two
models.
1098
4.2 Results of the HMM taggers
The baseline HMM model ported directly from the
English tagger, as described in Subsection 2.1, has
an overall tag accuracy of 93.12% on the test set,
which is fairly low compared to the 97% accuracy
of many state-of-the-art taggers on WSJ for English.
By approximating the unknown word emission
probability using the characters in the word as in
Equation 3, the performance of the HMM tagger im-
proves significantly to 93.43%, suggesting that char-
acters in different positions of a Chinese word help
to disambiguate the word class of the entire word, in
contrast to English for which suffixes are most help-
ful.
Figure 1 depicts the impact of combining the left-
to-right and right-to-left word emission models us-
ing different weighting values (i.e., ?) on the devel-
opment set. Note that emission probabilities of un-
known words are estimated based on characters us-
ing the same ? for combination. When ? = 1.0, the
model uses only the standard left-to-right prediction
of words, while when ? = 0 it uses only the right-to-
left estimation. It is interesting to note that the right-
to-left estimation results in greater accuracy than the
left-to-right estimation. This might be because there
is stronger interaction between a word and its next
tag. Also as shown in Figure 1, the estimations in
the two directions are complementary to each other,
with ? = 0.5 performing best. The performance of
the HMM taggers on the test set is given in Table 2
for the best operating point, as well as the two other
extreme operating points to compare the left-to-right
and right-to-left constraints. Our best HMM tagger
further improves the tag accuracy significantly from
93.43% (? = 1.0) to 94.01% (? = 0.5).
Figure 1: The accuracy of the HMM tagger on the
development set with various ? values for combin-
ing the word emission probabilities.
Overall Known Unknown
HMM baseline 93.12% 94.65% 69.08%
HMM, ?=1.0 93.43% 94.71% 73.41%
HMM, ?=0.0 93.65% 94.88% 74.23%
HMM, ?=0.5 94.01% 95.21% 75.15%
Table 2: The performance of various HMM taggers
on the test set.
4.3 Results of the Reranking Models
The HMM tagger with the best accuracy (i.e., the
one with ? = 0.5 in Table 2) is used to generate
the N-Best tagging candidates, with a maximum of
100 candidates. As shown in Table 3, a maximum of
100-Best provides a reasonable margin for improve-
ment in the reranking task.
We first test the performance of the reranking
methods using only the n-gram feature set, which
contains around 18 million features. Later, we
will investigate the addition of morphological fea-
tures and dependency features. The smoothing
parameter  (for Collins? method and the update-
once method) and the weight parameter ? (for
the regularization method) both have great im-
pact on reranking performance. We trained vari-
ous reranking models with  values of 0.0001 ?
{1, 2.5, 5, 7.5, 10, 25, 50, 75, 100}, and ? values of
{0.1, 0.25, 0.5, 0.75, 1}. For all these parameter val-
ues, 600,000 rounds of iterations were executed on
the training set. The development set was used to
determine the early stopping point in training. If
not mentioned explicitly, all the results reported are
based on the best parameters tuned on the develop-
ment set.
1-Best 50-Best 100-Best
train 93.48% 96.96% 97.13%
dev 93.75% 97.68% 97.84%
test 93.19% 97.19% 97.35%
Table 3: The oracle tag accuracies of the 1-Best, 50-
Best, and 100-Best candidates in the training, devel-
opment, and test sets for the reranking experiments.
Note that the tagging candidates are prepared using
the method described in Subsection 4.1.
Table 4 reports the performance of the best HMM
tagger and the three reranking taggers on the test set.
All three reranking methods improve the HMM tag-
ger significantly. Also, the update-once and regu-
larization methods both outperform Collins? original
training method significantly.
1099
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.38% 95.56% 75.85%
Update-once 94.50% 95.67% 76.13%
Regularized 94.54% 95.70% 76.48%
Table 4: The performance on the test set of the
HMM tagger, and the reranking methods using the
n-gram features.
We observed that no matter which value the
smoothing parameter  takes, there are only about
10,000 non-zero features finally selected by Collins?
original method. In contrast, the two new methods
select substantially more features, as shown in Ta-
ble 5. As mentioned before, there are some strong
features that only appear in positive or negative sam-
ples, i.e., either W+k or W
?
k equals zero. Although
introducing the smoothing parameter  in Equation 7
prevents infinite weight values, the update to the
feature weights is no longer optimal (in terms of
minimizing the error function). Since the update
is not optimal, subsequent iterations may still fo-
cus on these features (and thus ignore other weaker
but informative features) and always increase their
weights in one direction, leading to biased training.
The update-once method at each iteration selects
a new feature that has the most impact in reduc-
ing the training loss function. It has the advantage
of preventing increasingly large weights from being
assigned to the strong features, enabling the update
of other features. The regularization method allows
multiple updates and also penalizes large weights.
Once a feature is selected and has its weight updated,
no matter how strong the feature is, the weight value
is optimal in terms of the current weights of other
features, so that the training algorithm would choose
another feature to update. A previously selected fea-
ture may be selected again if it becomes suboptimal
due to a change in the weights of other features.
#iterations #features percent
Collins 115400 10020 8.68%
Update-once 545100 545100 100%
Regularized 92500 70131 75.82%
Table 5: The number of iterations (for the best
performance), the number of selected features, and
the percentage of selected features, by Collins?
method, the update-once method, and the regular-
ization method on the development set.
Overall Known Unknown
HMM, ?=0.5 94.01% 95.21% 75.15%
Collins 94.44% 95.55% 77.05%
Update-once 94.68% 95.68% 78.91%
Regularized 94.64% 95.71% 77.84%
Table 6: The performance on the test set of the
HMM tagger and the reranking methods using n-
gram and morphological features.
We next add morphological features to the n-gram
features selected by the reranking methods7. As
can be seen by comparing Table 6 to Table 4, mor-
phological features improve the tagging accuracy of
unknown words. It should be noted that the im-
provement made by both update-one and regulariza-
tion methods is statistically significant over using n-
gram features alone; however, the improvement by
Collins? original method is not significant. This sug-
gests that the two new methods are able to utilize a
greater variety of features than the original method.
We trained several Charniak parsers using the
same method for the HMM taggers to generate auto-
matic parse trees for training, development, and test
data. The update-once method is used to evaluate
the effectiveness of dependency features for rerank-
ing, as shown in Table 7. The parser has an overall
tagging accuracy that is greater than that of the best
HMM tagger, but worse than that of the reranking
models using n-gram and morphological features. It
is interesting to note that reranking with the depen-
dency features alone improves the tagging accuracy
significantly, outperforming reranking models using
n-gram and morphological features. This suggests
that the long distance features based on the syntactic
structure of the sentence are very beneficial for POS
tagging of Mandarin. Moreover, n-gram and mor-
phological features are complementary to the depen-
dency features, with their combination performing
the best. The n-gram features improve the accuracy
on known words, while the morphological features
improve the accuracy on unknown words. The best
accuracy of 95.11% is an 18% relative reduction in
error compared to the best HMM tagger.
7Because the size of the combined feature set of all n-gram
features and morphological features is too large to be handled
by our server, we chose to add morphological features to the
n-gram features selected by the reranking methods, and then
retrain the reranking model.
1100
Overall Known Unknown
Parser 94.31% 95.57% 74.52%
dep 94.93% 96.01% 77.87%
dep+ngram 95.00% 96.11% 77.49%
dep+morph 94.98% 96.01% 78.79%
dep+ngram+morph 95.11% 96.12% 79.32%
Table 7: The tagging performance of the parser
and the update-once reranking models with depen-
dency features and their combination with n-gram
and morphological features.
4.4 Comparison to Previous Work
So how is our performance compared to previous
work? When working on the same training/test data
(CTB5.0 with the same pre-processing procedures)
as in (Tseng et al, 2005), our HMM model ob-
tained an accuracy of 93.72%, as compared to their
93.74% accuracy. Our reranking model8 using n-
gram and morphological features improves the ac-
curacy to 94.16%. Note that we did not use all the
morphological features as in (Tseng et al, 2005),
which would probably provide additional improve-
ment. The dependency features are expected to fur-
ther improve the performance, although they are not
included here in order to provide a relatively fair
comparison.
5 Conclusions and Future Work
We have shown that the characters in a word are
informative of the POS type of the entire word in
Mandarin, reflecting the fact that the individual Chi-
nese characters carry POS information to some de-
gree. The syntactic relationship among characters
may provide further information, which we leave
as future work. We have also shown that the ad-
ditional right-to-left estimation of word emission
probabilities is useful for HMM tagging of Man-
darin. This suggests that explicit modeling of bi-
directional interactions captures more sequential in-
formation. This could possibly help in other sequen-
tial modeling tasks.
We have also investigated using the reranking al-
gorithm in (Collins and Koo, 2005) for the Man-
darin POS tagging task, and found it quite effective
8Tseng at el.?s training/test split uses up the entire CTB cor-
pus, leaving no development data for tuning parameters. In
order to roughly measure reranking performance, we use the
update-once method to train the reranking model for 600,000
rounds with the other parameters tuned in Section 4. This sac-
rifices performance to some extent.
in improving tagging accuracy. The original algo-
rithm has a tendency to focus on a small subset of
strong features and ignore some of the other useful
features. We were able to improve the performance
of the reranking algorithm by utilizing two different
methods that make better use of more features. Both
are simple and yet effective. The effectiveness of de-
pendency features suggests that syntax-based long
distance features are important for improving part-
of-speech tagging performance in Mandarin. Al-
though parsing is computationally more demanding
than tagging, we hope to identify related features
that can be extracted more efficiently.
In future efforts, we plan to extract additional
reranking features utilizing more explicitly the char-
acteristics of Mandarin. We also plan to extend our
work to speech transcripts for Broadcast News and
Broadcast Conversation corpora, and explore semi-
supervised training methods for reranking.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of DARPA. We gratefully acknowledge the com-
ments from the anonymous reviewers.
References
Thorsten Brants. 2000. TnT a statistical part-of-speech
tagger. In ANLP, pages 224?231.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132?139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
John Chen, Srinivas Bangalore, Michael Collins, and
Owen Rambow. 2002. Reranking an n-gram supertag-
ger. In the Sixth International Workshop on Tree Ad-
joining Grammars and Related Frameworks.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
1101
Michael Collins, Robert E. Schapire, and Yoram Singer.
2002. Logistic regression, adaboost and bregman dis-
tances. Machine Learning, 48(1):253?285.
Yoav Freund and Robert E. Schapire. 1997. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. Journal of Computer and System
Sciences, 1(55):119?139.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In the Fifteenth International
Conference on Machine Learning.
Heng Ji, Cynthia Rudin, and Ralph Grishman. 2006. Re-
ranking algorithms for name tagging. In HLT/NAACL
06 Workshop on Computationally Hard Problems and
Joint Inference in Speech and Language Processing.
Fernando Pereira John Lafferty, Andrew McCallum.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Jerome Packard. 2000. The Morphology of Chinese.
Cambridge University Press.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In EMNLP.
Brian Roark, Yang Liu, Mary Harper, Robin Stewart,
Matthew Lease, Matthew Snover, Izhak Shafran, Bon-
nie Dorr, John Hale, Anna Krasnyanskaya, and Lisa
Yung. 2006. Reranking for sentence boundary detec-
tion in conversational speech. In ICASSP.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In ACL, pages 175?182.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help pos tagging
of unknown words across language varieties. In the
Fourth SIGHAN Workshop on Chinese Language Pro-
cessing.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a large-scale annotated chinese corpus. In
COLING.
1102
Proceedings of NAACL HLT 2009: Short Papers, pages 265?268,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Anchored Speech Recognition for Question Answering
Sibel Yaman1, Gokhan Tur2, Dimitra Vergyri2, Dilek Hakkani-Tur1,
Mary Harper3 and Wen Wang2
1 International Computer Science Institute
2 SRI International
3 Hopkins HLT Center of Excellence, University of Maryland
{sibel,dilek}@icsi.berkeley.edu,{gokhan,dverg,wwang}@speech.sri.com,mharper@casl.umd.edu
Abstract
In this paper, we propose a novel question
answering system that searches for responses
from spoken documents such as broadcast
news stories and conversations. We propose a
novel two-step approach, which we refer to as
anchored speech recognition, to improve the
speech recognition of the sentence that sup-
ports the answer. In the first step, the sen-
tence that is highly likely to contain the an-
swer is retrieved among the spoken data that
has been transcribed using a generic automatic
speech recognition (ASR) system. This candi-
date sentence is then re-recognized in the sec-
ond step by constraining the ASR search space
using the lexical information in the question.
Our analysis showed that ASR errors caused
a 35% degradation in the performance of the
question answering system. Experiments with
the proposed anchored recognition approach
indicated a significant improvement in the per-
formance of the question answering module,
recovering 30% of the answers erroneous due
to ASR.
1 Introduction
In this paper, we focus on finding answers to user
questions from spoken documents, such as broad-
cast news stories and conversations. In a typical
question answering system, the user query is first
processed by an information retrieval (IR) system,
which finds out the most relevant documents among
massive document collections. Each sentence in
these relevant documents is processed to determine
whether or not it answers user questions. Once a
candidate sentence is determined, it is further pro-
cessed to extract the exact answer.
Answering factoid questions (i.e., questions like
?What is the capital of France??) using web makes
use of the redundancy of information (Whittaker et
al., 2006). However, when the document collection
is not large and when the queries are complex, as
in the task we focus on in this paper, more sophis-
ticated syntactic, semantic, and contextual process-
ing of documents and queries is performed to ex-
tract or construct the answer. Although much of the
work on question answering has been focused on
written texts, many emerging systems also enable
either spoken queries or spoken document collec-
tions (Lamel et al, 2008). The work we describe
in this paper also uses spoken data collections to
answer user questions but our focus is on improv-
ing speech recognition quality of the documents by
making use of the wording in the queries. Consider
the following example:
Manual transcription: We understand from Greek of-
ficials here that it was a Russian-made rocket which is
available in many countries but certainly not a weapon
used by the Greek military
ASR transcription: to stand firm greek officials here that
he was a a russian made rocket uh which is available in
many countries but certainly not a weapon used by he
great moments
Question: What is certainly not a weapon used by the
Greek military?
Answer: a Russian-made rocket
Answering such questions requires as good ASR
transcriptions as possible. In many cases, though,
there is one generic ASR system and a generic lan-
guage model to use. The approach proposed in this
paper attempts to improve the ASR performance by
re-recognizing the candidate sentence using lexical
information from the given question. The motiva-
265
tion is that the question and the candidate sentence
should share some common words, and therefore
the words of the answer sentence can be estimated
from the given question. For example, given a fac-
toid question such as: ?What is the tallest build-
ing in the world??, the sentence containing its an-
swer is highly likely to include word sequences such
as: ?The tallest building in the world is NAME?
or ?NAME, the highest building in the world, ...?,
where NAME is the exact answer.
Once the sentence supporting the answer is lo-
cated, it is re-recognized such that the candidate an-
swer is constrained to include parts of the question
word sequence. To achieve this, a word network is
formed to match the answer sentence to the given
question. Since the question words are taken as a ba-
sis to re-recognize the best-candidate sentence, the
question acts as an anchor, and therefore, we call
this approach anchored recognition.
In this work, we restrict out attention to questions
about the subject, the object and the locative, tempo-
ral, and causative arguments. For instance, the fol-
lowings are the questions of interest for the sentence
Obama invited Clinton to the White House to discuss
the recent developments:
Who invited Clinton to the White House?
Who did Obama invite to the White House?
Why did Obama invite Clinton to the White House?
2 Sentence Extraction
The goal in sentence extraction is determining the
sentence that is most likely to contain the answer
to the given question. Our sentence extractor relies
on non-stop word n-gram match between the ques-
tion and the candidate sentence, and returns the sen-
tence with the largest weighted average. Since not
all word n-grams have the same importance (e.g.
function vs. content words), we perform a weighted
sum as typically done in the IR literature, i.e., the
matching n-grams are weighted with respect to their
inverse document frequency (IDF) and length.
A major concern for accurate sentence extraction
is the robustness to speech recognition errors. An-
other concern is dealing with alternative word se-
quences for expressing the same meaning. To tackle
the second challenge, one can also include syn-
onyms, and compare paraphrases of the question and
the candidate answer. Since our main focus is on ro-
Predicate
Sentence ExtractionSemantic Roles
Answering Sentence
Answer Extraction Anchored Recognition
Document
Speech Recognition
Question
Searched Argument
Baseline
Proposed
Figure 1: Conceptual scheme of the baseline and pro-
posed information distillation system.
bustness to speech recognition errors, our data set
is limited to those questions that are worded very
similarly to the candidate answers. However, the
approach is more general, and can be extended to
tackle both challenges.
3 Answer Extraction
When the answer is to be extracted from ASR out-
put, the exact answers can be erroneous because (1)
the exact answer phrase might be misrecognized, (2)
other parts of the sentence might be misrecognized,
so the exact answer cannot be extracted either be-
cause parser fails or because the sentence cannot
match the query.
The question in the example in the Introduction
section is concerned with the object of the predicate
?is? rather than of the other predicates ?understand?
or ?was?. Therefore, a pre-processing step is needed
to correctly identify the object (in this example) that
is being asked, which is described next.
Once the best candidate sentence is estimated, a
syntactic parser (Harper and Huang, ) that also out-
puts function tags is used to parse both the ques-
tion and candidate answering sentence. The parser
is trained on Fisher, Switchboard, and speechified
Broadcast News, Brown, and Wall Street Journal
treebanks without punctuation and case to match in-
put the evaluation conditions.
An example of such a syntactic parse is given in
Figure 2. As shown there, the ?SBJ? marks the sur-
face subject of a given predicate, and the ?TMP? tag
marks the temporal argument. There are also the
?DIR? and ?LOC? tags indicating the locative ar-
gument and the ?PRP? tag indicating the causal ar-
gument. Such parses not only provide a mechanism
to extract information relating to the subject of the
predicate of interest, but also to extract the part of
266
Figure 2: The function tags assist in finding the subject,
object, and arguments of a given predicate.
the sentence that the question is about, in this ex-
ample ?a Russian-made rocket [which] is certainly
not a weapon used by the Greek military?. The ex-
traction of the relevant part is achieved by matching
the predicate of the question to the predicates of the
subsentences in the best candidate sentence. Once
such syntactic parses are obtained for the part of the
best-candidate sentence that matches the question, a
set of rules are used to extract the argument that can
answer the question.
4 Anchored Speech Recognition
In this study we employed a state-of-the-art broad-
cast news and conversations speech recognition
system (Stolcke et al, 2006). The recognizer
performs a total of seven decoding passes with
alternating acoustic front-ends: one based on
Mel frequency cepstral coefficients (MFCCs) aug-
mented with discriminatively estimated multilayer-
perceptron (MLP) features, and one based on per-
ceptual linear prediction (PLP) features. Acoustic
models are cross-adapted during recognition to out-
put from previous recognition stages, and the output
of the three final decoding steps are combined via
confusion networks.
Given a question whose answer we expect to find
in a given sentence, we construct a re-decoding net-
work to match that question. We call this process an-
chored speech recognition, where the anchor is the
question text. Note that this is different than forced
alignment, which enforces the recognition of an au-
dio stream to align with some given sentence. It is
used for detecting the start times of individual words
or for language learning applications to exploit the
acoustic model scores, since there is no need for a
language model.
Our approach is also different than the so-called
flexible alignment (Finke and Waibel, 1997), which
is basically forced alignment that allows skipping
any part of the given sentence, replacing it with a re-
ject token, or inserting hesitations in between words.
In our task, we require all the words in the ques-
tion to be in the best-candidate sentence without any
skips or insertions. If we allow flexible alignment,
then any part of the question could be deleted. In the
proposed anchored speech recognition scheme, we
allow only pauses and rejects between words, but do
not allow any deletions or skips.
The algorithm for extracting anchored recognition
hypotheses is as follows: (i) Construct new recogni-
tion and rescoring language models (LMs) by inter-
polating the baseline LMs with those trained from
only the question sentences and use the new LM
to generate lattices - this aims to bias the recogni-
tion towards word phrases that are included in the
questions. (ii) Construct for each question an ?an-
chored? word network that matches the word se-
quence of the question, allowing any other word se-
quence around it. For example if the question is
WHAT did Bruce Gordon say?, we construct a word
network to match Bruce Gordon said ANYTHING
where ?ANYTHING? is a filler that allows any word
(a word loop). (iii) Intersect the recognition lat-
tices from step (i) with the anchored network for
each question in (ii), thus extracting from the lattice
only the paths that match as answers to the ques-
tion. Then rescore that new lattice with higher order
LM and cross-word adapted acoustic models to get
the best path. (iv) If the intersection part in (iii) fails
then we use a more constrained recognition network:
Starting with the anchored network in (ii) we first
limit the vocabulary in the ANYTHING word-loop
sub-network to only the words that were included in
the recognition lattice from step (i). Then we com-
pose this network with the bigram LM (from step (i))
to add bigram probabilities to the network. Vocab-
ulary limitation is done for efficiency reasons. We
also allow optional filler words and pauses to this
network to allow for hesitations, non-speech events
and pauses within the utterance we are trying to
match. This may limit somewhat the potential im-
provement from this approach and we are working
267
Question Type ASR Output Manual Trans.
Subject 85% 98%
Object 75% 90%
Locative Arg. 81% 93%
Temporal Arg. 94% 98%
Reason 86% 100%
Total 83% 95%
Table 1: Performance figures for the sentence extraction
system using automatic and manual transcriptions.
Question ASR Manual Anchored
Type Output Trans. Output
Subject 51% 77% 61%
Object 41% 73% 51%
Locative Arg. 18% 22% 22%
Temporal Arg. 55% 73% 63%
Reason 26% 47% 26%
Total 44% 68% 52%
Table 2: Performance figures for the answer extraction
system using automatic and manual transcriptions com-
pared with anchored recognition outputs.
towards enhancing the vocabulary with more candi-
date words that could contain the spoken words in
the region. (v) Then we perform recognition with
the new anchored network and extract the best path
through it. Thus we enforce partial alignment of
the audio with the question given, while the regu-
lar recognition LM is still used for the parts outside
the question.
5 Experiments and Results
We performed experiments using a set of questions
and broadcast audio documents released by LDC for
the DARPA-funded GALE project Phase 3. In this
dataset we have 482 questions (177 subject, 160 ob-
ject, 73 temporal argument, 49 locative argument,
and 23 reason) from 90 documents. The ASR word
error rate (WER) for the sentences from which the
questions are constructed is 37% with respect to
noisy closed captions. To factor out IR noise we as-
sumed that the target document is given.
Table 1 presents the performance of the sentence
extraction system using manual and automatic tran-
scriptions. As seen, the system is almost perfect
when there is no noise, however performance de-
grades about 12% with the ASR output.
The next set of experiments demonstrate the per-
formance of the answer extraction system when the
correct sentence is given using both automatic and
manual transcriptions. As seen from Table 2, the
answer extraction performance degrades by about
35% relative using the ASR output. However, using
the anchored recognition approach, this improves to
23%, reducing the effect of the ASR noise signifi-
cantly1 by more than 30% relative. This is shown
in the last column of this table, demonstrating the
use of the proposed approach. We observe that the
WER of the sentences for which we now get cor-
rected answers is reduced from 45% to 28% with
this approach, a reduction of 37% relative.
6 Conclusions
We have presented a question answering system
for querying spoken documents with a novel an-
chored speech recognition approach, which aims to
re-decode an utterance given the question. The pro-
posed approach significantly lowers the error rate for
answer extraction. Our future work involves han-
dling audio in foreign languages, that is robust to
both ASR and machine translation noise.
Acknowledgments: This work was funded by DARPA un-
der contract No. HR0011-06-C-0023. Any conclusions or rec-
ommendations are those of the authors and do not necessarily
reflect the views of DARPA.
References
M. Finke and A. Waibel. 1997. Flexible transcription
alignment. In Proceedings of the IEEE ASRU Work-
shop, Santa Barbara, CA.
M. Harper and Z. Huang. Chinese Statistical Parsing,
chapter To appear.
L. Lamel, S. Rosset, C. Ayache, D. Mostefa, J. Turmo,
and P. Comas. 2008. Question answering on speech
transcriptions: the qast evaluation in clef. In Proceed-
ings of the LREC, Marrakech, Morocco.
A. Stolcke, B. Chen, H. Franco, V. R. R. Gadde,
M. Graciarena, M.-Y. Hwang, K. Kirchhoff, N. Mor-
gan, X. Lin, T. Ng, M. Ostendorf, K. Sonmez,
A. Venkataraman, D. Vergyri, W. Wang, J. Zheng,
and Q. Zhu. 2006. Recent innovations in speech-
to-text transcription at SRI-ICSI-UW. IEEE Trans-
actions on Audio, Speech, and Language Processing,
14(5):1729?1744, September.
E. W. D. Whittaker, J. Mrozinski, and S. Furui. 2006.
Factoid question answering with web, mobile and
speech interfaces. In Proceedings of the NAACL/HLT,
Morristown, NJ.
1according to the Z-test with 0.95 confidence interval
268
A Statistical Constraint Dependency Grammar (CDG) Parser
Wen Wang
Speech Technology and Research Lab
SRI International
Menlo Park, CA 94025,
U.S.A.,
wwang@speech.sri.com
Mary P. Harper
Electrical and Computer Engineering
Purdue University
West Lafayette, IN 47907-1285,
U.S.A.,
harper@ecn.purdue.edu
Abstract
CDG represents a sentence?s grammatical structure
as assignments of dependency relations to func-
tional variables associated with each word in the
sentence. In this paper, we describe a statistical
CDG (SCDG) parser that performs parsing incre-
mentally and evaluate it on the Wall Street Jour-
nal Penn Treebank. Using a tight integration of
multiple knowledge sources, together with distance
modeling and synergistic dependencies, this parser
achieves a parsing accuracy comparable to several
state-of-the-art context-free grammar (CFG) based
statistical parsers using a dependency-based eval-
uation metric. Factors contributing to the SCDG
parser?s performance are analyzed.
1 Introduction
Statistical parsing has been an important focus of
recent research (Magerman, 1995; Eisner, 1996;
Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999;
Charniak, 2000). Several of these parsers gen-
erate constituents by conditioning probabilities on
non-terminal labels, part-of-speech (POS) tags, and
some headword information (Collins, 1999; Rat-
naparkhi, 1999; Charniak, 2000). They utilize
non-terminals that go beyond the level of a sin-
gle word and do not explicitly use lexical fea-
tures. Collins? Model 2 parser (1999) learns the
distinction between complements and adjuncts by
using heuristics during training, distinguishes com-
plement and adjunct non-terminals, and includes
a probabilistic choice of left and right subcate-
gorization frames, while his Model 3 parser uses
gap features to model wh-movement. Charniak
(Charniak, 2000) developed a state-of-the-art sta-
tistical CFG parser and then built an effective lan-
guage model based on it (Charniak, 2001). But
his parser and language model were originally de-
signed to analyze complete sentences. Among the
statistical dependency grammar parsers, Eisner?s
(1996) best probabilistic dependency model used
unlabeled links between words and their heads, as
well as between words and their complements and
adjuncts. However, the parser does not distinguish
between complements and adjuncts or model wh-
movement. Collins? bilexical dependency grammar
parser (1999) used head-modifier relations between
pairs of words much as in a dependency grammar,
but they are limited to relationships between words
in reduced sentences with base NPs.
Our research interest focuses on building a high
quality statistical parser for language modeling. We
chose CDG as the underlying grammar for several
reasons. Since CDGs can be lexicalized at the word-
level, a CDG parser-based language model is an
important alternative to CFG parser-based models,
which must model both non-terminals and termi-
nals. Furthermore, the lexicalization of CDG parse
rules is able to include not only lexical category in-
formation, but also a rich set of lexical features to
model subcategorization and wh-movement. By us-
ing CDG, our statistical model is able to distinguish
between adjuncts and complements. Additionally,
CDG is more powerful than CFG and is able to
model languages with crossing dependencies and
free word ordering.
In this paper, we describe and evaluate a statisti-
cal CDG parser for which the probabilities of parse
prefix hypotheses are incrementally updated when
the next input word is available, i.e., it parses in-
crementally. Section 2 describes how CDG repre-
sents a sentence?s parse and then defines a Super-
ARV, which is a lexicalization of CDG parse rules
used in our parsing model. Section 3 presents the
parsing model, while Section 4 motivates the eval-
uation metric used to evaluate our parser. Section 5
presents and discusses the experimental results.
2 CDG Parsing
CDG (Harper and Helzerman, 1995) represents syn-
tactic structures using labeled dependencies be-
tween words. Consider an example CDG parse for
the sentence What did you learn depicted in the
white box of Figure 1. Each word in the parse has a
lexical category, a set of feature values, and a set of
roles that are assigned role values, each comprised
of a label indicating the grammatical role of the
word and its modifiee (i.e., the position of the word
it is modifying when it takes on that role). Consider
the role value assigned to the governor role (denoted
G) of you, np-2. The label np indicates the gram-
matical function of you when it is governed by its
head in position 2. Every word in a sentence must
have a governor role with an assigned role value.
Need roles are used to ensure that the grammatical
requirements of a word are met (e.g., subcategoriza-
tion).
pronoun
case=common
behavior=nominal
type=interrogative
agr=3s
G=np-4
verb
subcat=base
verbtype=past
voice=active
inverted=yes
type=none
gapp=yes
mood=whquestion
agr=all
G=vp-1
Need1=S-3
Need2=S-4
Need3=S-2
pronoun
case=common
behavior=nominal
type=personal
agr=2s
G=np-2
          1
        what
          2
        did
           3
        you
The SuperARV of the word "did":
 Category: Verb
           4
        learn
verb
subcat=obj
vtype=infinitive
voice=active
inverted=no
type=none
gapp=yes
mood=whquestion
agr=none
G=vp-2
Need1=S-4
Need2=S-1
Need3=S-4
 Features: {verbtype=past, voice=active, inverted=yes, 
 gapp=yes,mood=whquestion,agr=all}
 Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)
 Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)
 Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)
 Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)
 Dependent Positional Constraints:
 MX[G] < PX = MX[Need3] < MX[Need1] 
 < MX[Need2] MC
}
n
ee
d 
ro
le
 
co
n
st
ra
in
ts}}}CF }
}
(R,L,UC,MC)+
DC
Figure 1: An example of a CDG parse and the Super-
ARV of the word did in the sentence what did you learn.
PX and MX([R]) represent the position of a word and its
modifiee (for role R), respectively.
Note that CDG parse information can be easily
lexicalized at the word level. This lexicalization is
able to include not only lexical category and syn-
tactic constraints, but also a rich set of lexical fea-
tures to model subcategorization and wh-movement
without a combinatorial explosion of the parametric
space (Wang and Harper, 2002). CDG can distin-
guish between adjuncts and complements due to the
use of need roles (Harper and Helzerman, 1995),
is more powerful than CFG, and has the ability to
model languages with crossing dependencies and
free word ordering (hence, this research could be
applicable to a wide variety of languages).
An almost-parsing LM based on CDG has been
developed in (Wang and Harper, 2002). The un-
derlying hidden event of this LM is a SuperARV.
A SuperARV is formally defined as a four-tuple for
a word, ?C, F , (R, L, UC, MC)+, DC?, where C
is the lexical category of the word, F = {Fname1
= Fvalue1, . . . , FNamef = FV aluef} is a fea-
ture vector (where Fnamei is the name of a feature
and Fvaluei is its corresponding value), DC repre-
sents the relative ordering of the positions of a word
and all of its modifiees, (R, L, UC, MC)+ is a list
of one or more four-tuples, each representing an ab-
straction of a role value assignment, where R is a
role variable, L is a functionality label, UC repre-
sents the relative position relation of a word and its
dependent, and MC encodes some modifiee con-
straints, namely, the lexical category of the modifiee
for this dependency relation. The gray box of Figure
1 presents an example of a SuperARV for the word
did. From this example, it is easy to see that a Su-
perARV is a join on the role value assignments of a
word, with explicit position information replaced by
a relation that expresses whether the modifiee points
to the current word, a previous word, or a subse-
quent word. The SuperARV structure provides an
explicit way to organize information concerning one
consistent set of dependency links for a word that
can be directly derived from a CDG parse. Super-
ARVs encode lexical information as well as syntac-
tic and semantic constraints in a uniform represen-
tation that is much more fine-grained than part-of-
speech (POS). A sentence tagged with SuperARVs
is an almost-parse since all that remains is to spec-
ify the precise position of each modifiee. SuperARV
LMs have been effective at reducing word error rate
(WER) on wide variety of continuous speech recog-
nition (CSR) tasks, including Wall Street Journal
(Wang and Harper, 2002), Broadcast News (Wang
et al, 2003), and Switchboard tasks (Wang et al,
2004).
3 SCDG Parser
3.1 The Basic Parsing Algorithm
Our SCDG parser is a probabilistic generative
model. It can be viewed as consisting of two com-
ponents: SuperARV tagging and modifiee determi-
nation. These two steps can be either loosely or
tightly integrated. To simplify discussion, we de-
scribe the loosely integrated version, but we imple-
ment and evaluate both strategies. The basic parsing
algorithm for the loosely integrated case is summa-
rized in Figure 2, with the algorithm?s symbols de-
fined in Table 1. In the first step, the top N-best
SuperARV assignments are generated for an input
sentence w1, . . . , wn using token-passing (Young
et al, 1997) on a Hidden Markov Model with tri-
gram probabilistic estimations for both transition
and emission probabilities. Each SuperARV se-
quence for the sentence is represented as a sequence
of tuples: ?w1, s1?, . . . , ?wn, sn?, where ?wk, sk?
represents the word wk and its SuperARV assign-
ment sk. These assignments are stored in a stack
ranked in non-increasing order by tag assignment
probability.
During the second step, the modifiees are statis-
tically specified in a left-to-right manner. Note that
the algorithm utilizes modifiee lexical category con-
straints to filter out candidates with mismatched lex-
ical categories. When processing the word wk, k =
1, . . . , n, the algorithm attempts to determine the
left dependents of wk from the closest to the far-
thest. The dependency assignment probability when
choosing the (c + 1)th left dependent (with its posi-
tion denoted dep(k,?(c + 1))) is defined as:
Pr(link(sdep(k,?(c+1)), sk,?(c + 1)|syn,H))
where H = ?w, s?k, ?w, s?dep(k,?(c+1)), ?w, s?dep(k,?c)dep(k,?1).
The dependency assignment probability is con-
ditioned on the word identity and SuperARV
assignment of wk and wdep(k,?(c+1)) as well as
all of the c previously chosen left dependents
?w, s?dep(k,?c)dep(k,?1) for wk. A Boolean random variable
syn is used to model the synergistic relationship
between certain role pairs. This mechanism allows
us to elevate, for example, the probability that the
subject of a sentence wi is governed by a tensed
verb wj when the need role value of wj points to
wi as its subject. The syn value for a dependency
relation is determined heuristically based on the
lexical category, role name, and label information
of the two dependent words. After the algorithm
statistically specifies the left dependents for wk,
it must also determine whether wk could be the
(d+1)th right dependent of a previously seen word
wp, p = 1, . . . , k ? 1 (where d denotes the number
of already assigned right dependents of wp), as
shown in Figure 2.
After processing word wk in each partial parse on
the stack, the partial parses are re-ranked according
to their updated probabilities. This procedure is it-
erated until the top parse in the stack covers the en-
tire sentence. For the tightly coupled parser, the Su-
perARV assignment to a word and specification of
its modifiees are integrated into a single step. The
parsing procedure, which is completely incremen-
tal, is implemented as a simple best-first stack-based
search. To control time and memory complexity, we
used two pruning thresholds: maximum stack depth
and maximum difference between the log proba-
bilities of the top and bottom partial parses in the
stack. These pruning thresholds are tuned based on
the tradeoff of time/memory complexity and pars-
ing accuracy on a heldout set, and they both have
hard limits.
Note the maximum likelihood estimation of de-
pendency assignment probabilities in the basic
loosely coupled parsing algorithm presented in Fig-
ure 2 is likely to suffer from data sparsity, and the
estimates for the tightly coupled algorithm are likely
to suffer even more so. Hence, we smooth the prob-
abilities using Jelinek-Mercer smoothing (Jelinek,
1997), as described in (Wang and Harper, 2003;
Wang, 2003).
3.2 Additions to the Basic Model
Some additional features are added to the basic
model because of their potential to improve SCDG
parsing accuracy. Their efficacy is evaluated in Sec-
tion 5.
Modeling crossing dependencies: The basic pars-
ing algorithm was implemented to preclude cross-
ing dependencies; however, it is important to allow
them in order to model wh-movement in some cases
(e.g., wh-PPs).
Distance and barriers between dependents: Be-
cause distance between two dependent words is
an important factor in determining the modifiees
of a word, we evaluate an alternative model that
adds distance, ?dep(k,?(c+1)),k to H in Figure 2.
Note that ?dep(k,?(c+1)),k represents the distance
between position dep(k,?(c + 1)) and k. To avoid
data sparsity problems, distance is bucketed and a
discrete random variable is used to model it. We
also model punctuation and verbs based on prior
work. Like (Collins, 1999), we also found that
verbs appear to act as barriers that impact modifiee
links. Hence, a Boolean random variable that rep-
resents whether there is a verb between the depen-
dencies is added to condition the probability esti-
mations. Punctuation is treated similarly to coordi-
nation constructions with punctuation governed by
the headword of the following phrase, and heuris-
tic questions on punctuation were used to provide
additional constraints on dependency assignments
(Wang, 2003).
Modifiee lexical features: The SuperARV struc-
ture employed in the SuperARV LM (Wang and
Harper, 2002) uses only lexical categories of mod-
ifiees as modifiee constraints. In previous work
(Harper et al, 2001), modifiee lexical features were
central to increasing the selectivity of a CDG.
Hence, we have developed methods to add ad-
ditional relevant lexical features to modifiee con-
straints of a SuperARV structure (Wang, 2003).
4 Parsing Evaluation Metric
To evaluate our parser, which generates CDG anal-
yses rather than CFG constituent bracketing, we
Table 1: Definitions of symbols used in the basic parsing algorithm.
Term Denotes
L(sk), R(sk) all dependents of sk to the left and right of wk, respectively
N(L(sk)), N(R(sk)) the number of left and right dependents of sk, respectively
dep(k,?c), dep(k, c) cth left dependent and right dependent of sk, respectively
dep(k,?1), dep(k, 1) the position of the closest left dependent and right dependent of sk, respectively
dep(k,?N(L(sk))), dep(k, N(L(sk))) the position of the farthest left dependent and right dependent of sk, respectively
Cat(sk) the lexical category of sk
ModCat(sk,?c), ModCat(sk, c) the lexical category of sk?s cth left and right dependent (encoded in the SuperARV
structure), respectively
link(si, sj , k) the dependency relation between SuperARV si and sj with wi assigned as the kth
dependent of sj , e.g., link(sdep(k,?(c+1)), sk,?(c + 1)) indicates that
wdep(k,?(c+1)) is the (c + 1)th left dependent of sk.
D(L(sk)), D(R(sk))) the number of left and right dependents of sk already assigned, respectively
?w, s?dep(k,?c)dep(k,?1) words and SuperARVs of sk?s closest left dependent up to its cth left dependent
?w, s?dep(k,c)dep(k,1) words and SuperARVs of sk?s closest right dependent up to its cth right dependent
syn a random variable denoting the synergistic relation between some dependents
can either convert the CDG parses to CFG brack-
eting and then use PARSEVAL, or convert the CFG
bracketing generated from the gold standard CFG
parses to CDG parses and then use a metric based on
dependency links. Since our parser is trained using
a CFG-to-CDG transformer (Wang, 2003), which
maps a CFG parse tree to a unique CDG parse,
it is sensible to evaluate our parser?s accuracy us-
ing gold standard CDG parse relations. Further-
more, in the 1998 Johns Hopkins Summer work-
shop final report (Hajic et al, 1998), Collins et al
pointed out that in general the mapping from de-
pendencies to tree structures is one-to-many: there
are many possible trees that can be generated for
a given dependency structure since, although gen-
erally trees in the Penn Treebank corpus are quite
flat, they are not consistently ?flat.? This variability
adds a non-deterministic aspect to the mapping from
CDG dependencies to CFG parse trees that could
cause spurious PARSEVAL scoring errors. Addi-
tionally, when there are crossing dependencies, then
no tree can be generated for that set of dependen-
cies. Consequently, we have opted to use a trans-
former to convert CFG trees to CDG parses and de-
fine a new dependency-based metric adapted from
(Eisner, 1996). We define role value labeled pre-
cision (RLP) and role value labeled recall (RLR)
on dependency links as follows:
RLP = correct modifiee assignments
number of modifiees our parser found
RLR = correct modifiee assignments
number of modifiess in the gold test set parses
where a correct modifiee assignment for a word
wi in a sentence means that a three-tuple
?role id, role label, modifiee word position? (i.e.,
a role value) for wi is the same as the three-tuple
role value for the corresponding role id of wi in the
gold test parse. This differs from Eisner?s (1996)
precision and recall metrics which use no label in-
formation and score only parent (governor) assign-
ments, as in traditional dependency grammars. We
will evaluate role value labeled precision and recall
on all roles of the parse, as well as the governor-
only portion of a parse. Eisner (Eisner, 1996) and
Lin (Lin, 1995) argued that dependency link eval-
uation metrics are valuable for comparing parsers
since they are less sensitive than PARSEVAL to sin-
gle misattachment errors that may cause significant
error propagation to other constituents. This, to-
gether with the fact that we must train our parser
using CDG parses generated in a lossy manner from
a CFG treebank, we chose to use RLP and RLR to
compare our parsing accuracy with several state-of-
the-art parsers.
5 Evaluation and Discussion
All of the evaluations were performed on the Wall
Street Journal Penn Treebank task. Following the
traditional data setup, sections 02-21 are used for
training our parser, section 23 is used for testing,
and section 24 is used as the development set for pa-
rameter tuning and debugging. As in (Ratnaparkhi,
1999; Charniak, 2000; Collins, 1999), we evaluate
on all sentences with length ? 40 words (2,245 sen-
tences) and length ? 100 words (2,416 sentences).
For training our probabilistic CDG parser on this
task, the CFG bracketing of the training set is trans-
BASIC PARSING ALGORITHM
1. Using SuperARV tagging on word sequence w1, . . . , wn, obtain a set of N-best SuperARV sequences with each
element consisting of n (word, SuperARV) tuples, denoted ?w1, s1?, . . . , ?wn, sn?, which we will call an assignment.
2. For each SuperARV assignment, initialize the stack of parse prefixes with this assignment:
/? From left-to-right, process each ?word, tag? of the assignment and generate parse prefixes ?/
for k : = 1, n do
/? Step a: ?/
/* decide left dependents of ?wk, sk? from the nearest to the farthest */
for c from 0 to N(L(sk)) ? 1 do
/? Choose a position for the (c + 1)th left dependent of ?wk, sk? from the set of possible positions
C = {1, . . . , dep(k,?c) ? 1}. The position choice is denoted dep(k,?(c + 1)) ? /
/? In the following equations, different left dependent assignments will generate
different parse prefixes, each of which is stored in the stack ? /
for each dep(k,?(c + 1)) from positions C = {1, . . . , dep(k,?c) ? 1}
/? Check whether the lexical category of the choice matches the modifiee lexical
category of the (c + 1)th left dependent of ?wk, sk? ? /
if Cat(sdep(k,?(c+1))) == ModCat(sk,?(c + 1)) then
Pr(T ) : = Pr(T ) ? Pr(link(sdep(k,?(c+1)), sk,?(c + 1)|syn,H))
where H = ?w, s?k, ?w, s?dep(k,?(c+1)), ?w, s?dep(k,?c)dep(k,?1)
/? End of choosing left dependents of ?wk, sk? for this parse prefix ?/
/? Step b: ?/
/? For the word/tag pair ?wk, sk?, check whether it could be a right dependent of any previously
seen word within a parse prefix of ?w1, s1?, . . . , ?wk?1, sk?1? ?/
for p : = 1, k ? 1 do
/? If ?wp, sp? still has right dependents left unspecified, then try out?wk, sk? as a right dependent */
if D(R(sp)) 6= N(R(sp)) then
d : = D(R(sp))
/? If the lexical category of ?wk, sk? matches the modifiee lexical category of the(d + 1)th right
dependent of ?wp, sp?, then sk might be ?wp, sp??s (d + 1)th right dependent ? /
if Cat(sk) == ModCat(sp, d + 1) then
Pr(T ) : = Pr(T ) ? Pr(link(sk, sp, d + 1)|syn,H), where H = ?w, s?p, ?w, s?k, ?w, s?dep(p,d)dep(p,1)
Sort the parse prefixes in the stack according to logPr(T ) and apply pruning using the thresholds.
3. After processing w1, . . . , wn, pick the parse with the highest logPr(T ) in the stack as the parse for that sentence.
Figure 2: The basic loosely coupled parsing algorithm. Note the algorithm updates the probabilities of parse
prefix hypotheses incrementally when processing each input word.
formed into CDG annotations using a CFG-to-CDG
transformer (Wang, 2003). Note that the sound-
ness of the CFG-to-CDG transformer was evaluated
by examining the CDG parses generated from the
transformer on the Penn Treebank development set
to ensure that they were correct given our grammar
definition.
5.1 Contribution of Model Factors
First, we investigate the contribution of the model
additions described in Section 3 to parse accuracy.
Since these factors are independent of the coupling
between the SuperARV tagger and modifiee spec-
ification, we investigate their impact on a loosely
integrated SCDG parser by comparing four models:
(1) the basic loosely integrated model; (2) the ba-
sic model with crossing dependencies; (3) model 2
with distance and barrier information; (4) model 3
with SuperARVs augmented with additional modi-
fiee lexical feature constraints. Each model uses a
trigram SuperARV tagger to generate 40-best Su-
perARV sequences prior to modifiee specification.
Table 2 shows the results for each of the four models
including SuperARV tagging accuracy (%) and role
value labeled precision and recall (%). Allowing
crossing dependencies improves the overall parsing
accuracy, but using distance information with verb
barrier and punctuation heuristics produces an even
greater improvement especially on the longer sen-
tences. The accuracy is further improved by the ad-
ditional modifiee lexical feature constraints added to
the SuperARVs. Note that RLR is lower than RLP
in these investigations possibly due to SuperARV
tagging errors and the use of a tight stack pruning
threshold.
Next, we evaluate the impact of increasing the
context of the SuperARV tagger to a 4-gram while
increasing the size of the N-best list passed from
the tagger to the modifiee specification step of the
parser. For this evaluation, we use model (4)
Table 2: Results on Section 23 of the WSJ Penn Tree-
bank for four loosely-coupled model variations. The
evaluation metrics, RLR and RLP, are our dependency-
based role value labeled precision and recall. Note:
Model (1) denotes the basic model, Model (2) de-
notes (1)+crossing dependencies, Model (3) denotes
(2)+distance (punctuation) model, and Model (4) denotes
(3)+modifiee lexical features.
Models ? 40 words (2,245 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
(1) 94.7 90.6 90.3 86.8 86.2
(2) 95.0 90.7 90.5 87.0 86.5
(3) 95.7 91.1 90.9 87.4 87.0
(4) 96.2 91.5 91.2 88.0 87.4
Models ? 100 words (2,416 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
(1) 94.0 89.7 89.3 86.0 85.5
(2) 94.2 89.9 89.6 86.2 85.8
(3) 94.7 90.4 90.2 86.8 86.3
(4) 95.4 90.9 90.5 87.5 86.8
from Table 2, the most accurate model so far. We
also evaluate whether a tight integration of left-
to-right SuperARV tagging and modifiee specifica-
tion produces a greater parsing accuracy than the
best loosely coupled counterpart. Table 3 shows
the SuperARV tagging accuracy (%) and role value
labeled precision and recall (%) for each model.
Consistent with our intuition, a stronger SuperARV
tagger and a larger search space of SuperARV se-
quences produces greater parse accuracy. However,
tightly integrating SuperARV prediction with mod-
ifiee specification achieves the greatest overall ac-
curacy. Note that SuperARV tagging accuracy and
parse accuracy improve in tandem, as can be seen
in Tables 2 and 3. These results are consistent
with the observations of (Collins, 1999) and (Eis-
ner, 1996). It is important to note that each of the
factors contributing to improved parse accuracy in
these two experiments also improved the word pre-
diction capability of the corresponding parser-based
LM (Wang and Harper, 2003).
5.2 Comparing to Other Parsers
Charniak?s state-of-the-art PCFG parser (Charniak,
2000) has achieved the highest PARSEVAL LP/LR
when compared to Collins? Model 2 and Model
3 (Collins, 1999), Roark?s (Roark, 2001), Ratna-
parkhi?s (Ratnaparkhi, 1999), and Xu & Chelba?s
(Xu et al, 2002) parsers. Hence, we will com-
pare our best loosely integrated and tightly inte-
grated SCDG parsers to Charniak?s parser. Ad-
ditionally, we will compare with Collins? Model
Table 3: Results on Section 23 of the WSJ Penn Tree-
bank comparing models that utilize different SuperARV
taggers and N-best sizes with the tightly coupled imple-
mentation. Note L denotes Loose coupling and T de-
notes Tight coupling. Also (a) denotes trigram, 40-best;
(b) denotes trigram, 100-best; (c) denotes 4-gram, 40-
best; (d) denotes 4-gram, 100-best.
Models ? 40 words (2,245 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
L (a) 96.2 91.5 91.2 88.0 87.4
(b) 96.7 91.9 91.5 88.3 87.7
(c) 96.9 92.2 91.7 88.6 88.1
(d) 97.2 92.4 92.3 89.1 88.6
T 97.4 93.2 92.9 89.8 89.2
Models ? 100 words (2,416 sentences)
Tagging governor only all roles
Acc. RLP RLR RLP RLR
L (a) 95.4 90.9 90.5 87.5 86.8
(b) 95.8 91.3 90.8 87.7 87.0
(c) 96.0 91.7 91.2 88.0 87.4
(d) 96.3 91.8 91.5 88.5 87.8
T 96.6 92.6 92.2 89.1 88.5
2 since it makes the complement/adjunct distinc-
tion and Model 3 since it handles wh-movement
(Collins, 1999). Charniak?s parser does not explic-
itly model these phenomena.
Among the statistical CFG parsers to be com-
pared, only Collins? Model 3 produces trees with
information about wh-movement. Since the trans-
former uses empty node information to transform
the CFG parse trees to CDG parses, the accuracy
of Charniak?s parser and Collins? Model 2 may be
slightly reduced for sentences with empty nodes.
Hence, we compare results on two test sets: one that
omits all sentences with traces and one that does not.
As can be seen in Table 4, our tightly coupled parser
consistently produces an accuracy that equals or ex-
ceeds the accuracies of the other parsers, with one
exception (Collins? Model 3), regardless of whether
the test set contains sentences with traces.
Using our evaluation metrics, Collins? Model 3
achieves a better precision/recall than Model 2 and
Charniak?s parser. Since trace information is used
by the CFG-to-CDG transformer to generate cer-
tain lexical features (Wang, 2003), the output from
Model 3 is likely to be mapped to more accu-
rate CDG parses. Although Charniak?s maximum-
entropy inspired parser achieved the highest PAR-
SEVAL results, Collins? Model 3 is more accu-
rate using our dependency metric, possibly be-
cause it makes the complement/adjunct distinction
and models wh-movement. Since the statistical
Table 4: Evaluation of five models on Section 23 sentences with and without traces: L denotes the best loosely
coupled CDG parser and T the tightly coupled CDG parser.
Models ? 40 words (2,245 sentences)
Without TRACE All
(1,903 sentences) (2,245 sentences)
governor only all roles governor only all roles
RLP RLR RLP RLR RLP RLR RLP RLR
L 92.4 92.4 89.5 88.7 92.4 92.3 89.1 88.6
T 93.2 92.9 89.9 89.3 93.2 92.9 89.8 89.2
Charniak (Charniak, 2000) 92.6 92.5 89.4 88.9 92.5 92.3 88.9 88.7
Collins, Model 2 (Collins, 1999) 92.5 92.3 89.1 88.5 92.2 92.1 89.0 88.5
Collins, Model 3 (Collins, 1999) 92.8 92.7 89.9 89.4 92.7 92.4 89.3 89.1
Models ? 100 words (2,416 sentences)
Without TRACE All
(1,979 sentences) (2,416 sentences)
governor only all roles governor only all roles
RLP RLR RLP RLR RLP RLR RLP RLR
L 91.9 91.6 88.8 88.1 91.8 91.5 88.5 87.8
T 92.7 92.3 89.4 88.7 92.6 92.2 89.1 88.5
Charniak (Charniak, 2000) 92.0 91.8 88.8 88.2 91.9 91.6 88.4 87.9
Collins, Model 2 (Collins, 1999) 91.8 91.6 88.6 88.0 91.7 91.5 88.2 87.9
Collins, Model 3 (Collins, 1999) 92.2 92.1 89.4 88.8 92.1 91.9 88.8 88.5
CFG parsers may loose accuracy from the CFG-to-
CDG transformation, similarly to Collins? experi-
ment reported in (Hajic et al, 1998), we also trans-
formed our CDG parses to Penn Treebank style
CFG parse trees and scored them using PARSE-
VAL. On the WSJ PTB test set, Charniak?s parser
achieved 89.6% LR and 89.5% LP, Collins? Model 2
and 3 obtained 88.1% LR and 88.3% LP and 88.0%
LR and 88.3% LP, while the tightly coupled CDG
parser obtains 85.8% LR and 86.4% LP. It is im-
portant to remember that this score is impacted by
two lossy conversions, one for training and one for
testing.
We have conducted a non-parametric Monte
Carlo test to determine the significance of the differ-
ences between the parsing accuracy results in Table
3 and Table 4. We found that the difference between
the tightly and loosely coupled SCDG parsers is sta-
tistically significant, as well as the difference be-
tween the SCDG parser and Charniak?s parser and
Collins? Model 2. Although the difference between
our parser and Collins? Model 3 is not statistically
significant, our parser represents a first attempt to
build a high quality SCDG parser, and there is still
room for improvement, e.g., better handling of bar-
riers (including punctuation) and employing more
sophisticated search and pruning strategies.
This paper has presented a statistical implemen-
tation of a CDG parser, which is both genera-
tive and highly lexicalized. With a framework
of tightly integrated, multiple knowledge sources,
model distance, and synergistic dependencies, we
have achieved a parsing accuracy comparable to the
state-of-the-art statistical parsers trained on the Wall
Street Journal Penn Treebank corpus. However,
more work must be done to build a parser model
capable of coping with speech disfluencies present
in spontaneous speech. We also intend to investi-
gate a hybrid parser that combines the generality of
a CFG with the specificity of a CDG.
References
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial In-
telligence.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of the First Annual Meeting
of the North American Association for Computational
Linguistics.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL?2001.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
J. M. Eisner. 1996. An empirical comparison of prob-
ability models for dependency grammar. Technical
report, University of Pennsylvania, CIS Department,
Philadelphia PA 19104-6389.
J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann, and
D. Zeman. 1998. Core natural language processing
technology applicable to multiple languages ? Work-
shop ?98. Technical report, Johns Hopkins Univ.
M. P. Harper and R. A. Helzerman. 1995. Extensions
to constraint dependency parsing for spoken language
processing. Computer Speech and Language.
M. P. Harper, W. Wang, and C. M. White. 2001. Ap-
proaches for learning constraint dependency grammar
from corpora. In Proceedings of the Grammar and
Natural Language Processing Conference, Montreal,
Canada.
F. Jelinek. 1997. Statistical Methods For Speech Recog-
nition. The MIT Press.
D. Lin. 1995. A dependency-based method for evaluat-
ing broad-coverage parsers. In Proceedings of the In-
ternational Joint Conference on Artificial Intelligence,
pages 1420?1427.
D. M. Magerman. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 276?283.
A. Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine
Learning, 34:151?175.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
W. Wang and M. P. Harper. 2002. The SuperARV lan-
guage model: Investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proceed-
ings of Conference of Empirical Methods in Natural
Language Processing.
W. Wang and M. P. Harper. 2003. Language model-
ing using a statistical dependency grammar parser. In
Proceedings of International Workshop on Automatic
Speech Recognition and Understanding.
W. Wang, M. P. Harper, and A. Stolcke. 2003. The ro-
bustness of an almost-parsing language model given
errorful training data. In ICASSP 2003.
W. Wang, A. Stolcke, and M. P. Harper. 2004. The use
of a linguistically motivated language model in con-
versational speech recognition. In ICASSP 2004.
W. Wang. 2003. Statistical Parsing and Language Mod-
eling based on Constraint Dependency Grammar.
Ph.D. thesis, Purdue University.
P. Xu, C. Chelba, and F. Jelinek. 2002. A study on richer
syntactic dependencies for structured language mod-
eling. In Proceedings of ACL 2002.
S. J. Young, J. Odell, D. Ollason, V. Valtchev, and P. C.
Woodland, 1997. The HTK Book. Entropic Cam-
bridge Research Laboratory, Ltd.
The Effectiveness of Corpus-Induced Dependency Grammars for 
Post-processing Speech* 
M.  P. Harper ,  C.  M .  Whi te ,  W.  Wang,  M.  T .  Johnson ,  and  R .  A.  He lzerman 
School of Electrical and Computer  Engineer ing 
Purdue University 
West Lafayette,  IN 47907-1285 
{ harper, robot, wang28, m johnson, helz} @ecn.purdue. du 
Abst rac t  
This paper investigates the impact of Constraint 
Dependency Grammars (CDG) on the accuracy of 
an integrated speech recognition and CDG pars- 
ing system. We compare a conventional CDG with 
CDGs that are induced from annotated sentences 
and template-expanded s ntences. The grammars 
are evaluated on parsing speed, precision/coverage, 
and improvement of word and sentence accuracy of 
the integrated system. Sentence-derived CDGs sig- 
nificantly improve recognition accuracy over the con- 
ventional CDG but are less general. Expanding the 
sentences with templates provides us with a mech- 
anism for increasing the coverage of the grammar 
with only minor reductions in recognition accuracy. 
1 Background 
The question of when and how to integrate language 
models with speech recognition systems i gaining in 
importance as recognition tasks investigated by the 
speech community become increasingly more chal- 
lenging and as speech recognizers are used in hu- 
man/computer interfaces and dialog systems (Block, 
1997; Pieraccini and Levin, 1992; Schmid, 1994; 
Wright et al, 1994; Zue et al, 1991). Many sys- 
tems tightly integrate N-gram stochastic language 
models, with a power limited to a regular grammar, 
into the recognizer (Jeanrenaud et al, 1995; Ney et 
al., 1994; Placeway et al, 1993) to build more ac- 
curate speech recognizers. However, in order to act 
based on the spoken interaction with the user, the 
speech signal must be mapped to an internal repre- 
sentation. Obtaining a syntactic representation for 
the spoken utterance has a high degree of utility for 
mapping to a semantic representation. Without a 
structural analysis of an input, it is difficult o guar- 
antee the correctness ofthe mapping from a sentence 
to its interpretation (e.g., mathematical expressions 
to internal calculations). We believe that significant 
additional improvement in accuracy can be gained 
in specific domains by using a more complex lan- 
* This research was supported by grants from Intel, Purdue 
Research Foundation, and National Science Foundation IRI 
97-04358, CDA 96-17388, and ~9980054-BCS. 
guage model that combines syntactic, semantic, and 
domain knowledge. 
A language processing module that is more pow- 
erful than a regular grammar can be loosely, mod- 
erately, or tightly integrated with the spoken lan- 
guage system, and there are advantages and dis- 
advantages associated with each choice (Harper et 
al., 1994). To tightly integrate a language model 
with the power of a context-free grammar with the 
acoustic module requires that the power of the two 
modules be matched, making the integrated system 
fairly intractable and difficult to train. By separat- 
ing the language model from the acoustic model, it 
becomes possible to use a more powerful anguage 
model without increasing computational costs or the 
amount of acoustic training data required by the rec- 
ognizer. Furthermore, a loosely-integrated language 
model can be developed independently of the speech 
recognition component, which is clearly an advan- 
tage. Decoupling the acoustic and language mod- 
els also adds flexibility: a wide variety of language 
models can be tried with a single acoustic model. 
Systems that utilize a language model that operates 
as a post-processor to a speech recognizer include 
(Block, 1997; Seneff, 1992; Zue et al, 1991). 
The goal of this research is to construct and ex- 
perimentally evaluate a prototype of a spoken lan- 
guage system that loosely integrates a speech recog- 
nition component with an NLP component that uses 
syntactic, semantic, and domain-specific knowledge 
to more accurately select the sentence uttered by a 
speaker. First we describe the system we have built. 
Then we describe the mechanism used to rapidly de- 
velop a domain-specific grammar that improves ac- 
curacy of our speech recognizer. 
2 Our  Sys tem 
We have developed the prototype spoken language 
system depicted in Figure 1 that integrates a speech 
recognition component based on HMMs with a pow- 
erful grammar model based on Constraint Depen- 
dency Grammar (CDG). The speech recognizer is 
implemented as a multiple-mixture triphone HMM 
with a simple integrated word co-occurrence gram- 
102 
mar (Ent, 1997; Young et al, 1997). Mel-scale cep- 
stral coefficients, energy, and each of their their first 
and second order differences are used as the under- 
lying feature vector for each speech frame. Model 
training is done using standard Baum-Welch Max- 
imum Likelihood parameter re-estimation on diag- 
onal covariance Gaussian Mixture Model (GMM) 
feature distributions. The speech recognizer em- 
ploys a token-passing version of the Viterbi algo- 
rithm (Young et al, 1989) and pruning settings to 
produce a pruned recognition lattice. This pruned 
lattice contains the most likely alternative sentences 
that account for the sounds present in an utterance 
as well as their probabilities. Without any loss of in- 
formation, this lattice is then compressed into a word 
graph (Harper et al, 1999b; Johnson and Harper, 
1999), which acts as the interface between the rec- 
ognizer and the CDG parser. The word graph algo- 
rithm begins with the recognition lattice and elim- 
inates identical subgraphs by iteratively combining 
word nodes that have exactly the same preceding 
or following nodes (as well as edge probabilities), 
pushing excess probability to adjacent nodes when- 
ever possible. The resulting word graph represents 
all possible word-level paths without eliminating or 
adding any paths or modifying their probabilities. 
Word graphs increase the bandwidth of useful acous- 
tic information passed from the HMM to the CDG 
parser compared to most current speech recognition 
systems. 
The CDG parser parses the word graph to identify 
the best sentence consistent with both the acoustics 
of the utterance and its own additional knowledge. 
The loose coupling of the parser with the HMM 
allows us to construct a more powerful combined 
system without increasing the amount of training 
data for the HMM or the computational complex- 
ity of either of the component modules. Our NLP 
component is implemented using a CDG parser 
(Harper and Helzerman, 1995; Maruyama, 1990a; 
Maruyama, 1990b) because of its power and flexibil- 
ity, in particular: 
? It supports the use of syntactic, semantic, and 
domain-specific knowledge in a uniform frame- 
work. 
? Our CDG parser supports efficient simultaneous 
parsing of alternative sentence hypotheses in a 
word graph (Harper and Helzerman, 1995; Helz- 
erman and Harper, 1996). 
* Because CDG is a dependency grammar, it can 
better model free-order languages. Hence, CDG 
can be used in processing a wider variety of human 
languages than other grammar paradigms. 
? It is capable of representing and using context- 
dependent information unlike traditional gram- 
mar approaches, thus providing a finer degree of 
control over the syntactic analysis of a sentence. 
? A CDG can be extracted irectly from sentences 
annotated with dependency information (i.e., fea- 
ture and syntactic relationships). 
We hypothesize that the accuracy of the combined 
HMM/CDG system should benefit from the ability 
to create a grammar that covers the domain as pre- 
cisely as possible and that does not consider sen- 
tences that would not make sense given the domain. 
A corpus-based grammar is likely to have this degree 
of control. In the next section we describe how we 
construct a CDG from corpora. 
Figure 1: Block diagram of the loosely-coupled spo- 
ken language system. 
3 Learn ing  CDG Ru les  
In this section, we introduce CDG and then describe 
how CDG constraints can be learned from sentences 
annotated with grammatical information. 
3.1 In t roduct ion  to  CDG 
Constraint Dependency Grammar (CDG), first 
introduced by Maruyama (Maruyama, 1990a; 
Maruyama, 1990b), uses constraints to determine 
the grammatical dependencies for a sentence. The 
parsing algorithm is framed as a constraint satis- 
faction problem: the rules are the constraints and 
the solutions are the parses. A CDG is defined as 
a five-tuple, (2E, R, L, C, T), where ~ = {a l , . . . ,  c%} 
is a finite set of lexical categories (e.g., determiner), 
R = {r l , . . . , rp}  is a finite set of uniquely named 
roles or role ids (e.g., governor, needl, need2), L = 
{ l l , . . . , lq}  is a finite set of labels (e.g., subject), 
C is a constraint formula, and T is a table that 
specifies allowable category-role-label combinations. 
A sentence s - WlW2W3. . .wn has length n and 
is an element of ~*. For each word wi E ~ of a 
sentence s, there are up to p different roles (with 
most words needing only one or two (Harper et al, 
1999a)), yielding a maximum of n * p roles for the 
entire sentence. A role is a variable that is assigned 
a role value, an element of the set L ? (1, 2 , . . . ,  n}. 
Role values are denoted as l-m, where l E L and 
m E (1, 2 , . . . ,  n} is called the modifiee. Maruyama 
originally used a modifiee of NIL to indicate that a 
role value does not require a modifiee, but it is more 
parsimonious to indicate that there is no dependent 
by setting the modifiee to the position of its word. 
Role values are assigned to roles to record the syn- 
tactic dependencies between words in the sentence. 
103 
The governor ole is assigned role values such that 
the modifiee of the word indicates the position of the 
word's governor or head (e.g., DET-3, when assigned 
to the governor ole of a determiner, indicates its 
function and the position of its head). Every word 
in a sentence has a governor ole. Need roles are 
used to ensure the requirements of a word are met. 
For example, an object is required by a verb that 
subcategorizes for one, unless it has passive voice. 
The required object is accounted for by requiring 
the verb's need role to be assigned a role value with 
a modifiee that points at the object. Words can 
have more than one need role, depending on the lex- 
ical category of the word. The table T indicates the 
roles that a word with a particular lexical category 
must support. 
A sentence s is said to be generated  by the gram- 
mar G if there exists an assignment A that maps a 
role value to each of the roles for s such that C is 
satisfied. There may be more than one assignment 
of role values to the roles of a sentence that satisfies 
C, in which case there is ambiguity. C is a first- 
order predicate calculus formula over all roles that 
requires that an assignment of role values to roles be 
consistent with the formula; those role values incon- 
sistent with C can be eliminated. A subformula P~ 
of C is a predicate involving =, <, or >, or predi- 
cates joined by the logical connectives and, or, i f ,  
or not. A subformula is a unary constraint if it con- 
tains only a single variable (by convention, we use 
zl) and a binary constraint if it contains two vari- 
ables (by convention zl and z2). An example of 
a unary and binary constraint appears in Figure 2. 
A CDG has an arity parameter a, which indicates 
the maximum number of variables in the subformu- 
las of C, and a degree parameter d, which is the 
number of roles in the grammar. An arity of two 
suffices to represent a grammar at least as power- 
ful as a context-free grammar (Maruyama, 1990a; 
Maruyama, 1990b). In (Harper et al, 1999a), we 
developed a way to write constraints concerning the 
category and feature values of a modifiee of a role 
value (or role value pair). These constraints loosely 
capture binary constraint information in unary con- 
straints (or beyond binary for binary constraints) 
and results in more efficient parsing. 
A u, liwy ?~nst\]llnt requiring that ? role vmluo IlmlgnlKI to the vernot role of I determiner 
have the label D~ lind ? modlflee pointing to ? lub4NIqtNl~t wocd* 
(if (and (= (category x 1) determiner) 
(= (rid x 1 ) G)) 
(and (= (label x 1 ) DET) 
(> (rood x 1 ) (pos Xl))) ) 
A binary oonatrllnt requiring that ? role vllue 
with the libel S Illlgned to ? ne~dl role of one 
word pOklt It Imother word whole governor 
role I= mml~gnened ? role veltm with the libel 8UBJ 
and ? rnodlflee that point? beck at the flrat word. 
(if (and (= (label x I ) S) 
(= (rid Xl) N1) 
(= (rnod Xl) (pos x2)) 
(= (rid x2) G)) 
(and (= (label x2) SUBJ) 
(= (rood x2) (pos xl)))) 
Figure 2: A Unary and binary constraint for CDG. 
The white box in Figure 3 depicts a parse for 
the sentence Clear the screen from the Resource 
Management corpus (Price et al, 1988) (the ARV 
and ARVP in the gray box will be discussed later), 
which is a corpus we will use to evaluate our speech 
processing system. We have constructed a conven- 
tional CDG with around 1,500 unary and binary 
constraints (i.e., its arity is 2) that were designed 
to parse the sentences in the corpus. This CDG 
covers a wide variety of grammar constructs (includ- 
ing conjunctions and wh-movement) and has a fairly 
rich semantics. It uses 16 lexical categories, 4 roles 
(so its degree is 4), 24 labels, and 13 lexical fea- 
ture types (subcat,  agr, case, vtype (e.g., progres- 
sive), mood, gap, inverted, voice, behavior (e.g., 
mass), type (e.g., interrogative, relative), semtype, 
takesdet ,  and conj type) .  The parse in Figure 3 is 
an assignment of role values to roles that is consis- 
tent with the unary and binary constraints. A role 
value, when assigned to a role, has access to not only 
the label and modifiee of its role value, but also the 
role name of the role to which it is assigned, informa- 
tion specific to the word (i.e., the word's position in 
the sentence, its lexical category, and feature values 
for each feature), and information about the lexical 
class and feature values of its modifiee. Our unary 
and binary constraints use this information to elim- 
inate ungrammatical ssignments. 
Parse for "Clear the screen" 
I 1 th2e-~ 3 Clear ~ n  
?a~=comlnon 
t vtype=lnf =ulxa~t3= I behav=count 
Nmt/~ram 
-G=root-1 G=de~3 
N2=S~3 N3=S-1 
{'~t l=det erm~ner, type1 ==definite, subcat l=count3s, ddl=G ?bell=de (< pOSXl) mod Xl) ) 
J"~tl---determiner, typel=definite, subcat 1--cour~3s, ddl=G, 
~ ~,bell=det, (< (pos Xl) (rood Xl)), Cat2=noun, c~se2=common, I 1 
t b e.hav2=count, type2=none, semty~2=display, gl2=3s, J\] 
\] rid2=G, label2==obi, (< (rood x2) (POs x2)), 
(rood x2) (pos Xl)), (= (rood xl) (pos x2)) J = 
Figure 3: A CDG parse (see white box) is repre- 
sented by the assignment of role values to roles as- 
sociated with a word with a specific lexical category 
and one feature value per feature. ARVs and ARVPs 
(see gray box) represent grammatical relations that 
can be extracted from a sentence's parse. 
3.2 Learn ing  CDG Const ra in ts  
The grammaticality of a sentence in a language de- 
fined by a CDG was originally determined by apply- 
ing the constraints of the grammar to the possible 
104 
role value assignments. If the set of all possible role 
values assigned to the roles of a sentence of length n 
is denotedS1 =Y;.x RxPOSxLxMODx Ftx  
.. .  x Fk, where k is the number of feature types, 
Fi represents the set of feature values for that type, 
POS = {1, 2 , . . . ,  n} is the set of possible positions, 
MOD = {1, 2 , . . . ,  n} is the set of possible modi- 
flees, and n is sentence length (which can be any 
arbitrary natural number), then unary constraints 
partition $1 into grammatical and ungrammatical 
role values. Similarly, binary constraints partition 
the set $2 = $1 x $1 = S~ into compatible and in- 
compatible pairs. Building upon this concept of role 
value partitioning, it is possible to construct another 
way of representing unary and binary constraints 
because CDG constraints do not need to reference 
the exact position of a word or a modifiee in the 
sentence to parse sentences (Harper and Helzerman, 
1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 
1994; Menzel, 1995). 
To represent he relative, rather than the abso- 
lute, position information for the role values in a 
grammatical sentence, it is only necessary to repre- 
sent the positional relations between the modifiees 
and the positions of the role values. To support an 
arity of 2, these relations involve either equality or 
less-than relations over the modifiees and positions 
of role values assigned to the roles zl and x2. Since 
unary constraints operate over role values assigned 
to a single role, the only relative position relations 
that can be tested are between the role value's posi- 
tion (denoted as Pzl)  and its modifiee (denoted as 
Mzl); one and only one of the following three re- 
lations must be true: (P~:I < Mzl), (Mzl < Pzl) ,  
or (Pzl = Mzl). Since binary constraints operate 
over role values assigned to pairs of roles, zl and z2, 
the only possible relative position relations that can 
be tested are between Pzl  and Mxt, P:e2 and Mx2, 
Pz l  and Mz~, Pz2 and Mxt, Pzt  and Px2, Mxl and 
Mz2. Note that each of the six has three positional 
relations (as in the case of unary relations on Pzl  
and Mzt) such that one and only one of them is 
simultaneously true. 
The unary and binary positional relations provide 
the necessary mechanism to develop an alternative 
view of the unary and binary constraints. First, we 
develop the concept of an abstract role value (ARV), 
which is a finite characterization f all possible role 
values using relative, rather than absolute, position 
relations. Formally, an ARV for a particular gram- 
mar G = (~,, R, L, C, T, F t , . . . ,  Fk) is an element of 
the set: .dl = ExR? L xFt  ?. . .xFkxUC,  where UC 
encodes the three possible positional relations be- 
tween Pxl and Mxl. The gray box of Figure 3 shows 
an example of an ARV obtained from the parsed sen- 
tence. Note that .At is a finite set representing the 
space of all possible ARVs for the grammar1; hence, 
the set provides an alternative characterization of
the unary constraints for the grammar, which can 
be partitioned into positive (grammatical) and neg- 
ative (ungrammatical) ARVs. During parsing, if a 
role value does not match one of the elements in the 
positive ARV space, then it should be disallowed. 
Positive ARVs can be obtained directly from the 
parses of sentences: for each role value in a parse for 
a sentence, simply extract its category, feature, role, 
and label information, and then determine the po- 
sitional relation that holds between the role value's 
position and modifiee. 
Similarly the set of legal abstract role value pairs 
(ARVPs), A2 = \ ]ExRxLxFtx . . .xFkx~xRxLx  
F1 x . . .  x Fk x BC, where BC encodes the positional 
relations among Pxl,  Mxt, Px2, and Mx2, provides 
an alternative definition for the binary constraints 2. 
The gray box of Figure 3 shows an example of an 
ARVP obtained from the parsed sentence. Positive 
ARVPs can be obtained directly from the parses of 
sentences. For each pair of role values assigned to 
different roles, simply extract heir category, feature, 
role, and label information, and then determine the 
positional relations that hold between the positions 
and modifiees. 
An enumeration of the positive ARV/ARVPs can 
be used to represent he CDG constraints, C, and 
ARV/ARVPs are PAC-learnable from positive ex- 
amples, as can be shown using the techniques of 
(Natarajan, 1989; Valiant, 1984). ARV/ARVP con- 
straints can be enforced by using a fast table lookup 
method to see if a role value (or role value pair) is 
allowed (rather than propagating thousands of con- 
straints), thus speeding up the parser. 
4 Eva luat ion  Us ing  the  Nava l  
Resource  Management  Domain  
An experiment was conducted to determine the 
plausibility and the benefits of extracting CDG con- 
straints from a domain-specific corpus of sentences. 
For our speech application, the ideal CDG should be 
general enough to cover sentences imilar to those 
that appear in the corpus while being restrictive 
enough to eliminate sentences that are implausible 
given the observed sentences. Hence, we investigate 
whether a grammar extracted from annotated sen- 
tences in a corpus achieves this precision of cover- 
age. We also examine whether a learned grammar 
has the ability to filter out incorrect sentence hy- 
potheses produced by the HMM component of our 
system in Figure 1. To investigate these issues, we 
have performed an experiment using the standard 
1,fit 1 can also include information about  the possible lexical 
categories and feature values of the modifiee of Xl. 
2.A2 can also include information about  the possible lexical 
categories and feature values of the modifiees of Xl and x2. 
105 
Resource Management (RM) (Price et al, 1988) and 
Extended Resource Management (RM2) ((DARPA), 
1990) corpora. These mid-size speech corpora have 
a vocabulary of 991 words and contain utterances of 
sentences derived from sentence templates based on 
interviews with naval personnel familiar with naval 
resource management tasks. They were chosen for 
several reasons: they are two existing speech corpora 
from the same domain; their manageable sizes make 
them a good platform for the development of tech- 
niques that require extensive xperimentation; and 
the sentences have both syntactic variety and rea- 
sonably rich semantics. RM contains 5,190 separate 
utterances (3,990 testing, 1,200 training) of 2,845 
distinct sentences (2,245 training, 600 testing). We 
have extracted several types of CDGs from annota- 
tions of the RM sentences and tested their generality 
using the 7,396 sentences in RM2 (out of the 8,173) 
that are in the resource management domain but are 
distinct from the RM sentences. We compare these 
CDGs to each other and to the conventional CDG 
described previously. 
The corpus-based CDGs were created by extract- 
ing the allowable grammar elationships from the 
RM sentences that were annotated by language x- 
perts using the SENATOR annotation tool, a CGI 
(Common Gateway Interace) HTML script written 
in GNU C++ version 2.8.1 (White, 2000). We 
tested two major CDG variations: those derived di- 
rectly from the RM sentences (Sentence CDGs) and 
those derived from simple template-expanded RM
sentences (Template CDGs). For example, "List 
MIDPAC's deployments during (date)" is a sentence 
containing a date template which allows any date 
representations. For these experiments, we focused 
on templates for dates, years, times, numbers, and 
latitude and longitude coordinates. Each template 
name identifies a sub-grammar which was produced 
by annotating the appropriate strings. We then an- 
notated sentences containing the template names as 
if they were regular sentences. Approximately 25% 
of the 2,845 RM sentences were expanded with one 
or more templates. 
Although annotating a corpus of sentences can be 
a labor intensive task, we used an iterative approach 
that is based on parsing using grammars with vary- 
ing degrees of restrictiveness. A grammar can be 
made less restrictive by ignoring: 
* lexical information associated with a role value's 
modifiee in the ARVPs, 
o feature information of two role values in an ARVP 
not directly related based on their modifiee rela- 
tions, 
. syntactic information provided by two role values 
that are not directly related, 
? specific feature information (e.g., semantics or 
subcategorization). 
Initially, we bootstrapped the grammar by annotat- 
ing a 200 sentence subset of the RM corpus and ex- 
tracting a fairly general grammar from the annota- 
tions. Then using increasingly restrictive grammars 
at each iteration, we used the current grammar to 
identify sentences that required annotation and ver- 
ified the parse information for sentences that suc- 
ceeded. This iterative technique reduced the time 
required to build a CDG from about one year for the 
conventional CDG to around two months (White, 
2000). 
Several methods of extracting an ARV/ARVP 
grammar from sentences or template-extended sen- 
tences were investigated. The ARVPs are extracted 
differently for each method; whereas, the ARVs 
are extracted in the same manner egardless of the 
method. Recall that ARVs represent the set of ob- 
served role value assignments. In our implementa- 
tion, each ARV includes: the label of the role value, 
the role to which the role value was assigned, the 
lexical category and feature values of the word con- 
taining the role, the relative position of the word and 
the role value's modifiee, and the modifiee's lexical 
category and feature values (modifiee constraints). 
We use modifiee constraints for ARVs regardless of 
extraction method because their use does not change 
the coverage of the extracted grammar and not using 
the information would significantly slow the parser 
(Harper et al, 1999a). Because the ARVP space is 
larger than the ARV space, we investigate six varia- 
tions for extracting the pairs: 
1. Ful l  Mod:  contains all grammar and feature 
value information for all pairs of role values from 
annotated sentences, as well as modifiee con- 
straints. For a role value pair in a sentence to be 
considered valid during parsing with this gram- 
mar, it must match an ARVP extracted from the 
annotated sentences. 
2. Full: like Ful l  Mod except it does not impose 
modifiee constraints on a pair of role values during 
parsing. 
3. Feature  Mod:  contains all grammar elations 
between all pairs of role values, but it consid- 
ers feature and modifiee constraints only for pairs 
that are directly related by a modifiee link. Dur- 
ing parsing, if a role value pair is related by a 
modifiee link, then a corresponding ARVP with 
full feature and modifiee information must appear 
in the grammar for it to be allowed. If the pair 
is not directly related, then an ARVP must be 
stored for the grammar elations, ignoring feature 
and modifiee constraint information. 
4. Feature :  like Feature  Mod except it does not 
impose modifiee constraints on a pair of role val- 
ues during parsing. 
5. D i rect  Mod:  stores only the grammar, feature, 
and modifiee information for those pairs of role 
106 
Table 1: Number of ARVs and ARVPs extracted for 
each RM grammar. 
ARVP Sentence I Template Percent 
Variation CDG \[ CDG Increase 
Full Mod 270,034 408,912 
Full 165,480 200,792 
Feature Mod 
Feature 
Direct Mod 
Direct 
ARVs 
49,468 
36,558 
41,124 
28,214 
4,424 
56,758 
40,308 
47,004 
30,554 
4,648 
51.43% 
21.34% 
14.74% 
10.26% 
14.30% 
8.29% 
5.06% 
Table 2: Number of successfully parsed sentences in 
RM2 using the conventional CDG and CDGs derived 
from sentences only or template-expanded s ntences. 
ARVP ~: Parsed with ~ Parsed with 
Variation Sentence CDG Template CDG 
Full Mod 
Full 
Feature Mod 
Feature 
Direct Mod 
Direct 
Conventional 
3,735 (50.50%) 
4,509 (60.97%) 
5,365 (72.54%) 
5,772 (78.04%) 
5,464 (73.88%) 
5,931 (80.19%) 
7,144 (96.59%) 
4,461 (60.32%) 
5,316 (71.88%) 
5,927 (80.14%) 
6,208 (83.94%) 
5,979 (80.84%) 
6,275 (84.82%) 
not applicable 
values that are directly related by a modifiee link. 
During parsing, if a role value pair is related by 
such a link, then a corresponding ARVP must ap- 
pear in the grammar for it to be allowed. Any 
pair of role values not related by a modifiee link 
is allowed (an open-world assumption). 
6. Di rect :  like D i rect  Mod except it does not im- 
pose modifiee constraints on a pair of role values 
during parsing. 
Grammar sizes for these six grammars, extracted 
either directly from the 2,845 sentences or from the 
2,845 sentences expanded with our sub-grammar 
templates, appear in Table 1. The largest gram- 
mars were derived using the Full Mod extrac- 
tion method, with a fairly dramatic growth result- 
ing from processing template-expanded sentences. 
The Feature  and D i rect  variations are more man- 
ageable in size, even those derived from template- 
expanded sentences. 
Size is not the only important consideration for 
a grammar. Other important issues are grammar 
generality and the impact of the grammar on the 
accuracy of selecting the correct sentence from the 
recognition lattice of a spoken utterance. After 
extracting the CDG grammars from the RM sen- 
tences and template-expanded sentences, we tested 
the generality of the extracted grammars by using 
each grammar to parse the 7,396 RM2 sentences. 
See the results in Table 2. The grammar with the 
greatest generality was the conventional CDG for 
the RM corpus; however, this grammar also has 
the unfortunate attribute of being quite ambigu- 
ous. The most generalizable of extracted grammars 
uses the D i rect  method on template-expanded s n- 
tences. In all cases, the template-expanded sen- 
tence grammars gave better coverage than their cor- 
responding sentence-only grammars. 
We have also used the extracted grammars to 
post-process word graphs created by the word graph 
compression algorithm of (Johnson and Harper, 
1999) for the test utterances in the RM corpus. As 
was reported in (Johnson and Harper, 1999), the 
word-error rate of our HMM recognizer with an em- 
bedded word pair language model on the RM test set 
of 1200 utterances was 5.0%, the 1-best sentence ac- 
curacy was 72.1%, and the word graph coverage ac- 
curacy was 95.1%. Also, the average uncompressed 
word graph size was 75.15 nodes, and our compres- 
sion algorithm resulted in a average word graph size 
of 28.62 word nodes. When parsing the word graph, 
the probability associated with a word node can ei- 
ther represent its acoustic score or a combination 
of its acoustic and stochastic grammar score. We 
use the acoustic score because (Johnson and Harper, 
1999) showed that by using a word node's acoustic 
score alone when extracting the top sentence candi- 
date after parsing gave a 4% higher sentence accu- 
racy. 
For the parsing experiments, we processed the 
1,080 word graphs produced for the RM test set 
that contained 50 or fewer word nodes after com- 
pression (out of 1,200 total) in order to efficiently 
compare the 12 ARV/ARVP CDG grammars and 
the conventional CDG (the larger word graphs re- 
quire significant ime and space to parse using the 
conventional CDG). These 1,080 word graphs con- 
tain 24.95 word nodes on average with a standard 
deviation (SD) of 10.80, and result in 1-best sen- 
tence accuracy was 75% before parsing. The num- 
ber of role values prior to binary constraint propa- 
gation differ across the grammars with an average 
(and SD) for the conventional grammar of 504.99 
(442.00), for the sentence-only grammars of 133.37 
(119.48), and for the template-expanded grammars 
of 157.87 (145.16). Table 3 shows the word graph 
parsing speed and the path, node, and role value 
(RV) ambiguity after parsing; Table 4 shows the 
sentence accuracy and the accuracy and percent cor- 
rect for words. Note that percent correct words is 
calculated using N-D-S  and word accuracy using N 
N-D-S- I  where N is the number of words, D is N 
the number of deletions, S is the number of substi- 
tutions, and I is the number of insertions. 
The most selective RM sentence grammar, Full 
Mod,  achieves the highest sentence accuracy, but 
at a cost of a greater average parsing time than 
the other RM sentence grammars. Higher accu- 
107 
ARVP Variation Parse Time (sec.) 
Full Mod 33.89 (41.12) 
Template Full Mod 41.85 (51.75) 
Full 29.73 (36.68) 
Template Full 36.80 (46.90) 
Feature Mod 11.46 (14.46) 
Template Feature Mod 13.80 (18.47) 
Feature 11.60 (14.97) 
Template Feature 14.24 (19.63) 
Direct Mod 13.93 (19.73) 
Template Direct Mod 17.28 (26.56) 
Direct 19.95 (36.89) 
Template Direct 28.02 (69.50) 
Coventional 83.48 (167.51) 
No. Paths 
2.21 (1.74) 
2.78 (3.75) 
2.83 (2.92) 
3.40 (5.19) 
3.9 (5.97) 
4.22 (6.93) 
5.19 (8.36) 
6.86 (14.83) 
No. Nodes 
10.59 (3.44) 
10.76 (3.64) 
10.87 (3.54) 
11.03 (3.74) 
11.20 (3.94) 
11.28 (4.06) 
11.72 (4.22) 
11.94 (4.52) 
4.25 (6.49) 11.46 (4.27) 
4.62 (8.61) 11.45 (4.28) 
808 (18.52) 12.81 (5.73) 
9.98 (25.52) 12.95 (5.95) 
51.33 (132.43) 17.14 (8.02) 
No. RVs 
19.51 (8.32) 
19.93 (8.76) 
20.32 (8.86) 
20.77 (9.47) 
21.43 (10.49) 
21.81 (11.17) 
23.41 (12.72) 
24.47 (14.41) 
22.79 (13.44) 
22.95 (13.34) 
32.85 (34.65) 
33.36 (35.66) 
77.19 (76.26) 
Table 3: Average parse times (SD), number of paths (SD), number of nodes (SD), and number of role values 
(SD) remaining after parsing the 1,080 word graphs of 50 or fewer word nodes produced for the RM test set 
using the 13 CDGs. 
ARVP Variation Sentence Accuracy ~o Correct Words Word Accuracy 
Full Mod 
Template Full Mod 
Full 
Template Full 
Feature Mod 
Template Feature Mod 
Feature 
Template Feature 
Direct Mod 
Template Direct Mod 
Direct 
Template Direct 
Conventional 
91.94% 
91.57% 
91.57% 
91.20% 
90.56% 
90.19% 
90.28% 
89.91% 
90.46% 
90.09% 
89.91% 
89.44% 
81.20% 
98.55% 
98.50% 
98.49% 
98.45% 
98.38% 
98.34% 
98,35% 
98.29% 
98.37% 
98.32% 
98.30% 
98.25% 
97.11% 
98.19% 
98.14% 
98.11% 
98.05% 
97.95% 
97.90% 
97.91% 
97.85% 
97.91% 
97.86% 
97.82% 
97.75% 
96.10% 
Table 4: The sentence accuracy, percent correct words, and word accuracy from parsing 1,080 word graphs 
of 50 or fewer word nodes produced for the RM test set using the 13 CDGs. 
racy appears to be correlated with the ability of the 
constraints to eliminate word nodes from the word 
graph during parsing. The least restrictive sentence 
grammar, D i rect ,  is less accurate than the other 
sentence grammars and offers an intermediate speed 
of parsing, most likely due to the increased ambigu- 
ity in the parsing space. The fastest grammar was 
the Feature -Mod grammar, which also offers an 
intermediate l vel of accuracy. Its size (even with 
templates), restrictiveness, and speed make it very 
attractive. The template versions of each grammar 
showed a slight increase in average parse times (from 
processing a larger number of role values) and a 
slight decrease in parsing accuracy. The conven- 
tional grammar was the least competitive of the 
grammars both in speed and in accuracy. 
5 Conclus ion and Future  D i rect ions  
ity to improve sentence accuracy of our speech sys- 
tem. To achieve balance between precision and cov- 
erage of our corpus-induced grammars, we have ex- 
panded the RM sentences with templates for expres- 
sions like dates and times. The grammars extracted 
from these expanded sentences gave increased RM2 
coverage without sacrificing even 1% of the sentence 
accuracy. We are currently expanding the number of 
templates in our grammar in an attempt o obtain 
full coverage of the RM2 corpus using only template- 
expanded RM sentences. We have recently added 
ten semantic templates to the grammar and have 
improved the coverage by 9.19% without losing any 
sentence accuracy. We are also developing a stochas- 
tic version of CDG that uses a statistical ARV, which 
is similar to a supertag (Srinivas, 1996). 
References  
The ability to extract ARV/ARVP grammars with 
varying degrees of specificity provides us with the 
ability to rapidly develop a grammar with the abil- 
H. U. Block. 1997. Language components in VERB- 
MOBIL. In Proc. of the Int. Conf. of Acoustics, 
Speech, and Signal Proc., pages 79-82. 
108 
Defense Advanced Research Projects Agency 
(DARPA). 1990. Extended resource manage- 
ment: Continuous speech speaker-dependent 
corpus (RM2). CD-ROM. NIST Speech Discs 
3-1.2 and 3-2.2. 
Entropic Cambridge Research Laboratory, Ltd., 
1997. HTK: Hidden Markov Model Toolkit V2.1. 
M. P. Harper and R. A. Helzerman. 1995. Exten- 
sions to constraint dependency parsing for spoken 
language processing. Computer Speech and Lan- 
guage, 9:187-234. 
M. P. Harper, L. H. Jamieson, C. D. Mitchell, 
G. Ying, S. Potisuk, P. N. Srinivasan, R. Chen, 
C. B. Zoltowski, L. L. McPheters, B. Pellom, 
and R. A. Helzerman. 1994. Integrating language 
models with speech recognition. In Proc. of the 
AAAI Workshop on the Integration of Natural 
Language and Speech Processing, pages 139-146. 
M. P. Harper, S. A. Hockema, and C. M. White. 
1999a. Enhanced constraint dependency grammar 
parsers. In Proc. of the IASTED Int. Conf. on 
Artificial Intelligence and Soft Computing. 
M. P. Harper, M. T. Johnson, L. H. Jamieson, and 
C. M. White. 1999b. Interfacing a CDG parser 
with an HMM word recognizer using word graphs. 
In Proc. of the Int. Conf. of Acoustics, Speech, and 
Signal Proc. 
R. A. Helzerman and M. P. Harper. 1996. MUSE 
CSP: An extension to the constraint satisfaction 
problem. Journal of Artificial Intelligence Re- 
search, 5:239-288. 
P. Jeanrenaud, E. Eide, U. Chaudhari, J. Mc- 
Donough, K. Ng, M. Siu, and H. Gish. 1995. Re- 
ducing word error rate on conversational speech 
from the Switchboard corpus. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 53-56. 
M. T. Johnson and M. P. Harper. 1999. Near min- 
imal weighted word graphs for post-processing 
speech. In 1999 Int. Workshop on Automatic 
Speech Recognition and Understanding. 
H. Maruyama. 1990a. Constraint Dependency 
Grammar and its weak generative capacity. Com- 
puter Software. 
H. Maruyama. 1990b. Structural disambiguation 
with constraint propagation. In Proc. of the An- 
nual Meeting of Association for Computational 
Linguistics, pages 31-38. 
W. Menzel. 1994. Parsing of spoken language un- 
der time constraints. In 11th European Conf. on 
Artificial Intelligence, pages 560-564. 
W. Menzel. 1995. Robust processing of natural an- 
guage. In Proc. of the 19th Annual German Conf. 
on Artificial Intelligence. 
B. Natarajan. 1989. On learning sets and functions. 
Machine Learning, 4(1). 
H. Ney, U. Essen, and R. Kneser. 1994. On struc- 
turing probabilistic dependences in stochastic lan- 
guage modelling. Computer Speech and Language, 
8:1-38. 
R. Pieraccini and E. Levin. 1992. Stochastic repre- 
sentation of semantic structure for speech under- 
standing. Speech Communication, 11:283-288. 
P. Placeway, R. Schwartz, P. Fung, and L. Nguyen. 
1993. The estimation of powerful anguage mod- 
els from small and large corpora. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 33-36. 
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. 
1988. A database for continuous peech recog- 
nition in a 1000-word omain. In Proc. of the 
Int. Conf. of Acoustics, Speech, and Signal Proc., 
pages 651-654. 
L. A. Schmid. 1994. Parsing word graphs using a lin- 
guistic grammar and a statistical language model. 
In Proc. of the Int. Conf. of Acoustics, Speech, 
and Signal Proc., pages 41-44. 
S. Seneff. 1992. TINA: A natural language system 
for spoken language applications. American Jour- 
nal of Computational Linguistics, 18:61-86. 
B. Srinivas. 1996. 'Almost parsing' technique for 
language modeling. In Proc. of the Int. Conf. on 
Spoken Language Processing, pages 1173-1176. 
L. G. Valiant. 1984. A theory of the learnable. Com- 
munications of the ACM, 27(11):1134-1142. 
C. M. White. 2000. Rapid Grammar Development 
and Parsing Using Constraint Dependency Gram- 
mars with Abstract Role Values. Ph.D. thesis, 
Purdue University, School of Electrical and Com- 
puter Engineering, West Lafayette, IN. 
J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas. 
1994. Robust language model incorporating a
substring parser and extended N-grams. In Proc. 
of the Int. Conf. of Acoustics, Speech, and Signal 
Proc., pages 361-364. 
S. J. Young, N. H. Russell, and J. H. S. Thornton. 
1989. Token passing : a simple conceptual model 
for connected speech recognition systems. Tech- 
nical Report TR38, Cambridge University, Cam- 
bridge, England. 
S. J. Young, J. Odell, D. Ollason, V. Valtchev, and 
P. Woodland, 1997. The HTK Book. Entropic 
Cambridge Research Laboratory Ltd., 2.1 edition. 
V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, 
J. Polifroni, and S. Seneff. 1991. Integration of 
speech recognition and natural anguage process- 
ing in the MIT Voyager system. In Proe. of the 
Int. Conf. of Acoustics, Speech, and Signal Proe., 
pages 713-716. 
109 
A Question Answering System Developed as a Project in a 
Natural Language Processing Course* 
W. Wang, J. Auer, R. Parasuraman, I. Zubarev, D. Brandyberry, and M. P. Harpm 
Purdue University 
West Lafayette, IN 47907 
{wang28,jauer,pram,dbrandyb,harper}@ecn.purdue.edu an  zubarevi@cs.purdue.edu 
Abstract 
This paper describes the Question Answering Sys- 
tem constructed uring a one semester graduate- 
level course on Natural Language Processing (NLP). 
We hypothesized that by using a combination ofsyn- 
tactic and semantic features and machine learning 
techniques, we could improve the accuracy of ques- 
tion answering on the test set of the Remedia corpus 
over the reported levels. The approach, although 
novel, was not entirely successful in the time frame 
of the course. 
1 Introduction 
This paper describes a preliminary reading com- 
prehension system constructed as a semester-long 
project for a natural language processing course. 
This was the first exposure to this material for 
all but one student, and so much of the semester 
was spent learning about and constructing the tools 
that would be needed to attack this comprehen- 
sive problem. The course was structured around 
the project of building a question answering system 
following the HumSent evaluation as used by the 
Deep Read system (Hirschman eta\]., 1999). The 
Deep Read reading comprehension prototype system 
(Hirschman et al, 1999) achieves a level of 36% of 
the answers correct using a bag-of-words approach 
together with limited linguistic processing. Since the 
average number of sentences per passage is 19.41, 
this performance is much better than chance (i.e., 
5%). We hypothesized that by using a combina- 
tion of syntactic and semantic features and machine 
learning techniques, we could improve the accuracy 
of question answering on the test set of the Remedia 
corpus over these reported levels. 
2 System Description 
The overall architecture of our system is depicted 
in Figure 1. The story sentences and its five ques- 
tions (who, what, where, when, and why) are first 
preprocessed and tagged by the Brill part-of-speech 
* We wou ld  l ike to  thank  the  Deep Read group for giving us 
" access  to  the i r  test bed. 
P la in  Text  ( Story and Questions ) 
)i . . . . .  8__~'!P?_s__!,g_ ~L  . . . . . . .  
Tagged Text  
2 i Name Identification 
Propernoun Identified 
3 Tagged Text 
J r _  . . . . . . . . . . . . . . . . . . . . . . . . .  T . . . . . . . . . . .  
Word Lex\]cat Lexica$ and Ro le  ? In fonmat io r  L 
. . . . . . . .  Labe l ln fo rmal J~1 ~ . . . .  
. . . .  ~ Lex icon  . . . . . . .  ~ . . . . . . . . . . . . . .  
. . . . . . . .  L . . . . . . . . . . . . . . . . . .  
Wordnel . . . . . . . . . . .  Grammar Par t ia l  ~ : P ronoun 
Reso lu t ion  . . . . . . .  - . . . . . . . . . . . .  Rules Parser 4- - -  . . . . . . . . . . . . . . . .  - 
Gramnrmr  . . . . . .  ~ Pronouns  . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Resolved 
Sentence-to-Question 
. I, Compar i son  i 
. - -~=: :  . . . . .  ~__  . .  ~ . . . . . . .  ~ 
" Ru le -Based  " Neuro -  : Neura l  Genet ic  
Classifier .' Fuzzy  Net .  Network Algorithm . '  
? ? .ANS ANS ANS ~ - -  ~"  ANS 
Vot ing  
. . . . . . . . . . . . . . . . . . . . . . . . . .  ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Answer with i 
Highest Scores  i 
Figure 1: The architecture for our question answer- 
ing system. 
(POS) tagger distributed with the Deep Read sys- 
tem. This tagged text is then passed to the Name 
Identification Module, which updates the tags of 
named entities with semantic information and gen- 
der when appropriate." The Partial Parser Mod- 
ule then takes this updated text and breaks it into 
phrases while attempting to \]exically disambiguate 
the text. The Pronoun Resolution Module is con- 
sulted by the parser in order to resolve pronouns be- 
fore passing partially parsed sentences and questions 
to the Sentence-to-Question Comparison Module. 
The Comparison Module determines how strongly 
the phrases of a sentence are related to those of a 
question, and this information is passed to several 
28 
modules which attempt to learn which features of 
the comparison are the most important for identify- 
ing whether a sentence is a strong answer candidate. 
We intended to set up a voting scheme among vari- 
ous modules; however, this part of the work has not 
been completed (as indicated by the dashed lines). 
Our system, like Deep Read, uses as the develop- 
ment set 28 stories from grade 2 and 27 from grade 
5, each with five short answer questions (who, what, 
when, where, and why), and 60 stories with ques- 
tions from grades 3 and 4 for testing 1. We will refer 
to the development and testing data as the Remedia 
corpus. The following example shows the informa- 
tion added to a plain text sentence as it progresses 
through each module of the system we have created. 
Each module-is described in more detail in the fol- 
lowing sections. 
2.1 Name Ident i f i ca t ion  Modu le  
The Name Identification Module expects as in- 
put a file that has been tagged by the Brill 
tagger distributed with the Deep Read system. 
The most important named entities in the Re- 
media corpus are the names of people and the 
names of places. To distinguish between these 
two types, we created dictionaries for names of 
people and names of places. The first and last 
name dictionaries were derived from the files 
at http ://www. census, gov/genealogy/names/. 
First names had an associated gender feature; 
names that were either male or female included 
gender frequency. Place names were extracted from 
atlases and other references, and included names 
of countries, major cities and capital cities, major 
attractions and parks, continents, etc. WordNet 
was also consulted because of its coverage of place 
names. There are 5,165 first name entries, 88,798 
last name entries, and 1,086 place name entries in 
the dictionaries used by this module. 
The module looks up possible names to decide 
whether a word is a person's name or a location. If it 
cannot find the word in the dictionaries, it then looks 
at the POS tags provided in the input file to deter- 
mine whether or not it is a propernoun. Heuristics 
(e.g., looking at titles like Mr. or word endings like 
rifle) are then applied to decide the semantic type 
of the propernoun, and if the type cannot be deter- 
mined, the module returns both person and location 
as its type. The accuracy of the Name Identification 
Module on the testing set was 79.6%. The accuracy 
adjusted to take into account incorrect tagging was 
83.6%. 
There were differences between the Deep Read electronic 
version of the passages and the Remedia  published passages. 
We used the electronic passages. 
29 
Rm3-5  (... The club is fo r  boys who are under 12 years Bid.) They are called Cub ScButs. 
(Answer to Question 1 ) 
: POS tagging 
They are called i Cub Scouts 
PO_S _-.~_P.R. p_: P_O_ S_=_~V_B P" ' P_OS='__V_BN'_~ i_POS='N_N_ P21 POS_--. :NN__S" 
Name Identification 
They are ~ i called Cub Scouts' 
Pos.'t~s" : 
POS="PRP"  POS='VBP"  " POS='VBN"  PROP~PE. '~t  
They 
TYPE=NP 
ID=I 
LABEL=subject 
BASE=they 
AGR=3p 
GENDER=male 
SEM_TYPE=per~ .z~, 
are  
TYPE=VP 
ID=I 
LABEL=auK 
BASE=be 
AGR~,~p 
TENSE=present 
VOtCE=ac~,'e 
SEM TYPE=tan  
They are 
"P#PE=NP TYP~=VP 
11:>-i io=1 
IL'~'BE L=subjec~ LABEL~ 
~',SElthey BAS.~Mce 
AGR=3p , AGR.~3 p 
GENDER=nut  ~1 : TENBE=pQI=~I~ 
BEM TYPE=per  ' VOI CE,=active 
PRONRE F=boys ' = _S E_M TYF~i=b~-pu~.E 
They  are  
TYPE=NP TYPE.V'P 
ID=I IB=~ 
LABEL=subject LABEL=am 
BASE-be 
BASE=boy AGR=3p 
AGFt=3g TE NSIE.pms4mt 
GENDER=male voICE~ctive 
? SEM_ ~PE=~.~ers~ __. SE M "i'~f P _Ept?e-p~_ 
Who 
: ~ 'V~?-  . . . . . . . . .  
ID=I 
, LABEL=subject 
= BABE=who 
: AGR=3p 
: GENDER=male 
Initial Partial Parsing 
called \] 
TYPE.VP ! 
ID..2 
Lt~,B EL .  rnvb 
BASE.,call ! 
AGR.3p 
TENSE=pastp i 
VOICE=gas=ire 
SEM_TYPEiequat el 
Pronoun Resolution 
Y 
cal led 
TYPE=VP 
~O.2 
LABEL.nwb 
BASE=call 
AGFI=3p 
TENSE=pastp 
VOICE=pass)re 
L SEM TYPE=equate 
Updating Features 
called ! 
TYPE=VP 
1\[:),=2 
LABEL=mvb i 
BASE~an 
AGRB3p 
TENSE~p~tp  
VOICE=passive 
_ SEU='r~'Pe=~q2~:  
Bracketed Sentence 
are  
TypE~.VP 
JD.I 
LABEL~vb ! 
BABE.be 
AGR,,3p i 
TENSE=,preserd 
YOICE~ct i~ , 
? s E_M , .TY~,~ 
Bracketed Question 1 
Cub Scouts 
TYPE=NP 
1\[:)=2 
LABEL=object 
BASE =CubScouts 
AGR=3p 
GENDER=male 
SEM ~__PE-~_erson 
Cub Scouts 
TYPE=NP 
ID=2 
LAeEL,=objoct 
BASE=CubScouts 
AGR=3p ! 
GENDER.mate , 
_, SEM-_UPE=~__~ 
Cub Scouts ' 
TYPE=NP 
ID=2 
LABEL=object 
BASE=CubSco~t= ' 
AGR=3p 
GENDER=mak~ ; 
Cub  Scouts 
TYPE=NP 
ID=2 
LABEL=object 
8ABE=CubScout= i 
AGR=3p 
GENDER=male 
. . . .  SE_M= D'~,E:--p. ~. 
Figure 2: Processing an example sentence for match- 
ing with a question in our system. 
2.2 Par t ia l  Parser  Modu le  
The Partial Parser Module follows sequentially af- 
ter the Name Identification Module. The input is 
the set of story sentences and questions, such that 
the words in each are tagged with POS tags and 
the names are marked with type and gender infor- 
mation. Initially pronouns have not been resolved; 
the partial parser provides segmented text with rich 
lexical information and role labels directly to the 
Pronoun Resolution Modffle. After pronoun reso- 
lution, the segmented text with resolved pronouns 
is returned to the partial parser for the parser to 
update the feature values corresponding to the pro- 
nouns. Finally, the partial parser provides bracketed 
text to the Comparison Module, which extracts fea- 
tures that will be used to construct modules for an- 
swering questions. 
The Partial Parser Module utilizes information 
in a lexicon and a grammar to provide the partial 
parses. The lexicon and the parser will be detailed 
in the next two subsections. 
2.2.1 The  Lex icon 
There were two methods we used to construct he 
lexicon: open lexicon, which includes all words 
from the development set alng with all determiners, 
pronouns, prepositions, particles, and conjunctions 
(these words are essential to achieving ood sentence 
segmentation), and closed lexicon,  which includes 
all of the development and testing words 2. We con- 
structed the closed lexicon with the benefit of the 
development corpus only (i.e., we did not consult the 
test materials to design the entries). To improve cov- 
erage in the case of the open lexicon, we constructed 
a module for obtaining features fbr words that do 
not appear in the development set (unknown words) 
that interfaces with WordNet to determine a word's 
base/stem, semantic type, and synonyms. When an 
unknown word has multiple senses, we have opted to 
choose the first sense because WordNet orders senses 
by frequency of use. Ignoring numbers, there are 
1,999 unique words in the development set of the 
Remedia corpus, and 2,067 in the testing data, of 
which 1,008 do not appear in the development set. 
Overall, there are 3,007 unique words across both 
training and testing. 
One of our hypotheses was that by creating a lex- 
icon with a rich set of features, we would improve 
the accuracy of question answering. The entries in 
the lexicon were constructed using the conventions 
adopted for the Parsec parser (Harper and Helzer- 
man, 1995; Harper et al, 1995; Harper et al, 2000). 
Each word entry contains information about its root 
word (if there is one), its lexical category (or cate- 
gories) along with a corresponding set of allowable 
features and their corresponding values. Lexical cat- 
egories include noun, verb, pronoun, propernoun, 
adjective, adverb, preposition, particle, conjunction, 
determiner, cardinal, ordinal, predeterminer, noun 
modifier, and month. Feature types used in the 
lexicon include subcat,  gender, agr, case, vtype 
(e.g., progressive), mood, gap, inver ted ,  voice,  
behav ior  (e.g., mass), type (e.g., interrogative, rel- 
ative), semtype, and con j type  (e.g., noun-type, 
verb-type, etc.). We hypothesized that semtype 
should play a significant role in improving question 
answering performance, but the choice of semantic 
granularity is a difficult problem. We chose to keep 
the number of semantic values relatively small. By 
using the lexicographers' files in WordNet to group 
the semantic values, we selected 25 possible seman- 
tic values for the nouns and 15 for the verbs. A 
2Initially, we created the closed lexicon because this list 
of words was in the Deep Read materials. Once we spotted 
that the list contained words not in the development material, 
we kept it as an alternative to see how important full lexical 
knowledge would be for answering questions. 
30 
script was created to semi-automate he construc- 
tion of the lexicon from information extracted from 
previously existing dictionaries and from WordNet. 
2.2.2 The  Par t ia l  Parser  
The parser segments each sentence into either a noun 
phrase (NP), a verb phrase (VP), or a prepositional 
phrase (PP), each with various feature sets. NPs 
have the feature types: Base (the root word of the 
head word of the NP), AGR (number/person i for- 
mation), SemType (the semtype of the root form in 
the lexicon, e.g., person, object, event, artifact, or- 
ganization), Label (the role type of the word in the 
sentence, e.g., subject), and Gender. Verb phrases 
(VPs) have the feature types: Base, AGR, SemType 
(the semtype of the root form in the lexicon, e.g., 
contact, act, possession), Tense (e.g., present, past), 
and Voice. Prepositional phrases (PPs) have the 
feature types: Prep (the root form of the preposition 
word), SemType (the semtype of the root form in the 
lexicon, e.g., at-loc, at-time), Need (the object of the 
preposition), and NeedSemType (the semtype of the 
object of the preposition). Feature values are as- 
signed using the lexicon, Pronoun Resolution Mod- 
ule, and grammar ules. 
We implemented a bottom-up partial parser to 
segment each sentence into syntactic subparts. The 
grammar used in the bottom-up arser is shown be- 
low: 
1. NP -+ DET ADJ+ NOUN+ 
2. NP ~ DET NOUN 
3. NP ~ ADJ PROPERNOUN+ 
4. VP ~ (AUX-VERB) MAIN-VERB 
5. PP --~ ADV 
6. PP ~ ADJ (PRED) 
7. PP ~ PREP NP 
At the outset, the parser checks whether there are 
any punctuation marks in the sentence, with corn- 
mass and periods being the most helpful. A comma 
is used in two ways in the Remedia corpus: it acts 
as a signal for the conjunction of a group of nouns 
or propernouns, or it acts as punctuation signalling 
an auxiliary phrase (usually a PP) or sentence. In 
the NP conjunction case, the parser groups the con- 
joined nouns or propernouns together as a plural NP. 
In the second case, the sentence is partially parsed. 
The partial parser operates in a bottom-up fashion 
taking as input a POS:tagged and name-identified 
sentence and matching it to the right-hand side of 
the grammar ules. Starting from the beginning of 
the sentence or auxiliary phrase (or sentence), the 
parser looks for the POS tags of the words, trans- 
forming the POS tags into corresponding lexical cat- 
egories and tries to match the RHS of the rules. 
Phrases are maintained on an agenda until they are 
finalized. 
NPs often require merging sincesome consecutive 
NPs form a single multi-word token (i.e., multi-word 
names and conjunctions). An NP that results from 
merging two tokens into a single multi-word token 
has its Base as the rootword of the combined token, 
and AGR and SemType features are updated according 
to the information retrieved from the lexicon based 
on the multi-word token. In the case of an NP con- 
junction, the Base is the union of the Base of each 
NP, AGR is set to 3p, and SemType is assigned as that 
of the head word of the merged NP. The rule for find- 
ing the head word of an NP is: find the F IRST  con- 
secutive noun (propernoun) group in the NP, then 
the LAST  noun (propernoun) in this group is de- 
fined as the head word of the NP. 
The partial parser performs word-sense disam- 
biguation as it parses. Words such as Washington 
have multiple_semtype values in the lexicon for one 
lexical category. The following are rules for word- 
sense disambiguation used by the parser: 
? NP  plus VP  rules for word-sense disambiguation: 
If there are verbs such as name, call, or be, 
which have the semtype of equate, then the NPs  
that precede and follow the VP  have the same 
semtype. 
If a noun is the object of a verb, then the subcat  
feature value of the verb can be used to disam- 
biguate its word sense (e.g., take generally has 
the subcat  of obj+time). 
? PP  rules for word-sense disambiguation: 
For some nouns (propernouns) which are the 
object of a preposition, the intersection of the 
semtype value sets of the preposition word and 
its object determines their semtype. 
? NPs  in the date line of each passage are all ei- 
ther dates or places with the typical order be- 
ing place then time. For example, in (WASH- 
INGTON, June, 1989), Washington is assigned 
semtype of location rather than person. 
To process unknown words (the 1,008 words in the 
testing set that don't appear in the development set) 
in the case of the open lexicon, WordNet is used to 
assign the semtype feature for nouns and verbs, the 
AGR feature for verbs can be obtained in part from 
the POS tag, and AGR for unknown noun words can 
be determined when they are used as the subject of 
a sentence. For the closed lexicon, the only unknown 
words are numbers. If a number is a four-digit num- 
ber starting with 16 to 19 or is followed by A.D or 
B.C. then generally it is a year, so its semtype is de- 
fined as time. Other numbers tend to be modifiers 
or predicates and have the semtype of num. 
2.3 P ronoun Reso lu t ion  Modu le  
A pronoun resolution module was developed using 
the rules given in Allen's text (Allen, 1995) along 
with other rules described in the work of Hobbs 
(Hobbs, 1979). The module takes as input the 
feature-augmented and segmented text provided by 
the partial parser. Hence, the words are marked 
31 
with lexical (including gender) and semantic feature 
information, and the phrase structure is also avail- 
able. After the input file is provided by the Par- 
tial Parser Module, the Pronoun Resolution Module 
searches for the pronouns by looking through the 
NPs identified by the partial parser. Candidate an- 
tecedents are identified and a comparison of the fea- 
tures is made between the pronoun and the possible 
antecedent. The phrase that passes the most rule 
filters is chosen as the antecedent. First and second 
person pronouns are handled by using default val- 
ues (i.e., writer and reader). If the system fails to 
arrive at an antecedent, he pronoun is marked as 
non-referential, which is often the case for pronouns 
like it or they. Some of the most useful rules are 
listed below: 
? Reflexives must refer to an antecedent in the same 
sentence. For simplicity, we chose the closest 
noun preceding the pronoun in the sentence with 
matching Gender, AGR, and SemType. 
? Two NPs that co-refer must agree in AGR, Gender, 
and SemType (e.g., person, location). Since, in 
many cases the gender cannot be determined, this 
information was used only when available. 
? A subject was preferred over the object when the 
pronoun occurred as the subject in a sentence. 
? When it occurs in the beginning of a paragraph, 
it is considered non-referential. 
? We prefer a global entity (the first named entity in 
a paragraph) when there is a feature match. In the 
absence of such, we prefer the closest propernoun 
preceding the pronoun with a feature match. If 
that fails, we prefer the closest preceding noun or 
pronoun with a feature match. 
The accuracy of our pronoun resolution module 
on the training corpus was 79.5% for grade 2 and 
79.4% for grade 5. On testing, it was 81.33% for 
grade 3 and 80.39% for grade 4. The overall accu- 
racy of this module on both the testing and train- 
ing corpus was 80.17%. This was an improvement 
over the baseline Deep Read coreference module 
which achieved a 51.61% accuracy on training and 
a 50.91% accuracy on testing, giving an overall ac- 
curacy of 51.26%. This accuracy was determined 
based on Professor Harper's manual pronoun reso- 
lution of both the training and testing set (the per- 
fect coreference information was not included in the 
distribution of the Deep Read system). 
2.4 Sentence- to -Quest ion  Compar i son  
Modu le  
The Sentence-to-Question Comparison Module 
takes as input a set of tagged stories, for which 
phrase types and features have been identified. 
The semantic and syntactic information is coded as 
shown in Figure 2 (using XML tags). A mechanism 
to quantify a qualitative comparison of questions 
and sentences has been developed. The comparison 
:.'.' 
provides data about how questions compare to their 
answers and how questions compare to non-answers. 
The classification of answers and non-answers i im- 
plemented by using feature comparison vectors of 
phrase-to-phrase comparisons in questions and po- 
tential answer sentences. 
A comparison is made using phrase-to-phrase 
comparisons between each sentence and each ques- 
tion in a passage. In particular, NP-to-NP, VP-to- 
VP, PP-to-PP, and NP-to-PP comparisons are made 
between each sentence and each of the five questions. 
These comparisons are stored for each sentence in 
the following arrays. Note that in these arrays Q 
varies from 1 to 5, signifying the question that the 
sentence matches. F varies over the features for the 
phrase match. 
CN\[Q\]\[F\] Comparison of NP features (F = I{Base, 
h6R, and SemType}D between question 
Q and the sentence. 
CV\[Q\]\[F\] Comparison of VP features (F = i{Base, 
AGR, SemType, Tense}\[) between 
question Q and the sentence. 
CP\[Q\]\[F\] Comparison of PP features (F = \[{NeedBase, 
Prep, PPSemType, NeedSemType}\[) 
between question Q and the sentence. 
CPN\[Q\]\[F\] Comparison of PP features in sentence 
to NP features in question Q. Here F=2,  
comparing lWeedBase and Base, and 
NeedSemType and SemType. 
Values for these comparison matrices were calcu- 
lated for each sentence by comparing the features of 
each phrase type in the sentence to features of the 
indicated phrase types in each of the five questions. 
The individual matrix values describe the compari- 
son of the best match between a sentence and a ques- 
tion for NP-to-NP (the three feature match scores 
for the best matching NP pair of the sentence and 
question Q are stored in CN\[Q\]), VP-to-VP (stored 
in CV\[Q\]), PP-to-PP (stored in CP\[Q\]), and PP-to- 
NP (store in CPN\[Q\]). Selecting the phrase compar- 
ison vector for a phrase type that best matches a 
sentence phrase to a question phrase was chosen as 
a heuristic to avoid placing more importance on a 
sentence only because it contains more information. 
Comparisons between features were calculated us- 
ing the following equations. The first is used when 
comparing features such as Base, NeedBase, and 
Prep, where a partial match must be quantified. 
The second is used when comparing features uch as 
SemType, AGR, and Tense where only exact matches 
make sense. 
1 if Strl = Str2 
rain len~th(Strl,Str2) length(Sth)  # length(Str2) 
c = max length(Strl,Str2) A(Strl 6 Str2 V Str2 6 Strl ) 
0 if Strl -~ Str2 
1 if Strl = Str2 
c = 0 if Strl ~- Str2 
32 
The matrices for the development set were pro- 
vided to the algorithms in the Answer Module for 
training the component answer classifiers. The ma- 
trices for the testing set were also passed to the al- 
gorithms for testing. Additionally, specific informa- 
tion about the feature values for each sentence was 
passed to the Answer Module. 
2.5 Answer  Modu les  
Several methods were developed in parallel in an 
attempt o learn the features that were central to 
identifying the sentence from a story that correctly 
answer a question. These methods are described in 
the following subsections. Due to time constraints, 
the evaluations of these Answer Modules were car- 
ried out with a closed lexicon and perfect pronoun 
resolution. 
2.5.1 A Neuro--Fuzzy Network  Classif ier 
An Adaptive Network-based Fuzzy Inference System 
(ANFIS) (Jang, 1993) from the Matlab Fuzzy Logic 
Toolbox was used as one method to resolve the story 
questions. A separate network was trained for each 
question type in an attempt o make the networks 
learn relationships between phrases that classify an- 
swer sentences and non-answer sentences differently. 
ANFIS has the ability to learn complex relationships 
between its input variables. It was expected that 
by learning the relationships in the training set, the 
resolution of questions could be performed on the 
testing set. 
For ANFIS, the set of sentence-question pairs was 
divided into five groups according to question type. 
Currently the implementation of ANFIS on Matlab 
is restricted to 4 inputs. Hence, we needed to devise 
a way to aggregate the feature comparison informa- 
tion for each comparison vector. The comparison 
vectors for each phrase-to-phrase comparison were 
reduced to a single number for each comparison pair 
(i.e., NP-NP, VP-VP, PP-PP, NP-PP). This reduc- 
tion was performed by multiplying the vector values 
by a normalized weighting constant for the feature 
values (e.g., NP-comparison = (Base weight)*(Base 
comparison value) + (AGR weight)*(AGR compari- 
son value) + (SemType weight)*(SemType compari- 
son value), with the weights umming to 1). In most 
cases that a match is found, the comparison values 
are 1 (exact match). So weights were chosen that al- 
lowed the ANFIS to tell'something about the match 
characteristics (e.g., if the AGR weight is 0.15 and 
the SemType weight is 0.1, and the NP-comparison 
value was 0.25, it can be concluded that the NP 
that matched best between in the sentence-question 
pair had the same AGR and SemType features). The 
aggregation weights were chosen so that all com- 
binations of exact matches on features would have 
unique values and the magnitude of the weights were 
chosen based on the belief that the higher weighted 
features contribute more useful information. The 
weights, ordered to correspond to the features in the 
table on the previous page are: (.55, .15, .3) for CN, 
(.55, .1, .22, .13) for CV, (.55, .15, .2, .1) for CP, and 
(.55, .45) for CPN. 
ANFIS was trained using the update on the de- 
velopment set provided by the Sentence-to-Question 
Comparison Module as described above. During 
testing, the data, provided by the Comparison Mod- 
ule and updated as described above, is used as input 
to ANFIS. The output is a confidence value that de- 
scribes the likelihood of a sentence being a answer. 
Every sentence is compared with every question in 
ANFIS, and then within question, the sentences are 
ranked by the likelihood that they are a question's 
answer. 
The accuracy of the best classifier produced with 
ANFIS was quite poor. In the grade 3 set, we 
achieved an accuracy of 13.33% on who questions, 
6.67% on what questions, 0% on where questions, 
6.67% on when questions, and 3.33% on why ques- 
tions. In the grade 4 set, we achieved an accuracy of 
3.54% on who questions, 10.34% on what questions, 
10.34% on where questions, 0% on when questions, 
and 6.9% on why questions. Although the best rank- 
ing sentence produced poor accuracy results on the 
testing set, with some additional knowledge the top- 
ranking incorrect answers may be able to be elimi- 
nated. The plots in Figure 3 display the number of 
times the answer sentence was assigned a particular 
rank by ANFIS. The rank of the correct sentence 
tends to be in the top 10 fairly often for most ques- 
tion types. This rank tendency is most noticeable 
for who, what and when questions, but it is also 
present for where questions. The rank distribution 
for why questions appears to be random, which is 
consistent with our belief that they require a deeper 
analysis than would be possible with simple feature 
comparisons. 
2.5.2 A Neura l  Network  Classi f ier  
Like ANFIS, this module uses a neural network, but 
it has a different opology and uses an extended fea- 
ture set. The nn (Neureka) neural network sim- 
ulation system (Mat, 1998) was used to create a 
multi-layer (one hidden layer) back-propagation net- 
work. A single training/testing instance was gener- 
ated from each story sentence. The network contains 
an input layer with two groups of features. The sen- 
tence/question feature vectors that compare a sen- 
tence to each of the five story questions comprise the 
first group. Sentence features that are independent 
of the questions, i.e., contains a location, contains a 
time/date, and contains a human, comprise the sec- 
ond group. The hidden layer contains a number of 
nodes that was experimentally varied to achieve best 
performance. The output layer contains five nodes, 
each of which has a binary outpht value which indi- 
cates whether or not the sentence is the answer to 
the corresponding question (i.e., question 1 through 
5). 
Several training trials were performed to deter- 
mine the opt imum parameters for the network. We 
trained using various subsets of the full input fea- 
ture set since some features could be detrimental to 
creating a good classifier. However, in the end, the 
full set of features performed better than or equiva- 
lently to the various subsets. Increasing the number 
of hidden nodes can often improve the accuracy of 
the network because it can learn more complex re- 
lationships; however, this did not help much in the 
current domain, and so the number of hidden nodes 
was set to 16. For this domain, there are many more 
sentences that are not the answer to a question than 
that are. An effort was made to artificially change 
this distribution by replicating the answer sentences 
in the training set; however, no additional accuracy 
was gained by this experimentation. Finally, we cre- 
ated a neural network for each question type as in 
ANFIS; however, these small networks had lower ac- 
curacy than the single network approach. 
The overall test set accuracy of the best neural 
network classifier was 14%. In the grade 3 set, we 
achieved an accuracy of 30% on who questions, 0% 
on what questions, 23.3% on when questions, 13.3% 
on where questions, and 3.3% on why questions. In 
the grade 4 set, we achieved an accuracy of 17.2% 
on who questions, 10.3% on what questions, 23.6% 
on when questions, 10.3% on where questions, and 
3.4% on why questions. 
2.5.3 A Ru le -based  Classi f ier  based  on C5.0 
We attempted to learn rules for filtering out sen- 
tences that are not good candidates as answers to 
questions using C5.0 (Rul, 1999). First we ex- 
tracted information from the sentence-to-question 
correspondence data ignoring the comparison values 
to make the input C5.0-compatible, and produced 
five different files (one for each question type). These 
files were then fed to C5.0; however, the program did 
not produce a useful tree. The problem may have 
been that most sentences in the passages are nega- 
tive instances of answers to questions. 
2.5.4 GAS 
GAS (Jelasity and Dombi, 1998) is a steady genetic 
algorithm with subpopulation support. It is capa- 
ble of optimizing functions with a high number of 
local optima. The initial parameters were set theo- 
retically. In the current matching problem, because 
the number of local opt ima can be high due to the 
coarse level of sentence information (there can be 
several sentence candidates with very close scores), 
this algorithm is preferred over other common ge- 
netic algorithms. This algorithm was trained on the 
training set, but due to the high noise level in the 
33 
Who Questions 
~2 '~t <-~ :~ ~.  ~:~" ; ~ .~ ~ .~ ~~ ~. ,~! -  :-::~ i~',~ . '  
. . . . . . .  , t . . . ,~  ~ :~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ,  " - :  . . . . . . . . . . . .  :~ ....... 
N-best order of sentences 
What  Questions 
 t2  
N-best order of sentences 
When Questions 
18 
F~ 6.  
~- ~ ~ 
g 4 ~, . . . . . . . . . . . . . . . . . . . . . .  ~ ~ ,~ . . . . . . . . . .  :~  ~ 
N-best order of sentences 
Where Questions 
 10 \ [ ;p~? ,~z~, : : : : : : :~ ,~, :~: :~.~:~- :~:~:~: ,~.~? . : :~f f : , :~  
N-best order of sentences 
Why Questions 
4.5 ~i~:~:~ 
0 
N-best order of sentences 
Figure 3: Correct answers ordered by ANFIS preference. 
training data, the algorithm fails to produce a win- 
ning population based on the mean square minimiza- 
tion function. 
3 A Closer Look at the Features 
After observing the question answer accuracy results 
of the above classifiers, we concluded that the fea- 
tures we extracted for the classifiers are affected by 
noise. The fact that we take into consideration only 
the top matching phrase-to-phrase matches on a spe- 
cific set of features may have contributed to this 
noisiness. To analyze the noise source of features, 
given that SemType was hypothesized to be essen- 
tial for answer candidate discrimination, we exam- 
ined those SemType values that occurred most fre- 
quently and calculated statistics on how often the 
values occurred in story sentences that are answers 
versus non-answers to the questions. We observed 
the following phenomena: 
1. For who questions, the SemType value person 
plays an important role in ?identifying answer 
sentences, since 83.64% answers have person as 
its NP SemType value, and 21.82% have it as 
its PP NeedSemType value. However, 66.83% of 
the non-answer sentences also have person as its 
NP SemType and 15.85% as its PP NeedSemType. 
Phrases with person SemType appear in most sen- 
tences, whether they are answers or not, and this 
34 
weakens its ability to act as an effective filter. 
2. For what questions, the SemType value person ap- 
pears as the NP SemType of most answer and non- 
answer sentences. The next most dominant fea- 
ture is the SemType value object, which appears 
in the NP for 29.41% of the answer sentences and 
PP NeedSemType for 15.68% of the answer sen- 
tences. Most of the other SemType values such as 
time contribute trivially to distinguishing answers 
from non-answers, as might be expected. 
3. For when questions, person appears dominant 
among NP SemType values; however, time fea- 
tures appear to be second most dominant since 
19.30% of the answer sentences have time as their 
NP SemType, and 26.32% have at-time as their PP 
SemType. N_o.te that the PP NeedSeraType and VP 
SemType appear to be less capable of guiding the 
selection of the correct answer. 
4. For where questions, location features are impor- 
tant with 24.07% answer sentences having loca- 
tion as their NP SemType value, and 20.37% hav- 
ing at-loc as their PP SemType. However, the 
distribution of values for VP SemType and PP 
NeedSemType shows no interesting patterns. 
The current raining strategy weights the NP-NP, 
VP-VP, PP-PP, and NP-PP comparisons equiva- 
lently. The above observations suggest hat training 
classifiers based on these equally weighted compar- 
isons may have prevented the detection of a clear 
class boundary, resulting in poor classification per- 
formance. Since different phrase types do not appear 
to contribute in the same way across different ques- 
tion types, it may be better to generate a rule base 
as a prefitter to assign more weight to certain phrases 
or discard others before inputting the feature vector 
into the classifier for training. 
4 Future  D i rec t ions  
As a next step, we will try to tame our feature set. 
One possibility is to use a rule-based classifier that 
is less impacted by the serious imbalance between 
negative and positive instances than C5.0 in order 
to learn more effective feature sets for answer candi- 
date discrimination corresponding to different ques- 
tion types. We could then use the classifier as a pre- 
processing filter to discard those less relevant com- 
parison vector elements before inputting them into 
the classifiers, instead of inputting comparison re- 
sults based on the complete feature sets. This should 
help to reduce noise generated by irrelevant features. 
Also, we will perform additional data analysis on the 
classification results to gain further insight into the 
noise sources. 
The classifiers we developed covered a wide range 
of approaches. To optimize the classification perfor- 
mance, we would like to implement a voting mod- 
ule to process the answer candidates from different 
classifiers. The confidence rankings of the classifiers 
would be determined f rom their corresponding an- 
swer selection accuracy in the training set, and will 
be used horizontally over the classifiers to provide 
a weighted confidence measure for each sentence, 
giving a final ordered list, where the head of the 
list is the proposed answer sentence. We propose 
to use a voting neural network to train the confi- 
dence weights on different classifiers based on differ- 
ent question types, since we also want to explore the 
relationship of classifier performance with question 
types. We believe this voting scheme will optimize 
the bagging of different classifiers and improve the 
hypothesis accuracy. 
References  
J. Allen. 1995. Natural Language Understanding. 
The Benjamin/Cummings Publishing Company, 
Menlo Park, CA. 
M. P. Harper and R. A. Helzerman. 1995. Man- 
aging multiple knowledge sources in constraint- 
based parsing spoken language. Fundamenta In- 
formaticae, 23(2,3,4):303-353. 
M. P. Harper, R. A. Helzerman, C. B. Zoltowski, 
B. L. Yeo, Y. Chan, T. Stewart, and B. L. Pellom. 
1995. Implementation issues in the development 
of the parsec parser. SOFTWARE - Practice and 
Experience, 25:831-862. 
M. P. Harper, C. M. White, W. Wang, M. T. John- 
son, and R. A. Helzerman. 2000. Effectiveness 
of corpus-induced dependency grammars for post- 
processing speech. In Proceedings of the 1st An- 
nual Meeting of the North American Association 
for Computational Linguistics. 
L. Hirschman, M. Light, E. Breck, and J.D. Burger. 
1999. Deep Read: A reading comprehension sys- 
tem. In Proceedings of the 37th Annual Meeting 
of the Association for Computational Linguistics, 
pages 325-332. 
J. R. Hobbs. 1979. Coherence and coreference. Cog- 
nitive Science, 1:67-90. 
J-SR Jang. 1993. ANFIS: Adaptive-Network-based 
Fuzzy Inference System. IEEE Transactions on 
System, Man, and Cybernetics, 23(3):665-685. 
M. Jelasity and J. Dombi. 1998. GAS, a concept on 
modeling species in genetic algorithms. Artificial 
Intelligence, 99 (1) :1-19. 
The MathWorks, Inc., 1998. Neural Network Tool- 
box, v3. O. 1. 
Rulequest Research, 1999. Data 
Mining Tools See5 and C5. O. 
http ://www. rulequest, com/s eeS-in~o, html. 
35 
The SuperARV Language Model: Investigating the Eectiveness
of Tightly Integrating Multiple Knowledge Sources
Wen Wang and Mary P. Harper
School of Electrical and Computer Engineering
Purdue University
1285 The Electrical Engineering Building
West Lafayette, IN 47907-1285
fwang28,harperg@ecn.purdue.edu
Abstract
A new almost-parsing language model incorporat-
ing multiple knowledge sources that is based upon
the concept of Constraint Dependency Grammars is
presented in this paper. Lexical features and syn-
tactic constraints are tightly integrated into a uni-
form linguistic structure called a SuperARV that is
associated with a word in the lexicon. The Super-
ARV language model reduces perplexity and word er-
ror rate compared to trigram, part-of-speech-based,
and parser-based language models. The relative con-
tributions of the various knowledge sources to the
strength of our model are also investigated by using
constraint relaxation at the level of the knowledge
sources. We have found that although each knowl-
edge source contributes to language model quality,
lexical features are an outstanding contributor when
they are tightly integrated with word identity and
syntactic constraints. Our investigation also suggests
possible reasons for the reported poor performance
of several probabilistic dependency grammar models
in the literature.
1 Introduction
The purpose of a language model (LM) is to de-
termine the a priori probability of a word sequence
w1; : : : ; wn, P (w1; : : : ; wn). Language modeling is es-
sential in a wide variety of applications; we focus on
speech recognition in our research. Although word-
based LMs (with bigram and trigram being the most
common) remain the mainstay in many continuous
speech recognition systems, recent eorts have ex-
plored a variety of ways to improve LM performance
(Niesler and Woodland, 1996; Chelba et al, 1997;
Srinivas, 1997; Heeman, 1998; Chelba, 2000; Rosen-
feld, 2000; Goodman, 2001; Roark, 2001; Charniak,
2001).
Class-based LMs attempt to deal with data sparse-
ness and generalize better to unseen word sequences
by rst grouping words into classes and then using
these classes to compute n-gram probabilities. Part-
of-Speech (POS) tags were initially used as classes
by Jelinek (1990) in a conditional probabilistic
model (which predicts the tag sequence for a word
sequence rst and then uses it to predict the word
sequence):
Pr(wN1 ) 
X
t1;t2;:::;tN
N
Y
i=1
Pr(tijti?11 )Pr(wijti) (1)
However, Jelinek?s POS LM is less eective at pre-
dicting word candidates than an n-gram word-based
LM because it deletes important lexical information
for predicting the next word. Heeman?s (1998) POS
LM achieves a perplexity reduction compared to a
trigram LM by instead redening the speech recog-
nition problem as determining:
W; T = argmax
W;T
P (W;T jA)
= argmax
W;T
P (W;T)P (AjW;T)
 argmax
W;T
P (W;T)P (AjW )
where T is the POS sequence tN1 associated with the
word sequence W = wN1 given the speech utterance
A. The LM P (W;T ) is a joint probabilistic model
that accounts for both the sequence of words wN1
and their tag assignments tN1 by estimating the joint
probabilities of words and tags:
P (wN1 ; t
N
1 ) =
N
Y
i=1
P (wi; tijwi?11 ; ti?11 ) (2)
Johnson (2001) and Laerty et al (2001) provide
insight into why a joint model is superior to a con-
ditional model.
Recently, there has been good progress in devel-
oping structured models (Chelba, 2000; Charniak,
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 238-247.
                         Proceedings of the Conference on Empirical Methods in Natural
2001; Roark, 2001) that incorporate syntactic infor-
mation. These LMs capture the hierarchical char-
acteristics of a language rather than specic infor-
mation about words and their lexical features (e.g.,
case, number). In an attempt to incorporate even
more knowledge into a structured LM, Goodman
(1997) has developed a probabilistic feature gram-
mar (PFG) that conditions not only on structure
but also on a small set of grammatical features (e.g.,
number) and has achieved parse accuracy improve-
ment. Goodman?s work suggests that integrating
lexical features with word identity and syntax would
benet LM predictiveness. PFG uses only a small set
of lexical features because it integrates those features
at the level of the production rules, causing a signif-
icant increase in grammar size and a concomitant
data sparsity problem that preclude the addition of
richer features. This sparseness problem can be ad-
dressed by associating lexical features directly with
words.
We hypothesize that high levels of word predic-
tion capability can be achieved by tightly integrat-
ing structural constraints and lexical features at the
word level. Hence, we develop a new dependency-
grammar almost-parsing LM, SuperARV LM, which
uses enriched tags called SuperARVs. In Section 2,
we introduce our SuperARV LM. Section 3 compares
the performance of the SuperARV LM to other LMs.
Section 4 investigates the knowledge source contribu-
tions by constraint relaxation. Conclusions appear in
Section 5.
2 SuperARV Language Model
The SuperARV LM is a highly lexicalized probabilis-
tic LM based on the Constraint Dependency Gram-
mar (CDG) (Harper and Helzerman, 1995). CDG
represents a parse as assignments of dependency re-
lations to functional variables (denoted roles) asso-
ciated with each word in a sentence. Consider the
parse for What did you learn depicted in the white
box of Figure 1. Each word in the parse has a lexi-
cal category and a set of feature values. Also, each
word has a governor role (denoted G) which is as-
signed a role value, comprised of a label as well as a
modiee, which indicates the position of the word?s
governor or head. For example, the role value as-
signed to the governor role of did is vp-1, where its
label vp indicates its grammatical function and its
modiee 1 is the position of its head what. The need
roles (denoted N1, N2, and N3) are used to ensure
the grammatical requirements (e.g., subcategoriza-
tion) of a word are met, as in the case of the verb
did, which needs a subject and a base form verb (but
since the word takes no other complements, the mod-
pronoun
case=common
behavior=nominal
type=interrogative
semtype=inanimate
agr=3s
G=np-4
verb
subcat=base
verbtype=past
voice=active
inverted=yes
type=none
gapp=yes
mood=whquestion
semtype=auxiliary
agr=all
G=vp-1
Need1=S-3
Need2=S-4
Need3=S-2
pronoun
case=common
behavior=nominal
type=personal
semtype=human
agr=2s
G=subj-2
          1
        what
          2
        did
          
 3
        you
The SuperARV of the word "did":
 Category: Verb
           4
        learn
verb
subcat=obj
vtype=infinitive
voice=active
inverted=no
type=none
gapp=yes
mood=whquestion
semtype=behavior
agr=none
G=vp-2
Need1=S-4
Need2=S-1
Need3=S-4
 Features: {verbtype=past, voice=active, inverted=yes, type=none, 
 gapp=yes,mood=whquestion,agr=all}
 Role=G,         Label=vp, PX>MX,                (ModifieeCategory=pronoun)
 Role=Need1, Label=S,   PX<MX,                (ModifieeCategory=pronoun)
 Role=Need2, Label=S,   PX<MX,                (ModifieeCategory=verb)
 Role=Need3, Label=S,   PX=MX,                (ModifieeCategory=verb)
 Dependent Positional Constraints:
 MX[G] < PX = MX[Need3] < MX[Need1] 
 < MX[Need2] MC
}
n
ee
d 
ro
le
 
co
n
st
ra
in
ts}}}CF }
}
(R,L,UC,MC)+
DC
ARV for vp-1 assigned to G for did:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, 
gapp=yes, mood=whquestion, semtype=auxiliary, agr=all,
role1=G, label1=vp, (PX1>MX1)
ARVP for vp-1 assigned to G for did and subj-2 assigned to G for you:
cat1=verb, subcat=base, verbtype=past, voice=active, inverted=yes, type=none, gapp=yes,
 mood=whquestion, semtype=auxiliary, agr=all,
role1=G, label1=vp, (PX1>MX1)
cat2=pronoun, case=common, behavior=nominal, type=personal, semtype=human, agr=2s,
role2=G, label2=subj, (PX2>MX2)
(PX1<PX2), (MX1<MX2),(PX1=MX2),(MX1<PX2) 
Figure 1: An example of a CDG parse, an ARV and
ARVP, and the SuperARV of the word did in the sentence
what did you learn. Note: G represents the governor
role; the need roles, Need1, Need2, and Need3, are used
to ensure that the grammatical requirements of the word
are met. PX and MX([R]) represent the position of a
word and its modiee (for role R), respectively.
iee of the role value assigned to N3 is set equal to
its own position). Including need roles also provides
a mechanism for using non-headword dependencies
to constrain parse structures, which Bod (2001) has
shown contributes to improved parsing accuracy.
During parsing, the grammaticality of a sentence
in a language dened by a CDG is determined by
applying a set of constraints to the possible role
value assignments (Harper and Helzerman, 1995;
Maruyama, 1990). Originally, the constraints were
comprised of a set of hand-written rules specifying
which role values (unary constraints) and pairs of
role values (binary constraints) were grammatical
(Maruyama, 1990). In order to derive the constraints
directly from CDG annotated sentences, we have de-
veloped an algorithm to extract grammar relations
using information derived directly from annotated
sentences (Harper et al, 2000; Harper and Wang,
2001). Using the relationship between a role value?s
position and its modiee?s position, unary and bi-
nary constraints can be represented as a nite set of
abstract role values (ARVs) and abstract role value
pairs (ARVPs), respectively. The light gray box of
Figure 1 shows an example of an ARV and an ARVP.
The ARV for the governor role value of did indicates
its lexical category, lexical features, role, label, and
positional relation information. (PX1 > MX1) in-
dicates that did is governed by a word that precedes
it. Note that the constraints of a CDG can be ex-
tracted from a corpus of parsed sentences.
A super abstract role value (SuperARV) is an ab-
straction of the joint assignment of dependencies for
a word, which provides a mechanism for lexicaliz-
ing CDG parse rules. The dark gray box of Figure 1
presents an example of a SuperARV for the word did.
The SuperARV structure provides an explicit way to
organize information concerning one consistent set of
dependency links for a word that can be directly de-
rived from its parse assignments. SuperARVs encode
lexical information as well as syntactic and semantic
constraints in a uniform representation that is much
more ne-grained than POS. A SuperARV can be
thought of as providing admissibility constraints on
syntactic and lexical environments in which a word
may be used.
A SuperARV is formally dened as a four-tuple
for a word, hC; F , (R;L; UC;MC)+; DCi, where C
is the lexical category of the word, F = fFname1
= Fvalue1, : : : ; FNamef = FV aluefg is a fea-
ture vector (where Fnamei is the name of a feature
and Fvaluei is its corresponding value), (R, L, UC,
MC)+ is a list of one or more four-tuples, each rep-
resenting an abstraction of a role value assignment,
where R is a role variable, L is a functionality la-
bel, UC represents the relative position relation of
a word and its dependent, MC is the lexical cat-
egory of the modiee for this dependency relation,
and DC represents the relative ordering of the po-
sitions of a word and all of its modiees. The fol-
lowing features are used in our SuperARV LM: agr,
case, vtype (e.g., progressive), mood, gapp (e.g.,
gap or not), inverted, voice, behavior (e.g., mass,
count), type (e.g., interrogative, relative). These
lexical features constitute a much richer set than the
features used by the parser-based LMs in Section 1.
Since Harper et al (1999) found that enforcing mod-
iee constraints (e.g., the lexical categories of modi-
ees) in parsing results in ecient pruning, we also
include the modiee lexical category (MC) in our Su-
perARV structure to impose modiee constraints.
Words typically have more than one SuperARV to
indicate dierent types of word usage. The average
number of SuperARVs for words of dierent lexical
categories vary, with verbs having the greatest Su-
perARV ambiguity. This is mostly due to the vari-
ety of feature combinations and variations on com-
plement types and positions. We have observed in
several experiments that the number of SuperARVs
does not grow signicantly as training set size in-
creases; the moderate-sized Resource Management
corpus (Price et al, 1988) with 25,168 words pro-
duces 328 SuperARVs, compared to 538 SuperARVs
for the 1 million word Wall Street Journal (WSJ)
Penn Treebank set (Marcus et al, 1993), and 791 for
the 37 million word training set of the WSJ contin-
uous speech recognition task.
SuperARVs can be accumulated from a corpus an-
notated with CDG relations and stored directly with
words in a lexicon, so we can learn their frequency
of occurrence for the corresponding word. A Super-
ARV can then be selected from the lexicon and used
to generate role values that meet their constraints.
Since there are no large benchmark corpora anno-
tated with CDG information1, we have developed a
methodology to automatically transform constituent
bracketing found in available treebanks into CDG
annotations. In addition to generating dependency
structures by headword percolation (Chelba, 2000),
our transformer also utilizes a rule-based method to
determine lexical features and need role values for
words, as described by Wang et al (2001).
Our SuperARV LM estimates the joint probability
of words wN1 and their SuperARV tags t
N
1 :
Pr(wN1 t
N
1 ) =
N
Y
i=1
Pr(witijwi?11 ti?11 )
=
N
Y
i=1
Pr(tijwi?11 ti?11 )  Pr(wijwi?11 ti1)

N
Y
i=1
Pr(tijwi?1i?2ti?1i?2)  Pr(wijwi?1i?2tii?2) (3)
Notice we use a joint probabilistic model to enable
the joint prediction of words and their SuperARVs so
that word form information is tightly integrated at
the model level. Our SuperARV LM does not encode
the word identity directly at the data structure level
as was done in (Galescu and Ringger, 1999) since
this could cause serious data sparsity problems.
To estimate the probability distributions in Equa-
tion (3) from training data, we use recursive lin-
ear interpolation among probability estimations of
dierent orders. Representing each multiplicand
in Equation (3) as the conditional probability
P^ (xjy1; y2; : : : ; yn) where y1; y2; : : : ; yn belong to a
mixed set of words and SuperARVs, the recursive
linear interpolation is calculated as follows:
1We have annotated a moderate-sized corpus,
DARPA Naval Resource Management (Price et al, 1988),
with CDG parse relations as reported in (Harper et al,
2000; Harper and Wang, 2001).
P^n(xjy1; y2; : : : ; yn)
= (x; y1; y2; : : : ; yn)  Pn(xjy1; y2; : : : ; yn)
+(1? (x; y1; y2; : : : ; yn))  P^n?1(xjy1; y2; : : : ; yn?1)
where:
 y1; y2; : : : ; yn is the context of order n-gram to
predict x;
 Pn(xjy1; y2; : : : ; yn) is the order n-gram maximum
likelihood estimation.
Table 1 enumerates the n-grams and their order for
the interpolation smoothing of the two distributions
in Equation (3). The ordering was based on our hy-
pothesis that n-grams with more ne-grained history
information should be ranked higher in the n-gram
list since that information should be more helpful
for discerning word and SuperARVs based on their
history. The SuperARV LM hypothesizes categories
for out-of-vocabulary words using the leave-one-out
technique (Niesler and Woodland, 1996).
Table 1: The enumeration and order of n-grams for
smoothing the distributions in Equation (3).
n-grams P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)
highest P^ (tijwi?1i?2ti?1i?2) P^ (wijwi?1i?2tii?2)
P^ (tijwi?1ti?1i?2) P^ (wijwi?1i?2tii?1)
P^ (tijwi?1i?2ti?1) P^ (wijwi?1tii?2)
P^ (tijti?1i?2) P^ (wijwi?1tii?1)
P^ (tijwi?1ti?1) P^ (wijwi?1ti)
P^ (tijti?1) P^ (wijtii?1)
lowest P^ (ti) P^ (wijti)
In preliminary experiments, we compared several
algorithms for smoothing the probability estima-
tions for our SuperARV LM. The best performance
was achieved by using the modied Kneser-Ney
smoothing algorithm initially introduced in (Chen
and Goodman, 1998) and adapting it by employing
a heldout data set to optimize parameters, includ-
ing cutos for rare n-grams, by using Powell?s search
(Press et al, 1988). Parameters are chosen to opti-
mize the perplexity on a heldout set.
In order to compare our SuperARV LM with a
word-based LM, we must use the following equation
to calculate the word perplexity (PPL):
PPL = 2En (4)
En  ? 1
N
N
X
i=1
log2 P^ (wijw
i?1
i?2)
 ? 1
N
N
X
i=1
log2
P
ti?2;i
P^ (witijwi?1i?2ti?1i?2)P^(wi?1i?2ti?1i?2)
P
ti?2;i?1
P^ (wi?1i?2t
i?1
i?2)
Equation (4) is used by class-based LMs to calculate
word perplexity (Heeman, 1998). Parser-based LMs
use a similar procedure that sums over parses.
The SuperARV LM is most closely related to
the almost-parsing-based LM developed by Srinivas
(1997). Srinivas? LM, based on the notion of a su-
pertag , the elementary structure of Lexicalized Tree-
Adjoining Grammar, achieved a perplexity reduction
compared to a conditional POS n-gram LM (Niesler
and Woodland, 1996). By comparison, our LM in-
corporates dependencies directly on words instead
of through nonterminals, uses more lexical features
than the supertag LM, uses joint instead of con-
ditional probability estimations, and uses modied
Kneser-Ney rather than Katz smoothing.
3 Evaluating the SuperARV
Language Model
Traditionally, the LM quality in speech recognition is
evaluated on two metrics: perplexity and WER, with
the former commonly selected as a less computation-
ally expensive alternative. We carried out two exper-
iments, one using the Wall Street Journal Penn Tree-
bank (WSJ PTB), a text corpus on which perplexity
can be measured and compared to other LMs, and
the Wall Street Journal Continuous Speech Recog-
nition (WSJ CSR) task, a speech corpus on which
both perplexity and WER can be evaluated after LM
rescoring. These two experiments compare our Su-
perARV LM to a baseline trigram, a POS LM that
was implemented using Equation (3) (where for this
model t represents POS tags instead of SuperARV
tags) and modied Kneser-Ney smoothing (as used
in the SuperARV LM), and one or more parser-based
LMs. Additionally, we evaluate the performance of a
conditional probability SuperARV LM (denoted cSu-
perARV) implemented following Equation (1) rather
than Equation (3) to evaluate the importance of us-
ing joint probability estimations.
For the WSJ PTB task, we compare the Super-
ARV LMs to the parser LMs developed by Chelba
(2000), Roark (2001), and Charniak (2001). Al-
though Srinivas (1997) developed an almost-parsing
supertag-based LM, we cannot compare his LM with
the other LMs because he used a small non-standard
subset of the WSJ PTB2 and a trainable supertag
LM is unavailable. Because none of the parser LMs
has been fully trained for the WSJ CSR task, it is
essential that we retrain them for comparison. The
availability of a trainable version of Chelba?s model
enables us to train and test on the CSR task; how-
ever, because we do not have access to a trainable
version of Charniak?s or Roark?s LMs, they are not
considered in the CSR task. Note that for lattice
rescoring, however, Roark found that Chelba?s model
achieves a greater reduction on WER than his LM
(Roark, 2001).
3.1 Evaluating on the WSJ PTB
To evaluate the perplexity of the LMs on the WSJ
PTB task, we adopted the conventions of Chelba
(2000), Roark (2001), and Charniak (2001) for pre-
processing the data. The vocabulary is limited to
the most common 10K words, with all words outside
this vocabulary mapped to hUNKi. All punctuation
is removed and no case information is retained. All
symbols and digits are replaced by the symbol N.
Sections 0-20 (929,564 words) are used as the train-
ing set for collecting counts, sections 21-22 (73,760
words) as the development set for tuning parameters,
and sections 23-24 (82,430 words) for testing.
The baseline trigram uses Katz back-o model
with Good-Turing discounting for smoothing. The
POS, cSuperARV, and SuperARV LMs were imple-
mented as described previously. The results for the
parser-based LMs were initially taken from the lit-
erature. The perplexity on the test set using each
LM and their interpolation with the corresponding
trigram (and the interpolation weight) are shown in
the top six rows of Table 2.
As can be seen in Table 2, the SuperARV LM ob-
tains the lowest perplexity of all of the LMs (and
so it is depicted in bold face). The SuperARV LM
achieves the greatest perplexity reduction of 29.19%
compared to the trigram, with Charniak?s interpo-
lated trihead LM a close second at 24.91%. The cSu-
perARV LM is clearly inferior to the SuperARV LM,
even after interpolation. This result highlights the
value of tight coupling of word, lexical feature, and
syntactic knowledge both at the data structure level
(which is the same for the SuperARV and cSuper-
ARV LMs) and at the probability model level (which
is dierent).
Notice that the cSuperARV, Chelba?s, Roark?s,
and Charniak?s LMs obtain an improvement in per-
formance when interpolated with a trigram; whereas,
2Using the same 180,000 word training and 20,000
word test set as (Srinivas, 1997), our SuperARV LM ob-
tains a perplexity of 92.76, compared to a perplexity of
101 obtained by the supertag LM.
the POS LM and the SuperARV LM do not benet
from trigram interpolation3. To gain more insight
into why a trigram is eectively interpolated with
some, but not all, of the LMs, we calculate the cor-
relation of the trigram with each LM. A standard
correlation is calculated between the probabilities as-
signed to each test set sentence by the trigram LM
and the LM in question. This technique has been
used in (Wang et al, 2002) to identify whether two
LMs can be eectively interpolated.
Since we have access to an executable ver-
sion of Charniak?s LM trained on the WSJ PTB
(ftp.cs.brown.edu/pub/nlparser) and a trainable ver-
sion of Chelba?s LM, we are able to calculate their
correlations with our trigram LM. Chelba?s LM was
retrained using more parameter reestimation itera-
tions than in (Chelba, 2000) to optimize the per-
formance. Table 2 shows the correlation between
each of the executable LMs and the trigram LM.
The POS LM has the highest correlation with the
trigram, closely followed by the SuperARV LM. Be-
cause these two LMs tightly integrate the word infor-
mation jointly with the tag distribution, the trigram
information is already represented. In contrast, the
cSuperARV LM and Chelba?s and Charniak?s parser-
based LMs have much lower correlations, indicating
they have much lower overlap with the trigram. Be-
cause the cSuperARV LM only uses weak word dis-
tribution information in probability estimations, it
leaves room for the trigram LM to compensate for
the lack of word knowledge. The correlations for the
parser-based LMs suggest that they capture dierent
aspects of the words? distributions in the language
than the words themselves.
3.2 Evaluating on the WSJ CSR Task
Next we compare the eectiveness of using the tri-
gram word-based, POS, cSuperARV, SuperARV,
and Chelba?s LMs in rescoring hypotheses generated
by a speech recognizer. The training set of the WSJ
CSR task is composed of the 1987-1989 les con-
taining 37,243,300 words. The speech data for the
training set is used for building the acoustic model;
whereas, the parse trees for the training set are gen-
erated following the policy that if the context-free
grammar constituent bracketing can be found in the
WSJ PTB, it becomes the parse tree for the training
sentence; otherwise, we use the corresponding tree in
the BLLIP treebank (Charniak et al, 2000). Since
WSJ CSR is a speech corpus, there is no punctua-
tion or case information. All words outside the pro-
vided vocabulary are mapped to hUNKi. Note that
3In the remaining experiments, the POS LM and the
SuperARV LM are not interpolated with a trigram.
Perplexity
LM 3gram Model Intp (Weight) r
POS 167.14 142.55 142.55 (1.0) 0.95
SuperARV 167.14 118.35 118.35 (1.0) 0.92
cSuperARV 167.14 150.01 143.83 (0.65) 0.68
Chelba (2000) 167.14 158.28 148.90 (0.64) N/A
Roark (2001) 167.02 152.26 137.26 (0.64) N/A
Charniak (2001) 167.89 130.20 126.07 (0.64) N/A
Chelba 167.14 153.76 147.70 (0.64) 0.73
Charniak 167.14 130.20 126.03 (0.64) 0.69
Table 2: Comparing perplexity results for each LM on the WSJ PTB test set. 3gram represents the word-based
trigram LM, Intp (weight) the LM interpolated with a trigram (and the interpolation weight), and r the correlation
value. N/A means not available.
the word-level tokenization of treebank texts diers
from that used in the speech recognition task with
the major dierences being: numbers (e.g., \1.2%"
versus \one point two percent"), dates (e.g., \Dec.
20, 2001" versus \December twentieth, two thou-
sand one") , currencies (e.g., \$10.25" versus \ten
dollars and twenty ve cents"), common abbrevia-
tions (e.g., \Inc." versus \Incorporated"), acronyms
(e.g., \I.B.M." versus \I. B. M."), hyphenated and
period-delimited phrases (e.g., \red-carpet" versus
\red carpet"), and contractions and possessives (e.g.,
\do n?t" versus \don?t"). The POS, parser-based,
and SuperARV LMs are all trained using the text-
based tokenization from the treebank. Hence, during
testing, a transformation converts the output of the
recognizer to a form compatible with the text-based
tokenization (Roark, 2001) for rescoring.
For testing the LMs, we use the four available
WSJ CSR evaluation sets: 1992 5K closed vocab-
ulary (denoted 92-5k) with 330 utterances and 5,353
words, 1993 5K closed vocabulary (93-5k) with 215
utterances and 3,849 words, 1992 20K open vocabu-
lary (92-20k) with 333 utterances and 5,643 words,
and 1993 20K (93-20k) with 213 utterances and
3,446 words. We also employ a development set for
each vocabulary size: 93-5k-dt (513 utterances and
8,635 words) and 93-20k-dt (252 utterances and 4,062
words).
The trigram provided by LDC for the CSR task
was used due to its high quality. Before evaluation,
all the other LMs (i.e., the POS LM, the cSuperARV
and SuperARV LMs, and Chelba?s LM) are retrained
on the training set trees for the CSR task. Parameter
tuning for the LMs on each task uses the correspond-
ing development set4.
Perplexity Results Table 3 shows the perplexity
results for each test set with the best result for each
4The interpolation weight for cSuperARV for lattice
rescoring was 0.63 on the 5k tasks and 0.60 on the 20k
tasks, and 0.68 and 0.65 for Chelba?s LM, respectively.
in bold face. The SuperARV LM yields the lowest
perplexity, with Chelba?s LM a close second. The
perplexity reductions for the SuperARV LM over
the trigram across the test sets are 53.19%, 53.63%,
34.33%, and 32.05%, which is even higher than on
the WSJ PTB task. This is probably due to the fact
that more training data was used for the CSR task
(37 million words versus 1 million words).
LM 92-5k 93-5k 92-20k 93-20k
3gram 45.61 50.51 106.52 109.22
POS 44.21 30.26 98.79 96.64
cSuperARV 36.53 28.50 86.83 89.12
SuperARV 21.35 23.42 69.95 74.22
Chelba 23.92 25.07 77.16 79.37
Table 3: Comparing perplexity results for each LM on
the WSJ CSR test sets.
Rescoring Lattices Next using the same LMs, we
rescored the lattices generated by an acoustic recog-
nizer built using HTK (Ent, 1997). For each test
set sentence, we generated a word lattice. We tuned
the parameters of the LMs using the lattices on the
corresponding development sets to minimize WER.
Lattices were rescored using a Viterbi search for each
LM.
Table 4 shows the WER and sentence accuracy
(SAC) after rescoring lattices using each LM, with
the lowest WER and highest SAC for each test set
presented in bold face. We also give the lattice
WER/SAC which denes the best accuracy possible
given perfect knowledge. As can be seen from Table
4, the SuperARV LM produces the best reduction
in WER with Chelba?s LM the second best. When
rescoring lattices on the 92-5k, 93-5k, 92-20k, and
93-20k test sets, the SuperARV LM yields a relative
WER reduction of 13.54%, 9.70%, 8.64%, and 3.12%
compared to the trigram, respectively. SAC results
are similar: the SuperARV LM achieves an absolute
increase on SAC of 4.24%, 6.97%, 2.7%, and 3.75%,
compared to the trigram. Note that Chelba?s LM
tied once with the SuperARV LM on 93-20k SAC,
but always obtained higher WER across the four test
sets. Because Chelba?s LM focuses on developing the
complete parse structure for a word sequence, it en-
forces more strict pruning based on the entire sen-
tence. As can be seen in Table 4, the cSuperARV
LM, even when interpolated with a trigram LM, ob-
tains a lower accuracy than our SuperARV LM. This
result is consistent with the hypothesis that a con-
ditional model suers from label bias (Laerty et al,
2001).
The WER reported by Chelba (2000) on the 93-
20k test set was 13.0%. This WER is lower than
what we obtained for Chelba?s retrained LM on the
same task. This disparity is due to the fact that a
higher quality acoustic decoder was used in (Chelba,
2000), which is not available to us. We further com-
pare the LMs on Dr. Chelba?s 93-20K lattices kindly
provided by him, with the rescoring results shown
in the last column of Table 4. We observe that
Chelba?s retrained LM improves his original result,
but the SuperARV LM still obtains a greater accu-
racy. Sign tests show that the dierences between
the accuracies achieved by the SuperARV LM and
the trigram, POS, and cSuperARV LMs are statis-
tically signicant. Although there is no signicant
dierence between the SuperARV LM and Chelba?s
LM, the SuperARV LM has a much lower complexity
than Chelba?s LM.
4 Investigating the Knowledge
Source Contributions
Next, we attempt to explain the contrast between
the encouraging results from our SuperARV LM and
the reported poor performance of several probabilis-
tic dependency grammar models, i.e., the traditional
probabilistic dependency grammar (PDG) LM, the
probabilistic link grammar (PLG) (Laerty et al,
1992) LM, and Zeman?s probabilistic dependency
grammar model (ZPDG) (Hajic et al, 1998). ZPDG
was evaluated on the Prague Dependency Treebank
(Hajic, 1998) during the 1998 Johns Hopkins sum-
mer workshop (Hajic et al, 1998) and produced
a much lower parsing accuracy (under 60%) than
Collins? probabilistic context-free grammar parser
(80%) (Collins, 1996). Fong et al (1995) evalu-
ated the probabilistic link grammar LM described
in (Laerty et al, 1992) on small articial corpora
and found that the LM has a greater perplexity than
a standard bigram. Additionally, only a modest im-
provement on the bigram was achieved after Fong
and Wu (1995) revised the model to make grammar
rule learning feasible.
One possible reason for their poor performance, es-
pecially in the light of our SuperARV LM results, is
that these probabilistic dependency grammar mod-
els do not utilize sucient knowledge to achieve a
high level of accuracy. The knowledge sources the
SuperARV LM uses, represented as components of
the structure shown in Figure 1, include: lexical
category (denoted c), lexical features (denoted f),
role label or link type information (denoted L), a
governor role dependency relation constraint (R, L,
UC) (denoted g), a set of need role dependency rela-
tion constraints (R;L; UC)+ (denoted n), and mod-
iee constraints represented as the lexical category
of the modiee for each role (denoted m). Table
5 summarizes the knowledge sources that each of
the probabilistic dependency grammar models uses.
To determine whether the poor performance of the
three probabilistic dependency grammar models re-
sults from our hypothesis that they utilize insucient
knowledge, we will evaluate our SuperARV LM af-
ter eliminating those knowledge sources that are not
used by each of these models. Additionally, we will
evaluate the contribution of each of the knowledge
sources to the predictiveness of our SuperARV LM.
We use the methodology of selectively ignoring dif-
ferent types of knowledge as constraints to evaluate
the knowledge source contributions to our SuperARV
LM, as well as to approximate the performance of
the other probabilistic dependency grammar models.
The framework of CDG, on which our SuperARV LM
is built, allows constraints to be tightened by adding
more knowledge sources or loosened by ignoring cer-
tain knowledge. The SuperARV structure inherits
this capability from CDG; selective constraint re-
laxation is implemented by eliminating one or more
knowledge source in K = fc; f; L; g; n;mg from the
SuperARV structure. We have constructed nine dif-
ferent LMs based on reduced SuperARV structures
denoted SARV-k (i.e., a SuperARV structure after
removing k with k  K), where ?k represents the
deletion of a subset of knowledge types (e.g., f , mn,
cgmn). Each model is described next.
Modiee constraints potentially hamper grammar
generality, and so we consider their impact by delet-
ing them from the LM by using the SARV-m struc-
ture. Need roles are important for capturing the
structural requirements of dierent types of words
(e.g., subcategorization), and we investigate their ef-
fects by using the SARV-n structure. The model
based on SARV-L is built to investigate the im-
portance of link type information. We can investi-
gate the contribution of the combination of m and
n, fundamental to the enforcement of valency con-
straints, by using the SARV-mn structure. The
model based on SARV-f is used to evaluate whether
Our HTK Lattices Chelba?s
92-5k 93-5k 92-20k 93-20k 93-20k lattices
LM WER(SAC) WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52) 13.72(36.18)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39) 13.51(37.96)
cSuperARV 3.89(65.15) 6.42(48.84) 10.51(38.44) 14.45(32.86) 13.32 (38.22)
SuperARV 3.83(65.76) 6.24(50.23) 10.15(39.64) 14.28(34.27) 12.87(42.02)
Chelba 3.85(65.45) 6.26(49.77) 10.19(39.34) 14.36(34.27) 12.93(40.48)
lattice 1.79(79.40) 2.16(73.95) 4.93(59.46) 6.65(52.11) 3.41 (68.86)
accuracy
Table 4: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets.
knowledge PDG PLG ZPDG SARV
source
word identity X X lemma X
lexical X X X
category (c)
lexical morpho- X
features (f) logical
features
link type (L) X X X
link direction X X X X
(UC)
valency (n) X
modiee X X
constraints (m)
Table 5: Knowledge sources used by the three prob-
abilistic dependency grammar models compared to our
SuperARV LM. Note link type is dened as L and link
direction is dened as UC in the SuperARV structure.
lexical features improve or degrade LM quality. The
model based on SARV-fmn is very similar to the
standard probabilistic dependency grammar LM, in
which only word, POS, link type, and link di-
rection information is used for probability estima-
tions. The model based on SARV-gmn uses a fea-
ture augmentation of POS, and the model based on
SARV-cgmn uses lexical features only. Addition-
ally, we built the model ZPDG-SARV to approxi-
mate ZPDG. Zeman?s PDG (Hajic et al, 1998) dif-
fers signicantly from our original SuperARV LM in
that it ignores label information L and some lexi-
cal feature information (the morphological tags do
not include some lexical features having influence
on syntax, denoted syntactic lexical features, i.e.,
gapp, inverted, mood, type, case, voice), and does
not enforce valency constraints (instead, the model
only counts the number of links associated with a
word without discriminating whether the links repre-
sent governing or linguistic structural requirements).
Also, word identity information is not used, instead,
the model uses a loose integration of a word?s lemma
and its morphological tag. Given this analysis, we
built the model ZPDG-SARV based on a structure
including lexical category, morphological features,
LM 92-5k 93-5k 92-20k 93-20k
3gram 45.61 50.51 106.52 109.22
SARV-cgmn 45.58 48.37 102.00 104.59
ZPDG-SARV 45.50 47.98 101.89 104.21
POS 44.21 30.26 98.79 96.64
SARV-gmn 43.16 27.75 96.69 93.25
SARV-fmn 45.01 27.42 96.23 93.16
SARV-f 42.33 27.06 94.87 90.20
SARV-mn 40.38 26.96 90.23 89.54
SARV-n 35.02 26.08 87.32 88.04
SARV-L 28.76 25.71 82.45 84.82
SARV-m 26.86 25.58 80.24 83.12
SARV 21.35 23.42 69.95 74.22
Table 6: Comparing perplexity results for each LM on
the WSJ CSR test sets. The LMs appear in decreasing
order of perplexity.
and (G;UC;MC).
Table 6 shows the perplexity results on the WSJ
CSR test sets ordered from highest to lowest for
each test set, with the best result for each in bold
face. The full SuperARV LM yields the lowest per-
plexity. We found that ignoring modiee constraints
(SARV-m) increases perplexity the least, and ignor-
ing link type information (SARV-L) and need role
constraints (SARV-n) are a little worse than that.
Ignoring both knowledge sources (SARV-mn) should
result in even greater degradation, which is veried
by the results. However, ignoring lexical features
(SARV-f) produces an even greater increase in per-
plexity than relaxing both m and n. The SARV-
fmn, which is closest to the traditional probabilistic
dependency grammar LM, shows fairly poor qual-
ity, not much better than the POS LM. One might
hypothesize that lexical features individually con-
tribute the most to the overall performance of the Su-
perARV LM. However, using this knowledge source
by itself (SARV-cgmn) results in dramatic degrada-
tion on perplexity, in fact even worse than that of the
POS LM, but still slightly better than the baseline
trigram. However, as demonstrated by SARV-gmn,
the constraints from lexical features are strength-
ened by combining them with POS. Given the de-
scriptions in Table 5, we can approximate PLG by
a model based on a SuperARV structure eliminat-
ing f and m (which should have a quality between
SARV-f and SARV-fmn). It is noticeable that with-
out word identity information, syntactic lexical fea-
tures, and valency constraints, the ZPDG-SARV LM
performs worse than the POS-based LM and only
slightly better than the LM based on SARV-cgmn.
This suggests that ZPDG can be strengthened by
incorporating more knowledge.
The same ranking of the performance of the LMs
was obtained for WER/SAC after rescoring the lat-
tices using each LM, as shown in Table 7. Our exper-
iments with relaxed SuperARV LMs suggest likely
methods for improving PDG, PLG, and ZPDG mod-
els. The tight integration of word identity, lexical
category, lexical features, and structural dependency
constraints is likely to improve their performance.
Clearly the investigated knowledge sources are quite
synergistic, and their tight integration achieves the
greatest improvement on both perplexity and WER.
5 Conclusions
We have compared our SuperARV LM to a variety of
LMs and found that it achieves both perplexity and
WER reductions compared to a trigram, and despite
the fact that it is an almost-parsing LM, it outper-
forms (or performs comparably to) the more com-
plex parser-based LMs on both perplexity and rescor-
ing accuracy. Additional experiments reveal that se-
lecting a joint instead of a conditional probabilistic
model is an important factor in the performance of
our SuperARV LM. The SuperARV structure pro-
vides a flexible framework that tightly couples a va-
riety of knowledge sources without combinatorial ex-
plosion. We found that although each knowledge
source contributes to the performance of the LM, it
is the tight integration of the word level knowledge
sources (word identity, POS, and lexical features) to-
gether with the structural information of governor
and subcategorization dependencies that produces
the best level of LM performance. We are currently
extending the almost-parsing SuperARV LM to a full
parser-based LM.
6 Acknowledgments
This research was supported by Intel, Purdue Re-
search Foundation, and National Science Founda-
tion under Grant No. IRI 97-04358, CDA 96-17388,
and BCS-9980054. We would like to thank the
anonymous reviewers for their comments and sug-
gestions. We would also like to thank Dr. Charniak,
Dr. Chelba, and Dr. Srinivas for their help with this
research eort. Finally, we would like to thank Yang
Liu (Purdue University) for providing us with the
WSJ CSR test set lattices.
References
R. Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of ACL?2001.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and
M. Johnson. 2000. BLLIP WSJ Corpus. CD-
ROM. Linguistics Data Consortium.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL?2001.
C. Chelba, F. Jelinek, and S. Khudanpur. 1997.
Structure and performance of a dependency lan-
guage model. In Proceedings of Eurospeech, vol-
ume 5, pages 2775{2778.
C. Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
S. F. Chen and J. T. Goodman. 1998. An empir-
ical study of smoothing techniques for language
modeling. Technical report, Harvard University,
Computer Science Group.
M. J. Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL?1996, pages 184{191.
Entropic Cambridge Research Laboratory, Ltd.,
1997. HTK: Hidden Markov Model Toolkit V2.1.
E. W. Fong and D. Wu. 1995. Learning restricted
probabilistic link grammars. Technical Report
HKUST-CS95-27, University of Science and Tech-
nology, Clear Water Bay, Hong Kong.
L. Galescu and E. K. Ringger. 1999. Augmenting
words with linguistic information for n-gram lan-
guage models. In Proceedings of Eurospeech.
J. Goodman. 1997. Probabilistic feature grammars.
In Proceedings of the Fourth international work-
shop on parsing technologies.
J. Goodman. 2001. A bit of progress in language
modeling, extended version. Technical Report
MSR-TR-2001-72, Microsoft Research, Redmond,
WA.
J. Hajic, E. Brill, M. Collins, B. Hladka, D. Jones,
C. Kuo, L. Ramshaw, O. Schwartz, C. Tillmann,
and D. Zeman. 1998. Core natural language
processing technology applicable to multiple lan-
guages { Workshop ?98. Technical report, Johns
Hopkins Univ.
J. Hajic. 1998. Building a syntactically anno-
tated corpus: The Prague Dependency Treekbank.
In Issues of Valency and Meaning ( Festschrift
for Jarmila Panevova), pages 106{132. Carolina,
Charles University, Prague.
92-5k 93-5k 92-20k 93-20k
LM WER(SAC) WER(SAC) WER(SAC) WER(SAC)
3gram 4.43(61.52) 6.91(43.26) 11.11(36.94) 14.74(30.52)
SARV-cgmn 4.11(62.12) 6.78(45.12) 10.92(37.24) 14.63(31.46)
ZPDG-SARV 4.11(62.44) 6.71(46.02) 10.92(37.24) 14.63(31.65)
POS 3.92(64.85) 6.55(47.91) 10.58(38.14) 14.54(32.39)
SARV-gmn 3.92(64.85) 6.52(47.91) 10.56(38.14) 14.51(32.39)
SARV-fmn 3.92(64.85) 6.50(48.37) 10.53(38.14) 14.51(32.39)
SARV-f 3.92(64.85) 6.47(48.37) 10.49(38.14) 14.45(32.86)
SARV-mn 3.92(64.85) 6.44(48.37) 10.47(38.44) 14.42(32.86)
SARV-n 3.89(65.15) 6.39(48.37) 10.40(38.74) 14.39(33.33)
SARV-L 3.85(65.15) 6.29(48.92) 10.32(39.04) 14.39(33.33)
SARV-m 3.85(65.15) 6.29(49.77) 10.24(39.34) 14.35(33.80)
SARV 3.83(65.76) 6.24(50.23) 10.15(39.34) 14.28(34.27)
Table 7: Comparing WER and SAC after rescoring lattices using each LM on WSJ CSR 5k- and 20k- test sets. The
LMs appear in decreasing order of WER.
M. P. Harper and R. A. Helzerman. 1995. Exten-
sions to constraint dependency parsing for spoken
language processing. Computer Speech and Lan-
guage, 9:187{234.
M. P. Harper and W. Wang. 2001. Approaches for
learning constraint dependency grammar from cor-
pora. In Proceedings of the Grammar and Nat-
ural Language Processing Conference, Montreal,
Canada.
M. P. Harper, S. A. Hockema, and C. M. White.
1999. Enhanced constraint dependency grammar
parsers. In Proceedings of the IASTED Interna-
tional Conference on Articial Intelligence and
Soft Computing.
M. P. Harper, C. M. White, W. Wang, M. T.
Johnson, and R. A. Helzerman. 2000. Eec-
tiveness of corpus-induced dependency grammars
for post-processing speech. In Proceedings of
NAACL?2000.
P. A. Heeman. 1998. POS tagging versus classes
in language modeling. In Proceedings of the 6th
Workshop on Very Large Corpora, Montreal.
F. Jelinek. 1990. Self-organized language modeling
for speech recognition. In Alex Waibel and Kai-Fu
Lee, editors, Readings in Speech Recognition. Mor-
gan Kaufman Publishers, Inc., San Mateo, CA.
M. Johnson. 2001. Joint and conditional estimation
of tagging and parsing models. In Proceedings of
ACL?2001.
J. D. Laerty, D. Sleator, and D. Temperley. 1992.
Grammatical trigrams: A probabilistic model of
link grammar. In Proc. of AAAI Fall Symp. Prob-
abilistic Approaches to Natural Language, Cam-
bridge, MA.
J. Laerty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for
segmenting and labeling sequence data. In Pro-
ceedings of ICML?2001.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313{330.
H. Maruyama. 1990. Structural disambiguation
with constraint propagation. In The Proceedings
of ACL?1990, pages 31{38.
T. R. Niesler and P. C. Woodland. 1996. A variable-
length category-based N-gram language model. In
Proceedings of ICASSP, volume 1, pages 164{167.
W. H. Press, B. P. Flannery, S. A. Teukolsky, and
W. T. Vetterling. 1988. Numerical Recipes in C.
Cambridge University Press, Cambridge.
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett.
1988. A database for continuous speech recogni-
tion in a 1000-word domain. In Proceedings of
ICASSP?1988, pages 651{654.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249{276.
R. Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88:1270{1278.
B. Srinivas. 1997. Complexity of lexical descriptions
and its relevance to partial parsing. Ph.D. thesis,
University of Pennsylvania.
W. Wang and M. P. Harper. 2001. Investigating
probabilistic constraint dependency grammars in
language modeling. Technical Report TR-ECE-
01-4, Purdue University, School of Electrical En-
gineering.
W. Wang, Y. Liu, and M. P. Harper. 2002. Rescor-
ing eectiveness of language models using dierent
levels of knowledge and their integration. In Pro-
ceedings of ICASSP?2002.
Proceedings of NAACL-HLT 2013, pages 703?708,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Cross-language Study on Automatic Speech Disfluency Detection
Wen Wang
SRI International
Menlo Park, CA
wwang@speech.sri.com
Andreas Stolcke
Microsoft Research
Mountain View, CA
anstolck@microsoft.com
Jiahong Yuan, Mark Liberman
University of Pennsylvania
Philadelphia, PA
jiahong.yuan@gmail.com
markyliberman@gmail.com
Abstract
We investigate two systems for automatic dis-
fluency detection on English and Mandarin
conversational speech data. The first system
combines various lexical and prosodic fea-
tures in a Conditional Random Field model for
detecting edit disfluencies. The second system
combines acoustic and language model scores
for detecting filled pauses through constrained
speech recognition. We compare the contri-
butions of different knowledge sources to de-
tection performance between these two lan-
guages.
1 Introduction
Speech disfluencies are common phenomena in
spontaneous speech. They consist of spoken words
and phrases that represent self-correction, hesitation,
and floor-grabbing behaviors, but do not add seman-
tic information; removing them yields the intended,
fluent utterance. The presence of disfluencies in
conversational speech data can cause problems for
both downstream processing (parsing and other nat-
ural language processing tasks) and human readabil-
ity of speech transcripts. There has been much re-
search effort on automatic disfluency detection in
recent years (Shriberg and Stolcke, 1997; Snover
et al, 2004; Liu et al, 2006; Lin and Lee, 2009;
Schuler et al, 2010; Georgila et al, 2010; Zwarts
and Johnson, 2011), particularly from the DARPA
EARS (Effective, Affordable, Reusable Speech-to-
Text) MDE (MetaData Extraction) (DARPA Infor-
mation Processing Technology Office, 2003) pro-
gram, which focused on the automatic transcription
of sizable amounts of speech data and rendering
such transcripts in readable form, for both conversa-
tional telephone speech (CTS) and broadcast news
(BN).
However, the EARS MDE effort was focused on
English only, and there hasn?t been much research
on the effectiveness of similar automatic disfluency
detection approaches for multiple languages. This
paper presents three main innovations. First, we
extend the EARS MDE-style disfluency detection
approach combining lexical and prosodic features
using a Conditional Random Field (CRF) model,
which was employed for detecting disfluency on En-
glish conversational speech data (Liu et al, 2005),
to Mandarin conversational speech, as presented in
Section 2. Second, we implement an automatic
filled pause detection approach through constrained
speech recognition, as presented in Section 3. Third,
for both disfluency detection systems, we compare
side-by-side contributions of different knowledge
sources to detection performance for two languages,
English and Mandarin, as presented in Section 4.
Conclusions appear in Section 5.
2 EARS MDE Style Automatic Disfluency
Detection
We focus on two types of disfluencies, Fillers and
Edit disfluencies, following the EARS MDE disflu-
ency types modeled in (Liu et al, 2006). Fillers in-
clude filled pauses (FP), discourse markers (DM),
and explicit editing terms (ET). FPs are words used
by the speakers as floor holders to maintain con-
trol of a conversation. They can also indicate hes-
itations of the speaker. In this work, English FPs
703
comprise uh and um, based on English CTS cor-
pora. For Mandarin, Zhao and Jurafsky found that
Mandarin speakers intensively used both demonstra-
tives zhege (literally ?this?) and nage (literally ?that?)
and uh/mm as FPs based on a large speech corpus of
Mandarin telephone conversation (Zhao and Juraf-
sky, 2005). We study the same set of Chinese FPs in
this study. DMs are words or phrases related to the
structure of the discourse and help taking or keeping
a turn, or serving as acknowledgment, for example,
I mean, you know. An explicit ET is an editing term
in an edit disfluency that is not an FP or a DM. For
example, we have two action items  sorry three ac-
tion items from the meeting, where sorry is an ex-
plicit ET.
Edit disfluencies involve syntactically relevant
content that is either repeated, revised, or aban-
doned. The basic pattern for edit disfluencies has
the form (reparandum)  <editing term> correc-
tion. The reparandum is the portion of the utterance
that is corrected or abandoned entirely (in the case
of restarts). An interruption point (IP), marked with
?? in the pattern, is the point at which the speaker
breaks off the original utterance and then repeats,
revises, or restarts the utterance. The editing term
is optional and consists of one or more filler words.
The correction is the portion of the utterance that
corrects the original reparandum. Revisions denote
the cases when a speaker modifies the original utter-
ance with a similar syntactic structure, e.g., we have
two action items  sorry three action items from the
meeting. Restarts denote the cases when a speaker
abandons an utterance or a constituent and restarts
all over again, e.g., He  I like this idea.
We used a CRF model to combine lexical features,
shallow syntactic features, and prosodic features for
joint detection of edit words and IP words. A CRF
defines a global log-linear distribution of the state
(or label) sequence E conditioned on an observation
sequence, in our case including the word sequence
W and the features F , and optimized globally over
the entire sequence considering the context event in-
formation for making decisions at each point. We
used the Mallet package (McCallum, 2002) to im-
plement the CRF model. We used a first-order model
that includes only two sequential events in the fea-
ture set. The CRF model is trained to maximize
the conditional log-likelihood of a given training
set P (EjW; F ). During testing, the most likely se-
quence E is found using the Viterbi algorithm. To
avoid over-fitting, a zero-mean Gaussian prior (Mc-
Callum and Li, 2003) was applied to the parame-
ters, where the variance of the prior was optimized
on the development test set. Each word is associ-
ated with a class label, representing whether it is
an edit word or not. We included IP in the target
classes and used five states, as outside edit (O), be-
gin edit with an IP (B-E+IP), begin edit (B-E), in-
side edit with an IP (I-E+IP), and inside edit (I-
E) (Liu et al, 2006). State transitions are also the
same as in (Liu et al, 2006). We built a Hidden
Markov Model (HMM) based part-of-speech (POS)
taggers for English conversational speech and Man-
darin broadcast conversation data. After employing
the co-training approach described in (Wang et al,
2007), we achieved 94% POS tagging accuracy for
both data sets. The features for CRF modeling in-
clude: n-grams from words and automatically gen-
erated POS tags, speaker turns, whether there is a
repeated word sequence ending at a word bound-
ary, whether a word is a fragment, whether there
is a predefined filler phrase after the word bound-
ary, and the prosody model posterior probabilities
from a decision tree model (Shriberg and Stolcke,
1997) and discretized by cumulative binning (Liu et
al., 2006). The prosodic features were computed
for each interword boundary from words and pho-
netic alignments of the manual transcriptions. We
extracted the same set of prosodic features for En-
glish and Mandarin data, based on duration, funda-
mental frequency (f0), energy, and pause informa-
tion, and nonprosodic information such as speaker
gender and speaker change, for training and apply-
ing the decision-tree-based prosody model (Liu et
al., 2006).
We implemented a rule-based system for filler
word detection. We defined a list of possible Chi-
nese and English filler words, including filled pauses
and discourse markers. The rules also explore POS
tags assigned by our Chinese and English POS tag-
gers.
704
3 Constrained Speech Recognition for
Filled Pause Detection
We also propose an alternative approach for auto-
matic detection of FPs given speech transcripts that
omit FPs but are otherwise accurate. This approach
is motivated by situations where only an edited,
?cleaned-up? transcript is available, but where an
accurate verbatim transcript is to be recovered au-
tomatically. We treat this task as a constrained
speech recognition problem, and investigate how ef-
fectively it is solved by a state-of-the-art large vo-
cabulary continuous speech recognition (LVCSR)
system. Hence, this approach can be considered as
combining LVCSR acoustic model (AM) and lan-
guage model (LM) knowledge sources in a search
framework for FP detection. Compared to the FP
detection component in the disfluency detection sys-
tems described in Section 2, this alternative ap-
proach explores different knowledge sources. In
particular, the AMs explore different front-end fea-
tures compared to the lexical and prosodic features
explored in those disfluency detection systems pre-
sented in Section 2. Details of the front-end features
are illustrated below.
We evaluated this approach on both English and
Mandarin conversational speech. For detecting FPs
in English conversational speech, we used a mod-
ified and simplified form of the recognition sys-
tem developed for the 2004 NIST Rich Transcrip-
tion Conversational Telephone Speech (CTS) eval-
uations, described in (Stolcke et al, 2006). The
first pass of the recognizer uses a within-word
MFCC+MLP model (i.e, trained on Mel-frequency
cepstral coefficient (MFCC) features augmented
with Multi-Layer Perceptron (MLP) based phone-
posterior features), while the second pass uses a
cross-word model trained on Perceptual Linear Pre-
diction (PLP) features adapted (by speaker) to the
output of the first pass. For purposes of FP detec-
tion, the recognition is constrained to a word lat-
tice formed by the manually transcribed non-FP ref-
erence words, with optional FP words inserted be-
tween any two words and at the beginning and end
of each utterance. Both first and second pass de-
coding was constrained by the optional-FP lattices.
In the second pass, HTK lattices were generated
with bigram LM probabilities and rescored with a
4-gram LM. The consensus decoding output from
the rescored lattices was used for scoring FP detec-
tion. The system thus evaluates the posterior prob-
ability of an FP at every word boundary using both
acoustic model (AM) and language model (LM) ev-
idence. The acoustic model for the English recog-
nition system was trained on about 2300 hours of
CTS data. The language models (which models FP
like any other word) are bigram and 4-gram statisti-
cal word n-gram LMs estimated from the same data
plus additional non-CTS data and web data.
For detecting FPs in Mandarin broadcast con-
versation speech, we used a modified form of
the recognition system developed for the 2008
DARPA GALE (Global Autonomous Language Ex-
ploitation) Speech-to-Text evaluation, described in
(Lei et al, 2009). The system conducted a con-
strained decoding on the optional-FP lattices, using
a speaker-independent within-word triphone MPE-
trained MFCC+pitch+MLP model and a pruned
trigram LM. For the Mandarin ASR system, the
MFCC+MLP front-end features were augmented
with 3-dimension smoothed pitch features (Lei et al,
2006). HTK lattices were generated with probabil-
ities from the pruned trigram LM and rescored by
the full trigram LM. The consensus decoding output
from the rescored lattices was used for scoring FP
detection. The AMs for this system were trained on
1642 hours of Mandarin broadcast news and conver-
sation speech data and the LMs were trained on 1.4
billion words comprising a variety of resources. De-
tails of training data and system development were
illustrated in (Lei et al, 2009).
This procedure is similar to forced aligning the
word lattices to the audio data (Finke and Waibel,
1997). Both Finke et al?s approach (Finke and
Waibel, 1997) and our approach built a lattice from
each transcription sentence (in our approach, op-
tional filled pauses are inserted between any two
words and at the beginning and end of each utter-
ance). Then Finke et al force-aligned the lattice
with utterance; whereas, we used multi-pass con-
strained decoding with within-word and cross-word
models, MLLR adaptation of the acoustic models,
and rescoring with a higher-order n-gram LM, so the
performance will be better than just flexible align-
ment to the lattices. Note that when constructing
the word lattices with optional FP words, for En-
705
glish, the optional FP words are a choice between
uh and um. For Mandarin, the optional FP words
are a choice between uh, mm, zhege, and nage. We
assigned equal weights to FP words.
4 Experimental Results
Scoring of EARS MDE-style automatic disfluency
detection output is done using the NIST tools 1,
computing the error rate as the average number of
misclassified words per reference event word. For
English, the training and evaluation data were from
the 40 hours CTS data in the NIST RT-04F MDE
training data including speech, their transcriptions
and disfluency annotations by LDC. We randomly
held out two 3-hour subsets from this training data
set for evaluation and parameter tuning respectively,
and used the remaining data for training. Note
that for Mandarin, there is no LDC released Man-
darin MDE training data. We adapted the English
MDE annotation guidelines for Mandarin and man-
ually annotated the manual transcripts of 92 Man-
darin broadcast conversation (BC) shows released
by LDC under the DARPA GALE program, for edit
disfluencies and filler words. We randomly held out
two 3-hour subsets from the 92 shows for evalu-
ation and parameter tuning respectively, and man-
ually corrected disfluency annotation errors on the
evaluation set.
Table 1 shows the results in NIST error rate (%)
for edit word, IP, and filler word detection. We ob-
serve that adding POS features improves edit word,
edit IP, and filler word detection for both languages,
and adding a prosody model produced further im-
provement (note that filler word detection systems
did not employ prosodic features). The gains from
combining the word, POS, and prosody model over
the word n-gram baseline are statistically significant
for both languages (confidence level p < 0:05 using
matched pair test). Also, adding the prosody model
over word+POS yielded a larger relative gain in edit
word+IP detection performance for Mandarin than
for English data. A preliminary study of these re-
sults has shown that the prosody model contributes
differently for different types of disfluencies for En-
glish and Mandarin conversational speech and we
will continue this study in future work. We also plan
1www.itl.nist.gov/iad/mig/tests/rt/2004-fall/index.html
to investigate the prosodic features considering the
special characteristics of edited disfluencies in Man-
darin studied in (Lin and Lee, 2009).
Table 1: NIST error rate (%) for edit word, IP, and filler
word detection on the English and Mandarin test set,
using word n-gram features, POS n-gram features, and
prosody model.
Feature NIST Error Rate (%)
Edit Word Edit IP Filler Word
English
Word 53.0 38.7 31.2
+POS 52.6 38.2 29.8
++Prosody 52.3 38.0 29.8
Mandarin
Word 58.5 42.8 33.4
+POS 57.7 42.1 32.9
++Prosody 56.9 41.5 32.9
For evaluating constrained speech recognition for
FP detection, the English test set of conversational
speech data and word transcripts is derived from
the CTS subset of the NIST 2002 Rich Transcrip-
tion evaluation. The waveforms were segmented ac-
cording to utterance boundaries given by the human-
generated transcripts, resulting in 6554 utterance
segments with a total duration of 6.8 hours. We then
excluded turns that have fewer than five tokens or
have two or more FPs in a row (such as ?uh um? and
?uh, uh?), resulting in 3359 segments. This yields
the test set from which we computed English FP de-
tection scores. The transcripts of this test set con-
tain 54511 non-FP words and 1394 FPs, transcribed
as either uh or um. When evaluating FP detection
performance, these two orthographical forms were
mapped to a single token type, so recognizing one
form as the other is not penalized. The Mandarin
test set is the DARPA GALE 2008 Mandarin speech-
to-text development test set of 1 hour duration. The
transcripts of this test set contain 9820 non-FP words
and 370 FP words, transcribed as uh, mm, zhege,
and nage. We collapsed them to a single token type
for FP scoring. We evaluated FP detection perfor-
mance in terms of both false alarm (incorrect detec-
tion) and miss (failed detection) rates, shown in Ta-
ble 2. We observed that adding pronunciation scores
didn?t change the P
fa
and P
miss
. On the English
706
test set, adding LM scores degraded P
miss
but im-
proved P
fa
. However, on the Mandarin test set, in-
creasing LM weight improved both P
miss
and P
fa
,
suggesting that for the Mandarin LVCSR system in
this study, the LM could provide complementary in-
formation to the AM to discriminate FP and non-FP
words.
Table 2: Probabilities of false alarms (FAs) and misses in
FP detection on the English and Mandarin test set w.r.t.
acoustic model weight w
a
, language model weight w
g
,
and pronunciation score weight w
p
.
fw
a
; w
g
; w
p
g FAs (%) Misses (%)
English
f1,0,8g 1.76 3.23
f1,8,8g 1.18 4.73
Mandarin
f1,0,8g 1.19 19.68
f1,8,8g 0.76 16.76
5 Conclusion
In conclusion, we have presented two automatic dis-
fluency detection systems, one combining various
lexical and prosodic features, and the other com-
bining LVCSR acoustic and language model knowl-
edge sources. We observed significant improve-
ments in combining lexical and prosodic features
over just employing word n-gram features, for both
languages. When combining AM and LM knowl-
edge sources for FP detection in constrained speech
recognition, we found increasing LM weight im-
proved both false alarm and miss rates for Mandarin
but degraded the miss rate for English.
Acknowledgments
The authors thank all the anonymous reviewers of
this paper for valuable suggestions. This work is
supported in part by NSF grant IIS-0964556.
References
DARPA Information Processing Technology Office.
2003. Effective,affordable, reusable speech-to-text
(EARS). http://www.darpa.mil/ipto/programs/ears.
Michael Finke and Alex Waibel. 1997. Flexible tran-
scription alignment. In IEEE Workshop on Speech
Recognition and Understanding, pages 34?40.
K. Georgila, N. Wang, and J. Gratch. 2010. Cross-
domain speech disfluency detection. In Proceedings
of SIGDIAL, pages 237?240, Tokyo.
X. Lei, M. Siu, M.Y. Hwang, M. Ostendorf, and T. Lee.
2006. Improved tone modeling for Mandarin broad-
cast news speech recognition. In Proceedings of Inter-
speech.
X. Lei, W. Wu, W. Wang, A. Mandal, and A. Stolcke.
2009. Development of the 2008 SRI Mandarin speech-
to-text system for broadcast news and conversation. In
Proceedings of Interspeech, Brighton, UK.
C. K. Lin and L. S. Lee. 2009. Improved features
and models for detecting edit disfluencies in transcrib-
ing spontaneous mandarin speech. IEEE Transac-
tions on Audio, Speech, and Language Processing,
17(7):1263?1278, September.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, and Mary
Harper. 2005. Comparing HMM, maximum entropy,
and conditional random fields for disfluency detec-
tion. In Proc. Interspeech, pages 3313?3316, Lisbon,
September.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526?1540, September. Special Issue
on Progress in Rich Transcription.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields. In
Proceedings of the CoNLL.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.
2010. Broad-coverage incremental parsing using
human-like memory constraints. Computational Lin-
guistics, 36(1).
E. Shriberg and A. Stolcke. 1997. A prosody-only
decision-tree model for disfluency detection. In Pro-
ceedings of Eurospeech, pages 2383?2386.
M. Snover, B. Dorr, and R. Schwartz. 2004. A lexically-
driven algorithm for disfluency detection. In Su-
san Dumais, Daniel Marcu, and Salim Roukos, edi-
tors, Proc. HLT-NAACL, Boston, May. Association for
Computational Linguistics.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
So?nmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Trans. Audio, Speech, and Lang. Pro-
707
cess., 14(5):1729?1744, September. Special Issue on
Progress in Rich Transcription.
W. Wang, Z. Huang, and M. P. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In Proceedings of ICASSP,
pages 137?140.
Y. Zhao and D. Jurafsky. 2005. A preliminary study of
mandarin filled pause. In Proceedings of DISS, pages
179?182, Aix-en-Provence.
S. Zwarts and M. Johnson. 2011. The impact of lan-
guage models and loss functions on repair disfluency
detection. In Proceedings of ACL/HLT, pages 703?
711, Portland.
708
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 732?741,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
N-Best Rescoring Based on Pitch-accent Patterns
Je Hun Jeon1 Wen Wang2 Yang Liu1
1Department of Computer Science, The University of Texas at Dallas, USA
2Speech Technology and Research Laboratory, SRI International, USA
{jhjeon,yangl}@hlt.utdallas.edu, wwang@speech.sri.com
Abstract
In this paper, we adopt an n-best rescoring
scheme using pitch-accent patterns to improve
automatic speech recognition (ASR) perfor-
mance. The pitch-accent model is decoupled
from the main ASR system, thus allowing us
to develop it independently. N-best hypothe-
ses from recognizers are rescored by addi-
tional scores that measure the correlation of
the pitch-accent patterns between the acoustic
signal and lexical cues. To test the robustness
of our algorithm, we use two different data
sets and recognition setups: the first one is En-
glish radio news data that has pitch accent la-
bels, but the recognizer is trained from a small
amount of data and has high error rate; the sec-
ond one is English broadcast news data using
a state-of-the-art SRI recognizer. Our experi-
mental results demonstrate that our approach
is able to reduce word error rate relatively by
about 3%. This gain is consistent across the
two different tests, showing promising future
directions of incorporating prosodic informa-
tion to improve speech recognition.
1 Introduction
Prosody refers to the suprasegmental features of nat-
ural speech, such as rhythm and intonation, since
it normally extends over more than one phoneme
segment. Speakers use prosody to convey paralin-
guistic information such as emphasis, intention, atti-
tude, and emotion. Humans listening to speech with
natural prosody are able to understand the content
with low cognitive load and high accuracy. How-
ever, most modern ASR systems only use an acous-
tic model and a language model. Acoustic informa-
tion in ASR is represented by spectral features that
are usually extracted over a window length of a few
tens of milliseconds. They miss useful information
contained in the prosody of the speech that may help
recognition.
Recently a lot of research has been done in au-
tomatic annotation of prosodic events (Wightman
and Ostendorf, 1994; Sridhar et al, 2008; Anan-
thakrishnan and Narayanan, 2008; Jeon and Liu,
2009). They used acoustic and lexical-syntactic
cues to annotate prosodic events with a variety of
machine learning approaches and achieved good
performance. There are also many studies us-
ing prosodic information for various spoken lan-
guage understanding tasks. However, research using
prosodic knowledge for speech recognition is still
quite limited. In this study, we investigate leverag-
ing prosodic information for recognition in an n-best
rescoring framework.
Previous studies showed that prosodic events,
such as pitch-accent, are closely related with acous-
tic prosodic cues and lexical structure of utterance.
The pitch-accent pattern given acoustic signal is
strongly correlated with lexical items, such as syl-
lable identity and canonical stress pattern. There-
fore as a first study, we focus on pitch-accent in this
paper. We develop two separate pitch-accent de-
tection models, using acoustic (observation model)
and lexical information (expectation model) respec-
tively, and propose a scoring method for the cor-
relation of pitch-accent patterns between the two
models for recognition hypotheses. The n-best list
is rescored using the pitch-accent matching scores
732
combined with the other scores from the ASR sys-
tem (acoustic and language model scores). We show
that our method yields a word error rate (WER) re-
duction of about 3.64% and 2.07% relatively on two
baseline ASR systems, one being a state-of-the-art
recognizer for the broadcast news domain. The fact
that it holds across different baseline systems sug-
gests the possibility that prosody can be used to help
improve speech recognition performance.
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work
briefly. Section 3 explains the models and features
for pitch-accent detection. We provide details of our
n-best rescoring approach in Section 4. Section 5
describes our corpus and baseline ASR setup. Sec-
tion 6 presents our experiments and results. The last
section gives a brief summary along with future di-
rections.
2 Previous Work
Prosody is of interest to speech researchers be-
cause it plays an important role in comprehension
of spoken language by human listeners. The use
of prosody in speech understanding applications has
been quite extensive. A variety of applications
have been explored, such as sentence and topic seg-
mentation (Shriberg et al, 2000; Rosenberg and
Hirschberg, 2006), word error detection (Litman et
al., 2000), dialog act detection (Sridhar et al, 2009),
speaker recognition (Shriberg et al, 2005), and emo-
tion recognition (Benus et al, 2007), just to name a
few.
Incorporating prosodic knowledge is expected
to improve the performance of speech recogni-
tion. However, how to effectively integrate prosody
within the traditional ASR framework is a difficult
problem, since prosodic features are not well de-
fined and they come from a longer region, which is
different from spectral features used in current ASR
systems. Various research has been conducted try-
ing to incorporate prosodic information in ASR. One
way is to directly integrate prosodic features into
the ASR framework (Vergyri et al, 2003; Ostendorf
et al, 2003; Chen and Hasegawa-Johnson, 2006).
Such efforts include prosody dependent acoustic and
pronunciation model (allophones were distinguished
according to different prosodic phenomenon), lan-
guage model (words were augmented by prosody
events), and duration modeling (different prosodic
events were modeled separately and combined with
conventional HMM). This kind of integration has
advantages in that spectral and prosodic features are
more tightly coupled and jointly modeled. Alterna-
tively, prosody was modeled independently from the
acoustic and language models of ASR and used to
rescore recognition hypotheses in the second pass.
This approach makes it possible to independently
model and optimize the prosodic knowledge and to
combine with ASR hypotheses without any modi-
fication of the conventional ASR modules. In or-
der to improve the rescoring performance, various
prosodic knowledge was studied. (Ananthakrishnan
and Narayanan, 2007) used acoustic pitch-accent
pattern and its sequential information given lexi-
cal cues to rescore n-best hypotheses. (Kalinli and
Narayanan, 2009) used acoustic prosodic cues such
as pitch and duration along with other knowledge
to choose a proper word among several candidates
in confusion networks. Prosodic boundaries based
on acoustic cues were used in (Szaszak and Vicsi,
2007).
We take a similar approach in this study as the
second approach above in that we develop prosodic
models separately and use them in a rescoring
framework. Our proposed method differs from pre-
vious work in the way that the prosody model is used
to help ASR. In our approach, we explicitly model
the symbolic prosodic events based on acoustic and
lexical information. We then capture the correla-
tion of pitch-accent patterns between the two differ-
ent cues, and use that to improve recognition perfor-
mance in an n-best rescoring paradigm.
3 Prosodic Model
Among all the prosodic events, we use only pitch-
accent pattern in this study, because previous stud-
ies have shown that acoustic pitch-accent is strongly
correlated with lexical items, such as canonical
stress pattern and syllable identity that can be eas-
ily acquired from the output of conventional ASR
and pronunciation dictionary. We treat pitch-accent
detection as a binary classification task, that is, a
classifier is used to determine whether the base unit
is prominent or not. Since pitch-accent is usually
733
carried by syllables, we use syllables as our units,
and the syllable definition of each word is based
on CMU pronunciation dictionary which has lexi-
cal stress and syllable boundary marks (Bartlett et
al., 2009). We separately develop acoustic-prosodic
and lexical-prosodic models and use the correlation
between the two models for each syllable to rescore
the n-best hypotheses of baseline ASR systems.
3.1 Acoustic-prosodic Features
Similar to most previous work, the prosodic features
we use include pitch, energy, and duration. We also
add delta features of pitch and energy. Duration in-
formation for syllables is derived from the speech
waveform and phone-level forced alignment of the
transcriptions. In order to reduce the effect by both
inter-speaker and intra-speaker variation, both pitch
and energy values are normalized (z-value) with ut-
terance specific means and variances. For pitch, en-
ergy, and their delta values, we apply several cate-
gories of 12 functions to generate derived features.
? Statistics (7): minimum, maximum, range,
mean, standard deviation, skewness and kurto-
sis value. These are used widely in prosodic
event detection and emotion detection.
? Contour (5): This is approximated by taking
5 leading terms in the Legendre polynomial
expansion. The approximation of the contour
using the Legendre polynomial expansion has
been successfully applied in quantitative pho-
netics (Grabe et al, 2003) and in engineering
applications (Dehak et al, 2007). Each term
models a particular aspect of the contour, such
as the slope, and information about the curva-
ture.
We use 6 duration features, that is, raw, normal-
ized, and relative durations (ms) of the syllable and
vowel. Normalization (z-value) is performed based
on statistics for each syllable and vowel. The rela-
tive value is the difference between the normalized
current duration and the following one.
In the above description, we assumed that the
event of a syllable is only dependent on its observa-
tions, and did not consider contextual effect. To al-
leviate this restriction, we expand the features by in-
corporating information about the neighboring sylla-
bles. Based on the study in (Jeon and Liu, 2010) that
evaluated using left and right contexts, we choose to
use one previous and one following context in the
features. The total number of features used in this
study is 162.
3.2 Lexical-prosodic Features
There is a very strong correlation between pitch-
accent in an utterance and its lexical information.
Previous studies have shown that the lexical fea-
tures perform well for pitch-accent prediction. The
detailed features for training the lexical-prosodic
model are as follows.
? Syllable identity: We kept syllables that appear
more than 5 times in the training corpus. The
other syllables that occur less are collapsed into
one syllable representation.
? Vowel phone identity: We used vowel phone
identity as a feature.
? Lexical stress: This is a binary feature to rep-
resent if the syllable corresponds to a lexical
stress based on the pronunciation dictionary.
? Boundary information: This is a binary feature
to indicate if there is a word boundary before
the syllable.
For lexical features, based on the study in (Jeon
and Liu, 2010), we added two previous and two fol-
lowing contexts in the final features.
3.3 Prosodic Model Training
We choose to use a support vector machine (SVM)
classifier1 for the prosodic model based on previous
work on prosody labeling study in (Jeon and Liu,
2010). We use RBF kernel for the acoustic model,
and 3-order polynomial kernel for the lexical model.
In our experiments, we investigate two kinds
of training methods for prosodic modeling. The
first one is a supervised method where models are
trained using all the labeled data. The second is
a semi-supervised method using co-training algo-
rithm (Blum and Mitchell, 1998), described in Algo-
rithm 1. Given a set L of labeled data and a set U of
unlabeled data with two views, it then iterates in the
1LIBSVM ? A Library for Support Vector Machines, loca-
tion: http://www.csie.ntu.edu.tw/?cjlin/libsvm/
734
Algorithm 1 Co-training algorithm.
Given:
- L: labeled examples; U: unlabeled examples
- there are two views V1 and V2 on an example x
Initialize:
- L1=L, samples used to train classifiers h1
- L2=L, samples used to train classifiers h2
Loop for k iterations
- create a small pool U? choosing from U
- use V1(L1) to train classifier h1
and V2(L2) to train classifier h2
- let h1 label/select examples Dh1 from U?
- let h2 label/select examples Dh2 from U?
- add self-labeled examples Dh1 to L2
and Dh2 to L1
- remove Dh1 and Dh2 from U
following procedure. The algorithm first creates a
smaller pool U? containing unlabeled data from U. It
uses Li (i = 1, 2) to train two distinct classifiers: the
acoustic classifier h1, and the lexical classifier h2.
We use function Vi (i = 1, 2) to represent that only
a single view is used for training h1 or h2. These two
classifiers are used to make predictions for the unla-
beled setU?, and only when they agree on the predic-
tion for a sample, their predicted class is used as the
label for this sample. Then among these self-labeled
samples, the most confident ones by one classifier
are added to the data set Li for training the other
classifier. This iteration continues until reaching the
defined number of iterations. In our experiment, the
size of the pool U? is 5 times of the size of training
data Li, and the size of the added self-labeled ex-
ample set, Dhi , is 5% of Li. For the newly selected
Dhi , the distribution of the positive and negative ex-
amples is the same as that of the training data Li.
This co-training method is expected to cope with
two problems in prosodic model training. The first
problem is the different decision patterns between
the two classifiers: the acoustic model has relatively
higher precision, while the lexical model has rela-
tively higher recall. The goal of the co-training al-
gorithm is to learn from the difference of each clas-
sifier, thus it can improve the performance as well
as reduce the mismatch of two classifiers. The sec-
ond problem is the mismatch of data used for model
training and testing, which often results in system
performance degradation. Using co-training, we can
use the unlabeled data from the domain that matches
the test data, adapting the model towards test do-
main.
4 N-Best Rescoring Scheme
In order to leverage prosodic information for bet-
ter speech recognition performance, we augment the
standard ASR equation to include prosodic informa-
tion as following:
W? = argmax
W
p(W |As, Ap)
= argmax
W
p(As, Ap|W )p(W ) (1)
where As and Ap represent acoustic-spectral fea-
tures and acoustic-prosodic features. We can further
assume that spectral and prosodic features are con-
ditionally independent given a word sequence W ,
therefore, Equation 1 can be rewritten as following:
W? ? argmax
W
p(As|W )p(W )p(Ap|W ) (2)
The first two terms stand for the acoustic and lan-
guage models in the original ASR system, and the
last term means the prosody model we introduce. In-
stead of using the prosodic model in the first pass de-
coding, we use it to rescore n-best candidates from
a speech recognizer. This allows us to train the
prosody models independently and better optimize
the models.
For p(Ap|W ), the prosody score for a word se-
quence W , in this work we propose a method to es-
timate it, also represented as scoreW?prosody(W ).
The idea of scoring the prosody patterns is that there
is some expectation of pitch-accent patterns given
the lexical sequence (W ), and the acoustic pitch-
accent should match with this expectation. For in-
stance, in the case of a prominent syllable, both
acoustic and lexical evidence show pitch-accent, and
vice versa. In order to maximize the agreement be-
tween the two sources, we measure how good the
acoustic pitch-accent in speech signal matches the
given lexical cues. For each syllable Si in the n-best
list, we use acoustic-prosodic cues (ai) to estimate
the posterior probability that the syllable is promi-
nent (P), p(P |ai). Similarly, we use lexical cues (li)
735
to determine the syllable?s pitch-accent probability
p(P |li). Then the prosody score for a syllable Si is
estimated by the match of the pitch-accent patterns
between acoustic and lexical information using the
difference of the posteriors from the two models:
scoreS?prosody(Si) ? 1? | p(P |ai) ? p(P |li) | (3)
Furthermore, we take into account the effect due
to varying durations for different syllables. We no-
tice that syllables without pitch-accent have much
shorter duration than the prominent ones, and the
prosody scores for the short syllables tend to be
high. This means that if a syllable is split into two
consecutive non-prominent syllables, the agreement
score may be higher than a long prominent syllable.
Therefore, we introduce a weighting factor based on
syllable duration (dur(i)). For a candidate word se-
quence (W) consisting of n syllables, its prosodic
score is the sum of the prosodic scores for all the
syllables in it weighted by their duration (measured
using milliseconds), that is:
scoreW?prosody(W ) ?
n
?
i=1
log(scoreS?prosody(Si)) ? dur(i) (4)
We then combine this prosody score with the
original acoustic and language model likelihood
(P (As|W ) and P (W ) in Equation 2). In practice,
we need to weight them differently, therefore, the
combined score for a hypothesis W is:
Score(W ) = ? ? scoreW?prosody(W )
+ scoreASR(W ) (5)
where scoreASR(W ) is generated by ASR systems
(composed of acoustic and language model scores)
and ? is optimized using held out data.
5 Data and Baseline Systems
Our experiments are carried out using two different
data sets and two different recognition systems as
well in order to test the robustness of our proposed
method.
The first data set is the Boston University Radio
News Corpus (BU) (Ostendorf et al, 1995), which
consists of broadcast news style read speech. The
BU corpus has about 3 hours of read speech from
7 speakers (3 female, 4 male). Part of the data has
been labeled with ToBI-style prosodic annotations.
In fact, the reason that we use this corpus, instead of
other corpora typically used for ASR experiments,
is because of its prosodic labels. We divided the
entire data corpus into a training set and a test set.
There was no speaker overlap between training and
test sets. The training set has 2 female speakers (f2
and f3) and 3 male ones (m2, m3, m4). The test set is
from the other two speakers (f1 and m1). We use 200
utterances for the recognition experiments. Each ut-
terance in BU corpus consists of more than one sen-
tences, so we segmented each utterance based on
pause, resulting in a total number of 713 segments
for testing. We divided the test set roughly equally
into two sets, and used one for parameter tuning and
the other for rescoring test. The recognizer used for
this data set was based on Sphinx-32. The context-
dependent triphone acoustic models with 32 Gaus-
sian mixtures were trained using the training par-
tition of the BU corpus described above, together
with the broadcast new data. A standard back-off tri-
gram language model with Kneser-Ney smoothing
was trained using the combined text from the train-
ing partition of the BU, Wall Street Journal data, and
part of Gigaword corpus. The vocabulary size was
about 10K words and the out-of-vocabulary (OOV)
rate on the test set was 2.1%.
The second data set is from broadcast news (BN)
speech used in the GALE program. The recognition
test set contains 1,001 utterances. The n-best hy-
potheses for this data set are generated by a state-of-
the-art SRI speech recognizer, developed for broad-
cast news speech (Stolcke et al, 2006; Zheng et
al., 2007). This system yields much better perfor-
mance than the first one. We also divided the test
set roughly equally into two sets for parameter tun-
ing and testing. From the data used for training the
speech recognizer, we randomly selected 5.7 hours
of speech (4,234 utterances) for the co-training al-
gorithm for the prosodic models.
For prosodic models, we used a simple binary
representation of pitch-accent in the form of pres-
ence versus absence. The reference labels are de-
2CMU Sphinx - Speech Recognition Toolkit, location:
http://www.speech.cs.cmu.edu/sphinx/tutorial.html
736
rived from the ToBI annotation in the BU corpus,
and the ratio of pitch-accented syllables is about
34%. Acoustic-prosodic and lexical-prosodic mod-
els were separately developed using the features de-
scribed in Section 3. Feature extraction was per-
formed at the syllable level from force-aligned data.
For the supervised approach, we used those utter-
ances in the training data partition with ToBI labels
in the BU corpus (245 utterances, 14,767 syllables).
For co-training, the labeled data from BU corpus is
used as initial training, and the other unlabeled data
from BU and BN are used as unlabeled data.
6 Experimental Results
6.1 Pitch-accent Detection
First we evaluate the performance of our acoustic-
prosodic and lexical-prosodic models for pitch-
accent detection. For rescoring, not only the ac-
curacies of the two individual prosodic models are
important, but also the pitch-accent agreement score
between the two models (as shown in Equation 3)
is critical, therefore, we present results using these
two metrics. Table 1 shows the accuracy of each
model for pitch-accent detection, and also the av-
erage prosody score of the two models (i.e., Equa-
tion 3) for positive and negative classes (using ref-
erence labels). These results are based on the BU
labeled data in the test set. To compare our pitch ac-
cent detection performance with previous work, we
include the result of (Jeon and Liu, 2009) as a ref-
erence. Compared to previous work, the acoustic
model achieved similar performance, while the per-
formance of lexical model is a bit lower. The lower
performance of lexical model is mainly because we
do not use part-of-speech (POS) information in the
features, since we want to only use the word output
from the ASR system (without additional POS tag-
ging).
As shown in Table 1, when using the co-training
algorithm, as described in Section 3.3, the over-
all accuracies improve slightly and therefore the
prosody score is also increased. We expect this im-
proved model will be more beneficial for rescoring.
6.2 N-Best Rescoring
For the rescoring experiment, we use 100-best hy-
potheses from the two different ASR systems, as de-
Accuracy(%) Prosody score
Acoustic Lexical Pos Neg
Supervised 83.97 84.48 0.747 0.852
Co-training 84.54 84.99 0.771 0.867
Reference 83.53 87.92 - -
Table 1: Pitch accent detection results: performance of
individual acoustic and lexical models, and the agreement
between the twomodels (i.e., prosody score for a syllable,
Equation 3) for positive and negative classes. Also shown
is the reference result for pitch accent detection from Jeon
and Liu (2009).
scribed in Section 5. We apply the acoustic and lex-
ical prosodic models to each hypothesis to obtain its
prosody score, and combine it with ASR scores to
find the top hypothesis. The weights were optimized
using one test set and applied to the other. We report
the average result of the two testings.
Table 2 shows the rescoring results using the first
recognition system on BU data, which was trained
with a relatively small amount of data. The 1-
best baseline uses the first hypothesis that has the
best ASR score. The oracle result is from the best
hypothesis that gives the lowest WER by compar-
ing all the candidates to the reference transcript.
We used two prosodic models as described in Sec-
tion 3.3. The first one is the base prosodic model us-
ing supervised training (S-model). The second is the
prosodic model with the co-training algorithm (C-
model). For these rescoring experiments, we tuned
? (in Equation 5) when combining the ASR acous-
tic and language model scores with the additional
prosody score. The value in parenthesis in Table 2
means the relative WER reduction when compared
to the baseline result. We show the WER results for
both the development and the test set.
As shown in Table 2, we observe performance
improvement using our rescoring method. Using
the base S-model yields reasonable improvement,
and C-model further reduces WER. Even though the
prosodic event detection performance of these two
prosodic models is similar, the improved prosody
score between the acoustic and lexical prosodic
models using co-training helps rescoring. After
rescoring using prosodic knowledge, the WER is re-
duced by 0.82% (3.64% relative). Furthermore, we
notice that the difference between development and
737
WER (%)
1-best baseline 22.64
S-model
Dev 21.93 (3.11%)
Test 22.10 (2.39%)
C-model
Dev 21.76 (3.88%)
Test 21.81 (3.64%)
Oracle 15.58
Table 2: WER of the baseline system and after rescoring
using prosodic models. Results are based on the first ASR
system.
test data is smaller when using the C-model than S-
model, which means that the prosodic model with
co-training is more stable. In fact, we found that
the optimal value of ? is 94 and 57 for the two
folds using S-model, and is 99 and 110 for the C-
model. These verify again that the prosodic scores
contribute more in the combination with ASR likeli-
hood scores when using the C-model, and are more
robust across different tuning sets. Ananthakrish-
nan and Narayanan (2007) also used acoustic/lexical
prosodic models to estimate a prosody score and re-
ported 0.3% recognition error reduction on BU data
when rescoring 100-best list (their baseline WER is
22.8%). Although there is some difference in experi-
mental setup (data, classifier, features) between ours
and theirs, our S-model showed comparable perfor-
mance gain and the result of C-model is significantly
better than theirs.
Next we test our n-best rescoring approach using a
state-of-the-art SRI speech recognizer on BN data to
verify if our approach can generalize to better ASR
n-best lists. This is often the concern that improve-
ments observed on a poor ASR system do not hold
for better ASR systems. The rescoring results are
shown in Table 3. We can see that the baseline per-
formance of this recognizer is much better than that
of the first ASR system (even though the recogni-
tion task is also harder). Our rescoring approach
still yields performance gain even using this state-
of-the-art system. The WER is reduced by 0.29%
(2.07% relative). This error reduction is lower than
that in the first ASR system. There are several pos-
sible reasons. First, the baseline ASR performance
is higher, making further improvement hard; sec-
ond, and more importantly, the prosody models do
not match well to the test domain. We trained the
prosody model using the BU data. Even though co-
training is used to leverage unlabeled BN data to re-
duce data mismatch, it is still not as good as using
labeled in-domain data for model training.
WER (%)
1-best baseline 13.77
S-model
Dev 13.53 (1.78%)
Test 13.55 (1.63%)
C-model
Dev 13.48 (2.16%)
Test 13.49 (2.07%)
Oracle 9.23
Table 3: WER of the baseline system and after rescoring
using prosodic models. Results are based on the second
ASR system.
6.3 Analysis and Discussion
We also analyze what kinds of errors are reduced
using our rescoring approach. Most of the error re-
duction came from substitution and insertion errors.
Deletion error rate did not change much or some-
times even increased. For a better understanding of
the improvement using the prosody model, we ana-
lyzed the pattern of corrections (the new hypothesis
after rescoring is correct while the original 1-best is
wrong) and errors. Table 4 shows some positive and
negative examples from rescoring results using the
first ASR system. In this table, each word is asso-
ciated with some binary expressions inside a paren-
thesis, which stand for pitch-accent markers. Two
bits are used for each syllable: the first one is for
the acoustic-prosodic model and the second one is
for the lexical-prosodic model. For both bits, 1 rep-
resents pitch-accent, and 0 indicates none. These
hard decisions are obtained by setting a threshold of
0.5 for the posterior probabilities from the acoustic
or lexical models. For example, when the acoustic
classifier predicts a syllable as pitch-accented and
the lexical one as not accented, ?10? marker is as-
signed to the syllable. The number of such pairs of
pitch-accent markers is the same as the number of
syllables in a word. The bold words indicate correct
words and italic means errors. As shown in the pos-
itive example of Table 4, we find that our prosodic
model is effective at identifying an erroneous word
when it is split into two words, resulting in dif-
ferent pitch-accent patterns. Language models are
738
Positive example
1-best : most of the massachusetts
(11 ) (10) (00) (11 00 01 00)
rescored : most other massachusetts
(11 ) (11 00) (11 00 01 00)
Negative example
1-best : robbery and on a theft
(11 00 00) (00) (10) (00) (11)
rescored : robbery and lot of theft
(11 00 00) (00) (11) (00) (11)
Table 4: Examples of rescoring results. Binary expressions inside the parenthesis below a word represent pitch-accent
markers for the syllables in the word.
not good at correcting this kind of errors since both
word sequences are plausible. Our model also intro-
duces some errors, as shown in the negative exam-
ple, which is mainly due to the inaccurate prosody
model.
We conducted more prosody rescoring experi-
ments in order to understand the model behavior.
These analyses are based on the n-best list from the
first ASR system for the entire test set. In the first
experiment, among the 100 hypotheses in n-best list,
we gave a prosody score of 0 to the 100th hypothe-
sis, and used automatically obtained prosodic scores
for the other hypotheses. A zero prosody score
means the perfect agreement given acoustic and lex-
ical cues. The original scores from the recognizer
were combined with the prosodic scores for rescor-
ing. This was to verify that the range of the weight-
ing factor ? estimated on the development data (us-
ing the original, not the modified prosody scores for
all candidates) was reasonable to choose proper hy-
pothesis among all the candidates. We noticed that
27% of the times the last hypothesis on the list was
selected as the best hypothesis. This hypothesis has
the highest prosodic scores, but lowest ASR score.
This result showed that if the prosodic models were
accurate enough, the correct candidate could be cho-
sen using our rescoring framework.
In the second experiment, we put the reference
text together with the other candidates. We use the
same ASR scores for all candidates, and generated
prosodic scores using our prosody model. This was
to test that our model could pick up correct candi-
date using only the prosodic score. We found that
for 26% of the utterances, the reference transcript
was chosen as the best one. This was significantly
better than random selection (i.e., 1/100), suggest-
ing the benefit of the prosody model; however, this
percentage is not very high, implying the limitation
of prosodic information for ASR or the current im-
perfect prosodic models.
In the third experiment, we replaced the 100th
candidate with the reference transcript and kept its
ASR score. When using our prosody rescoring ap-
proach, we obtained a relative error rate reduction
of 6.27%. This demonstrates again that our rescor-
ing method works well ? if the correct hypothesis is
on the list, even though with a low ASR score, us-
ing prosodic information can help identify the cor-
rect candidate.
Overall the performance improvement we ob-
tained from rescoring by incorporating prosodic in-
formation is very promising. Our evaluation using
two different ASR systems shows that the improve-
ment holds even when we use a state-of-the-art rec-
ognizer and the training data for the prosody model
does not come from the same corpus. We believe
the consistent improvements we observed for differ-
ent conditions show that this is a direction worthy of
further investigation.
7 Conclusion
In this paper, we attempt to integrate prosodic infor-
mation for ASR using an n-best rescoring scheme.
This approach decouples the prosodic model from
the main ASR system, thus the prosodic model can
be built independently. The prosodic scores that we
use for n-best rescoring are based on the matching
of pitch-accent patterns by acoustic and lexical fea-
tures. Our rescoring method achieved a WER reduc-
tion of 3.64% and 2.07% relatively using two differ-
ent ASR systems. The fact that the gain holds across
different baseline systems (including a state-of-the-
739
art speech recognizer) suggests the possibility that
prosody can be used to improve speech recognition
performance.
As suggested by our experiments, better prosodic
models can result in more WER reduction. The per-
formance of our prosodic model was improved with
co-training, but there are still problems, such as the
imbalance of the two classifiers? prediction, as well
as for the two events. In order to address these prob-
lems, we plan to improve the labeling and selec-
tion method in the co-training algorithm, and also
explore other training algorithms to reduce domain
mismatch. Furthermore, we are also interested in
evaluating our approach on the spontaneous speech
domain, which is quite different from the data we
used in this study.
In this study, we used n-best rather than lattice
rescoring. Since the prosodic features we use in-
clude cross-word contextual information, it is not
straightforward to apply it directly to lattices. In
our future work, we will develop models with only
within-word context, and thus allowing us to explore
lattice rescoring, which we expect will yield more
performance gain.
References
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2007. Improved speech recognition using
acoustic and lexical correlated of pitch accent in a
n-best rescoring framework. Proc. of ICASSP, pages
65?68.
Sankaranarayanan Ananthakrishnan and Shrikanth
Narayanan. 2008. Automatic prosodic event detec-
tion using acoustic, lexical and syntactic evidence.
IEEE Transactions on Audio, Speech, and Language
Processing, 16(1):216?228.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. Proc. of
NAACL-HLT, pages 308?316.
Stefan Benus, Agust??n Gravano, and Julia Hirschberg.
2007. Prosody, emotions, and whatever. Proc. of In-
terspeech, pages 2629?2632.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. Proc. of the
Workshop on Computational Learning Theory, pages
92?100.
Ken Chen and Mark Hasegawa-Johnson. 2006. Prosody
dependent speech recognition on radio news corpus
of American English. IEEE Transactions on Audio,
Speech, and Language Processing, 14(1):232? 245.
Najim Dehak, Pierre Dumouchel, and Patrick Kenny.
2007. Modeling prosodic features with joint fac-
tor analysis for speaker verification. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2095?2103.
Esther Grabe, Greg Kochanski, and John Coleman. 2003.
Quantitative modelling of intonational variation. Proc.
of SASRTLM, pages 45?57.
Je Hun Jeon and Yang Liu. 2009. Automatic prosodic
events detection suing syllable-based acoustic and syn-
tactic features. Proc. of ICASSP, pages 4565?4568.
Je Hun Jeon and Yang Liu. 2010. Syllable-level promi-
nence detection with acoustic evidence. Proc. of Inter-
speech, pages 1772?1775.
Ozlem Kalinli and Shrikanth Narayanan. 2009. Contin-
uous speech recognition using attention shift decoding
with soft decision. Proc. of Interspeech, pages 1927?
1930.
Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.
2000. Predicting automatic speech recognition perfor-
mance using prosodic cues. Proc. of NAACL, pages
218?225.
Mari Ostendorf, Patti Price, and Stefanie Shattuck-
Hufnagel. 1995. The Boston University radio news
corpus. Linguistic Data Consortium.
Mari Ostendorf, Izhak Shafran, and Rebecca Bates.
2003. Prosody models for conversational speech
recognition. Proc. of the 2nd Plenary Meeting and
Symposium on Prosody and Speech Processing, pages
147?154.
Andrew Rosenberg and Julia Hirschberg. 2006. Story
segmentation of broadcast news in English, Mandarin
and Arabic. Proc. of HLT-NAACL, pages 125?128.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,
and Go?khan Tu?r. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32(1-2):127?154.
Elizabeth Shriberg, Luciana Ferrer, Sachin S. Kajarekar,
Anand Venkataraman, and Andreas Stolcke. 2005.
Modeling prosodic feature sequences for speaker
recognition. Speech Communication, 46(3-4):455?
472.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth S. Narayanan. 2008. Exploiting acous-
tic and syntactic features for automatic prosody label-
ing in a maximum entropy framework. IEEE Trans-
actions on Audio, Speech, and Language Processing,
16(4):797?811.
Vivek Kumar Rangarajan Sridhar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
740
dialog act tagging. Computer Speech and Language,
23(4):407?422.
Andreas Stolcke, Barry Chen, Horacio Franco, Venkata
Ramana Rao Gadde, Martin Graciarena, Mei-Yuh
Hwang, Katrin Kirchhoff, Arindam Mandal, Nelson
Morgan, Xin Lin, Tim Ng, Mari Ostendorf, Kemal
So?nmez, Anand Venkataraman, Dimitra Vergyri, Wen
Wang, Jing Zheng, and Qifeng Zhu. 2006. Recent in-
novations in speech-to-text transcription at SRI-ICSI-
UW. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 14(5):1729?1744. Special Issue on
Progress in Rich Transcription.
Gyorgy Szaszak and Klara Vicsi. 2007. Speech recogni-
tion supported by prosodic information for fixed stress
languages. Proc. of TSD Conference, pages 262?269.
Dimitra Vergyri, Andreas Stolcke, Venkata R. R. Gadde,
Luciana Ferrer, and Elizabeth Shriberg. 2003.
Prosodic knowledge sources for automatic speech
recognition. Proc. of ICASSP, pages 208?211.
Colin W. Wightman and Mari Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Transaction
on Speech and Auido Processing, 2(4):469?481.
Jing Zheng, Ozgur Cetin, Mei-Yuh Hwang, Xin Lei, An-
dreas Stolcke, and Nelson Morgan. 2007. Combin-
ing discriminative feature, transform, and model train-
ing for large vocabulary speech recognition. Proc. of
ICASSP, pages 633?636.
741
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 374?378,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Detection of Agreement and Disagreement in Broadcast Conversations
Wen Wang1 Sibel Yaman2y Kristin Precoda1 Colleen Richey1 Geoffrey Raymond3
1SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA
2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA
3University of California, Santa Barbara, CA, USA
fwwang,precoda,colleeng@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.edu
Abstract
We present Conditional Random Fields
based approaches for detecting agree-
ment/disagreement between speakers in
English broadcast conversation shows. We
develop annotation approaches for a variety
of linguistic phenomena. Various lexical,
structural, durational, and prosodic features
are explored. We compare the performance
when using features extracted from au-
tomatically generated annotations against
that when using human annotations. We
investigate the efficacy of adding prosodic
features on top of lexical, structural, and
durational features. Since the training data
is highly imbalanced, we explore two sam-
pling approaches, random downsampling
and ensemble downsampling. Overall, our
approach achieves 79.2% (precision), 50.5%
(recall), 61.7% (F1) for agreement detection
and 69.2% (precision), 46.9% (recall), and
55.9% (F1) for disagreement detection, on the
English broadcast conversation data.
1 Introduction
In this work, we present models for detecting
agreement/disagreement (denoted (dis)agreement)
between speakers in English broadcast conversation
shows. The Broadcast Conversation (BC) genre dif-
fers from the Broadcast News (BN) genre in that
it is more interactive and spontaneous, referring to
free speech in news-style TV and radio programs
and consisting of talk shows, interviews, call-in
programs, live reports, and round-tables. Previous
yThis work was performed while the author was at ICSI.
work on detecting (dis)agreements has been focused
on meeting data. (Hillard et al, 2003), (Galley
et al, 2004), (Hahn et al, 2006) used spurt-level
agreement annotations from the ICSI meeting cor-
pus (Janin et al, 2003). (Hillard et al, 2003) ex-
plored unsupervised machine learning approaches
and on manual transcripts, they achieved an over-
all 3-way agreement/disagreement classification ac-
curacy as 82% with keyword features. (Galley et
al., 2004) explored Bayesian Networks for the de-
tection of (dis)agreements. They used adjacency
pair information to determine the structure of their
conditional Markov model and outperformed the re-
sults of (Hillard et al, 2003) by improving the 3-
way classification accuracy into 86.9%. (Hahn et al,
2006) explored semi-supervised learning algorithms
and reached a competitive performance of 86.7%
3-way classification accuracy on manual transcrip-
tions with only lexical features. (Germesin and Wil-
son, 2009) investigated supervised machine learn-
ing techniques and yields competitive results on the
annotated data from the AMI meeting corpus (Mc-
Cowan et al, 2005).
Our work differs from these previous studies in
two major categories. One is that a different def-
inition of (dis)agreement was used. In the cur-
rent work, a (dis)agreement occurs when a respond-
ing speaker agrees with, accepts, or disagrees with
or rejects, a statement or proposition by a first
speaker. Second, we explored (dis)agreement de-
tection in broadcast conversation. Due to the dif-
ference in publicity and intimacy/collegiality be-
tween speakers in broadcast conversations vs. meet-
ings, (dis)agreement may have different character-
374
istics. Different from the unsupervised approaches
in (Hillard et al, 2003) and semi-supervised ap-
proaches in (Hahn et al, 2006), we conducted su-
pervised training. Also, different from (Hillard et
al., 2003) and (Galley et al, 2004), our classifica-
tion was carried out on the utterance level, instead
of on the spurt-level. Galley et al extended Hillard
et al?s work by adding features from previous spurts
and features from the general dialog context to in-
fer the class of the current spurt, on top of fea-
tures from the current spurt (local features) used by
Hillard et al Galley et al used adjacency pairs to
describe the interaction between speakers and the re-
lations between consecutive spurts. In this prelim-
inary study on broadcast conversation, we directly
modeled (dis)agreement detection without using ad-
jacency pairs. Still, within the conditional random
fields (CRF) framework, we explored features from
preceding and following utterances to consider con-
text in the discourse structure. We explored a wide
variety of features, including lexical, structural, du-
rational, and prosodic features. To our knowledge,
this is the first work to systematically investigate
detection of agreement/disagreement for broadcast
conversation data. The remainder of the paper is or-
ganized as follows. Section 2 presents our data and
automatic annotation modules. Section 3 describes
various features and the CRF model we explored.
Experimental results and discussion appear in Sec-
tion 4, as well as conclusions and future directions.
2 Data and Automatic Annotation
In this work, we selected English broadcast con-
versation data from the DARPA GALE pro-
gram collected data (GALE Phase 1 Release
4, LDC2006E91; GALE Phase 4 Release 2,
LDC2009E15). Human transcriptions and manual
speaker turn labels are used in this study. Also,
since the (dis)agreement detection output will be
used to analyze social roles and relations of an inter-
acting group, we first manually marked soundbites
and then excluded soundbites during annotation and
modeling. We recruited annotators to provide man-
ual annotations of speaker roles and (dis)agreement
to use for the supervised training of models. We de-
fined a set of speaker roles as follows. Host/chair
is a person associated with running the discussions
or calling the meeting. Reporting participant is a
person reporting from the field, from a subcommit-
tee, etc. Commentator participant/Topic participant
is a person providing commentary on some subject,
or person who is the subject of the conversation and
plays a role, e.g., as a newsmaker. Audience par-
ticipant is an ordinary person who may call in, ask
questions at a microphone at e.g. a large presenta-
tion, or be interviewed because of their presence at a
news event. Other is any speaker who does not fit in
one of the above categories, such as a voice talent,
an announcer doing show openings or commercial
breaks, or a translator.
Agreements and disagreements are com-
posed of different combinations of initiating
utterances and responses. We reformulated the
(dis)agreement detection task as the sequence
tagging of 11 (dis)agreement-related labels for
identifying whether a given utterance is initiating
a (dis)agreement opportunity, is a (dis)agreement
response to such an opportunity, or is neither of
these, in the show. For example, a Negative tag
question followed by a negation response forms an
agreement, that is, A: [Negative tag] This is not
black and white, is it? B: [Agreeing Response]
No, it isn?t. The data sparsity problem is serious.
Among all 27,071 utterances, only 2,589 utterances
are involved in (dis)agreement as initiating or
response utterances, about 10% only among all
data, while 24,482 utterances are not involved.
These annotators also labeled shows with a va-
riety of linguistic phenomena (denoted language
use constituents, LUC), including discourse mark-
ers, disfluencies, person addresses and person men-
tions, prefaces, extreme case formulations, and dia-
log act tags (DAT). We categorized dialog acts into
statement, question, backchannel, and incomplete.
We classified disfluencies (DF) into filled pauses
(e.g., uh, um), repetitions, corrections, and false
starts. Person address (PA) terms are terms that a
speaker uses to address another person. Person men-
tions (PM) are references to non-participants in the
conversation. Discourse markers (DM) are words
or phrases that are related to the structure of the
discourse and express a relation between two utter-
ances, for example, I mean, you know. Prefaces
(PR) are sentence-initial lexical tokens serving func-
tions close to discourse markers (e.g., Well, I think
375
that...). Extreme case formulations (ECF) are lexi-
cal patterns emphasizing extremeness (e.g., This is
the best book I have ever read). In the end, we man-
ually annotated 49 English shows. We preprocessed
English manual transcripts by removing transcriber
annotation markers and noise, removing punctuation
and case information, and conducting text normal-
ization. We also built automatic rule-based and sta-
tistical annotation tools for these LUCs.
3 Features and Model
We explored lexical, structural, durational, and
prosodic features for (dis)agreement detection. We
included a set of ?lexical? features, including n-
grams extracted from all of that speaker?s utter-
ances, denoted ngram features. Other lexical fea-
tures include the presence of negation and acquies-
cence, yes/no equivalents, positive and negative tag
questions, and other features distinguishing differ-
ent types of initiating utterances and responses. We
also included various lexical features extracted from
LUC annotations, denoted LUC features. These ad-
ditional features include features related to the pres-
ence of prefaces, the counts of types and tokens
of discourse markers, extreme case formulations,
disfluencies, person addressing events, and person
mentions, and the normalized values of these counts
by sentence length. We also include a set of features
related to the DAT of the current utterance and pre-
ceding and following utterances.
We developed a set of ?structural? and ?dura-
tional? features, inspired by conversation analysis,
to quantitatively represent the different participation
and interaction patterns of speakers in a show. We
extracted features related to pausing and overlaps
between consecutive turns, the absolute and relative
duration of consecutive turns, and so on.
We used a set of prosodic features including
pause, duration, and the speech rate of a speaker. We
also used pitch and energy of the voice. Prosodic
features were computed on words and phonetic
alignment of manual transcripts. Features are com-
puted for the beginning and ending words of an ut-
terance. For the duration features, we used the aver-
age and maximum vowel duration from forced align-
ment, both unnormalized and normalized for vowel
identity and phone context. For pitch and energy, we
calculated the minimum, maximum, range, mean,
standard deviation, skewness and kurtosis values. A
decision tree model was used to compute posteriors
from prosodic features and we used cumulative bin-
ning of posteriors as final features , similar to (Liu et
al., 2006).
As illustrated in Section 2, we reformulated the
(dis)agreement detection task as a sequence tagging
problem. We used the Mallet package (McCallum,
2002) to implement the linear chain CRF model for
sequence tagging. A CRF is an undirected graph-
ical model that defines a global log-linear distribu-
tion of the state (or label) sequence E conditioned
on an observation sequence, in our case including
the sequence of sentences S and the corresponding
sequence of features for this sequence of sentences
F . The model is optimized globally over the en-
tire sequence. The CRF model is trained to maxi-
mize the conditional log-likelihood of a given train-
ing set P (EjS; F ). During testing, the most likely
sequence E is found using the Viterbi algorithm.
One of the motivations of choosing conditional ran-
dom fields was to avoid the label-bias problem found
in hidden Markov models. Compared to Maxi-
mum Entropy modeling, the CRF model is opti-
mized globally over the entire sequence, whereas the
ME model makes a decision at each point individu-
ally without considering the context event informa-
tion.
4 Experiments
All (dis)agreement detection results are based on n-
fold cross-validation. In this procedure, we held
out one show as the test set, randomly held out an-
other show as the dev set, trained models on the
rest of the data, and tested the model on the held-
out show. We iterated through all shows and com-
puted the overall accuracy. Table 1 shows the re-
sults of (dis)agreement detection using all features
except prosodic features. We compared two condi-
tions: (1) features extracted completely from the au-
tomatic LUC annotations and automatically detected
speaker roles, and (2) features from manual speaker
role labels and manual LUC annotations when man-
ual annotations are available. Table 1 showed that
running a fully automatic system to generate auto-
matic annotations and automatic speaker roles pro-
376
duced comparable performance to the system using
features from manual annotations whenever avail-
able.
Table 1: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using features extracted from
manual speaker role labels and manual LUC annota-
tions when available, denoted Manual Annotation, and
automatic LUC annotations and automatically detected
speaker roles, denoted Automatic Annotation.
Agreement
P R F1
Manual Annotation 81.5 43.2 56.5
Automatic Annotation 79.5 44.6 57.1
Disagreement
P R F1
Manual Annotation 70.1 38.5 49.7
Automatic Annotation 64.3 36.6 46.6
We then focused on the condition of using fea-
tures from manual annotations when available and
added prosodic features as described in Section 3.
The results are shown in Table 2. Adding prosodic
features produced a 0.7% absolute gain on F1 on
agreement detection, and 1.5% absolute gain on F1
on disagreement detection.
Table 2: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection using manual annotations with-
out and with prosodic features.
Agreement
P R F1
w/o prosodic 81.5 43.2 56.5
with prosodic 81.8 44.0 57.2
Disagreement
P R F1
w/o prosodic 70.1 38.5 49.7
with prosodic 70.8 40.1 51.2
Note that only about 10% utterances among all
data are involved in (dis)agreement. This indicates
a highly imbalanced data set as one class is more
heavily represented than the other/others. We sus-
pected that this high imbalance has played a ma-
jor role in the high precision and low recall results
we obtained so far. Various approaches have been
studied to handle imbalanced data for classifications,
trying to balance the class distribution in the train-
ing set by either oversampling the minority class or
downsampling the majority class. In this prelimi-
nary study of sampling approaches for handling im-
balanced data for CRF training, we investigated two
approaches, random downsampling and ensemble
downsampling. Random downsampling randomly
downsamples the majority class to equate the num-
ber of minority and majority class samples. Ensem-
ble downsampling is a refinement of random down-
sampling which doesn?t discard any majority class
samples. Instead, we partitioned the majority class
samples into N subspaces with each subspace con-
taining the same number of samples as the minority
class. Then we train N CRF models, each based
on the minority class samples and one disjoint parti-
tion from the N subspaces. During testing, the pos-
terior probability for one utterance is averaged over
the N CRF models. The results from these two sam-
pling approaches as well as the baseline are shown
in Table 3. Both sampling approaches achieved sig-
nificant improvement over the baseline, i.e., train-
ing on the original data set, and ensemble downsam-
pling produced better performance than downsam-
pling. We noticed that both sampling approaches
degraded slightly in precision but improved signif-
icantly in recall, resulting in 4.5% absolute gain on
F1 for agreement detection and 4.7% absolute gain
on F1 for disagreement detection.
Table 3: Precision (%), recall (%), and F1 (%) of
(dis)agreement detection without sampling, with random
downsampling and ensemble downsampling. Manual an-
notations and prosodic features are used.
Agreement
P R F1
Baseline 81.8 44.0 57.2
Random downsampling 78.5 48.7 60.1
Ensemble downsampling 79.2 50.5 61.7
Disagreement
P R F1
Baseline 70.8 40.1 51.2
Random downsampling 67.3 44.8 53.8
Ensemble downsampling 69.2 46.9 55.9
In conclusion, this paper presents our work on
detection of agreements and disagreements in En-
377
glish broadcast conversation data. We explored a
variety of features, including lexical, structural, du-
rational, and prosodic features. We experimented
these features using a linear-chain conditional ran-
dom fields model and conducted supervised train-
ing. We observed significant improvement from
adding prosodic features and employing two sam-
pling approaches, random downsampling and en-
semble downsampling. Overall, we achieved 79.2%
(precision), 50.5% (recall), 61.7% (F1) for agree-
ment detection and 69.2% (precision), 46.9% (re-
call), and 55.9% (F1) for disagreement detection, on
English broadcast conversation data. In future work,
we plan to continue adding and refining features, ex-
plore dependencies between features and contextual
cues with respect to agreements and disagreements,
and investigate the efficacy of other machine learn-
ing approaches such as Bayesian networks and Sup-
port Vector Machines.
Acknowledgments
The authors thank Gokhan Tur and Dilek Hakkani-
Tu?r for valuable insights and suggestions. This
work has been supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Army Research Laboratory (ARL) contract num-
ber W911NF-09-C-0089. The U.S. Government is
authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right annotation thereon. The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, ARL, or the U.S. Government.
References
M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: Use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL.
S. Germesin and T. Wilson. 2009. Agreement detection
in multiparty conversation. In Proceedings of Interna-
tional Conference on Multimodal Interfaces.
S. Hahn, R. Ladner, and M. Ostendorf. 2006. Agree-
ment/disagreement classification: Exploiting unla-
beled data using constraint classifiers. In Proceedings
of HLT/NAACL.
D. Hillard, M. Ostendorf, and E. Shriberg. 2003. De-
tection of agreement vs. disagreement in meetings:
Training with unlabeled data. In Proceedings of
HLT/NAACL.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus. In
Proc. ICASSP, Hong Kong, April.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin
Hillard, Mari Ostendorf, and Mary Harper. 2006.
Enriching speech recognition with automatic detec-
tion of sentence boundaries and disfluencies. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 14(5):1526?1540, September. Special Issue
on Progress in Rich Transcription.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bour-
ban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec,
V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln,
A. Lisowska, W. Post, D. Reidsma, and P. Wellner.
2005. The AMI meeting corpus. In Proceedings of
Measuring Behavior 2005, the 5th International Con-
ference on Methods and Techniques in Behavioral Re-
search.
378
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Name-aware Machine Translation
Haibo Li? Jing Zheng? Heng Ji? Qi Li? Wen Wang?
? Computer Science Department and Linguistics Department
Queens College and Graduate Center, City University of New York
New York, NY, USA 10016
{lihaibo.c, hengjicuny, liqiearth}@gmail.com
? Speech Technology & Research Laboratory
SRI International
Menlo Park, CA, USA 94025
{zj, wwang}@speech.sri.com
Abstract
We propose a Name-aware Machine
Translation (MT) approach which can
tightly integrate name processing into MT
model, by jointly annotating parallel cor-
pora, extracting name-aware translation
grammar and rules, adding name phrase
table and name translation driven decod-
ing. Additionally, we also propose a new
MT metric to appropriately evaluate the
translation quality of informative words,
by assigning different weights to differ-
ent words according to their importance
values in a document. Experiments on
Chinese-English translation demonstrated
the effectiveness of our approach on en-
hancing the quality of overall translation,
name translation and word alignment over
a high-quality MT baseline1.
1 Introduction
A shrinking fraction of the world?s Web pages are
written in English, therefore the ability to access
pages across a range of languages is becoming in-
creasingly important. This need can be addressed
in part by cross-lingual information access tasks
such as entity linking (McNamee et al, 2011; Cas-
sidy et al, 2012), event extraction (Hakkani-Tur
et al, 2007), slot filling (Snover et al, 2011) and
question answering (Parton et al, 2009; Parton
and McKeown, 2010). A key bottleneck of high-
quality cross-lingual information access lies in the
performance of Machine Translation (MT). Tradi-
tional MT approaches focus on the fluency and
accuracy of the overall translation but fall short
in their ability to translate certain content word-
s including critical information, especially names.
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only trans-
late 60% person names correctly (Ji et al, 2009).
Incorrect segmentation and translation of names
which often carry central meanings of a sentence
can also yield incorrect translation of long con-
texts. Names have been largely neglected in the
prior MT research due to the following reasons:
? The current dominant automatic MT scoring
metrics (such as Bilingual Evaluation Under-
study (BLEU) (Papineni et al, 2002)) treat
all words equally, but names have relative low
frequency in text (about 6% in newswire and
only 3% in web documents) and thus are vast-
ly outnumbered by function words and com-
mon nouns, etc..
? Name translations pose a greater complexity
because the set of names is open and highly
dynamic. It is also important to acknowledge
that there are many fundamental differences
between the translation of names and other
tokens, depending on whether a name is ren-
dered phonetically, semantically, or a mixture
of both (Ji et al, 2009).
? The artificial settings of assigning low
weights to information translation (compared
to overall word translation) in some large-
scale government evaluations have discour-
aged MT developers to spend time and ex-
plore resources to tackle this problem.
We propose a novel Name-aware MT (NAMT)
approach which can tightly integrate name pro-
cessing into the training and decoding processes of
an end-to-end MT pipeline, and a new name-aware
metric to evaluate MT which can assign different
weights to different tokens according to their im-
portance values in a document. Compared to pre-
vious methods, the novel contributions of our ap-
proach are:
1. Tightly integrate joint bilingual name tag-
ging into MT training by coordinating tagged
604
names in parallel corpora, updating word seg-
mentation, word alignment and grammar ex-
traction (Section 3.1).
2. Tightly integrate name tagging and transla-
tion into MT decoding via name-aware gram-
mar (Section 3.2).
3. Optimize name translation and context trans-
lation simultaneously and conduct name
translation driven decoding with language
model (LM) based selection (Section 3.2).
4. Propose a new MT evaluation metric which
can discriminate names and non-informative
words (Section 4).
2 Baseline MT
As our baseline, we apply a high-performing
Chinese-English MT system (Zheng, 2008; Zheng
et al, 2009) based on hierarchical phrase-based
translation framework (Chiang, 2005). It is based
on a weighted synchronous context-free grammar
(SCFG). All SCFG rules are associated with a set
of features that are used to compute derivation
probabilities. The features include:
? Relative frequency in two directions P (?|?)
andP (?|?), estimating the likelihoods of one
side of the rule r: X ?< ?, ? > translating
into the other side, where ? and ? are strings
of terminals and non-terminals in the source
side and target side. Non-terminals in ? and
? are in one-to-one correspondence.
? Lexical weights in two directions: Pw(?|?)
andPw(?|?), estimating likelihoods of word-
s in one side of the rule r: X ?< ?, ? >
translating into the other side (Koehn et al,
2003).
? Phrase penalty: a penalty exp(1) for a rule
with no non-terminal being used in deriva-
tion.
? Rule penalty: a penalty exp(1) for a rule
with at least one non-terminal being used in
derivation.
? Glue rule penalty: a penalty exp(1) if a glue
rule used in derivation.
? Translation length: number of words in trans-
lation output.
Our previous work showed that combining mul-
tiple LMs trained from different sources can lead
to significant improvement. The LM used for de-
coding is a log-linear combination of four word
n-gram LMs which are built on different English
corpora (details described in section 5.1), with
the LM weights optimized on a development set
and determined by minimum error rate training
(MERT), to estimate the probability of a word giv-
en the preceding words. All four LMs were trained
using modified Kneser-Ney smoothing algorithm
(Chen and Goodman, 1996) and converted into
Bloom filter LMs (Talbot and Brants, 2008) sup-
porting memory map.
The scaling factors for all features are optimized
by minimum error rate training algorithm to max-
imize BLEU score (Och, 2003). Given an input
sentence in the source language, translation into
the target language is cast as a search problem,
where the goal is to find the highest-probability
derivation that generates the source-side sentence,
using the rules in our SCFG. The source-side
derivation corresponds to a synchronous target-
side derivation and the terminal yield of this target-
side derivation is the output of the system. We em-
ploy our CKY-style chart decoder, named SRInter-
p, to solve the search problem.
3 Name-aware MT
We tightly integrate name processing into the
above baseline to construct a NAMT model. Fig-
ure 1 depicts the general procedure.
3.1 Training
This basic training process of NAMT requires us
to apply a bilingual name tagger to annotate par-
allel training corpora. Traditional name tagging
approaches for single languages cannot address
this requirement because they were all built on da-
ta and resources which are specific to each lan-
guage without using any cross-lingual features.
In addition, due to separate decoding processes
the results on parallel data may not be consistent
across languages. We developed a bilingual joint
name tagger (Li et al, 2012) based on condition-
al random fields that incorporates both monolin-
gual and cross-lingual features and conducts join-
t inference, so that name tagging from two lan-
guages can mutually enhance each other and there-
fore inconsistent results can be corrected simulta-
neously. This joint name tagger achieved 86.3%
bilingual pair F-measure with manual alignment
and 84.4% bilingual pair F-measure with automat-
ic alignment as reported in (Li et al, 2012). Given
a parallel sentence pair we first apply Giza++ (Och
and Ney, 2003) to align words, and apply this join-
605
Dec
odin
g
Hier
arch
ical 
Phra
sed-
base
d M
T
Tran
slate
d Te
xt
Tran
slate
Bi-te
xt 
Data Sou
rce Text
Join
t
Nam
e Ta
gger
Sou
rce L
angu
age 
Nam
e Ta
gger
Nam
e Tr
ansl
ator
Trai
ning
Nam
e Pa
ir M
iner
Extr
act s
ourc
e lan
guag
e na
mes
 
and
 add
 the
m to
 dict
iona
ries 
for 
sour
ce la
ngu
age 
nam
e ta
gger
 E
xtra
ct n
ame
 pair
s an
d 
add
 the
m to
tran
slati
on 
dict
iona
ry
Extr
act a
nd a
dd 
nam
e pa
irs t
o
phra
se ta
ble
GIZA
++
Rule
 Extr
acto
r
Extr
act S
CFG
 rule
s wi
th 
com
bina
tion
 of n
ame
-rep
lace
d 
data
 and
 orig
inal 
bi-te
xt d
ata
Rep
lace
 nam
es w
ith 
non
-term
inals
 and
 
com
bine
 with
 the
 
orig
inal 
para
llel d
ata
Figure 1: Architecture of Name-aware Machine Translation System.
t bilingual name tagger to extract three types of
names: (Person (PER), Organization (ORG) and
Geo-political entities (GPE)) from both the source
side and the target side. We pair two entities from
two languages, if they have the same entity type
and are mapped together by word alignment. We
ignore two kinds of names: multi-word names
with conflicting boundaries in two languages and
names only identified in one side of a parallel sen-
tence.
We built a NAMT system from such name-
tagged parallel corpora. First, we replace tagged
name pairs with their entity types, and then
use Giza++ and symmetrization heuristics to re-
generate word alignment. Since the name tags ap-
pear very frequently, the existence of such tags
yields improvement in word alignment quality.
The re-aligned parallel corpora are used to train
our NAMT system based on SCFG. Since the joint
name tagger ensures that each tagged source name
has a corresponding translation on the target side
(and vice versa), we can extract SCFG rules by
treating the tagged names as non-terminals.
However, the original parallel corpora contain
many high-frequency names, which can already be
handled well by the baseline MT. Some of these
names carry special meanings that may influence
translations of the neighboring words, and thus re-
placing them with non-terminals can lead to infor-
mation loss and weaken the translation model. To
address this issue, we merged the name-replaced
parallel data with the original parallel data and ex-
tract grammars from the combined corpus. For ex-
ample, given the following sentence pair:
? -???e???e????? .
? China appeals to world for non involvement
in Angola conflict .
after name tagging it becomes
? GPE??e???e GPE?? .
? GPE appeals to world for non involvement in
GPE conflict .
Both sentence pairs are kept in the combined data
to build the translation model.
3.2 Decoding
During decoding phase, we extract names with
the baseline monolingual name tagger described
in (Li et al, 2012) from a source document. It-
s performance is comparable to the best report-
ed results on Chinese name tagging on Automat-
ic Content Extraction (ACE) data (Ji and Grish-
man, 2006; Florian et al, 2006; Zitouni and Flo-
rian, 2008; Nguyen et al, 2010). Then we ap-
ply a state-of-the-art name translation system (Ji
et al, 2009) to translate names into the target lan-
guage. The name translation system is composed
of the following steps: (1) Dictionary matching
based on 150,041 name translation pairs; (2) Sta-
tistical name transliteration based on a structured
perceptron model and a character based MT mod-
el (Dayne and Shahram, 2007); (3) Context infor-
mation extraction based re-ranking.
In our NAMT framework, we add the following
extensions to name translation.
We developed a name origin classifier based on
Chinese last name list (446 name characters) and
name structure parsing features to distinguish Chi-
nese person names and foreign person names (Ji,
2009), so that pinyin conversion is applied for Chi-
nese names while name transliteration is applied
only for foreign names. This classifier works rea-
sonably well in most cases (about 92% classifica-
tion accuracy), except when a common Chinese
last name appears as the first character of a foreign
606
name, such as ?1?? which can be translated ei-
ther as ?Jolie? or ?Zhu Li?.
For those names with fewer than five instances
in the training data, we use the name translation
system to provide translations; for the rest of the
names, we leave them to the baseline MT mod-
el to handle. The joint bilingual name tagger was
also exploited to mine bilingual name translation
pairs from parallel training corpora. The mapping
score between a Chinese name and an English
name was computed by the number of aligned to-
kens. A name pair is extracted if the mapping
score is the highest among all combinations and
the name types on both sides are identical. It is
necessary to incorporate word alignment as addi-
tional constraints because the order of names is of-
ten changed after translation. Finally, the extract-
ed 9,963 unique name translation pairs were also
used to create an additional name phrase table for
NAMT. Manual evaluation on 2,000 name pairs
showed the accuracy is 86%.
The non-terminals in SCFG rules are rewritten
to the extracted names during decoding, therefore
allow unseen names in the test data to be trans-
lated. Finally, based on LMs, our decoder ex-
ploits the dynamically created phrase table from
name translation, competing with originally ex-
tracted rules, to find the best translation for the
input sentence.
4 Name-aware MT Evaluation
Traditional MT evaluation metrics such as
BLEU (Papineni et al, 2002) and Translation Ed-
it Rate (TER) (Snover et al, 2006) assign the
same weights to all tokens equally. For exam-
ple, incorrect translations of ?the? and ?Bush? will
receive the same penalty. However, for cross-
lingual information processing applications, we
should acknowledge that certain informationally
critical words are more important than other com-
mon words. In order to properly evaluate the trans-
lation quality of NAMT methods, we propose to
modify the BLEU metric so that they can dynam-
ically assign more weights to names during evalu-
ation.
BLEU considers the correspondence between a
system translation and a human translation:
BLEU = BP ? exp
( N?
n=1
wn log pn
)
(1)
where BP is brevity penalty defined as follows:
BP =
{
1 if c > r,
e(1?r/c) if c ? r. (2)
where wn is a set of positive weights summing to
one and usually uniformly set as wn = 1/N , c is
the length of the system translation and r is the
length of reference translation, and pn is modified
n-gram precision defined as:
pn =
?
C?Candidates
?
n-gram?C
Countclip(n-gram)
?
C??Candidates
?
n-gram??C?
Countclip(n-gram?)
(3)
where C and C ? are translation candidates in the
candidate sentence set, if a source sentence is
translated to many candidate sentences.
As in BLEU metric, we first count the maxi-
mum number of times an n-gram occurs in any s-
ingle reference translation. The total count of each
candidate n-gram is clipped at sentence level by it-
s maximum reference count. Then we add up the
weights of clipped n-grams and divide them by the
total weight of all n-grams.
Based on BLEU score, we design a name-aware
BLEU metric as follows. Depending on whether a
token t is contained in a name in reference trans-
lation, we assign a weight weightt to t as follows:
weightt ={
1? e?tf(t,d)?idf(t,D), if t never appears in names
1 + PEZ , if t occurs in name(s)
(4)
where PE is the sum of penalties of non-name
tokens and Z is the number of tokens within all
names:
PE =
?
t never appears in names
e?tf(t,d)?idf(t,D) (5)
In this paper, the tf ? idf score is computed at sen-
tence level, therefore, D is the sentence set and
each d ? D is a sentence.
The weight of an n-gram in reference translation
is the sum of weights of all tokens it contains.
weightngram =
?
t?ngram
weightt (6)
Next, we compute the weighted modified n-
gram precision Countweight?clip(n-gram) as fol-
lows:
Countweight?clip(n-gram) =?
if the ngrami is correctly translated
weightngrami (7)
607
The Countclip(n-gram) in the equation 3 is
substituted with aboveCountweight?clip(n-gram).
When we sum up the total weight of all n-grams of
a candidate translation, some n-grams may contain
tokens which do not exist in reference translation.
We assign the lowest weight of tokens in reference
translation to these rare tokens.
We also add an item, name penalty NP , to
penalize the output sentences which contain too
many or too few names:
NP = e?(uv?1)2/2? (8)
where u is the number of name tokens in system
translation and v is the number of name tokens in
reference translation.
Finally the name-aware BLEU score is defined
as:
BLEUNA = BP ?NP ? exp
( N?
n=1
wn logwpn
)
(9)
This new metric can also be applied to evalu-
ate MT approaches which emphasize other types
of facts such as events, by simply replacing name
tokens by other fact tokens.
5 Experiments
In this section we present the experimental results
of NAMT compared to the baseline MT.
5.1 Data Set
We used a large Chinese-English MT training cor-
pus from various sources and genres (including
newswire, web text, broadcast news and broadcast
conversations) for our experiments. We also used
some translation lexicon data and Wikipedia trans-
lations. The majority of the data sets were col-
lected or made available by LDC for U.S. DARPA
Translingual Information Detection, Extraction
and Summarization (TIDES) program, Global Au-
tonomous Language Exploitation (GALE) pro-
gram, Broad Operational Language Translation
(BOLT) program and National Institute of Stan-
dards and Technology (NIST) MT evaluations.
The training corpus includes 1,686,458 sentence
pairs. The joint name tagger extracted 1,890,335
name pairs (295,087 Persons, 1,269,056 Geo-
political entities and 326,192 Organizations).
Four LMs, denoted LM1, LM2, LM3, and
LM4, were trained from different English cor-
pora. LM1 is a 7-gram LM trained on the tar-
get side of Chinese-English and Egyptian Arabic-
English parallel text, English monolingual discus-
sion forums data R1-R4 released in BOLT Phase
1 (LDC2012E04, LDC2012E16, LDC2012E21,
LDC2012E54), and English Gigaword Fifth Edi-
tion (LDC2011T07). LM2 is a 7-gram LM trained
only on the English monolingual discussion fo-
rums data listed above. LM3 is a 4-gram LM
trained on the web genre among the target side
of all parallel text (i.e., web text from pre-BOLT
parallel text and BOLT released discussion fo-
rum parallel text). LM4 is a 4-gram LM trained
on the English broadcast news and conversation
transcripts released under the DARPA GALE pro-
gram. Note that for LM4 training data, some tran-
scripts were quick transcripts and quick rich tran-
scripts released by LDC, and some were generated
by running flexible alignment of closed captions or
speech recognition output from LDC on the audio
data (Venkataraman et al, 2004).
In order to demonstrate the effectiveness and
generality of our approach, we evaluated our ap-
proach on seven test sets from multiple genres and
domains. We asked four annotators to annotate
names in four reference translations of each sen-
tence and an expert annotator to adjudicate result-
s. The detailed statistics and name distribution of
each test data set is shown in Table 1. The per-
centage of names occurred fewer than 5 times in
training data are listed in the brackets in the last
column of the table.
5.2 Overall Performance
Besides the new name-aware MT metric, we also
adopt two traditional metrics, TER to evaluate the
overall translation performance and Named Entity
Weak Accuracy (NEWA) (Hermjakob et al, 2008)
to evaluate the name translation performance.
TER measures the amount of edits required to
change a system output into one of the reference
translations. Specifically:
TER = # of editsaverage # of reference words (10)
Possible edits include insertion, substitution dele-
tion and shifts of words.
The NEWA metric is defined as follows. Us-
ing a manually assembled name variant table, we
also support the matching of name variants (e.g.,
?World Health Organization? and ?WHO?).
NEWA = Count # of correctly translated namesCount # of names in references (11)
608
Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)
BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)
BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)
BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)
BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)
BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)
NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)
NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)
Table 1: Statistics and Name Distribution of Test Data Sets.
Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008
BLEU
Baseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3
NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3
NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0
Name-aware BLEU
Baseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7
NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8
NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1
TER
Baseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0
NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9
NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5
NEWA
All
Baseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4
NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9
NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2
GPE
Baseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0
NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5
NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3
PER
Baseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2
NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9
NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3
ORG
Baseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3
NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7
NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8
Table 2: Translation Performance (%).
For better comparison with NAMT, besides the
original baseline, we develop the other baseline
system by adding name translation table into the
phrase table (NPhrase).
Table 2 presents the performance of overal-
l translation and name translation. We can see
that except for the BOLT3 data set with BLEU
metric, our NAMT approach consistently outper-
formed the baseline system for all data sets with
all metrics, and provided up to 23.6% relative er-
ror reduction on name translation. According to
Wilcoxon Matched-Pairs Signed-Ranks Test, the
improvement is not significant with BLEU metric,
but is significant at 98% confidence level with all
of the other metrics. The gains are more signifi-
cant for formal genres than informal genres main-
ly because most of the training data for name tag-
ging and name translation were from newswire.
Furthermore, using external name translation table
only did not improve translation quality in most
test sets except for BOLT2. Therefore, it is im-
portant to use name-replaced corpora for rule ex-
traction to fully take advantage of improved word
alignment.
Many errors from the baseline MT approach oc-
curred because some parts of out-of-vocabulary
names were mistakenly segmented into common
words. For example, the baseline MT system mis-
takenly translated a person name ?Y?? (Sun
Honglei)? into ?Sun red thunder?. In informal
genres such as discussion forums and web blogs,
even common names often appear in rare form-
s due to misspelling or morphing. For example,
?e8l (Obama)? was mistakenly translated into
?Ma Olympic?. Such errors can be compounded
when word re-ordering was applied. For example,
the following sentence: ????????/:
'J/iy (Guo Meimei?s strength real-
ly is formidable, I really admire her)? was mis-
takenly translated into ?Guo the strength of the
America and the America also really strong , ah
, really admire her? by the baseline MT system
because the person name ???? (Guomeimei)?
was mistakenly segmented into three words ??
(Guo)?, ?? (the America)? and ?? (the Ameri-
ca)?. But our NAMT approach successfully iden-
tified and translated this name and also generated
better overall translation: ?Guo Meimei ?s power
is also really strong , ah , really admire her?.
609
B L E U N a m e - a w a r eB L E U024681 01 2
1 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .  1  H u m .  2 H u m .  30 . 00 . 51 . 01 . 52 . 02 . 5
3 . 03 . 54 . 0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o n
Figure 2: Scores based on Automatic Metrics and Human
Evaluation.
5.3 Name-aware BLEU vs The Human
Evaluation
In order to investigate the correlation between
name-aware BLEU scores and human judgment
results, we asked three bi-lingual speakers to judge
our translation output from the baseline system
and the NAMT system, on a Chinese subset of 250
sentences (each sentence has two corresponding
translations from baseline and NAMT) extracted
randomly from 7 test corpora. The annotators rat-
ed each translation from 1 (very bad) to 5 (very
good) and made their judgments based on whether
the translation is understandable and conveys the
same meaning.
We computed the name-aware BLEU scores on
the subset and also the aggregated average scores
from human judgments. Figure 2 shows that
NAMT consistently achieved higher scores with
both name-aware BLEU metric and human judge-
ment. Furthermore, we calculated three Pearson
product-moment correlation coefficients between
human judgment scores and name-aware BLEU s-
cores of these two MT systems. Give the sample
size and the correlation coefficient value, the high
significance value of 0.99 indicates that name-
aware BLEU tracks human judgment well.
5.4 Word Alignment
It is also important to investigate the impact of our
NAMT approach on improving word alignmen-
t. We conducted the experiment on the Chinese-
English Parallel Treebank (Li et al, 2010) with
ground-truth word alignment. The detailed pro-
cedure following NAMT framework is as follows:
(1) Ran the joint bilingual name tagger; (2) Re-
placed each name string with its name type (PER,
ORG or GPE), and ran Giza++ on the replaced
sentences; (3) Ran Giza++ on the words within
Words Method P R F 
Baseline Giza++ 69.8 47.8 56.7 
Joint Name 
Tagging 
70.4 48.1 57.1 
 
Overall 
Words 
Ground-truth 
Name Tagging 
(Upper-bound) 
71.3 48.9 58.0 
Baseline Giza++ 86.0 31.4 46.0 Words 
Within 
Names 
Joint Name 
Tagging 
77.6 37.2 50.3 
 
 
 
 
 
 
 
 
 
 
Table 3: Impact of Joint Bilingual Name Tagging on Word
Alignment (%).
each name pair. (4) Merged (2) and (3) to pro-
duce the final word alignment results. In order to
compare with the upper-bound gains, we also mea-
sured the performance of applying ground-truth
name tagging with the above procedures.
The experiment results are shown in Table 3.
For the words within names, our approach provid-
ed significant gains by enhancing F-measure from
46.0% to 50.3%. Only 10.6% words are within
names, therefore the upper-bound gains on over-
all word alignment is only 1.3%. Our joint name
tagging approach achieved 0.4% (statistically sig-
nificant) improvement over the baseline. In Fig-
ure 3 we categorized the sentences according to
the percentage of name words in each sentence and
measured the improvement for each category. We
can clearly see that as the sentences include more
names, the gains achieved by our approach tend to
be greater.
5.5 Remaining Error Analysis
Although the proposed model has significantly en-
hanced translation quality, some challenges re-
main. We analyze some major sources of the re-
maining errors as follows.
1. Name Structure Parsing.
We found that the gains of our NAMT approach
were mainly achieved for names with one or two
components. When the name structure becomes
too complicated to parse, name tagging and name
translation are likely to produce errors, especially
for long nested organizations. For example, ??0
???b?@? (Anti-malfeasance Bureau of
Gutian County Procuratorate) consists of a nested
organization name with a GPE as modifier: ??
0???b? (Gutian County Procuratorate) and
an ORG name: ??@? (Anti-malfeasance Bu-
reau).
2. Name abbreviation tagging and translation.
Some organization abbreviations are also dif-
ficult to extract because our name taggers have
610
0~10 10~20 20~30 30~40 >40
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
F-Measure Gains in Overall Word Alignment (%)
#name tokens/#all tokens(%)
 Baseline Giza++
 Joint Name Tagging
 Ground-truth Name Tagging (Upper-bound)
Figure 3: Word alignment gains according to the percentage
of name words in each sentence.
not incorporated any coreference resolution tech-
niques. For example, without knowing that ?FAW?
refers to ?First Automotive Works? in ?FAW has
also utilized the capital market to directly fi-
nance, and now owns three domestic listed compa-
nies?, our system mistakenly labeled it as a GPE.
The same challenge exists in name alignment and
translation (for example, ? i (Min Ge)? refer-
s to ? -??Zi}?X