Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1792?1797,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Detecting Non-compositional MWE Components using Wiktionary
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We propose a simple unsupervised ap-
proach to detecting non-compositional
components in multiword expressions
based on Wiktionary. The approach makes
use of the definitions, synonyms and trans-
lations in Wiktionary, and is applicable to
any type of MWE in any language, assum-
ing the MWE is contained in Wiktionary.
Our experiments show that the proposed
approach achieves higher F-score than
state-of-the-art methods.
1 Introduction
A multiword expression (MWE) is a combina-
tion of words with lexical, syntactic or seman-
tic idiosyncrasy (Sag et al., 2002; Baldwin and
Kim, 2009). An MWE is considered (semanti-
cally) ?non-compositional? when its meaning is
not predictable from the meaning of its compo-
nents. Conversely, compositional MWEs are those
whose meaning is predictable from the meaning
of the components. Based on this definition, a
component is compositional within an MWE, if its
meaning is reflected in the meaning of the MWE,
and it is non-compositional otherwise.
Understanding which components are non-
compositional within an MWE is important in
NLP applications in which semantic information
is required. For example, when searching for
spelling bee, we may also be interested in docu-
ments about spelling, but not those which contain
only bee. For research project, on the other hand,
we are likely to be interested in documents which
contain either research or project in isolation, and
for swan song, we are only going to be interested
in documents which contain the phrase swan song,
and not just swan or song.
In this paper, we propose an unsupervised ap-
proach based on Wikitionary for predicting which
components of a given MWE have a composi-
tional usage. Experiments over two widely-used
datasets show that our approach outperforms state-
of-the-art methods.
2 Related Work
Previous studies which have considered MWE
compositionality have focused on either the iden-
tification of non-compositional MWE token in-
stances (Kim and Baldwin, 2007; Fazly et al.,
2009; Forthergill and Baldwin, 2011; Muzny and
Zettlemoyer, 2013), or the prediction of the com-
positionality of MWE types (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). The
identification of non-compositional MWE tokens
is an important task when a word combination
such as kick the bucket or saw logs is ambiguous
between a compositional (generally non-MWE)
and non-compositional MWE usage. Approaches
have ranged from the unsupervised learning of
type-level preferences (Fazly et al., 2009) to su-
pervised methods specific to particular MWE con-
structions (Kim and Baldwin, 2007) or applica-
ble across multiple constructions using features
similar to those used in all-words word sense
disambiguation (Forthergill and Baldwin, 2011;
Muzny and Zettlemoyer, 2013). The prediction
of the compositionality of MWE types has tradi-
tionally been couched as a binary classification
task (compositional or non-compositional: Bald-
win et al. (2003), Bannard (2006)), but more re-
cent work has moved towards a regression setup,
where the degree of the compositionality is pre-
dicted on a continuous scale (Reddy et al., 2011;
Salehi and Cook, 2013; Salehi et al., 2014). In ei-
ther case, the modelling has been done either over
the whole MWE (Reddy et al., 2011; Salehi and
Cook, 2013), or relative to each component within
the MWE (Baldwin et al., 2003; Bannard, 2006).
In this paper, we focus on the binary classification
of MWE types relative to each component of the
1792
MWE.
The work that is perhaps most closely related to
this paper is that of Salehi and Cook (2013) and
Salehi et al. (2014), who use translation data to
predict the compositionality of a given MWE rel-
ative to each of its components, and then combine
those scores to derive an overall compositionality
score. In both cases, translations of the MWE and
its components are sourced from PanLex (Bald-
win et al., 2010; Kamholz et al., 2014), and if
there is greater similarity between the translated
components and MWE in a range of languages,
the MWE is predicted to be more compositional.
The basis of the similarity calculation is unsuper-
vised, using either string similarity (Salehi and
Cook, 2013) or distributional similarity (Salehi et
al., 2014). However, the overall method is su-
pervised, as training data is used to select the
languages to aggregate scores across for a given
MWE construction. To benchmark our method,
we use two of the same datasets as these two pa-
pers, and repurpose the best-performing methods
of Salehi and Cook (2013) and Salehi et al. (2014)
for classification of the compositionality of each
MWE component.
3 Methodology
Our basic method relies on analysis of lexical
overlap between the component words and the def-
initions of the MWE in Wiktionary, in the man-
ner of Lesk (1986). That is, if a given component
can be found in the definition, then it is inferred
that the MWE carries the meaning of that compo-
nent. For example, the Wiktionary definition of
swimming pool is ?An artificially constructed pool
of water used for swimming?, suggesting that the
MWE is compositional relative to both swimming
and pool. If the MWE is not found in Wiktionary,
we use Wikipedia as a backoff, and use the first
paragraph of the (top-ranked) Wikipedia article as
a proxy for the definition.
As detailed below, we further extend the basic
method to incorporate three types of information
found in Wiktionary: (1) definitions of each word
in the definitions, (2) synonyms of the words in the
definitions, and (3) translations of the MWEs and
components.
3.1 Definition-based Similarity
The basic method uses Boolean lexical overlap be-
tween the target component of the MWE and a
definition. A given MWE will often have multiple
definitions, however, begging the question of how
to combine across them, for which we propose the
following three methods.
First Definition (FIRSTDEF): Use only the
first-listed Wiktionary definition for the MWE,
based on the assumption that this is the predom-
inant sense.
All Definitions (ALLDEFS): In the case that
there are multiple definitions for the MWE, cal-
culate the lexical overlap for each independently
and take a majority vote; in the case of a tie, label
the component as non-compositional.
Idiom Tag (ITAG): In Wiktionary, there is fa-
cility for users to tag definitions as idiomatic.
1
If,
for a given MWE, there are definitions tagged as
idiomatic, use only those definitions; if there are
no such definitions, use the full set of definitions.
3.2 Synonym-based Definition Expansion
In some cases, a component is not explicitly men-
tioned in a definition, but a synonym does occur,
indicating that the definition is compositional in
that component. In order to capture synonym-
based matches, we optionally look for synonyms
of the component word in the definition,
2
and ex-
pand our notion of lexical overlap to include these
synonyms.
For example, for the MWE china clay, the defi-
nition is kaolin, which includes neither of the com-
ponents. However, we find the component word
clay in the definition for kaolin, as shown below.
A fine clay, rich in kaolinite, used in ce-
ramics, paper-making, etc.
This method is compatible with the three
definition-based similarity methods described
above, and indicated by the +SYN suffix (e.g.
FIRSTDEF+SYN is FIRSTDEF with synonym-
based expansion).
3.3 Translations
A third information source in Wiktionary that can
be used to predict compositionality is sense-level
translation data. Due to the user-generated na-
ture of Wiktionary, the set of languages for which
1
Although the recall of these tags is low (Muzny and
Zettlemoyer, 2013).
2
After removing function words, based on a stopword list.
1793
ENC EVPC
WordNet 91.1% 87.5%
Wiktionary 96.7% 96.2%
Wiktionary+Wikipedia 100.0% 96.2%
Table 1: Lexical coverage of WordNet, Wik-
tionary and Wiktionary+Wikipedia over our two
datasets.
translations are provided varies greatly across lexi-
cal entries. Our approach is to take whatever trans-
lations happen to exist in Wiktionary for a given
MWE, and where there are translations in that lan-
guage for the component of interest, use the LCS-
based method of Salehi and Cook (2013) to mea-
sure the string similarity between the translation
of the MWE and the translation of the compo-
nents. Unlike Salehi and Cook (2013), however,
we do not use development data to select the opti-
mal set of languages in a supervised manner, and
instead simply take the average of the string simi-
larity scores across the available languages. In the
case of more than one translation in a given lan-
guage, we use the maximum string similarity for
each pairing of MWE and component translation.
Unlike the definition and synonym-based ap-
proach, the translation-based approach will pro-
duce real rather than binary values. To combine
the two approaches, we discretise the scores given
by the translation approach. In the case of dis-
agreement between the two approaches, we label
the given MWE as non-compositional. This re-
sults in higher recall and lower precision for the
task of detecting compositionality.
3.4 An Analysis of Wiktionary Coverage
A dictionary-based method is only as good as the
dictionary it is applied to. In the case of MWE
compositionality analysis, our primary concern is
lexical coverage in Wiktionary, i.e., what propor-
tion of a representative set of MWEs is contained
in Wiktionary. We measure lexical coverage rela-
tive to the two datasets used in this research (de-
scribed in detail in Section 4), namely 90 En-
glish noun compounds (ENCs) and 160 English
verb particle constructions (EVPCs). In each case,
we calculated the proportion of the dataset that
is found in Wiktionary, Wiktionary+Wikipedia
(where we back off to a Wikipedia document in the
case that a MWE is not found in Wiktionary) and
WordNet (Fellbaum, 1998). The results are found
in Table 1, and indicate perfect coverage in Wik-
tionary+Wikipedia for the ENCs, and very high
coverage for the EVPCs. In both cases, the cov-
erage of WordNet is substantially lower, although
still respectable, at around 90%.
4 Datasets
As mentioned above, we evaluate our method over
the same two datasets as Salehi and Cook (2013)
(which were later used, in addition to a third
dataset of German noun compounds, in Salehi
et al. (2014)): (1) 90 binary English noun com-
pounds (ENCs, e.g. spelling bee or swimming
pool); and (2) 160 English verb particle construc-
tions (EVPCs, e.g. stand up and give away). Our
results are not directly comparable with those of
Salehi and Cook (2013) and Salehi et al. (2014),
however, who evaluated in terms of a regression
task, modelling the overall compositionality of the
MWE. In our case, the task setup is a binary clas-
sification task relative to each of the two compo-
nents of the MWE.
The ENC dataset was originally constructed by
Reddy et al. (2011), and annotated on a contin-
uous [0, 5] scale for both overall compositional-
ity and the component-wise compositionality of
each of the modifier and head noun. The sampling
was random in an attempt to make the dataset bal-
anced, with 48% of compositional English noun
compounds, of which 51% are compositional in
the first component and 60% are compositional in
the second component. We generate discrete la-
bels by discretising the component-wise composi-
tionality scores based on the partitions [0, 2.5] and
(2.5, 5]. On average, each NC in this dataset has
1.4 senses (definitions) in Wiktionary.
The EVPC dataset was constructed by Ban-
nard (2006), and manually annotated for com-
positionality on a binary scale for each of the
head verb and particle. For the 160 EVPCs,
76% are verb-compositional and 48% are particle-
compositional. On average, each EVPC in this
dataset has 3.0 senses (definitions) in Wiktionary.
5 Experiments
The baseline for each dataset takes the form of
looking for a user-annotated idiom tag in the Wik-
tionary lexical entry for the MWE: if there is an id-
iomatic tag, both components are considered to be
non-compositional; otherwise, both components
are considered to be compositional. We expect
this method to suffer from low precision for two
1794
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 66.7 68.2 67.4 66.7 83.3 74.1
LCS 60.0 77.7 67.7 81.6 68.1 64.6
DS 62.1 88.6 73.0 80.5 86.4 71.2
DS+DSL2 62.5 92.3 74.5 78.4 89.4 70.6
LCS+DS+DSL2 66.3 87.5 75.4 82.1 80.6 70.1
FIRSTDEF 59.4 93.2 72.6 54.2 88.9 67.4
ALLDEFS 59.5 100.0 74.6 52.9 100.0 69.2
ITAG 60.3 100.0 75.2 54.5 100.0 70.6
FIRSTDEF+SYN 64.9 84.1 73.3 63.8 83.3 72.3
ALLDEFS+SYN 64.5 90.9 75.5 60.4 88.9 71.9
ITAG+SYN 64.5 90.9 75.5 61.8 94.4 74.7
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
82.9 85.3 84.1 81.9 80.0 69.8
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
81.2 88.1 84.5 87.3 80.6 73.3
ITAG+SYN
COMB(LCS+DS+DSL2)
81.0 88.1 84.1 88.0 81.1 73.9
Table 2: Compositionality prediction results over the ENC dataset, relative to the first component (the
modifier noun) and the second component (the head noun)
reasons: first, the guidelines given to the annota-
tors of our datasets might be different from what
Wiktionary contributors assume to be an idiom.
Second, the baseline method assumes that for any
non-compositional MWE, all components must be
equally non-compositional, despite the wealth of
MWEs where one or more components are com-
positional (e.g. from the Wiktionary guidelines
for idiom inclusion,
3
computer chess, basketball
player, telephone box).
We also compare our method with: (1) ?LCS?,
the string similarity-based method of Salehi and
Cook (2013), in which 54 languages are used;
(2) ?DS?, the monolingual distributional similarity
method of Salehi et al. (2014); (3) ?DS+DSL2?,
the multilingual distributional similarity method
of Salehi et al. (2014), including supervised lan-
guage selection for a given dataset, based on cross-
validation; and (4) ?LCS+DS+DSL2?, whereby
the first three methods are combined using a su-
pervised support vector regression model. In
each case, the continuous output of the model
is equal-width discretised to generate a binary
classification. We additionally present results for
the combination of each of the six methods pro-
posed in this paper with LCS, DS and DSL2, us-
ing a linear-kernel support vector machine (rep-
resented with the suffix ?
COMB(LCS+DS+DSL2)
? for
a given method). The results are based on cross-
3
http://en.wiktionary.org/wiki/
Wiktionary:Idioms_that_survived_RFD
validation, and for direct comparability, the parti-
tions are exactly the same as Salehi et al. (2014).
Tables 2 and 3 provide the results when our pro-
posed method for detecting non-compositionality
is applied to the ENC and EVPC datasets, respec-
tively. The inclusion of translation data was found
to improve all of precision, recall and F-score
across the board for all of the proposed methods.
For reasons of space, results without translation
data are therefore omitted from the paper.
Overall, the simple unsupervised methods pro-
posed in this paper are comparable with the unsu-
pervised and supervised state-of-the-art methods
of Salehi and Cook (2013) and Salehi et al. (2014),
with ITAG achieving the highest F-score for the
ENC dataset and for the verb components of the
EVPC dataset. The inclusion of synonyms boosts
results in most cases.
When we combine each of our proposed meth-
ods with the string and distributional similar-
ity methods of Salehi and Cook (2013) and
Salehi et al. (2014), we see substantial improve-
ments over the comparable combined method of
?LCS+DS+DSL2? in most cases, demonstrating
both the robustness of the proposed methods and
their complementarity with the earlier methods. It
is important to reinforce that the proposed meth-
ods make no language-specific assumptions and
are therefore applicable to any type of MWE and
any language, with the only requirement being that
the MWE of interest be listed in the Wiktionary for
1795
Method
First Component Second Component
Precision Recall F-score Precision Recall F-score
Baseline 24.6 36.8 29.5 59.6 40.5 48.2
LCS 36.5 49.2 39.3 61.5 63.7 60.3
DS 32.8 34.1 33.5 80.9 19.6 29.7
DS+DSL2 31.8 72.4 44.2 74.8 27.5 36.6
LCS+DS+DSL2 36.1 62.6 45.8 77.9 42.8 49.2
FIRSTDEF 24.8 84.2 38.3 54.5 94.0 69.0
ALLDEFS 25.0 97.4 39.8 53.6 97.6 69.2
ITAG 26.2 89.5 40.5 54.6 91.7 68.4
FIRSTDEF+SYN 32.9 65.8 43.9 60.4 65.5 62.9
ALLDEFS+SYN 28.4 81.6 42.1 62.5 77.4 69.1
ITAG+SYN 30.5 65.8 41.7 57.8 61.9 59.8
FIRSTDEF+SYN
COMB(LCS+DS+DSL2)
34.0 65.3 44.7 83.6 67.3 65.4
ALLDEFS+SYN
COMB(LCS+DS+DSL2)
37.4 70.9 48.9 80.4 65.9 63.0
ITAG+SYN
COMB(LCS+DS+DSL2)
35.6 70.9 47.4 83.5 64.9 64.2
Table 3: Compositionality prediction results over the EVPC dataset, relative to the first component (the
head verb) and the second component (the particle)
that language.
6 Error Analysis
We analysed all items in each dataset where the
system score differed from that of the human
annotators. For both datasets, the majority of
incorrectly-labelled items were compositional but
predicted to be non-compositional by our sys-
tem, as can be seen in the relatively low preci-
sion scores in Tables 2 and 3. In many of these
cases, the prediction based on definitions and syn-
onyms was compositional but the prediction based
on translations was non-compositional. In such
cases, we arbitrarily break the tie by labelling the
instance as non-compositional, and in doing so
favour recall over precision.
Some of the incorrectly-labelled ENCs have
a gold-standard annotation of around 2.5, or in
other words are semi-compositional. For exam-
ple, the compositionality score for game in game
plan is 2.82/5, but our system labels it as non-
compositional; a similar thing happens with figure
and the EVPC figure out. Such cases demonstrate
the limitation of approaches to MWE composi-
tionality that treat the problem as a binary clas-
sification task.
On average, the EVPCs have three senses,
which is roughly twice the number for ENCs. This
makes the prediction of compositionality harder,
as there is more information to combine across (an
effect that is compounded with the addition of syn-
onyms and translations). In future work, we hope
to address this problem by first finding the sense
which matches best with the sentences given to the
annotators.
7 Conclusion
We have proposed an unsupervised approach for
predicting the compositionality of an MWE rel-
ative to each of its components, based on lexi-
cal overlap using Wiktionary, optionally incorpo-
rating synonym and translation data. Our experi-
ments showed that the various instantiations of our
approach are superior to previous state-of-the-art
supervised methods. All code to replicate the re-
sults in this paper has been made publicly avail-
able at https://github.com/bsalehi/
wiktionary_MWE_compositionality.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
References
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
1796
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M.
Colowick. 2010. PanLex and LEXTRACT: Trans-
lating all words of all languages of the world. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Demonstrations, pages
37?40, Beijing, China.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Richard Forthergill and Timothy Baldwin. 2011.
Fleshing it out: A supervised approach to MWE-
token and MWE-type classification. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 911?
919, Chiang Mai, Thailand.
David Kamholz, Jonathan Pool, and Susan Colowick.
2014. PanLex: Building a resource for panlingual
lexical translation. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC?14), pages 3145?3150, Reyk-
javik, Iceland.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th Annual International Conference on Systems
Documentation, pages 24?26, Ontario, Canada.
Grace Muzny and Luke Zettlemoyer. 2013. Auto-
matic idiom identification in Wiktionary. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1417?
1421, Seattle, USA.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2014.
Using distributional similarity of multi-way transla-
tions to predict multiword expression composition-
ality. In Proceedings of the 14th Conference of the
EACL (EACL 2014), pages 472?481, Gothenburg,
Sweden.
1797
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472?481,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Using Distributional Similarity of Multi-way Translations to Predict
Multiword Expression Compositionality
Bahar Salehi,
??
Paul Cook
?
and Timothy Baldwin
??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
We predict the compositionality of multi-
word expressions using distributional sim-
ilarity between each component word and
the overall expression, based on transla-
tions into multiple languages. We evaluate
the method over English noun compounds,
English verb particle constructions and
German noun compounds. We show that
the estimation of compositionality is im-
proved when using translations into multi-
ple languages, as compared to simply us-
ing distributional similarity in the source
language. We further find that string sim-
ilarity complements distributional similar-
ity.
1 Compositionality of MWEs
Multiword expressions (hereafter MWEs) are
combinations of words which are lexically, syntac-
tically, semantically or statistically idiosyncratic
(Sag et al., 2002; Baldwin and Kim, 2009). Much
research has been carried out on the extraction and
identification of MWEs
1
in English (Schone and
Jurafsky, 2001; Pecina, 2008; Fazly et al., 2009)
and other languages (Dias, 2003; Evert and Krenn,
2005; Salehi et al., 2012). However, considerably
less work has addressed the task of predicting the
meaning of MWEs, especially in non-English lan-
guages. As a step in this direction, the focus of
this study is on predicting the compositionality of
MWEs.
An MWE is fully compositional if its meaning
is predictable from its component words, and it is
non-compositional (or idiomatic) if not. For ex-
ample, stand up ?rise to one?s feet? is composi-
1
In this paper, we follow Baldwin and Kim (2009) in
considering MWE ?identification? to be a token-level disam-
biguation task, and MWE ?extraction? to be a type-level lex-
icon induction task.
tional, because its meaning is clear from the mean-
ing of the components stand and up. However, the
meaning of strike up ?to start playing? is largely
unpredictable from the component words strike
and up.
In this study, following McCarthy et al. (2003)
and Reddy et al. (2011), we consider composition-
ality to be graded, and aim to predict the degree
of compositionality. For example, in the dataset
of Reddy et al. (2011), climate change is judged
to be 99% compositional, while silver screen is
48% compositional and ivory tower is 9% com-
positional. Formally, we model compositionality
prediction as a regression task.
An explicit handling of MWEs has been shown
to be useful in NLP applications (Ramisch, 2012).
As an example, Carpuat and Diab (2010) proposed
two strategies for integrating MWEs into statisti-
cal machine translation. They show that even a
large scale bilingual corpus cannot capture all the
necessary information to translate MWEs, and that
in adding the facility to model the compositional-
ity of MWEs into their system, they could improve
translation quality. Acosta et al. (2011) showed
that treating non-compositional MWEs as a sin-
gle unit in information retrieval improves retrieval
effectiveness. For example, while searching for
documents related to ivory tower, we are almost
certainly not interested in documents relating to
elephant tusks.
Our approach is to use a large-scale multi-way
translation lexicon to source translations of MWEs
and their component words, and then model the
relative similarity between each of the component
words and the MWE, using distributional similar-
ity based on monolingual corpora for the source
language and each of the target languages. Our
hypothesis is that using distributional similarity
in more than one language will improve the pre-
diction of compositionality. Importantly, in order
to make the method as language-independent and
472
broadly-applicable as possible, we make no use of
corpus preprocessing such as lemmatisation, and
rely only on the availability of a translation dictio-
nary and monolingual corpora.
Our results confirm our hypothesis that distri-
butional similarity over the source language in ad-
dition to multiple target languages improves the
quality of compositionality prediction. We also
show that our method can be complemented with
string similarity (Salehi and Cook, 2013) to further
improve compositionality prediction. We achieve
state-of-the-art results over two datasets.
2 Related Work
Most recent work on predicting the composi-
tionality of MWEs can be divided into two
categories: language/construction-specific and
general-purpose. This can be at either the token-
level (over token occurrences of an MWE in a cor-
pus) or type-level (over the MWE string, indepen-
dent of usage). The bulk of work on composition-
ality has been language/construction-specific and
operated at the token-level, using dedicated meth-
ods to identify instances of a given MWE, and
specific properties of the MWE in that language
to predict compositionality (Lin, 1999; Kim and
Baldwin, 2007; Fazly et al., 2009).
General-purpose token-level approaches such
as distributional similarity have been commonly
applied to infer the semantics of a word/MWE
(Schone and Jurafsky, 2001; Baldwin et al., 2003;
Reddy et al., 2011). These techniques are based
on the assumption that the meaning of a word is
predictable from its context of use, via the neigh-
bouring words of token-level occurrences of the
MWE. In order to predict the compositionality of
a given MWE using distributional similarity, the
different contexts of the MWE are compared with
the contexts of its components, and the MWE is
considered to be compositional if the MWE and
component words occur in similar contexts.
Identifying token instances of MWEs is not al-
ways easy, especially when the component words
do not occur sequentially. For example consider
put on in put your jacket on, and put your jacket
on the chair. In the first example put on is an
MWE while in the second example, put on is a
simple verb with prepositional phrase and not an
instance of an MWE. Moreover, if we adopt a con-
servative identification method, the number of to-
ken occurrences will be limited and the distribu-
tional scores may not be reliable. Additionally,
for morphologically-rich languages, it can be dif-
ficult to predict the different word forms a given
MWE type will occur across, posing a challenge
for our requirement of no language-specific pre-
processing.
Pichotta and DeNero (2013) proposed a token-
based method for identifying English phrasal
verbs based on parallel corpora for 50 languages.
They show that they can identify phrasal verbs bet-
ter when they combine information from multiple
languages, in addition to the information they get
from a monolingual corpus. This finding lends
weight to our hypothesis that using translation data
and distributional similarity from each of a range
of target languages, can improve compositionality
prediction. Having said that, the general applica-
bility of the method is questionable ? there are
many parallel corpora involving English, but for
other languages, this tends not to be the case.
Salehi and Cook (2013) proposed a general-
purpose type-based approach using translation
data from multiple languages, and string similar-
ity between the MWE and each of the compo-
nent words. They use training data to identify the
best-10 languages for a given family of MWEs, on
which to base the string similarity, and once again
find that translation data improves their results
substantially. Among the four string similarity
measures they experimented with, longest com-
mon substring was found to perform best. Their
proposed method is general and applicable to dif-
ferent families of MWEs in different languages. In
this paper, we reimplement the method of Salehi
and Cook (2013) using longest common substring
(LCS), and both benchmark against this method
and combine it with our distributional similarity-
based method.
3 Our Approach
To predict the compositionality of a given MWE,
we first measure the semantic similarity between
the MWE and each of its component words
2
using
distributional similarity based on a monolingual
corpus in the source language. We then repeat the
process for translations of the MWE and its com-
ponent words into each of a range of target lan-
guages, calculating distributional similarity using
2
Note that we will always assume that there are two
component words, but the method is easily generalisable to
MWEs with more than two components.
473
MWE component1 component2 
score1 score2 
Translations 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Translate 
(using Panlex) 
Translate 
(using Panlex) 
DS 
(using Wikiepdia) 
Figure 1: Outline of our approach to computing
the distributional similarity (DS) of translations
of an MWE with each of its component words,
for a given target language. score
1
and score
2
are the similarity for the first and second compo-
nents, respectively. We obtain translations from
Panlex, and use Wikipedia as our corpus for each
language.
a monolingual corpus in the target language (Fig-
ure 1). We additionally use supervised learning to
identify which target languages (or what weights
for each language) optimise the prediction of com-
positionality (Figure 2). We hypothesise that by
using multiple translations ? rather than only in-
formation from the source language ? we will be
able to better predict compositionality.
We optionally combine our proposed approach
with string similarity, calculated based on the
method of Salehi and Cook (2013), using LCS.
Below, we detail our method for calculating dis-
tributional similarity in a given language, the dif-
ferent methods for combining distributional simi-
larity scores into a single estimate of composition-
ality, and finally the method for selecting the target
languages to use in calculating compositionality.
3.1 Calculating Distributional Similarity
In order to be consistent across all languages and
be as language-independent as possible, we calcu-
CSmethod CSmethod 
Score1 for each language Score2 for each language 
21 )1( ss ?? ??
Compositionality  score 
s1 s2 
Figure 2: Outline of the method for combin-
ing distributional similarity scores from multiple
languages, across the components of the MWE.
CS
method
refers to one of the methods described
in Section 3.2 for calculating compositionality.
late distributional similarity in the following man-
ner for a given language.
Tokenisation is based on whitespace delimiters
and punctuation; no lemmatisation or case-folding
is carried out. Token instances of a given MWE
or component word are identified by full-token n-
gram matching over the token stream. We assume
that all full stops and equivalent characters for
other orthographies are sentence boundaries, and
chunk the corpora into (pseudo-)sentences on the
basis of them. For each language, we identify the
51st?1050th most frequent words, and consider
them to be content-bearing words, in the manner
of Sch?utze (1997). This is based on the assump-
tion that the top-50 most frequent words are stop
words, and not a good choice of word for calculat-
ing distributional similarity over. That is not to say
that we can?t calculate the distributional similarity
for stop words, however (as we will for the verb
particle construction dataset ? see Section 4.3.2)
they are simply not used as the dimensions in our
calculation of distributional similarity.
We form a vector of content-bearing words
across all token occurrences of the target word,
474
on the basis of these content-bearing words. Dis-
tributional similarity is calculated over these con-
text vectors using cosine similarity. Accord-
ing to Weeds (2003), using dependency rela-
tions with the neighbouring words of the target
word can better predict the meaning of the target
word. However, in line with our assumption of no
language-specific preprocessing, we just use word
co-occurrence.
3.2 Calculating Compositionality
First, we need to calculate a combined composi-
tionality score from the individual distributional
similarities between each component word and the
MWE. Following Reddy et al. (2011), we combine
the component scores using the weighted mean (as
shown in Figure 2):
comp = ?s
1
+ (1? ?)s
2
(1)
where s
1
and s
2
are the scores for the first and
the second component, respectively. We use dif-
ferent ? settings for each dataset, as detailed in
Section 4.3.
We experiment with a range of methods for cal-
culating compositionality, as follows:
CS
L1
: calculate distributional similarity using
only distributional similarity in the source
language corpus (This is the approach used
by Reddy et al. (2011), as discussed in Sec-
tion 2).
CS
L2N
: exclude the source language, and com-
pute the mean of the distributional similarity
scores for the best-N target languages. The
value of N is selected according to training
data, as detailed in Section 3.3.
CS
L1+L2N
: calculate distributional similarity
over both the source language (CS
L1
) and
the mean of the best-N languages (CS
L2N
),
and combine via the arithmetic mean.
3
This
is to examine the hypothesis that using
multiple target languages is better than just
using the source language.
CS
SVR(L1+L2 )
: train a support vector regressor
(SVR: Smola and Sch?olkopf (2004)) over the
distributional similarities for all 52 languages
(source and target languages).
3
We also experimented with taking the mean over all the
languages ? target and source ? but found it best to com-
bine the scores for the target languages first, to give more
weight to the source language.
CS
string
: calculate string similarity using the
LCS-based method of Salehi and Cook
(2013).
4
CS
string+L1
: calculate the mean of the string
similarity (CS
string
) and distributional sim-
ilarity in the source language (Salehi and
Cook, 2013).
CS
all
: calculate the mean of the string similarity
(CS
string
) and distributional similarity scores
(CS
L1
and CS
L2N
).
3.3 Selecting Target Languages
We experiment with two approaches for combin-
ing the compositionality scores from multiple tar-
get languages.
First, inCS
L2N
(andCS
L1+L2N
andCS
all
that
build off it), we use training data to rank the target
languages according to Pearson?s correlation be-
tween the predicted compositionality scores and
the gold-standard compositionality judgements.
Based on this ranking, we take the best-N lan-
guages, and combine the individual composition-
ality scores by taking the arithmetic mean. We se-
lect N by determining the value that optimises the
correlation over the training data. In other words,
the selection ofN and accordingly the best-N lan-
guages are based on nested cross-validation over
training data, independently of the test data for that
iteration of cross-validation.
Second in CS
SVR(L1+L2 )
, we combine the
compositionality scores from the source and all 51
target languages into a feature vector, and train an
SVR over the data using LIBSVM.
5
4 Resources
In this section, we describe the resources required
by our method, and also the datasets used to eval-
uate our method.
4.1 Monolingual Corpora for Different
Languages
We collected monolingual corpora for each of 52
languages (51 target languages + 1 source lan-
guage) from XML dumps of Wikipedia. These
languages are based on the 54 target languages
4
Due to differences in our random partitioning, our re-
ported results over the two English datasets differ slightly
over the results of Salehi and Cook (2013) using the same
method.
5
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm
475
used by Salehi and Cook (2013), excluding Span-
ish because we happened not to have a dump of
Spanish Wikipedia, and also Chinese and Japanese
because of the need for a language-specific word
tokeniser. The raw corpora were preprocessed us-
ing the WP2TXT toolbox
6
to eliminate XML tags,
HTML tags and hyperlinks, and then tokenisa-
tion based on whitespace and punctuation was per-
formed. The corpora vary in size from roughly
750M tokens for English, to roughly 640K tokens
for Marathi.
4.2 Multilingual Dictionary
To translate the MWEs and their components,
we follow Salehi and Cook (2013) in using Pan-
lex (Baldwin et al., 2010). This online dictio-
nary is massively multilingual, covering more than
1353 languages. For each MWE dataset (see Sec-
tion 4.3), we translate the MWE and component
words from the source language into each of the
51 languages.
In instances where there is no direct translation
in a given language for a term, we use a pivot lan-
guage to find translation(s) in the target language.
For example, the English noun compound silver
screen has direct translations in only 13 languages
in Panlex, including Vietnamese (ma`n bac) but
not French. There is, however, a translation of
ma`n bac into French (cine?ma), allowing us to
infer an indirect translation between silver screen
and cine?ma. In this way, if there are no direct
translations into a particular target language, we
search for a single-pivot translation via each of our
other target languages, and combine them all to-
gether as our set of translations for the target lan-
guage of interest.
In the case that no translation (direct or indirect)
can be found for a given source language term into
a particular target language, the compositionality
score for that target language is set to the average
across all target languages for which scores can be
calculated for the given term. If no translations are
available for any target language (e.g. the term is
not in Panlex) the compositionality score for each
target language is set to the average score for that
target language across all other source language
terms.
6
http://wp2txt.rubyforge.org/
4.3 Datasets
We evaluate our proposed method over three
datasets (two English, one German), as described
below.
4.3.1 English Noun Compounds (ENC)
Our first dataset is made up of 90 binary English
noun compounds, from the work of Reddy et al.
(2011). Each noun compound was annotated by
multiple annotators using the integer scale 0 (fully
non-compositional) to 5 (fully compositional). A
final compositionality score was then calculated
as the mean of the scores from the annotators.
If we simplistically consider 2.5 as the threshold
for compositionality, the dataset is relatively well
balanced, containing 48% compositional and 52%
non-compositional noun compounds. Following
Reddy et al. (2011), in combining the component-
wise distributional similarities for this dataset, we
weight the first component in Equation 1 higher
than the second (? = 0.7).
4.3.2 English Verb Particle Constructions
(EVPC)
The second dataset contains 160 English verb par-
ticle constructions (VPCs), from the work of Ban-
nard (2006). In this dataset, a verb particle con-
struction consists of a verb (the head) and a prepo-
sitional particle (e.g. hand in, look up or battle on).
For each component word (the verb and parti-
cle, respectively), multiple annotators were asked
whether the VPC entails the component word. In
order to translate the dataset into a regression task,
we calculate the overall compositionality as the
number of annotations of entailment for the verb,
divided by the total number of verb annotations for
that VPC. That is, following Bannard et al. (2003),
we only consider the compositionality of the verb
component in our experiments (and as such ? = 1
in Equation 1).
One area of particular interest with this dataset
will be the robustness of the method to function
words (the particles), both under translation and
in terms of calculating distributional similarity, al-
though the findings of Baldwin (2006) for English
prepositions are at least encouraging in this re-
spect. Additionally, English VPCs can occur in
?split? form (e.g. put your jacket on, from our
earlier example), which will complicate identifi-
cation, and the verb component will often be in-
flected and thus not match under our identification
strategy (for both VPCs and the component verbs).
476
Dataset Language Frequency Family
ENC
Italian 100 Romance
French 99 Romance
German 86 Germanic
Vietnamese 83 Viet-Muong
Portuguese 62 Romance
EVPC
Bulgarian 100 Slavic
Breton 100 Celtic
Occitan 100 Romance
Indonesian 100 Indonesian
Slovenian 100 Slavic
GNC
Polish 100 Slavic
Lithuanian 99 Baltic
Finnish 74 Uralic
Bulgarian 72 Slavic
Czech 40 Slavic
Table 1: The 5 best languages for the ENC, EVPC
and GNC datasets. The language family is based
on Voegelin and Voegelin (1977).
4.3.3 German Noun Compounds (GNC)
Our final dataset is made up of 246 German noun
compounds (von der Heide and Borgwaldt, 2009;
Schulte im Walde et al., 2013). Multiple anno-
tators were asked to rate the compositionality of
each German noun compound on an integer scale
of 1 (non-compositional) to 7 (compositional).
The overall compositionality score is then calcu-
lated as the mean across the annotators. Note that
the component words are provided as part of the
dataset, and that there is no need to perform de-
compounding. Following Schulte im Walde et al.
(2013), we weight the first component higher in
Equation 1 (? = 0.8) when calculating the overall
compositionality score.
This dataset is significant in being non-English,
and also in that German has relatively rich mor-
phology, which we expect to impact on the iden-
tification of both the MWE and the component
words.
5 Results
All experiments are carried out using 10 iterations
of 10-fold cross validation, randomly partitioning
the data independently on each of the 10 iterations,
and averaging across all 100 test partitions in our
presented results. In the case of CS
L2N
and other
methods that make use of it (i.e. CS
L1+L2N
and
CS
all
), the languages selected for a given training
fold are then used to compute the compositionality
scores for the instances in the test set. Figures 3a,
3b and 3c are histograms of the number of times
each N is selected over 100 folds on ENC, EVPC
and GNC datasets, respectively. From the his-
tograms, N = 6, N = 15 and N = 2 are the most
commonly selected settings for ENC, EVPC and
GNC, respectively. That is, multiple languages are
generally used, but more languages are used for
English VPCs than either of the compound noun
datasets. The 5 most-selected languages for ENC,
EVPC and GNC are shown in Table 1. As we
can see, there are some languages which are al-
ways selected for a given dataset, but equally the
commonly-selected languages vary considerably
between datasets.
Further analysis reveals that 32 (63%) target
languages for ENC, 25 (49%) target languages
for EVPC, and only 5 (10%) target languages for
GNC have a correlation of r ? 0.1 with gold-
standard compositionality judgements. On the
other hand, 8 (16%) target languages for ENC, 2
(4%) target languages for EVPC, and no target lan-
guages for GNC have a correlation of r ? ?0.1.
5.1 ENC Results
English noun compounds are relatively easy to
identify in a corpus,
7
because the components oc-
cur sequentially, and the only morphological vari-
ation is in noun number (singular vs. plural). In
other words, the precision for our token match-
ing method is very high, and the recall is also
acceptably high. Partly as a result of the ease
of identification, we get a high correlation of
r = 0.700 for CS
L1
(using only source language
data). Using only target languages (CS
L2N
), the
results drop to r = 0.434, but when we combine
the two (CS
L1+L2N
), the correlation is higher
than using only source or target language data, at
r = 0.725. When we combine all languages us-
ing SVR, the results rise slightly higher again to
r = 0.744, which is slightly above the correla-
tion of the state-of-the-art method of Salehi and
Cook (2013), which combines their method with
the method of Reddy et al. (2011) (CS
string+L1
).
These last two results support our hypothesis that
using translation data can improve the prediction
of compositionality. The results for string similar-
ity on its own (CS
string
, r = 0.644) are slightly
lower than those using only source language dis-
tributional similarity, but when combined with
7
Although see Lapata and Lascarides (2003) for discus-
sion of the difficulty of reliably identifying low-frequency
English noun compounds.
477
0 5 10 15 20 250
5
1015
2025
bestN
Frequency
(a) ENC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(b) EVPC
0 5 10 15 20 2502
468
101214
161820
best N
Frequency
(c) GNC
Figure 3: Histograms displaying how many times a given N is selected as the best number of languages
over each dataset. For example, according to the GNC chart, there is a peak for N = 2, which shows
that over 100 folds, the best-2 languages achieved the highest correlation on 18 folds.
Method Summary of the Method ENC EVPC GNC
CS
L1
Source language 0.700 0.177 0.141
CS
L2N
Best-N target languages 0.434 0.398 0.113
CS
L1+L2N
Source + best-N target languages 0.725 0.312 0.178
CS
SVR(L1+L2 )
SVR (Source + all 51 target languages) 0.744 0.389 0.085
CS
string
String Similarity (Salehi and Cook, 2013) 0.644 0.385 0.372
CS
string+L1
CS
string
+CS
L1
(Salehi and Cook, 2013) 0.739 0.360 0.353
CS
all
CS
L1
+ CS
L2N
+ CS
string
0.732 0.417 0.364
Table 2: Pearson?s correlation on the ENC, EVPC and GNC datasets
CS
L1+L2N
(i.e. CS
all
) there is a slight rise in cor-
relation (from r = 0.725 to r = 0.732).
5.2 EVPC Results
English VPCs are hard to identify. As discussed
in Section 2, VPC components may not occur se-
quentially, and even when they do occur sequen-
tially, they may not be a VPC. As such, our sim-
plistic identification method has low precision and
recall (hand analysis of 927 identified VPC in-
stances would suggest a precision of around 74%).
There is no question that this is a contributor to
the low correlation for the source language method
(CS
L1
; r = 0.177). When we use target lan-
guages instead of the source language (CS
L2N
),
the correlation jumps substantially to r = 0.398.
When we combine English and the target lan-
guages (CS
L1+L2N
), the results are actually lower
than just using the target languages, because of
the high weight on the target language, which is
not desirable for VPCs, based on the source lan-
guage results. Even for CS
SVR(L1+L2 )
, the re-
sults (r = 0.389) are slightly below the target
language-only results. This suggests that when
predicting the compositionality of MWEs which
are hard to identify in the source language, it may
actually be better to use target languages only. The
results for string similarity (CS
string
: r = 0.385)
are similar to those for CS
L2N
. However, as with
the ENC dataset, when we combine string simi-
larity and distributional similarity (CS
all
), the re-
sults improve, and we achieve the state-of-the-art
for the dataset.
In Table 3, we present classification-based eval-
478
Method Precision Recall F-score (? = 1) Accuracy
Bannard et al. (2003) 60.8 66.6 63.6 60.0
Salehi and Cook (2013) 86.2 71.8 77.4 69.3
CS
all
79.5 89.3 82.0 74.5
Table 3: Results (%) for the binary compositionality prediction task on the EVPC dataset
uation over a subset of EVPC, binarising the com-
positionality judgements in the manner of Bannard
et al. (2003). Our method achieves state-of-the-art
results in terms of overall F-score and accuracy.
5.3 GNC Results
German is a morphologically-rich language, with
marking of number and case on nouns. Given
that we do not perform any lemmatization or other
language-specific preprocessing, we inevitably
achieve low recall for the identification of noun
compound tokens, although the precision should
be nearly 100%. Partly because of the resultant
sparseness in the distributional similarity method,
the results for CS
L1
are low (r = 0.141), al-
though they are lower again when using target lan-
guages (r = 0.113). However, when we combine
the source and target languages (CS
L1+L2N
) the
results improve to r = 0.178. The results for
CS
SVR(L1+L2 )
, on the other hand, are very low
(r = 0.085). Ultimately, simple string similar-
ity achieves the best results for the dataset (r =
0.372), and this result actually drops slightly when
combined with the distributional similarities.
To better understand the reason for the lacklus-
tre results using SVR, we carried out error analysis
and found that, unlike the other two datasets, about
half of the target languages return scores which
correlate negatively with the human judgements.
When we filter these languages from the data, the
score for SVR improves appreciably. For example,
over the best-3 languages overall, we get a corre-
lation score of r = 0.179, which is slightly higher
than CS
L1+L2N
.
We further investigated the reason for getting
very low and sometimes negative correlations with
many of our target languages. We noted that
about 24% of the German noun compounds in
the dataset do not have entries in Panlex. This
contrasts with ENC where only one instance does
not have an entry in Panlex, and EVPC where all
VPCs have translations in at least one language in
Panlex. We experimented with using string sim-
ilarity scores in the case of such missing transla-
tions, as opposed to the strategy described in Sec-
tion 4.2. The results for CS
SVR(L1+L2 )
rose to
r = 0.269, although this is still below the correla-
tion for just using string similarity.
Our results on the GNC dataset using string
similarity are competitive with the state-of-the-art
results (r = 0.45) using a window-based distribu-
tional similarity approach over monolingual Ger-
man data (Schulte im Walde et al., 2013). Note,
however, that their method used part-of-speech in-
formation and lemmatisation, where ours does not,
in keeping with the language-independent philos-
ophy of this research.
6 Conclusion and Future Work
In this study, we proposed a method to predict the
compositionality of MWEs based on monolingual
distributional similarity between the MWE and
each of its component words, under translation
into multiple target languages. We showed that
using translation and multiple target languages en-
hances compositionality modelling, and also that
there is strong complementarity between our ap-
proach and an approach based on string similarity.
In future work, we hope to address the ques-
tion of translation sparseness, as observed for the
GNC dataset. We also plan to experiment with un-
supervised morphological analysis methods to im-
prove identification recall, and explore the impact
of tokenization. Furthermore, we would like to in-
vestigate the optimal number of stop words and
content-bearing words for each language, and to
look into the development of general unsupervised
methods for compositionality prediction.
Acknowledgements
We thank the anonymous reviewers for their
insightful comments and valuable suggestions.
NICTA is funded by the Australian government as
represented by Department of Broadband, Com-
munication and Digital Economy, and the Aus-
tralian Research Council through the ICT Centre
of Excellence programme.
479
References
Otavio Acosta, Aline Villavicencio, and Viviane Mor-
eira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval.
In Proceedings of the Workshop on Multiword Ex-
pressions: from Parsing and Generation to the Real
World, pages 101?109, Portland, USA.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Dam-
erau, editors, Handbook of Natural Language Pro-
cessing. CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisition and Treatment,
pages 89?96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all
words of all languages of the world. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics: Demonstrations, pages 37?
40, Beijing, China.
Timothy Baldwin. 2006. Distributional similarity and
preposition semantics. In Patrick Saint-Dizier, ed-
itor, Computational Linguistics Dimensions of Syn-
tax and Semantics of Prepositions, pages 197?210.
Springer, Dordrecht, Netherlands.
Colin Bannard, Timothy Baldwin, and Alex Las-
carides. 2003. A statistical approach to the seman-
tics of verb-particles. In Proceedings of the ACL
2003 workshop on Multiword expressions: analysis,
acquisition and treatment-Volume 18, pages 65?72,
Sapporo, Japan.
Colin James Bannard. 2006. Acquiring Phrasal Lexi-
cons from Corpora. Ph.D. thesis, University of Ed-
inburgh.
Marine Carpuat and Mona Diab. 2010. Task-based
evaluation of multiword expressions: a pilot study
in statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 242?245, Los
Angeles, USA.
Ga?el Dias. 2003. Multiword unit hybrid extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 41?48, Sapporo, Japan.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):450?466.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification of
idiomatic expressions. Computational Linguistics,
35(1):61?103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of English verb-particle construc-
tions using semantic similarity. In Proceedings of
the 7th Meeting of the Pacific Association for Com-
putational Linguistics (PACLING 2007), pages 40?
48, Melbourne, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of the 11th Conference of
the European Chapter for the Association of Compu-
tational Linguistics (EACL-2003), pages 235?242,
Budapest, Hungary.
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 317?324, College Park, USA.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis, ac-
quisition and treatment-Volume 18, pages 73?80,
Sapporo, Japan.
Pavel Pecina. 2008. Lexical Association Measures:
Collocation Extraction. Ph.D. thesis, Faculty of
Mathematics and Physics, Charles University in
Prague, Prague, Czech Republic.
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2013), Seattle, USA.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66, Jeju Island, Ko-
rea.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In Proceedings of IJCNLP, pages
210?218, Chiang Mai, Thailand.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on
Intelligent Text Processing Computational Linguis-
tics (CICLing-2002), pages 189?206, Mexico City,
Mexico.
Bahar Salehi and Paul Cook. 2013. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings
of the Second Joint Conference on Lexical and Com-
putational Semantics, volume 1, pages 266?275, At-
lanta, USA.
480
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly.
2012. Automatic identification of Persian light verb
constructions. In Proceedings of the 13th Inter-
national Conference on Intelligent Text Processing
Computational Linguistics (CICLing-2012), pages
201?210, New Delhi, India.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem. In Proceedings of the 6th
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2001), pages 100?108,
Hong Kong, China.
Sabine Schulte im Walde, Stefan M?uller, and Stephen
Roller. 2013. Exploring vector space models to
predict the compositionality of German noun-noun
compounds. In Proceedings of the Second Joint
Conference on Lexical and Computational Seman-
tics, Atlanta, USA.
Hinrich Sch?utze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI Publications, Stanford, USA.
Alex J Smola and Bernhard Sch?olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199?222.
Charles Frederick Voegelin and Florence Marie
Voegelin. 1977. Classification and index of the
world?s languages, volume 4. New York: Elsevier.
Claudia von der Heide and Susanne Borgwaldt. 2009.
Assoziationen zu Unter, Basis und Oberbegriffen.
Eine explorative Studie. In Proceedings of the 9th
Norddeutsches Linguistisches Kolloquium, pages
51?74.
Julie Elizabeth Weeds. 2003. Measures and applica-
tions of lexical distributional similarity. Ph.D. the-
sis, University of Sussex.
481
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 207?215, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UniMelb NLP-CORE: Integrating predictions from multiple domains and
feature sets for estimating semantic textual similarity
Spandana Gella,? Bahar Salehi,?? Marco Lui,??
Karl Grieser,? Paul Cook,? and Timothy Baldwin,??
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems, The University of Melbourne
sgella@student.unimelb.edu.au, bsalehi@student.unimelb.edu.au
mhlui@unimelb.edu.au, kgrieser@student.unimelb.edu.au
paulcook@unimelb.edu.au, tb@ldwin.net
Abstract
In this paper we present our systems for cal-
culating the degree of semantic similarity be-
tween two texts that we submitted to the Se-
mantic Textual Similarity task at SemEval-
2013. Our systems predict similarity using
a regression over features based on the fol-
lowing sources of information: string similar-
ity, topic distributions of the texts based on
latent Dirichlet alocation, and similarity be-
tween the documents returned by an informa-
tion retrieval engine when the target texts are
used as queries. We also explore methods for
integrating predictions using different training
datasets and feature sets. Our best system was
ranked 17th out of 89 participating systems.
In our post-task analysis, we identify simple
changes to our system that further improve our
results.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic similarity or equivalence between
a pair of short texts. STS is related to many natural
language processing applications such as text sum-
marisation (Aliguliyev, 2009), machine translation,
word sense disambiguation, and question answering
(De Boni and Manandhar, 2003; Jeon et al, 2005).
Two short texts are considered similar if they both
convey similar messages. Often it is the case that
similar texts will have a high degree of lexical over-
lap, although this isn?t always so. For example,
SC dismissed government?s review plea in Vodafone
tax case and SC dismisses govt?s review petition on
Vodafone tax verdict are semantically similar. These
texts have matches in terms of exact words (SC,
Vodafone, tax), morphologically-related words (dis-
missed and dismisses), and abbreviations (govern-
ment?s and govt?s). However, the usages (senses) of
plea and petition, and case and verdict are also sim-
ilar.
One straightforward way of estimating semantic
similarity of two texts is by using approaches based
on the similarity of the surface forms of the words
they contain. However, such methods are not capa-
ble of capturing similarity or relatedness at the lexi-
cal level, and moreover, they do not exploit the con-
text in which individual words are used in a target
text. Nevertheless, a variety of knowledge sources
? including part-of-speech, collocations, syntax,
and domain ? can be used to identify the usage or
sense of words in context (McRoy, 1992; Agirre and
Martinez, 2001; Agirre and Stevenson, 2006) to ad-
dress these issues.
Despite their limitations, string similarity mea-
sures have been widely used in previous seman-
tic similarity tasks (Agirre et al, 2012; Islam and
Inkpen, 2008). Latent variable models have also
been used to estimate the semantic similarity be-
tween words, word usages, and texts (Steyvers and
Griffiths, 2007; Lui et al, 2012; Guo and Diab,
2012; Dinu and Lapata, 2010).
In this paper, we consider three different ways of
measuring semantic similarity based on word and
word usage similarity:
1. String-based similarity to measure surface-
level lexical similarity, taking into account
morphology and abbreviations (e.g., dismisses
and dismissed, and government?s and govt?s);
207
2. Latent variable models of similarity to cap-
ture words that have different surface forms,
but that have similar meanings or that can be
used in similar contexts (e.g., petition and plea,
verdict and case); and
3. Topical/domain similarity of the texts with re-
spect to the similarity of documents in an ex-
ternal corpus (based on information-retrieval
methods) that are relevant to the target texts.
We develop features based on all three of these
knowledge sources to capture semantic similarity
from a variety of perspectives. We build a regres-
sion model, trained on STS training data which has
semantic similarity scores for pairs of texts, to learn
weights for the features and rate the similarity of test
instances. Our approach to the task is to explore the
utility of novel features or features that have not per-
formed well in previous research, rather than com-
bine these features with the myriad of features that
have been proposed by others for the task.
2 Text Similarity Measures
In this section we describe the various features used
in our system.
2.1 String Similarity Measures (SS)
Our first set of features contains various string simi-
larity measures (SS), which compare the target texts
in terms of the words they contain and the order
of the words (Islam and Inkpen, 2008). In the Se-
mEval 2012 STS task (Agirre et al, 2012) such
features were used by several participants (Biggins
et al, 2012; Ba?r et al, 2012; Heilman and Mad-
nani, 2012), including the first-ranked team (Ba?r et
al., 2012) who considered string similarity measures
alongside a wide range of other features.
For our string similarity features, the texts were
lemmatized using the implementation of Lancaster
Stemming in NLTK 2.0 (Bird, 2006), and all punc-
tuation was removed. Limited stopword removal
was carried out by eliminating the words a, and, and
the. The output of each string similarity measure
is normalized to the range of [0, 1], where 0 indi-
cates that the texts are completely different, while 1
means they are identical. The normalization method
for each feature is described in Salehi and Cook (to
appear), wherein the authors applied string similar-
ity measures successfully to the task of predicting
the compositionality of multiword expressions.
Identical Unigrams (IU): This feature measures
the number of words shared between the two texts,
irrespective of word order.
Longest Common Substring (LCS): This mea-
sures the longest sequence of words shared between
the two texts. For example, the longest common
substring between the following sentences is bolded:
A woman and man are dancing in the
rain.
A couple are dancing in the street.
Levenshtein (LEV1): Levenshtein distance (also
known as edit distance) calculates the number of
basic word-level edit operations (insertion, deletion
and substitution) to transform one text into the other:
Levenshtein with substitution penalty (LEV2):
This feature is a variant of LEV1 in which substi-
tution is considered as two edit operations: an inser-
tion and a deletion (Baldwin, 2009).
Smith Waterman (SW): This method is designed
to locally align two sequences of amino acids (Smith
and Waterman, 1981). The algorithm looks for
the longest similar regions by maximizing the num-
ber of matches and minimizing the number of in-
sertion/deletion/substitution operations necessary to
align the two sequences. In other words, it finds the
longest common sequence while tolerating a small
number of differences. We call this sequence, the
?aligned sequence?. It has length equal to or greater
than the longest common sequence.
Not Aligned Words (NAW): As mentioned
above, SW looks for similar regions in the given
texts. Our last string similarity feature shows the
number of identical words not aligned by the SW al-
gorithm. We used this feature to examine how simi-
lar the unaligned words are.
These six features (IU, LCS, LEV1, LEV2, SW,
and NAW) form our string similarity (SS) features.
LEV2, SW, and NAW have not been previously con-
sidered for STS.
208
2.2 Topic Modelling Similarity Measures (TM)
The topic modelling features (TM) are based on La-
tent Dirichlet Allocation (LDA), a generative prob-
abilistic model in which each document is mod-
eled as a distribution over a finite set of topics, and
each topic is represented as a distribution over words
(Blei et al, 2003). We build a topic model on a back-
ground corpus, and then for each target text we cre-
ate a topic vector based on the topic allocations of
its content words, based on the method developed
by Lui et al (2012) for predicting word usage simi-
larity.
The choice of the number of topics, T , can
have a big impact on the performance of this
method. Choosing a small T might give overly-
broad topics, while a large T might lead to un-
interpretable topics (Steyvers and Griffiths, 2007).
Moreover smaller numbers of topics have been
shown to perform poorly on both sentence simi-
larity (Guo and Diab, 2012) and word usage sim-
ilarity tasks (Lui et al, 2012). We therefore build
topic models for 33 values of T in the range
2, 3, 5, 8, 10, 50, 80, 100, 150, 200, ...1350.
The background corpus used for generating the
topic models is similar to the COL-WTMF sys-
tem (Guo and Diab, 2012) from the STS-2012 task,
which outperformed LDA. In particular, we use
sense definitions from WordNet, Wiktionary and all
sentences from the Brown corpus. Similarity be-
tween two texts is measured on the basis of the simi-
larity between their topic distributions. We consider
three vector-based similarity measures here: Cosine
similarity, Jensen-Shannon divergence and KL di-
vergence. Thus for each target text pair we extract
99 features corresponding to the 3 similarity mea-
sures for each of the 33 T settings. These features
are used as the TM feature set in the systems de-
scribed below.
2.3 IR Similarity Measures (IR)
The information retrieval?based features (IR) were
based on a dump of English Wikipedia from Novem-
ber 2009. The entire dump was stripped of markup
and tokenised using the OpenNLP tokeniser. The
tokenised documents were then parsed into TREC
format, with each article forming an individual doc-
ument. These documents were indexed using the
Indri IR engine1 with stopword removal. Each
of the two target texts was issued as a full text
query (without any phrases) to Indri, and the first
1000 documents for each text were returned, based
on Okapi term weighting (Robertson and Walker,
1994). These resultant document lists were then
converted into features using a number of set- and
rank-based measures: Dice?s coefficient, Jaccard in-
dex, average overlap, and rank-biased overlap (the
latter two are described in Webber et al (2010)).
The first two are based on simple set overlap and
ignore the ranks; average overlap takes into account
the rank, but equally weights high- and low-ranking
documents; and rank-biased overlap weights higher-
ranked items higher.
In addition to comparisons of the document rank-
ings for a given target text pair, we also consid-
ered a method that compared the top-ranking doc-
uments themselves. To compare two texts, we ob-
tain the top-100 documents using each text as a
query as above. We then calculate the similarity be-
tween these two sets of resultant documents using
the ?2-based corpus similarity measure of Kilgarriff
(2001). In this method the ?2 statistic is calculated
for the 500 most frequent words in the union of the
two sets of documents (corpora), and is interpreted
as the similarity between the sets of documents.
These 5 IR features (4 rank-based, and 1
document-based) are novel in the context of STS,
and are used in the compound systems described be-
low.
3 Compound systems
3.1 Ridge regression
Each of our features represents a (potentially noisy)
measurement of the semantic textual similarity be-
tween two texts. However, the scale of our fea-
tures varies, e.g., [0, 1] for the string similarity fea-
tures vs. unbounded for KL divergence (one of the
topic modelling features). To learn the mapping be-
tween these features and the graded [0, 5] scale of
the shared task, we made use of a statistical tech-
nique known as ridge regression, as implemented in
scikit-learn.2 Ridge regression is a form of
linear regression where the loss function is the ordi-
1http://www.lemurproject.org/indri/
2http://scikit-learn.org
209
nary least squares, but with an additional L2 regular-
ization term. In our empirical evaluation, we found
that ridge regression outperformed linear regression
on our feature set. For brevity, we only present re-
sults from ridge regression.
3.2 Domain Adaptation
Domain adaptation (Daume? and Marcu, 2006) is the
general term applied to techniques for using labelled
data from a related distribution to label data from a
target distribution. For the 2013 Shared Task, no
training data was provided for the target datasets,
making domain adaptation an important considera-
tion. In this work, we assume that each dataset rep-
resents a different domain, and on this basis develop
approaches that are sensitive to inter-domain differ-
ences.
We tested two simple approaches to including do-
main information in our trained model. The first ap-
proach, which we will refer to as flagging, simply in-
volves appending a boolean vector to each training
instance to indicate which training dataset it came
from. The vector has length D, equal to the number
of training datasets (3 for this task, because we train
on the STS 2012 training data). All the values of the
vector are 0, except for a single 1 according to the
dataset that the training instance is drawn from. For
test data, the entire vector consists of 0s.
The second approach we considered is based on
metalearning, and we will refer to it as domain
stacking. In domain stacking, we train a regressor
for each domain (the level 0 regressors (Wolpert,
1992)). Each of these regressors is then applied
to a test instance to produce a predicted value (the
level 0 prediction). These predictions are then com-
bined using a second regressor (the level 1 regres-
sor), to produce a final prediction for each instance
(the level 1 prediction). This approach is closely
related to feature stacking (Lui, 2012) and stacked
generalization (Wolpert, 1992). A general princi-
ple of metalearning is to combine multiple weaker
(?less accurate?) predictors ? termed level 0 pre-
dictors ? to produce a stronger (?more accurate?)
predictor ? the level 1 predictor. In stacked gener-
alization, the level 0 predictors are different learning
algorithms. In feature stacking, they are the same
algorithm trained on different subsets of features, in
this work corresponding to different methods for es-
timating STS (Section 2). In domain stacking, the
level 0 predictions are obtained from subsets of the
training data, where each subset corresponds to all
the instances from a single dataset (e.g. MSRpar or
SMTeuroparl). In terms of subsampling the training
data, this technique is related to bagging (Breiman,
1996). However, rather than generating new train-
ing sets by uniform sampling across the whole pool
of training data, we treat each domain in the train-
ing dataset as a unique sample. Finally, we also ex-
periment with feature-domain stacking, in which the
level 0 predictions are obtained from the cross prod-
uct of subsets of the training data (as per domain
stacking) and subsets of the feature set (as per fea-
ture stacking). We report results for all 3 variants in
Section 5.
This framework of feature-domain stacking can
be applied with any regression or classification al-
gorithm (indeed, the level 0 and level 1 predictors
could be trained using different algorithms). In this
work, all our regressors are trained using ridge re-
gression (Section 3.1).
4 Submitted Runs
In this section we describe the three official runs we
submitted to the shared task.
4.1 Run1 ? Bahar
For this run we used just the SS feature set, aug-
mented with flagging for domain adaptation. Ridge
regression was used to train a regressor across the
three training datasets (MSRvid, MSRpar, SMTeu-
roparl). Each instance was then labelled using the
output of the regressor, and the output range was lin-
early re-scaled to [0, 5] as it occasionally produced
values outside of this range. Although this approach
approximates STS using only lexical textual similar-
ity, it was our best-performing system on the training
data (Table 1). Furthermore the SS features are ap-
pealing because of their simplicity and because they
do not make use of any external resources.
4.2 Run2 ? Concat
In this run, we concatenated the feature vectors
from all three of our feature sets (SS, TM and
IR), and again trained a regressor on the union of
the MSRvid, MSRpar and SMTeuroparl training
datasets. As in Run1, the output of the regression
210
FSet FL FS DS MSRpar MSRvid SMTeuroparl Ave
SS 0.522 0.537 0.526 0.528
(*) SS X 0.552 0.533 0.562 0.549
TM 0.270 0.479 0.425 0.391
TM X 0.250 0.580 0.427 0.419
IR 0.264 0.759 0.407 0.477
IR X 0.291 0.754 0.400 0.482
(+) ALL 0.401 0.543 0.513 0.485
ALL X 0.377 0.595 0.516 0.496
ALL X 0.385 0.587 0.520 0.497
ALL X 0.452 0.637 0.472 0.521
ALL X X 0.429 0.619 0.526 0.524
ALL X X 0.429 0.627 0.526 0.527
(?) ALL X X X 0.441 0.645 0.527 0.538
Table 1: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each training dataset, and the
micro-average over all training datasets. (*), (+),
and (?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
was also linearly re-scaled to the [0, 5] range. Un-
like the previous run, the flagging approach to do-
main adaptation was not used. This approach re-
flects a simple application of machine learning to in-
tegrating data from multiple feature sets and training
datasets, and provides a useful point of comparison
against more sophisticated approaches (i.e., Run3).
4.3 Run3 ? Stacking
In this run, we focused on an alternative method
to integrating information from multiple feature sets
and training datasets, namely feature-domain stack-
ing, as discussed in Section 3.2. In this approach, we
train nine regressors using ridge regression on each
combination of the three training datasets and three
feature sets. Thus, the level 1 representation for each
instance is a vector of nine predictions. For the train-
ing data, when computing the level 1 features for the
same training dataset from which a given instance is
drawn, 10-fold cross-validation is used. Ridge re-
gression is again used to combine the level 1 repre-
sentations and produce the final prediction for each
instance. In addition to this, we also simultaneously
apply the flagging approach to domain adaptation.
This approach incorporates all of our domain adap-
tation efforts, and in initial experiments on the train-
ing data (Table 1) it was our second-best system.
FSet FL FS DS OnWN FNWN Headlines SMT Ave
SS 0.340 0.366 0.688 0.325 0.453
(*) SS X 0.349 0.381 0.711 0.350 0.473
TM 0.648 0.358 0.516 0.209 0.433
TM X 0.701 0.368 0.614 0.287 0.506
IR 0.561 -0.006 0.610 0.228 0.419
IR X 0.596 0.002 0.621 0.256 0.441
(+) ALL 0.679 0.337 0.709 0.323 0.542
ALL X 0.704 0.365 0.718 0.344 0.560
ALL X 0.673 0.298 0.714 0.324 0.539
ALL X 0.618 0.264 0.717 0.357 0.534
ALL X X 0.658 0.309 0.721 0.330 0.540
ALL X X 0.557 0.142 0.694 0.280 0.475
(?) ALL X X X 0.614 0.186 0.706 0314 0.509
Table 2: Pearson?s ? for each feature set (FSet),
as well as combinations of feature sets and adap-
tation strategies, on each test dataset, and the
micro-average over all test datasets. (*), (+), and
(?) denote Run1, Run2, and Run3, respectively,
our submissions to the shared task; FL=Flagging,
FS=Feature stacking, DS=Domain stacking.
5 Results
For the STS 2013 task, the organisers advised par-
ticipants to make use of the STS 2012 data; we took
this to mean only the training data. In our post-task
analysis, we realised that the entire 2012 dataset, in-
cluding the testing data, could be used. All our of-
ficial runs were trained only on the training data for
the 2012 task (made up of MSRpar, MSRvid and
SMTeuroparl). We first discuss preliminary find-
ings training and testing on the (STS 2012) training
data, and then present results for the (2013) test data.
Post-submission, we re-trained our systems includ-
ing the 2012 test data.
5.1 Experiments on Training Data
We evaluated our models based on a leave-one-out
cross-validation across the 3 training datasets. Thus,
for each of the training datasets, we trained a sep-
arate model using features from the other two. We
considered approaches based on each individual fea-
ture set, with and without flagging. We further con-
sidered combinations of feature sets using feature
concatenation, as well as feature and domain stack-
ing, again with and without flagging.3 Results are
3We did not consider domain stacking with flagging.
211
FSet FL FS DS OnWN (?) FNWN (?) Headlines (?) SMT (?) Ave (?)
SS 0.3566 (+.0157) 0.3741 (+.0071) 0.6994 (+.0111) 0.3386 (+.0131) 0.4663 (+.0133)
(*) SS X 0.3532 (+.0042) 0.3809 (?.0004) 0.7122 (+.0003) 0.3417 (?.0090) 0.4714 (?.0016)
TM 0.6748 (+.0265) 0.3939 (+.0349) 0.5930 (+.0770) 0.2563 (+.0472) 0.4844 (+.0514)
TM X 0.6269 (?.0743) 0.3519 (?.0162) 0.5999 (?.0142) 0.2653 (?.0223) 0.4743 (?.0317)
IR 0.6632 (+.1015) 0.1026 (+.1093) 0.6383 (?.0281) 0.2987 (+.0701) 0.4863 (+.0673)
IR X 0.6720 (+.0755) 0.0861 (+.0841) 0.6316 (+.0097) 0.2811 (+.0244) 0.4790 (+.0680)
(+) ALL 0.6976 (+.0006) 0.4350 (+.0976) 0.7071 (?.0014) 0.3329 (+.0099) 0.5571 (+.0151)
ALL X 0.6667 (?.0373) 0.4138 (+.0490) 0.7210 (+.0029) 0.3335 (?.0105) 0.5524 (?.0076)
ALL X 0.6889 (+.0149) 0.4620 (+.1636) 0.7309 (+.0167) 0.3538 (+.0295) 0.5721 (+.0331)
ALL X 0.6765 (?.0185) 0.4675 (+.1578) 0.7337 (+.0126) 0.3552 (+.0252) 0.5709 (+.0369)
ALL X X 0.6369 (+.0208) 0.3615 (+.0970) 0.7233 (+.0060) 0.3736 (+.0157) 0.5554 (+.0154)
ALL X X 0.6736 (+.1165) 0.4250 (+.2821) 0.7237 (+0.0297) 0.3404 (+0.0603) 0.5583(+.0833)
(?) ALL X X X 0.6772 (+.0632) 0.3992 (+.2127) 0.7315 (+.0251) 0.3300 (+0.0186) 0.5572 (+.0482)
Table 3: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adaptation
strategies, on each test dataset, and the micro-average over all test datasets, using features from all 2012
data (test + train). (*), (+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the
shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system
performance after adding the additional training data.
reported in Table 1.
The best results on the training data were achieved
using only our SS feature set with flagging (Run1),
with an average Pearson?s ? of 0.549. This fea-
ture set alo gave the best performance on MSR-
par and SMTeuroparl, although the IR feature set
was substantially better on MSRvid. On the training
datasets, our approaches that combine feature sets
did not give an improvement over the best individ-
ual feature set on any dataset, or overall.
5.2 Test Set Results
STS 2013 included four different test sets. Table 2
presents the Pearson?s ? for the same methods as
Section 5.1 ? including our submitted runs ? on
the test data. Run1 drops in performance on the test
set as compared to the training set, where the other
two runs are more consistent, suggesting that lexi-
cal similarity does not generalise well cross-domain.
Table 4 shows that all of our systems performed
above the baseline on each dataset, except Run3 on
FNWN. Table 4 also shows that Run2 consistently
performed well on all the datasets when compared
to the median of all the systems submitted to the task
(Agirre et al, to appear).
Run2, which was based on the concatenation of
all the feature sets, performed well compared to the
stacking-based approaches on the test set, whereas
the stacking approaches all outperformed Run2 on
the training datasets. This is likely due to the
SS features being more effective for STS predic-
tion in the training datasets as compared to the test
datasets. Based on the training datasets, the stack-
ing approaches placed greater weight on the pre-
dictions from the SS feature set. This hypothe-
sis is supported by the result on Headlines, where
the SS feature set does relatively well, and thus the
stacking approaches tend to outperform the simple
concatenation-based method. Finally, an extension
of Run2 with flagging (not submitted to the shared
task) was the best of our methods on the test data.
5.3 Error Analysis
To better understand the behaviour of our systems,
we examined test instances and made the following
observations. Systems based entirely on the TM fea-
tures and domain adaptation consistently performed
well on sentence pairs for which all of our other sys-
tems performed poorly. One example is the follow-
ing OnWN pair, which corresponds to definitions of
newspaper: an enterprise or company that publishes
newsprint and a business firm that publishes news-
papers. Because these texts do not share many com-
mon words, the SS features cannot capture their se-
mantic similarity.
Stacking based approaches performed well on text
pairs which are complex to comprehend, e.g., Two
German tourists, two pilots killed in Kenya air crash
and Senator Reid involved in Las Vegas car crash,
where the individual methods tend to score lower
212
System Headlines OnWN FNWN SMT Ave
(+) Run1 .711 (15) .349 (71) .381 (23) .351 (18) .473 (49)
(+) Run2 .709 (17) .679 (18) .337 (33) .323 (43) .542 (17)
(+) Run3 .706 (18) .614 (28) .187 (71) .314 (47) .509 (29)
Best .718 (14) .704 (15) .365 (28) .344 (24) .560 (7)
(?) Run1 .712 (14) .353 (70) .381 (23) .341 (25) .471 (54)
(?) Run2 .707 (18) .697 (14) .435 (9) .332 (35) .557 (9)
(?) Run3 .731 (11) .677 (19) .399 (17) .330 (38) .557 (8)
(?) Best .730 (11) .688 (17) .462 (7) .353 (18) .572 (4)
Baseline .540 (67) .283 (81) .215 (67) .286 (65) .364 (73)
Median .640 (45) .528 (45) .327 (45) .318 (45) .480 (45)
Best-Score .783 (1) .843 (1) .581 (1) .403 (1) .618 (1)
Table 4: Pearson?s ? (and projected ranking) of runs.
The upper 4 runs are trained only on STS 2012 train-
ing data. (+) denotes runs that were submitted for
evaluation. (?) denotes systems trained on STS 2012
training and test data. For comparison, we include
?Best?, the highest-scoring parametrization of our
system from our post-task analysis (Table 3). We
also include the organiser?s baseline, as well as the
median and best systems for each dataset across all
competitors.
than the human rating, but stacking was able to pre-
dict a higher score (presumably based on the fact
that no method predicted the text pair to be strongly
dissimilar; rather, all methods predicted there to be
somewhat low similarity).
In some cases, the texts are on a similar topic,
but semantically different, e.g., Nigeria mourns over
193 people killed in plane crash and Nigeria opens
probe into deadly air crash. In such cases, systems
based on SS features and stacking perform well.
Systems based on TM and IR features, on the other
hand, tend to predict overly-high scores because the
texts relate to similar topics and tend to have similar
relevant documents in an external corpus.
5.4 Results with the Full Training dataset
We re-trained all the above systems by extending the
training data to include the 2012 test data. Scores on
the 2013 test datasets and the change in Pearson?s ?
after adding the extra training data (denoted ?) are
presented in Table 3.
In general, the addition of the 2012 test data to
the training dataset improves the performance of the
system, though this is often not the case for the flag-
ging approach to domain adaptation, which in some
instances drops in performance after adding the ad-
ditional training data. The biggest improvements
were seen for feature-domain stacking, particularly
on FNWN. This suggests that feature-domain stack-
ing is more sensitive to the similarity between train-
ing data and test data than flagging, but also that it
is better able to cope with variety in training do-
mains than flagging. Given that the pool of anno-
tated data for the STS task continues to increase,
feature-domain stacking is a promising approach to
exploiting the differences between domains to im-
prove overall STS performance.
To facilitate comparison with the published re-
sults for the 2013 STS task, we present a condensed
summary of our results in Table 4, which shows the
absolute score as well as the projected ranking of
each of our systems. It also includes the median and
baseline results for comparison.
6 Conclusions and Future Work
In this paper we described our approach to the
STS SemEval-2013 shared task. While we did not
achieve high scores relative to the other submit-
ted systems on any of the datasets or overall, we
have identified some novel feature sets which we
show to have utility for the STS task. We have
also compared our proposed method?s performance
with a larger training dataset. In future work, we
intend to consider alternative ways for combining
features learned from different domains and training
datasets. Given the strong performance of our string
similarity features on particular datasets, we also in-
tend to consider combining string and distributional
similarity to capture elements of the texts that are not
currently captured by our string similarity features.
Acknowledgments
This work was supported by the European Erasmus
Mundus Masters Program in Language and Commu-
nication Technologies from the European Commis-
sion.
NICTA is funded by the Australian government
as represented by Department of Broadband, Com-
munication and Digital Economy, and the Australian
Research Council through the ICT Centre of Excel-
lence program.
213
References
Eneko Agirre and David Martinez. 2001. Knowl-
edge sources for word sense disambiguation. In Text,
Speech and Dialogue, pages 1?10. Springer.
Eneko Agirre and Mark Stevenson. 2006. Knowledge
sources for wsd. In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation, volume 33 of
Text, Speech and Language Technology, pages 217?
251. Springer Netherlands.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics ? Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 385?393,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. to appear. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Atlana, USA. Association for Computational Linguis-
tics.
Ramiz M Aliguliyev. 2009. A new sentence similarity
measure and sentence based extractive technique for
automatic text summarization. Expert Systems with
Applications, 36(4):7764?7772.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual simi-
larity by combining multiple content similarity mea-
sures. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Volume 1:
Proceedings of the main conference and the shared
task, and Volume 2: Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 435?440, Montre?al, Canada, 7-8 June.
Association for Computational Linguistics.
Sam Biggins, Shaabi Mohammed, Sam Oakley, Luke
Stringer, Mark Stevenson, and Judita Preiss. 2012.
University of sheffield: Two approaches to semantic
text similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 655?661, Montre?al, Canada, 7-8
June. Association for Computational Linguistics.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 69?72, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Leo Breiman. 1996. Bagging predictors. Machine learn-
ing, 24(2):123?140.
Hal Daume?, III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126, May.
Marco De Boni and Suresh Manandhar. 2003. The use
of sentence similarity as a semantic relevance metric
for question answering. In Proceedings of the AAAI
Symposium on New Directions in Question Answering,
Stanford, USA.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1162?1172, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Weiwei Guo and Mona Diab. 2012. Weiwei: A sim-
ple unsupervised latent semantics based approach for
sentence similarity. In *SEM 2012: The First Joint
Conference on Lexical and Computational Semantics
? Volume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evaluation
(SemEval 2012), pages 586?590, Montre?al, Canada,
7-8 June. Association for Computational Linguistics.
Michael Heilman and Nitin Madnani. 2012. Ets: Dis-
criminative edit models for paraphrase scoring. In
*SEM 2012: The First Joint Conference on Lexi-
cal and Computational Semantics ? Volume 1: Pro-
ceedings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth International
Workshop on Semantic Evaluation (SemEval 2012),
pages 529?535, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data (TKDD), 2(2):10.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, CIKM ?05, pages 84?90, New York, NY,
USA. ACM.
Adam Kilgarriff. 2001. Comparing corpora. Interna-
tional Journal of Corpus Linguistics, 6(1):97?133.
214
Marco Lui, Timothy Baldwin, and Diana McCarthy.
2012. Unsupervised estimation of word usage simi-
larity. In Proceedings of the Australasian Language
Technology Association Workshop 2012, pages 33?41,
Dunedin, New Zealand, December.
Marco Lui. 2012. Feature stacking for sentence clas-
sification in evidence-based medicine. In Proceed-
ings of the Australasian Language Technology Associ-
ation Workshop 2012, pages 134?138, Dunedin, New
Zealand, December.
Susan W McRoy. 1992. Using multiple knowledge
sources for word sense discrimination. Computational
Linguistics, 18(1):1?30.
Stephen E Robertson and Steve Walker. 1994. Some
simple effective approximations to the 2-poisson
model for probabilistic weighted retrieval. In Proceed-
ings of the 17th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?94, pages 232?241, Dublin, Ireland.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In *SEM 2013:
The Second Joint Conference on Lexical and Com-
putational Semantics, Atlana, USA. Association for
Computational Linguistics.
TF Smith and MS Waterman. 1981. Identification of
common molecular subsequences. Molecular Biology,
147:195?197.
Mark Steyvers and Tom Griffiths. 2007. Probabilistic
topic models. Handbook of latent semantic analysis,
427(7):424?440.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241?259.
215
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 266?275, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Predicting the Compositionality of Multiword Expressions
Using Translations in Multiple Languages
Bahar Salehi?? and Paul Cook?
? NICTA Victoria Research Laboratory
? Department of Computing and Information Systems
The University of Melbourne
Victoria 3010, Australia
bsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au
Abstract
In this paper, we propose a simple, language-
independent and highly effective method for
predicting the degree of compositionality of
multiword expressions (MWEs). We compare
the translations of an MWE with the trans-
lations of its components, using a range of
different languages and string similarity mea-
sures. We demonstrate the effectiveness of
the method on two types of English MWEs:
noun compounds and verb particle construc-
tions. The results show that our approach is
competitive with or superior to state-of-the-art
methods over standard datasets.
1 Compositionality of MWEs
A multiword expression (MWE) is any combina-
tion of words with lexical, syntactic or semantic
idiosyncrasy (Sag et al, 2002; Baldwin and Kim,
2009), in that the properties of the MWE are not
predictable from the component words. For exam-
ple, with ad hoc, the fact that neither ad nor hoc are
standalone English words, makes ad hoc a lexically-
idiosyncratic MWE; with shoot the breeze, on the
other hand, we have semantic idiosyncrasy, as the
meaning of ?to chat? in usages such as It was good
to shoot the breeze with you1 cannot be predicted
from the meanings of the component words shoot
and breeze.
Semantic idiosyncrasy has been of particular in-
terest to NLP researchers, with research on bi-
nary compositional/non-compositional MWE clas-
1The example is taken from http://www.
thefreedictionary.com
sification (Lin, 1999; Baldwin et al, 2003), or
a three-way compositional/semi-compositional/non-
compositional distinction (Fazly and Stevenson,
2007). There has also been research to suggest that
MWEs span the entire continuum from full compo-
sitionality to full non-compositionality (McCarthy et
al., 2003; Reddy et al, 2011).
Investigating the degree of MWE compositional-
ity has been shown to have applications in informa-
tion retrieval and machine translation (Acosta et al,
2011; Venkatapathy and Joshi, 2006). As an exam-
ple of an information retrieval system, if we were
looking for documents relating to rat race (mean-
ing ?an exhausting routine that leaves no time for
relaxation?2), we would not be interested in docu-
ments on rodents. These results underline the need
for methods for broad-coverage MWE composition-
ality prediction.
In this research, we investigate the possibility of
using an MWE?s translations in multiple languages
to measure the degree of the MWE?s compositional-
ity, and investigate how literal the semantics of each
component is within the MWE. We use Panlex to
translate the MWE and its components, and compare
the translations of the MWE with the translations
of its components using string similarity measures.
The greater the string similarity, the more composi-
tional the MWE is.
Whereas past research on MWE compositionality
has tended to be tailored to a specific MWE type
(McCarthy et al, 2007; Kim and Baldwin, 2007;
Fazly et al, 2009), our method is applicable to
any MWE type in any language. Our experiments
2This definition is from WordNet 3.1.
266
over two English MWE types demonstrate that our
method is competitive with state-of-the-art methods
over standard datasets.
2 Related Work
Most previous work on measuring MWE composi-
tionality makes use of lexical, syntactic or semantic
properties of the MWE. One early study on MWE
compositionality was Lin (1999), who claimed that
the distribution of non-compositional MWEs (e.g.
shoot the breeze) differs significantly from the dis-
tribution of expressions formed by substituting one
of the components with a semantically similar word
(e.g. shoot the wind). Unfortunately, the method
tends to fall down in cases of high statistical id-
iosyncrasy (or ?institutionalization?): consider fry-
ing pan which is compositional but distributionally
very different to phrases produced through word-
substitution such as sauteing pan or frying plate.
Some research has investigated the syntactic
properties of MWEs, to detect their composition-
ality (Fazly et al, 2009; McCarthy et al, 2007).
The assumption behind these methods is that non-
compositional MWEs are more syntactically fixed
than compositional MWEs. For example, make a de-
cision can be passivised, but shoot the breeze cannot.
One serious problem with syntax-based methods is
their lack of generalization: each type of MWE has
its own characteristics, and these characteristics dif-
fer from one language to another. Moreover, some
MWEs (such as noun compounds) are not flexible
syntactically, no matter whether they are composi-
tional or non-compositional (Reddy et al, 2011).
Much of the recent work on MWEs focuses on
their semantic properties, measuring the semantic
similarity between the MWE and its components us-
ing different resources, such as WordNet (Kim and
Baldwin, 2007) or distributional similarity relative
to a corpus (e.g. based on Latent Semantic Analysis:
Schone and Jurafsky (2001), Bannard et al (2003),
Reddy et al (2011)). The size of the corpus is im-
portant in methods based on distributional similarity.
Unfortunately, however, large corpora are not avail-
able for all languages.
Reddy et al (2011) hypothesize that the num-
ber of common co-occurrences between a given
MWE and its component words indicates the de-
gree of compositionality of that MWE. First, the co-
occurrences of a given MWE/word are considered
as the values of a vector. They then measure the
Cosine similarity between the vectors of the MWE
and its components. Bannard et al (2003) presented
four methods to measure the compositionality of En-
glish verb particle constructions. Their best result
is based on the previously-discussed method of Lin
(1999) for measuring compositionality, but uses a
more-general distributional similarity model to iden-
tify synonyms.
Recently, a few studies have investigated using
parallel corpora to detect the degree of composi-
tionality (Melamed, 1997; Moiro?n and Tiedemann,
2006; de Caseli et al, 2010; Salehi et al, 2012).
The general approach is to word-align the source
and target language sentences and analyse align-
ment patterns for MWEs (e.g. if the MWE is al-
ways aligned as a single ?phrase?, then it is a strong
indicator of non-compositionality). de Caseli et
al. (2010) consider non-compositional MWEs to be
those candidates that align to the same target lan-
guage unit, without decomposition into word align-
ments. Melamed (1997) suggests using mutual in-
formation to investigate how well the translation
model predicts the distribution of words in the tar-
get text given the distribution of words in the source
text. Moiro?n and Tiedemann (2006) show that en-
tropy is a good indicator of compositionality, be-
cause word alignment models are often confused by
non-compositional MWEs. However, this assump-
tion does not always hold, especially when deal-
ing with high-frequency non-compositional MWEs.
Salehi et al (2012) tried to solve this problem with
high frequency MWEs by using word alignment in
both directions.3 They computed backward and for-
ward entropy to try to remedy the problem with es-
pecially high-frequency phrases. However, their as-
sumptions were not easily generalisable across lan-
guages, e.g., they assume that the relative frequency
of a specific type of MWE (light verb constructions)
in Persian is much greater than in English.
Although methods using bilingual corpora are in-
tuitively appealing, they have a number of draw-
backs. The first and the most important problem
3The IBM models (Brown et al, 1993), e.g., are not bidi-
rectional, which means that the alignments are affected by the
alignment direction.
267
is data: they need large-scale parallel bilingual cor-
pora, which are available for relatively few language
pairs. Second, since they use statistical measures,
they are not suitable for measuring the composition-
ality of MWEs with low frequency. And finally,
most experiments have been carried out on English
paired with other European languages, and it is not
clear whether the results translate across to other
language pairs.
3 Resources
In this research, we use the translations of MWEs
and their components to estimate the relative de-
gree of compositionality of a MWE. There are
several resources available to translate words into
various languages such as Babelnet (Navigli and
Ponzetto, 2010),4 Wiktionary,5 Panlex (Baldwin et
al., 2010) and Google Translate.6 As we are ide-
ally after broad coverage over multiple languages
and MWEs/component words in a given language,
we exclude Babelnet and Wiktionary from our cur-
rent research. Babelnet covers only six languages
at the time of writing this paper, and in Wiktionary,
because it is constantly being updated, words and
MWEs do not have translations into the same lan-
guages. This leaves translation resources such as
Panlex and Google Translate. However, after man-
ually analysing the two resources for a range of
MWEs, we decided not to use Google Translate for
two reasons: (1) we consider the MWE out of con-
text (i.e., we are working at the type level and do not
consider the usage of the MWE in a particular sen-
tence), and Google Translate tends to generate com-
positional translations of MWEs out of context; and
(2) Google Translate provides only one translation
for each component word/MWE. This left Panlex.
Panlex is an online translation database that is
freely available. It contains lemmatized words and
MWEs in a large variety of languages, with lemma-
based (and less frequently sense-based) links be-
tween them. The database covers more than 1353
languages, and is made up of 12M lemmas and ex-
pressions. The translations are sourced from hand-
made electronic dictionaries, making it more accu-
4http://lcl.uniroma1.it/babelnet/
5http://www.wiktionary.org/
6http://translate.google.com/
rate than translation dictionaries generated automat-
ically, e.g. through word alignment. Usually there
are several direct translations for a word/MWE
from one language to another, as in translations
which were extracted from electronic dictionaries. If
there is no direct translation for a word/MWE in the
database, we can translate indirectly via one or more
pivot languages (indirect translation: Soderland et
al. (2010)). For example, English ivory tower has
direct translations in only 13 languages in Panlex,
including French (tour d?ivoire) but not Esperanto.
There is, however, a translation of tour d?ivoire into
Esperanto (ebura turo), allowing us to infer an indi-
rect translation between ivory tower and ebura turo.
4 Dataset
We evaluate our method over two datasets, as de-
scribed below.
REDDY (Reddy et al, 2011): 90 English (binary)
noun compounds (NCs), where the overall NC and
each component word has been annotated for com-
positionality on a scale from 0 (non-compositional)
to 5 (compositional). In order to avoid issues
with polysemy, the annotators were presented with
each NC in a sentential context. The authors tried
to achieve a balance of compositional and non-
compositional NCs: based on a threshold of 2.5, the
dataset consists of 43 (48%) compositional NCs, 46
(51%) NCs with a compositional usage of the first
component, and 54 (60%) NCs with a compositional
usage of the second component.
BANNARD (Bannard, 2006): 160 English verb
particle constructions (VPCs) were annotated for
compositionality relative to each of the two compo-
nent words (the verb and the particle). Each annota-
tor was asked to annotate each of the verb and parti-
cle as yes, no or don?t know. Based on the ma-
jority annotation, among the 160 VPCs, 122 (76%)
are verb-compositional and 76 (48%) are particle-
compositional.
We compute the proportion of yes tags to get the
compositionality score. This dataset, unlike REDDY,
does not include annotations for the compositional-
ity of the whole VPC, and is also less balanced, con-
taining more VPCs which are verb-compositional
than verb-non-compositional.
268
Score
Panlex
ComponentsMWE
Translate
Compare
...
...
Translations
Figure 1: Schematic of our proposed method
5 Method
To predict the degree of compositionality of an
MWE, we require a way to measure the semantic
similarity of the MWE with its components. Our
hypothesis is that compositional MWEs are more
likely to be word-for-word translations in a given
language than non-compositional MWEs. Hence, if
we can locate the translations of the components in
the translation of the MWE, we can deduce that it
is compositional. Our second hypothesis is that the
more languages we use as the basis for determin-
ing translation similarity between the MWE and its
component words, the more accurately we will be
able to estimate compositionality. Thus, rather than
using just one translation language, we experiment
with as many languages as possible.
Figure 1 provides a schematic outline of our
method. The MWE and its components are trans-
lated using Panlex. Then, we compare the transla-
tion of the MWE with the translations of its compo-
nents. In order to locate the translation of each com-
ponent in the MWE translation, we use string simi-
English Persian Translation
kick the bucket mord
kick zad
the ?
bucket satl
make a decision tasmim gereft
make sakht
a yek
decision tasmim
public service khadamaat omumi
public omumi
service khedmat
Table 1: English MWEs and their components with their
translation in Persian. Direct matches between the trans-
lation of a MWE and its components are shown in bold;
partial matches are underlined.
larity measures. The score shown in Figure 1 is de-
rived from a given language. In Section 6, we show
how to combine scores across multiple languages.
As an example of our method, consider the
English-to-Persian translation of kick the bucket as
a non-compositional MWE and make a decision as
a semi-compositional MWE (Table 1).7 By locating
the translation of decision (tasmim) in the translation
ofmake a decision (tasmim gereftan), we can deduce
that it is semi-compositional. However, we cannot
locate any of the component translations in the trans-
lation of kick the bucket. Therefore, we conclude
that it is non-compositional. Note that in this simple
example, the match is word-level, but that due to the
effects of morphophonology, the more likely situa-
tion is that the components don?t match exactly (as
we observe in the case of khadamaat and khedmat
for the public service example), which motivates our
use of string similarity measures which can capture
partial matches.
We consider the following string similarity mea-
sures to compare the translations. In each case,
we normalize the output value to the range [0, 1],
where 1 indicates identical strings and 0 indicates
completely different strings. We will indicate the
translation of the MWE in a particular language t as
MWE t, and the translation of a given component in
7Note that the Persian words are transliterated into English
for ease of understanding.
269
language t as component t.
Longest common substring (LCS): The LCS
measure finds the longest common substring be-
tween two strings. For example, the LCS between
ABABC and BABCAB is BABC. We calculate a nor-
malized similarity value based on the length of the
LCS as follows:
LongestCommonString (MWE t, component t)
min(len(MWE t), len(component t))
Levenshtein (LEV1): The Levenshtein distance
calculates for the number of basic edit operations re-
quired to transpose one word into the other. Edits
consist of single-letter insertions, deletions or sub-
stitutions. We normalize LEV1 as follows:
1? LEV1 (MWE
t, component t)
max(len(MWE t), len(component t))
Levenshtein with substitution penalty (LEV2):
One well-documented feature of Levenshtein dis-
tance (Baldwin, 2009) is that substitutions are in fact
the combination of an addition and a deletion, and as
such can be considered to be two edits. Based on this
observation, we experiment with a variant of LEV1
with this penalty applied for substitutions. Similarly
to LEV1, we normalize as follows:
1? LEV2 (MWE
t, component t)
len(MWE t) + len(component t)
Smith Waterman (SW) This method is based on
the Needleman-Wunsch algorithm,8 and was devel-
oped to locally-align two protein sequences (Smith
and Waterman, 1981). It finds the optimal simi-
lar regions by maximizing the number of matches
and minimizing the number of gaps necessary to
align the two sequences. For example, the opti-
mal local sequence for the two sequences below is
AT??ATCC, in which ?-? indicates a gap:
8The Needleman-Wunsch (NW) algorithm, was designed to
align two sequences of amino-acids (Needleman and Wunsch,
1970). The algorithm looks for the sequence alignment which
maximizes the similarity. As with the LEV score, NW min-
imizes edit distance, but also takes into account character-to-
character similarity based on the relative distance between char-
acters on the keyboard. We exclude this score, because it is
highly similar to the LEV scores, and we did not obtain encour-
aging results using NW in our preliminary experiments.
Seq1: ATGCATCCCATGAC
Seq2: TCTATATCCGT
As the example shows, it looks for the longest com-
mon string but has an in-built mechanism for includ-
ing gaps in the alignment (with penalty). This char-
acteristic of SW might be helpful in our task, be-
cause there may be morphophonological variations
between the MWE and component translations (as
seen above in the public service example). We nor-
malize SW similarly to LCS:
len(alignedSequence)
min(len(MWE t), len(component t))
6 Computational Model
Given the scores calculated by the aforementioned
string similarity measures between the translations
for a given component word and the MWE, we need
some way of combining scores across component
words.9 First, we measure the compositionality of
each component within the MWE (s1 and s2):
s1 = f1(sim1(w1,MWE), ..., simi(w1,MWE ))
s2 = f1(sim1(w2,MWE), ..., simi(w2,MWE ))
where sim is a string similarity measure, simi indi-
cates that the calculation is based on translations in
language i, and f1 is a score combination function.
Then, we compute the overall compositionality of
the MWE (s3) from s1 and s2 using f2:
s3 = f2(s1, s2)
Since we often have multiple translations for a given
component word/MWE in Panlex, we exhaustively
compute the similarity between each MWE transla-
tion and component translation, and use the highest
similarity as the result of simi. If an instance does
not have a direct/indirect translation in Panlex, we
assign a default value, which is the mean of the high-
est and lowest annotation score (2.5 for REDDY and
0.5 for BANNARD). Note that word order is not an
issue in our method, as we calculate the similarity
independently for each MWE component.
In this research, we consider simple functions for
f1 such as mean, median, product, min and max. f2
9Note that in all experiments we only combine scores given
by the same string similarity measure.
270
NC
Language Frequency Family
Czech 100 Slavic
Norwegian 100 Germanic
Portuguese 100 Romance
Thai 99 Kam-thai
French 95 Romance
Chinese 94 Chinese
Dutch 93 Germanic
Romanian 91 Romance
Hindi 67 Indic
Russian 43 Slavic
Table 2: The 10 best languages for REDDY using LCS.
was selected to be the same as f1 in all situations,
except when we use mean for f1. Here, following
Reddy et al (2011), we experimented with weighted
mean:
f2(s1, s2) = ?s1 + (1? ?)s2
Based on 3-fold cross validation, we chose ? = 0.7
for REDDY.10
Since we do not have judgements for the com-
positionality of the full VPC in BANNARD (we in-
stead have separate judgements for the verb and
particle), we cannot use f2 for this dataset. Ban-
nard et al (2003) observed that nearly all of the
verb-compositional instances were also annotated as
particle-compositional by the annotators. In line
with this observation, we use s1 (based on the verb)
as the compositionality score for the full VPC.
7 Language Selection
Our method is based on the translation of an MWE
into many languages. In the first stage, we chose 54
languages for which relatively large corpora were
available.11 The coverage, or the number of in-
stances which have direct/indirect translations in
Panlex, varies from one language to another. In
preliminary experiments, we noticed that there is
a high correlation (about 0.50 for BANNARD and
10We considered values of ? from 0 to 1, incremented by 0.1.
11In future work, we intend to look at the distribution of trans-
lations of the given MWE and its components in corpora for
many languages. The present method does not rely on the avail-
ability of large corpora.
VPC:verb
Language Frequency Family
Basque 100 Basque
Lithuanian 100 Baltic
Slovenian 100 Slavic
Hebrew 99 Semitic
Arabic 98 Semitic
Czech 95 Slavic
Slovak 92 Slavic
Latin 79 Italic
Tagalog 74 Austronesian
Polish 44 Slavic
Table 3: The 10 best languages for the verb component
of BANNARD using LCS.
VPC:particle
Language Frequency Family
French 100 Romance
Icelandic 100 Germanic
Thai 100 Kam-thai
Indonesian 92 Indonesian
Spanish 90 Romance
Tamil 87 Dravidian
Turkish 83 Turkic
Catalan 79 Romance
Occitan 76 Romance
Romanian 69 Romance
Table 4: The 10 best languages for the particle compo-
nent of BANNARD using LCS.
about 0.80 for REDDY) between the usefulness of
a language and its translation coverage on MWEs.
Therefore, we excluded languages with MWE trans-
lation coverage of less than 50%. Based on nested
10-fold cross validation in our experiments, we se-
lect the 10 most useful languages for each cross-
validation training partition, based on the Pearson
correlation between the given scores in that language
and human judgements.12 The 10 best languages
are selected based only on the training set for each
fold. (The languages selected for each fold will later
be used to predict the compositionality of the items
in the testing portion for that fold.) In Tables 2, 3
12Note that for VPCs, we calculate the compositionality of
only the verb part, because we don?t have the human judge-
ments for the whole VPC.
271
f1 sim() N1 N2 NC
Mean
SW 0.541 0.396 0.637
LCS 0.525 0.431 0.649
LEV1 0.405 0.200 0.523
LEV2 0.481 0.263 0.577
Prod
SW 0.451 0.287 0.410
LCS 0.430 0.233 0.434
LEV1 0.299 0.128 0.311
LEV2 0.294 0.188 0.364
Median
SW 0.443 0.334 0.544
LCS 0.408 0.365 0.553
LEV1 0.315 0.054 0.376
LEV2 0.404 0.134 0.523
Min
SW 0.420 0.176 0.312
LCS 0.347 0.225 0.307
LEV1 0.362 0.310 0.248
LEV2 0.386 0.345 0.338
Max
SW 0.371 0.408 0.345
LCS 0.406 0.430 0.335
LEV1 0.279 0.362 0.403
LEV2 0.380 0.349 0.406
Table 5: Correlation on REDDY (NCs). N1, N2 and NC,
are the first component of the noun compound, its second
component, and the noun compound itself, respectively.
and 4, we show how often each language was se-
lected in the top-10 languages over the combined
100 (10?10) folds of nested 10-fold cross valida-
tion, based on LCS.13 The tables show that the se-
lected languages were mostly consistent over the
folds. The languages are a mixture of Romance,
Germanic and languages from other families (based
on Voegelin and Voegelin (1977)), with no standout
language which performs well in all cases (indeed,
no language occurs in all three tables). Additionally,
there is nothing in common between the verb and the
particle top-10 languages.
8 Results
As mentioned before, we perform nested 10-fold
cross-validation to select the 10 best languages on
the training data for each fold. The selected lan-
guages for a given fold are then used to compute s1
13Since our later results show that LCS and SW have higher
results, we only show the best languages using LCS. These
largely coincide with those for SW.
f1 sim() Verb Particle
Mean
SW 0.369 0.510
LCS 0.406 0.509
LEV1 0.335 0.454
LEV2 0.340 0.460
Prod
SW 0.315 0.316
LCS 0.339 0.299
LEV1 0.322 0.280
LEV2 0.342 0.284
Median
SW 0.316 0.409
LCS 0.352 0.423
LEV1 0.295 0.387
LEV2 0.309 0.368
Min
SW 0.262 0.210
LCS 0.329 0.251
LEV1 0.307 0.278
LEV2 0.310 0.281
Max
SW 0.141 0.288
LCS 0.268 0.299
LEV1 0.145 0.450
LEV2 0.170 0.398
Table 6: Correlation on BANNARD (VPC), based on the
best-10 languages for the verb and particle individually
and s2 (and s3 for NCs) for each instance in the test
set for that fold. The scores are compared with hu-
man judgements using Pearson?s correlation. The
results are shown in Tables 5 and 6. Among the five
functions we experimented with for f1, Mean per-
forms much more consistently than the others. Me-
dian is less prone to noise, and therefore performs
better than Prod, Max and Min, but it is still worse
than Mean.
For the most part, LCS and SW perform better
than the other measures. There is little to separate
these two methods, partly because they both look for
a sequence of similar characters, unlike LEV1 and
LEV2 which do not consider contiguity of match.
The results support our hypothesis that using mul-
tiple target languages rather than one, results in a
more accurate prediction of MWE compositionality.
Our best result using the 10 selected languages on
REDDY is 0.649, as compared to the best single-
language correlation of 0.497 for Portuguese. On
BANNARD, the best LCS result for the verb com-
ponent is 0.406, as compared to the best single-
272
language correlation of 0.350 for Lithuanian.
Reddy et al (2011) reported a correlation of 0.714
on REDDY. Our best correlation is 0.649. Note that
Reddy et al (2011) base their method on identifi-
cation of MWEs in a corpus, thus requiring MWE-
specific identification. Given that this has been
shown to be difficult for MWE types including En-
glish VPCs (McCarthy et al, 2003; Baldwin, 2005),
the fact that our method is as competitive as this is
highly encouraging, especially when you consider
that it can equally be applied to different types of
MWEs in other languages. Moreover, the computa-
tional processing required by methods based on dis-
tributional similarity is greater than our method, as
it does not require processing a large corpus.
Finally, we experimented with combining our
method (STRINGSIMMEAN) with a reimplementation
of the method of Reddy et al (2011), based on sim-
ple averaging, as detailed in Table 7. The results are
higher than both component methods and the state-
of-the-art for REDDY, demonstrating the comple-
mentarity between our proposed method and meth-
ods based on distributional similarity.
In Table 8, we compare our results
(STRINGSIMMEAN) with those of Bannard et
al. (2003), who interpreted the dataset as a binary
classification task. The dataset used in their study
is a subset of BANNARD, containing 40 VPCs, of
which 29 (72%) were verb compositional and 23
(57%) were particle compositional. By applying a
threshold of 0.5 over the output of our regression
model, we binarize the VPCs into the compositional
and non-compositional classes. According to the
results shown in Table 6, LCS is a better similarity
measure for this task. Our proposed method has
higher results than the best results of Bannard et
al. (2003), in part due to their reliance on VPC
identification, and the low recall on the task, as
reported in the paper. Our proposed method does
not rely on a corpus or MWE identification.
9 Error Analysis
We analyse items in REDDY which have a high dif-
ference (more than 2.5) between the human anno-
tation and our scores (using LCS and Mean). The
words are cutting edge, melting pot, gold mine and
ivory tower, which are non-compositional accord-
ing to REDDY. After investigating their translations,
we came to the conclusion that the first three MWEs
have word-for-word translations in most languages.
Hence, they disagree with our hypothesis that word-
for-word translation is a strong indicator of compo-
sitionality. The word-for-word translations might be
because of the fact that they have both compositional
and non-compositional senses, or because they are
calques (loan translations). However, we have tried
to avoid such problems with calques by using trans-
lations into several languages.
For ivory tower (?a state of mind that is discussed
as if it were a place?)14 we noticed that we have a di-
rect translation into 13 languages. Other languages
have indirect translations. By checking the direct
translations, we noticed that, in French, the MWE is
translated to tour and tour d?ivoire. A noisy (wrong)
translation of tour ?tower? resulted in wrong indirect
translations for ivory tower and an inflated estimate
of compositionality.
10 Conclusion and Future Work
In this study, we proposed a method to predict MWE
compositionality based on the translation of the
MWE and its component words into multiple lan-
guages. We used string similarity measures between
the translations of the MWE and each of its compo-
nents to predict the relative degree of composition-
ality. Among the four similarity measures that we
experimented with, LCS and SW were found to be
superior to edit distance-based methods. Our best re-
sults were found to be competitive with state-of-the-
art results using vector-based approaches, and were
also shown to complement state-of-the-art methods.
In future work, we are interested in investigating
whether alternative ways of combining our proposed
method with vector-based models can lead to fur-
ther enhancements in results. These models could
be especially effective when comparing translations
which are roughly synonymous but not string-wise
similar.
Acknowledgments
We would like to thank Timothy Baldwin, Su Nam
Kim, and the anonymous reviewers for their valu-
able comments and suggestions.
14This definition is from Wordnet 3.1.
273
sim() STRINGSIMMEAN STRINGSIMMEAN + Reddy et al
SW 0.637 0.735
LCS 0.649 0.742
LEV1 0.523 0.724
LEV2 0.577 0.726
Table 7: Correlation after combining Reddy et al?s method and our method with Mean for f1 (STRINGSIMMEAN ). The
correlation using Reddy et al?s method is 0.714.
Method Precision Recall F-score (? = 1) Accuracy
Bannard et al (2003) 0.608 0.666 0.636 0.600
STRINGSIMMEAN 0.862 0.718 0.774 0.693
Table 8: Results for the classification task. STRINGSIMMEAN is our method using Mean for f1
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and the
Australian Research Council through the ICT Cen-
tre of Excellence program.
References
Otavio Costa Acosta, Aline Villavicencio, and Viviane P
Moreira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval. In
Proceedings of the ALC Workshop on MWEs: from
Parsing and Generation to the Real World (MWE
2011), pages 101?109.
Timothy Baldwin and Su Nam Kim. 2009. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing.
CRC Press, Boca Raton, USA, 2nd edition.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-2003 Workshop on Multiword Expres-
sions: Analysis, Acquisition and Treatment, pages 89?
96, Sapporo, Japan.
Timothy Baldwin, Jonathan Pool, and Susan M Colow-
ick. 2010. Panlex and lextract: Translating all words
of all languages of the world. In Proceedings of the
23rd International Conference on Computational Lin-
guistics: Demonstrations, pages 37?40.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expres-
sions, 19(4):398?414.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 65?72.
Colin James Bannard. 2006. Acquiring Phrasal Lexicons
from Corpora. Ph.D. thesis, University of Edinburgh.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Helena Medeiros de Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation, 44(1):59?77.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the ACL 2007Workshop on A Broader Per-
spective on Multiword Expressions, pages 9?16.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Su Nam Kim and Timothy Baldwin. 2007. Detecting
compositionality of english verb-particle constructions
using semantic similarity. In Proceedings of the 7th
Meeting of the Pacific Association for Computational
Linguistics (PACLING 2007), pages 40?48.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, pages 317?
324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 2003 workshop
274
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 73?80.
Diana McCarthy, Sriram Venkatapathy, and Aravind K
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 369?379.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Fifth Workshop on Very Large Cor-
pora. EMNLP.
Begona Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multi-wordexpressions in a multilingual
context, pages 33?40.
Roberto Navigli and Simone Paolo Ponzetto. 2010. Ba-
belnet: Building a very large multilingual semantic
network. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 216?225, Uppsala, Sweden.
Saul B Needleman and Christian D Wunsch. 1970. A
general method applicable to the search for similarities
in the amino acid sequence of two proteins. Journal of
molecular biology, 48(3):443?453.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of IJCNLP, pages 210?
218.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for nlp. In Proceed-
ings of the 3rd International Conference on Intelligent
Text Processing Computational Linguistics (CICLing-
2002), pages 189?206. Springer.
Bahar Salehi, Narjes Askarian, and Afsaneh Fazly. 2012.
Automatic identification of Persian light verb con-
structions. In Proceedings of the 13th International
Conference on Intelligent Text Processing Computa-
tional Linguistics (CICLing-2012), pages 201?210.
Patrick Schone and Dan Jurafsky. 2001. Is knowledge-
free induction of multiword unit dictionary headwords
a solved problem. In Proceedings of the 6th Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP 2001), pages 100?108.
TF Smith and MS Waterman. 1981. Identification of
commonmolecular subsequences. Molecular Biology,
147:195?197.
Stephen Soderland, Oren Etzioni, Daniel S Weld, Kobi
Reiter, Michael Skinner, Marcus Sammer, Jeff Bilmes,
et al 2010. Panlingual lexical translation via proba-
bilistic inference. Artificial Intelligence, 174(9):619?
637.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting
Underlying Properties, pages 20?27.
Charles Frederick Voegelin and FlorenceMarie Voegelin.
1977. Classification and index of the world?s lan-
guages, volume 4. Elsevier Science Ltd.
275
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 133?137, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Umelb: Cross-lingual Textual Entailment with Word Alignment and String
Similarity Features
Yvette Graham Bahar Salehi Timothy Baldwin
Department of Computing and Information Systems
The University of Melbourne
{ygraham,bsalehi,tbaldwin}@unimelb.edu.au
Abstract
This paper describes The University of Mel-
bourne NLP group submission to the Cross-
lingual Textual Entailment shared task, our
first tentative attempt at the task. The ap-
proach involves using parallel corpora and au-
tomatic word alignment to align text fragment
pairs, and statistics based on unaligned words
as features to classify items as forward and
backward before a compositional combination
into the final four classes, as well as exper-
iments with additional string similarity fea-
tures.
1 Introduction
Cross-lingual Textual Entailment (CLTE) (Negri et
al., 2012) proposes the task of automatically iden-
tifying the kind of relation that exists between pairs
of semantically-related text fragments written in two
distinct languages, a variant of the traditional Rec-
ognizing Textual Entailment (RTE) task (Bentivogli
et al, 2009; Bentivogli et al, 2010). The task tar-
gets the cross-lingual content synchronization sce-
nario proposed in Mehdad et al (2010, 2011). Com-
positional classification can be used by training two
distinct binary classifiers for forward and backward
entailment classification, before combining labels
into the four final entailment categories that now in-
clude bidirectional and no entailment labels. The
most similar previous work to this work is the cross-
lingual approach of the FBK system (Mehdad et
al., 2012) from Semeval 2012 (Negri et al, 2012),
in which the entailment classification is obtained
without translating T1 into T2 for the Spanish?
English language pair. We apply the cross-lingual
approach to German?English and instead of cross-
lingual matching features, we use Giza++ (Och et
al., 1999) and Moses (Koehn et al, 2007) to auto-
matically word align text fragment pairs to compute
statistics of unaligned words. In addition, we in-
clude some additional experiments using string sim-
ilarity features.
2 Compositional Classification
Given a pair of topically related fragments, T1 (Ger-
man) and T2 (English), we automatically annotate it
with one of the following entailment labels: bidi-
rectional, forward, backward, no entailment. We
take the compositional approach and separately train
a forward, as well as a backward binary classifier.
Each classifier is run separately on the set of text
fragment pairs to produce two binary labels for for-
ward and backward entailment. The two sets of la-
bels are logically combined to produce a final clas-
sification for each test pair of forward, backward,
bidirectional or no entailment.
3 Word Alignment Features
The test set of topically-related text fragments, T1
(German) and T2 (English) were added to Europarl
German?English parallel text (Koehn, 2005) and
Giza++ was used for automatic word alignment in
both language directions. Moses (Koehn et al,
2007) was then used for symmetrization with the
grow diag final and algorithm. This produces a
many-to-many alignment between the words of the
133
German, T1, and English, T2, with words also re-
maining unaligned.
The following features are computed for each test
pair feature scores for the forward classifier:
? A1: count of unaligned words in T2
? A2: count of words comprised soley of digits
in T2 not in T1
? A3: count of unaligned words in T2 with low
probability of appearing unaligned in Europarl
(with threshold p=0.11)
The number of words in T2 (English) that are not
aligned with anything in T1 (German) should pro-
vide an indication that, for example, the English text
fragment contains information not present in the cor-
responding German text fragment and subsequently
evidence against the presence of forward entailment.
We there include the feature, A1, that is simply a
count of unaligned words in English T2. In addi-
tion, we hypothesize that the absence of a number
from T2 may be a more significant missing element
of T2 from T1. We therefore include as a feature
the count of tokens comprised of digits in T2 that
are not also present in T1. The final word align-
ment feature attempts to refine A1, by distinguishing
words that are rarely unaligned in German?English
translations. Statistics are computed for every lexi-
cal item from German?English Europarl translations
to produce a lexical unalignment probability, com-
puted for each lexical item based on its relative fre-
quency in the corpus when it is not aligned to any
other word.
The backward classifier uses the same features but
computed for each test pair on counts of unaligned
T1 words.
4 Results
Results for several combinations of features are
shown in Table 1 when the system is trained on
the 500-pair development set training corpus and
tested on the 500-pair held-out development test set
(DEV), in addition to results for feature combina-
tions when trained on the entire 1000-pair develop-
ment data and tested on the held-out 500-pair gold
standard (TEST) (Negri et al, 2011), when the sys-
tem is evaluated as two separate binary forward and
backward classifiers (2-CLASS) as well as the final
evaluation including all four entailment classes (4-
CLASS). The highest accuracy is achieved by the
classifier using the single feature of counts of un-
aligned words, A1, of 34.6%. As two separate bi-
nary classifiers, the alignment features, A1+A2+A3,
achieve a relatively high accuracy of 74.0% for for-
ward with somewhat less accurate for backward
(65.8%) classification (both over the DEV data).
When combined to the final four CLTE classes, how-
ever, accuracy drops significantly to an overall accu-
racy of 50% (also over DEV). A main cause is inac-
curate labeling of no entailment gold standard test
pairs, as the most severe decline is for recall of test
pairs for this label (38.4%).
Accuracy on the development set for the word
alignment features, A1+A2+A3, compared to the
test set shows a sever decline, from 50% to 32%. On
the test data, however, a main cause of inaccuracy
is that backward gold standard test pairs, although
achieving close accuracy to forward when evaluated
as binary classifiers, are inaccurately labeled in the
4-class evaluation, as recall for backward drops to
only 18.4% for this label.
Another insight revealed for the alignment fea-
tures, A1+A2+A3, in the 4-class evaluation is that
when run on the development set, the classes for-
ward and backward achieve significantly higher
f-scores compared to no entailment. However,
the contrary is observed for the test data, as
no entailment achieve higher results than both uni-
directional classes. This appears at first to be a
somewhat counter-intuitive result, but in this case,
the system is simply better at predicting forward and
backward when no entailment exists for a translation
pair compared to when a unidirectional entailment is
present.
4.1 String Similarity Features
In addition to the word alignment features, subse-
quent to submitting results to the shared task, we
have carried out additional experiments using string
similarity features, based on our recent success in
apply string similarity to both the estimation of com-
positionality of MWEs (Salehi and Cook, to appear)
and also the estimation of similarity between short
134
2-CLASS 4-CLASS
Acc. Prec Recall F1 Acc. Prec Recall F1
D
E
V
A1 + A2 + A3
bwrd 65.80 63.12 76.00 68.96 50.00 bwrd 54.80 59.20 56.90
fwrd 74.00 72.22 78.00 75.00 fwrd 54.80 45.60 49.80
none 50.50 38.40 43.60
bidir 42.80 56.80 48.80
S1 + S2 + S3
bwrd 58.20 57.75 61.20 59.42 27.40 bwrd 14.30 0.80 1.50
fwrd 47.00 47.17 50.00 59.42 fwrd 0.00 0.00 0.00
none 30.70 39.70 39.70
bidir 25.60 52.80 34.50
T
E
S
T
A1
bwrd 57.00 58.54 48.00 52.75 34.60 bwrd 25.50 19.20 21.90
fwrd 58.40 58.75 56.40 57.55 fwrd 34.90 36.00 35.40
none 36.70 48.80 41.90
bidir 38.70 34.40 36.40
A2
bwrd 50.00 0.00 0.00 0.00 33.60 bwrd 24.70 18.40 21.10
fwrd 51.60 50.85 95.20 66.29 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A3
bwrd 54.80 55.61 47.60 51.29 34.20 bwrd 32.70 26.40 29.20
fwrd 61.20 61.57 59.60 60.57 fwrd 33.30 34.40 33.90
none 36.90 46.40 41.10
bidir 32.70 29.60 31.10
A1+A2
bwrd 57.60 57.72 56.80 57.26 33.60 bwrd 24.70 18.40 21.10
fwrd 59.80 58.84 65.20 61.86 fwrd 34.70 34.40 34.50
none 36.90 38.40 37.60
bidir 35.30 43.20 38.80
A1+A3
bwrd 57.20 57.96 52.40 55.04 33.00 bwrd 26.60 20.00 22.80
fwrd 58.60 58.05 62.00 59.96 fwrd 31.90 34.40 33.10
none 36.70 40.80 38.60
bidir 34.80 36.80 35.80
A2+A3
bwrd 54.80 55.83 46.00 50.44 33.40 bwrd 32.30 25.60 28.60
fwrd 61.00 61.70 58.00 59.79 fwrd 32.80 33.60 33.20
none 34.90 46.40 39.90
bidir 32.70 28.00 30.20
A1 + A2 + A3
bwrd 57.60 57.72 56.80 57.26 32.00 bwrd 24.00 18.40 20.80
fwrd 59.20 58.39 64.00 61.07 fwrd 32.30 32.00 32.10
none 36.20 37.60 36.90
bidir 34.70 41.60 37.80
S1 + S2 + S3
bwrd 53.20 53.77 45.60 49.35 26.00 bwrd 20.00 1.50 29.50
fwrd 48.60 48.36 41.20 44.49 fwrd 16.70 0.80 31.50
none 28.00 63.20 38.80
bidir 23.70 39.20 29.50
A1 + A2 + A3 + S1
bwrd 57.40 58.30 52.00 54.97 33.00 bwrd 27.60 19.20 22.60
fwrd 59.80 58.84 65.20 61.86 fwrd 29.80 33.60 31.60
none 38.20 41.60 39.80
bidir 34.60 37.60 36.00
A1 + A2 + A3 + S2
bwrd 57.80 58.52 53.60 55.95 32.60 bwrd 26.70 19.20 22.30
fwrd 59.60 58.70 64.80 61.60 fwrd 30.70 33.60 32.10
none 37.30 40.00 38.60
bidir 33.80 37.60 35.60
A1 + A2 + A3 +S3
bwrd 58.20 58.51 56.40 57.44 32.80 bwrd 24.70 19.20 21.60
fwrd 59.60 58.82 64.00 61.30 fwrd 32.00 32.80 32.40
none 37.40 39.20 38.30
bidir 34.70 40.00 37.20
Table 1: Cross-lingual Textual Entailment Results for Word alignment Features and String Similarity Measures, A1
= count of unaligned words in T2, A2 = count of unaligned numbers in T2, A3 = count of unaligned words in T2
with unaligned probability < 0.11, S1 = Number of matched words in the aligned sequence given by Smith-Waterman
algorithm, S2 = Penalty of aligning sentences using Smith-Waterman algorithm, S3 = Levenshtein distance between
the sentences
135
texts in the *SEM 2013 Shared Task (Gella et al,
to appear). Using the alignments, we replace each
English word with its corresponding word in Ger-
man. The resulting German sentence is compared
with the actual one using string similarity measures.
As the structure of both English and German sen-
tences are usually SVO, we hypothesize that when
there is no entailment between the two given sen-
tences, the newly-made German sentence and the
original German sentence will differ a lot in word
order.
In order to compare the two German sentences,
we use the Levenshtein (Levenshtein, 1966) and the
Smith-Waterman (Smith and Waterman, 1981) al-
gorithm. The Levenshtein algorithm measures the
number of world-level edits to change one sentence
into another. The edit operators consist of insertion
and deletion. We consider substitution as two edits
(combination of insertion and deletion) based on the
findings of Baldwin (2009).
We also use Smith-Waterman (SW) algorithm,
which was originally developed to find the most sim-
ilar region between two proteins. The algorithm
looks for the longest common substring, except that
it permits small numbers of penalized editions con-
sisting of insertion, deletion and substitution. We
call the best found substring the ?SW aligned se-
quence?. In this experiment, we consider the number
of matched words and the number of penalties in the
SW aligned sequence as features.
Results for the string similarity features are shown
in Table 1. Since the string similarity feature scores
do not take the entailment direction into account,
i.e. there is a single set of feature scores for each
text fragment pair as there is no distinction between
forward and backward entailment, and they are not
suited for standalone use in compositional classifica-
tion. We do, however, include these scores in Table
1 to illustrate how with the compositional approach
using the same set of features for forward and back-
ward ultimately results in a classification of test pairs
as either bidirectional or no entailment.
When individual string similarity features are
added to the word alignment features, minor gains in
accuracy are achieved over the word alignment fea-
tures alone, +1% for S1, +0.6% for S2 and +0.8%
for S3 (= Levenstein).
5 Possible Additions: Dictionary Features
We hypothesize that when there is no entailment be-
tween the two sentences, the aligner may not accu-
rately align words. An on-line dictionary contain-
ing lemmatized words, such as Panlex (Baldwin and
Colowick, 2010), could be used to avoid errors in
such cases. Dictionary-based feature scores based
on the presence or absence of alignments in the dic-
tionary could then be applied.
6 Conclusions
This paper describes a compositional cross-lingual
approach to CLTE with experiments carried out
for the German-English language pair. Our results
showed that in the first stages of binary classification
as forward and backward, the word alignment fea-
tures alone achieved good accuracy but when com-
bined suffer severely. Accuracy of the approach
using word alignment features could benefit from
a more directional multi-class classification as op-
posed to the compositional approach we used. In
addition, results showed minor increases in accuracy
can be achieved using string similarity measures.
Acknowledgments
This work was supported by the Australian Research
Council.
References
Timothy Baldwin and Jonathan Pool Susan M. Colowick.
2010. Panlex and lextract: Translating all words of all
languages of the world. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics:
Demonstrations, pages 37?40.
Timothy Baldwin. 2009. The hare and the tortoise:
Speed and reliability in translation retrieval. Machine
Translation, 23(4):195?240.
L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and
B. Magnini. 2009. The fifth PASCAL recognizing
textual entailment challenge. In TAC 2009 Workshop
Proceedings, Gaithersburg, MD.
L. Bentivogli, P. Clark, I. Dagan, H. T. Dang, and D. Gi-
ampiccolo. 2010. The sixth PASCAL recognizing
textual entailment challenge. In TAC 2010 Workshop
Proceedings, Gaithersburg, MD.
Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser,
Paul Cook, and Timothy Baldwin. to appear. Integrat-
ing predictions from multiple domains and feature sets
136
for estimating semantic textual similarity. In Proceed-
ings of *SEM 2013 Shared Task STS.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan HerbstHieu Hoang. 2007. Moses:
Open Source Toolkit for Statistical Machine Transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), demonstration session,
Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit, Phuket, Thailand.
Vladimir I Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. In Soviet
physics doklady, volume 10, page 707.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
cross-lingual textual entailment. In Proceedings of
NAACL-HLT.
Y. Mehdad, M. Negri, and M. Federico. 2011. Using par-
allel corpora for cross-lingual textual entailment. In
Proceedings of ACL-HLT 2011.
Yashar Mehdad, Matteo Negri, and Jose G. C. de Souza.
2012. Fbk: Cross-lingual textual entailment with-out
translation. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval2012).
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: Cross-lingual textual entailment
for content synchronization. In First Joint Conference
on Lexical and Computational Semantics, pages 399?
407, Montreal, Canada.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20?28, College Park, MD.
Bahar Salehi and Paul Cook. to appear. Predicting
the compositionality of multiword expressions using
translations in multiple languages. In Proceedings of
the Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM 2013).
Temple F Smith and Michael S Waterman. 1981. The
identification of common molecular subsequences.
Journal of Molecular Biology, 147:195?197.
137
