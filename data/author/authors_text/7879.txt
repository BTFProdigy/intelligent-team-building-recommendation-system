Quality-Sensitive Test Set Selection for a Speech Translation System 
Fumiaki Sugaya1, Keiji Yasuda2, Toshiyuki Takezawa and Seiichi Yamamoto 
ATR Spoken Language Translation Research Laboratories 
2-2-2 Hikari-dai Seika-cho, Soraku-gun, Kyoto, 619-0288, Japan 
{fumiaki.sugaya, keiji.yasuda, toshiyuki.takezawa,
seiichi.yamamoto}@atr.co.jp
 
 
                                                          
1 
2 
1Current affiliation: KDDI R&D Laboratories. Also at Graduate School of Science and Technology, Kobe University. 
2Also at Graduate School of Engineering, Doshisha University. 
  
Abstract 
We propose a test set selection method to 
sensitively evaluate the performance of a 
speech translation system. The proposed 
method chooses the most sensitive test 
sentences by removing insensitive 
sentences iteratively. Experiments are 
conducted on the ATR-MATRIX speech 
translation system, developed at ATR 
Interpreting Telecommunications 
Research Laboratories. The results show 
the effectiveness of the proposed method. 
According to the results, the proposed 
method can reduce the test set size to less 
than 40% of the original size while 
improving evaluation reliability. 
Introduction 
The translation paired comparison method 
precisely measures the capability of a speech 
translation system.  In this method, native speakers 
compare a system?s translation and the translations, 
made by examinees who have various TOEIC 
scores. The method requires two human costs: the 
data collection of examinees? translations and the 
comparison by native speakers.  In this paper, we 
propose a test set size reduction method that 
reduces the number of test set utterances.  The 
method chooses the most sensitive test utterances 
by removing the most insensitive utterances 
iteratively.    
In section 2, the translation paired comparison 
method is described. Section 3 explains the 
proposed method. In section 4, evaluation results 
for ATR-MATRIX are shown. Section 5 discusses 
the experimental results. In section 6, we state our 
conclusions. 
Translation paired comparison method 
The translation paired comparison method  
(Sugaya, 2000) is an effective evaluation method 
for precisely measuring the capability of a speech 
translation system. In this section, a description of 
the method is given. 
2.1 Methodology of the translation paired 
comparison method 
Figure 1 shows a diagram of the translation paired 
comparison method in the case of Japanese to 
English translation. The Japanese native-speaking 
examinees are asked to listen to Japanese text and 
provide an English translation on paper.  The 
Japanese text is spoken twice within one minute, 
with a pause in-between. To measure the English 
capability of the Japanese native speakers, the 
TOEIC score is used. The examinees are requested 
to present an official TOEIC score certificate 
showing that they have taken the test within the 
past six months. A questionnaire is given to them 
and the results show that the answer time is 
moderately difficult for the examinees. 
The test text is the SLTA1 test set, which 
consists of 330 utterances in 23 conversations from 
a bilingual travel conversation database (Morimoto, 
1994; Takezawa, 1999). The SLTA1 test set is 
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 109-116.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
open for both speech recognition and language 
translation. The answers written on paper are typed. 
In the proposed method, the typed translations 
made by the examinees and the outputs of the 
system are merged into evaluation sheets and are 
then compared by an evaluator who is a native 
English speaker. Each utterance information is 
shown on the evaluation sheets as the Japanese test 
text and the two translation results, i.e., translations 
by an examinee and by the system.  The two 
translations are presented in random order to 
eliminate bias by the evaluator.  The evaluator is 
asked to follow the procedure illustrated in Figure 
2. The four ranks in Figure 2 are the same as those 
used in Sumita (1999). The ranks A, B, C, and D 
indicate: (A) Perfect: no problems in both 
information and grammar; (B) Fair: easy-to-
understand with some unimportant information 
missing or flawed grammar; (C) Acceptable: 
broken but understandable with effort; (D) 
Nonsense: important information has been 
translated incorrectly. 
2.2 Evaluation result using the translation 
paired comparison method 
Figure 3 shows the result of a comparison between 
a language translation subsystem (TDMT) and the 
examinees. The input for TDMT included accurate 
transcriptions. The total number of examinees was 
thirty, with five people having scores in every 
hundred-point TOEIC range between the 300s and 
800s. In Figure 3, the horizontal axis represents the 
TOEIC score and the vertical axis the system 
winning rate (SWR) given by following equation: 
Translation 
Result by 
Human 
Evaluation 
Sheet 
Japanese Test 
Text Typing Paired Comparison 
Accurate Text 
 
 
 
 
where NTOTAL denotes the total number of 
utterances in the test set, NTDMT represents the 
number of  "TDMT won" utterances,  and NEVEN, 
indicates the number of  even (non-winner) 
utterances, i.e., no difference between the results of 
the TDMT and humans. The SWR ranges from 0 
to 1.0, signifying the degree of capability of the 
MT system relative to that of the examinee.  An 
SWR of 0.5 means that the TDMT has the same 
capability as the human examinee. 
Figure 3 shows that the SWR of TDMT is 
greater than 0.5 at TOEIC scores of around 300 
and 400, i.e., the TDMT system wins over humans 
with TOEIC scores of 300 and 400. Examinees, in 
contrast, win at scores of around 800. The 
capability balanced area is around a score of 600 to 
(1)                   
0.5
TOTAL
EVENTDMT
N
NNSWR
?+
=
Figure 1: Diagram of translation pair comparison method 
Japanese-to-English 
Language Translation 
(J-E TDMT) 
Japanese Recognition
(Japanese SPREC) 
Choose A, B, C, or D rank 
 
No 
Same rank?
 
Yes 
Consider naturalness 
 
YesNo 
Same? 
 
Select better result 
 
EVEN 
 
Figure 2: Procedure of comparison 
by native speaker 
 300 400 500 600 700 800 900
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
TO EIC score
SW
R
Figure 3: Evaluation results using translation 
paired comparison method 
Under the above condition, the standard deviation 
of the system's TOEIC score is calculated by 
 
(4)          
)(
)(1
2
2
0
2 ? ?
?
+=
XX
XC
n i
t ?
?
?
 
 
 
where n is the number of examinees, C0 is the 
system's TOEIC score, and X  is the average of 
the examinees' TOEIC scores. Equation (4) 
indicates that the minimum error is given when the 
system's TOEIC score equals the average of the 
examinees' TOEIC scores. 
By using a t-distribution, the confidence 
interval (CI) of the system's TOEIC score with 
confidence coefficient 1-?  is given by 
 700. To precisely determine the balanced point, we 
used regression analysis. The straight line in Figure 
3 is the regression line. The capability balanced 
point between the TDMT subsystem and the 
examinees is 0.5 of SWR. In Figure 3, the exact 
point is a TOEIC score of 708. We call this point 
the system's TOEIC score. Consequently, the 
translation capability of the language translation 
system equals that of the examinees at around a 
score of 700 points on the TOEIC.  
 
 
 
[ ]
(5)                                   )2;
2
( 
   , 00
??=
+?=
ntI
ICICCI
t
?
?
 
In the current study, we employ 0.01 for the 
value of ? .  
2.4 Costs for the translation paired comparison 
method 
The experimental result for ATR-MATRIX, 
which consists of a speech recognition subsystem 
and TDMT, has been also reported (Sugaya, 2000). 
This system?s TOEIC score is 548, where the 
number of speech recognition errors is a factor in 
the degradation of the score. 
The translation paired comparison method is an 
effective evaluation method because it can clearly 
express a system?s performance as a TOEIC (Test 
of English for International Communication)   
score. However, this method has excessive 
evaluation costs.    
Roughly speaking, one of these costs is the need 
to collect translations made by examinees of 
various TOEIC scores. As shown in Equations (4) 
and (5), n, the number of examinees, affects the 
confidence interval of the system?s TOEIC score. 
Therefore, a reduction in this number makes it 
difficult to obtain a reliable evaluation result. 
2.3 Error in the system?s TOEIC score 
The SWR (Yi) and TOEIC scores for the examinees 
(Xi) are assumed to satisfy the population 
regression equation:  
 (2)          ),...,2,1(    21 niXY iii =++= ??? 
The other cost is for the evaluation. Compared 
to a conventional evaluation method, such as a 
simple rank evaluation method, the translation 
paired comparison method uses a larger amount of 
labor because the evaluator must work on n 
evaluation sheets. Each sheet consists of 330 pairs 
of translation results to be evaluated. Even for an 
accomplished evaluator, it takes more than two 
weeks to finish the work, following the method 
explained in section 2.2. 
where 1? and 2? are population regression 
coefficients.  The error term ( i? ) is assumed to 
satisfy the following condition: 
 
 
0    (d)
   if     0),(),(    (c)
(3)                     ,...,2,1     ,)(    (b)
0)(    (a)
22
?
?==
==
=
i
jiji
i
i
jiECov
niV
E
?
????
??
?
 
 
 
 
3 Proposed method 
Yes 
No 
?
?
No
Yes
All candidates 
are calculated?
Set the number of iterations 
 
Remove worst utterances from 
candidates 
Is iteration 
achieved? 
Calculate iteration 
 
Update worst sentence, 
which causes maximum 
iteration 
Get next candidate 
 
As explained in the previous section, the 
translation paired comparison method has an 
excessive evaluation cost. Nevertheless, it is an 
effective evaluation method for measuring the 
capability of a speech translation system. 
Therefore, cost reduction for this evaluation 
method is an important subject for study. 
The proposed method reduces the evaluation 
cost by removing insensitive test utterances from 
the test set. In this section, we explain the 
optimization procedure of the proposed method.  
3.1 Optimization basis 
In the proposed method, the basis of test set 
optimization is the minimization of ? . As shown 
in Equations (4) and (5), this value has an 
influence on the confidence interval of the system's 
TOEIC score. Therefore, minimizing ?  brings 
about a reliable evaluation result.  
We introduce ? iteration, which is calculated in 
each iteration step. ? iteration is also calculated by 
using Equations (2) and (3). The difference 
between ? iteration and?  is the test set to be used 
for calculation. ? iteration is calculated using 
residual test utterances in each iteration step. 
However, the values of 1?  and 2?  are fixed, i.e., 
for the calculation of ? iteration, these 1?  and 2?  
are calculated using the original test set consisting 
of 330 test utterances. 
Optimization is conducted iteratively by 
picking up the test utterance that causes maximum 
?  iteration in each iteration step. The details of this 
procedure is explained in the next subsection. 
3.2 Methodology of the proposed method 
Figure 4 shows a diagram of the proposed method. 
In the first step, the number of iterations is set. 
This number is an actual number of removed test 
utterances. During the iterations, test utterances are 
removed one-by-one. To decide which test 
utterance to remove in each iteration, ? iteration is 
calculated for the condition of removing each test 
utterance. This calculation is done for all 
candidates, i.e., all constituents of residual test 
utterances.  
Figure 4: Procedure of proposed method 
 
At the end of each iteration step, the test 
utterance to be removed is decided. The removed 
test utterance is the one that maximizes ? iteration. 
We regard the utterance as maximizing? iteration if 
removing it from the test set gives minimum 
? iteration. 
70
720
740
760
TO
EI
C
 
sco
re
0 50 100 150 200 250 300
660
680
Iteration
   (upper)  C0 opt  +  Iopt 
  C0 opt
   (lower)  C0 opt   -  Iopt
20
30
40
?
t
 
o
p
t
      Random selection (Averaging of 10 trials)
      Optimzed  (Open)
      Optimzed  (Closed)
0 50 100 150 200 250 300
0
10
Iteration
 
Figure 6: Relationship between iteration 
and ? t opt 
Figure 5: Relationship between iteration  
and system?s TOEIC score 
 As shown in the figure, from iteration 1 to 
iteration 250, the value of C0 opt is stable and does 
not deviate from C0, which is 708. Furthermore, 
until around iteration 200, the value of Iopt 
decreases concurrently with the iteration. 
4 Experimental results 
In this section, we show experimental results of the 
proposed method. Here, we introduce the suffix 
?opt? to distinguish a variable calculated with the 
optimized test set from a variable calculated with 
the original test set. All of the above variables are 
calculated with the original test set. By joining the 
suffix ?opt? to these variables, we refer to variables 
calculated with the optimized test set, e.g., ?  opt 3, 
? t opt, Iopt, C0 opt, CI opt, and so on. 
This result suggests that the proposed may 
provide low-cost evaluation with high reliability. 
 
4.2 Experiment opened for examinees 
In the result shown in the previous subsection, the 
optimization and evaluation were conducted on the 
same examinees, i.e., the evaluation is closed for 
examinees. In this subsection, we look into the 
robustness of the proposed method against 
different examinees. We divided the group, 
consisting of 30 examinees, into two groups: a 
group of odd-numbered examinees and a group of 
even-numbered examinees. Individuals were sorted 
by TOEIC score from lowest to highest.  
4.1 Closed experiment 
This  subsection discusses an experimental result    
obtained for the same test set and examinees 
described in Section 2. Namely, the target test set 
for optimization consists of 330 utterances and the 
number of examinees is 30. 
Figure 5 shows the relationship between 
iteration and the system?s TOEIC score (C0 opt). In 
this figure, the horizontal axis represents the 
iteration number and the vertical axis the TOEIC 
score. The solid line represents C0 opt, which is the 
system?s TOEIC score using the optimized test in 
each iteration. The dotted line above the solid line 
represents the value of C0 opt + Iopt, and the dotted 
line below the solid line C0 opt - Iopt. 
One of the groups is used to optimize the test set. 
The other group is used for the translation paired 
comparison method. We use the term 
?optimization group? to refer to the first group and 
?evaluation group? to refer to the second group. 
Figure 6 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows? t opt. Three kinds of experimental results 
are shown in this figure. In each of three 
experiments, the translation paired comparison is 
conducted by the evaluation group. The differences 
                                                          
3 ? opt is different from? iteration. ? opt is calculated based on 
1?  opt and 2?  opt (not 1?  and 2? ) for the optimized test set.  
Figure 8: Relationship between iteration and 
t opt ?
0 50 100 150 200 250 300
0
5
10
15
20
25
30
Iteration
?
t
 
o
p
t
    Random selection (Averaging of 10 trials)
   Optimzed for TDM T
   Optimzed for ATR-M ATRIX
0 50 100 150 200 250 300
550
60
650
70
750
800
850
Iteration
C 0
 
opt
Random selection (Averaging of 10 trials) 
Optimized (Open)
Optimized (Closed)
Figure 7: Relationship between iteration and 
C0 opt 
among the three experiments are in the group to be 
used for optimization of the test set or the method 
used to reduce it. The double line represents the 
closed result using the test set, optimized on the 
evaluation group. The solid line represents the 
open result using the test set, optimized on the 
optimization group. The broken line represents the 
result using the test set, which is reduced by 
randomly removing test utterances one-by-one. 
The actual plotted broken line is averaged over 10 
random trials.  
0 50 100 150 200 250 300
460
480
500
520
540
560
580
Iteration
C 0
 
opt
   Optimized for TDM T
   Optimzed for ATR-M ATRIX
As shown in Figure 6, in the random selection 
result, t opt is on the rise. On the other hand, the 
open result is on the decline. 
?
Figure 7 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration 
and the vertical axis the TOEIC score. The 
denotation of each line is the same as that in Figure 
6. The error bar from the broken line represents 
? random, which is the standard deviation of the 
system?s TOEIC score over 10 random trials. 
Figure 9: Relationship between iteration and 
C0 opt 
In Figure 7, considering ? random, C0 opt of the 
open evaluation is more approximate to C0 than 
that of random selection, whereas C0 opt of the 
closed evaluation is much more approximate to C0. 
4.3 Experiment on ATR-MATRIX 
To be of actual use, the test set optimized for some 
system must be applicable for evaluation of other 
systems. In this subsection, we show the results of 
an experiment aimed at verifying this requirement 
is met. In this experiment, we apply the test set, 
which is optimized for TDMT, to evaluate ATR-
MATRIX. The experimental conditions are the 
same as in Section 4.1, except for the evaluation 
target. The results are shown in Figure 8 and 
Figure 9. 
Figure 8 shows the relationship between 
iteration and ? t opt. In this figure, the horizontal 
axis represents the iteration and the vertical axis 
shows ? t opt. The double line represents the result 
using the test set, optimized for ATR-MATRIX. 
The solid line represents the result using the test 
set, optimized for TDMT. The broken line 
represents the result using the test set, which is 
reduced by randomly removing test utterances one- 
6 Conclusions by-one. The actual plotted broken line is averaged 
over 10 random trials. 
We proposed a test set selection method for 
evaluating a speech translation system.  This 
method optimizes and drastically reduces the test 
set required by the translation paired comparison 
method. 
Figure 9 shows the relationship between 
iteration and the system?s TOEIC score. In this 
figure, the horizontal axis represents the iteration, 
and the vertical axis TOEIC score. The broken line 
and the solid line are plotted using the same 
denotation as that in Figure 8. Translation paired comparison is an effective 
method for measuring a system?s performance as a 
TOEIC score. However, this method has excessive 
evaluation costs. Therefore, cost reduction for this 
evaluation method is an important subject for study. 
In Figure 8, the solid line always lies on a lower 
position than the broken line. In Figure 9, from 
iteration 1 to around iteration 200, the broken line 
does not deviate from the actual system?s TOEIC 
score, which is 548. We applied the proposed method in an evaluation 
of ATR-MATRIX. Experimental results showed 
the effectiveness of the proposed method. This 
method reduced evaluation costs by more than  
60% and also improved the reliability of the 
evaluation result. 
Considering these results, the test set optimized 
for TDMT is shown to be applicable for evaluating 
ATR-MATRIX. 
5 Discussion 
Acknowledgement In this section, we discuss the experimental results 
shown in Section 4.  
The research reported here was supported in part 
by a contract with the Telecommunications 
Advancement Organization of Japan entitled, "A 
study of speech dialogue translation technology 
based on a large corpus." 
Looking at the broken lines in Figure 6 and 
Figure 8, test set reduction using random selection 
always causes an increase of ? t opt i.e., an increase 
in the scale of confidence interval. Therefore, this 
method causes the reliability of the evaluation 
result to deteriorate. Meanwhile, in the case of 
using the proposed method, looking at the solid 
lines on these figures, ? t opt is on the decline until 
around iteration 200. This means that we can 
achieve a more reliable evaluation result with a 
lower evaluation cost than when using the original 
test set. Here, looking at the solid lines in Figure 7 
and Figure 9, the Co opt system?s TOEIC score is 
nearly stable until iteration 200, and it does not 
deviate from Co. As mentioned before, Co for 
Figure 7 is 708 and Co for Figure 9 is 548. 
References 
Morimoto, T., Uratani, N., Takezawa, T., Furuse, 
O., Sobashima, Y., Iida, H., Nakamura, A., 
Sagisaka, Y., Higuchi, N. and Yamazaki, Y.  
1994. A speech and language database for 
speech translation research. In Proceedings of 
ICSLP `94, pages 1791-1794. 
Sugaya, F., Takezawa, T., Yokoo, A., Sagisaka, Y. 
and Yamamoto, S. 2000. Evaluation of the 
ATR-MATRIX Speech Translation System with 
a Pair Comparison Method between the System 
and Humans. In Proceedings of ICSLP 2000, 
pages 1105-1108. 
Considering these results, the proposed method 
can reduce the 330-utterance test set to a 130- 
utterance test set while reducing the scale of 
confidence interval. In other words, the proposed 
method both reduces evaluation costs by 60% and 
improves  reliability of the evaluation result. 
Sumita, E., Yamada, S., Yamamoto K., Paul, M., 
Kashioka, H., Ishikawa, K. and Shirai, S. 1999. 
Solutions to Problems Inherent in Spoken-
language Translation: The ATR-MATRIX 
Approach. In Proceedings of MT Summit `99, 
pages 229-235. 
Looking at Equations (4) and (5), the scale of 
confidence interval is also influenced by n.  When 
we allow the scale of confidence interval obtained 
from the original test set, we can use the proposed 
method?s reduction effect of ? t to compensate the 
? t 's increase by reducing n.  In this case, the 
actual achievable cost reduction will be more than 
60%.  
Takezawa, T. 1999. Building a bilingual travel 
conversation database for speech recognition 
research. In Proceedings of Oriental COCOSDA 
Workshop, pages 17-20. 
Takezawa, T., Morimoto, T., Sagisaka, Y., 
Campbell, N., Iida., H., Sugaya, F., Yokoo, A. 
and Yamamoto, S. 1998. A Japanese-to-English 
speech translation system: ATR-MATRIX. In 
Proceedings of ICSLP 1998, pages 2779-2782. 
371
372
373
374
375
376
377
378
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 161?164,
New York, June 2006. c?2006 Association for Computational Linguistics
Using the Web to Disambiguate Acronyms 
Eiichiro Sumita1, 2 
1 NiCT 
2 ATR SLC 
Kyoto 619-0288, JAPAN 
eiichiro.sumita@atr.jp 
Fumiaki Sugaya3 
3 KDDI R&D Labs 
 
Saitama 356-8502, JAPAN 
fsugaya@kddilabs.jp 
 
 
 
 
Abstract 
This paper proposes an automatic method 
for disambiguating an acronym with mul-
tiple definitions, considering the context 
surrounding the acronym. First, the 
method obtains the Web pages that in-
clude both the acronym and its definitions. 
Second, the method feeds them to the ma-
chine learner. Cross-validation tests re-
sults indicate that the current accuracy of 
obtaining the appropriate definition for an 
acronym is around 92% for two ambigu-
ous definitions and around 86% for five 
ambiguous definitions. 
1 Introduction 
Acronyms are short forms of multiword expres-
sions (we call them definitions) that are very con-
venient and commonly used, and are constantly 
invented independently everywhere. What each 
one stands for, however, is often ambiguous. For 
example, ?ACL? has many different definitions, 
including ?Anterior Cruciate Ligament (an in-
jury),? ?Access Control List (a concept in com-
puter security),? and ?Association for 
Computational Linguistics (an academic society).? 
People tend to write acronyms without their defini-
tion added nearby (Table 1), because acronyms are 
used to avoid the need to type long expressions. 
Consequently, there is a strong need to disambigu-
ate acronyms in order to correctly analyze or re-
trieve text. It is crucial to recognize the correct 
acronym definition in information retrieval such as 
a blog search. Moreover, we need to know the 
meaning of an acronym to translate it correctly. To 
the best of our knowledge, no other studies have 
approached this problem. 
 
 
Figure 1 Acronyms and their definitions co-
occur in some pages of the Web 
 
On the other side of the coin, an acronym 
should be defined in its neighborhood. For instance, 
one may find pages that include a certain acronym 
and its definition (Figure 1).  
First, our proposed method obtains Web pages 
that include both an acronym and its definitions. 
Second, the method feeds them to the machine 
learner, and the classification program can deter-
mine the correct definition according to the context 
information around the acronym in question.  
  
Definition 1 Anterior Cruciate Ligament http://www.ehealthmd.com/library/acltears 
She ended up with a torn ACL, MCL and did some other damage to her knee. (http://aphotofreak.blogspot.com/2006/01/ill-
give-you-everything-i-have-good.html) 
Definition 2 Access Control List http://en.wikipedia.org/wiki 
Calculating a user?s effective permissions requires more than simply looking up that user?s name in the ACL. 
(http://www.mcsa-exam.com/2006/02/02/effective-permissions.html) 
Definition 3 Association for Computational Linguistics http://www.aclweb.org/ 
It will be published in the upcoming leading ACL conference. (http://pahendra.blogspot.com/2005/06/june-14th.html) 
Table 1 Acronym ?ACL? without its definition in three different meanings found in blogs 
161
Here, we assume that the list of possible defi-
nitions for an acronym is given from sources ex-
ternal to this work. Listing pairs of acronyms and 
their original definitions, on which many studies 
have been done, such as Nadeau and Turney 
(2005), results in high performance. Some sites 
such as http://www.acronymsearch.com/ or 
http://www.findacronym.com/ provide us with 
this function. 
This paper is arranged as follows. Section 2 
explains our solution to the problem, and Section 
3 reports experimental results. In Sections 4 and 5 
we follow with some discussions and related 
works, and the paper concludes in Section 6. 
2 The proposal 
The idea behind this proposal is based on the ob-
servation that an acronym often co-occurs with its 
definition within a single Web page (Figure 1). 
For example, the acronym ACL co-occurs with 
one of its definitions, ?Association for Computa-
tional Linguistics,? 211,000 times according to 
google.com.  
Our proposal is a kind of word-sense disam-
biguation (Pedersen and Mihalcea, 2005). The hit 
pages can provide us with training data for disam-
biguating the acronym in question, and the snip-
pets in the pages are fed into the learner of a 
classifier. Features used in classification will be 
explained in the latter half of this subsection. 
We do not stick to a certain method of machine 
learning; any state-of-the-art method will suffice. 
In this paper we employed the decision-tree learn-
ing program provided in the WEKA project.  
 
Collecting the training data from the Web 
Our input is the acronym in question, A, and the 
set of its definitions, {Dk | k=1~K}.  
 
for all k =1~K do 
1. Search the Web using query of 
?A AND Dk.? 
2. Obtain the set of snippets, {Sl 
(A, Dk)| l=1~L}. 
3. Separate Dk from Sl and obtain 
the set of training 
data,{(Tl(A), Dk)| l=1~L}. 
End 
In the experiment, L is set to 1,000. Thus, we 
have for each definition Dk of A, at most 1,000 
training data.  
Training the classifier 
From training data Tl(A), we create feature vec-
tors, which are fed into the learner of the decision 
tree with correct definition Dk for the acronym A.  
Here, we write Tl(A) as W-m W-(m-1) ... W-2 W-1 
A W1 W2 ... Wm-1 Wm, where m is from 2 to M, 
which is called the window size hereafter.  
We use keywords within the window of the 
snippet as features, which are binary, i.e., if the 
keyword exists in Tl(A), then it is true. Otherwise, 
it is null.  
Keywords are defined in this experiment as the 
top N frequent words 1, but for A in the bag con-
sisting of all words in {Tl(A)}. For example, key-
words for ?ACL? are ?Air, Control, and, 
Advanced, Agents, MS, Computational, Akumiitti, 
Cruciate, org, of, CMOS, Language, BOS, Agent, 
gt, HTML, Meeting, with, html, Linguistics, List, 
Active, EOS, USA, is, access, Adobe, ACL, ACM, 
BETA, Manager, list, Proceedings, In, A, League, 
knee, Anterior, ligament, injuries, reconstruction, 
injury, on, The, tears, tear, control, as, a, Injury, lt, 
for, Annual, Association, Access, An, that, this, 
may, an, you, quot, in, the, one, can, This, by, or, 
be, to, Logic, 39, are, has, 1, from, middot.?  
3 Experiment 
3.1 Acronym and definition preparation 
We downloaded a list of acronyms in capital let-
ters only from Wikipedia and filtered them by 
eliminating acronyms shorter than three letters. 
Then we obtained definitions for each acronym 
from http://www.acronymsearch.com/ and dis-
carded acronyms that have less than five defini-
tions. Finally, we randomly selected 20 acronyms.  
We now have 20 typical acronyms whose am-
biguity is more than or equal to five. For each ac-
ronym A, a list of definitions { Dk  | k=1~K 
K>=5 }, whose elements are ordered by the count 
of page including A and Dk, is used for the ex-
periment.  
                                                          
1 In this paper, N is set to 100. 
162
3.2 Ambiguity and accuracy 
Here we examine the relationship between the 
degree of ambiguity and classification accuracy 
by using a cross-validation test for the training 
data. 
 
#Class M=2 M=5 M=10 Base 
2 88.7% 90.1% 92.4% 82.3%
Table 2 Ambiguity of two 
 
#Class M=2 M=5 M=10 Base 
5 78.6% 82.6% 86.0% 76.5%
Table 3 Ambiguity of five 
Ambiguity of two 
The first experiment was performed with the se-
lected twenty acronyms by limiting the top two 
most frequent definitions. Table 2 summarizes the 
ten-fold cross validation. While the accuracy 
changes acronym by acronym, the average is high 
about 90% of the time. The M in the table denotes 
the window size, and the longer the window, the 
higher the accuracy.  
The ?base? column displays the average accu-
racy of the baseline method that always picks the 
most frequent definition. The proposed method 
achieves better accuracy than the baseline. 
Ambiguity of five 
Next, we move on to the ambiguity of five (Table 
3). As expected, the performance is poorer than 
the abovementioned case, though it is still high, 
i.e., the average is about 80%. Other than this, our 
observations were similar to those for the ambigu-
ity of two. 
 
20.00%
30.00%
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
CEC POP SALT PAL PCI MIPS INT LSD HID RFC BBC UDP WAP ITU VDT NBA CRT JCB EFT ISP
Acronyms
C
la
ss
ifi
ca
tio
n 
ac
cu
ra
cy
Proposed (W = 10) Base
 
Figure 2 Bias in distribution of definitions (ambiguity of 5)
4 Discussion on biased data  
4.1 Problem caused by biased distribution 
and a countermeasure against it 
For some words, the baseline is more accurate 
than the proposed method because the baseline 
method reaches all occurrences on the Web thanks 
to the search engine, whereas our method limits 
the number of training data by L as mentioned in 
Section 2. The average quantity of training data 
was about 830 due to the limit of L, 1,000. The 
distribution of these training data is rather flat. 
This causes our classifier to fail in some cases. 
For example, for the acronym ?ISP,? the most fre-
quent definition out of five has a share of 99.9% 
(Table 4) on the Web, whereas the distribution in 
the training data is different from the sharp distri-
bution. Thus, our classification accuracy is not as 
good as that of the baseline. 
Considering the acronym ?CEC,? the most fre-
quent out of five definitions has the much smaller 
share of 26.3% on the Web (Table 5), whereas the 
163
distribution in the training data is similar to the 
flat distribution of real data. Furthermore, the de-
cision tree learns the classification well, whereas 
the baseline method performs terribly. 
These two extreme cases indicate that for some 
acronyms, our proposed method is beaten by the 
baseline method. The slanting line in Figure 2 
shows the baseline performance compared with 
our proposed method. In the case where our 
method is strong, the gain is large, and where our 
method is weak, the reduction is relatively small. 
The average performance of our proposed method 
is higher than that of the baseline. 
 
Definition Page hits
Internet Service Provider 3,590,000
International Standardized Profile 776
Integrated Support Plan 474
Interactive String Processor 287
Integrated System Peripheral control 266
Table 4 Sharp distribution for ?ISP? 
 
Definition Page hits
California Energy Commission 161,000
Council for Exceptional Children 159,000
Commission of the European Communities 138,000
Commission for Environmental Cooperation 77,400
Cation Exchange Capacity 76,400
Table 5 Flat distribution for ?CEC? 
 
A possible countermeasure to this problem 
would be to incorporate prior probability into the 
learning process. 
4.2 Possible dissimilarity of training and real 
data 
The training data used in the above experiment 
were only the type of snippets that contain acro-
nyms and their definitions; there is no guarantee 
for documents that contain only acronyms are 
similar to the training data. Therefore, learning is 
not necessarily successful for real data. However, 
we tested our algorithm for a similar problem in-
troduced in Section 5.1, where we conducted an 
open test and found a promising result, suggesting 
that the above-mentioned fear is groundless.  
5 Related works 
5.1 Reading proper names 
The contribution of this paper is to propose a 
method to use Web pages for a disambiguation 
task. The method is applicable to different prob-
lems such as reading Japanese proper names 
(Sumita and Sugaya, 2006). Using a Web page 
containing a name and its syllabary, it is possible 
to learn how to read proper names with multiple 
readings in a similar way. The accuracy in our 
experiment was around 90% for open data.   
5.2 The Web as a corpus 
Recently, the Web has been used as a corpus in 
the NLP community, where mainly counts of hit 
pages have been exploited (Kilgarriff and Grefen-
stette, 2003). However, our proposal, Web-Based 
Language Modeling (Sarikaya, 2005), and Boot-
strapping Large Sense-Tagged corpora (Mihalcea, 
2002) use the content within the hit pages. 
6 Conclusion 
This paper proposed an automatic method of dis-
ambiguating an acronym with multiple definitions, 
considering the context. First, the method obtains 
the Web pages that include both the acronym and 
its definitions. Second, the method feeds them to 
the learner for classification. Cross-validation test 
results obtained to date indicate that the accuracy 
of obtaining the most appropriate definition for an 
acronym is around 92% for two ambiguous defini-
tions and around 86% for five ambiguous defini-
tions. 
References 
A. Kilgarriff and G. Grefenstette. 2003. ?Introduction 
to the special issue on the Web as a corpus,? Com-
putational Linguistics 29(3): 333-348. 
Rada. F. Mihalcea, 2002. ?Bootstrapping Large Sense-
Tagged Corpora,? Proc. of LREC, pp. 1407-1411. 
David Nadeau and Peter D. Turney, 2005. ?A super-
vised learning approach to acronym identification,? 
18th Canadian Conference on Artificial Intelligence, 
LNAI3501. 
Ted Pedersen and Rada. F. Mihalcea, ?Advances in 
Word Sense Disambiguation,? tutorial at ACL 2005. 
http://www.d.umn.edu/~tpederse/WSDTutorial.html. 
Ruhi Sarikaya, Hong-kwang Jeff Kuo, and Yuqing Gao, 
2005. Impact of Web-Based Language Modeling on 
Speech Understanding, Proc. of ASRU, pp. 268-271. 
Eiichiro Sumita and Fumiaki Sugaya,. 2006. ?Word 
Pronunciation Disambiguation using the Web,? Proc. 
of HLT-NAACL 2006. 
164
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 165?168,
New York, June 2006. c?2006 Association for Computational Linguistics
 Word Pronunciation Disambiguation using the Web 
Eiichiro Sumita1, 2 
1 NiCT 
2 ATR SLC 
Kyoto 619-0288, JAPAN 
eiichiro.sumita@atr.jp 
Fumiaki Sugaya3 
3 KDDI R&D Labs 
 
Saitama 356-8502, JAPAN 
fsugaya@kddilabs.jp 
 
 
 
 
 
Abstract 
This paper proposes an automatic method 
of reading proper names with multiple 
pronunciations. First, the method obtains 
Web pages that include both the proper 
name and its pronunciation. Second, the 
method feeds them to the learner for clas-
sification. The current accuracy is around 
90% for open data.  
1 Introduction 
Within text-to-speech programs, it is very impor-
tant to deal with heteronyms, that is, words that are 
spelt the same but that have different readings, e.g. 
"bow" (a ribbon) and "bow" (of a ship). Reportedly, 
Japanese text-to-speech programs read sentences 
incorrectly more than 10 percent of the time. This 
problem is mainly caused by heteronyms and three 
studies have attempted to solve it (Yarowsky, 
1996; Li and Takeuchi, 1997; and Umemura and 
Shimizu, 2000).  
They assumed that the pronunciation of a word 
corresponded directly to the sense tag or part-of-
speech of that word. In other words, sense tagging 
and part-of-speech tagging can determine the read-
ing of a word. However, proper names have the 
same sense tag, for example, ?location? for land-
marks and the same part-of-speech, the ?noun.? 
Clearly then, reading proper names is outside the 
scope of previous studies. Also, the proper names 
of locations, people, organizations, and others are 
dominant sources of heteronyms. Here, we focus 
on proper names. Our proposal is similar to previ-
ous studies in that both use machine learning. 
However, previous methods used expensive re-
sources, e.g., a corpus in which words are 
manually tagged according to their pronunciation. 
Instead, we propose a method that automatically 
builds a pronunciation-tagged corpus using the 
Web as a source of training data for word pronun-
ciation disambiguation. 
This paper is arranged as follows. Section 2 
proposes solutions, and Sections 3 and 4 report 
experimental results. We offer our discussion in 
Section 5 and conclusions in Section 6. 
2 The Proposed Methods 
It is crucial to correctly read proper names in open-
domain text-to-speech programs, for example, ap-
plications that read Web pages or newspaper 
articles. To the best of our knowledge, no other 
studies have approached this problem. In this paper, 
we focus on the Japanese language. In this section, 
we first explain the Japanese writing system (Sec-
tions 2.1), followed by our proposal, the basic 
method (Section 2.2), and the improved method 
(Section 2.3). 
2.1 The Japanese writing system 
First, we should briefly explain the modern Japa-
nese writing system. The Japanese language is rep-
resented by three scripts: 
[i] Kanji, which are characters of Chinese ori-
gin; 
[ii] Hiragana, a syllabary (reading); and 
[iii] Katakana, also a syllabary (reading). 
 
Script Sample 
KANJI ?? 
HIRAGANA (reading) ????? 
KATAKANA (reading) ????? 
Table 1 Three writings of a single word 
 
165
As exemplified in Table 1, there are three writ-
ings for the word ???.? The lower two sam-
ples are representations of the same pronunciation 
of ?oo daira.? 
Listing possible readings can be done by con-
sulting a dictionary (see Section 3.1 for the ex-
periment). Therefore, in this paper, we assume that 
listing is performed prior to disambiguation. 
2.2 The basic method based on page hits 
The idea is based on the observation that proper 
names in Kanji often co-occur with their pro-
nunciation in Hiragana (or Katakana) within a sin-
gle Web page, as shown Figure 1. In the figure, 
the name ???? in Kanji is indicated with an 
oval, and its pronunciation in Katakana, ????
??,? is high-lighted with the dotted oval.  
According to Google, there are 464 pages in 
which ???? and ??????? co-occur.   
In this sense, the co-occurrence frequency 
suggests to us the most common pronunciation.  
 
 
Figure 1 On the Web, words written in Kanji 
often co-occur with the pronunciation written in 
Katakana 1 
 
Our simple proposal to pick up the most fre-
quent pronunciation achieves surprisingly high 
accuracy for open data, as Section 4 will later show. 
2.3 The improved method using a classifier 
The basic method mentioned above merely selects 
the most frequent pronunciation and neglects all 
others. This is not disambiguation at all.  
The improved method is similar to standard 
word-sense disambiguation. The hit pages can pro-
                                                          
1 
http://oyudokoro.mimo.com/area/C/cd/tng/000370/index.html 
vide us with training data for reading a particular 
word. We feed the downloaded data into the 
learner of a classifier. We do not stick to a certain 
method of machine learning; any state-of-the-art 
method will work. The features used in classifica-
tion will be explained in the latter half of this sub-
section. 
Collecting training data from the Web 
Our input is a particular word, W, and the set of its 
readings, {Rk | k=1~K}. 
 
In the experiments for this report, L is set to 
1,000.  Thus, for each reading Rk of W, we have, at 
most 1,000 training data Tl(W).  
Training the classifier 
From the training data Tl(W), we make feature 
vectors that are fed into the learner of the decision 
tree with the correct reading Rk for the word in 
question, W. 
Here, we write Tl(W) as W-m W-(m-1) ... W-2 W-1 
W W1 W2 ... Wm-1 Wm, where m is from 2 to M, 
which hereafter is called the window size.  
 
We use two kinds of features: 
z The part-of-speech of W-2 W-1 and W1 W2 
z Keywords within the snippet. In this ex-
periment, keywords are defined as the top 
N frequent words, but for W in the bag 
consisting of all words in {Tl(W)}. 
 
In this paper, N is set to 100. These features 
ground the pronunciation disambiguation task to 
the real world through the Web. In other words, 
they give us knowledge about the problem at hand, 
i.e., how to read proper names in a real-world con-
text. 
3 Experimental Data 
We conducted the experiments using proper loca-
tion names. 
For all k =1~K: 
i) search the Web using the query ?W AND 
Rk.? 
ii) obtain the set of snippets, {Sl (W, Rk)| 
l=1~L}. 
iii) separate Rk from Sl and obtain the set of 
training data,{(Tl(W), Rk)| l=1~L}. 
end 
166
3.1 Ambiguous name lists 
Japan Post openly provides postal address lists 
associated with pronunciations .  
From that list, we extracted 79,861 pairs of 
proper location names and their pronunciations. As 
the breakdown of Table 2 shows, 5.7% of proper 
location names have multiple pronunciations, 
while 94.3% have a single pronunciation. The av-
erage ambiguity is 2.26 for ambiguous types. Next, 
we took into consideration the frequency of each 
proper name on the Web. Frequency is surrogated 
by the page count when the query of a word itself 
is searched for using a search engine. About one 
quarter of the occurrences were found to be am-
biguous. 
 
Number of 
readings 
type %
1 70,232 94.3
2 3,443 
3 599 
4 150 
5 45 
6 11 
7 4 
8 2 
11 1 
5.7
total 74,487 100.0
Table 2 Pronunciation ambiguities in Japanese 
location names 
 
Our proposal depends on co-occurrences on a 
Web page. If the pairing of a word W and its read-
ing R do not occur on the Web, the proposal will 
not work. We checked this, and found that there 
was only one pair missing out of the 79,861 on our 
list. In this sense, the coverage is almost 100%. 
3.2 Open Data 
We tested the performance of our proposed meth-
ods on openly available data. 
Open data were obtained from the EDR corpus, 
which consists of sentences from Japanese news-
papers. Every word is tagged with part-of-speech 
and pronunciation.  
We extracted sentences that include location 
heteronyms, that is, those that contain Kanji that 
can be found in the above-mentioned list of loca-
tion heteronyms within the postal address data. 
There were 268 occurrences in total. There were 
72 types of heteronyms. 
4 Experiment Results 
We conducted two experiments: (1) an open test; 
and (2) a study on the degree of ambiguity. 
4.1 Open test 
We evaluated our proposals, i.e., the basic method 
and the improved method with the open data ex-
plained in Section 3.1. Both methods achieved a 
high rate of accuracy. 
 
Basic method performance  
In the basic method, the most common pronuncia-
tion on the Web is selected. The frequency is esti-
mated by the page count of the query for the 
pairing of the word W and its pronunciation, Ri.  
There are two variations based on the Hiragana 
and Katakana pronunciation scripts. The average 
accuracy for the open data was 89.2% for Hiragana 
and 86.6% for Katakana (Table 3). These results 
are very high, suggesting a strong bias of pronun-
ciation distribution in the open data.  
 
Scripts Accuracy 
HIRAGANA 89.2 
KATAKANA 86.6 
Table 3 Open test accuracy for the basic method 
 
Performance of the improved method 
Table 4 shows the average results for all 268 
occurrences. The accuracy of the basic method 
(Table 3) was lower than that of our improved 
proposal in all window sizes, and it was outper-
formed at a window size of ten by about 3.5% for 
both Hiragana and Katakana.  
 
Script M=2 M=5 M=10
HIRAGANA 89.9 90.3 92.9 
KATAKANA 89.2 88.4 89.9 
Table 4 Open test accuracy for the improved 
method 
 
167
4.2 Degree of ambiguity 
Here, we examine the relationship between the 
degree of pronunciation ambiguity and pronuncia-
tion accuracy using a cross-validation test for train-
ing data2 for the improved method with Hiragana. 
Average case 
We conducted the first experiment with twenty 
words 3 that were selected randomly from the Am-
biguous Name List (Section 3.1). The average am-
biguity was 2.1, indicating the average 
performance of the improved proposal.  
 
Class M=2 M=5 M=10 basic 
2.1 89.2 %  90.9 %  92.3 % 67.5%
Table 5 Average cases 
 
Table 5 summarizes the ten-fold cross valida-
tion, where M in the table is the training data size 
(window size). The accuracy changes word by 
word, though the average was high about 90% of 
the time.  
The ?basic? column shows the average accu-
racy of the basic method, i.e., the percentage for 
the most frequent pronunciation. The improved 
method achieves much better accuracy than the 
?basic? one. 
The most ambiguous case 
Next, we obtained the results (Table 6) for the 
most ambiguous cases, where the degree of ambi-
guity ranged from six to eleven4. The average am-
biguity was 7.1.  
 
Class M=2 M=5 M=10 basic 
7.1 73.9 %  77.3 %  79.9 % 57.5%
Table 6 Most ambiguous cases 
 
                                                          
2 There is some question as to whether the training data cor-
rectly catch all the pronunciations. The experiments in this 
subsection are independent of this problem, because our inten-
tion is to compare the performance of the average case and the 
most ambiguous case. 
3???, ???, ???, ?? ,???, ??, ???, ??
?, ???, ??, ???, ??, ???, ???, ????, 
???, ??, ???, ???, ???. 
4??, ???, ??, ??, ??, ??, ??, ??, ??, ?
??, ??, ???, ???, ???, ???, ???, ??
?, ??. 
As we expected, the performances were poorer 
than the average cases outlined above, although 
they were still high, i.e., the average ranged from 
about 70% to about 80 %. Again, the improved 
method achieved much better accuracy than the 
?basic? method. 5 
5 Discussion on Transliteration 
Transliteration (Knight and Graehl, 1998) is a 
mapping from one system of writing into another, 
automation of which has been actively studied be-
tween English and other languages such as Arabic, 
Chinese, Korean, Thai, and Japanese. If there are 
multiple translation candidates, by incorporating 
context in a way similar to our proposal, one will 
be able to disambiguate them. 
6 Conclusion 
This paper proposed a new method for reading 
proper names. In our proposed method, using Web 
pages containing Kanji and Hiragana (or Katakana) 
representations of the same proper names, we can 
learn how to read proper names with multiple read-
ings via a state-of-the-art machine learner. Thus, 
the proposed process requires no human interven-
tion. The current accuracy was around 90% for 
open data.     
References 
K. Knight and J. Graehl. 1998 Machine transliteration. 
Computational Linguistics, 24(4):599-612.  
H. Li and J. Takeuchi. 1997. Using Evidence that is 
both string and Reliable in Japanese Homograph Dis-
ambiguation, SIGNL119-9, IPSJ. 
Y. Umemura and T. Shimizu. 2000. Japanese homo-
graph disambiguation for speech synthesizers, Toy-
ota Chuo Kenkyujo R&D Review, 35(1):67-74. 
D. Yarowsky. 1996. Homograph Disambiguation in 
Speech Synthesis. In J. van Santen, R. Sproat, J. 
Olive and J. Hirschberg (eds.), Progress in Speech 
Synthesis. Springer-Verlag, pp. 159-175. 
                                                          
5 For some words, the basic accuracy is higher than the cross 
validation accuracy because the basic method reaches all oc-
currences on the Web thanks to the search engine, while our 
improved method limits the number of training data by L in 
Section 2.3. For example, the most frequent pronunciation of 
???? has 93.7% on the Web, whereas the distribution in the 
training data is different from such a sharp distribution due to 
the limitation of L. 
168
Automatic Measuring of English Language Proficiency using MT
Evaluation Technology
Keiji Yasuda
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
keiji.yasuda@atr.jp
Fumiaki Sugaya
KDDI R&D Laboratories
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
fsugaya@kddilabs.jp
Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Department of NLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
eiichiro.sumita@atr.jp
Toshiyuki Takezawa
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
toshiyuki.takezawa@atr.jp
Genichiro Kikui
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
genichiro.kikui@atr.jp
Seiichi Yamamoto
ATR Spoken Language Translation
Research Laboratories
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
seiichi.yamamoto@atr.jp
Abstract
Assisting in foreign language learning is one of
the major areas in which natural language pro-
cessing technology can contribute. This paper
proposes a computerized method of measuring
communicative skill in English as a foreign lan-
guage. The proposed method consists of two
parts. The first part involves a test sentence
selection part to achieve precise measurement
with a small test set. The second part is the ac-
tual measurement, which has three steps. Step
one asks proficiency-known human subjects to
translate Japanese sentences into English. Step
two gauges the match between the translations
of the subjects and correct translations based
on the n-gram overlap or the edit distance be-
tween translations. Step three learns the rela-
tionship between proficiency and match. By re-
gression it finds a straight-line fitting for the
scatter plot representing the proficiency and
matches of the subjects. Then, it estimates pro-
ficiency of proficiency-unknown users by using
the line and the match. Based on this approach,
we conducted experiments on estimating the
Test of English for International Communica-
tion (TOEIC) score. We collected two sets of
data consisting of English sentences translated
from Japanese. The first set consists of 330 sen-
tences, each translated to English by 29 subjects
with varied English proficiency. The second set
consists of 510 sentences translated in a similar
manner by a separate group of 18 subjects. We
found that the estimated scores correlated with
the actual scores.
1 Introduction
For effective second language learning, it is ab-
solutely necessary to test proficiency in the sec-
ond language. This testing can help in selecting
educational materials before learning, checking
learners? understanding after learning, and so
on.
To make learning efficient, it is important to
achieve testing with a short turnaround time.
Computer-based testing is one solution for this,
and several kinds of tests have been developed,
including CASEC (CASEC, 2004) and TOEFL-
CBT (TOEFL, 2004). However, these tests are
mainly based on cloze testing or multiple-choice
questions. Consequently, they require labour
costs for expert examination designers to make
the questions and the alternative ?detractor?
answers.
In this paper, we propose a method for the au-
tomatic measurement of English language pro-
ficiency by applying automatic evaluation tech-
niques. The proposed method selects adequate
test sentences from an existing corpus. Then,
it automatically evaluates the translations of
test sentences done by users. The core tech-
nology of the proposed method, i.e., the auto-
matic evaluation of translations, was developed
in research aiming at the efficient development
of Machine Translation (MT) technology (Su et
al., 1992; Papineni et al, 2002; NIST, 2002).
In the proposed method, we apply these MT
evaluation technologies to the measurement of
human English language proficiency. The pro-
posed method focuses on measuring the commu-
nicative skill of structuring sentences, which is
indispensable for writing and speaking. It does
not measure elementary capabilities including
vocabulary or grammar. This method also pro-
poses a test sentence selection scheme to enable
efficient testing.
Section 2 describes several automatic evalua-
tion methods applied to the proposed method.
Section 3 introduces the proposed evaluation
scheme. Section 4 shows the evaluation results
obtained by the proposed method. Section 5
concludes the paper.
2 MT Evaluation Technologies
In this section, we briefly describe automatic
evaluation methods of translation. These meth-
ods were proposed to evaluate MT output, but
they are applicable to translation by humans.
All of these methods are based on the same
idea, that is, to compare the target transla-
tion for evaluation with high-quality reference
translations that are usually done by skilled
translators. Therefore, these methods require a
corpus of high-quality human reference transla-
tions. We call these translations as ?references?.
2.1 DP-based Method
The DP score between a translation output and
references can be calculated by DP matching
(Su et al, 1992; Takezawa et al, 1999). First,
we define the DP score between sentence (i.e.,
word array) Wa and sentence Wb by the follow-
ing formula.
SDP (Wa,Wb) = T ? S ? I ?DT (1)
where T is the total number of words in Wa, S is
the number of substitution words for comparing
Wa to Wb, I is the number of inserted words for
comparing Wa to Wb, and D is the number of
deleted words for comparing Wa to Wb.
Using Equation 1, (Si(j)), that is, the test
sentence unit DP-score of the translation of test
sentence j done by subject i, can be calculated
by the following formula.
SDPi(j) =
max
k=1 to Nref
{
SDP (Wref(k)(j),Wsub(i)(j)), 0
}
(2)
where Nref is the number of references,
Wref(k)(j) is the k-th reference of the test sen-
tence j, and Wsub(i)(j) is the translation of the
test sentence j done by subject i.
Finally, SDPi , which is the test set unit DP-
score of subject i, can be calculated by the fol-
lowing formula.
SDPi =
1
Nsent
Nsent?
j=1
SDPi(j) (3)
where Nsent is the number of test sentences.
2.2 N-gram-based Method
Papineni et al (2002) proposed BLEU, which is
an automatic method for evaluating MT qual-
ity using N -gram matching. The National Insti-
tute of Standards and Technology also proposed
an automatic evaluation method called NIST
(2002), which is a modified method of BLEU.
In this research we use two kinds of units to
apply BLEU and NIST. One is a test sentence
unit and the other is a test set unit. The unit of
utterance corresponds to the unit of ?segment?
in the original BLEU and NIST studies (Pap-
ineni et al, 2002; NIST, 2002).
Equation 4 is the test sentence unit BLEU
score formulation of the translation of test sen-
tence j done by subject i.
SBLEUi (j) =
exp
{ N?
n=1
wn log(pn)?max
(L?ref
Lsys ? 1, 0
)}
(4)
where
pn =?
C?{Candidates}
?
n?gram?{C} Countclip (n?gram)?
C?{Candidates}
?
n?gram?{C} Count(n?gram)
wn = N?1
and
L?ref = the number of words in the reference
translation that is closest in length to the
translation being scored
Lsys = the number of words in the transla-
tion being scored
Equation 5 is the test sentence unit NIST
score formulation of the translation of test sen-
tence j done by subject i.
SNISTi(j) =
?N
n=1
{?
all w1...wn in sys output
info(w1...wn)?
all w1...wn in sys output
(1)
}
?exp
{
? log2
[
min
(
Lsys
Lref , 1
)]}
(5)
where
info(w1 . . . wn) =
log2
( the number of occurence of w1...wn?1
the number of occurence of w1...wn
)
Lref = the average number of words in a ref-
erence translation, averaged over all refer-
ence translations
Lsys = the number of words in the transla-
tion being scored
and ? is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the sys-
tem translation is 2/3 of the average number
of words in the reference translation. For Equa-
tions 4 and 7, N indicates the maximum n-gram
length. In this research we set N to 4 for BLEU
and to 5 for NIST.
We may consider the unit of the test set cor-
responding to the unit of ?document? or ?sys-
tem? in BLEU and NIST. However, we use for-
mulations for the test set unit scores that are
different from those of the original BLEU and
NIST.
Calculate correlation 
between TOEIC score and 
sentence unit automatic score
References translated
by bilinguals
English writing by 
proficiency-known
human subjects
English sentences
by proficiency
Japanese test set
Automatic evaluation
(sentence unit evaluation)
Corpus
Select test sentences
based on correlation
Figure 1: Flow of Test Set Selection
The test set unit scores of BLEU and NIST
are calculated by Equations 6 and 7.
SBLEUi =
1
Nsent
Nsent?
j=1
SBLEUi(j) (6)
SNISTi =
1
Nsent
Nsent?
j=1
SNISTi(j) (7)
3 The Proposed Method
The proposed method described in this paper
consists of two parts. One is the test set selec-
tion part and the other is the actual measure-
ment part. The measurement part is divided
into two phases: a parameter-estimation phase
and a testing phase. Here, we use the term ?sub-
jects? to refer to the human subjects in the test
set selection part and the parameter-estimation
phase of the measurement part; we use ?users?
to refer to the humans in the testing phase of
the measurement part.
Regression analysis using
proficiency and automatic
scores
References translated
by bilinguals
English writing by 
proficiency-known 
human subjects
English sentences
by proficiency
Japanese test set
Regression 
coefficient
Automatic evaluation
(Test set unit evaluation)
English writing by a user
Automatic evaluation
Estimation of English
proficiency
English sentences
Automatic score
English
proficiency
?Testing phase?
Corpus
?Parameter-estimation phase?
Figure 2: Flow of English Proficiency Measurment
We employ the Test of English for Interna-
tional Communication (TOEIC, 2004) as an ob-
jective measure of English proficiency.
3.1 Test Sentence Selection Method
Figure 1 shows the flow of the test sentence se-
lection. We first calculate the test sentence
unit automatic score by using Equation 2, 4 or
5 for each test sentence and subject. Second,
for each test sentence, we calculate the correla-
tion between the automatic scores and subjects?
TOEIC scores. Finally, using the above results,
we choose the test sentences that give high cor-
relation.
3.2 Method of Measuring English
Proficiency
Figure 2 shows the flow of measuring English
proficiency. In the parameter-estimation phase,
for each subject, we first calculate the test set
unit automatic score by using Equation 3, 6 or
7. Next, we apply regression analysis using the
automatic scores and subjects? TOEIC scores.
In the testing phase, we calculate a user?s
TOEIC score using the automatic score of the
user and the regression line calculated in the
parameter-estimation phase.
4 Experiments
4.1 Experimental Conditions
4.1.1 Test sets
For the experiments, we employ two differ-
ent test sets. One is BTEC (Basic Travel
Expression Corpus) (Takezawa et al, 2002)
and the other is SLTA1 (Takezawa, 1999).
Both BTEC and SLTA1 are parts of bilingual
corpora that have been collected for research
on speech translation systems. However, they
have different features. A detailed analysis
of these corpora was done by Kikui et al
(2003). Here, we briefly explain these test sets.
In this study, we use the Japanese side as a
test set and the English side as a reference for
automatic evaluation.
BTEC
BTEC was designed to cover expressions for
every potential subject in travel conversation.
This test set was collected by investigating
?phrasebooks? that contain Japanese/English
sentence pairs that experts consider useful for
tourists traveling abroad. One sentence con-
tains 8 words on average. The test set for this
experiment consists of 510 sentences from the
BTEC corpus.
The total number of examinees is 18, and
the range of their TOEIC scores is between the
400s and 900s. Every hundred-point range has
3 examinees.
SLTA1
SLTA1 consists of 330 sentences in 23 conver-
sations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). One sen-
tence contains 13 words on average. This corpus
was collected by simulated dialogues between
Japanese and English speakers through a pro-
fessional interpreter. The topics of the conver-
sations are mainly hotel conversations, such as
reservations, enquiries and so on.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excluding the 600s, every hundred-
point range has 5 examinees.
4.1.2 Reference
For the automatic evaluation, we collected 16
references for each test sentence. One of them
is from the English side of the test set, and the
remaining 15 were translated by 5 bilinguals (3
references by 1 bilingual).
4.2 Experimental Results
4.2.1 Experimental Results of Test Set
Selection
Figures 3 and 4 show the correlation between
the test sentence unit automatic score and the
subjects? TOEIC score. Here, the automatic
score is calculated using Equation 2, 4 or 5. Fig-
ure 3 shows the results on BTEC, and Fig. 4
shows the results on SLTA1. In these fig-
ures, the ordinate represents the correlation.
The filled circles indicate the results using the
DP-based automatic evaluation method. The
gray circles indicate the results using BLEU.
The empty circles indicate the results using
NIST. Looking at these figures, we find that
the three automatic evaluation methods show
a similar tendency. Comparing BTEC and
SLTA1, BTEC contains more cumbersome test
sentences. In BTEC, about 20% of the test sen-
tences give a correlation of less than 0. Mean-
while, in the SLTA1, this percentage is about
10%.
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 3: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(BTEC)
Table 1 shows examples of low-correlated test
sentences. As shown in the table, BTEC con-
tains more short and frequently used expres-
sions than does SLTA1. This kind of expres-
sion is thought to be too easy for testing, so
this low-correlation phenomenon is thought to
occur. SLTA1 still contains a few sentences of
this kind (?Example 1? of SLTA1 in the ta-
ble). Additionally, there is another contributing
factor explaining the low correlation in SLTA1.
Looking at ?Example 2? of SLTA1 in the ta-
ble, this expression is not very easy to translate.
For this test sentence, several expressions can
be produced as an English translation. Thus,
automatic evaluation methods cannot evaluate
correctly due to the insufficient variety of ref-
erences. Considering these results, this method
can remove inadequate test sentences due not
only to the easiness of the test sentence but
also to the difficulty of the automatic evalua-
tion. Figures 5 and 6 show the relationship
between the number of test sentences and cor-
relation. This correlation is calculated between
the test set unit automatic scores and the sub-
jects? TOEIC scores. Here, the automatic score
is calculated using Equation 3, 6 or 7. Figure
5 shows the results on BTEC, and Fig. 6 shows
the results on SLTA1.
In these figures, the abscissa represents the
number of test sentences, i.e., Nsent in Equa-
tions 3, 6 and 7, and the ordinate represents
the correlation. Definitions of the circles are
the same as those in the previous figure. Here,
the test sentence selection is based on the cor-
relation shown in Figs. 3 and 4.
Comparing Fig. 5 to Fig. 6, in the case of
Table 1: Example of low-correlated test sentences
Japanese English
Example 1
???????
Good night.
Example 2
????????????
Can I see a menu, please?
Example 1
??????????????????
Yes, with my Mastercard please
Example 2
???????????????????????
????????????????????????
I wish I could take that but we have a limited budget so
how much will that cost?
S
L
T
A
1
B
T
E
C
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 4: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(SLTA1)
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 5: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(BTEC)
using the full test set (510 test sentences for
BTEC, 330 test sentences for SLTA1), the cor-
relation of BTEC is lower than that of SLTA1.
As we mentioned above, the ratio of the low-
correlated test sentences in BTEC is higher than
that of SLTA1 (See Figs. 3 and 4). This issue
is thought to cause a decrease in the correlation
shown in Fig. 5. However, by applying the se-
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 6: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(SLTA1)
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 7: Standard error (BTEC)
lection based on sentence unit correlation, these
obstructive test sentences can be removed. This
permits the selection of high-correlated small-
sized test sets. In these figures, the highest cor-
relations are around 0.95.
4.2.2 Experimental Results of English
Proficiency Measurement
For the experiments on English proficiency mea-
surement, we carried out a leave-one-out cross
validation test. The leave-one-out cross valida-
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 8: Standard error (SLTA1)
tion test is conducted not only for the measure-
ment of the English proficiency but also for the
test set selection.
To evaluate the proficiency measurement by
the proposed method, we calculate the standard
error of the results of a leave-one-out cross val-
idation test. The following formula is the defi-
nition of the standard error.
?E =
???? 1
Nuser
Nuser?
i=1
(Ti ?Ai)2 (8)
where Nuser is the number of users, Ti is the
actual TOEIC score of user i, and Ai is user i?s
estimated TOEIC score by using the proposed
method.
Figures 7 and 8 show the relationship between
the number of test sentences and the standard
error.
In these figures, the abscissa represents the
number of test sentences, and the ordinate rep-
resents the standard error. Definitions of the
circles are the same as in the previous figure.
Here, the test sentence selection is based on the
correlation shown in Figs. 3 and 4.
Looking at Figs. 7 and 8, we can observe dif-
ferences between the standard errors of BTEC
and SLTA1. This is thought to be due to the
difference of the number of subjects in the ex-
periments (for the leave-one-out cross valida-
tion test, 17 subjects with BTEC and 28 sub-
jects with SLTA1). Even though these were
closed experiments, the results in Figs. 5 and
6 show an even higher correlation with BTEC
than with SLTA1 at the highest point. There-
fore, there is room for improvement by increas-
ing the number of subjects with BTEC.
In the test using 30 to 60 test sentences in
Figs. 7 and 8, the standard errors are much
smaller than in the test using the full test set
(510 test sentences for BTEC, 330 test sentences
for SLTA1). These results imply that the test
set selection works very well and that it enables
precise testing using a smaller size test set.
5 Conclusion
We proposed an automatic measurement
method for English language proficiency. The
proposed method applies automatic MT evalu-
ation to measure human English language pro-
ficiency. This method focuses on measuring the
communicative skill of structuring sentences,
which is indispensable in writing and speaking.
However, it does not measure elementary capa-
bilities such as vocabulary and grammar. The
method also involves a new test sentence selec-
tion scheme to enable efficient testing.
In the experiments, we used TOEIC as an ob-
jective measure of English language proficiency.
We then applied some currently available auto-
matic evaluation methods: BLEU, NIST and a
DP-based method. We carried out experiments
on two test sets: BTEC and SLTA1. Accord-
ing to the experimental results, the proposed
method gave a good measurement result on a
small-sized test set. The standard error of mea-
surement is around 120 points on the TOEIC
score with BTEC and less than 100 TOEIC
points score with SLTA1. In both cases, the
optimum size of the test set is 30 to 60 test sen-
tences.
The proposed method still needs human
labour to make the references. To obtain higher
portability, we will apply an automatic para-
phrase scheme (Finch et al, 2002; Shimohata
and Sumita, 2002) to make the references auto-
matically.
6 Acknowledgements
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy entitled ?A study of speech dialogue trans-
lation technology based on a large corpus?.
References
CASEC. 2004. Computer Assessment
System for English Communication.
http://www.ets.org/toefl/.
A. Finch, T. Watanabe, and E. Sumita. 2002.
?Paraphrasing by Statistical Machine Trans-
lation?. In Proceedings of the 1st Forum on
Information Technology (FIT2002), volume
E-53, pages 187?188.
G. Kikui, E. Sumita, T. Takezawa, and
S. Yamamoto. 2003. ?Creating Corpora for
Speech-to-Speech Translation?. In Proceed-
ings of EUROSPEECH, pages 381?384.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt
/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics (ACL), pages 311?318.
M. Shimohata and E. Sumita. 2002. ?Auto-
matic Paraphrasing Based on Parallel Corpus
for Normalization?. In Proceedings of Inter-
national Conference on Language Resources
and Evaluation (LREC), pages 453?457.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceedings of
the 14th International Conference on Com-
putational Linguistics(COLING), pages 433?
439.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method for
speech translation systems and a case study
on ATR-MATRIX from Japanese to English.
In Proceeding of Machine Translation Summit
(MT Summit), pages 299?307.
T. Takezawa, E. Sumita, F. Sugaya, H. Ya-
mamoto, and S. Yamamoto. 2002. ?Toward a
Broad-Coverage Bilingual Corpus for Speech
Translation of Travel Conversations in the
Real World?. In Proceedings of International
Conference on Language Resources and Eval-
uation (LREC), pages 147?152.
T. Takezawa. 1999. Building a bilingual travel
conversation database for speech translation
research. In Proceedings of the 2nd Inter-
national Workshop on East-Asian Language
Resources and Evaluation ? Oriental CO-
COSDA Workshop ?99 ?, pages 17?20.
TOEFL. 2004. Test of English as a Foreign
Language. http://www.ets.org/toefl/.
TOEIC. 2004. Test of English
for International Communication.
http://www.ets.org/toeic/.
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 61?68, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Measuring Non-native Speakers? Proficiency of English by Using a Test 
with Automatically-Generated Fill-in-the-Blank Questions 
Eiichiro SUMITA 
Spoken Language Communication 
Research Laboratories 
ATR 
Kyoto 619-0288 Japan 
eiichiro.sumita@atr.jp 
Fumiaki SUGAYA 
Text Information Processing Labo-
ratory 
KDDI R&D Laboratories Inc. 
Saitama 356-8502 Japan 
fsugaya@kddilabs.jp 
Seiichi Yamamoto 
Department of Information Systems 
Design 
Doshisha University 
Kyoto 610-0321 Japan 
seyamamo@mail.doshisha.ac.jp
&  
Spoken Language Communication 
Research Laboratories 
ATR 
  
Abstract  
This paper proposes the automatic generation 
of Fill-in-the-Blank Questions (FBQs) together 
with testing based on Item Response Theory 
(IRT) to measure English proficiency. First, the 
proposal generates an FBQ from a given sen-
tence in English. The position of a blank in the 
sentence is determined, and the word at that 
position is considered as the correct choice. 
The candidates for incorrect choices for the 
blank are hypothesized through a thesaurus. 
Then, each of the candidates is verified by us-
ing the Web. Finally, the blanked sentence, the 
correct choice and the incorrect choices surviv-
ing the verification are together laid out to 
form the FBQ. Second, the proficiency of non-
native speakers who took the test consisting of 
such FBQs is estimated through IRT.  
Our experimental results suggest that: 
(1) the generated questions plus IRT estimate 
the non-native speakers? English proficiency; 
(2) while on the other hand, the test can be 
completed almost perfectly by English native 
speakers; and (3) the number of questions can 
be reduced by using item information in IRT.  
The proposed method provides teach-
ers and testers with a tool that reduces time 
and expenditure for testing English profi-
ciency. 
1 Introduction 
                                                          
English has spread so widely that 1,500 million 
people, about a quarter of the world?s population, 
speak it, though at most about 400 million speak it 
as their native language (Crystal, 2003). Thus, 
English education for non-native speakers both 
now and in the near future is of great importance.  
The progress of computer technology is ad-
vancing an electronic tool for language learning 
called Computer-Assisted Language Learning 
(CALL) and for language testing called Computer-
Based Testing (CBT) or Computer-Adaptive Test-
ing (CAT). However, no computerized support for 
producing a test, a collection of questions for 
evaluating language proficiency, has emerged to 
date. * 
Fill-in-the-Blank Questions (FBQs) are widely 
used from the classroom level to far larger scales 
to measure peoples? proficiency at English as a 
second language. Examples of such tests include 
TOEFL (Test Of English as a Foreign Language, 
http://www.ets.org/toefl/) and TOEIC (Test Of 
English for International Communication, 
http://www.ets.org/toeic/).  
A test comprising FBQs has merits in that (1) it 
is easy for test-takers to input answers, (2) com-
puters can mark them, thus marking is invariable 
and objective, and (3) they are suitable for the 
modern testing theory, Item Response Theory 
(IRT).  
Because it is regarded that writing incorrect 
choices that distract only the non-proficient test-
taker is a highly skilled business (Alderson, 1996), 
FBQs have been written by human experts. Thus, 
test construction is time-consuming and expensive. 
As a result, utilizing up-to-date texts for question 
writing is not practical, nor is tuning in to individ-
ual students. 
 
* See the detailed discussion in Section 6. 
61
  To solve the problems of time and expenditure, 
this paper proposes a method for generating FBQs 
using a corpus, a thesaurus, and the Web. Experi-
ments have shown that the proficiency estimated 
through IRT with generated FBQs highly corre-
lates with non-native speakers? real proficiency. 
This system not only provides us with a quick and 
inexpensive testing method, but it also features the 
following advantages:  
(I) It provides ?anyone? individually with 
up-to-date and interesting questions for 
self-teaching. We have implemented a 
program that downloads any Web page 
such as a news site and generates ques-
tions from it.  
(II) It also enables on-demand testing at 
?anytime and anyplace.? We have im-
plemented a system that operates on a 
mobile phone. Questions are generated 
and pooled in the server, and upon a 
user?s request, questions are 
downloaded. CAT (Wainer, 2000) is 
then conducted on the phone. The sys-
tem for mobile phone is scheduled to be 
deployed in May of 2005 in Japan. 
 
The remainder of this paper is organized as fol-
lows. Section 2 introduces a method for making 
FBQ, Section 3 explains how to estimate test-
takers? proficiency, and Section 4 presents the ex-
periments that demonstrate the effectiveness of the 
proposal. Section 5 provides some discussion, and 
Section 6 explains the differences between our 
proposal and related work, followed by concluding 
remarks. 
2 
2.1 
Question Generation Method 
We will review an FBQ, and then explain our 
method for producing it. 
Fill-in-the-Blank Question (FBQ) 
FBQs are the one of the most popular types of 
questions in testing. Figure 1 shows a typical sam-
ple consisting of a partially blanked English sen-
tence and four choices for filling the blank. The 
tester ordinarily assumes that exactly one choice is 
correct (in this case, b)) and the other three choices 
are incorrect. The latter are often called distracters, 
because they fulfill a role to distract the less profi-
cient test-takers. 
Figure 1: A sample Fill-in-the-Blank Question 
(FBQ) 
Question 1 (FBQ)          
I only have to _______ my head above water one more 
week? 
a) reserve b) keep c) guarantee d) promise 
N.B. the correct choice is b) keep.  
2.2 
                                                          
Flow of generation 
Using question 1 above, the outline of generation 
is presented below (Figure 2). 
A seed sentence (in this case, ?I only have to 
keep my head above water one more week.?) is 
input from the designated source, e.g., a corpus or 
a Web page such as well-known news site. *  
 
 
Figure 2: Flow generating Fill-In-The-Blank Ques-
tion (FBQ) 
Seed Sentence Corpus
Testing 
knowledge
[a] Determine the blank position
[b] Generate distracter candidatesLexicon
[c] Verify the incorrectness 
[d] Form the question  
Question 
 
[a] The seed sentence is a correct English sen-
tence that is decomposed into a sentence 
with a blank (blanked sentence) and the 
correct choice for the blank. After the seed 
 
*  Selection of the seed sentence (source text) is an important 
open problem because the difficulty of the seed (text) should 
influence the difficulty of the generated question. As for text 
difficulty, several measures such as Lexile by MetaMetrics 
(http://www.Lexile.com) have been proposed. They are known 
as readability and are usually defined as a function of sentence 
length and word frequency. 
In this paper, we used corpora of business and travel con-
versations, because TOEIC itself is oriented toward business 
and daily conversation. 
62
 sentence is analyzed morphologically by a 
computer, according to the testing knowl-
edge* the blank position of the sentence is 
determined. In this paper?s experiment, the 
verb of the seed is selected, and we obtain 
the blanked sentence ?I only have to 
______ my head above water one more 
week.? and the correct choice ?keep.? 
[b] To be a good distracter, the candidates must 
maintain the grammatical characteristics of 
the correct choice, and these should be 
similar in meaning? . Using a thesaurus? , 
words similar to the correct choice are 
listed up as candidates, e.g., ?clear,? ?guar-
antee,? ?promise,? ?reserve,? and ?share? 
for the above ?keep.?  
[c] Verify (see Section 2.3 for details) the in-
correctness of the sentence restored by each 
candidate, and if it is not incorrect (in this 
case, ?clear? and ?share?), the candidate is 
given up. 
[d] If a sufficient number (in this paper, three) 
of candidates remain, form a question by 
randomizing the order of all the choices 
(?keep,? ?guarantee,? ?promise,? and ?re-
serve?); otherwise, another seed sentence is 
input and restart from step [a]. 
                                                           
2.3 Incorrectness Verification  
In FBQs, by definition, (1) the blanked sentence 
restored with the correct choice is correct, and (2) 
the blanked sentence restored with the distracter 
must be incorrect. 
In order to generate an FBQ, the incorrectness 
of the sentence restored by each distracter candi-
date must be verified and if the combination is not 
incorrect, the candidate is rejected. 
Zero-Hit Sentence 
The Web includes all manners of language data 
in vast quantities, which are for everyone easy to 
access through a networked computer. Recently, 
exploitation of the Web for various natural lan-
guage applications is rising (Grefenstette, 1999; 
Turney, 2001; Kilgarriff and Grefenstette, 2003; 
Tonoike et al, 2004).  
We also propose a Web-based approach. We 
dare to assume that if there is a sentence on the 
Web, that sentence is considered correct; other-
wise, the sentence is unlikely to be correct in that 
there is no sentence written on the Web despite the 
variety and quantity of data on it.  *  Testing knowledge tells us what part of the seed sentence 
should be blanked. For example, we selected the verb of the 
seed because it is one of the basic types of blanked words in 
popular FBQs such as in TOEIC. 
Figure 3 illustrates verification based on the re-
trieval from the Web. Here, s (x) is the blanked 
sentence, s (w) denotes the sentence restored by the 
word w, and hits (y) represents the number of 
documents retrieved from the Web for the key y. 
This can be a word of another POS (Part-Of-Speech). For 
this, we can use knowledge in the field of second-language 
education. Previous studies on errors in English usage by 
Japanese native speakers such as (Izumi and Isahara, 2004) 
unveiled patterns of errors specific to Japanese, e.g., (1) article 
selection error, which results from the fact there are no articles 
in Japanese; (2) preposition selection error, which results from 
the fact some Japanese counterparts have broader meaning; (3) 
adjective selection error, which results from mismatch of 
meaning between Japanese words and their counterpart. Such 
knowledge may generate questions harder for Japanese who 
study English. 
 
?  There are various aspects other than meaning, for example, 
spelling, pronunciation, and translation and so on. Depending 
on the aspect, lexical information sources other than a thesau-
rus should be consulted.  Figure 3: Incorrectness and Hits on the Web 
Blanked sentence:
s (x)= ?I only have to ____ my head above water 
one more week?? 
 
Hits of incorrect choice candidates: 
hits (s (?clear?)) = 176 ; correct 
hits (s (?guarantee?)) = 0 ; incorrect 
hits (s (?promise?)) = 0 ; incorrect 
hits (s (?reserve?)) = 0 ; incorrect  
hits (s (?share?)) = 3 ; correct 
?  We used an in-house English thesaurus whose hierarchy is 
based on one of the off-the-shelf thesauruses for Japanese, 
called Ruigo-Shin-Jiten (Ohno and Hamanishi, 1984). In the 
above examples, the original word ?keep? expresses two dif-
ferent concepts: (1) possession-or-disposal, which is shared by 
the words ?clear? and ?share,? and (2) promise, which is 
shared by the words ?guarantee,? ?promise,? and ?reserve.? 
Since this depends on the thesaurus used, some may sense a 
slight discomfort at these concepts. If a different thesaurus is 
used, the distracter candidates may differ. 
 
If hits (s (w)), is small, then the sentence re-
stored with the word w is unlikely, thus the word w 
should be a good distracter. If hits (s (w)), is large 
then the sentence restored with the word w is likely, 
then the word w is unlikely to be a good distracter 
and is given up.  
63
 We used the strongest condition. If hits (s (w)) 
is zero, then the sentence restored with the word w 
is unlikely, thus the word w should be a good dis-
tracter. If hits (s (w)), is not zero, then the sentence 
restored with the word w is likely, thus the word w 
is unlikely to be a good distracter and is given up.  
3 
3.1 
Estimating Proficiency 
Item Response Theory (IRT) 
Item Response Theory (IRT) is the basis of modern 
language tests such as TOEIC, and enables Com-
puterized Adaptive Testing (CAT). Here, we 
briefly introduce IRT. IRT, in which a question is 
called an item, calculates the test-takers? profi-
ciency based on the answers for items of the given 
test (Embretson, 2000).  
Retrieval NOT By Sentence  
It is often the case that retrieval by sentence does 
not work. Instead of a sentence, a sequence of 
words around a blank position, beginning with a 
content word (or sentence head) and ending with a 
content word (or sentence tail) is passed to a search 
engine automatically. For the abovementioned 
sample, the sequence of words passed to the engine 
is ?I only have to clear my head? and so on. 
The basic idea is the item response function, 
which relates the probability of test-takers answer-
ing particular items correctly to their proficiency. 
The item response functions are modeled as logis-
tic curves making an S-shape, which take the form 
(1) for item i.  
Web Search  
 
))(exp(1
1)(
ii
i ba
P ??+= ??   (1) We can use any search engine, though we have 
been using Google since February 2004. At that 
point in time, Google covered an enormous four 
billion pages. 
The test-taker parameter, ?, shows the profi-
ciency of the test-taker, with higher values indicat-
ing higher performance. The ?correct? hits may come from non-native 
speakers? websites and contain invalid language 
usage. To increase reliability, we could restrict 
Google searches to Websites with URLs based in 
English-speaking countries, although we have not 
done so yet. There is another concern: even if 
sentence fragments cannot be located on the Web, 
it does not necessarily mean they are illegitimate. 
Thus, the proposed verification based on the Web 
is not perfect; the point, however, is that with such 
limitations, the generated questions are useful for 
estimating proficiency as demonstrated in a later 
section. 
Each of the item parameters, ai and bi, controls 
the shape of the item response function. The a pa-
rameter, called discrimination, indexes how 
steeply the item response function rises. The b pa-
rameter is called difficulty. Difficult items feature 
larger b values and the item response functions are 
shifted to the right. These item parameters are usu-
ally estimated by a maximal likelihood method. 
For computations including the estimation, there 
are many commercial programs such as BILOG 
(http://www.assess.com/) available.  
3.2 Reducing test size by selection of effective 
items 
Setting aside the convenience provided by the 
off-the-shelf search engine, another search special-
ized for this application is possible, although the 
current implementation is fast enough to automate 
generation of FBQs, and the demand to accelerate 
the search is not strong. Rather, the problem of 
time needed for test construction has been reduced 
by our proposal. 
It is important to estimate the proficiency of the 
test-taker by using as few items as possible. For 
this, we have proposed a method based on item 
information. 
Expression (2) is the item information of item i 
at ?j, the proficiency of the test-taker j, which indi-
cates how much measurement discrimination an 
item provides. 
The throughput depends on the text from which 
a seed sentence comes and the network traffic 
when the Web is accessed. Empirically, one FBQ 
is obtained in 20 seconds on average and the total 
number of FBQs in a day adds up to over 4,000 on 
a single computer.  
The procedure is as follows.  
 
1. Initialize I by the set of all generated FBQs. 
 
64
 2. According to Equation (3), we select the item 
whose contribution to test information is 
maximal.  
3. We eliminate the selected item from I accord-
ing to Equation (4).  
4. If I is empty, we obtain the ordered list of ef-
fective items; otherwise, go back to step 2. 
 
))(1)(()( 2 jijiiji PPaI ??? ?=  (2) 
( )???
?
???
?= ??
?j Ii
ji
i
Ii ?maxarg?  (3) 
iII ??=  (4) 
4 
4.1 
                                                          
Experiment 
The FBQs for the experiment were generated in 
February of 2004. Seed sentences were obtained 
from ATR?s corpus (Kikui et al, 2003) of the 
business and travel domains. The vocabulary of the 
corpus comprises about 30,000 words. Sentences 
are relatively short, with the average length being 
6.47 words. For each domain 5,000 questions were 
generated automatically and each question consists 
of an English sentence with one blank and four 
choices. 
 Experiment with non-native speakers 
We used the TOEIC score as the experiment?s pro-
ficiency measure, and collected 100 Japanese sub-
jects whose TOEIC scores were scattered from 400 
to less than 900. The actual range for TOEIC 
scores is 10 to 990. Our subjects covered the 
dominant portion* of test-takers for TOEIC in Ja-
pan, excluding the highest and lowest extremes.? 
We had the subjects answer 320 randomly se-
lected questions from the 10,000 mentioned above. 
The raw marks were as follows: the average? mark 
was 235.2 (73.5%); the highest mark was 290 
(90.6%); and the lowest was 158 (49.4%)?This 
suggests that our FBQs are sensitive to test-takers? 
proficiency. In Figure 4, the y-axis represents es-
timated proficiency according to IRT (Section 3.1) 
and generated questions, while the x-axis is the 
real TOEIC score of each subject.  
As the graph illustrates, the IRT-estimated pro-
ficiency (?) and real TOEIC scores of subjects cor-
relate highly with a co-efficiency of about 80%.  
For comparison we refer to CASEC 
(http://casec.evidus.com/), an off-the-shelf test 
consisting of human-made questions and IRT. Its 
co-efficiency with real TOEIC scores is reported to 
be 86%. 
This means the proposed automatically gener-
ated questions are promising for measuring English 
proficiency, achieving a nearly competitive level 
with human-made questions but with a few reser-
vations: (1) whether the difference of 6% is large 
depends on the standpoint of possible users; (2) as 
for the number of questions to be answered, our 
proposal uses 320 questions in the experiments, 
while TOEIC uses 200 questions and CASEC uses 
only about 60 questions; (3) the proposed method 
uses FBQs only whereas CASEC and TOEIC use 
various types of questions.  
 
 
Figure 4: IRT-Estimated Proficiency (?) vs. Real 
TOEIC Score  
 ?
TOEIC Score 
9008070060050
3
2.5
2
1.5
1
0.5
0
-0.5
-1
400
 
4.2 
                                                          
Experiment with a native speaker 
To examine the quality of the generated questions, 
we asked a single subject? who is a native speaker 
of English to answer 4,000 questions (Table 1). * Over 70% of all test-takers are covered 
(http://www.toeic.or.jp/toeic/data/data02.html). The native speaker largely agreed with our gen-
eration, determining correct choices (type I). The 
?  We have covered only the range of TOEIC scores from 400 
to 900 due to expense of the experiment. In this restricted 
experiment, we do not claim that our proficiency estimation 
method covers the full range of TOEIC scores.  
 
?  Please note that the analysis is based on a single native-
speaker, thus we need further analysis by multiple subjects. ?  The standard deviation was 29.8 (9.3%). 
65
 rate was 93.50%, better than 90.6%, the highest 
mark among the non-native speakers. 
 
We present the problematic cases here.  
z Type II is caused by the seed sentence being 
incorrect for the native speaker, and a distracter is 
bad because it is correct. Or like type III, it con-
sists of ambiguous choices? 
z Type III is caused by some generated distracters 
being correct; therefore, the choices are ambiguous.  Figure 5 Correlation coefficient and Test size 
R
Test Size in Items 
350300250200150100500
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
z Type IV is caused by the seed sentence being 
incorrect and the generated distracters also being 
incorrect; therefore, the question cannot be an-
swered.  
z Type V is caused by the seed sentence being 
nonsense to the native speaker; the question, there-
fore, cannot be answered. 
 
Table 1 Responses of a Native speaker 
Type Explanation Count %
I Match 3,740 93.50
II 
Single 
Selection No match 55 1.38
III Ambiguous Choices 70 1.75
IV No Correct Choice 45 1.13
V 
No 
Selection 
Nonsense 90 2.25
 
Cases with bad seed sentences (portions of II, 
IV, and V) require cleaning of the corpus by a na-
tive speaker, and cases with bad distracters (por-
tions of II and III) require refinement of the 
proposed generation algorithm.  
Since the questions produced by this method 
can be flawed in ways which make them unan-
swerable even by native speakers (about 6.5% of 
the time) due to the above-mentioned reasons, it is 
difficult to use this method for high-stakes testing 
applications although it is useful for estimating 
proficiency as explained in the previous section.  
4.3 
5 Discussion 
5.1 
5.2 
 
This section explains the on-demand generation of 
FBQs according to individual preference, an im-
mediate extension and a limitation of our proposed 
method, and finally touches on free-format Q&A. 
Effects of Automatic FBQ Construction 
The method provides teachers and testers with a 
tool that reduces time and expenditure. Further-
more, the method can deal with any text. For ex-
ample, up-to-date and interesting materials such as 
news articles of the day can be a source of seed 
sentences (Figure 6 is a sample generated from an 
article (http://www.japantimes.co.jp/) on an earth-
quake that occurred in Japan), which enables reali-
zation of a personalized learning environment.  
 
 
Figure 6: On-demand construction ? a sample 
question from a Web news article in The Japan 
Times on ?an earthquake? 
N.B. The correct answer is c) originated. 
Question 2 (FBQ)
The second quake            10 km below the seabed some 
130 km east of Cape Shiono. 
 
a) put  b) came  c) originated d) opened 
 
We have generated questions from over 100 docu-
ments on various genres such as novels, speeches, 
academic papers and so on found in the enormous 
collection of e-Books provided by Project Guten-
berg (http://www.gutenberg.org/). 
Proficiency ? estimated with the reduced 
test and its relation to TOEIC Scores  
Figure 5 shows the relationship between reduction 
of the test size according to the method explained 
in Section 3.2 and the estimated proficiency based 
on the reduced test. The x-axis represents the size 
of the reduced test in number of items, while the y-
axis represents the correlation coefficient (R) be-
tween estimated proficiency and real TOEIC score. 
A Variation of Fill-in-the-Blank Ques-
tions for Grammar Checking 
In Section 2.2, we mentioned a constraint that a 
good distracter should maintain the grammatical 
characteristics of the correct choice originating in 
66
 the seed sentence. The question checks not the 
grammaticality but the semantic/pragmatic cor-
rectness.  
We can generate another type of FBQ by 
slightly modifying step [b] of the procedure in Sec-
tion 2.2 to retain the stem of the original word w 
and vary the surface form of the word w. This 
modified procedure generates a question that 
checks the grammatical ability of the test takers. 
Figure 7 shows a sample of this kind of question 
taken from a TOEIC-test textbook (Educational 
Testing Service, 2002). 
 
 
Figure 7: A variation on fill-in-the-blank questions 
5.3 
5.4 
6 
                                                          
Limitation of the Addressed FBQs  
The questions dealt with in this paper concern test-
ing reading ability, but these questions are not suit-
able for testing listening ability because they are 
presented visually and cannot be pronounced. To 
test listening ability, like in TOIEC, other types of 
questions should be used, and automated genera-
tion of them is yet to be developed.  
Free-Format Q&A 
Besides measuring one?s ability to receive infor-
mation in a foreign language, which has been ad-
dressed so far in this paper, it is important to 
measure a person?s ability to transmit information 
in a foreign language. For that purpose, tests for 
translating, writing, or speaking in a free format 
have been actively studied by many researchers 
(Shermis, 2003; Yasuda, 2004). 
Related Work*  
Here, we explain other studies on the generation of 
multiple-choice questions for language learning. 
There are a few previous studies on computer-
based generation such as Mitkov (2003) and Wil-
son (1997). 
 
6.1 
6.2 
7 Conclusion 
                                                          
Cloze Test 
A computer can generate questions by deleting 
words or parts of words randomly or at every N-th 
word from text. Test-takers are requested to restore 
the word that has been deleted. This is called a 
?cloze test.? The effectiveness of a ?cloze test? or 
its derivatives is a matter of controversy among 
researchers of language testing such as Brown 
(1993) and Alderson (1996). 
N.B. The correct answer is c) care.  
Question 3 (FBQ) 
                   
Because the equipment is very delicate, it must be han-
dled with ______? 
 
a) caring b) careful  c) care  d) carefully 
Tests on Facts  
Mitkov (2003) proposed a computer-aided proce-
dure for generating multiple-choice questions from 
textbooks. The differences from our proposal are 
that (1) Mitkov?s method generates questions not 
about language usage but about facts explicitly 
stated in a text?; (2) Mitkov uses techniques such 
as term extraction, parsing, transformation of trees, 
which are different from our proposal; and (3) Mit-
kov does not use IRT while we use it. 
This paper proposed the automatic construction of 
Fill-in-the-Blank Questions (FBQs). The proposed 
method generates FBQs using a corpus, a thesaurus, 
and the Web. The generated questions and Item 
Response Theory (IRT) then estimate second-
language proficiency.  
Experiments have shown that the proposed 
method is effective in that the estimated profi-
ciency highly correlates with non-native speakers? 
real proficiency as represented by TOEIC scores; 
native-speakers can achieve higher scores than 
non-native speakers. It is possible to reduce the 
size of the test by removing non-discriminative 
questions with item information in IRT. 
 
? Based on a fact stated in a textbook like, ?A prepositional 
phrase at the beginning of a sentence constitutes an introduc-
tory modifier,? Mitkov generates a question such as, ?What 
does a prepositional phrase at the beginning of a sentence 
constitute? i. a modifier that accompanies a noun; ii. an asso-
ciated modifier; iii. an introductory modifier; iv. a misplaced 
modifier.? 
* There are many works on item generation theory (ITG) such 
as Irvine and Kyllonen (2002), although we do not go any 
further into the area. We focus only on multiple-choice ques-
tions for language learning in this paper. 
67
 The method provides teachers, testers, and test 
takers with novel merits that enable low-cost test-
ing of second-language proficiency and provides 
learners with up-to-date and interesting materials 
suitable for individuals. 
Further research should be done on (1) large-
scale evaluation of the proposal, (2) application to 
different languages such as Chinese and Korean, 
and (3) generation of different types of questions. 
Acknowledgements 
The authors? heartfelt thanks go to anonymous review-
ers for providing valuable suggestions and Kadokawa-
Shoten for providing the thesaurus named Ruigo-Shin-
Jiten. The research reported here was supported in part 
by a contract with the NiCT entitled, ?A study of speech 
dialogue translation technology based on a large cor-
pus.? It was also supported in part by the Grants-in-Aid 
for Scientific Research (KAKENHI), contract with 
MEXT numbered 16300048. The study was conducted 
in part as a cooperative research project by KDDI and 
ATR.  
References 
Alderson, Charles. 1996. Do corpora have a role in 
language assessment? Using Corpora for Language 
Research, eds. Thomas, J. and Short, M., Longman: 
248?259. 
Brown, J. D. 1993. What are the characteristics of natu-
ral cloze tests? Language Testing 10: 93?116. 
Crystal, David. 2003. English as a Global Language, 
(Second Edition). Cambridge University Press: 212. 
Educational Testing Service 2002. TOEIC koushiki 
gaido & mondaishu. IIBC: 249. 
Embretson, Susan et al 2000. Item Response Theory for 
Psychologists. LEA: 371. 
Grefenstette, G. 1999. The WWW as a resource for ex-
ample-based MT tasks. ASLIB ?Translating and the 
Computer? conference. 
Irvine, H. S., and Kyllonen, P. C. (2002). Item genera-
tion for test development. LEA: 412. 
Izumi, E., and Isahara, H. (2004). Investigation into 
language learners' acquisition order based on the er-
ror analysis of the learner corpus. In Proceedings of 
Pacific-Asia Conference on Language, Information 
and Computation (PACLIC) 18 Satellite Workshop 
on E-Learning, Japan. (in printing) 
Kikui, G., Sumita, E., Takaezawa, T. and Yamamoto, S., 
?Creating Corpora for Speech-to-Speech Transla-
tion,? Special Session ?Multilingual Speech-to-
Speech Translation? of EuroSpeech, 2003. 
Kilgarriff, A. and Grefenstette, G. 2003. Special Issue 
on the WEB as Corpus. Computational Linguistics 29 
(3): 333?502. 
Mitkov, Ruslan and Ha, Le An. 2003. Computer-Aided 
Generation of Multiple-Choice Tests. HLT-NAACL 
2003 Workshop: Building Educational Applications 
Using Natural Language Processing: 17?22. 
Ohno, S. and Hamanishi, M. 1984. Ruigo-Shin-Jiten, 
Kadokawa, Tokyo (in Japanese) 
Shermis, M. D. and Burstein. J. C. 2003. Automated 
Essay Scoring. LEA: 238. 
Tonoike, M., Sato, S., and Utsuro, T. 2004. Answer 
Validation by Keyword Association. IPSJ, SIGNL, 
161: 53?60, (in Japanese). 
Turney, P.D. 2001. Mining the Web for synonyms: PMI-
IR vs. LSA on TOEFL. ECML 2001: 491?502. 
Wainer, Howard et al 2000. Conputerized Adaptive 
Testing: A Primer, (Second Edition). LEA: 335. 
Wilson, E. 1997. The Automatic Generation of CALL 
exercises from general corpora, in eds. Wichmann, 
A., Fligelstone, S., McEnery, T., Knowles, G., 
Teaching and Language Corpora, Harlow: Long-
man:116-130. 
Yasuda, K., Sugaya, F., Sumita, E., Takezawa, T., Kikui, 
G. and Yamamoto, S. 2004. Automatic Measuring of 
English Language Proficiency using MT Evaluation 
Technology, COLING 2004 eLearning for Computa-
tional Linguistics and Computational Linguistics for 
eLearning: 53-60.  
68
