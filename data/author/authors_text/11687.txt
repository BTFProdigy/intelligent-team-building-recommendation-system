Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74?81,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Incrementality, Speaker-Hearer Switching
and the Disambiguation Challenge
Ruth Kempson, Eleni Gregoromichelaki
King?s College London
{ruth.kempson, eleni.gregor}@kcl.ac.uk
Yo Sato
University of Hertfordshire
y.sato@herts.ac.uk
Abstract
Taking so-called split utterances as our
point of departure, we argue that a new
perspective on the major challenge of dis-
ambiguation becomes available, given a
framework in which both parsing and gen-
eration incrementally involve the same
mechanisms for constructing trees reflect-
ing interpretation (Dynamic Syntax: (Cann
et al, 2005; Kempson et al, 2001)). With
all dependencies, syntactic, semantic and
pragmatic, defined in terms of incremental
progressive tree growth, the phenomenon
of speaker/hearer role-switch emerges as
an immediate consequence, with the po-
tential for clarification, acknowledgement,
correction, all available incrementally at
any sub-sentential point in the interpreta-
tion process. Accordingly, at all interme-
diate points where interpretation of an ut-
terance subpart is not fully determined for
the hearer in context, uncertainty can be
resolved immediately by suitable clarifica-
tion/correction/repair/extension as an ex-
change between interlocutors. The result
is a major check on the combinatorial ex-
plosion of alternative structures and inter-
pretations at each choice point, and the ba-
sis for a model of how interpretation in
context can be established without either
party having to make assumptions about
what information they and their interlocu-
tor share in resolving ambiguities.
1 Introduction
A major characteristic of dialogue is effortless
switching between the roles of hearer and speaker.
Dialogue participants seamlessly shift between
parsing and generation bi-directionally across any
syntactic dependency, without any indication of
there being any problem associated with such
shifts (examples from Howes et al (in prep)):
(1) Conversation from A and B, to C:
A: We?re going
B: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:
A: Have you burnt
B the buns. Very thoroughly.
A: But did you burn
B: Myself? No. Luckily.
(3) A: Are you left or
B: Right-handed.
Furthermore, in no case is there any guarantee that
the way the shared utterance evolves is what ei-
ther party had in mind to say at the outset, indeed
obviously not, as otherwise the exchange risks be-
ing otiose. This flexibility provides a vehicle for
ongoing clarification, acknowledgement, correc-
tions, repairs etc. ((6)-(7) from (Mills, 2007)):
(4) A: I?m seeing Bill.
B: The builder?
A: Yeah, who lives with Monica.
(5) A: I saw Don
B: John?
A: Don, the guy from Bristol.
(6) A: I?m on the second switch
B: Switch?
A: Yeah, the grey thing
(7) A: I?m on the second row third on the left.
B: What?
A: on the left
The fragmental utterances that constitute such in-
cremental, joint contributions have been analysed
as falling into discrete structural types according
to their function, in all cases resolved to propo-
sitional types by combining with appropriate ab-
stractions from context (Ferna?ndez, 2006; Purver,
2004). However, any such fragment and their
resolution may occur as mid-turn interruptions,
well before any emergent propositional structure
is completed:
74
(8) A: They X-rayed me, and took a urine
sample, took a blood sample.
Er, the doctor ...
B: Chorlton?
A: Chorlton, mhm, he examined me, erm,
he, he said now they were on about a slight
[shadow] on my heart. [BNC: KPY
1005-1008]
The advantage of such ongoing, incremental, joint
conversational contributions is the effective nar-
rowing down of the search space out of which
hearers select (a) interpretations to yield some
commonly shared understanding, e.g. choice
of referents for NPs, and, (b) restricted struc-
tural frames which allow (grammatical) context-
dependent fragment resolution, i.e. exact speci-
fications of what contextually available structures
resolve elliptical elements. This seems to pro-
vide an answer as to why such fragments are so
frequent and undemanding elements of dialogue,
forming the basis for the observed coordination
between participants: successive resolution at sub-
sentential stages yields a progressively jointly es-
tablished common ground, that can thereafter be
taken as a secure, albeit individual, basis for filter-
ing out interpretations inconsistent with such con-
firmed knowledge-base (see (Poesio and Rieser,
2008; Ginzburg, forthcmg) etc). All such dialogue
phenomena, illustrated in (1)-(8), jointly and in-
crementally achieved, we address with the general
term split utterances.
However, such exchanges are hard to model
within orthodox grammatical frameworks, given
that usually it is the sentence/proposition that is
taken as the unit of syntactic/semantic analysis;
and they have not been addressed in detail within
such frameworks, being set aside as deviant, given
that such grammars in principle do not specify
a concept of grammaticality that relies on a de-
scription of the context of occurrence of a certain
structure (however, see Poesio and Rieser (2008)
for German completions). In so far as fragment
utterances are now being addressed, the pressure
of compatibility with sentence-based grammars
is at least partly responsible for analyses of e.g.
clarificatory-request fragments as sentential in na-
ture (Ginzburg and Cooper, 2004). But such anal-
yses fail to provide a basis for incrementally re-
solved clarification requests such as the interrup-
tion in (8) where no sentential basis is yet avail-
able over which to define the required abstraction
of contextually provided content.
In the psycholinguistic literature, on the other
hand, there is broad agreement that incrementality
is a crucial feature of parsing with semantic inter-
pretation taking place as early as possible at the
sub-sentential level (see e.g. (Sturt and Crocker,
1996)). Nonetheless, this does not, in and of it-
self, provide a basis for explaining the ease and
frequency of split utterances in dialogue: the inter-
active coordination between the parsing and pro-
duction activities, one feeding the other, remains
as a challenge.
In NLP modelling, parsing and generation algo-
rithms are generally dissociated from the descrip-
tion of linguistic entities and rules, i.e. the gram-
mar formalisms, which are considered either to be
independent of processing (?process-neutral?) or
to require some additional generation- or parsing-
specific mechanisms to be incorporated. However,
this point of view creates obstacles for a success-
ful account of data as in (1)-(8). Modelling those
would require that, for the current speaker, the ini-
tiated generation mechanism has to be displaced
mid-production without the propositional genera-
tion task having been completed. Then the parsing
mechanism, despite being independent of, indeed
in some sense the reverse of, the generation com-
ponent, has to take over mid-sentence as though, in
some sense there had been parsing involved up to
the point of switchover. Conversely, for the hearer-
turned-speaker, it would be necessary to somehow
connect their parse with what they are now about
to produce in order to compose the meaning of the
combined sentence. Moreover, in both directions
of switch, as (2) shows, this is not a phenomenon
of both interlocutors intending to say the same
sentence: as (3) shows, even the function of the
utterance (e.g. question/answer) can alter in the
switch of roles and such fragments can play two
roles (e.g. question/completion) at the same time
(e.g. (2)). Hence the grammatical integration of
such joint contributions must be flexible enough
to allow such switches which means that such
fragment resolutions must occur before the com-
putation of intentions at the pragmatic level. So
the ability of language users to successfully pro-
cess such utterances, even at sub-sentential levels,
means that modelling their grammar requires fine-
grained grammaticality definitions able to char-
acterise and integrate sub-sentential fragments in
turns jointly constructed by speaker and hearer.
75
This can be achieved straightforwardly if fea-
tures like incrementality and context-dependent
processing are built into the grammar architecture
itself. The modelling of split utterances then be-
comes straightforward as each successive process-
ing step exploits solely the grammatical apparatus
to succeed or fail. Such a view notably does not in-
voke high-level decisions about speaker/hearer in-
tentions as part of the mechanism itself. That this
is the right view to take is enhanced by the fact that
as all of (1)-(8) show, neither party in such role-
exchanges can definitively know in advance what
will emerge as the eventual joint proposition. If,
to the contrary, generation decisions are modelled
as involving intentions for whole utterances, there
will be no the basis for modelling how such in-
complete strings can be integrated in suitable con-
texts, with joint propositional structures emerging
before such joint intentions have been established.
An additional puzzle, equally related to both
the challenges of disambiguation and the status
of modelling speaker?s intentions as part of the
mechanism whereby utterance interpretation takes
place, is the common occurrence of hearers NOT
being constrained by any check on consistency
with speaker intentions in determining a putative
interpretation, failing to make use of well estab-
lished shared knowledge:
(9) A: I?m going to cook salmon, as John?s
coming.
B: What? John?s a vegetarian.
A: Not my brother. John Smith.
(10) A: Why don?t you have cheese and noodles?
B: Beef? You KNOW I?m a vegetarian
Such examples are problematic for any account
that proposes that interpretation mechanisms for
utterance understanding solely depend on selec-
tion of interpretations which either the speaker
could have intended (Sperber and Wilson, 1986;
Carston, 2002), or ones which are compati-
ble with checking consistency with the com-
mon ground/plans established between speaker
and hearer (Poesio and Rieser, 2008; Ginzburg,
forthcmg), mutual knowledge, etc. (Clark, 1996;
Brennan and Clark, 1996). To the contrary, the
data in (9)-(10) tend to show that the full range
of interpretations computable by the grammar has
in principle to be available at all choice points for
construal, without any filter based on plausibility
measures, thus leaving the disambiguation chal-
lenge still unresolved.
In this paper we show how with speaker and
hearer in principle using the same mechanisms for
construal, equally incrementally applied, such dis-
ambiguation issues can be resolved in a timely
manner which in turn reduces the multiplication
of structural/interpretive options. As we shall see,
what connects our diverse examples, and indeed
underpins the smooth shift in the joint endeav-
our of conversation, lies in incremental, context-
dependent processing and bidirectionality, essen-
tial ingredients of the Dynamic Syntax (Cann et al,
2005) dialogue model.
2 Incrementality in Dynamic Syntax
Dynamic Syntax (DS) is a procedure-oriented
framework, involving incremental processing, i.e.
strictly sequential, word-by-word interpretation of
linguistic strings. The notion of incrementality
in DS is closely related to another of its features,
the goal-directedness of BOTH parsing and gener-
ation. At each stage of processing, structural pre-
dictions are triggered that could fulfill the goals
compatible with the input, in an underspecified
manner. For example, when a proper name like
Bob is encountered sentence-initially in English,
a semantic predicate node is predicted to follow
(?Ty(e ? t)), amongst other possibilities.
By way of introducing the reader to the DS
devices, let us look at some formal details with
an example, Bob saw Mary. The ?complete? se-
mantic representation tree resulting after the com-
plete processing of this sentence is shown in Fig-
ure 2 below. A DS tree is formally encoded with
the tree logic LOFT (Blackburn and Meyer-Viol
(1994)), we omit these details here) and is gen-
erally binary configurational, with annotations at
every node. Important annotations here, see the
(simplified) tree below, are those which represent
semantic formulae along with their type informa-
tion (e.g. ?Ty(x)?) based on a combination of the
epsilon and lambda calculi1.
Such complete trees are constructed, starting
from a radically underspecified annotation, the ax-
iom, the leftmost minimal tree in Figure 2, and
going through monotonic updates of partial, or
structurally underspecified, trees. The outline of
this process is illustrated schematically in Figure
2. Crucial for expressing the goal-directedness
are requirements, i.e. unrealised but expected
1These are the adopted semantic representation languages
in DS but the computational formalism is compatible with
other semantic-representation formats
76
0?Ty(t),
?
7?
1
?Ty(t)
?Ty(e),? ?Ty(e? t)
7?
2
?Ty(t)
Ty(e),Bob? ?Ty(e? t),?
7?
3
?Ty(t)
Ty(e),
Bob? ?Ty(e? t)
?Ty(e),
?
Ty(e? (e? t)),
See?
7?
0(gen)/4
Ty(t),?
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 2: Monotonic tree growth in DS
Ty(t),
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 1: A DS complete tree
node/tree specifications, indicated by ??? in front
of annotations. The axiom says that a proposition
(of type t, Ty(t)) is expected to be constructed.
Furthermore, the pointer, notated with ??? indi-
cates the ?current? node in processing, namely the
one to be processed next, and governs word order.
Updates are carried out by means of applying
actions, which are divided into two types. Compu-
tational actions govern general tree-constructional
processes, such as moving the pointer, introducing
and updating nodes, as well as compiling interpre-
tation for all non-terminal nodes in the tree. In our
example, the update of (1) to (2) is executed via
computational actions specific to English, expand-
ing the axiom to the subject and predicate nodes,
requiring the former to be processed next by the
position of the ?. Construction of only weakly
specified tree relations (unfixed nodes) can also be
induced, characterised only as dominance by some
current node, with subsequent update required. In-
dividual lexical items also provide procedures for
building structure in the form of lexical actions,
inducing both nodes and annotations. For exam-
ple, in the update from (2) to (3), the set of lexical
actions for the word see is applied, yielding the
predicate subtree and its annotations. Thus partial
trees grow incrementally, driven by procedures as-
sociated with particular words as they are encoun-
tered.
Requirements embody structural predictions as
mentioned earlier. Thus unlike the conven-
tional bottom-up parsing,2 the DS model takes
the parser/generator to entertain some predicted
goal(s) to be reached eventually at any stage of
processing, and this is precisely what makes the
formalism incremental. This is the characteri-
sation of incrementality adopted by some psy-
cholinguists under the appellation of connected-
ness (Sturt and Crocker, 1996; Costa et al, 2002):
an encountered word always gets ?connected? to a
larger, predicted, tree.
Individual DS trees consist of predicates and
their arguments. Complex structures are obtained
via a general tree-adjunction operation licensing
the construction of so-called LINKed trees, pairs
of trees where sharing of information occurs. In
its simplest form this mechanism is the same one
which provides the potential for compiling in-
2The examples in (1)-(8) also suggest the implausibility
of purely bottom-up or head-driven parsing being adopted di-
rectly, because such strategies involve waiting until all the
daughters are gathered before moving on to their projection.
In fact, the parsing strategy adopted by DS is somewhat sim-
ilar to mixed parsing strategies like the left-corner or Earley
algorithm to a degree. These parsing strategic issues are more
fully discussed in Sato (forthcmg).
77
A consultant, a friend of Jo?s, is retiring: Ty(t), Retire?((?, x, Consultant?(x) ? Friend?(Jo?)(x)))
Ty(e), (?, x, Consultant?(x) ? Friend?(Jo?)(x)) Ty(e? t), Retire?
Ty(e), (?, x, Friend?(Jo?)(x))
Ty(cn), (x, Friend?(Jo?)(x))
x Friend?(Jo?)
Jo? Friend?
Ty(cn? e), ?P.?, P
Figure 3: Apposition in DS
terpretation for apposition constructions as can
be seen in Figure (3)3. The assumption in the
construction of such LINKed structures is that at
any arbitrary stage of development, some type-
complete subtree may constitute the context for
the subsequent parsing of the following string as
an adjunct structure candidate for incorporation
into the primary tree, hence the obligatory sharing
of information in the resulting semantic represen-
tation.
More generally, context in DS is defined as the
storage of parse states, i.e., the storing of par-
tial tree, word sequence parsed to date, plus the
actions used in building up the partial tree. For-
mally, a parse state P is defined as a set of triples
?T, W, A?, where: T is a (possibly partial) tree;
W is the associated sequence of words; A is the
associated sequence of lexical and computational
actions. At any point in the parsing process, the
context C for a particular partial tree T in the set
P can be taken to consist of: a set of triples P ? =
{. . . , ?Ti, Wi, Ai?, . . .} resulting from the previ-
ous sentence(s); and the triple ?T, W, A? itself,
the subtree currently being processed. Anaphora
and ellipsis construal generally involve re-use of
formulae, structures, and actions from the set C.
Grammaticality of a string of words is then de-
fined relative to its context C, a string being well-
formed iff there is a mapping from string onto
completed tree with no outstanding requirements
given the monotonic processing of that string rela-
tive to context. All fragments illustrated above are
processed by means of either extending the current
3Epsilon terms, like ?, x, Consultant?(x), stand for wit-
nesses of existentially quantified formulae in the epsilon cal-
culus and represent the semantic content of indefinites in DS.
Defined relative to the equivalence ?(?, x, ?(x)) = ?x?(x),
their defining property is their reflection of their contain-
ing environment, and accordingly they are particularly well-
suited to expressing the growth of terms secured by such ap-
positional devices.
tree, or constructing LINKed structures and trans-
fer of information among them so that one tree
provides the context for another, and are licensed
as wellformed relative to that context. In particu-
lar, fragments like the doctor in (8) are licensed by
the grammar because they occur at a stage in pro-
cessing at which the context contains an appropri-
ate structure within which they can be integrated.
The definite NP is taken as an anaphoric device,
relying on a substitution process from the context
of the partial tree to which the node it decorates is
LINKed to achieve the appropriate construal and
tree-update:
(11) The?parse? tree licensing production of the
doctor: LINK adjunction
?Ty(t)
Chorlton? ?Ty(e? t)
(Doctor?(Chorlton?)),?
3 Bidirectionality in DS
Crucially, for our current concern, this architec-
ture allows a dialogue model in which generation
and parsing function in parallel, following exactly
the same procedure in the same order. See Fig (2)
for a (simplified) display of the transitions manip-
ulated by a parse of Bob saw Mary, as each word
is processed and integrated to reach the complete
tree. Generation of this utterance from a complete
tree follows precisely the same actions and trees
from left to right, although the complete tree is
available from the start (this is why the complete
tree is marked ?0? for generation): in this case the
eventual message is known by the speaker, though
of course not by the hearer. What generation in-
volves in addition to the parse steps is reference
78
to this complete tree to check whether each pu-
tative step is consistent with it in order not to be
deviated from the legitimate course of action, that
is, a subsumption check. The trees (1-3) are li-
censed because each of these subsumes (4). Each
time then the generator applies a lexical action, it
is licensed to produce the word that carries that ac-
tion under successful subsumption check: at Step
(3), for example, the generator processes the lex-
ical action which results in the annotation ?See??,
and upon success and subsumption of (4) license
to generate the word see at that point ensues.
For split utterances, two more assumptions are
pertinent. On the one hand, speakers may have
initially only a partial structure to convey: this is
unproblematic, as all that is required by the for-
malism is monotonicity of tree growth, the check
being one of subsumption which can be carried
out on partial trees as well. On the other hand,
the utterance plan may change, even within a sin-
gle speaker. Extensions and clarifications in DS
can be straightforwardly generated by appending
a LINKed structure projecting the added material
to be conveyed (preserving the monotonicity con-
straint)4.
(12) I?m going home, with my brother, maybe
with his wife.
Such a model under which the speaker and
hearer essentially follow the same sets of actions,
updating incrementally their semantic representa-
tions, allows the hearer to ?mirror? the same series
of partial trees, albeit not knowing in advance what
the content of the unspecified nodes will be.
4 Parser/generator implementation
The process-integral nature of DS emphasised
thus far lends itself to the straightforward imple-
mentation of a parsing/generating system, since
the ?actions? defined in the grammar directly pro-
vide a major part of its implementation. By now it
should also be clear that the DS formalism is fully
bi-directional, not only in the sense that the same
grammar can be used for generation and parsing,
but also because the two sets of activities, conven-
tionally treated as ?reverse? processes, are mod-
elled to run in parallel. Therefore, not only can the
same sets of actions be used for both processes,
4Revisions however will involve shifting to a previous
partial tree as the newly selected context: I?m going home,
to my brother, sorry my mother.
but also a large part of the parsing and generation
algorithms can be shared.
This design architecture and a prototype im-
plementation are outlined in (Purver and Otsuka,
2003), and the effort is under way to scale up the
DS parsing/generating system incorporating the
results in (Gargett et al, 2008; Gregoromichelaki
et al, to appear).5 The parser starts from the axiom
(step 0 in Fig.2), which ?predicts? a proposition to
be built, and follows the applicable actions, lexi-
cal or general, to develop a complete tree. Now,
as has been described in this paper, the genera-
tor follows exactly the same steps: the axiom is
developed through successive updates into a com-
plete tree. The only material difference from ?
or rather in addition to? parsing is the complete
tree (Step 0(gen)/4), given from the very start of
the generation task, which is then referred to at
each tree update for subsumption check. The main
point is that despite the obvious difference in their
purposes ?outputting a string from a meaning ver-
sus outputting a meaning from a string? parsing
and generation indeed share the direction of pro-
cessing in DS. Moreover, as no intervening level
of syntactic structure over the string is ever com-
puted, the parsing/generation tasks are more effi-
ciently incremental in that semantic interpretation
is directly imposed at each stage of lexical integra-
tion, irrespective of whether some given partially
developed constituent is complete.
To clarify, see the pseudocode in the Prolog
format below, which is a close analogue of the
implemented function that both does parsing and
generation of a word (context manipulation is
ignored here for reasons of space). The plus
and minus signs attached to a variable indicate it
must/needn?t be instantiated, respectively. In ef-
fect, the former corresponds to the input, the latter
to the output.
(13) parse gen word(
+OldMeaning,?Word,?NewMeaning):-
apply lexical actions(+OldMeaning, ?Word,
+LexActions, ?IntermediateMeaning ),
apply computational actions(
+IntermediateMeaning, +CompActions,
?NewMeaning )
OldMeaning is an obligatory input item, which
corresponds to the semantic structure con-
structed so far (which might be just structural
tree information initially before any lexical
5The preliminary results are described in (Sato,
forthcmg).
79
input has been processed thus advocating a
strong predictive element even compared to
(Sturt and Crocker, 1996). Now notice that
the other two variables ?corresponding to the
word and the new (post-word) meaning? may
function either as the input or output. More
precisely, this is intended to be a shorthand
for either (+OldMeaning,+Word,?NewMeaning)
i.e. Word as input and NewMeaning as out-
put, or (+OldMeaning,?Word,+NewMeaning), i.e.
NewMeaning as input and Word as output, to repeat,
the former corresponding to parsing and the latter
to generation.
In either case, the same set of two sub-
procedures, the two kinds of actions described in
(13), are applied sequentially to process the input
to produce the output. These procedures corre-
spond to an incremental ?update? from one par-
tial tree to another, through a word. The whole
function is then recursively applied to exhaust the
words in the string, from left to right, either in
parsing or generation. Thus there is no differ-
ence between the two in the order of procedures
to be applied, or words to be processed. Thus it is
a mere switch of input/output that shifts between
parsing and generation.6
4.1 Split utterances in Dynamic Syntax
Split utterances follow as an immediate conse-
quence of these assumptions. For the dialogues in
(1)-(8), therefore, while A reaches a partial tree of
what she has uttered through successive updates
as described above, B as the hearer, will follow
the same updates to reach the same representation
of what he has heard. This provides him with the
ability at any stage to become the speaker, inter-
rupting to continue A?s utterance, repair, ask for
clarification, reformulate, or provide a correction,
as and when necessary7. According to our model
of dialogue, repeating or extending a constituent
of A?s utterance by B is licensed only if B, the
hearer turned now speaker, entertains a message
6Thus the parsing procedure is dictated by the grammar to
a large extent, but importantly, not completely. More specif-
ically, the grammar formalism specifies the state paths them-
selves, but not how the paths should be searched. The DS ac-
tions are defined in conditional terms, i.e. what to do as and
when a certain condition holds. If a number of actions can be
applied at some point during a parse, i.e. locally ambiguity
is encountered, then it is up to a particular implementation
of the parser to decide which should be traversed first. The
current implementation includes suggestions of search strate-
gies.
7The account extends the implementation reported in
(Purver et al, 2006)
to be conveyed that matches or extends the parse
tree of what he has heard in a monotonic fashion.
In DS, this message is a semantic representation
in tree format and its presence allows B to only ut-
ter the relevant subpart of A?s intended utterance.
Indeed, this update is what B is seeking to clarify,
extend or acknowledge. In DS, B can reuse the
already constructed (partial) parse tree in his con-
text, rather than having to rebuild an entire propo-
sitional tree or subtree.
The fact that the parsing formalism integrates
a strong element of predictivity, i.e. the parser
is always one step ahead from the lexical in-
put, allows a straightforward switch from pars-
ing to generation thus resulting in an explana-
tion of the facility with which split utterances oc-
cur (even without explicit reasoning processes).
Moreover, on the one hand, because of incremen-
tality, the issue of interpretation-selection can be
faced at any point in the process, with correc-
tions/acknowledgements etc. able to be provided
at any point; this results in the potential exponen-
tial explosion of interpretations being kept firmly
in check. And, structurally, such fragments can
be integrated in the current partial tree represen-
tation only (given the position of the pointer) so
there is no structural ambiguity multiplication. On
the other hand, for any one of these intermedi-
ate check points, bidirectionality entails that con-
sistency checking remains internal to the individ-
ual interlocutors? system, the fact of their mir-
roring each other resulting at their being at the
same point of tree growth. This is sufficient to en-
sure that any inconsistency with their own parse
recognised by one party as grounds for correc-
tion/repair can be processed AS a correction/repair
by the other party without requiring any additional
metarepresentation of their interlocutors? informa-
tion state (at least for these purposes). This allows
the possibility of building up apparently complex
assumptions of shared content, without any neces-
sity of constructing hypotheses of what is enter-
tained by the other, since all context-based selec-
tions are based on the context of the interlocutor
themselves. This, in its turn, opens up the possi-
bility of hearers constructing interpretations based
on selections made that transparently violate what
is knowledge shared by both parties, for no pre-
sumption of common ground is essential as input
to the interpretation process (see, e.g. (9)-(10)).
80
5 Conclusion
It is notable that, from this perspective, no pre-
sumption of common ground or hypothesis as to
what the speaker could have intended is necessary
to determine how the hearer selects interpretation.
All that is required is a concept of system-internal
consistency checking, the potential for clarifica-
tion in cases of uncertainty, and reliance at such
points on disambiguation/correction/repair by the
other party. The advantage of such a proposal, we
suggest, is the provision of a fully mechanistic ac-
count for disambiguation (cf. (Pickering and Gar-
rod, 2004)). The consequence of such an analysis
is that language use is essentially interactive (see
also (Ginzburg, forthcmg; Clark, 1996)): the only
constraint as to whether some hypothesised in-
terpretation assigned by either party is confirmed
turns on whether it is acknowledged or corrected
(see also (Healey, 2008)).
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962,
the EU ITALK project (FP7-214668) and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex
Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg
James Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 22:482?1493.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Robyn Carston. 2002. Thoughts and Utterances: The
Pragmatics of Explicit Communication. Blackwell.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
Patrick Sturt, and Giovanni Soda. 2002. Enhanc-
ing first-pass attachment prediction. In ECAI 2002:
508-512.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Andrew Gargett, Eleni Gregoromichelaki, Chris
Howes, and Yo Sato. 2008. Dialogue-grammar cor-
respondence in dynamic syntax. In Proceedings of
the 12th SEMDIAL (LONDIAL).
Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates
in dialogue. Linguistics and Philosophy, 27(3):297?
365.
Jonathan Ginzburg. forthcmg. Semantics for Conver-
sation. CSLI.
Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-
drew Gargett, and Christine Howes. to appear. Dia-
logue modelling and the remit of core grammar. In
Proceedings of IWCS 2009.
Patrick Healey. 2008. Interactive misalignment: The
role of repair in the development of group sub-
languages. In R. Cooper and R. Kempson, editors,
Language in Flux. College Publications.
Christine Howes, Patrick G. T. Healey, and Gregory
Mills. in prep. a: An experimental investigation
into. . . b: . . . split utterances.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
Gregory J. Mills. 2007. Semantic co-ordination in di-
alogue: the role of direct interaction. Ph.D. thesis,
Queen Mary University of London.
Martin Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences.
Massimo Poesio and Hannes Rieser. 2008. Comple-
tions, coordination, and alignment in dialogue. Ms.
Matthew Purver and Masayuki Otsuka. 2003. Incre-
mental generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceedings of
the 9th European Workshop in Natural Language
Generation (ENLG), pages 79?86.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computa-
tion, 4(2-3):289?326.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, Univer-
sity of London, forthcoming.
Yo Sato. forthcmg. Local ambiguity, search strate-
gies and parsing in dynamic syntax. In Eleni Gre-
goromichelaki and Ruth Kempson, editors, Dynamic
Syntax: Collected Papers. CSLI.
Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448?494.
81
Proceedings of the 8th International Conference on Computational Semantics, pages 128?139,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Dialogue Modelling and the Remit
of Core Grammar
Eleni Gregoromichelaki
?
, Yo Sato
?
, Ruth Kempson
?
Andrew Gargett
?
, Christine Howes
?
?
King?s College London,
?
Queen Mary University of London
1 Introduction
In confronting the challenge of providing formal models of dialogue, with
its plethora of fragments and rich variation in modes of context-dependent
construal, it might seem that linguists face two types of methodological
choice: either (a) conversation employs dialogue-specific mechanisms, for
which a grammar specific to such activity must be constructed; or (b) vari-
ation arises due to independent parsing/production systems which invoke
a process-neutral grammar. However, as dialogue research continues to de-
velop, there are intermediate possibilities, and in this paper we discuss the
approach developed within Dynamic Syntax (DS, Kempson et al 2001,
Cann et al 2005), a grammar framework within which, not only the parser,
but indeed ?syntax? itself are just a single mechanism allowing the pro-
gressive construction of semantic representations in context. Here we take
as a case study the set of phenomena classifiable as clarifications, reformu-
lations, fragment requests and corrections accompanied by extensions, and
argue that though these may seem to be uniquely constitutive of dialogue,
they are grounded in the mechanisms of apposition equivalently usable in
monologue for presenting reformulations, extensions, self-corrections etc.
2 Background
The data we focus on are non-repetitive fragment forms of acknowledge-
ments, clarifications and corrections (henceforth, A female, B male):
(1) A: Bob left.
B: (Yeah,) the accounts guy.
(2)
A: They X-rayed me, and took a urine sample, took a blood sample.
Er, the doctor
B: Chorlton?
A: Chorlton, mhm, he examined me, erm, he, he said now they
were on about a slight [shadow] on my heart.
128
(3) A: Bob left.
B: Rob?
A: (No,) (Bob,) the accounts guy.
Even though in the literature the fragments in (2)-(3) might be characterised
as illustrating distinct construction-types, in our view, they all illustrate how
speakers and hearers may contribute, in some sense to be made precise, to
the joint enterprise of establishing some shared communicative content, in
what might be loosely called split utterances. Even (1), an acknowledgement,
can be seen this way upon analysis: B?s addition is similar in structure to
an afterthought extension that might have been added by A herself to A?s
fully sentential utterance. It can be seen in (2) that such joint construction
of content can proceed incrementally: the clarification request in the form of
a reformulation is provided by B and resolved by A within the construction
of a single proposition. In (3) the fragment reply can be taken to involve
correction, in the sense that, according to the DS analysis of B?s fragment
question, he has provided content construable as equivalent to that derived
by processing Rob left? (see Kempson et al (2007)). Nevertheless such
corrections can also incorporate extensions in the above sense, enabling a
single conjoined propositional content to be derived in a single step.
It might seem that such illustration of diversity of fragment usage is am-
ple evidence of the need for conversation-specific rules. Indeed, Ferna?ndez
(2006) presents a thorough taxonomy, as well as detailed formal modelling
of Non-Sentential Utterances (NSUs), referring to contributions such as (1)
as repeated acknowledgements involving reformulation. Ferna?ndez models
such constructions via type-specific ?accommodation rules? which make a
constituent of the antecedent utterance ?topical?. The semantic effect of
acknowledgement is then derived by applying an appropriately defined ut-
terance type for such fragments to the newly constructed context. A distinct
form of contextual accommodation is employed to model so-called helpful
rejection fragments, as in (3) (without the reformulation), whereby a wh-
question is accommodated in the context by abstracting over the content
of one of the sub-constituents of the previous utterance. The content of
the correction is derived by applying this wh-question in the context to the
content of the fragment (see also Schlangen (2003) for another classification
and analysis).
In contrast, the alternative explored here is whether phenomena such
as (1)-(2), both of which are non-repetitive next-speaker contributions, can
be handled uniformly using the mechanisms for structure-building made
available in the core grammar, without recourse to construction-specific ex-
tensions of that grammar and contextual accommodation rules. This is
because, in our view, the range of interpretations these fragments receive
in actual dialogue seem to form continua with no well-defined boundaries
and mixing of functions (see also comments in Schlangen (2003)). Thus we
129
propose that the grammar itself simply provides mechanisms for process-
ing/integrating such fragments in the current structure while their precise
contribution to the interaction can be calculated by pragmatic inferencing
if needed (as in e.g. Schlangen (2003)) or, as seems most often to be the
case, be left underspecified without disruption to the dialogue.
One bonus of the stance taken here is the promise it offers for elucidating
the grammar-parser contribution to the disambiguation task. Part of the
challenge of modelling dialogue is the apparent multiplicity of interpretive
and structural options opened up during processing by the recurrent, of-
ten overlapping fragments as seen in (2) above. Thus, it might seem that
the rich array of elliptical fragments available in dialogue adds to its com-
plexity. However, an alternative point of view is to see such phenomena as
providing a window on how interlocutors exploit the incrementality afforded
by the grammar. The reliance of fragments on context for interpretation,
when employed incrementally, enables the hearer to immediately respond to
a previous utterance at any relevant point, in a constrained manner, with-
out ?recovering? a propositional unit. Three features of the Dynamic Syntax
model of dialogue (Purver et al (2006)), presented below, provide the flex-
ible control required for such processing: (a) word-by-word incrementality
(b) interaction with contextually provided information at every step of the
construction process (c) tight coordination of parsing and production.
3 Dynamic Syntax: A Sketch
Dynamic Syntax (DS ) is a parsing-based framework, involving strictly se-
quential interpretation of linguistic strings. The model is implemented via
goal-directed growth of tree structures and their annotations formalised us-
ing LOFT (Blackburn and Meyer-Viol (1994)), with modal operators ???, ???
to define concepts of mother and daughter, and their iterated counterparts,
??
?
?, ??
?
?, to define the notions be dominated by and dominate. Under-
specification and update are core aspects of the grammar itself and involve
strictly monotonic information growth for any dimension of tree structures
and annotations. Underspecification is employed at all levels of tree rela-
tions (mother, daughter etc.), as well as formulae and type values, each
having an associated requirement that drives the goal-directed process of
update. For example, an underspecified subject node of a tree may have a
requirement expressed in DS with the node annotation ?Ty(e), for which
the only legitimate updates are logical expressions of type entity (Ty(e));
but requirements may also take a modal form, e.g. ????Ty(e ? t), a con-
straint that the mother node be annotated with a formula of predicate type.
Requirements are essential to the dynamics informing the DS account: all
requirements must be satisfied if the construction process is to lead to a
successful outcome.
130
Semantic structure is built from lexical and general computational ac-
tions. Computational actions govern general tree-constructional processes,
such as introducing and updating nodes, as well as compiling interpretation
for all non-terminal nodes in the tree. Construction of only weakly spec-
ified tree relations (unfixed nodes) can also be induced, characterised only
as dominance by some current node, with subsequent update required. In-
dividual lexical items also provide procedures for building structure in the
form of lexical actions, inducing both nodes and annotations. Thus partial
trees grow incrementally, driven by procedures associated with particular
words as they are encountered, with a pointer, ?, recording the parser?s
progress (unlike van Leusen and Muskens (2003), partial trees are part of
the model and, unlike in other frameworks, incrementality is word-by-word
rather than sentence-by-sentence).
Complete individual trees are taken to correspond to predicate-argument
structures (with an event term associated with tense, suppressed in this
paper). The epsilon calculus (see e.g. Meyer-Viol (1995)) provides the se-
mantic representation language. Complex structures are obtained via a gen-
eral tree-adjunction operation licensing the construction of so-called linked
trees, hosting information that is eventually transferred onto the tree from
which the link is made (Kempson et al2001). Structures projected as such
paired trees range over restrictive relatives, nonrestrictive relatives, condi-
tionals, topic structures and appositions as here. As the semantic represen-
tations employ the epsilon calculus, eventual compound epsilon terms (e.g.
?, x, P (x)) are constructed incrementally through link-adjunction:
(4) A consultant, a friend of Jo?s, is retiring
Ty(t),Retire
?
((?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)))
Ty(e), (?, x, Consultant
?
(x) ? Friend
?
(Jo
?
)(x)) Ty(e? t), Retire
?
Ty(e), (?, x, Friend
?
(Jo
?
)(x))
Ty(cn), (x,Friend
?
(Jo
?
)(x))
x Friend
?
(Jo
?
)
Jo
?
Friend
?
Ty(cn? e), ?P.?, P
Underspecification of content as well as structure are central to facilitat-
ing successful linguistic interaction, our primary concern here. Pronouns,
the prototypical case, contribute a place-holding metavariable, noted as e.g.
U, plus an associated requirement for update by an appropriate term value:
??x.Fo(x). Equally, definite NPs contribute place-holders plus a constraint
providing a restriction/?presupposition? on the kind of entity picked out,
e.g., the man contributes the annotation U
Man
?
(U)
, T y(e). The subscript
specification is shorthand for a transition to a linked tree whose root node is
131
annotated with a formula Man
?
(U)
1
. The update of metavariables can be
accomplished if the context contains an appropriate term for substitution:
context involves storage of parse states, i.e., storing of partial tree, word se-
quence to date, plus the actions used in building up the partial tree (Purver
et al2006).
Scope dependencies between constructed terms or the index of evalua-
tion (e.g. S) are defined on completed propositional formulae, relative to
incrementally collected scope constraints (of the form x < y for constructed
terms containing variables x and y respectively). Constraints reflect on-line
processing considerations modulo over-riding lexical stipulations. For ex-
ample, proper names contribute as iota terms, i.e, epsilon terms reflecting
uniqueness in the context, ?, x,Bob
?
(x), and these project a scope depen-
dency solely on the index of evaluation reflecting their widest scope property
(cf Kamp and Reyle 1994). The structure projected from A?s utterance in
(1) is thus (5) (note that trees are the result of processing words but do
not encode the structure of strings, word order etc., only semantic content
derived in interaction with context, thus are the equivalents of DRSs):
(5)
S < x Ty(t),Leave
?
((?, x,Bob
?
(x)))
Ty(e), (?, x,Bob
?
(x)) Ty(e? t), Leave
?
The scope evaluation rule reflects the predicate-logic/epsilon-calculus equiv-
alence ?xF (x) ? F (?, x, F (x)) so evaluated terms eventually reflect their
containing structure. Hence, evaluation of (5) yields:
(6) Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
A major aspect of the DS dialogue model is that both generation and
parsing are goal-directed and incremental, and hence are governed by es-
sentially the same mechanism. Under this model, a human hearer-parser
builds a succession of partial parse trees based on what (s)he has heard
thus far. Importantly, however, unlike the conventional bottom-up parsing,
the DS model assumes a strong predictive element in parsing: a hearer is
assumed to entertain some goal to be reached eventually at any stage of
parsing. In (1), for example, as soon as the hearer encounters Bob, an un-
derspecified propositional tree is constructed, as in the first simplified and
schematised tree in Figure 1. Then the tree ?grows? monotonically, i.e. such
that at each word input, it is ?updated? to an ?incremented? tree that is
subsumed by the original tree, as depicted in the same Figure. This can be
described as a process of specifying the relevant nodes towards a complete
tree. This predictive element in DS allows a speaker-generator to be mod-
elled as doing exactly the same, i.e. going through monotonically updated
partial trees, the only difference being that (s)he also has available a more
1
These linked structures are suppressed in all diagrams.
132
fully specified goal tree representing what (s)he wishes to say, corresponding
to the rightmost tree in the Figure (with ?0? in the ?generation? row at the
bottom indicating it is entertained before utterance). Each licensed step in
generation, i.e. the utterance of a word, is governed by whatever step is
licensed by the parsing formalism, constrained via a required subsumption
relation of the goal tree. By updating their growing ?parse? tree relative
to the goal tree, speakers are licensed to produce the associated natural
language string.
Parsing: 1 2 3
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob
?
,?
?Ty(e ? t)
7?
?Ty(t)
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave
?
,?
7?
Ty(t),
Leave?(Bob?),?
q
q
q
q
q
q
q
M
M
M
M
M
M
M
Ty(e),
Bob?
Ty(e ? t),
Leave?
Generation: 1 2 0,3
Figure 1: Parallel parsing and generation in DS
This architecture allows for a dialogue model in which generation (what
a speaker does) and parsing (what a hearer does) function in parallel. The
speaker goes through partial trees subsuming a specified goal tree, while
the hearer attempts to ?mirror? the same series of partial trees, albeit not
knowing what the content of the unspecified nodes will be. For the dialogues
in (1)-(3), therefore, B as the hearer will have the partial representation of
what he has successfully parsed, required also for generation. This provides
him with the ability at any stage to become the speaker, interrupting to
ask for clarification, reformulating, or providing a correction, as and when
necessary. As we shall see, B?s parse tree reveals where need of clarifica-
tion or miscommunication occurs, as it will be at that node from which a
sub-routine extending it takes place
2
. According to our model of dialogue,
repeating or extending a constituent of A?s utterance by B is licensed only
if B, the hearer of A turned now a speaker, entertains currently a goal tree
that matches or extends the parse tree of what he has heard in a monotonic
fashion, although he only utters the relevant subpart of A?s utterance. In-
deed, this update is what B is seeking to clarify, correct or acknowledge. In
DS, B can reuse the already constructed (partial) parse tree in his context,
rather than having to rebuild an entire propositional tree or subtree
3
.
2
The account extends the implementation reported in Purver et al (2006)
3
Given the DS concept of linked trees projecting propositional content, we anticipate
that this mechanism will be extendable to fragment construal involving inference (see e.g.
Schlangen (2003), Schlangen and Lascarides (2003))
133
4 NSU fragments in Dynamic Syntax
4.1 Non-repetitive Acknowledgement
From a DS perspective, phenomena like reformulations as in (1), or exten-
sions to what one understands of the other speaker?s utterance, (2), can
be handled with exactly the same mechanisms as the sentence-internal phe-
nomenon independently identifiable as apposition, as in (4), and equally
usable by a single individual as a means of incrementally reformulating, cor-
recting or extending what they have just uttered. The update rule for such
structures, applicable to all terms, takes the two type e terms so formed and
yields a new term whose compound content is a combination of both.
We now have the basis for analysing extensions potentially functioning
as acknowledgements which build on what has been previously said as a way
of confirming the previous utterance. Recall (1), (2). There are two ways for
fragments which reformulate an interlocutor A?s utterance to occur: (a) as
interruptions of A?s utterance with immediate confirmation of identification
of the individual concerned, see (2); (b) as confirmations/extensions of A?s
utterance after the whole of her utterance has been integrated, see (1). Both
are modelled by DS as incremental additions.
Turning to (1), B?s response (Yeah,) the accounts guy constitutes a re-
formulation of A?s utterance and an extension of A?s referring expression,
yielding a similar content as that of an appositive expression Bob, the ac-
counts guy in this case jointly constructed. B?s reformulation/extension
counts in effect as an acknowledgement in virtue of signalling successful
processing of A?s utterance without objection raised. Thus there is no need
for a separate grammatical mechanism to process these structures. In DS
terms, after processing A?s utterance, B?s context consists of the following
tree:
(7) B?s Context for producing ?Yeah?
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x)) Leave
?
B, as a speaker, can now re-use this representation as point of departure
for generating the expression the accounts guy. In this case his goal tree,
the message to be expressed, will now be annotated with a composite term
made up from both the term recovered from parsing A?s utterance and the
new addition. This requires attaching a linked tree to the correct node
and an appropriate update of the context tree (for reasons of space, the
exact structure of the linked tree is condensed below, with subscripting as
shorthand):
134
(8) B?s goal tree:
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x))) ? Leave
?
(x))
(?, x,Bob
?
(x) ?Acc.guy
?
(x))
Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
In order to license generation of the expression the accounts guy, B now
needs to verify that processing these words in the context provided by the
tree in (7) will produce a tree that matches this goal tree in (8). To achieve
this, starting from (7), a series of simulated ?parse? trees are generated
which indeed result in the requisite matching. Steps include shifting the
pointer to the appropriate node, projection of a linked tree from that node
and test-processing the words the accounts guy, each step checking against
the goal tree that a subsumption relation between the current ?parse? tree
and the goal tree is always maintained:
(9) B?s parse tree licensing production of the accounts guy: link adjunction
Ty(t), Leave
?
(?, x,Bob
?
(x) ? Leave
?
(x))
(?, x,Bob
?
(x)) Leave
?
U
Acc.guy
?
(U)
,?
The only way to update this representation relative to both the restriction
on the metavariable and monotonicity of growth on any one node in a tree
involves replacing the metavariable with (?, x,Bob
?
(x)), as this is commen-
surate with an extension of the term annotating the node from which the
link transition was constructed:
(10) Updating B?s parse tree licensing production of the accounts guy
Ty(t), Leave
?
(?, x,Bob
?
(x) ?Acc.guy
?
(x) ? Leave
?
(x)),?
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Finally, the information is passed up to the top node of the main tree,
completing the parse tree to match B?s goal tree, (8), thus licensing the
utterance of the expression the accounts guy.
4.2 Non-repetitive Clarification
In the acknowledgement case above, the proposition relative to which the
linked structure is built is completed (with an already extended epsilon
term); but the same mechanism can be used when the interlocutor needs
clarification, prior to any such completion of the tree. In (2), B again, as
the speaker, takes as his goal tree a tree annotated with an expansion of the
term constructed from parsing A?s utterance but nevertheless picking out
the same individual. Using the very same mechanism as in (1) of building a
linked structure, B, interrupting A, provides a distinct expression, the name
Chorlton, this time before he has completed the parse tree for A?s utterance.
All that has been achieved at this point is the definite?s contribution of a
meta-variable with the restriction that the individual picked out must be a
doctor:
135
(11) A/B?s parse tree for Chorlton:
?Ty(t)
U
Doctor
?
(U)
,?
?Ty(e ? t)
(?, x, Chorlton
?
(x))
As in the acknowledgement case, but this time at the node initiating the
link transition, the only possible value to provide for the metavariable U
compatible both with its restriction and the monotoniticity constraint is
the composite term (?, x,Doctor
?
(x) ? Chorlton
?
(x)). The mechanism of
constructing paired structures involving type e terms across linked trees
is identical to that employed in B?s utterance in (1), though to a rather
different effect at this intermediate stage in the interpretation process. This
extension of the term is confirmed by A, this time replicating the composite
term which processing B?s utterance has led to. The eventual effect of the
process of inducing linked structures to be annotated by coreferential type e
terms may thus vary across monologue and different dialogue applications,
yielding different interpretations, but the mechanism is the same.
4.3 Correction
It might be argued nonetheless that correction is intrinsically a dialogue
phenomenon. Consider (3), for example. One of the possible interpretations
of (3), according to the DS analysis, is that B has offered the equivalent of the
content derived by processing Rob left?. That is, let?s assume here that B has
misheard and requests confirmation of what he has perceived A as saying.
A in turn rejects B?s understanding of her utterance and provides more
information. Presuming rejection as simple disagreement (i.e. the utterance
has been understood, but judged as incorrect), in DS terms, this means that
A has in mind a goal tree that licensed what she had produced, which is
distinct from the one derived by processing B?s clarification. As shown in
Kempson et al (2007), this means that A has been unable to process B?s
clarification request as an extension of her own context. Instead, she has to
parse the clarification by exploiting the potential for introducing an initially
structurally underspecified tree-node to accommodate the contribution of
the word Rob. Subsequently, by re-running the actions stored in context
previously by processing her own utterance of the word left, she is able to
complete the integration of the fragment in a new propositional structure.
Now, in order for A to produce the following correction, what is required
is for A to establish as the current most recent representation in context her
original goal tree. This can be monotonically achieved by recovering and
copying this original goal tree to serve as the current most immediate con-
text
4
. An option available to A at this point is to introduce, in addition
or exclusively, a reformulation of her original utterance in order to facilitate
4
Mistaken representations must be maintained in the context as they can provide an-
tecedents for subsequent anaphoric expressions.
136
identification of the named individual which proved problematic for B previ-
ously. She can answer B?s utterance of Rob? with (No,) Bob, the accounts
guy, as in (3) or simply with (No,) the accounts guy. Both are licensed
by the DS parsing mechanism without more ado. For both, the goal tree
will be as follows and it will always be the point of reference for checking
the subsumption relation relative to the simulated parsing steps described
further below:
(12) A?s goal tree
Ty(t), Leave
?
((?, x,Bob
?
(x) ?Acc.guy
?
(x)))
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) Leave
?
(?, x,Bob
?
(x))
Acc.guy
?
(x)
Under these circumstances, given the DS grammar-as-parser perspective,
several strategies are now available for the licensing of generation of the
fragment. A is licensed to repeat the name Bob by locally extending the
node in the context tree where the representation of the individual referred to
is located by using the rule of late*adjunction, a process which involves
building a node of type e from a dominating node of that type (illustrated in
Kempson et al 2007). An alternative way of licensing repetition of the word
Bob is to employ one of the strategies generally available for the parsing of
long distance dependencies i.e. constructing initial tree nodes as unfixed
(*adjunction). We show here how the latter strategy can be exploited to
license the production of the fragment by A.
(13) Parsing simulation licensing generation of Bob, the accounts guy
Step 1: *Adjunction Step 2: LINK-Adjuction + testing the accounts guy
?Ty(t)
(?, x,Bob
?
(x)),?
?Ty(t)
(?, x,Bob
?
(x)),?
U
Acc.guy
?
(U)
The only way to develop the constructed tree at Step 2 commensurate with
the goal tree (12) is to identify the value of U as (?, x,Bob
?
(x)), so this
is what is entered at the newly constructed linked tree, duly leading to
extension of the term originally given as annotating the unfixed node as
(?, x,Bob
?
(x) ? Acc.guy
?
(x)). The structure
5
derived by processing such an
extension is exactly that of (1) above (compare goal tree in (12) above
and tree in (8)). Now, as mentioned before, context, as defined in DS,
keeps track not only of tree representations and words but also of actions
contributed by the words and utilised in building up the tree representations.
Here, according to DS, production of the correction in (3) is licensed to be
5
Again note that DS trees represent derived content rather than structure over natural
language strings.
137
fragmental only because the original actions for parsing/producing the word
left are available in the context and can be recalled to complete the structure
initiated by processing/producing the name Bob. Now these stored actions
can be retrieved to develop the tree further:
(14) Parsing simulation licensing generation of Bob, the accounts guy
Step 3: test-processing stored actions for left
?Ty(t)
(?, x,Bob
?
(x) ?Acc.guy
?
(x)) ?Ty(e),? Leave?
(?, x,Bob
?
(x))
Acc.guy
?
(?,x,Bob
?
(x))
With this partial tree being commensurate with the goal tree, all actions that
follow are general computational processes for completing the tree: unifying
the unfixed node to determine the subject argument, applying the subject
to the predicate, evaluating the quantified terms. Nothing specific to this
structure is needed. Indeed, all these mechanisms are equally applicable by
an individual speaker, perhaps more familiar as right dislocation phenomena,
but equally available incrementally:
(15) Bob left, (Bob) the accounts guy.
5 Conclusion
As these fragments and their construal have demonstrated, despite serving
distinct functions in dialogue, the mechanisms which make such diversity
possible are general strategies for tree growth. In all cases, the advantage
which use of fragments provides is a ?least effort? means of re-employing
previous content/structure/actions which constitute the minimal local con-
text. As modelled in DS, it is more economical to reuse information from
this local context rather than constructing representations afresh (via costly
processes of lexical retrieval, choice of alternative parsing strategies, etc.).
A further quandary in dialogue construal is that, despite such avenues
for economising their efforts, interlocutors are nevertheless faced with an
increasing set of interpretive options at any point during the construction
of representations. One strategy available to hearers is to delay a disam-
biguating move until further input potentially resolves the uncertainty. How-
ever, as further input is processed and parsing/interpretive options increase
rapidly, the human processor struggles. The incremental definition of the
DS formalism allows for the modelling of an alternative strategy available
to hearers: at any (sub-sentential) point they could opt to intervene imme-
diately, and make a direct appeal to the speaker for more information as
illustrated by the clause-medial fragment interruption (2). It seems clear
that the grammar should allow the resources for modelling this behaviour
without any complications.
138
The phenomena examined here are also cases where speakers? and hear-
ers? representations, despite attempts at coordination, may nevertheless sep-
arate sufficiently for them to have to seek ?repair? (see especially (3)). In
the model presented here, the dynamics of interaction allow fully incremen-
tal generation and integration of fragmental utterances so that interlocutors
can be taken to constantly provide optimal evidence of each other?s represen-
tations with necessary adjuncts being able to be incrementally introduced.
But such mechanisms apply equally within an individual utterance, with self-
correction, extension, elaboration, repetition etc. The effect is that all the de-
vices which seem so characteristic of dialogue involve mechanisms invariably
available within an individual?s core grammar. This suggests a new inverse
methodology: it is the challenge of modelling dialogue that can be used as
a point of departure for modelling grammars for individual speakers, rather
than the other, more familiar, way round (see also Ginzburg (forthcmg)).
This reversibility is, notably, straightforwardly available to grammar for-
malisms in which the incremental dynamics of information growth is the
core structural concept because emergent dialogue structure crucially ex-
hibits and interpretively relies on such incrementality.
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962 and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex Davies, Arash Eshghi,
Jonathan Ginzburg, Pat Healey, Greg Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. Linguistics, logic and finite trees.
Bulletin of the IGPL, 2:3?31, 1994.
Raquel Ferna?ndez. Non-Sentential Utterances in Dialogue: Classification, Resolu-
tion and Use. PhD thesis, King?s College London, University of London, 2006.
Jonathan Ginzburg. Semantics for Conversation. CSLI, forthcmg.
Ruth Kempson, Andrew Gargett, and Eleni Gregoromichelaki. Clarification re-
quests: An incremental account. In Proceedings of the 11th Workshop on the
Semantics and Pragmatics of Dialogue (DECALOG), 2007.
Wilfried Meyer-Viol. Instantial Logic. PhD thesis, University of Utrecht, 1995.
Matthew Purver, Ronnie Cann, and Ruth Kempson. Grammars as parsers: Meeting
the dialogue challenge. Research on Language and Computation, 4(2-3):289?326,
2006.
David Schlangen. A Coherence-Based Approach to the Interpretation of Non-
Sentential Utterances in Dialogue. PhD thesis, University of Edinburgh, 2003.
David Schlangen and Alex Lascarides. The interpretation of non-sentential utter-
ances in dialogue. In Proceedings of the 4th SIGdial Workshop on Discourse and
Dialogue, pages 62?71, Sapporo, Japan, July 2003. Association for Computa-
tional Linguistics.
Noor van Leusen and Reinhard Muskens. Construction by description in discourse
representation. In J. Peregrin, editor, Meaning: The Dynamic Turn, chapter 12,
pages 33?65. 2003.
139
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 262?271,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Split Utterances in Dialogue: a Corpus Study
Matthew Purver, Christine Howes,
and Patrick G. T. Healey
Department of Computer Science
Queen Mary University of London
Mile End Road, London E1 4NS, UK
{mpurver,chrizba,ph}@dcs.qmul.ac.uk
Eleni Gregoromichelaki
Department of Philosophy
King?s College London
Strand, London WC2R 2LS, UK
eleni.gregor@kcl.ac.uk
Abstract
This paper presents a preliminary English
corpus study of split utterances (SUs), sin-
gle utterances split between two or more
dialogue turns or speakers. It has been
suggested that SUs are a key phenomenon
of dialogue, which this study confirms: al-
most 20% of utterances were found to fit
this general definition, with nearly 3% be-
ing the between-speaker case most often
studied. Other claims/assumptions in the
literature about SUs? form and distribu-
tion are investigated, with preliminary re-
sults showing: splits can occur within syn-
tactic constituents, apparently at any point
in the string; it is unusual for the sepa-
rate parts to be complete units in their own
right; explicit repair of the antecedent does
not occur very often. The theoretical con-
sequences of these results for claims in
the literature are pointed out. The prac-
tical implications for dialogue systems are
mentioned too.
1 Introduction
Split utterances (SUs) ? single utterances split be-
tween two or more dialogue turns/speakers ? have
been claimed to occur regularly in dialogue, espe-
cially according to the observations reported in the
Conversational Analysis (CA) literature, which is
based on the analysis of naturally occurring di-
alogues. SUs are of interest to dialogue theo-
rists as they are a clear sign of how turns cohere
with each other at all levels ? syntactic, seman-
tic and pragmatic. They also indicate the radi-
cal context-dependency of conversational contri-
butions. Turns can, in general, be highly ellip-
tical and nevertheless not disrupt the flow of the
dialogue. SUs are the most dramatic illustration
of this: contributions spread across turns/speakers
rely crucially on the dynamics of the unfolding
context, linguistic and extra-linguistic, in order to
guarantee successful processing and production.
Utterances that are split across speakers also
present a canonical example of participant coor-
dination in dialogue. The ability of one partic-
ipant to continue another interlocutor?s utterance
coherently, both at the syntactic and the seman-
tic level, suggests that both speaker and hearer are
highly coordinated in terms of processing and pro-
duction. The initial speaker must be able to switch
to the role of hearer, processing and integrating the
continuation of their utterance, whereas the ini-
tial hearer must be closely monitoring the gram-
mar and content of what they are being offered
so that they can take over and continue in a way
that respects the constraints set up by the first part
of the utterance. In fact there is (anecdotal) ev-
idence that such constraints are fully respected
across speaker and hearer in such utterances (see
e.g. Gregoromichelaki et al (2009)). A large pro-
portion of the CA literature on SUs tries to iden-
tify the conditions under which SUs usually oc-
cur (see section 2). However, this emphasis seems
to miss the important generalisation, confirmed
by the present study, that, syntactically, a speaker
switch may be able to occur anywhere in a string.
From a theoretical point of view, the implica-
tions of the above are that, if such observations
have an empirical foundation, the grammar em-
ployed by the interlocutors must be able to license
and the semantics interpret chunks much smaller
than the usual sentence/proposition units. More-
over, these observations have implications for the
nature of the grammar itself: dynamic, incremen-
tal formalisms seem more amenable to the mod-
262
elling of this phenomenon as the switch of roles
while syntactic/semantic dependencies are pend-
ing can be taken as evidence for direct involve-
ment of the grammar in the successful process-
ing/production of such utterances. Indeed, Poesio
and Rieser (to appear) claim that ?[c]ollaborative
completions . . . are among the strongest evidence
yet for the argument that dialogue requires coor-
dination even at the sub-sentential level? (italics
original).
From a psycholinguistic point of view, the phe-
nomenon of SUs is compatible with mechanis-
tic approaches as exemplified by the Interactive
Alignment model of Pickering and Garrod (2004)
where it is claimed that it should be as easy to
complete someone else?s sentence as one?s own
(Pickering and Garrod, 2004, p186). According
to this model, speaker and listener ought to be in-
terchangeable at any point. This is also the stance
taken by the grammatical framework of Dynamic
Syntax (DS) (Kempson et al, 2001; Cann et al,
2005). In DS, parsing and production are taken
to employ the same mechanisms, leading to a pre-
diction that split utterances ought to be strikingly
natural (Purver et al, 2006). However, from a
pragmatic point of view, utterance continuation
by another speaker might involve some kind of
guessing1 or preempting the other interlocutor?s
intended content. It has therefore been claimed
that a full account of this phenomenon requires
a complete model of pragmatics that can handle
intention recognition and formation. Indeed, Poe-
sio and Rieser (to appear) claim that ?the study
of sentence completions . . . may be used to com-
pare competing claims about coordination ? i.e.
whether it is best explained with an intentional
model like Clark (1996)?s . . . or with a model
based on simpler alignment models like Pickering
and Garrod (2004)?s.? They conclude that a model
which includes modelling of intentions better cap-
tures the data.
For computational models of dialogue, how-
ever, SUs pose a challenge. While Poesio and
Rieser (to appear) and Purver et al (2006) pro-
vide general foundational models for various parts
of the phenomenon, there are many questions that
remain if we are to begin automatic processing.
A computational dialogue system must be able
to identify SUs, match up their two (or more)
1Note that this says nothing about whether such a contin-
uation is the same as the initial speaker?s intended continua-
tion.
parts (which may not necessarily be adjacent), in-
tegrate them into some suitable syntactic and/or
semantic representation, and determine the over-
all pragmatic contribution to the dialogue context.
SUs also have implications for the organisation of
turn-taking in such models (see e.g. Sacks et al
(1974)), as regards what conditions (if any) allow
or prevent successful turn transfer. Additionally,
from a socio-linguistic point of view, turn-taking
operates (according to Schegloff (1995)) not on
individual conversational participants, but on ?par-
ties?. Lerner (1991) suggests that split utterances
can clarify the formation of such parties in that
they reveal evidence of how syntax can be em-
ployed to organise participants into ?groups?.
Analysis of SUs, when they can or cannot oc-
cur, and what effects they have on the coordina-
tion of agents in dialogue, is therefore an area of
interest not only for conversational analysts wish-
ing to characterise systematic interactions in di-
alogue, but also for linguists trying to formulate
grammars of dialogue, psychologists and sociolin-
guists interested in alignment mechanisms and so-
cial interaction, and those interested in building
automatic dialogue processing systems. In this pa-
per we present and examine empirical corpus data
in order to shed light on some of the questions and
controversies around this phenomenon.
2 Related Work
Most previous work on what we call SUs has ex-
amined specific sub-cases, generally of the cross-
speaker type, and have referred to these vari-
ously as collaborative turn sequences (Lerner,
1996; Lerner, 2004), collaborative completions
(Clark, 1996; Poesio and Rieser, to appear),
co-constructions (Sacks, 1992), joint produc-
tions (Helasvuo, 2004), co-participant comple-
tions (Hayashi, 1999; Lerner and Takagi, 1999),
collaborative productions (Szczepek, 2000) and
anticipatory completions (Fox and others, 2007)
(amongst others). Here we discuss some of these
views.
Conversation Analysis Lerner (1991) identifies
various structures typical of SUs which contain
characteristic split points. Firstly he gives a
number of ?compound? turn-constructional units
(TCUs), i.e., structures that include an initial con-
stituent that hearers can identify as introducing
some later final component. Examples include the
IF X-THEN Y, WHEN X-THEN Y and INSTEAD
263
OF X-Y constructions:
(1) A: Before that then if they were ill
G: They get nothing. [BNC H5H 110-111]
Other cues for potential anticipatory completions
include quotation markers (e.g. SHE SAID), paren-
thetical inserts and lists, as well as non-syntactic
cues such as contrast stress or prefaced disagree-
ments. Ru?hlemann (2007) uses corpus analysis to
examine sentence relatives as typical expansions
of another interlocutor?s turn (see also (16)):
(2) A: profit for the group is a hundred and
ninety thousand pounds.
B: Which is superb. [BNC FUK 2460-2461 ]
Opportunistic Cases Although Lerner focuses
on these projectable turn completions, he also
mentions that splits can occur at other points such
as ?intra-turn silence?, hesitations etc. which he
terms opportunistic completions:
(3) A: Well I do know last week thet=uh Al was
certainly very ? pause 0.5?
B: pissed off [(Lerner, 1996, p260)]
As he makes no claims regarding the frequency
of such devices for SUs, it would be interesting to
know how common these are (insomuch as they
occur at all and can be accordingly classified), es-
pecially as studies on SUs in Japanese (Hayashi,
1999) show that although SUs do occur, they do
not rely on compound TCUs.
Expansions vs. Completions Other classifica-
tions of SUs often distinguish between expansions
and completions (Ono and Thompson, 1993). Ex-
pansions are continuations which add, e.g., an ad-
junct, to an already complete syntactic element:
(4) T: It?ll be an E sharp.
G: Which will of course just be played as an
F. [BNC G3V 262-263]
whilst completions involve the addition of syntac-
tic material which is required to make the whole
utterance complete:
(5) A: . . . and then we looked along one deck, we
were high up, and down below there were
rows of, rows of lifeboats in case you see
B: There was an accident.
A: of an accident [BNC HDK 63-65]
In terms of frequency, the only estimate we
know of is Szczepek (2000), where there are ap-
parently 200 cross-person SUs in 40 hours of En-
glish conversation (there is no mention of the num-
ber of sentences or turns this equates to), of which
75% are completions.2 As briefly outlined above,
CA analyses of SUs tend to be broadly descriptive
of what they reveal for conversational practices.
Because such analyses present real examples they
establish that the phenomenon is a genuine one;
however, there is no discussion of its scale (with
the exception of Szczepek (2000), which offers ex-
tremely limited figures). Even though as a gen-
uine phenomenon it is of theoretical interest, the
lack of frequency statistics prevents generalisabil-
ity. Therefore, any claims that SUs are pervasive
in dialogue need empirical backing.
Linguistic Models Purver et al (2006) present
a grammatical model for split utterances, using an
inherently incremental grammar formalism, Dy-
namic Syntax (Kempson et al, 2001; Cann et al,
2005). This model shows how syntactic and se-
mantic processing can be accounted for no mat-
ter where the split occurs in a sentence; how-
ever, as their interest is in grammatical process-
ing, they give no account of any higher-level in-
ferences which may be required. Poesio and
Rieser (to appear) present a general model for col-
laborative completions based in the PTT frame-
work, using an incremental LTAG-based gram-
mar and an information-state-based approach to
context modelling. While many parts of their
model are compatible with a simple alignment-
based communication model like Pickering and
Garrod (2004)?s, they see intention recognition as
crucial to dialogue management. They conclude
that an intention-based model, more like Clark
(1996)?s, is more suitable. Their primary concern
is to show how such a model can account for the
hearer?s ability to infer a suitable continuation, but
their use of an incremental interpretation method
also allows an explanation of the low-level utter-
ance processing required. Nevertheless, the use
of an essentially head-driven grammar formalism
suggests that some syntactic splits that appear in
our corpus might be more problematic than oth-
ers.
Corpus Studies Skuplik (1999), as reported by
Poesio and Rieser (to appear), collected data from
German two-party task-oriented dialogue, and an-
notated for split utterance phenomena. She found
that expansions (cases where the part before the
split can be considered already complete) were
2However, this could be affected by her decision not to
include what she calls appendor questions in her data which
could also be argued to be expansion SUs.
264
more common than completions (where the first
part is incomplete as it stands). Given that this
study focuses on task-oriented dialogue, it needs
to be shown that its results can be replicated in nat-
urally occurring dialogue. In addition, de Ruiter
and van Dienst (in preparation) are also in the pro-
cess of studying other-initiated completions, in the
above sense, and their effect on the progressivity
of dialogue turns; however no results are available
to us at this point in time.
Dialogue Models We are not aware of any
system/model which treats other-person splits,
but same-person ones are now being looked at.
Skantze and Schlangen (2009) present an incre-
mental system design (for a limited domain) which
can react to user feedback, e.g., backchannels, and
resume with utterance completion if interrupted.
Some related empirical work regarding the issue
of turn-switch addressed here is also presented in
Schlangen (2006) but the emphasis there centered
mostly on prosodic rather than grammar/theory-
based factors.
3 Method
3.1 Terminology
In this paper, as our interest is general, we use the
term split utterances (SUs) to cover all instances
where an utterance is spread across more than one
dialogue contribution ? whether the contributions
are by the same or different speakers. We there-
fore use the term split point to refer to the point at
which the utterance is split (rather than e.g. tran-
sition point which is associated with a speaker
change). Cases where speaker does change across
the split will be called other-person splits; oth-
erwise same-person splits. One of the reasons
for including same-person splits is that there are
claims in the literature that the initial speaker may
strategically continue completing their own utter-
ance, after another person?s intervention, as an al-
ternative to acceptance or rejection of this inter-
vention (delayed completion, (Lerner, 1996)). In
addition, both grammatical formalisms (Purver et
al., 2006) and psycholinguistic models (Picker-
ing and Garrod, 2004) predict that SUs should be
equally natural in both the same- and other- person
conditions.
As not all cases will lead to complete contri-
butions, and not all will be split over exactly two
contributions, we also avoid terms like first-half,
second-half and completion: instead the contri-
butions on either side of a split point will be re-
ferred to as the antecedent and the continuation.
In cases where an utterance has more than one split
point, some portions may therefore act as the con-
tinuation for one split point, and the antecedent for
the next.
3.2 Questions
General Our first interest is in the general statis-
tics regarding SUs: how often do they occur, and
what is the balance between same- and other-
person splits? Do they usually fall into the specific
categories (with specific preferred split points) ex-
amined by e.g. Lerner (1991), or can the split
point be anywhere?
Completeness For a grammatical treatment
of SUs, as well as for implementing pars-
ing/production mechanisms for their processing,
we need to know about the likely completeness
of antecedent and continuation (if they are al-
ways complete in their own right, a standard head-
driven grammar may be suitable; if not, some-
thing more fundamentally incremental may be re-
quired). In addition, CA and other strategic anal-
yses of dialogue phenomena predict that split ut-
terances should occur at turn-transfer points that
are foreseeable by the participants. Complete syn-
tactic units serve this purpose from this point of
view and lack of such completeness will seem
to weaken this general claim. We therefore ask
how often antecedents and continuations are them-
selves complete,3 and look at the syntactic and lex-
ical categories which occur either side of the split.
Repair and Overlap Thirdly, we look at how
often splits involve explicit repair of antecedent
material, and how this depends on antecedent
completeness. Although, sometimes, repair might
be attributed to overlap or speaker uncertainty, it
also might indicate issues regarding preemptive
tactics on the part of the current speaker who needs
to reformulate the original contribution in order
to accommodate their novel offering or take into
account feedback offered while constructing their
utterance. Amount of repair also indicates the de-
gree of attempt the current speaker is making to
3For antecedents, we are more interested in whether they
end in a way that seems complete (they may have started ir-
regularly due to overlap or another split); for continuations,
whether they start in such a way (they may not get finished
for some other reason, but we want to know if they would be
complete if they do get finished).
265
Tag Value Explanation
end-complete y/n For all sentences: does this sentence end in such a way as to
yield a complete proposition or speech act?
continues sentence ID For all sentences: does this sentence continue the proposition
or speech act of a previous sentence? If so, which one?
repairs number of words For continuations: does this continuation explicitly repair
words in the antecedent? If so, how many?
start-complete y/n For continuations: does this continuation start in such a way as
to be able to stand alone as a complete proposition or speech
act?
Table 1: Annotation Tags
integrate syntactically their contribution with the
antecedent. However, we also examine how often
continuations involve overlap, which also has im-
plications for turn-taking management, and how
this depends on antecedent completeness.
3.3 Corpus
For this exercise we used the portion of the
BNC (Burnard, 2000) annotated by Ferna?ndez and
Ginzburg (2002), chosen to maintain a balance be-
tween context-governed dialogue (tutorials, meet-
ings, doctor?s appointments etc.) and general con-
versation. This portion comprises 11,469 sen-
tences taken from 200-turn sections of 53 separate
dialogues.
The BNC transcripts are already annotated for
overlapping speech, for non-verbal noises (laugh-
ter, coughing etc.) and for significant pauses.
Punctuation is included, based on the original au-
dio and the transcribers? judgements; as the au-
dio is not available, we allowed annotators to use
punctuation where it aided interpretation. The
BNC transcription protocol provides a sentence-
level annotation as well as an utterance (turn)-level
one, where turns may be made of several sentences
by the same speaker. We annotated at a sentence-
level, to allow self-continuations within a turn to
be examined. The BNC also forces turns to be
presented in linear order, which is vital if we are
to accurately assess whether turns are continua-
tions of one another; however, this has a side-
effect of forcing long turns to appear split into sev-
eral shorter turns when interrupted by intervening
backchannels. We will discuss this further below.
Annotation Scheme The initial stage of manual
annotation involved 4 tags: start-complete,
end-complete, continues and repairs ?
these are explained in Table 1 above. Sentences
which somehow require continuation (whether
they receive it or not) are therefore those marked
end-complete=n; sentences which act as
continuations are those marked with non-empty
continues tags; and their antecedents are the
values of those continues tags. Further specific
information about the syntactic or lexical nature of
antecedent or continuation components could then
be extracted (semi-)automatically, using the BNC
transcript and part-of-speech annotations.
Inter-Annotator Agreement Three annotators
were used, all linguistically knowledgeable. First,
all three annotators annotated one dialogue inde-
pendently, then compared results and discussed
differences. They then annotated 3 further di-
alogues independently to assess inter-annotator
agreement; kappa statistics (Carletta, 1996) are
shown in Table 2 below.
Tag KND KBG KB0
end-complete .86-.92 .80-1.0 .73-.90
continues (y/n) .89-.81 .76-.85 .77-.89
continues (ant) .90-.82 .74-.85 .76-.86
repairs 1.0-1.0 .55-.81 1.0-1.0
Table 2: Inter-Annotator ? statistic (min-max)
With the exception of the repairs tag for one
annotator pair for one dialogue, all are above 0.7;
the low figure results from a few disagreements
in a dialogue with only a very small number of
repairs instances. The remaining dialogues
were divided evenly between the three annotators.
4 Results and Discussion
The 11,469 sentences annotated yielded 2,228
SUs, of which 1,902 were same-person and 326
other-person splits; 111 examples involved an ex-
plicit repair by the continuation of some part of the
antecedent.
266
person: same other
overlapping 0 17
adjacent 840 260
sep. by overlap 320 10
sep. by backchnl 460 17
sep. by 1 sent 239 16
sep. by 2 sents 31 4
sep. by 3 sents 5 1
sep. by 4 sents 4 0
sep. by 5 sents 1 0
sep. by 6 sents 2 1
Total 1902 326
Table 3: Antecedent/continuation separation
General Same-person splits are much more
common than other-person; however, this is partly
an artefact of the BNC transcription protocol
(which forces contributions to be linearly ordered)
and our choice to annotate at the sentence level.
Around 44% of same-person cases are splits be-
tween sentences within the same-speaker turn;
and a further 17% are separated only by other-
speaker material which entirely overlaps with the
antecedent and therefore does not necessarily ac-
tually interrupt the turn. Both of these might be
considered as single utterances under some views.
However, we believe that splits between same-
turn sentences must be investigated in that the
transcription into separate sentences does indicate
some pause or other separating prosody and, from
a processing/psycholinguistic point of view, it
should be determined whether other-person splits
occur in the same places as same-person split
boundaries. Even in cases of overlap, one can-
not exclude the fact that the shape of the current
speaker?s utterance is influenced by receipt of the
feedback. Nevertheless, we will examine these
issues in further research and hence we exclude
within-turn splits of this type from here on.
Many splits are non-adjacent (see Table 3), with
the antecedent and continuation separated by at
least one intervening sentence. In same-person
cases, once we have excluded the within-turn
splits described above, this must in fact always
be the case; the intervening material is usually a
backchannel (62% of remaning cases) or a sin-
gle other sentence (32%, often e.g. a clarification
question), but two intervening sentences are possi-
ble (4%) with up to six being seen. In other-person
cases, 88% are adjacent or separated only by over-
lapping material, but again up to six intervening
person: same other
and/but/or 748 116
so/whereas 257 39
because 77 3
(pause) 56 5
which/who/etc 26 4
instead of 4 1
said/thought/etc 14 0
if then 1 0
when then 1 1
(other) 783 161
Table 4: Continuation categories
sentences were seen, with a single sentence most
common (10%, in half of which the intervening
sentence was a backchannel).
Many utterances have more than one split. In
same-person cases, a single utterance can be split
over as many as thirteen individual sentence con-
tributions; although such extreme cases occur gen-
erally within one-sided dialogues such as tutori-
als, many multi-split cases are also seen in general
conversation. Only 63% of cases consisted of only
two contributions. Antecedents can also receive
more than one competing continuation, although
this is rare: two continuations are seen in 2% of
cases.
CA Categories We searched for examples
which match CA categories (Lerner, 1991;
Ru?hlemann, 2007) by looking for particular lex-
ical items on either side of the split. Matching was
done loosely, to allow for the ungrammatical na-
ture of dialogue ? for example, an instance was
taken to match the IF X-THEN Y pattern if the con-
tinuation began with ?then? (modulo filled pauses
and non-verbal material) and the antecedent con-
tained ?if? at any point) ? so the counts may be
over-estimates. For Lerner (1996)?s opportunistic
cases, we looked for filled pauses (?er/erm? etc.)
or pauses explicitly annotated in the transcript, so
counts in this case may be underestimates.4 We
also chose some other broad categories based on
our observations of the most common cases. Re-
sults are shown in Table 4.5
The most common of the CA categories can be
4In further research we will examine other features as spe-
cialised laugh tokens, repetitions etc. as well as their particu-
lar positioning
5Note that the categories in Table 4 are not all mutually
exclusive (e.g. an example may have both an ?and?-initial
continuation and an antecedent ending in a pause), so column
sums will not match Table 3.
267
seen to be Lerner (1996)?s hesitation-related op-
portunistic cases, which make up at least 2-3% of
both same- and other-person splits. Ru?hlemann
(2007)?s sentence relative clause cases are next,
with over 1%; the others make up only small pro-
portions.
In contrast, by far the most common pattern (for
both same- and other-) is the addition of an ex-
tending clause, either a conjunction introduced by
?and/but/or/nor? (35-40%), or other clause types
with ?so/whereas/nevertheless/because?. Other
less obviously categorisable cases make up 40-
50% of continuations, with the most common first
words being ?you?, ?it?, ?I?, ?the?, ?in? and ?that?.
Completeness and repair Examination of the
end-complete annotations shows that about
8% of sentences in general are incomplete, but
that (perhaps surprisingly) only 63% of these get
continued. For both same- and other-person con-
tinuations, the vast majority (72% and 74%) con-
tinue an already complete antecedent, with only
26-28% therefore being completions in the sense
of e.g. de Ruiter and van Dienst (in preparation).
This does, however, mean that continuations are
significantly more likely than other sentences to
follow an incomplete antecedent (p < 0.001 us-
ing ?2(1)). Interestingly, though, continuations are
no more likely than other sentences to be complete
themselves.
The frequent clausal categories from Table 4 are
all more likely to continue complete antecedents
than incomplete ones, with the exception of the
(other) category; this suggests that split points
often occur at random points in a sentence, without
regard to particular clausal constructions (see also
A.1 for more examples and context):
(6) D: you know what the actual variations
U: entails
D: entails. you know what the actual quality
of the variations are.
[BNC G4V 114-117]
For the less frequent (e.g. ?if/then?, ?instead of?)
categories, the counts are too low to be sure.
Excluding all the clausal constructions (i.e.
looking only at the general (other) category),
and looking only at other-person cases, we see that
antecedents often end in a complete way (53%) but
that continuations do not often start in a complete
way (24%). Continuations are more than twice
as likely to start in a non-complete as opposed
to complete way, even after complete antecedents.
Explicit repair of some portion of the antecedent
is not common, only occurring in just under 5%
of splits. As might be expected, incomplete an-
tecedents are more likely to be repaired (13% vs.
2%, p < 0.001 using ?2(1)). Other-continuations
are also significantly more likely to repair their an-
tecedents than same-person cases (10% vs. 4%,
p < 0.001 using ?2(1)).
Problematic cases Examination of the data
shows that SUs is not necessarily an autonomous
well-defined category independent of other frag-
ment classifications in the literature. Besides cases
where it is not easy to identify whether a fragment
is a continuation or not or what the antecedent
is (see A.2), there are also cases where, as has
already been pointed out in the literature (Gre-
goromichelaki et al, 2009; Bunt, 2009), fragments
exhibit multifunctionality. This can be illustrated
by the following where the continuation could be
taken also as request for confirmation/question (7)
or a reply to a clarification request (8):
(7) M: It?s generated with a handle and
J: Wound round?
M: Yes [BNC K69 109-112]
(8) S: Quite a good word processor.
J: A word processor?
S: Which is vag- it?s basically a subset of
Word. [BNC H61 37-39]
In this respect, an interesting category is Lerner?s
delayed completions where often the continuation
also serves as some kind of repair or reformulation
(see e.g. (6) and A.3 (26)).
5 Conclusions
Although most of Lerner (1991)?s categories ap-
pear, they are not necessarily the most frequent.
On the other hand, the general results seem to in-
dicate that splits can occur anywhere in a string,
both in the same- or other- conditions. Both these
are consistent with models that advocate highly
coordinated resources between interlocutors and,
moreover, the need for highly incremental means
of processing (Purver et al, 2006; Skantze and
Schlangen, 2009). From a computational mod-
elling point of view, the results also indicate that
start-completeness of continuations is rare, which
means that a dialogue system has a chance of spot-
ting continuations from surface characteristics of
268
the input. This is hampered though by the fact
that the split can occur within any type of syn-
tactic constituent, hence no reliable grammatical
features can be employed securely. On the other
hand, end-incompleteness of antecedents is not as
common as would be expected and long distances
between antecedent and continuation are possible.
In this respect, locating the antecedent is not a
straightforward task for automated systems, espe-
cially again as this can be any type of constituent.
References
H. Bunt. 2009. Multifunctionality and multidimen-
sional dialogue semantics. In Proceedings of Dia-
Holmia, 13th SEMDIAL Workshop.
L. Burnard. 2000. Reference Guide for the British Na-
tional Corpus (World Edition). Oxford University
Computing Services http://www.natcorp.
ox.ac.uk/docs/userManual/.
R. Cann, R. Kempson, and L. Marten. 2005. The Dy-
namics of Language. Elsevier, Oxford.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?255.
H. Clark. 1996. Using Language. Cambridge Univer-
sity Press.
J. de Ruiter and M. van Dienst. in preparation. Com-
pleting other people?s utterances: evidence for for-
ward modeling in conversation. ms.
R. Ferna?ndez and J. Ginzburg. 2002. Non-sentential
utterances: A corpus-based study. Traitement Au-
tomatique des Langues, 43(2).
A. Fox et al 2007. Principles shaping grammati-
cal practices: an exploration. Discourse Studies,
9(3):299.
E. Gregoromichelaki, Y. Sato, R. Kempson, A. Gargett,
and C. Howes. 2009. Dialogue modelling and the
remit of core grammar. In Proceedings of IWCS.
M. Hayashi. 1999. Where Grammar and Interac-
tion Meet: A Study of Co-Participant Completion in
Japanese Conversation. Human Studies, 22(2):475?
499.
M. Helasvuo. 2004. Shared syntax: the gram-
mar of co-constructions. Journal of Pragmatics,
36(8):1315?1336.
R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001.
Dynamic Syntax: The Flow of Language Under-
standing. Blackwell.
G. Lerner and T. Takagi. 1999. On the place
of linguistic resources in the organization of talk-
in-interaction: A co-investigation of English and
Japanese grammatical practices. Journal of Prag-
matics, 31(1):49?75.
G. Lerner. 1991. On the syntax of sentences-in-
progress. Language in Society, pages 441?458.
G. Lerner. 1996. On the semi-permeable character
of grammatical units in conversation: Conditional
entry into the turn space of another speaker. In
E. Ochs, E. A. Schegloff, and S. A. Thompson,
editors, Interaction and grammar, pages 238?276.
Cambridge University Press.
G. Lerner. 2004. Collaborative turn sequences. In
Conversation analysis: Studies from the first gener-
ation, pages 225?256. John Benjamins.
T. Ono and S. Thompson. 1993. What can conversa-
tion tell us about syntax. In P. Davis, editor, Alterna-
tive Linguistics: Descriptive and Theoretical Modes.
Benjamin.
M. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?226.
M. Poesio and H. Rieser. to appear. Completions, co-
ordination, and alignment in dialogue. Ms.
M. Purver, R. Cann, and R. Kempson. 2006.
Grammars as parsers: Meeting the dialogue chal-
lenge. Research on Language and Computation,
4(2-3):289?326.
C. Ru?hlemann. 2007. Conversation in context: a
corpus-driven approach. Continuum.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
H. Sacks. 1992. Lectures on Conversation. Blackwell.
E. Schegloff. 1995. Parties and talking together: Two
ways in which numbers are significant for talk-in-
interaction. Situated order: Studies in the social
organization of talk and embodied activities, pages
31?42.
D. Schlangen. 2006. From reaction to prediction: Ex-
periments with computational models of turn-taking.
In Proceedings of the 9th International Conference
on Spoken Language Processing (INTERSPEECH -
ICSLP).
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the ACL (EACL 2009).
K. Skuplik. 1999. Satzkooperationen. definition und
empirische untersuchung. SFB 360 1999/03, Biele-
feld University.
B. Szczepek. 2000. Formal Aspects of Collaborative
Productions in English Conversation. Interaction
and Linguistic Structures (InLiSt), http://www.
uni-potsdam.de/u/inlist/issues/17/.
269
A Examples
A.1 Split points
(6) D: Yeah I mean if you?re looking at quan-
titative things it?s really you know how
much actual- How much variation hap-
pens whereas qualitative is ?pause? you
know what the actual variations
U: entails
D: entails. you know what the actual quality
of the variations are.
[BNC G4V 114-117]
(9) A: All the machinery was
G: [[All steam.]]6
A: [[operated]] by steam
[BNC H5G 177-179]
(10) K: I?ve got a scribble behind it, oh annual re-
port I?d get that from.
S: Right.
K: And the total number of [[sixth form stu-
dents in a division.]]
S: [[Sixth form stu-
dents in a division.]] Right.
[BNC H5D 123-127]
(11) M: 292 And another sixteen percent is the
other Ne- Nestle coffee ?pause? erm
Blend Thirty Seven which I used to drink
a long time ago and others ?laugh? and
twenty two percent is er ?pause?
U: Maxwell.
M: Maxwell House, which has become the
other local brand now seeing as how
Maxwell House is owned by Kraft, and
Kraft now own Terry?s.
[BNC G3U 292-294]
(12) A: Erm because as Moira said that Kraft is
erm ?pause? now what was she saying,
what was she saying Kraft is the same as
?pause?
M: Craft? [BNC G3U 412-413]
(13) J: And I couldn?t remember whether she
said at the end of the three months or
A: End of the month. [BNC H4P 17-18]
6Overlapping material is shown in double square brackets,
aligned with the material with which it co-occurs.
(14) G: Had their own men
A: unload the boats?
G: unload the boats, yes. [BNC H5H 91-93]
(15) G: That?s right they had to go on a rota.
A: Run by the Dock Commission?
G: Run by the Dock Commission.
[BNC H5H 100-102]
(16) A: So I thought, oh, I think I?ll put lace over
it, it?ll tone the lilac [[down.]]
B: [[down.]] Yes.
Which it is has done
[BNC KBC 3195-3198]
A.2 Uncertain antecedents
(17) C: Look you?re cleaning this ?pause?
[[with erm]]
G: [[That box.]]
C: [[This.]]
G: [[With]] this. [[And this.]]
C: [[And this.]] [[And this.]]
G: [[And this.]]
Whoops! [BNC KSR 9-17]
(18) S: You?re trying to be everything ?pause?
and they?re pushing it away cos it?s not
what they really want ?pause? and they, I
mean, all, all you can get from him is how
marvellous, you?re right, how marvellous
his brothers are ?pause? and yet, what I?ve
heard of the brothers they?re not
C: Not much, [[yeah.]]
S: [[they?re]] not all that marvel-
lous, they?re not really that much to look
[[up]]
C: [[Ah]].
S: to.
C: No [BNC KBG 76-81]
(19) S: Well this is why I think he?d be better
off, hi- his needs ?pause? are not met by a
class teacher. And I don?t think they have
been for this last
C: Mm, we need a support teacher [[to go
there.]]
S: [[for the
last]] year. But yo-, you need somebody
who?s gonna work with him every day
?pause? and ?pause? with an individual
programme and you just can?t offer that
?pause? in a class. [BNC KBG 56-60]
270
(20) M: I might be a bit biased, I think they still
do that but I think erm ?pause?
J: The television has ?pause?
M: the television has made a difference. I
think not only just at fire stations, I think
in the whole of life, hasn?t it?
[BNC K69 51-54]
(21)A5: I?ll definitely use that
U: ?reading?:[ Get a headache ]?
A5: [[in getting to know ]]
A2: [[Year seven ]]
A5: new [[year seven]]
A2: [[Oh yeah]] for year seven
[BNC J8D 190-195]
(22) G: Well a chain locker is where all the spare
chain used to like coil up
A: So it ?unclear? came in and it went round
G: round the barrel about three times round
the barrel then right down into the chain
locker but if you kept, let it ride what we
used to call let it ride well ?unclear? well
now it get so big then you have to run it
all off cos you had one lever, that?s what
you had and the steam valve could have
all steamed. [BNC H5G 174:176]
A.3 Multifunctionality of fragments
(7) Completion and confirmation request:
J: How does it generate?
M: It?s generated with a handle and
J: Wound round?
M: Yes, wind them round and this should,
should generate a charge which rang bells
and sounded bells and then er you lift up a
telephone and plug in a jack and, and take
a message in that way.
[BNC K69 109-112]
(23) Completion and confirmation request:
G: Had their own men
A: unload the boats?
G: unload the boats, yes. [BNC H5H 91-93]
(24) Late completion and (repetitive) confir-
mation:
N: Alistair [last or full name] erm he?s, he?s
made himself er he has made himself co-
ordinator.
U: And section engineer.
N: And section engineer.
N: I didn?t sign it as coordinator.
[BNC H48 141-144]
(25) Completion and clarification reply:
John: If you press N
Sarah: N?
John: N for name, it?ll let you type in the docu
document name. [BNC G4K 84-86]
(26) Expansion and reformulation/repair:
S: Secondly er
J: We guarantee P five.
S: We we are we?re guaranteeing P five plus
a noise level.
J: Yeah. [BNC JP3 167-170]
(27) Expansion and question:
I: I can?t remember exactly who lived on
the right hand side, I?ve forgotten but th
I know the Chief Clerk lived just a little
way down [address], you see, er
A: In one of those little red brick cottages?
[BNC HDK 124-125]
(28) Answer and expansion:
A: We could hear it from outside ?unclear?.
R: Oh you could hear it?
A: Occasionally yeah. [BNC J8D 13-15]
(29) Answer/reformulation and expansion:
G: [address], that was in the middle, more or
less in the middle of the town.
A: And you called that the manual?
G: The manual school, yes.
[BNC H5G 96-98]
271
