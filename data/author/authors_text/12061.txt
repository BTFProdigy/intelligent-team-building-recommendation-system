Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 277?280,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Parsing Speech Repair without Specialized Grammar Symbols?
Tim Miller
University of Minnesota
tmill@cs.umn.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
University of Minnesota
schuler@cs.umn.edu
Abstract
This paper describes a parsing model for
speech with repairs that makes a clear sep-
aration between linguistically meaningful
symbols in the grammar and operations
specific to speech repair in the operation of
the parser. This system builds a model of
how unfinished constituents in speech re-
pairs are likely to finish, and finishes them
probabilistically with placeholder struc-
ture. These modified repair constituents
and the restarted replacement constituent
are then recognized together in the same
way that two coordinated phrases of the
same type are recognized.
1 Introduction
Speech repair is a phenomenon in spontaneous
spoken language in which a speaker decides to
interrupt the flow of speech, replace some of the
utterance (the ?reparandum?), and continues on
(with the ?alteration?) in a way that makes the
whole sentence as transcribed grammatical only
if the reparandum is ignored. As Ferreira et al
(2004) note, speech repairs1 are the most disrup-
tive type of disfluency, as they seem to require
that a listener first incrementally build up syntac-
tic and semantic structure, then subsequently re-
move it and rebuild when the repair is made. This
difficulty combines with their frequent occurrence
to make speech repair a pressing problem for ma-
chine recognition of spontaneous speech.
This paper introduces a model for dealing with
one part of this problem, constructing a syntac-
tic analysis based on a transcript of spontaneous
spoken language. The model introduced here dif-
fers from other models attempting to solve the
?This research was supported by NSF CAREER award
0447685. The views expressed are not necessarily endorsed
by the sponsors .
1Ferreira et al use the term ?revisions?.
same problem, by completely separating the fluent
grammar from the operations of the parser. The
grammar thus has no representation of disfluency
or speech repair, such as the ?EDITED? category
used to represent a reparandum in the Switchboard
corpus, as such categories are seemingly at odds
with the typical nature of a linguistic constituent.
Rather, the approach presented here uses a
grammar that explicitly represents incomplete
constituents being processed, and repair is rep-
resented by rules which allow incomplete con-
stituents to be prematurely merged with existing
structure. While this model is interesting for its
elegance in representation, there is also reason
to hypothesize improved performance, since this
processing model requires no additional grammar
symbols, and only one additional operation to ac-
count for speech repair, and thus makes better use
of limited data resources.
2 Background
Previous work on parsing of speech with repairs
has shown that syntactic cues can be used to in-
crease accuracy of detection of reparanda, which
can increase overall parsing accuracy. The first
source of structure used to recognize repair is what
Levelt (1983) called the ?Well-formedness Rule.?
This rule essentially states that a speech repair acts
like a conjunction; that is, the reparandum and the
alteration must be of the same syntactic category.
Of course, the reparandum is often unfinished, so
the Well-formedness Rule allows for the reparan-
dum category to be inferred.
This source of structure has been used by two
related approaches, that of Hale et al (2006) and
Miller (2009). Hale and colleagues exploit this
structure by adding contextual information to the
standard reparandum label ?EDITED?. In their
terminology, daughter annotation takes the (pos-
sibly unfinished) constituent label of the reparan-
dum and appends it to the EDITED label. This
277
allows a learned probabilistic context-free gram-
mar to represent the likelihood of a reparandum of
a certain type being a sibling with a finished con-
stituent of the same type.
Miller?s approach exploited the same source of
structure, but changed the representation to use
a REPAIRED label for alterations instead of an
EDITED label for reparanda. The rationale for
that change is the fact that a speech repair does not
really begin until the interruption point, at which
point the alteration is started and the reparandum
is retroactively labelled as such. Thus, the argu-
ment goes, no special syntactic rules or symbols
should be necessary until the alteration begins.
3 Model Description
3.1 Right-corner transform
This work first uses a right-corner transform,
which turns right-branching structure into left-
branching structure, using category labels that use
a ?slash? notation ?/? to represent an incomplete
constituent of type ? ?looking for? a constituent
of type ? in order to complete itself.
This transform first requires that trees be bina-
rized. This binarization is done in a similar way to
Johnson (1998) and Klein and Manning (2003).
Rewrite rules for the right-corner transform are
as follows, first flattening right-branching struc-
ture:2
A
1
?
1
A
2
?
2
A
3
a
3
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
A
3
a
3
A
1
?
1
A
2
A
2
/A
3
?
2
. . .
?
A
1
A
1
/A
2
?
1
A
2
/A
3
?
2
. . .
then replacing it with left-branching structure:
A
1
A
1
/A
2
:?
1
A
2
/A
3
?
2
?
3
. . .
?
A
1
A
1
/A
3
A
1
/ A
2
:?
1
?
2
?
3
. . .
One problem with this notation is the represen-
tation given to unfinished constituents, as seen in
Figures 1 and 2. The standard representation of
2Here, all A
i
denote nonterminal symbols, and ?
i
denote
subtrees; the notationA
1
:?
0
indicates a subtree ?
0
with label
A
1
; and all rewrites are applied recursively, from leaves to
root.
S
. . . EDITED
PP
IN
as
NP-UNF
DT
a
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 1: Section of interest of a standard phrase
structure tree containing speech repair with unfin-
ished noun phrase (NP).
PP
PP/NP
PP/PP
PP/NP
PP/PP
EDITEDPP
EDITEDPP/NP-UNF
IN
as
NP-UNF
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NP
india
Figure 2: Right-corner transformed version of the
fragment above. This tree requires several special
symbols to represent the reparandum that starts
this fragment.
an unfinished constituent in the Switchboard cor-
pus is to append the -UNF label to the lowest un-
finished constituent (see Figure 1). Since one goal
of this work is separation of linguistic knowledge
from language processing mechanisms, the -UNF
tag should not be an explicit part of the gram-
mar. In theory, the incomplete category notation
induced by the right-corner transform is perfectly
suited to this purpose. For instance, the category
NP-UNF is a stand in category for several incom-
plete constituents, for example NP/NN, NP/NNS,
etc. However, since the sub-trees with -UNF la-
bels in the original corpus are by definition unfin-
ished, the label to the right of the slash (NN in
this case) is not defined. As a result, transformed
trees with unfinished structure have the represen-
tation of Figure 2, which gives away the positive
benefits of the right-corner transform in represent-
ing repair by propagating a special repair symbol
(EDITED) through the grammar.
3.2 Approximating unfinished constituents
It is possible to represent -UNF categories as stan-
dard unfinished constituents, and account for un-
finished constituents by having the parser prema-
278
turely end the processing of a given constituent.
However, in the example given above, this would
require predicting ahead of time that the NP-UNF
was only missing a common noun ? NN (for ex-
ample). This problem is addressed in this work
by probabilistically filling in placeholder final cat-
egories of unfinished constituents in the standard
phrase structure trees, before applying the right-
corner transform.
In order to fill in the placeholder with realistic
items, phrase completions are learned from cor-
pus statistics. First, this algorithm identifies an
unfinished constituent to be finished as well as its
existing children (in the continuing example, NP-
UNF with child labelled DT). Next, the corpus is
searched for fluent subtrees with matching root la-
bels and child labels (NP and DT), and a distri-
bution is computed of the actual completions of
those subtrees. In the model used in this work,
the most common completions are NN, NNS, and
NNP. The original NP-UNF subtree is then given a
placeholder completion by sampling from the dis-
tribution of completions computed above.
After this addition is complete, the UNF and
EDITED labels are removed from the reparandum
subtree, and if a restarted constituent of the same
type is a sibling of the reparandum (e.g. another
NP), the two subtrees are made siblings under a
new subtree with the same category label (NP).
See Figure 3 for a simple visual example of how
this works.
S
. . . EDITED
PP
IN
as
NP
DT
a
NN
eli
PP
IN
as
NP
NP
DT
a
NN
westerner
PP-LOC
IN
in
NP
NNP
india
. . .
Figure 3: Same tree as in Figure 1, with the un-
finished noun phrase now given a placeholder NN
completion (both bolded).
Next, these trees are modified using the right-
corner transform as shown in Figure 4. This tree
still contains placeholder words that will not be
in the text stream of an observed input sentence.
Thus, in the final step of the preprocessing algo-
rithm, the finished category label and the place-
holder right child are removed where found in a
right-corner tree. This results in a right-corner
transformed tree in which a unary child or right
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP
PP/NN
PP/NP
IN
as
DT
a
NN
eli
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 4: Right-corner transformed tree with
placeholder finished phrase.
PP
PP/NNP
PP/PP
PP/NP
PP/PP
PP/NN
PP/NP
IN
as
DT
a
IN
as
NP
NP/NN
DT
a
NN
westerner
IN
in
NNP
india
Figure 5: Final right-corner transformed state af-
ter excising placeholder completions to unfinished
constituents. The bolded label indicates the signal
of an unfinished category reparandum.
child subtree having an unfinished constituent type
(a slash category, e.g. PP/NN in Figure 5) at its
root represents a reparandum with an unfinished
category. The tree then represents and processes
the rest of the repair in the same way as a coordi-
nation.
4 Evaluation
This model was evaluated on the Switchboard cor-
pus (Godfrey et al, 1992) of conversational tele-
phone speech between two human interlocuters.
The input to this system is the gold standard
word transcriptions, segmented into individual ut-
terances. For comparison to other similar systems,
the system was given the gold standard part of
speech for each input word as well. The standard
train/test breakdown was used, with sections 2 and
3 used for training, and subsections 0 and 1 of sec-
tion 4 used for testing. Several sentences from the
end of section 4 were used during development.
For training, the data set was first standardized
by removing punctuation, empty categories, ty-
pos, all categories representing repair structure,
279
and partial words ? anything that would be diffi-
cult or impossible to obtain reliably with a speech
recognizer.
The two metrics used here are the standard Par-
seval F-measure, and Edit-finding F. The first takes
the F-score of labeled precision and recall of the
non-terminals in a hypothesized tree relative to the
gold standard tree. The second measure marks
words in the gold standard as edited if they are
dominated by a node labeled EDITED, and mea-
sures the F-score of the hypothesized edited words
relative to the gold standard.
System Configuration Parseval-F Edited-F
Baseline CYK 71.05 18.03
Hale et al 68.48 37.94
Plain RC Trees 69.07 30.89
Elided RC Trees 67.91 24.80
Merged RC Trees 68.88 27.63
Table 1: Results
Results of the testing can be seen in Ta-
ble 1. The first line (?Baseline CYK?) indi-
cates the results using a standard probabilistic
CYK parser, trained on the standardized input
trees. The following two lines are results from re-
implementations of the systems from Hale et al
(2006) andMiller (2009). The line marked ?Elided
trees? gives current results. Surprisingly, this re-
sult proves to be lower than the previous results.
Two observations in the output of the parser on
the development set gave hints as to the reasons
for this performance loss.
First, repairs using the slash categories (for un-
finished reparanda) were rare (relative to finished
reparanda). This led to the suspicion that there
was a state-splitting phenomenon, where cate-
gories previously lumped together as EDITED-NP
were divided into several unfinished categories
(NP/NN, NP/NNS, etc.). To test this suspicion, an-
other experiment was performed where all unary
child and right child subtrees with unfinished cat-
egory labels X/Y were replaced with EDITED-X.
This result is shown in line five of Table 1. This
result improves on the elided version, and sug-
gests that the state-splitting effect is most likely
one cause of decreased performance.
The second effect in the parser output was the
presence of several very long reparanda (more
than ten words), which are highly unlikely in nor-
mal speech. This phenomenon does not occur
in the ?Plain RC Trees? condition. One explana-
tion for this effect is that plain RC trees use the
EDITED label in each rule of the reparandum (see
Figure 2 for a short real-world example). This
essentially creates a reparandum rule set, mak-
ing expansion of a reparandum difficult due to the
likelihood of a long chain eventually requiring a
reparandum rule that was not found in the train-
ing data, or was not learned correctly in the much
smaller set of reparandum-specific training data.
5 Conclusion and Future Work
In conclusion, this paper has presented a new
model for speech containing repairs that enforces
a clean separation between linguistic categories
and parsing operations. Performance was below
expectations, but analysis of the interesting rea-
sons for these results suggests future directions. A
model which explicitly represents the distance that
a speaker backtracks when making a repair would
prevent the parser from hypothesizing the unlikely
reparanda of great length.
References
Fernanda Ferreira, Ellen F. Lau, and Karl G.D. Bai-
ley. 2004. Disfluencies, language comprehension,
and Tree Adjoining Grammars. Cognitive Science,
28:721?749.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Proc. ICASSP,
pages 517?520.
John Hale, Izhak Shafran, Lisa Yung, Bonnie Dorr,
Mary Harper, Anna Krasnyanskaya, Matthew Lease,
Yang Liu, Brian Roark, Matthew Snover, and Robin
Stewart. 2006. PCFGs with syntactic and prosodic
indicators of speech repairs. In Proceedings of the
45th Annual Conference of the Association for Com-
putational Linguistics (COLING-ACL).
Mark Johnson. 1998. PCFG models of linguistic tree
representation. Computational Linguistics, 24:613?
632.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 423?430.
Willem J.M. Levelt. 1983. Monitoring and self-repair
in speech. Cognition, 14:41?104.
Tim Miller. 2009. Improved syntactic models for pars-
ing speech with repairs. In Proceedings of the North
American Association for Computational Linguis-
tics, Boulder, CO.
280
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 37?46,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
An Analysis of Memory-based Processing Costs using Incremental
Deep Syntactic Dependency Parsing?
Marten van Schijndel
The Ohio State University
vanschm@ling.osu.edu
Luan Nguyen
University of Minnesota
lnguyen@cs.umn.edu
William Schuler
The Ohio State University
schuler@ling.osu.edu
Abstract
Reading experiments using naturalistic
stimuli have shown unanticipated facili-
tations for completing center embeddings
when frequency effects are factored out.
To eliminate possible confounds due to
surface structure, this paper introduces a
processing model based on deep syntac-
tic dependencies. Results on eye-tracking
data indicate that completing deep syntac-
tic embeddings yields significantly more
facilitation than completing surface em-
beddings.
1 Introduction
Self-paced reading and eye-tracking experiments
have often been used to support theories about
inhibitory effects of working memory operations
in sentence processing (Just and Carpenter, 1992;
Gibson, 2000; Lewis and Vasishth, 2005), but it
is possible that many of these effects can be ex-
plained by frequency (Jurafsky, 1996; Hale, 2001;
Karlsson, 2007). Experiments on large naturalis-
tic text corpora (Demberg and Keller, 2008; Wu et
al., 2010; van Schijndel and Schuler, 2013) have
shown significant memory effects at the ends of
center embeddings when frequency measures have
been included as separate factors, but these mem-
ory effects have been facilitatory rather than in-
hibitory.
Some of the memory-based measures that pro-
duce these facilitatory effects (Wu et al, 2010; van
Schijndel and Schuler, 2013) are defined in terms
of initiation and integration of connected compo-
nents of syntactic structure,1 with the presumption
?*Thanks to Micha Elsner and three anonymous review-
ers for their feedback. This work was funded by an Ohio State
University Department of Linguistics Targeted Investment
for Excellence (TIE) grant for collaborative interdisciplinary
projects conducted during the academic year 2012?13.
1Graph theoretically, the set of connected components
that referents that belong to the same connected
component may cue one another using content-
based features, while those that do not must rely
on noisier temporal features that just encode how
recently a referent was accessed. These measures,
based on left-corner parsing processes (Johnson-
Laird, 1983; Abney and Johnson, 1991), abstract
counts of unsatisfied dependencies from noun or
verb referents (Gibson, 2000) to cover all syntactic
dependencies, motivated by observations of Dem-
berg and Keller (2008) and Kwon et al (2010) of
the inadequacies of Gibson?s narrower measure.
But these experiments use naturalistic stimuli
without constrained manipulations and therefore
might be susceptible to confounds. It is possible
that the purely phrase-structure-based connected
components used previously may ignore some in-
tegration costs associated with filler-gap construc-
tions, making them an unsuitable generalization of
Gibson-style dependencies. It is also possible that
the facilitatory effect for integration operations in
naturally-occurring stimuli may be driven by syn-
tactic center embeddings that arise from modifiers
(e.g. The CEO sold [[the shares] of the com-
pany]), which do not require any dependencies
to be deferred, but which might be systematically
under-predicted by frequency measures, produc-
ing a confound with memory measures when fre-
quency measures are residualized out.
In order to eliminate possible confounds due to
exclusion of unbounded dependencies in filler-gap
constructions, this paper evaluates a processing
model that calculates connected components on
deep syntactic dependency structures rather than
surface phrase structure trees. This model ac-
counts unattached fillers and gaps as belonging
to separate connected components, and therefore
performs additional initiation and integration op-
of a graph ?V, E? is the set of maximal subsets of
it {?V1, E1?, ?V2, E2?, ...} such that any pair of vertices in
each Vi can be connected by edges in the corresponding Ei.
37
a) Noun Phrase
Relative Clause
Sentence w. Gap
Verb Phrase w. Gap
Sentence w. Gap
millionsstole
say
officials
who
Noun Phrase
personthe
b)
i1
i2 i3
i4
i5
i6
i7
1
2
1
2
1
1
say
officials
stole
who
person
millionsthe
0
0
0
0
0
00
Figure 1: Graphical representation of (a) a single
connected component of surface syntactic phrase
structure corresponding to (b) two connected com-
ponents of deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions, prior to the word say. Connections es-
tablished prior to the word say are shown in black;
subsequent connections are shown in gray.
erations in filler-gap constructions as hypothesized
by Gibson (2000) and others. Then, in order to
control for possible confounds due to modifier-
induced center embedding, this refined model is
applied to two partitions of an eye-tracking cor-
pus (Kennedy et al, 2003): one consisting of sen-
tences containing only non-modifier center em-
beddings, in which dependencies are deferred, and
the other consisting of sentences containing no
center embeddings or containing center embed-
dings arising from attachment of final modifiers,
in which no dependencies are deferred. Processing
this partitioned corpus with deep syntactic con-
nected components reveals a significant increase
in facilitation in the non-modifier partition, which
lends credibility to the observation of negative
integration cost in processing naturally-occurring
sentences.
2 Connected Components
The experiments described in this paper evalu-
ate whether inhibition and facilitation in reading
correlate with operations in a hierarchic sequen-
tial prediction model that initiate and integrate
connected components of hypothesized syntactic
structure during incremental parsing. The model
used in these experiments refines previous con-
nected component models by allowing fillers and
gaps to occur in separate connected components
of a deep syntactic dependency graph (Mel?c?uk,
1988; Kintsch, 1988), even when they belong to
the same connected component when defined on
surface structure.
For example, the surface syntactic phrase struc-
ture and deep syntactic dependency structure for
the noun phrase the person who officials say stole
millions are shown in Figure 1.2 Notice that af-
ter the word officials, there is only one connected
component of surface syntactic phrase structure
(from the root noun phrase to the verb phrase with
gap), but two disjoint connected components of
deep syntactic dependency structure (one ending
at i3, and another at i5). Only the deep syntactic
dependency structure corresponds to familiar (Just
and Carpenter, 1992; Gibson, 1998) notions of
how memory is used to store deferred dependen-
cies in filler-gap constructions. The next section
will describe a generalized categorial grammar,
which (i) can be viewed as context-free, to seed a
latent-variable probabilistic context-free grammar
to accurately derive parses of filler-gap construc-
tions, and (ii) can be viewed as a deep syntactic
dependency grammar, defining dependencies for
connected components in terms of function appli-
cations.
3 Generalized Categorial Grammar
In order to evaluate memory effects for hypothe-
sizing unbounded dependencies between referents
of fillers and referents of clauses containing gaps,
a memory-based processor must define connected
components in terms of deep syntactic dependen-
cies (including unbounded dependencies) rather
than in terms of surface syntactic phrase structure
trees. To do this, at least some phrase structure
edges must be removed from the set of connec-
tions that define a connected component.
Because these unbounded dependencies are not
represented locally in the original Treebank for-
mat, probabilities for operations on these modified
2Following Mel?c?uk (1988) and Kintsch (1988),
the graphical dependency structure adopted here uses
positionally-defined labels (?0? for the predicate label, ?1?
for the first argument ahead of a predicate, ?2? for the last
argument behind, etc.) but includes unbounded dependen-
cies between referents of fillers and referents of clauses
containing gaps. It is assumed that semantically-labeled
structures would be isomorphic to the structures defined
here, but would generalize across alternations such as active
and passive constructions, for example.
38
connected components are trained on a corpus an-
notated with generalized categorial grammar de-
pendencies for ?gap? arguments at all categories
that subsume a gap (Nguyen et al, 2012). This
representation is similar to the HPSG-like repre-
sentation used by Hale (2001) and Lewis and Va-
sishth (2005), but has a naturally-defined depen-
dency structure on which to calculate connected
components. This generalized categorial grammar
is then used to identify the first sign that introduces
a gap, at which point a deep syntactic connected
component containing the filler can be encoded
(stored), and a separate deep syntactic connected
component for a clause containing a gap can be
initiated.
A generalized categorial grammar (Bach, 1981)
consists of a set U of primitive category types;
a set O of type-constructing operators allowing a
recursive definition of a set of categories C =def
U ? (C ? O ? C); a set X of vocabulary items;
a mapping M from vocabulary items in X to se-
mantic functions with category types in C; and
a set R of inference rules for deriving functions
with category types inC from other functions with
category types in C. Nguyen et al (2012) use
primitive category types for clause types (e.g. V
for finite verb-headed clause, N for noun phrase
or nominal clause, D for determiners and pos-
sessive clauses, etc.), and use the generalized set
of type-constructing operators to characterize not
only function application dependencies between
arguments immediately ahead of and behind a
functor (-a and -b, corresponding to ?\? and ?/? in
Ajdukiewicz-Bar-Hillel categorial grammars), but
also long-distance dependencies between fillers
and categories subsuming gaps (-g), dependencies
between relative pronouns and antecedent modif-
icands of relative clauses (-r), and dependencies
between interrogative pronouns and their argu-
ments (-i), which remain unsatisfied in derivations
but function to distinguish categories for content
and polar questions. A lexicon can then be de-
fined in M to introduce lexical dependencies and
obligatory pronominal dependencies using num-
bered functions for predicates and deep syntactic
arguments, for example:
the ? (?i (0 i)=the) : D
person ? (?i (0 i)=person) : N-aD
who ? (?k i (0 i)=who ? (1 i)=k) : N-rN
officials ? (?i (0 i)=officials) : N
the
D
person
N-aD
N Aa
who
N-rN
officials
N
say
V-aN-bV
stole
V-aN-bN
millions
N
V-aN Ae
V-gN Ga
V-aN-gN
Ag
V-gN Ac
V-rN Fc
N R
Figure 2: Example categorization of the noun
phrase the person who officials say stole millions.
say ? (?i (0 i)=say) : V-aN-bV
stole ? (?i (0 i)=stole) : V-aN-bN
millions ? (?i (0 i)=millions) : N
Inference rules in R are then defined to com-
pose arguments and modifiers and propagate gaps.
Arguments g of type d ahead of functors h of
type c-ad are composed by passing non-local de-
pendencies ? ? {-g, -i, -r} ? C from premises to
conclusion in all combinations:
g:d h: c-ad ? ( fc-ad g h): c (Aa)
g:d? h: c-ad ? ?k ( fc-ad (g k) h): c? (Ab)
g:d h: c-ad? ? ?k ( fc-ad g (h k)): c? (Ac)
g:d? h: c-ad? ? ?k ( fc-ad (g k) (h k)): c? (Ad)
Similar rules compose arguments behind functors:
g: c-bd h:d ? ( fc-bd g h): c (Ae)
g: c-bd? h:d ? ?k ( fc-bd (g k) h): c? (Af)
g: c-bd h:d? ? ?k ( fc-bd g (h k)): c? (Ag)
g: c-bd? h:d? ? ?k ( fc-bd (g k) (h k)): c? (Ah)
These rules use composition functions fc-ad
and fc-bd for initial and final arguments, which de-
fine dependency edges numbered v from referents
of predicate functors i to referents of arguments j,
where v is the number of unsatisfied arguments
?1...?v ? {-a, -b} ?C in a category label:
fu?1..v?1-ac
def= ?g h i ? j (v i)= j ? (g j) ? (h i) (1a)
fu?1..v?1-bc
def= ?g h i ? j (v i)= j ? (g i) ? (h j) (1b)
R also contains inference rules to compose mod-
ifier functors g of type u-ad ahead of modifi-
cands h of type d:
g: u-ad h:c ? ( fIM g h):c (Ma)
g: u-ad? h:c ? ?k ( fIM (g k) h):c? (Mb)
g: u-ad h:c? ? ?k ( fIM g (h k)):c? (Mc)
39
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? ... ? ((g? f ):c i?)
xt ? f :d (?Fa)
?i1 j1.. i? j? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. i? j?i?+1 ... ? (g?:c/d { j?} i?) ? ( f :e i?+1)
xt ? f :e (+Fa)
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?):c/e { j?} i?)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(?La)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?):a/e { j??1} i??1)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
g:d h:e ? ( f g h):c or
g:d h:e ? ?k( f (g k) h):c or
g:d h:e ? ?k( f g (h k)):c or
g:d h:e ? ?k( f (g k) (h k)):c
(+La)
Figure 3: Basic processing productions of a right-corner parser.
g: u-ad? h:c? ? ?k ( fIM (g k) (h k)):c? (Md)
or for modifier functors behind a modificand:
g:c h: u-ad ? ( fFM g h):c (Me)
g:c? h: u-ad ? ?k ( fFM (g k) h):c? (Mf)
g:c h: u-ad? ? ?k ( fFM g (h k)):c? (Mg)
g:c? h: u-ad? ? ?k ( fFM (g k) (h k)):c? (Mh)
These rules use composition functions fIM and fFM
for initial and final modifiers, which define depen-
dency edges numbered ?1? from referents of mod-
ifier functors i to referents of modificands j:
fIM
def= ?g h j ?i (1 i)= j ? (g i) ? (h j) (2a)
fFM
def= ?g h j ?i (1 i)= j ? (g j) ? (h i) (2b)
R also contains inference rules for hypothesiz-
ing gaps -gd for arguments and modifiers:3
g: c-ad ? ?k ( fc-ad {k} g): c-gd (Ga)
g: c-bd ? ?k ( fc-ad {k} g): c-gd (Gb)
g:c ? ?k ( fIM {k} g):c-gd (Gc)
and for attaching fillers e, d-re, d-ie as gaps -gd:
g:e h: c-gd ? ?i ? j (g i) ? (h i j):e (Fa)
g:d-re h: c-gd ? ?k j ?i (g k i) ? (h i j): c-re (Fb)
g:d-ie h: c-gd ? ?k j ?i (g k i) ? (h i j): c-ie (Fc)
3Since these unary inferences perform no explicit compo-
sition, they are defined to use only initial versions composi-
tion functions fc-ad and fIM.
and for attaching modificands as antecedents of
relative pronouns:
g:e h:c-rd ? ?i ? j (g i) ? (h i j):e (R)
An example derivation of the noun phrase the per-
son who officials say stole millions using these
rules is shown in Figure 2. The semantic expres-
sion produced by this derivation consists of a con-
junction of terms defining the edges in the graph
shown in Figure 1b.
This GCG formulation captures many of the in-
sights of the HPSG-like context-free filler-gap no-
tation used by Hale (2001) or Lewis and Vasishth
(2005): inference rules with adjacent premises can
be cast as context-free grammars and weighted us-
ing probabilities, which allow experiments to cal-
culate frequency measures for syntactic construc-
tions. Applying a latent variable PCFG trainer
(Petrov et al, 2006) to this formulation was shown
to yield state-of-the-art accuracy for recovery of
unbounded dependencies (Nguyen et al, 2012).
Moreover, the functor-argument dependencies in
a GCG define deep syntactic dependency graphs
for all derivations, which can be used in incremen-
tal parsing to calculate connected components for
memory-based measures.
4 Incremental Processing
In order to obtain measures of memory opera-
tions used in incremental processing, these GCG
inference rules are combined into a set of parser
40
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? ... ? (gn:y/z? { jn} in) ? ... ? ((g?( f ?{ jn} f )):c i?)
xt ? ?k( f ?{k} f ):d
(?Fb)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) xt
?i1 j1.. in jn.. i? j?i?+1 ... ? (gn:y/z? { jn} in) ? ... ? (g?:c/d { j?} i?) ? (( f ?{ jn} f ):e i?+1)
xt ? ?k( f ?{k} f ):e
(+Fb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g?:d i?)
?i1 j1.. in jn.. i? j? ... ? (gn:y/z? { jn} in) ? ... ? (( f g?) ? ( f ?{ jn}):c?/e { j?} i?)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (?Lb)
?i1 j1.. in jn.. i??1 j??1i? ... ? (gn:y/z? { jn} in) ? ... ? (g??1:a/c? { j??1} i??1) ? (g?:d i?)
?i1 j1.. in jn.. i??1 j??1 ... ? (gn:y/z? { jn} in) ? ... ? (g??1 ? ( f g?) ? ( f ?{ jn}):a/e { j??1} i??1)
g:d h:e ? ?k( f g ( f ?{k} h)):c? (+Lb)
Figure 4: Additional processing productions for attaching a referent of a filler jn as the referent of a gap.
productions, similar to those of the ?right corner?
parser of van Schijndel and Schuler (2013), ex-
cept that instead of recognizing shallow hierarchi-
cal sequences of connected components of surface
structure, the parser recognizes shallow hierarchi-
cal sequences of connected components of deep
syntactic dependencies. This parser exploits the
observation (van Schijndel et al, in press) that left-
corner parsers and their variants do not need to ini-
tiate or integrate more than one connected compo-
nent at each word. These two operations are then
augmented with rules to introduce fillers and at-
tach fillers as gaps.
This parser is defined on incomplete connected
component states which consist of an active sign
(with a semantic referent and syntactic form or
category) lacking an awaited sign (also with a ref-
erent and category) yet to come. Semantic func-
tions of active and awaited signs are simplified to
denote only sets of referents, with gap arguments
(?k) stripped off and handled by separate con-
nected components. Incomplete connected com-
ponents, therefore, always denote semantic func-
tions from sets of referents to sets of referents.
This paper will notate semantic functions of
connected components using variables g and h, in-
complete connected component categories as c/d
(consisting of an active sign of category c and an
awaited sign of category d), and associations be-
tween them as g:c/d. The semantic representa-
tion used here is simply a deep syntactic depen-
dency structure, so a connected component func-
tion is satisfied if it holds for some output ref-
erent i given input referent j. This can be no-
tated ?i j (g:c/d { j} i), where the set { j} is equiva-
lent to (? j? j?= j). Connected component functions
that have a common referent j can then be com-
posed into larger connected components:4
?i jk (g { j} i) ? (h {k} j) ? ?i j (g?h {k} i) (3)
Hierarchies of ? connected compo-
nents can be represented as conjunctions:
?i1 j1... i? j? (g1:c1/d1 { j1} i1) ? ... ? (g?:c?/d? { j?} i?).
This allows constraints such as unbounded depen-
dencies between referents of fillers and referents
of clauses containing gaps to be specified across
connected components by simply plugging vari-
ables for filler referents into argument positions
for gaps.
A nondeterministic incremental parser can now
be defined as a deductive system, given an input
sequence consisting of an initial connected com-
ponent state of category T/T, corresponding to an
existing discourse context, followed by a sequence
of observations x1, x2, . . . , processed in time order.
As each xt is encountered, it is connected to an ex-
isting connected component or it introduces a new
disjoint component using the productions shown
in Figures 3, 4, and 5.
4These are connected components of dependency struc-
ture resulting from one or more composition functions being
composed, with each function?s output as the previous func-
tion?s second argument. This uses a standard definition of
function composition: (( f ? g) x) = ( f (g x)).
41
?i1 j1.. i??1 j??1i? ... ? (g?:d i?)
?i1 j1.. i? j? ... ? (( f g?) ? (?h k i (h k)):a/e? { j?} i?)
g:d h:e? ? ( f g h):c (?Lc)
?i1 j1.. i??1 j??1i? ... ? (g??1:a/c { j??1} i??1) ? (g?:d i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? ( f g?) ? (?h k i (h k)):a/e? { j??1} i??1)
g:d h:e? ? ( f g h):c (+Lc)
?i1 j1.. i? j? ... ? (g??1:c/d? { j??1} i??1) ? (g?:d?/e { j?} i?)
?i1 j1.. i??1 j??1 ... ? (g??1 ? (?h i? j(h j)) ? g?:c/e { j??1} i??1)
(+N)
Figure 5: Additional processing productions for hypothesizing filler-gap attachment.
Operations on dependencies that can be derived
from surface structure (see Figure 3) are taken
directly from van Schijndel and Schuler (2013).
First, if an observation xt can immediately fill
the awaited sign of the last connected component
g?:c/d, it is hypothesized to do so, turning this
incomplete connected component into a complete
connected component (g? f ):c (Production ?Fa); or
if the observation can serve as an initial sub-sign
of this awaited sign, it is hypothesized to form a
new complete sign f :e in a new component with xt
as its first observation (Production +Fa). Then,
if either of these resulting complete signs g?:d
can immediately attach as an initial child of the
awaited sign of the most recent connected com-
ponent g??1:a/c, it is hypothesized to merge and
extend this connected component, with xt as the
last observation of the completed connected com-
ponent (Production +La); or if it can serve as an
initial sub-sign of this awaited sign, it is hypoth-
esized to remain disjoint and form its own con-
nected component (Production ?La). The side
conditions of La productions are defined to unpack
gap propagation (instances of ?k that distinguish
rules Aa?h and Ma?h) from the inference rules
in Section 3, because this functionality will be re-
placed with direct substitution of referent variables
into subordinate semantic functions, below.
The Nguyen et al (2012) GCG was defined
to pass up unbounded dependencies, but in in-
cremental deep syntactic dependency processing,
unbounded dependencies are accounted as sepa-
rate connected components. When hypothesizing
an unbounded dependency, the processing model
simply cues the active sign of a previous connected
component containing a filler without completing
the current connected component. The four +F,
?F, +L, and ?L operations are therefore combined
with applications of unary rules Ga?c for hypoth-
esizing referents as fillers for gaps (providing f ?
in the equations in Figure 4). Productions ?Fb
and +Fb fill gaps in initial children, and Produc-
tions ?Lb and +Lb fill gaps in final children. Note
that the Fb and Lb productions apply to the same
types of antecedents as Fa and La productions re-
spectively, so members of these two sets of pro-
ductions cannot be applied together.
Applications of rules Fa?c and R for introduc-
ing fillers are applied to store fillers as existentially
quantified variable values in Lc productions (see
Figure 5). These Lc productions apply to the same
type of antecedent as La and Lb productions, so
these also cannot be applied together.
Finally, connected components separated by
gaps which are no longer hypothesized (?) are
reattached by a +N production. This +N pro-
duction may then be paired with a ?N production
which yields its antecedent unchanged as a conse-
quent. These N productions apply to antecedents
and consequents of the same type, so they may be
applied together with one F and one L production,
but since the +N production removes in its conse-
quent a ? argument required in its antecedent, it
may not apply more than once in succession (and
applying the ?N production more than once in suc-
cession has no effect).
An incremental derivation of the noun phrase
the person who officials say stole millions, using
these productions, is shown in Figure 6.
5 Evaluation
The F, L, and N productions defined in the pre-
vious section can be made probabilistic by first
computing a probabilistic context-free grammar
(PCFG) from a tree-annotated corpus, then trans-
forming that PCFG model into a model of prob-
abilities over incremental parsing operations us-
ing a grammar transform (Schuler, 2009). This
allows the intermediate PCFG to be optimized us-
ing an existing PCFG-based latent variable trainer
42
?i0 (.. :T/T {i0} i0) the
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/N-aD {i2} i2)
+Fa,?La,?N
person
?i0 i2 (.. :T/T {i0} i0) ? (.. :N/V-rN {i2} i2)
?Fa,?La,?N
who
?i0 i2 i3 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2)
+Fa,+Lc,?N
officials
?i0 i2 i3 i5 (.. :T/T {i0} i0) ? (.. :N/V-gN {i3} i2) ? (.. :V-gN/V-aN-gN {i5} i5)
+Fa,?La,?N
say
?i0 i2 i6 (.. :T/T {i0} i0) ? (.. :N/V-aN {i6} i2)
+Fb,+La,+N
stole
?i0 i2 i7 (.. :T/T {i0} i0) ? (.. :N/N {i7} i2)
+Fa,+La,?N
millions
?i0 (.. :T/T {i0} i0)
?Fa,+La,?N
Figure 6: Derivation of the person who officials say stole millions, showing connected components with
unique referent variables (calculated according to the equations in Section 4). Semantic functions are
abbreviated to ?..? for readability. This derivation yields the following lexical relations: (0 i1)=the,
(0 i2)=person, (0 i3)=who, (0 i4)=officials, (0 i5)=say, (0 i6)=stole, (0 i7)=millions, and the following
argument relations: (1 i2)=i1, (1 i3)=i2, (1 i5)=i4, (2 i5)=i6, (1 i6)=i3, (2 i6)=i7.
(Petrov et al, 2006). When applied to the output
of this trainer, this transform has been shown to
produce comparable accuracy to that of the origi-
nal Petrov et al (2006) CKY parser (van Schijn-
del et al, 2012). The transform used in these ex-
periments diverges from that of Schuler (2009), in
that the probability associated with introducing a
gap in a filler-gap construction is reallocated from
a ?F?L operation to a +F?L operation (to encode
the previously most subordinate connected com-
ponent with the filler as its awaited sign and be-
gin a new disjoint connected component), and the
probability associated with resolving such a gap is
reallocated from an implicit ?N operation to a +N
operation (to integrate the connected component
containing the gap with that containing the filler).
In order to verify that the modifications to the
transform correctly reallocate probability mass for
gap operations, the goodness of fit to reading
times of a model using this modified transform
is compared against the publicly-available base-
line model from van Schijndel and Schuler (2013),
which uses the original Schuler (2009) transform.5
To ensure a valid comparison, both parsers are
trained on a GCG-reannotated version of the Wall
Street Journal portion of the Penn Treebank (Mar-
cus et al, 1993) before being fit to reading times
using linear mixed-effects models (Baayen et al,
2008).6 This evaluation focuses on the process-
ing that can be done up to a given point in a sen-
tence. In human subjects, this processing includes
both immediate lexical access and regressions that
5The models used here also use random slopes to reduce
their variance, which makes them less anticonservative.
6The models are built using lmer from the lme4R package
(Bates et al, 2011; R Development Core Team, 2010).
aid in the integration of new information, so the
reading times of interest in this evaluation are log-
transformed go-past durations.7
The first and last word of each line in the
Dundee corpus, words not observed at least 5
times in the WSJ training corpus, and fixations af-
ter long saccades (>4 words) are omitted from the
evaluation to filter out wrap-up effects, parser in-
accuracies, and inattention and track loss of the
eyetracker. The following predictors are centered
and used in each baseline model: sentence posi-
tion, word length, whether or not the previous or
next word were fixated upon, and unigram and bi-
gram probabilities.8 Then each of the following
predictors is residualized off each baseline before
being centered and added to it to help residualize
the next factor: length of the go-past region, cumu-
lative total surprisal, total surprisal (Hale, 2001),
and cumulative entropy reduction (Hale, 2003).9
All 2-way interactions between these effects are
7Go-past durations are calculated by summing all fixa-
tions in a region of text, including regressions, until a new
region is fixated, which accounts for additional processing
that may take place after initial lexical access, but before the
next region is processed. For example, if one region ends at
word 5 in a sentence, and the next fixation lands on word 8,
then the go-past region consists of words 6-8 while go-past
duration sums all fixations until a fixation occurs after word
8. Log-transforming eye movements and fixations may make
their distributions more normal (Stephen and Mirman, 2010)
and does not substantially affect the results of this paper.
8For the n-gram model, this study uses the Brown corpus
(Francis and Kucera, 1979), the WSJ Sections 02-21 (Mar-
cus et al, 1993), the written portion of the British National
Corpus (BNC Consortium, 2007), and the Dundee corpus
(Kennedy et al, 2003) smoothed with modified Kneser-Ney
(Chen and Goodman, 1998) in SRILM (Stolcke, 2002).
9Non-cumulative metrics are calculated from the final
word of the go-past region; cumulative metrics are summed
over the go-past region.
43
included as predictors along with the predictors
from the previous go-past region (to account for
spillover effects). Finally, each model has sub-
ject and item random intercepts added in addition
to by-subject random slopes (cumulative total sur-
prisal, whether the previous word was fixated, and
length of the go-past region) and is fit to centered
log-transformed go-past durations.10
The Akaike Information Criterion (AIC)
indicates that the gap-reallocating model
(AIC = 128,605) provides a better fit to reading
times than the original model (AIC = 128,619).11
As described in Section 1, previous findings of
negative integration cost may be due to a confound
whereby center-embedded constructions caused
by modifiers, which do not require deep syntac-
tic dependencies to be deferred, may be driving
the effect. Under this hypothesis, embeddings
that do not arise from final adjunction of mod-
ifiers (henceforth canonical embeddings) should
yield a positive integration cost as found by Gib-
son (2000).
To investigate this potential confound, the
Dundee corpus is partitioned into two parts. First,
the model described in this paper is used to anno-
tate the Dundee corpus. From this annotated cor-
pus, all sentences are collected that contain canon-
ical embeddings and lack modifier-induced em-
beddings.12 This produces two corpora: one con-
sisting entirely of canonical center-embeddings
such as those used in self-paced reading exper-
iments with findings of positive integration cost
(e.g. Gibson 2000), the other consisting of the
remainder of the Dundee corpus, which contains
sentences with canonical embeddings but also in-
cludes modifier-caused embeddings.
The coefficient estimates for integration oper-
ations (?F+L and +N) on each of these corpora
are then calculated using the baseline described
above. To ensure embeddings are driving any ob-
served effect rather than sentence wrap-up effects,
the first and last words of each sentence are ex-
cluded from both data sets. Integration cost is
measured by the amount of probability mass the
parser allocates to ?F+L and +N operations, accu-
10Each fixed effect that has an absolute t-value greater than
10 when included in a random-intercepts only model is added
as a random slope by-subject.
11The relative likelihood of the original model to the gap-
sensitive model is 0.0009 (n = 151,331), which suggests the
improvement is significant.
12Modifier-induced embeddings are found by looking for
embeddings that arise from inference rules Ma-h in Section 3.
Model coeff std err t-score
Canonical -0.040 0.010 -4.05
Other -0.017 0.004 -4.20
Table 1: Fixed effect estimates for integration cost
when used to fit reading times over two partitions
of the Dundee corpus: one containing only canon-
ical center embeddings and the other composed of
the rest of the sentences in the corpus.
mulated over each go-past region, and this cost is
added as a fixed effect and as a random slope by
subject to the mixed model described earlier.13
The fixed effect estimate for cumulative inte-
gration cost from fitting each corpus is shown
in Table 1. Application of Welch?s t-test shows
that the difference between the estimated distri-
butions of these two parameters is highly signif-
icant (p < 0.0001).14 The strong negative corre-
lation of integration cost to reading times in the
purely canonical corpus suggests canonical (non-
modifier) integrations contribute to the finding of
negative integration cost.
6 Conclusion
This paper has introduced an incremental parser
capable of using GCG dependencies to distinguish
between surface syntactic embeddings and deep
syntactic embeddings. This parser was shown to
obtain a better fit to reading times than a surface-
syntactic parser and was used to parse the Dundee
eye-tracking corpus in two partitions: one consist-
ing of canonical embeddings that require deferred
dependencies and the other consisting of sentences
containing no center embeddings or center em-
beddings arising from the attachment of clause-
final modifiers, in which no dependencies are de-
ferred. Using linear mixed effects models, com-
pletion (integration) of canonical center embed-
dings was found to be significantly more nega-
tively correlated with reading times than comple-
tion of non-canonical embeddings. These results
suggest that the negative integration cost observed
in eye-tracking studies is at least partially due to
deep syntactic dependencies and not due to con-
founds related to surface forms.
13Integration cost is residualized off the baseline before be-
ing centered and added as a fixed effect.
14Integration cost is significant as a fixed effect (p = 0.001)
in both partitions: canonical (n = 16,174 durations) and
non-canonical (n = 131,297 durations).
44
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
R. Harald Baayen, D. J. Davidson, and Douglas M.
Bates. 2008. Mixed-effects modeling with crossed
random effects for subjects and items. Journal of
Memory and Language, 59:390?412.
Emmon Bach. 1981. Discontinuous constituents in
generalized categorial grammars. Proceedings of
the Annual Meeting of the Northeast Linguistic So-
ciety (NELS), 11:1?12.
Douglas Bates, Martin Maechler, and Ben Bolker,
2011. lme4: Linear mixed-effects models using S4
classes.
BNC Consortium. 2007. The british national corpus.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
W. Nelson Francis and Henry Kucera. 1979. The
brown corpus: A standard corpus of present-day
edited american english.
Edward Gibson. 1998. Linguistic complexity: Local-
ity of syntactic dependencies. Cognition, 68(1):1?
76.
Edward Gibson. 2000. The dependency locality the-
ory: A distance-based theory of linguistic complex-
ity. In Image, language, brain: Papers from the first
mind articulation project symposium, pages 95?126,
Cambridge, MA. MIT Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the sec-
ond meeting of the North American chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
John Hale. 2003. Grammar, Uncertainty and Sentence
Processing. Ph.D. thesis, Cognitive Science, The
Johns Hopkins University.
Philip N. Johnson-Laird. 1983. Mental models: to-
wards a cognitive science of language, inference,
and consciousness. Harvard University Press, Cam-
bridge, MA, USA.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science: A Multidisciplinary Journal, 20(2):137?
194.
Marcel Adam Just and Patricia A. Carpenter. 1992. A
capacity theory of comprehension: Individual differ-
ences in working memory. Psychological Review,
99:122?149.
Fred Karlsson. 2007. Constraints on multiple center-
embedding of clauses. Journal of Linguistics,
43:365?392.
Alan Kennedy, James Pynte, and Robin Hill. 2003.
The Dundee corpus. In Proceedings of the 12th Eu-
ropean conference on eye movement.
Walter Kintsch. 1988. The role of knowledge in dis-
course comprehension: A construction-integration
model. Psychological review, 95(2):163?182.
Nayoung Kwon, Yoonhyoung Lee, Peter C. Gordon,
Robert Kluender, and Maria Polinsky. 2010. Cog-
nitive and linguistic factors affecting subject/object
asymmetry: An eye-tracking study of pre-nominal
relative clauses in korean. Language, 86(3):561.
Richard L. Lewis and Shravan Vasishth. 2005.
An activation-based model of sentence processing
as skilled memory retrieval. Cognitive Science,
29(3):375?419.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Igor Mel?c?uk. 1988. Dependency syntax: theory and
practice. State University of NY Press, Albany.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING ?12), Mumbai,
India.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING/ACL?06).
R Development Core Team, 2010. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
William Schuler. 2009. Parsing with a bounded stack
using a model-based right-corner transform. In Pro-
ceedings of NAACL/HLT 2009, NAACL ?09, pages
344?352, Boulder, Colorado. Association for Com-
putational Linguistics.
Damian G. Stephen and Daniel Mirman. 2010. Inter-
actions dominate the dynamics of visual cognition.
Cognition, 115(1):154?165.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Seventh International Confer-
ence on Spoken Language Processing.
Marten van Schijndel and William Schuler. 2013. An
analysis of frequency- and recency-based processing
costs. In Proceedings of NAACL-HLT 2013. Associ-
ation for Computational Linguistics.
45
Marten van Schijndel, Andy Exley, and William
Schuler. 2012. Connectionist-inspired incremental
PCFG parsing. In Proceedings of CMCL 2012. As-
sociation for Computational Linguistics.
Marten van Schijndel, Andy Exley, and William
Schuler. in press. A model of language processing
as hierarchic sequential prediction. Topics in Cogni-
tive Science.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL?10), pages 1189?1198.
46
