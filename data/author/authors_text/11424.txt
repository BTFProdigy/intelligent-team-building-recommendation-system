Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 665?673,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A model of local coherence effects in human sentence processing
as consequences of updates from bottom-up prior to posterior beliefs
Klinton Bicknell and Roger Levy
Department of Linguistics
University of California, San Diego
9500 Gilman Dr, La Jolla, CA 92093-0108
{kbicknell,rlevy}@ling.ucsd.edu
Abstract
Human sentence processing involves integrat-
ing probabilistic knowledge from a variety of
sources in order to incrementally determine
the hierarchical structure for the serial input
stream. While a large number of sentence pro-
cessing effects have been explained in terms of
comprehenders? rational use of probabilistic
information, effects of local coherences have
not. We present here a new model of local
coherences, viewing them as resulting from a
belief-update process, and show that the rele-
vant probabilities in our model are calculable
from a probabilistic Earley parser. Finally, we
demonstrate empirically that an implemented
version of the model makes the correct predic-
tions for the materials from the original exper-
iment demonstrating local coherence effects.
1 Introduction
The task of human sentence processing, recovering
a hierarchical structure from a serial input fraught
with local ambiguities, is a complex and difficult
problem. There is ample evidence that comprehen-
ders understand sentences incrementally, construct-
ing interpretations of partial structure and expecta-
tions for future input (Tanenhaus et al, 1995; Alt-
mann and Kamide, 1999). Many of the main behav-
ioral findings in the study of human sentence pro-
cessing have now been explained computationally.
Using probabilistic models trained on large-scale
corpora, effects such as global and incremental dis-
ambiguation preferences have been shown to be a
result of the rational use of syntactic probabilities
(Jurafsky, 1996; Hale, 2001; Narayanan and Juraf-
sky, 2001; Levy, 2008b; Levy et al, 2009). Simi-
larly, a number of other effects in both comprehen-
sion and production have been modeled as resulting
from rational strategies of languages users that take
into account all the probabilistic information present
in the linguistic signal (Genzel and Charniak, 2002;
Genzel and Charniak, 2003; Keller, 2004; Levy and
Jaeger, 2007).
One class of results from the literature that has
not yet been explained in terms of a rational com-
prehender strategy is that of local coherence effects
(Tabor et al, 2004; Gibson, 2006; Konieczny and
Mu?ller, 2007), cases in which it appears that the
parser is systematically ignoring contextual infor-
mation about possible syntactic structures and pur-
suing analyses that are probable only locally. These
effects are problematic for rational models, because
of the apparent failure to use all of the available in-
formation. This paper describes a new model of lo-
cal coherence effects under rational syntactic com-
prehension, which proposes that they arise as a re-
sult of updating prior beliefs about the structures
that a given string of words is likely to have to pos-
terior beliefs about the likelihoods of those struc-
tures in context. The critical intuition embodied in
the model is that larger updates in probability distri-
butions should be more processing-intensive; hence,
the farther the posterior is from the prior, the more
radical the update required and the greater the pro-
cessing load. Section 2 describes the problem of lo-
cal coherences in detail and Section 3 describes ex-
isting models of the phenomenon. Following that,
Sections 4?5 describe our model and its computa-
665
S
NP
Det
the
N
player
VP
V
tossed
S
NP
D
The
N
coach
VP
V
smiled
PP
P
at
S
NP
D
the
N
player
VP
V
tossed
Figure 1: The difficulty of explaining local-
coherence effects as traditional garden-pathing.
tion from a probabilistic Earley parser. Section 6
presents the results of an experiment showing that
our model makes the correct predictions for the lo-
cal coherence effects seen in the original paper by
Tabor et al (2004). Finally, Section 7 concludes and
discusses the insight our model gives into human
performance.
2 Local coherences
The first studies to report effects of local coherences
are described in Tabor et al (2004). In Experiment
1, they use a self-paced reading task and materials
containing relative clauses (RCs) attached to nouns
in non-subject position as in (1).
(1) a. The coach smiled at the player tossed a
frisbee by the opposing team.
b. The coach smiled at the player who was
tossed a frisbee by the opposing team.
c. The coach smiled at the player thrown a
frisbee by the opposing team.
d. The coach smiled at the player who was
thrown a frisbee by the opposing team.
Their experimental design crossed RC reduction
with verb ambiguity. RCs are either reduced (1a,1c)
or unreduced (1b,1d), and the RC verb is either lex-
ically ambiguous between a past tense active and a
past participle (1a?1b), or is unambiguously a past
participle (1c?1d).
Tabor et al point out that in one of these four
conditions (1a) there is a locally coherent string the
player tossed a frisbee. Out of context (e.g., if it
were starting a sentence) this string would have a
likely parse in which tossed is a past tense active
verb, the player is its agent, and a frisbee is its
theme (Figure 1, left). The preceding context within
the sentence, however, should rule out this interpre-
tation because the player appears within a PP and
hence should not be able to be the subject of a new
sentence (Figure 1, right). That is, given the preced-
ing context, the player tossed a frisbee must begin
a reduced RC, such that there is no local ambiguity.
Thus, if comprehenders are making full use of the
linguistic context, (1a) should be no more difficult
than the other examples, except insofar as ambigu-
ous verbs are harder than unambiguous verbs, and
reduced RCs are harder than unreduced RCs, pre-
dicting there would be only the two main effects of
RC reduction and verb ambiguity on reading times
for the tossed a frisbee region.
Tabor et al, however, predict an interaction such
that (1a) will have added difficulty above and be-
yond these two effects, because of the interference
from the locally coherent parse of the player tossed a
frisbee. Concordant with their predictions, they find
an interaction in the tossed a frisbee region, such
that (1a) is super-additively difficult. Because this
result requires that an impossible parse influences a
word?s difficulty, it is in direct opposition to the pre-
dictions of theories of processing difficulty in which
the probability of a word given context is the pri-
mary source of parsing difficulty, and more gener-
ally appears to be in opposition to any rational the-
ory, in which comprehenders are making use of all
the information in the linguistic context.
3 Existing models
With the results showing local coherence effects
in mind, we can ask the question of what sorts
of theories do predict these effects. This section
briefly describes two recent examples of such the-
ories. The first involves dynamical systems models
to explain the effects and the second uses a mathe-
matical model of the combination of bottom-up and
top-down probabilistic information.
In Tabor and Hutchins?s (2004) SOPARSE (self-
organized parse) model, reading a word activates a
set of lexically anchored tree fragments. Through
spreading activation between compatible fragments
and inhibition between incompatible ones, these tree
fragments then compete in a process which is sen-
sitive only to the local environment, i.e., ignoring
the global grammatical context. Eventually, the sys-
666
tem stabilizes to the correct parse, and reading times
for each word are modeled as the time the system
takes to stabilize after reading a word. Stabilization
takes longer for locally coherent regions because the
locally coherent parse will be created and compete
with the globally grammatical parse.
There are, however, unresolved issues with this
model. The model has a number of free parameters,
relating to the equations used for the competition,
the method by which links between fragments are
formed, as well as the question of precisely what
tree fragments a given word will activate. While Ta-
bor and Hutchins (2004) work out these questions
in detail for the types of sentences they model, it is
unclear how the model could be scaled up to make
predictions for arbitrary types of sentences. That is,
there is no principled system for setting the three
types of parameters mentioned, and thus no clear in-
terpretation of their values. The model put forward
in this paper is an attempt to remedy this situation.
A recent proposal by Gibson (2006) can also ex-
plain some of the local coherence results. Gibson?s
proposal is that part-of-speech ambiguities have a
special status in parsing; in effect, lexical part-of-
speech ambiguities can be thought of as one-word
local coherences. In this model, a probability func-
tion P? is calculated over part-of-speech tags given
a word. This probability for tag ti and a word w,
P? (ti|w), is proportional to the context-independent
probability of ti given the word w, P (ti|w) ? the
bottom-up component ? multiplied by a smoothed
probability Ps of the tag given the context ? the top-
down component:
P? (ti|w) = P (ti|w)Ps(ti|context)?
t?T
P (t|w)Ps(t|context)
(1)
Difficulty is predicted to be high when the probabil-
ity P? of the correct tag is low.
Because the top-down probabilities are smoothed
to allow for all possible parts-of-speech, any word
which is lexically ambiguous will be more difficult
to process, regardless of whether it is ambiguous or
not in its context. This can thus explain some of the
difference between the ambiguous and unambiguous
verbs in Tabor et al (2004). It is not clear, however,
under such a model why the super-additive interac-
tion would obtain?that is, why (1a) should be so
much harder than (1b) starting at the word tossed.
In addition, Gibson?s model is a bit underspecified:
he does not discuss how the top-down probabilities
are calculated, nor what the precise linking hypothe-
sis is between the final P? and reading times. Finally,
it is not at all clear why the top-down expectations
should be smoothed, since the smoothing actually
has negative consequences on the processor?s per-
formance.
4 Parsing as belief update
The basic intuition behind the model presented here
is that incrementally processing a sentence can be
conceptualized as a process of updating one?s be-
liefs. Such an analogy has been used to moti-
vate surprisal-based theories of sentence processing
(Hale, 2001; Levy, 2008a), where beliefs about the
structure of a sentence after seeing the first i ? 1
words in the sentence, which we denote as wi?10 ,
are updated upon encountering wi. In this case, the
surprisal of a word (? logP (wi|wi?10 )) is equiva-
lent to the Kullback-Leibler divergence of the beliefs
after wi0 from the beliefs after wi?10 (Levy, 2008a).
Our model focuses on another belief-update process
in sentence processing: updating beliefs about the
structures that a string of words is likely to have in-
dependent of context to beliefs about what structures
it is likely to have in context. A bit more formally, it
views the process of integrating a string of words
wji into a sentence as beginning with a ?bottom-
up? prior distribution of syntactic structures likely to
spanwji and integrating that with ?top-down? knowl-
edge from the previous words in the sentence wi0 in
order to reach a posterior distribution conditioning
on wj0 over which structures actually can span wji .
This belief update process can be viewed as a ratio-
nal reconstruction of the Tabor and Hutchins (2004)
model, where ? instead of the system dynamics of
competition between arbitrary tree fragments ? dif-
ferences between prior and posterior probability dis-
tributions over syntactic structures determine pro-
cessing difficulty.
More formally still, when integrating wji into a
sentence, for each syntactic category X , we can de-
fine the prior probability conditioned only onwji that
wji will form the beginning of that category, i.e., that
anX exists which begins at index i and spans at least
667
through j:
Prior: P (Xk?ji |wji ) (2)
It is important to note here that this prior probability
is conditional only on the value of wji and not the
values of i or j; that is, in the prior probability, i and
j should be interpreted merely as a way to coindex
the start and end points of the string of words being
integrated with a category X potentially spanning
them, and not as making reference to position in the
full sentence string.
For each categoryX , this prior probability will be
updated to the posterior probability of that category
spanning wji given all the words seen so far:
Posterior: P (Xk?ji |wj0) (3)
In the equation for the posterior, of course, the in-
dices i and j are positions in the sentence string, and
not merely coindices.
Given these prior and posterior beliefs, we pre-
dict difficulty to arise in cases where the prior re-
quires substantial modification to reach the poste-
rior, that is, cases in which the prior and poste-
rior make substantially different predictions for cat-
egories. A strong local coherence will have sharply
different prior and posterior distributions, causing
difficulty. We represent the prior and posterior be-
liefs as vectors of the probabilities of each syntactic
category spanningwji , and measureMij , the amount
of modification required, as the summed K-L diver-
gence of the prior from the posterior vector. That is,
if N is the set of nonterminals in the grammar, the
size of the belief update is modeled as1
Mij def=
?
X?N
D
(
P (Xk?ji |wj0) ||P (Xk?ji |wji )
)
In the remainder of the paper, we show how to com-
pute Mij by using Bayesian inference on quanti-
ties calculated in ordinary probabilistic incremen-
tal Earley parsing with a stochastic context-free
1Note that for each syntactic category X ? N , the proba-
bility distribution P (Xk?ji |I) for some information I is over a
binary random variable indicating the presence of X . The dif-
ferent syntactic categories X that could span from i to any k
are not mutually exclusive, hence we cannot define size of be-
lief update as a single K-L divergence defined over multinomial
distributions.
grammar (SCFG), and show that our model makes
the correct predictions using an SCFG for English
on the original local-coherences experiment of Ta-
bor et al (2004).
5 Computing priors and posteriors
For SCFGs, a probabilistic Earley parser (Earley,
1970; Stolcke, 1995) provides the basic quantities
we need to compute the prior (2) and posterior
(3) for each category X . Following Stolcke, we
use capital Latin characters to denote non-terminal
categories and use lowercase Greek characters to
denote (possibly null) sequences of terminals and
non-terminals. We write the probability that a non-
terminal X can be recursively rewritten by SCFG
rules as a certain series of symbols ? by
P (X ?? ?)
An edge built from the rule X ? ?? where ? has
been recognized as beginning at position i and end-
ing at position j is denoted
j : Xi ? ?.?
The forward probability of that edge at position j,
?j , is defined to be the joint probability that the root
node will generate all words recognized so far wj0 as
well as the edge
?j(Xi ? ?.?)
With this terminology, we are now in a position to
describe how we calculate the posterior and prior
probability vectors for our model.
5.1 Calculating the posterior
To calculate the posterior, we first use the definition
of conditional probability to rewrite it as
P (Xk?ji |wj0) =
P (Xk?ji , wj0)
P (wj0)
In a context-free grammar, given the syntactic cat-
egory that dominates a string of words, the words?
probability is independent from everything outside
the category. Thus, this is equivalent to
P (Xk?ji |wj0) =
P (wio, Xi)P (wji |Xk?ji )
P (wj0)
= P (S
?? wi0X?)P (X ?? wji?)
P (S ?? wj0?)
668
5.1.1 Posterior: the numerator?s first term
The first term in the numerator P (S ?? wi0X?)
can be computed from a parse of wi0 by summing
forward probabilities of the form
?i(Xi ? .?) (4)
5.1.2 Posterior: the denominator
Similarly, the denominator P (S ?? wj0?) can be
computed from a parse of wj0 by summing forward
probabilities of the form
?j(Y ? ?wjj?1.?) (5)
for all Y. This is because the forward probability of
a state is conditioned on generating all the previous
words.
5.1.3 Posterior: the numerator?s second term
The second term in the numerator P (X ?? wji?)
for an arbitrary category X cannot necessarily be
calculated from a probabilistic Earley parse of the
sentence, because the parser does not construct
states that are not potentially useful in forming a sen-
tence (i.e., states that would have a forward proba-
bility of zero.) However, to calculate the probability
of X generating words wji we can parse wji sepa-
rately with a goal category of X . From this parse,
we can extract the probability of wji being generated
from X in the same way as we extracted the proba-
bility of wj0 being generated from S, i.e., as a sum of
forward probabilities at j (Eq. 5).2
5.2 Calculating the prior
To calculate the prior, we first use Bayes rule to
rewrite it as
P (Xk?ji |wji ) =
P (wji |Xk?ji )P (Xk?ji )
P (wji )
(6)
Recall that at this point, i and j do not refer to in-
dex positions in the actual string but rather serve to
identify the substring wji of interest. That is, P (wji )
denotes the probability that at an arbitrary point in
2To calculate the posterior, it is not necessary to parse wji
separately, since these states are only excluded from the parse
when their forward probability is zero, in which case the first
term in the numerator will also be zero. A separate parse is nec-
essary, however, when using this term to calculate the prior.
Table 1: Event space for the prior
Event Description
E0: There are at least i? words |w| ? i?
E1: A category X begins at i? Xi?
E2: An Xi? spans at least through j Xk?ji?
E3: There are at least j words |w| ? j
E4: Words wji? are these specific w?ji? wji? = w?ji?
an arbitrary sentence, the next j ? i words will be
wji , and P (Xk?ji ) denotes the probability that an ar-
bitrary point in an arbitrary sentence will be the left
edge of a category X that spans at least j ? i words.
None of the three terms in Eq. 6 can be directly ob-
tained. However, we can obtain a very good approx-
imation of Eq. 6 as follows. First, we marginalize
over the position within a sentence with which the
left edge i might be identified:
P (Xk?ji |wji ) =
?
i?=0,1,...
(
P (wji? |Xk?ji? )P (Xk?ji? )
P (wji?)
)
P (i = i?)
(7)
In Eq. 7, i? is identified with the actual string position
within the sentence.
Second, we rewrite the first term in this sum with
event space notation, using the event space given in
Table 1.
P (wji? |Xk?ji? )P (Xk?ji? )
P (wji?)
= P (E0,3,4|E0...3)P (E0...3)P (E0,3,4)
= P (E4|E0...3)P (Eo...3)P (Eo,3,4)
Applying the chain rule, we can further simplify.
= P (E4|E0...3)P (E1...3|E0)P (E0)P (E3,4|E0)P (E0)
= P (E4|E0...3)P (E1...3|E0)P (E3,4|E0)
= P (E2...4|E0, E1)P (E1|E0)P (E3,4|E0)
669
Switching back from event space notation and sub-
stituting this term into Eq. 7, we now have
P (Xk?ji |wji ) =
?
i?=0,1,...
(
P (wji? |Xi? , E0)P (Xi? |E0)
P (wji? |E0)
)
P (i = i?)
(8)
Thus, by conditioning all terms on E0, the presence
of at least i? words, we have transformed the proba-
bilities we need to calculate into these four terms,
which are easier to calculate from the parser. We
now consider how to calculate each of the terms.
5.2.1 Prior: the numerator?s first term
The first term in the numerator can be simplified
because our grammar is context-free:
P (wji? |Xi? , E0) = P (wji? |Xi?)
= P (X ?? wji?)
This can be computed as described in Section 5.1.3.
5.2.2 Prior: the numerator?s second term
The second term in the numerator can be rewritten
as follows:
P (Xi? |E0) = P (Xi? , E0)P (E0)
= P (S
?? w?i?0X?)
P (S ?? w?i?0 ?)
where w?i?0 denotes any sequence of i? words. Given
a value i? we can calculate both terms by parsing
the string w?i0X , where each word w? in w?i0X is a
special word that can freely act as any preterminal.
The denominator can then be calculated by summing
the forward probabilities of the last word w?ii?1 as in
Eq. 5, and the numerator by summing the forward
probability of X , as in Eq. 4.
5.2.3 Prior: the denominator
The denominator in the calculation of the prior
can be calculated in a way analogous to the numera-
tor?s second term (Section 5.2.2):
P (wji? |E0) =
P (wji? , E0)
P (E0)
= P (S
?? w?i?0wji??)
P (S ?? w?i?0 ?)
5.2.4 Prior: starting position probability
Finally, we must calculate the second term in
Eq. 8, the probability of the starting position
P (i = i?). Given that all our terms are conditional
on the existence of all words in the sentence up to
i? (E0), the probability of a starting position P (i) is
the probability of drawing i? randomly from the set
of positions in sentences generated by the grammar
such that all words up to that position exist. For most
language grammars, this distribution can be easily
approximated by a sample of sentences generated
from the SCFG, since most of the probability mass
is concentrated in small indices.
6 Experiment
We tested the predictions of an implemented ver-
sion of our model on the materials from Ta-
bor et al (2004). To generate quantitative predic-
tions, we created a small grammar of relevant syn-
tactic rules, and estimated the rule probabilities from
syntactically annotated text. We calculated summed
K-L divergence of the prior from the posterior vector
for each word in the Tabor et al items, and predict
this sum to be largest at the critical region when the
sentence has an effect of local coherence.
6.1 Methods
6.1.1 Grammar
We defined a small SCFG for the problem, and es-
timated its rule probabilities using the parsed Brown
corpus. The resulting SCFG is identical to that used
in Levy (2008b) and is given in Table 2.
6.1.2 Lexicon
Lexical rewrite probabilities for part-of-speech
tags were also estimated using the entire parsed
Brown corpus.
6.1.3 Materials
The materials were taken from Experiment 1 of
Tabor et al (2004). We removed 8 of their 20 items
for which our trained model either did not know the
critical verb or did not know the syntactic structure
of some part of the sentence. For the other 12 items,
we replaced unknown nouns (9 instances) and un-
known non-critical verbs (2 instances), changed one
plural noun to singular, and dropped one sentence-
initial prepositional phrase.
670
Table 2: The SCFG used in Experiment 3. Rule
weights given as negative log-probabilities in bits.
Rule Weight
ROOT ? S 0
S ? S-base CC S-base 7.3
S ? S-base 0.01
S-base ? NP-base VP 0
NP ? NP-base RC 4.1
NP ? NP-base 0.5
NP ? NP-base PP 2.0
NP-base ? DT NN NN 4.7
NP-base ? DT NN 1.9
NP-base ? DT JJ NN 3.8
NP-base ? PRP 1.0
NP-base ? NNP 3.1
VP/NP ? VBD NP 4.0
VP/NP ? VBD 0.1
VP ? VBD PP 2.0
VP ? VBD NP 0.7
VP ? VBD 2.9
RC ?WP S/NP 0.5
RC ? VP-pass/NP 2.0
RC ?WP FinCop VP-pass/NP 4.9
PP ? IN NP 0
S/NP ? VP 0.7
S/NP ? NP-base VP/NP 1.3
VP-pass/NP ? VBN NP 2.2
VP-pass/NP ? VBN 0.4
6.2 Procedure
For these 12 items, we ran our model on the four
conditions in (1). For each word, we calculated
the prior and posterior vectors for substrings of
three lengths at wi. The summed K-L divergence
is reported for a substring length of 1 word us-
ing a prior of P (Xk?ii?1 |wii?1), for a length of 2
using P (Xk?ii?2 |wii?2), and for a length of 3 us-
ing P (Xk?ii?3 |wii?3). For all lengths, we predict the
summed divergence to be greater at critical words
for the part-of-speech ambiguous conditions (1a,1b)
than for unambiguous (1c,1d), because the part-of-
speech unambiguous verbs cannot give rise to a prior
that predicts for a sentence to begin. For a substring
length of 3, we also predict that the divergence is
superadditively greatest in the ambiguous reduced
condition (1a), because of the possibility of starting
l
l
l l l
l
l
l0
5
10
15
20
25
Su
mm
ed
 K?
L D
ive
rge
nce
 (bi
ts)
l
at the player who was tossed/
thrown
a frisbee
l
l
tossed
who was tossed
thrown
who was thrown
Figure 2: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
1
l l
l
l l l
l
l
0
5
10
15
20
25
Su
mm
ed
 K?
L D
ive
rge
nce
 (bi
ts)
l
at the player who was tossed/
thrown
a frisbee
l
l
tossed
who was tossed
thrown
who was thrown
Figure 3: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
2
a sentence with the player tossed.
6.3 Results
The results of the experiment are shown in Figures
2?4. For all three substring lengths, the model pre-
dicts difficulty to be greater in the ambiguous condi-
tions at the critical words (tossed/thrown a frisbee).
For 1-word substrings, the effect is localized on the
critical verb (tossed/thrown), for 2-word substrings
it is localized on the word directly following the
critical verb (tossed/thrown a), and for 3-word sub-
strings there are two effects: one on the critical verb
(the player tossed/thrown) and one two words later
(tossed/thrown a frisbee). Furthermore, for 3-word
substrings, the effect is superadditively greatest for
the player tossed. These results thus nicely confirm
671
l l l l l l l
l
0
5
10
15
20
25
Su
mm
ed
 K?
L D
ive
rge
nce
 (bi
ts)
l
at the player who was tossed/
thrown
a frisbee
l
l
tossed
who was tossed
thrown
who was thrown
Figure 4: Summed K-L divergence of the prior from
the posterior vectors at each word: Substring length
3
both of our predictions and demonstrate that a model
in which large belief updates from a bottom-up prior
to a posterior induce difficulty is capable of account-
ing for effects of local coherences.
7 Conclusion
This paper has described a model of local coherence
effects in sentence processing, which views the pro-
cess of integrating a string of words wji into a sen-
tence as a process of updating prior beliefs about
the structures spanning those words to posterior be-
liefs. These prior beliefs are simply the probabilities
of those structures given only the words being inte-
grated, and the posterior beliefs are the probabilities
given the entire sentence processed thus far. Diffi-
culty is predicted to result whenever this update is
large ? which we model in terms of a large summed
K-L divergence of the prior from the posterior vec-
tor. We demonstrated a method of normatively cal-
culating these probabilities from probabilistic Ear-
ley parses and used this implemented model to make
predictions for the materials for the original experi-
mental result of effects of local coherences (Tabor et
al., 2004). Our results demonstrated that the model
predicts difficulty to occur at the correct part of the
sentence in the correct condition.
We improve on existing models in two ways.
First, we make predictions for where local coher-
ences should obtain for an arbitrary SCFG, not just
one particular class of sentences. This allows the
model to scale up for use with a broad coverage
grammar and to make predictions for arbitrary sen-
tences, which was not possible with a model such as
Tabor & Hutchins (2004).
Second, our model gives a rational basis to an ef-
fect which has typically been seen to result from ir-
rationality of the human sentence processor. Specif-
ically, the cost that our model describes of updating
bottom-up prior beliefs to in-context posterior be-
liefs can be viewed as resulting from a rational pro-
cess in the case that the bottom-up prior is available
to the human sentence processor more rapidly than
the in-context posterior. Interestingly, the fact that
the prior is actually more difficult to compute than
the posterior suggests that the only way it would be
available more rapidly is if it is precomputed. Thus,
our model provides the insight that, to the extent
that comprehenders are behaving rationally in pro-
ducing effects of local coherences, this may indi-
cate that they have precomputed the likely syntac-
tic structures of short sequences of words. While it
may be unlikely that they calculate these probabil-
ities for sequences directly from their grammar as
we do in this paper, there could be a number of ways
to approximate this prior: for example, given a large
enough corpus, these probabilities could be approx-
imated for any string of words that appears suffi-
ciently often by merely tracking the structures the
string has each time it occurs. Such a hypothesis for
how comprehenders approximate the prior could be
tested by manipulating the frequency of the relevant
substrings in sentences with local coherences.
This work can be extended in a number of ways.
As already mentioned, one logical step is using
a broad-coverage grammar. Another possibility re-
lates to the problem of correlations between the dif-
ferent components of the prior and posterior vec-
tors. For example, in our small grammar, whenever a
ROOT category begins, so does an S, an S-base, and
an NP-base. Dimensionality reduction techniques on
our vectors may be able to remove such correlations.
These steps and more exhaustive evaluation of a va-
riety of datasets remain for the future.
Acknowledgments
This research was supported by NIH Training Grant
T32-DC000041 from the Center for Research in
Language at UCSD to the first author.
672
References
Gerry T.M. Altmann and Yuki Kamide. 1999. Incremen-
tal interpretation at verbs: Restricting the domain of
subsequent reference. Cognition, 73:247?264.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In Proceedings of the 40th
annual meeting of the Association for Computational
Linguistics, pages 199?206, Philadelphia, July. Asso-
ciation for Computational Linguistics.
Dmitriy Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function of
the sentence number. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 65?72, Sapporo, Japan. Association for
Computational Linguistics.
Edward Gibson. 2006. The interaction of top-down and
bottom-up statistics in the resolution of syntactic cat-
egory ambiguity. Journal of Memory and Language,
54:363?388.
John Hale. 2001. A probabilistic Earley parser as a
psycholinguistic model. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, volume 2, pages
159?166, New Brunswick, NJ. Associate for Compu-
tational Linguistics.
Daniel Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20:137?194.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In Dekang Lin and Dekai Wu, edi-
tors, Proceedings of the 2004 Conference on Empirical
Methods in Natural Language Processing, pages 317?
324, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Lars Konieczny and Daniel Mu?ller. 2007. Local co-
herence interpretation in written and spoken language.
Presented at the 20th Annual CUNY Conference on
Human Sentence Processing. La Jolla, CA.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 849?856, Cambridge, MA. MIT Press.
Roger Levy, Florencia Reali, and Thomas L. Griffiths.
2009. Modeling the effects of memory on human on-
line sentence processing with particle filters. In Pro-
ceedings of NIPS.
Roger Levy. 2008a. Expectation-based syntactic com-
prehension. Cognition, 106:1126?1177.
Roger Levy. 2008b. A noisy-channel model of rational
human sentence comprehension under uncertain input.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 234?
243, Honolulu, Hawaii, October. Association for Com-
putational Linguistics.
Srini Narayanan and Daniel Jurafsky. 2001. A Bayesian
model predicts human parse preference and read-
ing time in sentence processing. In T.G. Dietterich,
S Becker, and Z. Ghahramani, editors, Advances in
Neural Information Processing Systems 14, pages 59?
65, Cambridge, MA. MIT Press.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165?
201.
Whitney Tabor and Sean Hutchins. 2004. Evidence
for self-organized sentence processing: Digging-in ef-
fects. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 30(2):431?450.
Whitney Tabor, Bruno Galantucci, and Daniel Richard-
son. 2004. Effects of merely local syntactic coher-
ence on sentence processing. Journal of Memory and
Language, 50:355?370.
Michael K Tanenhaus, Michael J Spivey-Knowlton,
Kathleen M Eberhard, and Julie C Sedivy. 1995. Inte-
gration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
673
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 19?20,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Computational psycholinguistics
Roger Levy, Klinton Bicknell, and Nathaniel Smith
University of California, San Diego
1. Brief Description
Over the last two decades, computational linguistics has been revolutionized as a result
of three closely related developments, two empirical and one theoretical: increases in 
computing power, the new availability of large linguistic datasets, and a paradigm shift 
toward the view that language processing by computers is best approached through the 
tools of statistical inference. During roughly the same time frame, there have been 
similar theoretical developments in cognitive psychology towards a view of major 
aspects of human cognition as instances of rational statistical inference. Developments 
in these two fields have set the stage for renewed interest in computational approaches 
to human language processing. In this tutorial, we briefly survey some of the key 
theoretical issues at the forefront of this interdisciplinary field today, and show how 
modeling techniques from NLP are being employed, extended, and coupled with 
experimental techniques from psycholinguistics to further our understanding of real-time 
human language use.
2. Tutorial structure
1. Introduction & summary of current state of key areas in psycholinguistics
(a) Key empirical findings involving ambiguity resolution, prediction, integration of
diverse information sources, and speaker choice in real-time language comprehen-
sion and production (?10 minutes)
(b) Framing of empirical findings and concomitant theoretical issues in terms that
can be cleanly related to leading NLP models and algorithms (?10 minutes)
2. Review of exact inference techniques for stochastic grammatical formalisms
(a) Weighted finite-state automata and context-free grammars (?10 minutes)
(b) Probabilistic Earley algorithm (?10 minutes)
(c) Weighted intersection of FSA and CFG (?10 minutes)
3. Modeling key results in ambiguity resolution and expectation-based facilitation
(a) Global disambiguation preferences (?5 minutes)
(b) Measuring online processing difficulty: intro to self-paced reading (?5 minutes)
(c) Expectation-based facilitation in unambiguous contexts (?5?10 minutes)
4. Coffee break
19
5. Online production: speaker choice
(a) Zipf?s second law (frequency & word length) and information-theoretic interpre-
tations (?10 minutes)
(b) Phonetic duration & reduction effects in online word production (?5 minutes)
(c) Morphological- and lexical-level reduction phenomena: modeling and empirical
investigation (?10 minutes)
6. Cognitive limitations and implications for modeling
(a) Memory limitations & garden pathing: empirical results (?10 minutes)
(b) Modeling approach I: incremental beam search & garden pathing (?5 minutes)
(c) Modeling approach II: stochastic incremental search & ?digging-in? effects (?10
minutes)
7. Additional theoretical challenges
(a) Bounds on rationality in real-time language use? ?Good-enough? comprehension
effects and ?local-coherence? effects (?10 minutes)
(b) Possible avenues of attack: more refined models introducing input uncertainty
(?10 minutes)
(c) More sophisticated experimental tools: eye-tracking (?5 minutes)
(d) New experimental findings on input uncertainty, ?hallucinated? garden paths (?5
minutes)
(e) Future directions (?5 minutes)
8. Summary and questions (?5?10 minutes)
3. Instructor
Roger Levy, rlevy@ling.ucsd.edu
My research focuses on theoretical and applied questions in the processing of natural
language. Inherently, linguistic communication involves the resolution of uncertainty 
over a potentially unbounded set of possible signals and meanings. How can a fixed set 
of knowledge and resources be deployed to manage this uncertainty? To address these 
questions I use a combination of computational modelling and psycholinguistic 
experimentation. This work furthers our understanding of the cognitive underpinning of 
language processing, and helps us design models and algorithms that will allow 
machines to process human language.
Klinton Bicknell and Nathaniel Smith are PhD students at the University of California, 
San Diego.
20
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1168?1178,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Rational Model of Eye Movement Control in Reading
Klinton Bicknell and Roger Levy
Department of Linguistics
University of California, San Diego
9500 Gilman Dr, La Jolla, CA 92093-0108
{kbicknell,rlevy}@ling.ucsd.edu
Abstract
A number of results in the study of real-
time sentence comprehension have been
explained by computational models as re-
sulting from the rational use of probabilis-
tic linguistic information. Many times,
these hypotheses have been tested in read-
ing by linking predictions about relative
word difficulty to word-aggregated eye
tracking measures such as go-past time. In
this paper, we extend these results by ask-
ing to what extent reading is well-modeled
as rational behavior at a finer level of anal-
ysis, predicting not aggregate measures,
but the duration and location of each fix-
ation. We present a new rational model of
eye movement control in reading, the cen-
tral assumption of which is that eye move-
ment decisions are made to obtain noisy
visual information as the reader performs
Bayesian inference on the identities of the
words in the sentence. As a case study,
we present two simulations demonstrating
that the model gives a rational explanation
for between-word regressions.
1 Introduction
The language processing tasks of reading, listen-
ing, and even speaking are remarkably difficult.
Good performance at each one requires integrat-
ing a range of types of probabilistic information
and making incremental predictions on the ba-
sis of noisy, incomplete input. Despite these re-
quirements, empirical work has shown that hu-
mans perform very well (e.g., Tanenhaus, Spivey-
Knowlton, Eberhard, & Sedivy, 1995). Sophisti-
cated models have been developed that explain
many of these effects using the tools of com-
putational linguistics and large-scale corpora to
make normative predictions for optimal perfor-
mance in these tasks (Genzel & Charniak, 2002,
2003; Keller, 2004; Levy & Jaeger, 2007; Jaeger,
2010). To the extent that the behavior of these
models looks like human behavior, it suggests that
humans are making rational use of all the infor-
mation available to them in language processing.
In the domain of incremental language compre-
hension, especially, there is a substantial amount
of computational work suggesting that humans be-
have rationally (e.g., Jurafsky, 1996; Narayanan &
Jurafsky, 2001; Levy, 2008; Levy, Reali, & Grif-
fiths, 2009). Most of this work has taken as its
task predicting the difficulty of each word in a sen-
tence, a major result being that a large component
of the difficulty of a word appears to be a function
of its probability in context (Hale, 2001; Smith &
Levy, 2008). Much of the empirical basis for this
work comes from studying reading, where word
difficulty can be related to the amount of time
that a reader spends on a particular word. To re-
late these predictions about word difficulty to the
data obtained in eye tracking experiments, the eye
movement record has been summarized through
word aggregate measures, such as the average du-
ration of the first fixation on a word, or the amount
of time between when a word is first fixated and
when the eyes move to its right (?go-past time?).
It is important to note that this notion of word
difficulty is an abstraction over the actual task of
reading, which is made up of more fine-grained
decisions about how long to leave the eyes in
their current position, and where to move them
next, producing the series of relatively stable pe-
riods (fixations) and movements (saccades) that
characterize the eye tracking record. While there
has been much empirical work on reading at
this fine-grained scale (see Rayner, 1998 for an
overview), and there are a number of successful
models (Reichle, Pollatsek, & Rayner, 2006; En-
gbert, Nuthmann, Richter, & Kliegl, 2005), little
is known about the extent to which human read-
ing behavior appears to be rational at this finer
1168
grained scale. In this paper, we present a new ratio-
nal model of eye movement control in reading, the
central assumption of which is that eye movement
decisions are made to obtain noisy visual informa-
tion, which the reader uses in Bayesian inference
about the form and structure of the sentence. As a
case study, we show that this model gives a ratio-
nal explanation for between-word regressions.
In Section 2, we briefly describe the leading
models of eye movements in reading, and in Sec-
tion 3, we describe how these models account for
between-word regressions and the intuition behind
our model?s account of them. Section 4 describes
the model and its implementation and Sections 5?
6 describe two simulations we performed with the
model comparing behavioral policies that make re-
gressions to those that do not. In Simulation 1, we
show that specific regressive policies outperform
specific non-regressive policies, and in Simulation
2, we use optimization to directly find optimal
policies for three performance measures. The re-
sults show that the regressive policies outperform
non-regressive policies across a wide range of per-
formance measures, demonstrating that our model
predicts that making between-word regressions is
a rational strategy for reading.
2 Models of eye movements in reading
The two most successful models of eye move-
ments in reading are E-Z Reader (Reichle, Pollat-
sek, Fisher, & Rayner, 1998; Reichle et al, 2006)
and SWIFT (Engbert, Longtin, & Kliegl, 2002;
Engbert et al, 2005). Both of these models charac-
terize the problem of reading as one of word iden-
tification. In E-Z Reader, for example, the system
identifies each word in the sentence serially, mov-
ing attention to the next word in the sentence only
after processing the current word is complete, and
(to slightly oversimplify), the eyes then follow the
attentional shifts at some lag. SWIFT works simi-
larly, but with the main difference being that pro-
cessing and attention are distributed over multiple
words, such that adjacent words can be identified
in parallel. While both of these models provide a
good fit to eye tracking data from reading, neither
model asks the higher level question of what a ra-
tional solution to the problem would look like.
The first model to ask this question, Mr. Chips
(Legge, Klitz, & Tjan, 1997; Legge, Hooven,
Klitz, Mansfield, & Tjan, 2002), predicts the op-
timal sequence of saccade targets to read a text
based on a principle of minimizing the expected
entropy in the distribution over identities of the
current word. Unfortunately, however, the Mr.
Chips model simplifies the problem of reading in
a number of ways: First, it uses a unigram model
as its language model, and thus fails to use any
information in the linguistic context to help with
word identification. Second, it only moves on to
the next word after unambiguous identification of
the current word, whereas there is experimental
evidence that comprehenders maintain some un-
certainty about the word identities. In other work,
we have extended the Mr. Chips model to remove
these two limitations, and show that the result-
ing model more closely matches human perfor-
mance (Bicknell & Levy, 2010). The larger prob-
lem, however, is that each of these models uses
an unrealistic model of visual input, which obtains
absolute knowledge of the characters in its visual
window. Thus, there is no reason for the model to
spend longer on one fixation than another, and the
model only makes predictions for where saccades
are targeted, and not how long fixations last.
Reichle and Laurent (2006) presented a rational
model that overcame the limitations of Mr. Chips
to produce predictions for both fixation durations
and locations, focusing on the ways in which eye
movement behavior is an adaptive response to the
particular constraints of the task of reading. Given
this focus, Reichle and Laurent used a very simple
word identification function, for which the time re-
quired to identify a word was a function only of its
length and the relative position of the eyes. In this
paper, we present another rational model of eye
movement control in reading that, like Reichle and
Laurent, makes predictions for fixation durations
and locations, but which focuses instead on the
dynamics of word identification at the core of the
task of reading. Specifically, our model identifies
the words in a sentence by performing Bayesian
inference combining noisy input from a realistic
visual model with a language model that takes
context into account.
3 Explaining between-word regressions
In this paper, we use our model to provide a
novel explanation for between-word regressive
saccades. In reading, about 10?15% of saccades
are regressive ? movements from right-to-left (or
to previous lines). To understand how models
such as E-Z Reader or SWIFT account for re-
1169
gressive saccades to previous words, recall that
the system identifies words in the sentence (gen-
erally) left to right, and that identification of a
word in these models takes a certain amount of
time and then is completed. In such a setup, why
should the eyes ever move backwards? Three ma-
jor answers have been put forward. One possibil-
ity given by E-Z Reader is as a response to over-
shoot; i.e., the eyes move backwards to a previ-
ous word because they accidentally landed fur-
ther forward than intended due to motor error.
Such an explanation could only account for small
between-word regressions, of about the magni-
tude of motor error. The most recent version,
E-Z Reader 10 (Reichle, Warren, & McConnell,
2009), has a new component that can produce
longer between-word regressions. Specifically, the
model includes a flag for postlexical integration
failure, that ? when triggered ? will instruct the
model to produce a between-word regression to
the site of the failure. That is, between-word re-
gressions in E-Z Reader 10 can arise because of
postlexical processes external to the model?s main
task of word identification. A final explanation for
between-word regressions, which arises as a result
of normal processes of word identification, comes
from the SWIFT model. In the SWIFT model, the
reader can fail to identify a word but move past
it and continue reading. In these cases, there is
a chance that the eyes will at some point move
back to this unidentified word to identify it. From
the present perspective, however, it is unclear how
it could be rational to move past an unidentified
word and decide to revisit it only much later.
Here, we suggest a new explanation for
between-word regressions that arises as a result
of word identification processes (unlike that of
E-Z Reader) and can be understood as rational
(unlike that of SWIFT). Whereas in SWIFT and
E-Z Reader, word recognition is a process that
takes some amount of time and is then ?com-
pleted?, some experimental evidence suggests that
word recognition may be best thought of as a
process that is never ?completed?, as comprehen-
ders appear to both maintain uncertainty about the
identity of previous input and to update that uncer-
tainty as more information is gained about the rest
of the sentence (Connine, Blasko, & Hall, 1991;
Levy, Bicknell, Slattery, & Rayner, 2009). Thus, it
is possible that later parts of a sentence can cause
a reader?s confidence in the identity of the previ-
ous regions to fall. In these cases, a rational way to
respond might be to make a between-word regres-
sive saccade to get more visual information about
the (now) low confidence previous region.
To illustrate this idea, consider the case of a lan-
guage composed of just two strings, AB and BA,
and assume that the eyes can only get noisy in-
formation about the identity of one character at a
time. After obtaining a little information about the
identity of the first character, the reader may be
reasonably confident that its identity is A and move
on to obtaining visual input about the second char-
acter. If the first noisy input about the second char-
acter also indicates that it is probably A, then the
normative probability that the first character is A
(and thus a rational reader?s confidence in its iden-
tity) will fall. This simple example just illustrates
the point that if a reader is combining noisy vi-
sual information with a language model, then con-
fidence in previous regions will sometimes fall.
There are two ways that a rational agent might
deal with this problem. The first option would be
to reach a higher level of confidence in the iden-
tity of each word before moving on to the right,
i.e., slowing down reading left-to-right to prevent
having to make right-to-left regressions. The sec-
ond option is to read left-to-right relatively more
quickly, and then make occasional right-to-left re-
gressions in the cases where probability in pre-
vious regions falls. In this paper, we present two
simulations suggesting that when using a rational
model to read natural language, the best strate-
gies for coping with the problem of confidence
about previous regions dropping ? for any trade-
off between speed and accuracy ? involve making
between-word regressions. In the next section, we
present the details of our model of reading and its
implementation, and then we present our two sim-
ulations in the sections following.
4 Reading as Bayesian inference
At its core, the framework we are proposing is one
of reading as Bayesian inference. Specifically, the
model begins reading with a prior distribution over
possible identities of a sentence given by its lan-
guage model. On the basis of that distribution, the
model decides whether or not to move its eyes (and
if so where to move them to) and obtains noisy
visual input about the sentence at the eyes? posi-
tion. That noisy visual input then gives the likeli-
hood term in a Bayesian belief update, where the
1170
model?s prior distribution over the identity of the
sentence given the language model is updated to a
posterior distribution taking into account both the
language model and the visual input obtained thus
far. On the basis of that new distribution, the model
again selects an action and the cycle repeats.
This framework is unique among models of eye
movement control in reading (except Mr. Chips)
in having a fully explicit model of how visual in-
put is used to discriminate word identity. This ap-
proach stands in sharp contrast to other models,
which treat the time course of word identifica-
tion as an exogenous function of other influenc-
ing factors (such as word length, frequency, and
predictability). The hope in our approach is that
the influence of these key factors on the eye move-
ment record will fall out as a natural consequence
of rational behavior itself. For example, it is well
known that the higher the conditional probabil-
ity of a word given preceding material, the more
rapidly that word is read (Boston, Hale, Kliegl,
Patil, & Vasishth, 2008; Demberg & Keller, 2008;
Ehrlich & Rayner, 1981; Smith & Levy, 2008).
E-Z Reader and SWIFT incorporate this finding by
specifying a dependency on word predictability in
the exogenous function determining word process-
ing time. In our framework, in contrast, we would
expect such an effect to emerge as a byproduct of
Bayesian inference: words with high prior proba-
bility (conditional on preceding fixations) will re-
quire less visual input to be reliably identified.
An implemented model in this framework must
formalize a number of pieces of the reading prob-
lem, including the possible actions available to the
reader and their consequences, the nature of vi-
sual input, a means of combining visual input with
prior expectations about sentence form and struc-
ture, and a control policy determining how the
model will choose actions on the basis of its poste-
rior distribution over the identities of the sentence.
In the remainder of this section, we present these
details of the formalization of the reading problem
we used for the simulations reported in this paper:
actions (4.1), visual input (4.2), formalization of
the Bayesian inference problem (4.3), control pol-
icy (4.4), and finally, implementation of the model
using weighted finite state automata (4.5).
4.1 Formal problem of reading: Actions
For our model, we assume a series of discrete
timesteps, and on each time step, the model first
obtains visual input around the current location
of the eyes, and then chooses between three ac-
tions: (a) continuing to fixate the currently fixated
position, (b) initiating a saccade to a new posi-
tion, or (c) stopping reading of the sentence. If
on the ith timestep, the model chooses option (a),
the timestep advances to i + 1 and another sam-
ple of visual input is obtained around the current
position. If the model chooses option (c), the read-
ing immediately ends. If a saccade is initiated (b),
there is a lag of two timesteps, roughly represent-
ing the time required to plan and execute a sac-
cade, during which the model again obtains visual
input around the current position and then the eyes
move ? with some motor error ? toward the in-
tended target ti, landing on position `i. On the next
time step, visual input is obtained around `i and
another decision is made. The motor error for sac-
cades follows the form of random error used by all
major models of eye movements in reading: the
landing position `i is normally distributed around
the intended target ti with standard deviation given
by a linear function of the intended distance1
`i ? N
(
ti,(?0 +?1|ti? `i?1|)2
)
(1)
for some linear coefficients ?0 and ?1. In the ex-
periments reported in this paper, we follow the
SWIFT model in using ?0 = 0.87,?1 = 0.084.
4.2 Noisy visual input
As stated earlier, the role of noisy visual input in
our model is as the likelihood term in a Bayesian
inference about sentence form and identity. There-
fore, if we denote the input obtained thus far from
a sentence as I, all the information pertinent to
the reader?s inferences can be encapsulated in the
form p(I|w) for possible sentences w. We assume
that the inputs deriving from each character posi-
tion are conditionally independent given sentence
identity, so that if w j denotes letter j of the sen-
tence and I( j) denotes the component of visual
input associated with that letter, then we can de-
compose p(I|w) as ? j p(I( j)|w j). For simplicity,
we assume that each character is either a lowercase
letter or a space. The visual input obtained from
an individual fixation can thus be summarized as
a vector of likelihoods p(I( j)|w j), as shown in
1In the terminology of the literature, the model has only
random motor error (variance), not systematic error (bias).
Following Engbert and Kr?gel (2010), systematic error may
arise from Bayesian estimation of the best saccade distance.
1171
. . . a s a c a
*
t s a t a t a t . . .
?
?????????????
a
c
.
.
.
s
t
.
.
.
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
0
0
.
.
.
0
0
.
.
.
1
?
?????????????
?
?????????????
.04
.04
.
.
.
.04
.04
.
.
.
0
?
?????????????
?
?????????????
.04
.04
.
.
.
.04
.04
.
.
.
0
?
?????????????
?
?????????????
.04
.04
.
.
.
.04
.04
.
.
.
0
?
?????????????
?
?????????????
.08
.02
.
.
.
.04
.03
.
.
.
0
?
?????????????
?
?????????????
.15
.07
.
.
.
.01
.01
.
.
.
0
?
?????????????
?
?????????????
.02
.25
.
.
.
.03
.01
.
.
.
0
?
?????????????
?
?????????????
.07
.01
.
.
.
.03
.003
.
.
.
0
?
?????????????
?
?????????????
.05
.01
.
.
.
.002
.05
.
.
.
0
?
?????????????
?
?????????????
.003
.005
.
.
.
.21
.02
.
.
.
0
?
?????????????
?
?????????????
.04
.01
.
.
.
.03
.07
.
.
.
0
?
?????????????
?
?????????????
.06
.01
.
.
.
.02
.12
.
.
.
0
?
?????????????
?
?????????????
.05
.05
.
.
.
.07
.05
.
.
.
0
?
?????????????
?
?????????????
.10
.08
.
.
.
.02
.05
.
.
.
0
?
?????????????
Figure 1: Peripheral and foveal visual input in the model. The asymmetric Gaussian curve indicates
declining perceptual acuity centered around the fixation point (marked by ?). The vector underneath each
letter position denotes the likelihood p(I( j)|w j) for each possible letter w j, taken from a single input
sample with ? = 1/
?
3 (see vector at the left edge of the figure for key, and Section 4.2). In peripheral
vision, the letter/whitespace distinction is veridical, but no information about letter identity is obtained.
Note in this particular sample, input from the fixated character and the following one is rather inaccurate.
Figure 1. As in the real visual system, our vi-
sual acuity function decreases with retinal eccen-
tricity; we follow the SWIFT model in assuming
that the spatial distribution of visual processing
rate follows an asymmetric Gaussian with ?L =
2.41,?R = 3.74, which we discretize into process-
ing rates for each character position. If ? denotes a
character?s eccentricity in characters from the cen-
ter of fixation, then the proportion of the total pro-
cessing rate at that eccentricity ? (?) is given by
integrating the asymmetric Gaussian over a char-
acter width centered on that position,
? (?) =
? ?+.5
??.5
1
Z
exp
(
?
x2
2?2
)
dx,? =
{
?L, x < 0
?R, x? 0
where the normalization constant Z is given by
Z =
?
pi
2
(?L +?R).
From this distribution, we derive two types of vi-
sual input, peripheral input giving word boundary
information and foveal input giving information
about letter identity.
4.2.1 Peripheral visual input
In our model, any eccentricity with a processing
rate proportion ? (?) at least 0.5% of the rate pro-
portion for the centrally fixated character (? ?
[?7,12]), yields peripheral visual input, defined
as veridical word boundary information indicat-
ing whether each character is a letter or a space.
This roughly corresponds to empirical estimates
that humans obtain useful information in reading
from about 19 characters, more from the right of
fixation than the left (Rayner, 1998). Hence in Fig-
ure 1, for example, left-peripheral visual input can
be represented as veridical knowledge of the initial
whitespace (denoted ), and a uniform distribution
over the 26 letters of English for the letter a.
4.2.2 Foveal visual input
In addition, for those eccentricities with a process-
ing rate proportion ? (?) that is at least 1% of the
total processing rate (? ? [?5,8]) the model re-
ceives foveal visual input, defined only for letters2
to give noisy information about the letter?s iden-
tity. This threshold of 1% roughly corresponds to
estimates that readers get information useful for
letter identification from about 4 characters to the
left and 8 to the right of fixation (Rayner, 1998).
In our model, each letter is equally confusable
with all others, following Norris (2006, 2009),
but ignoring work on letter confusability (which
could be added to future model revisions; Engel,
Dougherty, & Jones, 1973; Geyer, 1977). Visual
information about each character is obtained by
sampling. Specifically, we represent each letter as
a 26-dimensional vector, where a single element
is 1 and the other 25 are zeros, and given this rep-
resentation, foveal input for a letter is given as a
sample from a 26-dimensional Gaussian with a
2For white space, the model is already certain of the iden-
tity because of peripheral input.
1172
mean equal to the letter?s true identity and a di-
agonal covariance matrix ?(?) = ? (?)?1/2I. It is
relatively straightforward to show that under these
conditions, if we take the processing rate to be the
expected change in log-odds of the true letter iden-
tity relative to any other that a single sample brings
about, then the rate equals ? (?). We scale the over-
all processing rate by multiplying each rate by ?.
For the experiments in this paper, we set ? = 4.
For each fixation, we sample independently from
the appropriate distribution for each character po-
sition and then compute the likelihood given each
possible letter, as illustrated in the non-peripheral
region of Figure 1.
4.3 Inference about sentence identity
Given the visual input and a language model, in-
ferences about the identity of the sentence w can
be made by standard Bayesian inference, where
the prior is given by the language model and the
likelihood is a function of the total visual input ob-
tained from the first to the ith timestep I i1,
p(w|I i1) =
p(w)p(I i1|w)
?
w?
(w?)p(I i1|w
?)
. (2)
If we let I( j) denote the input received about char-
acter position j and let w j denote the jth character
in sentence identity w, then the likelihood can be
broken down by character position as
p(I i1|w) =
n
?
j=1
p(I i1( j)|w j)
where n is the final character about which there is
any visual input. Similarly, we can decompose this
into the product of the likelihoods of each sample
p(I i1|w) =
n
?
j=1
i
?
t=1
p(It( j)|w j). (3)
If the eccentricity of the jth character on the tth
timestep ? jt is outside of foveal input or the char-
acter is a space, the inner term is 0 or 1. If the sam-
ple was from a letter in foveal input ? jt ? [?5,8], it
is the probability of sampling It( j) from the mul-
tivariate Gaussian N (w j,??(? jt )).
4.4 Control policy
The model uses a simple policy to decide between
actions based on the marginal probability m of the
(a) m = [.6, .7, .6, .4, .3, .6]: Keep fixating (3)
(b) m = [.6, .4, .9, .4, .3, .6]: Move back (to 2)
(c) m = [.6, .7, .9, .4, .3, .6]: Move forward (to 6)
(d) m = [.6, .7, .9, .8, .7, .7]: Stop reading
Figure 2: Values of m for a 6 character sentence
under which a model fixating position 3 would
take each of its four actions, if ? = .7 and ? = .5.
most likely character c in position j,
m( j) = max
c
p(wn = c|I
i
1)
= max
c ?
w?:w?n=c
p(w?|I i1). (4)
Intuitively, a high value of m means that the model
is relatively confident about the character?s iden-
tity, and a low value that it is relatively uncertain.
Given the values of this statistic, our model de-
cides between four possible actions, as illustrated
in Figure 2. If the value of this statistic for the cur-
rent position of the eyes m(`i) is less than a pa-
rameter ? , the model chooses to continue fixating
the current position (2a). Otherwise, if the value
of m( j) is less than ? for some leftward position
j < `i, the model initiates a saccade to the closest
such position (2b). If m( j)? ? for all j < `i, then
the model initiates a saccade to n characters past
the closest position to the right j > `i for which
m( j)< ? (2c).3 Finally, if no such positions exist
to the right, the model stops reading the sentence
(2d). Intuitively, then, the model reads by making
a rightward sweep to bring its confidence in each
character up to ? , but pauses to move left if confi-
dence in a previous character falls below ? .
4.5 Implementation with wFSAs
This model can be efficiently and simply im-
plemented using weighted finite-state automata
(wFSAs; Mohri, 1997) as follows: First, we be-
gin with a wFSA representation of the language
model, where each arc emits a single character (or
is an epsilon-transition emitting nothing). To per-
form belief update given a new visual input, we
create a new wFSA to represent the likelihood of
each character from the sample. Specifically, this
wFSA has only a single chain of states, where,
e.g., the first and second state in the chain are con-
nected by 27 (or fewer) arcs, which emit each of
3The role of n is to ensure that the model does not cen-
ter its visual field on the first uncertain character. We did not
attempt to optimize this parameter, but fixed n at 2.
1173
the possible characters for w1 along with their re-
spective likelihoods given the visual input (as in
the inner term of Equation 3). Next, these two
wFSAs may simply be composed and then nor-
malized, which completes the belief update, re-
sulting in a new wFSA giving the posterior dis-
tribution over sentences. To calculate the statistic
m, while it is possible to calculate it in closed form
from such a wFSA relatively straightforwardly, for
efficiency we use Monte Carlo estimation based
on samples from the wFSA.
5 Simulation 1
With the description of our model in place, we
next proceed to describe the first simulation in
which we used the model to test the hypothesis
that making regressions is a rational way to cope
with confidence in previous regions falling. Be-
cause there is in general no single rational trade-
off between speed and accuracy, our hypothesis
is that, for any given level of speed and accu-
racy achieved by a non-regressive policy, there is a
faster and more accurate policy that makes a faster
left-to-right pass but occasionally does make re-
gressions. In the terms of our model?s policy pa-
rameters ? and ? described above, non-regressive
policies are exactly those with ? = 0, and a pol-
icy that is faster on the left-to-right pass but does
make regressions is one with a lower value of ?
but a non-zero ? . Thus, we tested the performance
of our model on the reading of a corpus of text typ-
ical of that used in reading experiments at a range
of reasonable non-regressive policies, as well as a
set of regressive policies with lower ? and posi-
tive ? . Our prediction is that the former set will
be strictly dominated in terms of both speed and
accuracy by the latter.
5.1 Methods
5.1.1 Policy parameters
We test 4 non-regressive policies (i.e., those with
? = 0) with values of ? ? {.90, .95, .97, .99}, and
in addition, test regressive policies with a lower
range of ? ? {.85, .90, .95, .97} and ? ? {.4, .7}.4
5.1.2 Language model
Our reader?s language model was an unsmoothed
bigram model created using a vocabulary set con-
4We tested all combinations of these values of ? and ?
except for [?,? ] = [.97, .4], because we did not believe that
a value of ? so low in relation to ? would be very different
from a non-regressive policy.
sisting of the 500 most frequent words in the
British National Corpus (BNC) as well as all the
words in our test corpus. From this vocabulary, we
constructed a bigram model using the counts from
every bigram in the BNC for which both words
were in vocabulary (about 222,000 bigrams).
5.1.3 wFSA implementation
We implemented our model with wFSAs using
the OpenFST library (Allauzen, Riley, Schalk-
wyk, Skut, & Mohri, 2007). Specifically, we
constructed the model?s initial belief state (i.e.,
the distribution over sentences given by its lan-
guage model) by directly translating the bigram
model into a wFSA in the log semiring. We
then composed this wFSA with a weighted finite-
state transducer (wFST) breaking words down
into characters. This was done in order to facili-
tate simple composition with the visual likelihood
wFSA defined over characters. In the Monte Carlo
estimation of m, we used 5000 samples from the
wFSA. Finally, to speed performance, we bounded
the wFSA to have exactly the number of char-
acters present in the actual sentence and then re-
normalized.
5.1.4 Test corpus
We tested our model?s performance by simulating
reading of the Schilling corpus (Schilling, Rayner,
& Chumbley, 1998). To ensure that our results
did not depend on smoothing, we only tested the
model on sentences in which every bigram oc-
curred in the BNC. Unfortunately, only 8 of the 48
sentences in the corpus met this criterion. Thus,
we made single-word changes to 25 more of the
sentences (mostly changing proper names and rare
nouns) to produce a total of 33 sentences to read,
for which every bigram did occur in the BNC.
5.2 Results and discussion
For each policy we tested, we measured the aver-
age number of timesteps it took to read the sen-
tences, as well as the average (natural) log prob-
ability of the correct sentence identity under the
model?s beliefs after reading ended ?Accuracy?.
The results are plotted in Figure 3. As shown in
the graph, for each non-regressive policy (the cir-
cles), there is a regressive policy that outperforms
it, both in terms of average number of timesteps
taken to read (further to the left) and the average
log probability of the sentence identity (higher).
Thus, for a range of policies, these results suggest
1174
Timesteps
Ac
cur
ac
y
?1.2
?1.0
?0.8
?0.6
l
l l
l
50 55 60 65 70
Beta
l non?regressive (beta=0)
regressive (beta=0.4)
regressive (beta=0.7)
Figure 3: Mean number of timesteps taken to read
a sentence and (natural) log probability of the true
identity of the sentence ?Accuracy? for a range of
values of ? and ? . Values of ? are not labeled,
but increase with the number of timesteps for a
constant value of ? . For each non-regressive pol-
icy (? = 0), there is a policy with a lower ? and
higher ? that achieves better accuracy in less time.
that making regressions when confidence about
previous regions falls is a rational reader strategy,
in that it appears to lead to better performance,
both in terms of speed and accuracy.
6 Simulation 2
In Simulation 2, we perform a more direct test of
the idea that making regressions is a rational re-
sponse to the problem of confidence falling about
previous regions using optimization techniques.
Specifically, we search for optimal policy param-
eter values (?,? ) for three different measures of
performance, each representing a different trade-
off between the importance of accuracy and speed.
6.1 Methods
6.1.1 Performance measures
We examine performance measures interpolating
between speed and accuracy of the form
L(1? ?)?T ? (5)
where L is the log probability of the true identity
of the sentence under the model?s beliefs at the end
of reading, and T is the total number of timesteps
before the model decided to stop reading. Thus,
each different performance measure is determined
by the weighting for time ? . We test three values of
? ? {.025, .1, .4}. The first of these weights accu-
racy highly, while the final one weights 1 timestep
almost as much as 1 unit of log probability.
6.1.2 Optimization of policy parameters
Searching directly for optimal values of ? and ?
for our stochastic reading model is difficult be-
cause each evaluation of the model with a partic-
ular set of parameters produces a different result.
We use the PEGASUS method (Ng & Jordan, 2000)
to transform this stochastic optimization problem
into a deterministic one on which we can use stan-
dard optimization algorithms.5 Then, we evaluate
the model?s performance at each value of ? and ?
by reading the full test corpus and averaging per-
formance. We then simply use coordinate ascent
(in logit space) to find the optimal values of ? and
? for each performance measure.
6.1.3 Language model
The language model used in this simulation be-
gins with the same vocabulary set as in Sim. 1,
i.e., the 500 most frequent words in the BNC and
every word that occurs in our test corpus. Because
the search algorithm demands that we evaluate the
performance of our model at a number of param-
eter values, however, it is too slow to optimize ?
and ? using the full language model that we used
for Sim. 1. Instead, we begin with the same set of
bigrams used in Sim. 1 ? i.e., those that contain
two in-vocabulary words ? and trim this set by re-
moving rare bigrams that occur less than 200 times
in the BNC (except that we do not trim any bi-
grams that occur in our test corpus). This reduces
our set of bigrams to about 19,000.
6.1.4 wFSA implementation
The implementation was the same as in Sim. 1.
6.1.5 Test corpus
The test corpus was the same as in Sim. 1.
6.2 Results and discussion
The optimal values of ? and ? for each ? ?
{.025, .1, .4} are given in Table 1 along with the
mean values for L and T found at those parameter
values. As the table shows, the optimization proce-
dure successfully found values of ? and ? , which
go up (slower reading) as ? goes down (valuing
accuracy more than time). In addition, we see that
the average results of reading at these parameter
values are also as we would expect, with T and L
going up as ? goes down. As predicted, the optimal
5Specifically, this involves fixing the random number gen-
erator for each run to produce the same values, resulting in
minimizing the variance in performance across evaluations.
1175
? ? ? Timesteps Log probability
.025 .90 .99 41.2 -0.02
.1 .36 .80 25.8 -0.90
.4 .18 .38 16.4 -4.59
Table 1: Optimal values of ? and ? found for each
performance measure ? tested and mean perfor-
mance at those values, measured in timesteps T
and (natural) log probability L.
values of ? found are non-zero across the range of
policies, which include policies that value speed
over accuracy much more than in Sim. 1. This
provides more evidence that whatever the partic-
ular performance measure used, policies making
regressive saccades when confidence in previous
regions falls perform better than those that do not.
There is one interesting difference between the
results of this simulation and those of Sim. 1,
which is that here, the optimal policies all have a
value of ? > ? . That may at first seem surprising,
since the model?s policy is to fixate a region un-
til its confidence becomes greater than ? and then
return if it falls below ? . It would seem, then, that
the only reasonable values of ? are those that are
strictly below ? . In fact, this is not the case be-
cause of the two time step delay between the de-
cision to move the eyes and the execution of that
saccade. Because of this delay, the model?s confi-
dence when it leaves a region (relevant to ? ) will
generally be higher than when it decided to leave
(determined by ?). In Simulation 2, because of the
smaller grammar that was used, the model?s confi-
dence in a region?s identity rises more quickly and
this difference is exaggerated.
7 Conclusion
In this paper, we presented a model that performs
Bayesian inference on the identity of a sentence,
combining a language model with noisy informa-
tion about letter identities from a realistic visual
input model. On the basis of these inferences, it
uses a simple policy to determine how long to
continue fixating the current position and where
to fixate next, on the basis of information about
where the model is uncertain about the sentence?s
identity. As such, it constitutes a rational model
of eye movement control in reading, extending the
insights from previous results about rationality in
language comprehension.
The results of two simulations using this model
support a novel explanation for between-word re-
gressive saccades in reading: that they are used to
gather visual input about previous regions when
confidence about them falls. Simulation 1 showed
that a range of policies making regressions in these
cases outperforms a range of non-regressive poli-
cies. In Simulation 2, we directly searched for op-
timal values for the policy parameters for three dif-
ferent performance measures, representing differ-
ent speed-accuracy trade-offs, and found that the
optimal policies in each case make substantial use
of between-word regressions when confidence in
previous regions falls. In addition to supporting
a novel motivation for between-word regressions,
these simulations demonstrate the possibility for
testing a range of questions that were impossi-
ble with previous models of reading related to the
goals of a reader, such as how should reading be-
havior change as accuracy is valued more.
There are a number of obvious ways for the
model to move forward. One natural next step is
to make the model more realistic by using letter
confusability matrices. In addition, the link to pre-
vious work in sentence processing can be made
tighter by incorporating syntax-based language
models. It also remains to compare this model?s
predictions to human data more broadly on stan-
dard benchmark measures for models of read-
ing. The most important future development, how-
ever, will be moving toward richer policy families,
which enable more intelligent decisions about eye
movement control, based not just on simple confi-
dence statistics calculated independently for each
character position, but rather which utilize the rich
structure of the model?s posterior beliefs about the
sentence identity (and of language itself) to make
more informed decisions about the best time to
move the eyes and the best location to direct them
next.
Acknowledgments
The authors thank Jeff Elman, Tom Griffiths,
Andy Kehler, Keith Rayner, and Angela Yu for
useful discussion about this work. This work bene-
fited from feedback from the audiences at the 2010
LSA and CUNY conferences. The research was
partially supported by NIH Training Grant T32-
DC000041 from the Center for Research in Lan-
guage at UC San Diego to K.B., by a research
grant from the UC San Diego Academic Senate
to R.L., and by NSF grant 0953870 to R.L.
1176
References
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,
& Mohri, M. (2007). OpenFst: A general and
efficient weighted finite-state transducer library.
In Proceedings of the Ninth International Con-
ference on Implementation and Application of
Automata, (CIAA 2007) (Vol. 4783, p. 11-23).
Springer.
Bicknell, K., & Levy, R. (2010). Rational eye
movements in reading combining uncertainty
about previous words with contextual probabil-
ity. In Proceedings of the 32nd Annual Confer-
ence of the Cognitive Science Society. Austin,
TX: Cognitive Science Society.
Boston, M. F., Hale, J. T., Kliegl, R., Patil, U., &
Vasishth, S. (2008). Parsing costs as predic-
tors of reading difficulty: An evaluation using
the potsdam sentence corpus. Journal of Eye
Movement Research, 2(1), 1?12.
Connine, C. M., Blasko, D. G., & Hall, M. (1991).
Effects of subsequent sentence context in audi-
tory word recognition: Temporal and linguistic
constraints. Journal of Memory and Language,
30, 234?250.
Demberg, V., & Keller, F. (2008). Data from eye-
tracking corpora as evidence for theories of syn-
tactic processing complexity. Cognition, 109,
193?210.
Ehrlich, S. F., & Rayner, K. (1981). Contextual
effects on word perception and eye movements
during reading. Journal of Verbal Learning and
Verbal Behavior, 20, 641?655.
Engbert, R., & Kr?gel, A. (2010). Readers use
Bayesian estimation for eye movement control.
Psychological Science, 21, 366?371.
Engbert, R., Longtin, A., & Kliegl, R. (2002). A
dynamical model of saccade generation in read-
ing based on spatially distributed lexical pro-
cessing. Vision Research, 42, 621?636.
Engbert, R., Nuthmann, A., Richter, E. M., &
Kliegl, R. (2005). SWIFT: A dynamical model
of saccade generation during reading. Psycho-
logical Review, 112, 777?813.
Engel, G. R., Dougherty, W. G., & Jones, B. G.
(1973). Correlation and letter recognition.
Canadian Journal of Psychology, 27, 317?326.
Genzel, D., & Charniak, E. (2002, July). Entropy
rate constancy in text. In Proceedings of the 40th
annual meeting of the Association for Computa-
tional Linguistics (pp. 199?206). Philadelphia:
Association for Computational Linguistics.
Genzel, D., & Charniak, E. (2003). Variation of
entropy and parse trees of sentences as a func-
tion of the sentence number. In M. Collins &
M. Steedman (Eds.), Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (pp. 65?72). Sapporo,
Japan: Association for Computational Linguis-
tics.
Geyer, L. H. (1977). Recognition and confusion
of the lowercase alphabet. Perception & Psy-
chophysics, 22, 487?490.
Hale, J. (2001). A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
Second Meeting of the North American Chapter
of the Association for Computational Linguistics
(Vol. 2, pp. 159?166). New Brunswick, NJ: As-
sociation for Computational Linguistics.
Jaeger, T. F. (2010). Redundancy and re-
duction: Speakers manage syntactic in-
formation density. Cognitive Psychology.
doi:10.1016/j.cogpsych.2010.02.002.
Jurafsky, D. (1996). A probabilistic model of
lexical and syntactic access and disambiguation.
Cognitive Science, 20, 137?194.
Keller, F. (2004). The entropy rate principle as
a predictor of processing effort: An evaluation
against eye-tracking data. In D. Lin & D. Wu
(Eds.), Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Pro-
cessing (pp. 317?324). Barcelona, Spain: As-
sociation for Computational Linguistics.
Legge, G. E., Hooven, T. A., Klitz, T. S., Mans-
field, J. S., & Tjan, B. S. (2002). Mr.
Chips 2002: new insights from an ideal-observer
model of reading. Vision Research, 42, 2219?
2234.
Legge, G. E., Klitz, T. S., & Tjan, B. S. (1997).
Mr. Chips: an Ideal-Observer model of reading.
Psychological Review, 104, 524?553.
Levy, R. (2008). A noisy-channel model of ra-
tional human sentence comprehension under un-
certain input. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (pp. 234?243). Honolulu,
Hawaii: Association for Computational Linguis-
tics.
Levy, R., Bicknell, K., Slattery, T., & Rayner,
K. (2009). Eye movement evidence that read-
ers maintain and act on uncertainty about past
linguistic input. Proceedings of the National
Academy of Sciences, 106, 21086?21090.
1177
Levy, R., & Jaeger, T. F. (2007). Speakers op-
timize information density through syntactic re-
duction. In B. Sch?lkopf, J. Platt, & T. Hoffman
(Eds.), Advances in Neural Information Pro-
cessing Systems 19 (pp. 849?856). Cambridge,
MA: MIT Press.
Levy, R., Reali, F., & Griffiths, T. L. (2009).
Modeling the effects of memory on human on-
line sentence processing with particle filters. In
D. Koller, D. Schuurmans, Y. Bengio, & L. Bot-
tou (Eds.), Advances in Neural Information Pro-
cessing Systems 21 (pp. 937?944).
Mohri, M. (1997). Finite-state transducers in lan-
guage and speech processing. Computational
Linguistics, 23, 269?311.
Narayanan, S., & Jurafsky, D. (2001). A Bayesian
model predicts human parse preference and
reading time in sentence processing. In T. Diet-
terich, S. Becker, & Z. Ghahramani (Eds.), Ad-
vances in Neural Information Processing Sys-
tems 14 (pp. 59?65). Cambridge, MA: MIT
Press.
Ng, A. Y., & Jordan, M. (2000). PEGASUS:
A policy search method for large MDPs and
POMDPs. In Uncertainty in Artificial Intelli-
gence, Proceedings of the Sixteenth Conference
(pp. 406?415).
Norris, D. (2006). The Bayesian reader: Explain-
ing word recognition as an optimal Bayesian de-
cision process. Psychological Review, 113, 327?
357.
Norris, D. (2009). Putting it all together: A unified
account of word recognition and reaction-time
distributions. Psychological Review, 116, 207?
219.
Rayner, K. (1998). Eye movements in reading and
information processing: 20 years of research.
Psychological Bulletin, 124, 372?422.
Reichle, E. D., & Laurent, P. A. (2006). Using
reinforcement learning to understand the emer-
gence of ?intelligent? eye-movement behavior
during reading. Psychological Review, 113,
390?408.
Reichle, E. D., Pollatsek, A., Fisher, D. L., &
Rayner, K. (1998). Toward a model of eye
movement control in reading. Psychological Re-
view, 105, 125?157.
Reichle, E. D., Pollatsek, A., & Rayner, K.
(2006). E-Z Reader: A cognitive-control, serial-
attention model of eye-movement behavior dur-
ing reading. Cognitive Systems Research, 7, 4?
22.
Reichle, E. D., Warren, T., & McConnell, K.
(2009). Using E-Z Reader to model the ef-
fects of higher level language processing on eye
movements during reading. Psychonomic Bul-
letin & Review, 16, 1?21.
Schilling, H. E. H., Rayner, K., & Chumbley, J. I.
(1998). Comparing naming, lexical decision,
and eye fixation times: Word frequency effects
and individual differences. Memory & Cogni-
tion, 26, 1270?1281.
Smith, N. J., & Levy, R. (2008). Optimal process-
ing times in reading: a formal model and empir-
ical investigation. In B. C. Love, K. McRae, &
V. M. Sloutsky (Eds.), Proceedings of the 30th
Annual Conference of the Cognitive Science So-
ciety (pp. 595?600). Austin, TX: Cognitive Sci-
ence Society.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eber-
hard, K. M., & Sedivy, J. C. (1995). Integration
of visual and linguistic information in spoken
language comprehension. Science, 268, 1632?
1634.
1178
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1094?1103,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Nonparametric Learning of Phonological Constraints in Optimality
Theory
Gabriel Doyle
Department of Linguistics
UC San Diego
La Jolla, CA, USA 92093
gdoyle@ucsd.edu
Klinton Bicknell
Department of Linguistics
Northwestern University
Evanston, IL, USA 60208
kbicknell@northwestern.edu
Roger Levy
Department of Linguistics
UC San Diego
La Jolla, CA, USA 92093
rlevy@ucsd.edu
Abstract
We present a method to jointly learn fea-
tures and weights directly from distri-
butional data in a log-linear framework.
Specifically, we propose a non-parametric
Bayesian model for learning phonologi-
cal markedness constraints directly from
the distribution of input-output mappings
in an Optimality Theory (OT) setting. The
model uses an Indian Buffet Process prior
to learn the feature values used in the log-
linear method, and is the first algorithm
for learning phonological constraints with-
out presupposing constraint structure. The
model learns a system of constraints that
explains observed data as well as the
phonologically-grounded constraints of a
standard analysis, with a violation struc-
ture corresponding to the standard con-
straints. These results suggest an alterna-
tive data-driven source for constraints in-
stead of a fully innate constraint set.
1 Introduction
Many aspects of human cognition involve the in-
teraction of constraints that push a decision-maker
toward different options, whether in something so
trivial as choosing a movie or so important as
a fight-or-flight response. These constraint-driven
decisions can be modeled with a log-linear system.
In these models, a set of constraints is weighted
and their violations are used to determine a prob-
ability distribution over outcomes. But where do
these constraints come from?
We consider this question by examining the
dominant framework in modern phonology, Opti-
mality Theory (Prince and Smolensky, 1993, OT),
implemented in a log-linear framework, MaxEnt
OT (Goldwater and Johnson, 2003), with output
forms? probabilities based on a weighted sum of
constraint violations. OT analyses generally as-
sume that the constraints are innate and univer-
sal, both to obviate the problem of learning con-
straints? identities and to limit the set of possible
languages.
We propose a new approach: to learn con-
straints with limited innate phonological knowl-
edge by identifying sets of constraint violations
that explain the observed distributional data, in-
stead of selecting constraints from an innate set
of constraint definitions. Because the constraints
are identified as sets of violations, this also per-
mits constraints specific to a given language to
be learned. This method, which we call IBPOT,
uses an Indian Buffet Process (IBP) prior to define
the space of possible constraint violation matri-
ces, and uses Bayesian reasoning to identify con-
straint matrices likely to have generated the ob-
served data. In identifying constraints solely by
their extensional violation profiles, this method
does not directly identify the intensional defini-
tions of the identified constraints, but to the extent
that the resulting violation profiles are phonologi-
cally interpretable, we may conclude that the data
themselves guide constraint identification. We test
IBPOT on tongue-root vowel harmony in Wolof, a
West African language.
The set of constraints learned by the model sat-
isfy two major goals: they explain the data as well
as the standard phonological analysis, and their vi-
olation structures correspond to the standard con-
straints. This suggests an alternative data-driven
genesis for constraints, rather than the traditional
assumption of fully innate constraints.
2 Phonology and Optimality Theory
2.1 OT structure
Optimality Theory has been used for constraint-
based analysis of many areas of language, but we
focus on its most successful application: phonol-
ogy. We consider an OT analysis of the mappings
1094
between underlying forms and their phonological
manifestations ? i.e., mappings between forms in
the mental lexicon and the actual vocalized forms
of the words.
1
Stated generally, an OT system takes some in-
put, generates a set of candidate outputs, deter-
mines what constraints each output violates, and
then selects a candidate output with a relatively
unobjectionable violation profile. To do this, an
OT system contains four major components: a
generator GEN, which generates candidate out-
put forms for the input; a set of constraints CON,
which penalize candidates; a evaluation method
EVAL, which selects an winning candidate; and
H , a language-particular weighting of constraints
that EVAL uses to determine the winning candi-
date. Previous OT work has focused on identifying
the appropriate formulation of EVAL and the val-
ues and acquisition of H , while taking GEN and
CON as given. Here, we expand the learning task
by proposing an acquisition method for CON.
To learn CON, we propose a data-driven
markedness constraint learning system that avoids
both innateness and tractability issues. Unlike pre-
vious OT learning methods, which assume known
constraint definitions and only learn the relative
strength of these constraints, the IBPOT learns
constraint violation profiles and weights for them
simultaneously. The constraints are derived from
sets of violations that effectively explain the ob-
served data, rather than being selected from a pre-
existing set of possible constraints.
2.2 OT as a weighted-constraint method
Although all OT systems share the same core
structure, different choices of EVAL lead to dif-
ferent behaviors. In IBPOT, we use the log-
linear EVAL developed by Goldwater and John-
son (2003) in their MaxEnt OT system. MEOT
extends traditional OT to account for variation
(cases in which multiple candidates can be the
winner), as well as gradient/probabilistic produc-
tions (Anttila, 1997) and other constraint interac-
tions (e.g., cumulativity) that traditional OT can-
not handle (Keller, 2000). MEOT also is motivated
by the general MaxEnt framework, whereas most
other OT formulations are ad hoc constructions
specific to phonology.
In MEOT, each constraint C
i
is associated with
1
Although phonology is usually framed in terms of sound,
sign languages also have components that serve equivalent
roles in the physical realization of signs (Stokoe, 1960).
a weight w
i
< 0. (Weights are always negative
in OT; a constraint violation can never make a
candidate more likely to win.) For a given input-
candidate pair (x, y), f
i
(y, x) is the number of vi-
olations of constraint C
i
by the pair. As a maxi-
mum entropy model, the probability of y given x
is proportional to the exponential of the weighted
sum of violations,
?
i
w
i
f
i
(y, x). If Y(x) is the
set of all output candidates for the input x, then
the probability of y as the winning output is:
p(y|x) =
exp (
?
i
w
i
f
i
(y, x))
?
z?Y(x)
exp (
?
i
w
i
f
i
(z, x))
(1)
This formulation represents a probabilistic
extension of the traditional formulation of
OT (Prince and Smolensky, 1993). Traditionally,
constraints form a strict hierarchy, where a single
violation of a high-ranked constraint is worse than
any number of violations of lower-ranked con-
straints. Traditional OT is also deterministic, with
the optimal candidate always selected. In MEOT,
the constraint weights define hierarchies of vary-
ing strictness, and some probability is assigned to
all candidates. If constraints? weights are close to-
gether, multiple violations of lower-weighted con-
straints can reduce a candidate?s probability below
that of a competitor with a single high-weight vio-
lation. As the distance between weights in MEOT
increases, the probability of a suboptimal candi-
date being chosen approaches zero; thus the tradi-
tional formulation is a limit case of MEOT.
2.3 OT in practice
Figure 1 shows tableaux, a visualization for
OT, applied in Wolof (Archangeli and Pulley-
blank, 1994; Boersma, 1999). We are interested
in four Wolof constraints that combine to induce
vowel harmony: *I, PARSE[rtr], HARMONY, and
PARSE[atr]. The meaning of these constraints will
be discussed in Sect. 4.1; for now, we will only
consider their violation profiles. Each column rep-
resents a constraint, with weights decreasing left-
to-right. Each tableau looks at a single input form,
noted in the top-left cell: ete, EtE, Ite, or itE.
Each row is a candidate output form. A black
cell indicates that the candidate, or input-candidate
pair, violates the constraint in that column.
2
A
white cell indicates no violation. Grey stripes are
2
In general, a constraint can be violated multiple times
by a given candidate, but we will be using binary constraints
(violated or not) in this work. See Sect. 5.2 for further discus-
sion.
1095
ete *? Parse(rtr) Harmony Parse(atr) Score ?te *? Parse(rtr) Harmony Parse(atr) Score
ete 0 ite -32
?te -24 ?te -80
et? -24 it? -56
?t? -8 ?t? -72
?t? *? Parse(rtr) Harmony Parse(atr) Score it? *? Parse(rtr) Harmony Parse(atr) Score
ete -32 ite -32
?te -48 ?te -120
et? -48 it? -16
?t? 0 ?t? -72
Figure 1: Tableaux for the Wolof input forms ete, EtE, Ite, and itE. Black indicates violation, white no
violation. Scores are calculated for a MaxEnt OT system with constraint weights of -64, -32, -16, and -8,
approximating a traditional hierarchical OT design. Values of grey-striped cells have negligible effects
on the distribution (see Sect. 4.3).
overlaid on cells whose value will have a negligi-
ble impact on the distribution due to the values of
higher-ranked constraint.
Constraints fall into two categories, faithful-
ness and markedness, which differ in what infor-
mation they use to assign violations. Faithfulness
constraints penalize mismatches between the in-
put and output, while markedness constraints con-
sider only the output. Faithfulness violations in-
clude phoneme additions or deletions between the
input and output; markedness violations include
penalizing specific phonemes in the output form,
regardless of whether the phoneme is present in
the input.
In MaxEnt OT, each constraint has a weight,
and the candidates? scores are the sums of the
weights of violated constraints. In the ete tableau
at top left, output ete has no violations, and there-
fore a score of zero. Outputs Ete and etE vio-
late both HARMONY (weight 16) and PARSE[atr]
(weight 8), so their scores are 24. Output EtE vi-
olates PARSE[atr], and has score 8. Thus the log-
probability of output EtE is 1/8 that of ete, and the
log-probability of disharmonious Ete and etE are
each 1/24 that of ete. As the ratio between scores
increases, the log-probability ratios can become
arbitrarily close to zero, approximating the deter-
ministic situation of traditional OT.
2.4 Learning Constraints
Choosing a winning candidate presumes that a
set of constraints CON is available, but where do
these constraints come from? The standard as-
sumption within OT is that CON is innate and
universal. But in the absence of direct evidence
of innate constraints, we should prefer a method
that can derive the constraints from cognitively-
general learning over one that assumes they are
pre-specified. Learning appropriate model features
has been an important idea in the development of
constraint-based models (Della Pietra et al, 1997).
The innateness assumption can induce tractabil-
ity issues as well. The strictest formulation of in-
nateness posits that virtually all constraints are
shared across all languages, even when there is
no evidence for the constraint in a particular lan-
guage (Tesar and Smolensky, 2000). Strict uni-
versality is undermined by the extremely large
set of constraints it must weight, as well as
the possible existence of language-particular con-
straints (Smith, 2004).
A looser version of universality supposes that
constraints are built compositionally from a set
of constraint templates or primitives or phono-
logical features (Hayes, 1999; Smith, 2004; Id-
sardi, 2006; Riggle, 2009). This version allows
language-particular constraints, but it comes with
a computational cost, as the learner must be able
to generate and evaluate possible constraints while
learning the language?s phonology. Even with rel-
atively simple constraint templates, such as the
phonological constraint learner of Hayes and Wil-
son (2008), the number of possible constraints ex-
pands exponentially. Depending on the specific
formulation of the constraints, the constraint iden-
tification problem may even be NP-hard (Idsardi,
2006; Heinz et al, 2009). Our approach of casting
the learning problem as one of identifying viola-
tion profiles is an attempt to determine the amount
that can be learned about the active constraints in a
paradigm without hypothesizing intensional con-
straint definitions. The violation profile informa-
1096
tion used by our model could then be used to nar-
row the search space for intensional constraints,
either by performing post-hoc analysis of the con-
straints identified by our model or by combining
intensional constraint search into the learning pro-
cess. We discuss each of these possibilities in Sec-
tion 5.2.
Innateness is less of a concern for faithfulness
than markedness constraints. Faithfulness viola-
tions are determined by the changes between an
input form and a candidate, yielding an indepen-
dent motivation for a universal set of faithfulness
constraints (McCarthy, 2008). Some markedness
constraints can also be motivated in a universal
manner (Hayes, 1999), but many markedness con-
straints lack such grounding.
3
As such, it is un-
clear where a universal set of markedness con-
straints would come from.
3 The IBPOT Model
3.1 Structure
The IBPOT model defines a generative process for
mappings between input and output forms based
on three latent variables: the constraint violation
matrices F (faithfulness) and M (markedness),
and the weight vector w. The cells of the violation
matrices correspond to the number of violations of
a constraint by a given input-output mapping. F
ijk
is the number of violations of faithfulness con-
straint F
k
by input-output pair type (x
i
, y
j
);M
jl
is
the number of violations of markedness constraint
M
?l
by output candidate y
j
. Note that M is shared
across inputs, as M
jl
has the same value for all
input-output pairs with output y
j
. The weight vec-
tor w provides weight for both F and M . Proba-
bilities of output forms are given by a log-linear
function:
p(y
j
|x
i
) =
exp (
?
k
w
k
F
ijk
+
?
l
w
l
M
jl
)
?
y
z
?Y(x
i
)
exp (
?
k
w
k
F
izk
+
?
l
w
l
M
zl
)
(2)
Note that this is the same structure as Eq. 1
but with faithfulness and markedness constraints
listed separately. As discussed in Sect. 2.4, we as-
sume that F is known as part of the output of GEN
(Riggle, 2009). The goal of the IBPOT model is to
3
McCarthy (2008, ?4.8) gives examples of ?ad hoc? in-
tersegmental constraints. Even well-known constraint types,
such as generalized alignment, can have disputed structures
(Hyde, 2012).
learn the markedness matrix M and weights w for
both the markedness and faithfulness constraints.
As for M , we need a non-parametric prior, as
there is no inherent limit to the number of marked-
ness constraints a language will use. We use the
Indian Buffet Process (Griffiths and Ghahramani,
2005), which defines a proper probability distri-
bution over binary feature matrices with an un-
bounded number of columns. The IBP can be
thought of as representing the set of dishes that
diners eat at an infinite buffet table. Each diner
(i.e., output form) first draws dishes (i.e., con-
straint violations) with probability proportional
to the number of previous diners who drew it:
p(M
jl
= 1|{M
zl
}
z<j
) = n
l
/j. After choosing
from the previously taken dishes, the diner can
try additional dishes that no previous diner has
had. The number of new dishes that the j-th cus-
tomer draws follows a Poisson(?/j) distribution.
The complete specification of the model is then:
M ? IBP (?); Y(x
i
) = Gen(x
i
)
w ? ??(1, 1); y|x
i
? LogLin(M,F,w,Y(x
i
))
3.2 Inference
To perform inference in this model, we adopt a
common Markov chain Monte Carlo estimation
procedure for IBPs (G?or?ur et al, 2006; Navarro
and Griffiths, 2007). We alternate approximate
Gibbs sampling over the constraint matrix M ,
using the IBP prior, with a Metropolis-Hastings
method to sample constraint weights w.
We initialize the model with a randomly-drawn
markedness violation matrix M and weight vector
w. To learn, we iterate through the output forms
y
j
; for each, we splitM
?j?
into ?represented? con-
straints (those that are violated by at least one
output form other than y
j
) and ?non-represented?
constraints (those violated only by y
j
). For each
represented constraintM
?l
, we re-sample the value
for the cell M
jl
. All non-represented constraints
are removed, and we propose new constraints, vi-
olated only by y
j
, to replace them. After each it-
eration throughM , we use Metropolis-Hastings to
update the weight vector w.
Represented constraint sampling We begin by
resampling M
jl
for all represented constraints
M
?l
, conditioned on the rest of the violations
(M
?(jl)
, F ) and the weights w. This is the sam-
pling counterpart of drawing existing features in
the IBP generative process. By Bayes? Rule, the
1097
posterior probability of a violation is propor-
tional to product of the likelihood p(Y |M
jl
=
1,M
?jl
, F, w) from Eq. 2 and the IBP prior prob-
ability p(M
jl
= 1|M
?jl
) = n
?jl
/n, where n
?jl
is the number of outputs other than y
j
that violate
constraint M
?l
.
Non-represented constraint sampling After
sampling the represented constraints for y
j
, we
consider the addition of new constraints that are
violated only by y
j
. This is the sampling coun-
terpart to the Poisson draw for new features in
the IBP generative process. Ideally, this would
draw new constraints from the infinite feature ma-
trix; however, this requires marginalizing the like-
lihood over possible weights, and we lack an ap-
propriate conjugate prior for doing so. We approx-
imate the infinite matrix with a truncated Bernoulli
draw over unrepresented constraints (G?or?ur et al,
2006). We consider in each sample at most K
?
new constraints, with weights based on the auxil-
iary vector w
?
. This approximation retains the un-
bounded feature set of the IBP, as repeated sam-
pling can add more and more constraints without
limit.
The auxiliary vector w
?
contains the weights
of all the constraints that have been removed in
the previous step. If the number of constraints
removed is less than K
?
, w
?
is filled out with
draws from the prior distribution over weights. We
then consider adding any subset of these new con-
straints to M , each of which would be violated
only by y
j
. Let M
?
represent a (possibly empty)
set of constraints paired with a subset of w
?
. The
posterior probability of drawingM
?
from the trun-
cated Bernoulli distribution is the product of the
prior probability of M
?
(
?
K
?
N
Y
+
?
K
?
)
and the like-
lihood p(Y |M
?
, w
?
,M,w, F ), including the new
constraints M
?
.
Weight sampling After sampling through
all candidates, we use Metropolis-Hastings
to estimate new weights for both con-
straint matrices. Our proposal distribution is
Gamma(w
k
2
/?, ?/w
k
), with mean w
k
and
mode w
k
?
?
w
k
(for w
k
> 1). Unlike Gibbs
sampling on the constraints, which occurs only on
markedness constraints, weights are sampled for
both markedness and faithfulness features.
4 Experiment
4.1 Wolof vowel harmony
We test the model by learning the markedness con-
straints driving Wolof vowel harmony (Archangeli
and Pulleyblank, 1994). Vowel harmony in gen-
eral refers to a phonological phenomenon wherein
the vowels of a word share certain features in the
output form even if they do not share them in the
input. In the case of Wolof, harmony encourages
forms that have consistent tongue root positions.
The Wolof vowel system has two relevant fea-
tures, tongue root position and vowel height. The
tongue root can either be advanced (ATR) or re-
tracted (RTR), and the body of the tongue can be in
the high, middle, or low part of the mouth. These
features define six vowels:
high mid low
ATR i e @
RTR I E a
We test IBPOT on the harmony system provided
in the Praat program (Boersma, 1999), previ-
ously used as a test case by Goldwater and John-
son (2003) for MEOT learning with known con-
straints. This system has four constraints:
4
? Markedness:
? *I: do not have I (high RTR vowel)
? HARMONY: do not have RTR and ATR
vowels in the same word
? Faithfulness:
? PARSE[rtr]: do not change RTR input to
ATR output
? PARSE[atr]: do not change ATR input to
RTR output
These constraints define the phonological stan-
dard that we will compare IBPOT to, with a rank-
ing from strongest to weakest of *I>> PARSE[rtr]
>> HARMONY >> PARSE[atr]. Under this rank-
ing, Wolof harmony is achieved by changing a
disharmonious ATR to an RTR, unless this cre-
ates an I vowel. We see this in Figure 1, where
three of the four winners are harmonic, but with
input itE, harmony would require violating one
of the two higher-ranked constraints. As in previ-
ous MEOT work, all Wolof candidates are faithful
4
The version in Praat includes a fifth constraint, but its
value never affects the choice of output in our data and is
omitted in this analysis.
1098
with respect to vowel height, either because height
changes are not considered by GEN, or because
of a high-ranked faithfulness constraint blocking
height changes.
5
The Wolof constraints provide an interesting
testing ground for the model, because it is a small
set of constraints to be learned, but contains the
HARMONY constraint, which can be violated by
non-adjacent segments. Non-adjacent constraints
are difficult for string-based approaches because
of the exponential number of possible relation-
ships across non-adjacent segments. However, the
Wolof results show that by learning violations di-
rectly, IBPOT does not encounter problems with
non-adjacent constraints.
The Wolof data has 36 input forms, each of the
form V
1
tV
2
, where V
1
and V
2
are vowels that agree
in height. Each input form has four candidate out-
puts, with one output always winning. The outputs
appear for multiple inputs, as shown in Figure 1.
The candidate outputs are the four combinations
of tongue-roots for the given vowel heights; the
inputs and candidates are known to the learner.
We generate simulated data by observing 1000 in-
stances of the winning output for each input.
6
The
model must learn the markedness constraints *I
and HARMONY, as well as the weights for all four
constraints.
We make a small modification to the constraints
for the test data: all constraints are limited to bi-
nary values. For constraints that can be violated
multiple times by an output (e.g., *I twice by ItI),
we use only a single violation. This is necessary in
the current model definition because the IBP pro-
duces a prior over binary matrices. We generate
the simulated data using only single violations of
each constraint by each output form. Overcoming
the binarity restriction is discussed in Sect. 5.2.
4.2 Experiment Design
We run the model for 10000 iterations, using de-
terministic annealing through the first 2500 it-
5
In the present experiment, we assume that GEN does not
generate candidates with unfaithful vowel heights. If unfaith-
ful vowel heights were allowed by GEN, these unfaithful can-
didates would incur a violation approximately as strong as *I,
as neither unfaithful-height candidates nor I candidates are at-
tested in the Wolof data.
6
Since data, matrix, and weight likelihoods all shape the
learned constraints, there must be enough data for the model
to avoid settling for a simple matrix that poorly explains the
data. This represents a similar training set size to previous
work (Goldwater and Johnson, 2003; Boersma and Hayes,
2001).
erations. The model is initialized with a ran-
dom markedness matrix drawn from the IBP and
weights from the exponential prior. We ran ver-
sions of the model with parameter settings be-
tween 0.01 and 1 for ?, 0.05 and 0.5 for ?, and
2 and 5 for K
?
. All these produced quantitatively
similar results; we report values for ? = 1, ? =
0.5, and K
?
= 5, which provides the least bias
toward small constraint sets.
To establish performance for the phonological
standard, we use the IBPOT learner to find con-
straint weights but do not update M . The resultant
learner is essentially MaxEnt OT with the weights
estimated through Metropolis sampling instead of
gradient ascent. This is done so that the IBPOT
weights and phonological standard weights are
learned by the same process and can be compared.
We use the same parameters for this baseline as
for the IBPOT tests. The results in this section are
based on nine runs each of IBPOT and MEOT; ten
MEOT runs were performed but one failed to con-
verge and was removed from analysis.
4.3 Results
A successful set of learned constraints will satisfy
two criteria: achieving good data likelihood (no
worse than the phonological-standard constraints)
and acquiring constraint violation profiles that are
phonologically interpretable. We find that both of
these criteria are met by IBPOT on Wolof.
Likelihood comparison First, we calculate the
joint probability of the data and model given the
priors, p(Y,M,w|F, ?), which is proportional to
the product of three terms: the data likelihood
p(Y |M,F,w), the markedness matrix probabil-
ity p(M |?), and the weight probability p(w). We
present both the mean and MAP values for these
over the final 1000 iterations of each run. Results
are shown in Table 1.
All eight differences are significant according
to t-tests over the nine runs. In all cases but mean
M , the IBPOT method has a better log-probability.
The most important differences are those in the
data probabilities, as the matrix and weight prob-
abilities are reflective primarily of the choice of
prior. By both measures, the IBPOT constraints
explain the observed data better than the phono-
logically standard constraints.
Interestingly, the mean M probability is lower
for IBPOT than for the phonological standard.
Though the phonologically standard constraints
1099
MAP Mean
IBPOT PS IBPOT PS
Data -1.52 -3.94 -5.48 -9.23
M -51.7 -53.3 -54.7 -53.3
w -44.2 -71.1 -50.6 -78.1
Joint -97.4 -128.4 -110.6 -140.6
Table 1: Data, markedness matrix, weight vec-
tor, and joint log-probabilities for the IBPOT and
the phonological standard constraints. MAP and
mean estimates over the final 1000 iterations for
each run. All IBPOT/PS differences are significant
(p < .005 for MAP M ; p < .001 for others).
exist independently of the IBP prior, they fit the
prior better than the average IBPOT constraints do.
This shows that the IBP?s prior preferences can be
overcome in order to have constraints that better
explain the data.
Constraint comparison Our second criterion
is the acquisition of meaningful constraints,
that is, ones whose violation profiles have
phonologically-grounded explanations. IBPOT
learns the same number of markedness constraints
as the phonological standard (two); over the final
1000 iterations of the model runs, 99.2% of the it-
erations had two markedness constraints, and the
rest had three.
Turning to the form of these constraints, Figure
2 shows violation profiles from the last iteration
of a representative IBPOT run.
7
Because vowel
heights must be faithful between input and out-
put, the Wolof data is divided into nine separate
paradigms, each containing the four candidates
(ATR/RTR ? ATR/RTR) for the vowel heights in
the input.
The violations on a given output form only
affect probabilities within its paradigm. As a
result, learned constraints are consistent within
paradigms, but across paradigms, the same con-
straint may serve different purposes.
For instance, the strongest learned markedness
constraint, shown as M1 in Figure 2, has the same
violations as the top-ranked constraint that ac-
tively distinguishes between candidates in each
paradigm. For the five paradigms with at least
one high vowel (the top row and left column),
M1 has the same violations as *I, as *I penal-
izes some but not all of the candidates. In the
7
Specifically, from the run with the median joint posterior.
other four paradigms, *I penalizes none of the
candidates, and the IBPOT learner has no rea-
son to learn it. Instead, it learns that M1 has
the same violations as HARMONY, which is the
highest-weighted constraint that distinguishes be-
tween candidates in these paradigms. Thus in the
high-vowel paradigms, M1 serves as *I, while in
the low/mid-vowel paradigms, it serves as HAR-
MONY.
The lower-weighted M2 is defined noisily, as
the higher-ranked M1 makes some values of M2
inconsequential. Consider the top-left paradigm of
Figure 2, the high-high input, in which only one
candidate does not violate M1 (*I). Because M1
has a much higher weight than M2, a violation of
M2 has a negligible effect on a candidate?s prob-
ability.
8
In such cells, the constraint?s value is in-
fluenced more by the prior than by the data. These
inconsequential cells are overlaid with grey stripes
in Figure 2.
The meaning of M2, then, depends only on the
consequential cells. In the high-vowel paradigms,
M2 matches HARMONY, and the learned and stan-
dard constraints agree on all consequential viola-
tions, despite being essentially at chance on the in-
distinguishable violations (58%). On the non-high
paradigms, the meaning of M2 is unclear, as HAR-
MONY is handled by M1 and *I is unviolated. In
all four paradigms, the model learns that the RTR-
RTR candidate violates M2 and the ATR-ATR can-
didate does not; this appears to be the model?s at-
tempt to reinforce a pattern in the lowest-ranked
faithfulness constraint (PARSE[atr]), which the
ATR-ATR candidate never violates.
Thus, while the IBPOT constraints are not
identical to the phonologically standard ones,
they reflect a version of the standard constraints
that is consistent with the IBPOT framework.
9
In paradigms where each markedness constraint
distinguishes candidates, the learned constraints
match the standard constraints. In paradigms
where only one constraint distinguishes candi-
dates, the top learned constraint matches it and the
second learned constraint exhibits a pattern con-
sistent with a low-ranked faithfulness constraint.
8
Given the learned weights in Fig. 2, if the losing candi-
date violates M1, its probability changes from 10
?12
when
the preferred candidate does not violate M2 to 10
?8
when it
does.
9
In fact, it appears this constraint organization is favored
by IBPOT as it allows for lower weights, hence the large dif-
ference in w log-probability in Table 1.
1100
*? Harmony M1 M2 *? Harmony M1 M2 *? Harmony M1 M2
iti eti ?ti
?ti ?ti ati
it? et? ?t?
?t? ?t? at?
ite ete ?te
?te ?te ate
it? et? ?t?
?t? ?t? at?
it? et? ?t?
?t? ?t? at?
ita eta ?ta
?ta ?ta ata
LearnedPhono. Std.
hi
hi
hi
mid
hi
lo
Phono. Std. Learned
mid
lo
mid
mid
mid
hi
Phono. Std. Learned
lo
hi
lo
mid
lo
lo
Figure 2: Phonologically standard (*I, HARMONY) and learned (M1,M2) constraint violation profiles for
the output forms. Learned weights for the standard constraints are -32.8 and -15.3; for M1 and M2, they
are -26.5 and -8.4. Black indicates violation, white no violation. Grey stripes indicate cells whose values
have negligible effects on the probability distribution.
5 Discussion and Future Work
5.1 Relation to phonotactic learning
Our primary finding from IBPOT is that it is possi-
ble to identify constraints that are both effective at
explaining the data and representative of theorized
phonologically-grounded constraints, given only
input-output mappings and faithfulness violations.
Furthermore, these constraints are successfully ac-
quired without any knowledge of the phonological
structure of the data beyond the faithfulness vio-
lation profiles. The model?s ability to infer con-
straint violation profiles without theoretical con-
straint structure provides an alternative solution to
the problems of the traditionally innate and univer-
sal OT constraint set.
As it jointly learns constraints and weights,
the IBPOT model calls to mind Hayes and
Wilson?s (2008) joint phonotactic learner. Their
learner also jointly learns weights and constraints,
but directly selects its constraints from a composi-
tional grammar of constraint definitions. This lim-
its their learner in practice by the rapid explosion
in the number of constraints as the maximum con-
straint definition size grows. By directly learning
violation profiles, the IBPOT model avoids this ex-
plosion, and the violation profiles can be automat-
ically parsed to identify the constraint definitions
that are consistent with the learned profile. The
inference method of the two models is different
as well; the phonotactic learner selects constraints
greedily, whereas the sampling on M in IBPOT
asymptotically approaches the posterior.
The two learners also address related but dif-
ferent phonological problems. The phonotactic
learner considers phonotactic problems, in which
only output matters. The constraints learned by
Hayes and Wilson?s learner are essentially OT
markedness constraints, but their learner does not
have to account for varied inputs or effects of faith-
fulness constraints.
5.2 Extending the learning model
IBPOT, as proposed here, learns constraints based
on binary violation profiles, defined extensionally.
A complete model of constraint acquisition should
provide intensional definitions that are phonolog-
ically grounded and cover potentially non-binary
constraints. We discuss how to extend the model
toward these goals.
IBPOT currently learns extensional constraints,
defined by which candidates do or do not violate
the constraint. Intensional definitions are needed
to extend constraints to unseen forms. Post hoc vi-
olation profile analysis, as in Sect. 4.3, provides
a first step toward this goal. Such analysis can
be integrated into the learning process using the
Rational Rules model (Goodman et al, 2008) to
identify likely constraint definitions composition-
ally. Alternately, phonological knowledge could
be integrated into a joint constraint learning pro-
cess in the form of a naturalness bias on the con-
straint weights or a phonologically-motivated re-
placement for the IBP prior.
The results presented here use binary con-
straints, where each candidate violates each con-
straint only once, a result of the IBP?s restriction
to binary matrices. Non-binarity can be handled
by using the binary matrix M to indicate whether
a candidate violates a constraint, with a second
1101
distribution determining the number of violations.
Alternately, a binary matrix can directly capture
non-binary constraints; Frank and Satta (1998)
converted existing non-binary constraints into a
binary OT system by representing non-binary con-
straints as a set of equally-weighted overlapping
constraints, each accounting for one violation. The
non-binary harmony constraint, for instance, be-
comes a set {*(at least one disharmony), *(at least
two disharmonies), etc.}.
Lastly, the Wolof vowel harmony problem pro-
vides a test case with overlaps in the candidate sets
for different inputs. This candidate overlap helps
the model find appropriate constraint structures.
Analyzing other phenomena may require the iden-
tification of appropriate abstractions to find this
same structural overlap. English regular plurals,
for instance, fall into broad categories depending
on the features of the stem-final phoneme. IBPOT
learning in such settings may require learning an
appropriate abstraction as well.
6 Conclusion
A central assumption of Optimality Theory has
been the existence of a fixed inventory of uni-
versal markedness constraints innately available to
the learner, an assumption by arguments regarding
the computational complexity of constraint iden-
tification. However, our results show for the first
time that nonparametric, data-driven learning can
identify sparse constraint inventories that both ac-
curately predict the data and are phonologically
meaningful, providing a serious alternative to the
strong nativist view of the OT constraint inventory.
Acknowledgments
We wish to thank Eric Bakovi?c, Emily Mor-
gan, Mark Mysl??n, the UCSD Computational Psy-
cholinguistics Lab, the Phon Company, and the re-
viewers for their discussions and feedback on this
work. This research was supported by NSF award
IIS-0830535 and an Alfred P. Sloan Foundation
Research Fellowship to RL.
References
Arto Anttila. 1997. Variation in Finnish phonology
and morphology. Ph.D. thesis, Stanford U.
Diana Archangeli and Douglas Pulleyblank. 1994.
Grounded phonology. MIT Press.
Paul Boersma. 1999. Empirical tests of the Gradual
Learning Algorithm. Linguistic Inquiry, 32:45?86.
Paul Boersma and Bruce Hayes. 2001. Optimality-
theoretic learning in the Praat program. In Proceed-
ings of the Institute of Phonetic Sciences of the Uni-
versity of Amsterdam.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19:380?393.
Robert Frank and Giorgio Satta. 1998. Optimality the-
ory and the generative complexity of constraint vio-
lability. Computational Linguistics, 24:307?315.
Sharon Goldwater and Mark Johnson. 2003. Learning
OT constraint rankings using a Maximum Entropy
model. In Proceedings of the Workshop on Variation
within Optimality Theory.
Noah Goodman, Joshua Tenebaum, Jacob Feldman,
and Tom Griffiths. 2008. A rational analysis of rule-
based concept learning. Cognitive Science, 32:108?
154.
Dilan G?or?ur, Frank J?akel, and Carl Rasmussen. 2006.
A choice model with infinitely many latent features.
In Proceedings of the 23rd International Conference
on Machine Learning.
Thomas Griffiths and Zoubin Ghahramani. 2005. Infi-
nite latent feature models and the Indian buffet pro-
cess. Technical Report 2005-001, Gatsby Computa-
tional Neuroscience Unit.
Bruce Hayes and Colin Wilson. 2008. A maximum en-
tropy model of phonotactics and phonotactic learn-
ing. Linguistic Inquiry, 39:379?440.
Bruce Hayes. 1999. Phonetically driven phonology:
the role of optimality theory and inductive ground-
ing. In Darnell et al editor, Formalism and Func-
tionalism in Linguistics, vol. 1. Benjamins.
Jeffrey Heinz, Gregory Kobele, and Jason Riggle.
2009. Evaluating the complexity of Optimality The-
ory. Linguistic Inquiry.
Brett Hyde. 2012. Alignment constraints. Natural
Language and Linguistic Theory, 30:789?836.
William Idsardi. 2006. A simple proof that Optimal-
ity Theory is computationally intractable. Linguistic
Inquiry, 37:271?275.
Frank Keller. 2000. Gradience in grammar: Ex-
perimental and computational aspects of degrees of
grammaticality. Ph.D. thesis, U. of Edinburgh.
John McCarthy. 2008. Doing Optimality Theory.
Blackwell.
Daniel Navarro and Tom Griffiths. 2007. A nonpara-
metric Bayesian method for inferring features from
similarity judgments. In Advances in Neural Infor-
mation Processing Systems 19.
1102
Alan Prince and Paul Smolensky. 1993. Optimality
theory: Constraint interaction in generative gram-
mar. Technical report, Rutgers Center for Cognitive
Science.
Jason Riggle. 2009. Generating contenders. Rutgers
Optimality Archive, 1044.
Jennifer Smith. 2004. Making constraints composi-
tional: toward a compositional model of Con. Lin-
gua, 114:1433?1464.
William Stokoe. 1960. Sign Language Structure. Lin-
stok Press.
Bruce Tesar and Paul Smolensky. 2000. Learnability
in Optimality Theory. MIT Press.
1103
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 21?30,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Why long words take longer to read:
the role of uncertainty about word length
Klinton Bicknell
Department of Psychology
University of California, San Diego
9500 Gilman Drive #109
La Jolla, CA 92093-0109
kbicknell@ucsd.edu
Roger Levy
Department of Linguistics
University of California, San Diego
9500 Gilman Drive #108
La Jolla, CA 92093-0108
rlevy@ucsd.edu
Abstract
Some of the most robust effects of linguis-
tic variables on eye movements in reading are
those of word length. Their leading explana-
tion states that they are caused by visual acu-
ity limitations on word recognition. However,
Bicknell (2011) presented data showing that a
model of eye movement control in reading that
includes visual acuity limitations and models
the process of word identification from visual
input (Bicknell & Levy, 2010) does not pro-
duce humanlike word length effects, provid-
ing evidence against the visual acuity account.
Here, we argue that uncertainty about word
length in early word identification can drive
word length effects. We present an extension
of Bicknell and Levy?s model that incorpo-
rates word length uncertainty, and show that it
produces more humanlike word length effects.
1 Introduction
Controlling the eyes while reading is a complex
task, and doing so efficiently requires rapid deci-
sions about when and where to move the eyes 3?
4 times per second. Research in psycholinguistics
has demonstrated that these decisions are sensitive
to a range of linguistic properties of the text be-
ing read, suggesting that the eye movement record
may be viewed as a detailed trace of the timecourse
of incremental comprehension. A number of cog-
nitive models of eye movement control in read-
ing have been proposed, the most well-known of
which are E-Z Reader (Reichle, Pollatsek, Fisher, &
Rayner, 1998; Reichle, Rayner, & Pollatsek, 2003)
and SWIFT (Engbert, Longtin, & Kliegl, 2002; En-
gbert, Nuthmann, Richter, & Kliegl, 2005). These
models capture a large range of the known proper-
ties of eye movements in reading, including effects
of the best-documented linguistic variables on eye
movements: the frequency, predictability, and length
of words.
Both models assume that word frequency, pre-
dictability, and length affect eye movements in read-
ing by affecting word recognition, yet neither one
models the process of identifying words from visual
information. Rather, each of these models directly
specifies the effects of these variables on exoge-
nous word processing functions, and the eye move-
ments the models produce are sensitive to these
functions? output. Thus, this approach cannot an-
swer the question of why these linguistic variables
have the effects they do on eye movement behav-
ior. Recently, Bicknell and Levy (2010) presented a
model of eye movement control in reading that di-
rectly models the process of identifying the text from
visual input, and makes eye movements to max-
imize the efficiency of the identification process.
Bicknell and Levy (2012) demonstrated that this
rational model produces effects of word frequency
and predictability that qualitatively match those of
humans: words that are less frequent and less pre-
dictable receive more and longer fixations. Because
this model makes eye movements to maximize the
efficiency of the identification process, this result
gives an answer for the reason why these variables
should have the effects that they do on eye move-
ment behavior: a model that works to efficiently
identify the text makes more and longer fixations on
21
words of lower frequency and predictability because
it needs more visual information to identify them.
Bicknell (2011) showed, however, that the ef-
fects of word length produced by the rational model
look quite different from those of human readers.
Because Bicknell and Levy?s (2010) model imple-
ments the main proposal for why word length effects
should arise, i.e., visual acuity limitations, the fact
that the model does not reproduce humanlike word
length effects suggests that our understanding of the
causes of word length effects may be incomplete.
In this paper, we argue that this result arose be-
cause of a simplifying assumption made in the ra-
tional model, namely, the assumption that the reader
has veridical knowledge about the number of char-
acters in a word being identified. We present an ex-
tension of Bicknell and Levy?s (2010) model which
does not make this simplifying assumption, and
show in two sets of simulations that effects of word
length produced by the extended model look more
like those of humans. We argue from these results
that uncertainty about word length is a necessary
component of a full understanding of word length
effects in reading.
2 Reasons for word length effects
The empirical effects of word length displayed by
human readers are simple to describe: longer words
receive more and longer fixations. The major rea-
son proposed in the literature on eye movements in
reading for this effect is that when fixating longer
words, the average visual acuity of all the letters in
the word will be lower than for shorter words, and
this poorer average acuity is taken to lead to longer
and more fixations. This intuition is built into the ex-
ogenous word processing functions in E-Z Reader
and SWIFT. Specifically, in both models, the word
processing rate slows as the average distance to the
fovea of all letters in the word increases, and this
specification of the effect of length on word process-
ing rates is enough to produce reasonable effects of
word length on eye movements: both models make
more and longer fixations on longer words ? similar
to the pattern of humans ? across a range of mea-
sures (Pollatsek, Reichle, & Rayner, 2006; Engbert
et al, 2005) including the duration of the first fixa-
tion on a word (first fixation duration), the duration
of all fixations on a word prior to leaving the word
(gaze duration), the rate at which a word is not fix-
ated prior to a fixation on a word beyond it (skip
rate), and the rate with which a word is fixated more
than once prior to a word beyond it (refixation rate).
There are, however, reasons to believe that this ac-
count may be incomplete. First, while it is the case
that the average visual acuity of all letters in a fixated
word must be lower for longer words, this is just be-
cause there are additional letters in the longer word.
While these additional letters pull down the aver-
age visual acuity of letters within the word, each ad-
ditional letter should still provide additional visual
information about the word?s identity, an argument
suggesting that longer words might require less ? not
more ? time to be identified. In fact, in SWIFT, the
exogenous word processing rate function slows as
both the average and the sum of the visual acuities of
the letters within the word decrease, but E-Z Reader
does not implement this idea in any way. Addi-
tionally, a factor absent from both E-Z Reader and
SWIFT, is that the visual neighborhoods of longer
words (at least in English) appear to be sparser,
when considering the number of words formed by
a single letter substitution (Balota, Cortese, Sergent-
Marshall, Spieler, & Yap, 2004), or the average or-
thographic Levenshtein distance of the most simi-
lar 20 words (Yarkoni, Balota, & Yap, 2008). Be-
cause reading words with more visual neighbors is
generally slower (Pollatsek, Perea, & Binder, 1999),
this argument gives another reason to expect longer
words to require less ? not more ? time to be read.
So while E-Z Reader and SWIFT produce rea-
sonable effects of word length on eye movement
measures (in which longer words receive more and
longer fixations) by assuming a particular effect of
visual acuity, it is less clear whether a visual acu-
ity account can yield reasonable word length effects
in a model that also includes the two opposing ef-
fects mentioned above. Determining how these dif-
ferent factors should interact to produce word length
effects requires a model of eye movements in read-
ing that models the process of word identification
from disambiguating visual input (Bicknell & Levy,
in press). The model presented by Bicknell and Levy
(2010) fits this description, and includes visual acu-
ity limitations (in fact, identical to the visual acuity
function in SWIFT). As already mentioned, how-
22
ever, Bicknell (2011) showed that the model did
not yield a humanlike length effect. Instead, while
longer words were skipped less often and refixated
more (as for humans), fixation durations generally
fell with word length ? the opposite of the pattern
shown by humans. This result suggests that visual
acuity limitations alone cannot explain the positive
effect of word length on fixation durations in the
presence of an opposing force such as the fact that
longer words have smaller visual neighborhoods.
We hypothesize that the reason for this pattern
of results relates to a simplifying assumption made
by Bicknell and Levy?s model. Specifically, while
visual input in the model yields noisy information
about the identities of letters, it gives veridical in-
formation about the number of letters in each word,
for reasons of computational convenience. There are
theoretical and empirical reasons to believe that this
simplifying assumption is incorrect, that early in the
word identification process human readers do have
substantial uncertainty about the number of letters
in a word, and further, that this may be especially
so for long words. For example, results with masked
priming have shown that recognition of a target word
is facilitated by a prime that is a proper subset of
the target?s letters (e.g., blcn?balcon; Peressotti &
Grainger, 1999; Grainger, Granier, Farioli, Van Ass-
che, & van Heuven, 2006), providing evidence that
words of different length have substantial similarity
in early processing. For these reasons, some recent
models of isolated word recognition (Gomez, Rat-
cliff, & Perea, 2008; Norris, Kinoshita, & van Cast-
eren, 2010) have suggested that readers have some
uncertainty about the number of letters in a word
early in processing.
If readers have uncertainty about the length of
words, we may expect that the amount of uncertainty
would grow proportionally to length, as uncertainty
is proportional to set size in other tasks of num-
ber estimation (Dehaene, 1997). This would agree
with the intuition that an 8-character word should
be more easily confused with a 9-character word
than a 3-character word with a 4-character word. In-
cluding uncertainty about word length that is larger
for longer words would have the effect of increas-
ing the number of visual neighbors for longer words
more than for shorter words, providing another rea-
son (in addition to visual acuity limitations) that
longer words may require more and longer fixations.
In the remainder of this paper, we describe an
extension of Bicknell and Levy?s (2010) model in
which visual input provides stochastic ? rather than
veridical ? information about the length of words,
yielding uncertainty about word length, and in which
the amount of uncertainty grows with length. We
then present two sets of simulations with this ex-
tended model demonstrating that it produces more
humanlike effects of word length, suggesting that
uncertainty about word length may be an important
component of a full understanding of the effects of
word length in reading.
3 A rational model of reading
In this section, we describe our extension of Bicknell
and Levy?s (2010) rational model of eye movement
control in reading. Except for the visual input sys-
tem, and a small change to the behavior policy to
allow for uncertainty about word length, the model
is identical to that described by Bicknell and Levy.
The reader is referred to that paper for further com-
putational details beyond what is described here.
In this model, the goal of reading is taken to be
efficient text identification. While it is clear that this
is not all that readers do ? inferring the underly-
ing structural relationships among words in a sen-
tence and discourse relationships between sentences
that determine text meaning is a fundamental part of
most reading ? all reader goals necessarily involve
identification of at least part of the text, so text iden-
tification is taken to be a reasonable first approxima-
tion. There are two sources of information relevant
to this goal: visual input and language knowledge,
which the model combines via Bayesian inference.
Specifically, it begins with a prior distribution over
possible identities of the text given by its language
model, and combines this with noisy visual input
about the text at the eyes? position, giving the likeli-
hood term, to form a posterior distribution over the
identity of the text taking into account both the lan-
guage model and the visual input obtained thus far.
On the basis of the posterior distribution, the model
decides whether or not to move its eyes (and if so
where to move them to) and the cycle repeats.
23
3.1 Formal problem of reading: Actions
The model assumes that on each of a series of
discrete timesteps, the model obtains visual input
around the current location of the eyes, and then
chooses between three actions: (a) continuing to fix-
ate the currently fixated position, (b) initiating a sac-
cade to a new position, or (c) stopping reading. If
the model chooses option (a), time simply advances,
and if it chooses option (c), then reading immedi-
ately ends. If a saccade is initiated (b), there is a lag
of two timesteps, representing the time required to
plan and execute a saccade, during which the model
again obtains visual input around the current posi-
tion, and then the eyes move toward the intended
target. Because of motor error, the actual landing po-
sition of the eyes is normally distributed around the
intended target with the standard deviation in char-
acters given by a linear function of the intended dis-
tance d (.87+ .084d; Engbert et al, 2005).1
3.2 Language knowledge
Following Bicknell and Levy (2010), we use very
simple probabilistic models of language knowledge:
word n-gram models (Jurafsky & Martin, 2009),
which encode the probability of each word condi-
tional on the n?1 previous words.
3.3 Formal model of visual input
Visual input in the model consists of noisy informa-
tion about the positions and identities of the charac-
ters in the text. Crucially, in this extended version
of the model, this includes noisy information about
the length of words. We begin with a visual acuity
function taken from Engbert et al (2005). This func-
tion decreases exponentially with retinal eccentric-
ity ? , and decreases asymmetrically, falling off more
slowly to the right than the left.2 The model obtains
visual input from the 19 character positions with the
highest acuity ? ? [?7,12], which we refer to as
the perceptual span. In order to provide the model
with information about the current fixation position
within the text, the model also obtains veridical in-
1In the terminology of the literature, the model has only ran-
dom motor error (variance), not systematic error (bias). Follow-
ing Engbert and Kr?gel (2010), systematic error may arise from
Bayesian estimation of the best saccade distance.
2While we refer to this function as visual acuity, it is clear
from its asymmetric nature that it has an attentional component.
formation about the number of word boundaries to
the left of the perceptual span.
Visual information from the perceptual span con-
sists of stochastic information about the number of
characters in the region and their identities. We make
the simplifying assumption that the only characters
are letters and spaces. Formally, visual input on a
given timestep is represented as a string of symbols,
each element of which has two features. One fea-
ture denotes whether the symbol represents a space
([+SPACE]) or a letter ([?SPACE]), an important dis-
tinction because spaces indicate word boundaries.
Symbols that are [+SPACE] veridically indicate the
occurrence of a space, while [?SPACE] symbols pro-
vide noisy information about the letter?s identity.
The other feature attached to each symbol speci-
fies whether the character in the text that the symbol
was emitted from was being fixated ([+FIX]) or not
([?FIX]). The centrally fixated character has special
status so that the model can recover the eyes? posi-
tion within the visual span.
This visual input string is generated by a pro-
cess of moving a marker from the beginning to the
end of the perceptual span, generally inserting a
symbol into the visual input string for each char-
acter it moves across (EMISSION). To provide only
noisy information about word length, however, this
process is not always one of EMISSION, but some-
times it inserts a symbol into the visual input string
that does not correspond to a character in the text
(INSERTION), and at other times it fails to insert
a symbol for a character in the text (SKIPPING).
Specifically, at each step of the process, a deci-
sion is first made about INSERTION, which occurs
with probability ? . If INSERTION occurs, then a
[?SPACE] identity for the character is chosen ac-
cording to a uniform distribution, and then noisy vi-
sual information about that character is generated in
the same way as for EMISSION (described below).
If a character is not inserted, and the marker has al-
ready moved past the last character in the perceptual
span, the process terminates. Otherwise, a decision
is made about whether to emit a symbol into the vi-
sual input string from the character at the marker?s
current position (EMISSION) or whether to skip out-
putting a symbol for that character (SKIPPING). In
either case, the marker is advanced to the next char-
acter position. If the character at the marker?s cur-
24
1 2 3
2
4
6
8
10
2
4
6
8
10
0.05
0.1
2 4 6 8 10 2 4 6 8 10 2 4 6 8 10Inferred word length (chars)
Act
ual
 wo
rd l
eng
th (c
hars
)
probability
0.2
0.4
0.6
0.8
Figure 1: The expectation for the posterior distribution
over the length of a word for actual lengths 1?10 after the
model has received 1, 2, or 3 timesteps of visual input
about the word, for two levels of length uncertainty: ? ?
{.05, .1}. These calculations use as a prior distribution
the empirical distribution of word length in the BNC and
assume no information about letter identity.
rent position is [+SPACE] or [+FIX], then EMISSION
is always chosen, but if it is any other character, then
SKIPPING occurs with probability ? .
A [?SPACE] symbol (produced through EMIS-
SION or INSERTION) contains noisy information
about the identity of the letter that generated it, ob-
tained via sampling. Specifically, we represent each
letter as a 26-dimensional vector, where a single el-
ement is 1 and the others are zeros. Given this rep-
resentation, a [?SPACE] symbol contains a sample
from a 26-dimensional Gaussian with a mean equal
to the letter?s true identity and a diagonal covariance
matrix ?(?) = ? (?)?1I, where ? (?) is the visual
acuity at eccentricity ? . We scale the overall process-
ing rate by multiplying each rate by ?, set to 8 for
the simulations reported here.
Allowing for INSERTION and SKIPPING means
that visual input yields noisy information about the
length of words, and this noise is such that uncer-
tainty is higher for longer words. Figure 1 gives a
visualization of this uncertainty. It shows the expec-
tation for the posterior distribution over the length
of a word for a range of actual word lengths, after
the model has received 1, 2, or 3 timesteps of visual
input about the word, at two levels of uncertainty.
This figure demonstrates two things: first, that there
is substantial uncertainty about word length even af-
ter three timesteps of visual input, and second, that
this uncertainty is larger for longer words.
(a) m = [.6, .7, .6, .4, .3, .6]: Keep fixating (3)
(b) m = [.6, .4, .9, .4, .3, .6]: Move back (to 2)
(c) m = [.6, .7, .9, .4, .3, .6]: Move forward (to 6)
(d) m = [.6, .7, .9, .8, .7, .7]: Stop reading
Figure 2: Values of m for a 6 character text under which
a model fixating position 3 would take each of its four
actions, if ? = .7 and ? = .5.
3.4 Inference about text identity
The model?s initial beliefs about the identity of
the text are given by the probability of each pos-
sible identity under the language model. On each
timestep, the model obtains a visual input string as
described above and calculates the likelihood of gen-
erating that string from each possible identity of the
text. The model then updates its beliefs about the
text via standard Bayesian inference: multiplying the
probability of each text identity under its prior be-
liefs by the likelihood of generating the visual input
string from that text identity and normalizing. We
compactly represent all of these distributions using
weighted finite-state transducers (Mohri, 1997) us-
ing the OpenFST library (Allauzen, Riley, Schalk-
wyk, Skut, & Mohri, 2007), and implement be-
lief update with transducer composition and weight
pushing.
3.5 Behavior policy
The model uses a simple policy with two parame-
ters, ? and ? , to decide between actions based on
the marginal probability m of the most likely char-
acter c in each position j,
m( j) = max
c
p(w j = c)
where w j indicates the character in the jth posi-
tion. A high value of m indicates relative confidence
about the character?s identity, and a low value rel-
ative uncertainty. Because our extension has uncer-
tainty about the absolute position of its eyes within
the text, each position j is now defined relative to the
centrally fixated character.
Figure 2 illustrates how the model decides among
four possible actions. If the value of m( j) for the cur-
rent position of the eyes is less than the parameter
? , the model continues fixating the current position
(2a). Otherwise, if the value of m( j) is less than the
25
parameter ? for some leftward position, the model
initiates a saccade to the closest such position (2b).
If no such positions exist to the left, the model initi-
ates a saccade to n characters past the closest posi-
tion to the right for which m( j) < ? (2c).3 Finally,
if no such positions exist, the model stops reading
(2d). Intuitively, then, the model reads by making a
rightward sweep to bring its confidence in each char-
acter up to ? , but pauses to move left to reread any
character whose confidence falls below ? .
4 Simulation 1: full model
We now assess the effects of word length produced
by the extended version of the model. Following
Bicknell (2011), we use the model to simulate read-
ing of a modified version of the Schilling, Rayner,
and Chumbley (1998) corpus of typical sentences
used in reading experiments. We compare three lev-
els of length uncertainty: ? ? {0, .05, .1}. The first
of these (? = 0) corresponds to Bicknell and Levy?s
(2010) model, which has no uncertainty about word
length. We predict that increasing the amount of
length uncertainty will make effects of word length
more like those of humans, and we compare the
model?s length effects to those of human readers of
the Schilling corpus.
4.1 Methods
4.1.1 Model parameters and language model
Following Bicknell (2011), the model?s language
knowledge was an unsmoothed bigram model using
a vocabulary set consisting of the 500 most frequent
words in the British National Corpus (BNC) as well
as all the words in the test corpus. Every bigram in
the BNC was counted for which both words were in
vocabulary, and ? due to the intense computation re-
quired for exact inference ? this set was trimmed by
removing rare bigrams that occur less than 200 times
(except for bigrams that occur in the test corpus), re-
sulting in a set of about 19,000 bigrams, from which
the bigram model was constructed.
4.1.2 Optimization of policy parameters
We set the parameters of the behavior policy
(?,? ) to values that maximize reading efficiency.
3The role of n is to ensure that the model does not center
its visual field on the first uncertain character. For the present
simulations, we did not optimize this parameter, but fixed n = 3.
We define reading efficiency E to be an interpolation
of speed and accuracy, E =(1??)L??T , where L is
the log probability of the true identity of the text un-
der the model?s beliefs at the end of reading, T is the
number of timesteps before the model stopped read-
ing, and ? gives the relative value of speed. For the
present simulations, we use ? = .1, which produces
reasonably accurate reading. To find optimal values
of the policy parameters ? and ? for each model, we
use the PEGASUS method (Ng & Jordan, 2000) to
transform this stochastic optimization problem into
a deterministic one amenable to standard optimiza-
tion algorithms, and then use coordinate ascent.
4.1.3 Test corpus
We test the model on a corpus of 33 sentences
from the Schilling corpus slightly modified by
Bicknell and Levy (2010) so that every bigram oc-
curred in the BNC, ensuring that the results do not
depend on smoothing.
4.1.4 Analysis
With each model, we performed 50 stochastic
simulations of the reading of the corpus. For each
run, we calculated the four standard eye movement
measures mentioned above for each word in the cor-
pus: first fixation duration, gaze duration, skipping
probability, and refixation probability. We then av-
eraged each of these four measures across runs for
each word token in the corpus, yielding a single
mean value for each measure for each word.
Comparing the fixation duration measures to hu-
mans required converting the model?s timesteps into
milliseconds. We performed this scaling by multi-
plying the duration of each fixation by a conversion
factor set to be equal to the mean human gaze du-
ration divided by the mean model gaze duration for
words with frequencies higher than 1 in 100, mean-
ing that the model predictions exactly match the hu-
man mean for gaze durations on these words.
4.2 Results
Figure 3 presents the results for all four measures
of interest. Looking first at the model with no un-
certainty, we see that the results replicate those of
Bicknell (2011): while there is a monotonic effect
of word length on skip rates and refixation rates in
the same direction as humans, longer words receive
26
180
200
220
240
260
280
300
2 4 6 8 10Word length (chars)
Firs
t fix
atio
n d
ura
tion
 (ms
)
200
250
300
350
2 4 6 8 10Word length (chars)
Gaz
e d
ura
tion
 (ms
)
0.0
0.2
0.4
0.6
0.8
2 4 6 8 10Word length (chars)
Ski
p ra
te
0.0
0.1
0.2
0.3
0.4
2 4 6 8 10Word length (chars)
Ref
ixat
ion
 rat
e
Figure 3: Effects of word length in three version of the
full model with ? = 0 (red), ? = 0.05 (green), and ? =
0.1 (blue) on first fixation durations, gaze durations, skip
rates, and refixation rates compared with the empirical
human data for this corpus (purple). Estimates obtained
via loess smoothing and plotted with standard errors.
shorter fixations in the model, opposite to the pattern
found in human data. As predicted, adding length
uncertainty begins to reverse this effect: as uncer-
tainty is increased, the effect of word length on fixa-
tion durations becomes less negative.
However, while these results look more like those
of humans, there are still substantial differences. For
one, even for the model with the most uncertainty,
the effect of word length ? while not negative ? is
also not really positive. Second, the effect appears
rather non-monotonic. We hypothesize that these
two problems are related to the aggressive trimming
we performed of the model?s language model. By re-
moving low frequency words and bigrams, we artifi-
cially trimmed especially the visual neighborhoods
of long words, since frequency and length are nega-
tively correlated. This could have led to another in-
verse word length effect, which even adding more
length uncertainty was unable to fully overcome. In
effect, extending the visual neighborhoods of long
words (by adding length uncertainty) may not have
much effect if we have removed all the words that
would be in those extended neighborhoods. In ad-
dition, the aggressive trimming could have been re-
sponsible for the non-monotonicities apparent in the
model?s predictions. We performed another set of
simulations using a language model with substan-
tially less trimming to test these hypotheses.
5 Simulation 2: model without context
In this simulation, we used a unigram language
model instead of the bigram language model used
in Simulation 1. Since this model cannot make use
of linguistic context, it will not show as robust ef-
fects of linguistic variables such as word predictabil-
ity (Bicknell & Levy, 2012), but since here our fo-
cus is on effects of word length, this limitation is
unlikely to concern us. Crucially, because of the
model?s simpler structure, it allows for the use of
a substantially larger vocabulary than the bigram
model used in Simulation 1. In addition, using this
model avoids the problems mentioned above associ-
ated with trimming bigrams. We predicted that this
language model would allow us to obtain effects of
word length on fixation durations that were actu-
ally positive (rather than merely non-negative), and
that there would be fewer non-monotonicities in the
function.
5.1 Methods
Except the following, the methods were identical
to those of Simulation 1. We replaced the bigram
language model with a unigram language model.
Training was performed in the same manner, except
that instead of including only the most common 500
words in the BNC, we included all words that occur
at least 200 times (corresponding to a frequency of
2 per million; about 19,000 words). Because of the
greater computational complexity for the two mod-
els with non-zero ? , we performed only 20 simula-
tions of the reading of the corpus instead of 50.
5.2 Results
Figure 4 presents the results for all four measures
of interest. Looking at the model with no uncer-
tainty, we see already that the predictions are a sub-
stantially better fit to human data than was the full
model. The skipping and refixation rates look sub-
stantially more like the human curves. And while
the word length effect on first fixation duration is
still negative, it is already non-negative for gaze du-
ration. This supports our hypotheses that aggres-
sive trimming were partly responsible for the full
model?s negative word length effect.
27
180
200
220
240
260
280
300
2 4 6 8 10Word length (chars)
Firs
t fix
atio
n d
ura
tion
 (ms
)
200
250
300
350
2 4 6 8 10Word length (chars)
Gaz
e d
ura
tion
 (ms
)
0.0
0.2
0.4
0.6
0.8
2 4 6 8 10Word length (chars)
Ski
p ra
te
0.0
0.1
0.2
0.3
0.4
2 4 6 8 10Word length (chars)
Ref
ixat
ion
 rat
e
Figure 4: Effects of word length in three version of the
model without context (unigram model) with ? = 0 (red),
? = 0.05 (green), and ? = 0.1 (blue) on first fixation dura-
tions, gaze durations, skip rates, and refixation rates com-
pared with the empirical human data for this corpus (pur-
ple). Estimates obtained via loess smoothing and plotted
with standard errors.
Moving on to the models with uncertainty, we see
that predictions are still in good agreement with hu-
mans for skip rates and refixation rates. More in-
terestingly, we see that adding length uncertainty
makes both durations measures relatively positive
functions of word length. While the overall size of
the effect is incorrect for first fixation durations, we
see striking similarities between the models predic-
tions and human data on both duration measures.
For first fixations, the human pattern is that dura-
tions go up from word lengths 1 to 2, down from 2
to 3 (presumably because of ?the?), and then up to 5,
after which the function is relatively flat. That pat-
tern also holds for both models with uncertainty. For
gaze duration, both models more or less reproduce
the human pattern of a steadily-increasing function
throughout the range, and again match the human
function in dipping for word length 3. For gaze du-
rations, even the overall size of the effect produced
by the model is similar to that of humans. These
results confirm our original hypothesis that adding
length uncertainty would lead to more humanlike
word length effects. In addition, comparing the re-
sults of Simulation 2 with Simulation 1 reveals the
importance to this account of words having realis-
tic visual neighborhoods. When the visual neighbor-
hoods of (especially longer) words were trimmed to
be artificially sparse, adding length uncertainty did
not allow the model to recover the human pattern.
6 Conclusion
In this paper, we argued that the success of major
models of eye movements in reading to reproduce
the (positive) human effect of word length via acuity
limitations may be a result of not including oppos-
ing factors such as the negative correlation between
visual neighborhood size and word length. We de-
scribed the failure of the rational model presented
in Bicknell and Levy (2010) to obtain humanlike ef-
fects of word length, despite including all of these
factors, suggesting that our understanding of word
length effects in reading is incomplete. We proposed
a new reason for word length effects ? uncertainty
about word length that is larger for longer words ?
and noted that this reason was not implemented in
Bicknell and Levy?s model because of a simplifying
assumption. We presented an extension of the model
relaxing this assumption, in which readers obtain
noisy information about word length, and showed
through two sets of simulations that the new model
produces effects of word length that look more like
those of human readers. Interestingly, while adding
length uncertainty made both models more human-
like, it was only in Simulation 2 ? in which words
had more realistic visual neighborhoods ? that all
measures of the effect of word length on eye move-
ments showed the human pattern, underscoring the
importance of the structure of the language for this
account of word length effects.
We take these results as evidence that word length
effects cannot be completely explained through lim-
itations on visual acuity. Rather, they suggest that a
full understanding of the reasons underlying word
length effects on eye movements in reading should
include a notion of uncertainty about the number of
letters in a word, which grows with word length.
Acknowledgments
This research was supported by NIH grant T32-
DC000041 from the Center for Research in Lan-
guage at UC San Diego to K. B. and by NSF grant
0953870 and NIH grant R01-HD065829 to R. L.
28
References
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W.,
& Mohri, M. (2007). OpenFst: A general
and efficient weighted finite-state transducer
library. In Proceedings of the Ninth Inter-
national Conference on Implementation and
Application of Automata, (CIAA 2007) (Vol.
4783, p. 11-23). Springer.
Balota, D. A., Cortese, M. J., Sergent-Marshall,
S. D., Spieler, D. H., & Yap, M. J. (2004).
Visual word recognition of single-syllable
words. Journal of Experimental Psychology:
General, 133, 283?316.
Bicknell, K. (2011). Eye movements in read-
ing as rational behavior. Unpublished doc-
toral dissertation, University of California,
San Diego.
Bicknell, K., & Levy, R. (2010). A rational model of
eye movement control in reading. In Proceed-
ings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL)
(pp. 1168?1178). Uppsala, Sweden: Associa-
tion for Computational Linguistics.
Bicknell, K., & Levy, R. (2012). Word predictabil-
ity and frequency effects in a rational model
of reading. In N. Miyake, D. Peebles, &
R. P. Cooper (Eds.), Proceedings of the 34th
Annual Conference of the Cognitive Science
Society. Austin, TX: Cognitive Science Soci-
ety.
Bicknell, K., & Levy, R. (in press). The utility of
modelling word identification from visual in-
put within models of eye movements in read-
ing. Visual Cognition.
Dehaene, S. (1997). The number sense: How the
mind creates mathematics. New York: Oxford
University Press.
Engbert, R., & Kr?gel, A. (2010). Readers use
Bayesian estimation for eye movement con-
trol. Psychological Science, 21, 366?371.
Engbert, R., Longtin, A., & Kliegl, R. (2002). A dy-
namical model of saccade generation in read-
ing based on spatially distributed lexical pro-
cessing. Vision Research, 42, 621?636.
Engbert, R., Nuthmann, A., Richter, E. M., & Kliegl,
R. (2005). SWIFT: A dynamical model of
saccade generation during reading. Psycho-
logical Review, 112, 777?813.
Gomez, P., Ratcliff, R., & Perea, M. (2008). The
Overlap model: A model of letter position
coding. Psychological Review, 115, 577?601.
Grainger, J., Granier, J.-P., Farioli, F., Van Assche,
E., & van Heuven, W. J. B. (2006). Letter
position information and printed word percep-
tion: The relative-position priming constraint.
Journal of Experimental Psychology: Human
Perception and Performance, 32, 865?884.
Jurafsky, D., & Martin, J. H. (2009). Speech and lan-
guage processing: An introduction to natural
language processing, computational linguis-
tics, and speech recognition (2nd ed.). Upper
Saddle River, NJ: Prentice Hall.
Mohri, M. (1997). Finite-state transducers in lan-
guage and speech processing. Computational
Linguistics, 23, 269?311.
Ng, A. Y., & Jordan, M. (2000). PEGASUS:
A policy search method for large MDPs and
POMDPs. In Uncertainty in Artificial Intel-
ligence, Proceedings of the Sixteenth Confer-
ence (pp. 406?415).
Norris, D., Kinoshita, S., & van Casteren, M. (2010).
A stimulus sampling theory of letter identity
and order. Journal of Memory and Language,
62, 254?271.
Peressotti, F., & Grainger, J. (1999). The role of let-
ter identity and letter position in orthographic
priming. Perception & Psychophysics, 61,
691?706.
Pollatsek, A., Perea, M., & Binder, K. S. (1999).
The effects of ?neighborhood size? in reading
and lexical decision. Journal of Experimen-
tal Psychology: Human Perception and Per-
formance, 25, 1142?1158.
Pollatsek, A., Reichle, E. D., & Rayner, K. (2006).
Tests of the E-Z Reader model: Explor-
ing the interface between cognition and eye-
movement control. Cognitive Psychology, 52,
1?56.
Reichle, E. D., Pollatsek, A., Fisher, D. L., &
Rayner, K. (1998). Toward a model of eye
movement control in reading. Psychological
Review, 105, 125?157.
Reichle, E. D., Rayner, K., & Pollatsek, A. (2003).
The E-Z Reader model of eye-movement con-
trol in reading: Comparisons to other models.
29
Behavioral and Brain Sciences, 26, 445?526.
Schilling, H. E. H., Rayner, K., & Chumbley, J. I.
(1998). Comparing naming, lexical decision,
and eye fixation times: Word frequency effects
and individual differences. Memory & Cogni-
tion, 26, 1270?1281.
Yarkoni, T., Balota, D. A., & Yap, M. J. (2008).
Moving beyond Coltheart?s N: A new mea-
sure of orthographic similarity. Psychonomic
Bulletin & Review, 15, 971?979.
30
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 11?20,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A model of generalization in distributional learning of phonetic categories
Bozena Pajak
Brain & Cognitive Sciences
University of Rochester
Rochester, NY 14627-0268
bpajak@bcs.rochester.edu
Klinton Bicknell
Psychology
UC San Diego
La Jolla, CA 92093-0109
kbicknell@ucsd.edu
Roger Levy
Linguistics
UC San Diego
La Jolla, CA 92093-0108
rlevy@ucsd.edu
Abstract
Computational work in the past decade
has produced several models accounting
for phonetic category learning from distri-
butional and lexical cues. However, there
have been no computational proposals for
how people might use another powerful
learning mechanism: generalization from
learned to analogous distinctions (e.g.,
from /b/?/p/ to /g/?/k/). Here, we present
a new simple model of generalization in
phonetic category learning, formalized in
a hierarchical Bayesian framework. The
model captures our proposal that linguis-
tic knowledge includes the possibility that
category types in a language (such as
voiced and voiceless) can be shared across
sound classes (such as labial and velar),
thus naturally leading to generalization.
We present two sets of simulations that
reproduce key features of human perfor-
mance in behavioral experiments, and we
discuss the model?s implications and di-
rections for future research.
1 Introduction
One of the central problems in language acqui-
sition is how phonetic categories are learned, an
unsupervised learning problem involving mapping
phonetic tokens that vary along continuous di-
mensions onto discrete categories. This task may
be facilitated by languages? extensive re-use of a
set of phonetic dimensions (Clements 2003), be-
cause learning one distinction (e.g., /b/?/p/ vary-
ing along the voice onset time (VOT) dimension)
might help learn analogous distinctions (e.g., /d/?
/t/, /g/?k/). Existing experimental evidence sup-
ports this view: both infants and adults general-
ize newly learned phonetic category distinctions to
untrained sounds along the same dimension (Mc-
Claskey et al 1983, Maye et al 2008, Perfors
& Dunbar 2010, Pajak & Levy 2011a). However,
while many models have been proposed to account
for learning of phonetic categories (de Boer &
Kuhl 2003, Vallabha et al 2007, McMurray et al
2009, Feldman et al 2009, Toscano & McMur-
ray 2010, Dillon et al 2013), there have been no
computational proposals for how generalization
to analogous distinctions may be accomplished.
Here, we present a new simple model of gener-
alization in phonetic category learning, formal-
ized in a hierarchical Bayesian framework. The
model captures our proposal that linguistic knowl-
edge includes the possibility that category types
in a language (such as voiced and voiceless) can
be shared across sound classes (defined as previ-
ously learned category groupings, such as vowels,
consonants, nasals, fricatives, etc.), thus naturally
leading to generalization.
One difficulty for the view that learning one dis-
tinction might help learn analogous distinctions is
that there is variability in how the same distinc-
tion type is implemented phonetically for differ-
ent sound classes. For example, VOT values are
consistently lower for labials (/b/?/p/) than for ve-
lars (/g/?/k/) (Lisker & Abramson 1970), and the
durations of singleton and geminate consonants
are shorter for nasals (such as /n/?/nn/) than for
voiceless fricatives (such as /s/?/ss/) (Giovanardi
& Di Benedetto 1998, Mattei & Di Benedetto
2000). Improving on our basic model, we imple-
ment a modification that deals with this difficulty
by explicitly building in the possibility for analo-
gous categories along the same dimension to have
different absolute phonetic values along that di-
mension (e.g., shorter overall durations for nasals
than for fricatives).
In Section 2 we discuss the relevant background
on phonetic category learning, including previ-
ous modeling work. Section 3 describes our ba-
sic computational model, and Section 4 presents
simulations demonstrating that the model can re-
11
produce the qualitative patterns shown by adult
learners in cases when there is no phonetic vari-
ability between sound classes. In Section 5 we
describe the extended model that accommodates
phonetic variability across sound classes, and in
Section 6 we show that the improved model qual-
itatively matches adult learner performance both
when the sound classes implement analogous dis-
tinction types in identical ways, and when they dif-
fer in the exact phonetic implementation. Section 7
concludes with discussion of future research.
2 Background
One important source of information for unsuper-
vised learning of phonetic categories is the shape
of the distribution of acoustic-phonetic cues. For
example, under the assumption that each phonetic
category has a unimodal distribution on a particu-
lar cue, the number of modes in the distribution
of phonetic cues can provide information about
the number of categories: a unimodal distribution
along some continuous acoustic dimension, such
as VOT, may indicate a single category (e.g., /p/,
as in Hawaiian); a bimodal distribution may sug-
gest a two-category distinction (e.g., /b/ vs. /p/, as
in English); and a trimodal distribution implies a
three-category distinction (e.g., /b/, /p/, and /ph/,
as in Thai). Infants extract this distributional infor-
mation from the speech signal (Maye et al 2002,
2008) and form category representations focused
around the modal values of categories (Kuhl 1991,
Kuhl et al 1992, Lacerda 1995). Furthermore, in-
formation about some categories bootstraps learn-
ing of others: infants exposed to a novel bimodal
distribution along the VOT dimension for one
place of articulation (e.g., alveolar) not only learn
that novel distinction, but also generalize it to an
analogous contrast for another (e.g., velar) place
of articulation (Maye et al 2008). This ability is
preserved beyond infancy, and is potentially used
during second language learning, as adults are also
able to both learn from distributional cues and use
this information when making category judgments
about untrained sounds along the same dimensions
(Maye & Gerken 2000, 2001, Perfors & Dunbar
2010, Pajak & Levy 2011a,b).
The phonetic variability in how different sound
classes implement the same distinction type might
in principle hinder generalization across classes.
However, there is evidence of generalization even
in cases when sound classes differ in the exact
phonetic implementation of a shared distinction
type. For example, learning a singleton/geminate
length contrast for the class of voiceless fricatives
(e.g., /s/?/ss/, /f/?/ff/) generalizes to the class of
sonorants (e.g., /n/?/nn/, /j/?/jj/) even when the ab-
solute durations of sounds in the two classes are
different ? overall longer for fricatives than for
sonorants (Pajak & Levy 2011a) ? indicating that
learners are able to accomodate the variability of
phonetic cues across different sound classes.
Phonetic categorization from distributional cues
has been modeled using Gaussian mixture mod-
els, where each category is represented as a Gaus-
sian distribution with a mean and covariance ma-
trix, and category learning involves estimating
the parameters of each mixture component and
? for some models ? the number of components
(de Boer & Kuhl 2003, Vallabha et al 2007, Mc-
Murray et al 2009, Feldman et al 2009, Toscano
& McMurray 2010, Dillon et al 2013).1 These
models are successful at accounting for distribu-
tional learning, but do not model generalization.
We build on this previous work (specifically, the
model in Feldman et al 2009) and implement gen-
eralization of phonetic distinctions across different
sound classes.
3 Basic generalization model
The main question we are addressing here con-
cerns the mechanisms underlying generalization.
How do learners make use of information about
some phonetic categories when learning other
categories? Our proposal is that learners expect
category types (such as singleton and geminate,
or voiced and voiceless) to be shared among
sound classes (such as sonorants and fricatives).
We implement this proposal with a hierarchical
Dirichlet process (Teh et al 2006), which allows
for sharing categories across data groups (here,
sound classes). We build on previous computa-
tional work in this area that models phonetic cate-
gories as Gaussian distributions. Furthermore, we
follow Feldman et al (2009) in using Dirichlet
processes (Ferguson 1973), which allow the model
to learn the number of categories from the data,
and implementing the process of learning from
distributional cues via nonparametric Bayesian in-
ference.
1In Dillon et al (2013) each phoneme is modeled as a
mixture of Gaussians, where each component is an allophone.
12
HG0?
Gc?0
zic
dic i ? {1..nc}
c ? C
Figure 1: The graphical representation of the basic
model.
H : ? ? N (?0, ?
2
?0 )
?2 ? InvChiSq(?0,?20 )
G0 ? DP(?,H)
Gc ? DP(?0,G0)
zic ? Gc
dic ? N (?zic ,?2zic)
fc ? N (0,?2f )
dic ? N (?zic ,?2zic)+ fc
Figure 2: Mathematical description of the model.
The variables below the dotted line refer to the ex-
tended model in Figure 6.
3.1 Model details
As a first approach, we consider a simplified sce-
nario of a language with a set of sound classes,
each of which contains an unknown number of
phonetic categories, with perceptual token defined
as a value along a single phonetic dimension.
The model learns the set of phonetic categories
in each sound class, and the number of categories
inferred for one class can inform the inferences
about the other class. Here, we make the simpli-
fying assumption that learners acquire a context-
independent distribution over sounds, although the
model could be extended to use linguistic con-
text (such as coarticulatory or lexical information;
Feldman et al 2009).
Figure 1 provides the graphical representation
of the model, and Figure 2 gives its mathematical
Variable Explanation
H
base distribution over means and
variances of categories
G0
distribution over possible
categories
Gc
distribution over categories in
class c
?,?0 concentration parameters
zic category for datapoint dic
dic datapoint (perceptual token)
nc number of datapoints in class c
C set of classes
fc offset parameter
? f standard deviation of prior on fc
Table 1: Key for the variables in Figures 1, 2,
and 6. The variables below the dotted line refer
to the extended model in Figure 6.
description. Table 1 provides the key to the model
variables. In the model, speech sounds are pro-
duced by selecting a phonetic category zic, which
is defined as a mean ?zic and variance ?2zic along
a single phonetic dimension,2 and then sampling
a phonetic value from a Gaussian with that mean
and variance. We assume a weak prior over cat-
egories that does not reflect learners? prior lan-
guage knowledge (but we return to the possible
role of prior language knowledge in the discus-
sion). Learners? beliefs about the sound inventory
(distribution over categories and mean and vari-
ance of each category) are encoded through a hier-
archical Dirichlet process. Each category is sam-
pled from the distribution Gc, which is the distri-
bution over categories in a single sound class. In
order to allow sharing of categories across classes,
the Gc distribution for each class is sampled from a
Dirichlet process with base distribution G0, which
is shared across classes, and concentration param-
eter ?0 (which determines the sparsity of the dis-
tribution over categories). G0, then, stores the full
set of categories realized in any class, and it is
sampled from a Dirichlet process with concentra-
tion parameter ? and base distribution H, which
is a normal inverse chi-squared prior on category
2Although we are modeling phonetic categories as having
values along a single dimension, the model can be straight-
forwardly extended to multiple dimensions, in which case the
variance would be replaced by a covariance matrix ?zic .
13
means and variances.3 The parameters of the nor-
mal inverse chi-squared distribution are: ?0 and ?0,
which can be thought of as pseudo-observations,
as well as ?0 and ?20 , which determine the prior
distribution over means and variances, as in Fig-
ure 2.
3.2 Inference
The model takes as input the parameters of the
base distribution H, the concentration parameters
?0 and ? , and the data, which is composed of a
list of phonetic values. The model infers a poste-
rior distribution over category labels for each data-
point via Gibbs sampling. Each iteration of Gibbs
sampling resamples the assignments of each data-
point to a lower-level category (in Gc) and also re-
samples the assignments of lower-level categories
to higher-level categories (in G0). We marginalize
over the category means and variances.
4 Simulations: basic model
The first set of simulations has three goals: first,
to establish that our model can successfully per-
form distributional learning and second, to show
that it can use information about one type of class
to influence judgements about another, in the case
that there is no variability in category structure
between classes. Finally, these simulations reveal
a limitation of this basic model, showing that it
cannot generalize in the presence of substantial
between-class variability in category realizations.
We address this limitation in Section 5.
4.1 The data
The data we use to evaluate the model come
from the behavioral experiments in Pajak & Levy
(2011a). Adult native English speakers were ex-
posed to novel words, where the middle conso-
nant varied along the length dimension from short
(e.g., [ama]) to long (e.g., [amma]). The distri-
butional information suggested either one cate-
gory along the length dimension (unimodal distri-
bution) or two categories (bimodal distribution),
as illustrated in Figure 3. In Experiment 1, the
training included sounds in the sonorant class (4
continua: [n]-...-[nn], [m]-...-[mm], [j]-...-[jj], [l]-
...-[ll]) with the duration range of 100?205msec.
In Experiment 2 the training included sounds in
3In the case of categories defined along multiple di-
mensions, the base distribution would be a normal inverse-
Wishart.
l l
l l
0
4
8
12
16
0
4
8
12
16
Expt1:sonorants
Expt2:fricatives
100 120 140 160 180 200 220 240 260 280
Stimuli length continuum (in msec)
Fa
m
ilia
riz
ati
on
 fre
qu
en
cy
bimodal unimodal
Figure 3: Experiment 1 & 2 training (Pajak and
Levy 2011a). The y axis reflects the frequency
of tokens from each training continuum. The four
points indicate the values of the untrained data-
points.
the voiceless fricative class (4 continua: [s]-...-
[ss], [f]-...-[ff], [T]-...-[TT], [S]-...-[SS]) with the du-
ration range of 140?280msec. The difference in
duration ranges between the two classes reflected
the natural duration distributions of sounds in
these classes: generally shorter for sonorants and
longer for fricatives (Greenberg 1996, Giovanardi
& Di Benedetto 1998, Mattei & Di Benedetto
2000).
Subsequently, participants? expectations about
the number of categories in the trained class and
another untrained class were probed by asking for
judgments about tokens at the endpoints of the
continua: participants were presented with pairs
of words (e.g., sonorant [ama]?[amma] or frica-
tive [asa]?[assa]) and asked whether these were
two different words in this language or two rep-
etitions of the same word. As illustrated in Ta-
ble 2, in the test phase of Experiment 1 the du-
rations of both the trained and the untrained class
were identical (100msec for any short consonant
and 205msec for any long consonant), whereas
in the test phase of Experiment 2 the durations
were class-specific: longer for trained fricatives
(140msec for a short fricative and 280msec for a
long fricative) and shorter for untrained sonorants
(100msec for a short sonorant and 205msec for a
long sonorant).
The experiment results are illustrated in Fig-
ure 4. The data from the ?trained? condition shows
that learners were able to infer the number of cat-
egories from distributional cues: they were more
14
Expt1?trained Expt1?untrained
Pro
po
rtio
n o
f 'd
iffe
re
nt'
 re
sp
on
se
s
0.0
0.2
0.4
0.6
0.8
1.0
Bimodal
Unimodal
Expt2?trained Expt2?untrained
Pro
po
rtio
n o
f 'd
iffe
re
nt'
 re
sp
on
se
s
0.0
0.2
0.4
0.6
0.8
1.0
Bimodal
Unimodal
Figure 4: Experiment 1 & 2 results: proportion of ?different? responses on ?different? trials (Pajak and
Levy, 2011a).
Expt. 1 Expt. 2
trained
(sonorants) (fricatives)
100ms ? 205ms 140ms ? 280ms
untrained
(fricatives) (sonorants)
100ms ? 205ms 100ms ? 205ms
Table 2: Experiment 1 & 2 test (Pajak and Levy,
2011a).
likely to posit two categories (i.e., respond ?dif-
ferent? on ?different? trials) when the distribution
was bimodal than when the distribution was uni-
modal. In addition, as demonstrated by the ?un-
trained? condition, learners used the information
about the trained class to make inferences about
the untrained class: they were more likely to ac-
cept length-based category distinctions for frica-
tives after learning the distinction for sonorants
(Expt. 1), and vice versa (Expt. 2). This general-
ization occurred both (a) when each class imple-
mented the distinction in exactly the same way
(with the same absolute durations; Expt. 1), and
(b) when the classes differed in how the shared dis-
tinction type was implemented (the absolute dura-
tions of the untrained class were shifted relative to
the trained class; Expt. 2).
The model simulations described below at-
tempt to replicate the key features of human per-
formance: distributional learning and generaliza-
tion. We model both experiments of Pajak &
Levy (2011a): (a) ?same durations? across classes
(Expt. 1), and (b) ?different durations? across
classes (Expt. 2). Thus, the datasets we used
were closely modeled after their experimental de-
sign: (1) Expt. 1 bimodal, (2) Expt. 1 unimodal,
(3) Expt. 2 bimodal, and (4) Expt. 2 unimodal. In
each dataset, the data consisted of a list of pho-
netic values (duration in msec), where each data-
point was tagged as belonging to either the sono-
rant or the fricative class. The frequencies of the
?trained? class were as listed in Figure 3 (simu-
lating a single training continuum). In addition to
the ?trained? class, each dataset included two dat-
apoints from the ?untrained? class with the values
as listed in Table 2 in the ?untrained? condition.
These two datapoints were included in order to
evaluate the model?s categorization of sounds for
which no distributional evidence is available, thus
assessing the extent of generalization. We sim-
ulated weak perceptual noise by adding to each
datapoint normally-distributed error with standard
deviation of 0.3 times the distance between adja-
cent continuum steps.
4.2 Methodology
We ran the basic model on each of the four
datasets. For each, we performed 1,000,000 iter-
ations of Gibbs sampling, and analyzed the re-
sults for the second half. To assess convergence,
we ran four Markov chains for each dataset, us-
ing two overdispersed initializations: (1) assigning
one category label to all datapoints, and (2) as-
signing a different label to each datapoint. We
used a weak prior base distribution H (?0 = .001;
?0 = .001; ?20 = 1; ?0 was set to the overall mean
of the data), and set the concentration parameters
? = ?0 = 1.
4.3 Results and discussion
The simulation results are illustrated in Figure 5,4
plotting the proportion of samples on which the
model assigned the datapoints to two different cat-
egories, as opposed to a single category.5 Note that
4All variables we report in all simulations appear to have
converged to the posterior, as assessed by R? values of 1.1 or
less, calculated across the 4 chains (Gelman & Rubin 1992).
5No models we report assign the trained category data-
points to more than two categories more than 1% of the time.
15
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Basic model:
Experiment 1
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Basic model:
Experiment 2
Figure 5: Simulation results for the basic model.
Error bars give 95% binomial confidence intervals,
computed using the estimated number of effec-
tively independent samples in the Markov chains.
in the ?trained? condition, this means categoriza-
tion of all datapoints along the continuum. In the
?untrained? condition, on the other hand, it is cat-
egorization of two datapoints: one from each end-
point of the continuum.
The results in the ?trained? conditions demon-
strate that the model was able to learn from the
distributional cues, thus replicating the success of
previous phonetic category learning models.
Of most interest here are the results in the ?un-
trained? condition. The figure on the left shows
the results modeling the ?same-durations? exper-
iment (Expt. 1), demonstrating that the model cat-
egorizes the two datapoints in the untrained sound
class in exactly the same way as it did for the
trained sound class: two categories in the bimodal
condition, and one category in the unimodal con-
dition. Thus, these results suggest that we can suc-
cessfully model generalization of distinction types
across sound classes in phonetic category learning
by assuming that learners have an expectation that
category types (such as short and long, or voice-
less and voiced) may be shared across classes.
The figure on the right shows the results model-
ing the ?different-durations? experiment (Expt. 2),
revealing a limitation of the model: failure to gen-
eralize when the untrained class has the same cat-
egory structure but different absolute phonetic val-
ues (overall shorter in the untrained class than in
the trained class). Instead, the model categorizes
both untrained datapoints as belonging to a single
category. This result diverges from the experimen-
tal results, where learners generalize the learned
distinction type in both cases, whether the abso-
lute phonetic values of the analogous categories
are identical or not. We address this problem in
the next section by implementing a modification
to the model that allows more flexibility in how
each class implements the same category types.
5 Extended generalization model
The goal of the extended model is to explicitly al-
low for phonetic variability across sound classes.
As a general approach, we could imagine func-
tions that transform categories across classes so
that the same categories can be ?reused? by be-
ing translated around to different parts of the pho-
netic space. These functions would be specific op-
erations representing any intrinsic differences be-
tween sound classes. Here, we use a very simple
function that can account for one widely attested
type of transformation: different absolute phonetic
values for analogous categories in distinct sound
classes (Ladefoged & Maddieson 1996), such as
longer overall durations for voiceless fricatives
than for sonorants. This type of transformation
has been successfully used in prior modeling work
to account for learning allophones of a single
phoneme that systematically vary in phonetic val-
ues along certain dimensions (Dillon et al 2013).
5.1 Model details
We implement the possibility for between-class
variability by allowing for one specific type of
idiosyncratic implementation of categories across
classes: learnable class-specific ?offsets? by which
the data in a class are shifted along the phonetic
dimension, as illustrated in Figure 6 (the key for
the variables is in Table 1).
5.2 Inference
Each iteration of MCMC now includes a
Metropolis-Hastings step to resample the offset
parameters fc, which uses a zero-mean Gaussian
proposal, with standard deviation ?p = range of data5 .
6 Simulations: extended model
This second set of simulations has two goals: (1) to
establish that the extended model can successfully
replicate the performance of the basic model in
both distributional learning and generalization in
the no-variability case, and (2) to show that ex-
plicitly allowing for variability across classes lets
the model generalize when there is between-class
variability in category realizations.
16
HG0?
Gc?0
zic
dic
fc
? f
i ? {1..nc}
c ? C
Figure 6: The graphical representation of the ex-
tended model.
6.1 Methodology
We used the same prior as in the first set of sim-
ulations, and used a Gaussian prior on the offset
parameter with standard deviation ? f = 1000. Be-
cause only the relative values of offset parameters
are important for category sharing across classes,
we set the offset parameter for one of the classes
to zero. The four Markov chains now crossed cate-
gory initialization with two different initial values
of the offset parameter.
6.2 Results and discussion
The simulation results are illustrated in Fig-
ure 7. The figure on the left demonstrates that
the extended model performs similarly to the ba-
sic model in the case of no variability between
classes. The figure on the right, on the other
hand, shows that ? unlike the basic model ?
the extended model succeeds in generalizing the
learned distinction type to an untrained sound
class when there is phonetic variability between
classes. These results suggest that allowing for
variability in category implementations across
sound classes may be necessary to account for
human learning. Taken together, these results are
consistent with our proposal that language learn-
ers have an expectation that category types can be
shared across sound classes. Furthermore, learn-
ers appear to have implicit knowledge of the ways
that sound classes can vary in their exact phonetic
implementations of different category types. This
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Extended model:
Experiment 1
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Extended model:
Experiment 2
Figure 7: Simulation results for the extended
model. Error bars give 95% binomial confidence
intervals, computed using the estimated number
of effectively independent samples in the Markov
chains.
type of knowledge may include ? as in our ex-
tended generalization model ? the possibility that
phonetic values of categories in one class can be
systematically shifted relative to another.
7 General discussion
In this paper we presented the first model of gen-
eralization in phonetic category learning, in which
learning a distinction type for one set of sounds
(e.g., /m/?/mm/) immediately generalizes to an-
other set of sounds (e.g., /s/?/ss/), thus reproduc-
ing the key features of adult learner performance
in behavioral experiments. This extends previous
computational work in phonetic category learn-
ing, which focused on modeling the process of
learning from distributional cues, and did not ad-
dress the question of generalization. The basic
premise of the proposed model is that learners?
knowledge of phonetic categories is represented
hierarchically: individual sounds are grouped into
categories, and individual categories are grouped
into sound classes. Crucially, the category struc-
ture established for one sound class can be di-
rectly shared with another class, although differ-
ent classes can implement the categories in id-
iosyncratic ways, thus mimicking natural variabil-
ity in how analogous categories (e.g., short /m/ and
/s/, or long /mm/ and /ss/) are phonetically imple-
mented for different sound classes.
The simulation results we presented succeed
in reproducing the human pattern of generaliza-
tion performance, in which the proportion of two-
category inferences about the untrained class is
17
very similar to that for the trained class. Note,
however, that there are clear quantitative dif-
ferences between the two in learning perfor-
mance: the model learns almost perfectly from
the available distributional cues (?trained? condi-
tion), while adult learners are overall very conser-
vative in accepting two categories along the length
dimension, as indicated by the overall low num-
ber of ?different? responses. There are two main
reasons why the model might be showing more
extreme categorization preferences than humans
in this particular task. First, humans have cogni-
tive limitations that the current model does not,
such as those related to memory or attention. In
particular, imperfect memory makes it harder for
humans to integrate the distributional information
from all the trials in the exposure, and longer train-
ing would presumably improve performance. Sec-
ond, adults have strong native-language biases that
affect learning of a second language (Flege 1995).
The population tested by Pajak & Levy (2011a)
consisted of adult native speakers of American En-
glish, a language in which length is not used con-
trastively. Thus, the low number of ?different? re-
sponses in the experiments can be attributed to
participants? prior bias against category distinc-
tions based on length. The model, on the other
hand, has only a weak prior that was meant to be
easily overridden by data.
This last point is of direct relevance for the
area of second language (L2) acquisition, where
one of the main research foci is to investigate
the effects of native-language knowledge on L2
learning. The model we proposed here can poten-
tially be used to systematically investigate the role
of native-language biases when learning category
distinctions in a new language. In particular, an L2
learner, whose linguistic representations include
two languages, could be implemented by adding
a language-level node to the model?s hierarchical
structure (through an additional Dirichlet process).
This extension will allow for category structures to
be shared not just within a language for different
sound classes, but also across languages, thus ef-
fectively acting as a native-language bias.
As a final note, we briefly discuss alternative
ways of modeling generalization in phonetic cat-
egory learning. In the model we described in this
paper, whole categories are generalized from one
class to another. However, one might imagine an-
other approach to this problem where generaliza-
tion is a byproduct of learners? attending more
to the dimension that they find to be relevant for
distinguishing between some categories in a lan-
guage. That is, learners? knowledge would not in-
clude the expectation that whole categories may
be shared across classes, as we argued here, but
rather that a given phonetic dimension is likely
to be reused to distinguish between categories in
multiple sound classes.
This intuition could be implemented in differ-
ent ways. In a Dirichlet process model of category
learning, the concentration parameter ? might be
learned, and shared for all classes along a given
phonetic dimension, thus producing a bias to-
ward having a similar number of categories across
classes. Alternatively, the variance of categories
along a given dimension might be learned, and
also shared for all classes. Under this scenario,
learning category variance along a given dimen-
sion would help categorize novel sounds along that
dimension. That is, two novel datapoints would be
likely categorized into separate categories if the in-
ferred variance along the relevant dimension was
smaller than the distance between the datapoints,
but into a single category if the inferred variance
was comparable to that distance.
Finally, this model assumes that sound classes
are given in advance, and that only the categories
within each class are learned. While this assump-
tion may seem warranted for some types of per-
ceptually dissimilar sound classes (e.g., conso-
nants and vowels), and also may be appropriate
for L2 acquisition, it is not clear that it is true for
all sound classes that allow for generalization in
infancy. It remains for future work to determine
how learners may generalize while simultaneously
learning the sound classes.
We plan to pursue all these directions in fu-
ture work with the ultimate goal of improving our
understanding how human learners represent their
linguistic knowledge and how they use it when ac-
quiring a new language.
Acknowledgments
We thank Gabriel Doyle and three anonymous
CMCL reviewers for useful feedback. This re-
search was supported by NIH Training Grant T32-
DC000041 from the Center for Research in Lan-
guage at UC San Diego to B.P. and NIH Training
Grant T32-DC000035 from the Center for Lan-
guage Sciences at University of Rochester to B.P.
18
References
de Boer, Bart & Patricia K. Kuhl. 2003. Inves-
tigating the role of infant-directed speech with
a computer model. Acoustic Research Letters
Online 4(4). 129?134.
Clements, George N. 2003. Feature economy in
sound systems. Phonology 20. 287?333.
Dillon, Brian, Ewan Dunbar & William Idsardi.
2013. A single-stage approach to learning
phonological categories: Insights from Inukti-
tut. Cognitive Science 37. 344?377.
Feldman, Naomi H., Thomas L. Griffiths &
James L. Morgan. 2009. Learning phonetic cat-
egories by learning a lexicon. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society, 2208?2213. Austin, TX: Cog-
nitive Science Society.
Ferguson, Thomas S. 1973. A Bayesian analy-
sis of some nonparametric problems. Annals of
Statistics 1. 209?230.
Flege, James E. 1995. Second-language speech
learning: theory, findings and problems. In
Winifred Strange (ed.), Speech perception and
linguistic experience: issues in cross-language
research, 229?273. Timonium, MD: York Press.
Gelman, Andrew & Donald B. Rubin. 1992. In-
ference from iterative simulation using multiple
sequences. Statistical Science 7. 457?511.
Giovanardi, Maurizio & Maria-Gabriella
Di Benedetto. 1998. Acoustic analysis of
singleton and geminate fricatives in Italian.
The European Journal of Language and Speech
(EACL/ESCA/ELSNET) 1998. 1?13.
Greenberg, Steven. 1996. The Switchboard tran-
scription project. Report prepared for the
1996 CLSP/JHU Workshop on Innovative Tech-
niques in Continuous Large Vocabulary Speech
Recognition.
Kuhl, Patricia K. 1991. Human adults and human
infants show a ?perceptual magnet effect? for
the prototypes of speech categories, monkeys
do not. Perception and Psychophysics 50(2).
93?107.
Kuhl, Patricia K., Karen A. Williams, Francisco
Lacerda, Kenneth N. Stevens & Bjo?rn Lind-
blom. 1992. Linguistic experience alters pho-
netic perception in infants by 6 months of age.
Science 255. 606?608.
Lacerda, Francisco. 1995. The perceptual magnet-
effect: An emergent consequence of exemplar-
based phonetic memory. In K. Ellenius &
P. Branderud (eds.), Proceedings of the 13th
International Congress of Phonetic Sciences,
140?147. Stockholm: KTH and Stockholm Uni-
versity.
Ladefoged, Peter & Ian Maddieson. 1996. The
sounds of the world?s languages. Oxford, UK;
Cambridge, MA: Blackwell.
Lisker, Leigh & Arthur S. Abramson. 1970. The
voicing dimensions: Some experiments in com-
parative phonetics. In Proceedings of the Sixth
International Congress of Phonetic Sciences,
Prague: Academia.
Mattei, Marco & Maria-Gabriella Di Benedetto.
2000. Acoustic analysis of singleton and gemi-
nate nasals in Italian. The European Journal of
Language and Speech (EACL/ESCA/ELSNET)
2000. 1?11.
Maye, Jessica & LouAnn Gerken. 2000. Learning
phonemes without minimal pairs. In S. Cather-
ine Howell, Sarah A. Fish & Thea Keith-Lucas
(eds.), Proceedings of the 24th Annual Boston
University Conference on Language Develop-
ment, 522?533. Somerville, MA: Cascadilla
Press.
Maye, Jessica & LouAnn Gerken. 2001. Learn-
ing phonemes: how far can the input take us?
In A. H-J. Do, L. Dom??nguez & A. Johansen
(eds.), Proceedings of the 25th Annual Boston
University Conference on Language Develop-
ment, 480?490. Somerville, MA: Cascadilla
Press.
Maye, Jessica, Daniel J. Weiss & Richard N.
Aslin. 2008. Statistical phonetic learning in
infants: facilitation and feature generalization.
Developmental Science 11(1). 122?134.
Maye, Jessica, Janet F. Werker & LouAnn Gerken.
2002. Infant sensitivity to distributional infor-
mation can affect phonetic discrimination. Cog-
nition 82. B101?B111.
McClaskey, Cynthia L., David B. Pisoni &
Thomas D. Carrell. 1983. Transfer of training
of a new linguistic contrast in voicing. Percep-
tion and Psychophysics 34(4). 323?330.
McMurray, Bob, Richard N. Aslin & Joseph C.
Toscano. 2009. Statistical learning of phonetic
categories: insights from a computational ap-
proach. Developmental Science 12(3). 369?
378.
Pajak, Bozena & Roger Levy. 2011a. How ab-
stract are phonological representations? Evi-
dence from distributional perceptual learning.
19
In Proceedings of the 47th Annual Meeting of
the Chicago Linguistic Society, Chicago, IL:
University of Chicago.
Pajak, Bozena & Roger Levy. 2011b. Phono-
logical generalization from distributional evi-
dence. In L. Carlson, C. Ho?lscher & T. Ship-
ley (eds.), Proceedings of the 33rd Annual Con-
ference of the Cognitive Science Society, 2673?
2678. Austin, TX: Cognitive Science Society.
Perfors, Amy & David Dunbar. 2010. Phonetic
training makes word learning easier. In S. Ohls-
son & R. Catrambone (eds.), Proceedings of the
32nd Annual Conference of the Cognitive Sci-
ence Society, 1613?1618. Austin, TX: Cogni-
tive Science Society.
Teh, Yee Whye, Michael I. Jordan, Matthew J.
Beal & David M. Blei. 2006. Hierarchical
Dirichlet processes. Journal of the American
Statistical Association 101(476). 1566?1581.
Toscano, Joseph C. & Bob McMurray. 2010. Cue
integration with categories: Weighting acoustic
cues in speech using unsupervised learning and
distributional statistics. Cognitive Science 34.
434?464.
Vallabha, Gautam K., James L. McClelland, Fer-
ran Pons, Janet F. Werker & Shigeaki Amano.
2007. Unsupervised learning of vowel cate-
gories from infant-directed speech. Proceedings
of the National Academy of Sciences 104(33).
13273?13278.
20
