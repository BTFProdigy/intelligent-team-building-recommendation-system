Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 41?44,
New York, June 2006. c?2006 Association for Computational Linguistics
Spectral Clustering for Example Based Machine Translation
Rashmi Gangadharaiah
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
rgangadh@andrew.cmu.edu
Ralf Brown
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
ralf@cs.cmu.edu
Jaime Carbonell
LTI
Carnegie Mellon University
Pittsburgh P.A. 15213
jgc@cs.cmu.edu
Abstract
Prior work has shown that generaliza-
tion of data in an Example Based Ma-
chine Translation (EBMT) system, re-
duces the amount of pre-translated text re-
quired to achieve a certain level of accu-
racy (Brown, 2000). Several word clus-
tering algorithms have been suggested to
perform these generalizations, such as k-
Means clustering or Group Average Clus-
tering. The hypothesis is that better con-
textual clustering can lead to better trans-
lation accuracy with limited training data.
In this paper, we use a form of spectral
clustering to cluster words, and this is
shown to result in as much as 29.08% im-
provement over the baseline EBMT sys-
tem.
1 Introduction
In EBMT, the source sentence to be translated
is matched against the source language sentences
present in a corpus of source-target sentence pairs.
When a partial match is found, the corresponding
target translations are obtained through subsenten-
tial alignment. These partial matches are put to-
gether to obtain the final translation by optimizing
translation and alignment scores and using a statisti-
cal target language model in the decoding process.
Prior work has shown that EBMT requires large
amounts of data (in the order of two to three mil-
lion words) (Brown, 2000) of pre-translated text, to
function reasonably well. Thus, some modification
of the basic EBMT method is required to make it ef-
fective when less data is available. In order to use
the available text efficiently, systems such as, (Veale
and Way, 1997) and (Brown, 1999), convert the ex-
amples in the corpus into templates against which
the new text can be matched. Thus, source-target
sentence pairs are converted to source-target gener-
alized template pairs. An example of such a pair is
shown below:
The session opened at 2p.m
La se?ance est ouverte a? 2 heures
The <event> <verb-past-tense> at <time>
La <event> <verb-past-tense> a <time>
This single template can be used to translate differ-
ent source sentences, including for example,
The session adjourned at 6p.m
The seminar opened at 8a.m
if ?session? and ?seminar? are both generalized to
?<event>?, ?opened? and ?adjourned? are both gen-
eralized to ?<verb-past-tense>? and finally ?6p.m?
and ?8a.m? are both generalized to ?<time>?.
The system used by (Brown, 1999) performs
its generalization using both equivalence classes of
words and a production rule grammar. This paper
describes the use of spectral clustering (Ng. et. al.,
2001; Zelnik-Manor and Perona, 2004), for auto-
mated extraction of equivalence classes. Spectral
clustering is seen to be superior to Group Average
Clustering (GAC) (Brown, 2000) both in terms of
semantic similarity of words falling in a single clus-
ter, and overall BLEU score (Papineni. et. al., 2002)
in a large scale EBMT system.
The next section explains the term vectors ex-
tracted for each word, which are then used to cluster
words into equivalence classes and provides an out-
line of the Standard GAC algorithm. Section 3 de-
scribes the spectral clustering algorithm used. Sec-
41
tion 4 lists results obtained in a full evaluation of the
algorithm. Section 5 concludes and discusses direc-
tions for future work.
2 Term vectors for clustering
Using a bilingual dictionary, usually created using
statistical methods such as those of (Brown et. al.,
1990) or (Brown, 1997), and the parallel text, a
rough mapping between source and target words can
be created. This word pair is then treated as an in-
divisible token for future processing. For each such
word pair we then accumulate counts for each to-
ken in the surrounding context of its occurrences
(N words, currently 3, immediately prior to and
N words immediately following). The counts are
weighted with respect to distance from occurrence,
with a linear decay (from 1 to 1/N) to give great-
est importance to the words immediately adjacent to
the word pair being examined. These counts form a
pseudo-document for each pair, which are then con-
verted into term vectors for clustering.
In this paper, we compare our algorithm against
the incremental GAC algorithm(Brown, 2000). This
method examines each word pair in turn, comput-
ing a similarity measure to every existing cluster.
If the best similarity measure is above a predeter-
mined threshold, the new word is placed in the cor-
responding cluster, otherwise a new cluster is cre-
ated if the maximum number of clusters has not yet
been reached.
3 Spectral clustering
Spectral clustering is a general term used to de-
scribe a group of algorithms that cluster points using
the eigenvalues of ?distance matrices? obtained from
data. In our case, the algorithm described in (Ng.
et. al., 2001) was performed with certain variations
that were proposed by (Zelnik-Manor and Perona,
2004) to compute the scaling factors automatically
and for the k-Means orthogonal treatment (Verma
and Meila, 2003) during the initialization. These
scaling factors help in self-tuning distances between
points according to the local statistics of the neigh-
borhoods of the points. The algorithm is briefly de-
scribed below.
1. Let S =s1, s2, ....sn, denote the term vectors to
be clustered into k classes.
2. Form the affinity matrix A defined by
Aij = exp(?d2(si, sj)/?i?j) for i 6= j
Aii = 1
Where, d(si, sj) = 1/(sim(si, sj) + )
sim(si, sj) is the Cosine similarity between si
and sj ,  is used to prevent the ratio from be-
coming infinity
?i is the set of local scaling parameters for si.
?i = d(si, sT ) where, sT is the T th neighbor of
point si for some fixed T (7 for this paper).
3. Define D to be the diagonal matrix given by,
Dii = ?jAij
4. Compute L = D?1/2AD?1/2
5. Select k eigenvectors corresponding to k
largest eigenvalues (k is presently an externally
set parameter). The eigenvectors are normal-
ized to have unit length. Form matrix U by
stacking all the eigenvectors in columns.
6. Form the matrix Y by normalizing U?s rows,
Yij = Uij/
?
(?jU2ij)
7. Perform k-Means clustering treating each row
of Y as a point in k dimensions. The k-Means
algorithm is initialized either with random cen-
ters or with orthogonal vectors.
8. After clustering, assign the point si to cluster c
if the corresponding row i of the matrix Y was
assigned to cluster c.
9. Sum the distances between the members and
the centroid of each cluster to obtain the classi-
fication cost.
10. Goto step 7, iterate for a fixed number of it-
erations. In this paper, 20 iterations were per-
formed with orthogonal k-Means initialization
and 5 iterations with random k-Means initial-
ization.
11. The clusters obtained from the iteration with
least classification cost are selected as the k
clusters.
4 Preliminary Results
The clusters obtained from the spectral clustering
method are seen by inspection to correspond to more
natural and intuitive word classes than those ob-
tained by GAC. Even though this is subjective and
not guaranteed to lead to improve translation perfor-
mance, it shows that maybe the increased power of
spectral clustering to represent non-convex classes
42
(non-convex in the term vector domain) could be
useful in a real translation experiment. Some ex-
ample classes are shown in Table 1. The first
class in an intuitive sense corresponds to measure-
ment units. We see that in the <units> case,
GAC misses some of the members which are ac-
tually distributed among many different classes and
hence these are not well generalized. In the second
class <months>, spectral clustering has primarily
the months in a single class whereas GAC adds a
number of seemingly unrelated words to the clus-
ter. The classes were all obtained by finding 80
clusters in a 20,000-sentence pair subset of the IBM
Hansard Corpus (Linguistic Data Consortium, 1997)
for spectral clustering. 80 was chosen as the number
of clusters since it gave the highest BLEU score in
the evaluation. For GAC, 300 clusters were used as
this gave the best performance.
To show the effectiveness of the clustering meth-
ods in an actual evaluation, we set up the following
experiment for an English to French translation task
on the Hansard corpus. The training data consists of
three sets of size 10,000 (set1), 20,000 (set2) and
30,000 (set3) sentence pairs chosen from the first
six files of the Hansard Corpus. Only sentences of
length 5 to 21 words were taken. Only words with
frequency of occurrence greater than 9 were chosen
for clustering because more contextual information
would be available when the word occurs frequently
and this would help in obtaining better clusters. The
test data was chosen to be a set of 500 sentences ob-
tained from files 20, 40, 60 and 80 of the Hansard
corpus with 125 sentences from each file. Each of
the methods was run with different number of clus-
ters and results are reported only for the optimal
number of clusters in each case.
The results in Table 2 show that spectral clus-
tering requires moderate amounts of data to get a
large improvement. For small amounts of data it is
slightly worse than GAC, but neither gives much im-
provement over the baseline. For larger amounts of
data, again both methods are very similar, though
spectral clustering is better. Finally, for moderate
amounts of data, when generalization is the most
useful, spectral clustering gives a significant im-
provement over the baseline as well as over GAC.
By looking at the clusters obtained with varying
amounts of data, it can be concluded that high pu-
Table 1: Clusters for <units> and <months>
Spectral clustering GAC
?adjourned? ?hre? ?adjourned? ?hre?
?cent? ?%?
?days? ?jours?
?families? ?familles? ?families? ?familles?
?hours? ?heures?
?million? ?millions? ?million? ?millions?
?minutes? ?minutes?
?o?clock? ?heures? ?o?clock? ?heures?
?p.m.? ?heures? ?p.m.? ?heures?
?p.m.? ?hre?
?people? ?personnes? ?people? ?personnes?
?per? ?%? ?per? ?%?
?times? ?fois? ?times? ?fois?
?years? ?ans?
?august? ?aou?t? ?august? ?aou?t?
?december? ?de?cembre? ?december? ?de?cembre?
?february? ?fe?vrier? ?february? ?fe?vrier?
?january? ?janvier? ?january? ?janvier?
?march? ?mars? ?march? ?mars?
?may? ?mai? ?may? ?mai?
?november? ?novembre? ?november? ?novembre?
?october? ?octobre? ?october? ?octobre?
?only? ?seulement? ?only? ?seulement?
?june? ?juin? ?june? ?juin?
?july? ?juillet? ?july? ?juillet?
?april? ?avril? ?april? ?avril?
?september? ?septembre? ?september? ?septembre?
?page? ?page?
?per? ?$?
?recognize? ?parole?
?recognized? ?parole?
?recorded? ?page?
?section? ?article?
?since? ?depuis? ?since? ?depuis?
?took? ?se?ance?
?under? ?loi?
43
Table 2: % Relative improvement over baseline EBMT
# clus is the number of clusters for best performance
GAC Spectral
% Rel imp #clus % Rel imp #clus
10k 3.33 50 1.37 20
20k 22.47 300 29.08 80
30k 2.88 300 3.88 200
rity clusters can be obtained with even just moderate
amounts of data.
5 Conclusions and future work
From the experimental results we see that spectral
clustering leads to relatively purer and more intu-
itive clusters. These clusters result in an improved
BLEU score in comparison with the clusters ob-
tained through GAC. GAC can only collect clusters
in convex regions in the term vector space, while
spectral clustering is not limited in this regard. The
ability of spectral clustering to represent non-convex
shapes arises due to the projection onto the eigen-
vectors as described in (Ng. et. al., 2001).
As future work, we would like to analyze the
variation in performance as the amount of data in-
creases. It is widely known that increasing the
amount of training data in a generalized EBMT sys-
tem eventually leads to saturation of performance,
where all clustering methods perform about as well
as baseline. Thus, all methods have an operating re-
gion where they are the most useful. We would like
to locate and extend this region for spectral cluster-
ing.
Also, it would be interesting to compare the clus-
ters obtained with spectral clustering and the Part of
Speech tags of the words in the same cluster, espe-
cially for languages such as English where good tag-
gers are available.
Finally, an important direction of research is in
automatically selecting the number of clusters for
the clustering algorithm. To do this, we could use
information from the eigenvalues or the distribution
of points in the clusters.
Acknowledgment
This work was funded by National Business Center
award NBCHC050082.
References
Andrew Ng, Michael Jordan, and Yair Weiss 2001. On
Spectral Clustering: Analysis and an algorithm. In Ad-
vances in Neural Information Processing Systems 14:
Proceeding of the 2001 Conference, pages 849-856,
Vancouver, British Columbia, Canada, December.
Deepak Verma and Marina Meila. 2003. Comparison of
Spectral Clustering Algorithms. http://www.ms.
washington.edu/?spectral/.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
Jing Zhu. 2002. BLEU: a method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002), pages 311-
318,Philadelphia, PA, July. http://acl.ldc.
upenn.edu/P/P02
Linguistic Data Consortium. 1997. Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.
edu/
L. Zelnik-Manor and P. Perona 2004 Self-Tuning Spec-
tral Clustering. In Advances in Neural Information
Processing Systems 17: Proceeding of the 2004 Con-
ference.
Peter Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F.
Jelinek, J. Lafferty, R. Mercer and P. Roossin. 1990.
A Statistical Approach to Machine Translation. Com-
putational Linguistics, 16:79-85.
Ralf D. Brown. 1997. Automated Dictionary Extrac-
tion for ?Knowledge-Free? Example-Based Transla-
tion. In Proceedings of the Seventh International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI-97), pages 111-118, Santa
Fe, New Mexico, July. http://www.cs.cmu.
edu/?ralf/papers.html
Ralf D. Brown. 1999. Adding Linguistic Knowledge
to a Lexical Example-Based Translation System. In
Proceedings of the Eighth International Conference
on Theoretical and Methodological Issues in Machine
Translation(TMI-99), pages 22-32, August. http:
//www.cs.cmu.edu/?ralf/papers.html
Ralf. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of Eighteenth
International Conference on Computational Linguis-
tics (COLING-2000), pages 125-131, Saarbru?cken,
Germany.
Tony Veale and Andy Way. 1997. Gaijin: A Template-
Driven Bootstrapping Approach to Example-Based
Machine Translation. In Proceedings of NeMNLP97,
New Methods in Natural Language Processing, Sofia,
Bulgaria, September. http://www.compapp.
dcu.ie/?tonyv/papers/gaijin.html.
44
Coling 2010: Poster Volume, pages 320?328,
Beijing, August 2010
Monolingual Distributional Profiles for Word Substitution in Machine
Translation
Rashmi Gangadharaiah
rgangadh@cs.cmu.edu
Ralf D. Brown
ralf@cs.cmu.edu
Language Technologies Institute,
Carnegie Mellon University
Jaime Carbonell
jgc@cs.cmu.edu
Abstract
Out-of-vocabulary (OOV) words present a
significant challenge for Machine Trans-
lation. For low-resource languages, lim-
ited training data increases the frequency
of OOV words and this degrades the qual-
ity of the translations. Past approaches
have suggested using stems or synonyms
for OOV words. Unlike the previous
methods, we show how to handle not just
the OOV words but rare words as well
in an Example-based Machine Transla-
tion (EBMT) paradigm. Presence of OOV
words and rare words in the input sentence
prevents the system from finding longer
phrasal matches and produces low qual-
ity translations due to less reliable lan-
guage model estimates. The proposed
method requires only a monolingual cor-
pus of the source language to find can-
didate replacements. A new framework
is introduced to score and rank the re-
placements by efficiently combining fea-
tures extracted for the candidate replace-
ments. A lattice representation scheme al-
lows the decoder to select from a beam
of possible replacement candidates. The
new framework gives statistically signif-
icant improvements in English-Chinese
and English-Haitian translation systems.
1 Introduction
An EBMT system makes use of a parallel corpus
to translate new sentences. Each input sentence
is matched against the source side of a training
corpus. When matches are found, the correspond-
ing translations in the target language are obtained
through sub-sentential alignment. In our EBMT
system, the final translation is obtained by com-
bining the partial target translations using a sta-
tistical target Language Model. EBMT systems,
like other data-driven approaches, require large
amounts of data to function well (Brown, 2000).
Having more training data is beneficial re-
sulting in log-linear improvement in translation
quality for corpus-based methods (EBMT, SMT).
Koehn (2002) shows translation scores for a num-
ber of language pairs with different training sizes
translated using the Pharaoh SMT toolkit (Koehn
et al, 2003). However, obtaining sizable paral-
lel corpora for many languages is time-consuming
and expensive. For rare languages, finding bilin-
gual speakers becomes especially difficult.
One of the main reasons for low quality transla-
tions is the presence of large number of OOV and
rare words (low frequency words in the training
corpus). Variation in domain and errors in spelling
increase the number of OOV words. Many of the
present translation systems either ignore these un-
known words or leave them untranslated in the fi-
nal target translation. When data is limited, the
number of OOV words increases, leading to the
poor performance of the translation models and
the language models due to the absence of longer
sequences of source word matches and less reli-
able language model estimates.
Approaches in the past have suggested using
stems or synonyms for OOV words as replace-
ments (Yang and Kirchhoff, 2006). Similarity
measures have been used to find words that are
closely related (Marton et al, 2009). For morpho-
320
logically rich languages, the OOV word is mor-
phologically analyzed and the stem is used as its
replacement (Popovic? and Ney, 2004).
This paper presents a simpler method inspired
by the Context-based MT approach (Carbonell et
al., 2006) to improve translation quality. The
method requires a large source language mono-
lingual corpus and does not require any other
language dependent resources to obtain replace-
ments. Approaches suggested in the past only
concentrated on finding replacements for the OOV
words and not the rare words. This paper pro-
poses a unified method to find possible replace-
ments for OOV words as well as rare words based
on the context in which these words appear. In
the case of rare words, the translated sentence is
traced back to find the origin of the translations
and the target translations of the replacements are
replaced with the translations of the rare words. In
the case of OOV words, the target translations are
replaced by the OOV word itself. The main idea
for adopting this approach is the belief that the
EBMT system will be able to find longer phrasal
matches and that the language model will be able
to give better probability estimates while decod-
ing if it is not forced to fragment text at OOV and
rare-word boundaries. This method is highly ben-
eficial for low-resource languages that do not have
morphological analysers or Part-of-Speech (POS)
taggers and in cases where the similarity measures
proposed in the past do not find closely related
words for certain OOV words.
The rest of the paper is organized as follows.
The next section (Section 2) discusses related
work in handling OOV words. Section 3 describes
the method adopted in this paper. Section 4 de-
scribes the experimental setup. Section 5 reports
the results obtained with the new framework for
English-Chinese and English-Haitian translation
systems. Section 6 concludes and suggests pos-
sible future work.
2 Related Work
Orthographic and morpho-syntactic techniques
for preprocessing training and test data have been
shown to reduce OOV word rates. Popovic?
and Ney (2004) demonstrated this on rich mor-
phological languages in an SMT system. They
introduced different types of transformations to
the verbs to reduce the number of unseen word
forms. Habash (2008) addresses spelling, name-
transliteration OOVs and morphological OOVs in
an Arabic-English Machine Translation system.
Phrases with the OOV replacements in the phrase
table of a phrase-based SMT system were ?recy-
cled? to create new phrases in which the replace-
ments were replaced by the OOV words.
Yang and Kirchhoff (2006) proposed a back-
off model for phrase-based SMT that translated
word forms in the source language by hierarchi-
cal morphological phrase level abstractions. If
an unknown word was found, the word was first
stemmed and the phrase table entries for words
sharing the same stem were modified by replacing
the words with their stems. If a phrase entry or a
single word phrase was found, the corresponding
translation was used, otherwise the model backed
off to the next level and applied compound split-
ting to the unknown word. The phrase table in-
cluded phrasal entries based on full word forms as
well as stemmed and split counterparts.
Vilar et al (2007) performed the translation
process treating both the source and target sen-
tences as a string of letters. Hence, there are
no unknown words when carrying out the actual
translation of a test corpus. The word-based sys-
tem did most of the translation work and the letter-
based system translated the OOV words.
The method proposed in this work to han-
dle OOV and rare words is very similar to the
method adopted by Carbonell et al (2006) to gen-
erate word and phrasal synonyms in their Context-
based MT system. Context-based MT does not
require parallel text but requires a large monolin-
gual target language corpus and a fullform bilin-
gual dictionary. The main principle is to find those
n-gram candidate translations from a large target
corpus that contain as many potential word and
phrase translations of the source text from the dic-
tionary and fewer spurious content words. The
overlap decoder combines the target n-gram trans-
lation candidates by finding maximal left and right
overlaps with the translation candidates of the pre-
vious and following n-grams. When the overlap
decoder does not find coherent sequences of over-
lapping target n-grams, more candidate transla-
321
tions are obtained by substituting words or phrases
in the target n-grams by their synonyms.
Barzilay and McKeown (2001) and Callison-
Burch et al (2006) extracted paraphrases from
monolingual parallel corpus where multiple trans-
lations were present for the same source. The syn-
onym generation in Carbonell et al (2006) differs
from the above in that it does not require paral-
lel resources containing multiple translations for
the same source language. In Carbonell et al
(2006), a list of paired left and right contexts that
contain the desired word or phrase are extracted
from the monolingual corpus. The same corpus
is used to find other words and phrases that fit the
paired contexts in the list. The idea is based on the
distributional hypothesis which states that words
with similar meanings tend to appear in similar
contexts (Harris, 1954). Hence, their approach
performed synonym generation on the target lan-
guage to find translation candidates that would
provide maximal overlap during decoding.
Marton et al (2009) proposed an approach sim-
ilar to Carbonell et al (2006) to obtain replace-
ments for OOV words, where monolingual dis-
tributional profiles for OOV words were con-
structed. Hence, the approach was applied on the
source language side as opposed to Carbonell et
al. (2006) which worked on the target language.
Only similarity scores and no other features were
used to rank the paraphrases (or replacements)
that occured in similar contexts. The high rank-
ing paraphrases were used to augment the phrase
table of phrase-based SMT.
All of the previously suggested methods only
handle OOV words (except Carbonell et al (2006)
which handles low frequency target phrases) and
no attempt is made to handle rare words. Many of
the methods explained above directly modify the
training corpus (or phrase table in phrase-based
SMT) increasing the size of the corpus. Our
method clusters words and phrases based on their
context as described by Carbonell et al (2006) but
uses the clustered words as replacements for not
just the OOV words but also for the rare words
on the source language side. Our method does
not make use of any morphological analysers,
POS taggers or manually created dictionaries
as they may not be available for many rare or
low-resource languages. The translation of the
replacements in the final decoded target sentence
is replaced by the translation of the original word
(or the source word itself in the OOV case),
hence, we do not specifically look for synonyms.
The only condition for a word to be a candidate
replacement is that its left and right context need
to match with that of the OOV/rare-word. Hence,
the clustered words could have different semantic
relations. For example,
(cluster1):?laugh, giggle, chuckle, cry, weep?
where ?laugh, giggle, chuckle? are synonyms and
?cry, weep? are antonyms of ?laugh?.
Clusters can also contain hypernyms (or hy-
ponyms), meronyms (or holonyms), troponyms
and coordinate terms along with synonyms and
antonyms. For example,
(cluster2):?country, region, place, area, dis-
trict, state, zone, United States, Canada, Korea,
Malaysia?.
where ?country? is a hypernym of ?United
States/Canada/Korea/Malaysia?. ?district? is a
meronym of ?state?. ?United States, Canada,
Korea, Malaysia? are coordinate terms sharing
?country? as their hypernym.
The contributions made by the paper are three-
fold: first, replacements are found for not just the
OOV words but for the rare words as well. Sec-
ond, the framework used allows scoring replace-
ments based on multiple features to permit op-
timization. Third, instead of directly modifying
the training corpus by replacing the candidate re-
placements by the OOV words, a new representa-
tion scheme is used for the test sentences to effi-
ciently handle a beam of possible replacements.
3 Proposed Method
Like Marton et al (2009), only a large monolin-
gual corpus is required to extract candidate re-
placements. To retrieve more replacements, the
monolingual corpus is pre-processed by first gen-
eralizing numbers, months and years by NUM-
BER, MONTH and YEAR tags, respectively.
322
3.1 OOV and Rare words
Words in the test sentence (new source sentence
to be translated) that do not appear in the training
corpus are called OOV words. Words in the test
sentence that appear less thanK times in the train-
ing corpus are considered as rare words (in this
paper K = 3). The method presented in the fol-
lowing sections holds for both OOV as well as rare
words. In the case of rare words, the final transla-
tion is postprocessed (Section 3.7) to include the
translation of the rare word.
The procedure adopted will be explained with
a real example T (the rest of the sentence is
removed for the sake of clarity) encountered in
the test data with ?hawks? as the OOV word,
T :a mobile base , hitting three hawks with
one arrow over the past few years ...
3.2 Context
As the goal is to obtain longer target phrasal trans-
lations for the test sentence before decoding,
only words that fit the left and right context of the
OOV/rare-word in the test sentence are extracted.
Unlike Marton et al (2009) where a context list
for each OOV is generated from the contexts
of their replacements, this paper uses only the
left and right context of the OOV/rare-word.
The default window size for the context is five
words (two words to the left and two words to the
right of the OOV/rare-word). If the windowed
words contain only function words, the window
is incremented until at least one content word is
present in the resulting context. This enables one
to find sensible replacements that fit the context
well. The contexts for T are:
Left-context (L): hitting three
Right-context (R): with one arrow
The above contexts are further processed to
generalize the numbers by a NUMBER tag
to produce more candidate replacements. The
resulting contexts are now:
Left-context (L): hitting NUMBER
Right-context (R): with NUMBER arrow
As a single L ? R context is used, a far
smaller number of replacements are extracted.
3.3 Finding Candidate replacements
The monolingual corpus (ML) of the source lan-
guage is used to find words and phrases (Xk) that
fitLXkR i.e., withL as its left context and/orR as
its right context. The maximum length for Xk is
set to 3 currently. The replacements are further fil-
tered to obtain only those replacements that con-
tain at least one content word. As illustrated ear-
lier, the resulting replacement candidates are not
necessarily synonyms.
3.4 Features
A local context of two to three words to the left
of an OOV/rare-word (wordi) and two to three
words to the right of wordi contain sufficient
clues for the word,wordi. Hence, local contextual
features are used to score each of the replacement
candidates (Xi,k) of wordi. Each Xi,k extracted
in the previous step is converted to a feature vector
containing 11 contextual features. Certainly more
features can be extracted with additional knowl-
edge sources. The framework allows adding more
features, but for the present results, only these 11
features were used.
As our aim is to assist the translation system in
finding longer target phrasal matches, the features
are constructed from the occurrence statistics of
Xi,k from the bilingual training corpus (BL). If a
candidate replacement does not occur in the BL,
then it is removed from the list of possible replace-
ment candidates.
Frequency counts for the features of a partic-
ular replacement, Xi,k, extracted in the context
of Li,?2Li,?1 (two preceding words of wordi)
and Ri,+1Ri,+2 (two following words of wordi)
(the remaining words in the left and right context
of wordi are not used for feature extraction) are
obtained as follows:
f1: frequency of Xi,kRi,+1
f2: frequency of Li,?1Xi,k
f3: frequency of Li,?1Xi,kRi,+1
f4: frequency of Li,?2Li,?1Xi,k
f5: frequency of Xi,kRi,+1Ri,+2
f6: frequency of Li,?2Li,?1Xi,kRi,+1
323
f7: frequency of Li,?1Xi,kRi,+1Ri,+2
f8: frequency of Li,?2Li,?1Xi,kRi,+1Ri,+2
f9: frequency of Xi,k in ML
f10: frequency of Xi,k in BL
f11: number of feature values (f1, ..f10) > 0
f11 is a vote feature which counts the num-
ber of features (f1 ... f10) that have a value
greater than zero. The features are normalized
to fall within [0, 1]. The sentences in ML, BL
and test data are padded with two begin markers
and two end markers for obtaining counts for
OOV/rare-words that appear at the beginning or
end of a test sentence.
3.5 Representation
Before we go on to explaining the lattice repre-
sentation, we would like to make a small clarifica-
tion in the terminalogy used. In the MT commu-
nity, a lattice usually refers to the list of possible
partially-overlapping target translations for each
possible source n?gram phrase in the input sen-
tence. Since we are using the term lattice to also
refer to the possible paths through the input sen-
tence, we will call the lattice used by the decoder,
the ?decoding lattice?. The lattice obtained from
the input sentence representing possible replace-
ment candidates will be called the ?input lattice?.
An input lattice (Figure 1) is constructed with
a beam of replacements for the OOV and rare
words. Each replacement candidate is given a
score (Eqn 1) indicating the confidence that a suit-
able replacement is found. The numbers in Fig-
ure 1 indicate the start and end indices (based
on character counts) of the words in the test sen-
tence. In T , two replacements were found for the
word ?hawks?: ?homers? and ?birds?. However,
?homers? was not found in the BL and hence, it
was removed from the replacement list.
The input lattice also includes the OOV word
with a low score (Eqn 2). This allows the EBMT
system to also include the OOV/rare-word dur-
ing decoding. In the Translation Model of the
EBMT system, this test lattice is matched against
the source sentences in the bilingual training cor-
pus. The matching process would now also look
for phrases with ?birds? and not just ?hawks?.
When a match is found, the corresponding trans-
  
T?:?????a?mobile?base?,?hitting?three?? hawks?with?one?arrow?.....input?lattice:0? 0? (???a???)1? 6? (???mobile???)7? 10? (???base???)11? 11? (??,??)12? 18? (???hitting???)13? 17? (???three???)18? 22? (???hawks?????0.0026)18? 22? (???birds???????0.9974)23? 26? (???with???)27? 29? (???one???)30? 34? (???arrow???)??????????????
Figure 1: Lattice of the input sentence T contain-
ing replacements for OOV words.
  
OOV/Rare word Candidate ReplacementsSpelling errorskrygyzstan kyrgyzstan,...
yusukuni yasukuni,..
kilomaters kilometers, miles, km, ...Coordinate termssomoa india, turkey, germany, russia, japan,...
ear body, arms, hands, feet, mind, car, ...
buyers dealer, inspector, the experts, smuggler,.Synonymsplummet drop, dropped, fell, ....Synonyms and Antonymsoptimal worse, better, minimal,....
Figure 2: Sample English candidate replacements
obtained.
lation in the target language is obtained through
sub-sentential alignment (Section 3.7). The scores
on the input lattice are later used by the decoder
(Section 3.7). Each replacement Xi,k for the
OOV/rare-word (wordi) is scored with a logistic
function (Bishop, 2006) to convert the dot product
of the features and weights (~? ? ~fi,k) to a score be-
tween 0 and 1 (Eqn 1 and Eqn 2).
p?(Xi,k|wordi) =
exp(~?? ~fi,k)
1+
?
j=1...S exp(~?? ~fi,j)
(1)
324
p?(wordi) =
1
1 +
?
j=1...S exp(~? ? ~fi,j)
(2)
where, ~fi,j is the feature vector for the jth replace-
ment candidate of wordi, S is the number of re-
placements, ~? is the weight vector indicating the
importance of the corresponding features.
3.6 Tuning feature weights
We would like to select those feature weights (~?)
which would lead to the least expected loss in
translation quality (Eqn 3). ?log(BLEU) (Pap-
ineni et al, 2002) is used to calculate the expected
loss over a development set. As this objective
function has many local minima and is piecewise
constant, the surface is smoothed using the L2-
norm regularization. Powell?s algorithm (Powell,
1964) with grid-based line optimization is used to
find the best weights. 7 different random guesses
are used to initialize the algorithm.
min
?
E?[L(ttune)] + ? ? ||?||2 (3)
The algorithm assumes that partial derivates of
the function are not available. Approximations of
the weights (?1, ..?N ) are generated successively
along each of the N standard base vectors. The
procedure is iterated with a stopping criteria based
on the amount of change in the weights and the
change in the loss. A cross-validation set (in ad-
dition to the regularization term) is used to pre-
vent overfitting at the end of each iteration of the
Powell?s algorithm. This process is repeated with
different values of ? , as in Deterministic Anneal-
ing (Rose, 1998). ? is initialized with a high value
and is halved after each process.
3.7 System Description
The EBMT system finds phrasal matches for the
test (or input) sentence from the source side of
the bilingual corpus. The corresponding tar-
get phrasal translations are obtained through sub-
sentential alignment. When an input lattice is
given instead of an input sentence, the system per-
forms the same matching process for all possible
phrases obtained from the input lattice. Hence,
the system also finds matches for source phrases
that contain the replacements for the OOV/rare-
word. Only the top C ranking replacement candi-
? ?
??????a????mobile??base???,??hitting???three???hawks????with???????one???????arrow??....????????????????????? ?????birds
       ?? ??   ? ?  ? ?
? ?? ?,?? ???? hawks
???three?birds??
??
Decoding?Lattice
 ? ? ?????three?birds?with?one?arrow??
Figure 3: Lattice containing possible phrasal tar-
get translations for the test sentence T .
dates for every OOV/rare word are used in build-
ing the input lattice. The optimal value of C was
empirically found to be 2. On examining the ob-
tained input lattices, the proposed method found
replacements for at the most 3 OOV/rare words in
each test sentence (Section 4). Hence, the number
of possible paths through the input lattice is not
substantially large.
The target translations of all the source phrases
are placed on a common decoding lattice. An
example of a decoding lattice for example T is
given in Figure 3. The system is now able to find
longer matches (? three birds with one arrow ?
and ? three birds ?) which was not possible earlier
with the OOV word, ?hawks?. The local order-
ing information between the translations of ?three
birds? and ?with one arrow? is well captured due
to the retrieval of the longer source phrasal match,
?three birds with one arrow?. Our ultimate goal
is to obtain translations for such longer n?gram
source phrases boosting the confidence of both the
translation model and the language model.
The decoder used in this paper (Brown, 2003)
works on this decoding lattice of possible
phrasal target translations (or fragments) for
source phrases present in the input lattice to gen-
erate the target translation. Similar to Pharaoh
(Koehn et al, 2003), the decoder uses multi-
level beam search with a priority queue formed
based on the number of source words translated.
Bonuses are given for paths that have overlapping
fragments. The total score (TS) for a path (Eqn
4) through the translation lattice is the arithmetic
average of the scores for each target word in the
325
path. The EBMT engine assigns each candidate
phrasal translation a quality score computed as
a log-linear combination of alignment score and
translation probability. The alignment score indi-
cates the engine?s confidence that the right target
translation has been chosen for a source phrase.
The translation probability is the proportion of
times each distinct alternative translation was en-
countered out of all the translations. If the path
includes a candidate replacement, the log of the
score, p?(wi), given for a candidate replacement
is incorporated into TS as an additional term with
a weight wt5.
TS = 1t
t?
i=1
[wt1 log(bi) + wt2 log(peni)
+wt3 log(qi) + wt4 log(P (wi|wi?2, wi?1))
+1I(wi=replacement)wt5 log(p?(wi)) ] (4)
where, t is the number of target words in the path,
wtj indicates the importance of each score, bi is
the bonus factor given for long phrasal matches,
peni is the penalty factor for source and target
phrasal-length mismatches, qi is the quality score
and P (wi|wi?2, wi?1) is the LM score. The pa-
rameters of the EBMT system (wtj) are tuned on
a development set.
The target translation is postprocessed to in-
clude the translation of the OOV/rare-word with
the help of the best path information from the
decoder. In the case of OOV words, since the
translation is not available, the OOV word is put
back into the final output translation in place of
the translation of its replacement. In the output
translation of the test example T , the translation
of ?birds? is replaced by the word, ?hawks?. For
rare words, knowing that the translation of the rare
word may not be correct (due to poor alignment
statistics), the target translation of the replacement
is replaced by the translation of the rare word
obtained from the dictionary. If the rare word
has multiple translations, the translation with the
highest score is chosen.
4 Experimental Setup
As we are interested in improving the per-
formance of low-resource EBMT, the English-
Haitian (Eng-Hai) newswire data (Haitian Cre-
ole, CMU, 2010) containing 15,136 sentence-
pairs was used. To test the performance in other
languages, we simulated sparsity by choosing less
training data for English-Chinese (Eng-Chi). For
the Eng-Chi experiments, we extracted 30k train-
ing sentence pairs from the FBIS (NIST, 2003)
corpus. The data was segmented using the Stan-
ford segmenter (Tseng et al, 2005). Although
we are only interested in small data sets, we also
performed experiments with a larger data set of
200k. 5-gram Language Models were built from
the target half of the training data with Kneser-
Ney smoothing. For the monolingual English cor-
pus, 9 million sentences were collected from the
Hansard Corpus (LDC, 1997) and FBIS data.
EBMT system without OOV/rare-word han-
dling is chosen as the Baseline system. The pa-
rameters of the EBMT system are tuned with 200
sentence pairs for both Eng-Chi and Eng-Hai. The
tuned EBMT parameters are used for the Base-
line system and the system with OOV/rare-word
handling. The feature weights for the proposed
method are then tuned on a seperate development
set of 200 sentence-pairs with source sentences
containing at least 1 OOV/rare-word. The cross-
validation set for this purpose is made up of 100
sentence-pairs. In the OOV case, 500 sentence
pairs containing at least 1 OOV word are used for
testing. For the rare word handling experiments,
500 sentence pairs containing at least 1 rare word
are used for testing.
To assess the translation quality, 4-gram word-
based BLEU is used for Eng-Hai and 3-gram
word-based BLEU is used for Eng-Chi. Since
BLEU scores have a few limitations, the NIST and
TER metrics are also used. The test data used for
comparing the system handling OOV words and
the Baseline (without OOV word handling) is dif-
ferent from the test data used for comparing the
system handling rare words and the Baseline sys-
tem (without rare word handling). In the former
case, the test data handles only OOV words and
in the latter, the test data only handles rare words.
Hence, the test data for both the cases do not com-
pletely overlap. As we are interested in determin-
ing whether handling rare words in test sentences
is useful, we keep both the test data sets seper-
ate and assess the improvements obtained by only
326
OOV/Rare system TER BLEU NISTOOV Baseline 77.89 18.61 4.8525Handling OOV 76.95 19.32 4.9664Rare Baseline 74.23 22.84 5.3803Handling Rare 74.02 23.12 5.4406
Table 1: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Hai.
handling OOV words and by only handling rare
words over their corresponding Baselines. As fu-
ture work, it would be interesting to create one test
data set to handle both OOV and rare words to see
the overall gain.
The test set is further split into 5 files and the
Wilcoxon (Wilcoxon, 1945) Signed-Rank test is
used to find the statistical significance.
5 Results
Sample replacements found are given in Figure 2.
For both Eng-Chi and Eng-Hai experiments, only
the top C ranking replacement candidates were
used. The value of C was tuned on the develop-
ment set and the optimal value was found to be
2. Translation quality scores obtained on the test
data with 30k and 200k Eng-Chi training data sets
are given in Table 2. Table 1 shows the results
obtained on Eng-Hai. Statistically significant im-
provements (p < 0.0001) were seen by handling
OOV words as well as rare words over their cor-
responding baselines.
As the goal of the approach was to obtain longer
target phrasal matches, we counted the number of
n-grams for each value of n present on the de-
coding lattice in the 30k Eng-Chi case. The sub-
plots: A and B in Figure 4, shows the frequency
of n-grams for higher values of n (for n > 5)
when handling OOV and rare words. The plots
clearly show the increase in number of longer tar-
get phrases when compared to the phrases ob-
tained by the baseline systems.
Since the BLEU and NIST scores were com-
puted only up to 3-grams, we further found the
number of n-gram matches (for n > 3) in the
final translation of the test data with respect to
the reference translations (subplots: C and D).
As expected, a larger number of longer n?gram
matches were found. For the OOV case, matches
6 7 8 9 10 11 12 13 14 150
20004000
60008000
1000012000
n?gram
#n?gr
ams o
n the 
decod
ing la
ttice
 
 BaselineHandling OOV words
6 7 8 9 10 11 12 13 14 150
5000
10000
15000
n?gram 
 BaselineHandling Rare words
4 5 6 7 8 9 10 110
50
100
150
n?gram 
 BaselineHandling OOV words
4 5 6 7 8 9 10 110
10
20
30
40
n?gram
#corre
ctly tr
ansla
ted n?
grams
 
 BaselineHandling Rare words
A C
B D
Figure 4: A, B: number of n-grams found for in-
creasing values of n on the decoding lattice. C, D:
number of target n-gram matches for increasing
values of n with respect to the reference transla-
tions.
OOV/Rare Training system TER BLEU NISTdata sizeOOV 30k Baseline 82.03 14.12 4.118630k Handling OOV 80.97 14.78 4.1798200k Baseline 79.41 19.90 4.6822200k Handling OOV 77.66 20.50 4.7654Rare 30k Baseline 82.09 15.36 4.362630k Handling Rare 80.02 16.03 4.4314200k Baseline 78.04 20.96 4.9647200k Handling Rare 77.35 21.17 5.0122
Table 2: Comparison of translation scores of the
Baseline system and system handling OOV and
Rare words for Eng-Chi.
up to 9-grams were found where the baseline only
found matches up to 8-grams.
6 Conclusion and Future Work
A simple approach to improve translation quality
by handling both OOV and rare words was pro-
posed. The framework allowed scoring and rank-
ing each replacement candidate efficiently.
The method was tested on two language pairs
and statistically significant improvements were
seen in both cases. The results showed that rare
words also need to be handled to see improve-
ments in translation quality.
In this paper, the proposed method was only ap-
plied on words, as future work we would like to
extend it to OOV and rare-phrases as well.
327
References
R. Barzilay and K. McKeown 2001. Extracting para-
phrases from a parallel corpus. In Proceedings
of the 39th Annual Meeting of the Association for
Computaional Linguistics, pp. 50-57.
C. M. Bishop 2006. Pattern Recognition and Machine
Learning, Springer.
R. D. Brown, R. Hutchinson, P. N. Bennett, J. G. Car-
bonell, P. Jansen. 2003. Reducing Boundary Fric-
tion Using Translation-Fragment Overlap. In Pro-
ceedings of The Ninth Machine Translation Summit,
pp. 24-31.
R. D. Brown. 2000. Automated Generalization of
Translation Examples. In Proceedings of The Inter-
national Conference on Computational Linguistics,
pp. 125-131.
C. Callison-Burch, P. Koehn and M. Osborne. 2006.
Improved Statistical Machine Translation Using
Paraphrases. In Proceedings of The North Ameri-
can Chapter of the Association for Computational
Linguistics, pp. 17-24.
J. Carbonell, S. Klien, D. Miller, M. Steinbaum, T.
Grassiany and J. Frey. 2006. Context-Based Ma-
chine Translation Using Paraphrases. In Proceed-
ings of The Association for Machine Translation in
the Americas, pp. 8-12.
N. Habash. 2008. Four Techniques for On-
line Handling of Out-of-Vocabulary Words in
Arabic-English Statistical Machine Translation. In
Proceedings of Association for Computational
Linguistics-08: HLT, pp. 57-60.
Public release of Haitian Creole lan-
guage data by Carnegie Mellon, 2010.
http://www.speech.cs.cmu.edu/haitian/
Z. Harris. 1954. Distributional structure. Word,
10(23): 146-162.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder for
Phrase-Based Statistical Machine Translation Mod-
els. The Association for Machine Translation.
P. Koehn, F. J. Och and D. Marcu. 2003. Statis-
tical Phrase-Based Translation. In Proceedings of
HLT:The North American Chapter of the Associa-
tion for Computational Linguistics.
P. Koehn 2002 Europarl: A multilingual corpus
for evaluation of machine translation. Unpublished,
http://www.isi.edu/koehn/publications/europarl/
Linguistic Data Consortium. 1997 Hansard Corpus of
Parallel English and French. Linguistic Data Con-
sortium, December. http://www.ldc.upenn.edu/
Y. Marton, C. Callison-Burch and P. Resnik. 2009.
Improved Statistical Machine Translation Using
Monolingually-derived Paraphrases. In Proceed-
ing of The Empirical Methods in Natural Language
Processing, pp. 381-390.
NIST. 2003. Machine translation evaluation.
http://nist.gov/speech/tests/mt/
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of The Associa-
tion for Computational Linguistics. pp. 311-318.
M. Popovic? and H. Ney. 2004. Towards the use of
Word Stems and Suffixes for Statistical Machine
Translation. In Proceedings of The International
Conference on Language Resources and Evalua-
tion.
M. J. D. Powell. 1964. An efficient method for find-
ing the minimum of a function of several variables
without calculating derivatives Computer Journal.
Volume 7, pp. 152-162.
K. Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related
optimization problems. In Proceedings of The In-
stitute of Electrical and Electronics Engineers, pp.
2210-2239.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky and C.
Manning. 2005. A Conditional Random Field
Word Segmenter. Fourth SIGHAN Workshop on
Chinese Language Processing.
D. Vilar, J. Peter, and H. Ney. 2007. Can we translate
letters? In Proceedings of Association Computa-
tional Linguistics Workshop on SMT, pp. 33-39.
M. Yang and K. Kirchhoff. 2006. Phrase-based
back-off models for machine translation of highly
inflected languages. In Proceedings of European
Chapter of the ACL, 41-48.
F. Wilcoxon. 1945. Individual comparisons by
ranking methods. Biometrics, 1, 80-83. tool:
http://faculty.vassar.edu/lowry/wilcoxon.html
328
Proceedings of the SIGDIAL 2014 Conference, pages 218?227,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Learning to Re-rank for Interactive Problem Resolution and Query
Refinement
Rashmi Gangadharaiah
IBM Research,
India Research Lab,
Bangalore, KA, India
rashgang@in.ibm.com
Balakrishnan Narayanaswamy and Charles Elkan
Department of CSE,
University of California, San Diego
La Jolla, CA, USA
{muralib, elkan}@cs.ucsd.edu
Abstract
We study the design of an information re-
trieval (IR) system that assists customer
service agents while they interact with
end-users. The type of IR needed is
difficult because of the large lexical gap
between problems as described by cus-
tomers, and solutions. We describe an
approach that bridges this lexical gap by
learning semantic relatedness using tensor
representations. Queries that are short and
vague, which are common in practice, re-
sult in a large number of documents be-
ing retrieved, and a high cognitive load
for customer service agents. We show
how to reduce this burden by providing
suggestions that are selected based on the
learned measures of semantic relatedness.
Experiments show that the approach offers
substantial benefit compared to the use of
standard lexical similarity.
1 Introduction
Information retrieval systems help businesses and
individuals make decisions by automatically ex-
tracting actionable intelligence from large (un-
structured) data (Musen et al., 2006; Antonio
Palma-dos Reis, 1999). This paper focuses on the
application of retrieval systems in a contact cen-
ters where the system assists agents while they are
helping customers with problem resolution.
Currently, most contact center information re-
trieval use (web based) front-ends to search en-
gines indexed with knowledge sources (Holland,
2005). Agents enter queries to retrieve documents
related to the customer?s problem. These sources
are often incomplete as it is unlikely that all pos-
sible customer problems can be identified before
product release. This is particularly true for re-
cently released and frequently updated products.
One approach, which we build on here, is to mine
problems and resolutions from online discussion
forums Yahoo! Answers
1
Ubuntu Forums
2
and
Apple Support Communities
3
. While these often
provide useful solutions within hours or days of
a problem surfacing, they are semantically noisy
(Gangadharaiah and Narayanaswamy, 2013).
Most contact centers and agents are evaluated
based on the number of calls they handle over a
period (Pinedo et al., 2000). As a result, queries
entered by agents into the search engine are usu-
ally underspecified. This, together with noise in
the database, results in a large number of docu-
ments being retrieved as relevant documents. This
in turn, increases the cognitive load on agents, and
reduces the effectiveness of the search system and
the efficiency of the contact center. Our first task
in this paper is to automatically make candidate
suggestions that reduce the search space of rel-
evant documents in a contact center application.
The agent/user then interacts with the system by
selecting one of the suggestions. This is used to
expand the original query and the process can be
repeated. We show that even one round of inter-
action, with a small set of suggestions, can lead to
high quality solutions to user problems.
In query expansion, the classical approach is to
automatically find suggestions either in the form
of words, phrases or similar queries (Kelly et al.,
2009; Feuer et al., 2007; Leung et al., 2008).
These can be obtained either from query logs or
based on their representativeness of the initial re-
trieved documents (Guo et al., 2008; Baeza-yates
et al., 2004). The suggestions are then ranked ei-
ther based on their frequencies or based on their
similarity to the original query (Kelly et al., 2009;
Leung et al., 2008). For example, if suggestions
and queries are represented as term vectors (e.g.
1
http://answers.yahoo.com/
2
http://ubuntuforums.org/
3
https://discussions.apple.com/
218
term frequency-inverse document frequency or tf-
idf) their similarity may be determined using simi-
larity measures such as cosine similarity or inverse
of euclidean distance (Salton and McGill, 1983).
However, in question-answering and problem-
resolution domains, and in contrast to traditional
Information Retrieval, most often the query and
the suggestions do not have many overlapping
words. This leads to low similarity scores, even
when the suggestion is highly relevant. Consider
the representative example in Table 1, taken from
our crawled dataset. Although the suggestions,
?does not support file transfer?, ?connection not
stable?, ?pairing failed? are highly relevant for the
problem of ?Bluetooth not working?, their lexi-
cal similarity score is zero. The second task that
this paper addresses is how to bridge this lexical
chasm between the query and the suggestions. For
this, we learn a measure of semantic-relatedness
between the query and the suggestions rather than
defining closeness based on lexical similarity.
Query Bluetooth not working .
Suggestions devices not discovered,
bluetooth greyed out,
bluetooth device did not respond,
does not support file transfer,
connection not stable,
pairing failed
Table 1: Suggestions for the Query or customer?s
problem, ?Bluetooth not working?.
The primary contributions of this paper are that:
? We show how tensor methods can be used
to learn measures of question-answer or
problem-resolution similarity. In addition,
we show that these learned measures can
be used directly with well studied classifica-
tion techniques like Support Vector Machines
(SVMs) and Logistic Classifiers to classify
whether suggestions are relevant. This results
in substantially improved performance over
using conventional similarity metrics.
? We show that along with the learned similar-
ity metric, a data dependent Information Gain
(which incorporates knowledge about the set
of documents in the database) can be used as
a feature to further boost accuracy.
? We demonstrate the efficacy of our approach
on a complete end-to-end problem-resolution
system, which includes crawled data from
online forums and gold standard user inter-
action annotations.
2 System outline
As discussed in the Introduction, online discus-
sion forums form a rich source of problems and
their corresponding resolutions. Thread initiators
or users of a product facing problems with their
product post in these forums. Other users post
possible solutions to the problem. At the same
time, there is noise due to unstructured content,
off-topic replies and other factors. Our interac-
tion system has two phases, as shown in Figure
1. The offline phase attempts to reduce noise in
the database, while the online phase assists users
deal with the cognitive overload caused by a large
set of retrieved documents. In our paper, threads
form the documents indexed by the system.
The goals of the offline phase are two-fold.
First, to reduce the aforementioned noise in the
database, we succinctly represent each document
(i.e., a thread in online discussion forums) by its
signature, which is composed of units extracted
from the first post of the underlying thread that
best describe the problem discussed in the thread.
Second, the system makes use of click-through
data, where users clicked on relevant suggestions
for their queries to build a relevancy model. As
mentioned before, the primary challenge is to
build a model that can identify units that are se-
mantically similar to a given query.
In the online phase, the agent who acts as the
mediator between the user and the Search Engine
enters the user?s/customer?s query to retrieve rele-
vant documents. From these retrieved documents,
the system then obtains candidate suggestions and
ranks these suggestions using the relevancy model
built in the offline phase to further better under-
stand the query and thereby reduce the space of
documents retrieved. The user then selects the
suggestion that is most relevant to his query. The
retrieved documents are then filtered displaying
only those documents that contain the selected
suggestion in their signatures. The process con-
tinues until the user quits.
2.1 Signatures of documents
In the offline phase, every document (correspond-
ing to a thread in online discussion forums) is
represented by units that best describe a problem.
We adopt the approach suggested in (Gangadhara-
219
iah and Narayanaswamy, 2013) to automatically
generate these signatures from each discussion
thread. We assume that the first post describes
the user?s problem, something we have found to
be true in practice. From the dependency parse
trees of the first posts, we extract three types of
units (i) phrases (e.g., sync server), (ii) attribute-
values (e.g., iOS, 4) and (iii) action-attribute tuples
(e.g., sync server: failed). Phrases form good base
problem descriptors. Attribute-value pairs provide
configurational contexts to the problem. Action-
attribute tuples, as suggested in (Gangadharaiah
and Narayanaswamy, 2013), capture segments of
the first post that indicate user wanting to perform
an action (?I cannot hear notifications on blue-
tooth?) or the problems caused by a user?s action
(?working great before I updated?). These make
them particularly valuable features for problem-
resolution and question-answering.
2.2 Representation of Queries and
Suggestions
Queries are represented as term vectors using the
term frequency-inverse document frequency (tf-
idf) representation forming the query space. The
term frequency is defined as the frequency with
which word appears in the query and the inverse
document frequency for a word is defined as the
frequency of queries in which the word appeared.
Similarly, units are represented as tf-idf term vec-
tors from the suggestion space. Term frequency in
the unit space is defined as the number of times
a word appears in the unit and its inverse docu-
ment frequency is defined in terms of the number
of units in which the word appeared. Since the
vocabulary used in the queries and documents are
different, the representations for queries and units
belong to different spaces of different dimensions.
For every query-unit pair, we learn a measure
of similarity as explained in Section 4. Addi-
tionally, we use similarity features based on co-
sine similarity between the query and the unit un-
der consideration. We also consider an additional
feature based on information gain (Gangadhara-
iah and Narayanaswamy, 2013). In particular, if
S represents the set all retrieved documents, S
1
is
a subset of S (S
1
? S) containing a unit unit
i
and
S
2
is a subset of S that does not contain unit
i
,
information gain with unit
i
is,
Gain(S, unit
i
) = E(S)?
|S
1
|
|S|
E(S
1
)?
|S
2
|
|S|
E(S
2
) (1)
E(S) =
?
k=1,...|S|
?p(doc
k
)log
2
p(doc
k
). (2)
The probability for each document is based on its
rank in the retrieved of results,
p(doc
j
) =
1
rank(doc
j
)
?
k=1,...|S|
1
rank(doc
k
)
. (3)
We crawled posts and threads from online forums
for the products of interest, as detailed in Sec-
tion 5.1, and these form the documents. We used
trial interactions and retrievals to collect the click-
though data, which we used as labeled data for
similarity metric learning. In particular, labels in-
dicate which candidate units were selected as rel-
evant suggestions by a human annotator. We now
explain our training (offline) and testing (online)
phases that use this data in more detail.
2.3 Training
The labeled (click-through) data for training the
relevance model is collected as follows. Anno-
tators were given pairs of queries. Each pair is
composed of an underspecified query and a spe-
cific query (Section 5.1 provides more informa-
tion on the creation of these queries). An un-
derspecified query is a query that reflects what a
user/agent typically enters into the system, and the
corresponding specific query is full-specified ver-
sion of the underspecified query. Annotators were
first asked to query the search engine with each
underspecified query. We use the Lemur search
engine (Strohman et al., 2004). From the resulting
set of retrieved documents, the system uses the in-
formation gain criteria (as given in (1) below) to
rank and display to the annotators the candidate
suggestions (i.e., the units that appear in the signa-
tures of the retrieved documents). Thus, our sys-
tem is bootstrapped using the information gain cri-
terion. The annotators then selects the candidate
suggestion that is most relevant to the correspond-
ing specific query. The interaction with the system
continues until the annotators quit.
We then provide a class label for each unit based
on the collected click-through information. In par-
ticular, if a unit s ? S(x) was clicked by a user for
his query x, from the list S we provide a + la-
bel to indicate that the unit is relevant suggestion
for the query. Similarly, for all other units that are
never clicked by users for x are labeled as?. This
forms the training data for the system. Details on
220
Forum Discussion  Threads Unit Extraction Suggestion units  for first posts 
Search Engine query results Interaction Module Finds suggestions 
Candidate  Suggestions 
User clicks on  (units, query) Learn Relevance Model ! Offline 
Online 
Figure 1: Outline of our interactive query refine-
ment system for problem resolution
the feature extraction and how the model is created
is given in Section 3.
2.4 Testing
In the online phase, the search engine retrieves
documents for the user?s query x
?
. Signatures for
the retrieved documents form the initial space of
candidate units. As done in training, for every pair
of x
?
and unit the label is predicted using the model
built in the training phase. Units that are predicted
as + are shown to the user. When a user clicks
on his most relevant suggestion, the retrieved re-
sults are filtered to show only those documents that
contain the suggestion (i.e., in its signature). This
process continues until the user quits.
3 Model
We consider underspecified queries x ? R
x
d
and
units y ? R
y
d
. Given an underspecified query x
we pass it through a search engine, resulting in a
list of results S(x).
As explained in Section 2.3, our training data
consists of labels r(x, y) ? +1,?1 for each
under-specified query, y ? S(x). r(x, y) = +1
if the unit is labeled a relevant suggestion and
r(x, y) = ?1 if it is not labeled relevant. Units
are relevant or not based on the final query, and
not just y, a distinction we expand upon below.
At each time step, our system proposes a list
Z(x) of possible query refinement suggestions z
to the user. The user can select one or none of
these suggestions. If the user selects z, only those
documents that contain the suggestion (i.e., in its
signature) are shown to the user, resulting in a fil-
tered set of results, S(x+ z).
This process can be repeated until a stopping
criterion is reached. Stopping criterion include the
size of the returned list is smaller than some num-
ber |S(x + z)| < N , in which case all remain-
ing documents are returned. Special cases include
when only one document is returned N = 1. We
will design query suggestions so that |S(x+z)| >
0. Another criterion we use is to return all remain-
ing documents after a certain maximum number of
interactions or until the user quits.
4 Our Approach
We specify our algorithm using a tensor notation.
We do this since tensors appear to subsume most
of the methods applied in practice, where different
algorithms use slightly different costs, losses and
constraints. These ideas are strongly motivated by,
but generalize to some extent, suggestions for this
problem presented in (Elkan, 2010).
For our purposes, we consider tensors as multi-
dimensional arrays, with the number of dimen-
sions defined as the order of the tensor. An M
order tensor X ? R
I
1
?I
2
...I
M
. As such tensors
subsume vectors (1st order tensors) and matrices
(2nd order tensors). The vectorization of a ten-
sor of order M is obtained by stacking elements
from the M dimensions into a vector of length
I
1
? I
2
? . . .? I
M
in the natural way.
The inner product of two tensors is defined as
?X,W? =
I
1
?
i
1
I
2
?
i
2
. . .
I
M
?
i
M
x
i
1
w
i
1
x
i
2
w
i
2
. . . x
i
M
w
i
M
(4)
Analogous to the definition for vectors, the
(Kharti-Rao) outer product A = X ?W of two
tensors X and W has A
ij
= X
i
W
j
where i and j
run over all elements of X and W . Thus, if X is
of order M
X
and W of order M
W
, A is of order
M
A
= M
X
+M
W
.
The particular tensor we are interested in is a
2-D tensor (matrix) X which is the outer product
of query and unit pairs (Feats). In particular, for a
query x and unit y, X
i,j
= x
i
y
j
.
Given this representation, standard classifica-
tion and regression methods from the machine
learning literature can often be extended to deal
with tensors. In our work we consider two clas-
sifiers that have been successful in many applica-
tions, logistic regression and support vector ma-
chines (SVMs) (Bishop, 2006).
221
In the case of logistic regression, the conditional
probability of a reward signal r(X) = r(x, y) is,
p(r(X) = +1) =
1
1 + exp(??X,W?+ b)
(5)
The parameters W and b can be obtained by min-
imizing the log loss L
reg
on the training data D
L
reg
(W, b) = (6)
?
(X,r(X))?D
log(1 + exp(?r(X)?X,W?+ b)
For SVMs with the hinge loss we select param-
eters to minimize L
hinge
,
L
hinge
(W, b) = ||X||
2
F
+ (7)
?
?
(X,r(X))?D
max[0, 1? (r(X)?X,W?+ b)]
where ||X||
F
is the Frobenius norm of tensor X.
Given the number of parameters in our system
(W, b) to limit overfitting, we have to regularize
these parameters. We use regularizers of the form
?(W, b) = ?
W
||W||
F
(8)
such regularizes have been successful in many
large scale machine learning tasks including
learning of high dimensional graphical models
(Ravikumar et al., 2010) and link prediction
(Menon and Elkan, 2011).
Thus, the final optimization problem we are
faced with is of the form
min
W,b
L(W, b) + ?(W, b) (9)
where L is L
reg
or L
hinge
as appropriate. Other
losses, classifiers and regularizers may be used.
The advantage of tensors over their vectorized
counterparts, that may be lost in the notation, is
that they do not lose the information that the dif-
ferent dimensions can (and in our case do) lie in
different spaces. In particular, in our case we use
different features to represent queries and units (as
discussed in Section 2.2) which are not of the same
length, and as a result trivially do not lie in the
same space.
Tensor methods also allow us to regularize the
components of queries and units separately in dif-
ferent ways. This can be done for example by,
i) forcing W = Q
1
Q
2
, where Q
1
and Q
2
are
constrained to be of fixed rank s ii) using trace or
Frobenius norms on Q
1
and Q
2
for separate regu-
larization as proxies for the rank iii) using different
sparsity promoting norms on the rows of Q
1
and
Q
2
iv) weighing these penalties differently for the
two matrices in the final loss function. Note that
by analogy to the vector case, we directly obtain
generalization error guarantees for our methods.
We also discuss the advantage of the tensor
representation above over a natural representation
X = [x; y] i.e. X is the column vector obtained
by stacking the query and unit representations.
Note that in this representation, for logistic regres-
sion, while a change in the query x can change
the probability for a unit P (r(X) = 1) it can-
not change the relative probability of two different
units. Thus, the ordering of all unit remains the
same for all queries. This flaw has been pointed
out in the literature in (Vert and Jacob, 2008) and
(Bai et al., 2009), but was brought to our attention
by (Elkan, 2010).
Finally, we note that by normalizing the query
and unit vectors (x and y), and selecting W = I
(the identity matrix) we can recover the cosine
similarity metric (Elkan, 2010). Thus, our rep-
resentation is atleast as accurate and we show
that learning the diagonal and off-diagonal com-
ponents of W can substantially improve accuracy.
Additionally, for every (query,unit) we also
compute information gain (IG) as given in (1), and
the lexical similarity (Sim) in terms of cosine sim-
ilarity between the query and the unit as additional
features in the feature vectors.
5 Results and Discussion
To evaluate our system, we built and simulated
a contact center information retrieval system for
iPhone problem resolution.
5.1 Description of the Dataset
We collected data by crawling forum discussion
threads from the Apple Discussion Forum, created
during the period 2007-2011, resulting in about
147,000 discussion threads. The underspecified
queries and specific queries were created as fol-
lows. Discussion threads were first clustered treat-
ing each discussion thread as a data point using a
tf-idf representation. The thread nearest the cen-
troid of the 60 largest clusters were marked as the
?most common? problems.
The first post is used as a proxy for the problem
description. An annotator was asked to then create
222
Underspecified query ?Safari not working?
1. safari:crashes
2. safari:cannot find:server
3. server:stopped responding
4. phone:freezes
5. update:failed
Table 2: Specific Queries generated with the un-
derspecified Query, ?Safari not working?.
a short query (underspecified) from the first post
of each of the 60 selected threads. These queries
were given to the Lemur search engine (Strohman
et al., 2004) to retrieve the 50 most similar threads
from an index built on the entire set of 147,000
threads. The annotator manually analyzed the first
posts of the retrieved threads to create contexts,
resulting in a total 200 specific queries.
We give an example to illustrate the data cre-
ation in Table 2. From an under-specified query
?Safari not working?, the annotator found 5 spe-
cific queries. Two other annotators, were given
these specific queries with the search engine?s
results from the corresponding under-specified
query. They were asked to choose the most rel-
evant results for the specific queries. The intersec-
tion of the choices of the annotators formed our
?gold standard? of relevant documents.
5.2 Results
We simulated a contact center retrieval systems (as
in Figure 1) to evaluate the approach proposed in
this paper. To evaluate the generality of our ap-
proach we conduct experiments with both SVMs
and Logistic Regression. Due to lack of space we
illustrate each result for only one kind of classifier.
5.2.1 Evaluating the Relevance Model
To measure the performance of the relevance
model for predicting the class labels or for finding
the most relevant units towards making the user?s
underspecified query more specific, we performed
the following experiment. 4000 random query-
unit pairs were picked from the training data, col-
lected as explained in Section 2. Since most units
are not relevant for a query, 90% of the pairs be-
longed to the ? class. On average, every spe-
cific query gave rise to 2.4 suggestions. Hence,
predicting ? for all pairs still achieves an error
rate of 10%. This data was then split into vary-
ing sizes of training and test sets. The relevancy
model was then built on the training half and the
classifiers were used to predict labels on the test
0 500 1000 1500 2000 2500 3000 3500 40000
0.01
0.02
0.03
0.04
0.05
0.06
Number of training query?suggestion pairs
Erro
r ra
te
 
 
SimFeats?IG?SimFeats+IG+Sim
Figure 2: Performance with Logistic Regression
using different features and various sizes of Train-
ing and Test sets. Feats-IG-Sim does not use co-
sine similarity (Sim) and information gain (IG).
Feats+IG+Sim considers Sim and IG.
set. Figure 2 shows error rate obtained with logis-
tic regression (a similar trend was observed with
SVMs) on various sizes of the training data and
test data. The plot shows that the model (Feats-
IG-Sim and Feats+IG+Sim) performs significantly
better at predicting the relevancy of units for un-
derspecified queries when compared to just us-
ing cosine similarity (Sim) as a feature. Feats-
IG-Sim does not make use of cosine similarity
as a feature or the information gain feature while
Feats+IG+Sim uses both these features for train-
ing the relevancy model and for predicting the rel-
evancy of units. As expected the performance of
the classifier improves as the size of the training
data is increased.
5.2.2 Evaluating the Interaction Engine
We evaluate a complete system with both the user
(the agent) and the search engine in the loop. We
measure the value of the interactions by an analy-
sis of which results ?rise to the top?. Users were
given a specific query and its underspecified query
along with the results obtained when the under-
specified query was input to the search engine.
They were presented with suggestions that were
predicted + for the underspecified query using
SVMs. The user was asked to select the most ap-
propriate suggestion that made the underspecified
query more specific. This process continues until
the user quits either because he is satisfied with the
retrieved results or does not obtain relevant sug-
gestions from the system. For example, for the
underspecified query in Table 2, one of the pre-
dicted suggestions was, ?server:stopped respond-
223
1 2 3 4 5 6 7 8 9 100
0.5
1
1.5
2
2.5
Size of retrieved list
Me
an 
Ave
rag
e P
rec
isio
n
 
 
Baseline
Feats?IG?SimFeats+IG+Sim
Figure 3: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of MAP at N.
ing?. If the user finds the suggestion relevant, he
clicks on it. The selected suggestion then reduces
the number of retrieved results. We then measured
the relevance of the reduced result, with respect
to the gold standard for that specific query, using
metrics used in IR - MRR, Mean Average Preci-
sion (MAP) and Success at rank N.
Figures 3, 4 and Table 3 evaluate the results ob-
tained with the interaction engine using Feats-IG-
Sim and Feats+IG+Sim. We compared the per-
formance of our algorithms with a Baseline that
does not perform any interaction and is evaluated
based on the retrieved results obtained with the un-
derspecified queries. The models for each of the
systems were trained using query-suggestion pairs
collected from 100 specific queries (data collected
as explained in Section 2). The remaining 100 spe-
cific queries were used for testing. We see that the
suggestions predicted by the classifiers using the
relevancy model indeed improves the performance
of the baseline. Also, adding the IG and Sim fea-
ture further boosts the performance of the system.
Systems MRR
Baseline 0.4218
Feats-IG-Sim 0.9449
Feats+IG+Sim 0.9968
Table 3: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of MRR.
5.3 Related Work
Learning affinities between queries an documents
is a well studied area. (Liu, 2009) provides an ex-
cellent survey of these approaches. In these meth-
1 2 3 4 5 6 7 8 9 100.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Size of retrieved list
Suc
ces
s a
t
 
 
Baseline
Feats?IG?SimFeats+IG+Sim
Figure 4: Comparison of the proposed approach
with respect to the Baseline that does not involve
interaction in terms of Success at N.
ods, there is a fixed feature function ?(x, y) de-
fined between any query-document pair. These
features are then used, along with labeled train-
ing data, to learn the parameters of a model that
can then be used to predict the relevance r(x, y)
of a new query-document pair. The output of the
model can also be used to re-rank the results of a
search engine. In contrast to this class of methods,
we define and parameterize the ? function and
jointly optimize the parameters of the feature map-
ping and the machine learning re-ranking model.
Latent tensor methods for regression and clas-
sification have recently become popular in the im-
age and signal processing domain. Most of these
methods solve an optimization problem similar to
our own (9), but add additional constraints limit-
ing the rank of the learned matrix W either ex-
plicitly or implicit by defining W = Q
1
Q
T
2
, and
defining Q
1
? R
d
x
?d
and Q
2
? R
d
y
?d
. This ap-
proach is used for example in (Pirsiavash et al.,
2009) and more recently in (Tan et al., 2013) (Guo
et al., 2012). While this reduces the number of pa-
rameters to be learned from d
x
d
y
to d(d
x
+ d
y
) it
makes the problem non-convex and introduces an
additional parameter d that must be selected.
This approach of restricting the rank was re-
cently suggested for information retrieval in (Wu
et al., 2013). They look at a regression problem,
using click-through rates as the reward function
r(x, y). In addition, (Wu et al., 2013) does not
use an initial search engine and hence must learn
an affinity function between all query-document
pairs. In contrast to this, we learn a classification
function that discriminates between the true and
false positive documents that are deemed similar
224
by the search engine. This has three beneficial ef-
fects : (i) it reduces the amount of labeled training
data required and the imbalance between the posi-
tive and negative classes which can make learning
difficult (He and Garcia, 2009) and (ii) allows us
to build on the strengths of fast and strong existing
search engines increasing accuracy and decreas-
ing retrieval time and (iii) allows the learnt model
to focus learning on the query-document pairs that
are most problematic for the search engine.
Bilinear forms of tensor models without the
rank restriction have recently been studied for link
prediction (Menon and Elkan, 2011) and image
processing (Kobayashi and Otsu, 2012). Since
the applications are different, there is no prelimi-
nary search engine which retrieves results, making
them ranking methods and ours a re-ranking ap-
proach. Related work in text IR includes (Beefer-
man and Berger, 2000), where two queries are
considered semantically similar if their clicks lead
to the same page. However, the probability that
different queries lead to common clicks of the
same URLs is very small, again increasing the
training data required. Approaches in the past
have also proposed techniques to automatically
find suggestions either in the form of words,
phrases (Kelly et al., 2009; Feuer et al., 2007;
Baeza-yates et al., 2004) or similar queries (Leung
et al., 2008) from query logs (Guo et al., 2008;
Baeza-yates et al., 2004) or based on their prob-
ability of representing the initial retrieved doc-
uments (Kelly et al., 2009; Feuer et al., 2007).
These suggestions are then ranked either based on
their frequencies or based on their closeness to the
query. Closeness is defined in terms of lexical sim-
ilarity to the query. However, most often the query
and the suggestions do not have any co-occurring
words leading to low similarity scores, even when
the suggestion is relevant.
(Gangadharaiah and Narayanaswamy, 2013)
use information gain to rank candidate sugges-
tions. However, the relevancy of the suggestions
highly depends on the relevancy of the initial re-
trieved documents. Our work here addresses the
question of how to bridge this lexical chasm be-
tween the query and the suggestions. For this, we
use semantic-relatedness between the query and
the suggestions as a measure of closeness rather
than defining closeness based on lexical similar-
ity. A related approach to handle this lexical gap
by applying alignment techniques from Statistical
Machine translation (Brown et al., 1993), in par-
ticular by building translation models for infor-
mation retrieval (Berger and Lafferty, 1999; Rie-
zler et al., 2007). These approaches require train-
ing data in the form of question-answer pairs, are
again limited to words or phrases and are not in-
tended for understanding the user?s problem better
through interaction, which is our focus.
6 Conclusions, Discussions and Future
Work
We studied the problem of designing Information
Retrieval systems for interactive problem resolu-
tion. We developed a system for bridging the
large lexical gap between short, incomplete prob-
lem queries and documents in a database of reso-
lutions. We showed that tensor representations are
a useful tool to learn measures of semantic relat-
edness, beyond the cosine similarity metric. Our
results show that with interaction, suggestions can
be effective in pruning large sets of retrieved doc-
uments. We showed that our approach offers sub-
stantial improvement over systems that only use
lexical similarities for retrieval and re-ranking, in
an end-to-end problem-resolution domain.
In addition to the classification losses consid-
ered in this paper, we can also use another loss
term based on ideas from recommender systems,
in particular (Menon and Elkan, 2011). Consider
the matrix T with all training queries as rows and
all units as the columns. If we view the query
refinement problem as a matrix completion prob-
lem, it is natural to assume that this matrix has low
rank, so that T can be written as T = U?V
T
,
where ? is a diagonal matrix and parameter of our
optimization. These can then be incorporated into
the training process by appropriate changes to the
cost and regularization terms.
Another benefit of the tensor representation is
that it can easily be extended to incorporate other
meta-information that may be available. For ex-
ample, if context sensitive features, like the iden-
tity of the agent, are available these can be incor-
porated as another dimension in the tensor. While
optimization over these higher dimensional ten-
sors may be more computationally complex, the
problems are still convex and can be solved ef-
ficiently. This is a direction of future research
we are pursuing. Finally, exploring the power of
information gain type features in larger database
systems is of interest.
225
References
Fatemeh Zahedi Antonio Palma-dos Reis. 1999. De-
signing personalized intelligent financial decision
support systems.
Ricardo Baeza-yates, Carlos Hurtado, and Marcelo
Mendoza. 2004. Query recommendation us-
ing query logs in search engines. In In Interna-
tional Workshop on Clustering Information over the
Web (ClustWeb, in conjunction with EDBT), Creete,
pages 588?596. Springer.
Bing Bai, Jason Weston, David Grangier, Ronan Col-
lobert, Kunihiko Sadamasa, Yanjun Qi, Corinna
Cortes, and Mehryar Mohri. 2009. Polynomial se-
mantic indexing. In NIPS, pages 64?72.
Doug Beeferman and Adam Berger. 2000. Agglomer-
ative clustering of a search engine query log. In Pro-
ceedings of the Sixth ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, KDD ?00, pages 407?416, New York, NY, USA.
ACM.
Adam Berger and John Lafferty. 1999. Information
retrieval as statistical translation. In Proceedings of
the 22Nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?99, pages 222?229, New York,
NY, USA. ACM.
Christopher M Bishop. 2006. Pattern recognition and
machine learning.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Charles Elkan. 2010. Learning affinity with biliear
models. Unpublished Notes.
Alan Feuer, Stefan Savev, and Javed A. Aslam. 2007.
Evaluation of phrasal query suggestions. In Pro-
ceedings of the Sixteenth ACM Conference on Con-
ference on Information and Knowledge Manage-
ment, CIKM ?07, pages 841?848, New York, NY,
USA. ACM.
Rashmi Gangadharaiah and Balakrishnan
Narayanaswamy. 2013. Natural language query
refinement for problem resolution from crowd-
sourced semi-structured data. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 243?251, Nagoya,
Japan, October. Asian Federation of Natural
Language Processing.
Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng. 2008.
A unified and discriminative model for query refine-
ment. In Sung-Hyon Myaeng, Douglas W. Oard,
Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew
Leong, editors, SIGIR, pages 379?386. ACM.
Weiwei Guo, Irene Kotsia, and Ioannis Patras. 2012.
Tensor learning for regression. Image Processing,
IEEE Transactions on, 21(2):816?827.
Haibo He and Edwardo A Garcia. 2009. Learning
from imbalanced data. Knowledge and Data Engi-
neering, IEEE Transactions on, 21(9):1263?1284.
Alexander Holland. 2005. Modeling uncertainty in
decision support systems for customer call center.
In Computational Intelligence, Theory and Applica-
tions, pages 763?770. Springer.
Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009.
A comparison of query and term suggestion fea-
tures for interactive searching. In Proceedings of the
32Nd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
SIGIR ?09, pages 371?378, New York, NY, USA.
ACM.
Takumi Kobayashi and Nobuyuki Otsu. 2012. Effi-
cient optimization for low-rank integrated bilinear
classifiers. In Computer Vision?ECCV 2012, pages
474?487. Springer.
Kenneth Wai-Ting Leung, Wilfred Ng, and Dik Lun
Lee. 2008. Personalized concept-based clustering
of search engine queries. IEEE Trans. on Knowl.
and Data Eng., 20(11):1505?1518, November.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval, 3(3):225?331.
Aditya Krishna Menon and Charles Elkan. 2011. Link
prediction via matrix factorization. In Machine
Learning and Knowledge Discovery in Databases,
pages 437?452. Springer.
Mark A Musen, Yuval Shahar, and Edward H Short-
liffe. 2006. Clinical decision-support systems.
Michael Pinedo, Sridhar Seshadri, and J George Shan-
thikumar. 2000. Call centers in financial services:
strategies, technologies, and operations. In Cre-
ating Value in Financial Services, pages 357?388.
Springer.
Hamed Pirsiavash, Deva Ramanan, and Charless
Fowlkes. 2009. Bilinear classifiers for visual recog-
nition. In NIPS, pages 1482?1490.
Pradeep Ravikumar, Martin J Wainwright, and John D
Lafferty. 2010. High-dimensional ising model se-
lection using 1-regularized logistic regression. The
Annals of Statistics, 38(3):1287?1319.
Stefan Riezler, Alexander Vasserman, Ioannis
Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007.
Statistical Machine Translation for Query Expan-
sion in Answer Retrieval. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 464?471, Prague, Czech
Republic, June. Association for Computational
Linguistics.
226
Gerard Salton and Michael J McGill. 1983. Introduc-
tion to modern information retrieval.
T. Strohman, D. Metzler, H. Turtle, and W. B. Croft.
2004. Indri: A language model-based search engine
for complex queries. Proceedings of the Interna-
tional Conference on Intelligence Analysis.
Xu Tan, Yin Zhang, Siliang Tang, Jian Shao, Fei Wu,
and Yueting Zhuang. 2013. Logistic tensor re-
gression for classification. In Intelligent Science
and Intelligent Data Engineering, pages 573?581.
Springer.
Jean-Philippe Vert and Laurent Jacob. 2008. Machine
learning for in silico virtual screening and chemical
genomics: new strategies. Combinatorial chemistry
& high throughput screening, 11(8):677.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learn-
ing bilinear model for matching queries and docu-
ments. The Journal of Machine Learning Research,
14(1):2519?2548.
227
