Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 345?352,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Collaborative Framework for Collecting Thai Unknown Words from
the Web
Choochart Haruechaiyasak, Chatchawal Sangkeettrakarn, Pornpimon Palingoon
Sarawoot Kongyoung and Chaianun Damrongrat
Information Research and Development Division (RDI)
National Electronics and Computer Technology Center (NECTEC)
Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand
rdi5@nnet.nectec.or.th
Abstract
We propose a collaborative framework for
collecting Thai unknown words found on
Web pages over the Internet. Our main
goal is to design and construct a Web-
based system which allows a group of in-
terested users to participate in construct-
ing a Thai unknown-word open dictionary.
The proposed framework provides sup-
porting algorithms and tools for automati-
cally identifying and extracting unknown
words from Web pages of given URLs.
The system yields the result of unknown-
word candidates which are presented to
the users for verification. The approved
unknown words could be combined with
the set of existing words in the lexicon
to improve the performance of many NLP
tasks such as word segmentation, infor-
mation retrieval and machine translation.
Our framework includes word segmenta-
tion and morphological analysis modules
for handling the non-segmenting charac-
teristic of Thai written language. To take
advantage of large available text resource
on the Web, our unknown-word boundary
identification approach is based on the sta-
tistical string pattern-matching algorithm.
Keywords: Unknown words, open dictio-
nary, word segmentation, morphological
analysis, word-boundary detection.
1 Introduction
The advent of the Internet and the increasing pop-
ularity of the Web have altered many aspects of
natural language usage. As more people turn to the
Internet as a new communicating channel, the tex-
tual information has increased tremendously and
is also widely accessible. More importantly, the
available information is varied largely in terms of
topic difference and multi-language characteristic.
It is not uncommon to find a Web page written in
Thai lies adjacent to aWeb page written in English
via a hyperlink, or a Web page containing both
Thai and English languages. In order to perform
well in this versatile environment, an NLP system
must be adaptive enough to handle the variation in
language usage. One of the problems which re-
quires special attention is unknown words.
As with most other languages, unknown words
also play an extremely important role in Thai-
language NLP. Unknown words are viewed as one
of the problematic sources of degrading the per-
formance of traditional NLP applications such as
MT (Machine Translation), IR (Information Re-
trieval) and TTS (Text-To-Speech). Reduction in
the amount of unknown words or being able to
correctly identify unknown words in these sys-
tems would help increase the overall system per-
formance.
The problem of unknown words in Thai lan-
guage is perhaps more severe than in English or
other latin-based languages. As a result of the
information technology revolution, Thai people
have become more familiar with other foreign lan-
guages especially English. It is not uncommon to
hear a few English words over a course of con-
versation between two Thai people. The foreign
words along with other Thai named entities are
among the new words which are continuously cre-
ated and widely circulated. To write a foreign
word, the transliterated form of Thai alphabets is
often used. The Royal Institute of Thailand is the
official organization in Thailand who has respon-
345
sibility and authority in defining and approving the
use of new words. The process of defining a new
word is manual and time-consuming as each word
must be approved by a working group of linguists.
Therefore, this traditional approach of construct-
ing the lexicon is not a suitable solution, especially
for systems running on the Web environment.
Due to the inefficiency of using linguists in
defining new lexicon, there must be a way to au-
tomatically or at least semi-automatically collect
new unknown words. In this paper, we propose
a collaborative framework for collecting unknown
words from Web pages over the Internet. Our
main purpose is to design and construct a system
which automatically identifies and extracts un-
known words found on Web pages of given URLs.
The compiled list of unknown-word candidates is
to be verified by a group of participants. The ap-
proved unknown words are then added to the ex-
isting lexicon along with the other related infor-
mation such as meaning and POS (part of speech).
This paper focuses on the underlying algorithms
for supporting the process of identifying and ex-
tracting unknown words. The overall process is
composed of two steps: unknown-word detection
and unknown-word boundary identification. The
first step is to detect the locations of unknown-
word occurrences from a given text. Since Thai
language belongs to the class of non-segmenting
language group in which words are written contin-
uously without using any explicit delimiting char-
acter, detection of unknown words could be ac-
complished mainly by using a word-segmentation
algorithm with a morphological analysis. By us-
ing a dictionary-based word-segmentation algo-
rithm, locations of words which are not previ-
ously included in the dictionary will be easily de-
tected. These unknown words belong to the class
of explicit unknown words and often represent the
transliteration of foreign words.
The other class of unknown words is hidden
unknown words. This class includes new words
which are created through the combination of
some existing words in the lexicon. The hidden
unknown words are usually named entities such
as a person?s name and an organization?s name.
The hidden unknown words could be identified us-
ing the approaches such as n-gram generation and
phrase chunking. The scope of this paper focuses
only on the extraction of the explicit unknown
words. However, the design of our framework also
includes the extraction of hidden unknown words.
We will continue to explore this issue in our future
works.
Once the location of an unknown word is de-
tected, the second step involves the identification
of its boundary. Since we use the Web as our
main resource, we could take advantage of its large
availability of textual contents. We are interested
in collecting unknown words which occur more
than once throughout the corpus. Unknown words
which occur only once in the large corpus are not
considered as being significant. These words may
be unusual words which are not widely accepted,
or could be misspelling words. Using this assump-
tion, our approach for identifying the unknown-
word boundary is based on a statistical pattern-
matching algorithm. The basic idea is that the
same unknown word which occurs more than once
would likely to appear in different surrounding
contexts. Therefore, a group of characters which
form the unknown word could be extracted by an-
alyzing the string matching patterns.
To evaluate the effectiveness of our proposed
framework, experiments using a real data set col-
lected from the Web are performed. The experi-
ments are designed to test each of the two main
steps of the framework. Variation of morphologi-
cal analysis are tested for the unknown-word de-
tection. The detection rate of unknown words
were found to be as high as approximately 96%.
Three variations of string pattern-matching tech-
niques were tested for unknown-word boundary
identification. The identification accuracy was
found to be as high as approximately 36%. The
relatively low accuracy is not the major concern
since the unknown-word candidates are to be ver-
ified and corrected by users before they are ac-
tually added to the dictionary. The system is
implemented via the Web-browser environment
which provides user-friendly interface for verifi-
cation process.
The rest of this paper is organized as fol-
lows. The next section presents and discusses
related works previously done in the unknown-
word problem. Section 3 provides an overview
of unknown-word problem in the relation to the
word-segmentation process. Section 4 presents the
proposed framework with underlying algorithms
in details. Experiments are performed in Section
5 with results and discussion. The conclusion is
given in Section 6.
346
2 Previous Works
The research and study in unknown-word prob-
lem have been extensively done over the past
decades. Unknown words are viewed as prob-
lematic source in the NLP systems. Techniques
in identifying and extracting unknown words are
somewhat language-dependent. However, these
techniques could be classified into two major cat-
egories, one for segmenting languages and an-
other for non-segmenting languages. Segment-
ing languages, such as latin-based languages, use
delimiting characters to separate written words.
Therefore, once the unknown words are detected,
their boundaries could be identified relatively eas-
ily when compared to those for non-segmenting
languages.
Some examples of techniques involving
segmenting languages are listed as follows.
Toole (2000) used multiple decision trees to
identify names and misspellings in English texts.
Features used in constructing the decision trees
are, for example, POS (Part-Of-Speech), word
length, edit distance and character sequence
frequency. Similarly, a decision-tree approach
was used to solve the POS disambiguation
and unknown word guessing in (Orphanos and
Christodoulakis, 1999). The research in the
unknown-word problem for segmenting lan-
guages is also closely related to the extraction of
named entities. The difference of these techniques
to those in non-segmenting languages is that
the approach needs to parse the written text in
word-level as opposed to character-level.
The research in unknown-word problem for
non-segmenting languages is highly active for
Chinese and Japanese. Many approaches have
been proposed and experimented with. Asahara
and Matsumoto (2004) proposed a technique of
SVM-based chunking to identify unknown words
from Japanese texts. Their approach used a sta-
tistical morphological analyzer to segment texts
into segments. The SVM was trained by using
POS tags to identify the unknown-word bound-
ary. Chen and Ma (2002) proposed a practical
unknown word extraction system by considering
both morphological and statistical rule sets for
word segmentation. Chang and Su (1997) pro-
posed an unsupervised iterative method for ex-
tracting unknown lexicons from Chinese text cor-
pus. Their idea is to include the potential unknown
words to the augmented dictionary in order to im-
prove the word segmentation process. Their pro-
posed approach also includes both contextual con-
straints and the joint character association metric
to filter the unlikely unknown words. Other ap-
proaches to identify unknown words include sta-
tistical or corpus-based (Chen and Bai, 1998), and
the use of heuristic knowledge (Nie et al , 1995)
and contextual information (Khoo and Loh, 2002).
Some extensions to unknown-word identification
have been done. An example include the determi-
nation of POS for unknown words (Nakagawa et
al. , 2001).
The research in unknown words for Thai lan-
guage has not been widely done as in other lan-
guages. Kawtrakul et al (1997) used the combina-
tion of a statistical model and a set of context sen-
sitive rules to detect unknown words. Our frame-
work has a different goal from previous works. We
consider unknown-word problem as collaborative
task among a group of interested users. As more
textual content is provided to the system, new un-
known words could be extracted with more accu-
racy. Thus, our framework can be viewed as col-
laborative and statistical or corpus-based.
3 Unknown-Word Problem in Word
Segmentation Algorithms
Similar to Chinese, Japanese and Korea, Thai lan-
guage belongs to the class of non-segmenting lan-
guages in which words are written continuously
without using any explicit delimiting character.
To handle non-segmenting languages, the first re-
quired step is to perform word segmentation. Most
word segmentation algorithms use a lexicon or
dictionary to parse texts at the character-level. A
typical word segmentation algorithm yields three
types of results: known words, ambiguous seg-
ments, and unknown segments. Known words are
existing words in the lexicon. Ambiguous seg-
ments are caused by the overlapping of two known
words. Unknown segments are the combination of
characters which are not defined in the lexicon.
In this paper, we are interested in extracting
the unknown words with high precision and re-
call results. Three types of unknown words are
hidden, explicit and mixed (Kawtrakul et al ,
1997). Hidden unknown words are composed by
different words existing in the lexicon. To illus-
trate the idea, let us consider an unknown word
ABCD where A, B, C, and D represents individ-
ual characters. Suppose that AB and CD both ex-
347
ist in a dictionary, then ABCD is considered as
a hidden unknown word. The explicit unknown
words are newly created words by using differ-
ent characters. Let us again consider an unknown
word ABCD. Suppose that there is no substring
of ABCD (i.e., AB, BC, CD, ABC, BCD) exists in
the dictionary, then ABCD is considered as explicit
unknown words. The mixed unknown words are
composed of both existing words in a dictionary
and non-existing substrings. From the example of
unknown string ABCD, if there is at least one sub-
string of ABCD (i.e., AB, BC, CD, ABC, BCD) ex-
ists in the dictionary, then ABCD is considered as
a mixed unknown word.
It can be immediately seen that the detection of
the hidden unknown words are not trivial since the
parser would mistakenly assume that all the frag-
ments of the words are valid, i.e., previously de-
fined in the dictionary. In this paper, we limit our-
self to the extraction of the explicit and mixed un-
known words. This type of unknown words usu-
ally represent the transliteration of foreign words.
Detection of these unknown words could be ac-
complished mainly by using a word-segmentation
algorithm with a morphological analysis. By using
a dictionary-based word-segmentation algorithm,
locations of words which are not previously de-
fined in the lexicon could be easily detected.
4 The Proposed Framework
The overall framework is shown in Figure 1.
Two major components are information agent and
unknown-word analyzer. The details of each com-
ponent are given as follows.
? Information agent: This module is com-
posed of a Web crawler and an HTML parser.
It is responsible for collecting HTML sources
from the given URLs and extracting the tex-
tual data from the pages. Our framework is
designed to support multi-user and collabora-
tive environment. The advantage of this de-
sign approach is that unknown words could
be collected and verified more efficiently.
More importantly, it allows users to select the
Web pages which suit their interests.
? Unknown-word analyzer: This module is
composed of many components for analyzing
and extracting unknown words. Word seg-
mentation module receives text strings from
the information agent and segments them
into a list of words. N-gram generation
module is responsible for generating hidden
unknown-word candidates. Morphological
analysis module is used to form initial ex-
plicit unknown-word segments. String pat-
tern matching unit performs unknown-word
boundary identification task. It takes the
intermediate unknown segments and iden-
tifies their boundaries by analyzing string
matching patterns The results are processed
unknown-word candidates which are pre-
sented to linguists for final post-processing
and verification. New unknown words are
combined with the dictionary to iteratively
improve the performance of the word seg-
mentation module. Details of each compo-
nent are given in the following subsections.
4.1 Unknown-Word Detection
As previously mentioned in Section 3, applying
a word-segmentation algorithm on a text string
yields three different segmented outputs: known,
ambiguous, and unknown segments. Since our
goal is to simply detect the unknown segments
without solving or analyzing other related issues
in word segmentation, using the longest-matching
word segmentation algorithm previously proposed
by Poowarawan (1986) is sufficient. An exam-
ple to illustrate the word-segmentation process is
given as follows.
Let the following string denotes a
text string written in Thai language:
{a1a2...aib1b2...bjc1c2...ck}. Suppose that
{a1a2...ai} and {c1c2...ck} are known words
from the dictionary, and {b1b2...bj} be an un-
known word. For the explicit unknown-word
case, applying the word-segmentation algo-
rithm would yield the following segments:
{a1a2...ai}{b1}{b2}...{bj}{c1c2...ck}. It can be
observed that the detected unknown positions for
a single unknown word are individual characters
in the unknown word itself. Based on the initial
statistical analysis of a Thai lexicon, it was found
that the averaged number of characters in a word
is equal to 7. This characteristic is quite different
from other non-segmenting languages such as
Chinese and Japanese in which a word could
be a character or a combination of only a few
characters. Therefore, to reduce the complexity
in unknown-word boundary identification task,
the unknown segments could be merged to
form multiple-character segments. For exam-
348
    
      	 
        
      
  
   
	 
    
  
  
    
  Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions, ACL-IJCNLP 2009, pages 11?14,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
QAST: Question Answering System for Thai Wikipedia
Wittawat Jitkrittum? Choochart Haruechaiyasak? Thanaruk Theeramunkong?
?School of Information, Computer and Communication Technology (ICT)
Sirindhorn International Institute of Technology (SIIT)
131 Moo 5 Tiwanont Rd., Bangkadi, Muang, Phathumthani, Thailand, 12000
wittawatj@gmail.com, thanaruk@siit.tu.ac.th
?Human Language Technology Laboratory (HLT)
National Electronics and Computer Technology Center (NECTEC)
Thailand Science Park, Klong Luang, Pathumthani 12120, Thailand
choochart.haruechaiyasak@nectec.or.th
Abstract
We propose an open-domain question an-
swering system using Thai Wikipedia as
the knowledge base. Two types of in-
formation are used for answering a ques-
tion: (1) structured information extracted
and stored in the form of Resource De-
scription Framework (RDF), and (2) un-
structured texts stored as a search index.
For the structured information, SPARQL
transformed query is applied to retrieve a
short answer from the RDF base. For the
unstructured information, keyword-based
query is used to retrieve the shortest text
span containing the questions?s key terms.
From the experimental results, the system
which integrates both approaches could
achieve an average MRR of 0.47 based on
215 test questions.
1 Introduction
Most keyword-based search engines available on-
line do not support the retrieval of precise infor-
mation. They only return a list of URLs, each re-
ferring to a web page, sorted by relevancy to the
user?s query. Users then have to manually scan
those documents for needed information. Due to
this limitation, many techniques for implement-
ing QA systems have been proposed in the past
decades.
From the literature reviews, previous and exist-
ing QA systems can be broadly categorized into
two types:
1. Knowledge Intensive: Knowledge intensive
systems focus on analyzing and understand-
ing the input questions. The system knows
exactly what to be answered, and also what
type the answer should be. The analysis
phase usually depends on an ontology or a
semantic lexicon like WordNet. The an-
swer is retrieved from a predefined organized
knowledge base. Natural Language Process-
ing (NLP) techniques are heavily used in a
knowledge intensive system.
2. Data Intensive: Data intensive systems,
which do not fully analyze the input ques-
tions, rely on the redundancy of huge amount
of data (Dumais et al, 2002). The idea is that
if we have a huge amount of data, a piece of
information is likely to be stated more than
once in different forms. As a result, the data-
intensive QA systems are not required to per-
form many complex NLP techniques.
In this paper, we propose an open-domain QA
system for Thai Wikipedia called QAST. The sys-
tem supports five types of close-ended questions:
person, organization, place, quantity, and date/-
time. Our system can be classified as a data in-
tensive type with an additional support of struc-
tured information. Structured information in Thai
Wikipedia is extracted and represented in the form
of RDF. We use SPARQL to retrieve specific in-
formation from the RDF base. If using SPARQL
cannot answer a given question, the system will re-
trieve answer candidates from the pre-constructed
search index using a technique based on Minimal
Span Weighting (Monz, 2003).
2 System Architecture
Figure 1 shows the system architecture of QAST
which consists of three main sub-systems: Data
Representation , Question Processor , and Answer
11
Figure 1: The system architecture of QAST
Processor .
2.1 Data Representation
The Data Representation part is a storage for all
information contained in Thai Wikipedia. Two
modules constitute this sub-system.
RDF Base: In QAST, RDF triples are generated
from Wikipedia?s infoboxes following similar ap-
proaches described in the works of Isbell and But-
ler (2007) and Auer and Lehmann (2007). To gen-
erate RDF triples from an infobox, we would have
the article title as the subject. The predicates are
the keys in the first column. The objects are the
values in the second column. Altogether, the num-
ber of generated triples corresponds to the number
of rows in the infobox.
In addition to the infoboxes, we also store syn-
onyms in the form of RDF triples. The synonyms
are extracted from redirect pages in Wikipedia.
For example, a request for the Wikipedia article
titled ?Car? will result in another article titled ?Au-
tomobile? to be shown up. The former page usu-
ally has no content and only acts as a pointer to
another page which contains the full content. The
relationship of these two pages implies that ?Car?
and ?Automobile? are synonymous. Synonyms
are useful in retrieving the same piece of informa-
tion with different texual expressions.
Search Index: QAST stores the textual content
as a search index. We used the well-known IR li-
brary, Lucene
1
, for our search backend. We in-
dexed 41,512 articles (as of February 5, 2009)
from a Thai Wikipedia dump with full term posi-
tions. Firstly, all template constructs and the Wiki-
Text markups are removed, leaving with only the
plain texts. A dictionary-based longest-matching
word segmentation is then performed to tokenize
the plain texts into series of terms. Finally, the
resulted list of non-stopwords are passed to the
Lucene indexing engine. The dictionary used for
word segmentation is a combination of word list
from the LEXiTRON
2
and all article titles from
Thai Wikipedia. In total, there are 81,345 words
in the dictionary.
2.2 Question Processor
Question processor sub-system consists of four
modules as follows.
1. Question Normalizer ? This first module is
to change the way the question is formed into
a normal form to ease the processing at lat-
ter stages. This includes correcting mistyped
words or unusual spelling such as f33t for
feet.
2. Word Segmenter ? This module performs
tokenizing on the normalized question to ob-
tain a list of non-stopwords.
3. Question Analyzer ? The question ana-
lyzer determines the expected type of answer
(i.e., quantity, person, organization, location,
date/time and unknown) and constructs an
appropriate query. Normally, a SPARQL
query is generated and used to retrieve a can-
didate answer from the RDF base. When the
SPARQL fails to find an answer, the system
will switch to the index search. In that case,
the module also defines a set of hint terms to
help in locating candidate answers.
4. Searcher ? This module executes the query
and retrieves candidate answers from the data
representation part.
To generate a SPARQL query, the input ques-
tion is compared against a set of predefined regu-
lar expression patterns. Currently, the system has
two types of patterns: pattern for definitional ques-
tions, and pattern for questions asking for a prop-
1
Apache Lucene, http://lucene.apache.org
2
LEXiTRON, http://lexitron.nectec.or.th
12
erty of an entity. The pattern for definitional ques-
tion is of the form a-rai-kue-X ?What is X ?? or
X-kue-a-rai ?X is what ??. After X is determined
from a user?s question, the first paragraph of the
article titled X is retrieved and directly returned to
the user. Since the first paragraph in any article is
usually the summary, it is appropriate to use the
first paragraph to answer a definitional question.
Questions asking for a property of an entity are
of the form a-rai-kue-P-kong-X ?What is P of X ??
e.g., ?When was SIIT established ?? which can be
answered by looking for the right information in
the RDF base. A simplified SPARQL query used
to retrieve an answer for this type of question is as
follows.

SELECT ? o
WHERE {
? tempPage h a s I n f o b o x ? tempBox .
? tempPage r d f s : l a b e l ?X? .
? tempBox ?P ? o .
}


 
The query matches an object of a RDF triple with
the predicate P (e.g., ?date of establishment?), pro-
vided that the triple is generated from an infobox
titled X (e.g.,?SIIT?) . The object of the year 1992
is then correctly returned as the answer.
When SPARQL fails, i.e., the question does
not match any known pattern or the answer does
not exist in the RDF base, the system switches to
the index search which performs the following the
steps.
1. Word Segmenter tokenizes the question into
a list of keywords q.
2. Question analyzer analyzes q, generates a ba-
sic Lucene? TermQuery, and defines a set of
hint terms H .
3. Retrieve the most relevant c documents using
Lucene?s default search scoring function
3
.
Denote D as the set of retrieved documents.
4. For each document d in D where d =
{t
1
, t
2
, . . . , t
|d|
} (t is a term),
(a) Find in d the start term index
mmsStart and end term index
mmsEnd of the shortest term span
containing all terms in q (Monz, 2003).
(b) spanLength ? 1 + mmsEnd ?
mmsStart
(c) If spanLength > 30, skip current d.
Go to the next document.
3
http://lucene.apache.org/java/2_3_0/
scoring.html
(d) Find minimal span weighting
score msw (Monz, 2003). If
|q ? d| = 1 then, msw =
RSV
n
(q, d). Otherwise, msw =
0.4 ?RSV
n
(q, d)+0.6 ?(
|q?d|
spanLength
)
1/8
?
(
|q?d|
|q|
) where RSV
n
(q, d) =
lucene(q, d)/max
d
lucene(q, d)
(e) mmsStart? max(mmsStart?s, 1)
(f) mmsEnd? min(mmsEnd + s, |d|)
(g) Find the weighting for hint terms hw
(0 ? hw ? 1).
(h) Calculate the span score
sp = msw ? (1 + hw)
(i) Add the text span to the span set P (Sort
P by sp in descending order).
5. Return the top k spans in P as answers.
In the actual implementation, we set c equal to
500 so that only the top 500 documents are con-
sidered. Although retrieving more texts from the
corpus would likely increase the chance of find-
ing the answer (Moldovan et al, 2002), our trial-
and-error showed that 500 documents seem to be
a good trade-off between speed and content cov-
erage. To look for an occurrence of hint terms,
each span is stretched backward and forward for
10 terms (i.e., s = 10). Finally, we set k equal to
5 to return only the top five spans as the answers.
2.3 Answer Processor
This sub-system contains two modules: Answer
Ranker and Answer Generator.
Answer Ranker concerns with how to rank the
retrieved answer candidates. In the case where
SPARQL query is used, this module is not re-
quired since most of the time there will be only
one result returned.
In the case when the search index is used, all
candidate answers are sorted by the heuristic span
score (i.e., sp = msw ? (1 + hw)). The func-
tion mostly relies on regular expressions defining
expected answer patterns. If a span has an occur-
rence of one of the defined patterns (i.e., hw > 0),
it is directly proportional to the suitability of the
occurrence with respect to the question, length and
rareness of the pattern occurrence. For example,
the hint terms of questions asking for a person
would be personal titles such as Ms. and Dr.
As for the final step, the Answer Generator
module formats the top five candidate answers into
an HTML table and returns the results to the user.
13
Question Type Index & RDF Index
Person 0.47 0.37
Organization 0.56 0.46
Place/Location 0.43 0.36
Quantity 0.51 0.44
Date/Time 0.39 0.34
Average MRR 0.47 0.39
Table 1: QAST?s performance comparison be-
tween (1) using both index and RDF and (2) using
only the index.
3 Evaluation Metric
To evaluate the system, 215 test questions (43
questions for each question type) and their cor-
rect answers were constructed based on the con-
tents of random articles in Thai Wikipedia. Mean
Reciprocal Rank (MRR), the official measurement
used for QA systems in TREC (Voorhees and Tice,
2000), is used as the performance measurement.
To evaluate the system, a question is said to be
correctly answered only when at least one of the
produced five ranked candidates contained the true
answer with the right context. Out-of-context can-
didate phrases which happen to contain the true
answers are not counted. If there is no correct an-
swer in any candidate, the score for that question
is equal to zero.
4 Experimental Results and Discussion
Table 1 shows a comparison of the MRR values
when using both index and RDF, and using only
the index. The approach of using only the index,
the overall MRR is equal to 0.39 which is fairly
high with respect to the answer retrieval method-
ology. The index search approach simply relies on
the fact that if the question keywords in a ranked
candidate document occur close together and at
least one occurrence of expected answer pattern
exists, then there is a high chance that the term
span contains an answer.
The MRR significantly increases to 0.47 (20.5%
improvement) when RDF (structured information)
is used together with the index. A thorough analy-
sis showed that out of 215 questions, 21 questions
triggered the RDF base. Among these, 18 ques-
tions were correctly answered. Therefore, using
the additional structured information helps answer
the definitional and factoid questions. We expect a
higher improvement when more structured infor-
mation is incorporated into the system.
5 Conclusions and Future Works
We proposed an open-domain QA system called
QAST. The system uses Thai Wikipedia as the cor-
pus and does not rely on any complex NLP tech-
nique in retrieving an answer.
As for future works, some possiblities for im-
proving the current QAST are as follows.
? An information extraction module may be
added to extract and generate RDF triples
from unstructured text.
? Infoboxes, wikipedia categories and internal
article links may be further explored to con-
struct an ontology which will allow an auto-
matic type inference of entities.
? More question patterns and the correspond-
ing SPARQL queries can be added so that
SPARQL is used more often.
Acknowledgement
The financial support from Young Scientist and
Technologist Programme, NSTDA (YSTP : SP-
51-NT-15) is gratefully acknowledged.
References
Soren Auer and Jens Lehmann. 2007. What Have
Innsbruck and Leipzig in Common? Extracting Se-
mantics from Wiki Content. In Proc. of the 4
th
Eu-
ropean conference on The Semantic Web: Research
and Applications, pp. 503-517.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web Question Answering:
Is More Always Better?. In Proc. of the 25th ACM
SIGIR, pp. 291-298.
Jonathan Isbell and Mark H. Butler. 2007. Extracting
and Re-using Structured Data from Wikis. Technical
Report HPL-2007-182, Hewlett-Packard.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and
Mihai Surdeanu . 2002. Performance Issues and Er-
ror Analysis in an Open-Domain Question Answer-
ing System. Proc. of the 40
th
ACL, pp. 33-40.
Christof Monz. 2003. From Document Retrieval to
Question Answering. Ph.D. Thesis. University of
Amsterdam.
Ellen M. Voorhees and Dawn Tice. 2000. Building a
Question Answering Test Collection. In 23
rd
ACM
SIGIR, pp. 200-207.
14
Proceedings of the 8th Workshop on Asian Language Resources, pages 64?71,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Constructing Thai Opinion Mining Resource:
A Case Study on Hotel Reviews
Choochart Haruechaiyasak, Alisa Kongthon,
Pornpimon Palingoon and Chatchawal Sangkeettrakarn
Human Language Technology Laboratory (HLT)
National Electronics and Computer Technology Center (NECTEC)
choochart.har@nectec.or.th, alisa.kon@nectec.or.th,
pornpimon.pal@nectec.or.th, chatchawal.san@nectec.or.th
Abstract
Opinion mining and sentiment analy-
sis has recently gained increasing atten-
tion among the NLP community. Opin-
ion mining is considered a domain-
dependent task. Constructing lexicons
for different domains is labor intensive.
In this paper, we propose a framework
for constructing Thai language resource
for feature-based opinion mining. The
feature-based opinion mining essentially
relies on the use of two main lexicons,
features and polar words. Our approach
for extracting features and polar words
from opinionated texts is based on syn-
tactic pattern analysis. The evaluation
is performed with a case study on ho-
tel reviews. The proposed method has
shown to be very effective in most cases.
However, in some cases, the extraction
is not quite straightforward. The rea-
sons are due to, firstly, the use of conver-
sational language in written opinionated
texts and, secondly, the language seman-
tic. We provide discussion with possible
solutions on pattern extraction for some
of the challenging cases.
1 Introduction
With the popularity of Web 2.0 or social net-
working websites, the amount of user-generated
contents has increased exponentially. One in-
teresting type of these user-generated contents
is texts which are written with some opinions
and/or sentiments. An in-depth analysis of these
opinionated texts could reveal potentially use-
ful information regarding the preferences of peo-
ple towards many different topics including news
events, social issues and commercial products.
Opinion mining and sentiment analysis is such
task for analyzing and summarizing what people
think about a certain topic.
Due to its potential and useful applications,
opinion mining has gained a lot of interest in text
mining and NLP communities (Ding et al, 2008;
Jin et al, 2009). Much work in this area focused
on evaluating reviews as being positive or nega-
tive either at the document level (Turney, 2002;
Pang et al, 2002; Dave et al, 2003; Beineke
et al, 2004) or sentence level (Kim and Hovy,
2004; Wiebe and Riloff, 2005; Wilson et al,
2009; Yu and Hatzivassiloglou, 2003). For in-
stance, given some reviews of a product, the sys-
tem classifies them into positive or negative re-
views. No specific details or features are identi-
fied about what customers like or dislike. To ob-
tain such details, a feature-based opinion mining
approach has been proposed (Hu and Liu, 2004;
Popescu and Etzioni, 2005). This approach typi-
cally consists of two following steps.
1. Identifying and extracting features of an ob-
ject, topic or event from each sentence upon
which the reviewers expressed their opin-
ion.
2. Determining whether the opinions regard-
ing the features are positive or negative.
The feature-based opinion mining could pro-
vide users with some insightful information re-
lated to opinions on a particular topic. For exam-
ple, for hotel reviews, the feature-based opinion
64
mining allows users to view positive or negative
opinions on hotel-related features such as price,
service, breakfast, room, facilities and activities.
Breaking down opinions into feature level is very
essential for decision making. Different cus-
tomers could have different preferences when se-
lecting hotels to stay for vacation. For example,
some might prefer hotels which provide full fa-
cilities, however, some might prefer to have good
room service.
The main drawback of the feature-based opin-
ion mining is the preparation of different lex-
icons including features and polar words. To
make things worse, these lexicons, especially the
features, are domain-dependent. For a partic-
ular domain, a set of features and polar words
must be prepared. The process for language
resource construction is generally labor inten-
sive and time consuming. Some previous works
have proposed different approaches for automat-
ically constructing the lexicons for the feature-
based opinion mining (Qiu et al, 2009; Riloff
and Wiebe, 2003; Sarmento et al, 2009). Most
approaches applied some machine learning al-
gorithms for learning the rules from the corpus.
The rules are used for extracting new features
and polar words from untagged corpus. Reviews
of different approaches are given in the related
work section.
In this paper, we propose a framework for
constructing Thai language resource for the
feature-based opinion mining. Our approach
is based on syntactic pattern analysis of two
lexicon types: domain-dependent and domain-
independent. The domain-dependent lexicons
include features, sub-features and polar words.
The domain-independent lexicons are particles,
negative words, degree words, auxiliary verbs,
prepositions and stop words. Using these lexi-
cons, we could construct a set of syntactic rules
based on the frequently occurred patterns. The
rule set can be used for extracting more unseen
sub-features and polar words from untagged cor-
pus.
We evaluated the proposed framework on the
domain of hotel reviews. The experimental re-
sults showed that our proposed method is very
effective in most cases, especially for extracting
polar words. However, in some cases, the extrac-
tion is not quite straightforward due to the use of
conversational language, idioms and hidden se-
mantic. We provide some discussion on the chal-
lenging cases and suggest some solutions as the
future work.
The remainder of this paper is organized as
follows. In next section, we review some related
works on different approaches for constructing
language resources for opinion mining and sen-
timent analysis. In Section 3, we present the pro-
posed framework for constructing Thai opinion
mining resource by using the dual pattern extrac-
tion method. In Section 4, we apply the proposed
framework with a case study of hotel reviews.
The performance evaluation is given with the ex-
periment results. Some difficult cases are dis-
cussed along with some possible solutions. Sec-
tion 5 concludes the paper with the future work.
2 Related work
The problem of developing subjectivity lexicons
for training and testing sentiment classifiers has
recently attracted some attention. The Multi-
perspective Question Answering (MPQA) opin-
ion corpus is a well-known resource for senti-
ment analysis in English (Wiebe et al, 2005).
It is a collection of news articles from a vari-
ety of news sources manually annotated at word
and phrase levels for opinions and other private
states (i.e., beliefs, emotions, sentiments, spec-
ulations, etc.). The annotation in this work also
took into account the context, which is essential
for resolving possible ambiguities and accurately
determining polarity.
Although most of the reference corpora has
been focused on English language, work on other
languages is growing as well. Kanayama et
al. (2006) proposed an unsupervised method to
detect sentiment words in Japanese. In this work,
they used clause level context coherency to iden-
tify candidate sentiment words from sentences
that appear successively with sentences contain-
ing seed sentiment words. Their assumption is
that unless the context is changed with adver-
sative expressions, sentences appearing together
65
in that context tend to have the same polari-
ties. Hence, if one of them contains sentiments
words, the other successive sentences are likely
to contain sentiment words as well. Ku and
Chen (2007) proposed the bag-of-characters ap-
proach to determine sentiment words in Chinese.
This approach calculates the observation proba-
bilities of characters from a set of seed sentiment
words first, then dynamically expands the set and
adjusts their probabilities. Later in 2009, Ku
et al (2009), extended their bag-of-characters
approach by including morphological structures
and syntactic structures between sentence seg-
ment. Their experiments showed better perfor-
mance of word polarity detection and opinion
sentence extraction.
Some other methods to automatically gener-
ate resources for subjectivity analysis for a for-
eign language have leveraged the resources and
tools available for English. For example, Be-
nea et al (2008) applied machine translation and
standard Naive Bayes and SVM for subjectiv-
ity classification for Romanian. Their exper-
iments showed promising results for applying
automatic translation to construct resources and
tools for opinion mining in a foreign language.
Wan (2009) also leveraged an available English
corpus for Chinese sentiment classification by
using the co-training approach to make full use
of both English and Chinese features in a uni-
fied framework. Jijkoun and Hofmann (2009)
also described a method for creating a Dutch
subjectivity lexicon based on an English lexi-
con. They applied a PageRank-like algorithm
that bootstraps a subjectivity lexicon from the
translation of the English lexicon and rank the
words in the thesaurus by polarity using the net-
work of lexical relations (e.g., synonymy, hy-
ponymy) in Wordnet.
3 The proposed framework
The performance of the feature-based opinion
mining relies on the design and completeness
of related lexicons. Our lexicon design dis-
tinguishes lexicons into two types, domain-
dependent and domain-independent. The design
of domain-dependent lexicons is based on the
feature-based opinion mining framework pro-
posed by Liu et al (2005). The framework starts
by setting the domain scope such as digital cam-
era. The next step is to design a set of features
associated with the given domain. For the do-
main of digital camera, features could be, for in-
stance, ?price?, ?screen size? and ?picture qual-
ity?. Features could contain sub-features. For
example, the picture quality could have the sub-
features as ?macro mode?, ?portrait mode? and
?night mode?. Preparing multiple feature levels
could be time-consuming, therefore, we limit the
features into two levels: main features and sub-
features.
Another domain-dependent lexicon is po-
lar words. Polar words are sentiment words
which represent either positive or negative views
on features. Although some polar words are
domain-independent and have explicit meanings
such as ?excellent?, ?beautiful?, ?expensive?
and ?terrible?. Some polar words are domain-
dependent and have implicit meanings depend-
ing on the contexts. For example, the word
?large? is generally considered positive for the
screen size feature of digital camera domain.
However, for the dimension feature of mobile
phone domain, the word ?large? could be con-
sidered as negative.
On the other hand, the domain-independent
lexicons are regular words which provide differ-
ent parts of speech (POS) and functions in the
sentence. For opinion mining task, we design
six different domain-independent lexicons as fol-
lows (some examples are shown in Table 1).
? Particles (PAR): In Thai language, these
words refer to the sentence endings which
are normally used to add politeness of the
speakers (Cooke, 1992).
? Negative words (NEG): Like English,
these words are used to invert the opinion
polarity. Examples are ?not?, ?unlikely?
and ?never?.
? Degree words (DEG): These words are
used as an intensifier to the polar words.
Examples are ?large?, ?very?, ?enormous?.
66
? Auxiliary verbs (AUX): These words are
used to modify verbs. Examples are
?should?, ?must? and ?then?.
? Prepositions (PRE): Like English, Thai
prepositions are used to mark the relations
between two words.
? Stop words (STO): These words are used
for grammaticalization. Thai language is
considered an isolating language, to form a
noun the words ?karn? and ?kwam? are nor-
mally placed in front of a verb or a noun,
respectively. Therefore, these words could
be neglected when analyzing opinionated
texts.
Table 1: Domain-independent lexicons
Although some of the above lexicons are sim-
ilar to English, however, some words are placed
in different position in a sentence. For example,
in Thai, a degree word is usually placed after a
polar word. For example, ?very good? would be
written as ?good very? in Thai.
Figure 1 shows all processes and work flow
under the proposed framework. The process
starts with a corpus which is tagged based on
two lexicon types. From the tagged corpus,
we construct patterns and lexicons. The pat-
tern construction is performed by collecting text
segments which contain both features and polar
words. All patterns are sorted by the frequency
of occurrence. The lexicon construction is per-
formed by simply collecting words which are al-
ready tagged with the lexicon types. The lex-
icons are used for performing the feature-based
opinion mining task such as classifying and sum-
marizing the reviews as positive and negative
based on different features. The completeness of
lexicons is very important for the feature-based
opinion mining. To collect more lexicons, pat-
terns are used in the dual pattern extraction pro-
cess to extract more features and polar words
from the untagged corpus.
Figure 1: The proposed opinion resource con-
struction framework based on the dual pattern
extraction.
4 A case study of hotel reviews
To evaluate the proposed framework, we per-
form some experiments with a case study of
hotel reviews. In Thailand, tourism is ranked
as one of the top industries. From the statis-
tics provided by the Office of Tourism Develop-
ment1, the number of international tourists vis-
iting Thailand in 2009 is approximately 14 mil-
1The Office of Tourism Development,
http://www.tourism.go.th
67
lions. The number of registered hotels in all re-
gions of Thailand is approximately 5,000. Pro-
viding an opinion mining system on hotel re-
views could be very useful for tourists to make
decision on hotel choice when planning a trip.
4.1 Corpus preparation
We collected customer reviews from the Agoda
website2. The total number of reviews in the cor-
pus is 8,436. Each review contains the name of
the hotel as the title and comments in free-form
text format. We designed a set of 13 main fea-
tures: service, cleanliness, hotel condition, loca-
tion, food, breakfast, room, facilities, price, com-
fort, quality, activities and security. The set of
main features is designed based on the features
obtained from the Agoda website. Some addi-
tional features, such as activities and security, are
added to provide users with more dimensions.
In this paper, we focus on two main fea-
tures: breakfast and service. Table 2 shows the
domain-dependent lexicons related to the break-
fast feature. For breakfast main feature (FEA),
we include all synonyms which could be used to
describe breakfast in Thai. These include En-
glish terms with their synonyms, transliterated
terms and abbreviations.
The breakfast sub-features (FEA*) are spe-
cific concepts of breakfast. Examples include
?menu?, ?taste?, ?service? and ?coffee?. It can
be observed that some of the sub-features could
also act as a main feature. For example, the
sub-feature ?service? of breakfast is also used
as the main feature ?service?. Providing sub-
feature level could help revealing more insight-
ful dimension for the users. However, designing
multiple feature levels could be time-consuming,
therefore, we limit the features into two levels,
i.e., main feature and sub-feature. The polar
words (POL) are also shown in the table. We
denote the positive and negative polar words by
placing [+] and [-] after each word. It can be
observed that some polar words are dependent
on sub-features. For example, the polar word
?long line? can only be used for the sub-feature
?restaurant?.
2Agoda website: http://www.agoda.com
Table 2: Domain-dependent lexicons for the
breakfast feature.
Table 3 shows the domain-dependent lexicons
related to the service feature. The main fea-
tures include synonyms, transliterated and En-
glish terms which describe the concept service.
The service sub-features are, for example, ?re-
ception?, ?security guard?, ?maid?, ?waiter? and
?concierge?. Unlike the breakfast feature, the
polar words for the service feature are quite gen-
eral and could mostly be applied for all sub-
features. Another observation is that some of the
polar words are based on Thai idiom. For ex-
ample, the phrase ?having rigid hands? in Thai
means ?impolite?. In Thai culture, people show
politeness by doing the ?wai? gesture.
4.2 Experiments and results
Using the tagged corpus and the extracted lexi-
cons, we construct the most frequently occurred
patterns. For two main features, breakfast and
service, the numbers of tagged reviews for each
feature are 301 and 831, respectively. We ran-
domly split the corpus into 80% as training set
and 20% as test set. We only consider the pat-
terns which contain both features (either main
features or sub-features) and polar words. For
the breakfast feature, the total number of ex-
tracted patterns is 86. For the service feature, the
total number of extracted patterns is 192. Table
4 and 5 show some examples of most frequently
68
Table 3: Domain-dependent lexicons for the ser-
vice feature.
occurred patterns extracted from the corpus. The
symbols of the tag set are as shown in Table 1
and 2 with the tag <OTH> denoting any other
words.
From the tables, two patterns which occur fre-
quently for both features are <FEA><POL>
and <FEA*><POL>. These two patterns are
very simple and show that the opinionated texts
in Thai are mostly very simple. Users just simply
write a word describing the feature followed by
a polar word (either positive or negative) with-
out using any verb in between. Some examples
for the pattern <FEA*><POL> are <coffee
cup><dirty> and <employee><friendly>. In
English, a verb ?to be? (is/am/are) is usually re-
quired between <FEA*> and <POL>.
Using the extracted patterns, we perform the
dual pattern extraction process to collect the
sub-features and polar words from the test data
set. Table 6 shows the evaluation results of
sub-features and polar words extraction for both
breakfast and service features. It can be observed
that the set of patterns could extract polar words
(POL) with higher accuracy than sub-features
(FEA*). This could be due to the patterns used to
Table 4: Top-ranked breakfast patterns with ex-
amples
describe the polar words are straightforward and
not complicated. This is especially true for the
case of breakfast feature in which the accuracy
is approximately 95%.
Table 5: Top-ranked service patterns with exam-
ples
4.3 Discussion
Table 7 and 8 show some examples of challeng-
ing cases for breakfast and service features, re-
spectively. The polar words shown in both tables
are very difficult to extract since the patterns can
69
Feature Accuracy (%)FEA* POL
Breakfast 80.00 95.74
Service 82.56 89.29
Table 6: Evaluation results of features and polar
words extraction.
not be easily captured. The difficulties are due to
many reasons including the language semantic
and the need of world knowledge. For example,
in case #5 of service feature, the whole phrase
can be interpreted as ?attentive?. It is difficult
for the system to generate the pattern based on
this phrase. Another example is case #4 of both
tables, the customers express their opinions by
comparing to other hotels. To analyze the senti-
ment correctly would require the knowledge of a
particular hotel or hotels in specific locations.
Table 7: Examples of difficult cases of breakfast
feature
5 Conclusion and future work
We proposed a framework for constructing Thai
opinion mining resource with a case study on
hotel reviews. Two sets of lexicons, domain-
dependent and domain-independent, are de-
signed to support the pattern extraction process.
The proposed method first constructs a set of pat-
terns from a tagged corpus. The extracted pat-
terns are then used to automatically extract and
collect more sub-features and polars words from
an untagged corpus. The performance evaluation
Table 8: Examples of difficult cases of service
feature
was done with a collection of hotel reviews ob-
tained from a hotel reservation website. From
the experimental results, polar words could be
extracted more easily than sub-features. This
is due to the polar words often appear in spe-
cific positions with repeated contexts in the opin-
ionated texts. In some cases, extraction of sub-
features and polar words are not straightforward
due to the difficulties in generalizing patterns.
For example, some subjectivity requires com-
plete phrases to describe the polarity. In some
cases, the sub-features are not explicitly shown
in the sentence. For future work, we plan to com-
plete the construction of the corpus by consider-
ing the rest of main features. Another plan is to
include the semantic analysis into the pattern ex-
traction process. For example, the phrase ?forget
something? could imply negative polarity for the
service feature.
References
Banea, Carmen, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. Proc. of the
2008 empirical methods in natural language pro-
cessing, 127?135.
Beineke, Philip, Trevor Hastie and Shivakumar
Vaithyanathan. 2004. The sentimental factor: im-
proving review classification via human-provided
information. Proc. of the 42nd Annual Meeting on
Association for Computational Linguistics, 263?
270.
70
Cooke, J.R. 1992. Thai sentence particles: putting
the puzzle together. Proc. of the The Third Inter-
national Symposium on Language and Linguistics,
1105?1119.
Dave, Kushal, Steve Lawrence and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product
reviews. Proc. of the 12th international confer-
ence on World Wide Web, 519?528.
Ding, Xiaowen, Bing Liu and Philip S. Yu. 2008. A
holistic lexicon-based approach to opinion mining.
Proc. of the int. conf. on web search and web data
mining, 231?240.
Hu, Minqing and Bing Liu. 2004. Mining and sum-
marizing customer reviews. Proc. of the 10th ACM
SIGKDD international conference on Knowledge
discovery and data mining, 168?177.
Jin, Wei, Hung Hay Ho and Rohini K. Srihari. 2009.
OpinionMiner: a novel machine learning system
for web opinion mining and extraction. Proc. of
the 15th ACM SIGKDD, 1195?1204.
Kim, Soo-Min and Eduard Hovy. 2004. Determining
the sentiment of opinions. Proc. of the 20th inter-
national conference on Computational Linguistics,
1367?1373.
Qiu, Guang, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon
through double propagation. Proc. of the 21st In-
ternational Joint Conferences on Artificial Intelli-
gence, 1199?1204.
Jijkoun, Valentin and Katja Hofmann. 2009. Gener-
ating a non-English subjectivity lexicon: relations
that matter. Proc. of the 12th Conference of the
European Chapter of the Association for Compu-
tational Linguistics, 398?405.
Kanayama, Hiroshi and Tetsuya Nasukawa. 2006.
Fully automatic lexicon expansion for domain-
oriented sentiment analysis. Proc. of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, 355?363.
Ku, Lun-Wei and Hsin-Hsi Chen. 2007 Mining opin-
ions from the Web: Beyond relevance retrieval.
Journal of American Society for Information Sci-
ence and Technology, 58(12):1838?1850.
Ku, Lun-Wei, Ting-Hao Huang and Hsin-Hsi Chen.
2009. Using morphological and syntactic struc-
tures for Chinese opinion analysis. Proc. of the
2009 empirical methods in natural language pro-
cessing, 1260?1269.
Liu, Bing, Minqing Hu and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the Web. Proc. of the 14th World Wide
Web, 342?351.
Pang, Bo, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. Proc. of the ACL-
02 conf. on empirical methods in natural language
processing, 79?86.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. Proc. of the conf. on human language tech-
nology and empirical methods in natural language
processing, 339?346.
Riloff, Ellen and Janyce Wiebe. 2003. Learning ex-
traction patterns for subjective expressions. Proc.
of the 2003 conference on empirical methods in
natural language processing, 105?112.
Sarmento, Lu??s, Paula Carvalho, Ma?rio J. Silva, and
Euge?nio de Oliveira. 2009. Automatic creation
of a reference corpus for political opinion min-
ing in user-generated content. Proc. of the 1st
CIKM workshop on topic-sentiment analysis for
mass opinion, 29?36.
Turney, Peter D. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised clas-
sification of reviews. Proc. of the 40th ACL, 417?
424.
Wan, Xiaojun. 2009. Co-training for cross-lingual
sentiment classification. Proc. of the joint conf. of
ACL and IJCNLP, 235?243.
Wiebe, Janyce and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. Proc. of Conference on Intelli-
gent Text Processing and Computational Linguis-
tics, 486?497.
Wiebe, Janyce, Theresa Wilson and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165?210.
Wilson, Theresa, Janyce Wiebe and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Comput. Linguist., 35(3):399?433.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: separating
facts from opinions and identifying the polarity of
opinion sentences. Proc. of the Conference on Em-
pirical Methods in Natural Language Processing,
129?136.
71
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 186?187,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
ThaiHerbMiner: A Thai Herbal Medicine Mining and Visualizing Tool
Choochart Haruechaiyasak? Jaruwat Pailai? Wasna Viratyosin? Rachada Kongkachandra?
?Human Language Technology Laboratory (HLT),
National Electronics and Computer Technology Center (NECTEC), Thailand 12120
?Department of Computer Science, Faculty of Science and Technology
Thammasat University, Thailand 12121
?BIOTEC Central Research Unit,
National Center for Genetic Engineering and Biotechnology, Thailand 12120
Abstract
Thai Traditional Medicine (TTM) has a long
history in Thailand and is nowadays consid-
ered an effective alternative approach to the
modern medicine. One of the main knowledge
in Thai traditional medicine is the use of var-
ious types of herbs to form medicines. Our
main goal is to bridge the gap between the tra-
ditional knowledge and the modern biomedi-
cal knowledge. Using text mining and visu-
alization techniques, some implicit relations
from one source could be used to verify and
enhance the knowledge discovery in another
source. In this paper, we present our ongoing
work, ThaiHerbMiner, a Thai herbal medicine
mining and visualizing tool. ThaiHerbMiner
applies text mining to extract some salient re-
lations from a collection of PubMed articles
related to Thai herbs. The extracted relations
can be browsed and viewed using information
visualization. Our proposed tool can also rec-
ommend a list of herbs which have similar
medical properties.
1 Introduction
In 1993, the Royal Thai Government instituted the
National Institute of Thai Traditional Medicine, un-
der the supervision of the Ministry of Public Health.
The goal of the institute is to systematize and
standardize the body of Thai Traditional Medicine
(TTM) knowledge. The main task is to gather, re-
vise, verify, classify, and explain the TTM knowl-
edge. There are many ongoing project collaboration
to digitize the TTM knowledge, many of which are
documented on palm leaves. The digitized contents
contain information on Thai medical herbal formu-
lations with the healing properties. A medical herbal
formulation could contain more than one herb and
combined with others for better effect.
Apart from the traditional knowledge, today
biomedical research has advanced into the genetic
level. Many researchers have performed in-depth
studies of herbs? medical properties on disease treat-
ment. The main goal of our research is to com-
bine the knowledge from traditional and modern
biomedical research. Using knowledge from one
source could support the knowledge discovery in
another source. To assist the researchers in Thai
herbal medicine, we propose ThaiHerbMiner, a text
mining and visualizing platform. ThaiHerbMiner?s
main task is to extract and visualize relations among
herbs, properties and other entities. Our work is sim-
ilar to the current ongoing research in mining Tradi-
tional Chinese Medicine (TCM) which has gained
increasing attention in recent years (He et al, 2011;
Lukman et al, 2007).
2 Design and implementation
Text mining has become a widely applied technique
for analyzing biomedical texts (Cohen and Hersh,
2005). The proposed ThaiHerbMiner is designed
with the standard text mining process. We started
by collecting PubMed articles by using herb names
as keywords. Currently, we have obtained approxi-
mately 18,000 articles related to Thai herbs such as
garlic, curcuma and ginger.
Figure 1 shows the text mining process of extract-
ing relations from given input texts. The process
includes sentence segmentation, tokenization, POS
186
Table 1: The text mining process for extracting relations
from input texts.
tagging and entity & relation recognition. We used
OpenNLP1 to perform all text processing tasks. For
relation recognition based on syntactic structure, we
focus on a group of causal verbs such as activate, in-
duce, inhibit, prevent, regulate and suppress. Then
the information visualization based on JavaScript 2
is applied to represent the extracted relations.
Figure 2 shows an example of a hyperbolic tree
visualizing relations between curcuma and other en-
tities. For example, curcuma has the property of in-
hibit with NF-kappaB, tumor and cancer. Figure 3
shows an example of a force-directed graph visual-
izing similar herbs sharing two entities, cancer and
NF-kappaB. The visualizing result is useful to re-
searchers for finding herbs which share similar med-
ical properties.
3 Conclusion and future work
The results of literature mining can be potentially
useful in revealing implicit relations underlying the
knowledge in herbal medicine. In particular, the re-
sults can be used in screening the research in Thai
herbal medicine to form a novel hypothesis. Our
next step is to perform comparative analysis on the
knowledge from Thai traditional medicine and the
knowledge extracted from the modern research pub-
lications.
1The OpenNLP Homepage, http://opennlp.sourceforge.net
2The JavaScript InfoVis Toolkit, http://thejit.org
Table 2: An example of relations between curcuma and
other relevant entities.
Table 3: An example of relations among different herbs
sharing the same entities.
References
Cohen, Aaron M. and William R. Hersh. 2005. A survey
of current work in biomedical text mining. Briefings
in Bioinformatics, 6(1):57-71.
He, Ping, Ke Deng, Zhihai Liu, Delin Liu, Jun S Liu
and Zhi Geng. 2011. Discovering herbal functional
groups of traditional Chinese medicine. Statistics in
medicine, March 17, 2011.
Lukman, Suryani, Yulan He, and Siu-Cheung Hui.
2007. Computational methods for Traditional Chinese
Medicine: A survey. Comput. Methods Prog. Biomed.,
88(3): 283?294.
187
