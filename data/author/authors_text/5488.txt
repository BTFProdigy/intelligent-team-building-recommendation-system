Discourse chunking: a tool in dialogue act tagging
T. Daniel Midgley
School of Computer Science and Software Engineering
Discipline of Linguistics
University of Western Australia
dmidgley@arts.uwa.edu.au
Abstract
Discourse chunking is a simple way to
segment dialogues according to how dia-
logue participants raise topics and negoti-
ate them. This paper explains a method
for arranging dialogues into chunks, and
also shows how discourse chunking can
be used to improve performance for a
dialogue act tagger that uses a case-based
reasoning approach.
1 Dialogue act tagging
A dialogue act (hereafter DA) is an encapsulation
of the speaker?s intentions in dialogue?what the
speaker is trying to accomplish by saying some-
thing. In DA tagging (similar to part-of-speech
tagging), utterances in a dialogue are tagged with
the most appropriate speech act from a tagset. DA
tagging has application in NLP work, including
speech recognition and language understanding.
The Verbmobil-2 corpus was used for this
study, with its accompanying tagset, shown in
Table 1.1.
Much of the work in DA tagging (Reithinger,
1997; Samuel, 2000; Stolcke et al 2000; Wright,
1998) uses lexical information (the words or n-
grams in an utterance), and to a lesser extent
syntactic and phonological information (as with
prosody). However, there has traditionally been a
lack of true discourse-level information in tasks
involving dialogue acts. Discourse information is
typically limited to looking at surrounding DA tags
(Reithinger, 1997; Samuel, 2000). Unfortunately,
knowledge of prior DA tags does not always
translate to an accurate guess of what?s coming
next, especially when this information is imperfect.
Theories about the structure of dialogue (for
example, centering [Grosz, Joshi, & Weinstein
1995], and more recently Dialogue Macrogame
Theory [Mann 2002]) have not generally been
applied to the DA tagging task. Their use amounts
to a separate tagging task of its own, with the
concomitant time-consuming corpus annotation.
In this work, I present the results from a DA
tagging project that uses a case-based reasoning
system (after Kolodner 1993). I show how the
results from this DA tagger are improved by the
use of a concept I call ?discourse chunking.?
Discourse chunking gives information about the
patterns of topic raising and negotiation in dia-
Tag Example
ACCEPT sounds good to me
BACKCHANNEL mhm
BYE see you
CLARIFY I said the third
CLOSE okay <uhm> so I guess that is it
COMMIT I will get that arranged then
CONFIRM well I will see you <uhm> at the
airport on the third
DEFER and I will get back to you on that
DELIBERATE so let us see
DEVIATE_SCENARIO oh I have tickets for the opera on
Friday
EXCLUDE January is basically shot for me
EXPLAINED_REJECT I am on vacation then
FEEDBACK gosh
FEEDBACK_NEGATIVE not really
FEEDBACK_POSITIVE okay
GIVE_REASON because that is when the express
flights are
GREET hello Miriam
INFORM <uhm> I I have a list of hotels
here
INIT so we need to schedule a trip to
Hanover
INTRODUCE Natalie this is Scott
NOT_CLASSIFIABLE and <uh>
OFFER <uhm> would you like me to call
POLITENESS_FORMULA good of you to stop by
REFER_TO_SETTING want to step into your office since
we are standing right outside of it
REJECT no that is bad for me unfortunately
REQUEST you think so?
REQUEST_CLARIFY I thought we had said twelve noon
REQUEST_COMMENT is that alright with you
REQUEST_COMMIT can you take care of <uhm>
arranging those reservations
REQUEST_SUGGEST do you have any preference
SUGGEST we could travel on a Monday
THANK okay thanks John
Table 1.1. The tagset for the Verbmobil-2 corpus.
(Verbmobil 2003)
logue, and where an utterance fits within these
patterns. It is also able to use existing DA tag
information within the corpus, without the need for
separate annotation.
2 Discourse chunking
In order to accomplish a mutual goal (for example,
two people trying to find a suitable appointment
time), dialogue participants engage in predictable
kinds of activity, structuring the conversation in a
coherent way in order to accomplish their goals.
Alexandersson et al (1997) have noted that
these conversations tend to follow certain patterns,
particularly with regard to the way that topics get
raised and dealt with:
Hello The dialogue participants greet each other. They
introduce themselves, unveil their affiliation, or the
institution or location they are from.
Opening The topic to be negotiated is introduced.
Negotiation The actual negotiation, between opening
and closing.
Closing The negotiation is finished (all participants
have agreed), and the agreed-upon topic is (sometimes)
recapitulated.
Good Bye The dialogue participants say good bye to
each other.
Within a conversation, the opening-negotiation-
closing steps are often repeated in a cyclical pat-
tern.
This work on discourse chunking combines the
opening, negotiation, and closing sections into a
single chunk. One reason for this is that these parts
of the conversation tend to act as a single chunk;
when they appear, they regularly appear together
and in the same order. Also, some of these parts
may be missing; a topic of negotiation is frequently
brought up and resolved without an explicit open-
ing or closing. Very often, the act of beginning a
topic of negotiation defines the opening by itself,
and the act of beginning a new negotiation entails
the closing of the previous one.
A slightly simplified model of conversation,
then, appears in Figure 2.1.
In this model, participants greet each other, en-
gage in a series of negotiations, and finish the
conversation when the goals of the dialogue are
satisfied.
These three parts of the conversation are ?dia-
logue chunks?. These chunks are relevant from a
DA tagging perspective. For example, the DA tags
used in one of these chunks are often not used in
other chunks. For an obvious example, it would be
almost unheard of for the GREET tag to appear in
the ?Good Bye? chunk. Other DA?s (such as
FEEDBACK_POSITIVE) can occur in any of the
three chunks. Knowing which chunk we are in, and
where we are within a chunk, can facilitate the
tagging task.
Within chunks, some patterns emerge. Note that
in the example from the Verbmobil-2 corpus
(shown in Table 2.1), a negotiation topic is raised,
and dealt with (by an ACCEPT speech act). Then
t h e r e  f o l l o w s  a  s e q u e n c e  o f
FEEDBACK_POSITIVEs as the negotiation topic
winds down. This ?winding down? activity is
common at the end of a negotiation chunk. Then a
new topic is raised, and the process continues.
One-word utterances such as ?okay? or ?yeah?
are particularly problematic in this kind of task
because they have rather general semantic content
and they are commonly used in a wide range of
contexts. The word ?yeah? on its own, for exam-
ple, can indicate acceptance of a proposition, mere
Speaker ID Words DA Tag
KNT some other time oh
actually I see that I
have got some free
time in like the fifth
sixth and seventh of
January
SUGGEST
KNT how does that NOT_CLASSI
FIABLE
LMT yeah that is fine ACCEPT
KNT great so let us do that
then
FEEDBACK_
POSITIVE
LMT okay FEEDBACK_
POSITIVE
KNT okay FEEDBACK_
POSITIVE
LMT okay good FEEDBACK_
POSITIVE
Table 2.1 An example of tagged conversation from
the Verbmobil-2 corpus.
Hello
Negotiation
Good Bye
Figure 2.1. A slightly simplified model of conversation.
acknowledgement of a proposition, feedback,
deliberation, or a few of these at once (Core &
Allen 1997). In Verbmobil-2, these utterances can
b e  l a b e l e d  e i t h e r  A C C E P T ,
FEEDBACK_POSITIVE, BACK-CHANNEL, or
REQUEST_COMMENT. Without knowing where
the utterance appears within the structure of the
dialogue, these utterances are very difficult to
classify.
Some previous work has used prosody to solve
this kind of problem (as with Stolcke 2000). I
propose discourse chunks as an alternative method.
It can pull information from the text alone, without
the computational overhead that prosody can
entail.
3 Chunk segmentation
Just where do the discourse chunk boundaries lie?
For this exercise, I have constructed a very simple
set of rules to determine chunk boundaries. These
rules come from my observations; future work will
involve automatic chunk segmentation. However,
these rules do arise from a principled assumption:
the raising of a new topic shows the beginning of a
discourse chunk. Therefore, a speech act that
(according to the definitions in Alexandersson
1997) contains a topic or proposition represents the
beginning of a discourse chunk.
By definition, only four DA?s contain or may
contain a topic or proposition. These are INIT,
EXCLUDE, REQUEST_SUGGEST, and SUGGEST.
3.1 Chunking rules
The chunking rules are as follows:
1. The first utterance in a dialogue is always the
start of chunk 1 (hello).
2. The first I N I T  or S U G G E S T  or
REQUEST_SUGGEST or EXCLUDE in a dia-
logue is the start of chunk 2 (negotiation).
3. INIT, SUGGEST, REQUEST_SUGGEST, or
EXCLUDE marks the start of a subchunk within
chunk 2.
4. If the previous utterance is also the start of a
chunk, and if it is spoken by the same person,
then this utterance is considered to be a con-
tinuation of the chunk, and is not marked.
5. The first BYE is the start of chunk 3 (good
bye).
Items within a chunk are numbered evenly from 1
(the first utterance in a chunk) to 100 (the last), as
shown in Table 3.1. This normalizes the chunk
distances to facilitate comparison between utter-
ances.
4 The case-based reasoning (CBR) tagger
A thorough discussion of this CBR tagger goes
beyond the scope of this paper, but a few com-
ments are in order.
Case-based reasoning (Kolodner 1993)  is a
form of machine learning that uses examples. In
general, classification using a case-based reasoner
involves comparing new instances (in this case,
utterances) against a database of correctly-tagged
instances. Each new instance is marked with the
same tag of its ?nearest neighbour? (that is, the
closest match) from the database. A k-nearest
neighbour approach selects the closest k matches
from the database to be committee members, and
the committee members ?vote? on the correct
classification. In this implementation, each com-
mittee member gets a vote equal to its similarity to
the test utterance. Different values of k performed
better in different aspects of the test, but this work
uses k = 7 to facilitate comparison of results.
Spkr
ID
Words Discourse
Chunk
DA Tag
KNT some other time
oh actually I see
that I have got
some free time in
like the fifth sixth
and seventh of
January
1 SUGGEST
KNT how does that 17.5 NOT_CLASS
IFIABLE
LMT yeah that is fine 34 ACCEPT
KNT great so let us do
that then
50.5 FEEDBACK_
POSITIVE
LMT okay 67 FEEDBACK_
POSITIVE
KNT okay 83.5 FEEDBACK_
POSITIVE
LMT okay good 100 FEEDBACK_
POSITIVE
Table 3.1 An example from the corpus, now tagged
with discourse chunks.
The choice of features largely follows those of
Samuel 2000, and are as follows:
? Speaker change
? Word number
? Word similarity
? n-gram similarity
? Previous DA tag
and the following two features not included in
that study,
? 2-previous DA tag
Inclusion of this feature enables more complete
analysis of previous DA tags. Both ?previous DA
tag? and ?2-previous DA tag? features use the ?best
guess? for previous utterances rather than the
?right answer?, so this run allows us to test per-
formance even with incomplete information.
? Discourse chunk tag
Distances for this tag were computed by dividing
the larger discourse chunk number from the
smaller. Comparing two ?chunk starter? utterances
would give the highest similarity of 1, and com-
paring a chunk starter (1) to a chunk-ender (100)
would give a lower similarity (.01).
Not all features are equally important, and so an
Evolutionary Programming algorithm (adapted
from Fogel 1994) was used to weight the features.
Weightings were initially chosen randomly for
each member of a population of 100, and the 10
best performers were allowed to ?survive? and
?mutate? their weightings by a Gaussian random
number. This was repeated for 10 generations, and
the weightings from the highest performer were
used for the CBR tagging runs.
A total of ten stopwords were used (the, of, and,
a, an, in, to, it, is, was), the ten most common
words from the BNC (Leech, Rayson, & Wilson
2001). These stopwords were removed when
considering word similarity, but not n-gram simi-
larity, since these low-content words are useful for
distinguishing sequences of words that would
otherwise be very similar.
The database consisted of 59 hand-tagged dia-
logues (8398 utterances) from the Verbmobil-2
corpus. This database was also automatically
tagged with discourse chunks according to the
rules above. The test corpus consisted of 20 dia-
logues (2604 utterances) from Verbmobil-2. This
corpus was tagged with correct information on
discourse chunks; however, no information was
given on the DA tags themselves.
5 Discussion and future work
Table 5.1 shows the results from two DA tagging
runs using the case-based reasoning tagger: one
run without discourse chunks, and one with.
Without discourse chunks With discourse chunks
53.68%
(1385/2604 utterances)
65.44%
(1704/2604 utterances)
Table 5.1: Overall accuracy for the CBR tagger
To put these results in perspective, human per-
formance has been estimated at about 84% (Stol-
cke 2000), since human taggers sometimes
disagree about intentions, especially when speakers
perform more than one dialogue act in the same
utterance. Much of the recent DA tagging work
(using 18-25 tags) scores around the mid-fifty to
mid-sixty percentiles in accuracy (see Stolcke 2000
for a review of similar work). This work uses the
Verbmobil-2 tagset of 32 tags.
It could be argued that the discourse chunk in-
formation, being based on tags, gives the DA
tagger extra information about the tags themselves,
and thus gives an unfair ?boost? to the perform-
ance. At present it is difficult to say if this is the
only reason for the performance gains. If this were
the case, we would expect to see improvement in
recognition for the four tags that are ?chunk start-
ers?, and less of a gain in those that are not.
In the test run with discourse chunks, however,
we see across-the-board gains in almost all catego-
ries, regardless of whether they begin a chunk or
not. Table 5.2 shows performance measured in
terms of the well-known standards of precision,
recall, and f-measure.
One notable exception to the upward trend is
EXCLUDE, a beginning-of-chunk marker, which
performed slightly worse with discourse chunks.
This would suggest that chunk information alone is
not enough to account for the overall gain. Both
ACCEPT and FEEDBACK_POSITIVE improved
slightly, suggesting that discourse chunks were
able to help disambiguate these two very similar
tags.
Table 5.3 shows the improvement in tagging
scores for one-word utterances, often difficult to
tag because of their general use and low informa-
tion. These words are more likely to be tagged
ACCEPT when they appear near the beginning of a
chunk, and FEEDBACK_POSITIVE when they
appear nearer the end.  Discourse chunks help their
classification by showing their place in the dia-
logue cycle.
One weakness of this project is that it assumes
knowledge of the correct chunk tag. The test
corpus was tagged with the ?right answers? for the
chunks. Under normal circumstances, the corpus
would be tagged with the ?best guess,? based on
the DA tags from an earlier run. However, the goal
for this project was to see if, given perfect infor-
mation, discourse chunking would aid DA tagging
performance. The performance gains are persua-
sive evidence that it does. Ongoing work involves
seeing how accurately a new corpus can be tagged
with discourse chunks, even when the DA tags are
unknown.
6 Acknowledgements
This work was supported by an Australian Post-
graduate Award. Thanks to Cara MacNish and
Shelly Harrison for supervision and advice. Many
thanks to Verbmobil for generously allowing use
of the corpus which formed the basis of this pro-
ject.
References
J. Alexandersson, B. Buschbeck-Wolf, T. Fujinami, E.
Maier, N. Reithinger, B. Schmitz, and M. Siegel.
1997. Dialogue Acts in Verbmobil-2. Verbmobil Re-
port 204.
M. G. Core, and J. F. Allen. 1997. Coding dialogs with
the DAMSL annotation scheme. In Working Notes of
the AAAI Fall Symposium on Communicative Action
in Humans and Machines. Cambridge, MA.
D. Fogel. 1994. An introduction to evolutionary com-
putation. Australian Journal of Intelligent Informa-
tion Processing Systems, 2:34?42.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Cen-
tering: A framework for modelling the local coher-
ence of discourse. Computational Linguistics,
21(2):203?225
J. Kolodner. 1993. Case-Based Reasoning. Academic
Press/Morgan Kaufmann.
G. Leech, P. Rayson, and A. Wilson. 2001. Word
Frequencies in Written and Spoken English: based
on the British National Corpus. Longman.
W. Mann. 2002. Dialogue Macrogame Theory. In
Proceedings of the 3rd SIGdial Workshop on Dis-
course and Dialogue, pages 129?141, Philadelphia
PA.
N. Reithinger and M. Klesen. 1997. Dialogue act
classification using language models. In G. Kokki-
nakis, N. Fakotakis, and E. Dermatas, editors, Pro-
ceedings of the 5th European Conference on Speech
Communication and Technology, volume 4, pages
2235-2238, Rhodes, Greece.
K. Samuel. 2000. Discourse learning: An investigation
of Dialogue Act tagging using transformation-based
learning. Ph.D. thesis, University of Delaware.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, M. Meteer. 2000. Dialogue act modeling
for automatic tagging and recognition of conversa-
t ional  speech.  Computational Linguistics,
26(3):339?373.
Verbmobil. 2003. ?Verbmobil? [online]. Available:
<http://verbmobil.dfki.de/>.
H. Wright. 1998. Automatic utterance type detection
using suprasegmental features. In ICSLP (Interna-
tional Conference on Spoken Language Processing)
'98. Sydney, Australia.
Without discourse chunks With discourse chunksTag precision recall f-measure precision recall f-measure
INIT 0.590 0.411 0.484 0.735 0.446 0.556
SUGGEST 0.446 0.399 0.421 0.778 0.912 0.839
REQUEST_SUGGEST 0.308 0.078 0.125 0.550 0.216 0.310
EXCLUDE 0.500 0.063 0.111 0.143 0.031 0.051
GREET 0.926 0.926 0.926 0.926 0.926 0.926
BACKCHANNEL 0.824 0.875 0.848 0.824 0.875 0.848
BYE 0.719 0.976 0.828 0.816 0.952 0.879
POLITENESS_FORMULA 0.821 0.742 0.780 0.889 0.774 0.828
THANK 0.875 0.636 0.737 0.875 0.636 0.737
FEEDBACK_POSITIVE 0.567 0.843 0.678 0.615 0.839 0.710
COMMIT 0.778 0.500 0.609 0.733 0.393 0.512
DELIBERATE 0.568 0.582 0.575 0.600 0.570 0.584
INFORM 0.493 0.682 0.572 0.655 0.812 0.725
FEEDBACK_NEGATIVE 0.700 0.304 0.424 0.667 0.348 0.457
REQUEST_COMMENT 0.425 0.327 0.370 0.500 0.288 0.366
REJECT 0.500 0.278 0.357 0.316 0.333 0.324
NOT_CLASSIFIABLE 0.534 0.265 0.354 0.696 0.274 0.393
DEFER 0.750 0.214 0.333 0.800 0.286 0.421
ACCEPT 0.392 0.290 0.333 0.476 0.429 0.451
REQUEST 0.351 0.191 0.248 0.525 0.456 0.488
REQUEST_CLARIFY 0.400 0.130 0.197 0.600 0.196 0.295
EXPLAINED_REJECT 0.333 0.133 0.190 0.600 0.600 0.600
GIVE_REASON 0.200 0.077 0.111 0.182 0.077 0.108
CLOSE 0.333 0.063 0.105 0.500 0.063 0.111
CLARIFY 0.400 0.056 0.098 0.000 0.000 0.000
CONFIRM 0.000 0.000 0.000 0.500 0.074 0.129
DEVIATE_SCENARIO 0.000 0.000 0.000 0.000 0.000 0.000
Table 5.2: Results for all DA types that appeared more than ten times in the corpus. The first group of four DA?s
represents those that signal the beginning of a discourse chunk; the second group shows those that do not.
Percent classified correctly without
discourse chunk information
Percent classified correctly with
discourse chunk information
okay 71.90 (151/210) 75.24 (158/210)
yeah 69.90 (72/103) 74.76 (77/103)
right 62.16 (23/37) 72.97 (27/37)
mhm 88.23 (60/68) 88.23 (60/68)
bye 93.33 (14/15) 93.33 (14/15)
Table 5.3: Some examples of one-word utterances in the corpus, before and after discourse chunking.
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 104?108,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Empirical Verification of Adjacency Pairs Using Dialogue
Segmentation
T. Daniel Midgley Shelly Harrison Cara MacNish
Discipline of Linguistics,
University of Western Australia
(dmidgley, shelley)@cyllene.uwa.edu.au
School of Computer Science and
Software Engineering,
University of Western Australia
cara@csse.uwa.edu.au
Abstract
A problem in dialogue research is that of
finding and managing expectations.
Adjacency pair theory has widespread
acceptance, but traditional classification
features (in particular, ?previous-tag?
type features) do not exploit this
information optimally. We suggest a
method of dialogue segmentation that
verifies adjacency pairs and allows us to
use dialogue-level information within the
entire segment and not just the previous
utterance. We also use the ?2 test for
statistical significance as ?noise
reduction? to refine a list of pairs.
Together, these methods can be used to
extend expectation beyond the traditional
classification features. 
1 Introduction
Adjacency pairs have had a long history in
dialogue research. The pairs of question/answer,
inform/backchannel, and others have been well-
known ever since they were proposed by Sacks
and Schegloff in 1973. They have been used by
dialogue researchers to assist in knowing ?what
comes next? in dialogue.
Unfortunately, this dialogue information has
been difficult to leverage. Most dialogue act
(DA) classification research uses some kind of
dialogue history, but this usually takes the form
of some kind of ?previous tag? feature, perhaps
even ?two-previous tag?. Dialogue information
from three or more utterances previous is not
normally used because, in the words of one
researcher, ?[n]o benefit was found from using
higher-order dialog grammars? (Venkataraman
et al 2002). This could be due to the sparse data
problem; more permutations means fewer
repetitions.
Part of the problem, then, may lie in the way
the ?previous tag? feature is used. Consider the
following example from the Verbmobil-2 corpus
(Verbmobil 2006)1:
A: how does does November
fourteenth and fifteenth look
SUGGEST
B: no REJECT
Here, the second pair part occurs directly after
the first pair part that occasioned it. But
sometimes performance factors intervene as in
the following example, where B is engaging in
floor-holding using a dialogue act annotated here
as DELIBERATE:
A: so that maybe I if I need to if I
need to order like a limo or
something
SUGGEST
B: <hes> let us see DELIBERATE
B: the this is the <hes> wrong
month
DELIBERATE
B: the third DELIBERATE
B: let us see DELIBERATE
B: I don't have anything scheduled
that morning and we are
leaving at one
INFORM
The response (INFORM) finally comes, but the
forgetful ?previous tag? feature is now looking
for what comes after DELIBERATE.
What is needed is a way to not only
determine what is likely to happen next, but to
retain that expectation over longer distances
when unfulfilled, until that expectation is no
longer needed. Such information would conform
more closely to this description of a
conversational game (but which could be applied
to any communicative subgoal):
1For a full description of the Verbmobil speech
acts, see Alexandersson 1997.
104
A conversational game is a sequence of
moves starting with an initiation and
encompassing all moves up until that
initiation?s purpose is either fulfilled or
abandoned. (Carletta 1997, italics mine.)
2 Dialogue segmentation
This work grew out of related research into
finding expectations in dialogue, but we were
also interested in dialogue segmentation.
Dialogues taken as a whole are very different
from each other, so segmentation is necessary to
derive meaningful information about their parts.
The question is, then, how best to segment
dialogues so as to reveal dialogue information or
to facilitate some language task, such as DA
classification?
Various schemes for dialogue segmentation
have been tried, including segmentation based
on fulfilment of expectation (Ludwig et al
1998), and segmenting by propositionality
(Midgley 2003).
One answer to the question of how to
segment dialogue came from the pioneering
work of Sacks and Schegloff (1973) article.
A basic rule of adjacency pair operation is:
given the recognizable production of a first
pair part, on its first possible completion its
speaker should stop and a next speaker
should start and produce a second pair part
from the same pair type of which the first is
recognizably a member. (p. 296, italics
mine.)
Thus, if a speaker stops speaking, it is likely that
such a handover has just taken place. The last
utterance of a speaker?s turn, then, will be the
point at which the first speaker has issued a first
pair part, and is now expecting a second pair part
from the other speaker. This suggests a natural
boundary.
This approach was also suggested by Wright
(1998), who used a ?most recent utterance by
previous speaker? feature in her work on DA
tagging. This feature alone has boosted
classification accuracy by about 2% in our
preliminary research, faring better than the
traditional ?previous tag? feature used in much
DA tagging work.
We collected a training corpus of 40
English-speaking dialogues from the
Verbmobil-2 corpus, totalling 5,170 utterances.
We then segmented the dialogues into chunks,
where a chunk included everything from the last
utterance of one speaker?s turn to the last-but-
one utterance of the next speaker.
3 Results of segmentation
This segmentation revealed some interesting
patterns. When ranked by frequency, the most
common chunks bear a striking resemblance to
the adjacency pairs posited by Schegloff and
Sacks. 
Here are the 25 most common chunks in our
training corpus, with the number of times they
appeared. The full list can be found at http:/
/www.csse.uwa.edu.au/~fontor/research/chi/
fullseg.txt
SUGGEST:ACCEPT 176
INFORM:FEEDBACK_POSITIVE 166
FEEDBACK_POSITIVE:FEEDBACK_POSITIVE
104
FEEDBACK_POSITIVE:INFORM 97
ACCEPT:FEEDBACK_POSITIVE 65
FEEDBACK_POSITIVE:SUGGEST 60
INFORM:INFORM 57
REQUEST:INFORM 46
INFORM:BACKCHANNEL 41
INFORM:SUGGEST 40
REQUEST_COMMENT:FEEDBACK_POSITIVE 40
INIT:FEEDBACK_POSITIVE 35
BYE:NONE 34
ACCEPT:INFORM 32
BYE:BYE 31
REQUEST:FEEDBACK_POSITIVE 30
POLITENESS_FORMULA:FEEDBACK_POSITIVE
29
REQUEST_CLARIFY:FEEDBACK_POSITIVE 28
BACKCHANNEL:INFORM 28
NOT_CLASSIFIABLE:INFORM 28
REQUEST_SUGGEST:SUGGEST 28
NONE:GREET 27
SUGGEST:SUGGEST 27
ACCEPT:SUGGEST 26
SUGGEST:REQUEST_CLARIFY 26
The data suggest a wide variety of language
behaviour, including traditional adjacency pairs
(e.g. SUGGEST: ACCEPT), acknowledgement
(INFORM: BACKCHANNEL), formalised
exchanges (POLITENESS_FORMULA:
FEEDBACK_POSITIVE) offers and counter-
offers (SUGGEST: SUGGEST), and it even
hints at negotiation subdialogues (SUGGEST:
REQUEST_CLARIFY). 
However, there are some drawbacks to this
list. Some of the items are not good examples of
adjacency pairs because the presence of the first
does not create an expectation for the second
half (e.g. NOT_CLASSIFIABLE: INFORM). In
105
some cases they appear backwards (ACCEPT:
SUGGEST). Legitimate pairs appear further down
the list than more-common bogus ones. For
example, SUGGEST: REJECT is a well-known
adjacency pair, but it does not appear on the list
until after several less-worthy-seeming pairs.
Keeping the less-intuitive chunks may help us
with classification, but it falls short of providing
empirical verification for pairs.
What we need, then, is some kind of noise
reduction that will strain out spurious pairs and
bring legitimate pairs closer to the top of the list.
We use the well-known ?2 test for statistical
significance.
4 The ?2 test
The ?2 test tells how the observed frequency of
an event compares with the expected frequency.
For our purposes, it tells whether the observed
frequency of an event (in this case, one kind of
speech act following a certain other act) can be
attributed to random chance. The test has been
used for such tasks as feature selection (Spitters
2000) and translation pair identification (Church
and Gale 1991).
The ?2 value for any two speech acts A and B
can be calculated by counting the times that an
utterance marked as tag A (or not) is followed by
an utterance marked as tag B (or not), as in
Table 1.
Ui = A Ui ? A
Ui+1 = B AB ?AB
Ui+1 ? B A?B ?A?B
Table 1. Obtaining counts for ?2.
These counts (as well as N, the total number
of utterances) are plugged into a variant of the ?2
equation used for 2x2 tables, as in Sch?tze et al
(1995).
?2=
N(AB ? ?A?B - A?B ? ?AB)
(AB + A?B)(AB + ?AB)(A?B + ?A?B)(?AB + ?A?B)
We trained the ?2 method on the aforementioned
chunks. Rather than restrict our focus to only
adjacent utterances, we allowed a match for pair
A:B if B occurred anywhere within the chunk
started by A. By doing so, we hoped to reduce
any acts that may have been interfering with the
adjacency pairs, especially hesitation noises
(usually classed as DELIBERATE) and
abandoned utterances (NOT_CLASSIFIABLE).
5 Results for ?2
Here are the 25 pairs with the highest ?2 scores.
With tail probability p = .0001, a ?2 value >
10.83 is statistically significant. The full list can
be found at http://www.csse.uwa.edu.au/~fontor/
research/chi/fullchi.txt.
NONE:GREET 1576.87
BYE:NONE 949.89
SUGGEST:ACCEPT 671.81
BYE:BYE 488.60
NONE:POLITENESS_FORMULA 300.46
POLITENESS_FORMULA:
POLITENESS_FORMULA 272.95
GREET:GREET 260.69
REQUEST_CLARIFY:CLARIFY 176.63
CLARIFY:CLARIFY 165.76
DEVIATE_SCENARIO: DEVIATE_SCENARIO
159.45
SUGGEST:FEEDBACK_POSITIVE 158.12
COMMIT:COMMIT 154.46
GREET:POLITENESS_FORMULA 111.19
INFORM:FEEDBACK_POSITIVE 84.82
REQUEST_SUGGEST:SUGGEST 83.17
SUGGEST:REJECT 83.11
THANK:THANK 76.25
SUGGEST:EXPLAINED_REJECT 69.31
POLITENESS_FORMULA:INIT 67.76
NONE:INIT 59.97
FEEDBACK_POSITIVE:ACCEPT 59.41
DEFER:ACCEPT 56.07
THANK:BYE 51.82
POLITENESS_FORMULA:THANK 50.21
POLITENESS_FORMULA:GREET 45.17
Using ?2 normalises the list; low-frequency acts
like REJECT and EXPLAINED_REJECT now
appear as a part of their respective pairs.
These results give empirical justification for
Sacks and Schegloff?s adjacency pairs, and
reveals more not mentioned elsewhere in the
literature, such as DEFER:ACCEPT. As such, it
gives a good idea of what kinds of speech acts
are expected within a chunk.
In addition, these results can be plotted into a
directed acyclic graph (seen in Figure 1). This
graph can be used as a sort of conversational
map.
6 Conclusions and Future Work
We can draw some tentative conclusions from
this work. First of all, the dialogue segmentation
combined with the ?2 test for significance yields
information about what is likely to happen, not
just for the next utterance, but somewhere in the
next chunk. This will help to overcome the
limitations imposed by the traditional ?previous
106
tag? feature. We are working to implement this
information into a model where the expectations
inherent in a first pair part are retained when not
immediately fulfilled. The expectations will also
decay with time.
Second, this approach provides empirical
evidence for adjacency pairs mentioned in the
literature on conversation analysis. The noise
reduction feature of the ?2 test gives more weight
to legitimate adjacency pairs where they appear
in the data.
An intriguing possibility for the chunked
data is that of chunk matching. Nearest-
neighbour algorithms are already used for
classification tasks (including DA tagging for
individual utterances), but once segmented, the
dialogue chunks could be compared against each
other as a classification tool as in a nearest-
neighbour algorithm.
References
J. Alexandersson, B. Buschbeck-Wolf, T.
Fujinami, E. Maier, N. Reithinger, B.
Schmitz, and M. Siegel. 1997. Dialogue acts
in Verbmobil-2. Verbmobil Report 204.
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G.
Doherty-Sneddon, and A. H. Anderson.
1997. The reliability of a dialogue structure
coding scheme. Computational Linguistics,
23(1):13--31.
K. W. Church and W. A. Gale. 1991.
Concordances for parallel text. In
Proceedings of the Seventh Annual
Conference of the UW Centre for the New
OED and Text Research, pages 40?62,
Oxford.
D. Midgley. 2003. Discourse chunking: a tool
for dialogue act tagging. In ACL-03
Companion Volume to the Proceedings of
the Conference, pages 58?63, Sapporo,
Japan.
E. A. Schegloff. and H. Sacks. 1973. Opening up
closings. Semiotica, 8(4):289?327.
H. Sch?tze, D. Hull, and J. Pedersen. 1995. A
comparison of classifiers and document
representations for the routing problem. In
Proceedings of SIGIR ?95, pages 229?237.
M. Spitters. 2000. ?Comparing feature sets for
learning text categorization.? In Proceedings
of RIAO 2000, April, 2000.
A. Venkataraman, A. Stolcke, E. Shriberg.
Automatic dialog act labeling with minimal
supervision. In Proceedings of the 9th
Australian International Conference on
Speech Science and Technology, Melbourne,
Australia, 2002.
Verbmobil. 2006. ?Verbmobil? [online].
Available <http://verbmobil.dfki.de/>.
H. Wright. 1998. Automatic utterance type
detection using suprasegmental features. In
ICSLP (International Conference on Spoken
Language Processing) ?98. Sydney,
Australia.
107
Figure 1. A directed acyclic graph using the ?2 data for the 40 highest pairs. For any pair of connected
nodes, the first node represents the last utterance in a speaker?s turn, and the second could be any
utterance in the other speaker?s turn. The numbers are ?2 scores. For illustrative purposes, higher ?2
values are shown by bold lines. The complete graph can be found at http://www.csse.uwa.edu.au/
~fontor/research/chi/fullchart.jpg
108
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 897?904,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dialogue Segmentation with Large Numbers of Volunteer Internet 
Annotators
T. Daniel Midgley
Discipline of Linguistics, School of Computer Science and Software Engineering
University of Western Australia
Perth, Australia
Daniel.Midgley@uwa.edu.au
Abstract
This paper shows the results of an 
experiment in dialogue segmentation. In this 
experiment, segmentation was done on a 
level of analysis similar to adjacency pairs. 
The method of annotation was somewhat 
novel: volunteers were invited to participate 
over the Web, and their responses were 
aggregated using a simple voting method. 
Though volunteers received a minimum of 
training, the aggregated responses of the 
group showed very high agreement with 
expert opinion. The group, as a unit, 
performed at the top of the list of 
annotators, and in many cases performed as 
well as or better than the best annotator.
1 Introduction
Aggregated human behaviour is a valuable 
source of information. The Internet  shows us 
many examples of collaboration as a means of 
resource creation. Wikipedia, Amazon.com 
reviews, and Yahoo! Answers are just some 
examples of large repositories of information 
powered by individuals who voluntarily 
contribute their time and talents. Some NLP 
projects are now using this idea, notably the 
?ESP  Game? (von Ahn 2004), a data collection 
effort presented as a game in which players label 
images from the Web. This paper presents an 
extension of this collaborative volunteer ethic in 
the area of dialogue annotation.
 For dialogue researchers, the prospect  of 
using volunteer annotators from the Web can be 
an attractive option. The task of training 
annotators can be time-consuming, expensive, 
and (if inter-annotator agreement turns out  to be 
poor) risky.
 Getting Internet volunteers for annotation has 
its own pitfalls. Dialogue annotation is often not 
very interesting, so it can be difficult  to attract 
willing participants. Experimenters will have 
little control over the conditions of the 
annotation and the skill of the annotators. 
Training will be minimal, limited to whatever an 
average Web surfer is willing to read. There may 
also be perverse or uncomprehending users 
whose answers may skew the data.
 This project began as an exploratory study 
about the intuitions of language users with 
regard to dialogue segmentation. We wanted 
information about how language users perceive 
dialogue segments, and we wanted to be able to 
use this information as a kind of gold standard 
against  which we could compare the 
performance of an automatic dialogue 
segmenter. For our experiment, the advantages 
of Internet  annotation were compelling. We 
could get  free data from as many language users 
as we could attract, instead of just two or three 
well-trained experts. Having more respondents 
meant  that our results could be more readily 
generalised to language users as a whole. 
 We expected that  multiple users would 
converge upon some kind of uniform result. 
What  we found (for this task at least) was that 
large numbers of volunteers show very strong 
tendencies that  correspond well to expert 
opinion, and that  these patterns of agreement  are 
surprisingly resilient in the face of noisy input 
from some users. We also gained some insights 
into the way that people perceived dialogue 
segments.
2 Segmentation
While much work in dialogue segmentation 
centers around topic (e.g. Galley et  al. 2003, 
Hsueh et  al. 2006, Purver et  al. 2006), we 
decided to examine dialogue at a more fine-
grained level. The level of analysis that we have 
chosen corresponds most closely to adjacency 
pairs (after Sacks, Schegloff and Jefferson 
1974), where a segment is made of matched sets 
of utterances from different speakers (e.g. 
question/answer or suggest/accept). We chose to 
segment  dialogues this way in order to improve 
dialogue act  tagging, and we think that 
897
examining the back-and-forth detail of the 
mechanics of dialogue will be the most helpful 
level of analysis for this task. 
 The back-and-forth nature of dialogue also 
appears in Clark and Schaefer?s (1989) 
influential work on contributions in dialogue. In 
this view, two-party dialogue is seen as a set of 
cooperative acts used to add information to the 
c o m m o n g r o u n d f o r t h e p u r p o s e o f 
accomplishing some joint  action. Clark and 
Schaefer map these speech acts onto 
contribution trees. Each utterance within a 
contribution tree serves either to present some 
proposition or to acknowledge a previous one. 
Accordingly, each contribution tree has a 
presentation phase and an acceptance phase. 
Participants in dialogue assume that items they 
present  will be added to the common ground 
unless there is evidence to the contrary. 
However, participants do not  always show 
acceptance of these items explicitly. Speaker B 
may repeat Speaker?s A?s information verbatim 
to show understanding (as one does with a 
phone number), but  for other kinds of 
information a simple ?uh-huh? will constitute 
adequate evidence of understanding. In general, 
less and less evidence will be required the 
farther on in the segment one goes. 
 In practice, then, segments have a tailing-off 
quality that we can see in many dialogues. Table 
1 shows one example from Verbmobil-2, a 
corpus of appointment scheduling dialogues. (A 
description of this corpus appears in 
Alexandersson 1997.)
 A segment begins when WJH brings a 
question to the table (utterances 1 and 2 in our 
example), AHS answers it  (utterance 3), and 
WJH acknowledges the response (utterance 4). 
At this point, the question is considered to be 
resolved, and a new contribution can be issued. 
WJH starts a new segment in utterance 5, and 
this utterance shows features that  will be 
familiar to dialogue researchers: the number of 
words increases, as does the incidence of new 
words. By the end of this segment (utterance 8), 
AHS only needs to offer a simple ?okay? to show 
acceptance of the foregoing.
 Our work is not intended to be a strict 
implementation of Clark and Schaefer?s 
contribution trees. The segments represented by 
these units is what we were asking our volunteer 
annotators to find. Other researchers have also 
used a level of analysis similar to our own. 
J?nsson?s (1991) initiative-response units is one 
example.
 Taking a cue from Mann (1987), we decided 
to describe the behaviour in these segments 
using an atomic metaphor: dialogue segments 
have nuclei, where someone says something, 
and someone says something back (roughly 
corresponding to adjacency pairs), and satellites, 
usually shorter utterances that give feedback on 
whatever the nucleus is about. 
 For our annotators, the process was simply to 
find the nuclei, with both speakers taking part, 
and then attach any nearby satellites that 
pertained to the segment.
 We did not attempt to distinguish nested 
adjacency pairs. These would be placed within 
the same segment. Eventually we plan to modify 
our system to recognise these nested pairs.
3 Experimental Design
3.1 Corpus
In the pilot phase of the experiment, volunteers 
could choose to segment  up to four randomly-
chosen dialogues from the Verbmobil-2 corpus. 
(One longer dialogue was separated into two.) 
We later ran a replication of the experiment  with 
eleven dialogues. For this latter phase, each 
volunteer started on a randomly chosen dialogue 
to ensure evenness of responses.
 The dialogues contained between 44 and 109 
utterances. The average segment was 3.59 
utterances in length, by our annotation.
 Two dialogues have not been examined 
because they will be used as held-out  data for 
the next  phase of our research. Results from the 
1 WJH <uhm> basically we have to be 
in Hanover for a day and a half
2 WJH correct
3 AHS right
4 WJH okay
5 WJH <uh> I am looking through my 
schedule for the next three 
months
6 WJH and I just noticed I am working 
all of Christmas week
7 WJH so I am going to do it in 
Germany if at all possible
8 AHS okay
Table 1. A sample of the corpus. Two segments are 
represented here.
898
other thirteen dialogues appear in part  4 of this 
paper.
3.2 Annotators
Volunteers were recruited via postings on 
various email lists and websites. This included a 
posting on the university events mailing list, 
sent  to people associated with the university, but 
with no particular linguistic training. Linguistics 
first-year students and Computer Science 
students and staff were also informed of the 
project. We sent advertisements to a variety of 
international mailing lists pertaining to 
language, computation, and cognition, since 
these lists were most likely to have a readership 
that was interested in language. These included 
Linguist  List, Corpora, CogLing-L, and 
HCSNet. An invitation also appeared on the 
personal blog of the first author.
 At the experimental website, volunteers were 
asked to read a brief description of how to 
annotate, including the descriptions of nuclei 
and satellites. The instruction page showed some 
examples of segments. Volunteers were 
requested not to return to the instruction page 
once they had started the experiment.
 The annotator guide with examples can be 
seen at the following URL:
 http://tinyurl.com/ynwmx9
 A scheme that relies on volunteer annotation 
will need to address the issue of motivation. 
People have a desire to be entertained, but 
dialogue annotation can often be tedious and 
difficult. We attempted humor as a way of 
keeping annotators amused and annotating for as 
long as possible. After submitting a dialogue, 
annotators would see an encouraging page, 
sometimes with pretend ?badges? like the one 
pictured in Figure 1. This was intended as a way 
of keeping annotators interested to see what 
comments would come next. Figure 2 shows 
statistics on how many dialogues were marked 
by any one IP address. While over half of the 
volunteers marked only one dialogue, many 
volunteers marked all four (or in the replication, 
all eleven) dialogues. Sometimes more than 
eleven dialogues were submitted from the same 
location, most likely due to multiple users 
sharing a computer.
 In all, we received 626 responses from about 
231 volunteers (though this is difficult to 
determine from only the volunteers? IP 
numbers). We collected between 32 and 73 
responses for each of the 15 dialogues. 
3.3 Method of Evaluation
We used the WindowDiff (WD) metric (Pevzner 
and Hearst 2002) to evaluate the responses of 
our volunteers against  expert  opinion (our 
responses). The WD algorithm calculates 
agreement  between a reference copy of the 
corpus and a volunteer?s hypothesis by moving a 
window over the utterances in the two corpora. 
The window has a size equal to half the average 
segment length. Within the window, the 
algorithm examines the number of segment 
boundaries in the reference and in the 
hypothesis, and a counter is augmented by one if 
they disagree. The WD score between the 
reference and the hypothesis is equal to the 
number of discrepancies divided by the number 
of measurements taken. A score of 0 would be 
given to two annotators who agree perfectly, and 
1 would signify perfect disagreement.
 Figure 3 shows the WD scores for the 
volunteers. Most volunteers achieved a WD 
score between .15 and .2, with an average of 
?245. 
 Cohen?s Kappa (!) (Carletta 1996) is another 
method of comparing inter-annotator agreement 
0
30
60
90
120
150
1 2 3 4 5 6 7 8 9 10 11 >11
120
25
10
32
3 4 3 1 2 0
17
2
Nu
m
be
r o
f a
nn
ot
at
or
s
Number of dialogues completed
Figure 2. Number of dialogues annotated by single 
IP addresses
Figure 1. One of the screens that appears after an 
annotator submits a marked form.
899
in segmentation that is widely used in 
computational language tasks. It measures the 
observed agreement (AO) against the agreement 
we should expect by chance (AE), as follows:
! =
AO - AE
1 - AE
For segmentation tasks, ! is a more stringent 
method than WindowDiff, as it does not 
consider near-misses. Even so, ! scores are 
reported in Section 4.
 About a third of the data came from 
volunteers who chose to complete all eleven of 
the dialogues. Since they contributed so much of 
the data, we wanted to find out  whether they 
were performing better than the other 
volunteers. This group had an average WD score 
of .199, better than the rest  of the group at .268. 
However, skill does not  appear to increase 
smoothly as more dialogues are completed. The 
highest  performance came from the group that 
completed 5 dialogues (average WD = .187), the 
0
25
50
75
100
125
150
  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Nu
m
be
r o
f r
es
po
ns
es
WindowDiff range
lowest  from those that  completed 8 dialogues (.
299).
3.4 Aggregation
We wanted to determine, insofar as was 
possible, whether there was a group consensus 
as to where the segment boundaries should go. 
We decided to try overlaying the results from all 
respondents on top of each other, so that each 
click from each respondent  acted as a sort  of 
vote. Figure 4 shows the result  of aggregating 
annotator responses from one dialogue in this 
way. There are broad patterns of agreement; 
high ?peaks? where many annotators agreed that 
an utterance was a segment  boundary, areas of 
uncertainty where opinion was split between 
two adjacent  utterances, and some background 
noise from near-random respondents.
 Group opinion is manifested in these peaks. 
Figure 5 shows a hypothetical example to 
illustrate how we defined this notion. A peak is 
any local maximum (any utterance u where u - 1 
< u > u + 1) above background noise, which we 
define as any utterance with a number of votes 
below the arithmetic mean. Utterance 5, being a 
local maximum, is a peak. Utterance 2, though a 
local maximum, is not a peak as it  is below the 
mean. Utterance 4 has a comparatively large 
number of votes, but it  is not considered a peak 
because its neighbour, utterance 5, is higher. 
Defining peaks this way allows us to focus on 
the points of highest agreement, while ignoring 
not only the relatively low-scoring utterances, 
Figure 4. The results for one dialogue. Each utterance in the dialogue is represented in sequence along the x axis. 
Numbers in dots represent the number of respondents that ?voted? for that utterance as a segment boundary.  Peaks 
appear where agreement is strongest. A circle around a data point indicates our choices for segment boundary.
0
5
10
15
20
25
30
35
40
45
af
te
r_
e0
59
ac
h1
_0
00
_A
NV
_0
0
af
te
r_
e0
59
ac
h1
_0
00
_A
NV
_0
1
af
te
r_
e0
59
ac
h2
_0
01
_C
NK
_0
2
af
te
r_
e0
59
ac
h2
_0
01
_C
NK
_0
3
af
te
r_
e0
59
ac
h1
_0
02
_A
NV
_0
4
af
te
r_
e0
59
ac
h1
_0
02
_A
NV
_0
5
af
te
r_
e0
59
ac
h1
_0
02
_A
NV
_0
6
af
te
r_
e0
59
ac
h2
_0
03
_C
NK
_0
7
af
te
r_
e0
59
ac
h1
_0
04
_A
NV
_0
8
af
te
r_
e0
59
ac
h2
_0
05
_C
NK
_0
9
af
te
r_
e0
59
ac
h1
_0
06
_A
NV
_1
0
af
te
r_
e0
59
ac
h1
_0
06
_A
NV
_1
1
af
te
r_
e0
59
ac
h1
_0
06
_A
NV
_1
2
af
te
r_
e0
59
ac
h2
_0
07
_C
NK
_1
3
af
te
r_
e0
59
ac
h2
_0
07
_C
NK
_1
4
af
te
r_
e0
59
ac
h2
_0
07
_C
NK
_1
5
af
te
r_
e0
59
ac
h1
_0
08
_A
NV
_1
6
af
te
r_
e0
59
ac
h1
_0
08
_A
NV
_1
7
af
te
r_
e0
59
ac
h1
_0
08
_A
NV
_1
8
af
te
r_
e0
59
ac
h1
_0
08
_A
NV
_1
9
af
te
r_
e0
59
ac
h2
_0
09
_C
NK
_2
0
af
te
r_
e0
59
ac
h1
_0
10
_A
NV
_2
1
af
te
r_
e0
59
ac
h1
_0
10
_A
NV
_2
2
af
te
r_
e0
59
ac
h1
_0
10
_A
NV
_2
3
af
te
r_
e0
59
ac
h1
_0
10
_A
NV
_2
4
af
te
r_
e0
59
ac
h2
_0
11
_C
NK
_2
5
af
te
r_
e0
59
ac
h1
_0
12
_A
NV
_2
6
af
te
r_
e0
59
ac
h1
_0
12
_A
NV
_2
7
af
te
r_
e0
59
ac
h2
_0
13
_C
NK
_2
8
af
te
r_
e0
59
ac
h2
_0
14
_C
NK
_2
9
af
te
r_
e0
59
ac
h1
_0
15
_A
NV
_3
0
af
te
r_
e0
59
ac
h1
_0
16
_A
NV
_3
1
af
te
r_
e0
59
ac
h1
_0
16
_A
NV
_3
2
af
te
r_
e0
59
ac
h1
_0
16
_A
NV
_3
3
af
te
r_
e0
59
ac
h2
_0
17
_C
NK
_3
4
af
te
r_
e0
59
ac
h1
_0
18
_A
NV
_3
5
af
te
r_
e0
59
ac
h1
_0
18
_A
NV
_3
6
af
te
r_
e0
59
ac
h1
_0
18
_A
NV
_3
7
af
te
r_
e0
59
ac
h2
_0
19
_C
NK
_3
8
af
te
r_
e0
59
ac
h2
_0
19
_C
NK
_3
9
af
te
r_
e0
59
ac
h1
_0
20
_A
NV
_4
0
af
te
r_
e0
59
ac
h2
_0
21
_C
NK
_4
1
af
te
r_
e0
59
ac
h2
_0
21
_C
NK
_4
2
af
te
r_
e0
59
ac
h1
_0
22
_A
NV
_4
3
af
te
r_
e0
59
ac
h2
_0
23
_C
NK
_4
4
af
te
r_
e0
59
ac
h1
_0
24
_A
NV
_4
5
af
te
r_
e0
59
ac
h2
_0
25
_C
NK
_4
6
af
te
r_
e0
59
ac
h1
_0
26
_A
NV
_4
7
af
te
r_
e0
59
ac
h2
_0
27
_C
NK
_4
8
af
te
r_
e0
59
ac
h1
_0
28
_A
NV
_4
9
af
te
r_
e0
59
ac
h2
_0
29
_C
NK
_5
0
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
1
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
2
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
3
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
4
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
5
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
6
af
te
r_
e0
59
ac
h2
_0
30
_C
NK
_5
7
af
te
r_
e0
59
ac
h1
_0
31
_A
NV
_5
8
af
te
r_
e0
59
ac
h2
_0
32
_C
NK
_5
9
af
te
r_
e0
59
ac
h1
_0
33
_A
NV
_6
0
af
te
r_
e0
59
ac
h2
_0
34
_C
NK
_6
1
af
te
r_
e0
59
ac
h1
_0
35
_A
NV
_6
2
af
te
r_
e0
59
ac
h1
_0
35
_A
NV
_6
3
af
te
r_
e0
59
ac
h2
_0
36
_C
NK
_6
4
af
te
r_
e0
59
ac
h1
_0
37
_A
NV
_6
5
af
te
r_
e0
59
ac
h2
_0
38
_C
NK
_6
6
af
te
r_
e0
59
ac
h2
_0
39
_C
NK
_6
7
af
te
r_
e0
59
ac
h1
_0
40
_A
NV
_6
8
af
te
r_
e0
59
ac
h1
_0
40
_A
NV
_6
9
af
te
r_
e0
59
ac
h1
_0
40
_A
NV
_7
0
af
te
r_
e0
59
ac
h2
_0
41
_C
NK
_7
1
af
te
r_
e0
59
ac
h1
_0
42
_A
NV
_7
2
af
te
r_
e0
59
ac
h1
_0
42
_A
NV
_7
3
af
te
r_
e0
59
ac
h2
_0
43
_C
NK
_7
4
af
te
r_
e0
59
ac
h1
_0
44
_A
NV
_7
5
22
37
3
41
11
24
2
6
30
6
3
6
43
37
3
0
3
24
2
12
0
23
38
44
9
1
8
6
32
38
12
23
8
2
4
17
1
42
221
37
322
00
4
1
32
15
2
35
44
25
10
21
26
16
1
4
7
28
43
36
e059 n = 42
m
ean = 9.89
Figure 3. WD scores for individual responses. A 
score of 0 indicates perfect agreement. 
900
but also the potentially misleading utterances 
near a peak. 
 There are three disagreements in the dialogue 
presented in Figure 4. For the first, annotators 
saw a break where we saw a continuation. The 
other two disagreements show the reverse: 
annotators saw a continuation of topic as a 
continuation of segment.
4 Results
Table 2 shows the agreement of the aggregated 
group votes with regard to expert  opinion. The 
aggregated responses from the volunteer 
annotators agree extremely well with expert 
opinion. Acting as a unit, the group?s 
WindowDiff scores always perform better than 
the individual annotators on average. While the 
individual annotators attained an average WD 
score of .245, the annotators-as-group scored 
WD = .108.
 On five of the thirteen dialogues, the group 
performed as well as or better than the best 
individual annotator. On the other eight 
dialogues, the group performance was toward 
the top of the group, bested by one annotator 
(three times), two annotators (once), four 
annotators (three times), or six annotators 
(once), out of a field of 32?73 individuals. This 
suggests that aggregating the scores in this way 
causes a ?majority rule? effect  that brings out  the 
best answers of the group.
 One drawback of the WD statistic (as 
opposed to !) is that  there is no clear consensus 
for what constitutes ?good agreement?. For 
computational linguistics, ! ! .67 is generally 
considered strong agreement. We found that ! 
for the aggregated group ranged from .71 to .94. 
Over all the dialogues, ! = ?84. This is 
surprisingly high agreement  for a dialogue-level 
task, especially considering the stringency of the 
! statistic, and that  the data comes from 
untrained volunteers, none of whom were 
dropped from the sample.
5 Comparison to Trivial Baselines
We used a number of trivial baselines to see if 
our results could be bested by simple means. 
These were random placement  of boundaries, 
majority class, marking the last  utterance in each 
turn as a boundary, and a set of hand-built rules 
we called ?the Trigger?. The results of these 
trials can be seen in Figure 6. 
Dialogue name
WD average as 
marked by 
volunteers
WD single 
annotator best
WD single 
annotator 
worst
WD for group 
opinion
How many 
annotators did 
better?
Number of 
annotators
e041a 0.210 0.094 0.766 0.094 0 39
e041b 0.276 0.127 0.794 0.095 0 39
e059 0.236 0.080 0.920 0.107 1 42
e081a 0.244 0.037 0.611 0.148 4 36
e081b 0.267 0.093 0.537 0.148 4 32
e096a 0.219 0.083 0.604 - - 32
e096b 0.160 0.000 0.689 0.044 1 36
e115 0.214 0.079 0.750 0.079 0 34
e119 0.241 0.102 0.610 - - 32
e123a 0.259 0.043 1.000 0.174 6 34
e123b 0.193 0.093 0.581 0.047 0 33
e030 0.298 0.110 0.807 0.147 2 55
e066 0.288 0.063 0.921 0.063 0 69
e076a 0.235 0.026 0.868 0.053 1 73
e076b 0.270 0.125 0.700 0.175 4 40
ALL 0.245 0.000 1.000 0.108 60 626
Table 2. Summary of WD results for dialogues. Data has not been aggregated for two dialogues because they 
are being held out for future work.
m
ean = 9.5
ut
t1
ut
t2
ut
t3
ut
t4
ut
t5
ut
t6
2
7
3
11
27
5
Figure 5. Defining the notion of ?peak?. Numbers in 
circles indicate number of ?votes? for that utterance 
as a boundary. 
901
5.1 Majority Class
This baseline consisted of marking every 
utterance with the most common classification, 
which was ?not a boundary?. (About one in four 
utterances was marked as the end of a segment 
in the reference dialogues.) This was one of the 
worst case baselines, and gave WD = .551 over 
all dialogues.
5.2 Random Boundary Placement
We used a random number generator to 
randomly place as many boundaries in each 
dialogue as we had in our reference dialogues. 
This method gave about the same accuracy as 
the ?majority class? method with WD = .544. 
5.3 Last Utterance in Turn
In these dialogues, a speaker?s turn could consist 
of more than one utterance. For this baseline, 
every final utterance in a turn was marked as the 
beginning of a segment, except when lone 
utterances would have created a segment  with 
only one speaker.
 This method was suggested by work from 
Sacks, Schegloff, and Jefferson (1974) who 
observed that the last  utterance in a turn tends to 
be the first pair part  for another adjacency pair. 
Wright, Poesio, and Isard (1999) used a variant 
of this idea in a dialogue act  tagger, including 
not only the previous utterance as a feature, but 
also the previous speaker?s last speech act type.
 This method gave a WD score of .392.
5.4 The Trigger
This method of segmentation was a set of hand-
built rules created by the author. In this method, 
two conditions have to exist  in order to start a 
new segment. 
? Both speakers have to have spoken.
? One utterance must contain four words or 
less.
The ?four words? requirement was determined 
empirically during the feature selection phase of 
an earlier experiment.
 Once both these conditions have been met, 
the ?trigger? is set. The next utterance to have 
more than four words is the start of a new 
segment.
 This method performed comparatively well, 
with WD = .210, very close to the average 
individual annotator score of .245.
 As mentioned, the aggregated annotator score 
was WD = .108.
0
0.1
0.2
0.3
0.4
0.5
0.6
Majority Random Last utterance Trigger Group
0.108
0.210
0.392
0.5440.551
W
D 
sc
or
es
Figure 6.  Comparison of the group?s aggregated 
responses to trivial baselines.
5.5 Comparison to Other Work
Comparing these results to other work is 
difficult because very little research focuses on 
dialogue segmentation at this level of analysis. 
J?nsson (1991) uses initiative-response pairs as 
a part  of a dialogue manager, but  does not 
attempt to recognise these segments explicitly. 
 Comparable statistics exist  for a different 
task, that  of multiparty topic segmentation. WD 
scores for this task fall consistently into the .25 
range, with Galley et al (2003) at  .254, Hsueh et 
al. (2006) at  .283, and Purver et al (2006) 
at  .?284. We can only draw tenuous conclusions 
between this task and our own, however this 
does show the kind of scores we should be 
expecting to see for a dialogue-level task. A 
more similar project would help us to make a 
more valid comparison.
6 Discussion
The discussion of results will follow the two 
foci of the project: first, some comments about 
the aggregation of the volunteer data, and then 
some comments about the segmentation itself.
6.1 Discussion of Aggregation
A combination of factors appear to have 
contributed to the success of this method, some 
involving the nature of the task itself, and some 
involving the nature of aggregated group 
opinion, which has been called ?the wisdom of 
crowds? (for an informal introduction, see 
Surowiecki 2004).
 The fact  that  annotator responses were 
aggregated means that no one annotator had to 
perform particularly well. We noticed a range of 
styles among our annotators. Some annotators 
agreed very well with the expert opinion. A few 
902
annotators seemed to mark utterances in near-
random ways. Some ?casual annotators? seemed 
to drop in, click only a few of the most obvious 
boundaries in the dialogue, and then submit the 
form. This kind of behaviour would give that 
annotator a disastrous individual score, but 
when aggregated, the work of the casual 
annotator actually contributes to the overall 
picture provided by the group. As long as the 
wrong responses are randomly wrong, they do 
not detract  from the overall pattern and no 
volunteers need to be dropped from the sample.
 It  may not  be surprising that people with 
language experience tend to arrive at more or 
less the same judgments on this kind of task, or 
that the aggregation of the group data would 
normalise out the individual errors. What is 
surprising is that the judgments of the group, 
aggregated in this way, correspond more closely 
to expert opinion than (in many cases) the best 
individual annotators. 
6.2 Discussion of Segmentation
The concept of segmentation as described here, 
including the description of nuclei and satellites, 
appears to be one that annotators can grasp even 
with minimal training.
 The task of segmentation here is somewhat 
different  from other classification tasks. 
Annotators were asked to find segment 
boundaries, making this essentially a two-class 
classification task where each utterance was 
marked as either a boundary or not a boundary. 
It  may be easier for volunteers to cope with 
fewer labels than with many, as is more common 
in dialogue tasks. The comparatively low 
perplexity would also help to ensure that 
volunteers would see the annotation through.
 One of the outcomes of seeing annotator 
opinion was that we could examine and learn 
from cases where the annotators voted 
overwhelmingly contrary to expert  opinion. This 
gave us a chance to learn from what  the human 
annotators thought  about language. Even though 
these results do not literally come from one 
person, it  is still interesting to look at  the general 
patterns suggested by these results.
 ?let?s  see?: This utterance usually appears 
near boundaries, but  does it  mark the end of a 
segment, or the beginning of a new one? We 
tended to place it  at the end of the previous 
segment, but human annotators showed a very 
strong tendency to group it with the next 
segment. This was despite an example on the 
training page that suggested joining these 
utterances with the previous segment.
 Topic: The segments under study here are 
different  from topic. The segments tend to be 
smaller, and they focus on the mechanics of the 
exchanges rather than centering around one 
topic to its conclusion. Even though the 
annotators were asked to mark for adjacency 
pairs, there was a distinct  tendency to mark 
longer units more closely pertaining to topic. 
Table 3 shows one example. We had marked the 
space between utterances 2 and 3 as a boundary; 
volunteers ignored it. It  was slightly more 
common for annotators to omit our boundaries 
than to suggest  new ones. The average segment 
length was 3.64 utterances for our volunteers, 
compared with 3.59 utterances for experts.
 Areas of uncertainty: At  certain points on 
the chart, opinion seemed to be split as one or 
more potential boundaries presented themselves. 
This seemed to happen most  often when two or 
more of the same speech act  appeared 
sequentially, e.g. two or more questions, 
information-giving statements, or the like.
7 Conclusions and Future Work
We drew a number of conclusions from this 
study, both about the viability of our method, 
and about the outcomes of the study itself.
 First, it  appears that for this task, aggregating 
the responses from a large number of 
anonymous volunteers is a valid method of 
annotation. We would like to see if this pattern 
holds for other kinds of classification tasks. If it 
does, it  could have tremendous implications for 
dialogue-level annotation. Reliable results could 
be obtained quickly and cheaply from large 
numbers of volunteers over the Internet, without 
the time, the expense, and the logistical 
complexity of training. At present, however, it  is 
unclear whether this volunteer annotation 
1 MGT so what time should we meet
2 ADB <uh> well it doesn't matter 
as long as we both checked 
in I mean whenever we meet 
is kind of irrelevant
3 ADB so maybe about try to
4 ADB you want to get some lunch 
at the airport before we go
5 MGT that is a good idea
Table 3. Example from a dialogue.
903
technique could be extended to other 
classification tasks. It  is possible that the strong 
agreement  seen here would also be seen on any 
two-class annotation problem. A retest is 
underway with annotation for a different two-
class annotation set and for a multi-class task.
 Second, it appears that the concept of 
segmentation on the adjacency pair level, with 
this description of nuclei and satellites, is one 
that annotators can grasp even with minimal 
training. We found very strong agreement 
between the aggregated group answers and the 
expert opinion.
 We now have a sizable amount of 
information from language users as to how they 
perceive dialogue segmentation. Our next step is 
to use these results as the corpus for a machine 
learning task that can duplicate human 
performance. We are consider ing the 
Transformation-Based Learning algorithm, 
which has been used successfully in NLP tasks 
such as part of speech tagging (Brill 1995) and 
dialogue act  classification (Samuel 1998). TBL 
is attractive because it allows one to start  from a 
marked up corpus (perhaps the Trigger, as the 
best-performing trivial baseline), and improves 
performance from there. 
 We also plan to use the information from the 
segmentation to examine the structure of 
segments, especially the sequences of dialogue 
acts within them, with a view to improving a 
dialogue act tagger.
Acknowledgements
Thanks to Alan Dench and to T. Mark Ellison 
for reviewing an early draft  of this paper. We 
especially wish to thank the individual 
volunteers who contributed the data for this 
research.
References
Luis von Ahn and Laura Dabbish. 2004. Labeling 
images with a computer game. In Proceedings of 
the SIGCHI Conference on Human Factors in 
Computing Systems. pp. 319?326.
Jan Alexandersson,  Bianka Buschbeck-Wolf, 
Tsutomu Fujinami, Elisabeth Maier, Norbert 
Reithinger, Birte Schmitz, and Melanie Siegel. 
1997.  Dialogue acts in VERBMOBIL-2 . 
Verbmobil Report 204, DFKI, University of 
Saarbruecken.
Eric Brill. 1995. Transformation-based error-driven 
learning and natural language processing: A case 
study in part-of-speech tagging. Computational 
Linguistics, 21(4): 543?565.
Jean C. Carletta. 1996. Assessing agreement on 
classification tasks: The kappa statistic. 
Computational Linguistics, 22(2): 249?254.
Herbert H. Clark and Edward F. Schaefer.  1989. 
Contributing to discourse. Cognitive Science, 
13:259?294.
Michael Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse 
segmentation of multi-party conversation. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, pp. 
562?569.
Pei-Yun Hsueh, Johanna Moore, and Steve Renals. 
2006. Automatic segmentation of multiparty 
dialogue. In Proceedings of the EACL 2006,  pp. 
273?280.
Arne J?nsson. 1991. A dialogue manager using 
initiative-response units and distributed control. In 
Proceedings of the Fifth Conference of the 
European Association for Computational 
Linguistics, pp. 233?238.
William C.  Mann and Sandra A. Thompson. 1987. 
Rhetorical structure theory: A framework for the 
analysis of texts. In IPRA Papers in Pragmatics 1: 
1-21.
Lev Pevzner and Marti A. Hearst. 2002. A critique 
and improvement of an evaluation metric for text 
segmentation. Computational Linguistics, 28(1): 
19?36.
Matthew Purver, Konrad P.  K?rding, Thomas L. 
Griffiths, and Joshua B. Tenenbaum. 2006. 
Unsupervised topic modelling for multi-party 
spoken discourse. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL, 
pp. 17?24.
Harvey Sacks, Emanuel A. Schegloff, and Gail 
Jefferson. 1974. A simplest systematics for the 
organization of turn-taking for conversation. 
Language, 50:696?735.
Ken Samuel,  Sandra Carberry, and K. Vijay-Shanker. 
1998. Dialogue act tagging with transformation-
based learning. In Proceedings of COLING/
ACL'98, pp. 1150?1156.
James Surowiecki. 2004. The wisdom of crowds: 
Why the many are smarter than the few.  Abacus: 
London, UK.
Helen Wright, Massimo Poesio,  and Stephen Isard. 
1999. Using high level dialogue information for 
dialogue act recognition using prosodic features. 
In DIAPRO-1999, pp. 139?143.
904
