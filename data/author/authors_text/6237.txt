Proceedings of NAACL HLT 2007, pages 572?579,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Computing Semantic Similarity between Skill Statements for Approxi-
mate Matching 
 
 
Feng Pan Robert G. Farrell 
USC Information Sciences Institute IBM T. J. Watson Research Center 
Marina del Rey, CA 90292 Hawthorne, NY 10532 
pan@isi.edu robfarr@us.ibm.com 
 
 
 
 
Abstract 
This paper explores the problem of com-
puting text similarity between verb 
phrases describing skilled human behav-
ior for the purpose of finding approximate 
matches. Four parsers are evaluated on a 
large corpus of skill statements extracted 
from an enterprise-wide expertise taxon-
omy. A similarity measure utilizing com-
mon semantic role features extracted from 
parse trees was found superior to an in-
formation-theoretic measure of similarity 
and comparable to the level of human 
agreement. 
1 Introduction 
Knowledge-intensive industries need to become 
more efficient at deploying the right expertise as 
quickly and smoothly as possible, thus it is desired 
to have systems that can quickly match and deploy 
skilled individuals to meet customer needs. The 
searches in most of the current matching systems 
are based on exact matches between skill state-
ments. However, exact matching is very likely to 
miss individuals who are very good matches to the 
job but didn?t select the exact skills that appeared 
in the open job description.  
It is always hard for individuals to find the per-
fect skills to describe their skill sets. For example, 
an individual might not know whether to choose a 
skill stating that refers to ?maintaining? a given 
product or ?supporting? it or whether to choose a 
skill about maintaining a ?database? or about 
maintaining ?DB2?. Thus, it is desirable for the job 
search system to be able to find approximate 
matches, instead of only exact matches, between 
available individuals and open job positions. More 
specifically, a skill similarity computation is 
needed to allow searches to be expanded to related 
skills, and return more potential matches.  
In this paper, we present our work on develop-
ing a skill similarity computation based upon se-
mantic commonalities between skill statements. 
Although there has been much work on text simi-
larity metrics (Lin, 1998a; Corley and Mihalcea, 
2005), most approaches treat texts as a bag of 
words and try to find shared words with certain 
statistical properties based on corpus frequencies. 
As a result, the structural information in the text is 
ignored in these approaches. We will describe a 
new semantic approach that takes the structural 
information of the text into consideration and 
matches skill statements on corresponding seman-
tic roles. We will demonstrate that it can outper-
form standard statistical text similarity techniques, 
and reach the level of human agreement.   
In Section 2, we first describe the skill state-
ments we extracted from an enterprise-wide exper-
tise taxonomy. In Section 3, we describe the 
performance of a standard statistical approach on 
this task. This motivates our semantic approach of 
matching skill statements on corresponding seman-
tic roles. We also compare and evaluate the per-
formance of four natural language parsers (the 
Charniak parser, the Stanford parser, the ESG 
parser, and MINIPAR) for the purpose of our task. 
An inter-rater agreement study and evaluation of 
572
our approach will be presented in Section 4. We 
end with a discussion and conclusion.  
2 Skill Statements 
An expertise taxonomy is a standardized, enter-
prise-wide language and structure to describe job 
role requirements and people capabilities (skill 
sets) across a corporation. In the taxonomy we util-
ize for this study, skills are associated with job 
roles. The taxonomy has 10667 skills. Each skill 
has a title, for example, ?Advise BAAN eBusiness 
ASP.? We refer to this title as the skill statement.  
The official taxonomy update policies require 
that skill statements be verb phrases using one of 
18 valid skill verbs (e.g., Advise, Architect, Code, 
Design, Implement, Sell, and Support).  
3 Computing Semantic Similarities be-
tween Skill Statements 
In this section, we first explain a statistical infor-
mation-theoretic approach we used as a baseline, 
and show examples of how it performs for our 
task. The error analysis of this approach motivates 
our semantic approach that takes the structural in-
formation of the text into consideration. In the re-
mainder of this section, we describe how we 
extract semantic role information from the syntac-
tic parse trees of the skill statements. Four natural 
language parsers are compared and evaluated for 
the purpose of our task. 
3.1 Statistical Approach 
In order to compute semantic similarities between 
skill statements, we first adopted one of the stan-
dard statistical approaches to the problem of com-
puting text similarities based on Lin?s information-
theoretic similarity measure (Lin 1998a). Lin de-
fined the commonality between A and B as  
)),(( BAcommonI  
where common(A, B) is a proportion that states the 
commonalities between A and B and where the 
amount of information in proposition s is 
)(log)( sPsI ?=  
The similarity between A and B is then defined as 
the ratio between the amount of information 
needed to state the commonality of A and B and 
the information needed to fully describe A and B: 
)),((log
)),((log
),(
BAndescriptioP
BAcommonP
BASim =   
In order to compute common(A,B) and descrip-
tion(A,B), we use standard bag-of-words features, 
i.e., unigram features -- the frequency of words 
computed from the entire corpus of the skill state-
ments. Thus common(A,B) is the unigrams that 
both skill statements share, and description(A,B) is 
the union of the unigrams from both skill state-
ments.  
The words are stemmed first so that the words 
with the same root (e.g., managing & manage-
ment) can be found as commonalities between two 
skill statements. A stop-word list is also used so 
that the commonly used words in most of the docu-
ments (e.g., the, a) are not used as features. A for-
mal evaluation of this approach will be presented 
in Section 4 where the similarity between 75 pairs 
of skill statements will be evaluated against human 
judgments, but we discuss some examples here.  
In order to see how to improve Lin?s statistical 
similarity measure, we examine sample skill state-
ment pairs which achieve high similarity scores 
from Lin?s measure but were rated consistently as 
dissimilar by human subjects in our evaluation. 
Here are two examples:  
1. Advise Business Knowledge of CAD function-
ality for FEM 
Advise on Business Knowledge of Process for 
FEM 
2. Advise on Money Market 
Advise on Money Center Banking 
In these two examples, although many words are 
shared between the two pairs of skill statements 
(Advise Business Knowledge of ... for FEM for the 
first pair; Advise on Money for the second pair), 
they are not similar to human judges. We conjec-
ture that this judgment of dissimilarity is due to the 
differences between the key components of the 
skill statements (CAD functionality vs. Process in 
the first pair; Money Market vs. Money Center 
Banking in the second pair). 
This kind of error is common for most statistical 
approaches to the problem, where common infor-
mation is computed without considering the struc-
tural information in the text. From the above 
examples, we can see that the similarity computa-
tion would be more accurate if the verb phrases 
match on corresponding semantic roles, instead of 
573
matching words from any location in the skill 
statements. By identifying semantic roles, we can 
provide more weights to those semantic roles criti-
cal for our task, i.e., the key components of the 
skill statements. 
3.2 Identifying and Assigning Semantic 
Roles 
The following example shows the kind of semantic 
roles we want to be able to identify and assign.  
[action Apply] [theme Knowledge of [concept IBM E-
business Middleware]] to [purpose PLM Solu-
tions] 
In this example, ?Apply? is the ?action? of the 
skill; ?Knowledge of IBM E-business Middle-
ware? is the ?theme? of the skill, where the ?con-
cept? semantic role (IBM E-business Middleware) 
specifies the key component of the skill require-
ment and is the most important role for skill 
matching; ?PLM Solutions? is the ?purpose? of the 
skill. 
Our goal was to extract all such semantic role 
patterns for all the skill statements, and match on 
corresponding semantic roles. Although there ex-
ists some automatic semantic role taggers (Gildea 
and Jurafsky, 2002; Giuglea and Moschitti, 2006), 
most of them were trained on PropBank (Palmer 
et. al., 2005) and/or FrameNet (Johnson et. al., 
2003), and perform much worse in other corpora 
(Pradhan et. al., 2004). Our corpus is from a very 
different domain (information technology) and 
there are many domain-specific terms in the skill 
statements, such as product names, company 
names, and company-specific nomenclature for 
product offerings. Given this, we would expect 
poor performance from these automatic semantic 
role taggers. Moreover, the semantic role informa-
tion we need to extract is more detailed and deeper 
than most of the automatic semantic role taggers 
can identify and extract (e.g., the ?concept? role 
embedded within the ?theme? role).  
We developed a specialized parser that extracts 
semantic role patterns from each of the 18 skill 
verbs. This semantic role parser can achieve a 
much higher performance than the general-purpose 
semantic role taggers. The inputs needed for the 
semantic role parser are syntactic parse trees gen-
erated by a natural language parse of the original 
skill statements.  
3.3 Preprocessing for Parsing 
We first used the Charniak parser (2000) to parse 
the original skill statements. However, among all 
the 10667 skill statements, 1217 were not parsed as 
verb phrases, leading to very poor performance. 
After examining the error cases, we found that ab-
breviations are used widely in the skill statements. 
For example, 
Advise Solns Supp Bus Proc Reeng for E&E 
Eng Procs 
These abbreviations made the system unable to 
determine the part of speech of some words, result-
ing in incorrect parses. Thus, the first step of the 
preprocessing was to expand abbreviations.  
There were 225 valid abbreviations already 
identified by the expertise taxonomy team. How-
ever, we found many abbreviations that appeared 
in the skill statements but were not listed there. 
Since most abbreviations are not words found in a 
dictionary, in order to find the abbreviations that 
appear frequently in the skill statements, we first 
found all the words in the skill statements that 
were not in WordNet (Miller, 1990). We then 
ranked them based on their frequencies, and manu-
ally identified high frequency abbreviations. Using 
this approach, we added another 187 abbreviations 
to the list (a total of 412).  
From the error cases, we also found that many 
words were mistagged as proper nouns, For exam-
ple, ?Technically? in  
Advise Technically for Simulation 
was parsed as a proper noun. We realized the rea-
son for this error was that all the words, except for 
prepositions, are capitalized in the original state-
ments and the parser tends to tag them as proper 
nouns. To solve this problem, we changed all the 
capitalized words to lower case, except for the first 
word and the acronyms (words that have all letters 
capitalized, e.g., IBM). After applying these two 
steps of preprocessing, we parsed the skill state-
ments again. This time, more than 200 additional 
skill statements were parsed as verb phrases after 
the preprocessing. 
When we examined the error cases more 
closely, we found the errors occur mostly when the 
skill verbs can be both a noun and a verb (e.g., de-
sign, plan). In those cases, the parser may parse the 
entire statement as one noun phrase, instead of a 
verb phrase. In order to disambiguate such cases, 
574
we added a subject (?Employees?) to all the skill 
statements to convert them into full sentences. Af-
ter applying this additional step of preprocessing, 
we parsed the skill statements again. This time, 
only 28 skill statements were not parsed as sen-
tences containing verb phrases, a significant im-
provement. The remaining errors were due to the 
use of some words as skill verbs, e.g., ?architect?1, 
not recognized as verbs by the parser. 
3.4 Parser Evaluation and Comparison 
While the Charniak parser performed well in our 
initial verb phrase (VP) test, we decided to com-
pare the Charniak parser?s performance with other 
parsers. For this evaluation, we compared it with 
the Stanford parser, the ESG parser, and 
MINIPAR.   
The Stanford parser (Klein and Manning, 
2003) is an unlexicalized statistical syntactic parser 
that was trained on the same corpus as the 
Charniak parser (the Penn TreeBank). Its parse tree 
has the same structure as the Charniak parser. 
The ESG (English Slot Grammar) parser 
(McCord, 1980) is a rule-based parser based on the 
slot grammar where each phrase has a head and 
dependent elements, and is also marked with a syn-
tactic role.  
MINIPAR (Lin, 1998b), as a dependency 
parser, is very similar to the ESG parser in terms of 
its output. It represents sentence structures as a set 
of dependency relationships between head words. 
Since our purpose is to use the syntactic parses 
as inputs to extract semantic role patterns, the cor-
rectness of the bracketing of the parses and the 
syntactic labels of the phrases (e.g., NP, VP, and 
PP) are the most important information for our pur-
poses, whereas the POS (Part-Of-Speech) labels of 
individual words (e.g., nouns vs. proper nouns) are 
not that important (also, there are too many do-
main-specific terms in our data). Thus, our evalua-
tion of the parses is only on the correctness of the 
bracketing and the syntactic labels of the phrases, 
not the correctness of the entire parse. For our task, 
the correctness of the prepositional phrase attach-
ment is especially important for extracting accurate 
semantic role patterns (Gildea and Jurafsky, 2002). 
For example, for the sentence 
                                                          
1 ?Architect? has no verb sense in WordNet and many other 
dictionaries, but it does have a verb sense in the Oxford Eng-
lish Dictionary (http://dictionary.oed.com/). 
Apply Knowledge of IBM E-business Middle-
ware to PLM Solutions. 
the correct bracketing should be 
Apply [Knowledge [of [IBM E-business Mid-
dleware]]] [to [PLM Solutions]].  
Thus the parser needs to correctly attach ?of IBM 
E-business Middleware? to ?Knowledge? and at-
tach ?to PLM Solutions? to ?Apply?, not ?Knowl-
edge?. 
To evaluate the performance of the parsers, we 
randomly picked 100 skill statements from our cor-
pus, preprocessed them, and then parsed them us-
ing the four different parsers. We then evaluated 
the parses using the above evaluation measures. 
The parses were rated as correct or incorrect. No 
partial score was given. Figure 1 shows the evalua-
tion results. The error analysis reveals four major 
sources of error for all the parsers, most of which 
are specific to the domain we are working on: 
(1) Many domain specific terms and acronyms. 
For example, ?SAP? in ?Employees advise on 
SAP R/3 logistics basic data.? was always 
tagged as a verb by the parsers.  
(2) Many long noun phrases. For example, ?Em-
ployees perform JD edwards foundation suite 
address book.?  
(3) Some specialized use of punctuation. For ex-
ample, ?Employees perform business transpor-
tation consultant-logistics.sys.?  
(4) Prepositional phrase attachment can be diffi-
cult. For example, in ?Employees apply IBM 
infrastructure knowledge for IDBS?, ?for 
IDBS? should attach to ?apply?, but many 
parsers mistakenly attach it to ?IBM infrastruc-
ture knowledge?. 
0%
10%
20%
30%
40%
50%
60%
70%
80%
Charniak Stanford ESG MINIPAR
 
Figure 1. An Evaluation of Four Parsers on the 
Task of Parsing Human Skill-related Verb Phrases  
We noticed that MINIPAR performed much 
worse compared with the other parsers. The main 
575
reason is that it always parses the phrase ?VERB 
knowledge of Y? (e.g., ?Employees apply knowl-
edge of web technologies.?) incorrectly -- the parse 
result always mistakenly attaches ?of Y? (e.g., of 
web technologies) to the VERB (e.g., apply), not 
?knowledge?. Since there were so many of phrases 
in the test set and in the corpus, this kind of error 
significantly reduced the performance for our task. 
These kinds of errors on prepositional phrase at-
tachment in MINIPAR were also mentioned in 
(Pantel and Lin, 2000).   
From the evaluation and comparison results we 
can see that the Charniak parser performs the best 
for our task among all the four parsers. This result 
is consistent with a more thorough evaluation 
(Swanson and Gordon, 2006) on a different corpus 
with a set of different target verbs, which showed 
the Charniak parser performed the best among 
three parsers (including the Stanford parser and 
MINPAR) for labeling semantic roles. We note 
that although the ESG parser performed a little 
worse than the Charniak parser, its parses contain 
much richer syntactic (e.g., subject, object) and 
semantic (e.g., word senses) slot-filling informa-
tion, which can be very useful to many natural lan-
guage applications.  
3.5 Extracted Semantic Role Patterns 
From the parse trees generated by the Charniak 
parser, we first automatically extracted patterns for 
each of the 18 skill verbs (e.g., ?Advise on NP for 
NP?), and then we manually identified the seman-
tic roles. For example, the semantic role patterns 
identified for the skill verb ?Advise? are: 
? Advise [Theme] (for [Purpose]) 
? Advise (technically) on/about [Theme] (for 
[Purpose]) 
? Advise clients/customers/employees/users 
on/regarding [Theme]  
The corpus also contains embedded sub-semantic-
role patterns, for example, for the ?Theme? role we 
extracted the following sub-patterns: 
? (application) knowledge of/for [Concept] 
? sales of [Concept] 
? (technical) implementation of [Concept]   
We have extracted and identified a total of 74 such 
semantic role patterns from the skill statements. 
4 Evaluation 
In order to evaluate the two approaches (semantic 
role parsing and statistical) to computing semantic 
similarity of skill statements in our domain, we 
first conducted an experiment to evaluate how hu-
mans agree on this task, which also provides us 
with an upper bound accuracy for the task. 
4.1 Inter-Rater Agreement and Upper 
Bound Accuracy 
To assess inter-rater agreement, we randomly se-
lected 75 skill pairs from the expertise taxonomy. 
Since random pairs of verbs would have little or no 
similarity, we selected skill pairs that share the 
same job role, or same secondary or primary job 
category, or from across the entire expertise taxon-
omy. 
These 75 skill pairs are then given to three raters 
to independently judge their similarities on a 5 
point scale from 1 as very similar to 5 as very dis-
similar. Since this 5 point scale is very fine-
grained, we also converted the judgments to a 
more coarse-grained measure -- binary judgment: 1 
and 2 count as similar; 3-5 as not similar. 
The metric we used is the kappa statistic (Car-
letta, 1996), which factors out the agreement that is 
expected by chance: 
)(1
)()(
EP
EPAP
?
?=? 
 
where P(A) is the observed agreement among the 
raters, and P(E) is the expected agreement, i.e., the 
probability that the raters agree by chance. 
Since the judgment on the 5 point scale is ordi-
nal data, the weighted kappa statistic is used to 
take the distance of disagreement into considera-
tion (e.g., the disagreement between 1 and 2 is 
smaller than that between 1 and 5). 
The inter-rater agreement results for both the 
fine-grained and coarse-grained judgments are 
shown in Table 1. In general, a kappa value above 
0.80 represents perfect agreement, 0.60-0.80 repre-
sents significant agreement, 0.40-0.60 represents 
moderate agreement, and 0.20-0.40 is fair agree-
ment (Chklovski and Mihalcea, 2003). We can see 
that the agreement on the fine-grained judgment is 
moderate, whereas the agreement on the coarse-
grained (binary) judgment is significant. 
 Fine-Grained  Coarse-Grained 
Kappa 0.412 0.602 
Table 1. Inter-Rater Agreement Results. 
576
From the inter-rater agreement evaluation, we 
can also get an upper bound accuracy for our task, 
i.e., human agreement without factoring out the 
agreement expected by chance (i.e., P(A) in the 
kappa statistic). The average P(A) for the coarse-
grained (binary) judgment is 0.81, and that consti-
tutes the upper bound accuracy for our task.   
4.2 Evaluation of the Statistical Approach 
We use the 75 skill pairs as test data to evaluate 
our semantic similarity approach against human 
judgments. Considering the reliability of the data, 
only the coarse-grained (binary) judgments are 
used. The gold standard is obtained by majority 
voting from the three raters, i.e., for a given skill 
pair, if two or more raters judge it as similar, then 
the gold standard answer is ?similar?, otherwise it 
is ?not similar?.  
We first evaluated Lin?s statistical approach de-
scribed in Section 3.1. Among 75 skill pairs, 53 of 
them were rated correctly according to the human 
judgments, that is, 70.67% accuracy. The error 
analysis shows that many of the errors can be cor-
rected if the skills are matched on their correspond-
ing semantic roles. We then evaluated the utility of 
the extracted semantic role information to see 
whether it can outperform the statistical approach. 
4.3 Evaluation of Semantic Role Matching 
Approach 
For simplicity, we will only report on evaluating 
semantic role matching on the "concept" role that 
specifies the key component of the skills, as intro-
duced in Section 3.2. 
There are at least two straightforward ways of 
performing semantic role matching for the skill 
similarity computation: 1) match on the entire se-
mantic role; 2) match on the head nouns only. But 
both have their drawbacks: the first approach is too 
strict and will miss many similar skill statements; 
the second approach may not only miss the similar 
skill statements, e.g., 
Perform [Web Services Planning]2   
Perform [Web Services Assessment]    
but also misclassify dissimilar ones as similar, e.g., 
                                                          
2 The ?concept? role is identified with brackets, and the head 
nouns are italic. 
Advise about [Async Transfer Mode (ATM) 
Solutions]   
Advise about [CTI Solutions] 
In order to solve these problems, we used a simple 
matching criterion from Tversky (1977). The simi-
larity of two texts t1 and t2 is determined by: 
      Similarity(t1, t2) =  
         
21
21
 tand in t features  total#
 ) tand between t featurescommon  (#  2?
 
This equation states that two texts are similar if 
shared features are a large percentage of the total 
features. We set a threshold of 0.5, requiring that at 
least 50% of the features be shared. We apply this 
criterion to the text contained in the ?concept? role.  
The words in the calculation are preprocessed 
first: abbreviations are expanded, stop-words are 
excluded (e.g., the and of don't count as shared 
words), and the remaining words are stemmed 
(e.g., manager and management are counted as 
shared words), as was done in our previous infor-
mation-theoretic approach. Words connected by 
punctuation (e.g., e-business, software/hardware) 
are treated as separate words. For example, 
Advise on [Field/Force Management] for Tele-
com 
Apply Knowledge of [Basic Field Force Auto-
mation]         
The shared words between the two ?concept? roles 
(bracketed) are ?Field? and ?Force?, and their 
shared percentage is (2*2)/7 = 57.14% > 50%, so 
they are similar. 
We have also evaluated this approach on our test 
set with the 75 skill pairs. Among 75 skill pairs, 60 
of them were rated correctly (i.e., 80% accuracy), 
which significantly outperforms the statistical ap-
proach, and is very close to the upper bound accu-
racy, i.e., human agreement (81%), as shown in 
Figure 2. 
64.00%
66.00%
68.00%
70.00%
72.00%
74.00%
76.00%
78.00%
80.00%
82.00%
Lin's Information-Theoretic Metric Semantic Role Matching Human Agreement
 
Figure 2. Evaluation on Semantic Similarity be-
tween Skill Statements 
577
The difference between this approach and Lin?s 
information content approach is that this computa-
tion is local -- no corpus statistics is used. Also, 
using this approach, it is easier to set an intuitive 
threshold (e.g., 50%) for a classification problem 
(e.g., similar or not for our task). With this ap-
proach, however, there are also cases that are 
mistagged as similar, for example, 
Apply Knowledge of [Basic Field Force Auto-
mation]  
Advise on [Sales Force Automation] 
Although ?Field Force Automation? and ?Sales 
Force Automation? seem similar on their surface 
form, they are two quite different concepts. Deeper 
domain knowledge (such as an ontology) is needed 
to distinguish such cases. 
5 Discussion  
We have also investigated several approaches to 
improving the semantic role text similarity meas-
ure we described. One approach is to also consider 
similarities between skill verbs. In this example:  
Implement Domino Mail Manager 
Develop for Domino Mail Manager 
although the key components of the skill state-
ments (Domino Mail Manager) are the same, their 
skill verbs are different (implement vs. develop 
for). The skills required for ?implementing? a sys-
tem or software product are usually different from 
those required for ?developing for? the same sys-
tem or software product. This example shows that 
a semantic similarity computation between skill 
verbs is required to distinguishing such cases. 
Many approaches to the problem of 
word/concept similarities are based on taxonomies, 
e.g., WordNet. The simplest approach is to count 
the number of nodes on the shortest path between 
two concepts in the taxonomy (Quillian, 1972). 
The fewer nodes on the path, the more similar the 
two concepts are. The assumption for this shortest 
path approach is that the links in the taxonomy rep-
resent uniform distances. However, in most tax-
onomies, sibling concepts deep in the taxonomy 
are usually more closely related than those higher 
up. Different approaches have been proposed to 
discount the depth of the concepts to overcome the 
problem. Budanitsky and Hirst (2006) thoroughly 
evaluated six of the approaches (Hirst and St-
Onge, Leacock and Chodorow, Jiang and Conrath, 
Lin, Resnik, Wu and Palmer), and found that Jiang 
and Conrath (1997) was superior to the other ap-
proaches based on their evaluation experiments. 
For our task, we compared two approaches to 
computing skill verb similarities: shortest path vs. 
Jiang and Conrath. Since the words are compared 
based on their specific senses, we first manually 
assigned one most appropriate sense for each of the 
18 skill verbs from WordNet. We then used the 
library developed by Pedersen et al (2004) to 
compute their similarity scores. 
Table 2 shows the top nine pairs of skill verbs 
with the highest similarity scores from the two ap-
proaches. We can see that the two approaches 
agree on the top four pairs, but disagree on the rest 
in the list. One intuitive example is the pair ?Lead? 
and ?Manage? which is ranked the 5th by the Jiang 
and Conrath approach but ranked the 46th by the 
shortest path approach. It seems that the Jiang and 
Conrath approach matches better with our human 
intuition for this example. While we didn?t com-
pare these results with human performance, in gen-
eral most of the similar skill verb pairs listed in the 
table don?t look very similar for our domain. This 
may be due to the fact that WordNet is a general-
purpose taxonomy -- although we have already 
selected the most appropriate sense for each verb, 
their relationship represented in the taxonomy may 
still be quite different from the relationship in our 
domain. A domain-specific taxonomy for skill 
verbs may improve the performance. The other 
reason may be due to the structure of WordNet?s 
verb taxonomy, as mentioned in (Resnik and Diab, 
2000), which is considerably wider and shallower 
than WordNet?s noun taxonomy. A different verb 
lexicon, e.g., VerbNet (Kipper et al, 2000), can be 
explored. 
  
Shortest Path Jiang and Conrath 
Apply Use Apply Use 
Design Plan Design Plan 
Apply Implement Apply Implement 
Implement Use Implement Use 
Analyze  Apply Lead Manage 
Analyze Perform Apply Support 
Analyze Support Support Use 
Analyze Use Apply Sell 
Perform Support Sell Use 
? ? ? ? 
Table 2. Top Similar Skill Verb Pairs 
578
6 Conclusion 
In this paper, we have presented our work on a se-
mantic similarity computation for skill statements 
in natural language. We compared and evaluated 
four different natural language parsers for our task, 
and matched skills on their corresponding semantic 
roles extracted from the parse trees generated by 
one of these parsers. The evaluation results showed 
that the skill similarity computation based on se-
mantic role matching can outperform a standard 
statistical approach and reach the level of human 
agreement.  
The extracted semantic role information can also 
be incorporated into the standard statistical ap-
proaches as additional features. One way is to give 
higher weights to those semantic role features 
deemed most important. This approach has 
achieved a high performance for a text categoriza-
tion task when combining extracted keywords with 
the full text (Hulth and Megyesi, 2006). 
We have shown that good results can be 
achieved for a domain-specific text matching task 
by performing a simple word-based feature com-
parison on corresponding structural elements of 
texts.  We have shown that the structural elements 
of importance can be identified by domain-specific 
pattern analysis of corresponding parse trees. We 
believe this approach can generalize to other do-
mains where phrases, sentences, or other short 
texts need to be compared. 
Acknowledgements  
The majority of this work was performed while the 
first author was a summer intern at IBM T. J. Wat-
son Research Center in Hawthorne, NY. Thanks to 
Yael Ravin and Jennifer Lai for supporting this 
work, Brian White for his help on the software, 
Michael McCord for assistance with the IBM ESG 
parser, and the IBM Expertise Taxonomy team for 
letting us use their data.    
References  
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness. 
Computational Linguistics. 32(1):13-47. 
J. Carletta. 1996. Assessing agreement on classification 
tasks: the kappa statistic. Computational Linguistics, 
22(2):249?254. 
E. Charniak. 2000. A maximum-entropy-inspired 
parser. In Proceedings of NAACL. 
T. Chklovski and R. Mihalcea. 2003. Exploiting Agree-
ment and Disagreement of Human Annotators for 
Word Sense Disambiguation. In Proceedings of 
RANLP. 
D. Gildea and D. Jurafsky. 2002. Automatic labeling of 
semantic roles. Computational Linguistics, 28(3): 
245 ? 288. 
A. Giuglea and A. Moschitti. 2006. Semantic Role La-
beling via FrameNet, VerbNet and PropBank. In 
Proceedings of COLING-ACL. 
A. Hulth and B. B. Megyesi. 2006. A Study on Auto-
matically Extracted Keywords in Text Categoriza-
tion. In Proceedings of COLING-ACL. 
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of ROCLING X. 
C. Johnson, M. Petruck, C. Baker, M. Ellsworth, J. Rup-
penhofer, and C. Fillmore. 2003. Framenet: Theory 
and practice. Berkeley, California. 
K. Kipper, H. T. Dang, M. Palmer. 2000. Class-Based 
Construction of a Verb Lexicon. In Proceedings of 
AAAI. 
D. Klein and C. D. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings of ACL. 
D. Lin. 1998a. An information-theoretic definition of 
similarity. In Proceedings of ICML. 
D. Lin. 1998b. Dependency-based evaluation of 
MINIPAR. In Proceedings of the Workshop at LREC 
on The Evaluation of Parsing Systems. 
M. C. McCord. 1980. Slot grammars. Computational 
Linguistics, 6: 31-43. 
G. A. Miller. 1990. WordNet: an On-line Lexical Data-
base. International Journal of Lexicography 3(4). 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The 
proposition bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1). 
P. Pantel and D. Lin. 2000. An unsupervised approach 
to prepositional phrase attachment using contextually 
similar words. In Proceedings of ACL. 
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. 
Wordnet::similarity - measuring the relatedness of 
concepts. In Proceedings of AAAI, Intelligent Sys-
tems Demonstration. 
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. 
Jurafsky. 2004. Shallow Semantic Parsing using 
Support Vector Machines. In Proceedings of 
HLT/NAACL. 
M. R. Quillian. 1972. Semantic Memory, Semantic In-
formation Processing. Semantic information process-
ing, Cambridge. 
P. Resnik and M. Diab. 2000. Measuring verb similar-
ity. In Proceedings of COGSCI. 
R. Swanson and A. S. Gordon. 2006. A Comparison of 
Alternative Parse Tree Paths for Labeling Semantic 
Roles. In Proceedings of COLING/ACL. 
A. Tversky. 1977. Features of Similarity, Psychological 
Review, vol. 84, no. 4, pages 327-352. 
579
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 393?400,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning Event Durations from Event Descriptions 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
We have constructed a corpus of news ar-
ticles in which events are annotated for 
estimated bounds on their duration. Here 
we describe a method for measuring in-
ter-annotator agreement for these event 
duration distributions. We then show that 
machine learning techniques applied to 
this data yield coarse-grained event dura-
tion information, considerably outper-
forming a baseline and approaching hu-
man performance. 
1 Introduction 
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events.  This paper describes one part 
of an exploration into how this information can 
be captured automatically.  Specifically, we have 
developed annotation guidelines to minimize dis-
crepant judgments and annotated 58 articles, 
comprising 2288 events; we have developed a 
method for measuring inter-annotator agreement 
when the judgments are intervals on a scale; and 
we have shown that machine learning techniques 
applied to the annotated data considerably out-
perform a baseline and approach human per-
formance.   
This research is potentially very important in 
applications in which the time course of events is 
to be extracted from news. For example, whether 
two events overlap or are in sequence often de-
pends very much on their durations.  If a war 
started yesterday, we can be pretty sure it is still 
going on today.  If a hurricane started last year, 
we can be sure it is over by now. 
The corpus that we have annotated currently 
contains all the 48 non-Wall-Street-Journal (non-
WSJ) news articles (a total of 2132 event in-
stances), as well as 10 WSJ articles (156 event 
instances), from the TimeBank corpus annotated 
in TimeML (Pustejovky et al, 2003). The non-
WSJ articles (mainly political and disaster news) 
include both print and broadcast news that are 
from a variety of news sources, such as ABC, 
AP, and VOA. 
In the corpus, every event to be annotated was 
already identified in TimeBank.  Annotators 
were instructed to provide lower and upper 
bounds on the duration of the event, encompass-
ing 80% of the possibilities, excluding anoma-
lous cases, and taking the entire context of the 
article into account. For example, here is the 
graphical output of the annotations (3 annotators) 
for the ?finished? event (underlined) in the sen-
tence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
393
This graph shows that the first annotator be-
lieves that the event lasts for minutes whereas the 
second annotator believes it could only last for 
several seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output because 
of the intuition that the difference between 1 sec-
ond and 20 seconds is significant, while the dif-
ference between 1 year 1 second and 1 year 20 
seconds is negligible.  
A preliminary exercise in annotation revealed 
about a dozen classes of systematic discrepancies 
among annotators? judgments.  We thus devel-
oped guidelines to make annotators aware of 
these cases and to guide them in making the 
judgments.  For example, many occurrences of 
verbs and other event descriptors refer to multi-
ple events, especially but not exclusively if the 
subject or object of the verb is plural.  In ?Iraq 
has destroyed its long-range missiles?, there is 
the time it takes to destroy one missile and the 
duration of the interval in which all the individ-
ual events are situated ? the time it takes to de-
stroy all its missiles.  Initially, there were wide 
discrepancies because some annotators would 
annotate one value, others the other.  Annotators 
are now instructed to make judgments on both 
values in this case.  The use of the annotation 
guidelines resulted in about 10% improvement in 
inter-annotator agreement (Pan et al, 2006), 
measured as described in Section 2. 
There is a residual of gross discrepancies in 
annotators? judgments that result from differ-
ences of opinion, for example, about how long a 
government policy is typically in effect.  But the 
number of these discrepancies was surprisingly 
small. 
The method and guidelines for annotation are 
described in much greater detail in (Pan et al, 
2006).  In the current paper, we focus on how 
inter-annotator agreement is measured, in Sec-
tion 2, and in Sections 3-5 on the machine learn-
ing experiments.  Because the annotated corpus 
is still fairly small, we cannot hope to learn to 
make fine-grained judgments of event durations 
that are currently annotated in the corpus, but as 
we demonstrate, it is possible to learn useful 
coarse-grained judgments.   
Although there has been much work on tem-
poral anchoring and event ordering in text 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), to our knowledge, there has been no seri-
ous published empirical effort to model and learn 
vague and implicit duration information in natu-
ral language, such as the typical durations of 
events, and to perform reasoning over this infor-
mation. (Cyc apparently has some fuzzy duration 
information, although it is not generally avail-
able; Rieger (1974) discusses the issue for less 
than a page; there has been work in fuzzy logic 
on representing and reasoning with imprecise 
durations (Godo and Vila, 1995; Fortemps, 
1997), but these make no attempt to collect hu-
man judgments on such durations or learn to ex-
tract them automatically from texts.) 
2 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. 
The kappa statistic (Krippendorff, 1980; Car-
letta, 1996) has become the de facto standard to 
assess inter-annotator agreement. It is computed 
as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement, 
which is the probability that the annotators agree 
by chance.  
In order to compute the kappa statistic for our 
task, we have to compute P(A) and P(E), but 
those computations are not straightforward.  
P(A): What should count as agreement among 
annotators for our task?  
P(E): What is the probability that the annota-
tors agree by chance for our task? 
2.1 What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments. For example, 
for a given event with a known gold standard 
duration range from 1 hour to 4 hours, if a ma-
chine learning program outputs a duration of 3 
hours to 5 hours, how should we evaluate this 
result? 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales. However, none of the tech-
niques directly apply to our data, which are 
ranges of durations from a lower bound to an 
upper bound. 
394
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the 
normal or Gaussian distribution to model our 
duration distributions. If the area between lower 
and upper bounds covers 80% of the entire dis-
tribution area, the bounds are each 1.28 standard 
deviations from the mean.  
Figure 1 shows the overlap in distributions for 
judgments of [10 minutes, 30 minutes] and [10 
minutes, 2 hours], and the overlap or agreement 
is 0.508706. 
2.2 Expected Agreement 
What is the probability that the annotators agree 
by chance for our task? The first quick response 
to this question may be 0, if we consider all the 
possible durations from 1 second to 1000 years 
or even positive infinity. 
However, not all the durations are equally pos-
sible. As in (Krippendorff, 1980), we assume 
there exists one global distribution for our task 
(i.e., the duration ranges for all the events), and 
?chance? annotations would be consistent with 
this distribution. Thus, the baseline will be an 
annotator who knows the global distribution and 
annotates in accordance with it, but does not read 
the specific article being annotated. Therefore, 
we must compute the global distribution of the 
durations, in particular, of their means and their 
widths. This will be of interest not only in deter-
mining expected agreement, but also in terms of  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
what it says about the genre of news articles and 
about fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
There are two peaks in this distribution. One is 
from 5 to 7 in the natural logarithmic scale, 
which corresponds to about 1.5 minutes to 30 
minutes. The other is from 14 to 17 in the natural 
logarithmic scale, which corresponds to about 8 
days to 6 months. One could speculate that this 
bimodal distribution is because daily newspapers 
report short events that happened the day before 
and place them in the context of larger trends.  
We also compute the distribution of the widths 
(i.e., Xupper ? Xlower) of all the annotated durations, 
and its histogram is shown in Figure 3, where the 
horizontal axis represents the width in the natural 
logarithmic scale and the vertical axis represents 
the number of annotated durations with that 
width. Note that it peaks at about a half order of 
magnitude (Hobbs and Kreinovich, 2001).  
Since the global distribution is determined by 
the above mean and width distributions, we can 
then compute the expected agreement, i.e., the 
probability that the annotators agree by chance, 
where the chance is actually based on this global 
distribution. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006). For both, P(E) is about 0.15. 
 
395
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r 
of
 A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
3 Features 
In this section, we describe the lexical, syntactic, 
and semantic features that we considered in 
learning event durations. 
3.1 Local Context 
For a given event, the local context features in-
clude a window of n tokens to its left and n to-
kens to its right, as well as the event itself, for n 
= {0, 1, 2, 3}. The best n determined via cross 
validation turned out to be 0, i.e., the event itself 
with no local context. But we also present results 
for n = 2 in Section 4.3 to evaluate the utility of 
local context. 
A token can be a word or a punctuation mark. 
Punctuation marks are not removed, because they 
can be indicative features for learning event du-
rations. For example, the quotation mark is a 
good indication of quoted reporting events, and 
the duration of such events most likely lasts for 
seconds or minutes, depending on the length of 
the quoted content. However, there are also cases 
where quotation marks are used for other pur-
poses, such as emphasis of quoted words and 
titles of artistic works. 
For each token in the local context, including 
the event itself, three features are included: the 
original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. The 
lemma of the token is extracted from parse trees 
generated by the CONTEX parser (Hermjakob 
and Mooney, 1997) which includes rich context 
information in parse trees, and the Brill tagger 
(Brill, 1992) is used for POS tagging. 
The context window doesn?t cross the bounda-
ries of sentences. When there are not enough to-
kens on either side of the event within the win-
dow, ?NULL? is used for the feature values. 
Features Original Lemma POS 
Event signed sign VBD 
1token-after the the DT 
2token-after plan plan NN 
1token-before Friday Friday NNP 
2token-before on on IN 
Table 1: Local context features for the ?signed? 
event in sentence (1) with n = 2. 
 
The local context features extracted for the 
?signed? event in sentence (1) is shown in Table 
1 (with a window size n = 2). The feature vector 
is [signed, sign, VBD, the, the, DT, plan, plan, 
NN, Friday, Friday, NNP, on, on, IN]. 
 
(1) The two presidents on Friday signed the 
plan. 
3.2 Syntactic Relations 
The information in the event?s syntactic envi-
ronment is very important in deciding the dura-
tions of events. For example, there is a difference 
in the durations of the ?watch? events in the 
phrases ?watch a movie? and ?watch a bird fly?. 
For a given event, both the head of its subject 
and the head of its object are extracted from the 
parse trees generated by the CONTEX parser. 
Similarly to the local context features, for both 
the subject head and the object head, their origi-
nal form, lemma, and POS tags are extracted as 
features. When there is no subject or object for 
an event, ?NULL? is used for the feature values. 
For the ?signed? event in sentence (1), the 
head of its subject is ?presidents? and the head of 
its object is ?plan?. The extracted syntactic rela-
tion features are shown in Table 2, and the fea-
ture vector is [presidents, president, NNS, plan, 
plan, NN]. 
3.3 WordNet Hypernyms 
Events with the same hypernyms may have simi-
lar durations. For example, events ?ask? and 
?talk? both have a direct WordNet (Miller, 1990) 
hypernym of ?communicate?, and most of the 
time they do have very similar durations in the 
corpus. 
However, closely related events don?t always 
have the same direct hypernyms. For example, 
?see? has a direct hypernym of ?perceive?, 
whereas ?observe? needs two steps up through 
the hypernym hierarchy before reaching ?per-
ceive?. Such correlation between events may be 
lost if only the direct hypernyms of the words are 
extracted. 
396
Features Original Lemma POS 
Subject presidents president NNS 
Object plan plan NN 
Table 2: Syntactic relation features for the 
?signed? event in sentence (1). 
 
Feature 1-hyper 2-hyper 3-hyper 
Event write communicate interact 
Subject corporate executive executive 
adminis-
trator 
Object idea content cognition 
Table 3: WordNet hypernym features for the 
event (?signed?), its subject (?presidents?), and 
its object (?plan?) in sentence (1). 
 
It is useful to extract the hypernyms not only 
for the event itself, but also for the subject and 
object of the event. For example, events related 
to a group of people or an organization usually 
last longer than those involving individuals, and 
the hypernyms can help distinguish such con-
cepts. For example, ?society? has a ?group? hy-
pernym (2 steps up in the hierarchy), and 
?school? has an ?organization? hypernym (3 
steps up). The direct hypernyms of nouns are 
always not general enough for such purpose, but 
a hypernym at too high a level can be too general 
to be useful. For our learning experiments, we 
extract the first 3 levels of hypernyms from 
WordNet. 
Hypernyms are only extracted for the events 
and their subjects and objects, not for the local 
context words. For each level of hypernyms in 
the hierarchy, it?s possible to have more than one 
hypernym, for example, ?see? has two direct hy-
pernyms, ?perceive? and ?comprehend?. For a 
given word, it may also have more than one 
sense in WordNet. In such cases, as in (Gildea 
and Jurafsky, 2002), we only take the first sense 
of the word and the first hypernym listed for each 
level of the hierarchy. A word disambiguation 
module might improve the learning performance. 
But since the features we need are the hypernyms, 
not the word sense itself, even if the first word 
sense is not the correct one, its hypernyms can 
still be good enough in many cases. For example, 
in one news article, the word ?controller? refers 
to an air traffic controller, which corresponds to 
the second sense in WordNet, but its first sense 
(business controller) has the same hypernym of 
?person? (3 levels up) as the second sense (direct 
hypernym). Since we take the first 3 levels of 
hypernyms, the correct hypernym is still ex-
tracted. 
 
P(A) P(E) Kappa 
0.528 0.740 0.877 
0.500 0.755 
Table 4: Inter-Annotator Agreement for Binary 
Event Durations. 
 
When there are less than 3 levels of hy-
pernyms for a given word, its hypernym on the 
previous level is used. When there is no hy-
pernym for a given word (e.g., ?go?), the word 
itself will be used as its hypernyms. Since 
WordNet only provides hypernyms for nouns 
and verbs, ?NULL? is used for the feature values 
for a word that is not a noun or a verb.  
For the ?signed? event in sentence (1), the ex-
tracted WordNet hypernym features for the event 
(?signed?), its subject (?presidents?), and its ob-
ject (?plan?) are shown in Table 3, and the fea-
ture vector is [write, communicate, interact, cor-
porate_executive, executive, administrator, idea, 
content, cognition]. 
4 Experiments 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
4.1 Inter-Annotator Agreement, Baseline, 
and Upper Bound 
Before evaluating the performance of different 
learning algorithms, the inter-annotator agree-
ment, the baseline and the upper bound for the 
learning task are assessed first.  
Table 4 shows the inter-annotator agreement 
results among 3 annotators for binary event dura-
tions. The experiments were conducted on the 
same data sets as in (Pan et al, 2006). Two 
kappa values are reported with different ways of 
measuring expected agreement (P(E)), i.e., 
whether or not the annotators have prior knowl-
edge of the global distribution of the task. 
The human agreement before reading the 
guidelines (0.877) is a good estimate of the upper 
bound performance for this binary classification 
task. The baseline for the learning task is always 
taking the most probable class. Since 59.0% of 
the total data is ?long? events, the baseline per-
formance is 59.0%. 
 
 
397
Class Algor. Prec. Recall F-Score
SVM 0.707 0.606 0.653 
NB 0.567 0.768 0.652 Short 
C4.5 0.571 0.600 0.585 
SVM 0.793 0.857 0.823 
NB 0.834 0.665 0.740 
Long 
 
C4.5 0.765 0.743 0.754 
Table 5: Test Performance of Three Algorithms. 
4.2 Data 
The original annotated data can be straightfor-
wardly transformed for this binary classification 
task. For each event annotation, the most likely 
(mean) duration is calculated first by averaging 
(the logs of) its lower and upper bound durations. 
If its most likely (mean) duration is less than a 
day (about 11.4 in the natural logarithmic scale), 
it is assigned to the ?short? event class, otherwise 
it is assigned to the ?long? event class. (Note that 
these labels are strictly a convenience and not an 
analysis of the meanings of ?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes 
(see Section 4.4). 
4.3 Experimental Results (non-WSJ) 
Learning Algorithms. Three supervised learn-
ing algorithms were evaluated for our binary 
classification task, namely, Support Vector Ma-
chines (SVM) (Vapnik, 1995), Na?ve Bayes 
(NB) (Duda and Hart, 1973), and Decision Trees 
C4.5 (Quinlan, 1993). The Weka (Witten and 
Frank, 2005) machine learning package was used 
for the implementation of these learning algo-
rithms. Linear kernel is used for SVM in our ex-
periments. 
Each event instance has a total of 18 feature 
values, as described in Section 3, for the event 
only condition, and 30 feature values for the lo-
cal context condition when n = 2. For SVM and 
C4.5, all features are converted into binary fea-
tures (6665 and 12502 features). 
Results. 10-fold cross validation was used to 
train the learning models, which were then tested 
on the unseen held-out test set, and the perform-
ance (including the precision, recall, and F-score1  
                                                 
1 F-score is computed as the harmonic mean of the preci-
sion and recall: F = (2*Prec*Rec)/(Prec+Rec). 
Algorithm Precision  
Baseline 59.0% 
C4.5 69.1% 
NB 70.3% 
SVM 76.6% 
Human Agreement 87.7% 
Table 6: Overall Test Precision on non-WSJ 
Data. 
 
for each class) of the three learning algorithms is 
shown in Table 5. The significant measure is 
overall precision, and this is shown for the three 
algorithms in Table 6, together with human a-
greement (the upper bound of the learning task) 
and the baseline. 
We can see that among all three learning algo-
rithms, SVM achieves the best F-score for each 
class and also the best overall precision (76.6%). 
Compared with the baseline (59.0%) and human 
agreement (87.7%), this level of performance is 
very encouraging, especially as the learning is 
from such limited training data. 
Feature Evaluation. The best performing 
learning algorithm, SVM, was then used to ex-
amine the utility of combinations of four differ-
ent feature sets (i.e., event, local context, syntac-
tic, and WordNet hypernym features). The de-
tailed comparison is shown in Table 7.  
We can see that most of the performance 
comes from event word or phrase itself. A sig-
nificant improvement above that is due to the 
addition of information about the subject and 
object. Local context does not help and in fact 
may hurt, and hypernym information also does 
not seem to help2. It is of interest that the most 
important information is that from the predicate 
and arguments describing the event, as our lin-
guistic intuitions would lead us to expect. 
4.4 Test on WSJ Data 
Section 4.3 shows the experimental results with 
the learned model trained and tested on the data 
with the same genre, i.e., non-WSJ articles. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). The performance (including the 
precision, recall, and F-score for each class) is 
shown in Table 8. The precision (75.0%) is very 
close to the test performance on the non-WSJ  
                                                 
2 In the ?Syn+Hyper? cases, the learning algorithm with and 
without local context gives identical results, probably be-
cause the other features dominate. 
398
Event Only (n = 0) Event Only + Syntactic Event + Syn + Hyper Class 
Prec. Rec. F Prec. Rec. F Prec. Rec. F 
Short 0.742 0.465  0.571 0.758 0.587 0.662 0.707    0.606 0.653 
Long 0.748 0.908 0.821 0.792 0.893 0.839 0.793 0.857 0.823 
Overall Prec. 74.7% 78.2% 76.6% 
 Local Context (n = 2) Context + Syntactic Context + Syn + Hyper 
Short 0.672 0.568 0.615 0.710 0.600    0.650 0.707    0.606 0.653 
Long 0.774 0.842 0.806 0.791 0.860 0.824 0.793 0.857 0.823 
Overall Prec. 74.2% 76.6% 76.6% 
Table 7: Feature Evaluation with Different Feature Sets using SVM. 
 
Class Prec. Rec. F 
Short 0.692   0.610 0.649
Long 0.779   0.835 0.806
Overall Prec. 75.0% 
Table 8: Test Performance on WSJ data. 
 
P(A) P(E) Kappa 
0.151 0.762 0.798 
0.143 0.764 
Table 9: Inter-Annotator Agreement for Most 
Likely Temporal Unit. 
 
data, and indicates the significant generalization 
capacity of the learned model. 
5 Learning the Most Likely Temporal 
Unit 
These encouraging results have prompted us to 
try to learn more fine-grained event duration in-
formation, viz., the most likely temporal units of 
event durations (cf. (Rieger 1974)?s ORDER-
HOURS, ORDERDAYS). 
For each original event annotation, we can ob-
tain the most likely (mean) duration by averaging 
its lower and upper bound durations, and assign-
ing it to one of seven classes (i.e., second, min-
ute, hour, day, week, month, and year) based on 
the temporal unit of its most likely duration.  
However, human agreement on this more fine-
grained task is low (44.4%). Based on this obser-
vation, instead of evaluating the exact agreement 
between annotators, an ?approximate agreement? 
is computed for the most likely temporal unit of 
events. In ?approximate agreement?, temporal 
units are considered to match if they are the same 
temporal unit or an adjacent one. For example, 
?second? and ?minute? match, but ?minute? and 
?day? do not. 
Some preliminary experiments have been con-
ducted for learning this multi-classification task. 
The same data sets as in the binary classification 
task were used. The only difference is that the 
class for each instance is now labeled with one 
Algorithm Precision  
Baseline 51.5% 
C4.5 56.4% 
NB 65.8% 
SVM 67.9% 
Human Agreement 79.8% 
Table 10: Overall Test Precisions. 
 
of the seven temporal unit classes. 
The baseline for this multi-classification task 
is always taking the temporal unit which with its 
two neighbors spans the greatest amount of data. 
Since the ?week?, ?month?, and ?year? classes 
together take up largest portion (51.5%) of the 
data, the baseline is always taking the ?month? 
class, where both ?week? and ?year? are also 
considered a match. Table 9 shows the inter-
annotator agreement results for most likely tem-
poral unit when using ?approximate agreement?. 
Human agreement (the upper bound) for this 
learning task increases from 44.4% to 79.8%. 
10-fold cross validation was also used to train 
the learning models, which were then tested on 
the unseen held-out test set. The performance of 
the three algorithms is shown in Table 10. The 
best performing learning algorithm is again SVM 
with 67.9% test precision. Compared with the 
baseline (51.5%) and human agreement (79.8%), 
this again is a very promising result, especially 
for a multi-classification task with such limited 
training data. It is reasonable to expect that when 
more annotated data becomes available, the 
learning algorithm will achieve higher perform-
ance when learning this and more fine-grained 
event duration information. 
Although the coarse-grained duration informa-
tion may look too coarse to be useful, computers 
have no idea at all whether a meeting event takes 
seconds or centuries, so even coarse-grained es-
timates would give it a useful rough sense of how 
long each event may take. More fine-grained du-
ration information is definitely more desirable 
for temporal reasoning tasks. But coarse-grained 
399
durations to a level of temporal units can already 
be very useful. 
6 Conclusion 
In the research described in this paper, we have 
addressed a problem -- extracting information 
about event durations encoded in event descrip-
tions -- that has heretofore received very little 
attention in the field.  It is information that can 
have a substantial impact on applications where 
the temporal placement of events is important.  
Moreover, it is representative of a set of prob-
lems ? making use of the vague information in 
text ? that has largely eluded empirical ap-
proaches in the past.  In (Pan et al, 2006), we 
explicate the linguistic categories of the phenom-
ena that give rise to grossly discrepant judgments 
among annotators, and give guidelines on resolv-
ing these discrepancies.  In the present paper, we 
describe a method for measuring inter-annotator 
agreement when the judgments are intervals on a 
scale; this should extend from time to other sca-
lar judgments.  Inter-annotator agreement is too 
low on fine-grained judgments.  However, for the 
coarse-grained judgments of more than or less 
than a day, and of approximate agreement on 
temporal unit, human agreement is acceptably 
high.  For these cases, we have shown that ma-
chine-learning techniques achieve impressive 
results.   
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
E. Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
R. O. Duda and P. E. Hart. 1973. Pattern Classifica-
tion and Scene Analysis. Wiley, New York. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
P. Fortemps. 1997. Jobshop Scheduling with Impre-
cise Durations: A Fuzzy Approach. IEEE Transac-
tions on Fuzzy Systems Vol. 5 No. 4. 
D. Gildea and D. Jurafsky. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245-288. 
L. Godo and L. Vila. 1995. Possibilistic Temporal 
Reasoning based on Fuzzy Temporal Constraints. 
In Proceedings of International Joint Conference 
on Artificial Intelligence (IJCAI). 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of the 38th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL). 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006. An Anno-
tated Corpus of Typical Durations of Events. In 
Proceedings of the Fifth International Conference 
on Language Resources and Evaluation (LREC), 
Genoa, Italy. 
J. Pustejovsky, P. Hanks, R. Saur?, A. See, R. Gai-
zauskas, A. Setzer, D. Radev, B. Sundheim, D. 
Day, L. Ferro and M. Lazo. 2003. The timebank 
corpus. In Corpus Linguistics, Lancaster, U.K. 
J. R. Quinlan. 1993. C4.5: Programs for Machine 
Learning. Morgan Kaufmann, San Francisco. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
V. Vapnik. 1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
I. H. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco. 
400
Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 38?45,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extending TimeML with Typical Durations of Events 
 
 
Feng Pan, Rutu Mulkar, and Jerry R. Hobbs 
Information Sciences Institute (ISI), University of Southern California 
4676 Admiralty Way, Marina del Rey, CA 90292, USA 
{pan, rutu, hobbs}@isi.edu 
 
  
 
Abstract 
In this paper, we demonstrate how to ex-
tend TimeML, a rich specification lan-
guage for event and temporal expressions 
in text, with the implicit typical durations 
of events, temporal information in text 
that has hitherto been largely unexploited. 
Event duration information can be very 
important in applications in which the 
time course of events is to be extracted 
from text. For example, whether two 
events overlap or are in sequence often 
depends very much on their durations.     
1 Introduction 
Temporal information processing has become 
more and more important in many natural lan-
guage processing (NLP) applications, such as 
question answering (Harabagiu and Bejan, 2005; 
Moldovan et. al., 2005; Saur? et. al., 2005), 
summarization (Mani and Schiffman, 2005), and 
information extraction (Surdeanu et. al., 2003). 
Temporal anchoring and event ordering are 
among the most important kinds of temporal in-
formation needed for NLP applications. Al-
though there has been much work on extracting 
and inferring such information from texts 
(Hitzeman et al, 1995; Mani and Wilson, 2000; 
Filatova and Hovy, 2001; Boguraev and Ando, 
2005), none of this work has exploited the im-
plicit event duration information from the text.  
Consider the sentence from a news article: 
George W. Bush met with Vladimir Putin in 
Moscow. 
How long was the meeting?  Our first reaction 
to this question might be that we have no idea.  
But in fact we do have an idea.  We know the 
meeting was longer than 10 seconds and less 
than a year.  How much tighter can we get the 
bounds to be?  Most people would say the meet-
ing lasted between an hour and three days. 
There is much temporal information in text 
that has hitherto been largely unexploited, en-
coded in the descriptions of events and relying 
on our knowledge of the range of usual durations 
of types of events, which can be very important 
in applications in which the time course of events 
is to be extracted from news.  For example, 
whether two events overlap or are in sequence 
often depends very much on their durations.  If a 
war started yesterday, we can be pretty sure it is 
still going on today.  If a hurricane started last 
year, we can be sure it is over by now. 
To extract such implicit event duration infor-
mation from texts automatically, we developed a 
corpus annotated with typical durations of events 
(Pan et al, 2006a) which currently contains all 
the 48 non-Wall-Street-Journal (non-WSJ) news 
articles (a total of 2132 event instances), as well 
as 10 WSJ articles (156 event instances), from 
the TimeBank corpus annotated in TimeML 
(Pustejovky et al, 2003).  
Because the annotated corpus is still fairly 
small, we cannot hope to learn to make fine-
grained judgments of event durations that are 
currently annotated in the corpus, but as we show 
in greater detail in (Pan et al, 2006b), it is possi-
ble to learn useful coarse-grained judgments that 
considerably outperform a baseline and approach 
human performance. 
This paper describes our work on extending 
TimeML with annotations of typical durations of 
events, which can enrich the expressiveness of 
TimeML, and provides NLP applications that 
exploit TimeML with this additional implicit 
event duration information for their temporal 
information processing tasks. 
In Section 2 we first describe the corpus of 
typical durations of events, including the annota-
tion guidelines, the representative event classes 
with examples, the inter-annotator agreement 
38
study, and the machine learning results. TimeML 
and its event classes will be described in Section 
3, and we will discuss how to integrate event du-
ration annotations into TimeML in Section 4.  
2 Annotating and Learning Typical Du-
ration of Events 
In the corpus of typical durations of events, every 
event to be annotated was already identified in 
the TimeBank corpus.  Annotators are asked to 
provide lower and upper bounds on the duration 
of the event, and a judgment of level of confi-
dence in those estimates on a scale from one to 
ten. An interface was built to facilitate the anno-
tation. Graphical output is displayed to enable us 
to visualize quickly the level of agreement 
among different annotators for each event. For 
example, here is the output of the annotations (3 
annotators) for the ?finished? event (in bold) in 
the sentence 
After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,... 
 
 
This graph shows that the first annotator believes 
that the event lasts for minutes whereas the sec-
ond annotator believes it could only last for sev-
eral seconds. The third annotates the event to 
range from a few seconds to a few minutes. A 
logarithmic scale is used for the output. 
2.1 Annotation Instructions 
Annotators are asked to identify upper and lower 
bounds that would include 80% of the possible 
cases, excluding anomalous cases.   
The judgments are to be made in context.  
First of all, information in the syntactic environ-
ment needs to be considered before annotating, 
and the events need to be annotated in light of 
the information provided by the entire article. 
Annotation is made easier and more consistent if 
coreferential and near-coreferential descriptions 
of events are identified initially. 
When the articles were completely annotated 
by the three annotators, the results were analyzed 
and the differences were reconciled. Differences 
in annotation could be due to the differences in 
interpretations of the event; however, we found 
that the vast majority of radically different judg-
ments can be categorized into a relatively small 
number of classes. Some of these correspond to 
aspectual features of events, which have been 
intensively investigated (e.g., Vendler, 1967; 
Dowty, 1979; Moens and Steedman, 1988; Pas-
sonneau, 1988). We then developed guidelines to 
cover those cases (see the next section). 
2.2 Event Classes 
Action vs. State: Actions involve change, such 
as those described by words like "speaking", 
"gave", and "skyrocketed". States involve things 
staying the same, such as being dead, being dry, 
and being at peace. When we have an event in 
the passive tense, sometimes there is an ambigu-
ity about whether the event is a state or an action. 
For example, 
Three people were injured in the attack. 
Is the ?injured? event an action or a state? This 
matters because they will have different dura-
tions. The state begins with the action and lasts 
until the victim is healed. Besides the general 
diagnostic tests to distinguish them (Vendler, 
1967; Dowty, 1979), another test can be applied 
to this specific case: Imagine someone says the 
sentence after the action had ended but the state 
was still persisting. Would they use the past or 
present tense? In the ?injured? example, it is 
clear we would say ?Three people were injured 
in the attack?, whereas we would say ?Three 
people are injured from the attack.? Our annota-
tion interface handles events of this type by al-
lowing the annotator to specify which interpreta-
tion he is giving. If the annotator feels it?s too 
ambiguous to distinguish, annotations can be 
given for both interpretations. 
 
Aspectual Events:  Some events are aspects of 
larger events, such as their start or finish. Al-
though they may seem instantaneous, we believe 
they should be considered to happen across some 
interval, i.e., the first or last sub-event of the lar-
ger event. For example,   
 After the victim, Linda Sanders, 35, had fin-
ished her cleaning and was waiting for her 
clothes to dry,? 
The ?finished? event should be considered as the 
last sub-event of the larger event (the ?cleaning? 
event), since it actually involves opening the 
door of the washer, taking out the clothes, clos-
ing the door, and so on. All this takes time. This 
39
interpretation will also give us more information 
on typical durations than simply assuming such 
events are instantaneous. 
 
Reporting Events: These are everywhere in the 
news. They can be direct quotes, taking exactly 
as long as the sentence takes to read, or they can 
be summarizations of long press conferences. We 
need to distinguish different cases: 
Quoted Report: This is when the reported 
content is quoted. The duration of the event 
should be the actual duration of the utterance of 
the quoted content. The time duration can be eas-
ily verified by saying the sentence out loud and 
timing it. For example, 
"It looks as though they panicked," a detective 
said of the robbers. 
This probably took between 1 and 3 seconds; it?s 
very unlikely it took more than 10 seconds. 
Unquoted Report: This is when the reporting 
description occurs without quotes that could be 
as short as just the duration of the actual utter-
ance of the reported content (lower bound), and 
as long as the duration of a briefing or press con-
ference (upper bound). 
If the sentence is very short, then it's likely 
that it is one complete sentence from the 
speaker's remarks, and a short duration should be 
given; if it is a long, complex sentence, then it's 
more likely to be a summary of a long discussion 
or press conference, and a longer duration should 
be given. For example, 
The police said it did not appear that anyone 
else was injured. 
A Brooklyn woman who was watching her 
clothes dry in a laundromat was killed Thursday 
evening when two would-be robbers emptied 
their pistols into the store, the police said. 
If the first sentence were quoted text, it would be 
very much the same. Hence the duration of the 
?said? event should be short. In the second sen-
tence everything that the spokesperson (here the 
police) has said is compiled into a single sen-
tence by the reporter, and it is unlikely that the 
spokesperson said only a single sentence with all 
this information. Thus, it is reasonable to give 
longer duration to this ?said? event. 
 
Multiple Events: Many occurrences of verbs 
and other event descriptors refer to multiple 
events, especially, but not exclusively, if the sub-
ject or object of the verb is plural.  For example,   
Iraq has destroyed its long-range missiles.  
Both single (i.e., destroyed one missile) and ag-
gregate (i.e., destroyed all missiles) events hap-
pened. This was a significant source in dis-
agreements in our first round of annotation. 
Since both judgments provide useful informa-
tion, our current annotation interface allows the 
annotator to specify the event as multiple, and 
give durations for both the single and aggregate 
events.  
 
Events Involving Negation: Negated events 
didn't happen, so it may seem strange to specify 
their duration. But whenever negation is used, 
there is a certain class of events whose occur-
rence is being denied. Annotators should con-
sider this class, and make a judgment about the 
likely duration of the events in it. In addition, 
there is the interval during which the nonoccur-
rence of the events holds. For example,  
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
There is the typical amount of time of ?being 
attacked?, i.e., the duration of a single attack, and 
a longer period of time of ?not being attacked?. 
Similarly to multiple events, annotators are asked 
to give durations for both the event negated and 
the negation of that event.   
 
Positive Infinite Durations: These are states 
which continue essentially forever once they be-
gin. For example, 
He is dead. 
Here the time continues for an infinite amount 
of time, and we allow this as an annotation. 
2.3 Inter-Annotator Agreement 
Although the graphical output of the annotations 
enables us to visualize quickly the level of agree-
ment among different annotators for each event, 
a quantitative measurement of the agreement is 
needed. The kappa statistic (Krippendorff, 1980; 
Carletta, 1996) has become the de facto standard 
to assess inter-annotator agreement. It is com-
puted as: 
)(1
)()(
EP
EPAP
?
?=?  
P(A) is the observed agreement among the an-
notators, and P(E) is the expected agreement,  
 
40
 
Figure 1: Overlap of Judgments of [10 minutes, 
30 minutes] and [10 minutes, 2 hours]. 
 
which is the probability that the annotators agree 
by chance.  
2.3.1  What Should Count as Agreement? 
Determining what should count as agreement is 
not only important for assessing inter-annotator 
agreement, but is also crucial for later evaluation 
of machine learning experiments.  
We first need to decide what scale is most ap-
propriate. One possibility is just to convert all the 
temporal units to seconds. However, this would 
not correctly capture our intuitions about the 
relative relations between duration ranges. For 
example, the difference between 1 second and 20 
seconds is significant; while the difference be-
tween 1 year 1 second and 1 year 20 seconds is 
negligible. In order to handle this problem, we 
use a logarithmic scale for our data. After first 
converting from temporal units to seconds, we 
then take the natural logarithms of these values. 
This logarithmic scale also conforms to the half 
orders of magnitude (HOM) (Hobbs and Kreino-
vich, 2001) which was shown to have utility in 
several very different linguistic contexts. 
In the literature on the kappa statistic, most au-
thors address only category data; some can han-
dle more general data, such as data in interval 
scales or ratio scales (Krippendorff, 1980; Car-
letta, 1996). However, none of the techniques 
directly apply to our data, which are ranges of 
durations from a lower bound to an upper bound. 
In fact, what coders were instructed to anno-
tate for a given event is not just a range, but a 
duration distribution for the event, where the 
area between the lower bound and the upper 
bound covers about 80% of the entire distribution 
area. Since it?s natural to assume the most likely 
duration for such distribution is its mean (aver-
age) duration, and the distribution flattens out 
toward the upper and lower bounds, we use the  
-5 0 5 10 15 20 25 30
0
20
40
60
80
100
120
140
160
180
Means of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 2: Distribution of Means of Annotated 
Durations. 
 
normal or Gaussian distribution to model our 
duration distributions. 
In order to determine a normal distribution, we 
need to know two parameters: the mean and the 
standard deviation. For our duration distributions 
with given lower and upper bounds, the mean is 
the average of the bounds. Under the assumption 
that the area between lower and upper bounds 
covers 80% of the entire distribution area, the 
lower and upper bounds are each 1.28 standard 
deviations from the mean.  
With this data model, the agreement between 
two annotations can be defined as the overlap-
ping area between two normal distributions. The 
agreement among many annotations is the aver-
age overlap of all the pairwise overlapping areas. 
For example, the overlap of judgments of [10 
minutes, 30 minutes] and [10 minutes, 2 hours] 
are as in Figure 1. The overlap or agreement is 
0.508706. 
2.3.2  Expected Agreement 
As in (Krippendorff, 1980), we assume there ex-
ists one global distribution for our task (i.e., the 
duration ranges for all the events), and ?chance? 
annotations would be consistent with this distri-
bution. Thus, the baseline will be an annotator 
who knows the global distribution and annotates 
in accordance with it, but does not read the spe-
cific article being annotated. Therefore, we must 
compute the global distribution of the durations, 
in particular, of their means and their widths. 
This will be of interest not only in determining 
expected agreement, but also in terms of what it 
says about the genre of news articles and about 
fuzzy judgments in general. 
We first compute the distribution of the means 
of all the annotated durations. Its histogram is 
shown in Figure 2, where the horizontal axis 
41
-5 0 5 10 15 20 25
0
50
100
150
200
250
300
350
400
Widths of Annotated Durations
N
um
be
r o
f A
nn
ot
at
ed
 D
ur
at
io
ns
 
Figure 3: Distribution of Widths of Annotated 
Durations. 
 
represents the mean values in the natural loga-
rithmic scale and the vertical axis represents the 
number of annotated durations with that mean. 
We also compute the distribution of the widths 
(i.e., upper bound ? lower bound) of all the anno-
tated durations, and its histogram is shown in 
Figure 3, where the horizontal axis represents the 
width in the natural logarithmic scale and the 
vertical axis represents the number of annotated 
durations with that width. 
Two different methods were used to compute 
the expected agreement (baseline), both yielding 
nearly equal results. These are described in detail 
in (Pan et al, 2006a). For both, P(E) is about 
0.15. 
Experimental results show that the use of the 
annotation guidelines resulted in about 10% im-
provement in inter-annotator agreement, meas-
ured as described in this section, see (Pan et al, 
2006a) for details. 
2.4 Machine Learning Experiments 
2.4.1  Features 
Local Context. For a given event, the local con-
text features include a window of n tokens to its 
left and n tokens to its right, as well as the event 
itself. The best n was determined via cross vali-
dation. A token can be a word or a punctuation 
mark. For each token in the local context, includ-
ing the event itself, three features are included: 
the original form of the token, its lemma (or root 
form), and its part-of-speech (POS) tag. 
Syntactic Relations. The information in the 
event?s syntactic environment is very important 
in deciding the durations of events. For a given 
event, both the head of its subject and the head of 
its object are extracted from the parse trees gen-
erated by the CONTEX parser (Hermjakob and 
Mooney, 1997). Similarly to the local context 
features, for both the subject head and the object 
head, their original form, lemma, and POS tags 
are extracted as features. 
 WordNet Hypernyms. Events with the same 
hypernyms may have similar durations. But 
closely related events don?t always have the 
same direct hypernyms. We extract the hy-
pernyms not only for the event itself, but also for 
the subject and object of the event, since events 
related to a group of people or an organization 
usually last longer than those involving individu-
als, and the hypernyms can help distinguish such 
concepts. For our learning experiments, we ex-
tract the first 3 levels of hypernyms from Word-
Net (Miller, 1990). 
2.4.2  Learning Coarse-grained Binary 
Event Durations 
The distribution of the means of the annotated 
durations in Figure 2 is bimodal, dividing the 
events into those that take less than a day and 
those that take more than a day. Thus, in our first 
machine learning experiment, we have tried to 
learn this coarse-grained event duration informa-
tion as a binary classification task. 
Data. The original annotated data can be 
straightforwardly transformed for this binary 
classification task. For each event annotation, the 
most likely (mean) duration is calculated first by 
averaging (the logs of) its lower and upper bound 
durations. If its most likely (mean) duration is 
less than a day (about 11.4 in the natural loga-
rithmic scale), it is assigned to the ?short? event 
class, otherwise it is assigned to the ?long? event 
class. (Note that these labels are strictly a con-
venience and not an analysis of the meanings of 
?short? and ?long?.) 
We divide the total annotated non-WSJ data 
(2132 event instances) into two data sets: a train-
ing data set with 1705 event instances (about 
80% of the total non-WSJ data) and a held-out 
test data set with 427 event instances (about 20% 
of the total non-WSJ data). The WSJ data (156 
event instances) is kept for further test purposes. 
Results. The learning results in Figure 4 show 
that among all three learning algorithms explored 
(Na?ve Bayes (NB), Decision Trees C4.5, and 
Support Vector Machines (SVM)), SVM with 
linear kernel achieves the best overall precision 
(76.6%). Compared with the baseline (59.0%) 
and human agreement (87.7%), this level of per-
formance is very encouraging, especially as the 
learning is from such limited training data. 
 
42
 
Figure 4: Overall Test Precision on non-WSJ 
Data. 
 
Feature evaluation in (Pan et al, 2006b) shows 
that most of the performance comes from event 
word or phrase itself. A significant improvement 
above that is due to the addition of information 
about the subject and object. Local context does 
not help and in fact may hurt, and hypernym in-
formation also does not seem to help. It is grati-
fying to see that the most important information 
is that from the predicate and arguments describ-
ing the event, as our linguistic intuitions would 
lead us to expect. 
In order to evaluate whether the learned model 
can perform well on data from different news 
genres, we tested it on the unseen WSJ data (156 
event instances). A precision of 75.0%, which is 
very close to the test performance on the non-
WSJ data, proves the great generalization capac-
ity of the learned model. 
Some preliminary experimental results of 
learning the more fine-grained event duration 
information, i.e., the most likely temporal unit 
(cf. (Rieger 1974)?s ORDERHOURS, ORDERDAYS), 
are shown in (Pan et al, 2006b). SVM again 
achieves the best performance with 67.9% test 
precision (baseline 51.5% and human agreement 
79.8%) in ?approximate agreement? where tem-
poral units are considered to match if they are the 
same temporal unit or an adjacent one. 
3 TimeML and Its Event Classes 
TimeML (Pustejovsky et al, 2003) is a rich 
specification language for event and temporal 
expressions in natural language text. Unlike most 
previous attempts at event and temporal specifi-
cation, TimeML separates the representation of 
event and temporal expressions from the anchor-
ing or ordering dependencies that may exist in a 
given text. 
TimeML includes four major data structures: 
EVENT, TIMEX3, SIGNAL, AND LINK. 
EVENT is a cover term for situations that happen 
or occur, and also those predicates describing 
states or circumstances in which something ob-
tains or holds true. TIMEX3, which extends 
TIMEX2 (Ferro, 2001), is used to mark up ex-
plicit temporal expressions, such as time, dates, 
and durations. SIGNAL is used to annotate sec-
tions of text, typically function words that indi-
cate how temporal objects are related to each 
other (e.g., ?when?, ?during?, ?before?). The set 
of LINK tags encode various relations that exist 
between the temporal elements of a document, 
including three subtypes: TLINK (temporal 
links), SLINK (subordination links), and ALINK 
(aspectual links). 
Our event duration annotations can be inte-
grated into the EVENT tag. In TimeML each 
event belongs to one of the seven event classes, 
i.e., reporting, perception, aspectual, I-action, I-
state, state, occurrence. TimeML annotation 
guidelines1 give detailed description for each of 
the classes: 
Reporting. This class describes the action of a 
person or an organization declaring something, 
narrating an event, informing about an event, etc 
(e.g., say, report, tell, explain, state). 
Perception. This class includes events involv-
ing the physical perception of another event (e.g., 
see, watch, view, hear). 
Aspectual. In languages such as English and 
French, there is a grammatical device of aspec-
tual predication, which focuses on different fac-
ets of event history, i.e., initiation, reinitiation, 
termination, culmination, continuation (e.g., be-
gin, stop, finish, continue). 
I-Action. An I-Action is an Intensional Action. 
It introduces an event argument (which must be 
in the text explicitly) describing an action or 
situation from which we can infer something 
given its relation with the I-Action (e.g., attempt, 
try, promise). 
I-State.  This class of events are similar to the 
previous class. This class includes states that re-
fer to alternative or possible worlds (e.g., believe, 
intend, want). 
State. This class describes circumstances in 
which something obtains or holds true (e.g., on 
board, kidnapped, peace). 
Occurrence. This class includes all the many 
other kinds of events describing something that 
happens or occurs in the world (e.g., die, crash, 
build, sell). 
                                                 
1http://www.cs.brandeis.edu/~jamesp/arda/time/time
MLdocs/annguide12wp.pdf 
43
4 Integrating Event Duration Annota-
tions into TimeML 
Our event duration annotations can be integrated 
into TimeML by adding two more attributes to 
the EVENT tag for the lower bound and upper 
bound duration annotations (e.g., ?lowerBound-
Duration? and ?upperBoundDuration? attributes). 
To minimize changes of the existing TimeML 
specifications caused by the integration, we can 
try to share as much as possible our event classes 
as described in Section 2.2 with the existing ones 
in TimeML as described in Section 3.  
We can see that four event classes are shared 
with very similar definitions, i.e., reporting, as-
pectual, state, and action/occurrence. For the 
other three event classes that only belong to Ti-
meML (i.e., perception, I-action, I-state), the I-
action and perception classes can be treated as 
special subclasses of the action/occurrence class, 
and the I-state class as a special subclass of the 
state class. 
However, there are still three classes that only 
belong to the event duration annotations (i.e., 
multiple, negation, and positive infinite). The 
positive infinite class can be treated as a special 
subclass of the state class with a special duration 
annotation for positive infinity.  
Each multiple event has two annotations. For 
example, for 
Iraq has destroyed its long-range missiles.  
there is the time it takes to destroy one missile 
and the duration of the interval in which all the 
individual events are situated ? the time it takes 
to destroy all its missiles.  
Since the single event is usually more likely to 
be encountered in multiple documents, and thus 
the duration of the single event is usually more 
likely to be shared and re-used, to simplify the 
specification, we can take only the duration an-
notation of the single events for the multiple 
event class, and the single event can be assigned 
with one of the seven TimeML event classes. For 
example, the ?destroyed? event in the above ex-
ample is assigned with the occurrence class in 
TimeBank. 
The events involving negation can be simpli-
fied similarly. Since the event negated is usually 
more likely to be encountered in multiple docu-
ments, we can take only the duration annotation 
of the negated event for this class. For example, 
in 
He was willing to withdraw troops in ex-
change for guarantees that Israel would not be 
attacked. 
the event negated is the ?being attacked? event 
and it is assigned with the occurrence class in 
TimeBank.  Alternatively, TimeML could be 
extended to treat negations of events as states. 
The format used for annotated durations is 
consistent with that for the value of the DURA-
TION type in TimeML. For example, the sen-
tence 
The official said these sites could only be vis-
ited by a special team of U.N. monitors and dip-
lomats. 
can be marked up in TimeML as: 
 
The official <EVENT eid="e63" 
class="REPORTING"> said </EVENT> 
these sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE"> 
visited </EVENT> by a special team 
of <ENAMEX TYPE="ORGANIZATION"> U.N. 
</ENAMEX> monitors and diplomats. 
 
If we annotate the ?said? event with the dura-
tion annotation of [5 seconds, 5 minutes], and the 
?visited? event with [10 minutes, 1 day], the ex-
tended mark-up becomes: 
 
The official <EVENT eid="e63" 
class="REPORTING" lowerBoundDura-
tion="PT5S" upperBoundDura-
tion="PT5M"> said </EVENT> these 
sites <SIGNAL sid="s65" 
>could</SIGNAL> only be <EVENT 
eid="e64" class="OCCURRENCE" lower-
BoundDuration="PT10M" upperBoundDu-
ration="P1D"> visited </EVENT> by a 
special team of <ENAMEX 
TYPE="ORGANIZATION"> U.N. </ENAMEX> 
monitors and diplomats. 
5 Conclusion 
In this paper we have demonstrated how to ex-
tend TimeML with typical durations of events. 
We can see that the extension is very straight-
forward. Other interesting temporal information 
can be extracted or learned. For example, for 
each event class, we can generate its own mean 
and widths graphs, and learn their durations 
separately from other classes, which may capture 
different duration characteristics associated with 
each event class.   
44
Acknowledgments 
This work was supported by the Advanced Re-
search and Development Activity (ARDA), now 
the Disruptive Technology Office (DTO), under 
DOD/DOI/ARDA Contract No. NBCHC040027. 
The authors have profited from discussions with 
Hoa Trang Dang, Donghui Feng, Kevin Knight, 
Daniel Marcu, James Pustejovsky, Deepak Ravi-
chandran, and Nathan Sobo. 
References 
B. Boguraev and R. K. Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
gustics, 22(2):249?254. 
D. R. Dowty. 1979. Word Meaning and Montague 
Grammar, Dordrecht, Reidel. 
L. Ferro. 2001. Instruction Manual for the Annotation 
of Temporal Expressions. Mitre Technical Report 
MTR 01W0000046, the MITRE Corporation, 
McLean, Virginia. 
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps to Event-Clauses. Proceedings of ACL 
Workshop on Temporal and Spatial Reasoning. 
S. Harabagiu and C. Bejan. 2005. Question Answer-
ing Based on Temporal Inference. In Proceedings 
of the AAAI-2005 Workshop on Inference for Tex-
tual Question Answering, Pittsburgh, PA. 
U. Hermjakob and R. J. Mooney. 1997. Learning 
Parse and Translation Decisions from Examples 
with Rich Context. In Proceedings of the 35th An-
nual Meeting of the Association for Computational 
Linguistics (ACL). 
J. Hitzeman, M. Moens, and C. Grover. 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of EACL. Dublin, Ire-
land. 
J. R. Hobbs and V. Kreinovich. 2001. Optimal Choice 
of Granularity in Commonsense Estimation: Why 
Half Orders of Magnitude, In Proceedings of Joint 
9th IFSA World Congress and 20th NAFIPS Inter-
national Conference, Vacouver, British Columbia. 
K. Krippendorf. 1980. Content Analysis: An introduc-
tion to its methodology. Sage Publications. 
I. Mani and G. Wilson. 2000. Robust Temporal Proc-
essing of News. In Proceedings of Annual Confer-
ence of the Association for Computational Linguis-
tics (ACL). 
I. Mani and B. Schiffman. 2005. Temporally Anchor-
ing and Ordering Events in News. In J. Pustejovsky 
and R. Gaizauskas ed. Time and Event Recognition 
in Natural Language. John Benjamins. 
G. A. Miller. 1990. WordNet: an On-line Lexical Da-
tabase. International Journal of Lexicography 3(4). 
M. Moens and M. Steedman. 1988. Temporal Ontol-
ogy and Temporal Reference. Computational Lin-
guistics 14(2): 15-28. 
D. Moldovan, C. Clark, and S. Harabagiu. 2005. 
Temporal Context Representation and Reasoning. 
In Proceedings of the International Joint Confer-
enceon Artificial Intelligence (IJCAI). 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006a. An Anno-
tated Corpus of Typical Durations of Events. To 
appear in Proceedings of the Fifth International 
Conference on Language Resources and Evalua-
tion (LREC), Genoa, Italy. 
F. Pan, R. Mulkar, and J. R. Hobbs. 2006b. Learning 
Event Durations from Event Descriptions. To ap-
pear in Proceedings of the 44th Conference of the 
Association for Computational Linguistics (COL-
ING-ACL), Sydney, Australia. 
R. J. Passonneau. 1988. A Computational Model of 
the Semantics of Tense and Aspect. Computational 
Linguistics 14:2.44-60. 
J. Pustejovsky, J. Castano, R. Ingria, R. Saur?, R. Gai-
zauskas, A. Setzer, and G. Katz. 2003. TimeML: 
Robust specification of event and temporal expres-
sions in text. In Proceedings of the AAAI Spring 
Symposium on New Directions in Question-
Answering. 
C. J. Rieger. 1974. Conceptual memory: A theory and 
computer program for processing and meaning 
content of natural language utterances. Stanford 
AIM-233. 
R. Saur?, R. Knippen, M. Verhagen and J. Puste-
jovsky. 2005. Evita: A Robust Event Recognizer 
for QA Systems. In Proceedings of HLT/EMNLP. 
M. Surdeanu, S. Harabagiu, J. Williams, and P. 
Aarseth. 2003. Using predicate-argument structures 
for information extraction. In Proceedings of the 
41th Annual Conference of the Association for 
Computational Linguistics (ACL-03), pages 8?15. 
Z. Vendler. 1967. Linguistics in Philosophy, Ithaca, 
Cornell University Press. 
45
